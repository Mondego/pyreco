__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# pylearn2 documentation build configuration file
# It is based on Theano documentation build
#
# This file is execfile()d with the current directory set to its containing dir.
#
# The contents of this file are pickled, so don't put values in the namespace
# that aren't pickleable (module imports are okay, they're removed automatically).
#
# All configuration values have a default value; values that are commented out
# serve to show the default value.

import sys, os

# If your extensions are in another directory, add it here. If the directory
# is relative to the documentation root, use os.path.abspath to make it
# absolute, like shown here.
#sys.path.append(os.path.abspath('some/directory'))

# General configuration
# ---------------------

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc', 'sphinx.ext.todo', 'numpydoc',
              'sphinx.ext.autosummary']  #, 'ext']

#Needed otherwise, there is many autosummary error done by numpydo:
#https://github.com/phn/pytpm/issues/3#issuecomment-12133978
numpydoc_show_class_members = False

todo_include_todos = True

try:
    from sphinx.ext import pngmath
    extensions.append('sphinx.ext.pngmath')
except ImportError:
    pass


# Add any paths that contain templates here, relative to this directory.
templates_path = ['.templates']

# The suffix of source filenames.
source_suffix = '.txt'

# The master toctree document.
master_doc = 'index'

# General substitutions.
project = 'Pylearn2'
copyright = '2011, LISA lab'

# The default replacements for |version| and |release|, also used in various
# other places throughout the built documents.
#
# The short X.Y version.
version = 'dev'
# The full version, including alpha/beta/rc tags.
release = 'dev'

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
today_fmt = '%B %d, %Y'

# List of documents that shouldn't be included in the build.
#unused_docs = []

# List of directories, relative to source directories, that shouldn't be searched
# for source files.
exclude_dirs = ['images', 'scripts', 'sandbox']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'


# Options for HTML output
# -----------------------

# The style sheet to use for HTML and HTML Help pages. A file of that name
# must exist either in Sphinx' static/ path, or in one of the custom paths
# given in html_static_path.
#html_style = 'default.css'
html_theme = 'solar'
html_theme_path = ["./themes"]

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (within the static path) to place at the top of
# the sidebar.
#html_logo = 'images/theano_logo-200x67.png'
#html_logo = 'images/theano_logo_allblue_200x46.png'

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = [] # '.static', 'images']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_use_modindex = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, the reST sources are included in the HTML build as _sources/<name>.
#html_copy_source = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# If nonempty, this is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = ''

# Output file base name for HTML help builder.
htmlhelp_basename = 'theanodoc'


# Options for LaTeX output
# ------------------------

# The paper size ('letter' or 'a4').
#latex_paper_size = 'letter'

# The font size ('10pt', '11pt' or '12pt').
latex_font_size = '11pt'

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, document class [howto/manual]).
latex_documents = [
  ('index', 'pylearn2.tex', 'Pylearn2 Documentation',
   'LISA lab, University of Montreal', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = 'images/snake_theta2-trans.png'
latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# Additional stuff for the LaTeX preamble.
#latex_preamble = ''

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_use_modindex = True

########NEW FILE########
__FILENAME__ = docgen

from collections import defaultdict
import inspect
import getopt
import os
import shutil
import sys


if __name__ == '__main__':

    throot = "/".join(sys.path[0].split("/")[:-2])

    options = defaultdict(bool)
    options.update(dict([x, y or True] for x, y in getopt.getopt(sys.argv[1:], 'o:', ['epydoc', 'rst', 'help', 'nopdf', 'test'])[0]))
    if options['--help']:
        print 'Usage: %s [OPTIONS]' % sys.argv[0]
        print '  -o <dir>: output the html files in the specified dir'
        print '  --rst: only compile the doc (requires sphinx)'
        print '  --nopdf: do not produce a PDF file from the doc, only HTML'
        print '  --help: this help'
        # --test: build the docs with warnings=errors to test them (exclusive)
        sys.exit(0)

    options['--all'] = not options['--rst']

    def mkdir(path):
        try:
            os.mkdir(path)
        except OSError:
            pass

    outdir = options['-o'] or (throot + '/html')
    mkdir(outdir)
    os.chdir(outdir)
    mkdir("doc")

    # Make sure the appropriate 'theano' directory is in the PYTHONPATH
    pythonpath = os.environ.get('PYTHONPATH', '')
    pythonpath = throot + ':' + pythonpath
    os.environ['PYTHONPATH'] = pythonpath

    if options['--test']:
        import sphinx
        sys.path[0:0] = [os.path.join(throot, 'doc')]
        out = sphinx.main(['', '-b' 'text', '-W',
                           '-E', os.path.join(throot, 'doc'), '.'])
        sys.exit(out)
    elif options['--all'] or options['--rst']:
        import sphinx
        sys.path[0:0] = [os.path.join(throot, 'doc')]
        sphinx.main(['', '-E', os.path.join(throot, 'doc'), '.'])

        if not options['--nopdf']:
            # Generate latex file in a temp directory
            import tempfile
            workdir = tempfile.mkdtemp()
            sphinx.main(['', '-E', '-b', 'latex',
                os.path.join(throot, 'doc'), workdir])
            # Compile to PDF
            os.chdir(workdir)
            os.system('make')
            try:
                shutil.copy(os.path.join(workdir, 'pylearn2.pdf'), outdir)
                os.chdir(outdir)
                shutil.rmtree(workdir)
            except OSError, e:
                print 'OSError:', e
            except IOError, e:
                print 'IOError:', e

########NEW FILE########
__FILENAME__ = autoencoder
import numpy
import pickle

class AutoEncoder:

    def __init__(self, nvis, nhid, iscale=0.1,
            activation_fn=numpy.tanh,
            params=None):

        self.nvis = nvis
        self.nhid = nhid
        self.activation_fn = activation_fn

        if params is None:
            self.W = iscale * numpy.random.randn(nvis, nhid)
            self.bias_vis = numpy.zeros(nvis)
            self.bias_hid = numpy.zeros(nhid)
        else:
            self.W = params[0]
            self.bias_vis = params[1]
            self.bias_hid = params[2]

    def __str__(self):
        rval  = '%s\n' % self.__class__.__name__
        rval += '\tnvis = %i\n' % self.nvis
        rval += '\tnhid = %i\n' % self.nhid
        rval += '\tactivation_fn = %s\n' % str(self.activation_fn)
        rval += '\tmean std(weights) = %.2f\n' % self.W.std(axis=0).mean()
        return rval

    def save(self, fname):
        fp = open(fname, 'w')
        pickle.dump([self.W, self.bias_vis, self.bias_hid], fp)
        fp.close()


if __name__ == '__main__':
    import os
    from StringIO import StringIO
    from pylearn2.config import yaml_parse

    example1 = """
        !obj:yaml_tutorial.autoencoder.AutoEncoder {
           "nvis": 784,
           "nhid": 100,
           "iscale": 0.2,
        }
    """
    stream = StringIO()
    stream.write(example1)
    stream.seek(0)
    print 'Example 1: building basic auto-encoder.'
    model = yaml_parse.load(stream)
    print model
    stream.close()

    example2 = """
        !obj:yaml_tutorial.autoencoder.AutoEncoder {
           "nvis": &nvis 100,
           "nhid": *nvis,
        }
    """
    stream = StringIO()
    stream.write(example2)
    stream.seek(0)
    print 'Example 2: anchors and references.'
    model = yaml_parse.load(stream)
    print model
    stream.close()

    example3 = """
        !obj:yaml_tutorial.autoencoder.AutoEncoder {
           "nvis": 784,
           "nhid": 100,
           "iscale": 1.0,
           "activation_fn": !import 'pylearn2.expr.nnet.sigmoid_numpy', 
        }
    """
    stream = StringIO()
    stream.write(example3)
    stream.seek(0)
    print 'Example 3: dynamic imports through !import.'
    model = yaml_parse.load(stream)
    model.save('example3_weights.pkl')
    print model
    stream.close()

    example4 = """
        !obj:yaml_tutorial.autoencoder.AutoEncoder {
           "nvis": 784,
           "nhid": 100,
           "params": !pkl: 'example3_weights.pkl',
        }
    """
    stream = StringIO()
    stream.write(example4)
    stream.seek(0)
    print 'Example 4: loading data with !pkl command.'
    model = yaml_parse.load(stream)
    print model
    stream.close()


########NEW FILE########
__FILENAME__ = base
"""
.. note::

    pylearn2.base is deprecated. It is now called pylearn2.blocks,
    since it was not actually the base of the library. pylearn2.base
    may be removed from the library on or after 2014-09-06.
"""
import warnings

warnings.warn("pylearn2.base is deprecated. It is now called pylearn2.blocks,"
        "since it was not actually the base of the library. pylearn2.base may"
        " be removed from the library on or after 2014-09-06.", stacklevel=2)

from pylearn2.blocks import Block
from pylearn2.blocks import StackedBlocks

########NEW FILE########
__FILENAME__ = blocks
"""
Feedforward processing objects. Similar to MLP layers, but specialized
for operation on design matrices rather than generic Spaces, and without
a concept of parameters.
"""
# Standard library imports
import warnings

# Third-party imports
import theano
from theano import tensor
try:
    from theano.sparse import SparseType
except ImportError:
    warnings.warn("Could not import theano.sparse.SparseType")
from theano.compile.mode import get_default_mode

theano.config.warn.sum_div_dimshuffle_bug = False

use_slow_rng = 0
if use_slow_rng:
    print 'WARNING: using SLOW rng'
    RandomStreams = tensor.shared_randomstreams.RandomStreams
else:
    import theano.sandbox.rng_mrg
    RandomStreams = theano.sandbox.rng_mrg.MRG_RandomStreams


class Block(object):
    """
    Basic building block that represents a simple transformation. By chaining
    Blocks together we can represent complex feed-forward transformations.
    """
    # TODO: Give this input and output spaces to make it different from a
    #       theano Op. Supporting CompositeSpace would allow more complicated
    #       structures than just chains.
    def __init__(self):
        self.fn = None
        self.cpu_only = False

    def __call__(self, inputs):
        """
        .. todo::

            WRITEME

        * What does this function do?
        * How should inputs be formatted? is it a single tensor, a list of
          tensors, a tuple of tensors?
        """
        raise NotImplementedError(str(type(self)) + 'does not implement ' +
                                  'Block.__call__')

    def function(self, name=None):
        """
        Returns a compiled theano function to compute a representation

        Parameters
        ----------
        name : string, optional
            name of the function
        """
        inputs = tensor.matrix()
        if self.cpu_only:
            return theano.function([inputs], self(inputs), name=name,
                                   mode=get_default_mode().excluding('gpu'))
        else:
            return theano.function([inputs], self(inputs), name=name)

    def perform(self, X):
        """
        .. todo::

            WRITEME
        """
        if self.fn is None:
            self.fn = self.function("perform")
        return self.fn(X)

    def inverse(self):
        """
        .. todo::

            WRITEME
        """
        raise NotImplementedError()

    def set_input_space(self, space):
        """
        .. todo::

            WRITEME
        """
        raise NotImplementedError(
                "%s does not implement set_input_space yet" % str(type(self)))

    def get_input_space(self):
        """
        .. todo::

            WRITEME
        """
        raise NotImplementedError(
                "%s does not implement get_input_space yet" % str(type(self)))

    def get_output_space(self):
        """
        .. todo::

            WRITEME
        """
        raise NotImplementedError(
                "%s does not implement get_output_space yet" % str(type(self)))


class StackedBlocks(Block):
    """
    A stack of Blocks, where the output of a block is the input of the next.

    Parameters
    ----------
    layers : list of Blocks
        The layers to be stacked, ordered from bottom (input) to top
        (output)
    """

    def __init__(self, layers):
        super(StackedBlocks, self).__init__()

        self._layers = layers
        # Do not duplicate the parameters if some are shared between layers
        self._params = set([p for l in self._layers for p in l._params])

    def layers(self):
        """
        .. todo::

            WRITEME
        """
        return list(self._layers)

    def __len__(self):
        """
        .. todo::

            WRITEME
        """
        return len(self._layers)

    def __call__(self, inputs):
        """
        Return the output representation of all layers, including the inputs.

        Parameters
        ----------
        inputs : tensor_like or list of tensor_likes
            Theano symbolic (or list thereof) representing the input
            minibatch(es) to be encoded. Assumed to be 2-tensors, with
            the first dimension indexing training examples and the
            second indexing data dimensions.

        Returns
        -------
        reconstructed : tensor_like or list of tensor_like
            A list of theano symbolic (or list thereof), each
            containing the representation at one level.
            The first element is the input.
        """
        # Build the hidden representation at each layer
        repr = [inputs]

        for layer in self._layers:
            outputs = layer(repr[-1])
            repr.append(outputs)

        return repr

    def function(self, name=None, repr_index=-1, sparse_input=False):
        """
        Compile a function computing representations on given layers.

        Parameters
        ----------
        name : string, optional
            name of the function
        repr_index : int, optional
            Index of the hidden representation to return.
            0 means the input, -1 the last output.
        sparse_input : bool, optional
            WRITEME

        Returns
        -------
        WRITEME
        """

        if sparse_input:
            inputs = SparseType('csr', dtype=theano.config.floatX)()
        else:
            inputs = tensor.matrix()

        return theano.function(
                [inputs],
                outputs=self(inputs)[repr_index],
                name=name)

    def concat(self, name=None, start_index=-1, end_index=None):
        """
        Compile a function concatenating representations on given layers.

        Parameters
        ----------
        name : string, optional
            name of the function
        start_index : int, optional
            Index of the hidden representation to start the concatenation.
            0 means the input, -1 the last output.
        end_index : int, optional
            Index of the hidden representation from which to stop
            the concatenation. We must have start_index < end_index.

        Returns
        -------
        WRITEME
        """
        inputs = tensor.matrix()
        return theano.function([inputs],
            outputs=tensor.concatenate(self(inputs)[start_index:end_index]),
            name=name)

    def append(self, layer):
        """
        Add a new layer on top of the last one

        Parameters
        ----------
        layer : WRITEME
        """
        self._layers.append(layer)
        self._params.update(layer._params)

    def get_input_space(self):
        """
        .. todo::

            WRITEME
        """
        return self._layers[0].get_input_space()

    def get_output_space(self):
        """
        .. todo::

            WRITEME
        """
        return self._layers[-1].get_output_space()

    def set_input_space(self, space):
        """
        .. todo::

            WRITEME
        """
        for layer in self._layers:
            layer.set_input_space(space)
            space = layer.get_output_space()

########NEW FILE########
__FILENAME__ = classifier
"""
.. note::

    pylearn2.classifier has been deprecated and will be removed from
    the library on or after Aug 24, 2014.
"""
import warnings

warnings.warn("pylearn2.classifier has been deprecated and will be removed "
        "from the library on or after Aug 24, 2014.")

from deprecated.classifier import Block
from deprecated.classifier import CumulativeProbabilitiesLayer
from deprecated.classifier import LogisticRegressionLayer
from deprecated.classifier import Model
from deprecated.classifier import VectorSpace
from deprecated.classifier import numpy
from deprecated.classifier import sharedX
from deprecated.classifier import tensor
from deprecated.classifier import theano

########NEW FILE########
__FILENAME__ = old_config
import yaml
import inspect
import types

global resolvers

def load(json_file_path):
    """ given a file path to a json file, returns the dictionary definde by the json file """

    f = open(json_file_path)
    lines = f.readlines()
    f.close()

    content = ''.join(lines)

    return yaml.load(content)

def get_field(d, key):
    try:
        rval = d[key]
    except:
        raise ValueError('Could not access "'+key+'" in \n'+str(d))
    return rval

def get_str(d, key):
    rval = get_field(d, key)

    if not isinstance(rval,str):
        raise TypeError('"'+key+'" entry is not a string in the following: \n'+str(d))

    return rval

def get_tag(d):
    return get_str(d, 'tag')

def resolve(d):
    """ given a dictionary d, returns the object described by the dictionary """

    tag = get_tag(d)

    try:
        resolver = resolvers[tag]
    except:
        raise TypeError('config does not know of any object type "'+tag+'"')

    return resolver(d)

def resolve_model(d):
    assert False

def resolve_dataset(d):
    import pylearn2.datasets.config
    return pylearn2.datasets.config.resolve(d)

def resolve_train_algorithm(d):
    assert False

resolvers = {
        'model'             : resolve_model,
        'dataset'           : resolve_dataset,
        'train_algorithm'   : resolve_train_algorithm
        }



########NEW FILE########
__FILENAME__ = test_yaml_parse
"""
Unit tests for ./yaml_parse.py
"""

import os
import numpy as np
import cPickle
import tempfile
from numpy.testing import assert_
from pylearn2.config.yaml_parse import load, load_path, initialize
from os import environ
from decimal import Decimal
import yaml
from pylearn2.models.mlp import MLP, Sigmoid

from pylearn2.models.rbm import GaussianBinaryRBM
from pylearn2.space import Conv2DSpace
from pylearn2.linear.conv2d import make_random_conv2D
from pylearn2.energy_functions.rbm_energy import grbm_type_1


def test_load_path():
    fd, fname = tempfile.mkstemp()
    with os.fdopen(fd, 'wb') as f:
        f.write("a: 23")
    loaded = load_path(fname)
    assert_(loaded['a'] == 23)
    os.remove(fname)


def test_obj():
    loaded = load("a: !obj:decimal.Decimal { value : '1.23' }")
    assert_(isinstance(loaded['a'], Decimal))


def test_floats():
    loaded = load("a: { a: -1.23, b: 1.23e-1 }")
    assert_(isinstance(loaded['a']['a'], float))
    assert_(isinstance(loaded['a']['b'], float))
    assert_((loaded['a']['a'] + 1.23) < 1e-3)
    assert_((loaded['a']['b'] - 1.23e-1) < 1e-3)


def test_import():
    loaded = load("a: !import 'decimal.Decimal'")
    assert_(loaded['a'] == Decimal)


def test_import_string():
    loaded = load("a: !import decimal.Decimal")
    assert_(loaded['a'] == Decimal)


def test_import_colon():
    loaded = load("a: !import:decimal.Decimal")
    assert_(loaded['a'] == Decimal)


def test_preproc_rhs():
    environ['TEST_VAR'] = '10'
    loaded = load('a: "${TEST_VAR}"')
    print "loaded['a'] is %s" % loaded['a']
    assert_(loaded['a'] == "10")
    del environ['TEST_VAR']


def test_preproc_pkl():
    fd, fname = tempfile.mkstemp()
    with os.fdopen(fd, 'wb') as f:
        d = ('a', 1)
        cPickle.dump(d, f)
    environ['TEST_VAR'] = fname
    loaded = load('a: !pkl: "${TEST_VAR}"')
    assert_(loaded['a'] == d)
    del environ['TEST_VAR']


def test_late_preproc_pkl():
    fd, fname = tempfile.mkstemp()
    with os.fdopen(fd, 'wb') as f:
        array = np.arange(10)
        np.save(f, array)
    environ['TEST_VAR'] = fname
    loaded = load('a: !obj:pylearn2.datasets.npy_npz.NpyDataset '
                  '{ file: "${TEST_VAR}"}\n')
    # Assert the unsubstituted TEST_VAR is in yaml_src
    assert_(loaded['a'].yaml_src.find("${TEST_VAR}") != -1)
    del environ['TEST_VAR']


def test_unpickle():
    fd, fname = tempfile.mkstemp()
    with os.fdopen(fd, 'wb') as f:
        d = {'a': 1, 'b': 2}
        cPickle.dump(d, f)
    loaded = load("{'a': !pkl: '%s'}" % fname)
    assert_(loaded['a'] == d)
    os.remove(fname)


def test_unpickle_key():
    fd, fname = tempfile.mkstemp()
    with os.fdopen(fd, 'wb') as f:
        d = ('a', 1)
        cPickle.dump(d, f)
    loaded = load("{!pkl: '%s': 50}" % fname)
    assert_(loaded.keys()[0] == d)
    assert_(loaded.values()[0] == 50)
    os.remove(fname)


def test_multi_constructor_obj():
    """
    Tests whether multi_constructor_obj throws an exception when
    the keys in mapping are None.
    """
    try:
        loaded = load("a: !obj:decimal.Decimal { 1 }")
    except TypeError as e:
        assert str(e) == "Received non string object (1) as key in mapping."
        pass
    except Exception, e:
        error_msg = "Got the unexpected error: %s" % (e)
        raise ValueError(error_msg)


def test_duplicate_keywords():
    """
    Tests whether there are doublicate keywords in the yaml
    """
    initialize()
    yamlfile = """{
            "model": !obj:pylearn2.models.mlp.MLP {
            "layers": [
                     !obj:pylearn2.models.mlp.Sigmoid {
                         "layer_name": 'h0',
                         "dim": 20,
                         "sparse_init": 15,
                     }],
            "nvis": 784,
            "nvis": 384,
        }
    }"""

    try:
        loaded = load(yamlfile)
    except yaml.constructor.ConstructorError, e:
        message = str(e)
        assert message.endswith("found duplicate key (nvis)")
        pass
    except Exception, e:
        error_msg = "Got the unexpected error: %s" % (e)
        raise TypeError(error_msg)


def test_duplicate_keywords_2():
    """
    Tests whether duplicate keywords as independent parameters works fine.
    """
    initialize()
    yamlfile = """{
             "model": !obj:pylearn2.models.rbm.GaussianBinaryRBM {

                 "vis_space" : &vis_space !obj:pylearn2.space.Conv2DSpace {
                    "shape" : [32,32],
                    "num_channels" : 3
                },
                "hid_space" : &hid_space !obj:pylearn2.space.Conv2DSpace {
                    "shape" : [27,27],
                    "num_channels" : 10
                },
                "transformer" :
                        !obj:pylearn2.linear.conv2d.make_random_conv2D {
                    "irange" : .05,
                    "input_space" : *vis_space,
                    "output_space" : *hid_space,
                    "kernel_shape" : [6,6],
                    "batch_size" : &batch_size 5
                },
                "energy_function_class" :
                     !obj:pylearn2.energy_functions.rbm_energy.grbm_type_1 {},
                "learn_sigma" : True,
                "init_sigma" : .3333,
                "init_bias_hid" : -2.,
                "mean_vis" : False,
                "sigma_lr_scale" : 1e-3

             }
    }"""

    loaded = load(yamlfile)

def test_parse_null_as_none():
    """
    Tests whether None may be passed via yaml kwarg null.
    """ 
    initialize()
    yamlfile = """{
             "model": !obj:pylearn2.models.autoencoder.Autoencoder {

                 "nvis" : 1024,
                 "nhid" : 64,
                 "act_enc" : Null,
                 "act_dec" : null

             }
    }"""

if __name__ == "__main__":
    test_multi_constructor_obj()
    test_duplicate_keywords()
    test_duplicate_keywords_2()
    test_unpickle_key()

########NEW FILE########
__FILENAME__ = yaml_parse
"""Support code for YAML parsing of experiment descriptions."""
import re
import yaml
from pylearn2.utils.call_check import checked_call
from pylearn2.utils import serial
from pylearn2.utils.string_utils import preprocess
from pylearn2.utils.string_utils import match
import logging
import warnings
import collections


is_initialized = False
additional_environ = None
logger = logging.getLogger(__name__)



def load(stream, overrides=None, environ=None, **kwargs):
    """
    Loads a YAML configuration from a string or file-like object.

    Parameters
    ----------
    stream : str or object
        Either a string containing valid YAML or a file-like object \
        supporting the .read() interface.
    overrides : dict, optional [DEPRECATED]
        A dictionary containing overrides to apply. The location of \
        the override is specified in the key as a dot-delimited path \
        to the desired parameter, e.g. "model.corruptor.corruption_level".
    environ : dict, optional
        A dictionary used for ${FOO} substitutions in addition to
        environment variables. If a key appears both in `os.environ`
        and this dictionary, the value in this dictionary is used.

    Returns
    -------
    graph : dict or object
        The dictionary or object (if the top-level element specified
        a Python object to instantiate).

    Notes
    -----
    Other keyword arguments are passed on to `yaml.load`.
    """
    global is_initialized
    global additional_environ
    if not is_initialized:
        initialize()
    additional_environ = environ

    if isinstance(stream, basestring):
        string = stream
    else:
        string = '\n'.join(stream.readlines())

    proxy_graph = yaml.load(string, **kwargs)

    if overrides is not None:
        warnings.warn("The 'overrides' keyword is deprecated and will "
                      "be removed on or after June 8, 2014.")
        handle_overrides(proxy_graph, overrides)
    return instantiate_all(proxy_graph)


def load_path(path, overrides=None, environ=None, **kwargs):
    """
    Convenience function for loading a YAML configuration from a file.

    Parameters
    ----------
    path : str
        The path to the file to load on disk.
    overrides : dict, optional
        A dictionary containing overrides to apply. The location of \
        the override is specified in the key as a dot-delimited path \
        to the desired parameter, e.g. "model.corruptor.corruption_level".
    environ : dict, optional
        A dictionary used for ${FOO} substitutions in addition to
        environment variables. If a key appears both in `os.environ`
        and this dictionary, the value in this dictionary is used.

    Returns
    -------
    graph : dict or object
        The dictionary or object (if the top-level element specified a \
        Python object to instantiate).

    Notes
    -----
    Other keyword arguments are passed on to `yaml.load`.
    """
    f = open(path, 'r')
    content = ''.join(f.readlines())
    f.close()

    if not isinstance(content, str):
        raise AssertionError("Expected content to be of type str, got " +
                             str(type(content)))

    return load(content, environ=environ, **kwargs)


def handle_overrides(graph, overrides):
    """
    Handle any overrides for this model configuration.

    Parameters
    ----------
    graph : dict or object
        A dictionary (or an ObjectProxy) containing the object graph \
        loaded from a YAML file.
    overrides : dict
        A dictionary containing overrides to apply. The location of \
        the override is specified in the key as a dot-delimited path \
        to the desired parameter, e.g. "model.corruptor.corruption_level".
    """
    for key in overrides:
        levels = key.split('.')
        part = graph
        for lvl in levels[:-1]:
            try:
                part = part[lvl]
            except KeyError:
                raise KeyError("'%s' override failed at '%s'", (key, lvl))
        try:
            part[levels[-1]] = overrides[key]
        except KeyError:
            raise KeyError("'%s' override failed at '%s'", (key, levels[-1]))


def instantiate_all(graph):
    """
    Instantiate all ObjectProxy objects in a nested hierarchy.

    Parameters
    ----------
    graph : dict or object
        A dictionary (or an ObjectProxy) containing the object graph \
        loaded from a YAML file.

    Returns
    -------
    graph : dict or object
        The dictionary or object resulting after the recursive instantiation.
    """

    def should_instantiate(obj):
        classes = (ObjectProxy, dict, list)
        return isinstance(obj, classes)

    if not isinstance(graph, list):
        for key in graph:
            if should_instantiate(graph[key]):
                graph[key] = instantiate_all(graph[key])
            if isinstance(graph[key], basestring):       # preprocess strings
                graph[key] = preprocess(graph[key], additional_environ)

        if hasattr(graph, 'keys'):
            for key in graph.keys():
                if should_instantiate(key):
                    new_key = instantiate_all(key)
                    graph[new_key] = graph[key]
                    del graph[key]

    if isinstance(graph, ObjectProxy):
        graph = graph.instantiate()

    if isinstance(graph, list):
        for i, elem in enumerate(graph):
            if should_instantiate(elem):
                graph[i] = instantiate_all(elem)

    return graph


class ObjectProxy(object):
    """
    Class used to delay instantiation of objects so that overrides can be
    applied.

    Parameters
    ----------
    cls : WRITEME
    kwds : WRITEME
    yaml_src : WRITEME
    """
    def __init__(self, cls, kwds, yaml_src):
        self.cls = cls
        self.kwds = kwds
        self.yaml_src = yaml_src
        self.instance = None

    def __setitem__(self, key, value):
        """
        .. todo::

            WRITEME
        """
        self.kwds[key] = value

    def __getitem__(self, key):
        """
        .. todo::

            WRITEME
        """
        return self.kwds[key]

    def __iter__(self):
        """
        .. todo::

            WRITEME
        """
        return self.kwds.__iter__()

    def keys(self):
        """
        .. todo::

            WRITEME
        """
        return list(self.kwds)

    def instantiate(self):
        """
        Instantiate this object with the supplied parameters in `self.kwds`,
        or if already instantiated, return the cached instance.
        """
        if self.instance is None:
            self.instance = checked_call(self.cls, self.kwds)
        #endif
        try:
            self.instance.yaml_src = self.yaml_src
        except AttributeError:
            pass
        return self.instance


def try_to_import(tag_suffix):
    """
    .. todo::

        WRITEME
    """
    components = tag_suffix.split('.')
    modulename = '.'.join(components[:-1])
    try:
        exec('import %s' % modulename)
    except ImportError, e:
        # We know it's an ImportError, but is it an ImportError related to
        # this path,
        # or did the module we're importing have an unrelated ImportError?
        # and yes, this test can still have false positives, feel free to
        # improve it
        pieces = modulename.split('.')
        str_e = str(e)
        found = True in [piece.find(str(e)) != -1 for piece in pieces]

        if found:
            # The yaml file is probably to blame.
            # Report the problem with the full module path from the YAML
            # file
            raise ImportError("Could not import %s; ImportError was %s" %
                              (modulename, str_e))
        else:

            pcomponents = components[:-1]
            assert len(pcomponents) >= 1
            j = 1
            while j <= len(pcomponents):
                modulename = '.'.join(pcomponents[:j])
                try:
                    exec('import %s' % modulename)
                except:
                    base_msg = 'Could not import %s' % modulename
                    if j > 1:
                        modulename = '.'.join(pcomponents[:j - 1])
                        base_msg += ' but could import %s' % modulename
                    raise ImportError(base_msg + '. Original exception: '
                                      + str(e))
                j += 1
    try:
        obj = eval(tag_suffix)
    except AttributeError, e:
        try:
            # Try to figure out what the wrong field name was
            # If we fail to do it, just fall back to giving the usual
            # attribute error
            pieces = tag_suffix.split('.')
            module = '.'.join(pieces[:-1])
            field = pieces[-1]
            candidates = dir(eval(module))

            msg = ('Could not evaluate %s. ' % tag_suffix +
                   'Did you mean ' + match(field, candidates) + '? ' +
                   'Original error was ' + str(e))

        except:
            warnings.warn("Attempt to decipher AttributeError failed")
            raise AttributeError('Could not evaluate %s. ' % tag_suffix +
                                 'Original error was ' + str(e))
        raise AttributeError(msg)
    return obj


def initialize():
    """
    Initialize the configuration system by installing YAML handlers.
    Automatically done on first call to load() specified in this file.
    """
    global is_initialized

    # Add the custom multi-constructor
    yaml.add_multi_constructor('!obj:', multi_constructor_obj)
    yaml.add_multi_constructor('!pkl:', multi_constructor_pkl)
    yaml.add_multi_constructor('!import:', multi_constructor_import)

    yaml.add_constructor('!import', constructor_import)
    yaml.add_constructor("!float", constructor_float)

    is_initialized = True


###############################################################################
# Callbacks used by PyYAML

def multi_constructor_obj(loader, tag_suffix, node):
    """
    Callback used by PyYAML when a "!obj:" tag is encountered.

    See PyYAML documentation for details on the call signature.
    """
    yaml_src = yaml.serialize(node)
    construct_mapping(node)
    mapping = loader.construct_mapping(node)

    assert hasattr(mapping, 'keys')
    assert hasattr(mapping, 'values')

    for key in mapping.keys():
        if not isinstance(key, basestring):
            message = "Received non string object (%s) as " \
                      "key in mapping." % str(key)
            raise TypeError(message)

    if '.' not in tag_suffix:
        classname = tag_suffix
        rval = ObjectProxy(classname, mapping, yaml_src)
    else:
        classname = try_to_import(tag_suffix)
        rval = ObjectProxy(classname, mapping, yaml_src)

    return rval


def multi_constructor_pkl(loader, tag_suffix, node):
    """
    Callback used by PyYAML when a "!pkl:" tag is encountered.
    """
    global additional_environ
    if tag_suffix != "" and tag_suffix != u"":
        raise AssertionError('Expected tag_suffix to be "" but it is "'
                             + tag_suffix +
                             '": Put space between !pkl: and the filename.')

    mapping = loader.construct_yaml_str(node)
    rval = ObjectProxy(None, {}, yaml.serialize(node))
    rval.instance = serial.load(preprocess(mapping, additional_environ))

    return rval


def multi_constructor_import(loader, tag_suffix, node):
    """
    Callback used by PyYAML when a "!import:" tag is encountered.
    """
    if '.' not in tag_suffix:
        raise yaml.YAMLError("!import: tag suffix contains no '.'")
    return try_to_import(tag_suffix)


def constructor_import(loader, node):
    """
    Callback used by PyYAML when a "!import <str>" tag is encountered.
    This tag exects a (quoted) string as argument.
    """
    value = loader.construct_scalar(node)
    if '.' not in value:
        raise yaml.YAMLError("import tag suffix contains no '.'")
    return try_to_import(value)


def constructor_float(loader, node):
    """
    Callback used by PyYAML when a "!float <str>" tag is encountered.
    This tag exects a (quoted) string as argument.
    """
    value = loader.construct_scalar(node)
    return float(value)


def construct_mapping(node, deep=False):
    # This is a modified version of yaml.BaseConstructor.construct_mapping
    # in which a repeated key raises a ConstructorError
    if not isinstance(node, yaml.nodes.MappingNode):
        const = yaml.constructor
        message = "expected a mapping node, but found"
        raise const.ConstructorError(None, None,
                                     "%s %s " % (message, node.id),
                                     node.start_mark)
    mapping = {}
    constructor = yaml.constructor.BaseConstructor()
    for key_node, value_node in node.value:
        key = constructor.construct_object(key_node, deep=False)
        try:
            hash(key)
        except TypeError, exc:
            const = yaml.constructor
            raise const.ConstructorError("while constructing a mapping",
                                         node.start_mark,
                                         "found unacceptable key (%s)" % exc,
                                         key_node.start_mark)
        if key in mapping:
            const = yaml.constructor
            raise const.ConstructorError("while constructing a mapping",
                                         node.start_mark,
                                         "found duplicate key (%s)" % key)
        value = constructor.construct_object(value_node, deep=False)
        mapping[key] = value
    return mapping

if __name__ == "__main__":
    initialize()
    # Demonstration of how to specify objects, reference them
    # later in the configuration, etc.
    yamlfile = """{
        "corruptor" : !obj:pylearn2.corruption.GaussianCorruptor &corr {
            "corruption_level" : 0.9
        },
        "dae" : !obj:pylearn2.models.autoencoder.DenoisingAutoencoder {
            "nhid" : 20,
            "nvis" : 30,
            "act_enc" : null,
            "act_dec" : null,
            "tied_weights" : true,
            # we could have also just put the corruptor definition here
            "corruptor" : *corr
        }
    }"""
    # yaml.load can take a string or a file object
    loaded = yaml.load(yamlfile)
    logger.info(loaded)
    # These two things should be the same object
    assert loaded['corruptor'] is loaded['dae'].corruptor

########NEW FILE########
__FILENAME__ = corruption
"""
Corruptor classes: classes that encapsulate the noise process for the DAE
training criterion.
"""
# Third-party imports
import numpy
import theano
from theano import tensor
T = tensor
from pylearn2.utils.rng import make_np_rng

# Shortcuts
theano.config.warn.sum_div_dimshuffle_bug = False

if 0:
    print 'WARNING: using SLOW rng'
    RandomStreams = tensor.shared_randomstreams.RandomStreams
else:
    import theano.sandbox.rng_mrg
    RandomStreams = theano.sandbox.rng_mrg.MRG_RandomStreams

from pylearn2.expr.activations import rescaled_softmax

class Corruptor(object):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    corruption_level : float
        Some measure of the amount of corruption to do. What this means
        will be implementation specific.
    rng : RandomState object or seed, optional
        NumPy random number generator object (or seed for creating one)
        used to initialize a `RandomStreams`.
    """

    def __init__(self, corruption_level, rng=2001):
        # The default rng should be build in a deterministic way
        rng = make_np_rng(rng, which_method=['randn', 'randint'])
        seed = int(rng.randint(2 ** 30))
        self.s_rng = RandomStreams(seed)
        self.corruption_level = corruption_level

    def __call__(self, inputs):
        """
        (Symbolically) corrupt the inputs with a noise process.

        Parameters
        ----------
        inputs : tensor_like, or list of tensor_likes
            Theano symbolic(s) representing a (list of) (mini)batch of
            inputs to be corrupted, with the first dimension indexing
            training examples and the second indexing data dimensions.

        Returns
        -------
        corrupted : tensor_like, or list of tensor_likes
            Theano symbolic(s) representing the corresponding corrupted
            inputs.
        """
        if isinstance(inputs, tensor.Variable):
            return self._corrupt(inputs)
        else:
            return [self._corrupt(inp) for inp in inputs]

    def _corrupt(self, x):
        """
        Corrupts a single tensor_like object.

        Parameters
        ----------
        x : tensor_like
            Theano symbolic representing a (mini)batch of inputs to be
            corrupted, with the first dimension indexing training
            examples and the second indexing data dimensions.

        Returns
        -------
        corrupted : tensor_like
            Theano symbolic representing the corresponding corrupted input.

        Notes
        -----
        This is the method that all subclasses should implement. The logic in
        Corruptor.__call__ handles mapping over multiple tensor_like inputs.
        """
        raise NotImplementedError()

    def corruption_free_energy(self, corrupted_X, X):
        """
        .. todo::

            WRITEME
        """
        raise NotImplementedError()


class DummyCorruptor(Corruptor):
    """
    .. todo::

        WRITEME
    """

    def __call__(self, inputs):
        """
        .. todo::

            WRITEME
        """
        return inputs


class BinomialCorruptor(Corruptor):
    """
    A binomial corruptor that sets inputs to 0 with probability
    0 < `corruption_level` < 1.
    """

    def _corrupt(self, x):
        """
        Corrupts a single tensor_like object.

        Parameters
        ----------
        x : tensor_like
            Theano symbolic representing a (mini)batch of inputs to be
            corrupted, with the first dimension indexing training
            examples and the second indexing data dimensions.

        Returns
        -------
        corrupted : tensor_like
            Theano symbolic representing the corresponding corrupted input.
        """
        return self.s_rng.binomial(
            size=x.shape,
            n=1,
            p=1 - self.corruption_level,
            dtype=theano.config.floatX
        ) * x


class DropoutCorruptor(BinomialCorruptor):
    """
    Sets inputs to 0 with probability of corruption_level and then
    divides by (1 - corruption_level) to keep expected activation
    constant.
    """

    def _corrupt(self, x):
        """
        Corrupts a single tensor_like object.

        Parameters
        ----------
        x : tensor_like
            Theano symbolic representing a (mini)batch of inputs to be
            corrupted, with the first dimension indexing training
            examples and the second indexing data dimensions.

        Returns
        -------
        corrupted : tensor_like
            Theano symbolic representing the corresponding corrupted input.
        """
        # for stability
        if self.corruption_level < 1e-5:
            return x

        dropped = super(DropoutCorruptor, self)._corrupt(x)
        return 1.0 / (1.0 - self.corruption_level) * dropped


class GaussianCorruptor(Corruptor):
    """
    A Gaussian corruptor transforms inputs by adding zero mean isotropic
    Gaussian noise.

    Parameters
    ----------
    stdev : WRITEME
    rng : WRITEME
    """

    def __init__(self, stdev, rng=2001):
        super(GaussianCorruptor, self).__init__(corruption_level=stdev,
                                                rng=rng)

    def _corrupt(self, x):
        """
        Corrupts a single tensor_like object.

        Parameters
        ----------
        x : tensor_like
            Theano symbolic representing a (mini)batch of inputs to be
            corrupted, with the first dimension indexing training
            examples and the second indexing data dimensions.

        Returns
        -------
        corrupted : tensor_like
            Theano symbolic representing the corresponding corrupted input.
        """
        noise = self.s_rng.normal(
            size=x.shape,
            avg=0.,
            std=self.corruption_level,
            dtype=theano.config.floatX
        )

        return noise + x

    def corruption_free_energy(self, corrupted_X, X):
        """
        .. todo::

            WRITEME
        """
        axis = range(1, len(X.type.broadcastable))

        rval = (T.sum(T.sqr(corrupted_X - X), axis=axis) /
                (2. * (self.corruption_level ** 2.)))
        assert len(rval.type.broadcastable) == 1
        return rval


class SaltPepperCorruptor(Corruptor):
    """
    Corrupts the input with salt and pepper noise.

    Sets some elements of the tensor to 0 or 1. Only really makes sense
    to use on binary valued matrices.
    """
    def _corrupt(self, x):
        """
        Corrupts a single tensor_like object.

        Parameters
        ----------
        x : tensor_like
            Theano symbolic representing a (mini)batch of inputs to be
            corrupted, with the first dimension indexing training
            examples and the second indexing data dimensions.

        Returns
        -------
        corrupted : tensor_like
            Theano symbolic representing the corresponding corrupted input.
        """
        a = self.s_rng.binomial(
            size=x.shape,
            p=(1 - self.corruption_level),
            dtype=theano.config.floatX
        )

        b = self.s_rng.binomial(
            size=x.shape,
            p=0.5,
            dtype=theano.config.floatX
        )

        c = T.eq(a, 0) * b
        return x * a + c


class OneHotCorruptor(Corruptor):
    """
    Corrupts a one-hot vector by changing active element with some
    probability.
    """
    def _corrupt(self, x):
        """
        Corrupts a single tensor_like object.

        Parameters
        ----------
        x : tensor_like
            Theano symbolic representing a (mini)batch of inputs to be
            corrupted, with the first dimension indexing training
            examples and the second indexing data dimensions.

        Returns
        -------
        corrupted : tensor_like
            Theano symbolic representing the corresponding corrupted input.
        """
        num_examples = x.shape[0]
        num_classes = x.shape[1]

        keep_mask = T.addbroadcast(
            self.s_rng.binomial(
                size=(num_examples, 1),
                p=1 - self.corruption_level,
                dtype='int8'
            ),
            1
        )

        # generate random one-hot matrix
        pvals = T.alloc(1.0 / num_classes, num_classes)
        one_hot = self.s_rng.multinomial(size=(num_examples,), pvals=pvals)

        return keep_mask * x + (1 - keep_mask) * one_hot

class SmoothOneHotCorruptor(Corruptor):
    """
    Corrupts a one-hot vector in a way that preserves some information.

    This adds Gaussian noise to a vector and then computes the softmax.
    """

    def _corrupt(self, x):
        """
        Corrupts a single tensor_like object.

        Parameters
        ----------
        x : tensor_like
            Theano symbolic representing a (mini)batch of inputs to be
            corrupted, with the first dimension indexing training
            examples and the second indexing data dimensions.

        Returns
        -------
        corrupted : tensor_like
            Theano symbolic representing the corresponding corrupted input.
        """
        noise = self.s_rng.normal(
            size=x.shape,
            avg=0.,
            std=self.corruption_level,
            dtype=theano.config.floatX
        )

        return rescaled_softmax(x + noise)


class BinomialSampler(Corruptor):
    """
    .. todo::

        WRITEME
    """

    def __init__(self, *args, **kwargs):
        # pass up a 0 because corruption_level is not relevant here
        super(BinomialSampler, self).__init__(0, *args, **kwargs)

    def _corrupt(self, x):
        """
        Corrupts a single tensor_like object.

        Parameters
        ----------
        x : tensor_like
            Theano symbolic representing a (mini)batch of inputs to be
            corrupted, with the first dimension indexing training
            examples and the second indexing data dimensions.

        Returns
        -------
        corrupted : tensor_like
            Theano symbolic representing the corresponding corrupted input.
        """
        return self.s_rng.binomial(size=x.shape, p=x,
                                   dtype=theano.config.floatX)


class MultinomialSampler(Corruptor):
    """
    .. todo::

        WRITEME
    """

    def __init__(self, *args, **kwargs):
        # corruption_level isn't relevant here
        super(MultinomialSampler, self).__init__(0, *args, **kwargs)

    def _corrupt(self, x):
        """
        Treats each row in matrix as a multinomial trial.

        Parameters
        ----------
        x : tensor_like
            x must be a matrix where all elements are non-negative
            (with at least one non-zero element)
        Returns
        -------
        y : tensor_like
            y will have the same shape as x. Each row in y will be a
            one hot vector, and can be viewed as the outcome of the
            multinomial trial defined by the probabilities of that row
            in x.
        """
        normalized = x / x.sum(axis=1, keepdims=True)
        return self.s_rng.multinomial(pvals=normalized, dtype=theano.config.floatX)


class ComposedCorruptor(Corruptor):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    corruptors : list of Corruptor objects
        The corruptors are applied in reverse order. This matches the
        typical function application notation. Thus
        `ComposedCorruptor(a, b)._corrupt(X)` is the same as `a(b(X))`.

    Notes
    -----
    Does NOT call Corruptor.__init__, so does not contain all of the
    standard fields for Corruptors.
    """

    def __init__(self, *corruptors):
        # pass up the 0 for corruption_level (not relevant here)
        assert len(corruptors) >= 1
        self._corruptors = corruptors

    def _corrupt(self, x):
        """
        Corrupts a single tensor_like object.

        Parameters
        ----------
        x : tensor_like
            Theano symbolic representing a (mini)batch of inputs to be
            corrupted, with the first dimension indexing training
            examples and the second indexing data dimensions.

        Returns
        -------
        corrupted : tensor_like
            Theano symbolic representing the corresponding corrupted input.
        """
        result = x
        for c in reversed(self._corruptors):
            result = c(result)
        return result


##################################################
def get(str):
    """ Evaluate str into a corruptor object, if it exists """
    obj = globals()[str]
    if issubclass(obj, Corruptor):
        return obj
    else:
        raise NameError(str)

########NEW FILE########
__FILENAME__ = autoencoder
"""
.. todo::

    WRITEME
"""
from theano import tensor
import theano.sparse
from pylearn2.costs.cost import Cost, DefaultDataSpecsMixin
from theano.tensor.shared_randomstreams import RandomStreams


class GSNFriendlyCost(DefaultDataSpecsMixin, Cost):
    """
    .. todo::

        WRITEME
    """

    @staticmethod
    def cost(target, output):
        """
        .. todo::

            WRITEME
        """
        raise NotImplementedError

    def expr(self, model, data, *args, **kwargs):
        """
        .. todo::

            WRITEME
        """
        self.get_data_specs(model)[0].validate(data)
        X = data
        return self.cost(X, model.reconstruct(X))


class MeanSquaredReconstructionError(GSNFriendlyCost):
    """
    .. todo::

        WRITEME
    """

    @staticmethod
    def cost(a, b):
        """
        .. todo::

            WRITEME
        """
        return ((a - b) ** 2).sum(axis=1).mean()

class MeanBinaryCrossEntropy(GSNFriendlyCost):
    """
    .. todo::

        WRITEME
    """

    @staticmethod
    def cost(target, output):
        """
        .. todo::

            WRITEME
        """
        return tensor.nnet.binary_crossentropy(output, target).sum(axis=1).mean()

class SampledMeanBinaryCrossEntropy(DefaultDataSpecsMixin, Cost):
    """
    .. todo::

        WRITEME properly

    CE cost that goes with sparse autoencoder with L1 regularization on activations

    For theory:
    Y. Dauphin, X. Glorot, Y. Bengio. ICML2011
    Large-Scale Learning of Embeddings with Reconstruction Sampling

    Parameters
    ----------
    L1 : WRITEME
    ratio : WRITEME
    """

    def __init__(self, L1, ratio):
        self.random_stream = RandomStreams(seed=1)
        self.L1 = L1
        self.one_ratio = ratio

    def expr(self, model, data, ** kwargs):
        """
        .. todo::

            WRITEME
        """
        self.get_data_specs(model)[0].validate(data)
        X = data
        # X is theano sparse
        X_dense = theano.sparse.dense_from_sparse(X)
        noise = self.random_stream.binomial(size=X_dense.shape, n=1,
                                            prob=self.one_ratio, ndim=None)

        # a random pattern that indicates to reconstruct all the 1s and some of the 0s in X
        P = noise + X_dense
        P = theano.tensor.switch(P>0, 1, 0)
        P = tensor.cast(P, theano.config.floatX)

        # L1 penalty on activations
        reg_units = theano.tensor.abs_(model.encode(X)).sum(axis=1).mean()

        # penalty on weights, optional
        # params = model.get_params()
        # W = params[2]

        # there is a numerical problem when using
        # tensor.log(1 - model.reconstruct(X, P))
        # Pascal fixed it.
        before_activation = model.reconstruct_without_dec_acti(X, P)

        cost = ( 1 * X_dense *
                 tensor.log(tensor.log(1 + tensor.exp(-1 * before_activation))) +
                 (1 - X_dense) *
                 tensor.log(1 + tensor.log(1 + tensor.exp(before_activation)))
               )

        cost = (cost * P).sum(axis=1).mean()

        cost = cost + self.L1 * reg_units

        return cost



class SampledMeanSquaredReconstructionError(MeanSquaredReconstructionError):
    """
    mse cost that goes with sparse autoencoder with L1 regularization on activations

    For theory:
    Y. Dauphin, X. Glorot, Y. Bengio. ICML2011
    Large-Scale Learning of Embeddings with Reconstruction Sampling

    Parameters
    ----------
    L1 : WRITEME
    ratio : WRITEME
    """

    def __init__(self, L1, ratio):
        self.random_stream = RandomStreams(seed=1)
        self.L1 = L1
        self.ratio = ratio

    def expr(self, model, data, ** kwargs):
        """
        .. todo::

            WRITEME
        """
        self.get_data_specs(model)[0].validate(data)
        X = data
        # X is theano sparse
        X_dense=theano.sparse.dense_from_sparse(X)
        noise = self.random_stream.binomial(size=X_dense.shape, n=1, prob=self.ratio, ndim=None)

        # a random pattern that indicates to reconstruct all the 1s and some of the 0s in X
        P = noise + X_dense
        P = theano.tensor.switch(P>0, 1, 0)
        P = tensor.cast(P, theano.config.floatX)

        # L1 penalty on activations
        L1_units = theano.tensor.abs_(model.encode(X)).sum(axis=1).mean()

        # penalty on weights, optional
        #params = model.get_params()
        #W = params[2]
        #L1_weights = theano.tensor.abs_(W).sum()

        cost = ((model.reconstruct(X, P) - X_dense) ** 2)

        cost = (cost * P).sum(axis=1).mean()

        cost = cost + self.L1 * L1_units

        return cost


#class MeanBinaryCrossEntropyTanh(Cost):
#     def expr(self, model, data):
#        self.get_data_specs(model)[0].validate(data)
#        X = data
#        X = (X + 1) / 2.
#        return (
#            tensor.xlogx.xlogx(model.reconstruct(X)) +
#            tensor.xlogx.xlogx(1 - model.reconstruct(X))
#        ).sum(axis=1).mean()
#
#    def get_data_specs(self, model):
#        return (model.get_input_space(), model.get_input_source())


class SparseActivation(DefaultDataSpecsMixin, Cost):
    """
    Autoencoder sparse activation cost.
    
    Regularize on KL divergence from desired average activation of each
    hidden unit as described in Andrew Ng's CS294A Lecture Notes. See
    http://www.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf.

    Parameters
    ----------
    coeff : float
        Coefficient for this regularization term in the objective
        function.
    p : float
        Desired average activation of each hidden unit.
    """
    def __init__(self, coeff, p):
        self.coeff = coeff
        self.p = p

    def expr(self, model, data, **kwargs):
        """
        Calculate regularization penalty.
        """
        X = data
        p = self.p
        p_hat = tensor.abs_(model.encode(X)).mean(axis=0)
        kl = p * tensor.log(p / p_hat) + (1 - p) * \
            tensor.log((1 - p) / (1 - p_hat))
        penalty = self.coeff * kl.sum()
        penalty.name = 'sparse_activation_penalty'
        return penalty

########NEW FILE########
__FILENAME__ = cost
"""
Classes representing cost functions.

Currently, these are primarily used to specify the objective function for
the SGD and BGD training algorithms.
"""

import functools
import logging
import warnings
from itertools import izip

import theano.tensor as T
from theano.compat.python2x import OrderedDict

from pylearn2.utils import safe_zip
from pylearn2.utils import safe_union
from pylearn2.space import CompositeSpace, NullSpace
from pylearn2.utils.data_specs import DataSpecsMapping


logger = logging.getLogger(__name__)

class Cost(object):
    """
    Represents an objective function to be minimized by some
    `TrainingAlgorithm`.

    Notes
    -----
    While functions may be represented just as theano graphs,
    this class allows us to add extra functionality. The
    `get_gradients` methods allows us to use a method other
    than `theano.tensor.grad` to compute the gradient of the
    cost function. This enables using approximate gradients
    of cost functions that are not differentiable or whose
    true gradient is computationally intractable. Additionally,
    the get_monitoring_channels method allows monitoring of
    quantities that are useful when training with a given
    objective function (such as termination criteria that are
    usually used with the function).
    """

    # TODO: remove this when it is no longer necessary, it should be
    # mostly phased out due to the new data_specs interface.
    # If True, the data argument to expr and get_gradients must be a
    # (X, Y) pair, and Y cannot be None.
    supervised = False

    def expr(self, model, data, ** kwargs):
        """
        Returns a theano expression for the cost function.

        Returns a symbolic expression for a cost function applied to the
        minibatch of data.
        Optionally, may return None. This represents that the cost function
        is intractable but may be optimized via the get_gradients method.

        Parameters
        ----------
        model : a pylearn2 Model instance
        data : a batch in cost.get_data_specs() form
        kwargs : dict
            Optional extra arguments. Not used by the base class.
        """
        raise NotImplementedError(str(type(self)) + " does not implement "
                                  "expr.")

    def get_gradients(self, model, data, ** kwargs):
        """
        Provides the gradients of the cost function with respect to the model
        parameters.

        These are not necessarily those obtained by theano.tensor.grad
        --you may wish to use approximate or even intentionally incorrect
        gradients in some cases.

        Parameters
        ----------
        model : a pylearn2 Model instance
        data : a batch in cost.get_data_specs() form
        kwargs : dict
            Optional extra arguments, not used by the base class.

        Returns
        -------
        gradients : OrderedDict
            a dictionary mapping from the model's parameters
            to their gradients
            The default implementation is to compute the gradients
            using T.grad applied to the value returned by expr.
            However, subclasses may return other values for the gradient.
            For example, an intractable cost may return a sampling-based
            approximation to its gradient.
        updates : OrderedDict
            a dictionary mapping shared variables to updates that must
            be applied to them each time these gradients are computed.
            This is to facilitate computation of sampling-based approximate
            gradients.
            The parameters should never appear in the updates dictionary.
            This would imply that computing their gradient changes
            their value, thus making the gradient value outdated.
        """

        try:
            cost = self.expr(model=model, data=data, **kwargs)
        except TypeError, e:
            # If anybody knows how to add type(self) to the exception message
            # but still preserve the stack trace, please do so
            # The current code does neither
            e.message += " while calling " + str(type(self)) + ".expr"
            logger.error(type(self))
            logger.error(e.message)
            raise e

        if cost is None:
            raise NotImplementedError(str(type(self)) +
                                      " represents an intractable cost and "
                                      "does not provide a gradient "
                                      "approximation scheme.")

        params = list(model.get_params())

        grads = T.grad(cost, params, disconnected_inputs='ignore')

        gradients = OrderedDict(izip(params, grads))

        updates = OrderedDict()

        return gradients, updates

    def get_monitoring_channels(self, model, data, **kwargs):
        """
        .. todo::

            WRITEME

        .. todo::

            how do you do prereqs in this setup? (I think PL changed
            it, not sure if there still is a way in this context)

        Returns a dictionary mapping channel names to expressions for
        channel values.

        Parameters
        ----------
        model : Model
            the model to use to compute the monitoring channels
        data : batch
            (a member of self.get_data_specs()[0])
            symbolic expressions for the monitoring data
        kwargs : dict
            used so that custom algorithms can use extra variables
            for monitoring.

        Returns
        -------
        rval : dict
            Maps channels names to expressions for channel values.
        """
        self.get_data_specs(model)[0].validate(data)
        return OrderedDict()

    def get_fixed_var_descr(self, model, data):
        """
        Subclasses should override this if they need variables held
        constant across multiple updates to a minibatch.

        TrainingAlgorithms that do multiple updates to a minibatch should
        respect this. See the FixedVarDescr class for details.

        Parameters
        ----------
        model : Model
        data : theano.gof.Variable or tuple
            A valid member of the Space used to train `model` with this
            cost.

        Returns
        -------
        fixed_var_descr : FixedVarDescr
            A description of how to hold the necessary variables constant
        """
        self.get_data_specs(model)[0].validate(data)
        fixed_var_descr = FixedVarDescr()
        return fixed_var_descr

    def get_data_specs(self, model):
        """
        Returns a specification of the Space the data should lie in and
        its source (what part of the dataset it should come from).

        Parameters
        ----------
        model : Model
            The model to train with this cost

        Returns
        -------
        data_specs : tuple
            The tuple should be of length two.
            The first element of the tuple should be a Space (possibly a
            CompositeSpace) describing how to format the data.
            The second element of the tuple describes the source of the
            data. It probably should be a string or nested tuple of strings.

        See Also
        --------
        For many common cases, rather than implementing this method
        yourself, you probably want
        to just inherit from `DefaultDataSpecsMixin` or NullDataSpecsMixin.

        Notes
        -----
        .. todo

            figure out return format for sure. PL seems to have documented
            this method incorrectly.

        """
        raise NotImplementedError(str(type(self)) + " does not implement " +
                                  "get_data_specs.")

class SumOfCosts(Cost):
    """
    Combines multiple costs by summing them.

    Parameters
    ----------
    costs : list
        List of Cost objects or (coeff, Cost) pairs
    """

    def __init__(self, costs):
        """
        Initialize the SumOfCosts object and make sure that the list of costs
        contains only Cost instances.

        Parameters
        ----------
        costs : list
            List of Cost objects or (coeff, Cost) pairs
        """
        assert isinstance(costs, list)
        assert len(costs) > 0

        self.costs = []
        self.coeffs = []

        for cost in costs:
            if isinstance(cost, (list, tuple)):
                coeff, cost = cost
            else:
                coeff = 1.
            self.coeffs.append(coeff)
            self.costs.append(cost)

            if not isinstance(cost, Cost):
                raise ValueError("one of the costs is not "
                                 "Cost instance")

        # TODO: remove this when it is no longer necessary
        self.supervised = any([cost_.supervised for cost_ in self.costs])

    def expr(self, model, data, ** kwargs):
        """
        Returns the sum of the costs the SumOfCosts instance was given at
        initialization.

        Parameters
        ----------
        model : pylearn2.models.model.Model
            the model for which we want to calculate the sum of costs
        data : flat tuple of tensor_like variables.
            data has to follow the format defined by self.get_data_specs(),
            but this format will always be a flat tuple.
        """
        self.get_data_specs(model)[0].validate(data)
        composite_specs, mapping = self.get_composite_specs_and_mapping(model)
        nested_data = mapping.nest(data)
        costs = []
        for cost, cost_data in safe_zip(self.costs, nested_data):
            costs.append(cost.expr(model, cost_data, **kwargs))
        assert len(costs) > 0

        if any([cost is None for cost in costs]):
            sum_of_costs = None
        else:
            costs = [coeff * cost
                     for coeff, cost in safe_zip(self.coeffs, costs)]
            assert len(costs) > 0
            sum_of_costs = reduce(lambda x, y: x + y, costs)

        return sum_of_costs

    def get_composite_data_specs(self, model):
        """
        Build and return a composite data_specs of all costs.

        The returned space is a CompositeSpace, where the components are
        the spaces of each of self.costs, in the same order. The returned
        source is a tuple of the corresponding sources.

        Parameters
        ----------
        model : pylearn2.models.Model
        """
        spaces = []
        sources = []
        for cost in self.costs:
            space, source = cost.get_data_specs(model)
            spaces.append(space)
            sources.append(source)

        # Build composite space representing all inputs
        composite_space = CompositeSpace(spaces)
        sources = tuple(sources)
        return (composite_space, sources)

    def get_composite_specs_and_mapping(self, model):
        """
        Build the composite data_specs and a mapping to flatten it, return both

        Build the composite data_specs described in `get_composite_specs`, and
        build a DataSpecsMapping that can convert between it and a flat
        equivalent version. In particular, it helps building a flat data_specs
        to request data, and nesting this data back to the composite
        data_specs, so it can be dispatched among the different sub-costs.

        Parameters
        ----------
        model : pylearn2.models.Model

        Notes
        -----
        This is a helper function used by `get_data_specs` and `get_gradients`,
        and possibly other methods.
        """
        composite_space, sources = self.get_composite_data_specs(model)
        mapping = DataSpecsMapping((composite_space, sources))
        return (composite_space, sources), mapping

    def get_data_specs(self, model):
        """
        Get a flat data_specs containing all information for all sub-costs.

        Parameters
        ----------
        model : pylearn2.models.Model
            TODO WRITEME

        Notes
        -----
        This data_specs should be non-redundant. It is built by flattening
        the composite data_specs returned by `get_composite_specs`.

        This is the format that SumOfCosts will request its data in. Then,
        this flat data tuple will be nested into the composite data_specs,
        in order to dispatch it among the different sub-costs.
        """
        composite_specs, mapping = self.get_composite_specs_and_mapping(model)
        composite_space, sources = composite_specs
        flat_composite_space = mapping.flatten(composite_space)
        flat_sources = mapping.flatten(sources)
        data_specs = (flat_composite_space, flat_sources)
        return data_specs

    @functools.wraps(Cost.get_gradients)
    def get_gradients(self, model, data, ** kwargs):
        indiv_results = []
        composite_specs, mapping = self.get_composite_specs_and_mapping(model)
        nested_data = mapping.nest(data)
        for cost, cost_data in safe_zip(self.costs, nested_data):
            result = cost.get_gradients(model, cost_data, ** kwargs)
            indiv_results.append(result)

        grads = OrderedDict()
        updates = OrderedDict()
        params = model.get_params()

        for coeff, packed in zip(self.coeffs, indiv_results):
            g, u = packed
            for param in g:
                if param not in params:
                    raise ValueError("A shared variable (" +
                                     str(param) +
                                     ") that is not a parameter appeared "
                                     "a cost gradient dictionary.")
            for param in g:
                assert param.ndim == g[param].ndim
                v = coeff * g[param]
                if param not in grads:
                    grads[param] = v
                else:
                    grads[param] = grads[param] + v
                assert grads[param].ndim == param.ndim
            assert not any([state in updates for state in u])
            assert not any([state in params for state in u])
            updates.update(u)

        return grads, updates

    @functools.wraps(Cost.get_monitoring_channels)
    def get_monitoring_channels(self, model, data, ** kwargs):
        self.get_data_specs(model)[0].validate(data)
        rval = OrderedDict()
        composite_specs, mapping = self.get_composite_specs_and_mapping(model)
        nested_data = mapping.nest(data)

        for i, cost in enumerate(self.costs):
            cost_data = nested_data[i]
            try:
                channels = cost.get_monitoring_channels(model, cost_data,
                                                        **kwargs)
                rval.update(channels)
            except TypeError:
                logger.error('SumOfCosts.get_monitoring_channels encountered '
                             'TypeError while calling {0}'
                             '.get_monitoring_channels'.format(type(cost)))
                raise

            value = cost.expr(model, cost_data, ** kwargs)
            if value is not None:
                name = ''
                if hasattr(value, 'name') and value.name is not None:
                    name = '_' + value.name
                rval['term_' + str(i) + name] = value

        return rval

    def get_fixed_var_descr(self, model, data):
        """
        .. todo::

            WRITEME

        Parameters
        ----------
        model : Model
        data : theano.gof.Variable or tuple
            A valid member of the Space defined by
            self.get_data_specs(model)[0]
        """
        data_specs = self.get_data_specs(model)
        data_specs[0].validate(data)
        composite_specs, mapping = self.get_composite_specs_and_mapping(model)
        nested_data = mapping.nest(data)

        descrs = [cost.get_fixed_var_descr(model, cost_data)
                  for cost, cost_data in safe_zip(self.costs, nested_data)]

        return reduce(merge, descrs)

def scaled_cost(cost, scaling):
    """
    Deprecated. Switch to SumOfCosts([[scaling, cost]]), or just quit using it.

    Parameters
    ----------
    cost : Cost
        cost to be scaled
    scaling : float
        scaling of the cost
    """

    warnings.warn("""\
scaled_cost is deprecated and may be removed on or after 2014-08-05.
SumOfCosts allows you to scale individual terms, and if this is the only cost,
you may as well just change the learning rate.""")

    return SumOfCosts([[scaling, cost]])

class NullDataSpecsMixin(object):
    """
    Use multiple inheritance with both this object and Cost in order to
    obtain a data specification corresponding to not using data at all.

    Due to method resolution order, you want Cost to appear after
    NullDataSpecsMixin in the superclass list.
    """

    def get_data_specs(self, model):
        """
        Provides an implementation of `Cost.expr`.

        Returns data specifications corresponding to not using any
        data at all.

        Parameters
        ----------
        model : pylearn2.models.Model
        """
        return (NullSpace(), '')

class DefaultDataSpecsMixin(object):
    """
    Use multiple inheritance with both this object and Cost in order to
    obtain the default data specification.

    Due to method resolution order, you want Cost to appear after
    DefaultDataSpecsMixin in the superclass list.
    """

    def get_data_specs(self, model):
        """
        Provides a default data specification.

        The cost requests input features from the model's input space and
        input source. `self` must contain a bool field called `supervised`.
        If this field is True, the cost requests targets as well.

        Parameters
        ----------
        model : pylearn2.models.Model
            TODO WRITEME
        """
        if self.supervised:
            space = CompositeSpace([model.get_input_space(),
                                    model.get_output_space()])
            sources = (model.get_input_source(), model.get_target_source())
            return (space, sources)
        else:
            return (model.get_input_space(), model.get_input_source())

class LpPenalty(NullDataSpecsMixin, Cost):
    """
    L-p penalty of the tensor variables provided.

    Parameters
    ----------
    variables : list
        list of tensor variables to be regularized
    p : int
        p in "L-p penalty"
    """
    def __init__(self, variables, p):
        """
        Parameters
        ----------
        variables : list
            list of tensor variables to be regularized
        p : int
            p in "L-p penalty"
        """
        self.variables = variables
        self.p = p

    def expr(self, model, data, **kwargs):
        """
        Return the L-p penalty term. The optional parameters are never used;
        they're only there to provide an interface that's consistent with
        the Cost superclass.

        Parameters
        ----------
        model : a pylearn2 Model instance
        data : a batch in cost.get_data_specs() form
        kwargs : dict
            Optional extra arguments. Not used by the base class.
        """
        # This Cost does not depend on any data, and get_data_specs does not
        # ask for any data, so we should not be provided with some.
        self.get_data_specs(model)[0].validate(data)

        penalty = 0
        for var in self.variables:
            # Absolute value handles odd-valued p cases
            penalty = penalty + abs(var ** self.p).sum()
        return penalty


class CrossEntropy(DefaultDataSpecsMixin, Cost):
    """
    DEPRECATED
    """

    def __init__(self):
        warnings.warn("CrossEntropy is deprecated. You should use a "
                "model-specific cross entropy cost function. CrossEntropy"
                "will be removed on or after August 3, 2014", stacklevel=2)
        self.supervised = True

    def expr(self, model, data, ** kwargs):
        """
        DEPRECATED

        Parameters
        ----------
        model : DEPRECATED
        data : DEPRECATED
        """
        self.get_data_specs(model)[0].validate(data)

        # unpack data
        (X, Y) = data
        return (-Y * T.log(model(X)) -
                (1 - Y) * T.log(1 - model(X))).sum(axis=1).mean()


class MethodCost(Cost):
    """
    A cost specified via the string name of a method of the model.

    Parameters
    ----------
    method : a string specifying the name of the method of the model
            that should be called to generate the objective function.
    data_specs : a string specifying the name of a method/property of
            the model that describe the data specs required by
            method
    """

    def __init__(self, method, data_specs=None):
        """
        .. todo::

            WRITEME

        Parameters
        ----------
        method : a string specifying the name of the method of the model
                that should be called to generate the objective function.
        data_specs : a string specifying the name of a method/property of
                the model that describe the data specs required by
                method
        """
        self.method = method
        self.data_specs = data_specs

    def expr(self, model, data, *args, **kwargs):
        """
        Patches calls through to a user-specified method of the model

        Parameters
        ----------
        model : pylearn2.models.model.Model
            the model for which we want to calculate the sum of costs
        data : flat tuple of tensor_like variables.
            data has to follow the format defined by self.get_data_specs(),
            but this format will always be a flat tuple.
        """
        self.get_data_specs(model)[0].validate(data)
        fn = getattr(model, self.method)
        return fn(data, *args, **kwargs)

    @functools.wraps(Cost.get_data_specs)
    def get_data_specs(self, model):
        if self.data_specs is not None:
            fn = getattr(model, self.data_specs)
        else:
            # To be compatible with earlier scripts,
            # try (self.method)_data_specs
            fn = getattr(model, '%s_data_specs' % self.method)

        if callable(fn):
            return fn()
        else:
            return fn


def _no_op(data):
    """
    An on_load_batch callback that does nothing.
    """

class FixedVarDescrDataSpecsError(TypeError):
    """
    An error raised when code attempts to use the unused
    data_specs field of FixedVarDescr
    """

class FixedVarDescr(object):
    """
    An object used to describe variables that influence the cost but that
    should be held fixed for each minibatch, even if the learning algorithm
    makes multiple changes to the parameters on this minibatch, i.e., for a
    line search, etc.

    Attributes

    - fixed_vars : dict
        maps string names to shared variables or some sort of data
        structure surrounding shared variables.
        Any learning algorithm that does multiple updates on the same
        minibatch should pass fixed_vars to the cost's expr and
        get_gradient methods as keyword arguments.
    - on_load_batch : list
        A list of callable objects that the learning algorithm should
        call with input data.
        All of these callables must take an argument with the same
        (space, source) format as the cost used for training.
        TODO: It can be hard for a human user to know the right format
        ahead of time if you use SumOfCosts, make a better way of handling
        this.
        PL had added a data_specs field to this class which
        was meant to define the (space, source) format for each of
        the members of on_load_batch, but the doc was internally
        inconsistent, none of the TrainingAlgorithms obeyed it,
        and the Cost's handling of it was buggy. IG removed this
        broken functionality so that at least singleton costs can
        used FixedVarDescr but it would be good to restore functionality
        to composite costs.
    """

    def __init__(self):
        self.fixed_vars = {}
        self.on_load_batch = []

    def _data_specs_err(self, x = None):
        raise FixedVarDescrDataSpecsError("The data_specs field of "
                "FixedVarDescr has been "
                "removed. While this field existed and was documented at "
                "one time, no TrainingAlgorithm respected it. The "
                "data_specs of all members of on_load_batch must match "
                "those of the cost.")

    data_specs = property(_data_specs_err, _data_specs_err)


def merge(left, right):
    """
    Combine two FixedVarDescrs

    Parameters
    ----------
    left : FixedVarDescr
    right : FixedVarDescr

    Returns
    -------
    merged : FixedVarDescr
        a new FixedVarDescr describing all variables and operations
        described by `left` and `right`
    """

    # We assume aliasing is a bug
    assert left is not right
    assert left.fixed_vars is not right.fixed_vars
    assert left.on_load_batch is not right.on_load_batch

    merged = FixedVarDescr()
    for key in left.fixed_vars:
        if key in right.fixed_vars:
            raise ValueError("Can't merge these FixedVarDescrs, "
                             "both contain " + key)
    assert not any([key in left.fixed_vars for key in right.fixed_vars])
    merged.fixed_vars.update(left.fixed_vars)
    merged.fixed_vars.update(right.fixed_vars)

    merged.on_load_batch = safe_union(left.on_load_batch,
                                        right.on_load_batch)

    return merged

########NEW FILE########
__FILENAME__ = dbm
"""
This module contains cost functions to use with deep Boltzmann machines
(pylearn2.models.dbm).
"""

__authors__ = ["Ian Goodfellow", "Vincent Dumoulin"]
__copyright__ = "Copyright 2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"

import numpy as np
import logging
import warnings

from theano.compat.python2x import OrderedDict
from theano import config
from theano.sandbox.rng_mrg import MRG_RandomStreams
RandomStreams = MRG_RandomStreams
from theano import tensor as T

import pylearn2
from pylearn2.costs.cost import Cost
from pylearn2.costs.cost import (
    FixedVarDescr, DefaultDataSpecsMixin, NullDataSpecsMixin
)
from pylearn2.models import dbm
from pylearn2.models.dbm import BinaryVectorMaxPool
from pylearn2.models.dbm import flatten
from pylearn2.models.dbm.layer import BinaryVector
from pylearn2.models.dbm import Softmax
from pylearn2 import utils
from pylearn2.utils import make_name
from pylearn2.utils import safe_izip
from pylearn2.utils import safe_zip
from pylearn2.utils import sharedX
from pylearn2.utils.rng import make_theano_rng


logger = logging.getLogger(__name__)


class BaseCD(Cost):
    """
    Parameters
    ----------
    num_chains : int
        The number of negative chains to use with PCD / SML.
        WRITEME : how is this meant to be used with CD? Do you just need to
        set it to be equal to the batch size? If so: TODO, get rid of this
        redundant aspect of the interface.
    num_gibbs_steps : int
        The number of Gibbs steps to use in the negative phase. (i.e., if
        you want to use CD-k or PCD-k, this is "k").
    supervised : bool
        If True, requests class labels and models the joint distrbution over
        features and labels.
    toronto_neg : bool
        If True, use a bit of mean field in the negative phase.
        Ruslan Salakhutdinov's matlab code does this.
    theano_rng : MRG_RandomStreams, optional
        If specified, uses this object to generate all random numbers.
        Otherwise, makes its own random number generator.
    """

    def __init__(self, num_chains, num_gibbs_steps, supervised=False,
                 toronto_neg=False, theano_rng=None):
        self.__dict__.update(locals())
        del self.self
        self.theano_rng = make_theano_rng(theano_rng, 2012+10+14,
                which_method="binomial")
        assert supervised in [True, False]

    def expr(self, model, data):
        """
        .. todo::

            WRITEME

        The partition function makes this intractable.
        """
        self.get_data_specs(model)[0].validate(data)

        return None

    def get_monitoring_channels(self, model, data):
        """
        .. todo::

            WRITEME
        """
        self.get_data_specs(model)[0].validate(data)
        rval = OrderedDict()

        if self.supervised:
            X, Y = data
        else:
            X = data
            Y = None

        history = model.mf(X, return_history = True)
        q = history[-1]

        if self.supervised:
            assert len(data) == 2
            Y_hat = q[-1]
            true = T.argmax(Y, axis=1)
            pred = T.argmax(Y_hat, axis=1)

            #true = Print('true')(true)
            #pred = Print('pred')(pred)

            wrong = T.neq(true, pred)
            err = T.cast(wrong.mean(), X.dtype)
            rval['misclass'] = err

            if len(model.hidden_layers) > 1:
                q = model.mf(X, Y=Y)
                pen = model.hidden_layers[-2].upward_state(q[-2])
                Y_recons = model.hidden_layers[-1].mf_update(state_below=pen)
                pred = T.argmax(Y_recons, axis=1)
                wrong = T.neq(true, pred)

                rval['recons_misclass'] = T.cast(wrong.mean(), X.dtype)

        return rval

    def get_gradients(self, model, data):
        """
        .. todo::

            WRITEME
        """
        self.get_data_specs(model)[0].validate(data)
        if self.supervised:
            X, Y = data
            assert Y is not None
        else:
            X = data
            Y = None

        pos_phase_grads, pos_updates = self._get_positive_phase(model, X, Y)

        neg_phase_grads, neg_updates = self._get_negative_phase(model, X, Y)

        updates = OrderedDict()
        for key, val in pos_updates.items():
            updates[key] = val
        for key, val in neg_updates.items():
            updates[key] = val

        gradients = OrderedDict()
        for param in list(pos_phase_grads.keys()):
            gradients[param] = neg_phase_grads[param] + pos_phase_grads[param]

        return gradients, updates

    def _get_toronto_neg(self, model, layer_to_chains):
        """
        .. todo::

            WRITEME
        """
        # Ruslan Salakhutdinov's undocumented negative phase from
        # http://www.mit.edu/~rsalakhu/code_DBM/dbm_mf.m
        # IG copied it here without fully understanding it, so it
        # only applies to exactly the same model structure as
        # in that code.

        assert isinstance(model.visible_layer, BinaryVector)
        assert isinstance(model.hidden_layers[0], BinaryVectorMaxPool)
        assert model.hidden_layers[0].pool_size == 1
        assert isinstance(model.hidden_layers[1], BinaryVectorMaxPool)
        assert model.hidden_layers[1].pool_size == 1
        assert isinstance(model.hidden_layers[2], Softmax)
        assert len(model.hidden_layers) == 3

        params = list(model.get_params())

        V_samples = layer_to_chains[model.visible_layer]
        H1_samples, H2_samples, Y_samples = [layer_to_chains[layer] for
                                             layer in model.hidden_layers]

        H1_mf = model.hidden_layers[0].mf_update(
            state_below=model.visible_layer.upward_state(V_samples),
            state_above=model.hidden_layers[1].downward_state(H2_samples),
            layer_above=model.hidden_layers[1])
        Y_mf = model.hidden_layers[2].mf_update(
            state_below=model.hidden_layers[1].upward_state(H2_samples))
        H2_mf = model.hidden_layers[1].mf_update(
            state_below=model.hidden_layers[0].upward_state(H1_mf),
            state_above=model.hidden_layers[2].downward_state(Y_mf),
            layer_above=model.hidden_layers[2])

        expected_energy_p = model.energy(
            V_samples, [H1_mf, H2_mf, Y_samples]
        ).mean()

        constants = flatten([V_samples, H1_mf, H2_mf, Y_samples])

        neg_phase_grads = OrderedDict(
            safe_zip(params, T.grad(-expected_energy_p, params,
                                    consider_constant=constants)))
        return neg_phase_grads

    def _get_standard_neg(self, model, layer_to_chains):
        """
        .. todo::

            WRITEME

        TODO:reduce variance of negative phase by
             integrating out the even-numbered layers. The
             Rao-Blackwellize method can do this for you when
             expected gradient = gradient of expectation, but
             doing this in general is trickier.
        """
        params = list(model.get_params())

        # layer_to_chains = model.rao_blackwellize(layer_to_chains)
        expected_energy_p = model.energy(
            layer_to_chains[model.visible_layer],
            [layer_to_chains[layer] for layer in model.hidden_layers]
        ).mean()

        samples = flatten(layer_to_chains.values())
        for i, sample in enumerate(samples):
            if sample.name is None:
                sample.name = 'sample_'+str(i)

        neg_phase_grads = OrderedDict(
            safe_zip(params, T.grad(-expected_energy_p, params,
                                    consider_constant=samples,
                                    disconnected_inputs='ignore'))
        )
        return neg_phase_grads

    def _get_variational_pos(self, model, X, Y):
        """
        .. todo::

            WRITEME
        """
        if self.supervised:
            assert Y is not None
            # note: if the Y layer changes to something without linear energy,
            # we'll need to make the expected energy clamp Y in the positive
            # phase
            assert isinstance(model.hidden_layers[-1], Softmax)

        q = model.mf(X, Y)

        """
            Use the non-negativity of the KL divergence to construct a lower
            bound on the log likelihood. We can drop all terms that are
            constant with repsect to the model parameters:

            log P(v) = L(v, q) + KL(q || P(h|v))
            L(v, q) = log P(v) - KL(q || P(h|v))
            L(v, q) = log P(v) - sum_h q(h) log q(h) + q(h) log P(h | v)
            L(v, q) = log P(v) + sum_h q(h) log P(h | v) + const
            L(v, q) = log P(v) + sum_h q(h) log P(h, v)
                               - sum_h q(h) log P(v) + const
            L(v, q) = sum_h q(h) log P(h, v) + const
            L(v, q) = sum_h q(h) -E(h, v) - log Z + const

            so the cost we want to minimize is
            expected_energy + log Z + const


            Note: for the RBM, this bound is exact, since the KL divergence
                  goes to 0.
        """

        variational_params = flatten(q)

        # The gradients of the expected energy under q are easy, we can just
        # do that in theano
        expected_energy_q = model.expected_energy(X, q).mean()
        params = list(model.get_params())
        gradients = OrderedDict(
            safe_zip(params, T.grad(expected_energy_q,
                                    params,
                                    consider_constant=variational_params,
                                    disconnected_inputs='ignore'))
        )
        return gradients

    def _get_sampling_pos(self, model, X, Y):
        """
        .. todo::

            WRITEME
        """
        layer_to_clamp = OrderedDict([(model.visible_layer, True)])
        layer_to_pos_samples = OrderedDict([(model.visible_layer, X)])
        if self.supervised:
            # note: if the Y layer changes to something without linear energy,
            #       we'll need to make the expected energy clamp Y in the
            #       positive phase
            assert isinstance(model.hidden_layers[-1], Softmax)
            layer_to_clamp[model.hidden_layers[-1]] = True
            layer_to_pos_samples[model.hidden_layers[-1]] = Y
            hid = model.hidden_layers[:-1]
        else:
            assert Y is None
            hid = model.hidden_layers

        for layer in hid:
            mf_state = layer.init_mf_state()

            def recurse_zeros(x):
                if isinstance(x, tuple):
                    return tuple([recurse_zeros(e) for e in x])
                return x.zeros_like()
            layer_to_pos_samples[layer] = recurse_zeros(mf_state)

        layer_to_pos_samples = model.mcmc_steps(
            layer_to_state=layer_to_pos_samples,
            layer_to_clamp=layer_to_clamp,
            num_steps=self.num_gibbs_steps,
            theano_rng=self.theano_rng)

        q = [layer_to_pos_samples[layer] for layer in model.hidden_layers]

        pos_samples = flatten(q)

        # The gradients of the expected energy under q are easy, we can just
        # do that in theano
        expected_energy_q = model.energy(X, q).mean()
        params = list(model.get_params())
        gradients = OrderedDict(
            safe_zip(params, T.grad(expected_energy_q, params,
                                    consider_constant=pos_samples,
                                    disconnected_inputs='ignore'))
        )
        return gradients


class PCD(DefaultDataSpecsMixin, BaseCD):
    """
    An intractable cost representing the negative log likelihood of a DBM.
    The gradient of this bound is computed using a persistent
    markov chain.

    TODO add citation to Tieleman paper, Younes paper

    See Also
    --------
    BaseCD : The base class of this class (where the constructor
        parameters are documented)
    """

    def _get_positive_phase(self, model, X, Y=None):
        """
        .. todo::

            WRITEME
        """
        return self._get_sampling_pos(model, X, Y), OrderedDict()

    def _get_negative_phase(self, model, X, Y=None):
        """
        .. todo::

            WRITEME
        """
        layer_to_chains = model.make_layer_to_state(self.num_chains)

        def recurse_check(l):
            if isinstance(l, (list, tuple)):
                for elem in l:
                    recurse_check(elem)
            else:
                assert l.get_value().shape[0] == self.num_chains

        recurse_check(layer_to_chains.values())

        model.layer_to_chains = layer_to_chains

        # Note that we replace layer_to_chains with a dict mapping to the new
        # state of the chains
        updates, layer_to_chains = model.get_sampling_updates(
            layer_to_chains, self.theano_rng, num_steps=self.num_gibbs_steps,
            return_layer_to_updated=True)

        if self.toronto_neg:
            neg_phase_grads = self._get_toronto_neg(model, layer_to_chains)
        else:
            neg_phase_grads = self._get_standard_neg(model, layer_to_chains)

        return neg_phase_grads, updates


class VariationalPCD(DefaultDataSpecsMixin, BaseCD):
    """
    An intractable cost representing the variational upper bound
    on the negative log likelihood of a DBM.
    The gradient of this bound is computed using a persistent
    markov chain.

    TODO add citation to Tieleman paper, Younes paper

    See Also
    --------
    BaseCD : The base class of this class (where the constructor
        parameters are documented)
    """

    def expr(self, model, data):
        """
        .. todo::

            WRITEME

        The partition function makes this intractable.
        """
        self.get_data_specs(model)[0].validate(data)
        return None

    def _get_positive_phase(self, model, X, Y=None):
        """
        .. todo::

            WRITEME
        """
        return self._get_variational_pos(model, X, Y), OrderedDict()

    def _get_negative_phase(self, model, X, Y=None):
        """
        .. todo::

            WRITEME

        d/d theta log Z = (d/d theta Z) / Z
                        = (d/d theta sum_h sum_v exp(-E(v,h)) ) / Z
                        = (sum_h sum_v - exp(-E(v,h)) d/d theta E(v,h) ) / Z
                        = - sum_h sum_v P(v,h)  d/d theta E(v,h)
        """
        layer_to_chains = model.make_layer_to_state(self.num_chains)

        def recurse_check(l):
            if isinstance(l, (list, tuple)):
                for elem in l:
                    recurse_check(elem)
            else:
                assert l.get_value().shape[0] == self.num_chains

        recurse_check(layer_to_chains.values())

        model.layer_to_chains = layer_to_chains

        # Note that we replace layer_to_chains with a dict mapping to the new
        # state of the chains
        updates, layer_to_chains = model.get_sampling_updates(
            layer_to_chains,
            self.theano_rng, num_steps=self.num_gibbs_steps,
            return_layer_to_updated=True)

        if self.toronto_neg:
            neg_phase_grads = self._get_toronto_neg(model, layer_to_chains)
        else:
            neg_phase_grads = self._get_standard_neg(model, layer_to_chains)

        return neg_phase_grads, updates


class VariationalPCD_VarianceReduction(DefaultDataSpecsMixin, Cost):
    """
    Like pylearn2.costs.dbm.VariationalPCD, indeed a copy-paste of it,
    but with a variance reduction rule hard-coded for 2 binary
    hidden layers and a softmax label layer
    The variance reduction rule used here is to average together the expected
    energy you get by integrating out the odd numbered layers and the
    expected energy you get by integrating out the even numbered layers.
    This is the most "textbook correct" implementation of the negative
    phase, though not the one works the best in practice ("toronto_neg").
    """

    def __init__(self, num_chains, num_gibbs_steps, supervised = False):
        """
        """
        self.__dict__.update(locals())
        del self.self
        self.theano_rng = MRG_RandomStreams(2012 + 10 + 14)
        assert supervised in [True, False]

    def expr(self, model, data):
        """
        The partition function makes this intractable.
        """

        if self.supervised:
            X, Y = data
            assert Y is not None

        return None

    def get_monitoring_channels(self, model, data):
        rval = OrderedDict()

        if self.supervised:
            X, Y = data
        else:
            X = data
            Y = None

        history = model.mf(X, return_history = True)
        q = history[-1]

        if self.supervised:
            assert Y is not None
            Y_hat = q[-1]
            true = T.argmax(Y,axis=1)
            pred = T.argmax(Y_hat, axis=1)

            #true = Print('true')(true)
            #pred = Print('pred')(pred)

            wrong = T.neq(true, pred)
            err = T.cast(wrong.mean(), X.dtype)
            rval['misclass'] = err

            if len(model.hidden_layers) > 1:
                q = model.mf(X, Y = Y)
                pen = model.hidden_layers[-2].upward_state(q[-2])
                Y_recons = model.hidden_layers[-1].mf_update(state_below = pen)
                pred = T.argmax(Y_recons, axis=1)
                wrong = T.neq(true, pred)

                rval['recons_misclass'] = T.cast(wrong.mean(), X.dtype)


        return rval

    def get_gradients(self, model, data):
        """
        PCD approximation to the gradient of the bound.
        Keep in mind this is a cost, so we are upper bounding
        the negative log likelihood.
        """

        if self.supervised:
            X, Y = data
            assert Y is not None
            # note: if the Y layer changes to something without linear energy,
            # we'll need to make the expected energy clamp Y in the positive
            # phase
            assert isinstance(model.hidden_layers[-1], dbm.Softmax)
        else:
            X = data
            Y = None



        q = model.mf(X, Y)


        """
        Use the non-negativity of the KL divergence to construct a lower bound
        on the log likelihood. We can drop all terms that are constant with
        respect to the model parameters:

        log P(v) = L(v, q) + KL(q || P(h|v))
        L(v, q) = log P(v) - KL(q || P(h|v))
        L(v, q) = log P(v) - sum_h q(h) log q(h) + q(h) log P(h | v)
        L(v, q) = log P(v) + sum_h q(h) log P(h | v) + const
        L(v, q) = log P(v) + sum_h q(h) log P(h, v) - sum_h q(h) log P(v) + C
        L(v, q) = sum_h q(h) log P(h, v) + C
        L(v, q) = sum_h q(h) - E(h, v) - log Z + C

        so the cost we want to minimize is
        expected_energy + log Z + C


        Note: for the RBM, this bound is exact, since the KL divergence
        goes to 0.
        """

        variational_params = flatten(q)

        # The gradients of the expected energy under q are easy, we can just
        # do that in theano
        expected_energy_q = model.expected_energy(X, q).mean()
        params = list(model.get_params())
        gradients = OrderedDict(safe_zip(params, T.grad(expected_energy_q,
            params,
            consider_constant = variational_params,
            disconnected_inputs = 'ignore')))

        """
        d/d theta log Z = (d/d theta Z) / Z
                        = (d/d theta sum_h sum_v exp(-E(v,h)) ) / Z
                        = (sum_h sum_v - exp(-E(v,h)) d/d theta E(v,h) ) / Z
                        = - sum_h sum_v P(v,h)  d/d theta E(v,h)
        """

        layer_to_chains = model.make_layer_to_state(self.num_chains)

        def recurse_check(l):
            if isinstance(l, (list, tuple)):
                for elem in l:
                    recurse_check(elem)
            else:
                assert l.get_value().shape[0] == self.num_chains

        recurse_check(layer_to_chains.values())

        model.layer_to_chains = layer_to_chains

        # Note that we replace layer_to_chains with a dict mapping to the new
        # state of the chains
        updates, layer_to_chains = model.get_sampling_updates(layer_to_chains,
                self.theano_rng, num_steps=self.num_gibbs_steps,
                return_layer_to_updated = True)

        # Variance reduction is hardcoded for this exact model
        assert isinstance(model.visible_layer, dbm.BinaryVector)
        assert isinstance(model.hidden_layers[0], dbm.BinaryVectorMaxPool)
        assert model.hidden_layers[0].pool_size == 1
        assert isinstance(model.hidden_layers[1], dbm.BinaryVectorMaxPool)
        assert model.hidden_layers[1].pool_size == 1
        assert isinstance(model.hidden_layers[2], dbm.Softmax)
        assert len(model.hidden_layers) == 3

        V_samples = layer_to_chains[model.visible_layer]
        H1_samples, H2_samples, Y_samples = [layer_to_chains[layer] for layer
                in model.hidden_layers]

        V_mf = model.visible_layer.inpaint_update(layer_above=\
                model.hidden_layers[0],
                state_above=\
                model.hidden_layers[0].downward_state(H1_samples))
        H1_mf = model.hidden_layers[0].mf_update(state_below=\
                model.visible_layer.upward_state(V_samples),
                state_above=model.hidden_layers[1].downward_state(H2_samples),
                layer_above=model.hidden_layers[1])
        H2_mf = model.hidden_layers[1].mf_update(state_below=\
                model.hidden_layers[0].upward_state(H1_samples),
                state_above=model.hidden_layers[2].downward_state(Y_samples),
                layer_above=model.hidden_layers[2])
        Y_mf = model.hidden_layers[2].mf_update(state_below=\
                model.hidden_layers[1].upward_state(H2_samples))

        expected_energy_p = 0.5 * model.energy(V_samples, [H1_mf,
            H2_samples, Y_mf]).mean() + \
                            0.5 * model.energy(V_mf, [H1_samples,
                                H2_mf, Y_samples]).mean()

        constants = flatten([V_samples, V_mf, H1_samples, H1_mf, H2_samples,
            H2_mf, Y_mf, Y_samples])

        neg_phase_grads = OrderedDict(safe_zip(params, T.grad(
            -expected_energy_p, params, consider_constant = constants)))


        for param in list(gradients.keys()):
            gradients[param] = neg_phase_grads[param] + gradients[param]

        return gradients, updates

class VariationalCD(DefaultDataSpecsMixin, BaseCD):
    """
    An intractable cost representing the negative log likelihood of a DBM.
    The gradient of this bound is computed using a markov chain initialized
    with the training example.

    Source: Hinton, G. Training Products of Experts by Minimizing
            Contrastive Divergence
    """

    def _get_positive_phase(self, model, X, Y=None):
        """
        .. todo::

            WRITEME
        """
        return self._get_variational_pos(model, X, Y), OrderedDict()

    def _get_negative_phase(self, model, X, Y=None):
        """
        .. todo::

            WRITEME

        d/d theta log Z = (d/d theta Z) / Z
                        = (d/d theta sum_h sum_v exp(-E(v,h)) ) / Z
                        = (sum_h sum_v - exp(-E(v,h)) d/d theta E(v,h) ) / Z
                        = - sum_h sum_v P(v,h)  d/d theta E(v,h)
        """
        layer_to_clamp = OrderedDict([(model.visible_layer, True)])

        layer_to_chains = model.make_layer_to_symbolic_state(self.num_chains,
                                                             self.theano_rng)
        # The examples are used to initialize the visible layer's chains
        layer_to_chains[model.visible_layer] = X
        # If we use supervised training, we need to make sure the targets are
        # also clamped.
        if self.supervised:
            assert Y is not None
            # note: if the Y layer changes to something without linear energy,
            # we'll need to make the expected energy clamp Y in the positive
            # phase
            assert isinstance(model.hidden_layers[-1], Softmax)
            layer_to_clamp[model.hidden_layers[-1]] = True
            layer_to_chains[model.hidden_layers[-1]] = Y

        model.layer_to_chains = layer_to_chains

        # Note that we replace layer_to_chains with a dict mapping to the new
        # state of the chains
        # We first initialize the chain by clamping the visible layer and the
        # target layer (if it exists)
        layer_to_chains = model.mcmc_steps(layer_to_chains,
                                           self.theano_rng,
                                           layer_to_clamp=layer_to_clamp,
                                           num_steps=1)
        # We then do the required mcmc steps
        layer_to_chains = model.mcmc_steps(layer_to_chains,
                                           self.theano_rng,
                                           num_steps=self.num_gibbs_steps)

        if self.toronto_neg:
            neg_phase_grads = self._get_toronto_neg(model, layer_to_chains)
        else:
            neg_phase_grads = self._get_standard_neg(model, layer_to_chains)

        return neg_phase_grads, OrderedDict()

class MF_L1_ActCost(DefaultDataSpecsMixin, Cost):
    """
    L1 activation cost on the mean field parameters.

    Adds a cost of:

    coeff * max( abs(mean_activation - target) - eps, 0)

    averaged over units

    for each layer.

    """

    def __init__(self, targets, coeffs, eps, supervised):
        """
        targets: a list, one element per layer, specifying the activation
                each layer should be encouraged to have
                    each element may also be a list depending on the
                    structure of the layer.
                See each layer's get_l1_act_cost for a specification of
                    what the state should be.
        coeffs: a list, one element per layer, specifying the coefficient
                to put on the L1 activation cost for each layer
        supervised: If true, runs mean field on both X and Y, penalizing
                the layers in between only
        """
        self.__dict__.update(locals())
        del self.self

    def expr(self, model, data, ** kwargs):

        if self.supervised:
            X, Y = data
            H_hat = model.mf(X, Y= Y)
        else:
            X = data
            H_hat = model.mf(X)

        hidden_layers = model.hidden_layers
        if self.supervised:
            hidden_layers = hidden_layers[:-1]
            H_hat = H_hat[:-1]

        layer_costs = []
        for layer, mf_state, targets, coeffs, eps in \
            safe_zip(hidden_layers, H_hat, self.targets, self.coeffs,
                    self.eps):
            cost = None
            try:
                cost = layer.get_l1_act_cost(mf_state, targets, coeffs, eps)
            except NotImplementedError:
                assert isinstance(coeffs, float) and coeffs == 0.
                assert cost is None # if this gets triggered, there might
                    # have been a bug, where costs from lower layers got
                    # applied to higher layers that don't implement the cost
                cost = None
            if cost is not None:
                layer_costs.append(cost)


        assert T.scalar() != 0. # make sure theano semantics do what I want
        layer_costs = [cost_ for cost_ in layer_costs if cost_ != 0.]

        if len(layer_costs) == 0:
            return T.as_tensor_variable(0.)
        else:
            total_cost = reduce(lambda x, y: x + y, layer_costs)
        total_cost.name = 'MF_L1_ActCost'

        assert total_cost.ndim == 0

        return total_cost

class MF_L2_ActCost(DefaultDataSpecsMixin, Cost):
    """
    An L2 penalty on the amount that the hidden unit mean field parameters
    deviate from desired target values.

    TODO: write up parameters list
    """

    def __init__(self, targets, coeffs, supervised=False):
        targets = fix(targets)
        coeffs = fix(coeffs)

        self.__dict__.update(locals())
        del self.self

    def expr(self, model, data, return_locals=False, **kwargs):
        """
        .. todo::

            WRITEME

        If returns locals is True, returns (objective, locals())
        Note that this means adding / removing / changing the value of
        local variables is an interface change.
        In particular, TorontoSparsity depends on "terms" and "H_hat"
        """
        self.get_data_specs(model)[0].validate(data)
        if self.supervised:
            (X, Y) = data
        else:
            X = data
            Y = None

        H_hat = model.mf(X, Y=Y)

        terms = []

        hidden_layers = model.hidden_layers
        #if self.supervised:
        #    hidden_layers = hidden_layers[:-1]

        for layer, mf_state, targets, coeffs in \
                safe_zip(hidden_layers, H_hat, self.targets, self.coeffs):
            try:
                cost = layer.get_l2_act_cost(mf_state, targets, coeffs)
            except NotImplementedError:
                if isinstance(coeffs, float) and coeffs == 0.:
                    cost = 0.
                else:
                    raise
            terms.append(cost)


        objective = sum(terms)

        if return_locals:
            return objective, locals()
        return objective


def fix(l):
    """
    Parameters
    ----------
    l : object

    Returns
    -------
    l : object
        If `l` is anything but a string, the return is the
        same as the input, but it may have been modified in place.
        If `l` is a string, the return value is `l` converted to a float.
        If `l` is a list, this function explores all nested lists inside
        `l` and turns all string members into floats.
    """
    if isinstance(l, list):
        return [fix(elem) for elem in l]
    if isinstance(l, str):
        return float(l)
    return l

class TorontoSparsity(Cost):
    """
    TODO: add link to Ruslan Salakhutdinov's paper that this is based on
    TODO: write up parameters list
    """
    def __init__(self, targets, coeffs, supervised=False):
        self.__dict__.update(locals())
        del self.self

        self.base_cost = MF_L2_ActCost(targets=targets,
                coeffs=coeffs, supervised=supervised)

    def expr(self, model, data, return_locals=False, **kwargs):
        """
        .. todo::

            WRITEME
        """
        self.get_data_specs(model)[0].validate(data)
        return self.base_cost.expr(model, data, return_locals=return_locals,
                **kwargs)

    def get_gradients(self, model, data, **kwargs):
        """
        .. todo::

            WRITEME
        """
        self.get_data_specs(model)[0].validate(data)
        obj, scratch = self.base_cost.expr(model, data, return_locals=True,
                                           **kwargs)
        if self.supervised:
            assert isinstance(data, (list, tuple))
            assert len(data) == 2
            (X, Y) = data
        else:
            X = data

        H_hat = scratch['H_hat']
        terms = scratch['terms']
        hidden_layers = scratch['hidden_layers']

        grads = OrderedDict()

        assert len(H_hat) == len(terms)
        assert len(terms) == len(hidden_layers)
        num_layers = len(hidden_layers)
        for i in xrange(num_layers):
            state = H_hat[i]
            layer = model.hidden_layers[i]
            term = terms[i]

            if term == 0.:
                continue
            else:
                logger.info('term is {0}'.format(term))

            if i == 0:
                state_below = X
                layer_below = model.visible_layer
            else:
                layer_below = model.hidden_layers[i-1]
                state_below = H_hat[i-1]
            state_below = layer_below.upward_state(state_below)

            components = flatten(state)

            real_grads = T.grad(term, components)

            fake_state = layer.linear_feed_forward_approximation(state_below)

            fake_components = flatten(fake_state)
            real_grads = OrderedDict(safe_zip(fake_components, real_grads))

            params = list(layer.get_params())
            fake_grads = pylearn2.utils.grad(
                cost=None,
                consider_constant=flatten(state_below),
                wrt=params,
                known_grads=real_grads
            )

            for param, grad in safe_zip(params, fake_grads):
                if param in grads:
                    grads[param] = grads[param] + grad
                else:
                    grads[param] = grad

        return grads, OrderedDict()

    def get_data_specs(self, model):
        """
        .. todo::

            WRITEME
        """
        return self.base_cost.get_data_specs(model)

class WeightDecay(NullDataSpecsMixin, Cost):
    """
    A Cost that applies the following cost function:

    coeff * sum(sqr(weights))
    for each set of weights.

    Parameters
    ----------
    coeffs : list
        One element per layer, specifying the coefficient
        to put on the L1 activation cost for each layer.
        Each element may in turn be a list, ie, for CompositeLayers.
    """

    def __init__(self, coeffs):
        self.__dict__.update(locals())
        del self.self

    def expr(self, model, data, ** kwargs):
        """
        .. todo::

            WRITEME
        """
        self.get_data_specs(model)[0].validate(data)
        layer_costs = [ layer.get_weight_decay(coeff)
            for layer, coeff in safe_izip(model.hidden_layers, self.coeffs) ]

        assert T.scalar() != 0. # make sure theano semantics do what I want
        layer_costs = [ cost for cost in layer_costs if cost != 0.]

        if len(layer_costs) == 0:
            rval =  T.as_tensor_variable(0.)
            rval.name = '0_weight_decay'
            return rval
        else:
            total_cost = reduce(lambda x, y: x + y, layer_costs)
        total_cost.name = 'DBM_WeightDecay'

        assert total_cost.ndim == 0

        total_cost.name = 'weight_decay'

        return total_cost


class MultiPrediction(DefaultDataSpecsMixin, Cost):
    """
    If you use this class in your research work, please cite:

    Multi-prediction deep Boltzmann machines. Ian J. Goodfellow, Mehdi Mirza,
    Aaron Courville, and Yoshua Bengio. NIPS 2013.

    .. todo::

            WRITEME : parameters list
    """
    def __init__(self,
            monitor_multi_inference = False,
                    mask_gen = None,
                    noise = False,
                    both_directions = False,
                    l1_act_coeffs = None,
                    l1_act_targets = None,
                    l1_act_eps = None,
                    range_rewards = None,
                    stdev_rewards = None,
                    robustness = None,
                    supervised = False,
                    niter = None,
                    block_grad = None,
                    vis_presynaptic_cost = None,
                    hid_presynaptic_cost = None,
                    reweighted_act_coeffs = None,
                    reweighted_act_targets = None,
                    toronto_act_targets = None,
                    toronto_act_coeffs = None,
                    monitor_each_step = False,
                    use_sum = False
                    ):
        self.__dict__.update(locals())
        del self.self


    def get_monitoring_channels(self, model, data, drop_mask = None,
            drop_mask_Y = None, **kwargs):
        """
        .. todo::

            WRITEME
        """

        if self.supervised:
            X, Y = data
        else:
            X = data
            Y = None

        if self.supervised:
            assert Y is not None

        rval = OrderedDict()

        # TODO: shouldn't self() handle this?
        if drop_mask is not None and drop_mask.ndim < X.ndim:
            if self.mask_gen is not None:
                assert self.mask_gen.sync_channels
            if X.ndim != 4:
                raise NotImplementedError()
            drop_mask = drop_mask.dimshuffle(0,1,2,'x')

        if Y is None:
            data = X
        else:
            data = (X, Y)
        scratch = self.expr(model, data, drop_mask = drop_mask,
                drop_mask_Y = drop_mask_Y,
                return_locals = True)

        history = scratch['history']
        new_history = scratch['new_history']
        new_drop_mask = scratch['new_drop_mask']
        new_drop_mask_Y = None
        drop_mask = scratch['drop_mask']
        if self.supervised:
            drop_mask_Y = scratch['drop_mask_Y']
            new_drop_mask_Y = scratch['new_drop_mask_Y']

        ii = 0
        for name in ['inpaint_cost', 'l1_act_cost', 'toronto_act_cost',
                'reweighted_act_cost']:
            var = scratch[name]
            if var is not None:
                rval['total_inpaint_cost_term_'+str(ii)+'_'+name] = var
                ii = ii + 1

        if self.monitor_each_step:
            for ii, packed in enumerate(safe_izip(history, new_history)):
                state, new_state = packed
                rval['all_inpaint_costs_after_' + str(ii)] = \
                        self.cost_from_states(state,
                        new_state,
                        model, X, Y, drop_mask, drop_mask_Y,
                        new_drop_mask, new_drop_mask_Y)

                if ii > 0:
                    prev_state = history[ii-1]
                    V_hat = state['V_hat']
                    prev_V_hat = prev_state['V_hat']
                    rval['max_pixel_diff[%d]'%ii] = abs(V_hat-prev_V_hat).max()

        final_state = history[-1]

        #empirical beta code--should be moved to gaussian visible layer,
        #should support topo data
        #V_hat = final_state['V_hat']
        #err = X - V_hat
        #masked_err = err * drop_mask
        #sum_sqr_err = T.sqr(masked_err).sum(axis=0)
        #recons_count = T.cast(drop_mask.sum(axis=0), 'float32')

        # empirical_beta = recons_count / sum_sqr_err
        # assert empirical_beta.ndim == 1


        #rval['empirical_beta_min'] = empirical_beta.min()
        #rval['empirical_beta_mean'] = empirical_beta.mean()
        #rval['empirical_beta_max'] = empirical_beta.max()

        layers = model.get_all_layers()
        states = [ final_state['V_hat'] ] + final_state['H_hat']

        for layer, state in safe_izip(layers, states):
            d = layer.get_monitoring_channels_from_state(state)
            for key in d:
                mod_key = 'final_inpaint_' + layer.layer_name + '_' + key
                assert mod_key not in rval
                rval[mod_key] = d[key]

        if self.supervised:
            inpaint_Y_hat = history[-1]['H_hat'][-1]
            err = T.neq(T.argmax(inpaint_Y_hat, axis=1), T.argmax(Y, axis=1))
            assert err.ndim == 1
            assert drop_mask_Y.ndim == 1
            err =  T.dot(err, drop_mask_Y) / drop_mask_Y.sum()
            if err.dtype != inpaint_Y_hat.dtype:
                err = T.cast(err, inpaint_Y_hat.dtype)

            rval['inpaint_err'] = err

            Y_hat = model.mf(X)[-1]

            Y = T.argmax(Y, axis=1)
            Y = T.cast(Y, Y_hat.dtype)

            argmax = T.argmax(Y_hat,axis=1)
            if argmax.dtype != Y_hat.dtype:
                argmax = T.cast(argmax, Y_hat.dtype)
            err = T.neq(Y , argmax).mean()
            if err.dtype != Y_hat.dtype:
                err = T.cast(err, Y_hat.dtype)

            rval['err'] = err

            if self.monitor_multi_inference:
                Y_hat = model.inference_procedure.multi_infer(X)

                argmax = T.argmax(Y_hat,axis=1)
                if argmax.dtype != Y_hat.dtype:
                    argmax = T.cast(argmax, Y_hat.dtype)
                err = T.neq(Y , argmax).mean()
                if err.dtype != Y_hat.dtype:
                    err = T.cast(err, Y_hat.dtype)

                rval['multi_err'] = err

        return rval

    def expr(self, model, data, drop_mask = None, drop_mask_Y = None,
            return_locals = False, include_toronto = True, ** kwargs):
        """
        .. todo::

            WRITEME
        """
        if self.supervised:
            X, Y = data
        else:
            X = data
            Y = None

        if not self.supervised:
            assert drop_mask_Y is None
            # ignore Y if some other cost is supervised and has made it get
            # passed in (can this still happen after the (space, source)
            # interface change?)
            Y = None
        if self.supervised:
            assert Y is not None
            if drop_mask is not None:
                assert drop_mask_Y is not None

        if not hasattr(model,'cost'):
            model.cost = self
        if not hasattr(model,'mask_gen'):
            model.mask_gen = self.mask_gen

        dbm = model

        X_space = model.get_input_space()

        if drop_mask is None:
            if self.supervised:
                drop_mask, drop_mask_Y = self.mask_gen(X, Y, X_space=X_space)
            else:
                drop_mask = self.mask_gen(X, X_space=X_space)

        if drop_mask_Y is not None:
            assert drop_mask_Y.ndim == 1

        if drop_mask.ndim < X.ndim:
            if self.mask_gen is not None:
                assert self.mask_gen.sync_channels
            if X.ndim != 4:
                raise NotImplementedError()
            drop_mask = drop_mask.dimshuffle(0,1,2,'x')

        if not hasattr(self,'noise'):
            self.noise = False

        history = dbm.do_inpainting(X, Y = Y, drop_mask = drop_mask,
                drop_mask_Y = drop_mask_Y, return_history = True,
                noise = self.noise,
                niter = self.niter, block_grad = self.block_grad)
        final_state = history[-1]

        new_drop_mask = None
        new_drop_mask_Y = None
        new_history = [ None for state in history ]

        if not hasattr(self, 'both_directions'):
            self.both_directions = False
        if self.both_directions:
            new_drop_mask = 1. - drop_mask
            if self.supervised:
                new_drop_mask_Y = 1. - drop_mask_Y
            new_history = dbm.do_inpainting(X, Y = Y,
                    drop_mask=new_drop_mask,
                    drop_mask_Y=new_drop_mask_Y, return_history=True,
                    noise = self.noise,
                    niter = self.niter, block_grad = self.block_grad)

        new_final_state = new_history[-1]

        total_cost, sublocals = self.cost_from_states(final_state,
                new_final_state, dbm, X, Y, drop_mask, drop_mask_Y,
                new_drop_mask, new_drop_mask_Y,
                return_locals=True)
        l1_act_cost = sublocals['l1_act_cost']
        inpaint_cost = sublocals['inpaint_cost']
        reweighted_act_cost = sublocals['reweighted_act_cost']

        if not hasattr(self, 'robustness'):
            self.robustness = None
        if self.robustness is not None:
            inpainting_H_hat = history[-1]['H_hat']
            mf_H_hat = dbm.mf(X, Y=Y)
            if self.supervised:
                inpainting_H_hat = inpainting_H_hat[:-1]
                mf_H_hat = mf_H_hat[:-1]
                for ihh, mhh in safe_izip(flatten(inpainting_H_hat),
                        flatten(mf_H_hat)):
                    total_cost += self.robustness * T.sqr(mhh-ihh).sum()

        if not hasattr(self, 'toronto_act_targets'):
            self.toronto_act_targets = None
        toronto_act_cost = None
        if self.toronto_act_targets is not None and include_toronto:
            toronto_act_cost = 0.
            H_hat = history[-1]['H_hat']
            for s, c, t in zip(H_hat, self.toronto_act_coeffs,
                    self.toronto_act_targets):
                if c == 0.:
                    continue
                s, _ = s
                m = s.mean(axis=0)
                toronto_act_cost += c * T.sqr(m-t).mean()
            total_cost += toronto_act_cost

        if return_locals:
            return locals()

        total_cost.name = 'total_inpaint_cost'

        return total_cost

    def get_fixed_var_descr(self, model, data):
        """
        .. todo::

            WRITEME
        """
        X, Y = data

        assert Y is not None

        batch_size = model.batch_size

        drop_mask_X = sharedX(
                model.get_input_space().get_origin_batch(batch_size))
        drop_mask_X.name = 'drop_mask'

        X_space = model.get_input_space()

        updates = OrderedDict()
        rval = FixedVarDescr()
        inputs=[X, Y]

        if not self.supervised:
            update_X = self.mask_gen(X, X_space = X_space)
        else:
            drop_mask_Y = sharedX(np.ones(batch_size,))
            drop_mask_Y.name = 'drop_mask_Y'
            update_X, update_Y = self.mask_gen(X, Y, X_space)
            updates[drop_mask_Y] = update_Y
            rval.fixed_vars['drop_mask_Y'] =  drop_mask_Y
        if self.mask_gen.sync_channels:
            n = update_X.ndim
            assert n == drop_mask_X.ndim - 1
            update_X.name = 'raw_update_X'
            zeros_like_X = T.zeros_like(X)
            zeros_like_X.name = 'zeros_like_X'
            update_X = zeros_like_X + update_X.dimshuffle(0,1,2,'x')
            update_X.name = 'update_X'
        updates[drop_mask_X] = update_X

        rval.fixed_vars['drop_mask'] = drop_mask_X

        if hasattr(model.inference_procedure, 'V_dropout'):
            include_prob = model.inference_procedure.include_prob
            include_prob_V = model.inference_procedure.include_prob_V
            include_prob_Y = model.inference_procedure.include_prob_Y

            theano_rng = make_theano_rng(None, 2012+10+20,
                    which_method="binomial")
            for elem in flatten([model.inference_procedure.V_dropout]):
                updates[elem] = theano_rng.binomial(p=include_prob_V,
                        size=elem.shape, dtype=elem.dtype, n=1) / \
                                include_prob_V
            if "Softmax" in str(type(model.hidden_layers[-1])):
                hid = model.inference_procedure.H_dropout[:-1]
                y = model.inference_procedure.H_dropout[-1]
                updates[y] = theano_rng.binomial(p=include_prob_Y,
                        size=y.shape, dtype=y.dtype, n=1) / include_prob_Y
            else:
                hid = model.inference_procedure.H_dropout
            for elem in flatten(hid):
                updates[elem] =  theano_rng.binomial(p=include_prob,
                        size=elem.shape, dtype=elem.dtype, n=1) / include_prob

        rval.on_load_batch = [utils.function(inputs, updates=updates)]

        return rval

    def get_gradients(self, model, X, Y = None, **kwargs):
        """
        .. todo::

            WRITEME
        """

        if Y is None:
            data = X
        else:
            data = (X, Y)

        scratch = self.expr(model, data, include_toronto = False,
                return_locals=True, **kwargs)

        total_cost = scratch['total_cost']

        params = list(model.get_params())
        grads = dict(safe_zip(params, T.grad(total_cost, params,
            disconnected_inputs='ignore')))

        if self.toronto_act_targets is not None:
            H_hat = scratch['history'][-1]['H_hat']
            for i, packed in enumerate(safe_zip(H_hat,
                self.toronto_act_coeffs, self.toronto_act_targets)):
                s, c, t = packed
                if c == 0.:
                    continue
                s, _ = s
                m = s.mean(axis=0)
                m_cost = c * T.sqr(m-t).mean()
                real_grads = T.grad(m_cost, s)
                if i == 0:
                    below = X
                else:
                    below = H_hat[i-1][0]
                W, = model.hidden_layers[i].transformer.get_params()
                assert W in grads
                b = model.hidden_layers[i].b

                ancestor = T.scalar()
                hack_W = W + ancestor
                hack_b = b + ancestor

                fake_s = T.dot(below, hack_W) + hack_b
                if fake_s.ndim != real_grads.ndim:
                    logger.error(fake_s.ndim)
                    logger.error(real_grads.ndim)
                    assert False
                sources = [ (fake_s, real_grads) ]

                fake_grads = T.grad(cost=None, known_grads=dict(sources),
                        wrt=[below, ancestor, hack_W, hack_b])

                grads[W] = grads[W] + fake_grads[2]
                grads[b] = grads[b] + fake_grads[3]


        return grads, OrderedDict()

    def get_inpaint_cost(self, dbm, X, V_hat_unmasked, drop_mask, state,
            Y, drop_mask_Y):
        """
        .. todo::

            WRITEME
        """
        rval = dbm.visible_layer.recons_cost(X, V_hat_unmasked, drop_mask,
                use_sum=self.use_sum)

        if self.supervised:
            # pyflakes is too dumb to see that both branches define `scale`
            scale = None
            if self.use_sum:
                scale = 1.
            else:
                scale = 1. / float(dbm.get_input_space().get_total_dimension())
            Y_hat_unmasked = state['Y_hat_unmasked']
            rval = rval + \
                    dbm.hidden_layers[-1].recons_cost(Y, Y_hat_unmasked,
                            drop_mask_Y,
                            scale)

        return rval

    def cost_from_states(self, state, new_state, dbm, X, Y, drop_mask,
            drop_mask_Y,
            new_drop_mask, new_drop_mask_Y, return_locals = False):
        """
        .. todo::

            WRITEME
        """

        if not self.supervised:
            assert drop_mask_Y is None
            assert new_drop_mask_Y is None
        if self.supervised:
            assert drop_mask_Y is not None
            if self.both_directions:
                assert new_drop_mask_Y is not None
            assert Y is not None

        V_hat_unmasked = state['V_hat_unmasked']
        assert V_hat_unmasked.ndim == X.ndim

        if not hasattr(self, 'use_sum'):
            self.use_sum = False

        inpaint_cost = self.get_inpaint_cost(dbm, X, V_hat_unmasked, drop_mask,
                state, Y, drop_mask_Y)

        if not hasattr(self, 'both_directions'):
            self.both_directions = False

        assert self.both_directions == (new_state is not None)

        if new_state is not None:

            new_V_hat_unmasked = new_state['V_hat_unmasked']

            new_inpaint_cost = dbm.visible_layer.recons_cost(X,
                    new_V_hat_unmasked, new_drop_mask)
            if self.supervised:
                new_Y_hat_unmasked = new_state['Y_hat_unmasked']
                scale = None
                raise NotImplementedError("This branch appears to be broken,"
                        "needs to define scale.")
                new_inpaint_cost = new_inpaint_cost + \
                        dbm.hidden_layers[-1].recons_cost(Y,
                                new_Y_hat_unmasked, new_drop_mask_Y, scale)
            # end if include_Y
            inpaint_cost = 0.5 * inpaint_cost + 0.5 * new_inpaint_cost
        # end if both directions

        total_cost = inpaint_cost

        if not hasattr(self, 'range_rewards'):
            self.range_rewards = None
        if self.range_rewards is not None:
            for layer, mf_state, coeffs in safe_izip(
                    dbm.hidden_layers,
                    state['H_hat'],
                    self.range_rewards):
                try:
                    layer_cost = layer.get_range_rewards(mf_state, coeffs)
                except NotImplementedError:
                    if coeffs == 0.:
                        layer_cost = 0.
                    else:
                        raise
                if layer_cost != 0.:
                    total_cost += layer_cost

        if not hasattr(self, 'stdev_rewards'):
            self.stdev_rewards = None
        if self.stdev_rewards is not None:
            assert False # not monitored yet
            for layer, mf_state, coeffs in safe_izip(
                    dbm.hidden_layers,
                    state['H_hat'],
                    self.stdev_rewards):
                try:
                    layer_cost = layer.get_stdev_rewards(mf_state, coeffs)
                except NotImplementedError:
                    if coeffs == 0.:
                        layer_cost = 0.
                    else:
                        raise
                if layer_cost != 0.:
                    total_cost += layer_cost

        l1_act_cost = None
        if self.l1_act_targets is not None:
            l1_act_cost = 0.
            if self.l1_act_eps is None:
                self.l1_act_eps = [ None ] * len(self.l1_act_targets)
            for layer, mf_state, targets, coeffs, eps in \
                    safe_izip(dbm.hidden_layers, state['H_hat'],
                            self.l1_act_targets, self.l1_act_coeffs,
                            self.l1_act_eps):

                assert not isinstance(targets, str)

                try:
                    layer_cost = layer.get_l1_act_cost(mf_state, targets,
                            coeffs, eps)
                except NotImplementedError:
                    if coeffs == 0.:
                        layer_cost = 0.
                    else:
                        raise
                if layer_cost != 0.:
                    l1_act_cost += layer_cost
                # end for substates
            # end for layers
            total_cost += l1_act_cost
        # end if act penalty

        if not hasattr(self, 'hid_presynaptic_cost'):
            self.hid_presynaptic_cost = None
        if self.hid_presynaptic_cost is not None:
            assert False # not monitored yet
            for c, s, in safe_izip(self.hid_presynaptic_cost, state['H_hat']):
                if c == 0.:
                    continue
                s = s[1]
                assert hasattr(s, 'owner')
                owner = s.owner
                assert owner is not None
                op = owner.op

                if not hasattr(op, 'scalar_op'):
                    raise ValueError("Expected V_hat_unmasked to be generated"
                            "by an Elemwise op, got " + str(op) + " of type "
                            + str(type(op)))
                assert isinstance(op.scalar_op, T.nnet.sigm.ScalarSigmoid)
                z ,= owner.inputs

                total_cost += c * T.sqr(z).mean()

        if not hasattr(self, 'reweighted_act_targets'):
            self.reweighted_act_targets = None
        reweighted_act_cost = None
        if self.reweighted_act_targets is not None:
            reweighted_act_cost = 0.
            warnings.warn("reweighted_act_cost is hardcoded for sigmoid "
                    "layers and doesn't check that this is what we get.")
            for c, t, s in safe_izip(self.reweighted_act_coeffs,
                    self.reweighted_act_targets, state['H_hat']):
                if c == 0:
                    continue
                s, _ = s
                m = s.mean(axis=0)
                d = T.sqr(m-t)
                weight = 1./(1e-7+s*(1-s))
                reweighted_act_cost += c * (weight * d).mean()
            total_cost += reweighted_act_cost

        total_cost.name = 'total_cost(V_hat_unmasked = %s)' % \
                V_hat_unmasked.name

        if return_locals:
            return total_cost, locals()

        return total_cost

default_seed = 20120712
class MaskGen:
    """
    A class that generates masks for multi-prediction training.

    Parameters
    ----------
    drop_prob : float
        The probability of dropping out a unit (making it a target of
        the training criterion)
    balance : bool
        WRITEME
    sync_channels : bool
        If True:
        Rather than dropping each pixel individually, drop spatial locations.
        i.e., we either drop the red, the green, and the blue pixel at (x, y),
        or we drop nothing at (x, y).
        If False:
        Drop each pixel independently.
    drop_prob_y : float, optional
        If specified, use a different drop probability for the class labels.
    seed : int
        The seed to use with MRG_RandomStreams for generating the random
        masks.
    """

    def __init__(self, drop_prob, balance = False, sync_channels = True,
            drop_prob_y = None, seed = default_seed):
        self.__dict__.update(locals())
        del self.self


    def __call__(self, X, Y = None, X_space=None):
        """
        Provides the mask for multi-prediction training. A 1 in the mask
        corresponds to a variable that should be used as an input to the
        inference process. A 0 corresponds to a variable that should be
        used as a prediction target of the multi-prediction training
        criterion.

        Parameters
        ----------
        X : Variable
            A batch of input features to mask for multi-prediction training
        Y : Variable
            A batch of input class labels to mask for multi-prediction
            Training

        Returns
        -------
        drop_mask : Variable
            A Theano expression for a random binary mask in the same shape as
            `X`
        drop_mask_Y : Variable, only returned if `Y` is not None
            A Theano expression for a random binary mask in the same shape as
            `Y`

        Notes
        -----
        Calling this repeatedly will yield the same random numbers each time.
        """
        assert X_space is not None
        self.called = True
        assert X.dtype == config.floatX
        theano_rng = make_theano_rng(getattr(self, 'seed', None), default_seed,
                                     which_method="binomial")

        if X.ndim == 2 and self.sync_channels:
            raise NotImplementedError()

        p = self.drop_prob

        if not hasattr(self, 'drop_prob_y') or self.drop_prob_y is None:
            yp = p
        else:
            yp = self.drop_prob_y

        batch_size = X_space.batch_size(X)

        if self.balance:
            flip = theano_rng.binomial(
                    size = (batch_size,),
                    p = 0.5,
                    n = 1,
                    dtype = X.dtype)

            yp = flip * (1-p) + (1-flip) * p

            dimshuffle_args = ['x'] * X.ndim

            if X.ndim == 2:
                dimshuffle_args[0] = 0
                assert not self.sync_channels
            else:
                dimshuffle_args[X_space.axes.index('b')] = 0
                if self.sync_channels:
                    del dimshuffle_args[X_space.axes.index('c')]

            flip = flip.dimshuffle(*dimshuffle_args)

            p = flip * (1-p) + (1-flip) * p

        # size needs to have a fixed length at compile time or the
        # theano random number generator will be angry
        size = tuple([ X.shape[i] for i in xrange(X.ndim) ])
        if self.sync_channels:
            del size[X_space.axes.index('c')]

        drop_mask = theano_rng.binomial(
                    size = size,
                    p = p,
                    n = 1,
                    dtype = X.dtype)

        X_name = make_name(X, 'anon_X')
        drop_mask.name = 'drop_mask(%s)' % X_name

        if Y is not None:
            assert isinstance(yp, float) or yp.ndim < 2
            drop_mask_Y = theano_rng.binomial(
                    size = (batch_size, ),
                    p = yp,
                    n = 1,
                    dtype = X.dtype)
            assert drop_mask_Y.ndim == 1
            Y_name = make_name(Y, 'anon_Y')
            drop_mask_Y.name = 'drop_mask_Y(%s)' % Y_name
            return drop_mask, drop_mask_Y

        return drop_mask

########NEW FILE########
__FILENAME__ = ebm_estimation
"""
Training costs for unsupervised learning of energy-based models
"""
import functools
from itertools import izip
import logging
import numpy as np
import sys

from theano.compat.python2x import OrderedDict
from theano import scan
import theano.tensor as T

from pylearn2.costs.cost import Cost, DefaultDataSpecsMixin
from pylearn2.utils import py_integer_types
from pylearn2.utils.rng import make_theano_rng
from pylearn2.models.rbm import BlockGibbsSampler


logger = logging.getLogger(__name__)

logger.debug("Cost changing the recursion limit.")
# We need this to be high enough that the big theano graphs we make
# when unrolling inference don't cause python to complain.
# python intentionally declares stack overflow well before the stack
# segment is actually exceeded. But we can't make this value too big
# either, or we'll get seg faults when the python interpreter really
# does go over the stack segment.
# IG encountered seg faults on eos3 (a machine at LISA labo) when using
# 50000 so for now it is set to 40000.
# I think the actual safe recursion limit can't be predicted in advance
# because you don't know how big of a stack frame each function will
# make, so there is not really a "correct" way to do this. Really the
# python interpreter should provide an option to raise the error
# precisely when you're going to exceed the stack segment.
sys.setrecursionlimit(40000)

class NCE(DefaultDataSpecsMixin, Cost):
    """
    Noise-Contrastive Estimation

    See "Noise-Contrastive Estimation: A new estimation principle for unnormalized models "
    by Gutmann and Hyvarinen

    Parameters
    ----------
    noise : WRITEME
        A Distribution from which noisy examples are generated
    noise_per_clean : WRITEME
        Number of noisy examples to generate for each clean example given
    """
    def h(self, X, model):
        """
        .. todo::

            WRITEME
        """
        return - T.nnet.sigmoid(self.G(X, model))

    def G(self, X, model):
        """
        .. todo::

            WRITEME
        """
        return model.log_prob(X) - self.noise.log_prob(X)

    def expr(self, model, data, noisy_data=None):
        """
        .. todo::

            WRITEME
        """
        # noisy_data is not considered part of the data.
        #If you don't pass it in, it will be generated internally
        #Passing it in lets you keep it constant while doing
        #a learn search across several theano function calls
        #and stuff like that
        space, source = self.get_data_specs(model)
        space.validate(data)
        X = data
        if X.name is None:
            X_name = 'X'
        else:
            X_name = X.name

        m_data = X.shape[0]
        m_noise = m_data * self.noise_per_clean

        if noisy_data is not None:
            space.validate(noisy_data)
            Y = noisy_data
        else:
            Y = self.noise.random_design_matrix(m_noise)

        #Y = Print('Y',attrs=['min','max'])(Y)

        #hx = self.h(X, model)
        #hy = self.h(Y, model)

        log_hx = -T.nnet.softplus(-self.G(X,model))
        log_one_minus_hy = -T.nnet.softplus(self.G(Y,model))


        #based on equation 3 of the paper
        #ours is the negative of theirs because they maximize it and we minimize it
        rval = -T.mean(log_hx)-T.mean(log_one_minus_hy)

        rval.name = 'NCE('+X_name+')'

        return rval

    def __init__(self, noise, noise_per_clean):
        self.noise = noise

        assert isinstance(noise_per_clean, py_integer_types)
        self.noise_per_clean = noise_per_clean


class SM(DefaultDataSpecsMixin, Cost):
    """
    (Regularized) Score Matching

    See:

    - "Regularized estimation of image statistics by Score Matching",
      D. Kingma, Y. LeCun, NIPS 2010
    - eqn. 4 of "On Autoencoders and Score Matching for Energy Based Models"
      Swersky et al 2011

    Uses the mean over visible units rather than sum over visible units
    so that hyperparameters won't depend as much on the # of visible units

    Parameters
    ----------
    lambd : WRITEME
    """
    def __init__(self, lambd = 0):
        assert lambd >= 0
        self.lambd = lambd

    def expr(self, model, data):
        """
        .. todo::

            WRITEME
        """
        self.get_data_specs(model)[0].validate(data)
        X = data
        X_name = 'X' if X.name is None else X.name

        def f(i, _X, _dx):
            return T.grad(_dx[:,i].sum(), _X)[:,i]

        dx = model.score(X)
        ddx, _ = scan(f, sequences = [T.arange(X.shape[1])], non_sequences = [X, dx])
        ddx = ddx.T

        assert len(ddx.type.broadcastable) == 2

        rval = T.mean(0.5 * dx**2 + ddx + self.lambd * ddx**2)
        rval.name = 'sm('+X_name+')'

        return rval


class SMD(DefaultDataSpecsMixin, Cost):
    """
    Denoising Score Matching
    See eqn. 4.3 of "A Connection Between Score Matching and Denoising Autoencoders"
    by Pascal Vincent for details

    Note that instead of using half the squared norm we use the mean squared error,
    so that hyperparameters don't depend as much on the # of visible units

    Parameters
    ----------
    corruptor : WRITEME
        WRITEME
    """

    def __init__(self, corruptor):
        super(SMD, self).__init__()
        self.corruptor = corruptor

    @functools.wraps(Cost.expr)
    def expr(self, model, data):
        self.get_data_specs(model)[0].validate(data)
        X = data
        X_name = 'X' if X.name is None else X.name

        corrupted_X = self.corruptor(X)

        if corrupted_X.name is None:
            corrupted_X.name = 'corrupt('+X_name+')'

        model_score = model.score(corrupted_X)
        assert len(model_score.type.broadcastable) == len(X.type.broadcastable)
        parzen_score = T.grad( - T.sum(self.corruptor.corruption_free_energy(corrupted_X,X)), corrupted_X)
        assert len(parzen_score.type.broadcastable) == len(X.type.broadcastable)

        score_diff = model_score - parzen_score
        score_diff.name = 'smd_score_diff('+X_name+')'


        assert len(score_diff.type.broadcastable) == len(X.type.broadcastable)


        #TODO: this could probably be faster as a tensordot, but we don't have tensordot for gpu yet
        sq_score_diff = T.sqr(score_diff)

        #sq_score_diff = Print('sq_score_diff',attrs=['mean'])(sq_score_diff)

        smd = T.mean(sq_score_diff)
        smd.name = 'SMD('+X_name+')'

        return smd

    def get_data_specs(self, model):
        return (model.get_input_space(), model.get_input_source())

class SML(Cost):
    """
    Stochastic Maximum Likelihood

    See "On the convergence of Markovian stochastic algorithms with rapidly
    decreasing ergodicity rates" by Laurent Younes (1998)

    Also known as Persistent Constrastive Divergence (PCD)
    See "Training restricted boltzmann machines using approximations to
    the likelihood gradient" by Tijmen Tieleman  (2008)

    The number of particles fits the batch size.

    Parameters
    ----------
    batch_size: int
        Batch size of the training algorithm
    nsteps: int
        Number of steps made by the block Gibbs sampler between each epoch
    """
    def __init__(self, batch_size, nsteps ):
        super(SML, self).__init__()
        self.nchains = batch_size
        self.nsteps  = nsteps

    def get_gradients(self, model, data, **kwargs):
        cost = self._cost(model,data,**kwargs)

        params = list(model.get_params())

        grads = T.grad(cost, params, disconnected_inputs = 'ignore',
                       consider_constant = [self.sampler.particles])

        gradients = OrderedDict(izip(params, grads))

        updates = OrderedDict()

        sampler_updates = self.sampler.updates()
        updates.update(sampler_updates)
        return gradients, updates

    def _cost(self, model, data):

        if not hasattr(self,'sampler'):
            self.sampler = BlockGibbsSampler(
                rbm=model,
                particles=0.5+np.zeros((self.nchains,model.get_input_dim())),
                rng=model.rng,
                steps=self.nsteps)

        # compute negative phase updates
        sampler_updates = self.sampler.updates()

        # Compute SML cost
        pos_v = data
        neg_v = self.sampler.particles

        ml_cost = (model.free_energy(pos_v).mean()-
                   model.free_energy(neg_v).mean())

        return ml_cost

    def expr(self, model, data):
        return None

    def get_data_specs(self, model):
        return (model.get_input_space(), model.get_input_source())

class CDk(Cost):
    """
    Contrastive Divergence

    See "Training products of experts by minimizing contrastive divergence"
    by Geoffrey E. Hinton (2002)

    Parameters
    ----------
    nsteps : int
        Number of Markov chain steps for the negative sample
    seed : int
        Seed for the random number generator
    """
    def __init__(self, nsteps, seed=42):
        super(CDk, self).__init__()
        self.nsteps  = nsteps
        self.rng = make_theano_rng(seed, which_method='binomial')

    def _cost(self, model, data):
        pos_v = data
        neg_v = data

        for k in range(self.nsteps):
            [neg_v, _locals] = model.gibbs_step_for_v(neg_v,self.rng)

        # Compute CD cost
        ml_cost = (model.free_energy(pos_v).mean()-
                   model.free_energy(neg_v).mean())

        return ml_cost, neg_v

    def get_gradients(self, model, data, **kwargs):
        cost, neg_v = self._cost(model,data,**kwargs)

        params = list(model.get_params())

        grads = T.grad(cost, params, disconnected_inputs = 'ignore',
                       consider_constant = [neg_v])

        gradients = OrderedDict(izip(params, grads))

        updates = OrderedDict()

        return gradients, updates

    def expr(self, model, data):
        return None

    def get_data_specs(self, model):
        return (model.get_input_space(), model.get_input_source())

########NEW FILE########
__FILENAME__ = gsn
"""
.. todo::

    WRITEME
"""
from theano.compat.python2x import OrderedDict

from pylearn2.costs.cost import Cost
from pylearn2.costs.autoencoder import GSNFriendlyCost
from pylearn2.space import CompositeSpace
from pylearn2.utils import safe_zip

class GSNCost(Cost):
    """
    Customizable cost class for GSNs.

    This class currently can only handle datasets with only one or two sets
    of vectors. The get_input_source and get_target_source methods on the model
    instance are called to get the names for the fields in the dataset.
    get_input_source() is used for the name of the first set of vectors and
    get_target_source() is used for the second set of vectors.

    The explicit use of get_input_source and get_target_source (and the
    non-existance of similar hooks) is what limits this class to learning
    the joint distribution between only 2 sets of vectors. The allow for more
    than 2 sets of vectors, the Model class would need to be modified, preferably
    in a way that allows reference to arbitrarily many sets of vectors within
    one dataset.

    Parameters
    ----------
    costs : list of (int, double, GSNFriendlyCost or callable) tuples
        The int component of each tuple is the index of the layer at
        which we want to compute this cost.
        The double component of the tuple is the coefficient to associate
        to with the cost.
        The GSNFriendlyCost instance is the cost that will be computed.
        If that is a callable rather than an instance of GSN friendly
        cost, it will be called with 2 arguments: the initial value
        followed by the reconstructed value.
        Costs must be of length 1 or 2 (explained in docstring for
        GSNCost class) and the meaning of the ordering of the costs
        parameter is explained in the docstring for the mode parameter.
    walkback : int
        How many steps of walkback to perform
    mode : str
        Must be either 'joint', 'supervised', or 'anti_supervised'.
        The terms "input layer" and "label layer" are used below in the
        description of the modes. The "input layer" refers to the layer
        at the index specified in the first tuple in the costs parameter,
        and the "label layer" refers to the layer at the index specified
        in the second tuple in the costs parameter.
        'joint' means setting all of the layers and calculating
        reconstruction costs.
        'supervised' means setting just the input layer and attempting to
        predict the label layer
        'anti_supervised' is attempting to predict the input layer given
        the label layer.
    """

    def __init__(self, costs, walkback=0, mode="joint"):
        super(GSNCost, self).__init__()
        self.walkback = walkback

        assert mode in ["joint", "supervised", "anti_supervised"]
        if mode in ["supervised", "anti_supervised"]:
            assert len(costs) == 2
        self.mode = mode

        assert len(costs) in [1, 2], "This is (hopefully) a temporary restriction"
        assert len(set(c[0] for c in costs)) == len(costs), "Must have only" +\
            " one cost function per index"
        self.costs = costs

        # convert GSNFriendCost instances into just callables
        for i, cost_tup in enumerate(self.costs):
            if isinstance(cost_tup[2], GSNFriendlyCost):
                mutable = list(cost_tup)
                mutable[2] = cost_tup[2].cost
                self.costs[i] = tuple(mutable)
            else:
                assert callable(cost_tup[2])

    @staticmethod
    def _get_total_for_cost(idx, costf, init_data, model_output):
        """
        Computes the total cost contribution from one layer given the full
        output of the GSN.

        Parameters
        ----------
        idx : int
            init_data and model_output both contain a subset of the layer \
            activations at each time step. This is the index of the layer we \
            want to evaluate the cost on WITHIN this subset. This is \
            generally equal to the idx of the cost function within the \
            GSNCost.costs list.
        costf : callable
            Function of two variables that computes the cost. The first \
            argument is the target value, and the second argument is the \
            predicted value.
        init_data : list of tensor_likes
            Although only the element at index "idx" is accessed/needed, this \
            parameter is a list so that is can directly handle the data \
            format from GSN.expr.
        model_output : list of list of tensor_likes
            The output of GSN.get_samples as called by GSNCost.expr.
        """
        total = 0.0
        for step in model_output:
            total += costf(init_data[idx], step[idx])

        # normalize for number of steps
        return total / len(model_output)

    def _get_samples_from_model(self, model, data):
        """
        .. todo::

            WRITEME properly
        
        Handles the different GSNCost modes.
        """
        layer_idxs = [idx for idx, _, _ in self.costs]
        zipped = safe_zip(layer_idxs, data)
        if self.mode == "joint":
            use = zipped
        elif self.mode == "supervised":
            # don't include label layer
            use = zipped[:1]
        elif self.mode == "anti_supervised":
            # don't include features
            use = zipped[1:]
        else:
            raise ValueError("Unknown mode \"%s\" for GSNCost" % self.mode)

        return model.get_samples(use,
                                 walkback=self.walkback,
                                 indices=layer_idxs)

    def expr(self, model, data):
        """
        Parameters
        ----------
        model : GSN object
            WRITEME
        data : list of tensor_likes
            Data must be a list or tuple of the same length as self.costs. \
            All elements in data must be a tensor_like (cannot be None).

        Returns
        -------
        y : tensor_like
            The actual cost that is backpropagated on.
        """
        self.get_data_specs(model)[0].validate(data)
        output = self._get_samples_from_model(model, data)

        total = 0.0
        for cost_idx, (_, coeff, costf) in enumerate(self.costs):
            total += (coeff *
                      self._get_total_for_cost(cost_idx, costf, data, output))

        coeff_sum = sum(coeff for _, coeff, _ in self.costs)

        # normalize for coefficients on each cost
        return total / coeff_sum

    def get_monitoring_channels(self, model, data, **kwargs):
        """
        .. todo::

            WRITEME properly
        
        Provides monitoring of the individual costs that are being added together.

        This is a very useful method to subclass if you need to monitor more
        things about the model.
        """
        self.get_data_specs(model)[0].validate(data)

        rval = OrderedDict()

        # if there's only 1 cost, then no need to split up the costs
        if len(self.costs) > 1:
            output = self._get_samples_from_model(model, data)

            rval['reconstruction_cost'] =\
                self._get_total_for_cost(0, self.costs[0][2], data, output)

            rval['classification_cost'] =\
                self._get_total_for_cost(1, self.costs[1][2], data, output)

        return rval

    def get_data_specs(self, model):
        """
        .. todo::

            WRITEME
        """
        # get space for layer i of model
        get_space = lambda i: (model.aes[i].get_input_space() if i==0
                               else model.aes[i - 1].get_output_space())

        # get the spaces for layers that we have costs at
        spaces = map(lambda c: get_space(c[0]), self.costs)

        sources = [model.get_input_source()]
        if len(self.costs) == 2:
            sources.append(model.get_target_source())

        return (CompositeSpace(spaces), tuple(sources))

########NEW FILE########
__FILENAME__ = dropout
"""
Functionality for training with dropout.
"""
__authors__ = 'Ian Goodfellow'
__copyright__ = "Copyright 2013, Universite de Montreal"

from pylearn2.costs.cost import DefaultDataSpecsMixin, Cost

class Dropout(DefaultDataSpecsMixin, Cost):
    """
    Implements the dropout training technique described in
    "Improving neural networks by preventing co-adaptation of feature
    detectors"
    Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever,
    Ruslan R. Salakhutdinov
    arXiv 2012

    This paper suggests including each unit with probability p during training,
    then multiplying the outgoing weights by p at the end of training.
    We instead include each unit with probability p and divide its
    state by p during training. Note that this means the initial weights should
    be multiplied by p relative to Hinton's.
    The SGD learning rate on the weights should also be scaled by p^2 (use
    W_lr_scale rather than adjusting the global learning rate, because the
    learning rate on the biases should not be adjusted).

    During training, each input to each layer is randomly included or excluded
    for each example. The probability of inclusion is independent for each
    input and each example. Each layer uses "default_input_include_prob"
    unless that layer's name appears as a key in input_include_probs, in which
    case the input inclusion probability is given by the corresponding value.

    Each feature is also multiplied by a scale factor. The scale factor for
    each layer's input scale is determined by the same scheme as the input
    probabilities.

    Parameters
    ----------
    default_input_include_prob : float
        The probability of including a layer's input, unless that layer appears
        in `input_include_probs`
    input_include_probs : dict
        A dictionary mapping string layer names to float include probability
        values. Overrides `default_input_include_prob` for individual layers.
    default_input_scale : float
        During training, each layer's input is multiplied by this amount to
        compensate for fewer of the input units being present. Can be
        overridden by `input_scales`.
    input_scales : dict
        A dictionary mapping string layer names to float values to scale that
        layer's input by. Overrides `default_input_scale` for individual
        layers.
    per_example : bool
        If True, chooses separate units to drop for each example. If False,
        applies the same dropout mask to the entire minibatch.
    """

    supervised = True

    def __init__(self, default_input_include_prob=.5, input_include_probs=None,
            default_input_scale=2., input_scales=None, per_example=True):

        if input_include_probs is None:
            input_include_probs = {}

        if input_scales is None:
            input_scales = {}

        self.__dict__.update(locals())
        del self.self

    def expr(self, model, data, ** kwargs):
        """
        .. todo::

            WRITEME
        """
        space, sources = self.get_data_specs(model)
        space.validate(data)
        (X, Y) = data
        Y_hat = model.dropout_fprop(
            X,
            default_input_include_prob=self.default_input_include_prob,
            input_include_probs=self.input_include_probs,
            default_input_scale=self.default_input_scale,
            input_scales=self.input_scales,
            per_example=self.per_example
        )
        return model.cost(Y, Y_hat)

########NEW FILE########
__FILENAME__ = missing_target_cost
"""
.. todo::

    WRITEME
"""
__author__ = 'Vincent Archambault-Bouffard'

import theano.tensor as T
from pylearn2.costs.cost import Cost
from pylearn2.space import CompositeSpace


class MissingTargetCost(Cost):
    """
    A cost when some targets are missing. The missing target is indicated by a
    value of -1.

    Parameters
    ----------
    dropout_args : WRITEME
    """

    supervised = True

    def __init__(self, dropout_args=None):
        self.__dict__.update(locals())
        del self.self

    def expr(self, model, data):
        """
        .. todo::

            WRITEME
        """
        space, sources = self.get_data_specs(model)
        space.validate(data)
        (X, Y) = data
        if self.dropout_args:
            Y_hat = model.dropout_fprop(X, **self.dropout_args)
        else:
            Y_hat = model.fprop(X)
        costMatrix = model.layers[-1].cost_matrix(Y, Y_hat)
        costMatrix *= T.neq(Y, -1)  # This sets to zero all elements where Y == -1
        return model.cost_from_cost_matrix(costMatrix)

    def get_data_specs(self, model):
        """
        .. todo::

            WRITEME
        """
        space = CompositeSpace([model.get_input_space(), model.get_output_space()])
        sources = (model.get_input_source(), model.get_target_source())
        return (space, sources)

########NEW FILE########
__FILENAME__ = test_lp_penalty_cost
"""
Test LpPenalty cost
"""
import numpy
import theano
from pylearn2.models.mlp import Linear
from pylearn2.models.mlp import Softmax
from pylearn2.models.mlp import MLP
from pylearn2.costs.cost import LpPenalty


def test_correctness():
    model = MLP(
        layers=[Linear(dim=10, layer_name='linear', irange=1.0),
                Softmax(n_classes=2, layer_name='softmax', irange=1.0)],
        batch_size=10,
        nvis=10
    )

    cost = LpPenalty(variables=model.get_params(), p=2)

    penalty = cost.expr(model, None)

    penalty_function = theano.function(inputs=[], outputs=penalty)

    p = penalty_function()

    actual_p = 0
    for param in model.get_params():
        actual_p += numpy.sum(param.get_value() ** 2)

    assert numpy.allclose(p, actual_p)


if __name__ == '__main__':
    test_correctness()

########NEW FILE########
__FILENAME__ = test_sparse_activation
"""
Test autoencoder sparse activation cost.
"""
from pylearn2.config import yaml_parse


def test_sparse_activation():
    """Test autoencoder sparse activation cost."""
    trainer = yaml_parse.load(test_yaml)
    trainer.main_loop()

test_yaml = """
!obj:pylearn2.train.Train {
    dataset: &train
        !obj:pylearn2.testing.datasets.random_one_hot_dense_design_matrix
        {
            rng: !obj:numpy.random.RandomState { seed: 1 },
            num_examples: 10,
            dim: 5,
            num_classes: 2,
    },
    model: !obj:pylearn2.models.autoencoder.Autoencoder {
        nvis: 5,
        nhid: 10,
        act_enc: sigmoid,
        act_dec: linear,
    },
    algorithm: !obj:pylearn2.training_algorithms.bgd.BGD {
        batch_size: 5,
        line_search_mode: exhaustive,
        conjugate: 1,
        cost: !obj:pylearn2.costs.cost.SumOfCosts {
            costs: [
              !obj:pylearn2.costs.autoencoder.MeanSquaredReconstructionError {
              },
              !obj:pylearn2.costs.autoencoder.SparseActivation {
                  coeff: 0.5,
                  p: 0.2,
              },
            ],
        },
        monitoring_dataset: {
            'train': *train,
        },
        termination_criterion: !obj:pylearn2.termination_criteria.EpochCounter
        {
            max_epochs: 1,
        },
    },
}
"""

########NEW FILE########
__FILENAME__ = blocks
"""
Cross-validation with blocks.
"""
from pylearn2.blocks import StackedBlocks


class StackedBlocksCV(object):
    """
    Multi-layer transforms using cross-validation models.

    Parameters
    ----------
    layers : iterable (list of lists)
        Cross-validation models for each layer. Should be a list of lists,
        where the first index is for the layer and the second index is for
        the cross-validation fold.
    """
    def __init__(self, layers):
        stacked_blocks = []
        n_folds = len(layers[0])
        assert all([len(layer) == n_folds for layer in layers])

        # stack the k-th block from each layer
        for k in xrange(n_folds):
            this_blocks = []
            for i, layer in enumerate(layers):
                this_blocks.append(layer[k])
            this_stacked_blocks = StackedBlocks(this_blocks)
            stacked_blocks.append(this_stacked_blocks)

        # _folds contains a StackedBlocks instance for each CV fold
        self._folds = stacked_blocks

    def select_fold(self, k):
        """
        Choose a single cross-validation fold to represent.

        Parameters
        ----------
        k : int
            Index of selected fold.
        """
        return self._folds[k]

    def get_input_space(self):
        """Get input space."""
        return self._folds[0][0].get_input_space()

    def get_output_space(self):
        """Get output space."""
        return self._folds[0][-1].get_output_space()

    def set_input_space(self, space):
        """
        Set input space.

        Parameters
        ----------
        space : WRITEME
            Input space.
        """
        for fold in self._folds:
            this_space = space
            for layer in fold._layers:
                layer.set_input_space(this_space)
                this_space = layer.get_output_space()

########NEW FILE########
__FILENAME__ = dataset_iterators
"""
Cross-validation dataset iterators.
"""
import numpy as np
import warnings
try:
    from sklearn.cross_validation import (KFold, StratifiedKFold, ShuffleSplit,
                                          StratifiedShuffleSplit)
except ImportError:
    warnings.warn("Could not import from sklearn.")

from theano.compat import OrderedDict

from pylearn2.cross_validation.blocks import StackedBlocksCV
from pylearn2.datasets.dense_design_matrix import DenseDesignMatrix
from pylearn2.datasets.transformer_dataset import TransformerDataset


class DatasetCV(object):
    """
    Construct a new DenseDesignMatrix for each subset.

    Parameters
    ----------
    dataset : object
        Full dataset for use in cross validation.
    subset_iterator : iterable
        Iterable that returns (train, test) or (train, valid, test) indices
        or masks for partitioning the dataset during cross-validation.
    preprocessor : Preprocessor or None
        Preprocessor to apply to child datasets.
    fit_preprocessor : bool
        Whether preprocessor can fit parameters when applied to training
        data.
    which_set : str, list or None
        If None, return all subset datasets. If one or more of 'train',
        'valid', or 'test', return only the dataset(s) corresponding to the
        given subset(s).
    return_dict : bool
        Whether to return subset datasets as a dictionary. If True,
        returns a dict with keys 'train', 'valid', and/or 'test' (if
        subset_iterator returns two subsets per partition, 'train' and
        'test' are used, and if subset_iterator returns three subsets per
        partition, 'train', 'valid', and 'test' are used). If False,
        returns a list of datasets matching the subset order given by
        subset_iterator.
    """
    def __init__(self, dataset, subset_iterator, preprocessor=None,
                 fit_preprocessor=False, which_set=None, return_dict=True):
        self.dataset = dataset
        self.subset_iterator = list(subset_iterator)  # allow generator reuse
        dataset_iterator = dataset.iterator(mode='sequential', num_batches=1,
                                            data_specs=dataset.data_specs,
                                            return_tuple=True)
        self._data = dataset_iterator.next()
        self.preprocessor = preprocessor
        self.fit_preprocessor = fit_preprocessor
        self.which_set = which_set
        if which_set is not None:
            which_set = np.atleast_1d(which_set)
            assert len(which_set)
            for label in which_set:
                if label not in ['train', 'valid', 'test']:
                    raise ValueError("Unrecognized subset '{}'".format(label))
            self.which_set = which_set
        self.return_dict = return_dict

    def get_data_subsets(self):
        """
        Partition the dataset according to cross-validation subsets and
        return the raw data in each subset.
        """
        for subsets in self.subset_iterator:
            labels = None
            if len(subsets) == 3:
                labels = ['train', 'valid', 'test']
            elif len(subsets) == 2:
                labels = ['train', 'test']
            # data_subsets is an OrderedDict to maintain label order
            data_subsets = OrderedDict()
            for i, subset in enumerate(subsets):
                subset_data = tuple(data[subset] for data in self._data)
                if len(subset_data) == 2:
                    X, y = subset_data
                else:
                    X, = subset_data
                    y = None
                data_subsets[labels[i]] = (X, y)
            yield data_subsets

    def __iter__(self):
        """
        Create a DenseDesignMatrix for each dataset subset and apply any
        preprocessing to the child datasets.
        """
        for data_subsets in self.get_data_subsets():
            datasets = {}
            for label, data in data_subsets.items():
                X, y = data
                datasets[label] = DenseDesignMatrix(X=X, y=y)

            # preprocessing
            if self.preprocessor is not None:
                self.preprocessor.apply(datasets['train'],
                                        can_fit=self.fit_preprocessor)
                for label, dataset in datasets.items():
                    if label == 'train':
                        continue
                    self.preprocessor.apply(dataset, can_fit=False)

            # which_set
            if self.which_set is not None:
                for label, dataset in datasets.items():
                    if label not in self.which_set:
                        del datasets[label]
                        del data_subsets[label]
                if not len(datasets):
                    raise ValueError("No matching dataset(s) for " +
                                     "{}".format(self.which_set))

            if not self.return_dict:
                # data_subsets is an OrderedDict to maintain label order
                datasets = list(datasets[label]
                                for label in data_subsets.keys())
                if len(datasets) == 1:
                    datasets, = datasets
            yield datasets


class StratifiedDatasetCV(DatasetCV):
    """
    Subclass of DatasetCV for stratified experiments, where
    the relative class proportions of the full dataset are maintained in
    each partition.

    Parameters
    ----------
    dataset : object
        Full dataset for use in cross validation.
    subset_iterator : iterable
        Iterable that returns (train, test) or (train, valid, test) indices
        or masks for partitioning the dataset during cross-validation.
    preprocessor : Preprocessor or None
        Preprocessor to apply to child datasets.
    fit_preprocessor : bool
        Whether preprocessor can fit parameters when applied to training
        data.
    which_set : str, list or None
        If None, return all subset datasets. If one or more of 'train',
        'valid', or 'test', return only the dataset(s) corresponding to the
        given subset(s).
    return_dict : bool
        Whether to return subset datasets as a dictionary. If True,
        returns a dict with keys 'train', 'valid', and/or 'test' (if
        subset_iterator returns two subsets per partition, 'train' and
        'test' are used, and if subset_iterator returns three subsets per
        partition, 'train', 'valid', and 'test' are used). If False,
        returns a list of datasets matching the subset order given by
        subset_iterator.
    """
    @staticmethod
    def get_y(dataset):
        """
        Stratified cross-validation requires label information for
        examples. This function gets target values for a dataset,
        converting from one-hot encoding to a 1D array as needed.

        Parameters
        ----------
        dataset : object
            Dataset containing target values for examples.
        """
        y = np.asarray(dataset.y)
        if y.ndim > 1:
            assert np.array_equal(np.unique(y), [0, 1])
            y = np.argmax(y, axis=1)
        return y


class TransformerDatasetCV(object):
    """
    Cross-validation with dataset transformations. This class returns
    dataset subsets after transforming them with one or more pretrained
    models.

    Parameters
    ----------
    dataset_iterator : DatasetCV
        Cross-validation iterator providing (test, train) or (test, valid,
        train) indices for partitioning the dataset.
    transformers : Model or iterable
        Transformer model(s) to use for transforming datasets.
    """
    def __init__(self, dataset_iterator, transformers):
        self.dataset_iterator = dataset_iterator
        self.transformers = transformers

    def __iter__(self):
        """
        Construct a Transformer dataset for each partition.
        """
        for k, datasets in enumerate(self.dataset_iterator):
            if isinstance(self.transformers, list):
                transformer = self.transformers[k]
            elif isinstance(self.transformers, StackedBlocksCV):
                transformer = self.transformers.select_fold(k)
            else:
                transformer = self.transformers
            if isinstance(datasets, list):
                for i, dataset in enumerate(datasets):
                    datasets[i] = TransformerDataset(dataset, transformer)
            else:
                for key, dataset in datasets.items():
                    datasets[key] = TransformerDataset(dataset, transformer)
            yield datasets


class DatasetKFold(DatasetCV):
    """
    K-fold cross-validation.

    Parameters
    ----------
    dataset : object
        Dataset to use for cross-validation.
    n_folds : int
        Number of cross-validation folds.
    indices : bool
        Whether to return indices for dataset slicing. If false, returns
        a boolean mask.
    shuffle : bool
        Whether to shuffle the dataset before partitioning.
    random_state : int or RandomState
        Random number generator used for shuffling.
    kwargs : dict
        Keyword arguments for DatasetCV.
    """
    def __init__(self, dataset, n_folds=3, indices=True, shuffle=False,
                 random_state=None, **kwargs):
        n = dataset.X.shape[0]
        cv = KFold(n, n_folds, indices, shuffle, random_state)
        super(DatasetKFold, self).__init__(dataset, cv, **kwargs)


class StratifiedDatasetKFold(StratifiedDatasetCV):
    """
    Stratified K-fold cross-validation.

    Parameters
    ----------
    dataset : object
        Dataset to use for cross-validation.
    n_folds : int
        Number of cross-validation folds.
    indices : bool
        Whether to return indices for dataset slicing. If false, returns
        a boolean mask.
    kwargs : dict
        Keyword arguments for DatasetCV.
    """
    def __init__(self, dataset, n_folds=3, indices=True, **kwargs):
        y = self.get_y(dataset)
        cv = StratifiedKFold(y, n_folds, indices)
        super(StratifiedDatasetKFold, self).__init__(dataset, cv, **kwargs)


class DatasetShuffleSplit(DatasetCV):
    """
    Shuffle-split cross-validation.

    Parameters
    ----------
    dataset : object
        Dataset to use for cross-validation.
    n_iter : int
        Number of shuffle-split iterations.
    test_size : float, int, or None
        If float, intepreted as the proportion of examples in the test set.
        If int, interpreted as the absolute number of examples in the test
        set. If None, adjusted to the complement of train_size.
    train_size : float, int, or None
        If float, intepreted as the proportion of examples in the training
        set. If int, interpreted as the absolute number of examples in the
        training set. If None, adjusted to the complement of test_size.
    indices : bool
        Whether to return indices for dataset slicing. If false, returns
        a boolean mask.
    random_state : int or RandomState
        Random number generator used for shuffling.
    kwargs : dict
        Keyword arguments for DatasetCV.
    """
    def __init__(self, dataset, n_iter=10, test_size=0.1, train_size=None,
                 indices=True, random_state=None, **kwargs):
        n = dataset.X.shape[0]
        cv = ShuffleSplit(n, n_iter, test_size, train_size, indices,
                          random_state)
        super(DatasetShuffleSplit, self).__init__(dataset, cv, **kwargs)


class StratifiedDatasetShuffleSplit(StratifiedDatasetCV):
    """
    Stratified shuffle-split cross-validation.

    Parameters
    ----------
    dataset : object
        Dataset to use for cross-validation.
    n_iter : int
        Number of shuffle-split iterations.
    test_size : float, int, or None
        If float, intepreted as the proportion of examples in the test set.
        If int, interpreted as the absolute number of examples in the test
        set. If None, adjusted to the complement of train_size.
    train_size : float, int, or None
        If float, intepreted as the proportion of examples in the training
        set. If int, interpreted as the absolute number of examples in the
        training set. If None, adjusted to the complement of test_size.
    indices : bool
        Whether to return indices for dataset slicing. If false, returns
        a boolean mask.
    random_state : int or RandomState
        Random number generator used for shuffling.
    kwargs : dict
        Keyword arguments for DatasetCV.
    """
    def __init__(self, dataset, n_iter=10, test_size=0.1, train_size=None,
                 indices=True, random_state=None, **kwargs):
        y = self.get_y(dataset)
        cv = StratifiedShuffleSplit(y, n_iter, test_size, train_size, indices,
                                    random_state)
        super(StratifiedDatasetShuffleSplit, self).__init__(dataset, cv,
                                                            **kwargs)

########NEW FILE########
__FILENAME__ = test_cross_validation
"""
Tests for cross-validation module.
"""
import os
import tempfile

from pylearn2.config import yaml_parse
from pylearn2.testing.skip import skip_if_no_sklearn


def test_train_cv():
    """Test TrainCV class."""
    skip_if_no_sklearn()
    handle, layer0_filename = tempfile.mkstemp()
    handle, layer1_filename = tempfile.mkstemp()
    handle, layer2_filename = tempfile.mkstemp()

    # train the first hidden layer (unsupervised)
    # (test for TrainCV)
    trainer = yaml_parse.load(test_yaml_layer0 %
                              {'layer0_filename': layer0_filename})
    trainer.main_loop()

    # train the second hidden layer (unsupervised)
    # (test for TransformerDatasetCV)
    trainer = yaml_parse.load(test_yaml_layer1 %
                              {'layer0_filename': layer0_filename,
                               'layer1_filename': layer1_filename})
    trainer.main_loop()

    # train the third hidden layer (unsupervised)
    # (test for StackedBlocksCV)
    trainer = yaml_parse.load(test_yaml_layer2 %
                              {'layer0_filename': layer0_filename,
                               'layer1_filename': layer1_filename,
                               'layer2_filename': layer2_filename})
    trainer.main_loop()

    # train the full model (supervised)
    # (test for PretrainedLayerCV)
    trainer = yaml_parse.load(test_yaml_layer3 %
                              {'layer0_filename': layer0_filename,
                               'layer1_filename': layer1_filename,
                               'layer2_filename': layer2_filename})
    trainer.main_loop()

    # clean up
    os.remove(layer0_filename)
    os.remove(layer1_filename)


def test_dataset_k_fold():
    """Test DatasetKFold."""
    skip_if_no_sklearn()
    mapping = {'dataset_iterator': 'DatasetKFold'}
    test_yaml = test_yaml_dataset_iterator % mapping
    trainer = yaml_parse.load(test_yaml)
    trainer.main_loop()


def test_stratified_dataset_k_fold():
    """Test StratifiedDatasetKFold."""
    skip_if_no_sklearn()
    mapping = {'dataset_iterator': 'StratifiedDatasetKFold'}
    test_yaml = test_yaml_dataset_iterator % mapping
    trainer = yaml_parse.load(test_yaml)
    trainer.main_loop()


def test_dataset_shuffle_split():
    """Test DatasetShuffleSplit."""
    skip_if_no_sklearn()
    mapping = {'dataset_iterator': 'DatasetShuffleSplit'}
    test_yaml = test_yaml_dataset_iterator % mapping
    trainer = yaml_parse.load(test_yaml)
    trainer.main_loop()


def test_stratified_dataset_shuffle_split():
    """Test StratifiedDatasetShuffleSplit."""
    skip_if_no_sklearn()
    mapping = {'dataset_iterator': 'StratifiedDatasetShuffleSplit'}
    test_yaml = test_yaml_dataset_iterator % mapping
    trainer = yaml_parse.load(test_yaml)
    trainer.main_loop()


def test_which_set():
    """Test which_set selector."""
    skip_if_no_sklearn()

    # one label
    # must be 'train' or Train object won't have any data to train on
    this_yaml = test_yaml_which_set % {'which_set': 'train'}
    trainer = yaml_parse.load(this_yaml)
    trainer.main_loop()

    # one label, not 'train'
    # this tests any internal stuff in the dataset iterator that might
    # require training data, such as preprocessing
    try:
        this_yaml = test_yaml_which_set % {'which_set': 'test'}
        trainer = yaml_parse.load(this_yaml)
        trainer.main_loop()
    except KeyError:
        pass

    # multiple labels
    this_yaml = test_yaml_which_set % {'which_set': ['train', 'test']}
    trainer = yaml_parse.load(this_yaml)
    trainer.main_loop()

    # improper label (this iterator only returns 'train' and 'test' subsets)
    this_yaml = test_yaml_which_set % {'which_set': 'valid'}
    try:
        trainer = yaml_parse.load(this_yaml)
        trainer.main_loop()
        raise AssertionError
    except ValueError:
        pass

    # bogus label (not in approved list)
    this_yaml = test_yaml_which_set % {'which_set': 'bogus'}
    try:
        yaml_parse.load(this_yaml)
        raise AssertionError
    except ValueError:
        pass


def test_no_targets():
    """Test cross-validation without targets."""
    skip_if_no_sklearn()
    trainer = yaml_parse.load(test_yaml_no_targets)
    trainer.main_loop()


def test_preprocessing():
    """Test cross-validation with preprocessing."""
    skip_if_no_sklearn()
    trainer = yaml_parse.load(test_yaml_preprocessing)
    trainer.main_loop()

test_yaml_layer0 = """
!obj:pylearn2.cross_validation.TrainCV {
    dataset_iterator:
        !obj:pylearn2.cross_validation.dataset_iterators.DatasetKFold {
        dataset:
            !obj:pylearn2.testing.datasets.random_one_hot_dense_design_matrix
            {
                rng: !obj:numpy.random.RandomState { seed: 1 },
                num_examples: 100,
                dim: 10,
                num_classes: 2,
            },
    },
    model: !obj:pylearn2.models.autoencoder.Autoencoder {
        nvis: 10,
        nhid: 8,
        act_enc: 'sigmoid',
        act_dec: 'linear'
    },
    algorithm: !obj:pylearn2.training_algorithms.bgd.BGD {
        batch_size: 50,
        line_search_mode: 'exhaustive',
        conjugate: 1,
        termination_criterion:
            !obj:pylearn2.termination_criteria.EpochCounter {
                    max_epochs: 1,
        },
        cost: !obj:pylearn2.costs.autoencoder.MeanSquaredReconstructionError {
        },
    },
    save_path: %(layer0_filename)s,
}
"""

test_yaml_layer1 = """
!obj:pylearn2.cross_validation.TrainCV {
    dataset_iterator:
    !obj:pylearn2.cross_validation.dataset_iterators.TransformerDatasetCV {
        dataset_iterator:
            !obj:pylearn2.cross_validation.dataset_iterators.DatasetKFold {
            dataset:
            !obj:pylearn2.testing.datasets.random_one_hot_dense_design_matrix
                {
                    rng: !obj:numpy.random.RandomState { seed: 1 },
                    num_examples: 100,
                    dim: 10,
                    num_classes: 2,
                },
        },
        transformers: !pkl: %(layer0_filename)s,
    },
    model: !obj:pylearn2.models.autoencoder.Autoencoder {
        nvis: 8,
        nhid: 6,
        act_enc: 'sigmoid',
        act_dec: 'linear'
    },
    algorithm: !obj:pylearn2.training_algorithms.bgd.BGD {
        batch_size: 50,
        line_search_mode: 'exhaustive',
        conjugate: 1,
        termination_criterion:
            !obj:pylearn2.termination_criteria.EpochCounter {
                    max_epochs: 1,
        },
        cost: !obj:pylearn2.costs.autoencoder.MeanSquaredReconstructionError {
        },
    },
    save_path: %(layer1_filename)s,
}
"""

test_yaml_layer2 = """
!obj:pylearn2.cross_validation.TrainCV {
    dataset_iterator:
    !obj:pylearn2.cross_validation.dataset_iterators.TransformerDatasetCV {
        dataset_iterator:
        !obj:pylearn2.cross_validation.dataset_iterators.DatasetKFold {
            dataset:
            !obj:pylearn2.testing.datasets.random_one_hot_dense_design_matrix
            {
                rng: !obj:numpy.random.RandomState { seed: 1 },
                num_examples: 100,
                dim: 10,
                num_classes: 2,
            },
        },
        transformers: !obj:pylearn2.cross_validation.blocks.StackedBlocksCV {
            layers: [
                !pkl: %(layer0_filename)s,
                !pkl: %(layer1_filename)s,
            ],
        },
    },
    model: !obj:pylearn2.models.autoencoder.Autoencoder {
        nvis: 6,
        nhid: 4,
        act_enc: 'sigmoid',
        act_dec: 'linear'
    },
    algorithm: !obj:pylearn2.training_algorithms.bgd.BGD {
        batch_size: 50,
        line_search_mode: 'exhaustive',
        conjugate: 1,
        termination_criterion:
            !obj:pylearn2.termination_criteria.EpochCounter {
                    max_epochs: 1,
        },
        cost: !obj:pylearn2.costs.autoencoder.MeanSquaredReconstructionError {
        },
    },
    save_path: %(layer2_filename)s,
}
"""

test_yaml_layer3 = """
!obj:pylearn2.cross_validation.TrainCV {
    dataset_iterator:
        !obj:pylearn2.cross_validation.dataset_iterators.DatasetKFold {
        dataset:
      &train !obj:pylearn2.testing.datasets.random_one_hot_dense_design_matrix
      {
            rng: !obj:numpy.random.RandomState { seed: 1 },
            num_examples: 100,
            dim: 10,
            num_classes: 2,
      },
    },
    model: !obj:pylearn2.models.mlp.MLP {
        nvis: 10,
        layers: [
            !obj:pylearn2.cross_validation.mlp.PretrainedLayerCV {
                layer_name: 'h0',
                layer_content: !pkl: %(layer0_filename)s,
            },
            !obj:pylearn2.cross_validation.mlp.PretrainedLayerCV {
                layer_name: 'h1',
                layer_content: !pkl: %(layer1_filename)s,
            },
            !obj:pylearn2.cross_validation.mlp.PretrainedLayerCV {
                layer_name: 'h2',
                layer_content: !pkl: %(layer2_filename)s,
            },
            !obj:pylearn2.models.mlp.Softmax {
                layer_name: 'y',
                n_classes: 2,
                irange: 0.,
            },
        ],
    },
    algorithm: !obj:pylearn2.training_algorithms.bgd.BGD {
        batch_size: 50,
        line_search_mode: 'exhaustive',
        conjugate: 1,
        termination_criterion:
            !obj:pylearn2.termination_criteria.EpochCounter {
                    max_epochs: 1,
        },
    },
}
"""

test_yaml_dataset_iterator = """
!obj:pylearn2.cross_validation.TrainCV {
    dataset_iterator:
        !obj:pylearn2.cross_validation.dataset_iterators.%(dataset_iterator)s {
        dataset:
            !obj:pylearn2.testing.datasets.random_one_hot_dense_design_matrix
            {
                rng: !obj:numpy.random.RandomState { seed: 1 },
                num_examples: 100,
                dim: 10,
                num_classes: 2,
            },
    },
    model: !obj:pylearn2.models.autoencoder.Autoencoder {
        nvis: 10,
        nhid: 8,
        act_enc: 'sigmoid',
        act_dec: 'linear'
    },
    algorithm: !obj:pylearn2.training_algorithms.bgd.BGD {
        batch_size: 50,
        line_search_mode: 'exhaustive',
        conjugate: 1,
        termination_criterion:
            !obj:pylearn2.termination_criteria.EpochCounter {
                    max_epochs: 1,
        },
        cost: !obj:pylearn2.costs.autoencoder.MeanSquaredReconstructionError {
        },
    },
}
"""

test_yaml_which_set = """
!obj:pylearn2.cross_validation.TrainCV {
    dataset_iterator:
        !obj:pylearn2.cross_validation.dataset_iterators.DatasetKFold {
        dataset:
            !obj:pylearn2.testing.datasets.random_one_hot_dense_design_matrix
            {
                rng: !obj:numpy.random.RandomState { seed: 1 },
                num_examples: 100,
                dim: 10,
                num_classes: 2,
            },
        which_set: %(which_set)s,
    },
    model: !obj:pylearn2.models.autoencoder.Autoencoder {
        nvis: 10,
        nhid: 8,
        act_enc: 'sigmoid',
        act_dec: 'linear'
    },
    algorithm: !obj:pylearn2.training_algorithms.bgd.BGD {
        batch_size: 50,
        line_search_mode: 'exhaustive',
        conjugate: 1,
        termination_criterion:
            !obj:pylearn2.termination_criteria.EpochCounter {
                    max_epochs: 1,
        },
        cost: !obj:pylearn2.costs.autoencoder.MeanSquaredReconstructionError {
        },
    },
}
"""

test_yaml_no_targets = """
!obj:pylearn2.cross_validation.TrainCV {
    dataset_iterator:
        !obj:pylearn2.cross_validation.dataset_iterators.DatasetKFold {
        dataset:
            !obj:pylearn2.testing.datasets.random_dense_design_matrix
            {
                rng: !obj:numpy.random.RandomState { seed: 1 },
                num_examples: 100,
                dim: 10,
                num_classes: 0,
            },
    },
    model: !obj:pylearn2.models.autoencoder.Autoencoder {
        nvis: 10,
        nhid: 8,
        act_enc: 'sigmoid',
        act_dec: 'linear'
    },
    algorithm: !obj:pylearn2.training_algorithms.bgd.BGD {
        batch_size: 50,
        line_search_mode: 'exhaustive',
        conjugate: 1,
        termination_criterion:
            !obj:pylearn2.termination_criteria.EpochCounter {
                    max_epochs: 1,
        },
        cost: !obj:pylearn2.costs.autoencoder.MeanSquaredReconstructionError {
        },
    },
}
"""

test_yaml_preprocessing = """
!obj:pylearn2.cross_validation.TrainCV {
    dataset_iterator:
        !obj:pylearn2.cross_validation.dataset_iterators.DatasetKFold {
        dataset:
            !obj:pylearn2.testing.datasets.random_one_hot_dense_design_matrix
            {
                rng: !obj:numpy.random.RandomState { seed: 1 },
                num_examples: 100,
                dim: 10,
                num_classes: 2,
            },
        preprocessor: !obj:pylearn2.datasets.preprocessing.Standardize {},
        fit_preprocessor: 1,
    },
    model: !obj:pylearn2.models.autoencoder.Autoencoder {
        nvis: 10,
        nhid: 8,
        act_enc: 'sigmoid',
        act_dec: 'linear'
    },
    algorithm: !obj:pylearn2.training_algorithms.bgd.BGD {
        batch_size: 50,
        line_search_mode: 'exhaustive',
        conjugate: 1,
        termination_criterion:
            !obj:pylearn2.termination_criteria.EpochCounter {
                    max_epochs: 1,
        },
        cost: !obj:pylearn2.costs.autoencoder.MeanSquaredReconstructionError {
        },
    },
}
"""

########NEW FILE########
__FILENAME__ = adult
"""
Wrapper for the Adult UCI dataset:
http://archive.ics.uci.edu/ml/datasets/Adult
"""
__author__ = "Ian Goodfellow"

import numpy as np
import os

from pylearn2.datasets.dense_design_matrix import DenseDesignMatrix
from pylearn2.format.target_format import convert_to_one_hot
from pylearn2.utils.string_utils import preprocess


def adult(which_set):
    """
    Parameters
    ----------
    which_set : str
        'train' or 'test'

    Returns
    -------
    adult : DenseDesignMatrix
        Contains the Adult dataset.

    Notes
    -----
    This discards all examples with missing features. It would be trivial
    to modify this code to not do so, provided with a convention for how to
    treat the missing features.
    Categorical values are converted into a one-hot code.
    """

    base_path = os.path.join(preprocess("${PYLEARN2_DATA_PATH}"), "adult")

    set_file = {'train' : 'adult.data', 'test' : 'adult.test'}[which_set]

    full_path = os.path.join(base_path, set_file)

    content = open(full_path, 'r').readlines()

    # strip off stupid header line they added to only the test set
    if which_set == 'test':
        content = content[1:]
    # strip off empty final line
    content = content[:-1]

    # verify # of examples
    num_examples = {'train': 32561, 'test' : 16281}[which_set]
    assert len(content) == num_examples, (len(content), num_examples)

    # strip out examples with missing features, verify number of remaining
    # examples
    content = [line for line in content if line.find('?') == -1]
    num_examples = {'train': 30162, 'test' : 15060}[which_set]
    assert len(content) == num_examples

    # strip off endlines, separate entries
    content = map(lambda l: l[:-1].split(', '), content)

    # split data into features and targets
    features = map(lambda l: l[:-1], content)
    targets = map(lambda l: l[-1], content)
    del content

    # convert targets to binary
    assert all(map(lambda l: l in ['>50K', '<=50K', '>50K.', '<=50K.'],
        targets))
    y = map(lambda l: l[0] == '>', targets)
    y = np.array(y)
    del targets

    # Process features into a design matrix
    variables = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',
            'marital-status', 'occupation', 'relationship', 'race', 'sex',
            'capital-gain', 'capital-loss', 'hours-per-week', 'native-country']
    continuous = set(['age', 'fnlwgt', 'education-num', 'capital-gain',
        'capital-loss', 'hours-per-week'])
    assert all(var in variables for var in continuous)
    assert all(map(lambda l: len(l) == len(variables), features))
    pieces = []

    for i, var in enumerate(variables):
        data = map(lambda l: l[i], features)
        assert len(data) == num_examples
        if var in continuous:
            data = map(lambda l: float(l), data)
            data = np.array(data)
            data = data.reshape(data.shape[0], 1)
        else:
            unique_values = list(set(data))
            data = map(lambda l: unique_values.index(l), data)
            data = convert_to_one_hot(data)
        pieces.append(data)

    X = np.concatenate(pieces, axis=1)

    return DenseDesignMatrix(X=X, y=y)

if __name__ == "__main__":
    adult(which_set='train')
    adult(which_set='test')

########NEW FILE########
__FILENAME__ = avicenna
"""
.. todo::

    WRITEME
"""
from pylearn2.datasets import utlc
import numpy as N

class Avicenna(object):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    which_set : WRITEME
    standardize : WRITEME
    """
    def __init__(self, which_set, standardize):
        train, valid, test = utlc.load_ndarray_dataset('avicenna')

        if which_set == 'train':
            self.X = train
        elif which_set == 'valid':
            self.X = valid
        elif which_set == 'test':
            self.X = test
        else:
            assert False

        if standardize:
            union = N.concatenate([train,valid,test],axis=0)
            self.X -= union.mean(axis=0)
            std = union.std(axis=0)
            std[std < 1e-3] = 1e-3
            self.X /= std

    def get_design_matrix(self):
        """
        .. todo::

            WRITEME
        """
        return self.X


########NEW FILE########
__FILENAME__ = binarizer
"""
.. todo::

    WRITEME
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

from pylearn2.datasets.transformer_dataset import TransformerDataset
from pylearn2.expr.sampling import SampleBernoulli


class Binarizer(TransformerDataset):
    """
    A TransformerDataset that takes examples with features in the interval
    [0,1], and uses these as Bernoulli parameters to sample examples
    with features in {0,1}.

    Parameters
    ----------
    raw : pylearn2 Dataset
        It must provide examples with features in the interval [0,1].
    seed : integer or list of integers, optional
        The seed passed to MRG_RandomStreams to make the Bernoulli
        samples. If not specified, all class instances default to
        the same seed so two instances can be run synchronized side
        by side.
    """

    def __init__(self, raw, seed=None):
        transformer = SampleBernoulli(seed=seed)

        super(Binarizer, self).__init__(raw, transformer, space_preserving=True)


    def get_design_matrix(self, topo=None):
        """
        .. todo::

            WRITEME
        """
        if topo is not None:
            return self.raw.get_design_matrix(topo)
        X = self.raw.get_design_matrix()
        return self.transformer.perform(X)

########NEW FILE########
__FILENAME__ = cache
"""
Dataset preloading tool

This file provides the ability to make a local cache of a dataset or
part of it. It is meant to help in the case where multiple jobs are
reading the same dataset from ${PYLEARN2_DATA_PATH}, which may cause a
great burden on the network.

With this file, it is possible to make a local copy
(in ${PYLEARN2_LOCAL_DATA_PATH}) of any required file and have multiple
processes use it simultaneously instead of each acquiring its own copy
over the network.

Whenever a folder or a dataset copy is created locally, it is granted
the same access as it has under ${PYLEARN2_LOCAL_DATA_PATH}. This is
gauranteed by default copy.

"""

import atexit
import logging
import os
import stat
import time

import theano.gof.compilelock as compilelock

from pylearn2.utils import string_utils


log = logging.getLogger(__name__)


class LocalDatasetCache:
    """
    A local cache for remote files for faster access and reducing
    network stress.
    """

    def __init__(self):
        default_path = '${PYLEARN2_DATA_PATH}'
        local_path = '${PYLEARN2_LOCAL_DATA_PATH}'
        self.pid = os.getpid()

        try:
            self.dataset_remote_dir = string_utils.preprocess(default_path)
            self.dataset_local_dir = string_utils.preprocess(local_path)
        except (ValueError, string_utils.NoDataPathError,
                string_utils.EnvironmentVariableError):
            # Local cache seems to be deactivated
            self.dataset_remote_dir = ""
            self.dataset_local_dir = ""

        if self.dataset_remote_dir == "" or self.dataset_local_dir == "":
            log.debug("Local dataset cache is deactivated")

    def cache_file(self, filename):
        """
        Caches a file locally if possible. If caching was succesfull, or if
        the file was previously successfully cached, this method returns the
        path to the local copy of the file. If not, it returns the path to
        the original file.

        Parameters
        ----------
        filename : string
            Remote file to cache locally

        Returns
        -------
        output : string
            Updated (if needed) filename to use to access the remote
            file.
        """

        remote_name = string_utils.preprocess(filename)

        # Check if a local directory for data has been defined. Otherwise,
        # do not locally copy the data
        if self.dataset_local_dir == "":
            return filename

        # Make sure the file to cache exists and really is a file
        if not os.path.exists(remote_name):
            log.error("Error : Specified file %s does not exist" %
                      remote_name)
            return filename

        if not os.path.isfile(remote_name):
            log.error("Error : Specified name %s is not a file" %
                      remote_name)
            return filename

        if not remote_name.startswith(self.dataset_remote_dir):
            log.warning(
                "We cache in the local directory only what is"
                " under $PYLEARN2_DATA_PATH: %s" %
                remote_name)
            return filename

        # Create the $PYLEARN2_LOCAL_DATA_PATH folder if needed
        self.safe_mkdir(self.dataset_local_dir)

        # Determine local path to which the file is to be cached
        local_name = os.path.join(self.dataset_local_dir,
                                  os.path.relpath(remote_name,
                                                  self.dataset_remote_dir))

        # Create the folder structure to receive the remote file
        local_folder = os.path.split(local_name)[0]
        self.safe_mkdir(local_folder)

        # Acquire writelock on the local file to prevent the possibility
        # of any other process modifying it while we cache it if needed.
        # Also, if another process is currently caching the same file,
        # it forces the current process to wait for it to be done before
        # using the file.
        self.get_writelock(local_name)

        # If the file does not exist locally, consider creating it
        if not os.path.exists(local_name):

            # Check that there is enough space to cache the file
            if not self.check_enough_space(remote_name, local_name):
                log.warning("File %s not cached: Not enough free space" %
                            remote_name)
                self.release_writelock()
                return filename

            # There is enough space; make a local copy of the file
            self.copy_from_server_to_local(remote_name, local_name)
            log.info("File %s has been locally cached to %s" %
                     (remote_name, local_name))
        elif os.path.getmtime(remote_name) > os.path.getmtime(local_name):
            log.warning("File %s in cache will not be used: The remote file "
                        "(modified %s) is newer than the locally cached file "
                        "%s (modified %s)."
                        % (remote_name,
                           time.strftime(
                               '%Y-%m-%d %H:%M:%S',
                               time.localtime(os.path.getmtime(remote_name))
                           ),
                           local_name,
                           time.strftime(
                               '%Y-%m-%d %H:%M:%S',
                               time.localtime(os.path.getmtime(local_name))
                           )))
            return filename
        elif os.path.getsize(local_name) != os.path.getsize(remote_name):
            log.warning("File %s not cached: The remote file (%d bytes) is of "
                        "a different size than the locally cached file %s "
                        "(%d bytes). The local cache might be corrupt."
                        % (remote_name, os.path.getsize(remote_name),
                           local_name, os.path.getsize(local_name)))
            return filename
        else:
            log.debug("File %s has previously been locally cached to %s" %
                      (remote_name, local_name))

        # Obtain a readlock on the downloaded file before releasing the
        # writelock. This is to prevent having a moment where there is no
        # lock on this file which could give the impression that it is
        # unused and therefore safe to delete.
        self.get_readlock(local_name)
        self.release_writelock()

        return local_name

    def copy_from_server_to_local(self, remote_fname, local_fname):
        """
        Copies a remote file locally

        Parameters
        ----------
        remote_fname : string
            Remote file to copy
        local_fname : string
            Path and name of the local copy to be made of the remote
            file.
        """

        head, tail = os.path.split(local_fname)
        head += os.path.sep
        if not os.path.exists(head):
            os.makedirs(os.path.dirname(head))

        command = 'cp ' + remote_fname + ' ' + local_fname
        os.system(command)
        # Copy the original group id and file permission
        st = os.stat(remote_fname)
        os.chmod(local_fname, st.st_mode)
        # If the user have read access to the data, but not a member
        # of the group, he can't set the group. So we must catch the
        # exception. But we still want to do this, for directory where
        # only member of the group can read that data.
        try:
            os.chown(local_fname, -1, st.st_gid)
        except OSError:
            pass

        # Need to give group write permission to the folders
        # For the locking mechanism
        # Try to set the original group as above
        dirs = os.path.dirname(local_fname).replace(self.dataset_local_dir, '')
        sep = dirs.split(os.path.sep)
        if sep[0] == "":
            sep = sep[1:]
        for i in range(len(sep)):
            orig_p = os.path.join(self.dataset_remote_dir, *sep[:i + 1])
            new_p = os.path.join(self.dataset_local_dir, *sep[:i + 1])
            orig_st = os.stat(orig_p)
            new_st = os.stat(new_p)
            if not new_st.st_mode & stat.S_IWGRP:
                os.chmod(new_p, new_st.st_mode | stat.S_IWGRP)
            if orig_st.st_gid != new_st.st_gid:
                try:
                    os.chown(new_p, -1, orig_st.st_gid)
                except OSError:
                    pass

    def disk_usage(self, path):
        """
        Return free usage about the given path, in bytes

        Parameters
        ----------
        path : string
            Folder for which to return disk usage

        Returns
        -------
        output : tuple
            Tuple containing total space in the folder and currently
            used space in the folder
        """

        st = os.statvfs(path)
        total = st.f_blocks * st.f_frsize
        used = (st.f_blocks - st.f_bfree) * st.f_frsize
        return total, used

    def check_enough_space(self, remote_fname, local_fname,
                           max_disk_usage=0.9):
        """
        Check if the given local folder has enough space to store
        the specified remote file

        Parameters
        ----------
        remote_fname : string
            Path to the remote file
        remote_fname : string
            Path to the local folder
        max_disk_usage : float
            Fraction indicating how much of the total space in the
            local folder can be used before the local cache must stop
            adding to it.

        Returns
        -------
        output : boolean
            True if there is enough space to store the remote file.
        """

        storage_need = os.path.getsize(remote_fname)
        storage_total, storage_used = self.disk_usage(self.dataset_local_dir)

        # Instead of only looking if there's enough space, we ensure we do not
        # go over max disk usage level to avoid filling the disk/partition
        return ((storage_used + storage_need) <
                (storage_total * max_disk_usage))

    def safe_mkdir(self, folderName):
        """
        Create the specified folder. If the parent folders do not
        exist, they are also created. If the folder already exists,
        nothing is done.

        Parameters
        ----------
        folderName : string
            Name of the folder to create
        """
        if not os.path.exists(folderName):
            os.makedirs(folderName)

    def get_readlock(self, path):
        """
        Obtain a readlock on a file

        Parameters
        ----------
        path : string
            Name of the file on which to obtain a readlock
        """

        timestamp = int(time.time() * 1e6)
        lockdirName = "%s.readlock.%i.%i" % (path, self.pid, timestamp)
        os.mkdir(lockdirName)

        # Register function to release the readlock at the end of the script
        atexit.register(self.release_readlock, lockdirName=lockdirName)

    def release_readlock(self, lockdirName):
        """
        Release a previously obtained readlock

        Parameters
        ----------
        lockdirName : string
            Name of the previously obtained readlock
        """

        # Make sure the lock still exists before deleting it
        if (os.path.exists(lockdirName) and os.path.isdir(lockdirName)):
            os.rmdir(lockdirName)

    def get_writelock(self, filename):
        """
        Obtain a writelock on a file.
        Only one write lock may be held at any given time.

        Parameters
        ----------
        filename : string
            Name of the file on which to obtain a writelock
        """

        # compilelock expect locks to be on folder. Since we want a lock on a
        # file, we will have to ask compilelock for a folder with a different
        # name from the file we want a lock on or else compilelock will
        # try to create a folder with the same name as the file
        compilelock.get_lock(filename + ".writelock")

    def release_writelock(self):
        """
        Release the previously obtained writelock
        """
        compilelock.release_lock()


datasetCache = LocalDatasetCache()

########NEW FILE########
__FILENAME__ = cifar10
"""
.. todo::

    WRITEME
"""
import os, cPickle, logging
_logger = logging.getLogger(__name__)

import numpy as np
N = np
from pylearn2.datasets import dense_design_matrix
from pylearn2.expr.preprocessing import global_contrast_normalize


class CIFAR10(dense_design_matrix.DenseDesignMatrix):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    which_set : str
        One of 'train', 'test'
    center : WRITEME
    rescale : WRITEME
    gcn : float, optional
        Multiplicative constant to use for global contrast normalization.
        No global contrast normalization is applied, if None
    one_hot : WRITEME
    start : WRITEME
    stop : WRITEME
    axes : WRITEME
    toronto_prepro : WRITEME
    preprocessor : WRITEME
    """

    def __init__(self, which_set, center = False, rescale = False, gcn = None,
            one_hot = False, start = None, stop = None, axes=('b', 0, 1, 'c'),
            toronto_prepro = False, preprocessor = None):
        # note: there is no such thing as the cifar10 validation set;
        # pylearn1 defined one but really it should be user-configurable
        # (as it is here)

        self.axes = axes

        # we define here:
        dtype  = 'uint8'
        ntrain = 50000
        nvalid = 0  # artefact, we won't use it
        ntest  = 10000

        # we also expose the following details:
        self.img_shape = (3,32,32)
        self.img_size = N.prod(self.img_shape)
        self.n_classes = 10
        self.label_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
                            'dog', 'frog','horse','ship','truck']

        # prepare loading
        fnames = ['data_batch_%i' % i for i in range(1,6)]
        lenx = N.ceil((ntrain + nvalid) / 10000.)*10000
        x = N.zeros((lenx,self.img_size), dtype=dtype)
        y = N.zeros(lenx, dtype=dtype)

        # load train data
        nloaded = 0
        for i, fname in enumerate(fnames):
            data = CIFAR10._unpickle(fname)
            x[i*10000:(i+1)*10000, :] = data['data']
            y[i*10000:(i+1)*10000] = data['labels']
            nloaded += 10000
            if nloaded >= ntrain + nvalid + ntest: break;

        # load test data
        data = CIFAR10._unpickle('test_batch')

        # process this data
        Xs = {
                'train' : x[0:ntrain],
                'test'  : data['data'][0:ntest]
            }

        Ys = {
                'train' : y[0:ntrain],
                'test'  : data['labels'][0:ntest]
            }

        X = N.cast['float32'](Xs[which_set])
        y = Ys[which_set]

        if isinstance(y,list):
            y = np.asarray(y)

        if which_set == 'test':
            assert y.shape[0] == 10000


        self.one_hot = one_hot
        if one_hot:
            one_hot = np.zeros((y.shape[0],10),dtype='float32')
            for i in xrange(y.shape[0]):
                one_hot[i,y[i]] = 1.
            y = one_hot

        if center:
            X -= 127.5
        self.center = center

        if rescale:
            X /= 127.5
        self.rescale = rescale

        if toronto_prepro:
            assert not center
            assert not gcn
            X = X / 255.
            if which_set == 'test':
                other = CIFAR10(which_set='train')
                oX = other.X
                oX /= 255.
                X = X - oX.mean(axis=0)
            else:
                X = X - X.mean(axis=0)
        self.toronto_prepro = toronto_prepro

        self.gcn = gcn
        if gcn is not None:
            gcn = float(gcn)
            X = global_contrast_normalize(X, scale=gcn)

        if start is not None:
            # This needs to come after the prepro so that it doesn't change the pixel
            # means computed above for toronto_prepro
            assert start >= 0
            assert stop > start
            assert stop <= X.shape[0]
            X = X[start:stop, :]
            y = y[start:stop]
            assert X.shape[0] == y.shape[0]

        if which_set == 'test':
            assert X.shape[0] == 10000

        view_converter = dense_design_matrix.DefaultViewConverter((32,32,3), axes)

        super(CIFAR10, self).__init__(X=X, y=y, view_converter=view_converter)

        assert not np.any(np.isnan(self.X))

        if preprocessor:
            preprocessor.apply(self)

    def adjust_for_viewer(self, X):
        """
        .. todo::

            WRITEME
        """
        #assumes no preprocessing. need to make preprocessors mark the new ranges
        rval = X.copy()

        #patch old pkl files
        if not hasattr(self,'center'):
            self.center = False
        if not hasattr(self,'rescale'):
            self.rescale = False
        if not hasattr(self,'gcn'):
            self.gcn = False

        if self.gcn is not None:
            rval = X.copy()
            for i in xrange(rval.shape[0]):
                rval[i,:] /= np.abs(rval[i,:]).max()
            return rval

        if not self.center:
            rval -= 127.5

        if not self.rescale:
            rval /= 127.5

        rval = np.clip(rval,-1.,1.)

        return rval

    def adjust_to_be_viewed_with(self, X, orig, per_example = False):
        """
        .. todo::

            WRITEME
        """
        # if the scale is set based on the data, display X oring the scale determined
        # by orig
        # assumes no preprocessing. need to make preprocessors mark the new ranges
        rval = X.copy()

        #patch old pkl files
        if not hasattr(self,'center'):
            self.center = False
        if not hasattr(self,'rescale'):
            self.rescale = False
        if not hasattr(self,'gcn'):
            self.gcn = False

        if self.gcn is not None:
            rval = X.copy()
            if per_example:
                for i in xrange(rval.shape[0]):
                    rval[i,:] /= np.abs(orig[i,:]).max()
            else:
                rval /= np.abs(orig).max()
            rval = np.clip(rval, -1., 1.)
            return rval

        if not self.center:
            rval -= 127.5

        if not self.rescale:
            rval /= 127.5

        rval = np.clip(rval,-1.,1.)

        return rval

    def get_test_set(self):
        """
        .. todo::

            WRITEME
        """
        return CIFAR10(which_set='test', center=self.center, rescale=self.rescale, gcn=self.gcn,
                one_hot=self.one_hot, toronto_prepro=self.toronto_prepro, axes=self.axes)


    @classmethod
    def _unpickle(cls, file):
        """
        .. todo::

            What is this? why not just use serial.load like the CIFAR-100
            class? Whoever wrote it shows up as "unknown" in git blame.
        """
        from pylearn2.utils import string_utils
        fname = os.path.join(
                string_utils.preprocess('${PYLEARN2_DATA_PATH}'),
                'cifar10',
                'cifar-10-batches-py',
                file)
        if not os.path.exists(fname):
            raise IOError(fname+" was not found. You probably need to download "
                    "the CIFAR-10 dataset by using the download script in pylearn2/scripts/datasets/download_cifar10.sh "
                    "or manually from http://www.cs.utoronto.ca/~kriz/cifar.html")
        _logger.info('loading file %s' % fname)
        fo = open(fname, 'rb')
        dict = cPickle.load(fo)
        fo.close()
        return dict

########NEW FILE########
__FILENAME__ = cifar100
"""
The CIFAR-100 dataset.
"""
import numpy as np
N = np
from pylearn2.datasets import dense_design_matrix
from pylearn2.utils import serial


class CIFAR100(dense_design_matrix.DenseDesignMatrix):
    """
    The CIFAR-100 dataset.

    Parameters
    ----------
    which_set : WRITEME
    center : WRITEME
    gcn : WRITEME
    toronto_prepro : WRITEME
    axes : WRITEME
    start : WRITEME
    stop : WRITEME
    one_hot : WRITEME
    """

    def __init__(self, which_set, center = False,
            gcn = None, toronto_prepro = False,
            axes = ('b', 0, 1, 'c'),
            start = None, stop = None, one_hot = False):
        assert which_set in ['train','test']

        path = "${PYLEARN2_DATA_PATH}/cifar100/cifar-100-python/"+which_set

        obj = serial.load(path)
        X = obj['data']

        assert X.max() == 255.
        assert X.min() == 0.

        X = np.cast['float32'](X)
        y = np.asarray(obj['fine_labels'])

        self.center = center

        self.one_hot = one_hot
        if one_hot:
            one_hot = np.zeros((y.shape[0],100),dtype='float32')
            for i in xrange(y.shape[0]):
                one_hot[i,y[i]] = 1.
            y = one_hot

        if center:
            X -= 127.5

        if toronto_prepro:
            assert not center
            assert not gcn
            if which_set == 'test':
                raise NotImplementedError(
                        "Need to subtract the mean of the *training* set.")
            X = X / 255.
            X = X - X.mean(axis=0)
        self.toronto_prepro = toronto_prepro

        self.gcn = gcn
        if gcn is not None:
            assert isinstance(gcn,float)
            X = (X.T - X.mean(axis=1)).T
            X = (X.T / np.sqrt(np.square(X).sum(axis=1))).T
            X *= gcn

        if start is not None:
            # This needs to come after the prepro so that it doesn't change
            # the pixel means computed above
            assert start >= 0
            assert stop > start
            assert stop <= X.shape[0]
            X = X[start:stop, :]
            y = y[start:stop]
            assert X.shape[0] == y.shape[0]

        self.axes = axes
        view_converter = dense_design_matrix.DefaultViewConverter((32,32,3),
                axes)

        super(CIFAR100,self).__init__(X=X, y=y, view_converter=view_converter)

        assert not N.any(N.isnan(self.X))

        # need to support start, stop
        # self.y_fine = N.asarray(obj['fine_labels'])
        # self.y_coarse = N.asarray(obj['coarse_labels'])



    def adjust_for_viewer(self, X):
        """
        .. todo::

            WRITEME
        """
        # assumes no preprocessing. need to make preprocessors mark the new
        # ranges
        rval = X.copy()

        #patch old pkl files
        if not hasattr(self,'center'):
            self.center = False
        if not hasattr(self,'rescale'):
            self.rescale = False
        if not hasattr(self,'gcn'):
            self.gcn = False

        if self.gcn is not None:
            rval = X.copy()
            for i in xrange(rval.shape[0]):
                rval[i,:] /= np.abs(rval[i,:]).max()
            return rval

        if not self.center:
            rval -= 127.5

        if not self.rescale:
            rval /= 127.5

        rval = np.clip(rval,-1.,1.)

        return rval

    def adjust_to_be_viewed_with(self, X, orig, per_example = False):
        """
        .. todo::

            WRITEME
        """
        # if the scale is set based on the data, display X oring the scale
        # determined
        # by orig
        # assumes no preprocessing. need to make preprocessors mark the new
        # ranges
        rval = X.copy()

        #patch old pkl files
        if not hasattr(self,'center'):
            self.center = False
        if not hasattr(self,'rescale'):
            self.rescale = False
        if not hasattr(self,'gcn'):
            self.gcn = False

        if self.gcn is not None:
            rval = X.copy()
            if per_example:
                for i in xrange(rval.shape[0]):
                    rval[i,:] /= np.abs(orig[i,:]).max()
            else:
                rval /= np.abs(orig).max()
            rval = np.clip(rval, -1., 1.)
            return rval

        if not self.center:
            rval -= 127.5

        if not self.rescale:
            rval /= 127.5

        rval = np.clip(rval,-1.,1.)

        return rval

    def get_test_set(self):
        """
        .. todo::

            WRITEME
        """
        return CIFAR100(which_set='test', center=self.center,
                rescale=self.rescale,
                gcn=self.gcn,
                one_hot=self.one_hot, toronto_prepro=self.toronto_prepro,
                axes=self.axes)


########NEW FILE########
__FILENAME__ = config
"""
.. todo::

    WRITEME
"""
global resolvers


def resolve(d):
    """
    .. todo::

        WRITEME
    """
    tag = pylearn2.config.get_tag(d)

    if tag != 'dataset':
        raise TypeError('pylearn2.datasets.config asked to resolve a config dictionary with tag "'+tag+'"')

    t = pylearn2.config.get_str(d,'typename')

    try:
        resolver = resolvers[t]
    except:
        raise TypeError('pylearn2.datasets does not know of a dataset type "'+t+'"')

    return resolver(d)


def resolve_avicenna(d):
    """
    .. todo::

        WRITEME
    """
    import pylearn2.datasets.avicenna
    return pylearn2.config.checked_call(pylearn2.datasets.avicenna.Avicenna,d)

resolvers = {
            'avicenna' : resolve_avicenna
        }

import pylearn2.config

########NEW FILE########
__FILENAME__ = control
"""
.. todo::

    WRITEME
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"


load_data = [ True ]

def pop_load_data():
    """
    .. todo::

        WRITEME
    """
    global load_data

    del load_data[-1]

def push_load_data(setting):
    """
    .. todo::

        WRITEME
    """
    global load_data

    load_data.append(setting)

def get_load_data():
    """
    .. todo::

        WRITEME
    """
    return load_data[-1]

########NEW FILE########
__FILENAME__ = cos_dataset
"""
.. todo::

    WRITEME
"""
import numpy as N
import copy
from theano import config
import theano.tensor as T
from pylearn2.utils.rng import make_np_rng


class CosDataset(object):
    """
    Makes a dataset that streams randomly generated 2D examples.
    The first coordinate is sampled from a uniform distribution.
    The second coordinate is the cosine of the first coordinate,
    plus some gaussian noise.
    """

    def __init__(self, min_x=-6.28, max_x=6.28, std=.05, rng=None):
        """
        .. todo::

            WRITEME
        """
        self.min_x, self.max_x, self.std = min_x, max_x, std
        rng = make_np_rng(rng, [17, 2, 946], which_method=['uniform', 'randn'])
        self.default_rng = copy.copy(rng)
        self.rng = rng

    def energy(self, mat):
        """
        .. todo::

            WRITEME
        """
        x = mat[:, 0]
        y = mat[:, 1]
        rval = (y - N.cos(x)) ** 2. / (2. * (self.std ** 2.))
        return rval

    def pdf_func(self, mat):
        """
        .. todo::

            WRITEME properly
        
        This dataset can generate an infinite amount of examples.
        This function gives the pdf from which the examples are drawn.
        """
        x = mat[:, 0]
        y = mat[:, 1]
        rval = N.exp(-(y - N.cos(x)) ** 2. / (2. * (self.std ** 2.)))
        rval /= N.sqrt(2.0 * N.pi * (self.std ** 2.))
        rval /= (self.max_x - self.min_x)
        rval *= x < self.max_x
        rval *= x > self.min_x
        return rval

    def free_energy(self, X):
        """
        .. todo::

            WRITEME properly
        
        This dataset can generate an infinite amount of examples.
        This function gives the energy function for the distribution from which the examples are drawn.
        """
        x = X[:, 0]
        y = X[:, 1]
        rval = T.sqr(y - T.cos(x)) / (2. * (self.std ** 2.))
        mask = x < self.max_x
        mask = mask * (x > self.min_x)
        rval = mask * rval + (1 - mask) * 1e30
        return rval

    def pdf(self, X):
        """
        .. todo::

            WRITEME properly
        
        This dataset can generate an infinite amount of examples.
        This function gives the pdf from which the examples are drawn.
        """
        x = X[:, 0]
        y = X[:, 1]
        rval = T.exp(-T.sqr(y - T.cos(x)) / (2. * (self.std ** 2.)))
        rval /= N.sqrt(2.0 * N.pi * (self.std ** 2.))
        rval /= (self.max_x - self.min_x)
        rval *= x < self.max_x
        rval *= x > self.min_x
        return rval

    def get_stream_position(self):
        """
        .. todo::

            WRITEME
        """
        return copy.copy(self.rng)

    def set_stream_position(self, s):
        """
        .. todo::

            WRITEME
        """
        self.rng = copy.copy(s)

    def restart_stream(self):
        """
        .. todo::

            WRITEME
        """
        self.reset_RNG()

    def reset_RNG(self):
        """
        .. todo::

            WRITEME
        """
        if 'default_rng' not in dir(self):
            self.default_rng = N.random.RandomState([17, 2, 946])
        self.rng = copy.copy(self.default_rng)

    def apply_preprocessor(self, preprocessor, can_fit=False):
        """
        .. todo::

            WRITEME
        """
        raise NotImplementedError()

    def get_batch_design(self, batch_size):
        """
        .. todo::

            WRITEME
        """
        x = N.cast[config.floatX](self.rng.uniform(self.min_x, self.max_x,
                                            (batch_size, 1)))
        y = N.cos(x) + (N.cast[config.floatX](self.rng.randn(*x.shape)) *
                        self.std)
        rval = N.hstack((x, y))
        return rval

########NEW FILE########
__FILENAME__ = csv_dataset
# -*- coding: utf-8 -*-
"""
A simple general csv dataset wrapper for pylearn2.
Can do automatic one-hot encoding based on labels present in a file.
"""
__authors__ = "Zygmunt Zajc"
__copyright__ = "Copyright 2013, Zygmunt Zajc"
__credits__ = ["Zygmunt Zajc"]
__license__ = "3-clause BSD"
__maintainer__ = "?"
__email__ = "zygmunt@fastml.com"

import csv
import numpy as np
import os

from pylearn2.datasets.dense_design_matrix import DenseDesignMatrix
from pylearn2.utils import serial
from pylearn2.utils.string_utils import preprocess


class CSVDataset(DenseDesignMatrix):
    """
    A generic class for accessing CSV files
    labels, if present, should be in the first column
    if there's no labels, set expect_labels to False
    if there's no header line in your file, set expect_headers to False

    Parameters
    ----------
    path : WRITEME
    one_hot : WRITEME
    expect_labels : WRITEME
    expect_headers : WRITEME
    delimiter : WRITEME
    """

    def __init__(self, 
            path = 'train.csv',
            one_hot = False,
            expect_labels = True,
            expect_headers = True,
            delimiter = ','):
        """
        .. todo::

            WRITEME
        """
        self.path = path
        self.one_hot = one_hot
        self.expect_labels = expect_labels
        self.expect_headers = expect_headers
        self.delimiter = delimiter
        
        self.view_converter = None

        # and go

        self.path = preprocess(self.path)
        X, y = self._load_data()
        
        super(CSVDataset, self).__init__(X=X, y=y)

    def _load_data(self):
        """
        .. todo::

            WRITEME
        """
        assert self.path.endswith('.csv')
    
        if self.expect_headers:
            data = np.loadtxt(self.path, delimiter = self.delimiter, skiprows = 1)
        else:
            data = np.loadtxt(self.path, delimiter = self.delimiter)
        
        if self.expect_labels:
            y = data[:,0]
            X = data[:,1:]
            
            # get unique labels and map them to one-hot positions
            labels = np.unique(y)
            #labels = { x: i for i, x in enumerate(labels) }    # doesn't work in python 2.6
            labels = dict((x, i) for (i, x) in enumerate(labels))

            if self.one_hot:
                one_hot = np.zeros((y.shape[0], len(labels)), dtype='float32')
                for i in xrange(y.shape[0]):
                    label = y[i]
                    label_position = labels[label]
                    one_hot[i,label_position] = 1.
                y = one_hot

        else:
            X = data
            y = None

        return X, y

########NEW FILE########
__FILENAME__ = dataset
"""
A module defining the Dataset class.
"""


class Dataset(object):
    """
    Abstract interface for Datasets.
    """

    def __iter__(self):
        """
        .. todo::

            WRITEME
        """
        return self.iterator()

    def iterator(self, mode=None, batch_size=None, num_batches=None,
                 topo=None, targets=False, rng=None):
        """
        Return an iterator for this dataset with the specified
        behaviour. Unspecified values are filled-in by the default.

        .. todo::

            Parameters : targets

            DWF or LD should fill this in, but IG thinks it is just
            a bool saying whether to include the targets or not

        Parameters
        ----------
        mode : str or object, optional
            One of 'sequential', 'random_slice', or 'random_uniform',
            *or* a class that instantiates an iterator that returns
            slices or index sequences on every call to next().
        batch_size : int, optional
            The size of an individual batch. Optional if `mode` is
            'sequential' and `num_batches` is specified (batch size
            will be calculated based on full dataset size).
        num_batches : int, optional
            The total number of batches. Unnecessary if `mode` is
            'sequential' and `batch_size` is specified (number of
            batches will be calculated based on full dataset size).
        topo : boolean, optional
            Whether batches returned by the iterator should present
            examples in a topological view or not. Defaults to
            `False`.
        rng : int, object or array_like, optional
            Either an instance of `numpy.random.RandomState` (or
            something with a compatible interface), or a seed value
            to be passed to the constructor to create a `RandomState`.
            See the docstring for `numpy.random.RandomState` for
            details on the accepted seed formats. If unspecified,
            defaults to using the dataset's own internal random
            number generator, which persists across iterations
            through the dataset and may potentially be shared by
            multiple iterator objects simultaneously (see "Notes"
            below).
        targets: TODO
            TODO

        Returns
        -------
        iter_obj : object
            An iterator object implementing the standard Python
            iterator protocol (i.e. it has an `__iter__` method that
            return the object itself, and a `next()` method that
            returns results until it raises `StopIteration`).

        Notes
        -----
        Arguments are passed as instantiation parameters to classes
        that derive from `pylearn2.utils.iteration.SubsetIterator`.

        Iterating simultaneously with multiple iterator objects
        sharing the same random number generator could lead to
        difficult-to-reproduce behaviour during training. It is
        therefore *strongly recommended* that each iterator be given
        its own random number generator with the `rng` parameter
        in such situations.
        """
        # TODO: See how much of the logic from DenseDesignMatrix.iterator
        # can be handled here.
        raise NotImplementedError()

    def adjust_for_viewer(self, X):
        """
        .. todo::

            WRITEME properly

        X: a tensor in the same space as the data
        returns the same tensor shifted and scaled by a transformation
        that maps the data range to [-1, 1] so that it can be displayed
        with pylearn2.gui.patch_viewer tools

        for example, for MNIST X will lie in [0,1] and the return value
            should be X*2-1

        Default is to do nothing
        """

        return X

    def has_targets(self):
        """ Returns true if the dataset includes targets """

        raise NotImplementedError()

    def get_topo_batch_axis(self):
        """
        Returns the index of the axis that corresponds to different examples
        in a batch when using topological_view.
        """

        # Subclasses that support topological view must implement this to
        # specify how their data is formatted.
        raise NotImplementedError()

    def get_batch_design(self, batch_size, include_labels=False):
        """
        Returns a randomly chosen batch of data formatted as a design
        matrix.

        This method is not guaranteed to have any particular properties
        like not repeating examples, etc. It is mostly useful for getting
        a single batch of data for a unit test or a quick-and-dirty
        visualization. Using this method for serious learning code is
        strongly discouraged. All code that depends on any particular
        example sampling properties should use Dataset.iterator.

        .. todo::

            Refactor to use `include_targets` rather than `include_labels`,
            to make the terminology more consistent with the rest of the
            library.

        Parameters
        ----------
        batch_size : int
            The number of examples to include in the batch.
        include_labels : bool
            If True, returns the targets for the batch, as well as the
            features.

        Returns
        -------
        batch : member of feature space, or member of (feature, target) space.
            Either numpy value of the features, or a (features, targets) tuple
            of numpy values, depending on the value of `include_labels`.
        """
        raise NotImplementedError(str(type(self))+" does not implement "
                "get_batch_design.")

    def get_batch_topo(self, batch_size):
        """
        Returns a topology-preserving batch of data.

        The first index is over different examples, and has length
        batch_size. The next indices are the topologically significant
        dimensions of the data, i.e. for images, image rows followed by
        image columns.  The last index is over separate channels.
        """
        raise NotImplementedError()

########NEW FILE########
__FILENAME__ = debug
"""
.. todo::

    WRITEME
"""
import numpy as N
from pylearn2.datasets import dense_design_matrix


class DebugDataset(dense_design_matrix.DenseDesignMatrix):
    """
    .. todo::

        WRITEME
    """
    def __init__(self):
        """
        .. todo::

            WRITEME
        """

        view_converter = dense_design_matrix.DefaultViewConverter((32,32,3))

        super(DebugDataset,self).__init__(X = N.asarray([[1.0,0.0],[0.0,1.0]]) , view_converter = view_converter)

        assert not N.any(N.isnan(self.X))

########NEW FILE########
__FILENAME__ = dense_design_matrix
"""
The DenseDesignMatrix class and related code. Functionality for representing
data that can be described as a dense matrix (rather than a sparse matrix)
with each row containing an example and each column corresponding to a
different feature. DenseDesignMatrix also supports other "views" of the data,
for example a dataset of images can be viewed either as a matrix of flattened
images or as a stack of 2D multi-channel images. However, the images must all
be the same size, so that each image may be mapped to a matrix row by the same
transformation.
"""
__authors__ = "Ian Goodfellow and Mehdi Mirza"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"
import functools

import logging
import warnings

import numpy as np

from pylearn2.datasets import cache
from pylearn2.utils.iteration import (
    FiniteDatasetIterator,
    resolve_iterator_class
)

import copy
# Don't import tables initially, since it might not be available
# everywhere.
tables = None


from pylearn2.datasets.dataset import Dataset
from pylearn2.datasets import control
from pylearn2.space import CompositeSpace, Conv2DSpace, VectorSpace, IndexSpace
from pylearn2.utils import safe_zip
from pylearn2.utils.rng import make_np_rng
from theano import config


logger = logging.getLogger(__name__)


def ensure_tables():
    """
    Makes sure tables module has been imported
    """

    global tables
    if tables is None:
        import tables


class DenseDesignMatrix(Dataset):
    """
    A class for representing datasets that can be stored as a dense design
    matrix (and optionally, associated targets).


    Parameters
    ----------
    X : ndarray, 2-dimensional, optional
        Should be supplied if `topo_view` is not. A design \
        matrix of shape (number examples, number features) \
        that defines the dataset.
    topo_view : ndarray, optional
        Should be supplied if X is not.  An array whose first \
        dimension is of length number examples. The remaining \
        dimensions are examples with topological significance, \
        e.g. for images the remaining axes are rows, columns, \
        and channels.
    y : ndarray, optional

        Targets for each example (e.g., class ids, values to be predicted
        in a regression task).

        Currently three formats are supported:

        - None:
            Pass `None` if there are no target values. In this case the
            dataset may not be some tasks such as supervised learning
            or evaluation of a supervised learning system, but it can
            be used for some other tasks. For example, a supervised
            learning system can make predictions on it, or an unsupervised
            learning system can be trained on it.
        - 1D ndarray of integers:
            This format may be used when the targets are class labels.
            In this format, the array should have one entry for each
            example. Each entry should be an integer, in the range
            [0, N) where N is the number of classes.
            This is the format that the `SVM` class expects.
        - 2D ndarray, data type optional:
            This is the most common format and can be used for a variety
            of problem types. Each row of the matrix becomes the target
            for a different example. Specific models / costs can interpret
            this target vector differently. For example, the `Linear`
            output layer for the `MLP` class expects the target for each
            example to be a vector of real-valued regression targets. (It
            can be a vector of size one if you only have one regression
            target). The `Softmax` output layer of the `MLP` class expects
            the target to be a vector of N elements, where N is the number
            of classes, and expects all but one of the elements to 0. One
            element should have value 1., and the index of this element
            identifies the target class.
    view_converter : object, optional
        An object for converting between the design matrix \
        stored internally and the topological view of the data.
    rng : object, optional
        A random number generator used for picking random \
        indices into the design matrix when choosing minibatches.
    X_labels : int, optional
        If X contains labels then X_labels must be passed to indicate the
        total number of possible labels e.g. the size of a the vocabulary
        when X contains word indices. This will make the set use
        IndexSpace.
    y_labels : int, optional
        If y contains labels then y_labels must be passed to indicate the
        total number of possible labels e.g. 10 for the MNIST dataset
        where the targets are numbers. This will make the set use
        IndexSpace.

    See Also
    --------
    DenseDesignMatrixPytables : Use this class if your data is too big to fit
        in memory.

    Notes
    -----
    - What kind of data can be stored in this way?
        A design matrix is a matrix where each row contains a single
        example. Each column within the row is a feature of that example.

        By dense, we mean that every entry in the matrix is explicitly given a
        value.

        Examples of datasets that can be stored this way include MNIST and
        CIFAR10.

        Some datasets cannot be stored as a design matrix. For example, a
        collection of images, each image having a different size, can't be
        stored in this way, because we can't reshape each image to the same
        length of matrix row.

        Some datasets can, conceptually, be represented as a design matrix, but
        it may not be efficient to store them as dense matrices. For example,
        a dataset of sentences with a bag of words representation, might have a
        very high number of features but most of the values are zero, so it
        would be better to store the data as a sparse matrix.

    - What if my examples aren't best thought of as vectors?
        The DenseDesignMatrix class supports two views of the data, the "design
        matrix view" in which each example is just a vector, and the
        "topological view" in which each example is formatted using some kind
        of data structure with meaningful topology. For example, a dataset of
        images can be viewed as a design matrix where each row contains a
        flattened version of each image, or it can be viewed as a 4D tensor,
        where each example is a 3D subtensor, with one axis corresponding to
        rows of the image, one axis corresponding to columns of the image, and
        one axis corresponding to the color channels. This structure can be
        thought of as having meaningful topology because neighboring
        coordinates on the row and column axes correspond to neighboring
        pixels in the image.
    """
    _default_seed = (17, 2, 946)

    def __init__(self, X=None, topo_view=None, y=None,
                 view_converter=None, axes=('b', 0, 1, 'c'),
                 rng=_default_seed, preprocessor=None, fit_preprocessor=False,
                 max_labels=None, X_labels=None, y_labels=None):
        self.X = X
        self.y = y
        self.X_labels = X_labels
        self.y_labels = y_labels

        if max_labels is not None:
            warnings.warn("The max_labels argument to DenseDesignMatrix is "
                          "deprecated. Use the y_labels argument instead. The "
                          "max_labels argument will be removed on or after "
                          "6 October 2014", stacklevel=2)
            assert y_labels is None
            self.y_labels = max_labels

        if X_labels is not None:
            assert X is not None
            assert view_converter is None
            assert X.ndim <= 2
            assert np.all(X < X_labels)

        if y_labels is not None:
            assert y is not None
            assert y.ndim <= 2
            assert np.all(y < y_labels)

        if topo_view is not None:
            assert view_converter is None
            self.set_topological_view(topo_view, axes)
        else:
            assert X is not None, ("DenseDesignMatrix needs to be provided "
                                   "with either topo_view, or X")
            if view_converter is not None:
                self.view_converter = view_converter

                # Get the topo_space (usually Conv2DSpace) from the
                # view_converter
                if not hasattr(view_converter, 'topo_space'):
                    raise NotImplementedError("Not able to get a topo_space "
                                              "from this converter: %s"
                                              % view_converter)

                # self.X_topo_space stores a "default" topological space that
                # will be used only when self.iterator is called without a
                # data_specs, and with "topo=True", which is deprecated.
                self.X_topo_space = view_converter.topo_space
            else:
                self.X_topo_space = None

            # Update data specs, if not done in set_topological_view
            X_source = 'features'
            if X_labels is None:
                X_space = VectorSpace(dim=X.shape[1])
            else:
                if X.ndim == 1:
                    dim = 1
                else:
                    dim = X.shape[-1]
                X_space = IndexSpace(dim=dim, max_labels=X_labels)
            if y is None:
                space = X_space
                source = X_source
            else:
                if y.ndim == 1:
                    dim = 1
                else:
                    dim = y.shape[-1]
                if y_labels is not None:
                    y_space = IndexSpace(dim=dim, max_labels=y_labels)
                else:
                    y_space = VectorSpace(dim=dim)
                y_source = 'targets'

                space = CompositeSpace((X_space, y_space))
                source = (X_source, y_source)
            self.data_specs = (space, source)
            self.X_space = X_space

        self.compress = False
        self.design_loc = None
        self.rng = make_np_rng(rng, which_method="random_integers")
        # Defaults for iterators
        self._iter_mode = resolve_iterator_class('sequential')
        self._iter_topo = False
        self._iter_targets = False
        self._iter_data_specs = (self.X_space, 'features')

        if preprocessor:
            preprocessor.apply(self, can_fit=fit_preprocessor)
        self.preprocessor = preprocessor

    @functools.wraps(Dataset.iterator)
    def iterator(self, mode=None, batch_size=None, num_batches=None,
                 topo=None, targets=None, rng=None, data_specs=None,
                 return_tuple=False):

        if topo is not None or targets is not None:
            if data_specs is not None:
                raise ValueError('In DenseDesignMatrix.iterator, both the '
                                 '"data_specs" argument and deprecated '
                                 'arguments "topo" or "targets" were '
                                 'provided.',
                                 (data_specs, topo, targets))

            warnings.warn("Usage of `topo` and `target` arguments are "
                          "being deprecated, and will be removed "
                          "around November 7th, 2013. `data_specs` "
                          "should be used instead.",
                          stacklevel=2)

            # build data_specs from topo and targets if needed
            if topo is None:
                topo = getattr(self, '_iter_topo', False)
            if topo:
                # self.iterator is called without a data_specs, and with
                # "topo=True", so we use the default topological space
                # stored in self.X_topo_space
                assert self.X_topo_space is not None
                X_space = self.X_topo_space
            else:
                X_space = self.X_space

            if targets is None:
                targets = getattr(self, '_iter_targets', False)
            if targets:
                assert self.y is not None
                y_space = self.data_specs[0].components[1]
                space = CompositeSpace((X_space, y_space))
                source = ('features', 'targets')
            else:
                space = X_space
                source = 'features'

            data_specs = (space, source)
            convert = None

        else:
            if data_specs is None:
                data_specs = self._iter_data_specs

            # If there is a view_converter, we have to use it to convert
            # the stored data for "features" into one that the iterator
            # can return.
            space, source = data_specs
            if isinstance(space, CompositeSpace):
                sub_spaces = space.components
                sub_sources = source
            else:
                sub_spaces = (space,)
                sub_sources = (source,)

            convert = []
            for sp, src in safe_zip(sub_spaces, sub_sources):
                if src == 'features' and \
                   getattr(self, 'view_converter', None) is not None:
                    conv_fn = (lambda batch, self=self, space=sp:
                               self.view_converter.get_formatted_batch(batch,
                                                                       space))
                else:
                    conv_fn = None

                convert.append(conv_fn)

        # TODO: Refactor
        if mode is None:
            if hasattr(self, '_iter_subset_class'):
                mode = self._iter_subset_class
            else:
                raise ValueError('iteration mode not provided and no default '
                                 'mode set for %s' % str(self))
        else:
            mode = resolve_iterator_class(mode)

        if batch_size is None:
            batch_size = getattr(self, '_iter_batch_size', None)
        if num_batches is None:
            num_batches = getattr(self, '_iter_num_batches', None)
        if rng is None and mode.stochastic:
            rng = self.rng
        return FiniteDatasetIterator(self,
                                     mode(self.X.shape[0],
                                          batch_size,
                                          num_batches,
                                          rng),
                                     data_specs=data_specs,
                                     return_tuple=return_tuple,
                                     convert=convert)

    def get_data(self):
        """
        Returns all the data, as it is internally stored.
        The definition and format of these data are described in
        `self.get_data_specs()`.

        Returns
        -------
        data : numpy matrix or 2-tuple of matrices
            The data
        """
        if self.y is None:
            return self.X
        else:
            return (self.X, self.y)

    def use_design_loc(self, path):
        """
        Caling this function changes the serialization behavior of the object
        permanently.

        If this function has been called, when the object is serialized, it
        will save the design matrix to `path` as a .npy file rather
        than pickling the design matrix along with the rest of the dataset
        object. This avoids pickle's unfortunate behavior of using 2X the RAM
        when unpickling.

        TODO: Get rid of this logic, use custom array-aware picklers (joblib,
        custom pylearn2 serialization format).

        Parameters
        ----------
        path : str
            The path to save the design matrix to
        """

        if not path.endswith('.npy'):
            raise ValueError("path should end with '.npy'")

        self.design_loc = path

    def get_topo_batch_axis(self):
        """
        The index of the axis of the batches

        Returns
        -------
        axis : int
            The axis of a topological view of this dataset that corresponds
            to indexing over different examples.
        """
        axis = self.view_converter.axes.index('b')
        return axis

    def enable_compression(self):
        """
        If called, when pickled the dataset will be saved using only
        8 bits per element.

        .. todo::

            Not sure this should be implemented as something a base dataset
            does. Perhaps as a mixin that specific datasets (i.e. CIFAR10)
            inherit from.
        """
        self.compress = True

    def __getstate__(self):
        """
        .. todo::

            WRITEME
        """
        rval = copy.copy(self.__dict__)
        # TODO: Not sure this should be implemented as something a base dataset
        # does. Perhaps as a mixin that specific datasets (i.e. CIFAR10)
        # inherit from.
        if self.compress:
            rval['compress_min'] = rval['X'].min(axis=0)
            # important not to do -= on this line, as that will modify the
            # original object
            rval['X'] = rval['X'] - rval['compress_min']
            rval['compress_max'] = rval['X'].max(axis=0)
            rval['compress_max'][rval['compress_max'] == 0] = 1
            rval['X'] *= 255. / rval['compress_max']
            rval['X'] = np.cast['uint8'](rval['X'])

        if self.design_loc is not None:
            # TODO: Get rid of this logic, use custom array-aware picklers
            # (joblib, custom pylearn2 serialization format).
            np.save(self.design_loc, rval['X'])
            del rval['X']

        return rval

    def __setstate__(self, d):
        """
        .. todo::

            WRITEME
        """
        if d['design_loc'] is not None:
            if control.get_load_data():
                fname = cache.datasetCache.cache_file(d['design_loc'])
                d['X'] = np.load(fname)
            else:
                d['X'] = None

        if d['compress']:
            X = d['X']
            mx = d['compress_max']
            mn = d['compress_min']
            del d['compress_max']
            del d['compress_min']
            d['X'] = 0
            self.__dict__.update(d)
            if X is not None:
                self.X = np.cast['float32'](X) * mx / 255. + mn
            else:
                self.X = None
        else:
            self.__dict__.update(d)

        # To be able to unpickle older data after the addition of
        # the data_specs mechanism
        if not all(m in d for m in ('data_specs', 'X_space',
                                    '_iter_data_specs', 'X_topo_space')):
            X_space = VectorSpace(dim=self.X.shape[1])
            X_source = 'features'
            if self.y is None:
                space = X_space
                source = X_source
            else:
                y_space = VectorSpace(dim=self.y.shape[-1])
                y_source = 'targets'

                space = CompositeSpace((X_space, y_space))
                source = (X_source, y_source)

            self.data_specs = (space, source)
            self.X_space = X_space
            self._iter_data_specs = (X_space, X_source)

            view_converter = d.get('view_converter', None)
            if view_converter is not None:
                # Get the topo_space from the view_converter
                if not hasattr(view_converter, 'topo_space'):
                    raise NotImplementedError("Not able to get a topo_space "
                                              "from this converter: %s"
                                              % view_converter)

                # self.X_topo_space stores a "default" topological space that
                # will be used only when self.iterator is called without a
                # data_specs, and with "topo=True", which is deprecated.
                self.X_topo_space = view_converter.topo_space

    def _apply_holdout(self, _mode="sequential", train_size=0, train_prop=0):
        """
        This function splits the dataset according to the number of
        train_size if defined by the user with respect to the mode provided
        by the user. Otherwise it will use the
        train_prop to divide the dataset into a training and holdout
        validation set. This function returns the training and validation
        dataset.

        Parameters
        -----------
        _mode : WRITEME
        train_size : int
            Number of examples that will be assigned to the training dataset.
        train_prop : float
            Proportion of training dataset split.

        Returns
        -------
        WRITEME
        """

        """
        This function splits the dataset according to the number of
        train_size if defined by the user with respect to the mode provided
        by the user. Otherwise it will use the
        train_prop to divide the dataset into a training and holdout
        validation set. This function returns the training and validation
        dataset.

        Parameters
        -----------
        _mode : WRITEME
        train_size : int
            Number of examples that will be assigned to the training dataset.
        train_prop : float
            Proportion of training dataset split.

        Returns
        -------
        WRITEME
        """
        if train_size != 0:
            size = train_size
        elif train_prop != 0:
            size = np.round(self.num_examples * train_prop)
        else:
            raise ValueError("Initialize either split ratio and split size to "
                             "non-zero value.")
        if size < self.num_examples-size:
            dataset_iter = self.iterator(mode=_mode,
                                         batch_size=(self.num_examples - size))
            valid = dataset_iter.next()
            train = dataset_iter.next()[:self.num_examples-valid.shape[0]]
        else:
            dataset_iter = self.iterator(mode=_mode,
                                         batch_size=size)
            train = dataset_iter.next()
            valid = dataset_iter.next()[:self.num_examples-train.shape[0]]
        return (train, valid)

    def split_dataset_nfolds(self, nfolds=0):
        """
        This function splits the dataset into to the number of n folds
        given by the user. Returns an array of folds.

        Parameters
        ----------
        nfolds : int, optional
            The number of folds for the  the validation set.

        Returns
        -------
        WRITEME
        """

        folds_iter = self.iterator(mode="sequential", num_batches=nfolds)
        folds = list(folds_iter)
        return folds

    def split_dataset_holdout(self, train_size=0, train_prop=0):
        """
        This function splits the dataset according to the number of
        train_size if defined by the user.

        Otherwise it will use the train_prop to divide the dataset into a
        training and holdout validation set. This function returns the
        training and validation dataset.

        Parameters
        ----------
        train_size : int
            Number of examples that will be assigned to the training
            dataset.
        train_prop : float
            Proportion of dataset split.
        """
        return self._apply_holdout("sequential", train_size, train_prop)

    def bootstrap_nfolds(self, nfolds, rng=None):
        """
        This function splits the dataset using the random_slice and into the
        n folds. Returns the folds.

        Parameters
        ----------
        nfolds : int
            The number of folds for the  dataset.
        rng : WRITEME
            Random number generation class to be used.
        """

        folds_iter = self.iterator(mode="random_slice",
                                   num_batches=nfolds,
                                   rng=rng)
        folds = list(folds_iter)
        return folds

    def bootstrap_holdout(self, train_size=0, train_prop=0, rng=None):
        """
        This function splits the dataset according to the number of
        train_size defined by the user.

        Parameters
        ----------
        train_size : int
            Number of examples that will be assigned to the training dataset.
        nfolds : int
            The number of folds for the  the validation set.
        rng : WRITEME
            Random number generation class to be used.
        """
        return self._apply_holdout("random_slice", train_size, train_prop)

    def get_stream_position(self):
        """
        If we view the dataset as providing a stream of random examples to
        read, the object returned uniquely identifies our current position in
        that stream.
        """
        return copy.copy(self.rng)

    def set_stream_position(self, pos):
        """
        .. todo::

            WRITEME properly

        Return to a state specified by an object returned from
        get_stream_position.

        Parameters
        ----------
        pos : object
            WRITEME
        """
        self.rng = copy.copy(pos)

    def restart_stream(self):
        """
        Return to the default initial state of the random example stream.
        """
        self.reset_RNG()

    def reset_RNG(self):
        """
        Restore the default seed of the rng used for choosing random
        examples.
        """

        if 'default_rng' not in dir(self):
            self.default_rng = make_np_rng(None, [17, 2, 946],
                                           which_method="random_integers")
        self.rng = copy.copy(self.default_rng)

    def apply_preprocessor(self, preprocessor, can_fit=False):
        """
        .. todo::

            WRITEME

        Parameters
        ----------
        preprocessor : object
            preprocessor object
        can_fit : bool, optional
            WRITEME
        """
        preprocessor.apply(self, can_fit)

    def get_topological_view(self, mat=None):
        """
        Convert an array (or the entire dataset) to a topological view.

        Parameters
        ----------
        mat : ndarray, 2-dimensional, optional
            An array containing a design matrix representation of training
            examples. If unspecified, the entire dataset (`self.X`) is used
            instead.
            This parameter is not named X because X is generally used to
            refer to the design matrix for the current problem. In this
            case we want to make it clear that `mat` need not be the design
            matrix defining the dataset.
        """
        if self.view_converter is None:
            raise Exception("Tried to call get_topological_view on a dataset "
                            "that has no view converter")
        if mat is None:
            mat = self.X
        return self.view_converter.design_mat_to_topo_view(mat)

    def get_formatted_view(self, mat, dspace):
        """
        Convert an array (or the entire dataset) to a destination space.

        Parameters
        ----------
        mat : ndarray, 2-dimensional
            An array containing a design matrix representation of
            training examples.

        dspace : Space
            A Space we want the data in mat to be formatted in.
            It can be a VectorSpace for a design matrix output,
            a Conv2DSpace for a topological output for instance.
            Valid values depend on the type of `self.view_converter`.

        Returns
        -------
        WRITEME
        """
        if self.view_converter is None:
            raise Exception("Tried to call get_formatted_view on a dataset "
                            "that has no view converter")

        self.X_space.np_validate(mat)
        return self.view_converter.get_formatted_batch(mat, dspace)

    def get_weights_view(self, mat):
        """
        .. todo::

            WRITEME properly

        Return a view of mat in the topology preserving format. Currently
        the same as get_topological_view.

        Parameters
        ----------
        mat : ndarray, 2-dimensional
            WRITEME
        """

        if self.view_converter is None:
            raise Exception("Tried to call get_weights_view on a dataset "
                            "that has no view converter")

        return self.view_converter.design_mat_to_weights_view(mat)

    def set_topological_view(self, V, axes=('b', 0, 1, 'c')):
        """
        Sets the dataset to represent V, where V is a batch
        of topological views of examples.

        .. todo::

            Why is this parameter named 'V'?

        Parameters
        ----------
        V : ndarray
            An array containing a design matrix representation of
            training examples.
        axes : WRITEME
        """
        assert not np.any(np.isnan(V))
        rows = V.shape[axes.index(0)]
        cols = V.shape[axes.index(1)]
        channels = V.shape[axes.index('c')]
        self.view_converter = DefaultViewConverter([rows, cols, channels],
                                                   axes=axes)
        self.X = self.view_converter.topo_view_to_design_mat(V)
        # self.X_topo_space stores a "default" topological space that
        # will be used only when self.iterator is called without a
        # data_specs, and with "topo=True", which is deprecated.
        self.X_topo_space = self.view_converter.topo_space
        assert not np.any(np.isnan(self.X))

        # Update data specs
        X_space = VectorSpace(dim=self.X.shape[1])
        X_source = 'features'
        if self.y is None:
            space = X_space
            source = X_source
        else:
            if self.y.ndim == 1:
                dim = 1
            else:
                dim = self.y.shape[-1]
            # This is to support old pickled models
            if getattr(self, 'y_labels', None) is not None:
                y_space = IndexSpace(dim=dim, max_labels=self.y_labels)
            elif getattr(self, 'max_labels', None) is not None:
                y_space = IndexSpace(dim=dim, max_labels=self.max_labels)
            else:
                y_space = VectorSpace(dim=dim)
            y_source = 'targets'
            space = CompositeSpace((X_space, y_space))
            source = (X_source, y_source)

        self.data_specs = (space, source)
        self.X_space = X_space
        self._iter_data_specs = (X_space, X_source)

    def get_design_matrix(self, topo=None):
        """
        Return topo (a batch of examples in topology preserving format),
        in design matrix format.

        Parameters
        ----------
        topo : ndarray, optional
            An array containing a topological representation of training
            examples. If unspecified, the entire dataset (`self.X`) is used
            instead.

        Returns
        -------
        WRITEME
        """
        if topo is not None:
            if self.view_converter is None:
                raise Exception("Tried to convert from topological_view to "
                                "design matrix using a dataset that has no "
                                "view converter")
            return self.view_converter.topo_view_to_design_mat(topo)

        return self.X

    def set_design_matrix(self, X):
        """
        .. todo::

            WRITEME

        Parameters
        ----------
        X : ndarray
            WRITEME
        """
        assert len(X.shape) == 2
        assert not np.any(np.isnan(X))
        self.X = X

    def get_targets(self):
        """
        .. todo::

            WRITEME
        """
        return self.y

    @property
    def num_examples(self):
        """
        .. todo::

            WRITEME
        """
        return self.X.shape[0]

    def get_batch_design(self, batch_size, include_labels=False):
        """
        .. todo::

            WRITEME

        Parameters
        ----------
        batch_size : int
            WRITEME
        include_labels : bool
            WRITEME
        """
        try:
            idx = self.rng.randint(self.X.shape[0] - batch_size + 1)
        except ValueError:
            if batch_size > self.X.shape[0]:
                raise ValueError("Requested %d examples from a dataset "
                                 "containing only %d." %
                                 (batch_size, self.X.shape[0]))
            raise
        rx = self.X[idx:idx + batch_size, :]
        if include_labels:
            if self.y is None:
                return rx, None
            ry = self.y[idx:idx + batch_size]
            return rx, ry
        rx = np.cast[config.floatX](rx)
        return rx

    def get_batch_topo(self, batch_size, include_labels=False):
        """
        .. todo::

            WRITEME

        Parameters
        ----------
        batch_size : int
            WRITEME
        include_labels : bool
            WRITEME
        """

        if include_labels:
            batch_design, labels = self.get_batch_design(batch_size, True)
        else:
            batch_design = self.get_batch_design(batch_size)

        rval = self.view_converter.design_mat_to_topo_view(batch_design)

        if include_labels:
            return rval, labels

        return rval

    def view_shape(self):
        """
        .. todo::

            WRITEME
        """
        return self.view_converter.view_shape()

    def weights_view_shape(self):
        """
        .. todo::

            WRITEME
        """
        return self.view_converter.weights_view_shape()

    def has_targets(self):
        """
        .. todo::

            WRITEME
        """
        return self.y is not None

    def restrict(self, start, stop):
        """
        .. todo::

            WRITEME properly

        Restricts the dataset to include only the examples
        in range(start, stop). Ignored if both arguments are None.

        Parameters
        ----------
        start : int
            start index
        stop : int
            stop index
        """
        assert (start is None) == (stop is None)
        if start is None:
            return
        assert start >= 0
        assert stop > start
        assert stop <= self.X.shape[0]
        assert self.X.shape[0] == self.y.shape[0]
        self.X = self.X[start:stop, :]
        if self.y is not None:
            self.y = self.y[start:stop, :]
        assert self.X.shape[0] == self.y.shape[0]
        assert self.X.shape[0] == stop - start

    def convert_to_one_hot(self, min_class=0):
        """
        .. todo::

            WRITEME properly

        If y exists and is a vector of ints, converts it to a binary matrix
        Otherwise will raise some exception

        Parameters
        ----------
        min_class : int
            WRITEME
        """

        if self.y is None:
            raise ValueError("Called convert_to_one_hot on a "
                             "DenseDesignMatrix with no labels.")

        if self.y.ndim != 1:
            raise ValueError("Called convert_to_one_hot on a "
                             "DenseDesignMatrix whose labels aren't scalar.")

        if 'int' not in str(self.y.dtype):
            raise ValueError("Called convert_to_one_hot on a "
                             "DenseDesignMatrix whose labels aren't "
                             "integer-valued.")

        self.y = self.y - min_class

        if self.y.min() < 0:
            raise ValueError("We do not support negative classes. You can use "
                             "the min_class argument to remap negative "
                             "classes to positive values, but we require this "
                             "to be done explicitly so you are aware of the "
                             "remapping.")
        # Note: we don't check that the minimum occurring class is exactly 0,
        # since this dataset could be just a small subset of a larger dataset
        # and may not contain all the classes.

        num_classes = self.y.max() + 1

        y = np.zeros((self.y.shape[0], num_classes))

        for i in xrange(self.y.shape[0]):
            y[i, self.y[i]] = 1

        self.y = y

        # Update self.data_specs with the updated dimension of self.y
        init_space, source = self.data_specs
        X_space, init_y_space = init_space.components
        new_y_space = VectorSpace(dim=num_classes)
        new_space = CompositeSpace((X_space, new_y_space))
        self.data_specs = (new_space, source)

    def adjust_for_viewer(self, X):
        """
        .. todo::

            WRITEME

        Parameters
        ----------
        X : ndarray
            The data to be adjusted
        """
        return X / np.abs(X).max()

    def adjust_to_be_viewed_with(self, X, ref, per_example=None):
        """
        .. todo::

            WRITEME

        Parameters
        ----------
        X : int
            WRITEME
        ref : float
            WRITEME
        per_example : obejct, optional
            WRITEME
        """
        if per_example is not None:
            logger.warning("ignoring per_example")
        return np.clip(X / np.abs(ref).max(), -1., 1.)

    def get_data_specs(self):
        """
        Returns the data_specs specifying how the data is internally stored.

        This is the format the data returned by `self.get_data()` will be.
        """
        return self.data_specs

    def set_view_converter_axes(self, axes):
        """
        .. todo::

            WRITEME properly

        Change the axes of the view_converter, if any.

        This function is only useful if you intend to call self.iterator
        without data_specs, and with "topo=True", which is deprecated.

        Parameters
        ----------
        axes : WRITEME
            WRITEME
        """
        assert self.view_converter is not None

        self.view_converter.set_axes(axes)
        # Update self.X_topo_space, which stores the "default"
        # topological space, which is the topological output space
        # of the view_converter
        self.X_topo_space = self.view_converter.topo_space


class DenseDesignMatrixPyTables(DenseDesignMatrix):
    """
    DenseDesignMatrix based on PyTables

    Parameters
    ----------
    X : ndarray, 2-dimensional, optional
        Should be supplied if `topo_view` is not. A design matrix of shape
        (number examples, number features) that defines the dataset.
    topo_view : ndarray, optional
        Should be supplied if X is not.  An array whose first dimension is of
        length number examples. The remaining dimensions are xamples with
        topological significance, e.g. for images the remaining axes are rows,
        columns, and channels.
    y : ndarray, 1-dimensional(?), optional
        Labels or targets for each example. The semantics here are not quite
        nailed down for this yet.
    view_converter : object, optional
        An object for converting between design matrices and topological views.
        Currently DefaultViewConverter is the only type available but later we
        may want to add one that uses the retina encoding that the U of T group
        uses.
    axes : WRITEME
        WRITEME
    rng : object, optional
        A random number generator used for picking random indices into the
        design matrix when choosing minibatches.
    """

    _default_seed = (17, 2, 946)

    def __init__(self,
                 X=None,
                 topo_view=None,
                 y=None,
                 view_converter=None,
                 axes=('b', 0, 1, 'c'),
                 rng=_default_seed):
        super_self = super(DenseDesignMatrixPyTables, self)
        super_self.__init__(X=X,
                            topo_view=topo_view,
                            y=y,
                            view_converter=view_converter,
                            axes=axes,
                            rng=rng)
        ensure_tables()
        if not hasattr(self, 'filters'):
            self.filters = tables.Filters(complib='blosc', complevel=5)

    def set_design_matrix(self, X, start=0):
        """
        .. todo::

            WRITEME
        """
        assert len(X.shape) == 2
        assert not np.any(np.isnan(X))
        DenseDesignMatrixPyTables.fill_hdf5(file_handle=self.h5file,
                                            data_x=X,
                                            start=start)

    def set_topological_view(self, V, axes=('b', 0, 1, 'c'), start=0):
        """
        Sets the dataset to represent V, where V is a batch
        of topological views of examples.

        .. todo::

            Why is this parameter named 'V'?

        Parameters
        ----------
        V : ndarray
            An array containing a design matrix representation of training \
            examples. If unspecified, the entire dataset (`self.X`) is used \
            instead.
        axes : WRITEME
            WRITEME
        start : WRITEME
        """
        assert not np.any(np.isnan(V))
        rows = V.shape[axes.index(0)]
        cols = V.shape[axes.index(1)]
        channels = V.shape[axes.index('c')]
        self.view_converter = DefaultViewConverter([rows, cols, channels],
                                                   axes=axes)
        X = self.view_converter.topo_view_to_design_mat(V)
        assert not np.any(np.isnan(X))
        DenseDesignMatrixPyTables.fill_hdf5(file_handle=self.h5file,
                                            data_x=X,
                                            start=start)

    def init_hdf5(self, path, shapes):
        """
        .. todo::

            WRITEME properly

        Initialize hdf5 file to be used ba dataset
        """

        x_shape, y_shape = shapes
        # make pytables
        ensure_tables()
        h5file = tables.openFile(path, mode="w", title="SVHN Dataset")
        gcolumns = h5file.createGroup(h5file.root, "Data", "Data")
        atom = (tables.Float32Atom() if config.floatX == 'float32'
                else tables.Float64Atom())
        h5file.createCArray(gcolumns, 'X', atom=atom, shape=x_shape,
                            title="Data values", filters=self.filters)
        h5file.createCArray(gcolumns, 'y', atom=atom, shape=y_shape,
                            title="Data targets", filters=self.filters)
        return h5file, gcolumns

    @staticmethod
    def fill_hdf5(file_handle,
                  data_x,
                  data_y=None,
                  node=None,
                  start=0,
                  batch_size=5000):
        """
        .. todo::

            WRITEME properly

        PyTables tends to crash if you write large data on them at once.
        This function write data on file_handle in batches

        start: the start index to write data
        """

        if node is None:
            node = file_handle.getNode('/', 'Data')

        data_size = data_x.shape[0]
        last = np.floor(data_size / float(batch_size)) * batch_size
        for i in xrange(0, data_size, batch_size):
            stop = (i + np.mod(data_size, batch_size) if i >= last
                    else i + batch_size)
            assert len(range(start + i, start + stop)) == len(range(i, stop))
            assert (start + stop) <= (node.X.shape[0])
            node.X[start + i: start + stop, :] = data_x[i:stop, :]
            if data_y is not None:
                node.y[start + i: start + stop, :] = data_y[i:stop, :]

            file_handle.flush()

    def resize(self, h5file, start, stop):
        """
        .. todo::

            WRITEME
        """
        ensure_tables()
        # TODO is there any smarter and more efficient way to this?

        data = h5file.getNode('/', "Data")
        try:
            gcolumns = h5file.createGroup('/', "Data_", "Data")
        except tables.exceptions.NodeError:
            h5file.removeNode('/', "Data_", 1)
            gcolumns = h5file.createGroup('/', "Data_", "Data")

        start = 0 if start is None else start
        stop = gcolumns.X.nrows if stop is None else stop

        atom = (tables.Float32Atom() if config.floatX == 'float32'
                else tables.Float64Atom())
        x = h5file.createCArray(gcolumns,
                                'X',
                                atom=atom,
                                shape=((stop - start, data.X.shape[1])),
                                title="Data values",
                                filters=self.filters)
        y = h5file.createCArray(gcolumns,
                                'y',
                                atom=atom,
                                shape=((stop - start, 10)),
                                title="Data targets",
                                filters=self.filters)
        x[:] = data.X[start:stop]
        y[:] = data.y[start:stop]

        h5file.removeNode('/', "Data", 1)
        h5file.renameNode('/', "Data", "Data_")
        h5file.flush()
        return h5file, gcolumns


class DefaultViewConverter(object):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    shape : WRITEME
    axes : WRITEME
    """
    def __init__(self, shape, axes=('b', 0, 1, 'c')):
        self.shape = shape
        self.pixels_per_channel = 1
        for dim in self.shape[:-1]:
            self.pixels_per_channel *= dim
        self.axes = axes
        self._update_topo_space()

    def view_shape(self):
        """
        .. todo::

            WRITEME
        """
        return self.shape

    def weights_view_shape(self):
        """
        .. todo::

            WRITEME
        """
        return self.shape

    def design_mat_to_topo_view(self, X):
        """
        .. todo::

            WRITEME
        """
        assert len(X.shape) == 2
        batch_size = X.shape[0]

        channel_shape = [batch_size, self.shape[0], self.shape[1], 1]
        dimshuffle_args = [('b', 0, 1, 'c').index(axis) for axis in self.axes]
        if self.shape[-1] * self.pixels_per_channel != X.shape[1]:
            raise ValueError('View converter with ' + str(self.shape[-1]) +
                             ' channels and ' + str(self.pixels_per_channel) +
                             ' pixels per channel asked to convert design'
                             ' matrix with ' + str(X.shape[1]) + ' columns.')

        def get_channel(channel_index):
            start = self.pixels_per_channel * channel_index
            stop = self.pixels_per_channel * (channel_index + 1)
            data = X[:, start:stop]
            return data.reshape(*channel_shape).transpose(*dimshuffle_args)

        channels = [get_channel(i) for i in xrange(self.shape[-1])]

        channel_idx = self.axes.index('c')
        rval = np.concatenate(channels, axis=channel_idx)
        assert len(rval.shape) == len(self.shape) + 1
        return rval

    def design_mat_to_weights_view(self, X):
        """
        .. todo::

            WRITEME
        """
        rval = self.design_mat_to_topo_view(X)

        # weights view is always for display
        rval = np.transpose(rval, tuple(self.axes.index(axis)
                                        for axis in ('b', 0, 1, 'c')))

        return rval

    def topo_view_to_design_mat(self, V):
        """
        .. todo::

            WRITEME
        """

        V = V.transpose(self.axes.index('b'),
                        self.axes.index(0),
                        self.axes.index(1),
                        self.axes.index('c'))

        num_channels = self.shape[-1]
        if np.any(np.asarray(self.shape) != np.asarray(V.shape[1:])):
            raise ValueError('View converter for views of shape batch size '
                             'followed by ' + str(self.shape) +
                             ' given tensor of shape ' + str(V.shape))
        batch_size = V.shape[0]

        rval = np.zeros((batch_size, self.pixels_per_channel * num_channels),
                        dtype=V.dtype)

        for i in xrange(num_channels):
            ppc = self.pixels_per_channel
            rval[:, i * ppc:(i + 1) * ppc] = V[..., i].reshape(batch_size, ppc)
        assert rval.dtype == V.dtype

        return rval

    def get_formatted_batch(self, batch, dspace):
        """
        .. todo::

            WRITEME properly

        Reformat batch from the internal storage format into dspace.
        """
        if isinstance(dspace, VectorSpace):
            # If a VectorSpace is requested, batch should already be in that
            # space. We call np_format_as anyway, in case the batch needs to be
            # cast to dspace.dtype. This also validates the batch shape, to
            # check that it's a valid batch in dspace.
            return dspace.np_format_as(batch, dspace)
        elif isinstance(dspace, Conv2DSpace):
            # design_mat_to_topo_view will return a batch formatted
            # in a Conv2DSpace, but not necessarily the right one.
            topo_batch = self.design_mat_to_topo_view(batch)
            if self.topo_space.axes != self.axes:
                warnings.warn("It looks like %s.axes has been changed "
                              "directly, please use the set_axes() method "
                              "instead." % self.__class__.__name__)
                self._update_topo_space()

            return self.topo_space.np_format_as(topo_batch, dspace)
        else:
            raise ValueError("%s does not know how to format a batch into "
                             "%s of type %s."
                             % (self.__class__.__name__, dspace, type(dspace)))

    def __setstate__(self, d):
        """
        .. todo::

            WRITEME
        """
        # Patch old pickle files that don't have the axes attribute.
        if 'axes' not in d:
            d['axes'] = ['b', 0, 1, 'c']
        self.__dict__.update(d)

        # Same for topo_space
        if 'topo_space' not in self.__dict__:
            self._update_topo_space()

    def _update_topo_space(self):
        """Update self.topo_space from self.shape and self.axes"""
        rows, cols, channels = self.shape
        self.topo_space = Conv2DSpace(shape=(rows, cols),
                                      num_channels=channels,
                                      axes=self.axes)

    def set_axes(self, axes):
        """
        .. todo::

            WRITEME
        """
        self.axes = axes
        self._update_topo_space()


def from_dataset(dataset, num_examples):
    """
    .. todo::

        WRITEME
    """
    try:

        V, y = dataset.get_batch_topo(num_examples, True)

    except:

        # This patches a case where control.get_load_data() is false so
        # dataset.X is None This logic should be removed whenever we implement
        # lazy loading

        if isinstance(dataset, DenseDesignMatrix) and \
           dataset.X is None and \
           not control.get_load_data():
            warnings.warn("from_dataset wasn't able to make subset of "
                          "dataset, using the whole thing")
            return DenseDesignMatrix(X=None,
                                     view_converter=dataset.view_converter)
        raise

    rval = DenseDesignMatrix(topo_view=V, y=y)
    rval.adjust_for_viewer = dataset.adjust_for_viewer

    return rval


def dataset_range(dataset, start, stop):
    """
    Returns a new dataset formed by extracting a range of examples from an
    existing dataset.

    Parameters
    ----------
    dataset : DenseDesignMatrix
        The existing dataset to extract examples from.
    start : int
        Extract examples starting at this index.
    stop : int
        Stop extracting examples at this index. Do not include this index
        itself (like the python `range` builtin)

    Returns
    -------
    sub_dataset : DenseDesignMatrix
        The new dataset containing examples [start, stop).
    """

    if dataset.X is None:
        return DenseDesignMatrix(X=None,
                                 y=None,
                                 view_converter=dataset.view_converter)
    X = dataset.X[start:stop, :].copy()
    if dataset.y is None:
        y = None
    else:
        if dataset.y.ndim == 2:
            y = dataset.y[start:stop, :].copy()
        else:
            y = dataset.y[start:stop].copy()
        assert X.shape[0] == y.shape[0]
    assert X.shape[0] == stop - start
    topo = dataset.get_topological_view(X)
    rval = DenseDesignMatrix(topo_view=topo, y=y)
    rval.adjust_for_viewer = dataset.adjust_for_viewer
    return rval


def convert_to_one_hot(dataset, min_class=0):
    """
    .. todo::

        WRITEME properly

    Convenient way of accessing convert_to_one_hot from a yaml file
    """
    dataset.convert_to_one_hot(min_class=min_class)
    return dataset


def set_axes(dataset, axes):
    """
    .. todo::

        WRITEME
    """
    dataset.set_view_converter_axes(axes)
    return dataset

########NEW FILE########
__FILENAME__ = exc
"""
.. todo::

    WRITEME
"""
__author__ = "Ian Goodfellow"
"""
Exceptions related to datasets
"""

from pylearn2.utils.exc import EnvironmentVariableError, NoDataPathError
from pylearn2.utils.common_strings import environment_variable_essay


class NotInstalledError(Exception):
    """
    Exception raised when a dataset appears not to be installed.
    This is different from an individual file missing within a dataset,
    the file not loading correctly, etc.
    This exception is used to make unit tests skip testing of datasets
    that haven't been installed.
    We do want the unit test to run and crash if the dataset is installed
    incorrectly.
    """

########NEW FILE########
__FILENAME__ = filetensor
"""
Read and write the matrix file format described at
 http://www.cs.nyu.edu/~ylclab/data/norb-v1.0/index.html

The format is for dense tensors:

- magic number indicating type and endianness - 4bytes
- rank of tensor - int32
- dimensions - int32, int32, int32, ...
- <data>

The number of dimensions and rank is slightly tricky:

- for scalar: rank=0, dimensions = [1, 1, 1]
- for vector: rank=1, dimensions = [?, 1, 1]
- for matrix: rank=2, dimensions = [?, ?, 1]

For rank >= 3, the number of dimensions matches the rank exactly.
"""
import bz2
import gzip
import logging

import numpy

logger = logging.getLogger(__name__)


def _prod(lst):
    """
    .. todo::

        WRITEME
    """
    p = 1
    for l in lst:
        p *= l
    return p

_magic_dtype = {
        0x1E3D4C51 : ('float32', 4),
        #0x1E3D4C52 : ('packed matrix', 0), #what is a packed matrix?
        0x1E3D4C53 : ('float64', 8),
        0x1E3D4C54 : ('int32', 4),
        0x1E3D4C55 : ('uint8', 1),
        0x1E3D4C56 : ('int16', 2),
        }
_dtype_magic = {
        'float32': 0x1E3D4C51,
        #'packed matrix': 0x1E3D4C52,
        'float64': 0x1E3D4C53,
        'int32': 0x1E3D4C54,
        'uint8': 0x1E3D4C55,
        'int16': 0x1E3D4C56
        }

def _read_int32(f):
    """unpack a 4-byte integer from the current position in file f"""
    s = f.read(4)
    s_array = numpy.fromstring(s, dtype='int32')
    return s_array.item()

def _read_header(f, debug=False, fromgzip=None):
    """
    Parameters
    ----------
    f : file or gzip.GzipFile
        An open file handle.
    fromgzip : bool or None
        If None determine the type of file handle.

    Returns
    -------
    data type, element size, rank, shape, size
    """
    if fromgzip is None:
        fromgzip = isinstance(f, (gzip.GzipFile, bz2.BZ2File))

    #what is the data type of this matrix?
    #magic_s = f.read(4)
    #magic = numpy.fromstring(magic_s, dtype='int32')
    magic = _read_int32(f)
    magic_t, elsize = _magic_dtype[magic]
    if debug:
        logger.debug('header magic {0} {1} {2}'.format(magic, magic_t, elsize))
    if magic_t == 'packed matrix':
        raise NotImplementedError('packed matrix not supported')

    #what is the rank of the tensor?
    ndim = _read_int32(f)
    if debug:
        logger.debug('header ndim {0}'.format(ndim))

    #what are the dimensions of the tensor?
    if fromgzip:
        d = f.read(max(ndim,3)*4)
        dim = numpy.fromstring(d, dtype='int32')[:ndim]
    else:
        dim = numpy.fromfile(f, dtype='int32', count=max(ndim,3))[:ndim]
    dim_size = _prod(dim)
    if debug:
        logger.debug('header dim {0} {1}'.format(dim, dim_size))

    return magic_t, elsize, ndim, dim, dim_size

class arraylike(object):
    """
    Provide an array-like interface to the filetensor in f.

    The rank parameter to __init__ controls how this object interprets the underlying tensor.
    Its behaviour should be clear from the following example.
    Suppose the underlying tensor is MxNxK.

        - If rank is 0, self[i] will be a scalar and len(self) == M*N*K.
        - If rank is 1, self[i] is a vector of length K, and len(self) == M*N.
        - If rank is 3, self[i] is a 3D tensor of size MxNxK, and len(self)==1.
        - If rank is 5, self[i] is a 5D tensor of size 1x1xMxNxK, and len(self) == 1.


    Note: Objects of this class generally require exclusive use of the underlying file handle, because
    they call seek() every time you access an element.
    """

    f = None
    """File-like object"""

    magic_t = None
    """numpy data type of array"""

    elsize = None
    """number of bytes per scalar element"""

    ndim = None
    """Rank of underlying tensor"""

    dim = None
    """tuple of array dimensions (aka shape)"""

    dim_size = None
    """number of scalars in the tensor (prod of dim)"""

    f_start = None
    """The file position of the first element of the tensor"""

    readshape = None
    """tuple of array dimensions of the block that we read"""

    readsize = None
    """
    number of elements we must read for each block
    """

    def __init__(self, f, rank=0, debug=False):
        """
        .. todo::

            WRITEME
        """
        self.f = f
        self.magic_t, self.elsize, self.ndim, self.dim, self.dim_size = _read_header(f,debug)
        self.f_start = f.tell()

        if rank <= self.ndim:
          self.readshape = tuple(self.dim[self.ndim-rank:])
        else:
          self.readshape = tuple(self.dim)

        #self.readshape = tuple(self.dim[self.ndim-rank:]) if rank <= self.ndim else tuple(self.dim)

        if rank <= self.ndim:
          padding = tuple()
        else:
          padding = (1,) * (rank - self.ndim)

        #padding = tuple() if rank <= self.ndim else (1,) * (rank - self.ndim)
        self.returnshape = padding + self.readshape
        self.readsize = _prod(self.readshape)
        if debug:
            logger.debug('READ PARAM {0} {1}'.format(self.readshape,
                                                     self.returnshape,
                                                     self.readsize))

    def __len__(self):
        """
        .. todo::

            WRITEME
        """
        return _prod(self.dim[:self.ndim-len(self.readshape)])

    def __getitem__(self, idx):
        """
        .. todo::

            WRITEME
        """
        if idx >= len(self):
            raise IndexError(idx)
        self.f.seek(self.f_start + idx * self.elsize * self.readsize)
        return numpy.fromfile(self.f,
                dtype=self.magic_t,
                count=self.readsize).reshape(self.returnshape)


#
# TODO: implement item selection:
#  e.g. load('some mat', subtensor=(:6, 2:5))
#
#  This function should be memory efficient by:
#  - allocating an output matrix at the beginning
#  - seeking through the file, reading subtensors from multiple places
def read(f, subtensor=None, debug=False):
    """
    Load all or part of file tensorfile 'f' into a numpy ndarray

    Parameters
    ----------
    f : file, gzip.Gzip or bz2.BZ2File like object
        Open file descriptor to read data from
    subtensor : None or a slice argument accepted __getitem__
        If subtensor is not None, it should be like the argument to
        numpy.ndarray.__getitem__.  The following two expressions should return
        equivalent ndarray objects, but the one on the left may be faster and more
        memory efficient if the underlying file f is big.

        .. code-block:: none

            read(f, subtensor) <===> read(f)[*subtensor]

        Support for subtensors is currently spotty, so check the code to see if your
        particular type of subtensor is supported.

    Returns
    -------
    y : ndarray
        Data read from disk
    """
    magic_t, elsize, ndim, dim, dim_size = _read_header(f,debug)
    f_start = f.tell()

    rval = None
    if isinstance(f, (gzip.GzipFile, bz2.BZ2File)):
        assert subtensor is None, "Not implemented the subtensor case for gzip file"
        d = f.read(_prod(dim)*elsize)
        rval = numpy.fromstring(d, dtype=magic_t).reshape(dim)
        del d
    elif subtensor is None:
        rval = numpy.fromfile(f, dtype=magic_t, count=_prod(dim)).reshape(dim)
    elif isinstance(subtensor, slice):
        if subtensor.step not in (None, 1):
            raise NotImplementedError('slice with step', subtensor.step)
        if subtensor.start not in (None, 0):
            bytes_per_row = _prod(dim[1:]) * elsize
            f.seek(f_start+subtensor.start * bytes_per_row)
        dim[0] = min(dim[0], subtensor.stop) - subtensor.start
        rval = numpy.fromfile(f, dtype=magic_t, count=_prod(dim)).reshape(dim)
    else:
        raise NotImplementedError('subtensor access not written yet:', subtensor)

    return rval

def write(f, mat):
    """ Write a ndarray to tensorfile.

    Parameters
    ----------
    f : file
        Open file to write into
    mat : ndarray
        Array to save
    """
    def _write_int32(f, i):
        i_array = numpy.asarray(i, dtype='int32')
        if 0:
            logger.debug('writing int32 {0} {1}'.format(i, i_array))
        i_array.tofile(f)

    try:
        _write_int32(f, _dtype_magic[str(mat.dtype)])
    except KeyError:
        raise TypeError('Invalid ndarray dtype for filetensor format', mat.dtype)

    _write_int32(f, len(mat.shape))
    shape = mat.shape
    if len(shape) < 3:
        shape = list(shape) + [1] * (3 - len(shape))
    if 0:
        logger.debug('writing shape = {0}'.format(shape))
    for sh in shape:
        _write_int32(f, sh)
    mat.tofile(f)

########NEW FILE########
__FILENAME__ = four_regions
"""
The four regions task. A synthetic dataset from the late 1980s,
a 4-class classification problem.
"""
__authors__ = "David Warde-Farley"
__copyright__ = "Copyright 2013, Universite de Montreal"
__credits__ = ["David Warde-Farley"]
__license__ = "3-clause BSD"
__maintainer__ = "David Warde-Farley"
__email__ = "wardefar@iro"


import numpy as np
from theano import config
from pylearn2.datasets import DenseDesignMatrix
from pylearn2.utils.rng import make_np_rng


def _four_regions_labels(points):
    """
    Returns labels for points in [-1, 1]^2 from the
    "four regions" benchmark task first described by
    Singhal and Wu.

    Parameters
    ----------
    points : array_like, 2-dimensional
        An (N, 2) list of 2-dimensional points.

    Returns
    -------
    labels : ndarray, 1-dimensional
        An N-length array of labels in [1, 2, 3, 4].

    References
    ----------
    .. [1] S. Singhal and L. Wu, "Training multilayer perceptrons
      with the extended Kalman algorithm". Advances in Neural
      Information Processing Systems, 1, (1988) pp 133-140.
      http://books.nips.cc/papers/files/nips01/0133.pdf
    """
    points = np.asarray(points)
    region = np.zeros(points.shape[0], dtype='uint8')
    tophalf = points[:, 1] > 0
    righthalf = points[:, 0] > 0
    dists = np.sqrt(np.sum(points ** 2, axis=1))

    # The easy ones -- the outer shelf.
    region[dists > np.sqrt(2)] = 255
    outer = dists > 5. / 6.
    region[np.logical_and(tophalf, outer)] = 3
    region[np.logical_and(np.logical_not(tophalf), outer)] = 0

    firstring = np.logical_and(dists > 1. / 6., dists <= 1. / 2.)
    secondring = np.logical_and(dists > 1. / 2., dists <= 5. / 6.)

    # Region 2 -- right inner and left outer, excluding center nut
    region[np.logical_and(firstring, righthalf)] = 2
    region[np.logical_and(secondring, np.logical_not(righthalf))] = 2

    # Region 1 -- left inner and right outer, including center nut
    region[np.logical_and(secondring, righthalf)] = 1
    region[np.logical_and(np.logical_not(righthalf), dists < 1. / 2.)] = 1
    region[np.logical_and(righthalf, dists < 1. / 6.)] = 1
    assert np.all(region >= 0) and np.all(region <= 3)
    return region


class FourRegions(DenseDesignMatrix):
    """
    Constructs a dataset based on the four regions
    benchmark by sampling random uniform points in [-1, 1]^2
    and constructing the label.

    Parameters
    ----------
    num_examples : int
        The number of examples to generate.

    rng : RandomState or seed
        A random number generator or a seed used to construct it.

    References
    ----------
    .. [1] S. Singhal and L. Wu, "Training multilayer perceptrons
      with the extended Kalman algorithm". Advances in Neural
      Information Processing Systems, 1, (1988) pp 133-140.
      http://books.nips.cc/papers/files/nips01/0133.pdf
    """
    _default_seed = (2013, 05, 17)

    def __init__(self, num_examples, one_hot=False, rng=(2013, 05, 17)):
        """
        .. todo::

            WRITEME
        """
        rng = make_np_rng(rng, self._default_seed, which_method='uniform')
        X = rng.uniform(-1, 1, size=(num_examples, 2))
        if not one_hot:
            y = _four_regions_labels(X)
        else:
            y = np.zeros((num_examples, 4), dtype=config.floatX)
            labels = _four_regions_labels(X)
            y.flat[np.arange(0, 4 * num_examples, 4) + labels] = 1.
        super(FourRegions, self).__init__(X=X, y=y)

########NEW FILE########
__FILENAME__ = hdf5
"""Objects for datasets serialized in HDF5 format (.h5)."""
import warnings
try:
    import h5py
except ImportError:
    warnings.warn("Could not import h5py")
from pylearn2.datasets.dense_design_matrix import DenseDesignMatrix


class HDF5Dataset(DenseDesignMatrix):
    """Dense dataset loaded from an HDF5 file."""
    def __init__(self, filename, X=None, topo_view=None, y=None, **kwargs):
        """
        Loads data and labels from HDF5 file.

        Parameters
        ----------
        filename: str
            HDF5 file name.
        X: str
            Key into HDF5 file for dataset design matrix.
        topo_view: str
            Key into HDF5 file for topological view of dataset.
        y: str
            Key into HDF5 file for dataset targets.
        kwargs: dict
            Keyword arguments passed to `DenseDesignMatrix`.
        """
        with h5py.File(filename) as f:
            if X is not None:
                X = f[X][:]
            if topo_view is not None:
                topo_view = f[topo_view][:]
            if y is not None:
                y = f[y][:]

        super(HDF5Dataset, self).__init__(X=X, topo_view=topo_view, y=y,
                                          **kwargs)

########NEW FILE########
__FILENAME__ = hepatitis
"""
.. todo::

    WRITEME
"""
__author__ = "Ian Goodfellow"

# TODO: add citation
# http://archive.ics.uci.edu/ml/datasets/Hepatitis

import numpy as np

from pylearn2.datasets.dense_design_matrix import DenseDesignMatrix

class Hepatitis(DenseDesignMatrix):
    """
    .. todo::

        WRITEME
    """

    def __init__(self, preprocessor=None, start=None, stop=None):
        """
        .. todo::

            WRITEME
        """

        self.class_names = ['DIE', 'LIVE']
        lines = hepatitis_data.split('\n')
        X = []
        y = []
        for line in lines:
            row = line.split(',')
            row = [neg_missing(elem) for elem in row]
            X.append([float(elem) for elem in row[1:]])
            y.append(float(row[0]))
        X = np.array(X)

        NUM_FEATURES = 19
        NUM_EXAMPLES = 155
        NUM_CLASSES = 2

        AGE, SEX, STEROID, ANTIVIRALS, FATIGUE, MALAISE, ANOREXIA, \
        LIVER_BIG, LIVER_FIRM, SPLEEN_PALPABLE, SPIDERS, ASCITES, \
        VARICES, BILIRUBIN, ALK_PHOSPHATE, SGOT, ALBUMIN, PROTIME, \
        HISTOLOGY = range(NUM_FEATURES)

        real_mask = np.zeros((NUM_FEATURES,), dtype='bool')
        real_mask[[AGE, BILIRUBIN, ALK_PHOSPHATE, SGOT, ALBUMIN, PROTIME, HISTOLOGY]] = 1
        real_X = X[:, real_mask]
        binary_mask = (1 - real_mask).astype('bool')
        binary_X = X[:, binary_mask]
        binary_X[binary_X == 1] = 0
        binary_X[binary_X == 2] = 1
        for i in xrange(binary_X.shape[0]):
            for j in xrange(binary_X.shape[1]):
                assert binary_X[i,j] in [-1., 0., 1.], (binary_X[i,j], i, j)

        X = np.concatenate((real_X, binary_X), axis=1)

        assert X.shape == (NUM_EXAMPLES, NUM_FEATURES)
        assert len(y) == NUM_EXAMPLES

        y = np.asarray(y) - 1
        assert min(y) == 0
        assert max(y) == NUM_CLASSES - 1

        one_hot = np.zeros((NUM_EXAMPLES, NUM_CLASSES))
        for i in xrange(len(y)):
            one_hot[i, y[i] - 1] = 1


        super(Hepatitis, self).__init__(X=X, y=one_hot, preprocessor=preprocessor)

        self.restrict(start, stop)

def neg_missing(s):
    """
    .. todo::

        WRITEME
    """
    if s == "?":
        return "-1"
    return s

hepatitis_data = \
"""2,30,2,1,2,2,2,2,1,2,2,2,2,2,1.00,85,18,4.0,?,1
2,50,1,1,2,1,2,2,1,2,2,2,2,2,0.90,135,42,3.5,?,1
2,78,1,2,2,1,2,2,2,2,2,2,2,2,0.70,96,32,4.0,?,1
2,31,1,?,1,2,2,2,2,2,2,2,2,2,0.70,46,52,4.0,80,1
2,34,1,2,2,2,2,2,2,2,2,2,2,2,1.00,?,200,4.0,?,1
2,34,1,2,2,2,2,2,2,2,2,2,2,2,0.90,95,28,4.0,75,1
1,51,1,1,2,1,2,1,2,2,1,1,2,2,?,?,?,?,?,1
2,23,1,2,2,2,2,2,2,2,2,2,2,2,1.00,?,?,?,?,1
2,39,1,2,2,1,2,2,2,1,2,2,2,2,0.70,?,48,4.4,?,1
2,30,1,2,2,2,2,2,2,2,2,2,2,2,1.00,?,120,3.9,?,1
2,39,1,1,1,2,2,2,1,1,2,2,2,2,1.30,78,30,4.4,85,1
2,32,1,2,1,1,2,2,2,1,2,1,2,2,1.00,59,249,3.7,54,1
2,41,1,2,1,1,2,2,2,1,2,2,2,2,0.90,81,60,3.9,52,1
2,30,1,2,2,1,2,2,2,1,2,2,2,2,2.20,57,144,4.9,78,1
2,47,1,1,1,2,2,2,2,2,2,2,2,2,?,?,60,?,?,1
2,38,1,1,2,1,1,1,2,2,2,2,1,2,2.00,72,89,2.9,46,1
2,66,1,2,2,1,2,2,2,2,2,2,2,2,1.20,102,53,4.3,?,1
2,40,1,1,2,1,2,2,2,1,2,2,2,2,0.60,62,166,4.0,63,1
2,38,1,2,2,2,2,2,2,2,2,2,2,2,0.70,53,42,4.1,85,2
2,38,1,1,1,2,2,2,1,1,2,2,2,2,0.70,70,28,4.2,62,1
2,22,2,2,1,1,2,2,2,2,2,2,2,2,0.90,48,20,4.2,64,1
2,27,1,2,2,1,1,1,1,1,1,1,2,2,1.20,133,98,4.1,39,1
2,31,1,2,2,2,2,2,2,2,2,2,2,2,1.00,85,20,4.0,100,1
2,42,1,2,2,2,2,2,2,2,2,2,2,2,0.90,60,63,4.7,47,1
2,25,2,1,1,2,2,2,2,2,2,2,2,2,0.40,45,18,4.3,70,1
2,27,1,1,2,1,1,2,2,2,2,2,2,2,0.80,95,46,3.8,100,1
2,49,1,1,1,1,1,1,2,1,2,1,2,2,0.60,85,48,3.7,?,1
2,58,2,2,2,1,2,2,2,1,2,1,2,2,1.40,175,55,2.7,36,1
2,61,1,1,2,1,2,2,1,1,2,2,2,2,1.30,78,25,3.8,100,1
2,51,1,1,1,1,1,2,2,2,2,2,2,2,1.00,78,58,4.6,52,1
1,39,1,1,1,1,1,2,2,1,2,2,2,2,2.30,280,98,3.8,40,1
1,62,1,1,2,1,1,2,?,?,2,2,2,2,1.00,?,60,?,?,1
2,41,2,2,1,1,1,1,2,2,2,2,2,2,0.70,81,53,5.0,74,1
2,26,2,1,2,2,2,2,2,1,2,2,2,2,0.50,135,29,3.8,60,1
2,35,1,2,2,1,2,2,2,2,2,2,2,2,0.90,58,92,4.3,73,1
1,37,1,2,2,1,2,2,2,2,2,1,2,2,0.60,67,28,4.2,?,1
2,23,1,2,2,1,1,1,2,2,1,2,2,2,1.30,194,150,4.1,90,1
2,20,2,1,2,1,1,1,1,1,1,1,2,2,2.30,150,68,3.9,?,1
2,42,1,1,2,2,2,2,2,2,2,2,2,2,1.00,85,14,4.0,100,1
2,65,1,2,2,1,1,2,2,1,1,1,1,2,0.30,180,53,2.9,74,2
2,52,1,1,1,2,2,2,2,2,2,2,2,2,0.70,75,55,4.0,21,1
2,23,1,2,2,2,2,2,?,?,?,?,?,?,4.60,56,16,4.6,?,1
2,33,1,2,2,2,2,2,2,2,2,2,2,2,1.00,46,90,4.4,60,1
2,56,1,1,2,1,2,2,2,2,2,2,2,2,0.70,71,18,4.4,100,1
2,34,1,2,2,2,2,2,2,2,2,2,2,2,?,?,86,?,?,1
2,28,1,2,2,1,1,2,2,2,2,2,2,2,0.70,74,110,4.4,?,1
2,37,1,1,2,2,2,2,2,1,2,1,2,2,0.60,80,80,3.8,?,1
2,28,2,2,2,1,1,2,2,1,2,2,2,2,1.80,191,420,3.3,46,1
2,36,1,1,2,2,2,2,2,2,1,2,2,2,0.80,85,44,4.2,85,1
2,38,1,2,1,1,1,1,2,2,2,1,2,2,0.70,125,65,4.2,77,1
2,39,1,1,2,2,2,2,2,2,2,2,2,2,0.90,85,60,4.0,?,1
2,39,1,2,2,2,2,2,2,2,2,2,2,2,1.00,85,20,4.0,?,1
2,44,1,2,2,2,2,2,2,2,2,2,2,2,0.60,110,145,4.4,70,1
2,40,1,2,1,1,2,2,2,1,1,2,2,2,1.20,85,31,4.0,100,1
2,30,1,2,2,1,2,2,2,2,2,2,2,2,0.70,50,78,4.2,74,1
2,37,1,1,2,1,1,1,2,2,2,2,2,2,0.80,92,59,?,?,1
2,34,1,1,2,?,?,?,?,?,?,?,?,?,?,?,?,?,?,1
2,30,1,2,1,2,2,2,2,2,2,2,2,2,0.70,52,38,3.9,52,1
2,64,1,2,1,1,1,2,1,1,2,2,2,2,1.00,80,38,4.3,74,1
2,45,2,1,2,1,1,2,2,2,1,2,2,2,1.00,85,75,?,?,1
2,37,1,2,2,2,2,2,2,2,2,2,2,2,0.70,26,58,4.5,100,1
2,32,1,2,2,2,2,2,2,2,2,2,2,2,0.70,102,64,4.0,90,1
2,32,1,2,2,1,1,1,2,2,2,1,2,1,3.50,215,54,3.4,29,1
2,36,1,1,2,2,2,2,1,1,1,2,2,2,0.70,164,44,3.1,41,1
2,49,1,2,2,1,1,2,2,2,2,2,2,2,0.80,103,43,3.5,66,1
2,27,1,2,2,2,2,2,2,2,2,2,2,2,0.80,?,38,4.2,?,1
2,56,1,1,2,2,2,2,2,2,2,2,2,2,0.70,62,33,3.0,?,1
1,57,1,2,2,1,1,1,2,2,2,1,1,2,4.10,?,48,2.6,73,1
2,39,1,2,2,1,2,2,2,2,2,2,2,2,1.00,34,15,4.0,54,1
2,44,1,1,2,1,1,2,2,2,2,2,2,2,1.60,68,68,3.7,?,1
2,24,1,2,2,2,2,2,2,2,2,2,2,2,0.80,82,39,4.3,?,1
1,34,1,1,2,1,1,2,1,1,2,1,2,2,2.80,127,182,?,?,1
2,51,1,2,2,1,1,1,?,?,?,?,?,?,0.90,76,271,4.4,?,1
2,36,1,1,2,1,1,1,2,1,2,2,2,2,1.00,?,45,4.0,57,1
2,50,1,2,2,2,2,2,2,2,2,2,2,2,1.50,100,100,5.3,?,1
2,32,1,1,1,1,1,2,2,2,2,2,2,2,1.00,55,45,4.1,56,1
1,58,1,2,2,1,2,2,1,1,1,1,2,2,2.00,167,242,3.3,?,1
2,34,2,1,1,2,2,2,2,1,2,2,2,2,0.60,30,24,4.0,76,1
2,34,1,1,2,1,2,2,1,1,2,1,2,2,1.00,72,46,4.4,57,1
2,28,1,2,2,2,2,2,2,2,2,2,2,2,0.70,85,31,4.9,?,1
2,23,1,2,2,1,1,1,2,2,2,2,2,2,0.80,?,14,4.8,?,1
2,36,1,2,2,2,2,2,2,2,2,2,2,2,0.70,62,224,4.2,100,1
2,30,1,1,2,2,2,2,2,2,2,2,2,2,0.70,100,31,4.0,100,1
2,67,2,1,2,1,1,2,2,2,?,?,?,?,1.50,179,69,2.9,?,1
2,62,2,2,2,1,1,2,2,1,2,1,2,2,1.30,141,156,3.9,58,1
2,28,1,1,2,1,1,1,2,1,2,2,2,2,1.60,44,123,4.0,46,1
1,44,1,1,2,1,1,2,2,2,1,2,2,1,0.90,135,55,?,41,2
1,30,1,2,2,1,1,1,2,1,2,1,1,1,2.50,165,64,2.8,?,2
1,38,1,1,2,1,1,1,2,1,2,1,1,1,1.20,118,16,2.8,?,2
2,38,1,1,2,1,1,1,1,1,2,2,2,2,0.60,76,18,4.4,84,2
2,50,2,1,2,1,2,2,1,1,1,1,2,2,0.90,230,117,3.4,41,2
1,42,1,1,2,1,1,1,2,2,1,1,2,1,4.60,?,55,3.3,?,2
2,33,1,2,2,2,2,2,?,?,2,2,2,2,1.00,?,60,4.0,?,2
2,52,1,1,2,2,2,2,2,2,2,2,2,2,1.50,?,69,2.9,?,2
1,59,1,1,2,1,1,2,2,1,1,1,2,2,1.50,107,157,3.6,38,2
2,40,1,1,1,1,1,1,1,1,2,2,2,2,0.60,40,69,4.2,67,2
2,30,1,1,2,1,1,2,2,1,2,1,2,2,0.80,147,128,3.9,100,2
2,44,1,1,2,1,1,2,1,1,2,1,2,2,3.00,114,65,3.5,?,2
1,47,1,2,2,2,2,2,2,2,2,1,2,1,2.00,84,23,4.2,66,2
2,60,1,1,2,1,2,2,1,1,1,1,2,2,?,?,40,?,?,2
1,48,1,1,2,1,1,2,2,1,2,1,1,1,4.80,123,157,2.7,31,2
2,22,1,2,2,2,2,2,2,2,2,2,2,2,0.70,?,24,?,?,2
2,27,1,1,2,1,2,2,2,1,2,2,2,2,2.40,168,227,3.0,66,2
2,51,1,1,2,1,1,1,2,1,1,1,2,1,4.60,215,269,3.9,51,2
1,47,1,2,2,1,1,2,2,1,2,2,1,1,1.70,86,20,2.1,46,2
2,25,1,2,2,2,2,2,2,2,2,2,2,2,0.60,?,34,6.4,?,2
1,35,1,1,2,1,2,2,?,?,1,1,1,2,1.50,138,58,2.6,?,2
2,45,1,1,2,1,1,1,2,2,2,2,2,2,2.30,?,648,?,?,2
2,54,1,1,1,2,2,2,1,1,2,2,2,2,1.00,155,225,3.6,67,2
1,33,1,1,2,1,1,2,2,2,2,2,1,2,0.70,63,80,3.0,31,2
2,7,1,2,2,2,2,2,2,1,1,2,2,2,0.70,256,25,4.2,?,2
1,42,1,1,1,1,1,2,2,2,2,1,2,2,0.50,62,68,3.8,29,2
2,52,1,1,2,1,2,2,2,2,2,2,2,2,1.00,85,30,4.0,?,2
2,45,1,1,2,1,2,2,2,1,1,2,2,2,1.20,81,65,3.0,?,1
2,36,1,1,2,2,2,2,2,2,2,2,2,2,1.10,141,75,3.3,?,2
2,69,2,2,2,1,2,2,2,2,2,2,2,2,3.20,119,136,?,?,2
2,24,1,1,2,1,2,2,2,2,2,2,2,2,1.00,?,34,4.1,?,2
2,50,1,2,2,2,2,2,2,2,2,2,2,2,1.00,139,81,3.9,62,2
1,61,1,1,2,1,1,2,?,?,2,1,2,2,?,?,?,?,?,2
2,54,1,2,2,1,2,2,1,1,2,2,2,2,3.20,85,28,3.8,?,2
1,56,1,1,2,1,1,1,1,1,2,1,2,2,2.90,90,153,4.0,?,2
2,20,1,1,2,1,1,1,2,2,2,1,1,2,1.00,160,118,2.9,23,2
2,42,1,2,2,2,2,2,2,2,1,2,2,2,1.50,85,40,?,?,2
2,37,1,1,2,1,2,2,2,2,2,1,2,2,0.90,?,231,4.3,?,2
2,50,1,2,2,2,2,2,2,1,1,1,2,2,1.00,85,75,4.0,72,2
2,34,2,2,2,1,1,1,1,1,2,1,2,2,0.70,70,24,4.1,100,2
2,28,1,2,2,1,1,1,?,?,2,1,1,2,1.00,?,20,4.0,?,2
1,50,1,2,2,1,2,2,2,1,1,2,1,1,2.80,155,75,2.4,32,2
2,54,1,1,2,1,1,2,2,2,2,2,1,2,1.20,85,92,3.1,66,2
1,57,1,1,2,1,1,2,2,2,2,1,1,2,4.60,82,55,3.3,30,2
2,54,1,2,2,2,2,2,2,2,2,2,2,2,1.00,85,30,4.5,0,2
1,31,1,1,2,1,1,1,2,2,1,2,2,2,8.00,?,101,2.2,?,2
2,48,1,2,2,1,1,1,2,1,2,1,2,2,2.00,158,278,3.8,?,2
2,72,1,2,1,1,2,2,2,1,2,2,2,2,1.00,115,52,3.4,50,2
1,38,1,1,2,2,2,2,2,1,2,2,2,2,0.40,243,49,3.8,90,2
2,25,1,2,2,1,2,2,1,1,1,1,1,1,1.30,181,181,4.5,57,2
2,51,1,2,2,2,2,2,1,1,2,1,2,2,0.80,?,33,4.5,?,2
2,38,1,2,2,2,2,2,2,1,2,1,2,1,1.60,130,140,3.5,56,2
1,47,1,2,2,1,1,2,2,1,2,1,1,1,1.00,166,30,2.6,31,2
2,45,1,2,1,2,2,2,2,2,2,2,2,2,1.30,85,44,4.2,85,2
2,36,1,1,2,1,1,1,1,1,2,1,2,1,1.70,295,60,2.7,?,2
1,54,1,1,2,1,1,2,?,?,1,2,1,2,3.90,120,28,3.5,43,2
2,51,1,2,2,1,2,2,2,1,1,1,2,1,1.00,?,20,3.0,63,2
1,49,1,1,2,1,1,2,2,2,1,1,2,2,1.40,85,70,3.5,35,2
1,45,1,2,2,1,1,1,2,2,2,1,1,2,1.90,?,114,2.4,?,2
2,31,1,1,2,1,2,2,2,2,2,2,2,2,1.20,75,173,4.2,54,2
1,41,1,2,2,1,2,2,2,1,1,1,2,1,4.20,65,120,3.4,?,2
1,70,1,1,2,1,1,1,?,?,?,?,?,?,1.70,109,528,2.8,35,2
2,20,1,1,2,2,2,2,2,?,2,2,2,2,0.90,89,152,4.0,?,2
2,36,1,2,2,2,2,2,2,2,2,2,2,2,0.60,120,30,4.0,?,2
1,46,1,2,2,1,1,1,2,2,2,1,1,1,7.60,?,242,3.3,50,2
2,44,1,2,2,1,2,2,2,1,2,2,2,2,0.90,126,142,4.3,?,2
2,61,1,1,2,1,1,2,1,1,2,1,2,2,0.80,75,20,4.1,?,2
2,53,2,1,2,1,2,2,2,2,1,1,2,1,1.50,81,19,4.1,48,2
1,43,1,2,2,1,2,2,2,2,1,1,1,2,1.20,100,19,3.1,42,2"""

########NEW FILE########
__FILENAME__ = icml07
"""
Datasets introduced in:

    An Empirical Evaluation of Deep Architectures on Problems with Many Factors of Variation
    Hugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra and Yoshua Bengio,
    International Conference on Machine Learning, 2007
"""

import os
import numpy as np

from pylearn2.utils.string_utils import preprocess
from pylearn2.datasets.cache import datasetCache
from pylearn2.datasets.dense_design_matrix import (
    DenseDesignMatrix, DefaultViewConverter)


class ICML07DataSet(DenseDesignMatrix):
    """
    Base class for ICML07 datasets.
    
    All these datasets can be displayed as 28x28 pixel datapoints.
    """

    def __init__(self, npy_filename, which_set, one_hot, split):
        assert which_set in ['train', 'valid', 'test']

        self.one_hot = one_hot
        self.split = split

        # Load data from .npy file
        npy_filename_root = os.path.join(preprocess('${PYLEARN2_DATA_PATH}'), 'icml07data', 'npy', npy_filename)

        x_file = npy_filename_root + '_inputs.npy'
        y_file = npy_filename_root + '_labels.npy'
        x_file = datasetCache.cache_file(x_file)
        y_file = datasetCache.cache_file(y_file)
        data_x = np.load(x_file, mmap_mode='r')
        data_y = np.load(y_file, mmap_mode='r')

        # some sanity checkes
        assert np.isfinite(data_x).all()
        assert np.isfinite(data_y).all()
        assert data_x.shape[0] == data_y.shape[0]

        # extract 
        n_train, n_valid, n_test = split
        sets = {
            'train' : (0              , n_train),
            'valid' : (n_train        , n_train+n_valid),
            'test'  : (n_train+n_valid, n_train+n_valid+n_test)
        }
        start, end = sets[which_set]

        data_x = data_x[start:end]
        data_y = data_y[start:end]

        if one_hot:
            n_examples = data_y.shape[0]
            n_classes = data_y.max()+1

            data_oh = np.zeros( (n_examples, n_classes), dtype = 'float32')
            for i in xrange(data_y.shape[0]):
                data_oh[i, data_y[i]] = 1.
            data_y = data_oh

        view_converter = DefaultViewConverter((28,28,1))
        super(ICML07DataSet, self).__init__(X = data_x, y = data_y, view_converter = view_converter)
    
    def get_test_set(self):
        """
        .. todo::

            WRITEME
        """
        return self.__class__(which_set='test', one_hot=self.one_hot, split=self.split)

#
# Actual datasets


class MNIST_rotated_background(ICML07DataSet):
    """ ICML07: Rotated MNIST dataset with background."""

    def __init__(self, which_set, one_hot=False, split=(10000, 2000, 10000)):
        """
        Load ICML07 Rotated MNIST with background dataset.

        Parameters
        ----------
        which_set : 'train', 'valid', 'test'
            Choose a dataset 
        one_hot : bool
            Encode labels one-hot
        split : (n_train, n_valid, n_test)
            Choose a split into train, validateion and test datasets
        
        Default split: 10000 training, 2000 validation and 10000 in test dataset.
        """
        super(MNIST_rotated_background, self).__init__('mnist_rotated_background_images', which_set, one_hot, split)
   

class Convex(ICML07DataSet):
    """
    ICML07: Recognition of Convex Sets datasets.

    All data values are binary, and the classification task is binary.
    """
    def __init__(self, which_set, one_hot=False, split=(6000, 2000, 50000) ):
        """
        Load ICML07 Convex shapes dataset.

        Parameters
        ----------
        which_set : 'train', 'valid', 'test'
            Choose a dataset 
        one_hot : bool
            Encode labels one-hot
        split : (n_train, n_valid, n_test)
            Choose a split into train, validateion and test datasets
 
        Default split: 6000 training, 2000 validation and 50000 in test dataset.
        """
        super(Convex, self).__init__('convex', which_set, one_hot, split)



class Rectangles(ICML07DataSet):
    """
    ICML07: Discrimination between Tall and Wide Rectangles.

    All data values are binary, and the classification task is binary.
    """
    def __init__(self, which_set, one_hot=False, split=(1000,200,50000)):
        """
        Load ICML07 Rectangle dataset:

        Parameters
        ----------
        which_set : 'train', 'valid', 'test'
            Choose a dataset 
        one_hot : bool
            Encode labels one-hot
        split : (n_train, n_valid, n_test)
            Choose a split into train, validateion and test datasets
 
        Default split: 1000 training, 200 validation and 50000 in test dataset.
        """
        super(Rectangles, self).__init__('rectangles', which_set, one_hot, split)


class RectanglesImage(ICML07DataSet):
    """
    ICML07: Discrimination between tall and wide rectangles.

    The classification task is binary.
    """
    def __init__(self, which_set, one_hot=False, split=(10000, 2000, 50000)):
        """
        Load ICML07 Rectangles/images dataset:
 
        Parameters
        ----------
        which_set : 'train', 'valid', 'test'
            Choose a dataset 
        one_hot : bool
            Encode labels one-hot
        split : (n_train, n_valid, n_test)
            Choose a split into train, validateion and test datasets
        
        Default split: 10000 training, 2000 validation and 50000 in test dataset.
        """
        super(RectanglesImage, self).__init__('rectangles_images', which_set, one_hot, split)


########NEW FILE########
__FILENAME__ = iris
"""
.. todo::

    WRITEME
"""
__author__ = "Ian Goodfellow"

# TODO: add citation

import numpy as np

from pylearn2.datasets.dense_design_matrix import DenseDesignMatrix

class Iris(DenseDesignMatrix):
    """
    .. todo::

        WRITEME
    """

    def __init__(self, preprocessor=None):
        """
        .. todo::

            WRITEME
        """
        self.class_names = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']
        lines = iris_data.split('\n')
        X = []
        y = []
        for line in lines:
            row = line.split(',')
            X.append([float(elem) for elem in row[:-1]])
            y.append(self.class_names.index(row[-1]))
        X = np.array(X)

        assert X.shape == (150, 4)
        assert len(y) == 150

        assert min(y) == 0
        assert max(y) == 2

        one_hot = np.zeros((150,3))
        for i in xrange(len(y)):
            one_hot[i, y[i]] = 1


        super(Iris, self).__init__(X=X, y=one_hot, preprocessor=preprocessor)




iris_data = \
"""5.1,3.5,1.4,0.2,Iris-setosa
4.9,3.0,1.4,0.2,Iris-setosa
4.7,3.2,1.3,0.2,Iris-setosa
4.6,3.1,1.5,0.2,Iris-setosa
5.0,3.6,1.4,0.2,Iris-setosa
5.4,3.9,1.7,0.4,Iris-setosa
4.6,3.4,1.4,0.3,Iris-setosa
5.0,3.4,1.5,0.2,Iris-setosa
4.4,2.9,1.4,0.2,Iris-setosa
4.9,3.1,1.5,0.1,Iris-setosa
5.4,3.7,1.5,0.2,Iris-setosa
4.8,3.4,1.6,0.2,Iris-setosa
4.8,3.0,1.4,0.1,Iris-setosa
4.3,3.0,1.1,0.1,Iris-setosa
5.8,4.0,1.2,0.2,Iris-setosa
5.7,4.4,1.5,0.4,Iris-setosa
5.4,3.9,1.3,0.4,Iris-setosa
5.1,3.5,1.4,0.3,Iris-setosa
5.7,3.8,1.7,0.3,Iris-setosa
5.1,3.8,1.5,0.3,Iris-setosa
5.4,3.4,1.7,0.2,Iris-setosa
5.1,3.7,1.5,0.4,Iris-setosa
4.6,3.6,1.0,0.2,Iris-setosa
5.1,3.3,1.7,0.5,Iris-setosa
4.8,3.4,1.9,0.2,Iris-setosa
5.0,3.0,1.6,0.2,Iris-setosa
5.0,3.4,1.6,0.4,Iris-setosa
5.2,3.5,1.5,0.2,Iris-setosa
5.2,3.4,1.4,0.2,Iris-setosa
4.7,3.2,1.6,0.2,Iris-setosa
4.8,3.1,1.6,0.2,Iris-setosa
5.4,3.4,1.5,0.4,Iris-setosa
5.2,4.1,1.5,0.1,Iris-setosa
5.5,4.2,1.4,0.2,Iris-setosa
4.9,3.1,1.5,0.1,Iris-setosa
5.0,3.2,1.2,0.2,Iris-setosa
5.5,3.5,1.3,0.2,Iris-setosa
4.9,3.1,1.5,0.1,Iris-setosa
4.4,3.0,1.3,0.2,Iris-setosa
5.1,3.4,1.5,0.2,Iris-setosa
5.0,3.5,1.3,0.3,Iris-setosa
4.5,2.3,1.3,0.3,Iris-setosa
4.4,3.2,1.3,0.2,Iris-setosa
5.0,3.5,1.6,0.6,Iris-setosa
5.1,3.8,1.9,0.4,Iris-setosa
4.8,3.0,1.4,0.3,Iris-setosa
5.1,3.8,1.6,0.2,Iris-setosa
4.6,3.2,1.4,0.2,Iris-setosa
5.3,3.7,1.5,0.2,Iris-setosa
5.0,3.3,1.4,0.2,Iris-setosa
7.0,3.2,4.7,1.4,Iris-versicolor
6.4,3.2,4.5,1.5,Iris-versicolor
6.9,3.1,4.9,1.5,Iris-versicolor
5.5,2.3,4.0,1.3,Iris-versicolor
6.5,2.8,4.6,1.5,Iris-versicolor
5.7,2.8,4.5,1.3,Iris-versicolor
6.3,3.3,4.7,1.6,Iris-versicolor
4.9,2.4,3.3,1.0,Iris-versicolor
6.6,2.9,4.6,1.3,Iris-versicolor
5.2,2.7,3.9,1.4,Iris-versicolor
5.0,2.0,3.5,1.0,Iris-versicolor
5.9,3.0,4.2,1.5,Iris-versicolor
6.0,2.2,4.0,1.0,Iris-versicolor
6.1,2.9,4.7,1.4,Iris-versicolor
5.6,2.9,3.6,1.3,Iris-versicolor
6.7,3.1,4.4,1.4,Iris-versicolor
5.6,3.0,4.5,1.5,Iris-versicolor
5.8,2.7,4.1,1.0,Iris-versicolor
6.2,2.2,4.5,1.5,Iris-versicolor
5.6,2.5,3.9,1.1,Iris-versicolor
5.9,3.2,4.8,1.8,Iris-versicolor
6.1,2.8,4.0,1.3,Iris-versicolor
6.3,2.5,4.9,1.5,Iris-versicolor
6.1,2.8,4.7,1.2,Iris-versicolor
6.4,2.9,4.3,1.3,Iris-versicolor
6.6,3.0,4.4,1.4,Iris-versicolor
6.8,2.8,4.8,1.4,Iris-versicolor
6.7,3.0,5.0,1.7,Iris-versicolor
6.0,2.9,4.5,1.5,Iris-versicolor
5.7,2.6,3.5,1.0,Iris-versicolor
5.5,2.4,3.8,1.1,Iris-versicolor
5.5,2.4,3.7,1.0,Iris-versicolor
5.8,2.7,3.9,1.2,Iris-versicolor
6.0,2.7,5.1,1.6,Iris-versicolor
5.4,3.0,4.5,1.5,Iris-versicolor
6.0,3.4,4.5,1.6,Iris-versicolor
6.7,3.1,4.7,1.5,Iris-versicolor
6.3,2.3,4.4,1.3,Iris-versicolor
5.6,3.0,4.1,1.3,Iris-versicolor
5.5,2.5,4.0,1.3,Iris-versicolor
5.5,2.6,4.4,1.2,Iris-versicolor
6.1,3.0,4.6,1.4,Iris-versicolor
5.8,2.6,4.0,1.2,Iris-versicolor
5.0,2.3,3.3,1.0,Iris-versicolor
5.6,2.7,4.2,1.3,Iris-versicolor
5.7,3.0,4.2,1.2,Iris-versicolor
5.7,2.9,4.2,1.3,Iris-versicolor
6.2,2.9,4.3,1.3,Iris-versicolor
5.1,2.5,3.0,1.1,Iris-versicolor
5.7,2.8,4.1,1.3,Iris-versicolor
6.3,3.3,6.0,2.5,Iris-virginica
5.8,2.7,5.1,1.9,Iris-virginica
7.1,3.0,5.9,2.1,Iris-virginica
6.3,2.9,5.6,1.8,Iris-virginica
6.5,3.0,5.8,2.2,Iris-virginica
7.6,3.0,6.6,2.1,Iris-virginica
4.9,2.5,4.5,1.7,Iris-virginica
7.3,2.9,6.3,1.8,Iris-virginica
6.7,2.5,5.8,1.8,Iris-virginica
7.2,3.6,6.1,2.5,Iris-virginica
6.5,3.2,5.1,2.0,Iris-virginica
6.4,2.7,5.3,1.9,Iris-virginica
6.8,3.0,5.5,2.1,Iris-virginica
5.7,2.5,5.0,2.0,Iris-virginica
5.8,2.8,5.1,2.4,Iris-virginica
6.4,3.2,5.3,2.3,Iris-virginica
6.5,3.0,5.5,1.8,Iris-virginica
7.7,3.8,6.7,2.2,Iris-virginica
7.7,2.6,6.9,2.3,Iris-virginica
6.0,2.2,5.0,1.5,Iris-virginica
6.9,3.2,5.7,2.3,Iris-virginica
5.6,2.8,4.9,2.0,Iris-virginica
7.7,2.8,6.7,2.0,Iris-virginica
6.3,2.7,4.9,1.8,Iris-virginica
6.7,3.3,5.7,2.1,Iris-virginica
7.2,3.2,6.0,1.8,Iris-virginica
6.2,2.8,4.8,1.8,Iris-virginica
6.1,3.0,4.9,1.8,Iris-virginica
6.4,2.8,5.6,2.1,Iris-virginica
7.2,3.0,5.8,1.6,Iris-virginica
7.4,2.8,6.1,1.9,Iris-virginica
7.9,3.8,6.4,2.0,Iris-virginica
6.4,2.8,5.6,2.2,Iris-virginica
6.3,2.8,5.1,1.5,Iris-virginica
6.1,2.6,5.6,1.4,Iris-virginica
7.7,3.0,6.1,2.3,Iris-virginica
6.3,3.4,5.6,2.4,Iris-virginica
6.4,3.1,5.5,1.8,Iris-virginica
6.0,3.0,4.8,1.8,Iris-virginica
6.9,3.1,5.4,2.1,Iris-virginica
6.7,3.1,5.6,2.4,Iris-virginica
6.9,3.1,5.1,2.3,Iris-virginica
5.8,2.7,5.1,1.9,Iris-virginica
6.8,3.2,5.9,2.3,Iris-virginica
6.7,3.3,5.7,2.5,Iris-virginica
6.7,3.0,5.2,2.3,Iris-virginica
6.3,2.5,5.0,1.9,Iris-virginica
6.5,3.0,5.2,2.0,Iris-virginica
6.2,3.4,5.4,2.3,Iris-virginica
5.9,3.0,5.1,1.8,Iris-virginica"""

########NEW FILE########
__FILENAME__ = matlab_dataset
"""
.. todo::

    WRITEME
"""
import numpy as N
import warnings
try:
    from scipy import io
except ImportError:
    warnings.warn("Could not import scipy")
from pylearn2.datasets import dense_design_matrix
from theano import config


class MatlabDataset(dense_design_matrix.DenseDesignMatrix):
    """
    .. todo::

        WRITEME
    """
    def __init__(self, path, which_set):
        """
        .. todo::

            WRITEME
        """
        Xs = io.loadmat(path)
        X = Xs[which_set]
        super(MatlabDataset, self).__init__(X=N.cast[config.floatX](X))
        assert not N.any(N.isnan(self.X))

########NEW FILE########
__FILENAME__ = mnist
"""
.. todo::

    WRITEME
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

import numpy as N
import warnings
np = N
from pylearn2.datasets import dense_design_matrix
from pylearn2.datasets import control
from pylearn2.datasets import cache
from pylearn2.utils import serial
from pylearn2.utils.mnist_ubyte import read_mnist_images
from pylearn2.utils.mnist_ubyte import read_mnist_labels
from pylearn2.utils.rng import make_np_rng


class MNIST(dense_design_matrix.DenseDesignMatrix):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    which_set : WRITEME
    center : WRITEME
    shuffle : WRITEME
    one_hot : WRITEME
    binarize : WRITEME
    start : WRITEME
    stop : WRITEME
    axes : WRITEME
    preprocessor : WRITEME
    fit_preprocessor : WRITEME
    fit_test_preprocessor : WRITEME
    """

    def __init__(self, which_set, center=False, shuffle=False,
                 one_hot=None, binarize=False, start=None,
                 stop=None, axes=['b', 0, 1, 'c'],
                 preprocessor=None,
                 fit_preprocessor=False,
                 fit_test_preprocessor=False):
        self.args = locals()

        if which_set not in ['train', 'test']:
            if which_set == 'valid':
                raise ValueError(
                    "There is no such thing as the MNIST validation set. MNIST"
                    "consists of 60,000 train examples and 10,000 test"
                    "examples. If you wish to use a validation set you should"
                    "divide the train set yourself. The pylearn2 dataset"
                    "implements and will only ever implement the standard"
                    "train / test split used in the literature.")
            raise ValueError(
                'Unrecognized which_set value "%s".' % (which_set,) +
                '". Valid values are ["train","test"].')

        def dimshuffle(b01c):
            """
            .. todo::

                WRITEME
            """
            default = ('b', 0, 1, 'c')
            return b01c.transpose(*[default.index(axis) for axis in axes])

        if control.get_load_data():
            path = "${PYLEARN2_DATA_PATH}/mnist/"
            if which_set == 'train':
                im_path = path + 'train-images-idx3-ubyte'
                label_path = path + 'train-labels-idx1-ubyte'
            else:
                assert which_set == 'test'
                im_path = path + 't10k-images-idx3-ubyte'
                label_path = path + 't10k-labels-idx1-ubyte'
            # Path substitution done here in order to make the lower-level
            # mnist_ubyte.py as stand-alone as possible (for reuse in, e.g.,
            # the Deep Learning Tutorials, or in another package).
            im_path = serial.preprocess(im_path)
            label_path = serial.preprocess(label_path)

            # Locally cache the files before reading them
            datasetCache = cache.datasetCache
            im_path = datasetCache.cache_file(im_path)
            label_path = datasetCache.cache_file(label_path)

            topo_view = read_mnist_images(im_path, dtype='float32')
            y = np.atleast_2d(read_mnist_labels(label_path)).T
        else:
            if which_set == 'train':
                size = 60000
            elif which_set == 'test':
                size = 10000
            else:
                raise ValueError(
                    'Unrecognized which_set value "%s".' % (which_set,) +
                    '". Valid values are ["train","test"].')
            topo_view = np.random.rand(size, 28, 28)
            y = np.random.randint(0, 10, (size, 1))

        if binarize:
            topo_view = (topo_view > 0.5).astype('float32')

        max_labels = 10
        if one_hot is not None:
            warnings.warn("the `one_hot` parameter is deprecated. To get "
                          "one-hot encoded targets, request that they "
                          "live in `VectorSpace` through the `data_specs` "
                          "parameter of MNIST's iterator method. "
                          "`one_hot` will be removed on or after "
                          "September 20, 2014.", stacklevel=2)

        m, r, c = topo_view.shape
        assert r == 28
        assert c == 28
        topo_view = topo_view.reshape(m, r, c, 1)

        if which_set == 'train':
            assert m == 60000
        elif which_set == 'test':
            assert m == 10000
        else:
            assert False

        if center:
            topo_view -= topo_view.mean(axis=0)

        if shuffle:
            self.shuffle_rng = make_np_rng(None, [1, 2, 3], which_method="shuffle")
            for i in xrange(topo_view.shape[0]):
                j = self.shuffle_rng.randint(m)
                # Copy ensures that memory is not aliased.
                tmp = topo_view[i, :, :, :].copy()
                topo_view[i, :, :, :] = topo_view[j, :, :, :]
                topo_view[j, :, :, :] = tmp
                # Note: slicing with i:i+1 works for one_hot=True/False
                tmp = y[i:i+1].copy()
                y[i] = y[j]
                y[j] = tmp

        super(MNIST, self).__init__(topo_view=dimshuffle(topo_view), y=y,
                                    axes=axes, y_labels=max_labels)

        assert not N.any(N.isnan(self.X))

        if start is not None:
            assert start >= 0
            if stop > self.X.shape[0]:
                raise ValueError('stop=' + str(stop) + '>' +
                                 'm=' + str(self.X.shape[0]))
            assert stop > start
            self.X = self.X[start:stop, :]
            if self.X.shape[0] != stop - start:
                raise ValueError("X.shape[0]: %d. start: %d stop: %d"
                                 % (self.X.shape[0], start, stop))
            if len(self.y.shape) > 1:
                self.y = self.y[start:stop, :]
            else:
                self.y = self.y[start:stop]
            assert self.y.shape[0] == stop - start

        if which_set == 'test':
            assert fit_test_preprocessor is None or \
                (fit_preprocessor == fit_test_preprocessor)

        if self.X is not None and preprocessor:
            preprocessor.apply(self, fit_preprocessor)

    def adjust_for_viewer(self, X):
        """
        .. todo::

            WRITEME
        """
        return N.clip(X * 2. - 1., -1., 1.)

    def adjust_to_be_viewed_with(self, X, other, per_example=False):
        """
        .. todo::

            WRITEME
        """
        return self.adjust_for_viewer(X)

    def get_test_set(self):
        """
        .. todo::

            WRITEME
        """
        args = {}
        args.update(self.args)
        del args['self']
        args['which_set'] = 'test'
        args['start'] = None
        args['stop'] = None
        args['fit_preprocessor'] = args['fit_test_preprocessor']
        args['fit_test_preprocessor'] = None
        return MNIST(**args)


class MNIST_rotated_background(dense_design_matrix.DenseDesignMatrix):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    which_set : WRITEME
    center : WRITEME
    one_hot : WRITEME
    """

    def __init__(self, which_set, center=False, one_hot=False):
        path = "${PYLEARN2_DATA_PATH}/mnist/mnist_rotation_back_image/" \
            + which_set

        obj = serial.load(path)
        X = obj['data']
        X = N.cast['float32'](X)
        y = N.asarray(obj['labels'])

        self.one_hot = one_hot
        if one_hot:
            one_hot = N.zeros((y.shape[0], 10), dtype='float32')
            for i in xrange(y.shape[0]):
                one_hot[i, y[i]] = 1.
            y = one_hot

        if center:
            X -= X.mean(axis=0)

        view_converter = dense_design_matrix.DefaultViewConverter((28, 28, 1))

        super(MNIST_rotated_background, self).__init__(X=X, y=y, view_converter=view_converter)

        assert not N.any(N.isnan(self.X))

########NEW FILE########
__FILENAME__ = mnistplus
"""
.. todo::

    WRITEME
"""
import numpy as np
from theano import config
from pylearn2.datasets import dense_design_matrix
from pylearn2.utils.serial import load


class MNISTPlus(dense_design_matrix.DenseDesignMatrix):
    """
    Pylearn2 wrapper for the MNIST-Plus dataset.

    Parameters
    ----------
    which_set : str
        Dataset to load. One of ['train','valid','test'].
    label_type : str or None, optional
        String specifies which contents of dictionary are used as "labels"
    azimuth : bool, optional
        Load version where lighting is a factor of variation
    rotation : bool, optional
        Load version where MNIST digits are rotated
    texture : bool,optional
        Load version where MNIST is jointly embossed on a textured background.
    center : bool, optional
        If True, remove mean (across examples) for each pixel
    contrast_normalize : bool, optional
        If True, for each image, remove mean and divide by standard deviation.
    seed : int, optional
        WRITEME
    """

    idx = {'train': slice(0,50000),
           'valid': slice(50000,60000),
           'test':  slice(60000,70000)}

    def __init__(self, which_set, label_type=None,
                 azimuth=False, rotation=False, texture=False,
                 center = False, contrast_normalize=False, seed=132987):
        assert which_set in ['train','valid','test']
        assert label_type in [None,'label','azimuth','rotation','texture_id']

        # load data
        fname = '${PYLEARN2_DATA_PATH}/mnistplus/mnistplus'
        if azimuth:
            fname += '_azi'
        if rotation:
            fname += '_rot'
        if texture:
            fname += '_tex'

        data = load(fname + '.pkl')

        # get images and cast to floatX
        data_x = np.cast[config.floatX](data['data'])
        data_x = data_x[MNISTPlus.idx[which_set]]

        if contrast_normalize:
            meanx = np.mean(data_x, axis=1)[:,None]
            stdx  = np.std(data_x, axis=1)[:,None]
            data_x = (data_x - meanx) / stdx

        if center:
            data_x -= np.mean(data_x, axis=0)
 
        # get labels
        data_y = None
        if label_type is not None:

            data_y = data[label_type]
            
            # convert to float for performing regression
            if label_type in ['azimuth','rotation']:
                data_y = np.cast[config.floatX](data_y / 360.)

            # retrieve only subset of data
            data_y = data_y[MNISTPlus.idx[which_set]]

        # create view converting for retrieving topological view
        view_converter = dense_design_matrix.DefaultViewConverter((48, 48))

        # init the super class
        super(MNISTPlus, self).__init__(X = data_x, y = data_y, view_converter = view_converter)

        assert not np.any(np.isnan(self.X))


########NEW FILE########
__FILENAME__ = norb
"""
An interface to the small NORB dataset. Unlike `./norb_small.py`, this reads
the original NORB file format, not the LISA lab's `.npy` version.

Currently only supports the Small NORB Dataset.

Download the dataset from
`here <http://www.cs.nyu.edu/~ylclab/data/norb-v1.0-small/>`_.

NORB dataset(s) by Fu Jie Huang and Yann LeCun.
"""

__authors__ = "Guillaume Desjardins and Matthew Koichi Grimes"
__copyright__ = "Copyright 2010-2014, Universite de Montreal"
__credits__ = __authors__.split(" and ")
__license__ = "3-clause BSD"
__maintainer__ = "Matthew Koichi Grimes"
__email__ = "mkg alum mit edu (@..)"


import bz2
import gzip
import logging
import os
import warnings

import numpy
import theano


from pylearn2.datasets import dense_design_matrix
from pylearn2.datasets.cache import datasetCache
from pylearn2.space import VectorSpace, Conv2DSpace, CompositeSpace


logger = logging.getLogger(__name__)


class SmallNORB(dense_design_matrix.DenseDesignMatrix):
    """
    An interface to the small NORB dataset.

    If instantiated with default arguments, target labels are integers
    representing categories, which can be looked up using

      category_name = SmallNORB.get_category(label).

    If instantiated with multi_target=True, labels are vectors of indices
    representing:

      [ category, instance, elevation, azimuth, lighting ]

    Like with category, there are class methods that map these ints to their
    actual values, e.g:

      category = SmallNORB.get_category(label[0])
      elevation = SmallNORB.get_elevation_degrees(label[2])

    Parameters
    ----------
    which_set: str
        Must be 'train' or 'test'.
    multi_target: bool, optional
        If False, each label is an integer labeling the image catergory. If
        True, each label is a vector: [category, instance, lighting, elevation,
        azimuth]. All labels are given as integers. Use the categories,
        elevation_degrees, and azimuth_degrees arrays to map from these
        integers to actual values.
    """

    # Actual image shape may change, e.g. after being preprocessed by
    # datasets.preprocessing.Downsample
    original_image_shape = (96, 96)

    _categories = ['animal',  # four-legged animal
                   'human',  # human figure
                   'airplane',
                   'truck',
                   'car']

    @classmethod
    def get_category(cls, scalar_label):
        """
        Returns the category string corresponding to an integer category label.
        """
        return cls._categories[int(scalar_label)]

    @classmethod
    def get_elevation_degrees(cls, scalar_label):
        """
        Returns the elevation, in degrees, corresponding to an integer
        elevation label.
        """
        scalar_label = int(scalar_label)
        assert scalar_label >= 0
        assert scalar_label < 9
        return 30 + 5 * scalar_label

    @classmethod
    def get_azimuth_degrees(cls, scalar_label):
        """
        Returns the azimuth, in degrees, corresponding to an integer
        label.
        """
        scalar_label = int(scalar_label)
        assert scalar_label >= 0
        assert scalar_label <= 34
        assert (scalar_label % 2) == 0
        return scalar_label * 10

    # Maps azimuth labels (ints) to their actual values, in degrees.
    azimuth_degrees = numpy.arange(0, 341, 20)

    # Maps a label type to its index within a label vector.
    label_type_to_index = {'category': 0,
                           'instance': 1,
                           'elevation': 2,
                           'azimuth': 3,
                           'lighting': 4}

    # Number of labels, for each label type.
    num_labels_by_type = (len(_categories),
                          10,  # instances
                          9,   # elevations
                          18,  # azimuths
                          6)   # lighting

    # [mkg] Dropped support for the 'center' argument for now. In Pylearn 1, it
    # shifted the pixel values from [0:255] by subtracting 127.5. Seems like a
    # form of preprocessing, which might be better implemented separately using
    # the Preprocess class.
    def __init__(self, which_set, multi_target=False, stop=None):
        assert which_set in ['train', 'test']

        self.which_set = which_set

        subtensor = None
        if stop:
            subtensor = slice(0, stop)

        X = SmallNORB.load(which_set, 'dat', subtensor=subtensor)

        # Casts to the GPU-supported float type, using theano._asarray(), a
        # safer alternative to numpy.asarray().
        #
        # TODO: move the dtype-casting to the view_converter's output space,
        #       once dtypes-for-spaces is merged into master.
        X = theano._asarray(X, theano.config.floatX)

        # Formats data as rows in a matrix, for DenseDesignMatrix
        X = X.reshape(-1, 2*numpy.prod(self.original_image_shape))

        # This is uint8
        y = SmallNORB.load(which_set, 'cat', subtensor=subtensor)
        if multi_target:
            y_extra = SmallNORB.load(which_set, 'info')
            y = numpy.hstack((y[:, numpy.newaxis], y_extra))

        datum_shape = ((2, ) +  # two stereo images
                       self.original_image_shape +
                       (1, ))  # one color channel

        # 's' is the stereo channel: 0 (left) or 1 (right)
        axes = ('b', 's', 0, 1, 'c')
        view_converter = StereoViewConverter(datum_shape, axes)

        super(SmallNORB, self).__init__(X=X,
                                        y=y,
                                        view_converter=view_converter)

    @classmethod
    def load(cls, which_set, filetype, subtensor):
        """Reads and returns a single file as a numpy array."""

        assert which_set in ['train', 'test']
        assert filetype in ['dat', 'cat', 'info']

        def getPath(which_set):
            dirname = os.path.join(os.getenv('PYLEARN2_DATA_PATH'),
                                   'norb_small/original')
            if which_set == 'train':
                instance_list = '46789'
            elif which_set == 'test':
                instance_list = '01235'

            filename = 'smallnorb-5x%sx9x18x6x2x96x96-%s-%s.mat' % \
                (instance_list, which_set + 'ing', filetype)

            return os.path.join(dirname, filename)

        def parseNORBFile(file_handle, subtensor=None, debug=False):
            """
            Load all or part of file 'file_handle' into a numpy ndarray

            .. todo::

                WRITEME properly

            :param file_handle: file from which to read file can be opended
              with open(), gzip.open() and bz2.BZ2File()
              @type file_handle: file-like object. Can be a gzip open file.

            :param subtensor: If subtensor is not None, it should be like the
              argument to numpy.ndarray.__getitem__.  The following two
              expressions should return equivalent ndarray objects, but the one
              on the left may be faster and more memory efficient if the
              underlying file f is big.

              read(file_handle, subtensor) <===> read(file_handle)[*subtensor]

              Support for subtensors is currently spotty, so check the code to
              see if your particular type of subtensor is supported.
              """

            def readNums(file_handle, num_type, count):
                """
                Reads 4 bytes from file, returns it as a 32-bit integer.
                """
                num_bytes = count * numpy.dtype(num_type).itemsize
                string = file_handle.read(num_bytes)
                return numpy.fromstring(string, dtype=num_type)

            def readHeader(file_handle, debug=False, from_gzip=None):
                """
                .. todo::

                    WRITEME properly

                :param file_handle: an open file handle.
                :type file_handle: a file or gzip.GzipFile object

                :param from_gzip: bool or None
                :type from_gzip: if None determine the type of file handle.

                :returns: data type, element size, rank, shape, size
                """

                if from_gzip is None:
                    from_gzip = isinstance(file_handle,
                                           (gzip.GzipFile, bz2.BZ2File))

                key_to_type = {0x1E3D4C51: ('float32', 4),
                               # what is a packed matrix?
                               # 0x1E3D4C52: ('packed matrix', 0),
                               0x1E3D4C53: ('float64', 8),
                               0x1E3D4C54: ('int32', 4),
                               0x1E3D4C55: ('uint8', 1),
                               0x1E3D4C56: ('int16', 2)}

                type_key = readNums(file_handle, 'int32', 1)[0]
                elem_type, elem_size = key_to_type[type_key]
                if debug:
                    logger.debug("header's type key, type, type size: "
                                 "{0} {1} {2}".format(type_key, elem_type,
                                                      elem_size))
                if elem_type == 'packed matrix':
                    raise NotImplementedError('packed matrix not supported')

                num_dims = readNums(file_handle, 'int32', 1)[0]
                if debug:
                    logger.debug('# of dimensions, according to header: '
                                 '{0}'.format(num_dims))

                if from_gzip:
                    shape = readNums(file_handle,
                                     'int32',
                                     max(num_dims, 3))[:num_dims]
                else:
                    shape = numpy.fromfile(file_handle,
                                           dtype='int32',
                                           count=max(num_dims, 3))[:num_dims]

                if debug:
                    logger.debug('Tensor shape, as listed in header: '
                                 '{0}'.format(shape))

                return elem_type, elem_size, shape

            elem_type, elem_size, shape = readHeader(file_handle, debug)
            beginning = file_handle.tell()

            num_elems = numpy.prod(shape)

            result = None
            if isinstance(file_handle, (gzip.GzipFile, bz2.BZ2File)):
                assert subtensor is None, \
                    "Subtensors on gzip files are not implemented."
                result = readNums(file_handle,
                                  elem_type,
                                  num_elems*elem_size).reshape(shape)
            elif subtensor is None:
                result = numpy.fromfile(file_handle,
                                        dtype=elem_type,
                                        count=num_elems).reshape(shape)
            elif isinstance(subtensor, slice):
                if subtensor.step not in (None, 1):
                    raise NotImplementedError('slice with step',
                                              subtensor.step)
                if subtensor.start not in (None, 0):
                    bytes_per_row = numpy.prod(shape[1:]) * elem_size
                    file_handle.seek(beginning+subtensor.start * bytes_per_row)
                shape[0] = min(shape[0], subtensor.stop) - subtensor.start
                num_elems = numpy.prod(shape)
                result = numpy.fromfile(file_handle,
                                        dtype=elem_type,
                                        count=num_elems).reshape(shape)
            else:
                raise NotImplementedError('subtensor access not written yet:',
                                          subtensor)

            return result
        fname = getPath(which_set)
        fname = datasetCache.cache_file(fname)
        file_handle = open(fname)

        return parseNORBFile(file_handle, subtensor)

    def get_topological_view(self, mat=None, single_tensor=True):
        """
        .. todo::

            WRITEME
        """
        result = super(SmallNORB, self).get_topological_view(mat)

        if single_tensor:
            warnings.warn("The single_tensor argument is True by default to "
                          "maintain backwards compatibility. This argument "
                          "will be removed, and the behavior will become that "
                          "of single_tensor=False, as of August 2014.")
            axes = list(self.view_converter.axes)
            s_index = axes.index('s')
            assert axes.index('b') == 0
            num_image_pairs = result[0].shape[0]
            shape = (num_image_pairs, ) + self.view_converter.shape

            # inserts a singleton dimension where the 's' dimesion will be
            mono_shape = shape[:s_index] + (1, ) + shape[(s_index+1):]

            for i, res in enumerate(result):
                logger.info("result {0} shape: {1}".format(i, str(res.shape)))

            result = tuple(t.reshape(mono_shape) for t in result)
            result = numpy.concatenate(result, axis=s_index)
        else:
            warnings.warn("The single_tensor argument will be removed on "
                          "August 2014. The behavior will be the same as "
                          "single_tensor=False.")

        return result


class StereoViewConverter(object):
    """
    Converts stereo image data between two formats:

    #. A dense design matrix, one stereo pair per row (`VectorSpace`)
    #. An image pair (`CompositeSpace` of two `Conv2DSpace`)

    The arguments describe how the data is laid out in the design matrix.

    Parameters
    ----------
    shape: tuple
        A tuple of 4 ints, describing the shape of each datum. This is the size
        of each axis in `<axes>`, excluding the `b` axis.
    axes : tuple
        Tuple of the following elements in any order:

        * 'b' : batch axis
        * 's' : stereo axis
        *  0  : image axis 0 (row)
        *  1  : image axis 1 (column)
        * 'c' : channel axis
    """
    def __init__(self, shape, axes=None):
        shape = tuple(shape)

        if not all(isinstance(s, int) for s in shape):
            raise TypeError("Shape must be a tuple/list of ints")

        if len(shape) != 4:
            raise ValueError("Shape array needs to be of length 4, got %s." %
                             shape)

        datum_axes = list(axes)
        datum_axes.remove('b')
        if shape[datum_axes.index('s')] != 2:
            raise ValueError("Expected 's' axis to have size 2, got %d.\n"
                             "  axes:       %s\n"
                             "  shape:      %s" %
                             (shape[datum_axes.index('s')],
                              axes,
                              shape))
        self.shape = shape
        self.set_axes(axes)

        def make_conv2d_space(shape, axes):
            shape_axes = list(axes)
            shape_axes.remove('b')
            image_shape = tuple(shape[shape_axes.index(axis)]
                                for axis in (0, 1))
            conv2d_axes = list(axes)
            conv2d_axes.remove('s')
            return Conv2DSpace(shape=image_shape,
                               num_channels=shape[shape_axes.index('c')],
                               axes=conv2d_axes)

        conv2d_space = make_conv2d_space(shape, axes)
        self.topo_space = CompositeSpace((conv2d_space, conv2d_space))
        self.storage_space = VectorSpace(dim=numpy.prod(shape))

    def get_formatted_batch(self, batch, space):
        """
        .. todo::

            WRITEME
        """
        return self.storage_space.np_format_as(batch, space)

    def design_mat_to_topo_view(self, design_mat):
        """
        Called by DenseDesignMatrix.get_formatted_view(), get_batch_topo()
        """
        return self.storage_space.np_format_as(design_mat, self.topo_space)

    def design_mat_to_weights_view(self, design_mat):
        """
        Called by DenseDesignMatrix.get_weights_view()
        """
        return self.design_mat_to_topo_view(design_mat)

    def topo_view_to_design_mat(self, topo_batch):
        """
        Used by `DenseDesignMatrix.set_topological_view()` and
        `DenseDesignMatrix.get_design_mat()`.
        """
        return self.topo_space.np_format_as(topo_batch, self.storage_space)

    def view_shape(self):
        """
        .. todo::

            WRITEME
        """
        return self.shape

    def weights_view_shape(self):
        """
        .. todo::

            WRITEME
        """
        return self.view_shape()

    def set_axes(self, axes):
        """
        .. todo::

            WRITEME
        """
        axes = tuple(axes)

        if len(axes) != 5:
            raise ValueError("Axes must have 5 elements; got %s" % str(axes))

        for required_axis in ('b', 's', 0, 1, 'c'):
            if required_axis not in axes:
                raise ValueError("Axes must contain 'b', 's', 0, 1, and 'c'. "
                                 "Got %s." % str(axes))

        if axes.index('b') != 0:
            raise ValueError("The 'b' axis must come first (axes = %s)." %
                             str(axes))

        def get_batchless_axes(axes):
            axes = list(axes)
            axes.remove('b')
            return tuple(axes)

        if hasattr(self, 'axes'):
            # Reorders the shape vector to match the new axis ordering.
            assert hasattr(self, 'shape')
            old_axes = get_batchless_axes(self.axes)
            new_axes = get_batchless_axes(axes)
            new_shape = tuple(self.shape[old_axes.index(a)] for a in new_axes)
            self.shape = new_shape

        self.axes = axes

########NEW FILE########
__FILENAME__ = norb_small
"""
.. todo::

    WRITEME
"""
import numpy
np = numpy
import os

from pylearn2.datasets import dense_design_matrix
from pylearn2.datasets import retina
from pylearn2.datasets.cache import datasetCache


class NORBSmall(dense_design_matrix.DenseDesignMatrix):
    """
    A pylearn2 dataset object for the small NORB dataset (v1.0).

    Parameters
    ----------
    which_set : WRITEME
        one of ['train','test']
    center : WRITEME
        data is in range [0,256], center=True subtracts 127.5.
    multi_target : WRITEME
        load extra information as additional labels.
    """

    @classmethod
    def load(cls, which_set, desc):
        """
        .. todo::

            WRITEME
        """
        assert desc in ['dat', 'cat', 'info']

        base = '%s/norb_small/original_npy/smallnorb-'
        base = base % os.getenv('PYLEARN2_DATA_PATH')
        if which_set == 'train':
            base += '5x46789x9x18x6x2x96x96-training'
        else:
            base += '5x01235x9x18x6x2x96x96-testing'

        fname = base + '-%s.npy' % desc
        fname = datasetCache.cache_file(fname)
        fp = open(fname, 'r')
        data = numpy.load(fp)
        fp.close()

        return data

    def __init__(self, which_set, center=False, multi_target=False):
        assert which_set in ['train', 'test']

        X = NORBSmall.load(which_set, 'dat')

        # put things in pylearn2's DenseDesignMatrix format
        X = numpy.cast['float32'](X)
        X = X.reshape(-1, 2*96*96)

        # this is uint8
        y = NORBSmall.load(which_set, 'cat')
        if multi_target:
            y_extra = NORBSmall.load(which_set, 'info')
            y = numpy.hstack((y[:, numpy.newaxis], y_extra))

        if center:
            X -= 127.5

        view_converter = dense_design_matrix.DefaultViewConverter((96, 96, 2))

        super(NORBSmall, self).__init__(X=X, y=y,
                                        view_converter=view_converter)


class FoveatedNORB(dense_design_matrix.DenseDesignMatrix):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    which_set : WRITEME
        One of ['train','test']
    center : WRITEME
        Data is in range [0,256], center=True subtracts 127.5.
        # TODO: check this comment, sure it means {0, ..., 255}
    scale : WRITEME
    start : WRITEME
    stop : WRITEME
    one_hot : WRITEME
    restrict_instances : WRITEME
    preprocessor : WRITEME
    """

    @classmethod
    def load(cls, which_set):

        base = '%s/norb_small/foveated/smallnorb-'
        base = base % os.getenv('PYLEARN2_DATA_PATH')
        if which_set == 'train':
            base += '5x46789x9x18x6x2x96x96-training-dat'
        else:
            base += '5x01235x9x18x6x2x96x96-testing-dat'

        fname = base + '.npy'
        fname = datasetCache.cache_file(fname)
        data = numpy.load(fname, 'r')
        return data

    def __init__(self, which_set, center=False, scale=False,
                 start=None, stop=None, one_hot=False, restrict_instances=None,
                 preprocessor=None):

        self.args = locals()

        if which_set not in ['train', 'test']:
            raise ValueError("Unrecognized which_set value: " + which_set)

        X = FoveatedNORB.load(which_set)
        X = numpy.cast['float32'](X)

        # this is uint8
        y = NORBSmall.load(which_set, 'cat')
        y_extra = NORBSmall.load(which_set, 'info')

        assert y_extra.shape[0] == y.shape[0]
        instance = y_extra[:, 0]
        assert instance.min() >= 0
        assert instance.max() <= 9
        self.instance = instance

        if center:
            X -= 127.5
            if scale:
                X /= 127.5
        else:
            if scale:
                X /= 255.

        view_converter = retina.RetinaCodingViewConverter((96, 96, 2),
                                                          (8, 4, 2, 2))

        super(FoveatedNORB, self).__init__(X=X, y=y,
                                           view_converter=view_converter,
                                           preprocessor=preprocessor)

        if one_hot:
            self.convert_to_one_hot()

        if restrict_instances is not None:
            assert start is None
            assert stop is None
            self.restrict_instances(restrict_instances)

        self.restrict(start, stop)

        self.y = self.y.astype('float32')

    def get_test_set(self):
        """
        .. todo::

            WRITEME
        """
        test_args = {'which_set': 'test'}

        for key in self.args:
            if key in ['which_set', 'restrict_instances',
                       'self', 'start', 'stop']:
                continue
            test_args[key] = self.args[key]

        return FoveatedNORB(**test_args)

    def restrict_instances(self, instances):
        """
        .. todo::

            WRITEME
        """
        mask = reduce(np.maximum, [self.instance == ins for ins in instances])
        mask = mask.astype('bool')
        self.instance = self.instance[mask]
        self.X = self.X[mask, :]
        if self.y.ndim == 2:
            self.y = self.y[mask, :]
        else:
            self.y = self.y[mask]
        assert self.X.shape[0] == self.y.shape[0]
        expected = sum([(self.instance == ins).sum() for ins in instances])
        assert self.X.shape[0] == expected

########NEW FILE########
__FILENAME__ = npy_npz
"""Objects for datasets serialized in the NumPy native format (.npy/.npz)."""
import functools
import numpy
from theano import config
from pylearn2.datasets.dense_design_matrix import DenseDesignMatrix

class NpyDataset(DenseDesignMatrix):
    """A dense dataset based on a single array stored as a .npy file."""
    def __init__(self, file, mmap_mode=None):
        """
        Creates an NpyDataset object.

        Parameters
        ----------
        file : file-like object or str
            A file-like object or string indicating a filename. Passed
            directly to `numpy.load`.
        mmap_mode : str, optional
            Memory mapping options for memory-mapping an array on disk,
            rather than loading it into memory. See the `numpy.load`
            docstring for details.
        """
        self._path = file
        self._loaded = False

    def _deferred_load(self):
        """
        .. todo::

            WRITEME
        """
        self._loaded = True
        loaded = numpy.load(self._path)
        assert isinstance(loaded, numpy.ndarray), (
            "single arrays (.npy) only"
        )
        if len(loaded.shape) == 2:
            super(NpyDataset, self).__init__(X=loaded)
        else:
            super(NpyDataset, self).__init__(topo_view=loaded)

    @functools.wraps(DenseDesignMatrix.get_design_matrix)
    def get_design_matrix(self, topo=None):
        if not self._loaded:
            self._deferred_load()
        return super(NpyDataset, self).get_design_matrix(topo)

    @functools.wraps(DenseDesignMatrix.get_topological_view)
    def get_topological_view(self, mat=None):
        if not self._loaded:
            self._deferred_load()
        return super(NpyDataset, self).get_topological_view(mat)

    @functools.wraps(DenseDesignMatrix.iterator)
    def iterator(self, *args, **kwargs):
        # TODO: Factor this out of iterator() and into something that
        # can be called by multiple methods. Maybe self.prepare().
        if not self._loaded:
            self._deferred_load()
        return super(NpyDataset, self).iterator(*args, **kwargs)


class NpzDataset(DenseDesignMatrix):
    """A dense dataset based on a .npz archive."""
    def __init__(self, file, key, target_key=None):
        """
        Creates an NpzDataset object.

        Parameters
        ----------
        file : file-like object or str
            A file-like object or string indicating a filename. Passed
            directly to `numpy.load`.
        key : str
            A string indicating which key name to use to pull out the
            input data.
        target_key : str, optional
            A string indicating which key name to use to pull out the
            output data.
        """
        loaded = numpy.load(file)
        assert not isinstance(loaded, numpy.ndarray), (
            "zipped groups of arrays (.npz) only"
        )
        assert key in loaded, "%s not found in loaded NPZFile" % key
        
        if target_key != None:
            assert target_key in loaded, "%s not found in loaded NPZFile" % target_key
            y = loaded[target_key]
        else:
            y = None
        
        if len(loaded[key].shape) == 2:
            super(NpzDataset, self).__init__(X=loaded[key], y=y)
        else:
            super(NpzDataset, self).__init__(topo_view=loaded[key], y=y)

        loaded.close ()

########NEW FILE########
__FILENAME__ = ocr
"""
.. todo::

    WRITEME
"""
__authors__ = "Mehdi Mirza"
__copyright__ = "Copyright 2010-2013, Universite de Montreal"
__credits__ = ["Mehdi Mirza"]
__license__ = "3-clause BSD"
__maintainer__ = "Mehdi Mirza"
__email__ = "mirzamom@iro"

import numpy
from pylearn2.datasets import dense_design_matrix
from pylearn2.utils import serial

class OCR(dense_design_matrix.DenseDesignMatrix):
    """
    OCR dataset

    http://ai.stanford.edu/~btaskar/ocr/

    NOTE:
        Split is based on, but it's unclear if it's first shuffled or not.
        An Efficient Learning Procedure for Deep Boltzmann Machines
        Ruslan Salakhutdinov and Geoffrey Hinton
        Neural Computation, August 2012
    """

    data_split = {"train" : 32152, "valid" : 10000, "test" : 10000 }

    def __init__(self, which_set, one_hot = False, axes=['b', 0, 1, 'c']):
        """
        .. todo::

            WRITEME
        """
        self.args = locals()

        assert which_set in self.data_split.keys()

        path = serial.preprocess("${PYLEARN2_DATA_PATH}/ocr_letters/letter.data")
        with open(path, 'r') as data_f:
            data = data_f.readlines()
            data = [line.split("\t") for line in data]

        data_x = [map(int, item[6:-1]) for item in data]
        data_letters = [item[1] for item in data]
        data_fold = [int(item[5]) for item in data]

        letters = list(numpy.unique(data_letters))
        data_y = [letters.index(item) for item in data_letters]

        if which_set == 'train':
            split = slice(0, self.data_split['train'])
        elif which_set == 'valid':
            split = slice(self.data_split['train'], self.data_split['train'] + \
                    self.data_split['valid'])
        elif which_set == 'test':
            split = slice(self.data_split['train'] + self.data_split['valid'], \
                    self.data_split['train'] + self.data_split['valid'] + self.data_split['test'])

        data_x = numpy.asarray(data_x[split])
        data_y = numpy.asarray(data_y[split])
        data_fold = numpy.asarray(data_y[split])
        assert data_x.shape[0] == data_y.shape[0]
        assert data_x.shape[0] == self.data_split[which_set]

        self.one_hot = one_hot
        if one_hot:
            one_hot = numpy.zeros((data_y.shape[0], len(letters)), dtype = 'float32')
            for i in xrange(data_y.shape[0]):
                one_hot[i, data_y[i]] = 1.
            data_y = one_hot

        view_converter = dense_design_matrix.DefaultViewConverter((16, 8, 1), axes)
        super(OCR, self).__init__(X = data_x, y = data_y, view_converter = view_converter)

        assert not numpy.any(numpy.isnan(self.X))
        self.fold = data_fold


    def get_test_set(self):
        """
        .. todo::

            WRITEME
        """
        return OCR('test', one_hot = self.one_hot)


########NEW FILE########
__FILENAME__ = preprocessing
"""
Functionality for preprocessing Datasets.
"""

__authors__ = "Ian Goodfellow, David Warde-Farley, Guillaume Desjardins, " \
              "and Mehdi Mirza"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow", "David Warde-Farley", "Guillaume Desjardins",
               "Mehdi Mirza"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"


import copy, logging, time, warnings, os, numpy, scipy
try:
    from scipy import linalg
except ImportError:
    warnings.warn("Could not import scipy.linalg")
import theano
from theano import function, tensor

from pylearn2.blocks import Block
from pylearn2.linear.conv2d import Conv2D
from pylearn2.space import Conv2DSpace, VectorSpace
from pylearn2.expr.preprocessing import global_contrast_normalize
from pylearn2.utils.insert_along_axis import insert_columns
from pylearn2.utils import sharedX
from pylearn2.utils.rng import make_np_rng


log = logging.getLogger(__name__)

convert_axes = Conv2DSpace.convert_numpy


class Preprocessor(object):
    """
        Abstract class.

        An object that can preprocess a dataset.

        Preprocessing a dataset implies changing the data that
        a dataset actually stores. This can be useful to save
        memory--if you know you are always going to access only
        the same processed version of the dataset, it is better
        to process it once and discard the original.

        Preprocessors are capable of modifying many aspects of
        a dataset. For example, they can change the way that it
        converts between different formats of data. They can
        change the number of examples that a dataset stores.
        In other words, preprocessors can do a lot more than
        just example-wise transformations of the examples stored
        in the dataset.
    """

    def apply(self, dataset, can_fit=False):
        """
        .. todo::

            WRITEME

        Parameters
        ----------
        dataset : Dataset
            The dataset to act on.
        can_fit : bool
            If True, the Preprocessor can adapt internal parameters
            based on the contents of dataset. Otherwise it must not
            fit any parameters, or must re-use old ones.
            Subclasses should still have this default to False, so
            that the behavior of the preprocessors is uniform.

        Notes
        -----
        Typical usage:

        .. code-block::  python

            # Learn PCA preprocessing and apply it to the training set
            my_pca_preprocessor.apply(training_set, can_fit = True)
            # Now apply the same transformation to the test set
            my_pca_preprocessor.apply(test_set, can_fit = False)

        This method must take a dataset, rather than a numpy ndarray, for a
        variety of reasons:

        - Preprocessors should work on any dataset, and not all
          datasets will store their data as ndarrays.
        - Preprocessors often need to change a dataset's
          metadata.  For example, suppose you have a
          DenseDesignMatrix dataset of images. If you implement
          a fovea Preprocessor that reduces the dimensionality
          of images by sampling them finely near the center and
          coarsely with blurring at the edges, then your
          preprocessor will need to change the way that the
          dataset converts example vectors to images for
          visualization.
        """

        raise NotImplementedError(str(type(self)) +
                                  " does not implement an apply method.")

    def invert(self):
        """
        Do any necessary prep work to be able to support the "inverse" method
        later. Default implementation is no-op.
        """
        pass


class ExamplewisePreprocessor(Preprocessor):
    """
    Abstract class.

    A Preprocessor that restricts the actions it can do in its
    apply method so that it could be implemented as a Block's
    perform method.

    In other words, this Preprocessor can't modify the Dataset's
    metadata, etc.

    TODO: can these things fit themselves in their apply method?
    That seems like a difference from Block.
    """

    def as_block(self):
        raise NotImplementedError(str(type(self)) +
                                  " does not implement as_block.")


class BlockPreprocessor(ExamplewisePreprocessor):
    """
    An ExamplewisePreprocessor implemented by a Block.

    Parameters
    ----------
    block : WRITEME
    """

    def __init__(self, block):
        self.block = block

    def apply(self, dataset, can_fit=False):
        """
        .. todo::

            WRITEME
        """
        assert not can_fit
        dataset.X = self.block.perform(dataset.X)


class Pipeline(Preprocessor):
    """
    A Preprocessor that sequentially applies a list
    of other Preprocessors.

    Parameters
    ----------
    items : WRITEME
    """

    def __init__(self, items=None):
        self.items = items if items is not None else []

    def apply(self, dataset, can_fit=False):
        """
        .. todo::

            WRITEME
        """
        for item in self.items:
            item.apply(dataset, can_fit)


class ExtractGridPatches(Preprocessor):
    """
    Converts a dataset of images into a dataset of patches extracted along a
    regular grid from each image.  The order of the images is
    preserved.

    Parameters
    ----------
    patch_shape : WRITEME
    patch_stride : WRITEME
    """

    def __init__(self, patch_shape, patch_stride):
        self.patch_shape = patch_shape
        self.patch_stride = patch_stride

    def apply(self, dataset, can_fit=False):
        """
        .. todo::

            WRITEME
        """
        X = dataset.get_topological_view()
        num_topological_dimensions = len(X.shape) - 2
        if num_topological_dimensions != len(self.patch_shape):
            raise ValueError("ExtractGridPatches with "
                             + str(len(self.patch_shape))
                             + " topological dimensions called on"
                             + " dataset with " +
                             str(num_topological_dimensions) + ".")
        num_patches = X.shape[0]
        max_strides = [X.shape[0] - 1]
        for i in xrange(num_topological_dimensions):
            patch_width = self.patch_shape[i]
            data_width = X.shape[i + 1]
            last_valid_coord = data_width - patch_width
            if last_valid_coord < 0:
                raise ValueError('On topological dimension ' + str(i) +
                                 ', the data has width ' + str(data_width) +
                                 ' but the requested patch width is ' +
                                 str(patch_width))
            stride = self.patch_stride[i]
            if stride == 0:
                max_stride_this_axis = 0
            else:
                max_stride_this_axis = last_valid_coord / stride
            num_strides_this_axis = max_stride_this_axis + 1
            max_strides.append(max_stride_this_axis)
            num_patches *= num_strides_this_axis
        # batch size
        output_shape = [num_patches]
        # topological dimensions
        for dim in self.patch_shape:
            output_shape.append(dim)
        # number of channels
        output_shape.append(X.shape[-1])
        output = numpy.zeros(output_shape, dtype=X.dtype)
        channel_slice = slice(0, X.shape[-1])
        coords = [0] * (num_topological_dimensions + 1)
        keep_going = True
        i = 0
        while keep_going:
            args = [coords[0]]
            for j in xrange(num_topological_dimensions):
                coord = coords[j + 1] * self.patch_stride[j]
                args.append(slice(coord, coord + self.patch_shape[j]))
            args.append(channel_slice)
            patch = X[args]
            output[i, :] = patch
            i += 1
            # increment coordinates
            j = 0
            keep_going = False
            while not keep_going:
                if coords[-(j + 1)] < max_strides[-(j + 1)]:
                    coords[-(j + 1)] += 1
                    keep_going = True
                else:
                    coords[-(j + 1)] = 0
                    if j == num_topological_dimensions:
                        break
                    j = j + 1
        dataset.set_topological_view(output)

        # fix lables
        if dataset.y is not None:
            dataset.y = numpy.repeat(dataset.y, num_patches / X.shape[0])


class ReassembleGridPatches(Preprocessor):
    """
    Converts a dataset of patches into a dataset of full examples.

    This is the inverse of ExtractGridPatches for patch_stride=patch_shape.

    Parameters
    ----------
    orig_shape : WRITEME
    patch_shape : WRITEME
    """

    def __init__(self, orig_shape, patch_shape):
        self.patch_shape = patch_shape
        self.orig_shape = orig_shape

    def apply(self, dataset, can_fit=False):
        """
        .. todo::

            WRITEME
        """
        patches = dataset.get_topological_view()

        num_topological_dimensions = len(patches.shape) - 2

        if num_topological_dimensions != len(self.patch_shape):
            raise ValueError("ReassembleGridPatches with " +
                             str(len(self.patch_shape)) +
                             " topological dimensions called on dataset " +
                             " with " +
                             str(num_topological_dimensions) + ".")
        num_patches = patches.shape[0]
        num_examples = num_patches
        for im_dim, patch_dim in zip(self.orig_shape, self.patch_shape):
            if im_dim % patch_dim != 0:
                raise Exception('Trying to assemble patches of shape ' +
                                str(self.patch_shape) + ' into images of ' +
                                'shape ' + str(self.orig_shape))
            patches_this_dim = im_dim / patch_dim
            if num_examples % patches_this_dim != 0:
                raise Exception('Trying to re-assemble ' + str(num_patches) +
                                ' patches of shape ' + str(self.patch_shape) +
                                ' into images of shape ' + str(self.orig_shape)
                               )
            num_examples /= patches_this_dim

        # batch size
        reassembled_shape = [num_examples]
        # topological dimensions
        for dim in self.orig_shape:
            reassembled_shape.append(dim)
        # number of channels
        reassembled_shape.append(patches.shape[-1])
        reassembled = numpy.zeros(reassembled_shape, dtype=patches.dtype)
        channel_slice = slice(0, patches.shape[-1])
        coords = [0] * (num_topological_dimensions + 1)
        max_strides = [num_examples - 1]
        for dim, pd in zip(self.orig_shape, self.patch_shape):
            assert dim % pd == 0
            max_strides.append(dim / pd - 1)
        keep_going = True
        i = 0
        while keep_going:
            args = [coords[0]]
            for j in xrange(num_topological_dimensions):
                coord = coords[j + 1]
                args.append(slice(coord * self.patch_shape[j],
                                  (coord + 1) * self.patch_shape[j]))
                next_shape_coord = reassembled.shape[j + 1]
                assert (coord + 1) * self.patch_shape[j] <= next_shape_coord

            args.append(channel_slice)

            try:
                patch = patches[i, :]
            except IndexError:
                raise IndexError('Gave index of ' + str(i) +
                                 ', : into thing of shape ' +
                                 str(patches.shape))
            reassembled[args] = patch
            i += 1
            j = 0
            keep_going = False
            while not keep_going:
                if coords[-(j + 1)] < max_strides[-(j + 1)]:
                    coords[-(j + 1)] += 1
                    keep_going = True
                else:
                    coords[-(j + 1)] = 0
                    if j == num_topological_dimensions:
                        break
                    j = j + 1

        dataset.set_topological_view(reassembled)

        # fix labels
        if dataset.y is not None:
            dataset.y = dataset.y[::patches.shape[0] / reassembled_shape[0]]


class ExtractPatches(Preprocessor):
    """
    Converts an image dataset into a dataset of patches
    extracted at random from the original dataset.

    Parameters
    ----------
    patch_shape : WRITEME
    num_patches : WRITEME
    rng : WRITEME
    """

    def __init__(self, patch_shape, num_patches, rng=None):
        self.patch_shape = patch_shape
        self.num_patches = num_patches

        self.start_rng = make_np_rng(copy.copy(rng), [1,2,3], which_method="randint")

    def apply(self, dataset, can_fit=False):
        """
        .. todo::

            WRITEME
        """
        rng = copy.copy(self.start_rng)

        X = dataset.get_topological_view()

        num_topological_dimensions = len(X.shape) - 2

        if num_topological_dimensions != len(self.patch_shape):
            raise ValueError("ExtractPatches with "
                             + str(len(self.patch_shape))
                             + " topological dimensions called on "
                             + "dataset with "
                             + str(num_topological_dimensions) + ".")

        # batch size
        output_shape = [self.num_patches]
        # topological dimensions
        for dim in self.patch_shape:
            output_shape.append(dim)
        # number of channels
        output_shape.append(X.shape[-1])
        output = numpy.zeros(output_shape, dtype=X.dtype)
        channel_slice = slice(0, X.shape[-1])
        for i in xrange(self.num_patches):
            args = []
            args.append(rng.randint(X.shape[0]))

            for j in xrange(num_topological_dimensions):
                max_coord = X.shape[j + 1] - self.patch_shape[j]
                coord = rng.randint(max_coord + 1)
                args.append(slice(coord, coord + self.patch_shape[j]))
            args.append(channel_slice)
            output[i, :] = X[args]
        dataset.set_topological_view(output)
        dataset.y = None


class ExamplewiseUnitNormBlock(Block):
    """
    A block that takes n-tensors, with training examples indexed along
    the first axis, and normalizes each example to lie on the unit
    sphere.

    Parameters
    ----------
    input_space : WRITEME
    """

    def __init__(self, input_space=None):
        super(ExamplewiseUnitNormBlock, self).__init__()
        self.input_space = input_space

    def __call__(self, batch):
        """
        .. todo::

            WRITEME
        """
        if self.input_space:
            self.input_space.validate(batch)
        squared_batch = batch ** 2
        squared_norm = squared_batch.sum(axis=1)
        norm = tensor.sqrt(squared_norm)
        return batch / norm

    def set_input_space(self, space):
        """
        .. todo::

            WRITEME
        """
        self.input_space = space

    def get_input_space(self):
        """
        .. todo::

            WRITEME
        """
        if self.input_space is not None:
            return self.input_space
        raise ValueError("No input space was specified for this Block (%s). "
                         "You can call set_input_space to correct that." %
                         str(self))

    def get_output_space(self):
        """
        .. todo::

            WRITEME
        """
        return self.get_input_space()


class MakeUnitNorm(ExamplewisePreprocessor):
    """
    .. todo::

        WRITEME
    """

    def apply(self, dataset, can_fit=False):
        """
        .. todo::

            WRITEME
        """
        X = dataset.get_design_matrix()
        X_norm = numpy.sqrt(numpy.sum(X ** 2, axis=1))
        X /= X_norm[:, None]
        dataset.set_design_matrix(X)

    def as_block(self):
        """
        .. todo::

            WRITEME
        """
        return ExamplewiseUnitNormBlock()


class ExamplewiseAddScaleTransform(Block):
    """
    A block that encodes an per-feature addition/scaling transform.
    The addition/scaling can be done in either order.

    Parameters
    ----------
    add : array_like or scalar, optional
        Array or array-like object or scalar, to be added to each
        training example by this Block.
    multiply : array_like, optional
        Array or array-like object or scalar, to be element-wise
        multiplied with each training example by this Block.
    multiply_first : bool, optional
        Whether to perform the multiplication before the addition.
        (default is False).
    input_space : Space, optional
        The input space describing the data
    """

    def __init__(self, add=None, multiply=None, multiply_first=False,
                 input_space=None):
        self._add = numpy.asarray(add)
        self._multiply = numpy.asarray(multiply)
        # TODO: put the constant somewhere sensible.
        if multiply is not None:
            self._has_zeros = numpy.any(abs(multiply) < 1e-14)
        else:
            self._has_zeros = False
        self._multiply_first = multiply_first
        self.input_space = input_space

    def _multiply(self, batch):
        """
        .. todo::

            WRITEME
        """
        if self._multiply is not None:
            batch *= self._multiply
        return batch

    def _add(self, batch):
        """
        .. todo::

            WRITEME
        """
        if self._add is not None:
            batch += self._add
        return batch

    def __call__(self, batch):
        """
        .. todo::

            WRITEME
        """
        if self.input_space:
            self.input_space.validate(batch)
        cur = batch
        if self._multiply_first:
            batch = self._add(self._multiply(batch))
        else:
            batch = self._multiply(self._add(batch))
        return batch

    def inverse(self):
        """
        .. todo::

            WRITEME
        """
        if self._multiply is not None and self._has_zeros:
            raise ZeroDivisionError("%s transformation not invertible "
                                    "due to (near-) zeros in multiplicand" %
                                    self.__class__.__name__)
        else:
            mult_inverse = self._multiply ** -1.
            return self.__class__(add=-self._add, multiply=mult_inverse,
                                  multiply_first=not self._multiply_first)

    def set_input_space(self, space):
        """
        .. todo::

            WRITEME
        """
        self.input_space = space

    def get_input_space(self):
        """
        .. todo::

            WRITEME
        """
        if self.input_space is not None:
            return self.input_space
        raise ValueError("No input space was specified for this Block (%s). "
                         "You can call set_input_space to correct that." %
                         str(self))

    def get_output_space(self):
        """
        .. todo::

            WRITEME
        """
        return self.get_input_space()


class RemoveMean(ExamplewisePreprocessor):
    """
    Subtracts the mean along a given axis, or from every element
    if `axis=None`.

    Parameters
    ----------
    axis : int or None, optional
        Axis over which to take the mean, with the exact same
        semantics as the `axis` parameter of `numpy.mean`.
    """

    def __init__(self, axis=0):
        self._axis = axis
        self._mean = None

    def apply(self, dataset, can_fit=True):
        """
        .. todo::

            WRITEME
        """
        X = dataset.get_design_matrix()
        if can_fit:
            self._mean = X.mean(axis=self._axis)
        else:
            if self._mean is None:
                raise ValueError("can_fit is False, but RemoveMean object "
                                 "has no stored mean or standard deviation")
        X -= self._mean
        dataset.set_design_matrix(X)

    def as_block(self):
        """
        .. todo::

            WRITEME
        """
        if self._mean is None:
            raise ValueError("can't convert %s to block without fitting"
                             % self.__class__.__name__)
        return ExamplewiseAddScaleTransform(add=-self._mean)


class Standardize(ExamplewisePreprocessor):
    """
    Subtracts the mean and divides by the standard deviation.

    Parameters
    ----------
    global_mean : bool, optional
        If `True`, subtract the (scalar) mean over every element
        in the design matrix. If `False`, subtract the mean from
        each column (feature) separately. Default is `False`.
    global_std : bool, optional
        If `True`, after centering, divide by the (scalar) standard
        deviation of every element in the design matrix. If `False`,
        divide by the column-wise (per-feature) standard deviation.
        Default is `False`.
    std_eps : float, optional
        Stabilization factor added to the standard deviations before
        dividing, to prevent standard deviations very close to zero
        from causing the feature values to blow up too much.
        Default is `1e-4`.
    """

    def __init__(self, global_mean=False, global_std=False, std_eps=1e-4):
        self._global_mean = global_mean
        self._global_std = global_std
        self._std_eps = std_eps
        self._mean = None
        self._std = None

    def apply(self, dataset, can_fit=False):
        """
        .. todo::

            WRITEME
        """
        X = dataset.get_design_matrix()
        if can_fit:
            self._mean = X.mean() if self._global_mean else X.mean(axis=0)
            self._std = X.std() if self._global_std else X.std(axis=0)
        else:
            if self._mean is None or self._std is None:
                raise ValueError("can_fit is False, but Standardize object "
                                 "has no stored mean or standard deviation")
        new = (X - self._mean) / (self._std_eps + self._std)
        dataset.set_design_matrix(new)

    def as_block(self):
        """
        .. todo::

            WRITEME
        """
        if self._mean is None or self._std is None:
            raise ValueError("can't convert %s to block without fitting"
                             % self.__class__.__name__)
        return ExamplewiseAddScaleTransform(add=-self._mean,
                                            multiply=self._std ** -1)


class ColumnSubsetBlock(Block):
    """
    .. todo::

        WRITEME
    """
    def __init__(self, columns, total):
        self._columns = columns
        self._total = total

    def __call__(self, batch):
        """
        .. todo::

            WRITEME
        """
        if batch.ndim != 2:
            raise ValueError("Only two-dimensional tensors are supported")
        return batch.dimshuffle(1, 0)[self._columns].dimshuffle(1, 0)

    def inverse(self):
        """
        .. todo::

            WRITEME
        """
        return ZeroColumnInsertBlock(self._columns, self._total)

    def get_input_space(self):
        """
        .. todo::

            WRITEME
        """
        return VectorSpace(dim=self._total)

    def get_output_space(self):
        """
        .. todo::

            WRITEME
        """
        return VectorSpace(dim=self._columns)


class ZeroColumnInsertBlock(Block):
    def __init__(self, columns, total):
        """
        .. todo::

            WRITEME
        """
        self._columns = columns
        self._total = total

    def __call__(self, batch):
        """
        .. todo::

            WRITEME
        """
        if batch.ndim != 2:
            raise ValueError("Only two-dimensional tensors are supported")
        return insert_columns(batch, self._total, self._columns)

    def inverse(self):
        """
        .. todo::

            WRITEME
        """
        return ColumnSubsetBlock(self._columns, self._total)

    def get_input_space(self):
        """
        .. todo::

            WRITEME
        """
        return VectorSpace(dim=self._columns)

    def get_output_space(self):
        """
        .. todo::

            WRITEME
        """
        return VectorSpace(dim=self._total)


class RemoveZeroColumns(ExamplewisePreprocessor):
    """
    .. todo::

        WRITEME
    """
    _eps = 1e-8

    def __init__(self):
        self._block = None

    def apply(self, dataset, can_fit=False):
        """
        .. todo::

            WRITEME
        """
        design_matrix = dataset.get_design_matrix()
        mean = design_matrix.mean(axis=0)
        var = design_matrix.var(axis=0)
        columns, = numpy.where((var < self._eps) & (mean < self._eps))
        self._block = ColumnSubsetBlock

    def as_block(self):
        """
        .. todo::

            WRITEME
        """
        if self._block is None:
            raise ValueError("can't convert %s to block without fitting"
                             % self.__class__.__name__)
        return self._block


class RemapInterval(ExamplewisePreprocessor):
    """
    .. todo::

        WRITEME
    """
    # TODO: Implement as_block

    def __init__(self, map_from, map_to):
        assert map_from[0] < map_from[1] and len(map_from) == 2
        assert map_to[0] < map_to[1] and len(map_to) == 2
        self.map_from = [numpy.float(x) for x in map_from]
        self.map_to = [numpy.float(x) for x in map_to]

    def apply(self, dataset, can_fit=False):
        """
        .. todo::

            WRITEME
        """
        X = dataset.get_design_matrix()
        X = (X - self.map_from[0]) / numpy.diff(self.map_from)
        X = X * numpy.diff(self.map_to) + self.map_to[0]
        dataset.set_design_matrix(X)


class PCA_ViewConverter(object):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    to_pca : WRITEME
    to_input : WRITEME
    to_weights : WRITEME
    orig_view_converter : WRITEME
    """
    def __init__(self, to_pca, to_input, to_weights, orig_view_converter):
        self.to_pca = to_pca
        self.to_input = to_input
        self.to_weights = to_weights
        if orig_view_converter is None:
            raise ValueError("It doesn't make any sense to make a PCA view "
                             "converter when there's no original view "
                             "converter to define a topology in the first "
                             "place.")
        self.orig_view_converter = orig_view_converter

    def view_shape(self):
        """
        .. todo::

            WRITEME
        """
        return self.orig_view_converter.shape

    def design_mat_to_topo_view(self, X):
        """
        .. todo::

            WRITEME
        """
        to_input = self.to_input(X)
        return self.orig_view_converter.design_mat_to_topo_view(to_input)

    def design_mat_to_weights_view(self, X):
        """
        .. todo::

            WRITEME
        """
        to_weights = self.to_weights(X)
        return self.orig_view_converter.design_mat_to_weights_view(to_weights)

    def topo_view_to_design_mat(self, V):
        """
        .. todo::

            WRITEME
        """
        return self.to_pca(self.orig_view_converter.topo_view_to_design_mat(V))

    def get_formatted_batch(self, batch, dspace):
        """
        .. todo::

            WRITEME
        """
        if isinstance(dspace, VectorSpace):
            # Return the batch in the original storage space
            dspace.np_validate(batch)
            return batch
        else:
            # Uncompress and go through the original view converter
            to_input = self.to_input(batch)
            return self.orig_view_converter.get_formatted_batch(to_input,
                                                                dspace)


class PCA(object):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    num_components : WRITEME
    """

    def __init__(self, num_components):
        self._num_components = num_components
        self._pca = None
        # TODO: Is storing these really necessary? This computation
        # can't really be merged since we're basically creating the
        # functions in apply(); I see no reason to keep these around.
        self._input = tensor.matrix()
        self._output = tensor.matrix()

    def apply(self, dataset, can_fit=False):
        """
        .. todo::

            WRITEME
        """
        if self._pca is None:
            if not can_fit:
                raise ValueError("can_fit is False, but PCA preprocessor "
                                 "object has no fitted model stored")
            from pylearn2 import pca
            self._pca = pca.CovEigPCA(self._num_components)
            self._pca.train(dataset.get_design_matrix())
            self._transform_func = function([self._input],
                                            self._pca(self._input))
            self._invert_func = function([self._output],
                                         self._pca.reconstruct(self._output))
            self._convert_weights_func = function(
                [self._output],
                self._pca.reconstruct(self._output, add_mean=False)
            )

        orig_data = dataset.get_design_matrix()
        dataset.set_design_matrix(
            self._transform_func(dataset.get_design_matrix())
        )
        proc_data = dataset.get_design_matrix()
        orig_var = orig_data.var(axis=0)
        proc_var = proc_data.var(axis=0)
        assert proc_var[0] > orig_var.max()

        log.info('original variance: {0}'.format(orig_var.sum()))
        log.info('processed variance: {0}'.format(proc_var.sum()))
        if hasattr(dataset, 'view_converter'):
            if dataset.view_converter is not None:
                new_converter = PCA_ViewConverter(self._transform_func,
                                                  self._invert_func,
                                                  self._convert_weights_func,
                                                  dataset.view_converter)
                dataset.view_converter = new_converter


class Downsample(object):
    """
    Downsamples the topological view

    Parameters
    ----------
    sampling_factor : list or array
        One element for each topological
        dimension of the data
    """

    def __init__(self, sampling_factor):
        self.sampling_factor = sampling_factor

    def apply(self, dataset, can_fit=False):
        """
        .. todo::

            WRITEME
        """
        X = dataset.get_topological_view()
        d = len(X.shape) - 2
        assert d in [2, 3]
        assert X.dtype == 'float32' or X.dtype == 'float64'
        if d == 2:
            X = X.reshape([X.shape[0], X.shape[1], X.shape[2], 1, X.shape[3]])
        kernel_size = 1
        kernel_shape = [X.shape[-1]]
        for factor in self.sampling_factor:
            kernel_size *= factor
            kernel_shape.append(factor)
        if d == 2:
            kernel_shape.append(1)
        kernel_shape.append(X.shape[-1])
        kernel_value = 1. / float(kernel_size)
        kernel = numpy.zeros(kernel_shape, dtype=X.dtype)
        for i in xrange(X.shape[-1]):
            kernel[i, :, :, :, i] = kernel_value
        from theano.tensor.nnet.Conv3D import conv3D
        X_var = tensor.TensorType(broadcastable=[s == 1 for s in X.shape],
                                  dtype=X.dtype)()
        downsampled = conv3D(X_var, kernel, numpy.zeros(X.shape[-1], X.dtype),
                             kernel_shape[1:-1])
        f = function([X_var], downsampled)
        X = f(X)
        if d == 2:
            X = X.reshape([X.shape[0], X.shape[1], X.shape[2], X.shape[4]])
        dataset.set_topological_view(X)


class GlobalContrastNormalization(Preprocessor):
    """
    .. todo::

        WRITEME properly

    See the docstring for `global_contrast_normalize` in
    `pylearn2.expr.preprocessing`.

    Parameters
    ----------
    batch_size : int or None, optional
        If specified, read, apply and write the transformed data
        in batches no larger than `batch_size`.
    sqrt_bias : float, optional
        Defaults to 0 if nothing is specified
    use_std : bool, optional
        Defaults to False if nothing is specified
    """

    def __init__(self, subtract_mean=True,
                 scale=1., sqrt_bias=0., use_std=False, min_divisor=1e-8,
                 batch_size=None):
        self._subtract_mean = subtract_mean
        self._use_std = use_std
        self._sqrt_bias = sqrt_bias
        self._scale = scale
        self._min_divisor = min_divisor
        if batch_size is not None:
            batch_size = int(batch_size)
            assert batch_size > 0, "batch_size must be positive"
        self._batch_size = batch_size

    def apply(self, dataset, can_fit=False):
        """
        .. todo::

            WRITEME
        """
        if self._batch_size is None:
            X = global_contrast_normalize(dataset.get_design_matrix(),
                                          scale=self._scale,
                                          subtract_mean=self._subtract_mean,
                                          use_std=self._use_std,
                                          sqrt_bias=self._sqrt_bias,
                                          min_divisor=self._min_divisor)
            dataset.set_design_matrix(X)
        else:
            data = dataset.get_design_matrix()
            data_size = data.shape[0]
            last = (numpy.floor(data_size / float(self._batch_size)) *
                    self._batch_size)
            for i in xrange(0, data_size, self._batch_size):
                stop = i + self._batch_size
                log.info("GCN processing data from %d to %d" % (i, stop))
                X = data[i:stop]
                X = global_contrast_normalize(X,
                                              scale=self._scale,
                                              subtract_mean=self._subtract_mean,
                                              use_std=self._use_std,
                                              sqrt_bias=self._sqrt_bias,
                                              min_divisor=self._min_divisor)
                dataset.set_design_matrix(X, start=i)


class ZCA(Preprocessor):
    """
    Performs ZCA whitening.

    .. TODO::

        WRITEME properly
        add reference

    Parameters
    ----------
    n_components : WRITEME
    n_drop_components : WRITEME
    filter_bias : float, optional
        TODO: verify that default of 0.1 is what was used in the
        Coates and Ng paper, add reference
    store_inverse : bool, optional
        When self.apply(dataset, can_fit=True) store not just the
        preprocessing matrix, but its inverse. This is necessary when
        using this preprocessor to instantiate a ZCA_Dataset.
    """

    def __init__(self, n_components=None, n_drop_components=None,
                 filter_bias=0.1, store_inverse=True):
        warnings.warn("This ZCA preprocessor class is known to yield very "
                      "different results on different platforms. If you plan "
                      "to conduct experiments with this preprocessing on "
                      "multiple machines, it is probably a good idea to do "
                      "the preprocessing on a single machine and copy the "
                      "preprocessed datasets to the others, rather than "
                      "preprocessing the data independently in each "
                      "location.")
        # TODO: test to see if differences across platforms
        # e.g., preprocessing STL-10 patches in LISA lab versus on
        # Ian's Ubuntu 11.04 machine
        # are due to the problem having a bad condition number or due to
        # different version numbers of scipy or something
        self.n_components = n_components
        self.n_drop_components = n_drop_components
        self.copy = True
        self.filter_bias = numpy.cast[theano.config.floatX](filter_bias)
        self.has_fit_ = False
        self.store_inverse = store_inverse
        self.P_ = None  # set by fit()
        self.inv_P_ = None  # set by fit(), if self.store_inverse is True

        # Analogous to DenseDesignMatrix.design_loc. If not None, the
        # matrices P_ and inv_P_ will be saved together in <save_path>
        # (or <save_path>.npz, if the suffix is omitted).
        self.matrices_save_path = None

    @staticmethod
    def _gpu_matrix_dot(matrix_a, matrix_b, matrix_c=None):
        """
        Performs matrix multiplication.

        Attempts to use the GPU if it's available. If the matrix multiplication
        is too big to fit on the GPU, this falls back to the CPU after throwing
        a warning.

        Parameters
        ----------
        matrix_a : WRITEME
        matrix_b : WRITEME
        matrix_c : WRITEME
        """
        if not hasattr(ZCA._gpu_matrix_dot, 'theano_func'):
            ma, mb = theano.tensor.matrices('A', 'B')
            mc = theano.tensor.dot(ma, mb)
            ZCA._gpu_matrix_dot.theano_func = theano.function([ma, mb], mc,
                    allow_input_downcast=True)

        theano_func = ZCA._gpu_matrix_dot.theano_func

        try:
            if matrix_c is None:
                return theano_func(matrix_a, matrix_b)
            else:
                matrix_c[...] = theano_func(matrix_a, matrix_b)
                return matrix_c
        except MemoryError, me:
            warnings.warn('Matrix multiplication too big to fit on GPU. '
                          'Re-doing with CPU. Consider using '
                          'THEANO_FLAGS="device=cpu" for your next '
                          'preprocessor run')
            return numpy.dot(matrix_a, matrix_b, matrix_c)

    @staticmethod
    def _gpu_mdmt(mat, diags):
        """
        Performs the matrix multiplication M * D * M^T.

        First tries to do this on the GPU. If this throws a MemoryError, it
        falls back to the CPU, with a warning message.

        Parameters
        ----------
        mat : WRITEME
        diags : WRITEME
        """

        floatX = theano.config.floatX

        # compile theano function
        if not hasattr(ZCA._gpu_mdmt, 'theano_func'):
            t_mat = theano.tensor.matrix('M')
            t_diags = theano.tensor.vector('D')
            result = theano.tensor.dot(t_mat * t_diags, t_mat.T)
            ZCA._gpu_mdmt.theano_func = theano.function(
                [t_mat, t_diags],
                result,
                allow_input_downcast=True)

        try:
            # function()-call above had to downcast the data. Emit warnings.
            if str(mat.dtype) != floatX:
                warnings.warn('Implicitly converting mat from dtype=%s to '
                              '%s for gpu' % (mat.dtype, floatX))
            if str(diags.dtype) != floatX:
                warnings.warn('Implicitly converting diag from dtype=%s to '
                              '%s for gpu' % (diags.dtype, floatX))

            return ZCA._gpu_mdmt.theano_func(mat, diags)

        except MemoryError:
            # fall back to cpu
            warnings.warn('M * D * M^T was too big to fit on GPU. '
                          'Re-doing with CPU. Consider using '
                          'THEANO_FLAGS="device=cpu" for your next '
                          'preprocessor run')
            return numpy.dot(mat * diags, mat.T)

    def set_matrices_save_path(self, matrices_save_path):
        """
        Analogous to DenseDesignMatrix.use_design_loc().

        If a matrices_save_path is set, when this ZCA is pickled, the internal
        parameter matrices will be saved separately to `matrices_save_path`, as
        a numpy .npz archive. This uses half the memory that a normal pickling
        does.

        Parameters
        ----------
        matrices_save_path : WRITEME
        """
        if matrices_save_path is not None:
            assert isinstance(matrices_save_path, str)
            matrices_save_path = os.path.abspath(matrices_save_path)

            if os.path.isdir(matrices_save_path):
                raise IOError('Matrix save path "%s" must not be an existing '
                              'directory.')

            assert matrices_save_path[-1] not in ('/', '\\')
            if not os.path.isdir(os.path.split(matrices_save_path)[0]):
                raise IOError('Couldn\'t find parent directory:\n'
                              '\t"%s"\n'
                              '\t of matrix path\n'
                              '\t"%s"')

        self.matrices_save_path = matrices_save_path

    def __getstate__(self):
        """
        Used by pickle.  Returns a dictionary to pickle in place of
        self.__dict__.

        If self.matrices_save_path is set, this saves the matrices P_ and
        inv_P_ separately in matrices_save_path as a .npz archive, which uses
        much less space & memory than letting pickle handle them.
        """
        result = copy.copy(self.__dict__)  # shallow copy
        if self.matrices_save_path is not None:
            matrices = {'P_': self.P_}
            if self.inv_P_ is not None:
                matrices['inv_P_'] = self.inv_P_

            numpy.savez(self.matrices_save_path, **matrices)

            # Removes the matrices from the dictionary to be pickled.
            for key, matrix in matrices.items():
                del result[key]

        return result

    def __setstate__(self, state):
        """
        Used to unpickle.

        Parameters
        ----------
        state : dict
            The dictionary created by __setstate__, presumably unpickled
            from disk.
        """

        # Patch old pickle files
        if 'matrices_save_path' not in state:
            state['matrices_save_path'] = None

        if state['matrices_save_path'] is not None:
            matrices = numpy.load(state['matrices_save_path'])

            # puts matrices' items into state, overriding any colliding keys in
            # state.
            state = dict(state.items() + matrices.items())
            del matrices

        self.__dict__.update(state)

    def fit(self, X):
        """
        Fits this `ZCA` instance to a design matrix `X`.

        Parameters
        ----------
        X : ndarray
            A matrix where each row is a datum.

        Notes
        -----
        Implementation details:
        Stores result as `self.P_`.
        If self.store_inverse is true, this also computes `self.inv_P_`.
        """

        assert X.dtype in ['float32', 'float64']
        assert not numpy.any(numpy.isnan(X))
        assert len(X.shape) == 2
        n_samples = X.shape[0]
        if self.copy:
            X = X.copy()
        # Center data
        self.mean_ = numpy.mean(X, axis=0)
        X -= self.mean_

        log.info('computing zca of a {0} matrix'.format(X.shape))
        t1 = time.time()

        bias = self.filter_bias * scipy.sparse.identity(X.shape[1],
                                                        theano.config.floatX)

        covariance = ZCA._gpu_matrix_dot(X.T, X) / X.shape[0] + bias
        t2 = time.time()
        log.info("cov estimate took {0} seconds".format(t2-t1))

        t1 = time.time()
        eigs, eigv = linalg.eigh(covariance)
        t2 = time.time()
        log.info("eigh() took {0} seconds".format(t2 - t1))
        assert not numpy.any(numpy.isnan(eigs))
        assert not numpy.any(numpy.isnan(eigv))
        assert eigs.min() > 0
        if self.n_components:
            eigs = eigs[:self.n_components]
            eigv = eigv[:, :self.n_components]

        if self.n_drop_components:
            eigs = eigs[self.n_drop_components:]
            eigv = eigv[:, self.n_drop_components:]

        t1 = time.time()

        sqrt_eigs = numpy.sqrt(eigs)
        try:
            self.P_ = ZCA._gpu_mdmt(eigv, 1.0/sqrt_eigs)
        except MemoryError:
            warnings.warn()
            self.P_ = numpy.dot(eigv * (1.0 / sqrt_eigs), eigv.T)

        t2 = time.time()
        assert not numpy.any(numpy.isnan(self.P_))
        self.has_fit_ = True

        if self.store_inverse:
            self.inv_P_ = ZCA._gpu_mdmt(eigv, sqrt_eigs)
        else:
            self.inv_P_ = None

    def apply(self, dataset, can_fit=False):
        """
        .. todo::

            WRITEME
        """
        # Compiles apply.x_minus_mean_times_p(), a numeric Theano function that
        # evauates dot(X - mean, P)
        if not hasattr(ZCA, '_x_minus_mean_times_p'):
            x_symbol = tensor.matrix('X')
            mean_symbol = tensor.vector('mean')
            p_symbol = tensor.matrix('P_')
            new_x_symbol = tensor.dot(x_symbol - mean_symbol, p_symbol)
            ZCA._x_minus_mean_times_p = theano.function([x_symbol,
                                                         mean_symbol,
                                                         p_symbol],
                                                        new_x_symbol)

        X = dataset.get_design_matrix()
        assert X.dtype in ['float32', 'float64']
        if not self.has_fit_:
            assert can_fit
            self.fit(X)

        new_X = ZCA._gpu_matrix_dot(X - self.mean_, self.P_)
        dataset.set_design_matrix(new_X)

    def inverse(self, X):
        """
        .. todo::

            WRITEME
        """
        assert X.ndim == 2
        return self._gpu_matrix_dot(X, self.inv_P_) + self.mean_


class LeCunLCN(ExamplewisePreprocessor):
    """
    Yann LeCun local contrast normalization

    .. todo::

        WRITEME properly

    Parameters
    ----------
    img_shape : WRITEME
    kernel_size : int, optional
        local contrast kernel size
    batch_size: int, optional
        If dataset is based on PyTables use a batch size smaller than
        10000. Otherwise any batch size diffrent than datasize is not
        supported yet.
    threshold : float
        Threshold for denominator
    channels : list or None, optional
        List of channels to normalize.
        If none, will apply it on all channels.
    """

    def __init__(self, img_shape, kernel_size=7, batch_size=5000,
                 threshold=1e-4, channels=None):
        self._img_shape = img_shape
        self._kernel_size = kernel_size
        self._batch_size = batch_size
        self._threshold = threshold
        if channels is None:
            self._channels = range(3)
        else:
            if isinstance(channels, list) or isinstance(channels, tuple):
                self._channels = channels
            elif isinstance(channels, int):
                self._channels = [channels]
            else:
                raise ValueError("channesl should be either a list or int")

    def transform(self, x):
        """
        .. todo::

            WRITEME properly

        Parameters
        ----------
        X : WRITEME
            data with axis [b, 0, 1, c]
        """
        for i in self._channels:
            assert isinstance(i, int)
            assert i >= 0 and i <= x.shape[3]

            x[:, :, :, i] = lecun_lcn(x[:, :, :, i],
                                      self._img_shape,
                                      self._kernel_size,
                                      self._threshold)
            return x

    def apply(self, dataset, can_fit=False):
        """
        .. todo::

            WRITEME
        """
        axes = ['b', 0, 1, 'c']
        data_size = dataset.X.shape[0]

        if self._channels is None:
            self._channels

        last = (numpy.floor(data_size / float(self._batch_size)) *
                self._batch_size)
        for i in xrange(0, data_size, self._batch_size):
            stop = (i + numpy.mod(data_size, self._batch_size)
                    if i >= last else
                    i + self._batch_size)
            log.info("LCN processing data from {0} to {1}".format(i, stop))
            transformed = self.transform(convert_axes(
                dataset.get_topological_view(dataset.X[i:stop, :]),
                dataset.view_converter.axes, axes))
            transformed = convert_axes(transformed,
                                       axes,
                                       dataset.view_converter.axes)
            if self._batch_size != data_size:
                if isinstance(dataset.X, numpy.ndarray):
                    # TODO have a separate class for non pytables datasets
                    transformed = convert_axes(transformed,
                                               dataset.view_converter.axes,
                                               ['b', 0, 1, 'c'])
                    transformed = transformed.reshape(transformed.shape[0],
                                                      transformed.shape[1] *
                                                      transformed.shape[2] *
                                                      transformed.shape[3])
                    dataset.X[i:stop] = transformed
                else:
                    dataset.set_topological_view(transformed,
                                                 dataset.view_converter.axes,
                                                 start=i)

        if self._batch_size == data_size:
            dataset.set_topological_view(transformed,
                                         dataset.view_converter.axes)


class RGB_YUV(ExamplewisePreprocessor):
    """
    Converts image color channels from rgb to yuv and vice versa

    Parameters
    ----------
    rgb_yuv : bool, optional
        If true converts from rgb to yuv,
        if false converts from yuv to rgb
    batch_size : int, optional
        Batch_size to make conversions in batches
    """

    def __init__(self, rgb_yuv=True, batch_size=5000):

        self._batch_size = batch_size
        self._rgb_yuv = rgb_yuv

    def yuv_rgb(self, x):
        """
        .. todo::

            WRITEME
        """
        y = x[:, :, :, 0]
        u = x[:, :, :, 1]
        v = x[:, :, :, 2]

        r = y + 1.13983 * v
        g = y - 0.39465 * u - 0.58060 * v
        b = y + 2.03211 * u

        x[:, :, :, 0] = r
        x[:, :, :, 1] = g
        x[:, :, :, 2] = b

        return x

    def rgb_yuv(self, x):
        """
        .. todo::

            WRITEME
        """
        r = x[:, :, :, 0]
        g = x[:, :, :, 1]
        b = x[:, :, :, 2]

        y = 0.299 * r + 0.587 * g + 0.114 * b
        u = -0.14713 * r - 0.28886 * g + 0.436 * b
        v = 0.615 * r - 0.51499 * g - 0.10001 * b

        x[:, :, :, 0] = y
        x[:, :, :, 1] = u
        x[:, :, :, 2] = v

        return x

    def transform(self, x, dataset_axes):
        """
        .. todo::

            WRITEME
        """
        axes = ['b', 0, 1, 'c']
        x = convert_axes(x, dataset_axes, axes)
        if self._rgb_yuv:
            x = self.rgb_yuv(x)
        else:
            x = self.yuv_rgb(x)
        x = convert_axes(x, axes, dataset_axes)
        return x

    def apply(self, dataset, can_fit=False):
        """
        .. todo::

            WRITEME
        """
        X = dataset.X
        data_size = X.shape[0]
        last = (numpy.floor(data_size / float(self._batch_size)) *
                self._batch_size)
        for i in xrange(0, data_size, self._batch_size):
            stop = (i + numpy.mod(data_size, self._batch_size)
                    if i >= last else
                    i + self._batch_size)
            log.info("RGB_YUV processing data from {0} to {1}".format(i, stop))
            data = dataset.get_topological_view(X[i:stop])
            transformed = self.transform(data, dataset.view_converter.axes)

            # TODO have a separate class for non pytables datasets
            # or add start option to dense_design_matrix
            if isinstance(dataset.X, numpy.ndarray):
                transformed = convert_axes(transformed,
                                           dataset.view_converter.axes,
                                           ['b', 0, 1, 'c'])
                transformed = transformed.reshape(transformed.shape[0],
                                                  transformed.shape[1] *
                                                  transformed.shape[2] *
                                                  transformed.shape[3])
                dataset.X[i:stop] = transformed
            else:
                dataset.set_topological_view(transformed,
                                             dataset.view_converter.axes,
                                             start=i)


class CentralWindow(Preprocessor):
    """
    Preprocesses an image dataset to contain only the central window.

    Parameters
    ----------
    window_shape : WRITEME
    """

    def __init__(self, window_shape):
        self.__dict__.update(locals())
        del self.self

    def apply(self, dataset, can_fit=False):
        """
        .. todo::

            WRITEME
        """
        w_rows, w_cols = self.window_shape

        arr = dataset.get_topological_view()

        try:
            axes = dataset.view_converter.axes
        except AttributeError:
            raise NotImplementedError("I don't know how to tell what the axes "
                                      "of this kind of dataset are.")

        needs_transpose = not axes[1:3] == (0, 1)

        if needs_transpose:
            arr = numpy.transpose(arr,
                                  (axes.index('c'),
                                   axes.index(0),
                                   axes.index(1),
                                   axes.index('b')))

        r_off = (arr.shape[1] - w_rows) // 2
        c_off = (arr.shape[2] - w_cols) // 2
        new_arr = arr[:, r_off:r_off + w_rows, c_off:c_off + w_cols, :]

        if needs_transpose:
            index_map = tuple(('c', 0, 1, 'b').index(axis) for axis in axes)
            new_arr = numpy.transpose(new_arr, index_map)

        dataset.set_topological_view(new_arr, axes=axes)


def lecun_lcn(input, img_shape, kernel_shape, threshold=1e-4):
    """
    Yann LeCun's local contrast normalization

    Original code in Theano by: Guillaume Desjardins

    Parameters
    ----------
    input : WRITEME
    img_shape : WRITEME
    kernel_shape : WRITEME
    threshold : WRITEME
    """
    input = input.reshape(input.shape[0], input.shape[1], input.shape[2], 1)
    X = tensor.matrix(dtype=input.dtype)
    X = X.reshape((len(input), img_shape[0], img_shape[1], 1))

    filter_shape = (1, 1, kernel_shape, kernel_shape)
    filters = sharedX(gaussian_filter(kernel_shape).reshape(filter_shape))

    input_space = Conv2DSpace(shape=img_shape, num_channels=1)
    transformer = Conv2D(filters=filters, batch_size=len(input),
                         input_space=input_space,
                         border_mode='full')
    convout = transformer.lmul(X)

    # For each pixel, remove mean of 9x9 neighborhood
    mid = int(numpy.floor(kernel_shape / 2.))
    centered_X = X - convout[:, mid:-mid, mid:-mid, :]

    # Scale down norm of 9x9 patch if norm is bigger than 1
    transformer = Conv2D(filters=filters,
                         batch_size=len(input),
                         input_space=input_space,
                         border_mode='full')
    sum_sqr_XX = transformer.lmul(X**2)

    denom = tensor.sqrt(sum_sqr_XX[:, mid:-mid, mid:-mid, :])
    per_img_mean = denom.mean(axis=[1, 2])
    divisor = tensor.largest(per_img_mean.dimshuffle(0, 'x', 'x', 1), denom)
    divisor = tensor.maximum(divisor, threshold)

    new_X = centered_X / divisor
    new_X = tensor.flatten(new_X, outdim=3)

    f = function([X], new_X)
    return f(input)


def gaussian_filter(kernel_shape):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    kernel_shape : WRITEME
    """
    x = numpy.zeros((kernel_shape, kernel_shape),
                    dtype=theano.config.floatX)

    def gauss(x, y, sigma=2.0):
        Z = 2 * numpy.pi * sigma**2
        return 1. / Z * numpy.exp(-(x**2 + y**2) / (2. * sigma**2))

    mid = numpy.floor(kernel_shape / 2.)
    for i in xrange(0, kernel_shape):
        for j in xrange(0, kernel_shape):
            x[i, j] = gauss(i - mid, j - mid)

    return x / numpy.sum(x)


class ShuffleAndSplit(Preprocessor):
    """
    .. todo::

        WRITEME properly

    Allocates a numpy rng with the specified seed.
    Note: this must be a seed, not a RandomState. A new RandomState is
    re-created with the same seed every time the preprocessor is called.
    This way if you save the preprocessor and re-use it later it will give
    the same dataset regardless of whether you save the preprocessor before
    or after applying it.
    Shuffles the data, then takes examples in range (start, stop)

    Parameters
    ----------
    seed : WRITEME
    start : int
        WRITEME
    stop : int
        WRITEME
    """

    def __init__(self, seed, start, stop):
        self.__dict__.update(locals())
        del self.self

    def apply(self, dataset, can_fit=False):
        """
        .. todo::

            WRITEME
        """
        start = self.start
        stop = self.stop
        rng = make_np_rng(self.seed, which_method="randint")
        X = dataset.X
        y = dataset.y

        if y is not None:
            assert X.shape[0] == y.shape[0]

        for i in xrange(X.shape[0]):
            j = rng.randint(X.shape[0])
            tmp = X[i, :].copy()
            X[i, :] = X[j, :].copy()
            X[j, :] = tmp

            if y is not None:
                tmp = y[i, :].copy()
                y[i, :] = y[j, :].copy()
                y[j, :] = tmp
        assert start >= 0
        assert stop > start
        assert stop <= X.shape[0]

        dataset.X = X[start:stop, :]
        if y is not None:
            dataset.y = y[start:stop, :]

########NEW FILE########
__FILENAME__ = retina
"""
Retina-inspired preprocessing as described in
    Salakhutdinov, R. and Hinton, G. Deep Boltzmann machines.
    In *AISTATS* 2009.
"""
import numpy
from pylearn2.datasets.dense_design_matrix import DefaultViewConverter
from pylearn2.space import Conv2DSpace


def foveate_channel(img, rings, output, start_idx):
    """For a given channel (image), perform pooling on peripheral vision.

    .. todo::

        Write parameter list
    """
    ring_w = numpy.sum(rings)

    # extract image center, which remains dense
    inner_img = img[:, ring_w : img.shape[1] - ring_w , ring_w : img.shape[2] - ring_w]

    # flatten and write to dense output matrix
    inner_img = inner_img.reshape(len(output), -1)
    end_idx = start_idx + inner_img.shape[1]
    output[:, start_idx : end_idx] = inner_img

    # start by downsampling the periphery of the images
    idx = 0
    start_idx = end_idx
    for rd in rings:
        # downsample the ring with top-left corner (idx,idx) of width rd
        # results are written in output[start_idx:]
        start_idx = downsample_ring(img, idx, rd, output, start_idx)
        idx += rd

    return start_idx

def downsample_ring(img, coord, width, output, start_idx):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    img : WRITEME
        numpy matrix in topological order
        (batch size, rows, cols, channels)
    coord : WRITEME
        perform average pooling starting at coordinate (coord,coord)
    width : WRITEME
        width of "square ring" to average pool
    output : WRITEME
        dense design matrix, of shape (batch size, rows*cols*channels)
    start_idx : WRITEME
        column index where to start writing the output
    """
    (img_h,img_w) = img.shape[1:3]

    # left column, full height
    start_idx = downsample_rect(img, coord, coord, img_h - coord, coord + width, width, output, start_idx)
    # right column, full height
    start_idx = downsample_rect(img, coord, img_w - coord - width, img_h - coord, img_w - coord, width, output, start_idx)
    # top row, between columns
    start_idx = downsample_rect(img, coord, coord + width, coord + width, img_w - coord - width, width, output, start_idx)
    # bottom row, between columns
    start_idx = downsample_rect(img, img_h - coord - width, coord + width, img_h - coord, img_w - coord - width, width, output, start_idx)

    return start_idx


def downsample_rect(img, start_row, start_col, end_row, end_col, width, output, start_idx):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    img : WRITEME
        numpy matrix in topological order
        (batch size, rows, cols, channels)
    start_row : WRITEME
        row index of top-left corner of rectangle to average pool
    start_col : WRITEME
        col index of top-left corner of rectangle to average pool
    end_row : WRITEME
        row index of bottom-right corner of rectangle to average pool
    end_col : WRITEME
        col index of bottom-right corner of rectangle to average pool
    width : WRITEME
        take the mean over rectangular block of this width
    output : WRITEME
        dense design matrix, of shape (batch size, rows*cols*channels)
    start_idx : WRITEME
        column index where to start writing the output
    """
    idx = start_idx

    for i in xrange(start_row, end_row - width + 1, width):
        for j in xrange(start_col, end_col - width + 1, width):
            block = img[:, i:i+width, j:j+width]
            output[:,idx] = numpy.apply_over_axes(numpy.mean, block, axes=[1,2])[:,0,0]
            idx += 1

    return idx


def defoveate_channel(img, rings, dense_input, start_idx):
    """
    Defoveate a single channel of the DenseDesignMatrix dense_input into the
    variable, stored in topological ordering.

    Parameters
    ----------
    img : WRITEME
        channel for defoveated image of shape (batch, img_h, img_w)
    rings : WRITEME
        list of ring_sizes which were used to generate dense_input
    dense_input : WRITEME
        DenseDesignMatrix containing foveated dataset, of shape 
        (batch, dims)
    start_idx : WRITEME
        channel pointed to by img starts at dense_input[start_idx]
    """
    ring_w = numpy.sum(rings)

    # extract image center, which remains dense
    inner_h = img.shape[1] - 2*ring_w
    inner_w = img.shape[2] - 2*ring_w
    end_idx = start_idx + inner_h * inner_w
    inner_img = dense_input[:, start_idx : end_idx].reshape(-1, inner_h, inner_w)

    # now restore image center in uncompressed image
    img[:, ring_w:ring_w+inner_h, ring_w:ring_w+inner_w] = inner_img

    # now undo downsampling along the periphery
    idx = 0
    start_idx = end_idx
    for rd in rings:
        # downsample the ring with top-left corner (idx,idx) of width rd
        # results are written in img[idx:idx+rd, idx:idx+rd]
        start_idx = restore_ring(img, idx, rd, dense_input, start_idx)
        idx += rd

    return start_idx


def restore_ring(output, coord, width, dense_input, start_idx):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    output : WRITEME
        output matrix in topological order
        (batch, height, width, channels)
    coord : WRITEME
        perform average pooling starting at coordinate (coord,coord)
    width : WRITEME
        width of "square ring" to average pool
    dense_input : WRITEME
        dense design matrix to convert (batchsize, dims)
    start_idx : WRITEME
        column index where to start writing the output
    """
    (img_h, img_w) = output.shape[1:3]

    # left column, full height
    start_idx = restore_rect(output, coord, coord, img_h - coord, coord + width, width, dense_input, start_idx)
    # right column, full height
    start_idx = restore_rect(output, coord, img_w - coord - width, img_h - coord, img_w - coord, width, dense_input, start_idx)
    # top row, between columns
    start_idx = restore_rect(output, coord, coord + width, coord + width, 96 - coord - width, width, dense_input, start_idx)
    # bottom row, between columns
    start_idx = restore_rect(output, img_h - coord - width, coord + width, img_h - coord, img_w - coord - width, width, dense_input, start_idx)

    return start_idx


def restore_rect(output, start_row, start_col, stop_row, stop_col, width, dense_input, start_idx):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    output : WRITEME
        output matrix in topological order
        (batch, height, width, channels)
    start_row : WRITEME
        row index of top-left corner of rectangle to average pool
    start_col : WRITEME
        col index of top-left corner of rectangle to average pool
    end_row : WRITEME
        row index of bottom-right corner of rectangle to average pool
    end_col : WRITEME
        col index of bottom-right corner of rectangle to average pool
    width : WRITEME
        take the mean over rectangular block of this width
    dense_input : WRITEME
        dense design matrix to convert (batchsize, dims)
    start_idx : WRITEME
        column index where to start writing the output
    """
    idx = start_idx

    for i in xrange(start_row, stop_row - width + 1, width):
        for j in xrange(start_col, stop_col - width + 1, width):
            # broadcast along the width and height of the block
            output[:, i:i+width, j:j+width] = dense_input[:, idx][:, None, None]
            idx += 1

    return idx


def get_encoded_size(img_h, img_w, rings):
    """
    .. todo::

        WRITEME
    """
    pool_len = 0

    # count number of pixels after compression
    for r in rings:
        if (img_h%r) != 0 or (img_w%r) != 0:
            raise ValueError('Image width (%i) or height (%i) is not a multiple of ring size %i' %
                             (img_h, img_w, r))
        pool_len +=  2*img_h/r +  2*(img_w - 2*r)/r
        img_h -= 2*r
        img_w -= 2*r

    return pool_len + img_h * img_w


def encode(topo_X, rings):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    topo_X : WRITEME
        dataset matrix in topological format (batch, rows, cols, chans)
    rings : WRITEME
        list of ring_sizes which were used to generate dense_input
    """
    (batch_size, img_h, img_w, chans) = topo_X.shape

    # determine output shape
    out_size = get_encoded_size(img_h, img_w, rings)
    output = numpy.zeros((batch_size, out_size * chans))

    start_idx = 0
    # perform retina encoding on each channel separately
    for chan_i in xrange(chans):
        channel = topo_X[..., chan_i]
        start_idx = foveate_channel(channel, rings, output, start_idx)

    return output

def decode(dense_X, img_shp, rings):
    """
    .. todo::

        WRITEME

	Parameters
    ----------
    dense_X : WRITEME
        matrix in DenseDesignMatrix format (batch, dim)
    img_shp : WRITEME
        tuple of image dimensions (rows, cols, chans)
    rings : WRITEME
        list of ring_sizes which were used to generate dense_input
    """
    out_shp = [len(dense_X)] + list(img_shp)
    output = numpy.zeros(out_shp)

    start_idx = 0
    # perform retina encoding on each channel separately
    for chan_i in xrange(out_shp[-1]):
        channel = output[..., chan_i]
        start_idx = defoveate_channel(channel, rings, dense_X, start_idx)

    return output


class RetinaEncodingBlock(object):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    rings : WRITEME
    """

    def __init__(self, rings):
        self.rings = rings

    def perform(self, V):
        assert V.ndim == 4
        return encode(V, self.rings)

    def apply(self, dataset, can_fit=False):
        topo_X = dataset.get_topological_view()
        fov_X = encode(topo_X, self.rings)
        dataset.set_design_matrix(fov_X)


class RetinaDecodingBlock(object):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    img_shp : WRITEME
    rings : WRITEME
    """
    def __init__(self, img_shp, rings):
        """
        .. todo::

            WRITEME
        """
        self.img_shp = img_shp
        self.rings = rings

    def apply(self, dataset, can_fit=False):
        """
        .. todo::

            WRITEME
        """
        X = dataset.get_design_matrix()
        topo_X = self.perform(X)
        dataset.set_topological_view(topo_X)

    def perform(self, X):
        """
        .. todo::

            WRITEME
        """
        return decode(X, self.img_shp, self.rings)


class RetinaCodingViewConverter(DefaultViewConverter):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    shape : iterable
        List or tuple of three ints: rows, cols, channels
    rings : WRITEME
    """

    def __init__(self, shape, rings):
        self.shape = shape
        self.rings = rings

        rows, cols, channels = shape

        self.topo_space = Conv2DSpace(shape=[rows, cols], num_channels=channels)

        self.decoder = RetinaDecodingBlock(shape, rings)
        self.encoder = RetinaEncodingBlock(rings)

    def design_mat_to_topo_view(self, X):
        """
        .. todo::

            WRITEME
        """
        return self.decoder.perform(X)

    def design_mat_to_weights_view(self, X):
        """
        .. todo::

            WRITEME
        """
        return self.design_mat_to_topo_view(X)

    def topo_view_to_design_mat(self, V):
        """
        .. todo::

            WRITEME
        """
        return self.encoder.perform(V)

########NEW FILE########
__FILENAME__ = sparse_dataset
"""
.. todo::

    WRITEME
"""
import functools

from pylearn2.datasets.dataset import Dataset
from pylearn2.utils import wraps
import logging
import numpy
import warnings
try:
    import scipy.sparse
except ImportError:
    warnings.warn("Couldn't import scipy.sparse")
import theano
import gzip
floatX = theano.config.floatX
logger = logging.getLogger(__name__)
from pylearn2.space import CompositeSpace, VectorSpace
from pylearn2.utils import safe_zip
from pylearn2.utils.iteration import (
    FiniteDatasetIterator,
    resolve_iterator_class
)


class SparseDataset(Dataset):
    """
    SparseDataset is a class for representing datasets that can be
    stored as a sparse matrix.

    Parameters
    ----------
    load_path : str or None, optional
        the path to read the sparse dataset
        from_scipy_sparse_dataset is not used if load_path is specified
    from_scipy_sparse_dataset : matrix of type scipy.sparse or None, optional
        In case load_path is not provided,
        the sparse dataset is passed directly to the class by
        using from_scipy_sparse_dataset parameter.
    zipped_npy : bool, optional
        used only when load_path is specified.
        indicates whether the input matrix is zipped or not.
        defaults to True.
    """

    def __init__(self, load_path=None,
                 from_scipy_sparse_dataset=None, zipped_npy=True):

        self.load_path = load_path
        self.y = None

        if self.load_path is not None:
            if zipped_npy is True:
                logger.info('... loading sparse data set from a zip npy file')
                self.X = scipy.sparse.csr_matrix(
                    numpy.load(gzip.open(load_path)), dtype=floatX)
            else:
                logger.info('... loading sparse data set from a npy file')
                self.X = scipy.sparse.csr_matrix(
                    numpy.load(load_path).item(), dtype=floatX)
        else:
            logger.info('... building from given sparse dataset')
            self.X = from_scipy_sparse_dataset
            if not scipy.sparse.issparse(from_scipy_sparse_dataset):
                msg = "from_scipy_sparse_dataset is not sparse : %s" \
                      % type(self.X)
                raise TypeError(msg)

        X_space = VectorSpace(dim=self.X.shape[1], sparse=True)
        self.X_space = X_space
        space = self.X_space
        source = 'features'
        self._iter_data_specs = (space, source)
        self.data_specs = (space, source)

    def get_design_matrix(self):
        """
        .. todo::

            WRITEME
        """
        return self.X

    @wraps(Dataset.get_batch_design)
    def get_batch_design(self, batch_size, include_labels=False):
        """Method inherited from Dataset"""
        self.iterator(mode='shuffled_sequential',
                      batch_size=batch_size, num_batches=None, topo=None)
        return self.next()

    @wraps(Dataset.get_batch_topo)
    def get_batch_topo(self, batch_size):
        """Method inherited from Dataset"""
        raise NotImplementedError('Not implemented for sparse dataset')

    @wraps(Dataset.iterator)
    def iterator(self, mode=None, batch_size=None, num_batches=None,
                 topo=None, targets=None, rng=None, data_specs=None,
                 return_tuple=False):

        if topo is not None or targets is not None:
            warnings.warn("Usage of `topo` and `target` arguments are "
                          "being deprecated, and will be removed "
                          "around November 7th, 2013. `data_specs` "
                          "should be used instead. Here these two "
                          "arguments are not used",
                          stacklevel=2)

        if data_specs is None:
            data_specs = self._iter_data_specs

        # If there is a view_converter, we have to use it to convert
        # the stored data for "features" into one that the iterator
        # can return.
        space, source = data_specs
        if isinstance(space, CompositeSpace):
            sub_spaces = space.components
            sub_sources = source
        else:
            sub_spaces = (space,)
            sub_sources = (source,)

        convert = []
        for sp, src in safe_zip(sub_spaces, sub_sources):
            if src == 'features' and \
               getattr(self, 'view_converter', None) is not None:
                conv_fn = (lambda batch, self=self, space=sp:
                           self.view_converter.get_formatted_batch(batch,
                                                                   space))
            else:
                conv_fn = None

            convert.append(conv_fn)

        # TODO: Refactor
        if mode is None:
            if hasattr(self, '_iter_subset_class'):
                mode = self._iter_subset_class
            else:
                raise ValueError('iteration mode not provided and no default '
                                 'mode set for %s' % str(self))
        else:
            mode = resolve_iterator_class(mode)

        if batch_size is None:
            batch_size = getattr(self, '_iter_batch_size', None)
        if num_batches is None:
            num_batches = getattr(self, '_iter_num_batches', None)
        if rng is None and mode.stochastic:
            rng = self.rng
        return FiniteDatasetIterator(self,
                                     mode(self.X.shape[0],
                                          batch_size,
                                          num_batches,
                                          rng),
                                     data_specs=data_specs,
                                     return_tuple=return_tuple,
                                     convert=convert)

    def __iter__(self):
        """
        .. todo::

            WRITEME
        """
        return self

    def next(self):
        """
        .. todo::

            WRITEME
        """
        indx = self.subset_iterator.next()
        try:
            mini_batch = self.X[indx]
        except IndexError, e:
            raise ValueError("Index out of range"+str(e))
            # the ind of minibatch goes beyond the boundary
        return mini_batch

    def get_data_specs(self):
        """
        Returns the data_specs specifying how the data is internally stored.

        This is the format the data returned by `self.get_data()` will be.
        """
        return self.data_specs

    def get_data(self):
        """
        Returns
        -------
        data : numpy matrix or 2-tuple of matrices
            Returns all the data, as it is internally stored.
            The definition and format of these data are described in
            `self.get_data_specs()`.
        """
        if self.y is None:
            return self.X
        else:
            return (self.X, self.y)

########NEW FILE########
__FILENAME__ = stl10
"""
.. todo::

    WRITEME
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"
import numpy as np
from pylearn2.datasets import dense_design_matrix
from pylearn2.utils.serial import load

class STL10(dense_design_matrix.DenseDesignMatrix):
    """
    The STL-10 dataset

    Adam Coates, Honglak Lee, Andrew Y. Ng An Analysis of Single Layer
    Networks in Unsupervised Feature Learning AISTATS, 2011

    http://www.stanford.edu/~acoates//stl10/

    When reporting results on this dataset, you are meant to use a somewhat
    unusal evaluation procedure.

    Use STL10(which_set='train') to load the training set. Then restrict the
    training set to one of the ten folds using the restrict function below. You
    must then train only on the data from that fold.

    For the test set, report the average test set performance over the ten
    trials obtained by training on each of the ten folds.

    The folds here do not define the splits you should use for cross
    validation. You are free to make your own split within each fold.

    Parameters
    ----------
    which_set : WRITEME
    center : WRITEME
    example_range : WRITEME
    """

    def __init__(self, which_set, center = False, example_range = None):
        """
        .. todo::

            WRITEME
        """
        if which_set == 'train':
            train = load('${PYLEARN2_DATA_PATH}/stl10/stl10_matlab/train.mat')

            #Load the class names
            self.class_names = [array[0].encode('utf-8') for array in train['class_names'][0] ]

            #Load the fold indices
            fold_indices = train['fold_indices']
            assert fold_indices.shape == (1,10)
            self.fold_indices = np.zeros((10,1000),dtype='uint16')
            for i in xrange(10):
                indices = fold_indices[0,i]
                assert indices.shape == (1000,1)
                assert indices.dtype == 'uint16'
                self.fold_indices[i,:] = indices[:,0]

            #The data is stored as uint8
            #If we leave it as uint8, it will cause the CAE to silently fail
            #since theano will treat derivatives wrt X as 0
            X = np.cast['float32'](train['X'])

            assert X.shape == (5000, 96*96*3)

            if example_range is not None:
                X = X[example_range[0]:example_range[1],:]

            #this is uint8
            y = train['y'][:,0]
            assert y.shape == (5000,)
        elif which_set == 'test':
            test = load('${PYLEARN2_DATA_PATH}/stl10_matlab/test.mat')

            #Load the class names
            self.class_names = [array[0].encode('utf-8') for array in test['class_names'][0] ]

            #The data is stored as uint8
            #If we leave it as uint8, it will cause the CAE to silently fail
            #since theano will treat derivatives wrt X as 0

            X = np.cast['float32'](test['X'])
            assert X.shape == (8000, 96*96*3)

            if example_range is not None:
                X = X[example_range[0]:example_range[1],:]

            #this is uint8
            y = test['y'][:,0]
            assert y.shape == (8000,)

        elif which_set == 'unlabeled':
            unlabeled = load('${PYLEARN2_DATA_PATH}/stl10_matlab/unlabeled.mat')

            X =  unlabeled['X']

            #this file is stored in HDF format, which transposes everything
            assert X.shape == (96*96*3, 100000)
            assert X.dtype == 'uint8'

            if example_range is None:
                X = X.value
            else:
                X = X.value[:,example_range[0]:example_range[1]]
            X = np.cast['float32'](X.T)

            unlabeled.close()

            y = None
        else:
            raise ValueError('"'+which_set+'" is not an STL10 dataset. '
                    'Recognized values are "train", "test", and "unlabeled".')
        if center:
            X -= 127.5

        view_converter = dense_design_matrix.DefaultViewConverter((96,96,3))

        super(STL10,self).__init__(X = X, y = y, view_converter = view_converter)


        for i in xrange(self.X.shape[0]):
            mat = X[i:i+1,:]
            topo = self.get_topological_view(mat)
            for j in xrange(topo.shape[3]):
                temp = topo[0,:,:,j].T.copy()
                topo[0,:,:,j] = temp
            mat = self.get_design_matrix(topo)
            X[i:i+1,:] = mat

        assert not np.any(np.isnan(self.X))


def restrict(dataset, fold):
    """Restricts the dataset to use the specified fold."""
    fold_indices = dataset.fold_indices
    assert fold_indices.shape == (10, 1000)

    idxs = fold_indices[fold, :] - 1

    dataset.X = dataset.X[idxs, :].copy()
    assert dataset.X.shape[0] == 1000

    dataset.y = dataset.y[idxs, ...].copy()
    assert dataset.y.shape[0] == 1000

    return dataset

########NEW FILE########
__FILENAME__ = svhn
"""
.. todo::

    WRITEME
"""
import os
import gc
import warnings
try:
    import tables
except ImportError:
    warnings.warn("Couldn't import tables, so far SVHN is "
            "only supported with PyTables")
import numpy
from theano import config
from pylearn2.datasets import dense_design_matrix
from pylearn2.utils.serial import load
from pylearn2.utils.string_utils import preprocess
from pylearn2.utils.rng import make_np_rng


class SVHN(dense_design_matrix.DenseDesignMatrixPyTables):
    """
    Only for faster access there is a copy of hdf5 file in PYLEARN2_DATA_PATH
    but it mean to be only readable.  If you wish to modify the data, you
    should pass a local copy to the path argument.

    Parameters
    ----------
    which_set : WRITEME
    path : WRITEME
    center : WRITEME
    scale : WRITEME
    start : WRITEME
    stop : WRITEME
    axes : WRITEME
    preprocessor : WRITEME
    """

    mapper = {'train': 0, 'test': 1, 'extra': 2, 'train_all': 3,
                'splitted_train': 4, 'valid': 5}

    data_path = '${PYLEARN2_DATA_PATH}/SVHN/format2/'

    def __init__(self, which_set, path = None, center = False, scale = False,
            start = None, stop = None, axes = ('b', 0, 1, 'c'),
            preprocessor = None):

        assert which_set in self.mapper.keys()

        self.__dict__.update(locals())
        del self.self

        if path is None:
            path = '${PYLEARN2_DATA_PATH}/SVHN/format2/'
            mode = 'r'
        else:
            mode = 'r+'
            warnings.warn("Because path is not same as PYLEARN2_DATA_PATH "
                          "be aware that data might have been "
                          "modified or pre-processed.")

        if mode == 'r' and (scale or center or (start != None) or
                        (stop != None)):
            raise ValueError("Only for speed there is a copy of hdf5 " +\
                    "file in PYLEARN2_DATA_PATH but it meant to be only " +\
                    "readable. If you wish to modify the data, you should " +\
                    "pass a local copy to the path argument.")

        # load data
        path = preprocess(path)
        file_n = "{0}_32x32.h5".format(os.path.join(path, "h5", which_set))
        if os.path.isfile(file_n):
            make_new = False
        else:
            make_new = True
            warnings.warn("Over riding existing file: {0}".format(file_n))

        # if hdf5 file does not exist make them
        if make_new:
            self.filters = tables.Filters(complib='blosc', complevel=5)
            self.make_data(which_set, path)

        self.h5file = tables.openFile(file_n, mode = mode)
        data = self.h5file.getNode('/', "Data")

        if start != None or stop != None:
            self.h5file, data = self.resize(self.h5file, start, stop)

        # rescale or center if permitted
        if center and scale:
            data.X[:] -= 127.5
            data.X[:] /= 127.5
        elif center:
            data.X[:] -= 127.5
        elif scale:
            data.X[:] /= 255.

        view_converter = dense_design_matrix.DefaultViewConverter((32, 32, 3),
                                                                        axes)
        super(SVHN, self).__init__(X = data.X, y = data.y,
                                    view_converter = view_converter)

        if preprocessor:
            if which_set in ['train', 'train_all', 'splitted_train']:
                can_fit = True
            preprocessor.apply(self, can_fit)

        self.h5file.flush()


    def get_test_set(self):
        """
        .. todo::

            WRITEME
        """
        return SVHN(which_set = 'test', path = self.path,
                    center = self.center, scale = self.scale,
                    start = self.start, stop = self.stop,
                    axes = self.axes, preprocessor = self.preprocessor)

    def make_data(self, which_set, path, shuffle = True):
        """
        .. todo::

            WRITEME
        """
        sizes = {'train': 73257, 'test': 26032, 'extra': 531131,
                'train_all': 604388, 'valid': 6000, 'splitted_train' : 598388}
        image_size = 32 * 32 * 3
        h_file_n = "{0}_32x32.h5".format(os.path.join(path, "h5", which_set))
        h5file, node = self.init_hdf5(h_file_n, ([sizes[which_set],
                            image_size], [sizes[which_set], 10]))

        # For consistency between experiments better to make new random stream
        rng = make_np_rng(None, 322, which_method="shuffle")

        def design_matrix_view(data_x, data_y):
            """reshape data_x to deisng matrix view
            and data_y to one_hot
            """

            data_x = numpy.transpose(data_x, axes = [3, 2, 0, 1])
            data_x = data_x.reshape((data_x.shape[0], 32 * 32 * 3))
            # TODO assuming one_hot as default for now
            one_hot = numpy.zeros((data_y.shape[0], 10), dtype = config.floatX)
            for i in xrange(data_y.shape[0]):
                one_hot[i, data_y[i] - 1] = 1.
            return data_x, one_hot

        def load_data(path):
            "Loads data from mat files"

            data = load(path)
            data_x = numpy.cast[config.floatX](data['X'])
            data_y = data['y']
            del data
            gc.collect()
            return design_matrix_view(data_x, data_y)

        def split_train_valid(path, num_valid_train = 400,
                                    num_valid_extra = 200):
            """
            Extract number of class balanced samples from train and extra
            sets for validation, and regard the remaining as new train set.

            Parameters
            ----------
            num_valid_train : int, optional
                Number of samples per class from train
            num_valid_extra : int, optional
                Number of samples per class from extra
            """

            # load difficult train
            data = load("{0}train_32x32.mat".format(SVHN.data_path))
            valid_index = []
            for i in xrange(1, 11):
                index = numpy.nonzero(data['y'] == i)[0]
                index.flags.writeable = 1
                rng.shuffle(index)
                valid_index.append(index[:num_valid_train])

            valid_index = set(numpy.concatenate(valid_index))
            train_index = set(numpy.arange(data['X'].shape[3])) - valid_index
            valid_index = list(valid_index)
            train_index = list(train_index)

            train_x = data['X'][:, :, :, train_index]
            train_y = data['y'][train_index, :]
            valid_x = data['X'][:, :, :, valid_index]
            valid_y = data['y'][valid_index, :]

            train_size = data['X'].shape[3]
            assert train_x.shape[3] == train_size - num_valid_train * 10
            assert train_y.shape[0] == train_size - num_valid_train * 10
            assert valid_x.shape[3] == num_valid_train * 10
            assert valid_y.shape[0] == num_valid_train * 10
            del data
            gc.collect()

            # load extra train
            data = load("{0}extra_32x32.mat".format(SVHN.data_path))
            valid_index = []
            for i in xrange(1, 11):
                index = numpy.nonzero(data['y'] == i)[0]
                index.flags.writeable = 1
                rng.shuffle(index)
                valid_index.append(index[:num_valid_extra])

            valid_index = set(numpy.concatenate(valid_index))
            train_index = set(numpy.arange(data['X'].shape[3])) - valid_index
            valid_index = list(valid_index)
            train_index = list(train_index)

            train_x = numpy.concatenate((train_x,
                                data['X'][:, :, :, train_index]), axis = 3)
            train_y = numpy.concatenate((train_y, data['y'][train_index, :]))
            valid_x = numpy.concatenate((valid_x,
                                data['X'][:, :, :, valid_index]), axis = 3)
            valid_y = numpy.concatenate((valid_y, data['y'][valid_index, :]))

            extra_size = data['X'].shape[3]
            sizes['valid'] = (num_valid_train + num_valid_extra) * 10
            sizes['splitted_train'] = train_size + extra_size - sizes['valid']
            assert train_x.shape[3] == sizes['splitted_train']
            assert train_y.shape[0] == sizes['splitted_train']
            assert valid_x.shape[3] == sizes['valid']
            assert valid_y.shape[0] == sizes['valid']
            del data
            gc.collect()

            train_x = numpy.cast[config.floatX](train_x)
            valid_x = numpy.cast[config.floatX](valid_x)

            return design_matrix_view(train_x, train_y),\
                    design_matrix_view(valid_x, valid_y)

        # The original splits
        if which_set in ['train', 'test']:
            data_x, data_y = load_data("{0}{1}_32x32.mat".format(path,
                                                         which_set))

        # Train valid splits
        elif which_set in ['splitted_train', 'valid']:
            train_data, valid_data = split_train_valid(path)
            if which_set == 'splitted_train':
                data_x, data_y = train_data
            else:
                data_x, data_y = valid_data
                del train_data

        # extra data
        elif which_set in ['train_all', 'extra']:
            data_x, data_y = load_data("{0}extra_32x32.mat".format(path))
            if which_set == 'train_all':
                train_x, train_y = load_data("{0}train_32x32.mat".format(path))
                data_x = numpy.concatenate((data_x, train_x))
                data_y = numpy.concatenate((data_y, data_y))

        if shuffle:
            index = range(data_x.shape[0])
            rng.shuffle(index)
            data_x = data_x[index, :]
            data_y = data_y[index, :]

        assert data_x.shape[0] == sizes[which_set]
        assert data_y.shape[0] == sizes[which_set]

        SVHN.fill_hdf5(h5file, data_x, data_y, node)
        h5file.close()


class SVHN_On_Memory(dense_design_matrix.DenseDesignMatrix):
    """
    A version of SVHN dataset that loads everything into the memory instead of
    using pytables.

    Parameters
    ----------
    which_set : WRITEME
    center : WRITEME
    scale : WRITEME
    start : WRITEME
    stop : WRITEME
    axes : WRITEME
    preprocessor : WRITEME
    """

    mapper = {'train': 0, 'test': 1, 'extra': 2, 'train_all': 3,
                'splitted_train': 4, 'valid': 5}

    def __init__(self, which_set, center = False, scale = False,
            start = None, stop = None, axes = ('b', 0, 1, 'c'),
            preprocessor = None):

        assert which_set in self.mapper.keys()

        self.__dict__.update(locals())
        del self.self

        path = '${PYLEARN2_DATA_PATH}/SVHN/format2/'

        # load data
        path = preprocess(path)
        data_x, data_y = self.make_data(which_set, path)

        # rescale or center if permitted
        if center and scale:
            data_x -= 127.5
            data_x /= 127.5
        elif center:
            data_x -= 127.5
        elif scale:
            data_x /= 255.

        view_converter = dense_design_matrix.DefaultViewConverter((32, 32, 3),
                                                                        axes)
        super(SVHN_On_Memory, self).__init__(X = data_x, y = data_y,
                                    view_converter = view_converter)

        if preprocessor:
            if which_set in ['train', 'train_all', 'splitted_train']:
                can_fit = True
            else:
                can_fit = False
            preprocessor.apply(self, can_fit)

        del data_x, data_y
        gc.collect()

    def get_test_set(self):
        """
        .. todo::

            WRITEME
        """
        return SVHN_On_Memory(which_set = 'test', path = self.path,
                    center = self.center, scale = self.scale,
                    start = self.start, stop = self.stop,
                    axes = self.axes, preprocessor = self.preprocessor)

    def make_data(self, which_set, path, shuffle = True):
        """
        .. todo::

            WRITEME
        """
        sizes = {'train': 73257, 'test': 26032, 'extra': 531131,
                'train_all': 604388, 'valid': 6000, 'splitted_train' : 598388}
        image_size = 32 * 32 * 3

        # For consistency between experiments better to make new random stream
        rng = make_np_rng(None, 322, which_method="shuffle")

        def design_matrix_view(data_x, data_y):
            """reshape data_x to deisng matrix view
            and data_y to one_hot
            """

            data_x = numpy.transpose(data_x, axes = [3, 2, 0, 1])
            data_x = data_x.reshape((data_x.shape[0], 32 * 32 * 3))
            # TODO assuming one_hot as default for now
            one_hot = numpy.zeros((data_y.shape[0], 10), dtype = config.floatX)
            for i in xrange(data_y.shape[0]):
                one_hot[i, data_y[i] - 1] = 1.
            return data_x, one_hot

        def load_data(path):
            "Loads data from mat files"

            data = load(path)
            data_x = numpy.cast[config.floatX](data['X'])
            import ipdb
            ipdb.set_trace()
            data_y = data['y']
            del data
            gc.collect()
            return design_matrix_view(data_x, data_y)

        def split_train_valid(path, num_valid_train = 400,
                                    num_valid_extra = 200):
            """
            Extract number of class balanced samples from train and extra
            sets for validation, and regard the remaining as new train set.

            Parameters
            ----------
            num_valid_train : int, optional
                Number of samples per class from train
            num_valid_extra : int, optional
                Number of samples per class from extra
            """

            # load difficult train
            data = load("{0}train_32x32.mat".format(path))
            valid_index = []
            for i in xrange(1, 11):
                index = numpy.nonzero(data['y'] == i)[0]
                index.flags.writeable = 1
                rng.shuffle(index)
                valid_index.append(index[:num_valid_train])

            valid_index = set(numpy.concatenate(valid_index))
            train_index = set(numpy.arange(data['X'].shape[3])) - valid_index
            valid_index = list(valid_index)
            train_index = list(train_index)

            train_x = data['X'][:, :, :, train_index]
            train_y = data['y'][train_index, :]
            valid_x = data['X'][:, :, :, valid_index]
            valid_y = data['y'][valid_index, :]

            train_size = data['X'].shape[3]
            assert train_x.shape[3] == train_size - num_valid_train * 10
            assert train_y.shape[0] == train_size - num_valid_train * 10
            assert valid_x.shape[3] == num_valid_train * 10
            assert valid_y.shape[0] == num_valid_train * 10
            del data
            gc.collect()

            # load extra train
            data = load("{0}extra_32x32.mat".format(path))
            valid_index = []
            for i in xrange(1, 11):
                index = numpy.nonzero(data['y'] == i)[0]
                index.flags.writeable = 1
                rng.shuffle(index)
                valid_index.append(index[:num_valid_extra])

            valid_index = set(numpy.concatenate(valid_index))
            train_index = set(numpy.arange(data['X'].shape[3])) - valid_index
            valid_index = list(valid_index)
            train_index = list(train_index)

            train_x = numpy.concatenate((train_x,
                                data['X'][:, :, :, train_index]), axis = 3)
            train_y = numpy.concatenate((train_y, data['y'][train_index, :]))
            valid_x = numpy.concatenate((valid_x,
                                data['X'][:, :, :, valid_index]), axis = 3)
            valid_y = numpy.concatenate((valid_y, data['y'][valid_index, :]))

            extra_size = data['X'].shape[3]
            sizes['valid'] = (num_valid_train + num_valid_extra) * 10
            sizes['splitted_train'] = train_size + extra_size - sizes['valid']
            assert train_x.shape[3] == sizes['splitted_train']
            assert train_y.shape[0] == sizes['splitted_train']
            assert valid_x.shape[3] == sizes['valid']
            assert valid_y.shape[0] == sizes['valid']
            del data
            gc.collect()

            train_x = numpy.cast[config.floatX](train_x)
            valid_x = numpy.cast[config.floatX](valid_x)
            return design_matrix_view(train_x, train_y),\
                    design_matrix_view(valid_x, valid_y)

        # The original splits
        if which_set in ['train', 'test']:
            data_x, data_y = load_data("{0}{1}_32x32.mat".format(path,
                                                         which_set))

        # Train valid splits
        elif which_set in ['splitted_train', 'valid']:
            train_data, valid_data = split_train_valid(path)
            if which_set == 'splitted_train':
                data_x, data_y = train_data
            else:
                data_x, data_y = valid_data
                del train_data

        # extra data
        elif which_set in ['train_all', 'extra']:
            data_x, data_y = load_data("{0}extra_32x32.mat".format(path))
            if which_set == 'train_all':
                train_x, train_y = load_data("{0}train_32x32.mat".format(path))
                data_x = numpy.concatenate((data_x, train_x))
                data_y = numpy.concatenate((data_y, data_y))

        if shuffle:
            index = range(data_x.shape[0])
            rng.shuffle(index)
            data_x = data_x[index, :]
            data_y = data_y[index, :]

        assert data_x.shape[0] == sizes[which_set]
        assert data_y.shape[0] == sizes[which_set]

        return data_x, data_y

########NEW FILE########
__FILENAME__ = test_cifar10
import unittest
import numpy as np
from pylearn2.datasets.cifar10 import CIFAR10
from pylearn2.space import Conv2DSpace
from pylearn2.testing.skip import skip_if_no_data


class TestCIFAR10(unittest.TestCase):
    def setUp(self):
        skip_if_no_data()
        self.train = CIFAR10(which_set = 'train')
        self.test = CIFAR10(which_set = 'test')

    def test_topo(self):
        """Tests that a topological batch has 4 dimensions"""
        topo = self.train.get_batch_topo(1)
        assert topo.ndim == 4

    def test_topo_c01b(self):
        """
        Tests that a topological batch with axes ('c',0,1,'b')
        can be dimshuffled back to match the standard ('b',0,1,'c')
        format.
        """
        batch_size = 100
        c01b_test = CIFAR10(which_set='test', axes=('c', 0, 1, 'b'))
        c01b_X = c01b_test.X[0:batch_size,:]
        c01b = c01b_test.get_topological_view(c01b_X)
        assert c01b.shape == (3, 32, 32, batch_size)
        b01c = c01b.transpose(3,1,2,0)
        b01c_X = self.test.X[0:batch_size,:]
        assert c01b_X.shape == b01c_X.shape
        assert np.all(c01b_X == b01c_X)
        b01c_direct = self.test.get_topological_view(b01c_X)
        assert b01c_direct.shape == b01c.shape
        assert np.all(b01c_direct == b01c)

    def test_iterator(self):
        # Tests that batches returned by an iterator with topological
        # data_specs are the same as the ones returned by calling
        # get_topological_view on the dataset with the corresponding order
        batch_size = 100
        b01c_X = self.test.X[0:batch_size, :]
        b01c_topo = self.test.get_topological_view(b01c_X)
        b01c_b01c_it = self.test.iterator(
            mode='sequential',
            batch_size=batch_size,
            data_specs=(Conv2DSpace(shape=(32, 32),
                                    num_channels=3,
                                    axes=('b', 0, 1, 'c')),
                        'features'))
        b01c_b01c = b01c_b01c_it.next()
        assert np.all(b01c_topo == b01c_b01c)

        c01b_test = CIFAR10(which_set='test', axes=('c', 0, 1, 'b'))
        c01b_X = c01b_test.X[0:batch_size, :]
        c01b_topo = c01b_test.get_topological_view(c01b_X)
        c01b_c01b_it = c01b_test.iterator(
            mode='sequential',
            batch_size=batch_size,
            data_specs=(Conv2DSpace(shape=(32, 32),
                                    num_channels=3,
                                    axes=('c', 0, 1, 'b')),
                        'features'))
        c01b_c01b = c01b_c01b_it.next()
        assert np.all(c01b_topo == c01b_c01b)

        # Also check that samples from iterators with the same data_specs
        # with Conv2DSpace do not depend on the axes of the dataset
        b01c_c01b_it = self.test.iterator(
            mode='sequential',
            batch_size=batch_size,
            data_specs=(Conv2DSpace(shape=(32, 32),
                                    num_channels=3,
                                    axes=('c', 0, 1, 'b')),
                        'features'))
        b01c_c01b = b01c_c01b_it.next()
        assert np.all(b01c_c01b == c01b_c01b)

        c01b_b01c_it = c01b_test.iterator(
            mode='sequential',
            batch_size=batch_size,
            data_specs=(Conv2DSpace(shape=(32, 32),
                                    num_channels=3,
                                    axes=('b', 0, 1, 'c')),
                        'features'))
        c01b_b01c = c01b_b01c_it.next()
        assert np.all(c01b_b01c == b01c_b01c)

########NEW FILE########
__FILENAME__ = test_csv_dataset
import os
import pylearn2
from pylearn2.datasets.csv_dataset import CSVDataset
import numpy as np

def test_loading():
    test_path = os.path.join(pylearn2.__path__[0], 'datasets', 'tests', 'test.csv') 
    d = CSVDataset(path = test_path, expect_headers = False)
    assert(np.array_equal(d.X, np.array([[1., 2., 3.], [4., 5., 6.]])))
    assert(np.array_equal(d.y, np.array([0., 1.])))
    
    


########NEW FILE########
__FILENAME__ = test_dense_design_matrix
import numpy as np

from pylearn2.datasets.dense_design_matrix import DenseDesignMatrix
from pylearn2.datasets.dense_design_matrix import DenseDesignMatrixPyTables
from pylearn2.datasets.dense_design_matrix import DefaultViewConverter
from pylearn2.utils import serial


def test_init_with_X_or_topo():
    #tests that constructing with topo_view works
    #tests that construction with design matrix works
    #tests that conversion from topo_view to design matrix and back works
    #tests that conversion the other way works too
    rng = np.random.RandomState([1,2,3])
    topo_view = rng.randn(5,2,2,3)
    d1 = DenseDesignMatrix(topo_view = topo_view)
    X = d1.get_design_matrix()
    d2 = DenseDesignMatrix(X = X, view_converter = d1.view_converter)
    topo_view_2 = d2.get_topological_view()
    assert np.allclose(topo_view,topo_view_2)
    X = rng.randn(*X.shape)
    topo_view_3 = d2.get_topological_view(X)
    X2 = d2.get_design_matrix(topo_view_3)
    assert np.allclose(X,X2)

def test_convert_to_one_hot():
    rng = np.random.RandomState([2013, 11, 14])
    m = 11
    d = DenseDesignMatrix(
            X=rng.randn(m, 4),
            y=rng.randint(low=0, high=10, size=(m,)))
    d.convert_to_one_hot()

def test_init_with_vc():
    rng = np.random.RandomState([4,5,6])
    d = DenseDesignMatrix(
            X=rng.randn(12, 5),
            view_converter = DefaultViewConverter([1,2,3]))

def get_rnd_design_matrix():
    rng = np.random.RandomState([1,2,3])
    topo_view = rng.randn(10,2,2,3)
    d1 = DenseDesignMatrix(topo_view = topo_view)
    return d1

def test_split_datasets():
    #Test the split dataset function.
    ddm = get_rnd_design_matrix()
    (train, valid) = ddm.split_dataset_holdout(train_prop=0.5)
    assert valid.shape[0] == np.ceil(ddm.num_examples * 0.5)
    assert train.shape[0] == (ddm.num_examples - valid.shape[0])

def test_split_nfold_datasets():
    #Load and create ddm from cifar100
    ddm = get_rnd_design_matrix()
    folds = ddm.split_dataset_nfolds(10)
    assert folds[0].shape[0] == np.ceil(ddm.num_examples / 10)

def test_pytables():
    """
    tests wether DenseDesignMatrixPyTables can be loaded and
    initialize iterator
    """
    # TODO more through test

    x = np.ones((2, 3))
    y = np.ones(2)
    ds = DenseDesignMatrixPyTables(X = x, y = y)

    it = ds.iterator(mode = 'sequential', batch_size = 1)
    it.next()

########NEW FILE########
__FILENAME__ = test_four_regions
import numpy as np
from pylearn2.datasets.four_regions import FourRegions


def test_four_regions():
    dataset = FourRegions(5000)
    X = dataset.get_design_matrix()
    np.testing.assert_(((X < 1.) & (X > -1.)).all())
    y = dataset.get_targets()
    np.testing.assert_equal(np.unique(y), [0, 1, 2, 3])

########NEW FILE########
__FILENAME__ = test_hdf5
from pylearn2.config import yaml_parse
from pylearn2.testing.skip import skip_if_no_data, skip_if_no_h5py
import unittest
import os


class TestHDF5Dataset(unittest.TestCase):
    """Trains the model described in scripts/papers/maxout/mnist_pi.yaml
    using HDF5 datasets and a max_epochs termination criterion."""
    def setUp(self):
        skip_if_no_h5py()
        import h5py
        skip_if_no_data()
        from pylearn2.datasets.mnist import MNIST

        # save MNIST data to HDF5
        train = MNIST(which_set='train', one_hot=1, start=0, stop=100)
        for name, dataset in [('train', train)]:
            with h5py.File("{}.h5".format(name), "w") as f:
                f.create_dataset('X', data=dataset.get_design_matrix())
                f.create_dataset('topo_view',
                                 data=dataset.get_topological_view())
                f.create_dataset('y', data=dataset.get_targets())

        # instantiate Train object
        self.train = yaml_parse.load(trainer_yaml)

    def test_hdf5(self):
        """Run trainer main loop."""
        self.train.main_loop()

    def tearDown(self):
        os.remove("train.h5")

# trainer is a modified version of scripts/papers/maxout/mnist_pi.yaml
trainer_yaml = """
!obj:pylearn2.train.Train {
    dataset: &train !obj:pylearn2.datasets.hdf5.HDF5Dataset {
        filename: 'train.h5',
        X: 'X',
        y: 'y',
    },
    model: !obj:pylearn2.models.mlp.MLP {
        layers: [
                 !obj:pylearn2.models.maxout.Maxout {
                     layer_name: 'h0',
                     num_units: 10,
                     num_pieces: 2,
                     irange: .005,
                     max_col_norm: 1.9365,
                 },
                 !obj:pylearn2.models.mlp.Softmax {
                     max_col_norm: 1.9365,
                     layer_name: 'y',
                     n_classes: 10,
                     irange: .005
                 }
                ],
        nvis: 784,
    },
    algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {
        batch_size: 100,
        learning_rate: .1,
        learning_rule:
            !obj:pylearn2.training_algorithms.learning_rule.Momentum {
                init_momentum: .5,
            },
        monitoring_dataset:
            {
                'train' : *train,
            },
        cost: !obj:pylearn2.costs.mlp.dropout.Dropout {
            input_include_probs: { 'h0' : .8 },
            input_scales: { 'h0': 1. }
        },
        termination_criterion:
            !obj:pylearn2.termination_criteria.EpochCounter {
                max_epochs: 1,
            },
    },
}
"""

########NEW FILE########
__FILENAME__ = test_icml07
import numpy as np
import unittest

from pylearn2.testing.skip import skip_if_no_data
import pylearn2.datasets.icml07 as icml07


# Basic tests to see if data is loadable
def test_MNIST_rot_back():
    skip_if_no_data()
    data = icml07.MNIST_rotated_background(which_set='train')
    data = icml07.MNIST_rotated_background(which_set='valid')
    data = icml07.MNIST_rotated_background(which_set='test')

def test_Convex():
    skip_if_no_data()
    data = icml07.Convex(which_set='train')
    data = icml07.Convex(which_set='valid')
    data = icml07.Convex(which_set='test')

def test_Rectangles():
    skip_if_no_data()
    data = icml07.Rectangles(which_set='train')
    data = icml07.Rectangles(which_set='valid')
    data = icml07.Rectangles(which_set='test')

def test_RectanglesImage():
    skip_if_no_data()
    data = icml07.RectanglesImage(which_set='train')
    data = icml07.RectanglesImage(which_set='valid')
    data = icml07.RectanglesImage(which_set='test')

# Test features
def test_split():
    skip_if_no_data()
    n_train=100
    n_valid=200
    n_test=300

    data = icml07.MNIST_rotated_background(which_set='train', split=(n_train, n_valid, n_test))
    assert data.X.shape[0] == n_train, "Unexpected size of train set"
    assert data.y.shape[0] == n_train, "Unexpected size of train set"

    data = icml07.MNIST_rotated_background(which_set='valid', split=(n_train, n_valid, n_test))
    assert data.X.shape[0] == n_valid, "Unexpected size of validation set"
    assert data.y.shape[0] == n_valid, "Unexpected size of validation set"

    data = icml07.MNIST_rotated_background(which_set='test', split=(n_train, n_valid, n_test))
    assert data.X.shape[0] == n_test, "Unexpected size of test set"
    assert data.y.shape[0] == n_test, "Unexpected size of test set"

def test_one_hot():
    skip_if_no_data()
    data = icml07.MNIST_rotated_background(which_set='train', one_hot=True, split=(100,100,100))
    assert data.y.shape[1] == 10   # MNITS hast 10 classes

    data = icml07.Rectangles(which_set='train', one_hot=True, split=(100,100,100))
    assert data.y.shape[1] == 2   # Two classes


########NEW FILE########
__FILENAME__ = test_imports

def test_mnist_imports():

    from pylearn2.datasets.mnist import MNIST
    MNIST.shut_up_the_syntax_highlighting = 1

########NEW FILE########
__FILENAME__ = test_mnist
from pylearn2.datasets.mnist import MNIST
from pylearn2.space import IndexSpace, VectorSpace
import unittest
from pylearn2.testing.skip import skip_if_no_data
import numpy as np

class TestMNIST(unittest.TestCase):
    def setUp(self):
        skip_if_no_data()
        self.train = MNIST(which_set = 'train')
        self.test = MNIST(which_set = 'test')

    def test_range(self):
        """Tests that the data spans [0,1]"""
        for X in [self.train.X, self.test.X ]:
            assert X.min() == 0.0
            assert X.max() == 1.0

    def test_topo(self):
        """Tests that a topological batch has 4 dimensions"""
        topo = self.train.get_batch_topo(1)
        assert topo.ndim == 4

    def test_topo_c01b(self):
        """
        Tests that a topological batch with axes ('c',0,1,'b')
        can be dimshuffled back to match the standard ('b',0,1,'c')
        format.
        """
        batch_size = 100
        c01b_test = MNIST(which_set='test', axes=('c', 0, 1, 'b'))
        c01b_X = c01b_test.X[0:batch_size,:]
        c01b = c01b_test.get_topological_view(c01b_X)
        assert c01b.shape == (1, 28, 28, batch_size)
        b01c = c01b.transpose(3,1,2,0)
        b01c_X = self.test.X[0:batch_size,:]
        assert c01b_X.shape == b01c_X.shape
        assert np.all(c01b_X == b01c_X)
        b01c_direct = self.test.get_topological_view(b01c_X)
        assert b01c_direct.shape == b01c.shape
        assert np.all(b01c_direct == b01c)

    def test_y_index_space(self):
        """
        Tests that requesting the targets to be in IndexSpace and iterating
        over them works
        """
        data_specs = (IndexSpace(max_labels=10, dim=1), 'targets')
        it = self.test.iterator(mode='sequential',
                                data_specs=data_specs,
                                batch_size=100)
        for y in it:
            pass

    def test_y_vector_space(self):
        """
        Tests that requesting the targets to be in VectorSpace and iterating
        over them works
        """
        data_specs = (VectorSpace(dim=10), 'targets')
        it = self.test.iterator(mode='sequential',
                                data_specs=data_specs,
                                batch_size=100)
        for y in it:
            pass

########NEW FILE########
__FILENAME__ = test_norb
"""
Unit tests for ./norb.py
"""

import unittest
import numpy
from pylearn2.datasets.norb import SmallNORB
from pylearn2.datasets.norb_small import FoveatedNORB
from pylearn2.utils import safe_zip
from pylearn2.testing.skip import skip_if_no_data


class TestNORB(unittest.TestCase):
    def setUp(self):
        skip_if_no_data()

    def test_foveated_norb(self):

        # Test that the FoveatedNORB class can be instantiated
        norb_train = FoveatedNORB(which_set="train",
                                  scale=1, restrict_instances=[4, 6, 7, 8],
                                  one_hot=1)

    def test_get_topological_view(self):
        # This is just to lower the memory usage. Otherwise, the
        # buildbot use close to 10G of ram.
        norb = SmallNORB('train', stop=1000)

        # Get a topological view as a single "(b, s, 0 1, c)" tensor.
        topo_tensor = norb.get_topological_view(single_tensor=True)
        shape = (norb.X.shape[0], 2) + SmallNORB.original_image_shape + (1, )
        expected_topo_tensor = norb.X.reshape(shape)
        # We loop to lower the peak memory usage
        for i in range(topo_tensor.shape[0]):
            assert numpy.all(topo_tensor[i] == expected_topo_tensor[i])

        # Get a topological view as two "(b, 0, 1, c)" tensors
        topo_tensors = norb.get_topological_view(single_tensor=False)
        expected_topo_tensors = tuple(expected_topo_tensor[:, i, ...]
                                      for i in range(2))

        for topo_tensor, expected_topo_tensor in safe_zip(
                topo_tensors, expected_topo_tensors):
            assert numpy.all(topo_tensor == expected_topo_tensor)

########NEW FILE########
__FILENAME__ = test_npy_npz
from pylearn2.datasets.npy_npz import NpyDataset, NpzDataset
import unittest
from pylearn2.testing.skip import skip_if_no_data
import numpy as np
import os


def test_npy_npz():
    skip_if_no_data()
    arr = np.array([[3,4,5],[4,5,6]])
    np.save('test.npy', arr)
    np.savez('test.npz', arr)
    npy = NpyDataset(file='test.npy')
    npy._deferred_load()
    npz = NpzDataset(file='test.npz', key='arr_0')
    assert np.all(npy.X == npz.X)
    os.remove('test.npy')
    os.remove('test.npz')

########NEW FILE########
__FILENAME__ = test_preprocessing
"""
Unit tests for ./preprocessing.py
"""

import numpy as np

from theano import config
import theano

from pylearn2.utils import as_floatX
from pylearn2.datasets import dense_design_matrix
from pylearn2.datasets.dense_design_matrix import DenseDesignMatrix
from pylearn2.datasets.preprocessing import (GlobalContrastNormalization,
                                             ExtractGridPatches,
                                             ReassembleGridPatches,
                                             LeCunLCN,
                                             RGB_YUV,
                                             ZCA)


class testGlobalContrastNormalization:
    """Tests for the GlobalContrastNormalization class """

    def test_zero_vector(self):
        """ Test that passing in the zero vector does not result in
            a divide by 0 """

        dataset = DenseDesignMatrix(X=as_floatX(np.zeros((1, 1))))

        #the settings of subtract_mean and use_norm are not relevant to
        #the test
        #std_bias = 0.0 is the only value for which there should be a risk
        #of failure occurring
        preprocessor = GlobalContrastNormalization(subtract_mean=True,
                                                   sqrt_bias=0.0,
                                                   use_std=True)

        dataset.apply_preprocessor(preprocessor)

        result = dataset.get_design_matrix()

        assert not np.any(np.isnan(result))
        assert not np.any(np.isinf(result))

    def test_unit_norm(self):
        """ Test that using std_bias = 0.0 and use_norm = True
            results in vectors having unit norm """

        tol = 1e-5

        num_examples = 5
        num_features = 10

        rng = np.random.RandomState([1, 2, 3])

        X = as_floatX(rng.randn(num_examples, num_features))

        dataset = DenseDesignMatrix(X=X)

        #the setting of subtract_mean is not relevant to the test
        #the test only applies when std_bias = 0.0 and use_std = False
        preprocessor = GlobalContrastNormalization(subtract_mean=False,
                                                   sqrt_bias=0.0,
                                                   use_std=False)

        dataset.apply_preprocessor(preprocessor)

        result = dataset.get_design_matrix()

        norms = np.sqrt(np.square(result).sum(axis=1))

        max_norm_error = np.abs(norms-1.).max()

        tol = 3e-5

        assert max_norm_error < tol


def test_extract_reassemble():
    """ Tests that ExtractGridPatches and ReassembleGridPatches are
    inverse of each other """

    rng = np.random.RandomState([1, 3, 7])

    topo = rng.randn(4, 3*5, 3*7, 2)

    dataset = DenseDesignMatrix(topo_view=topo)

    patch_shape = (3, 7)

    extractor = ExtractGridPatches(patch_shape, patch_shape)
    reassemblor = ReassembleGridPatches(patch_shape=patch_shape,
                                        orig_shape=topo.shape[1:3])

    dataset.apply_preprocessor(extractor)
    dataset.apply_preprocessor(reassemblor)

    new_topo = dataset.get_topological_view()

    assert new_topo.shape == topo.shape

    if not np.all(new_topo == topo):
        assert False


class testLeCunLCN:
    """
    Test LeCunLCN
    """

    def test_random_image(self):
        """
        Test on a random image if the per-processor loads and works without
        anyerror and doesn't result in any nan or inf values

        """

        rng = np.random.RandomState([1, 2, 3])
        X = as_floatX(rng.randn(5, 32*32*3))

        axes = ['b', 0, 1, 'c']
        view_converter = dense_design_matrix.DefaultViewConverter((32, 32, 3),
                                                                  axes)
        dataset = DenseDesignMatrix(X=X, view_converter=view_converter)
        dataset.axes = axes
        preprocessor = LeCunLCN(img_shape=[32, 32])
        dataset.apply_preprocessor(preprocessor)
        result = dataset.get_design_matrix()

        assert not np.any(np.isnan(result))
        assert not np.any(np.isinf(result))

    def test_zero_image(self):
        """
        Test on zero-value image if cause any division by zero
        """

        X = as_floatX(np.zeros((5, 32*32*3)))

        axes = ['b', 0, 1, 'c']
        view_converter = dense_design_matrix.DefaultViewConverter((32, 32, 3),
                                                                  axes)
        dataset = DenseDesignMatrix(X=X, view_converter=view_converter)
        dataset.axes = axes
        preprocessor = LeCunLCN(img_shape=[32, 32])
        dataset.apply_preprocessor(preprocessor)
        result = dataset.get_design_matrix()

        assert not np.any(np.isnan(result))
        assert not np.any(np.isinf(result))

    def test_channel(self):
        """
        Test if works fine withe different number of channel as argument
        """

        rng = np.random.RandomState([1, 2, 3])
        X = as_floatX(rng.randn(5, 32*32*3))

        axes = ['b', 0, 1, 'c']
        view_converter = dense_design_matrix.DefaultViewConverter((32, 32, 3),
                                                                  axes)
        dataset = DenseDesignMatrix(X=X, view_converter=view_converter)
        dataset.axes = axes
        preprocessor = LeCunLCN(img_shape=[32, 32], channels=[1, 2])
        dataset.apply_preprocessor(preprocessor)
        result = dataset.get_design_matrix()

        assert not np.any(np.isnan(result))
        assert not np.any(np.isinf(result))


def test_rgb_yuv():
    """
    Test on a random image if the per-processor loads and works without
    anyerror and doesn't result in any nan or inf values

    """

    rng = np.random.RandomState([1, 2, 3])
    X = as_floatX(rng.randn(5, 32*32*3))

    axes = ['b', 0, 1, 'c']
    view_converter = dense_design_matrix.DefaultViewConverter((32, 32, 3),
                                                              axes)
    dataset = DenseDesignMatrix(X=X, view_converter=view_converter)
    dataset.axes = axes
    preprocessor = RGB_YUV()
    dataset.apply_preprocessor(preprocessor)
    result = dataset.get_design_matrix()

    assert not np.any(np.isnan(result))
    assert not np.any(np.isinf(result))


def test_zca():
    """
    Confirm that ZCA.inv_P_ is the correct inverse of ZCA.P_.
    There's a lot else about the ZCA class that could be tested here.
    """

    rng = np.random.RandomState([1, 2, 3])
    X = as_floatX(rng.randn(15, 10))
    preprocessor = ZCA()
    preprocessor.fit(X)

    def is_identity(matrix):
        identity = np.identity(matrix.shape[0], theano.config.floatX)
        abs_difference = np.abs(identity - matrix)
        return (abs_difference < .0001).all()

    assert preprocessor.P_.shape == (X.shape[1], X.shape[1])
    assert not is_identity(preprocessor.P_)
    assert is_identity(np.dot(preprocessor.P_, preprocessor.inv_P_))

def test_zca_dtypes():
    """
    Confirm that ZCA.fit works regardless of dtype of data and config.floatX
    """

    orig_floatX = config.floatX

    try:
        for floatX in ['float32', 'float64']:
            for dtype in ['float32', 'float64']:
                rng = np.random.RandomState([1, 2, 3])
                X = rng.randn(15, 10).astype(dtype)
                preprocessor = ZCA()
                preprocessor.fit(X)
    finally:
        config.floatX = orig_floatX

########NEW FILE########
__FILENAME__ = test_sparse_dataset
"""
Unit tests for ../sparse_dataset.py
"""

import numpy as np
from pylearn2.datasets.sparse_dataset import SparseDataset
from pylearn2.train import Train
from pylearn2.models.model import Model
from pylearn2.space import VectorSpace
from pylearn2.termination_criteria import EpochCounter
from scipy.sparse import csr_matrix
from pylearn2.costs.cost import Cost, DefaultDataSpecsMixin
from pylearn2.training_algorithms.sgd import SGD
from pylearn2.utils import sharedX
from pylearn2.utils import wraps
import theano.tensor as T


class SoftmaxModel(Model):
    """
    A dummy model used for testing.

    Parameters
    ----------
    dim : int
        the input dimension of the Softmax Model

    Notes
    -----
    Important properties:
    has a parameter (P) for SGD to act on
    has a get_output_space method, so it can tell the
    algorithm what kind of space the targets for supervised
    learning live in
    has a get_input_space method, so it can tell the
    algorithm what kind of space the features live in
    """
    def __init__(self, dim):
        self.dim = dim
        rng = np.random.RandomState([2014, 4, 22])
        self.P = sharedX(rng.uniform(-1., 1., (dim,)))
        self.force_batch_size = None

    @wraps(Model.get_params)
    def get_params(self):
        return [self.P]

    @wraps(Model.get_input_space)
    def get_input_space(self):
        return VectorSpace(self.dim)

    @wraps(Model.get_output_space)
    def get_output_space(self):
        return VectorSpace(self.dim)

    def __call__(self, X):
        """
        Compute and return the softmax transformation of sparse data.
        """
        assert X.ndim == 2
        return T.nnet.softmax(X*self.P)


class DummyCost(DefaultDataSpecsMixin, Cost):
    """
    A dummy cost used for testing.

    Notes
    -----
    Important properties:
    has a expr method which takes a model and a
    dataset and returns as cost the mean squared
    difference between the data and the models'
    output using that data.
    """

    @wraps(Cost.expr)
    def expr(self, model, data):
        """
        Returns as cost the mean squared
        difference between the data and the models'
        output using that data.
        TODO: make this a real docstring instead of
        a comment appearing after the misuse of wraps
        """
        space, sources = self.get_data_specs(model)
        space.validate(data)
        X = data
        return T.square(model(X) - X).mean()


def test_iterator():
    """
    Tests whether SparseDataset can be loaded and
    initializes iterator
    """

    x = csr_matrix([[1, 2, 0], [0, 0, 3], [4, 0, 5]])
    ds = SparseDataset(from_scipy_sparse_dataset=x)
    it = ds.iterator(mode='sequential', batch_size=1)
    it.next()


def test_training_a_model():
    """
    tests wether SparseDataset can be trained
    with a dummy model.
    """

    dim = 3
    m = 10
    rng = np.random.RandomState([22, 4, 2014])

    X = rng.randn(m, dim)
    ds = csr_matrix(X)
    dataset = SparseDataset(from_scipy_sparse_dataset=ds)

    model = SoftmaxModel(dim)
    learning_rate = 1e-1
    batch_size = 5

    epoch_num = 2
    termination_criterion = EpochCounter(epoch_num)

    cost = DummyCost()

    algorithm = SGD(learning_rate, cost, batch_size=batch_size,
                    termination_criterion=termination_criterion,
                    update_callbacks=None,
                    init_momentum=None,
                    set_batch_size=False)

    train = Train(dataset, model, algorithm, save_path=None,
                  save_freq=0, extensions=None)

    train.main_loop()

if __name__ == '__main__':
    test_iterator()
    test_training_a_model()

########NEW FILE########
__FILENAME__ = test_tfd
import unittest
import numpy as np
from pylearn2.datasets.tfd import TFD
from pylearn2.space import Conv2DSpace
from pylearn2.testing.skip import skip_if_no_data


class TestTFD(unittest.TestCase):
    def setUp(self):
        skip_if_no_data()
        self.train = TFD(which_set='train')
        self.test = TFD(which_set='test')
        valid = TFD(which_set='valid')
        unlabeled = TFD(which_set='unlabeled')
        full_train = TFD(which_set='full_train')
        large = TFD(which_set='test', image_size=96)
        fold1 = TFD(which_set='test', fold=1)
        fold2 = TFD(which_set='test', fold=2)
        fold3 = TFD(which_set='test', fold=3)
        fold4 = TFD(which_set='test', fold=4)

    def test_topo(self):
        """Tests that a topological batch has 4 dimensions"""
        topo = self.train.get_batch_topo(1)
        assert topo.ndim == 4

    def test_topo_c01b(self):
        """
        Tests that a topological batch with axes ('c',0,1,'b')
        can be dimshuffled back to match the standard ('b',0,1,'c')
        format.
        """
        batch_size = 100
        c01b_test = TFD(which_set='test', axes=('c', 0, 1, 'b'))
        c01b_X = c01b_test.X[0:batch_size, :]
        c01b = c01b_test.get_topological_view(c01b_X)
        assert c01b.shape == (1, 48, 48, batch_size)
        b01c = c01b.transpose(3, 1, 2, 0)
        b01c_X = self.test.X[0:batch_size, :]
        assert c01b_X.shape == b01c_X.shape
        assert np.all(c01b_X == b01c_X)
        b01c_direct = self.test.get_topological_view(b01c_X)
        assert b01c_direct.shape == b01c.shape
        assert np.all(b01c_direct == b01c)

    def test_iterator(self):
        # Tests that batches returned by an iterator with topological
        # data_specs are the same as the ones returned by calling
        # get_topological_view on the dataset with the corresponding order
        batch_size = 100
        b01c_X = self.test.X[0:batch_size, :]
        b01c_topo = self.test.get_topological_view(b01c_X)
        b01c_b01c_it = self.test.iterator(
            mode='sequential',
            batch_size=batch_size,
            data_specs=(Conv2DSpace(shape=(48, 48),
                                    num_channels=1,
                                    axes=('b', 0, 1, 'c')),
                        'features'))
        b01c_b01c = b01c_b01c_it.next()
        assert np.all(b01c_topo == b01c_b01c)

        c01b_test = TFD(which_set='test', axes=('c', 0, 1, 'b'))
        c01b_X = c01b_test.X[0:batch_size, :]
        c01b_topo = c01b_test.get_topological_view(c01b_X)
        c01b_c01b_it = c01b_test.iterator(
            mode='sequential',
            batch_size=batch_size,
            data_specs=(Conv2DSpace(shape=(48, 48),
                                    num_channels=1,
                                    axes=('c', 0, 1, 'b')),
                        'features'))
        c01b_c01b = c01b_c01b_it.next()
        assert np.all(c01b_topo == c01b_c01b)

        # Also check that samples from iterators with the same data_specs
        # with Conv2DSpace do not depend on the axes of the dataset
        b01c_c01b_it = self.test.iterator(
            mode='sequential',
            batch_size=batch_size,
            data_specs=(Conv2DSpace(shape=(48, 48),
                                    num_channels=1,
                                    axes=('c', 0, 1, 'b')),
                        'features'))
        b01c_c01b = b01c_c01b_it.next()
        assert np.all(b01c_c01b == c01b_c01b)

        c01b_b01c_it = c01b_test.iterator(
            mode='sequential',
            batch_size=batch_size,
            data_specs=(Conv2DSpace(shape=(48, 48),
                                    num_channels=1,
                                    axes=('b', 0, 1, 'c')),
                        'features'))
        c01b_b01c = c01b_b01c_it.next()
        assert np.all(c01b_b01c == b01c_b01c)

########NEW FILE########
__FILENAME__ = test_tl_challenge
import unittest
import numpy as np
from pylearn2.datasets.tl_challenge import TL_Challenge
from pylearn2.space import Conv2DSpace
from pylearn2.testing.skip import skip_if_no_data


class TestTL_Challenge(unittest.TestCase):
    def setUp(self):
        skip_if_no_data()
        self.train = TL_Challenge(which_set='train')
        self.unlabeled = TL_Challenge(which_set='unlabeled')
        self.test = TL_Challenge(which_set='test')

    def test_topo(self):
        """Tests that a topological batch has 4 dimensions"""
        topo = self.train.get_batch_topo(1)
        assert topo.ndim == 4

########NEW FILE########
__FILENAME__ = test_utlc
import unittest

import numpy
import scipy.sparse

from pylearn2.testing.skip import skip_if_no_data
import pylearn2.datasets.utlc as utlc

def test_ule():
    skip_if_no_data()
    # Test loading of transfer data
    train, valid, test, transfer = utlc.load_ndarray_dataset("ule", normalize=True, transfer=True)
    assert train.shape[0]==transfer.shape[0]


#@unittest.skip("Slow and needs >8 GB of RAM")
def test_all_utlc():
    skip_if_no_data()
    for name in ['avicenna','harry','ule']:   # not testing rita, because it requires a lot of memorz and is slow
        print "Loading ", name
        train, valid, test = utlc.load_ndarray_dataset(name, normalize=True)
        print "dtype, max, min, mean, std"
        print train.dtype, train.max(), train.min(), train.mean(), train.std()
        assert isinstance(train, numpy.ndarray), "train is not an ndarray in %s dataset" % name
        assert isinstance(valid, numpy.ndarray), "valid is not an ndarray in %s dataset" % name
        assert isinstance(test, numpy.ndarray), "test is not an ndarray in %s dataset" % name
        assert train.shape[1]==test.shape[1]==valid.shape[1], "shapes of datasets does not match for %s" % name

def test_sparse_ule():
    skip_if_no_data()
    # Test loading of transfer data
    train, valid, test, transfer = utlc.load_sparse_dataset("ule", normalize=True, transfer=True)
    assert train.shape[0]==transfer.shape[0]

def test_all_sparse_utlc():
    skip_if_no_data()
    for name in ['harry','terry','ule']:
        print "Loading sparse ", name
        train, valid, test = utlc.load_sparse_dataset(name, normalize=True)
        nb_elem = numpy.prod(train.shape)
        mi = train.data.min()
        ma = train.data.max()
        mi = min(0, mi)
        ma = max(0, ma)
        su = train.data.sum()
        mean = float(su)/nb_elem
        print name,"dtype, max, min, mean, nb non-zero, nb element, %sparse"
        print train.dtype, ma, mi, mean, train.nnz, nb_elem, (nb_elem-float(train.nnz))/nb_elem
        print name,"max, min, mean, std (all stats on non-zero element)"
        print train.data.max(), train.data.min(), train.data.mean(), train.data.std()
        assert scipy.sparse.issparse(train), "train is not sparse for %s dataset" % name
        assert scipy.sparse.issparse(valid), "valid is not sparse for %s dataset" % name
        assert scipy.sparse.issparse(test), "test is not sparse for %s dataset" % name
        assert train.shape[1]==test.shape[1]==valid.shape[1], "shapes of sparse %s dataset do  not match" % name


########NEW FILE########
__FILENAME__ = test_vector_spaces_dataset
# Currently this test file does nothing but make sure the module can be
# imported.
from pylearn2.datasets import vector_spaces_dataset


########NEW FILE########
__FILENAME__ = test_wiskott
"""module to test datasets.wiskott"""
from pylearn2.datasets.wiskott import Wiskott
import unittest
from pylearn2.testing.skip import skip_if_no_data
import numpy as np


def test_wiskott():
    """loads wiskott dataset"""
    skip_if_no_data()
    data = Wiskott()
    assert not np.any(np.isinf(data.X))
    assert not np.any(np.isnan(data.X))

########NEW FILE########
__FILENAME__ = tfd
"""
.. todo::

    WRITEME
"""
import numpy as np
from pylearn2.datasets import dense_design_matrix
from pylearn2.utils.serial import load
from pylearn2.utils.rng import make_np_rng

class TFD(dense_design_matrix.DenseDesignMatrix):
    """
    Pylearn2 wrapper for the Toronto Face Dataset.
    http://aclab.ca/users/josh/TFD.html

    Parameters
    ----------
    which_set : str
        Dataset to load. One of ['train','valid','test','unlabeled'].
    fold : int in {0,1,2,3,4}
        TFD contains 5 official folds for train, valid and test.
    image_size : int in [48,96]
        Load smaller or larger dataset variant.
    example_range : array_like or None, optional
        Load only examples in range [example_range[0]:example_range[1]].
    center : bool, optional
        Move data from range [0., 255.] to [-127.5, 127.5]
        False by default.
    scale : bool, optional
        Move data from range [0., 255.] to [0., 1.], or
        from range [-127.5, 127.5] to [-1., 1.] if center is True
        False by default.
    shuffle : WRITEME
    one_hot : WRITEME
    rng : WRITEME
    seed : WRITEME
    preprocessor : WRITEME
    axes : WRITEME
    """

    mapper = {'unlabeled': 0, 'train': 1, 'valid': 2, 'test': 3,
            'full_train': 4}

    def __init__(self, which_set, fold = 0, image_size = 48,
                 example_range = None, center = False, scale = False,
                 shuffle=False, one_hot = False, rng=None, seed=132987,
                 preprocessor = None, axes = ('b', 0, 1, 'c')):
        if which_set not in self.mapper.keys():
            raise ValueError("Unrecognized which_set value: %s. Valid values are %s." % (str(which_set), str(self.mapper.keys())))
        assert (fold >=0) and (fold <5)

        # load data
        path = '${PYLEARN2_DATA_PATH}/faces/TFD/'
        if image_size == 48:
            data = load(path + 'TFD_48x48.mat')
        elif image_size == 96:
            data = load(path + 'TFD_96x96.mat')
        else:
            raise ValueError("image_size should be either 48 or 96.")

        # retrieve indices corresponding to `which_set` and fold number
        if self.mapper[which_set] == 4:
            set_indices = (data['folds'][:, fold] == 1) + (data['folds'][:,fold] == 2)
        else:
            set_indices = data['folds'][:, fold] == self.mapper[which_set]
        assert set_indices.sum() > 0

        # limit examples returned to `example_range`
        ex_range = slice(example_range[0], example_range[1]) \
                         if example_range else slice(None)

        # get images and cast to float32
        data_x = data['images'][set_indices]
        data_x = np.cast['float32'](data_x)
        data_x = data_x[ex_range]
        # create dense design matrix from topological view
        data_x = data_x.reshape(data_x.shape[0], image_size ** 2)

        if center and scale:
            data_x[:] -= 127.5
            data_x[:] /= 127.5
        elif center:
            data_x[:] -= 127.5
        elif scale:
            data_x[:] /= 255.

        if shuffle:
            rng = make_np_rng(rng, seed, which_method='permutation')
            rand_idx = rng.permutation(len(data_x))
            data_x = data_x[rand_idx]

        # get labels
        if which_set != 'unlabeled':
            data_y = data['labs_ex'][set_indices]
            data_y = data_y[ex_range] -1

            data_y_identity = data['labs_id'][set_indices]
            data_y_identity = data_y_identity[ex_range]

            if shuffle:
                data_y = data_y[rand_idx]
                data_y_identity = data_y_identity[rand_idx]

            self.one_hot = one_hot
            if one_hot:
                one_hot = np.zeros((data_y.shape[0], 7), dtype = 'float32')
                for i in xrange(data_y.shape[0]):
                    one_hot[i, data_y[i]] = 1.
                data_y = one_hot
        else:
            data_y = None
            data_y_identity = None

        # create view converting for retrieving topological view
        view_converter = dense_design_matrix.DefaultViewConverter((image_size, image_size, 1),
                axes)

        # init the super class
        super(TFD, self).__init__(X = data_x, y = data_y, view_converter = view_converter)

        assert not np.any(np.isnan(self.X))

        self.y_identity = data_y_identity
        self.axes = axes

        if preprocessor is not None:
            preprocessor.apply(self)

########NEW FILE########
__FILENAME__ = tl_challenge
"""The dataset for the NIPS 2011 Transfer Learning Challenge"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"
import numpy as N
from pylearn2.datasets import dense_design_matrix
from pylearn2.utils.string_utils import preprocess


class TL_Challenge(dense_design_matrix.DenseDesignMatrix):
    """
    .. todo::

       WRITEME

    Parameters
    ----------
    which_set : WRITEME
    center : WRITEME
    custom_path : WRITEME
    """

    def __init__(self, which_set, center=False, custom_path=None):
        assert which_set in ['train', 'test', 'unlabeled', 'custom']

        path = "${PYLEARN2_DATA_PATH}/TLChallenge"

        if which_set == 'train':
            path += '/training/training-data.dat'
        elif which_set == 'test':
            path += '/test/test-data.dat'
        elif which_set == 'unlabeled':
            path += '/unlabelled_tiny.dat'
        elif which_set == 'custom':
            path = custom_path

        path = preprocess(path)

        X = N.fromfile(path, dtype=N.uint8, sep=' ')

        X = X.reshape(X.shape[0]/(32*32*3), 32*32*3, order='F')

        assert X.max() == 255
        assert X.min() == 0

        X = N.cast['float32'](X)
        y = None

        if center:
            X -= 127.5

        view_converter = dense_design_matrix.DefaultViewConverter((32, 32, 3))

        X = view_converter.design_mat_to_topo_view(X)

        X = N.transpose(X, (0, 2, 1, 3))

        X = view_converter.topo_view_to_design_mat(X)

        super(TL_Challenge, self).__init__(X=X,
                                           y=y,
                                           view_converter=view_converter)

        assert not N.any(N.isnan(self.X))

        if which_set == 'train' or which_set == 'test':
            labels_path = path[:-8] + 'labels.dat'
            self.y_fine = N.fromfile(labels_path, dtype=N.uint8, sep=' ')
            assert len(self.y_fine.shape) == 1
            assert self.y_fine.shape[0] == X.shape[0]
            #0 :  aquatic_mammals
            #1 :  fish
            #2 :  flowers
            FOOD_CONTAINER = 3
            FRUIT = 4
            #5 :  household_electrical_devices
            FURNITURE = 6
            INSECTS = 7
            #8 :  large_carnivores
            #9 :  large_man-made_outdoor_things
            #10 :  large_natural_outdoor_scenes
            LARGE_OMNIVORES_HERBIVORES = 11
            MEDIUM_MAMMAL = 12
            #13 :  non-insect_invertebrates
            #14 :  people
            #15 :  reptiles
            #16 :  small_mammals
            #17 :  trees
            #18 :  vehicles_1
            #19 :  vehicles_2

            self.y_coarse = self.y_fine.copy()
            self.y_coarse[self.y_coarse == 100] = INSECTS
            self.y_coarse[self.y_coarse == 101] = LARGE_OMNIVORES_HERBIVORES
            self.y_coarse[self.y_coarse == 102] = LARGE_OMNIVORES_HERBIVORES
            self.y_coarse[self.y_coarse == 103] = LARGE_OMNIVORES_HERBIVORES
            self.y_coarse[self.y_coarse == 104] = FRUIT
            self.y_coarse[self.y_coarse == 105] = FOOD_CONTAINER
            self.y_coarse[self.y_coarse == 106] = FRUIT
            self.y_coarse[self.y_coarse == 107] = MEDIUM_MAMMAL
            self.y_coarse[self.y_coarse == 108] = FRUIT
            self.y_coarse[self.y_coarse == 109] = FURNITURE

            assert self.y_coarse.min() == 3
            assert self.y_coarse.max() == 12

            for i in xrange(120):
                if self.y_coarse[i] == FRUIT:

                    assert self.y_fine[i] in [104, 106, 108]

########NEW FILE########
__FILENAME__ = transformer_dataset
"""
.. todo::

    WRITEME
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

from pylearn2.datasets.dataset import Dataset
from pylearn2.space import CompositeSpace
from pylearn2.utils.data_specs import is_flat_specs


class TransformerDataset(Dataset):
    """
        A dataset that applies a transformation on the fly
        as examples are requested.
    """

    def __init__(self, raw, transformer, cpu_only = False,
            space_preserving=False):
        """
            .. todo::

                WRITEME properly

            Parameters
            ----------
            raw : pylearn2 Dataset
                Provides raw data
            transformer: pylearn2 Block
                To transform the data
        """
        self.__dict__.update(locals())
        del self.self

    def get_batch_design(self, batch_size, include_labels=False):
        """
        .. todo::

            WRITEME
        """
        raw = self.raw.get_batch_design(batch_size, include_labels)
        if include_labels:
            X, y = raw
        else:
            X = raw
        X = self.transformer.perform(X)
        if include_labels:
            return X, y
        return X

    def get_test_set(self):
        """
        .. todo::

            WRITEME
        """
        return TransformerDataset(raw=self.raw.get_test_set(),
                transformer=self.transformer,
                cpu_only=self.cpu_only,
                space_preserving=self.space_preserving)


    def get_batch_topo(self, batch_size):
        """
        If the transformer has changed the space, we don't have a good
        idea of how to do topology in the new space.
        If the transformer just changes the values in the original space,
        we can have the raw dataset provide the topology.
        """
        X = self.get_batch_design(batch_size)
        if self.space_preserving:
            return self.raw.get_topological_view(X)
        return X.reshape(X.shape[0],X.shape[1],1,1)

    def iterator(self, mode=None, batch_size=None, num_batches=None,
                 topo=None, targets=None, rng=None, data_specs=None,
                 return_tuple=False):
        """
        .. todo::

            WRITEME
        """
        # Build the right data_specs to query self.raw
        if data_specs is not None:
            assert is_flat_specs(data_specs)
            space, source = data_specs
            if not isinstance(source, tuple):
                source = (source,)
            if isinstance(space, CompositeSpace):
                space = tuple(space.components)
            else:
                space = (space,)

            # Put 'features' first, as this is what TransformerIterator
            # is expecting
            if 'features' not in source:
                # 'features is not needed, get things directly from
                # the original data
                raw_data_specs = data_specs
            else:
                feature_idx = source.index('features')
                if self.space_preserving:
                    # Ask self.raw for the data in the expected space,
                    # and self.transformer will operate in that space
                    feature_input_space = space[feature_idx]
                else:
                    # We need to ask the transformer what its input space is
                    feature_input_space = self.transformer.get_input_space()

                raw_space = CompositeSpace(
                                (feature_input_space,)
                                + space[:feature_idx]
                                + space[feature_idx + 1:])
                raw_source = (('features',)
                              + source[:feature_idx]
                              + source[feature_idx + 1:])
                raw_data_specs = (raw_space, raw_source)
        else:
            raw_data_specs = None

        raw_iterator = self.raw.iterator(mode=mode, batch_size=batch_size,
                num_batches=num_batches, topo=topo, targets=targets, rng=rng,
                data_specs=raw_data_specs, return_tuple=return_tuple)

        final_iterator = TransformerIterator(raw_iterator, self,
                                             data_specs=data_specs)

        return final_iterator

    def has_targets(self):
        """
        .. todo::

            WRITEME
        """
        return self.raw.has_targets()

    def adjust_for_viewer(self, X):
        """
        .. todo::

            WRITEME
        """
        if self.space_preserving:
            return self.raw.adjust_for_viewer(X)
        return X

    def get_weights_view(self, *args, **kwargs):
        """
        .. todo::

            WRITEME
        """
        if self.space_preserving:
            return self.raw.get_weights_view(*args, **kwargs)
        raise NotImplementedError()

    def get_topological_view(self, *args, **kwargs):
        """
        .. todo::

            WRITEME
        """
        if self.space_preserving:
            return self.raw.get_weights_view(*args, **kwargs)
        raise NotImplementedError()

    def adjust_to_be_viewed_with(self, *args, **kwargs):
        """
        .. todo::

            WRITEME
        """
        return self.raw.adjust_to_be_viewed_with(*args, **kwargs)


class TransformerIterator(object):
    """
    .. todo::

        WRITEME
    """

    def __init__(self, raw_iterator, transformer_dataset, data_specs):
        """
        .. todo::

            WRITEME
        """
        self.raw_iterator = raw_iterator
        self.transformer_dataset = transformer_dataset
        self.stochastic = raw_iterator.stochastic
        self.uneven = raw_iterator.uneven
        self.data_specs = data_specs

    def __iter__(self):
        """
        .. todo::

            WRITEME
        """
        return self

    def next(self):
        """
        .. todo::

            WRITEME
        """
        raw_batch = self.raw_iterator.next()

        # Apply transformation on raw_batch, and format it
        # in the requested Space
        transformer = self.transformer_dataset.transformer
        out_space = self.data_specs[0]
        if isinstance(out_space, CompositeSpace):
            out_space = out_space.components[0]

        if self.transformer_dataset.space_preserving:
            # If the space is preserved, then raw_batch is already provided
            # in the requested space
            rval_space = out_space
        else:
            rval_space = transformer.get_output_space()

        def transform(X_batch):
            rval = transformer.perform(X_batch)
            if rval_space != out_space:
                rval = rval_space.np_format_as(rval, out_space)
            return rval

        if not isinstance(raw_batch, tuple):
            # Only one source, return_tuple is False
            rval = transform(raw_batch)
        else:
            # Apply the transformer only on the first element
            rval = (transform(raw_batch[0]),) + raw_batch[1:]

        return rval

    @property
    def num_examples(self):
        """
        .. todo::

            WRITEME
        """
        return self.raw_iterator.num_examples

########NEW FILE########
__FILENAME__ = utlc
"""
Utility functions to load data from the UTLC challenge (Unsupervised Transfer Learning).

The user should use the load_ndarray_dataset or load_sparse_dataset function
See the file ${PYLEARN2_DATA_PATH}/UTLC/README for details on the datasets.
"""

import cPickle
import gzip
import os

import numpy
import theano

import pylearn2.datasets.filetensor as ft
from pylearn2.utils.string_utils import preprocess
from pylearn2.utils.rng import make_np_rng


def load_ndarray_dataset(name, normalize=True, transfer=False,
                         normalize_on_the_fly=False, randomize_valid=False,
                         randomize_test=False):
    """ 
    Load the train,valid,test data for the dataset `name` and return it in ndarray format.

    We suppose the data was created with ift6266h11/pretraitement/to_npy.py that
    shuffle the train. So the train should already be shuffled.

    Parameters
    ----------
    name : 'avicenna', 'harry', 'rita', 'sylvester' or 'ule'
        Which dataset to load
    normalize : bool
        If True, we normalize the train dataset before returning it
    transfer : bool
        If True also return the transfer labels
    normalize_on_the_fly : bool
        If True, we return a Theano Variable that will give as output the normalized 
        value. If the user only take a subtensor of that variable, Theano optimization
        should make that we will only have in memory the subtensor portion that is 
        computed in normalized form. We store the original data in shared memory in 
        its original dtype. This is usefull to have the original data in its original
        dtype in memory to same memory. Especialy usefull to be able to use rita and
        harry with 1G per jobs.
    randomize_valid : bool
        Do we randomize the order of the valid set?  We always use the same random order
        If False, return in the same order as downloaded on the web
    randomize_test : bool
        Do we randomize the order of the test set?  We always use the same random order
        If False, return in the same order as downloaded on the web

    Returns
    -------
    train, valid, test : ndarrays
        Datasets returned if transfer = False
    train, valid, test, transfer : ndarrays
        Datasets returned if transfer = False
    
    """
    assert not (normalize and normalize_on_the_fly), "Can't normalize in 2 way at the same time!"

    assert name in ['avicenna','harry','rita','sylvester','ule']
    common = os.path.join(preprocess('${PYLEARN2_DATA_PATH}'),'UTLC','filetensor',name+'_')
    trname,vname,tename = [common+subset+'.ft' for subset in ['train','valid','test']]

    train = load_filetensor(trname)
    valid = load_filetensor(vname)
    test = load_filetensor(tename)
    if randomize_valid:
        rng = make_np_rng(None, [1,2,3,4], which_method='permutation')
        perm = rng.permutation(valid.shape[0])
        valid = valid[perm]
    if randomize_test:
        rng = make_np_rng(None, [1,2,3,4], which_method='permutation')
        perm = rng.permutation(test.shape[0])
        test = test[perm]

    if normalize or normalize_on_the_fly:
        if normalize_on_the_fly:
            # Shared variables of the original type
            train = theano.shared(train, borrow=True, name=name+"_train")
            valid = theano.shared(valid, borrow=True, name=name+"_valid")
            test = theano.shared(test, borrow=True, name=name+"_test")
            # Symbolic variables cast into floatX
            train = theano.tensor.cast(train, theano.config.floatX)
            valid = theano.tensor.cast(valid, theano.config.floatX)
            test = theano.tensor.cast(test, theano.config.floatX)
        else:
            train = numpy.asarray(train, theano.config.floatX)
            valid = numpy.asarray(valid, theano.config.floatX)
            test = numpy.asarray(test, theano.config.floatX)

        if name == "ule":
            train /= 255
            valid /= 255
            test /= 255
        elif name in ["avicenna", "sylvester"]:
            if name == "avicenna":
                train_mean = 514.62154022835455
                train_std = 6.829096494224145
            else:
                train_mean = 403.81889927027686
                train_std = 96.43841050784053
            train -= train_mean
            valid -= train_mean
            test -= train_mean
            train /= train_std
            valid /= train_std
            test /= train_std
        elif name == "harry":
            std = 0.69336046033925791#train.std()slow to compute
            train /= std
            valid /= std
            test /= std
        elif name == "rita":
            v = numpy.asarray(230, dtype=theano.config.floatX)
            train /= v
            valid /= v
            test /= v
        else:
            raise Exception("This dataset don't have its normalization defined")
    if transfer:
        transfer = load_ndarray_transfer(name)
        return train, valid, test, transfer
    else:
        return train, valid, test

def load_sparse_dataset(name, normalize=True, transfer=False,
                        randomize_valid=False,
                        randomize_test=False):
    """ 
    Load the train,valid,test data for the dataset `name` and return it in sparse format.

    We suppose the data was created with ift6266h11/pretraitement/to_npy.py that
    shuffle the train. So the train should already be shuffled.

    name : 'avicenna', 'harry', 'rita', 'sylvester' or 'ule'
        Which dataset to load
    normalize : bool
        If True, we normalize the train dataset before returning it
    transfer : 
        If True also return the transfer label
    randomize_valid : bool
        Do we randomize the order of the valid set?  We always use the same random order
        If False, return in the same order as downloaded on the web
    randomize_test : bool
        Do we randomize the order of the test set?  We always use the same random order
        If False, return in the same order as downloaded on the web

    Returns
    -------
    train, valid, test : ndarrays
        Datasets returned if transfer = False
    train, valid, test, transfer : ndarrays
        Datasets returned if transfer = False
    """
    assert name in ['harry','terry','ule']
    common = os.path.join(preprocess('${PYLEARN2_DATA_PATH}'),'UTLC','sparse',name+'_')
    trname,vname,tename = [common+subset+'.npy' for subset in ['train','valid','test']]

    train = load_sparse(trname)
    valid = load_sparse(vname)
    test = load_sparse(tename)

    # Data should already be in csr format that support
    # this type of indexing.
    if randomize_valid:
        rng = make_np_rng(None, [1,2,3,4], which_method='permutation')
        perm = rng.permutation(valid.shape[0])
        valid = valid[perm]
    if randomize_test:
        rng = make_np_rng(None, [1,2,3,4], which_method='permutation')
        perm = rng.permutation(test.shape[0])
        test = test[perm]

    if normalize:
        if name == "ule":
            train = train.astype(theano.config.floatX) / 255
            valid = valid.astype(theano.config.floatX) / 255
            test = test.astype(theano.config.floatX) / 255
        elif name == "harry":
            train = train.astype(theano.config.floatX)
            valid = valid.astype(theano.config.floatX)
            test = test.astype(theano.config.floatX)
            std = 0.69336046033925791#train.std()slow to compute
            train = (train) / std
            valid = (valid) / std
            test = (test) / std
        elif name == "terry":
            train = train.astype(theano.config.floatX)
            valid = valid.astype(theano.config.floatX)
            test = test.astype(theano.config.floatX)
            train = (train) / 300
            valid = (valid) / 300
            test = (test) / 300
        else:
            raise Exception("This dataset don't have its normalization defined")
    if transfer:
        fname = os.path.join(preprocess("${PYLEARN2_DATA_PATH}"), "UTLC", "filetensor", name+"_transfer.ft")
        transfer = load_filetensor(fname)
        return train, valid, test, transfer
    else:
        return train, valid, test

def load_ndarray_transfer(name):
    """
    Load the transfer labels for the training set of data set `name`.
        
    Parameters
    ----------
    name : 'avicenna', 'harry', 'rita', 'sylvester' or 'ule'
        Which dataset to load

    Returns
    -------
    transfer : ndarray
        Transfer dataset loaded
    """
    assert name in ['avicenna','harry','rita','sylvester','terry','ule']
    
    fname = os.path.join(preprocess('${PYLEARN2_DATA_PATH}'), 'UTLC', 'filetensor', name+'_transfer.ft')
    transfer = load_filetensor(fname)
    return transfer

def load_ndarray_label(name):
    """ Load the train,valid,test label data for the dataset `name` and return it in ndarray format.
        This is only available for the toy dataset ule.

    Parameters
    ----------
    name : 'ule'
        Must be 'ule'

    Returns
    -------
    train_l. valid_l, test_l : ndarray
        Label data loaded
    """
    assert name in ['ule']

    common_path = os.path.join(preprocess('${PYLEARN2_DATA_PATH}'), 'UTLC', 'filetensor', name+'_')
    trname,vname,tename = [common_path+subset+'.tf' for subset in ['trainl','validl','testl']]

    trainl = load_filetensor(trname)
    validl = load_filetensor(vname)
    testl = load_filetensor(tename)
    return trainl, validl, testl

def load_filetensor(fname):
    """
    .. todo::

        WRITEME
    """
    f = None
    try:
        if not os.path.exists(fname):
            fname = fname+'.gz'
            f = gzip.open(fname)
        elif fname.endswith('.gz'):
            f = gzip.open(fname)
        else:
            f = open(fname)
        d = ft.read(f)
    finally:
        if f:
            f.close()

    return d

def load_sparse(fname):
    """
    .. todo::

        WRITEME
    """
    f = None
    try:
        if not os.path.exists(fname):
            fname = fname+'.gz'
            f = gzip.open(fname)
        elif fname.endswith('.gz'):
            f = gzip.open(fname)
        else:
            f = open(fname)
        d = cPickle.load(f)
    finally:
        if f:
            f.close()
    return d

########NEW FILE########
__FILENAME__ = vector_spaces_dataset
"""TODO: module-level docstring."""
__authors__ = "Pascal Lamblin and Razvan Pascanu"
__copyright__ = "Copyright 2010-2013, Universite de Montreal"
__credits__ = ["Pascal Lamblin", "Razvan Pascanu", "Ian Goodfellow", "Mehdi Mirza"]
__license__ = "3-clause BSD"
__maintainer__ = "Pascal Lamblin"
__email__ = "lamblinp@iro"
import functools

import numpy as np
from pylearn2.utils.iteration import (
    FiniteDatasetIterator,
    resolve_iterator_class
)
N = np
# Don't import tables initially, since it might not be available
# everywhere.
tables = None


from pylearn2.datasets.dataset import Dataset
from pylearn2.utils.data_specs import is_flat_specs
from pylearn2.utils.rng import make_np_rng

def ensure_tables():
    """Makes sure tables module has been imported"""
    global tables
    if tables is None:
        import tables


class VectorSpacesDataset(Dataset):
    """
    A class representing datasets being stored as a number of VectorSpaces.

    This can be seen as a generalization of DenseDesignMatrix where
    there can be any number of sources, not just X and possibly y.

    Parameters
    ----------
    data : ndarray, or tuple of ndarrays, containing the data.
        It is formatted as specified in `data_specs`. For instance, if
        `data_specs` is (VectorSpace(nfeat), 'features'), then `data` has to be
        a 2-d ndarray, of shape (nb examples, nfeat), that defines an unlabeled
        dataset. If `data_specs` is (CompositeSpace(Conv2DSpace(...),
        VectorSpace(1)), ('features', 'target')), then `data` has to be an
        (X, y) pair, with X being an ndarray containing images stored in the
        topological view specified by the `Conv2DSpace`, and y being a 2-D
        ndarray of width 1, containing the labels or targets for each example.
    data_specs : (space, source) pair
        space is an instance of `Space` (possibly a `CompositeSpace`), 
        and `source` is a string (or tuple of strings, if `space` is a 
        `CompositeSpace`), defining the format and labels associated 
        to `data`.
    rng : object, optional
        A random number generator used for picking random indices into the
        design matrix when choosing minibatches.
    preprocessor: WRITEME
    fit_preprocessor: WRITEME
    """
    _default_seed = (17, 2, 946)

    def __init__(self, data=None, data_specs=None, rng=_default_seed,
                 preprocessor=None, fit_preprocessor=False):
        # data_specs should be flat, and there should be no
        # duplicates in source, as we keep only one version
        assert is_flat_specs(data_specs)
        if isinstance(data_specs[1], tuple):
            assert sorted(set(data_specs[1])) == sorted(data_specs[1])
        self.data = data
        self.data_specs = data_specs

        self.compress = False
        self.design_loc = None
        self.rng = make_np_rng(rng, which_method='random_integers')
        # Defaults for iterators
        self._iter_mode = resolve_iterator_class('sequential')

        if preprocessor:
            preprocessor.apply(self, can_fit=fit_preprocessor)
        self.preprocessor = preprocessor

    @functools.wraps(Dataset.iterator)
    def iterator(self, mode=None, batch_size=None, num_batches=None,
                 topo=None, targets=None, rng=None, data_specs=None):

        if topo is not None or targets is not None:
            raise ValueError("You should use the new interface iterator")

        if mode is None:
            if hasattr(self, '_iter_subset_class'):
                mode = self._iter_subset_class
            else:
                raise ValueError('iteration mode not provided and no default '
                                 'mode set for %s' % str(self))
        else:
            mode = resolve_iterator_class(mode)

        if batch_size is None:
            batch_size = getattr(self, '_iter_batch_size', None)
        if num_batches is None:
            num_batches = getattr(self, '_iter_num_batches', None)
        if rng is None and mode.stochastic:
            rng = self.rng
        if data_specs is None:
            data_specs = self.data_specs
        return FiniteDatasetIterator(
                self,
                mode(self.data_specs[0].get_batch_size(self.data),
                     batch_size, num_batches, rng),
                data_specs=data_specs)

    def get_data(self):
        """
        .. todo::

            WRITEME
        """
        return self.data

    def set_data(self, data, data_specs):
        """
        .. todo::

            WRITEME
        """
        # data is organized as data_specs
        # keep self.data_specs, and convert data
        data_specs[0].validate(data)
        assert not [N.any(N.isnan(X)) for X in data]
        raise NotImplementedError()

    def get_source(self, name):
        """
        .. todo::

            WRITEME
        """
        raise NotImplementedError()

    @property
    def num_examples(self):
        """
        .. todo::

            WRITEME
        """
        return self.data_specs[0].get_batch_size(self.data)

    def get_batch(self, batch_size, data_specs=None):
        """
        .. todo::

            WRITEME
        """
        raise NotImplementedError()
        """
        try:
            idx = self.rng.randint(self.X.shape[0] - batch_size + 1)
        except ValueError:
            if batch_size > self.X.shape[0]:
                raise ValueError("Requested "+str(batch_size)+" examples"
                    "from a dataset containing only "+str(self.X.shape[0]))
            raise
        rx = self.X[idx:idx + batch_size, :]
        if include_labels:
            if self.y is None:
                return rx, None
            ry = self.y[idx:idx + batch_size]
            return rx, ry
        rx = np.cast[config.floatX](rx)
        return rx
        """

########NEW FILE########
__FILENAME__ = wiskott
"""
.. todo::

    WRITEME
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

import numpy as N
from pylearn2.datasets import dense_design_matrix
from pylearn2.utils.serial import load

class Wiskott(dense_design_matrix.DenseDesignMatrix):
    """
    .. todo::

        WRITEME
    """
    def __init__(self):
        path = "${PYLEARN2_DATA_PATH}/wiskott/wiskott"\
             + "_fish_layer0_15_standard_64x64_shuffled.npy"

        X = 1. - load(path)

        view_converter = dense_design_matrix.DefaultViewConverter((64,64,1))

        super(Wiskott,self).__init__(X = X, view_converter = view_converter)

        assert not N.any(N.isnan(self.X))

########NEW FILE########
__FILENAME__ = zca_dataset
"""
The ZCA Dataset class.

This is basically a prototype for a more general idea of being
able to invert preprocessors and view data in more than one
format. This should be expected to change, but had to go in
pylearn2 to support pylearn2/scripts/papers/maxout
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

import logging
import warnings
import numpy as np
from pylearn2.datasets.dense_design_matrix import DenseDesignMatrix
from pylearn2.config import yaml_parse
from pylearn2.datasets import control


logger = logging.getLogger(__name__)


class ZCA_Dataset(DenseDesignMatrix):
    """
    A Dataset that was created by ZCA whitening a DenseDesignMatrix.
    Supports viewing the data both in the new ZCA whitened space and
    mapping examples back to the original space.
    """

    def get_test_set(self):
        """
        .. todo::

            WRITEME
        """
        yaml = self.preprocessed_dataset.yaml_src
        yaml = yaml.replace('train', 'test')
        args = {}
        args.update(self.args)
        del args['self']
        args['start'] = None
        args['stop'] = None
        args['preprocessed_dataset'] = yaml_parse.load(yaml)
        return ZCA_Dataset(**args)

    def __init__(self,
                 preprocessed_dataset,
                 preprocessor,
                 convert_to_one_hot=True,
                 start=None,
                 stop=None,
                 axes=['b', 0, 1, 'c']):
        """
        .. todo::

            WRITEME
        """
        self.args = locals()

        self.preprocessed_dataset = preprocessed_dataset
        self.preprocessor = preprocessor
        self.rng = self.preprocessed_dataset.rng
        self.data_specs = preprocessed_dataset.data_specs
        self.X_space = preprocessed_dataset.X_space
        self.X_topo_space = preprocessed_dataset.X_topo_space
        self.view_converter = preprocessed_dataset.view_converter

        self.y = preprocessed_dataset.y
        self.y_labels = preprocessed_dataset.y_labels
        if convert_to_one_hot:
            if not (self.y.min() == 0):
                raise AssertionError("Expected y.min == 0 but y.min == %g" %
                                     self.y.min())
            nclass = self.y.max() + 1
            y = np.zeros((self.y.shape[0], nclass), dtype='float32')
            for i in xrange(self.y.shape[0]):
                y[i, self.y[i]] = 1.
            self.y = y
            assert self.y is not None
            space, source = self.data_specs
            space.components[source.index('targets')].dim = nclass

        if control.get_load_data():
            if start is not None:
                self.X = preprocessed_dataset.X[start:stop, :]
                if self.y is not None:
                    self.y = self.y[start:stop, :]
                assert self.X.shape[0] == stop-start
            else:
                self.X = preprocessed_dataset.X
        else:
            self.X = None
        if self.X is not None:
            if self.y is not None:
                assert self.y.shape[0] == self.X.shape[0]

        #self.mn = self.X.min()
        #self.mx = self.X.max()

        if getattr(preprocessor, "inv_P_", None) is None:
            warnings.warn("ZCA preprocessor.inv_P_ was none. Computing "
                          "inverse of preprocessor.P_ now. This will take "
                          "some time. For efficiency, it is recommended that "
                          "in the future you compute the inverse in ZCA.fit() "
                          "instead, by passing it compute_inverse=True.")
            logger.info('inverting...')
            preprocessor.inv_P_ = np.linalg.inv(preprocessor.P_)
            logger.info('...done inverting')

        self.view_converter.set_axes(axes)

    def has_targets(self):
        """
        .. todo::

            WRITEME
        """
        return self.preprocessed_dataset.has_targets()

    def adjust_for_viewer(self, X):
        """
        .. todo::

            WRITEME
        """
        #rval = X - self.mn
        #rval /= (self.mx-self.mn)

        #rval *= 2.
        #rval -= 1.
        rval = X.copy()

        #rval = np.clip(rval,-1.,1.)

        for i in xrange(rval.shape[0]):
            rval[i, :] /= np.abs(rval[i, :]).max() + 1e-12

        return rval

    def adjust_to_be_viewed_with(self, X, other, per_example=False):
        """
        .. todo::

            WRITEME
        """
        #rval = X - self.mn
        #rval /= (self.mx-self.mn)

        #rval *= 2.
        #rval -= 1.

        assert X.shape == other.shape, (X.shape, other.shape)

        rval = X.copy()

        if per_example:
            for i in xrange(rval.shape[0]):
                rval[i, :] /= np.abs(other[i, :]).max()
        else:
            rval /= np.abs(other).max()

        rval = np.clip(rval, -1., 1.)

        return rval

    def mapback_for_viewer(self, X):
        """
        .. todo::

            WRITEME
        """
        assert X.ndim == 2
        rval = self.preprocessor.inverse(X)
        rval = self.preprocessed_dataset.adjust_for_viewer(rval)

        return rval

    def mapback(self, X):
        """
        .. todo::

            WRITEME
        """
        return self.preprocessor.inverse(X)

########NEW FILE########
__FILENAME__ = dataset-get
#!/usr/bin/env python
# -*- coding: utf-8

########################################
#
#
# This file is intentionally monolithic.
# It also intentionally restricts itself
# to standard library modules, with no
# extra dependencies.
#

__authors__   = "Steven Pigeon"
__copyright__ = "(c) 2012, Universit de Montral"
__contact__   = "Steven Pigeon: pigeon@iro.umontreal.ca"
__version__   = "dataset-get 0.1"
__licence__   = "BSD 3-Clause http://www.opensource.org/licenses/BSD-3-Clause "

import logging
import re,os,sys,shutil,time
import warnings
import urllib,urllib2
import tarfile
import subprocess

logger = logging.getLogger(__name__)


########################################
class package_info:
    """
    A simple class to structure
    the package's information
    """
    def __init__(self, cf, name,ts,rs,src,whr):
        self.configuration_file=cf # in which configuration file was it found?
        self.name=name         # the short name, e.g., "mnist"
        self.timestamp=int(ts) # a unix ctime
        self.readable_size=rs  # a human-readable size, e.g., "401.3MB"
        self.source=src        # the web source
        self.where=whr         # where on this machine


########################################
#
# Global variables for the whole module.
#
dataset_sources="sources.lst"
dataset_web="http://www.stevenpigeon.org/secret"
dataset_conf_path=""
dataset_data_path=""
root_conf_path=None
root_data_path=None
user_conf_path=None
user_data_path=None
super_powers=False

# both dictionaries for fast search
# (but are semantically lists)
packages_sources={}
installed_packages_list={}

########################################
def local_path_as_url( filename ):
    """
    Takes a local, OS-specific path or
    filename and transforms it into an
    url starting with file:// (it
    simplifies a lot of things).

    :param filename: a relative or absolute pathname
    :returns: the urlified absolute path
    """
    return "file://"+urllib.pathname2url(os.path.abspath(filename))

########################################
def has_super_powers():
    """
    Determines whether or not the program
    is run as root.

    :returns: true if run as root, false otherwise
    """
    return os.geteuid()==0

########################################
def corename( filename ):
    """
    returns the 'corename' of a file. For
    example, corename("thingie.tar.bz2")
    returns "thingie" (a totally correct
    way of doing this would be to use
    MIME-approved standard extensions, in
    order to distinguish from, say a file
    "thingie.tar.bz2" and another file
    "my.data.tar.bz2"---for which we would
    have only "my" as corename)

    :param filename: a (base) filename
    :returns: the "core" filename
    """

    f1=None
    f2=os.path.basename(filename)

    # repeatedly remove the right-most
    # extension, until none is found
    #
    while f1 != f2:
        f1=f2
        (f2,ext)=os.path.splitext(f1)

    return f2

########################################
def get_timestamp_from_url( url ):
    """
    Gets the Last-Modified field from the
    http header associated with the file
    pointed to by the url. Raises whatever
    exception urllib2.urlopen raises.

    It can't lookup local file, unless they
    are presented as a file:/// url.

    :param url: a filename or an url
    :returns: the last-modified timestamp
    """

    obj = urllib2.urlopen( url )

    return time.strptime(
        obj.info()["Last-Modified"],
        "%a, %d %b %Y %H:%M:%S GMT") # RFC 2822 date format

########################################
def download_from_url( url, filename=None, progress_hook=None ):
    """
    Downloads a file from an URL in the
    specificed filename (or, if filename
    is None, to a temporary location).
    Returns the location of the downloaded
    file.

    :param url: url of the file to download
    :param filename: filename to download to (None means a temp file is created)
    :param progress_hook: a download hook to display progress
    :returns: the filename where the file was downloaded
    """
    (temp_filename, headers)=urllib.urlretrieve( url,filename,progress_hook )

    return temp_filename


########################################
def file_access_rights( filename, rights, check_above=False ):
    """
    Determines if a file has given rights.
    If the file exists, it tests for its
    rights. If it does not exist, and
    check_above=True, then it checks for
    the directory's rights, to test for
    write/creation rights.

    :param filename: filename of the file to assess
    :param rights: rights to be tested
    :param check_above: Check directory rights if file does not exist.
    :returns: boolean, whether 'rights' rights are OK
    """
    if os.path.exists(filename):
        return os.access(filename, rights)
    else:
        if check_above:
            return os.access(os.path.dirname(os.path.abspath(filename)), rights)
        else:
            return False



########################################
def atomic_replace( src_filename, dst_filename ):
    """
    Does an "atomic" replace of a file by another.

    If both files reside on the fame FS
    device, atomic_replace does a regular
    move. If not, the source file is first
    copied to a temporary location on the
    same FS as the destination, then a
    regular move is performed.

    caveat: the destination FS must have
    enough storage left for the temporary
    file.

    :param src_filename: The file to replace from
    :param dst_filename: The file to be replaced
    :raises: whatever shutil raises
    """

    ####################################
    def same_fs( filename_a, filename_b):
        """
        Checks if both files reside on the
        same FS device
        """
        stats_a = os.stat(filename_a)
        stats_b = os.stat(filename_b)
        return stats_a.st_dev == stats_b.st_dev;

    if os.path.exists(dst_filename) and not same_fs(src_filename,dst_filename):
        # deals with non-atomic move
        #
        dst_path = os.path.dirname(os.path.abspath(dst_filename))
        dst_temp_filename=os.tempnam(dst_path);
        shutil.copy(src_filename, dst_temp_filename) # non-atomic
        shutil.move(dst_temp_filename,dst_filename)  # atomic
    else:
        # an atomic move is performed
        # (both files are on the same device,
        # or the destination doesn't exist)
        #
        shutil.move(src_filename, dst_filename)




########################################
def set_defaults():
    """
    Detects whether the program is run
    as an ordinary user or as root, and
    then sets defauts directories for
    packages, configurations, and sources.

    caveat: this is an FreeDesktop-friendly
    version, and we will need eventually
    to have Windows- and OSX-friendly
    versions.

    See: http://freedesktop.org/wiki/Home
    and: http://www.linuxfoundation.org/collaborate/workgroups/lsb/fhs
    """

    global dataset_conf_path, \
           dataset_data_path, \
           root_conf_path, \
           root_data_path, \
           user_conf_path, \
           super_powers

    # a conspicuously LINUX version
    # (on windows, if we ever do a
    # windows version, these would
    # be different, and we may even
    # not have 'root' per se.)
    #
    root_conf_path="/etc/pylearn/"
    root_data_path="/usr/share/pylearn/dataset/"
    user_conf_path=os.path.join(os.environ["HOME"],".local/share/pylearn/")
    user_data_path=os.path.join(os.environ["HOME"],".local/share/pylearn/dataset/")

    if has_super_powers():
        dataset_conf_path=root_conf_path
        dataset_data_path=root_data_path
        super_powers=True
    else:
        dataset_conf_path=user_conf_path
        dataset_data_path=user_data_path
        super_powers=False


    # check if directories exist, and if not,
    # create them, and then fetch source.lst
    #
    if not os.path.exists(dataset_conf_path):
        os.makedirs(dataset_conf_path)

    if not os.path.exists(os.path.join(dataset_conf_path,dataset_sources)):
        atomic_update(os.path.join(dataset_web,dataset_sources),
                      os.path.join(dataset_conf_path,dataset_sources),
                      progress_bar)

    if not os.path.exists(dataset_data_path):
        os.makedirs(dataset_data_path)

    read_packages_sources()
    read_installed_packages_list();

########################################
def read_packages_sources():
    """
    Reads the sources.lst file and
    populates the available packages
    list.

    caveat: parsing of the sources.lst
    is pathetic

    Assigns: packages_sources
    :raises: RuntimeError if sources.lst cannot be read
    """

    def read_from_file(config_filename):
        """
        Reads a sources.lst file from a given location

        :param config_filename: the configuration file to read
        """

        global packages_sources
        try:
            f=open(config_filename,"r")
        except Exception as e:
            # not a problem if not found in a given location
            pass
        else:
            # file opened
            for line in f:
                t=line.rstrip().split(' ') # rstrips strips whitespaces at the end (\n)
                packages_sources[t[0]]=\
                    this_package=package_info(
                    config_filename,
                    t[0], # name
                    t[1], # timestamp
                    t[2], # human-readable size
                    urllib.unquote(t[3]), # source on the web
                    None)  # None as not installed (from source) (may be overridden later)

    if super_powers:
        read_from_file(os.path.join(dataset_conf_path,dataset_sources))
    else:
        # read root, user, then paths.
        paths=[ os.path.join(root_conf_path,dataset_sources),
                os.path.join(user_conf_path,dataset_sources) ]
        try:
            paths+=[ os.path.join(x,dataset_sources) for x in re.split(":|;",os.environ["PYLEARN2_DATA_PATH"]) ]
        except:
            # PYLEARN2_DATA_PATH may or mayn't be defined
            pass

    for path in paths:
        read_from_file(path)
    if len(packages_sources)==0:
        raise RuntimeError( "[cf] fatal: could not find/read sources.lst (unexpected!)" )


########################################
def read_installed_packages_list():
    """
    Reads the various installed.lst files
    found on the system. First it searches
    for the root-installed installed.lst,
    then the user's, then searches the
    locations specified by the environment
    variable PYLEARN2_DATA_PATH (which is
    a standard :-separated list of locations)

    Assigns: installed_packages_list
    """
    # note: we add and overwrite rather
    # than clearing and filling (so we can
    # read many installed.lst, but the last
    # ones read overrides the earlier ones)
    #


    def read_from_file(config_filename):
        """
        Reads an installed.lst file from a given location

        :param config_filename: the configuration file to read
        """

        global installed_packages_list
        try:
            installed_list_file=open(config_filename)
        except IOError, e:
            # not a problem if not found in a location
            pass
        else:
            # read from file and
            # create a dictionary
            #
            for line in installed_list_file:
                l=line.rstrip().split(' ') # removes trailing whitespaces (\n)

                if l:
                    installed_packages_list[l[0]]=\
                        this_package=package_info(
                        config_filename,
                        l[0], # name
                        l[1], # timestamp
                        l[2], # human-readable size
                        urllib.unquote(l[3]), # source on the web
                        urllib.unquote(l[4]))  # where installed
                else:
                    pass # skip blank lines (there shouldn't be any)


    if super_powers:
        # then read only root
        read_from_file(os.path.join(dataset_conf_path,"installed.lst"))
    else:
        # read root, user, then paths.
        paths=[ os.path.join(root_conf_path,"installed.lst"),
                os.path.join(user_conf_path,"installed.lst") ]
        try:
            paths+=[ os.path.join(x,"installed.lst") for x in re.split(":|;",os.environ["PYLEARN2_DATA_PATH"]) ]
        except:
            # PYLEARN2_DATA_PATH may or mayn't be defined
            pass

    for path in paths:
        read_from_file(path)
    if len(installed_packages_list)==0:
        logger.warning("[cf] no install.lst found "
                       "(will be created on install/upgrade)")


########################################
def write_installed_packages_list():
    """
    Saves the installed package list and
    their location (file over-writen depends
    on run as root or as a normal user)
    """
    global installed_packages_list
    try:
        tmp=open(os.path.join(dataset_conf_path,"installed.lst.2"),"w")
    except IOError,e:
        raise RuntimeError("[cf] fatal: cannot create temp file")
    else:
        # ok, probably worked?
        for package in installed_packages_list.values():
            # adds only packages that are readable for
            # this user (maybe some site-installed datasets
            # are out of his reach)
            #
            if package.where!=None and \
                    file_access_rights(os.path.join(package.where,package.name),
                                      os.F_OK | os.R_OK):
                print >>tmp,\
                    " ".join(map(str,[ package.name,
                                       package.timestamp,
                                       package.readable_size,
                                       urllib.quote(package.source,"/:~"),
                                       urllib.quote(package.where,"/:~") ] ))

        # replace the installed.lst in
        # a safe way
        atomic_replace(os.path.join(dataset_conf_path,"installed.lst.2"),
                       os.path.join(dataset_conf_path,"installed.lst"))


########################################
def atomic_update( remote_src, local_dst, hook=None ):
    """
    Takes a (possibly) remote file an checks
    if it is newer than a(n obligatoritly)
    local file. If the source is newer, an
    "atomic update" is performed.

    Atomic here means that the source is
    downloaded in a distinct location, and
    only if download is successful is the
    destination file replaced atomically.

    :param remote_src: Url to a (possibly) remote file
    :param local_dst: file to update
    :param hook: download progress hook
    :raises: various IOErrors
    """

    global hook_download_filename # hook-related

    try:
        remote_date = get_timestamp_from_url(remote_src);
    except IOError as e:
        raise IOError("[ts] %s %s" % (str(e),remote_src))
    else:
        if os.path.exists(local_dst):
            # it exists, test for update
            try:
                local_date = get_timestamp_from_url(local_path_as_url(local_dst))
            except Exception as e:
                raise IOError("[ts] %s %s" % (str(e),local_dst))
            else:
                if (local_date<remote_date):
                    # OK, the file seems to be out-of-date
                    # let's update it
                    #
                    if file_access_rights(local_dst,os.W_OK,check_above=True):
                        # we have write access to the file, or if it doesn't
                        # exist, to the directory where we want to write it.
                        #
                        try:
                            hook_download_filename=remote_src # hook-related
                            temp_filename=download_from_url(remote_src, filename=None, progress_hook=hook)
                        except Exception as e:
                            raise IOError("[dl] %s %s" % (str(e),remote_src))
                        else:
                            # download to temporary was successful,
                            # let's (try to) perform the atomic replace
                            #
                            try:
                                atomic_replace(temp_filename,local_dst)
                            except Exception as e:
                                raise IOError("[ac] %s %s --> %s" % (str(e),temp_filename,local_dst))
                    else:
                        raise IOError("[rw] no write access to %s " % local_dst )
                else:
                    # file's up to date, everything's fine
                    # and there's nothing else to do
                    #
                    pass
        else:
            # file does not exist, just download!
            #
            if file_access_rights(local_dst,os.W_OK,check_above=True):

                try:
                    hook_download_filename=remote_src # hook-related
                    temp_filename=download_from_url(remote_src, filename=None, progress_hook=hook)
                except Exception as e:
                    raise IOError("[dl] %s %s" % (str(e),remote_src))
                else:
                    # yay! download successful!
                    #
                    try:
                        atomic_replace(temp_filename,local_dst)
                    except Exception as e:
                        raise IOError("[ac] %s %s --> %s" % (str(e),temp_filename,local_dst))
            else:
                raise IOError("[rw] no right access to %s" % local_dst)


########################################
def unpack_tarball( tar_filename, dest_path ):
    """
    Unpacks a (bzipped2) tarball to a destination
    directory

    :param tar_filename: the bzipped2 tar file
    :param dest_path: a path to where expand the tarball
    :raises: various IOErrors
    """

    if os.path.exists(tar_filename):
        if file_access_rights(dest_path,os.W_OK,check_above=False):
            try:
                # open the tarball as read, bz2
                #
                this_tar_file=tarfile.open(tar_filename,"r:bz2")
            except Exception as e:
                raise IOError("[tar] cannot open '%s'" % tar_filename)
            else:
                # ok, it's openable, let's expand it
                #
                try:
                    this_tar_file.extractall(dest_path)
                except Exception as e:
                    raise IOError("[tar] error while extracting '%s'" %tar_filename)
                else:
                    # yay! success!
                    pass
        else:
            raise IOError("[tar] no right access to '%s'" % dest_path)

    else:
        raise IOError("'%s' not found" % tar_filename)


########################################
def run_scripts( package_location, scripts ):
    """
    Search for installation scripts speficied
    by the scripts list

    :param package_location: "root" path for the package
    :param scripts: list of scripts to look for (and execute)
    :raises: subprocess exceptions
    """

    path=os.path.join(package_location,"scripts/")

    cwd=os.getcwd()
    os.chdir(path)

    for script in scripts:

        if os.path.exists( script ):
            # throws CalledProcessError if return
            # return code is not zero.
            #
            try:
                subprocess.check_call( script, stdout=sys.stdout, stderr=sys.stderr  )
            except Exception:
                os.chdir(cwd)
                raise

    # ok, success (or not), let's unstack
    os.chdir(cwd)


########################################
def install_package( package, src, dst ):
    """
    Unpacks a (bzipped2) tarball and
    expands it to the given location.

    If unpacking is successful, installation
    scripts are run.

    :param package: package information
    :param src: the source tarball
    :param dst: the destination directory
    :raises: IOErrors and subprocess exceptions
    """

    #FIXME: change creation flags to group-public
    #       readable when invoked with super-powers
    #
    unpack_tarball(src,dst)
    run_scripts(dst+package.name, scripts=["getscript","postinst"] )


########################################
def remove_package(package,dst):
    """
    Removes a script by running the
    various removal scripts, then by
    deleting files and directories.

    :param package: package information
    :param dst: packages root (where packages are installed)
    """

    #path=os.path.join(dst,package.name)
    path=os.path.join(package.where,package.name)
    #print path

    run_scripts(path,scripts=["prerm"])
    shutil.rmtree(os.path.join(path,"data/"))
    shutil.rmtree(os.path.join(path,"docs/"))

    run_scripts(os.path.join(dst,package.name),scripts=["postrm"])
    shutil.rmtree(os.path.join(path,"scripts/"))
    shutil.rmtree(path)

    update_installed_list("r",package)




########################################
def update_installed_list( op, package ):
    """
    Updates the internal list of installed
    packages. The operation is either "i"
    for install and update, or "r" for removal

    :param op: the operation performed
    :param package: the package information
    :param dst: where the package was installed
    """

    if op=="i":
        installed_packages_list[package.name]=package;
    elif op=="r":
        # remove the package from the list
        del installed_packages_list[package.name]
    else:
        raise RuntimeError("[cf] fatal: invalid configuration op '%s'." % op)

    write_installed_packages_list()


########################################
def show_packages():
    """
    List all available packages, both
    installed or from remove sources
    """
    logger.info("These packages are available:")
    for this_package in packages_sources.values():
        if this_package.name in installed_packages_list:
            state="u" if installed_packages_list[this_package.name].timestamp<this_package.timestamp else 'i';
        else:
            state="-"
        package_time = time.strftime("%a, %d %b %Y %H:%M:%S",
                                     time.gmtime(this_package.timestamp))

        logger.info("{0} {1:<20} {2:<8} "
                    "{3:<30} {4}".format(state,
                                         this_package.name,
                                         this_package.readable_size,
                                         package_time,
                                         this_package.source))

########################################
def install_upgrade( package, upgrade=False,progress_hook=None ):
    """
    This function installs or upgrades a package.

    :param package: package information
    :param upgrade: If True, performs and upgrade, installs underwise
    :param progress_hook: a download progress hook
    """

    global hook_download_filename # hook-related

    if upgrade:
        operation = "[up] upgrading"
    else:
        operation = "[in] installing"
    logger.info("{0} '{1}' to {2}".format(operation,
                                          package.name, dataset_data_path))


    remote_src=package.source

    # install location is determined by super-powers
    # (so a root package can be upgraded locally!)
    package.where=dataset_data_path;

    # TODO: to add caching, first lookup the
    # tarball in the package cache (but there's'nt
    # one for now)
    #
    cached=False;

    if not cached:
        hook_download_filename=remote_src # hook-related
        temp_filename=download_from_url(remote_src,filename=None,progress_hook=progress_hook)
    else:
        # assign filename to cached package
        pass

    logger.info("[in] running install scripts "
                "for package '{0}'".format(package.name))

    # runs through the .../package_name/scripts/
    # directory and executes the scripts in a
    # specific order (which shouldn't display
    # much unless they fail)
    #
    install_package(package,temp_filename,dataset_data_path)
    update_installed_list("i",package)



########################################
def upgrade_packages(packages_to_upgrade, hook=None ):
    """
    Upgrades packages.

    If no packages are supplied, it will perform
    an "update-all" operation, finding all packages
    that are out of date.

    If packages names are supplied, only those
    are checked for upgrade (and upgraded if out
    of date)

    :param packages_to_upgrade: list of package names.
    :raises: IOErrors (from downloads/rights)
    """

    # get names only
    if packages_to_upgrade==[]:
        packages_to_upgrade=installed_packages_list.keys() # all installed!
        all_packages=True
    else:
        all_packages=False

    # check what packages are in the list,
    # and really to be upgraded.
    #
    packages_really_to_upgrade=[]
    for this_package in packages_to_upgrade:
        if this_package in installed_packages_list:

            # check if there's a date
            installed_date=installed_packages_list[this_package].timestamp

            if this_package in packages_sources:
                repo_date=packages_sources[this_package].timestamp

                if installed_date < repo_date:
                    # ok, there's a newer version
                    logger.info(this_package)
                    packages_really_to_upgrade.append(this_package)
                else:
                    # no newer version, nothing to update
                    pass
            else:
                logger.warning("[up] '{0}' is unknown "
                               "(installed from file?).".format(this_package))
        else:
            # not installed?
            if not all_packages:
                logger.warning("[up] '{0}' is not installed, "
                               "cannot upgrade.".format(this_package))
                pass


    # once we have determined which packages
    # are to be updated, we show them to the
    # user for him to confirm
    #
    if packages_really_to_upgrade!=[]:
        logger.info("[up] the following package(s) will be upgraded:")
        for this_package in packages_really_to_upgrade:
            readable_size = packages_sources[this_package].readable_size
            logger.info("{0} ({1})".format(this_package, readable_size))

        r=raw_input("Proceed? [yes/N] ")
        if r=='y' or r=='yes':
            for  this_package in packages_really_to_upgrade:
                install_upgrade( packages_sources[this_package], upgrade=True, progress_hook=hook )
        else:
            logger.info("[up] Taking '{0}' for no, so there.".format(r))
    else:
        # ok, nothing to upgrade,
        # move along.
        pass



########################################
#
# installs the packages, and forces if
# they already exist
#
# packages must be supplied as argument.
#
#
def install_packages( packages_to_install, force_install=False, hook=None ):
    """
    Installs the packages, possibly forcing installs.

    :param packages_to_install: list of package names
    :param force_install: if True, re-installs even if installed.
    :param hook: download progress hook
    :raises: IOErrors
    """

    if packages_to_install==[]:
        raise RuntimeError("[in] fatal: need packages names to install.")

    if force_install:
        logger.warning("[in] using the force")

    packages_really_to_install=[]
    for this_package in packages_to_install:
        if this_package in packages_sources:

            if force_install or not this_package in installed_packages_list:
                packages_really_to_install.append(this_package)
            else:
                logger.warning("[in] package '{0}' "
                               "is already installed".format(this_package))
        else:
            logger.warning("[in] unknown package '{0}'".format(this_package))

    if packages_really_to_install!=[]:
        logger.info("[in] The following package(s) will be installed:")
        for this_package in packages_really_to_install:
            readable_size = packages_sources[this_package].readable_size
            logger.info("{0} ({1})".format(this_package, readable_size))

        r=raw_input("Proceed? [yes/N] ")
        if r=='y' or r=='yes':
            for  this_package in packages_really_to_install:
                install_upgrade( packages_sources[this_package], upgrade=False, progress_hook=hook )
        else:
            logger.info("[in] Taking '{0}' for no, so there.".format(r))
    else:
        # ok, nothing to upgrade,
        # move along.
        pass



########################################
def install_packages_from_file( packages_to_install ):
    """
    (Force)Installs packages from files, but does
    not update installed.lst files.

    caveat: not as tested as everything else.

    :param packages_to_install: list of files to install
    :raises: IOErrors
    """
    if packages_to_install==[]:
        raise RuntimeError("[in] fatal: need packages names to install.")

    packages_really_to_install=[]
    for this_package in packages_to_install:
        if os.path.exists(this_package):
            packages_really_to_install.append(this_package)
        else:
            logger.warning("[in] package '{0}' not found".format(this_package))

    if packages_really_to_install!=[]:
        logger.info("[in] The following package(s) will be installed:")
        packages = []
        for this_package in packages_really_to_install:
            packages.append(corename(this_package))
        logger.info(' '.join(packages))

        r=raw_input("Proceed? [yes/N] ")
        if r=='y' or r=='yes':
            for  this_package in packages_really_to_install:
                #install_upgrade( this_package, upgrade=False, progress_hook=hook )
                if os.path.exists(dataset_data_path+corename(this_package)):
                    r=raw_input("[in] '%s' already installed, overwrite? [yes/N] " % corename(this_package))

                    if r!='y' and r!='yes':
                        logger.info("[in] skipping package "
                                    "'{0}'".format(corename(this_package)))
                        continue
                install_package( corename(this_package), this_package, dataset_data_path)
                #update_installed_list("i",(make a package object here),dataset_data_path)

        else:
            logger.info("[in] Taking '{0}' for no, so there.".format(r))




########################################
#
# uninstall packages, whether or not they
# are found in the sources.lst file (to
# account for the packages installed from
# file)
#
# like install, it expects a list, if there's
# no list, nothing happens. It will test
# whether or not the packages are installed, and
# will ask the user for a confirmation.
#
def remove_packages( packages_to_remove ):
    """
    Uninstall packages, whether or not they
    are found in the source.lst (so it can
    remove datasets installed from file).

    :param packages_to_remove: list of package names
    :raises: IOErrors
    """

    if packages_to_remove==[]:
        raise RuntimeError("[rm] fatal: need packages names to remove.")

    packages_really_to_remove=[]
    for this_package in packages_to_remove:
        if this_package in packages_sources:

            #this_data_set_location=os.path.join(dataset_data_path,this_package)

            # check if in the installed.lst
            # then if directory actually exists
            # then if you have rights to remove it
            if this_package in installed_packages_list:

                this_data_set_location=os.path.join( installed_packages_list[this_package].where,
                                                     installed_packages_list[this_package].name )

                if os.path.exists(this_data_set_location):
                    if (file_access_rights(this_data_set_location,os.W_OK)):
                        # ok, you may have rights to delete it
                        packages_really_to_remove.append(this_package)
                    else:
                        logger.warning("[rm] insufficient rights "
                                       "to remove '{0}'".format(this_package))
                else:
                    logger.warning("[rm] package '{0}' found in config file "
                                   "but not installed".format(this_package))
            else:
                logger.warning("[rm] package '{0}' "
                               "not installed".format(this_package))
        else:
            logger.warning("[rm] unknown package '{0}'".format(this_package))

    if packages_really_to_remove!=[]:
        logger.info("[rm] the following packages will be removed permanently:")
        packages = []
        for this_package in packages_really_to_remove:
            packages.append(this_package)
        logger.info(' '.join(packages))

        r=raw_input("Proceed? [yes/N] ")
        if r=='y' or r=='yes':
            for  this_package in packages_really_to_remove:
                remove_package( installed_packages_list[this_package], dataset_data_path )
        else:
            logger.info("[up] Taking '{0}' for no, so there.".format(r))
    else:
        # ok, nothing to remove, filenames where bad.
        pass




########################################
hook_download_filename=""
def progress_bar( blocks, blocksize, totalsize ):
    """
    Simple hook to show download progress.

    caveat: not that great-looking, fix later to
            a cooler progress bar or something.
    """
    print "\r[dl] %6.2f%% %s" % (min(totalsize,blocks*blocksize)*100.0/totalsize, hook_download_filename),
    sys.stdout.flush()



########################################
def process_arguments():
    """
    Processes the installation arguments (from
    the command line)

    The possible arguments are:

    list
         lists available datasets from
         sources.lst

    update
         updates sources.lst

    upgrade
         upgrades datasets that are out
         of date

    install <dataset1> <dataset2> ... <datasetn>
         uses sources.lst to locate the
         package and perform the installation

     force-install <dataset1> ... <datasetn>
         performs an install even if the data
         sets seem to be there.

     remove <dataset1> <dataset2> ... <datasetn>
         removes the dataset

     clean
         empties package cache (does nothing
         for now, because no cache.)
    """

    if len(sys.argv)>1:

        # due to the relative simplicity of the
        # arguments, we won't use optparse (2.3-2.6)
        # nor argparse (2.7+), although in the future
        # it may pose problems

        if sys.argv[1]=="list":
            show_packages()

        elif sys.argv[1]=="update":
            atomic_update( os.path.join(dataset_web,dataset_sources),
                           os.path.join(dataset_conf_path,dataset_sources),
                           hook=progress_bar)

        elif sys.argv[1]=="upgrade":
            upgrade_packages(sys.argv[2:],
                             hook=progress_bar)

        elif sys.argv[1]=="install":
            install_packages(sys.argv[2:],
                             hook=progress_bar)

        elif sys.argv[1]=="install-from-file":
            install_packages_from_file(sys.argv[2:])

        elif sys.argv[1]=="force-install":
            install_packages(sys.argv[2:],
                             force_install=True,
                             hook=progress_bar)

        elif sys.argv[1]=="remove":
            remove_packages(sys.argv[2:])

        elif sys.argv[1]=="clean":
            # does nothing, no cache implemented
            # yet.
            pass


        elif sys.argv[1]=="version":
            logger.info(__version__)

        else:
            raise RuntimeError("[cl] unknown command '%s'" % sys.argv[1])
    else:
        raise RuntimeError("[cl] missing command")



########################################
if __name__ == "__main__":
    # to remove RuntimeWarnings about how
    # tempfilename is unsafe.
    #
    warnings.simplefilter("ignore", RuntimeWarning)

    # OK, let's construct the environment
    # needed by dataset-get
    try:
        set_defaults()
    except Exception as e:
        logger.exception(e)
        exit(1) # fail!

    try:
        process_arguments()
    except Exception as e:
        logger.exception(e)
        exit(1)

########NEW FILE########
__FILENAME__ = dataset_resolver
#!/usr/bin/env python

"""A simple resolution mechanism to find datasets"""

import logging
import re,os,urllib

logger = logging.getLogger(__name__)


class dataset_resolver:

    ########################################
    class package_info:
        """
        A simple class to structure
        the package's information
        """
        def __init__(self, cf, name,ts,rs,src,whr):
            self.configuration_file=cf # in which configuration file was it found?
            self.name=name         # the short name, e.g., "mnist"
            self.timestamp=int(ts) # a unix ctime
            self.readable_size=rs  # a human-readable size, e.g., "401.3MB"
            self.source=src        # the web source
            self.where=whr         # where on this machine



    installed_packages_list={}

    def read_installed_packages_list(self, from_location):
        """
        Reads a given configuration file, and updates the
        internal installed packages list.

        :param from_location: a path (string) to a directory containing an installed.lst file
        """

        try:
            installed_list_file=open(from_location+"/installed.lst")
        except IOError, e:
            # maybe not a problem, but
            # FIXME: print a warning if exists,
            # but cannot be read (permissions)
            pass
        else:
            # read from file and
            # create a dictionary
            for line in installed_list_file:
                l=line.rstrip().split(' ')
                if l:
                    self.installed_packages_list[l[0]]=\
                        this_package=self.package_info(
                        from_location, # from which configuration files it comes
                        l[0], # name
                        l[1], # timestamp
                        l[2], # human-readable size
                        urllib.unquote(l[3]), # source on the web
                        urllib.unquote(l[4]))  # where installed
                else:
                    pass# skip blank lines (there shouldn't be any)


    def resolve_dataset(self,dataset_name):
        """
        Looks up a dataset name and return its location,
        or None if it's unknown.

        :param dataset_name: a canonical dataset name, 'mnist' for e.g.
        :returns: A path, if dataset is found, or None otherwise.
        """
        if dataset_name in self.installed_packages_list:
            return os.path.join( self.installed_packages_list[dataset_name].where,
                                 self.installed_packages_list[dataset_name].name )
        else:
            return None


    def __init__(self):
        """
        Scans possible locations to load installed.lst files. It first
        scans the root install, the user install, then the paths, from left
        to right, specified by the PYLEARN2_DATA_PATH environment variable.
        """

        paths= ["/etc/pylearn/", os.environ["HOME"]+"/.local/share/pylearn/"]
        try:
            paths+=re.split(":|;",os.environ["PYLEARN2_DATA_PATH"])
        except:
            # PYLEARN2_DATA_PATH may or mayn't be defined
            pass

        for path in paths:
            self.read_installed_packages_list(path)




if __name__=="__main__":
    # simplest tests
    x=dataset_resolver()
    logger.info(x.resolve_dataset("toaster-oven"))
    logger.info(x.resolve_dataset("fake-dataset"))

########NEW FILE########
__FILENAME__ = make-archive
#!/usr/bin/env python
# -*- coding: utf-8

__authors__   = "Steven Pigeon"
__copyright__ = "(c) 2012 Universit de Montral"
__contact__   = "Steven Pigeon: pigeon@iro.umontreal.ca"
__version__   = "make-archive 0.1"
__licence__   = "BSD 3-Clause http://www.opensource.org/licenses/BSD-3-Clause "


import logging
import os,re,sys,tarfile

logger = logging.getLogger(__name__)


########################################
def checks(path):
    """
    Checks if pretty much everything is
    there, aborts if mandatory elements
    are missing, warns if strongly
    suggested are not found.

    :param path: path to the root of the dataset
    :returns: True, if the archive passed the test, False otherwise.
    """
    # path,
    # m for mandatory,
    # o for optional
    # s for strongly suggested,
    #
    check_for=[ ("data/",'m'),
                ("docs/",'m'),
                ("docs/license.txt",'m'),
                ("scripts/",'m'),
                ("scripts/getscript",'o'),
                ("scripts/postinst",'o'),
                ("scripts/prerm",'o'),
                ("scripts/postrm",'o'),
                ("readme.1rst",'s')
                ]

    found=0
    for (filename,mode) in check_for:
        this_check=os.path.join(path,filename)
        if os.path.exists(this_check):
            if os.path.isdir(this_check):
                if len(os.listdir(this_check))==0:
                    logger.warning("directory '{0}' "
                                   "is empty.".format(this_check))
            found+=1;
        else:
            if mode=='m':
                # fatal
                logger.error("'{0}' not found "
                             "but mandatory".format(this_check))
                return False
            elif mode=='s':
                # benign
                logger.warning("no '{0}' found".format(this_check))
            else:
                # whatever
                pass
    return (found>0)

########################################
def create_archive( source, archive_name ):

    if os.path.exists(archive_name):
        r= raw_input("'%s' exists, overwrite? [yes/N] " % archive_name)
        if (r!="y") and (r!="yes"):
            logger.info("taking '{0}' for no, so there.".format(r))
            #bail out
            return

    try:
        tar=tarfile.open(archive_name,mode="w:bz2")
    except Exception, e:
        logger.exception(e)
        return
    else:
        for root, dirs, files in os.walk(source):
            for filename in files:
                this_file = os.path.join(root,filename)
                logger.info("adding '{0}'".format(this_file))
                tar.add(this_file)
        tar.close()


if __name__=="__main__":
    filename=sys.argv[1]
    if checks(filename):
        basename=os.path.basename(filename)
        ext=".tar.bz2"
        archive_name=basename+ext
        logger.info("Creating Archive '{0}'".format(archive_name))
        create_archive(filename,archive_name)
    else:
        logger.info("nothing found, aborting.")

########NEW FILE########
__FILENAME__ = make-sources
#!/usr/bin/env python
# -*- coding: utf-8

__authors__   = "Steven Pigeon"
__copyright__ = "(c) 2012, Universit de Montral"
__contact__   = "Steven Pigeon: pigeon@iro.umontreal.ca"
__version__   = "make-sources 0.1"
__licence__   = "BSD 3-Clause http://www.opensource.org/licenses/BSD-3-Clause "


import logging
import sys,os

logger = logging.getLogger(__name__)


########################################
def corename( filename ):
    """
    returns the 'corename' of a file. For
    example, corename("thingie.tar.bz2")
    returns "thingie" (a totally correct
    way of doing this would be to use
    MIME-approved standard extensions, in
    order to distinguish from, say a file
    "thingie.tar.bz2" and another file
    "my.data.tar.bz2"---for which we would
    have only "my" as corename)

    :param filename: a (base) filename
    :returns: the "core" filename
    """

    f1=None
    f2=os.path.basename(filename)

    # repeatedly remove the right-most
    # extension, until none is found
    #
    while f1 != f2:
        f1=f2
        (f2,ext)=os.path.splitext(f1)

    return f2

########################################
def human_readable_size(size):
    """
    Returns an approximate, human-readable,
    file size.

    :param size: an integer-like object
    :returns: a human-readable size as a string
    :raises: RunetimeError if file size exceeds petabytes (or is negative)
    """
    if (size>=0):
        for x in ['B','KB','MB','GB','TB','PB']:
            if size<1024:
                return ("%3.1f%s" if x!='B' else "%d%s") % (size, x)
            size/=1024.0

    raise RuntimeError("file size suspiciously large")


########################################
def print_table(where):
    """
    Generates the sources.lst table from path 'where'

    :param where: a (simple) path where to look for archives
    """
    for this_file in os.listdir(where):
        if ".tar.bz2" in this_file:
            full_file = os.path.join(where,this_file)
            size = human_readable_size(os.stat(full_file).st_size)
            logger.info("{0} {1} {2} "
                        "{3}".format(corename(this_file),
                                     os.path.getctime(full_file),
                                     size,
                                     os.path.join(repos_root, this_file)))

########################################
if __name__=="__main__":
    repos_root=sys.argv[1]
    print_table(sys.argv[2] if len(sys.argv)>2 else ".")

########NEW FILE########
__FILENAME__ = classifier
"""
Logistic regression.

Logistic regression is a probabilistic, linear classifier. It is parametrized
by a weight matrix :math:`W` and a bias vector :math:`b`. Classification is
done by projecting data points onto a set of hyperplanes, the distance to
which is used to determine a class membership probability.

Mathematically, this can be written as:

.. math::
  P(Y=i \\mid x, W, b) = \\text{softmax}(W x + b)_i
                       = \\frac {e^{W_i x + b_i}}{\\sum_j e^{W_j x + b_j}}

The output of the model or prediction is then done by taking the argmax of
the vector whose i'th element is :math:`P(Y=i \\mid x)`.

.. math::

  y_{pred} = \\arg\\max_i P(Y=i \\mid x, W, b)
"""

__docformat__ = 'restructedtext en'

# Third-party imports
import numpy
import theano
from theano import tensor

# Local imports
from pylearn2.base import Block
from pylearn2.utils import sharedX
from pylearn2.space import VectorSpace
from pylearn2.models import Model

class LogisticRegressionLayer(Block, Model):
    """
    Multi-class Logistic Regression Class

    This class contains only the part that computes the output (prediction),
    not the classification cost; see `cost.OneHotCrossEntropy` for that.

    Parameters
    ----------
    nvis : int
        number of input units, the dimension of the space in which the
        datapoints lie.
    nclasses : int
        number of output units, the dimension of the space in which the labels
        lie.
    """
    def __init__(self, nvis, nclasses):
        super(LogisticRegressionLayer, self).__init__()

        assert nvis >= 0, "Number of visible units must be non-negative"
        self.input_space = VectorSpace(nvis)
        self.output_space = VectorSpace(nclasses)
        assert nclasses >= 0, "Number of classes must be non-negative"

        self.nvis = nvis
        self.nclasses = nclasses

        # initialize with 0 the weights W as a matrix of shape (nvis, nclasses)
        self.W = sharedX(numpy.zeros((nvis, nclasses)), name='W', borrow=True)
        # initialize the biases b as a vector of nclasses 0s
        self.b = sharedX(numpy.zeros((nclasses,)), name='b', borrow=True)

        # parameters of the model
        self._params = [self.W, self.b]

    def p_y_given_x(self, inp):
        """
        Computes :math:`P(Y = i \\mid x)` corresponding to the layer's output.

        Parameters
        ----------
        inp : tensor_like
            The input used to compute `p_y_given_x`

        Returns
        -------
        p_y_given_x : tensor_like
            Theano symbolic expression for :math:`P(Y = i \\mid x)`
        """
        # compute vector of class-membership probabilities in symbolic form
        return tensor.nnet.softmax(tensor.dot(inp, self.W) + self.b)

    def predict_y(self, inp):
        """
        Predicts y given x by choosing :math:`\\arg\\max_i P(Y=i \\mid x,W,b)`.

        Parameters
        ----------
        inp : tensor_like
            The input used to predict y

        Returns
        -------
        predict_y : tensor_like
            Theano symbolic expression for the predicted y
        """
        # compute prediction as class whose probability is maximal in
        # symbolic form
        return tensor.argmax(self.p_y_given_x(inp), axis=1)

    def __call__(self, inp):
        """
        Forward propagate (symbolic) input through this module.

        This just aliases the `p_y_given_x` function for syntactic
        sugar/convenience.
        """
        return self.p_y_given_x(inp)

    # Use version defined in Model, rather than Block (which raises
    # NotImplementedError).
    get_input_space = Model.get_input_space
    get_output_space = Model.get_output_space


class CumulativeProbabilitiesLayer(LogisticRegressionLayer):
    """
    A layer whose output is seen as a discrete cumulative distribution
    function, i.e. unit i outputs :math:`P(Y \\leq i \\mid x)`.

    To ensure that the outputs are in ascending order, the weights
    matrix is shared between all units and the biases :math:`b_i` are
    transformed into

    .. math::

        c_i = c_{i-1} + \\text{softplus}(b_i), \\quad c_0 = b_0,

    so they're in ascending order.

    The outputs are used to compute `p_y_given_x` as

    .. math::

        P(Y = i \\mid x) = P(Y \\leq i \\mid x) - P(Y \\leq i - 1 \\mid x)

    In the special case in which units are saturated, some
    :math:`P(Y = i \\mid x) = 0` may appear. This can cause problems with
    regular negative log-likelihood. It is therefore recommended that the cost
    function used for this layer relies on :math:`p(Y \\leq i \\mid x)`.

    Parameters
    ----------
    nvis : int
        number of input units, the dimension of the space in which the
        datapoints lie.
    nclasses : int
        number of output units, the dimension of the space in which the labels
        lie.
    """

    def __init__(self, nvis, nclasses):
        super(CumulativeProbabilitiesLayer, self).__init__(nvis, nclasses)

        self.W = sharedX(numpy.zeros((nvis, 1)), name='W', borrow=True)
        self._params = [self.W, self.b]


    def p_y_ie_n(self, inp):
        """
        Computes the :math:`P(Y \\leq i \\mid x)` vector given an input.

        The implementation of this function relies on transformation
        matrices, which are explained within the code.

        Parameters
        ----------
        inp : tensor_like
            The input used to compute `p_y_ie_n`

        Returns
        -------
        p_y_ie_n : tensor_like
            Theano symbolic expression for :math:`P(Y \\leq i \\mid x)`
        """
        # As explained in the class docstring, to ensure that the outputs
        # are in ascending order, the W weights matrix is shared between
        # units and the bias vector is transformed so its elements are in
        # ascending order. We use
        #
        #  c_0 = b_0
        #  c_i = c_{i-1} + softplus(b_i)
        #      = c_{i-2} + softplus(b_{i-1}) + softplus(b_i)
        #      = ... = c_0 + softplus(b_1) + ... + softplus(b_i)
        #
        # which can be matricially represented as
        #
        # | c_0 |   | 1 0 ... 0 |   | b_0 |   | 0 0 0 ... 0 |           | b_0 |
        # | c_1 | = | 1 0 ... 0 | * | b_1 | + | 0 1 0 ... 0 | * softplus| b_1 |
        # | ... |   |     ...   |   | ... |   |     ...     |           | ... |
        # | c_n |   | 1 0 ... 0 |   | b_n |   | 0 1 1 ... 1 |           | b_n |
        #
        # or
        #
        # C = K_1 * B + K_2 * softplus(B)
        #
        # Here we generate K_1. Since B is a 1-dimension vector, we must
        # transpose K_1 to make it work.
        k1_val = numpy.zeros((self.nclasses, self.nclasses),
                             dtype=theano.config.floatX)
        k1_val[:, 0] = 1.0
        k1_val = numpy.transpose(k1_val)
        k1 = theano.shared(value=k1_val, name='k1')
        # Here we generate K_2, which is transposed for the same reason.
        # We first create a (nclasses - 1) x (nclasses - 1) lower triangular
        # matrix and then append the necessary left and upper zeros.
        k2_val = numpy.ones((self.nclasses - 1, self.nclasses - 1),
                            dtype=theano.config.floatX)
        for i in xrange(self.nclasses - 1):
            for j in xrange(self.nclasses - 1):
                if(j > i):
                    k2_val[i, j] = 0
        k2_val = numpy.append(numpy.zeros((self.nclasses - 1, 1),
                                          dtype=theano.config.floatX),
                              k2_val, axis=1)
        k2_val = numpy.append(numpy.zeros((1, self.nclasses),
                                          dtype=theano.config.floatX),
                              k2_val, axis=0)
        k2_val = numpy.transpose(k2_val)
        k2 = theano.shared(value=k2_val, name='k2')

        # The K_1 * B term
        b0_term = tensor.dot(self.b, k1)

        # The K_2 * softplus(B) term
        softplus_term = tensor.dot(tensor.nnet.softplus(self.b), k2)

        # The constructed bias that is ensured to be in ascending order
        c = b0_term + softplus_term

        # Since W is a column vector, we need to expand it into a matrix.
        weights_constructor_val = numpy.ones((1, self.nclasses))
        weights_constructor = theano.shared(value=weights_constructor_val,
                                            name='weights_constructor')
        weights_matrix = tensor.dot(self.W,  weights_constructor)

        # Compute p(y <= i | x)
        return tensor.nnet.sigmoid(tensor.dot(inp, weights_matrix) + c)

    def p_y_given_x(self, inp):
        """
        Computes the :math:`P(Y = i \\mid x)` vector as

        .. math::

            P(Y = i \\mid x) = P(Y \\leq i \\mid x) - P(Y \\leq i - 1 \\mid x)

        As mentioned in the `p_y_ie_n` method, the cost function should
        not rely on `p_y_given_x` because of the zero probabilities that can
        arise if the output units are saturated. It is instead recommended
        to use `p_y_ie_n` in the cost function expression.

        Parameters
        ----------
        inp : tensor_like
            The input used to compute `p_y_given_x`

        Returns
        -------
        p_y_given_x : tensor_like
            Theano symbolic expression for :math:`P(Y = i \\mid x)`
        """
        # The expression for p(y = i | x) can be represented in matricial
        # form as
        #
        # P = A * P_prime, P = p(y = i | x), P_prime = p(y <= i | x),
        #                  A = |  1  0  0  0  ...  0  0  |
        #                      | -1  1  0  0  ...  0  0  |
        #                      |  0 -1  1  0  ...  0  0  |
        #                      |          ...      0  0  |
        #                      |  0  0  0  0  ... -1  1  |
        #
        # Here we compute A_transposed because p_y_given_x is a 1-dimension
        # vector.
        a = numpy.eye(self.nclasses, dtype=theano.config.floatX)
        for i in xrange(self.nclasses):
            if(i > 0):
                a[i - 1, i] = -1

        p_y_given_x_mat = theano.shared(value=a, name='p_y_given_x_mat')

        # Compute p_y_given_x
        return tensor.dot(self.p_y_ie_n(inp), p_y_given_x_mat)

########NEW FILE########
__FILENAME__ = list_files
"""Code for listing files that belong to the library."""
import logging
import pylearn2
import os
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

logger = logging.getLogger(__name__)


def list_files(suffix=""):
    """
    Returns a list of all files in pylearn2 with the given suffix.

    Parameters
    ----------
    suffix : str

    Returns
    -------

    file_list : list
        A list of all files in pylearn2 whose filepath ends with `suffix`
    """

    pl2_path, = pylearn2.__path__

    file_list = _list_files(pl2_path, suffix)

    return file_list


def _list_files(path, suffix=""):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    path : str
        a filepath
    suffix : str

    Returns
    -------
    l : list
        A list of all files ending in `suffix` contained within `path`.
        (If `path` is a file rather than a directory, it is considered
        to "contain" itself)
    """
    if os.path.isdir(path):
        incomplete = os.listdir(path)
        complete = [os.path.join(path, entry) for entry in incomplete]
        lists = [_list_files(subpath, suffix) for subpath in complete]
        flattened = []
        for l in lists:
            for elem in l:
                flattened.append(elem)
        return flattened
    else:
        assert os.path.exists(path)
        if path.endswith(suffix):
            return [path]
        return []

if __name__ == '__main__':
    # Print all .py files in the library
    result = list_files('.py')
    for path in result:
        logger.info(path)

########NEW FILE########
__FILENAME__ = nan_guard
"""
Functionality for detecting NaNs in a Theano graph.
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

import logging
from theano.compile import Mode
import theano
import numpy as np
from pylearn2.models.dbm import flatten


logger = logging.getLogger(__name__)


class NanGuardMode(Mode):
    """
    A Theano compilation Mode that makes the compiled function automatically
    detect NaNs and Infs and detect an error if they occur.

    Parameters
    ----------
    nan_is_error : bool
        If True, raise an error anytime a NaN is encountered
    inf_is_error: bool
        If True, raise an error anytime an Inf is encountered.  Note that some
        pylearn2 modules currently use np.inf as a default value (e.g.
        mlp.max_pool) and these will cause an error if inf_is_error is True.
    big_is_error: bool
        If True, raise an error when a value greater than 1e10 is encountered.
    """
    def __init__(self, nan_is_error, inf_is_error, big_is_error=True):
        def do_check_on(var, nd, f, is_input):
            """
            Checks `var` for NaNs / Infs. If detected, raises an exception
            and / or prints information about `nd`, `f`, and `is_input` to
            help the user determine the cause of the invalid values.

            Parameters
            ----------
            var : numpy.ndarray
                The value to be checked.
            nd : theano.gof.Apply
                The Apply node being executed
            f : callable
                The thunk for the apply node
            is_input : bool
                If True, `var` is an input to `nd`.
                If False, it is an output.
            """
            error = False
            if nan_is_error:
                if np.any(np.isnan(var)):
                    logger.error('NaN detected')
                    error = True
            if inf_is_error:
                if np.any(np.isinf(var)):
                    logger.error('Inf detected')
                    error = True
            if big_is_error:
                if np.abs(var).max() > 1e10:
                    logger.error('Big value detected')
                    error = True
            if error:
                if is_input:
                    logger.error('In an input')
                else:
                    logger.error('In an output')
                logger.error('Inputs: ')
                for ivar, ival in zip(nd.inputs, f.inputs):
                    logger.error('var')
                    logger.error(ivar)
                    logger.error(theano.printing.min_informative_str(ivar))
                    logger.error('val')
                    logger.error(ival)
                logger.error('Node:')
                logger.error(nd)
                assert False

        def nan_check(i, node, fn):
            """
            Runs `fn` while checking its inputs and outputs for NaNs / Infs

            Parameters
            ----------
            i : currently ignored (TODO: determine why it is here or remove)
            node : theano.gof.Apply
                The Apply node currently being executed
            fn : callable
                The thunk to execute for this Apply node
            """
            inputs = fn.inputs
            # TODO: figure out why individual inputs are themselves lists sometimes
            for x in flatten(inputs):
                do_check_on(x, node, fn, True)
            fn()
            outputs = fn.outputs
            for j, x in enumerate(flatten(outputs)):
                do_check_on(x, node, fn, False)

        wrap_linker = theano.gof.WrapLinkerMany([theano.gof.OpWiseCLinker()], [nan_check])
        super(NanGuardMode, self).__init__(wrap_linker, optimizer='fast_run')

########NEW FILE########
__FILENAME__ = record
"""
This functionality has been moved to theano. Links here maintained to
avoid breaking old import statements
"""
from theano.tests.record import MismatchError, Record, RecordMode

import warnings
warnings.warn("pylearn2.devtools.record is deprecated and may be removed "
        "on or after Aug 20, 2014. This functionality has been moved to "
        "theano.tests.record.")

########NEW FILE########
__FILENAME__ = run_pyflakes
"""
Can be run as a script or imported as a module.

Module exposes the run_pyflakes method which returns a dictionary.

As a script:

    python run_pyflakes.py <no_warnings>

    prints out all the errors in the library
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

import logging
from pylearn2.devtools.list_files import list_files
from pylearn2.utils.shell import run_shell_command


logger = logging.getLogger(__name__)


def run_pyflakes(no_warnings = False):
    """
    Return a description of all errors pyflakes finds in Pylearn2.

    Parameters
    ----------
    no_warnings : bool
        If True, omits pyflakes outputs that don't correspond to actual
        errors.

    Returns
    -------
    rval : dict
        Keys are pylearn2 .py filepaths
        Values are outputs from pyflakes
    """

    files = list_files(".py")

    rval = {}

    for filepath in files:
        output, rc = run_shell_command('pyflakes ' + filepath)
        if 'pyflakes: not found' in output:
            # The return code alone does not make it possible to detect
            # if pyflakes is present or not. When pyflakes is not present,
            # the return code seems to always be 127, but 127 can also be
            # the result of finding 127 warnings in a file.
            # Therefore, we examine the output instead.
            raise RuntimeError("Couldn't run 'pyflakes " + filepath + "'. "
                    "Error code returned:" + str(rc)\
                    + " Output was: " + output)

        output = _filter(output, no_warnings)

        if output is not None:
            rval[filepath] = output

    return rval

def _filter(output, no_warnings):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    output : str
        The output of pyflakes for a single.py file
    no_warnings: bool
        If True, removes lines corresponding to warnings rather than errors

    Returns
    -------
    rval : None or str
        `output` with blank lines and optionally lines corresponding to
        warnings removed, or, if all lines are removed, returns None.
        A return value of None indicates that the file is validly formatted.
    """
    lines = output.split('\n')

    lines = [ line for line in lines if line != '' ]


    if no_warnings:

        lines = [ line for line in lines if
                line.find("is assigned to but never used") == -1 ]

        lines = [ line for line in lines if
                line.find('imported but unused') == -1 ]

        lines = [ line for line in lines if
                line.find('redefinition of unused ') == -1 ]


    if len(lines) == 0:
        return None
    return '\n'.join(lines)

if __name__ == '__main__':
    import sys
    if len(sys.argv) > 1:
        no_warnings = bool(sys.argv[1])
    else:
        no_warnings = False

    d = run_pyflakes( no_warnings = no_warnings)

    for key in d:
        logger.info('{0}:'.format(key))
        for l in d[key].split('\n'):
            logger.info('\t{0}'.format(l))

########NEW FILE########
__FILENAME__ = docscrape
"""Extract reference documentation from the NumPy source tree.

"""

import inspect
from nose.plugins.skip import SkipTest
import re
import sys
import types

class Reader(object):
    """A line-based string reader.

    """
    def __init__(self, data):
        """
        Parameters
        ----------
        data : str
           String with lines separated by '\n'.

        """
        if isinstance(data,list):
            self._str = data
        else:
            self._str = data.split('\n') # store string as list of lines

        self.reset()

    def __getitem__(self, n):
        return self._str[n]

    def reset(self):
        self._l = 0 # current line nr

    def read(self):
        if not self.eof():
            out = self[self._l]
            self._l += 1
            return out
        else:
            return ''

    def seek_next_non_empty_line(self):
        for l in self[self._l:]:
            if l.strip():
                break
            else:
                self._l += 1

    def eof(self):
        return self._l >= len(self._str)

    def read_to_condition(self, condition_func):
        start = self._l
        for line in self[start:]:
            if condition_func(line):
                return self[start:self._l]
            self._l += 1
            if self.eof():
                return self[start:self._l+1]
        return []

    def read_to_next_empty_line(self):
        self.seek_next_non_empty_line()
        def is_empty(line):
            return not line.strip()
        return self.read_to_condition(is_empty)

    def read_to_next_unindented_line(self):
        def is_unindented(line):
            return (line.strip() and (len(line.lstrip()) == len(line)))
        return self.read_to_condition(is_unindented)

    def peek(self,n=0):
        if self._l + n < len(self._str):
            return self[self._l + n]
        else:
            return ''

    def is_empty(self):
        return not ''.join(self._str).strip()

    def __iter__(self):
        for line in self._str:
            yield line

class NumpyDocString(object):
    def __init__(self, docstring, name=None):
        if name:
            self.name = name
        docstring = docstring.split('\n')

        # De-indent paragraph
        try:
            indent = min(len(s) - len(s.lstrip()) for s in docstring
                         if s.strip())
        except ValueError:
            indent = 0

        for n,line in enumerate(docstring):
            docstring[n] = docstring[n][indent:]

        self._doc = Reader(docstring)
        self._parsed_data = {
            'Signature': '',
            'Summary': '',
            'Extended Summary': [],
            'Parameters': [],
            'Other Parameters': [],
            'Returns': [],
            'Raises': [],
            'Warns': [],
            'See Also': [],
            'Notes': [],
            'References': '',
            'Examples': '',
            'index': {},
            'Attributes': [],
            'Methods': [],
            }
        self.section_order = []

        self._parse()

    def __getitem__(self,key):
        return self._parsed_data[key]

    def __setitem__(self,key,val):
        if not self._parsed_data.has_key(key):
            raise ValueError("Unknown section %s" % key)
        else:
            self._parsed_data[key] = val

    def _is_at_section(self):
        self._doc.seek_next_non_empty_line()

        if self._doc.eof():
            return False

        l1 = self._doc.peek().strip()  # e.g. Parameters

        if l1.startswith('.. index::'):
            return True

        l2 = self._doc.peek(1).strip() #    ----------
        return (len(l1) == len(l2) and l2 == '-'*len(l1))

    def _strip(self,doc):
        i = 0
        j = 0
        for i,line in enumerate(doc):
            if line.strip(): break

        for j,line in enumerate(doc[::-1]):
            if line.strip(): break

        return doc[i:len(doc)-j]

    def _read_to_next_section(self):
        section = self._doc.read_to_next_empty_line()

        while not self._is_at_section() and not self._doc.eof():
            if not self._doc.peek(-1).strip(): # previous line was empty
                section += ['']

            section += self._doc.read_to_next_empty_line()

        return section

    def _read_sections(self):
        while not self._doc.eof():
            data = self._read_to_next_section()
            name = data[0].strip()

            if name.startswith('..'): # index section
                yield name, data[1:]
            elif len(data) < 2:
                yield StopIteration
            else:
                yield name, self._strip(data[2:])

    def _parse_param_list(self,content):
        r = Reader(content)
        params = []
        while not r.eof():
            header = r.read().strip()
            if ' : ' in header:
                arg_name, arg_type = header.split(' : ')[:2]
            else:
                arg_name, arg_type = header, ''

            desc = r.read_to_next_unindented_line()
            for n,line in enumerate(desc):
                desc[n] = line.strip()
            desc = desc #'\n'.join(desc)

            params.append((arg_name,arg_type,desc))

        return params

    def _parse_see_also(self, content):
        """
        func_name : Descriptive text
            continued text
        another_func_name : Descriptive text
        func_name1, func_name2, func_name3

        """
        functions = []
        current_func = None
        rest = []
        for line in content:
            if not line.strip(): continue
            if ':' in line:
                if current_func:
                    functions.append((current_func, rest))
                r = line.split(':', 1)
                current_func = r[0].strip()
                r[1] = r[1].strip()
                if r[1]:
                    rest = [r[1]]
                else:
                    rest = []
            elif not line.startswith(' '):
                if current_func:
                    functions.append((current_func, rest))
                    current_func = None
                    rest = []
                if ',' in line:
                    for func in line.split(','):
                        func = func.strip()
                        if func:
                            functions.append((func, []))
                elif line.strip():
                    current_func = line.strip()
            elif current_func is not None:
                rest.append(line.strip())
        if current_func:
            functions.append((current_func, rest))
        return functions

    def _parse_index(self, section, content):
        """
        .. index: default
           :refguide: something, else, and more

        """
        def strip_each_in(lst):
            return [s.strip() for s in lst]

        out = {}
        section = section.split('::')
        if len(section) > 1:
            out['default'] = strip_each_in(section[1].split(','))[0]
        for line in content:
            line = line.split(':')
            if len(line) > 2:
                out[line[1]] = strip_each_in(line[2].split(','))
        return out

    def _parse_summary(self):
        """Grab signature (if given) and summary"""
        summary = self._doc.read_to_next_empty_line()
        summary_str = "\n".join([s.strip() for s in summary])
        if re.compile('^([\w. ]+=)?[\w\.]+\(.*\)$').match(summary_str):
            self['Signature'] = summary_str
            if not self._is_at_section():
                self['Summary'] = self._doc.read_to_next_empty_line()
        elif re.compile('^[\w]+\n[-]+').match(summary_str):
            self['Summary'] = ''
            self._doc.reset()
        else:
            self['Summary'] = summary

        if not self._is_at_section():
            self['Extended Summary'] = self._read_to_next_section()

    def _parse(self):
        self._doc.reset()
        self._parse_summary()
        for (section, content) in self._read_sections():
            if not section.startswith('..'):
                section = ' '.join([s.capitalize()
                                    for s in section.split(' ')])
            if section in ('Parameters', 'Other Parameters', 'Returns',
                           'Raises', 'Warns', 'Attributes', 'Methods'):
                self[section] = self._parse_param_list(content)
                self.section_order.append(section)
            elif section.startswith('.. index::'):
                self['index'] = self._parse_index(section, content)
                self.section_order.append('index')
            elif section.lower() == 'see also':
                self['See Also'] = self._parse_see_also(content)
                self.section_order.append('See Also')
            else:
                self[section] = content
                self.section_order.append(section)

    # string conversion routines

    def _str_header(self, name, symbol='-'):
        return [name, len(name)*symbol]

    def _str_indent(self, doc, indent=4):
        out = []
        for line in doc:
            out += [' '*indent + line]
        return out

    def _str_signature(self):
        if not self['Signature']:
            return []
        return ["*%s*" % self['Signature'].replace('*','\*')] + ['']

    def _str_summary(self):
        return self['Summary'] + ['']

    def _str_extended_summary(self):
        return self['Extended Summary'] + ['']

    def _str_param_list(self, name):
        out = []
        if self[name]:
            out += self._str_header(name)
            for param,param_type,desc in self[name]:
                out += ['%s : %s' % (param, param_type)]
                out += self._str_indent(desc)
            out += ['']
        return out

    def _str_section(self, name):
        out = []
        if self[name]:
            out += self._str_header(name)
            out += self[name]
            out += ['']
        return out

    def _str_see_also(self):
        if not self['See Also']: return []
        out = []
        out += self._str_header("See Also")
        last_had_desc = True
        for func, desc in self['See Also']:
            if desc or last_had_desc:
                out += ['']
                out += ["`%s`_" % func]
            else:
                out[-1] += ", `%s`_" % func
            if desc:
                out += self._str_indent(desc)
                last_had_desc = True
            else:
                last_had_desc = False
        out += ['']
        return out

    def _str_index(self):
        idx = self['index']
        out = []
        out += ['.. index:: %s' % idx.get('default','')]
        for section, references in idx.iteritems():
            if section == 'default':
                continue
            out += ['   :%s: %s' % (section, ', '.join(references))]
        return out

    def __str__(self):
        out = []
        out += self._str_signature()
        out += self._str_summary()
        out += self._str_extended_summary()
        for param_list in ('Parameters', 'Other Parameters',
                           'Returns', 'Raises', 'Warns'):
            out += self._str_param_list(param_list)
        out += self._str_see_also()
        for s in ('Notes','References','Examples'):
            out += self._str_section(s)
        out += self._str_index()
        return '\n'.join(out)

    # --

    def get_errors(self, check_order=True):
        errors = []
        self._doc.reset()
        for j, line in enumerate(self._doc):
            if len(line) > 75:
                if hasattr(self, 'name'):
                    errors.append("%s: Line %d exceeds 75 chars"
                            ": \"%s\"..." % (self.name, j+1, line[:30]))
                else:
                    errors.append("Line %d exceeds 75 chars"
                                  ": \"%s\"..." % (j+1, line[:30]))

        if check_order:
            canonical_order = ['Signature', 'Summary', 'Extended Summary',
                               'Attributes', 'Methods', 'Parameters',
                               'Other Parameters','Returns', 'Raises', 'Warns',
                               'See Also', 'Notes', 'References', 'Examples',
                               'index']

            canonical_order_copy = list(canonical_order)

            for s in self.section_order:
                while canonical_order_copy and s != canonical_order_copy[0]:
                    canonical_order_copy.pop(0)
                    if not canonical_order_copy:
                        errors.append(
                            "Sections in wrong order (starting at %s). The"
                            " right order is %s" % (s, canonical_order))

        return errors

def indent(str,indent=4):
    indent_str = ' '*indent
    if str is None:
        return indent_str
    lines = str.split('\n')
    return '\n'.join(indent_str + l for l in lines)

class NumpyFunctionDocString(NumpyDocString):
    def __init__(self, docstring, function):
        super(NumpyFunctionDocString, self).__init__(docstring)
        args, varargs, keywords, defaults = inspect.getargspec(function)
        if (args and args != ['self']) or varargs or keywords or defaults:
            self.has_parameters = True
        else:
            self.has_parameters = False

    def _parse(self):
        self._parsed_data = {
            'Signature': '',
            'Summary': '',
            'Extended Summary': [],
            'Parameters': [],
            'Other Parameters': [],
            'Returns': [],
            'Raises': [],
            'Warns': [],
            'See Also': [],
            'Notes': [],
            'References': '',
            'Examples': '',
            'index': {},
            }
        return NumpyDocString._parse(self)

    def get_errors(self):
        errors = NumpyDocString.get_errors(self)

        if not self['Signature']:
            #errors.append("No function signature") #this check is currently
                                                    #too restrictive. Disabling
                                                    #it for now
            pass

        if not self['Summary']:
            errors.append("No function summary line")

        if len(" ".join(self['Summary'])) > 3*80:
            errors.append("Brief function summary is longer than 3 lines")

        if not self['Parameters'] and self.has_parameters:
            errors.append("No Parameters section")

        return errors

class NumpyClassDocString(NumpyDocString):
    def __init__(self, docstring, class_name, class_object):
        super(NumpyClassDocString, self).__init__(docstring)
        self.class_name = class_name
        methods = dict((name, func) for name, func
                       in inspect.getmembers(class_object))

        self.has_parameters = False
        if '__init__' in methods:
            # verify if __init__ is a Python function. If it isn't
            # (e.g. the function is implemented in C), getargspec will fail
            if not inspect.ismethod(methods['__init__']):
                return
            args, varargs, keywords, defaults = inspect.getargspec(
                methods['__init__'])
            if (args and args != ['self']) or varargs or keywords or defaults:
                self.has_parameters = True

    def _parse(self):
        self._parsed_data = {
            'Signature': '',
            'Summary': '',
            'Extended Summary': [],
            'Parameters': [],
            'Other Parameters': [],
            'Raises': [],
            'Warns': [],
            'See Also': [],
            'Notes': [],
            'References': '',
            'Examples': '',
            'index': {},
            'Attributes': [],
            'Methods': [],
            }
        return NumpyDocString._parse(self)

    def __str__(self):
        out = []
        out += self._str_signature()
        out += self._str_summary()
        out += self._str_extended_summary()
        for param_list in ('Attributes', 'Methods', 'Parameters', 'Raises',
                           'Warns'):
            out += self._str_param_list(param_list)
        out += self._str_see_also()
        for s in ('Notes','References','Examples'):
            out += self._str_section(s)
        out += self._str_index()
        return '\n'.join(out)

    def get_errors(self):
        errors = NumpyDocString.get_errors(self)
        if not self['Parameters'] and self.has_parameters:
            errors.append("%s class has no Parameters section"
                          % self.class_name)
        return errors

class NumpyModuleDocString(NumpyDocString):
    """
    Module doc strings: no parsing is done.

    """

    def _parse(self):
        self.out = []

    def __str__(self):
        return "\n".join(self._doc._str)

    def get_errors(self):
        errors = NumpyDocString.get_errors(self, check_order=False)
        return errors

def header(text, style='-'):
    return text + '\n' + style*len(text) + '\n'


class SphinxDocString(NumpyDocString):
    # string conversion routines
    def _str_header(self, name, symbol='`'):
        return ['**' + name + '**'] + [symbol*(len(name)+4)]

    def _str_indent(self, doc, indent=4):
        out = []
        for line in doc:
            out += [' '*indent + line]
        return out

    def _str_signature(self):
        return ['``%s``' % self['Signature'].replace('*','\*')] + ['']

    def _str_summary(self):
        return self['Summary'] + ['']

    def _str_extended_summary(self):
        return self['Extended Summary'] + ['']

    def _str_param_list(self, name):
        out = []
        if self[name]:
            out += self._str_header(name)
            out += ['']
            for param,param_type,desc in self[name]:
                out += self._str_indent(['**%s** : %s' % (param, param_type)])
                out += ['']
                out += self._str_indent(desc,8)
                out += ['']
        return out

    def _str_section(self, name):
        out = []
        if self[name]:
            out += self._str_header(name)
            out += ['']
            content = self._str_indent(self[name])
            out += content
            out += ['']
        return out

    def _str_index(self):
        idx = self['index']
        out = []
        out += ['.. index:: %s' % idx.get('default','')]
        for section, references in idx.iteritems():
            if section == 'default':
                continue
            out += ['   :%s: %s' % (section, ', '.join(references))]
        return out

    def __str__(self, indent=0):
        out = []
        out += self._str_summary()
        out += self._str_extended_summary()
        for param_list in ('Parameters','Returns','Raises','Warns'):
            out += self._str_param_list(param_list)
        for s in ('Notes','References','Examples'):
            out += self._str_section(s)
        #        out += self._str_index()
        out = self._str_indent(out,indent)
        return '\n'.join(out)

class FunctionDoc(object):
    def __init__(self,func):
        self._f = func

    def __str__(self):
        out = ''
        doclines = inspect.getdoc(self._f) or ''
        try:
            doc = SphinxDocString(doclines)
        except Exception, e:
            print '*'*78
            print "ERROR: '%s' while parsing `%s`" % (e, self._f)
            print '*'*78
            #print "Docstring follows:"
            #print doclines
            #print '='*78
            return out

        if doc['Signature']:
            out += '%s\n' % header('**%s**' %
                                   doc['Signature'].replace('*','\*'), '-')
        else:
            try:
                # try to read signature
                argspec = inspect.getargspec(self._f)
                argspec = inspect.formatargspec(*argspec)
                argspec = argspec.replace('*','\*')
                out += header('%s%s' % (self._f.__name__, argspec), '-')
            except TypeError, e:
                out += '%s\n' % header('**%s()**'  % self._f.__name__, '-')

        out += str(doc)
        return out


class ClassDoc(object):
    def __init__(self,cls,modulename=''):
        if not inspect.isclass(cls):
            raise ValueError("Initialise using an object")
        self._cls = cls

        if modulename and not modulename.endswith('.'):
            modulename += '.'
        self._mod = modulename
        self._name = cls.__name__

    @property
    def methods(self):
        return [name for name,func in inspect.getmembers(self._cls)
                if not name.startswith('_') and callable(func)]

    def __str__(self):
        out = ''

        def replace_header(match):
            return '"'*(match.end() - match.start())

        for m in self.methods:
            print "Parsing `%s`" % m
            out += str(FunctionDoc(getattr(self._cls,m))) + '\n\n'
            out += '.. index::\n   single: %s; %s\n\n' % (self._name, m)

        return out


def handle_function(val, name):
    func_errors = []
    docstring = inspect.getdoc(val)
    if docstring is None:
        func_errors.append((name, '**missing** function-level docstring'))
    else:
        func_errors = [
            (name, e) for e in
            NumpyFunctionDocString(docstring, val).get_errors()
        ]
    return func_errors


def handle_module(val, name):
    module_errors = []
    docstring = val
    if docstring is None:
        module_errors.append((name, '**missing** module-level docstring'))
    else:
        module_errors = [
            (name, e) for e in NumpyModuleDocString(docstring).get_errors()
        ]
    return module_errors


def handle_method(method, method_name, class_name):
    method_errors = []

    # Skip out-of-library inherited methods
    module = inspect.getmodule(method)
    if module is not None:
        if not module.__name__.startswith('pylearn2'):
            return method_errors

    docstring = inspect.getdoc(method)
    if docstring is None:
        method_errors.append((class_name, method_name,
                              '**missing** method-level docstring'))
    else:
        method_errors = [
            (class_name, method_name, e) for e in
            NumpyFunctionDocString(docstring, method).get_errors()
        ]
    return method_errors


def handle_class(val, class_name):
    cls_errors = []
    docstring = inspect.getdoc(val)
    if docstring is None:
        cls_errors.append((class_name,
                           '**missing** class-level docstring'))
    else:
        cls_errors = [
            (e,) for e in
            NumpyClassDocString(docstring, class_name, val).get_errors()
        ]
        # Get public methods and parse their docstrings
        methods = dict(((name, func) for name, func in inspect.getmembers(val)
                        if not name.startswith('_') and callable(func) and type(func) is not type))
        for m_name, method in methods.iteritems():
            # skip error check if the method was inherited
            # from a parent class (which means it wasn't
            # defined in this source file)
            if inspect.getmodule(method) is not None:
                continue
            cls_errors.extend(handle_method(method, m_name, class_name))
    return cls_errors


def docstring_errors(filename, global_dict=None):
    """
    Run a Python file, parse the docstrings of all the classes
    and functions it declares, and return them.

    Parameters
    ----------
    filename : str
        Filename of the module to run.

    global_dict : dict, optional
        Globals dictionary to pass along to `execfile()`.

    Returns
    -------
    all_errors : list
        Each entry of the list is a tuple, of length 2 or 3, with
        format either

        (func_or_class_name, docstring_error_description)
        or
        (class_name, method_name, docstring_error_description)
    """
    if global_dict is None:
        global_dict = {}
    if '__file__' not in global_dict:
        global_dict['__file__'] = filename
    if '__doc__' not in global_dict:
        global_dict['__doc__'] = None
    try:
        execfile(filename, global_dict)
    except SystemExit:
        pass
    except SkipTest:
        raise AssertionError("Couldn't verify format of " + filename +
                "due to SkipTest")
    all_errors = []
    for key, val in global_dict.iteritems():
        if not key.startswith('_'):
            module_name = ""
            if hasattr(inspect.getmodule(val), '__name__'):
                module_name = inspect.getmodule(val).__name__
            if (inspect.isfunction(val) or inspect.isclass(val)) and\
                    (inspect.getmodule(val) is None
                     or module_name == '__builtin__'):
                if inspect.isfunction(val):
                    all_errors.extend(handle_function(val, key))
                elif inspect.isclass(val):
                    all_errors.extend(handle_class(val, key))
        elif key == '__doc__':
            all_errors.extend(handle_module(val, key))
    if all_errors:
        all_errors.insert(0, ("%s:"%filename,))
    return all_errors

if __name__ == "__main__":
    all_errors = docstring_errors(sys.argv[1])
    if len(all_errors) > 0:
        print "*" * 30, "docstring errors", "*" * 30
        for line in all_errors:
            print ':'.join(line)
    sys.exit(int(len(all_errors) > 0))

########NEW FILE########
__FILENAME__ = pep8
#!/usr/bin/env python
# pep8.py - Check Python source code formatting, according to PEP 8
# Copyright (C) 2006-2009 Johann C. Rocholl <johann@rocholl.net>
# Copyright (C) 2009-2013 Florent Xicluna <florent.xicluna@gmail.com>
#
# Permission is hereby granted, free of charge, to any person
# obtaining a copy of this software and associated documentation files
# (the "Software"), to deal in the Software without restriction,
# including without limitation the rights to use, copy, modify, merge,
# publish, distribute, sublicense, and/or sell copies of the Software,
# and to permit persons to whom the Software is furnished to do so,
# subject to the following conditions:
#
# The above copyright notice and this permission notice shall be
# included in all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
# BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
# ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

r"""
Check Python source code formatting, according to PEP 8:
http://www.python.org/dev/peps/pep-0008/

For usage and a list of options, try this:
$ python pep8.py -h

This program and its regression test suite live here:
http://github.com/jcrocholl/pep8

Groups of errors and warnings:
E errors
W warnings
100 indentation
200 whitespace
300 blank lines
400 imports
500 line length
600 deprecation
700 statements
900 syntax error
"""
__version__ = '1.5.1'

import os
import sys
import re
import time
import inspect
import keyword
import tokenize
from optparse import OptionParser
from fnmatch import fnmatch
try:
    from configparser import RawConfigParser
    from io import TextIOWrapper
except ImportError:
    from ConfigParser import RawConfigParser

DEFAULT_EXCLUDE = '.svn,CVS,.bzr,.hg,.git,__pycache__'
DEFAULT_IGNORE = 'E123,E226,E24'
if sys.platform == 'win32':
    DEFAULT_CONFIG = os.path.expanduser(r'~\.pep8')
else:
    DEFAULT_CONFIG = os.path.join(os.getenv('XDG_CONFIG_HOME') or
                                  os.path.expanduser('~/.config'), 'pep8')
PROJECT_CONFIG = ('setup.cfg', 'tox.ini', '.pep8')
TESTSUITE_PATH = os.path.join(os.path.dirname(__file__), 'testsuite')
MAX_LINE_LENGTH = 79
REPORT_FORMAT = {
    'default': '%(path)s:%(row)d:%(col)d: %(code)s %(text)s',
    'pylint': '%(path)s:%(row)d: [%(code)s] %(text)s',
}

PyCF_ONLY_AST = 1024
SINGLETONS = frozenset(['False', 'None', 'True'])
KEYWORDS = frozenset(keyword.kwlist + ['print']) - SINGLETONS
UNARY_OPERATORS = frozenset(['>>', '**', '*', '+', '-'])
ARITHMETIC_OP = frozenset(['**', '*', '/', '//', '+', '-'])
WS_OPTIONAL_OPERATORS = ARITHMETIC_OP.union(['^', '&', '|', '<<', '>>', '%'])
WS_NEEDED_OPERATORS = frozenset([
    '**=', '*=', '/=', '//=', '+=', '-=', '!=', '<>', '<', '>',
    '%=', '^=', '&=', '|=', '==', '<=', '>=', '<<=', '>>=', '='])
WHITESPACE = frozenset(' \t')
SKIP_TOKENS = frozenset([tokenize.COMMENT, tokenize.NL, tokenize.NEWLINE,
                         tokenize.INDENT, tokenize.DEDENT])
BENCHMARK_KEYS = ['directories', 'files', 'logical lines', 'physical lines']

INDENT_REGEX = re.compile(r'([ \t]*)')
RAISE_COMMA_REGEX = re.compile(r'raise\s+\w+\s*,')
RERAISE_COMMA_REGEX = re.compile(r'raise\s+\w+\s*,.*,\s*\w+\s*$')
ERRORCODE_REGEX = re.compile(r'\b[A-Z]\d{3}\b')
DOCSTRING_REGEX = re.compile(r'u?r?["\']')
EXTRANEOUS_WHITESPACE_REGEX = re.compile(r'[[({] | []}),;:]')
WHITESPACE_AFTER_COMMA_REGEX = re.compile(r'[,;:]\s*(?:  |\t)')
COMPARE_SINGLETON_REGEX = re.compile(r'([=!]=)\s*(None|False|True)')
COMPARE_NEGATIVE_REGEX = re.compile(r'\b(not)\s+[^[({ ]+\s+(in|is)\s')
COMPARE_TYPE_REGEX = re.compile(r'(?:[=!]=|is(?:\s+not)?)\s*type(?:s.\w+Type'
                                r'|\s*\(\s*([^)]*[^ )])\s*\))')
KEYWORD_REGEX = re.compile(r'(\s*)\b(?:%s)\b(\s*)' % r'|'.join(KEYWORDS))
OPERATOR_REGEX = re.compile(r'(?:[^,\s])(\s*)(?:[-+*/|!<=>%&^]+)(\s*)')
LAMBDA_REGEX = re.compile(r'\blambda\b')
HUNK_REGEX = re.compile(r'^@@ -\d+(?:,\d+)? \+(\d+)(?:,(\d+))? @@.*$')

# Work around Python < 2.6 behaviour, which does not generate NL after
# a comment which is on a line by itself.
COMMENT_WITH_NL = tokenize.generate_tokens(['#\n'].pop).send(None)[1] == '#\n'


##############################################################################
# Plugins (check functions) for physical lines
##############################################################################


def tabs_or_spaces(physical_line, indent_char):
    r"""
    Never mix tabs and spaces.

    The most popular way of indenting Python is with spaces only.  The
    second-most popular way is with tabs only.  Code indented with a mixture
    of tabs and spaces should be converted to using spaces exclusively.  When
    invoking the Python command line interpreter with the -t option, it issues
    warnings about code that illegally mixes tabs and spaces.  When using -tt
    these warnings become errors.  These options are highly recommended!

    Okay: if a == 0:\n        a = 1\n        b = 1
    E101: if a == 0:\n        a = 1\n\tb = 1
    """
    indent = INDENT_REGEX.match(physical_line).group(1)
    for offset, char in enumerate(indent):
        if char != indent_char:
            return offset, "E101 indentation contains mixed spaces and tabs"


def tabs_obsolete(physical_line):
    r"""
    For new projects, spaces-only are strongly recommended over tabs.  Most
    editors have features that make this easy to do.

    Okay: if True:\n    return
    W191: if True:\n\treturn
    """
    indent = INDENT_REGEX.match(physical_line).group(1)
    if '\t' in indent:
        return indent.index('\t'), "W191 indentation contains tabs"


def trailing_whitespace(physical_line):
    r"""
    JCR: Trailing whitespace is superfluous.
    FBM: Except when it occurs as part of a blank line (i.e. the line is
         nothing but whitespace). According to Python docs[1] a line with only
         whitespace is considered a blank line, and is to be ignored. However,
         matching a blank line to its indentation level avoids mistakenly
         terminating a multi-line statement (e.g. class declaration) when
         pasting code into the standard Python interpreter.

         [1] http://docs.python.org/reference/lexical_analysis.html#blank-lines

    The warning returned varies on whether the line itself is blank, for easier
    filtering for those who want to indent their blank lines.

    Okay: spam(1)\n#
    W291: spam(1) \n#
    W293: class Foo(object):\n    \n    bang = 12
    """
    physical_line = physical_line.rstrip('\n')    # chr(10), newline
    physical_line = physical_line.rstrip('\r')    # chr(13), carriage return
    physical_line = physical_line.rstrip('\x0c')  # chr(12), form feed, ^L
    stripped = physical_line.rstrip(' \t\v')
    if physical_line != stripped:
        if stripped:
            return len(stripped), "W291 trailing whitespace"
        else:
            return 0, "W293 blank line contains whitespace"


def trailing_blank_lines(physical_line, lines, line_number):
    r"""
    JCR: Trailing blank lines are superfluous.

    Okay: spam(1)
    W391: spam(1)\n
    """
    if not physical_line.rstrip() and line_number == len(lines):
        return 0, "W391 blank line at end of file"


def missing_newline(physical_line):
    """
    JCR: The last line should have a newline.

    Reports warning W292.
    """
    if physical_line.rstrip() == physical_line:
        return len(physical_line), "W292 no newline at end of file"


def maximum_line_length(physical_line, max_line_length, multiline):
    """
    Limit all lines to a maximum of 79 characters.

    There are still many devices around that are limited to 80 character
    lines; plus, limiting windows to 80 characters makes it possible to have
    several windows side-by-side.  The default wrapping on such devices looks
    ugly.  Therefore, please limit all lines to a maximum of 79 characters.
    For flowing long blocks of text (docstrings or comments), limiting the
    length to 72 characters is recommended.

    Reports error E501.
    """
    line = physical_line.rstrip()
    length = len(line)
    if length > max_line_length and not noqa(line):
        # Special case for long URLs in multi-line docstrings or comments,
        # but still report the error when the 72 first chars are whitespaces.
        chunks = line.split()
        if ((len(chunks) == 1 and multiline) or
            (len(chunks) == 2 and chunks[0] == '#')) and \
                len(line) - len(chunks[-1]) < max_line_length - 7:
            return
        if hasattr(line, 'decode'):   # Python 2
            # The line could contain multi-byte characters
            try:
                length = len(line.decode('utf-8'))
            except UnicodeError:
                pass
        if length > max_line_length:
            return (max_line_length, "E501 line too long "
                    "(%d > %d characters)" % (length, max_line_length))


##############################################################################
# Plugins (check functions) for logical lines
##############################################################################


def blank_lines(logical_line, blank_lines, indent_level, line_number,
                previous_logical, previous_indent_level):
    r"""
    Separate top-level function and class definitions with two blank lines.

    Method definitions inside a class are separated by a single blank line.

    Extra blank lines may be used (sparingly) to separate groups of related
    functions.  Blank lines may be omitted between a bunch of related
    one-liners (e.g. a set of dummy implementations).

    Use blank lines in functions, sparingly, to indicate logical sections.

    Okay: def a():\n    pass\n\n\ndef b():\n    pass
    Okay: def a():\n    pass\n\n\n# Foo\n# Bar\n\ndef b():\n    pass

    E301: class Foo:\n    b = 0\n    def bar():\n        pass
    E302: def a():\n    pass\n\ndef b(n):\n    pass
    E303: def a():\n    pass\n\n\n\ndef b(n):\n    pass
    E303: def a():\n\n\n\n    pass
    E304: @decorator\n\ndef a():\n    pass
    """
    if line_number < 3 and not previous_logical:
        return  # Don't expect blank lines before the first line
    if previous_logical.startswith('@'):
        if blank_lines:
            yield 0, "E304 blank lines found after function decorator"
    elif blank_lines > 2 or (indent_level and blank_lines == 2):
        yield 0, "E303 too many blank lines (%d)" % blank_lines
    elif logical_line.startswith(('def ', 'class ', '@')):
        if indent_level:
            if not (blank_lines or previous_indent_level < indent_level or
                    DOCSTRING_REGEX.match(previous_logical)):
                yield 0, "E301 expected 1 blank line, found 0"
        elif blank_lines != 2:
            yield 0, "E302 expected 2 blank lines, found %d" % blank_lines


def extraneous_whitespace(logical_line):
    """
    Avoid extraneous whitespace in the following situations:

    - Immediately inside parentheses, brackets or braces.

    - Immediately before a comma, semicolon, or colon.

    Okay: spam(ham[1], {eggs: 2})
    E201: spam( ham[1], {eggs: 2})
    E201: spam(ham[ 1], {eggs: 2})
    E201: spam(ham[1], { eggs: 2})
    E202: spam(ham[1], {eggs: 2} )
    E202: spam(ham[1 ], {eggs: 2})
    E202: spam(ham[1], {eggs: 2 })

    E203: if x == 4: print x, y; x, y = y , x
    E203: if x == 4: print x, y ; x, y = y, x
    E203: if x == 4 : print x, y; x, y = y, x
    """
    line = logical_line
    for match in EXTRANEOUS_WHITESPACE_REGEX.finditer(line):
        text = match.group()
        char = text.strip()
        found = match.start()
        if text == char + ' ':
            # assert char in '([{'
            yield found + 1, "E201 whitespace after '%s'" % char
        elif line[found - 1] != ',':
            code = ('E202' if char in '}])' else 'E203')  # if char in ',;:'
            yield found, "%s whitespace before '%s'" % (code, char)


def whitespace_around_keywords(logical_line):
    r"""
    Avoid extraneous whitespace around keywords.

    Okay: True and False
    E271: True and  False
    E272: True  and False
    E273: True and\tFalse
    E274: True\tand False
    """
    for match in KEYWORD_REGEX.finditer(logical_line):
        before, after = match.groups()

        if '\t' in before:
            yield match.start(1), "E274 tab before keyword"
        elif len(before) > 1:
            yield match.start(1), "E272 multiple spaces before keyword"

        if '\t' in after:
            yield match.start(2), "E273 tab after keyword"
        elif len(after) > 1:
            yield match.start(2), "E271 multiple spaces after keyword"


def missing_whitespace(logical_line):
    """
    JCR: Each comma, semicolon or colon should be followed by whitespace.

    Okay: [a, b]
    Okay: (3,)
    Okay: a[1:4]
    Okay: a[:4]
    Okay: a[1:]
    Okay: a[1:4:2]
    E231: ['a','b']
    E231: foo(bar,baz)
    E231: [{'a':'b'}]
    """
    line = logical_line
    for index in range(len(line) - 1):
        char = line[index]
        if char in ',;:' and line[index + 1] not in WHITESPACE:
            before = line[:index]
            if char == ':' and before.count('[') > before.count(']') and \
                    before.rfind('{') < before.rfind('['):
                continue  # Slice syntax, no space required
            if char == ',' and line[index + 1] == ')':
                continue  # Allow tuple with only one element: (3,)
            yield index, "E231 missing whitespace after '%s'" % char


def indentation(logical_line, previous_logical, indent_char,
                indent_level, previous_indent_level):
    r"""
    Use 4 spaces per indentation level.

    For really old code that you don't want to mess up, you can continue to
    use 8-space tabs.

    Okay: a = 1
    Okay: if a == 0:\n    a = 1
    E111:   a = 1

    Okay: for item in items:\n    pass
    E112: for item in items:\npass

    Okay: a = 1\nb = 2
    E113: a = 1\n    b = 2
    """
    if indent_char == ' ' and indent_level % 4:
        yield 0, "E111 indentation is not a multiple of four"
    indent_expect = previous_logical.endswith(':')
    if indent_expect and indent_level <= previous_indent_level:
        yield 0, "E112 expected an indented block"
    if indent_level > previous_indent_level and not indent_expect:
        yield 0, "E113 unexpected indentation"


def continued_indentation(logical_line, tokens, indent_level, hang_closing,
                          indent_char, noqa, verbose):
    r"""
    Continuation lines should align wrapped elements either vertically using
    Python's implicit line joining inside parentheses, brackets and braces, or
    using a hanging indent.

    When using a hanging indent the following considerations should be applied:

    - there should be no arguments on the first line, and

    - further indentation should be used to clearly distinguish itself as a
      continuation line.

    Okay: a = (\n)
    E123: a = (\n    )

    Okay: a = (\n    42)
    E121: a = (\n   42)
    E122: a = (\n42)
    E123: a = (\n    42\n    )
    E124: a = (24,\n     42\n)
    E125: if (\n    b):\n    pass
    E126: a = (\n        42)
    E127: a = (24,\n      42)
    E128: a = (24,\n    42)
    E129: if (a or\n    b):\n    pass
    E131: a = (\n    42\n 24)
    """
    first_row = tokens[0][2][0]
    nrows = 1 + tokens[-1][2][0] - first_row
    if noqa or nrows == 1:
        return

    # indent_next tells us whether the next block is indented; assuming
    # that it is indented by 4 spaces, then we should not allow 4-space
    # indents on the final continuation line; in turn, some other
    # indents are allowed to have an extra 4 spaces.
    indent_next = logical_line.endswith(':')

    row = depth = 0
    valid_hangs = (4,) if indent_char != '\t' else (4, 8)
    # remember how many brackets were opened on each line
    parens = [0] * nrows
    # relative indents of physical lines
    rel_indent = [0] * nrows
    # for each depth, collect a list of opening rows
    open_rows = [[0]]
    # for each depth, memorize the hanging indentation
    hangs = [None]
    # visual indents
    indent_chances = {}
    last_indent = tokens[0][2]
    visual_indent = None
    # for each depth, memorize the visual indent column
    indent = [last_indent[1]]
    if verbose >= 3:
        print(">>> " + tokens[0][4].rstrip())
    last_token_multiline = None
    for token_type, text, start, end, line in tokens:

        newline = row < start[0] - first_row
        if newline:
            row = start[0] - first_row
            newline = (not last_token_multiline and
                       token_type not in (tokenize.NL, tokenize.NEWLINE))

        if newline:
            # this is the beginning of a continuation line.
            last_indent = start
            if verbose >= 3:
                print("... " + line.rstrip())

            # record the initial indent.
            rel_indent[row] = expand_indent(line) - indent_level

            # identify closing bracket
            close_bracket = (token_type == tokenize.OP and text in ']})')

            # is the indent relative to an opening bracket line?
            for open_row in reversed(open_rows[depth]):
                hang = rel_indent[row] - rel_indent[open_row]
                hanging_indent = hang in valid_hangs
                if hanging_indent:
                    break
            if hangs[depth]:
                hanging_indent = (hang == hangs[depth])
            # is there any chance of visual indent?
            visual_indent = (not close_bracket and hang > 0 and
                             indent_chances.get(start[1]))

            if close_bracket and indent[depth]:
                # closing bracket for visual indent
                if start[1] != indent[depth]:
                    yield (start, "E124 closing bracket does not match "
                           "visual indentation")
            elif close_bracket and not hang:
                # closing bracket matches indentation of opening bracket's line
                if hang_closing:
                    yield start, "E133 closing bracket is missing indentation"
            elif indent[depth] and start[1] < indent[depth]:
                if visual_indent is not True:
                    # visual indent is broken
                    yield (start, "E128 continuation line "
                           "under-indented for visual indent")
            elif hanging_indent or (indent_next and rel_indent[row] == 8):
                # hanging indent is verified
                if close_bracket and not hang_closing:
                    yield (start, "E123 closing bracket does not match "
                           "indentation of opening bracket's line")
                hangs[depth] = hang
            elif visual_indent is True:
                # visual indent is verified
                indent[depth] = start[1]
            elif visual_indent in (text, str):
                # ignore token lined up with matching one from a previous line
                pass
            else:
                # indent is broken
                if hang <= 0:
                    error = "E122", "missing indentation or outdented"
                elif indent[depth]:
                    error = "E127", "over-indented for visual indent"
                elif not close_bracket and hangs[depth]:
                    error = "E131", "unaligned for hanging indent"
                else:
                    hangs[depth] = hang
                    if hang > 4:
                        error = "E126", "over-indented for hanging indent"
                    else:
                        error = "E121", "under-indented for hanging indent"
                yield start, "%s continuation line %s" % error

        # look for visual indenting
        if (parens[row] and token_type not in (tokenize.NL, tokenize.COMMENT)
                and not indent[depth]):
            indent[depth] = start[1]
            indent_chances[start[1]] = True
            if verbose >= 4:
                print("bracket depth %s indent to %s" % (depth, start[1]))
        # deal with implicit string concatenation
        elif (token_type in (tokenize.STRING, tokenize.COMMENT) or
              text in ('u', 'ur', 'b', 'br')):
            indent_chances[start[1]] = str
        # special case for the "if" statement because len("if (") == 4
        elif not indent_chances and not row and not depth and text == 'if':
            indent_chances[end[1] + 1] = True
        elif text == ':' and line[end[1]:].isspace():
            open_rows[depth].append(row)

        # keep track of bracket depth
        if token_type == tokenize.OP:
            if text in '([{':
                depth += 1
                indent.append(0)
                hangs.append(None)
                if len(open_rows) == depth:
                    open_rows.append([])
                open_rows[depth].append(row)
                parens[row] += 1
                if verbose >= 4:
                    print("bracket depth %s seen, col %s, visual min = %s" %
                          (depth, start[1], indent[depth]))
            elif text in ')]}' and depth > 0:
                # parent indents should not be more than this one
                prev_indent = indent.pop() or last_indent[1]
                hangs.pop()
                for d in range(depth):
                    if indent[d] > prev_indent:
                        indent[d] = 0
                for ind in list(indent_chances):
                    if ind >= prev_indent:
                        del indent_chances[ind]
                del open_rows[depth + 1:]
                depth -= 1
                if depth:
                    indent_chances[indent[depth]] = True
                for idx in range(row, -1, -1):
                    if parens[idx]:
                        parens[idx] -= 1
                        break
            assert len(indent) == depth + 1
            if start[1] not in indent_chances:
                # allow to line up tokens
                indent_chances[start[1]] = text

        last_token_multiline = (start[0] != end[0])

    if indent_next and expand_indent(line) == indent_level + 4:
        if visual_indent:
            code = "E129 visually indented line"
        else:
            code = "E125 continuation line"
        yield (last_indent, "%s with same indent as next logical line" % code)


def whitespace_before_parameters(logical_line, tokens):
    """
    Avoid extraneous whitespace in the following situations:

    - Immediately before the open parenthesis that starts the argument
      list of a function call.

    - Immediately before the open parenthesis that starts an indexing or
      slicing.

    Okay: spam(1)
    E211: spam (1)

    Okay: dict['key'] = list[index]
    E211: dict ['key'] = list[index]
    E211: dict['key'] = list [index]
    """
    prev_type, prev_text, __, prev_end, __ = tokens[0]
    for index in range(1, len(tokens)):
        token_type, text, start, end, __ = tokens[index]
        if (token_type == tokenize.OP and
            text in '([' and
            start != prev_end and
            (prev_type == tokenize.NAME or prev_text in '}])') and
            # Syntax "class A (B):" is allowed, but avoid it
            (index < 2 or tokens[index - 2][1] != 'class') and
                # Allow "return (a.foo for a in range(5))"
                not keyword.iskeyword(prev_text)):
            yield prev_end, "E211 whitespace before '%s'" % text
        prev_type = token_type
        prev_text = text
        prev_end = end


def whitespace_around_operator(logical_line):
    r"""
    Avoid extraneous whitespace in the following situations:

    - More than one space around an assignment (or other) operator to
      align it with another.

    Okay: a = 12 + 3
    E221: a = 4  + 5
    E222: a = 4 +  5
    E223: a = 4\t+ 5
    E224: a = 4 +\t5
    """
    for match in OPERATOR_REGEX.finditer(logical_line):
        before, after = match.groups()

        if '\t' in before:
            yield match.start(1), "E223 tab before operator"
        elif len(before) > 1:
            yield match.start(1), "E221 multiple spaces before operator"

        if '\t' in after:
            yield match.start(2), "E224 tab after operator"
        elif len(after) > 1:
            yield match.start(2), "E222 multiple spaces after operator"


def missing_whitespace_around_operator(logical_line, tokens):
    r"""
    - Always surround these binary operators with a single space on
      either side: assignment (=), augmented assignment (+=, -= etc.),
      comparisons (==, <, >, !=, <>, <=, >=, in, not in, is, is not),
      Booleans (and, or, not).

    - Use spaces around arithmetic operators.

    Okay: i = i + 1
    Okay: submitted += 1
    Okay: x = x * 2 - 1
    Okay: hypot2 = x * x + y * y
    Okay: c = (a + b) * (a - b)
    Okay: foo(bar, key='word', *args, **kwargs)
    Okay: alpha[:-i]

    E225: i=i+1
    E225: submitted +=1
    E225: x = x /2 - 1
    E225: z = x **y
    E226: c = (a+b) * (a-b)
    E226: hypot2 = x*x + y*y
    E227: c = a|b
    E228: msg = fmt%(errno, errmsg)
    """
    parens = 0
    need_space = False
    prev_type = tokenize.OP
    prev_text = prev_end = None
    for token_type, text, start, end, line in tokens:
        if token_type in (tokenize.NL, tokenize.NEWLINE, tokenize.ERRORTOKEN):
            # ERRORTOKEN is triggered by backticks in Python 3
            continue
        if text in ('(', 'lambda'):
            parens += 1
        elif text == ')':
            parens -= 1
        if need_space:
            if start != prev_end:
                # Found a (probably) needed space
                if need_space is not True and not need_space[1]:
                    yield (need_space[0],
                           "E225 missing whitespace around operator")
                need_space = False
            elif text == '>' and prev_text in ('<', '-'):
                # Tolerate the "<>" operator, even if running Python 3
                # Deal with Python 3's annotated return value "->"
                pass
            else:
                if need_space is True or need_space[1]:
                    # A needed trailing space was not found
                    yield prev_end, "E225 missing whitespace around operator"
                else:
                    code, optype = 'E226', 'arithmetic'
                    if prev_text == '%':
                        code, optype = 'E228', 'modulo'
                    elif prev_text not in ARITHMETIC_OP:
                        code, optype = 'E227', 'bitwise or shift'
                    yield (need_space[0], "%s missing whitespace "
                           "around %s operator" % (code, optype))
                need_space = False
        elif token_type == tokenize.OP and prev_end is not None:
            if text == '=' and parens:
                # Allow keyword args or defaults: foo(bar=None).
                pass
            elif text in WS_NEEDED_OPERATORS:
                need_space = True
            elif text in UNARY_OPERATORS:
                # Check if the operator is being used as a binary operator
                # Allow unary operators: -123, -x, +1.
                # Allow argument unpacking: foo(*args, **kwargs).
                if prev_type == tokenize.OP:
                    binary_usage = (prev_text in '}])')
                elif prev_type == tokenize.NAME:
                    binary_usage = (prev_text not in KEYWORDS)
                else:
                    binary_usage = (prev_type not in SKIP_TOKENS)

                if binary_usage:
                    need_space = None
            elif text in WS_OPTIONAL_OPERATORS:
                need_space = None

            if need_space is None:
                # Surrounding space is optional, but ensure that
                # trailing space matches opening space
                need_space = (prev_end, start != prev_end)
            elif need_space and start == prev_end:
                # A needed opening space was not found
                yield prev_end, "E225 missing whitespace around operator"
                need_space = False
        prev_type = token_type
        prev_text = text
        prev_end = end


def whitespace_around_comma(logical_line):
    r"""
    Avoid extraneous whitespace in the following situations:

    - More than one space around an assignment (or other) operator to
      align it with another.

    Note: these checks are disabled by default

    Okay: a = (1, 2)
    E241: a = (1,  2)
    E242: a = (1,\t2)
    """
    line = logical_line
    for m in WHITESPACE_AFTER_COMMA_REGEX.finditer(line):
        found = m.start() + 1
        if '\t' in m.group():
            yield found, "E242 tab after '%s'" % m.group()[0]
        else:
            yield found, "E241 multiple spaces after '%s'" % m.group()[0]


def whitespace_around_named_parameter_equals(logical_line, tokens):
    """
    Don't use spaces around the '=' sign when used to indicate a
    keyword argument or a default parameter value.

    Okay: def complex(real, imag=0.0):
    Okay: return magic(r=real, i=imag)
    Okay: boolean(a == b)
    Okay: boolean(a != b)
    Okay: boolean(a <= b)
    Okay: boolean(a >= b)

    E251: def complex(real, imag = 0.0):
    E251: return magic(r = real, i = imag)
    """
    parens = 0
    no_space = False
    prev_end = None
    message = "E251 unexpected spaces around keyword / parameter equals"
    for token_type, text, start, end, line in tokens:
        if no_space:
            no_space = False
            if start != prev_end:
                yield (prev_end, message)
        elif token_type == tokenize.OP:
            if text == '(':
                parens += 1
            elif text == ')':
                parens -= 1
            elif parens and text == '=':
                no_space = True
                if start != prev_end:
                    yield (prev_end, message)
        prev_end = end


def whitespace_before_comment(logical_line, tokens):
    """
    Separate inline comments by at least two spaces.

    An inline comment is a comment on the same line as a statement.  Inline
    comments should be separated by at least two spaces from the statement.
    They should start with a # and a single space.

    Each line of a block comment starts with a # and a single space
    (unless it is indented text inside the comment).

    Okay: x = x + 1  # Increment x
    Okay: x = x + 1    # Increment x
    Okay: # Block comment
    E261: x = x + 1 # Increment x
    E262: x = x + 1  #Increment x
    E262: x = x + 1  #  Increment x
    E265: #Block comment
    """
    prev_end = (0, 0)
    for token_type, text, start, end, line in tokens:
        if token_type == tokenize.COMMENT:
            inline_comment = line[:start[1]].strip()
            if inline_comment:
                if prev_end[0] == start[0] and start[1] < prev_end[1] + 2:
                    yield (prev_end,
                           "E261 at least two spaces before inline comment")
            symbol, sp, comment = text.partition(' ')
            bad_prefix = symbol not in ('#', '#:')
            if inline_comment:
                if bad_prefix or comment[:1].isspace():
                    yield start, "E262 inline comment should start with '# '"
            elif bad_prefix:
                if text.rstrip('#') and (start[0] > 1 or symbol[1] != '!'):
                    yield start, "E265 block comment should start with '# '"
        elif token_type != tokenize.NL:
            prev_end = end


def imports_on_separate_lines(logical_line):
    r"""
    Imports should usually be on separate lines.

    Okay: import os\nimport sys
    E401: import sys, os

    Okay: from subprocess import Popen, PIPE
    Okay: from myclas import MyClass
    Okay: from foo.bar.yourclass import YourClass
    Okay: import myclass
    Okay: import foo.bar.yourclass
    """
    line = logical_line
    if line.startswith('import '):
        found = line.find(',')
        if -1 < found and ';' not in line[:found]:
            yield found, "E401 multiple imports on one line"


def compound_statements(logical_line):
    r"""
    Compound statements (multiple statements on the same line) are
    generally discouraged.

    While sometimes it's okay to put an if/for/while with a small body
    on the same line, never do this for multi-clause statements. Also
    avoid folding such long lines!

    Okay: if foo == 'blah':\n    do_blah_thing()
    Okay: do_one()
    Okay: do_two()
    Okay: do_three()

    E701: if foo == 'blah': do_blah_thing()
    E701: for x in lst: total += x
    E701: while t < 10: t = delay()
    E701: if foo == 'blah': do_blah_thing()
    E701: else: do_non_blah_thing()
    E701: try: something()
    E701: finally: cleanup()
    E701: if foo == 'blah': one(); two(); three()

    E702: do_one(); do_two(); do_three()
    E703: do_four();  # useless semicolon
    """
    line = logical_line
    last_char = len(line) - 1
    found = line.find(':')
    while -1 < found < last_char:
        before = line[:found]
        if (before.count('{') <= before.count('}') and  # {'a': 1} (dict)
            before.count('[') <= before.count(']') and  # [1:2] (slice)
            before.count('(') <= before.count(')') and  # (Python 3 annotation)
                not LAMBDA_REGEX.search(before)):       # lambda x: x
            yield found, "E701 multiple statements on one line (colon)"
        found = line.find(':', found + 1)
    found = line.find(';')
    while -1 < found:
        if found < last_char:
            yield found, "E702 multiple statements on one line (semicolon)"
        else:
            yield found, "E703 statement ends with a semicolon"
        found = line.find(';', found + 1)


def explicit_line_join(logical_line, tokens):
    r"""
    Avoid explicit line join between brackets.

    The preferred way of wrapping long lines is by using Python's implied line
    continuation inside parentheses, brackets and braces.  Long lines can be
    broken over multiple lines by wrapping expressions in parentheses.  These
    should be used in preference to using a backslash for line continuation.

    E502: aaa = [123, \\n       123]
    E502: aaa = ("bbb " \\n       "ccc")

    Okay: aaa = [123,\n       123]
    Okay: aaa = ("bbb "\n       "ccc")
    Okay: aaa = "bbb " \\n    "ccc"
    """
    prev_start = prev_end = parens = 0
    backslash = None
    for token_type, text, start, end, line in tokens:
        if start[0] != prev_start and parens and backslash:
            yield backslash, "E502 the backslash is redundant between brackets"
        if end[0] != prev_end:
            if line.rstrip('\r\n').endswith('\\'):
                backslash = (end[0], len(line.splitlines()[-1]) - 1)
            else:
                backslash = None
            prev_start = prev_end = end[0]
        else:
            prev_start = start[0]
        if token_type == tokenize.OP:
            if text in '([{':
                parens += 1
            elif text in ')]}':
                parens -= 1


def comparison_to_singleton(logical_line, noqa):
    """
    Comparisons to singletons like None should always be done
    with "is" or "is not", never the equality operators.

    Okay: if arg is not None:
    E711: if arg != None:
    E712: if arg == True:

    Also, beware of writing if x when you really mean if x is not None --
    e.g. when testing whether a variable or argument that defaults to None was
    set to some other value.  The other value might have a type (such as a
    container) that could be false in a boolean context!
    """
    match = not noqa and COMPARE_SINGLETON_REGEX.search(logical_line)
    if match:
        same = (match.group(1) == '==')
        singleton = match.group(2)
        msg = "'if cond is %s:'" % (('' if same else 'not ') + singleton)
        if singleton in ('None',):
            code = 'E711'
        else:
            code = 'E712'
            nonzero = ((singleton == 'True' and same) or
                       (singleton == 'False' and not same))
            msg += " or 'if %scond:'" % ('' if nonzero else 'not ')
        yield match.start(1), ("%s comparison to %s should be %s" %
                               (code, singleton, msg))


def comparison_negative(logical_line):
    r"""
    Negative comparison, either identity or membership, should be
    done using "not in" and "is not".

    Okay: if x not in y:\n    pass
    Okay: assert (X in Y or X is Z)
    Okay: if not (X in Y):\n    pass
    Okay: zz = x is not y
    E713: Z = not X in Y
    E713: if not X.B in Y:\n    pass
    E714: if not X is Y:\n    pass
    E714: Z = not X.B is Y
    """
    match = COMPARE_NEGATIVE_REGEX.search(logical_line)
    if match:
        pos = match.start(1)
        if match.group(2) == 'in':
            yield pos, "E713 test for membership should be 'not in'"
        else:
            yield pos, "E714 test for object identity should be 'is not'"


def comparison_type(logical_line):
    """
    Object type comparisons should always use isinstance() instead of
    comparing types directly.

    Okay: if isinstance(obj, int):
    E721: if type(obj) is type(1):

    When checking if an object is a string, keep in mind that it might be a
    unicode string too! In Python 2.3, str and unicode have a common base
    class, basestring, so you can do:

    Okay: if isinstance(obj, basestring):
    Okay: if type(a1) is type(b1):
    """
    match = COMPARE_TYPE_REGEX.search(logical_line)
    if match:
        inst = match.group(1)
        if inst and isidentifier(inst) and inst not in SINGLETONS:
            return  # Allow comparison for types which are not obvious
        yield match.start(), "E721 do not compare types, use 'isinstance()'"


def python_3000_has_key(logical_line, noqa):
    r"""
    The {}.has_key() method is removed in the Python 3.
    Use the 'in' operation instead.

    Okay: if "alph" in d:\n    print d["alph"]
    W601: assert d.has_key('alph')
    """
    pos = logical_line.find('.has_key(')
    if pos > -1 and not noqa:
        yield pos, "W601 .has_key() is deprecated, use 'in'"


def python_3000_raise_comma(logical_line):
    """
    When raising an exception, use "raise ValueError('message')"
    instead of the older form "raise ValueError, 'message'".

    The paren-using form is preferred because when the exception arguments
    are long or include string formatting, you don't need to use line
    continuation characters thanks to the containing parentheses.  The older
    form is removed in Python 3.

    Okay: raise DummyError("Message")
    W602: raise DummyError, "Message"
    """
    match = RAISE_COMMA_REGEX.match(logical_line)
    if match and not RERAISE_COMMA_REGEX.match(logical_line):
        yield match.end() - 1, "W602 deprecated form of raising exception"


def python_3000_not_equal(logical_line):
    """
    != can also be written <>, but this is an obsolete usage kept for
    backwards compatibility only. New code should always use !=.
    The older syntax is removed in Python 3.

    Okay: if a != 'no':
    W603: if a <> 'no':
    """
    pos = logical_line.find('<>')
    if pos > -1:
        yield pos, "W603 '<>' is deprecated, use '!='"


def python_3000_backticks(logical_line):
    """
    Backticks are removed in Python 3.
    Use repr() instead.

    Okay: val = repr(1 + 2)
    W604: val = `1 + 2`
    """
    pos = logical_line.find('`')
    if pos > -1:
        yield pos, "W604 backticks are deprecated, use 'repr()'"


##############################################################################
# Helper functions
##############################################################################


if '' == ''.encode():
    # Python 2: implicit encoding.
    def readlines(filename):
        f = open(filename)
        try:
            return f.readlines()
        finally:
            f.close()
    isidentifier = re.compile(r'[a-zA-Z_]\w*').match
    stdin_get_value = sys.stdin.read
else:
    # Python 3
    def readlines(filename):
        f = open(filename, 'rb')
        try:
            coding, lines = tokenize.detect_encoding(f.readline)
            f = TextIOWrapper(f, coding, line_buffering=True)
            return [l.decode(coding) for l in lines] + f.readlines()
        except (LookupError, SyntaxError, UnicodeError):
            f.close()
            # Fall back if files are improperly declared
            f = open(filename, encoding='latin-1')
            return f.readlines()
        finally:
            f.close()
    isidentifier = str.isidentifier

    def stdin_get_value():
        return TextIOWrapper(sys.stdin.buffer, errors='ignore').read()
readlines.__doc__ = "    Read the source code."
noqa = re.compile(r'# no(?:qa|pep8)\b', re.I).search


def expand_indent(line):
    r"""
    Return the amount of indentation.
    Tabs are expanded to the next multiple of 8.

    >>> expand_indent('    ')
    4
    >>> expand_indent('\t')
    8
    >>> expand_indent('    \t')
    8
    >>> expand_indent('       \t')
    8
    >>> expand_indent('        \t')
    16
    """
    if '\t' not in line:
        return len(line) - len(line.lstrip())
    result = 0
    for char in line:
        if char == '\t':
            result = result // 8 * 8 + 8
        elif char == ' ':
            result += 1
        else:
            break
    return result


def mute_string(text):
    """
    Replace contents with 'xxx' to prevent syntax matching.

    >>> mute_string('"abc"')
    '"xxx"'
    >>> mute_string("'''abc'''")
    "'''xxx'''"
    >>> mute_string("r'abc'")
    "r'xxx'"
    """
    # String modifiers (e.g. u or r)
    start = text.index(text[-1]) + 1
    end = len(text) - 1
    # Triple quotes
    if text[-3:] in ('"""', "'''"):
        start += 2
        end -= 2
    return text[:start] + 'x' * (end - start) + text[end:]


def parse_udiff(diff, patterns=None, parent='.'):
    """Return a dictionary of matching lines."""
    # For each file of the diff, the entry key is the filename,
    # and the value is a set of row numbers to consider.
    rv = {}
    path = nrows = None
    for line in diff.splitlines():
        if nrows:
            if line[:1] != '-':
                nrows -= 1
            continue
        if line[:3] == '@@ ':
            hunk_match = HUNK_REGEX.match(line)
            row, nrows = [int(g or '1') for g in hunk_match.groups()]
            rv[path].update(range(row, row + nrows))
        elif line[:3] == '+++':
            path = line[4:].split('\t', 1)[0]
            if path[:2] == 'b/':
                path = path[2:]
            rv[path] = set()
    return dict([(os.path.join(parent, p), rows)
                 for (p, rows) in rv.items()
                 if rows and filename_match(p, patterns)])


def normalize_paths(value, parent=os.curdir):
    """Parse a comma-separated list of paths.

    Return a list of absolute paths.
    """
    if not value or isinstance(value, list):
        return value
    paths = []
    for path in value.split(','):
        if '/' in path:
            path = os.path.abspath(os.path.join(parent, path))
        paths.append(path.rstrip('/'))
    return paths


def filename_match(filename, patterns, default=True):
    """
    Check if patterns contains a pattern that matches filename.
    If patterns is unspecified, this always returns True.
    """
    if not patterns:
        return default
    return any(fnmatch(filename, pattern) for pattern in patterns)


##############################################################################
# Framework to run all checks
##############################################################################


_checks = {'physical_line': {}, 'logical_line': {}, 'tree': {}}


def register_check(check, codes=None):
    """
    Register a new check object.
    """
    def _add_check(check, kind, codes, args):
        if check in _checks[kind]:
            _checks[kind][check][0].extend(codes or [])
        else:
            _checks[kind][check] = (codes or [''], args)
    if inspect.isfunction(check):
        args = inspect.getargspec(check)[0]
        if args and args[0] in ('physical_line', 'logical_line'):
            if codes is None:
                codes = ERRORCODE_REGEX.findall(check.__doc__ or '')
            _add_check(check, args[0], codes, args)
    elif inspect.isclass(check):
        if inspect.getargspec(check.__init__)[0][:2] == ['self', 'tree']:
            _add_check(check, 'tree', codes, None)


def init_checks_registry():
    """
    Register all globally visible functions where the first argument name
    is 'physical_line' or 'logical_line'.
    """
    mod = inspect.getmodule(register_check)
    for (name, function) in inspect.getmembers(mod, inspect.isfunction):
        register_check(function)
init_checks_registry()


class Checker(object):
    """
    Load a Python source file, tokenize it, check coding style.
    """

    def __init__(self, filename=None, lines=None,
                 options=None, report=None, **kwargs):
        if options is None:
            options = StyleGuide(kwargs).options
        else:
            assert not kwargs
        self._io_error = None
        self._physical_checks = options.physical_checks
        self._logical_checks = options.logical_checks
        self._ast_checks = options.ast_checks
        self.max_line_length = options.max_line_length
        self.multiline = False  # in a multiline string?
        self.hang_closing = options.hang_closing
        self.verbose = options.verbose
        self.filename = filename
        if filename is None:
            self.filename = 'stdin'
            self.lines = lines or []
        elif filename == '-':
            self.filename = 'stdin'
            self.lines = stdin_get_value().splitlines(True)
        elif lines is None:
            try:
                self.lines = readlines(filename)
            except IOError:
                (exc_type, exc) = sys.exc_info()[:2]
                self._io_error = '%s: %s' % (exc_type.__name__, exc)
                self.lines = []
        else:
            self.lines = lines
        if self.lines:
            ord0 = ord(self.lines[0][0])
            if ord0 in (0xef, 0xfeff):  # Strip the UTF-8 BOM
                if ord0 == 0xfeff:
                    self.lines[0] = self.lines[0][1:]
                elif self.lines[0][:3] == '\xef\xbb\xbf':
                    self.lines[0] = self.lines[0][3:]
        self.report = report or options.report
        self.report_error = self.report.error

    def report_invalid_syntax(self):
        (exc_type, exc) = sys.exc_info()[:2]
        if len(exc.args) > 1:
            offset = exc.args[1]
            if len(offset) > 2:
                offset = offset[1:3]
        else:
            offset = (1, 0)
        self.report_error(offset[0], offset[1] or 0,
                          'E901 %s: %s' % (exc_type.__name__, exc.args[0]),
                          self.report_invalid_syntax)
    report_invalid_syntax.__doc__ = "    Check if the syntax is valid."

    def readline(self):
        """
        Get the next line from the input buffer.
        """
        self.line_number += 1
        if self.line_number > len(self.lines):
            return ''
        line = self.lines[self.line_number - 1]
        if self.indent_char is None and line[:1] in WHITESPACE:
            self.indent_char = line[0]
        return line

    def run_check(self, check, argument_names):
        """
        Run a check plugin.
        """
        arguments = []
        for name in argument_names:
            arguments.append(getattr(self, name))
        return check(*arguments)

    def check_physical(self, line):
        """
        Run all physical checks on a raw input line.
        """
        self.physical_line = line
        for name, check, argument_names in self._physical_checks:
            result = self.run_check(check, argument_names)
            if result is not None:
                (offset, text) = result
                self.report_error(self.line_number, offset, text, check)
                if text[:4] == 'E101':
                    self.indent_char = line[0]

    def build_tokens_line(self):
        """
        Build a logical line from tokens.
        """
        self.mapping = []
        logical = []
        comments = []
        length = 0
        previous = None
        for token in self.tokens:
            (token_type, text) = token[0:2]
            if token_type == tokenize.COMMENT:
                comments.append(text)
                continue
            if token_type in SKIP_TOKENS:
                continue
            if token_type == tokenize.STRING:
                text = mute_string(text)
            if previous:
                (end_row, end) = previous[3]
                (start_row, start) = token[2]
                if end_row != start_row:    # different row
                    prev_text = self.lines[end_row - 1][end - 1]
                    if prev_text == ',' or (prev_text not in '{[('
                                            and text not in '}])'):
                        logical.append(' ')
                        length += 1
                elif end != start:  # different column
                    fill = self.lines[end_row - 1][end:start]
                    logical.append(fill)
                    length += len(fill)
            self.mapping.append((length, token))
            logical.append(text)
            length += len(text)
            previous = token
        self.logical_line = ''.join(logical)
        self.noqa = comments and noqa(''.join(comments))
        # With Python 2, if the line ends with '\r\r\n' the assertion fails
        # assert self.logical_line.strip() == self.logical_line

    def check_logical(self):
        """
        Build a line from tokens and run all logical checks on it.
        """
        self.build_tokens_line()
        self.report.increment_logical_line()
        token0 = self.mapping[0][1] if self.mapping else self.tokens[0]
        first_line = self.lines[token0[2][0] - 1]
        indent = first_line[:token0[2][1]]
        self.indent_level = expand_indent(indent)
        if self.verbose >= 2:
            print(self.logical_line[:80].rstrip())
        for name, check, argument_names in self._logical_checks:
            if self.verbose >= 4:
                print('   ' + name)
            for result in self.run_check(check, argument_names) or ():
                (offset, text) = result
                if isinstance(offset, tuple):
                    (orig_number, orig_offset) = offset
                else:
                    orig_number = token0[2][0]
                    orig_offset = token0[2][1] + offset
                    for token_offset, token in self.mapping:
                        if offset >= token_offset:
                            orig_number = token[2][0]
                            orig_offset = (token[2][1] + offset - token_offset)
                self.report_error(orig_number, orig_offset, text, check)
        if self.logical_line:
            self.previous_indent_level = self.indent_level
            self.previous_logical = self.logical_line
        self.tokens = []

    def check_ast(self):
        try:
            tree = compile(''.join(self.lines), '', 'exec', PyCF_ONLY_AST)
        except (SyntaxError, TypeError):
            return self.report_invalid_syntax()
        for name, cls, __ in self._ast_checks:
            checker = cls(tree, self.filename)
            for lineno, offset, text, check in checker.run():
                if not self.lines or not noqa(self.lines[lineno - 1]):
                    self.report_error(lineno, offset, text, check)

    def generate_tokens(self):
        if self._io_error:
            self.report_error(1, 0, 'E902 %s' % self._io_error, readlines)
        tokengen = tokenize.generate_tokens(self.readline)
        try:
            for token in tokengen:
                self.maybe_check_physical(token)
                yield token
        except (SyntaxError, tokenize.TokenError):
            self.report_invalid_syntax()

    def maybe_check_physical(self, token):
        """
        If appropriate (based on token), check current physical line(s).
        """
        # Called after every token, but act only on end of line.
        if token[0] in (tokenize.NEWLINE, tokenize.NL):
            # Obviously, a newline token ends a single physical line.
            self.check_physical(token[4])
        elif token[0] == tokenize.STRING and '\n' in token[1]:
            # Less obviously, a string that contains newlines is a
            # multiline string, either triple-quoted or with internal
            # newlines backslash-escaped. Check every physical line in the
            # string *except* for the last one: its newline is outside of
            # the multiline string, so we consider it a regular physical
            # line, and will check it like any other physical line.
            #
            # Subtleties:
            # - we don't *completely* ignore the last line; if it contains
            #   the magical "# noqa" comment, we disable all physical
            #   checks for the entire multiline string
            # - have to wind self.line_number back because initially it
            #   points to the last line of the string, and we want
            #   check_physical() to give accurate feedback
            if noqa(token[4]):
                return
            self.multiline = True
            self.line_number = token[2][0]
            for line in token[1].split('\n')[:-1]:
                self.check_physical(line + '\n')
                self.line_number += 1
            self.multiline = False

    def check_all(self, expected=None, line_offset=0):
        """
        Run all checks on the input file.
        """
        self.report.init_file(self.filename, self.lines, expected, line_offset)
        if self._ast_checks:
            self.check_ast()
        self.line_number = 0
        self.indent_char = None
        self.indent_level = 0
        self.previous_indent_level = 0
        self.previous_logical = ''
        self.tokens = []
        self.blank_lines = blank_lines_before_comment = 0
        parens = 0
        for token in self.generate_tokens():
            self.tokens.append(token)
            token_type, text = token[0:2]
            if self.verbose >= 3:
                if token[2][0] == token[3][0]:
                    pos = '[%s:%s]' % (token[2][1] or '', token[3][1])
                else:
                    pos = 'l.%s' % token[3][0]
                print('l.%s\t%s\t%s\t%r' %
                      (token[2][0], pos, tokenize.tok_name[token[0]], text))
            if token_type == tokenize.OP:
                if text in '([{':
                    parens += 1
                elif text in '}])':
                    parens -= 1
            elif not parens:
                if token_type == tokenize.NEWLINE:
                    if self.blank_lines < blank_lines_before_comment:
                        self.blank_lines = blank_lines_before_comment
                    self.check_logical()
                    self.blank_lines = blank_lines_before_comment = 0
                elif token_type == tokenize.NL:
                    if len(self.tokens) == 1:
                        # The physical line contains only this token.
                        self.blank_lines += 1
                        del self.tokens[0]
                    else:
                        self.check_logical()
                elif token_type == tokenize.COMMENT and len(self.tokens) == 1:
                    if blank_lines_before_comment < self.blank_lines:
                        blank_lines_before_comment = self.blank_lines
                    self.blank_lines = 0
                    if COMMENT_WITH_NL:
                        # The comment also ends a physical line
                        text = text.rstrip('\r\n')
                        self.tokens = [(token_type, text) + token[2:]]
                        self.check_logical()
        return self.report.get_file_results()


class BaseReport(object):
    """Collect the results of the checks."""
    print_filename = False

    def __init__(self, options):
        self._benchmark_keys = options.benchmark_keys
        self._ignore_code = options.ignore_code
        # Results
        self.elapsed = 0
        self.total_errors = 0
        self.counters = dict.fromkeys(self._benchmark_keys, 0)
        self.messages = {}

    def start(self):
        """Start the timer."""
        self._start_time = time.time()

    def stop(self):
        """Stop the timer."""
        self.elapsed = time.time() - self._start_time

    def init_file(self, filename, lines, expected, line_offset):
        """Signal a new file."""
        self.filename = filename
        self.lines = lines
        self.expected = expected or ()
        self.line_offset = line_offset
        self.file_errors = 0
        self.counters['files'] += 1
        self.counters['physical lines'] += len(lines)

    def increment_logical_line(self):
        """Signal a new logical line."""
        self.counters['logical lines'] += 1

    def error(self, line_number, offset, text, check):
        """Report an error, according to options."""
        code = text[:4]
        if self._ignore_code(code):
            return
        if code in self.counters:
            self.counters[code] += 1
        else:
            self.counters[code] = 1
            self.messages[code] = text[5:]
        # Don't care about expected errors or warnings
        if code in self.expected:
            return
        if self.print_filename and not self.file_errors:
            print(self.filename)
        self.file_errors += 1
        self.total_errors += 1
        return code

    def get_file_results(self):
        """Return the count of errors and warnings for this file."""
        return self.file_errors

    def get_count(self, prefix=''):
        """Return the total count of errors and warnings."""
        return sum([self.counters[key]
                    for key in self.messages if key.startswith(prefix)])

    def get_statistics(self, prefix=''):
        """
        Get statistics for message codes that start with the prefix.

        prefix='' matches all errors and warnings
        prefix='E' matches all errors
        prefix='W' matches all warnings
        prefix='E4' matches all errors that have to do with imports
        """
        return ['%-7s %s %s' % (self.counters[key], key, self.messages[key])
                for key in sorted(self.messages) if key.startswith(prefix)]

    def print_statistics(self, prefix=''):
        """Print overall statistics (number of errors and warnings)."""
        for line in self.get_statistics(prefix):
            print(line)

    def print_benchmark(self):
        """Print benchmark numbers."""
        print('%-7.2f %s' % (self.elapsed, 'seconds elapsed'))
        if self.elapsed:
            for key in self._benchmark_keys:
                print('%-7d %s per second (%d total)' %
                      (self.counters[key] / self.elapsed, key,
                       self.counters[key]))


class FileReport(BaseReport):
    """Collect the results of the checks and print only the filenames."""
    print_filename = True


class StandardReport(BaseReport):
    """Collect and print the results of the checks."""

    def __init__(self, options):
        super(StandardReport, self).__init__(options)
        self._fmt = REPORT_FORMAT.get(options.format.lower(),
                                      options.format)
        self._repeat = options.repeat
        self._show_source = options.show_source
        self._show_pep8 = options.show_pep8

    def init_file(self, filename, lines, expected, line_offset):
        """Signal a new file."""
        self._deferred_print = []
        return super(StandardReport, self).init_file(
            filename, lines, expected, line_offset)

    def error(self, line_number, offset, text, check):
        """Report an error, according to options."""
        code = super(StandardReport, self).error(line_number, offset,
                                                 text, check)
        if code and (self.counters[code] == 1 or self._repeat):
            self._deferred_print.append(
                (line_number, offset, code, text[5:], check.__doc__))
        return code

    def get_file_results(self):
        """Print the result and return the overall count for this file."""
        self._deferred_print.sort()
        for line_number, offset, code, text, doc in self._deferred_print:
            print(self._fmt % {
                'path': self.filename,
                'row': self.line_offset + line_number, 'col': offset + 1,
                'code': code, 'text': text,
            })
            if self._show_source:
                if line_number > len(self.lines):
                    line = ''
                else:
                    line = self.lines[line_number - 1]
                print(line.rstrip())
                print(' ' * offset + '^')
            if self._show_pep8 and doc:
                print(doc.lstrip('\n').rstrip())
        return self.file_errors


class DiffReport(StandardReport):
    """Collect and print the results for the changed lines only."""

    def __init__(self, options):
        super(DiffReport, self).__init__(options)
        self._selected = options.selected_lines

    def error(self, line_number, offset, text, check):
        if line_number not in self._selected[self.filename]:
            return
        return super(DiffReport, self).error(line_number, offset, text, check)


class StyleGuide(object):
    """Initialize a PEP-8 instance with few options."""

    def __init__(self, *args, **kwargs):
        # build options from the command line
        self.checker_class = kwargs.pop('checker_class', Checker)
        parse_argv = kwargs.pop('parse_argv', False)
        config_file = kwargs.pop('config_file', None)
        parser = kwargs.pop('parser', None)
        # build options from dict
        options_dict = dict(*args, **kwargs)
        arglist = None if parse_argv else options_dict.get('paths', None)
        options, self.paths = process_options(
            arglist, parse_argv, config_file, parser)
        if options_dict:
            options.__dict__.update(options_dict)
            if 'paths' in options_dict:
                self.paths = options_dict['paths']

        self.runner = self.input_file
        self.options = options

        if not options.reporter:
            options.reporter = BaseReport if options.quiet else StandardReport

        options.select = tuple(options.select or ())
        if not (options.select or options.ignore or
                options.testsuite or options.doctest) and DEFAULT_IGNORE:
            # The default choice: ignore controversial checks
            options.ignore = tuple(DEFAULT_IGNORE.split(','))
        else:
            # Ignore all checks which are not explicitly selected
            options.ignore = ('',) if options.select else tuple(options.ignore)
        options.benchmark_keys = BENCHMARK_KEYS[:]
        options.ignore_code = self.ignore_code
        options.physical_checks = self.get_checks('physical_line')
        options.logical_checks = self.get_checks('logical_line')
        options.ast_checks = self.get_checks('tree')
        self.init_report()

    def init_report(self, reporter=None):
        """Initialize the report instance."""
        self.options.report = (reporter or self.options.reporter)(self.options)
        return self.options.report

    def check_files(self, paths=None):
        """Run all checks on the paths."""
        if paths is None:
            paths = self.paths
        report = self.options.report
        runner = self.runner
        report.start()
        try:
            for path in paths:
                if os.path.isdir(path):
                    self.input_dir(path)
                elif not self.excluded(path):
                    runner(path)
        except KeyboardInterrupt:
            print('... stopped')
        report.stop()
        return report

    def input_file(self, filename, lines=None, expected=None, line_offset=0):
        """Run all checks on a Python source file."""
        if self.options.verbose:
            print('checking %s' % filename)
        fchecker = self.checker_class(
            filename, lines=lines, options=self.options)
        return fchecker.check_all(expected=expected, line_offset=line_offset)

    def input_dir(self, dirname):
        """Check all files in this directory and all subdirectories."""
        dirname = dirname.rstrip('/')
        if self.excluded(dirname):
            return 0
        counters = self.options.report.counters
        verbose = self.options.verbose
        filepatterns = self.options.filename
        runner = self.runner
        for root, dirs, files in os.walk(dirname):
            if verbose:
                print('directory ' + root)
            counters['directories'] += 1
            for subdir in sorted(dirs):
                if self.excluded(subdir, root):
                    dirs.remove(subdir)
            for filename in sorted(files):
                # contain a pattern that matches?
                if ((filename_match(filename, filepatterns) and
                     not self.excluded(filename, root))):
                    runner(os.path.join(root, filename))

    def excluded(self, filename, parent=None):
        """
        Check if options.exclude contains a pattern that matches filename.
        """
        if not self.options.exclude:
            return False
        basename = os.path.basename(filename)
        if filename_match(basename, self.options.exclude):
            return True
        if parent:
            filename = os.path.join(parent, filename)
        filename = os.path.abspath(filename)
        return filename_match(filename, self.options.exclude)

    def ignore_code(self, code):
        """
        Check if the error code should be ignored.

        If 'options.select' contains a prefix of the error code,
        return False.  Else, if 'options.ignore' contains a prefix of
        the error code, return True.
        """
        if len(code) < 4 and any(s.startswith(code)
                                 for s in self.options.select):
            return False
        return (code.startswith(self.options.ignore) and
                not code.startswith(self.options.select))

    def get_checks(self, argument_name):
        """
        Find all globally visible functions where the first argument name
        starts with argument_name and which contain selected tests.
        """
        checks = []
        for check, attrs in _checks[argument_name].items():
            (codes, args) = attrs
            if any(not (code and self.ignore_code(code)) for code in codes):
                checks.append((check.__name__, check, args))
        return sorted(checks)


def get_parser(prog='pep8', version=__version__):
    parser = OptionParser(prog=prog, version=version,
                          usage="%prog [options] input ...")
    parser.config_options = [
        'exclude', 'filename', 'select', 'ignore', 'max-line-length',
        'hang-closing', 'count', 'format', 'quiet', 'show-pep8',
        'show-source', 'statistics', 'verbose']
    parser.add_option('-v', '--verbose', default=0, action='count',
                      help="print status messages, or debug with -vv")
    parser.add_option('-q', '--quiet', default=0, action='count',
                      help="report only file names, or nothing with -qq")
    parser.add_option('-r', '--repeat', default=True, action='store_true',
                      help="(obsolete) show all occurrences of the same error")
    parser.add_option('--first', action='store_false', dest='repeat',
                      help="show first occurrence of each error")
    parser.add_option('--exclude', metavar='patterns', default=DEFAULT_EXCLUDE,
                      help="exclude files or directories which match these "
                           "comma separated patterns (default: %default)")
    parser.add_option('--filename', metavar='patterns', default='*.py',
                      help="when parsing directories, only check filenames "
                           "matching these comma separated patterns "
                           "(default: %default)")
    parser.add_option('--select', metavar='errors', default='',
                      help="select errors and warnings (e.g. E,W6)")
    parser.add_option('--ignore', metavar='errors', default='',
                      help="skip errors and warnings (e.g. E4,W)")
    parser.add_option('--show-source', action='store_true',
                      help="show source code for each error")
    parser.add_option('--show-pep8', action='store_true',
                      help="show text of PEP 8 for each error "
                           "(implies --first)")
    parser.add_option('--statistics', action='store_true',
                      help="count errors and warnings")
    parser.add_option('--count', action='store_true',
                      help="print total number of errors and warnings "
                           "to standard error and set exit code to 1 if "
                           "total is not null")
    parser.add_option('--max-line-length', type='int', metavar='n',
                      default=MAX_LINE_LENGTH,
                      help="set maximum allowed line length "
                           "(default: %default)")
    parser.add_option('--hang-closing', action='store_true',
                      help="hang closing bracket instead of matching "
                           "indentation of opening bracket's line")
    parser.add_option('--format', metavar='format', default='default',
                      help="set the error format [default|pylint|<custom>]")
    parser.add_option('--diff', action='store_true',
                      help="report only lines changed according to the "
                           "unified diff received on STDIN")
    group = parser.add_option_group("Testing Options")
    if os.path.exists(TESTSUITE_PATH):
        group.add_option('--testsuite', metavar='dir',
                         help="run regression tests from dir")
        group.add_option('--doctest', action='store_true',
                         help="run doctest on myself")
    group.add_option('--benchmark', action='store_true',
                     help="measure processing speed")
    return parser


def read_config(options, args, arglist, parser):
    """Read both user configuration and local configuration."""
    config = RawConfigParser()

    user_conf = options.config
    if user_conf and os.path.isfile(user_conf):
        if options.verbose:
            print('user configuration: %s' % user_conf)
        config.read(user_conf)

    local_dir = os.curdir
    parent = tail = args and os.path.abspath(os.path.commonprefix(args))
    while tail:
        if config.read([os.path.join(parent, fn) for fn in PROJECT_CONFIG]):
            local_dir = parent
            if options.verbose:
                print('local configuration: in %s' % parent)
            break
        (parent, tail) = os.path.split(parent)

    pep8_section = parser.prog
    if config.has_section(pep8_section):
        option_list = dict([(o.dest, o.type or o.action)
                            for o in parser.option_list])

        # First, read the default values
        (new_options, __) = parser.parse_args([])

        # Second, parse the configuration
        for opt in config.options(pep8_section):
            if options.verbose > 1:
                print("  %s = %s" % (opt, config.get(pep8_section, opt)))
            if opt.replace('_', '-') not in parser.config_options:
                print("Unknown option: '%s'\n  not in [%s]" %
                      (opt, ' '.join(parser.config_options)))
                sys.exit(1)
            normalized_opt = opt.replace('-', '_')
            opt_type = option_list[normalized_opt]
            if opt_type in ('int', 'count'):
                value = config.getint(pep8_section, opt)
            elif opt_type == 'string':
                value = config.get(pep8_section, opt)
                if normalized_opt == 'exclude':
                    value = normalize_paths(value, local_dir)
            else:
                assert opt_type in ('store_true', 'store_false')
                value = config.getboolean(pep8_section, opt)
            setattr(new_options, normalized_opt, value)

        # Third, overwrite with the command-line options
        (options, __) = parser.parse_args(arglist, values=new_options)
    options.doctest = options.testsuite = False
    return options


def process_options(arglist=None, parse_argv=False, config_file=None,
                    parser=None):
    """Process options passed either via arglist or via command line args."""
    if not parser:
        parser = get_parser()
    if not parser.has_option('--config'):
        if config_file is True:
            config_file = DEFAULT_CONFIG
        group = parser.add_option_group("Configuration", description=(
            "The project options are read from the [%s] section of the "
            "tox.ini file or the setup.cfg file located in any parent folder "
            "of the path(s) being processed.  Allowed options are: %s." %
            (parser.prog, ', '.join(parser.config_options))))
        group.add_option('--config', metavar='path', default=config_file,
                         help="user config file location (default: %default)")
    # Don't read the command line if the module is used as a library.
    if not arglist and not parse_argv:
        arglist = []
    # If parse_argv is True and arglist is None, arguments are
    # parsed from the command line (sys.argv)
    (options, args) = parser.parse_args(arglist)
    options.reporter = None

    if options.ensure_value('testsuite', False):
        args.append(options.testsuite)
    elif not options.ensure_value('doctest', False):
        if parse_argv and not args:
            if options.diff or any(os.path.exists(name)
                                   for name in PROJECT_CONFIG):
                args = ['.']
            else:
                parser.error('input not specified')
        options = read_config(options, args, arglist, parser)
        options.reporter = parse_argv and options.quiet == 1 and FileReport

    options.filename = options.filename and options.filename.split(',')
    options.exclude = normalize_paths(options.exclude)
    options.select = options.select and options.select.split(',')
    options.ignore = options.ignore and options.ignore.split(',')

    if options.diff:
        options.reporter = DiffReport
        stdin = stdin_get_value()
        options.selected_lines = parse_udiff(stdin, options.filename, args[0])
        args = sorted(options.selected_lines)

    return options, args


def _main():
    """Parse options and run checks on Python source."""
    pep8style = StyleGuide(parse_argv=True, config_file=True)
    options = pep8style.options
    if options.doctest or options.testsuite:
        #disabled test suite
        #from testsuite.support import run_tests
        #report = run_tests(pep8style)
        pass
    else:
        report = pep8style.check_files()
    if options.statistics:
        report.print_statistics()
    if options.benchmark:
        report.print_benchmark()
    if options.testsuite and not options.quiet:
        #disabled test suite
        #report.print_results()
        pass
    if report.total_errors:
        if options.count:
            sys.stderr.write(str(report.total_errors) + '\n')
        sys.exit(1)

if __name__ == '__main__':
    _main()

########NEW FILE########
__FILENAME__ = test_format
"""
Unit tests for format checking
"""

from nose.plugins.skip import SkipTest

import os
import pylearn2

from pylearn2.devtools.tests.docscrape import docstring_errors
from pylearn2.devtools.list_files import list_files
from pylearn2.devtools.tests.pep8.pep8 import StyleGuide

whitelist_pep8 = [
    "rbm_tools.py",
    "training_algorithms/tests/test_learning_rule.py",
    "distributions/mnd.py",
    "models/sparse_autoencoder.py",
    "models/tests/test_dbm.py",
    "models/tests/test_autoencoder.py",
    "models/tests/test_s3c_inference.py",
    "models/tests/test_mnd.py",
    "models/tests/test_s3c_misc.py",
    "models/gsn.py",
    "models/dbm/layer.py",
    "models/dbm/__init__.py",
    "models/dbm/ising.py",
    "models/dbm/inference_procedure.py",
    "models/differentiable_sparse_coding.py",
    "models/local_coordinate_coding.py",
    "models/mnd.py",
    "models/s3c.py",
    "models/autoencoder.py",
    "tests/test_monitor.py",
    "kmeans.py",
    "packaged_dependencies/theano_linear/conv2d.py",
    "packaged_dependencies/theano_linear/imaging.py",
    "packaged_dependencies/theano_linear/pyramid.py",
    "packaged_dependencies/theano_linear/unshared_conv/"
    "test_gpu_unshared_conv.py",
    "packaged_dependencies/theano_linear/unshared_conv/"
    "test_localdot.py",
    "packaged_dependencies/theano_linear/unshared_conv/localdot.py",
    "packaged_dependencies/theano_linear/unshared_conv/"
    "unshared_conv.py",
    "packaged_dependencies/theano_linear/linear.py",
    "packaged_dependencies/theano_linear/test_spconv.py",
    "packaged_dependencies/theano_linear/test_matrixmul.py",
    "packaged_dependencies/theano_linear/spconv.py",
    "expr/tests/test_coding.py",
    "expr/tests/test_normalize.py",
    "expr/tests/test_stochastic_pool.py",
    "expr/stochastic_pool.py",
    "expr/sampling.py",
    "expr/information_theory.py",
    "expr/basic.py",
    "testing/datasets.py",
    "testing/cost.py",
    "gui/graph_2D.py",
    "sandbox/cuda_convnet/weight_acts.py",
    "sandbox/cuda_convnet/filter_acts.py",
    "sandbox/cuda_convnet/tests/test_filter_acts_strided.py",
    "sandbox/cuda_convnet/tests/test_probabilistic_max_pooling.py",
    "sandbox/cuda_convnet/tests/test_filter_acts.py",
    "sandbox/cuda_convnet/tests/test_weight_acts_strided.py",
    "sandbox/cuda_convnet/tests/test_image_acts_strided.py",
    "sandbox/cuda_convnet/tests/test_img_acts.py",
    "sandbox/cuda_convnet/tests/test_weight_acts.py",
    "sandbox/cuda_convnet/tests/test_stochastic_pool.py",
    "sandbox/cuda_convnet/specialized_bench.py",
    "sandbox/cuda_convnet/response_norm.py",
    "sandbox/cuda_convnet/__init__.py",
    "sandbox/cuda_convnet/img_acts.py",
    "sandbox/cuda_convnet/convnet_compile.py",
    "sandbox/cuda_convnet/base_acts.py",
    "sandbox/cuda_convnet/pthreads.py",
    "sandbox/cuda_convnet/pool.py",
    "sandbox/cuda_convnet/bench.py",
    "sandbox/cuda_convnet/stochastic_pool.py",
    "sandbox/cuda_convnet/probabilistic_max_pooling.py",
    "sandbox/tuple_var.py",
    "sandbox/lisa_rl/bandit/average_agent.py",
    "sandbox/lisa_rl/bandit/classifier_bandit.py",
    "sandbox/lisa_rl/bandit/classifier_agent.py",
    "sandbox/lisa_rl/bandit/plot_reward.py",
    "config/old_config.py",
    "config/yaml_parse.py",
    "datasets/utlc.py",
    "datasets/mnistplus.py",
    "datasets/cos_dataset.py",
    "datasets/cifar10.py",
    "datasets/svhn.py",
    "datasets/tests/test_csv_dataset.py",
    "datasets/tests/test_icml07.py",
    "datasets/tests/test_utlc.py",
    "datasets/preprocessing.py",
    "datasets/config.py",
    "datasets/tfd.py",
    "datasets/icml07.py",
    "datasets/filetensor.py",
    "datasets/hepatitis.py",
    "datasets/wiskott.py",
    "datasets/mnist.py",
    "datasets/sparse_dataset.py",
    "datasets/csv_dataset.py",
    "datasets/tl_challenge.py",
    "datasets/retina.py",
    "datasets/ocr.py",
    "datasets/stl10.py",
    "datasets/vector_spaces_dataset.py",
    "datasets/debug.py",
    "datasets/binarizer.py",
    "utils/utlc.py",
    "utils/tests/test_serial.py",
    "utils/common_strings.py",
    "utils/serial.py",
    "utils/mem.py",
    "dataset_get/dataset-get.py",
    "dataset_get/helper-scripts/make-archive.py",
    "dataset_get/dataset_resolver.py",
    "monitor.py",
    "optimization/batch_gradient_descent.py",
    "optimization/minres.py",
    "costs/ebm_estimation.py",
    "costs/gsn.py",
    "costs/mlp/missing_target_cost.py",
    "costs/autoencoder.py",
    "linear/conv2d.py",
    "linear/local_c01b.py",
    "linear/linear_transform.py",
    "linear/conv2d_c01b.py",
    "energy_functions/rbm_energy.py",
    "scripts/tests/test_autoencoder.py",
    "scripts/summarize_model.py",
    "scripts/lcc_tangents/make_dataset.py",
    "scripts/pkl_inspector.py",
    "scripts/show_binocular_greyscale_examples.py",
    "scripts/jobman/tester.py",
    "scripts/dbm/show_samples.py",
    "scripts/dbm/show_reconstructions.py",
    "scripts/dbm/dbm_metrics.py",
    "scripts/dbm/top_filters.py",
    "scripts/papers/maxout/svhn_preprocessing.py",
    "scripts/papers/jia_huang_wkshp_11/fit_final_model.py",
    "scripts/papers/jia_huang_wkshp_11/evaluate.py",
    "scripts/papers/jia_huang_wkshp_11/extract_features.py",
    "scripts/papers/jia_huang_wkshp_11/assemble.py",
    "scripts/gpu_pkl_to_cpu_pkl.py",
    "scripts/datasets/make_cifar10_whitened.py",
    "scripts/datasets/make_cifar100_patches_8x8.py",
    "scripts/datasets/make_cifar100_patches.py",
    "scripts/datasets/make_cifar10_gcn_whitened.py",
    "scripts/datasets/make_cifar100_whitened.py",
    "scripts/datasets/make_stl10_patches_8x8.py",
    "scripts/datasets/make_cifar100_gcn_whitened.py",
    "scripts/datasets/make_stl10_whitened.py",
    "scripts/datasets/make_stl10_patches.py",
    "scripts/gsn_example.py",
    "scripts/tutorials/deep_trainer/run_deep_trainer.py",
    "scripts/tutorials/grbm_smd/make_dataset.py",
    "scripts/tutorials/grbm_smd/test_grbm_smd.py",
    "scripts/icml_2013_wrepl/multimodal/"
    "extract_layer_2_kmeans_features.py",
    "scripts/icml_2013_wrepl/multimodal/make_submission.py",
    "scripts/icml_2013_wrepl/multimodal/lcn.py",
    "scripts/icml_2013_wrepl/multimodal/extract_kmeans_features.py",
    "scripts/icml_2013_wrepl/emotions/emotions_dataset.py",
    "scripts/icml_2013_wrepl/emotions/make_submission.py",
    "scripts/icml_2013_wrepl/black_box/black_box_dataset.py",
    "scripts/icml_2013_wrepl/black_box/make_submission.py",
    "scripts/diff_monitor.py",
    "corruption.py",
    "devtools/nan_guard.py",
    "sandbox/lisa_rl/bandit/gaussian_bandit.py",
    "config/tests/test_yaml_parse.py",
    "utils/iteration.py",
    "utils/track_version.py",
    "scripts/get_version.py",
    "blocks.py",
    "training_algorithms/tests/test_bgd.py",
    "training_algorithms/tests/test_sgd.py",
    "training_algorithms/tests/test_default.py",
    "training_algorithms/learning_rule.py",
    "training_algorithms/bgd.py",
    "training_algorithms/default.py",
    "training_algorithms/training_algorithm.py",
    "training_algorithms/sgd.py",
    "distributions/tests/test_mnd.py",
    "distributions/parzen.py",
    "distributions/uniform_hypersphere.py",
    "models/setup.py",
    "models/independent_multiclass_logistic.py",
    "models/softmax_regression.py",
    "models/tests/test_svm.py",
    "models/tests/test_reflection_clip.py",
    "models/tests/test_mlp.py",
    "models/tests/test_maxout.py",
    "models/tests/test_rbm.py",
    "models/tests/test_convelemwise_sigm.py",
    "models/mlp.py",
    "models/dbm/sampling_procedure.py",
    "models/svm.py",
    "models/rbm.py",
    "models/pca.py",
    "tests/test_train.py",
    "tests/test_theano.py",
    "packaged_dependencies/theano_linear/unshared_conv/gpu_unshared_conv.py",
    "packaged_dependencies/theano_linear/unshared_conv/test_unshared_conv.py",
    "packaged_dependencies/theano_linear/linearmixin.py",
    "packaged_dependencies/theano_linear/util.py",
    "packaged_dependencies/theano_linear/__init__.py",
    "packaged_dependencies/theano_linear/test_linear.py",
    "expr/tests/test_nnet.py",
    "expr/nnet.py",
    "expr/image.py",
    "expr/coding.py",
    "expr/normalize.py",
    "expr/probabilistic_max_pooling.py",
    "testing/tests/test.py",
    "testing/skip.py",
    "testing/prereqs.py",
    "testing/__init__.py",
    "gui/get_weights_report.py",
    "gui/patch_viewer.py",
    "sandbox/cuda_convnet/tests/test_response_norm.py",
    "sandbox/cuda_convnet/tests/profile_probabilistic_max_pooling.py",
    "sandbox/cuda_convnet/tests/test_rop_pool.py",
    "sandbox/cuda_convnet/tests/test_pool.py",
    "sandbox/cuda_convnet/tests/test_common.py",
    "sandbox/cuda_convnet/shared_code.py",
    "sandbox/cuda_convnet/code_templates.py",
    "sandbox/cuda_convnet/debug.py",
    "sandbox/lisa_rl/bandit/agent.py",
    "sandbox/lisa_rl/bandit/algorithm.py",
    "sandbox/lisa_rl/bandit/environment.py",
    "sandbox/lisa_rl/__init__.py",
    "space/__init__.py",
    "datasets/tests/test_preprocessing.py",
    "datasets/tests/test_mnist.py",
    "datasets/tests/test_cifar10.py",
    "datasets/tests/test_dense_design_matrix.py",
    "datasets/tests/test_vector_spaces_dataset.py",
    "datasets/tests/test_npy_npz.py",
    "datasets/avicenna.py",
    "datasets/iris.py",
    "datasets/adult.py",
    "datasets/zca_dataset.py",
    "datasets/npy_npz.py",
    "datasets/control.py",
    "datasets/cifar100.py",
    "datasets/transformer_dataset.py",
    "datasets/dataset.py",
    "termination_criteria/__init__.py",
    "__init__.py",
    "utils/logger.py",
    "utils/tests/test_mnist_ubyte.py",
    "utils/tests/test_data_specs.py",
    "utils/tests/test_bit_strings.py",
    "utils/tests/test_iteration.py",
    "utils/tests/test_string_utils.py",
    "utils/image.py",
    "utils/string_utils.py",
    "utils/theano_graph.py",
    "utils/__init__.py",
    "utils/datasets.py",
    "utils/data_specs.py",
    "utils/insert_along_axis.py",
    "utils/environ.py",
    "utils/call_check.py",
    "utils/python26.py",
    "deprecated/classifier.py",
    "train.py",
    "classifier.py",
    "dataset_get/helper-scripts/make-sources.py",
    "pca.py",
    "optimization/test_linesearch.py",
    "optimization/test_minres.py",
    "optimization/test_batch_gradient_descent.py",
    "optimization/linear_cg.py",
    "optimization/test_feature_sign.py",
    "optimization/feature_sign.py",
    "optimization/test_linear_cg.py",
    "optimization/linesearch.py",
    "costs/mlp/__init__.py",
    "costs/mlp/dropout.py",
    "costs/cost.py",
    "costs/dbm.py",
    "linear/tests/test_conv2d.py",
    "linear/tests/test_conv2d_c01b.py",
    "linear/matrixmul.py",
    "energy_functions/energy_function.py",
    "scripts/make_weights_image.py",
    "scripts/plot_monitor.py",
    "scripts/show_weights.py",
    "scripts/show_examples.py",
    "scripts/print_monitor.py",
    "scripts/num_parameters.py",
    "scripts/benchmark/time_relu.py",
    "scripts/jobman/experiment.py",
    "scripts/jobman/__init__.py",
    "scripts/dbm/show_negative_chains.py",
    "scripts/papers/maxout/compute_test_err.py",
    "scripts/papers/jia_huang_wkshp_11/npy2mat.py",
    "scripts/datasets/step_through_small_norb.py",
    "scripts/datasets/step_through_norb_foveated.py",
    "scripts/datasets/make_downsampled_stl10.py",
    "scripts/datasets/browse_small_norb.py",
    "scripts/datasets/make_mnistplus.py",
    "scripts/mlp/predict_csv.py",
    "scripts/find_gpu_fields.py",
    "scripts/tutorials/convolutional_network/tests/test_convnet.py",
    "scripts/tutorials/deep_trainer/test_deep_trainer.py",
    "scripts/icml_2013_wrepl/multimodal/make_wordlist.py",
    "base.py",
    "devtools/tests/test_via_pyflakes.py",
    "devtools/tests/test_shebangs.py",
    "devtools/tests/pep8/pep8.py",
    "devtools/tests/test_record.py",
    "devtools/tests/docscrape.py",
    "devtools/run_pyflakes.py",
    "devtools/record.py",
    "train_extensions/tests/test_window_flip.py",
    "train_extensions/__init__.py",
    "train_extensions/window_flip.py",
    "train_extensions/best_params.py"
]

whitelist_docstrings = [
    'scripts/datasets/step_through_norb_foveated.py',
    'blocks.py',
    'datasets/hdf5.py',
    'rbm_tools.py',
    'training_algorithms/tests/test_bgd.py',
    'training_algorithms/tests/test_sgd.py',
    'training_algorithms/tests/test_default.py',
    'training_algorithms/learning_rule.py',
    'training_algorithms/bgd.py',
    'training_algorithms/default.py',
    'training_algorithms/training_algorithm.py',
    'training_algorithms/__init__.py',
    'training_algorithms/sgd.py',
    'distributions/tests/test_mnd.py',
    'distributions/multinomial.py',
    'distributions/parzen.py',
    'distributions/__init__.py',
    'distributions/mnd.py',
    'distributions/uniform_hypersphere.py',
    'models/setup.py',
    'models/independent_multiclass_logistic.py',
    'models/softmax_regression.py',
    'models/sparse_autoencoder.py',
    'models/tests/test_svm.py',
    'models/tests/test_reflection_clip.py',
    'models/tests/test_dbm.py',
    'models/tests/test_gsn.py',
    'models/tests/test_dropout.py',
    'models/tests/test_autoencoder.py',
    'models/tests/test_mlp.py',
    'models/tests/test_s3c_inference.py',
    'models/tests/test_maxout.py',
    'models/tests/test_mnd.py',
    'models/tests/test_rbm.py',
    'models/tests/test_s3c_misc.py',
    'models/gsn.py',
    'models/dbm/sampling_procedure.py',
    'models/dbm/layer.py',
    'models/dbm/__init__.py',
    'models/dbm/dbm.py',
    'models/dbm/ising.py',
    'models/dbm/inference_procedure.py',
    'models/differentiable_sparse_coding.py',
    'models/local_coordinate_coding.py',
    'models/maxout.py',
    'models/s3c.py',
    'models/mnd.py',
    'models/svm.py',
    'models/rbm.py',
    'models/autoencoder.py',
    'tests/test_dbm_metrics.py',
    'tests/test_monitor.py',
    'tests/test_train.py',
    'tests/test_theano.py',
    'tests/rbm/test_ais.py',
    'kmeans.py',
    'packaged_dependencies/__init__.py',
    'packaged_dependencies/theano_linear/imaging.py',
    'packaged_dependencies/theano_linear/unshared_conv/__init__.py',
    'packaged_dependencies/theano_linear/unshared_conv/unshared_conv.py',
    'packaged_dependencies/theano_linear/linearmixin.py',
    'packaged_dependencies/theano_linear/linear.py',
    'packaged_dependencies/theano_linear/test_spconv.py',
    'expr/activations.py',
    'expr/tests/test_probabilistic_max_pooling.py',
    'expr/tests/test_preprocessing.py',
    'expr/tests/test_nnet.py',
    'expr/tests/test_coding.py',
    'expr/tests/test_normalize.py',
    'expr/tests/test_stochastic_pool.py',
    'expr/preprocessing.py',
    'expr/nnet.py',
    'expr/image.py',
    'expr/coding.py',
    'expr/__init__.py',
    'expr/stochastic_pool.py',
    'expr/sampling.py',
    'expr/normalize.py',
    'expr/probabilistic_max_pooling.py',
    'expr/information_theory.py',
    'expr/basic.py',
    'testing/tests/test.py',
    'testing/skip.py',
    'testing/prereqs.py',
    'testing/__init__.py',
    'testing/datasets.py',
    'testing/cost.py',
    'gui/graph_2D.py',
    'gui/get_weights_report.py',
    'gui/__init__.py',
    'gui/patch_viewer.py',
    'scalar.py',
    'sandbox/cuda_convnet/weight_acts.py',
    'sandbox/cuda_convnet/filter_acts.py',
    'sandbox/cuda_convnet/tests/test_filter_acts_strided.py',
    'sandbox/cuda_convnet/tests/test_probabilistic_max_pooling.py',
    'sandbox/cuda_convnet/tests/test_filter_acts.py',
    'sandbox/cuda_convnet/tests/test_img_acts.py',
    'sandbox/cuda_convnet/tests/test_response_norm.py',
    'sandbox/cuda_convnet/tests/profile_probabilistic_max_pooling.py',
    'sandbox/cuda_convnet/tests/test_weight_acts.py',
    'sandbox/cuda_convnet/tests/test_rop_pool.py',
    'sandbox/cuda_convnet/tests/test_pool.py',
    'sandbox/cuda_convnet/tests/test_common.py',
    'sandbox/cuda_convnet/tests/test_stochastic_pool.py',
    'sandbox/cuda_convnet/shared_code.py',
    'sandbox/cuda_convnet/__init__.py',
    'sandbox/cuda_convnet/img_acts.py',
    'sandbox/cuda_convnet/base_acts.py',
    'sandbox/cuda_convnet/pool.py',
    'sandbox/cuda_convnet/stochastic_pool.py',
    'sandbox/cuda_convnet/code_templates.py',
    'sandbox/cuda_convnet/probabilistic_max_pooling.py',
    'sandbox/tuple_var.py',
    'sandbox/__init__.py',
    'sandbox/lisa_rl/bandit/simulator.py',
    'sandbox/lisa_rl/bandit/agent.py',
    'sandbox/lisa_rl/bandit/algorithm.py',
    'sandbox/lisa_rl/bandit/environment.py',
    'sandbox/lisa_rl/bandit/average_agent.py',
    'sandbox/lisa_rl/bandit/classifier_bandit.py',
    'sandbox/lisa_rl/bandit/__init__.py',
    'sandbox/lisa_rl/bandit/classifier_agent.py',
    'sandbox/lisa_rl/bandit/gaussian_bandit.py',
    'sandbox/lisa_rl/__init__.py',
    'config/old_config.py',
    'config/tests/test_yaml_parse.py',
    'config/yaml_parse.py',
    'space/tests/test_space.py',
    'space/__init__.py',
    'datasets/norb.py',
    'datasets/utlc.py',
    'datasets/mnistplus.py',
    'datasets/cos_dataset.py',
    'datasets/cifar10.py',
    'datasets/svhn.py',
    'datasets/tests/test_preprocessing.py',
    'datasets/tests/test_mnist.py',
    'datasets/tests/test_imports.py',
    'datasets/tests/test_cifar10.py',
    'datasets/tests/test_norb.py',
    'datasets/tests/test_dense_design_matrix.py',
    'datasets/tests/test_vector_spaces_dataset.py',
    'datasets/tests/test_four_regions.py',
    'datasets/tests/test_csv_dataset.py',
    'datasets/tests/test_icml07.py',
    'datasets/tests/test_utlc.py',
    'datasets/preprocessing.py',
    'datasets/avicenna.py',
    'datasets/iris.py',
    'datasets/config.py',
    'datasets/dense_design_matrix.py',
    'datasets/adult.py',
    'datasets/tfd.py',
    'datasets/icml07.py',
    'datasets/zca_dataset.py',
    'datasets/filetensor.py',
    'datasets/npy_npz.py',
    'datasets/hepatitis.py',
    'datasets/wiskott.py',
    'datasets/control.py',
    'datasets/exc.py',
    'datasets/__init__.py',
    'datasets/mnist.py',
    'datasets/sparse_dataset.py',
    'datasets/csv_dataset.py',
    'datasets/cifar100.py',
    'datasets/tl_challenge.py',
    'datasets/transformer_dataset.py',
    'datasets/norb_small.py',
    'datasets/retina.py',
    'datasets/dataset.py',
    'datasets/ocr.py',
    'datasets/stl10.py',
    'datasets/matlab_dataset.py',
    'datasets/vector_spaces_dataset.py',
    'datasets/four_regions.py',
    'datasets/debug.py',
    'datasets/binarizer.py',
    'termination_criteria/__init__.py',
    '__init__.py',
    'utils/utlc.py',
    'utils/setup.py',
    'utils/compile.py',
    'utils/logger.py',
    'utils/general.py',
    'utils/one_hot.py',
    'utils/testing.py',
    'utils/tests/test_mnist_ubyte.py',
    'utils/tests/test_data_specs.py',
    'utils/tests/test_video.py',
    'utils/tests/test_bit_strings.py',
    'utils/tests/test_one_hot.py',
    'utils/tests/test_rng.py',
    'utils/tests/test_pooling.py',
    'utils/tests/test_iteration.py',
    'utils/tests/test_string_utils.py',
    'utils/tests/test_insert_along_axis.py',
    'utils/tests/test_utlc.py',
    'utils/tests/test_compile.py',
    'utils/tests/test_key_aware.py',
    'utils/key_aware.py',
    'utils/image.py',
    'utils/video.py',
    'utils/string_utils.py',
    'utils/bit_strings.py',
    'utils/iteration.py',
    'utils/pooling.py',
    'utils/theano_graph.py',
    'utils/exc.py',
    'utils/common_strings.py',
    'utils/datasets.py',
    'utils/serial.py',
    'utils/data_specs.py',
    'utils/shell.py',
    'utils/rng.py',
    'utils/insert_along_axis.py',
    'utils/environ.py',
    'utils/call_check.py',
    'utils/mnist_ubyte.py',
    'utils/track_version.py',
    'utils/mem.py',
    'utils/python26.py',
    'utils/timing.py',
    'deprecated/__init__.py',
    'deprecated/classifier.py',
    'train.py',
    'format/tests/test_target_format.py',
    'format/__init__.py',
    'dataset_get/dataset-get.py',
    'dataset_get/helper-scripts/make-sources.py',
    'dataset_get/helper-scripts/make-archive.py',
    'dataset_get/dataset_resolver.py',
    'pca.py',
    'monitor.py',
    'optimization/batch_gradient_descent.py',
    'optimization/__init__.py',
    'optimization/test_batch_gradient_descent.py',
    'optimization/linear_cg.py',
    'optimization/minres.py',
    'optimization/test_feature_sign.py',
    'optimization/feature_sign.py',
    'optimization/linesearch.py',
    'costs/tests/test_lp_penalty_cost.py',
    'costs/gsn.py',
    'costs/__init__.py',
    'costs/mlp/__init__.py',
    'costs/mlp/dropout.py',
    'costs/mlp/missing_target_cost.py',
    'costs/dbm.py',
    'costs/autoencoder.py',
    'linear/conv2d.py',
    'linear/tests/test_matrixmul.py',
    'linear/local_c01b.py',
    'linear/matrixmul.py',
    'linear/__init__.py',
    'linear/linear_transform.py',
    'linear/conv2d_c01b.py',
    'energy_functions/tests/__init__.py',
    'energy_functions/rbm_energy.py',
    'energy_functions/__init__.py',
    'energy_functions/energy_function.py',
    'scripts/plot_monitor.py',
    'scripts/print_model.py',
    'scripts/tests/test_autoencoder.py',
    'scripts/tests/__init__.py',
    'scripts/show_weights.py',
    'scripts/show_examples.py',
    'scripts/summarize_model.py',
    'scripts/pkl_inspector.py',
    'scripts/get_version.py',
    'scripts/print_monitor.py',
    'scripts/show_binocular_greyscale_examples.py',
    'scripts/num_parameters.py',
    'scripts/jobman/tester.py',
    'scripts/jobman/experiment.py',
    'scripts/jobman/__init__.py',
    'scripts/dbm/__init__.py',
    'scripts/dbm/dbm_metrics.py',
    'scripts/papers/__init__.py',
    'scripts/papers/jia_huang_wkshp_11/extract_features.py',
    'scripts/print_channel_doc.py',
    'scripts/gpu_pkl_to_cpu_pkl.py',
    'scripts/datasets/step_through_small_norb.py',
    'scripts/datasets/download_mnist.py',
    'scripts/datasets/browse_small_norb.py',
    'scripts/datasets/make_mnistplus.py',
    'scripts/__init__.py',
    'scripts/gsn_example.py',
    'scripts/mlp/predict_csv.py',
    'scripts/mlp/__init__.py',
    'scripts/find_gpu_fields.py',
    'scripts/tutorials/dbm_demo/train_dbm.py',
    'scripts/tutorials/dbm_demo/__init__.py',
    'scripts/tutorials/tests/test_dbm.py',
    'scripts/tutorials/tests/test_mlp_nested.py',
    'scripts/tutorials/multilayer_perceptron/tests/test_mlp.py',
    'scripts/tutorials/convolutional_network/tests/test_convnet.py',
    'scripts/tutorials/softmax_regression/tests/test_softmaxreg.py',
    'scripts/tutorials/deep_trainer/__init__.py',
    'scripts/tutorials/deep_trainer/run_deep_trainer.py',
    'scripts/tutorials/grbm_smd/make_dataset.py',
    'scripts/tutorials/grbm_smd/__init__.py',
    'scripts/tutorials/grbm_smd/test_grbm_smd.py',
    'scripts/tutorials/__init__.py',
    'scripts/tutorials/jobman_demo/utils.py',
    'scripts/tutorials/jobman_demo/__init__.py',
    'scripts/tutorials/stacked_autoencoders/tests/test_dae.py',
    'scripts/icml_2013_wrepl/__init__.py',
    'scripts/icml_2013_wrepl/multimodal/extract_layer_2_kmeans_features.py',
    'scripts/icml_2013_wrepl/multimodal/make_submission.py',
    'scripts/icml_2013_wrepl/multimodal/lcn.py',
    'scripts/icml_2013_wrepl/multimodal/__init__.py',
    'scripts/icml_2013_wrepl/multimodal/extract_kmeans_features.py',
    'scripts/icml_2013_wrepl/emotions/emotions_dataset.py',
    'scripts/icml_2013_wrepl/emotions/make_submission.py',
    'scripts/icml_2013_wrepl/emotions/__init__.py',
    'scripts/icml_2013_wrepl/black_box/black_box_dataset.py',
    'scripts/icml_2013_wrepl/black_box/make_submission.py',
    'scripts/icml_2013_wrepl/black_box/__init__.py',
    'scripts/train.py',
    'scripts/diff_monitor.py',
    'base.py',
    'devtools/tests/test_via_pyflakes.py',
    'devtools/tests/test_shebangs.py',
    'devtools/tests/test_record.py',
    'devtools/tests/__init__.py',
    'devtools/tests/docscrape.py',
    'devtools/run_pyflakes.py',
    'devtools/nan_guard.py',
    'devtools/__init__.py',
    'devtools/record.py',
    'train_extensions/best_params.py',
    'corruption.py',
    'datasets/tests/test_tl_challenge.py',
    'datasets/tests/test_tfd.py',
    'datasets/tests/test_npy_npz.py',
    'linear/tests/test_conv2d.py',
    'devtools/tests/pep8/pep8.py',
    'devtools/tests/pep8/__init__.py']

# add files which have long execution time to whitelist_docstrings
whitelist_docstrings.extend([
    'sandbox/cuda_convnet/debug.py',
    'energy_functions/tests/test_rbm_energy.py',
    'scripts/icml_2013_wrepl/multimodal/make_wordlist.py',
    'scripts/make_weights_image.py', 'costs/ebm_estimation.py',
    'classifier.py', 'scripts/lcc_tangents/make_dataset.py',
    'scripts/datasets/make_cifar10_whitened.py',
    'scripts/datasets/make_cifar100_patches.py',
    'scripts/datasets/make_cifar10_gcn_whitened.py',
    'scripts/datasets/make_stl10_patches_8x8.py',
    'scripts/datasets/make_cifar100_gcn_whitened.py',
    'scripts/datasets/make_stl10_whitened.py',
    'scripts/datasets/make_stl10_patches.py'])

# add files which fail to run to whitelist_docstrings
whitelist_docstrings.extend([
    'training_algorithms/tests/test_learning_rule.py',
    'models/pca.py',
    'datasets/tests/test_hdf5.py',
    'linear/tests/test_conv2d_c01b.py',
    'packaged_dependencies/theano_linear/conv2d.py',
    'packaged_dependencies/theano_linear/pyramid.py',
    'packaged_dependencies/theano_linear/unshared_conv/gpu_unshared_conv.py',
    'packaged_dependencies/theano_linear/unshared_conv/'
    'test_gpu_unshared_conv.py',
    'packaged_dependencies/theano_linear/unshared_conv/test_localdot.py',
    'packaged_dependencies/theano_linear/unshared_conv/test_unshared_conv.py',
    'packaged_dependencies/theano_linear/unshared_conv/localdot.py',
    'packaged_dependencies/theano_linear/util.py',
    'packaged_dependencies/theano_linear/__init__.py',
    'packaged_dependencies/theano_linear/test_matrixmul.py',
    'packaged_dependencies/theano_linear/test_linear.py',
    'packaged_dependencies/theano_linear/spconv.py',
    'sandbox/cuda_convnet/tests/test_weight_acts_strided.py',
    'sandbox/cuda_convnet/tests/test_image_acts_strided.py',
    'sandbox/cuda_convnet/specialized_bench.py',
    'sandbox/cuda_convnet/response_norm.py',
    'sandbox/cuda_convnet/convnet_compile.py',
    'sandbox/cuda_convnet/pthreads.py',
    'sandbox/cuda_convnet/bench.py',
    'sandbox/lisa_rl/bandit/plot_reward.py',
    'sandbox/lisa_rl/bandit/simulate.py',
    'config/__init__.py',
    'utils/__init__.py',
    'optimization/test_linesearch.py',
    'optimization/test_minres.py',
    'optimization/test_linear_cg.py',
    'scripts/dbm/show_samples.py',
    'scripts/dbm/show_reconstructions.py',
    'scripts/dbm/top_filters.py',
    'scripts/dbm/show_negative_chains.py',
    'scripts/papers/maxout/svhn_preprocessing.py',
    'scripts/papers/maxout/compute_test_err.py',
    'scripts/papers/jia_huang_wkshp_11/fit_final_model.py',
    'scripts/papers/jia_huang_wkshp_11/evaluate.py',
    'scripts/papers/jia_huang_wkshp_11/npy2mat.py',
    'scripts/papers/jia_huang_wkshp_11/assemble.py',
    'scripts/datasets/make_cifar100_patches_8x8.py',
    'scripts/datasets/make_downsampled_stl10.py',
    'scripts/datasets/make_cifar100_whitened.py',
    'scripts/tutorials/deep_trainer/test_deep_trainer.py',
    'scripts/icml_2013_wrepl/black_box/learn_zca.py',
    'train_extensions/tests/test_window_flip.py',
    'train_extensions/window_flip.py',
    'linear/tests/test_local_c01b.py'])


def test_format_pep8():
    """
    Test if pep8 is respected.
    """
    pep8_checker = StyleGuide()
    files_to_check = []
    for path in list_files(".py"):
        rel_path = os.path.relpath(path, pylearn2.__path__[0])
        if rel_path in whitelist_pep8:
            continue
        else:
            files_to_check.append(path)
    report = pep8_checker.check_files(files_to_check)
    if report.total_errors > 0:
        raise AssertionError("PEP8 Format not respected")


def print_files_information_pep8():
    """
    Print the list of files which can be removed from the whitelist and the
    list of files which do not respect PEP8 formatting that aren't in the
    whitelist
    """
    infracting_files = []
    non_infracting_files = []
    pep8_checker = StyleGuide(quiet=True)
    for path in list_files(".py"):
        number_of_infractions = pep8_checker.input_file(path)
        rel_path = os.path.relpath(path, pylearn2.__path__[0])
        if number_of_infractions > 0:
            if rel_path not in whitelist_pep8:
                infracting_files.append(path)
        else:
            if rel_path in whitelist_pep8:
                non_infracting_files.append(path)
    print "Files that must be corrected or added to whitelist:"
    for file in infracting_files:
        print file
    print "Files that can be removed from whitelist:"
    for file in non_infracting_files:
        print file


def test_format_docstrings():
    """
    Test if docstrings are well formatted.
    """

    try:
        verify_format_docstrings()
    except SkipTest, e:
        import traceback
        traceback.print_exc(e)
        raise AssertionError(
            "Some file raised SkipTest on import, and inadvertently"
            " canceled the documentation testing."
        )


def verify_format_docstrings():
    """
    Implementation of `test_format_docstrings`. The implementation is
    factored out so it can be placed inside a guard against SkipTest.
    """
    format_infractions = []

    for path in list_files(".py"):
        rel_path = os.path.relpath(path, pylearn2.__path__[0])
        if rel_path in whitelist_docstrings:
            continue
        try:
            format_infractions.extend(docstring_errors(path))
        except StandardError as e:
            format_infractions.append(["%s failed to run so format cannot "
                                       "be checked. Error message:\n %s" %
                                       (rel_path, e)])

    if len(format_infractions) > 0:
        msg = "\n".join(':'.join(line) for line in format_infractions)
        raise AssertionError("Docstring format not respected:\n%s" % msg)

if __name__ == "__main__":
    print_files_information_pep8()

########NEW FILE########
__FILENAME__ = test_record
from pylearn2.devtools.record import Record
from pylearn2.devtools.record import RecordMode
from pylearn2.devtools.record import MismatchError
from theano import function
from theano.tensor import iscalar
import cStringIO
import warnings

warnings.warn("These tests should be moved to theano.")

def test_record_good():

    """
    Tests that when we record a sequence of events, then
    repeat it exactly, the Record class:
        1) Records it correctly
        2) Does not raise any errors
    """

    # Record a sequence of events
    output = cStringIO.StringIO()

    recorder = Record(file_object=output, replay=False)

    num_lines = 10

    for i in xrange(num_lines):
        recorder.handle_line(str(i)+'\n')

    # Make sure they were recorded correctly
    output_value = output.getvalue()

    assert output_value == ''.join(str(i)+'\n' for i in xrange(num_lines))

    # Make sure that the playback functionality doesn't raise any errors
    # when we repeat them
    output = cStringIO.StringIO(output_value)

    playback_checker = Record(file_object=output,  replay=True)

    for i in xrange(num_lines):
        playback_checker.handle_line(str(i)+'\n')


def test_record_bad():

    """
    Tests that when we record a sequence of events, then
    do something different on playback, the Record class catches it.
    """

    # Record a sequence of events
    output = cStringIO.StringIO()

    recorder = Record(file_object=output, replay=False)

    num_lines = 10

    for i in xrange(num_lines):
        recorder.handle_line(str(i)+'\n')

    # Make sure that the playback functionality doesn't raise any errors
    # when we repeat some of them
    output_value = output.getvalue()
    output = cStringIO.StringIO(output_value)

    playback_checker = Record(file_object=output,  replay=True)

    for i in xrange(num_lines // 2):
        playback_checker.handle_line(str(i)+'\n')

    # Make sure it raises an error when we deviate from the recorded sequence
    try:
        playback_checker.handle_line('0\n')
    except MismatchError:
        return
    raise AssertionError("Failed to detect mismatch between recorded sequence "
            " and repetition of it.")


def test_record_mode_good():

    """
    Like test_record_good, but some events are recorded by the
    theano RecordMode. We don't attempt to check the
    exact string value of the record in this case.
    """

    # Record a sequence of events
    output = cStringIO.StringIO()

    recorder = Record(file_object=output, replay=False)

    record_mode = RecordMode(recorder)

    i = iscalar()
    f = function([i], i, mode=record_mode, name='f')

    num_lines = 10

    for i in xrange(num_lines):
        recorder.handle_line(str(i)+'\n')
        f(i)

    # Make sure that the playback functionality doesn't raise any errors
    # when we repeat them
    output_value = output.getvalue()
    output = cStringIO.StringIO(output_value)

    playback_checker = Record(file_object=output,  replay=True)

    playback_mode = RecordMode(playback_checker)

    i = iscalar()
    f = function([i], i, mode=playback_mode, name='f')

    for i in xrange(num_lines):
        playback_checker.handle_line(str(i)+'\n')
        f(i)

def test_record_mode_bad():

    """
    Like test_record_bad, but some events are recorded by the
    theano RecordMode, as is the event that triggers the mismatch
    error.
    """

    # Record a sequence of events
    output = cStringIO.StringIO()

    recorder = Record(file_object=output, replay=False)

    record_mode = RecordMode(recorder)

    i = iscalar()
    f = function([i], i, mode=record_mode, name='f')

    num_lines = 10

    for i in xrange(num_lines):
        recorder.handle_line(str(i)+'\n')
        f(i)

    # Make sure that the playback functionality doesn't raise any errors
    # when we repeat them
    output_value = output.getvalue()
    output = cStringIO.StringIO(output_value)

    playback_checker = Record(file_object=output,  replay=True)

    playback_mode = RecordMode(playback_checker)

    i = iscalar()
    f = function([i], i, mode=playback_mode, name='f')

    for i in xrange(num_lines // 2):
        playback_checker.handle_line(str(i)+'\n')
        f(i)

    # Make sure a wrong event causes a MismatchError
    try:
        f(0)
    except MismatchError:
        return
    raise AssertionError("Failed to detect a mismatch.")

########NEW FILE########
__FILENAME__ = test_shebangs
__author__ = "Ian Goodfellow"

from pylearn2.devtools.list_files import list_files

def test_shebangs():
    # Make sure all scripts that use shebangs use /usr/bin/env
    # (instead of the non-standard /bin/env or hardcoding the path to
    # the interpreter). This test allows any shebang lines that start
    # with /usr/bin/env. Examples:
    #   "#!/usr/bin/env python"
    #   "#! /usr/bin/env python"
    #   "#!/usr/bin/env ipython"
    #   "#!/usr/bin/env ipython --pylab --"
    # etc.
    files = list_files('.py')
    for f in files:
        fd = open(f, 'r')
        l = fd.readline()
        fd.close()
        if l.startswith("#!"):
            if not l[2:].strip().startswith("/usr/bin/env"):
                print l
                print f
                raise AssertionError("Bad shebang")

########NEW FILE########
__FILENAME__ = test_via_pyflakes
from pylearn2.devtools.run_pyflakes import run_pyflakes
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

def test_via_pyflakes():
    d = run_pyflakes(True)
    if len(d.keys()) != 0:
        print 'Errors detected by pyflakes'
        for key in d.keys():
            print key+':'
            for l in d[key].split('\n'):
                print '\t',l

        raise AssertionError("You have errors detected by pyflakes")

########NEW FILE########
__FILENAME__ = mnd
"""A Multivariate Normal Distribution."""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"
import warnings
try:
    from scipy.linalg import cholesky, det, solve
except ImportError:
    warnings.warn("Could not import some scipy.linalg functions")
import theano.tensor as T
from theano import config
from pylearn2.utils import sharedX
from pylearn2.utils.rng import make_theano_rng
import numpy as np
N = np


class MND(object):
    """
    A Multivariate Normal Distribution

    .. todo::

        WRITEME properly
    
    Parameters
    -----------
    sigma : WRITEME
        A numpy ndarray of shape (n,n)
    mu : WRITEME
        A numpy ndarray of shape (n,)
    seed : WRITEME
        The seed for the theano random number generator used to sample from
        this distribution
    """
    def __init__(self, sigma, mu, seed=42):
        self.sigma = sigma
        self.mu = mu
        if not (len(mu.shape) == 1):
            raise Exception('mu has shape ' + str(mu.shape) +
                            ' (it should be a vector)')

        self.sigma_inv = solve(self.sigma, N.identity(mu.shape[0]),
                               sym_pos=True)
        self.L = cholesky(self.sigma)

        self.s_rng = make_theano_rng(seed, which_method='normal')

        #Compute logZ
        #log Z = log 1/( (2pi)^(-k/2) |sigma|^-1/2 )
        # = log 1 - log (2pi^)(-k/2) |sigma|^-1/2
        # = 0 - log (2pi)^(-k/2) - log |sigma|^-1/2
        # = (k/2) * log(2pi) + (1/2) * log |sigma|
        k = float(self.mu.shape[0])
        self.logZ = 0.5 * (k * N.log(2. * N.pi) + N.log(det(sigma)))

    def free_energy(self, X):
        """
        .. todo::

            WRITEME
        """
        #design matrix format
        return .5 * T.sum(T.dot(X - self.mu,
                                T.dot(self.sigma_inv,
                                      T.transpose(X - self.mu))))

    def log_prob(self, X):
        """
        .. todo::

            WRITEME
        """
        return - self.free_energy(X) - self.logZ

    def random_design_matrix(self, m):
        """
        .. todo::

            WRITEME
        """
        Z = self.s_rng.normal(size=(m, self.mu.shape[0]),
                              avg=0., std=1., dtype=config.floatX)
        return self.mu + T.dot(Z, self.L.T)


def fit(dataset, n_samples=None):
    """
    .. todo::

        WRITEME properly
    
    Returns an MND fit to n_samples drawn from dataset.

    Not a class method because we currently don't have a means
    of calling class methods from YAML files.
    """
    if n_samples is not None:
        X = dataset.get_batch_design(n_samples)
    else:
        X = dataset.get_design_matrix()
    return MND(sigma=N.cov(X.T), mu=X.mean(axis=0))


class AdditiveDiagonalMND:
    """
    A conditional distribution that adds gaussian noise with diagonal precision
    matrix beta to another variable that it conditions on

    Parameters
    ----------
    init_beta : WRITEME
    nvis : WRITEME
    """

    def __init__(self, init_beta, nvis):
        self.__dict__.update(locals())
        del self.self

        self.beta = sharedX(np.ones((nvis,))*init_beta)
        assert self.beta.ndim == 1

        self.s_rng = make_theano_rng(None, 17, which_method='normal')

    def random_design_matrix(self, X):
        """
        .. todo::

            WRITEME properly

        Parameters
        ----------
        X : WRITEME
            A theano variable containing a design matrix of 
            observations of the random vector to condition on.
        """
        Z = self.s_rng.normal(size=X.shape,
                              avg=X, std=1./T.sqrt(self.beta), dtype=config.floatX)
        return Z

    def is_symmetric(self):
        """
        .. todo::

            WRITEME properly

        A property of conditional distributions
        P(Y|X)
        Return true if P(y|x) = P(x|y) for all x,y
        """

        return True

########NEW FILE########
__FILENAME__ = multinomial
"""
.. todo::

    WRITEME
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"
import numpy as N


class Multinomial(object):
    """
    .. todo::

        WRITEME
    """

    def __init__(self, rng, pi, renormalize=False):
        self.pi = pi
        assert self.pi.min() >= 0.0
        self.rng = rng
        if renormalize:
            self.pi = self.pi / self.pi.sum()
        else:
            assert abs(1.0 - self.pi.sum()) < 1e-10

    def sample_integer(self, m):
        """
        .. todo::

            WRITEME
        """
        return N.nonzero(
            self.rng.multinomial(pvals=self.pi, n=1, size=(m,))
        )[1]

########NEW FILE########
__FILENAME__ = parzen
"""
.. todo::

    WRITEME
"""
import numpy
import theano
T = theano.tensor


def log_mean_exp(a):
    """
    .. todo::

        WRITEME
    """
    max_ = a.max(1)
    return max_ + T.log(T.exp(a - max_.dimshuffle(0, 'x')).mean(1))

def make_lpdf(mu, sigma):
    """
    Makes a Theano function that allows the evalution of a Parzen windows
    estimator (aka kernel density estimator) where the Kernel is a normal
    distribution with stddev sigma and with points at mu.

    Parameters
    -----------
    mu : numpy matrix
        Contains the data points over which this distribution is based.
    sigma : scalar
        The standard deviation of the normal distribution around each data \
        point.

    Returns
    -------
    lpdf : callable
        Estimator of the log of the probability density under a point.
    """
    x = T.matrix()
    mu = theano.shared(mu)

    a = ( x.dimshuffle(0, 'x', 1) - mu.dimshuffle('x', 0, 1) ) / sigma

    E = log_mean_exp(-0.5*(a**2).sum(2))

    Z = mu.shape[1] * T.log(sigma * numpy.sqrt(numpy.pi * 2))

    return theano.function([x], E - Z)


class ParzenWindows(object):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    samples : numpy matrix
        See description for make_lpdf
    sigma : scalar
        See description for make_lpdf
    """
    def __init__(self, samples, sigma):
        # just keeping these for debugging/examination, not needed
        self._samples = samples
        self._sigma = sigma

        self.lpdf = make_lpdf(samples, sigma)

    def get_ll(self, x, batch_size=10):
        """
        Evaluates the log likelihood of a set of datapoints with respect to the
        probability distribution.

        Parameters
        ----------
        x : numpy matrix
            The set of points for which you want to evaluate the log \
            likelihood.
        """
        inds = range(x.shape[0])
        n_batches = int(numpy.ceil(float(len(inds)) / batch_size))

        lls = []
        for i in range(n_batches):
            lls.extend(self.lpdf(x[inds[i::n_batches]]))

        return numpy.array(lls).mean()

########NEW FILE########
__FILENAME__ = test_mnd
import numpy as np
from pylearn2.distributions.mnd import MND
from theano import function
from pylearn2.testing.skip import skip_if_no_scipy

def test_seed_same():
    """Verifies that two MNDs initialized with the same
    seed produce the same samples """

    skip_if_no_scipy()

    rng = np.random.RandomState([1,2,3])

    #the number in the argument here is the limit on
    #seed value
    seed = rng.randint(2147462579)

    dim = 3

    mu = rng.randn(dim)

    rank = dim

    X = rng.randn(rank,dim)

    cov = np.dot(X.T,X)

    mnd1 = MND( sigma = cov, mu = mu, seed = seed)

    num_samples = 5

    rd1 = mnd1.random_design_matrix(num_samples)
    rd1 = function([],rd1)()

    mnd2 = MND( sigma = cov, mu = mu, seed = seed)

    rd2 = mnd2.random_design_matrix(num_samples)
    rd2 = function([],rd2)()

    assert np.all(rd1 == rd2)


def test_seed_diff():
    """Verifies that two MNDs initialized with different
    seeds produce samples that differ at least somewhat
    (theoretically the samples could match even under
    valid behavior but this is extremely unlikely)"""

    skip_if_no_scipy()

    rng = np.random.RandomState([1,2,3])

    #the number in the argument here is the limit on
    #seed value, and we subtract 1 so it will be
    #possible to add 1 to it for the second MND
    seed = rng.randint(2147462579) -1

    dim = 3

    mu = rng.randn(dim)

    rank = dim

    X = rng.randn(rank,dim)

    cov = np.dot(X.T,X)

    mnd1 = MND( sigma = cov, mu = mu, seed = seed)

    num_samples = 5

    rd1 = mnd1.random_design_matrix(num_samples)
    rd1 = function([],rd1)()

    mnd2 = MND( sigma = cov, mu = mu, seed = seed + 1)

    rd2 = mnd2.random_design_matrix(num_samples)
    rd2 = function([],rd2)()

    assert np.any(rd1 != rd2)

########NEW FILE########
__FILENAME__ = uniform_hypersphere
"""
.. todo::

    WRITEME
"""
uthors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"
import numpy as N
import theano.tensor as T
from theano import config
from scipy.special import gammaln
from pylearn2.utils.rng import make_theano_rng


class UniformHypersphere(object):
    """
    .. todo::

        WRITEME
    """
    def __init__(self, dim, radius):
        self.dim = dim
        self.radius = radius
        self.s_rng = make_theano_rng(None, 42, which_method='normal')
        log_C = ((float(self.dim) / 2.) * N.log(N.pi) -
                 gammaln(1. + float(self.dim) / 2.))
        self.logZ = N.log(self.dim) + log_C + (self.dim - 1) * N.log(radius)
        assert not N.isnan(self.logZ)
        assert not N.isinf(self.logZ)

    def free_energy(self, X):
        """
        .. todo::

            WRITEME properly

        Parameters
        ----------
        X : WRITEME
            Must contain only examples that lie on the hypersphere
        """
        #design matrix format

        return T.zeros_like(X[:, 0])

    def log_prob(self, X):
        """
        .. todo::

            WRITEME
        """
        return - self.free_energy(X) - self.logZ

    def random_design_matrix(self, m):
        """
        .. todo::

            WRITEME
        """
        Z = self.s_rng.normal(size=(m, self.dim),
                              avg=0., std=1., dtype=config.floatX)
        Z.name = 'UH.rdm.Z'
        sq_norm_Z = T.sum(T.sqr(Z), axis=1)
        sq_norm_Z.name = 'UH.rdm.sq_norm_Z'
        eps = 1e-6
        mask = sq_norm_Z < eps
        mask.name = 'UH.rdm.mask'
        Z = (Z.T * (1. - mask) + mask).T
        Z.name = 'UH.rdm.Z2'
        sq_norm_Z = sq_norm_Z * (1. - mask) + self.dim * mask
        sq_norm_Z.name = 'UH.rdm.sq_norm_Z2'
        norm_Z = T.sqrt(sq_norm_Z)
        norm_Z.name = 'UH.rdm.sq_norm_Z2'
        rval = self.radius * (Z.T / norm_Z).T
        rval.name = 'UH.rdm.rval'
        return rval

########NEW FILE########
__FILENAME__ = energy_function
"""
.. todo::

    WRITEME
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"
import theano.tensor as T


class EnergyFunction(object):
    """
    .. todo::

        WRITEME
    """

    def __init__(self):
        pass

    def score(self, X):
        """
        .. todo::

            WRITEME
        """
        assert X.dtype.find('int') == -1

        X_name = 'X' if X.name is None else X.name

        E = self.free_energy(X)

        #There should be one energy value for each example in the batch
        assert len(E.type.broadcastable) == 1

        dummy = T.sum(E)
        rval = T.grad(dummy, X)
        rval.name = 'score(' + X_name + ')'
        return rval

    def free_energy(self, X):
        """
        .. todo::

            WRITEME
        """
        raise NotImplementedError(str(type(self)) +
                                  ' has not implemented free_energy(self,X)')

    def energy(self, varlist):
        """
        .. todo::

            WRITEME
        """
        raise NotImplementedError(str(type(self)) +
                                  ' has not implemented energy(self,varlist)')

    def __call__(self, varlist):
        """
        .. todo::

            WRITEME
        """
        return self.energy(varlist)

########NEW FILE########
__FILENAME__ = rbm_energy
"""
.. todo::

    WRITEME
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"
from pylearn2.energy_functions.energy_function import EnergyFunction
import theano.tensor as T

class RBM_EnergyFunction(EnergyFunction):
    """
    .. todo::

        WRITEME
    """

    def __init__(self):
        pass

class GRBM_EnergyFunction(RBM_EnergyFunction):
    """
    .. todo::

        WRITEME
    """

    def supports_vector_sigma(self):
        """
        .. todo::

            WRITEME
        """
        raise NotImplementedError()

    def log_P_H_given_V(self, H, V):
        """
        .. todo::

            WRITEME
        """
        p_one = self.mean_H_given_V(V)

        rval =  T.log(H * p_one + (1.-H) * (1.-p_one)).sum(axis=1)

        return rval

    def mean_H_given_V(self, V):
        """
        .. todo::

            WRITEME
        """
        raise NotImplementedError()


class GRBM_Type_1(GRBM_EnergyFunction):
    """
    .. todo::

        WRITEME properly
    
    TODO: give a better name

    This GRBM energy function is designed to make
    it easy to interpret score matching as being a denoising autoencoder.

    It is not the same energy function as presented in equation 4.6 of
    Pascal Vincent's paper on denoising autoencoders and score matching,
    because that energy function has no latent variables and therefore
    is not an RBM. However, this is a very similar energy function,
    and has the property that

    J_SMD = (1/sigma)^4 J_DAE

    when the same sigma is used for both the gaussian corruption process
    and the model

    E(v,h) = -(bias_vis^T v + v^T W h + bias_hid^T h - (1/2) v^T v ) / sigma^2
    P(v|h) = N( Wh + bias_vis, sigma^2)
    P(h|v) = sigmoid( (v^T Wh + bias_hid) / sigma^2 )
    F(v) = ( (1/2) v^T v - bias_vis^T v) / sigma^2 - sum_i softplus( ( v^T W + c) / sigma^2 )_i
    score(v) = -( v - bias_vis - sigmoid( (v^T W + bias_hid) / sigma^2 ) W^T )/sigma^2

    This parameterization is motivated by this last property, that the entire score
    function is divided by sigma^2, which makes the equivalence with denosing
    autoencoders possible.

    (As far as I know, I, Ian Goodfellow, just made this parameterization
    of GRBMs up as a way of testing SMD, so don't try to use it to exactly
    reproduce any published GRBM results, as they probably use one of the
    other parameterizations)

    Parameters
    ----------
    transformer : WRITEME
    bias_hid : WRITEME
    bias_vis : WRITEME
    sigma : WRITEME
    """

    def __init__(self, transformer, bias_hid, bias_vis, sigma):
        super(GRBM_Type_1,self).__init__()

        self.transformer = transformer
        self.bias_hid = bias_hid
        self.bias_vis = bias_vis
        self.sigma = sigma

    @classmethod
    def supports_vector_sigma(cls):
        """
        .. todo::

            WRITEME
        """
        return False

    def energy(self, varlist):
        """
        .. todo::

            WRITEME
        """
        V, H = varlist
        return - (
                    T.dot(V, self.bias_vis) +
                    (self.transformer.lmul(V) * H).sum(axis=1) +
                    T.dot(H, self.bias_hid) -
                    0.5 * T.sqr(V).sum(axis=1)
                ) / T.sqr(self.sigma)


    def mean_H_given_V(self, V):
        """
        .. todo::

            WRITEME
        """
        V_name = 'V'
        if hasattr(V, 'name') and V.name is not None:
            V_name = V.name

        rval =  T.nnet.sigmoid( \
                ( \
                    self.bias_hid + \
                    self.transformer.lmul(V) \
                ) / T.sqr(self.sigma) \
                        )

        rval.name = 'mean_H_given_V( %s )' % V_name

        return rval

    def reconstruct(self, V):
        """
        .. todo::

            WRITEME
        """
        H = self.mean_H_given_V(V)
        R = self.mean_V_given_H(H)
        return R

    def mean_V_given_H(self, H):
        """
        .. todo::

            WRITEME
        """
        H_name = 'H'
        if hasattr(H,'name') and H.name is not None:
            H_name = H.name

        transpose = self.transformer.lmul_T(H)
        transpose.name = 'transpose'

        rval =  self.bias_vis + transpose
        rval.name = 'mean_V_given_H(%s)' % H_name

        return rval

    def free_energy(self, V):
        """
        .. todo::

            WRITEME
        """
        V_name = 'V' if V.name is None else V.name

        assert V.ndim == 2

        bias_term = T.dot(V,self.bias_vis)
        bias_term.name = 'bias_term'
        assert len(bias_term.type.broadcastable) == 1

        sq_term = 0.5 * T.sqr(V).sum(axis=1)
        sq_term.name = 'sq_term'
        assert len(sq_term.type.broadcastable) == 1

        softplus_term =  T.nnet.softplus( (self.transformer.lmul(V)+self.bias_hid) / T.sqr(self.sigma)).sum(axis=1)
        assert len(softplus_term.type.broadcastable) == 1
        softplus_term.name = 'softplus_term'

        return (
                sq_term
                - bias_term
                ) / T.sqr(self.sigma) - softplus_term

    def score(self, V):
        """
        .. todo::

            WRITEME
        """
        #score(v) = ( v - bias_vis - sigmoid( beta v^T W + bias_hid ) W^T )/sigma^2

        rval =  -( V \
                - self.reconstruct(V) \
                ) / \
            T.sqr(self.sigma)

        rval.name = 'score'

        return rval

def grbm_type_1():
    """
    .. todo::

        WRITEME
    """
    return GRBM_Type_1

########NEW FILE########
__FILENAME__ = test_rbm_energy
import theano
theano.config.compute_test_value = 'off'
from pylearn2.energy_functions.rbm_energy import GRBM_Type_1
import numpy as N
import theano.tensor as T
from theano import function
from pylearn2.utils import as_floatX
from pylearn2.utils import sharedX
from pylearn2.linear.matrixmul import MatrixMul

test_m = 2

rng = N.random.RandomState([1, 2, 3])
nv = 3
nh = 4

vW = rng.randn(nv, nh)
W = sharedX(vW)
vbv = as_floatX(rng.randn(nv))
bv = T.as_tensor_variable(vbv)
bv.tag.test_value = vbv
vbh = as_floatX(rng.randn(nh))
bh = T.as_tensor_variable(vbh)
bh.tag.test_value = bh
vsigma = as_floatX(rng.uniform(0.1, 5))
sigma = T.as_tensor_variable(vsigma)
sigma.tag.test_value = vsigma

E = GRBM_Type_1(transformer=MatrixMul(W), bias_vis=bv,
                bias_hid=bh, sigma=sigma)

V = T.matrix()
V.tag.test_value = as_floatX(rng.rand(test_m, nv))
H = T.matrix()
H.tag.test_value = as_floatX(rng.rand(test_m, nh))

E_func = function([V, H], E([V, H]))
F_func = function([V], E.free_energy(V))
log_P_H_given_V_func = function([H, V], E.log_P_H_given_V(H, V))
score_func = function([V], E.score(V))

F_of_V = E.free_energy(V)
dummy = T.sum(F_of_V)
negscore = T.grad(dummy, V)
score = - negscore

generic_score_func = function([V], score)


class TestGRBM_Type_1:
    def test_mean_H_given_V(self):
        tol = 1e-6

        # P(h_1 | v) / P(h_2 | v) = a
        # => exp(-E(v, h_1)) / exp(-E(v,h_2)) = a
        # => exp(E(v,h_2)-E(v,h_1)) = a
        # E(v,h_2) - E(v,h_1) = log(a)
        # also log P(h_1 | v) - log P(h_2) = log(a)

        rng = N.random.RandomState([1, 2, 3])

        m = 5

        Vv = as_floatX(N.zeros((m, nv)) + rng.randn(nv))

        Hv = as_floatX(rng.randn(m, nh) > 0.)

        log_Pv = log_P_H_given_V_func(Hv, Vv)

        Ev = E_func(Vv, Hv)

        for i in xrange(m):
            for j in xrange(i + 1, m):
                log_a = log_Pv[i] - log_Pv[j]
                e = Ev[j] - Ev[i]

                assert abs(e-log_a) < tol

    def test_free_energy(self):

        rng = N.random.RandomState([1, 2, 3])

        m = 2 ** nh

        Vv = as_floatX(N.zeros((m, nv)) + rng.randn(nv))

        F, = F_func(Vv[0:1, :])

        Hv = as_floatX(N.zeros((m, nh)))

        for i in xrange(m):
            for j in xrange(nh):
                Hv[i, j] = (i & (2 ** j)) / (2 ** j)

        Ev = E_func(Vv, Hv)

        Fv = -N.log(N.exp(-Ev).sum())
        assert abs(F-Fv) < 1e-6

    def test_score(self):
        rng = N.random.RandomState([1, 2, 3])

        m = 10

        Vv = as_floatX(rng.randn(m, nv))

        Sv = score_func(Vv)
        gSv = generic_score_func(Vv)

        assert N.allclose(Sv, gSv)

########NEW FILE########
__FILENAME__ = activations
"""
.. todo::

    WRITEME
"""
import theano
T = theano.tensor


def identity(x):
    """
    .. todo::

        WRITEME properly

    Importable identity function. Created for the purposes of pickling.
    """
    return x


def relu(x):
    """
    .. todo::

        WRITEME properly

    Rectified linear activation
    """
    return T.max(0, x)


def _rescale_softmax(sm, min_val):
    """
    .. todo::

        WRITEME
    """
    n_classes = sm.shape[-1]
    # Avoid upcast to float64 when floatX==float32 and n_classes is int64
    n_classes = n_classes.astype(theano.config.floatX)
    return sm * (1 - n_classes * min_val) + min_val


def rescaled_softmax(x, min_val=1e-5):
    """
    .. todo::

        WRITEME
    """
    return _rescale_softmax(T.nnet.softmax(x), min_val=min_val)

########NEW FILE########
__FILENAME__ = basic
"""
Very simple and basic mathematical expressions used often throughout the library.
"""
__authors__ = "Ian Goodfellow and Razvan Pascanu"
__copyright__ = "Copyright 2013, Universite de Montreal"
__credits__ = ["Ian Goodfellow and Razvan Pascanu"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

import numpy as np
import theano.tensor as T

from pylearn2.blocks import Block
from pylearn2.utils import as_floatX, constantX


def numpy_norms(W):
    """
    .. todo::

        WRITEME properly

    returns a vector containing the L2 norm of each
    column of W, where W and the return value are
    numpy ndarrays
    """
    return np.sqrt(1e-8+np.square(W).sum(axis=0))


def theano_norms(W):
    """
    .. todo::

        WRITEME properly

    returns a vector containing the L2 norm of each
    column of W, where W and the return value are symbolic
    theano variables
    """
    return T.sqrt(as_floatX(1e-8)+T.sqr(W).sum(axis=0))


def full_min(var):
    """
    .. todo::

        WRITEME properly

    returns a symbolic expression for the value of the minimal
    element of symbolic tensor. T.min does something else as of
    the time of this writing.
    """
    return var.min(axis=range(0,len(var.type.broadcastable)))


def full_max(var):
    """
    .. todo::

        WRITEME properly

    returns a symbolic expression for the value of the maximal
    element of a symbolic tensor. T.max does something else as of the
    time of this writing.
    """
    return var.max(axis=range(0,len(var.type.broadcastable)))


def multiple_switch(*args):
    """
    .. todo::

        WRITEME properly

    Applies a cascade of ifelse. The output will be a Theano expression
    which evaluates:

    .. code-block:: none

        if args0:
            then arg1
        elif arg2:
            then arg3
        elif arg4:
            then arg5
        ....
    """
    if len(args) == 3:
        return T.switch(*args)
    else:
        return T.switch(args[0],
                         args[1],
                         multiple_switch(*args[2:]))


def symGivens2(a, b):
    """
    Stable Symmetric Givens rotation plus reflection

    Parameters
    ----------
    a : theano scalar
        first element of a two-vector  [a; b]
    b : theano scalar
        second element of a two-vector [a; b]

    Returns
    -------
    c : WRITEME
        cosine(theta), where theta is the implicit angle of
        rotation (counter-clockwise) in a plane-rotation
    s : WRITEME
        sine(theta)
    d : WRITEME
        two-norm of [a; b]

    Notes
    -----
    * See also:

      - Algorithm 4.9, stable *unsymmetric* Givens rotations in Golub
        and van Loan's book Matrix Computations, 3rd edition.

      - MATLAB's function PLANEROT.

    * This method gives c and s such that

      .. math::

          \\begin{pmatrix} c & s \\\ s & -c \\end{pmatrix}
          \\begin{pmatrix} a \\\ b \\end{pmatrix} =
          \\begin{pmatrix} d \\\ 0 \\end{pmatrix}

      where

      :math:`d = \\left\Vert \\begin{pmatrix} a \\\ b \\end{pmatrix}
      \\right\Vert _{2}`,
      :math:`c = a / \sqrt{a^2 + b^2} = a / d`,
      :math:`s = b / \sqrt{a^2 + b^2} = b / d`.

      The implementation guards against overflow in computing
      :math:`\sqrt{a^2 + b^2}`.

    * Observation: Implementing this function as a single op in C might
      improve speed considerably .
    """
    c_branch1 = T.switch(T.eq(a, constantX(0)),
                          constantX(1),
                          T.sgn(a))
    c_branch21 = (a / b) * T.sgn(b) / \
            T.sqrt(constantX(1) + (a / b) ** 2)
    c_branch22 = T.sgn(a) / T.sqrt(constantX(1) + (b / a) ** 2)

    c_branch2 = T.switch(T.eq(a, constantX(0)),
                          constantX(0),
                          T.switch(T.gt(abs(b), abs(a)),
                                    c_branch21,
                                    c_branch22))
    c = T.switch(T.eq(b, constantX(0)),
                  c_branch1,
                  c_branch2)

    s_branch1 = T.sgn(b) / T.sqrt(constantX(1) + (a / b) ** 2)
    s_branch2 = (b / a) * T.sgn(a) / T.sqrt(constantX(1) + (b / a) ** 2)
    s = T.switch(T.eq(b, constantX(0)),
                  constantX(0),
                  T.switch(T.eq(a, constantX(0)),
                            T.sgn(b),
                            T.switch(T.gt(abs(b), abs(a)),
                                      s_branch1,
                                      s_branch2)))

    d_branch1 = b / (T.sgn(b) / T.sqrt(constantX(1) + (a / b) ** 2))
    d_branch2 = a / (T.sgn(a) / T.sqrt(constantX(1) + (b / a) ** 2))
    d = T.switch(T.eq(b, constantX(0)),
                  abs(a),
                  T.switch(T.eq(a, constantX(0)),
                            abs(b),
                            T.switch(T.gt(abs(b), abs(a)),
                                      d_branch1,
                                      d_branch2)))
    return c, s, d


def sqrt_inner_product(xs, ys=None):
    """
    .. todo::

        WRITEME properly

    Compute the square root of the inner product between `xs` and `ys`.
    If `ys` is not provided, computes the norm between `xs` and `xs`.
    Since `xs` and `ys` are list of tensor, think of it as the norm
    between the vector obtain by concatenating and flattening all
    tenors in `xs` and the similar vector obtain from `ys`. Note that
    `ys` should match `xs`.

    Parameters
    ----------
    xs : list of theano expressions
        WRITEME
    ys : None or list of theano expressions, optional
        WRITEME
    """
    if ys is None:
        ys = [x for x in xs]
    return T.sqrt(sum((x * y).sum() for x, y in zip(xs, ys)))


def inner_product(xs, ys=None):
    """
    .. todo::

        WRITEME properly

    Compute the inner product between `xs` and `ys`. If ys is not provided,
    computes the square norm between `xs` and `xs`.
    Since `xs` and `ys` are list of tensor, think of it as the inner
    product between the vector obtain by concatenating and flattening all
    tenors in `xs` and the similar vector obtain from `ys`. Note that
    `ys` should match `xs`.

    Parameters
    ----------
    xs : list of theano expressions
        WRITEME
    ys : None or list of theano expressions, optional
        WRITEME
    """
    if ys is None:
        ys = [x for x in xs]
    return sum((x * y).sum() for x, y in zip(xs, ys))


def is_binary(x):
    """
    .. todo::

        WRITEME
    """
    return np.all( (x == 0) + (x == 1))


class Identity(Block):
    """
    A Block that computes the identity transformation. Mostly useful as
    a placeholder.

    Parameters
    ----------
    input_space : WRITEME
    """

    def __init__(self, input_space=None):
        super(Identity, self).__init__()
        self.input_space = input_space

    def __call__(self, inputs):
        """
        .. todo::

            WRITEME
        """
        if self.input_space:
            self.input_space.validate(inputs)
        return inputs

    def set_input_space(self, space):
        """
        .. todo::

            WRITEME
        """
        self.input_space = space

    def get_input_space(self):
        """
        .. todo::

            WRITEME
        """
        if self.input_space is not None:
            return self.input_space
        raise ValueError("No input space was specified for this Block (%s). "
                "You can call set_input_space to correct that." % str(self))

    def get_output_space(self):
        """
        .. todo::

            WRITEME
        """
        return self.get_input_space()

########NEW FILE########
__FILENAME__ = coding
""" Expressions for encoding features """

import theano.tensor as T
from theano.printing import Print


def triangle_code(X, centroids):
    """
    .. todo::

        WRITEME properly

    Compute the triangle activation function used in Adam Coates' AISTATS 2011
    paper

    Parameters
    ----------
    X : WRITEME
        design matrix
    centroids : WRITEME
        k-means dictionary, one centroid in each row

    Returns
    -------
    WRITEME
        A design matrix of triangle code activations
    """

    X_sqr = T.sqr(X).sum(axis=1).dimshuffle(0,'x')
    c_sqr = T.sqr(centroids).sum(axis=1).dimshuffle('x',0)
    c_sqr.name = 'c_sqr'
    Xc = T.dot(X, centroids.T)
    Xc.name = 'Xc'

    sq_dists =  c_sqr + X_sqr - 2. * Xc

    #TODO: why do I have to do this and Adam doesn't?
    #is it just because he uses float64 and I usually use
    #float32? or are our libraries numerically unstable somehow,
    #or does matlab handle sqrt differently?
    sq_dists_safe = T.clip(sq_dists,0.,1e30)

    Z = T.sqrt( sq_dists_safe)
    Z.name = 'Z'

    mu = Z.mean(axis=1)
    mu.name = 'mu'

    mu = mu.dimshuffle(0,'x')
    mu.name = 'mu_broadcasted'

    rval = T.clip( mu - Z, 0., 1e30)
    rval.name = 'triangle_code'

    return rval



########NEW FILE########
__FILENAME__ = image
""" Mathematical expressions related to image processing. """


def color_to_gray(color):
    """
    .. todo::

        WRITEME properly

    Standard conversion from color to luma

    Y' = W_R * red_channel + W_G * green_channel + W_B * blue_channel

    with
    W_R = 0.299
    W_G = 0.587
    W_B = 0.114

    Parameters
    ----------
    color : numpy or theano 4-tensor
        The channel index must be last

    Returns
    -------
    tensor
        Has the same number of dimensions, but with the final dimension
        changed to 1.

    References
    ----------
    http://en.wikipedia.org/wiki/YUV#Conversion_to.2Ffrom_RGB
    """

    W_R = 0.299
    W_B = 0.114
    W_G = 0.587

    red_channel = color[:,:,:,0:1]
    blue_channel = color[:,:,:,2:3]
    green_channel = color[:,:,:,1:2]

    Y_prime = W_R * red_channel + W_G * green_channel + W_B * blue_channel

    return Y_prime


########NEW FILE########
__FILENAME__ = information_theory
"""
.. todo::

    WRITEME
"""
import theano.tensor as T
from theano.gof.op import get_debug_values
from theano.gof.op import debug_assert
import numpy as np
from theano.tensor.xlogx import xlogx

def entropy_binary_vector(P):
    """
    .. todo::

        WRITEME properly

    If P[i,j] represents the probability of some binary random variable X[i,j]
    being 1, then rval[i] gives the entropy of the random vector X[i,:]
    """

    for Pv in get_debug_values(P):
        assert Pv.min() >= 0.0
        assert Pv.max() <= 1.0

    oneMinusP = 1.-P

    PlogP = xlogx(P)
    omPlogOmP = xlogx(oneMinusP)

    term1 = - T.sum( PlogP , axis=1)
    assert len(term1.type.broadcastable) == 1

    term2 = - T.sum( omPlogOmP , axis =1 )
    assert len(term2.type.broadcastable) == 1

    rval = term1 + term2

    for plp, olo, t1, t2, rv in get_debug_values(PlogP, omPlogOmP, term1, term2, rval):
        debug_assert(not np.any(np.isnan(plp)))
        debug_assert(not np.any(np.isinf(olo)))
        debug_assert(not np.any(np.isnan(plp)))
        debug_assert(not np.any(np.isinf(olo)))

        debug_assert(not np.any(np.isnan(t1)))
        debug_assert(not np.any(np.isnan(t2)))
        debug_assert(not np.any(np.isnan(rv)))

    return rval

########NEW FILE########
__FILENAME__ = nnet
"""
Useful expressions common to many neural network applications.
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2013, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

import numpy as np
import theano
from theano.printing import Print
from theano import tensor as T


def softmax_numpy(x):
    """
    .. todo::

        WRITEME properly

    Parameters
    ----------
    x : matrix

    Returns
    -------
    rval : vector
        rval[i] is the softmax of row i of x
    """
    stable_x = (x.T - x.max(axis=1)).T
    numer = np.exp(stable_x)
    return (numer.T / numer.sum(axis=1)).T

def pseudoinverse_softmax_numpy(x):
    """
    .. todo::

        WRITEME properly

    Parameters
    ----------
    x : vector

    Returns
    -------
    y : vector
        softmax(y) = x

    Notes
    -----
    This problem is underdetermined, so we also impose y.mean() = 0
    """
    rval = np.log(x)
    rval -= rval.mean()
    return rval

def sigmoid_numpy(x):
    """
    .. todo::

        WRITEME
    """
    assert not isinstance(x, theano.gof.Variable)
    return 1. / (1. + np.exp(-x))

def inverse_sigmoid_numpy(x):
    """
    .. todo::

        WRITEME
    """
    return np.log(x / (1. - x))

def arg_of_softmax(Y_hat):
    """
    .. todo::

        WRITEME properly

    Parameters
    ----------
    Y_hat : Variable
        softmax(Z)

    Returns
    -------
    Z : Variable
        The variable that was passed to the Softmax op to create `Y_hat`.
        Raises an error if `Y_hat` is not actually the output of a
        Softmax.
    """
    assert hasattr(Y_hat, 'owner')
    owner = Y_hat.owner
    assert owner is not None
    op = owner.op
    if isinstance(op, Print):
        assert len(owner.inputs) == 1
        Y_hat, = owner.inputs
        owner = Y_hat.owner
        op = owner.op
    if not isinstance(op, T.nnet.Softmax):
        raise ValueError("Expected Y_hat to be the output of a softmax, "
                "but it appears to be the output of " + str(op) + " of type "
                + str(type(op)))
    z ,= owner.inputs
    assert z.ndim == 2
    return z

def kl(Y, Y_hat, batch_axis):
    """
    Warning: This function expects a sigmoid nonlinearity in the
    output layer. Returns a batch (vector) of mean across units of
    KL divergence for each example,
    KL(P || Q) where P is defined by Y and Q is defined by Y_hat:

    p log p - p log q + (1-p) log (1-p) - (1-p) log (1-q)
    For binary p, some terms drop out:
    - p log q - (1-p) log (1-q)
    - p log sigmoid(z) - (1-p) log sigmoid(-z)
    p softplus(-z) + (1-p) softplus(z)

    Parameters
    ----------
    Y : Variable
        targets for the sigmoid outputs. Currently Y must be purely binary.
        If it's not, you'll still get the right gradient, but the
        value in the monitoring channel will be wrong.
    Y_hat : Variable
        predictions made by the sigmoid layer. Y_hat must be generated by
        fprop, i.e., it must be a symbolic sigmoid.
    batch_axis : list
        list of axes to compute average kl divergence across.

    Returns
    -------
    ave : Variable
        average kl divergence between Y and Y_hat.
    """
    assert hasattr(Y_hat, 'owner')
    assert batch_axis is not None

    owner = Y_hat.owner
    assert owner is not None
    op = owner.op

    if not hasattr(op, 'scalar_op'):
        raise ValueError("Expected Y_hat to be generated by an Elemwise "
                         "op, got "+str(op)+" of type "+str(type(op)))
    assert isinstance(op.scalar_op, T.nnet.sigm.ScalarSigmoid)
    z, = owner.inputs

    term_1 = Y * T.nnet.softplus(-z)
    term_2 = (1 - Y) * T.nnet.softplus(z)

    total = term_1 + term_2
    naxes = total.ndim
    axes_to_reduce = range(naxes)
    del axes_to_reduce[batch_axis]
    ave = total.mean(axis=axes_to_reduce)

    return ave


def softmax_ratio(numer, denom):
    """
    .. todo::

        WRITEME properly

    Parameters
    ----------
    numer : Variable
        Output of a softmax.
    denom : Variable
        Output of a softmax.

    Returns
    -------
    ratio : Variable
        numer / denom, computed in a numerically stable way
    """

    numer_Z = arg_of_softmax(numer)
    denom_Z = arg_of_softmax(denom)
    numer_Z -= numer_Z.max(axis=1).dimshuffle(0, 'x')
    denom_Z -= denom_Z.min(axis=1).dimshuffle(0, 'x')

    new_num = T.exp(numer_Z - denom_Z) * (T.exp(denom_Z).sum(
        axis=1).dimshuffle(0, 'x'))
    new_den = (T.exp(numer_Z).sum(axis=1).dimshuffle(0, 'x'))

    return new_num / new_den

def compute_precision(tp, fp):
    """
    Computes the precision for the binary decisions.
    Computed as tp/(tp + fp).

    Parameters
    ----------
    tp : Variable
        True positives.
    fp : Variable
        False positives.

    Returns
    -------
    precision : Variable
        Precision of the binary classifications.
    """
    precision = tp / T.maximum(1., tp + fp)
    return precision

def compute_recall(y, tp):
    """
    Computes the recall for the binary classification.

    Parameters
    ----------
    y : Variable
        Targets for the binary classifications.
    tp : Variable
        True positives.

    Returns
    -------
    recall : Variable
        Recall for the binary classification.
    """
    recall = tp / T.maximum(1., y.sum())
    return recall

def compute_f1(precision, recall):
    """
    Computes the f1 score for the binary classification.
    Computed as,

    f1 = 2 * precision * recall / (precision + recall)

    Parameters
    ----------
    precision : Variable
        Precision score of the binary decisions.
    recall : Variable
        Recall score of the binary decisions.

    Returns
    -------
    f1 : Variable
        f1 score for the binary decisions.
    """
    f1 = (2. * precision * recall /
            T.maximum(1, precision + recall))
    return f1


########NEW FILE########
__FILENAME__ = normalize
"""
Code for normalizing outputs of MLP / convnet layers.
"""
__authors__ = "Ian Goodfellow and David Warde-Farley"
__copyright__ = "Copyright 2013, Universite de Montreal"
__credits__ = ["Ian Goodfellow and David Warde-Farley"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

import theano.tensor as T

from pylearn2.sandbox.cuda_convnet.response_norm import CrossMapNorm

class CrossChannelNormalizationBC01(object):
    """
    BC01 version of CrossChannelNormalization

    Parameters
    ----------
    alpha : WRITEME
    k : WRITEME
    beta : WRITEME
    n : WRITEME
    """

    def __init__(self, alpha = 1e-4, k=2, beta=0.75, n=5):
        self.__dict__.update(locals())
        del self.self

        if n % 2 == 0:
            raise NotImplementedError("Only works with odd n for now")

    def __call__(self, bc01):
        """
        .. todo::

            WRITEME
        """
        half = self.n // 2

        sq = T.sqr(bc01)

        b, ch, r, c = bc01.shape

        extra_channels = T.alloc(0., b, ch + 2*half, r, c)

        sq = T.set_subtensor(extra_channels[:,half:half+ch,:,:], sq)

        scale = self.k

        for i in xrange(self.n):
            scale += self.alpha * sq[:,i:i+ch,:,:]

        scale = scale ** self.beta

        return bc01 / scale

class CrossChannelNormalization(object):
    """
    See "ImageNet Classification with Deep Convolutional Neural Networks"
    Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton
    NIPS 2012

    Section 3.3, Local Response Normalization

    .. todo::

        WRITEME properly

    f(c01b)_[i,j,k,l] = c01b[i,j,k,l] / scale[i,j,k,l]

    scale[i,j,k,l] = (k + sqr(c01b)[clip(i-n/2):clip(i+n/2),j,k,l].sum())^beta

    clip(i) = T.clip(i, 0, c01b.shape[0]-1)

    Parameters
    ----------
    alpha : WRITEME
    k : WRITEME
    beta : WRITEME
    n : WRITEME
    """

    def __init__(self, alpha = 1e-4, k=2, beta=0.75, n=5):
        self.__dict__.update(locals())
        del self.self

        if n % 2 == 0:
            raise NotImplementedError("Only works with odd n for now")

    def __call__(self, c01b):
        """
        .. todo::

            WRITEME
        """
        half = self.n // 2

        sq = T.sqr(c01b)

        ch, r, c, b = c01b.shape

        extra_channels = T.alloc(0., ch + 2*half, r, c, b)

        sq = T.set_subtensor(extra_channels[half:half+ch,:,:,:], sq)

        scale = self.k

        for i in xrange(self.n):
            scale += self.alpha * sq[i:i+ch,:,:,:]

        scale = scale ** self.beta

        return c01b / scale

class CudaConvNetCrossChannelNormalization(object):
    """
    .. todo::

        WRITEME properly

    I kept the same parameter names where I was sure they
    actually are the same parameters (with respect to
    CrossChannelNormalization).

    Parameters
    ----------
    alpha : WRITEME
    beta : WRITEME
    size_f : WRITEME
    blocked : WRITEME
    """
    def __init__(self, alpha=1e-4, beta=0.75, size_f=5, blocked=True):
        self._op = CrossMapNorm(size_f=size_f, add_scale=alpha,
                                pow_scale=beta, blocked=blocked)

    def __call__(self, c01b):
        """
        .. todo::

            WRITEME properly

        NOTE: c01b must be CudaNdarrayType."""
        return self._op(c01b)[0]

########NEW FILE########
__FILENAME__ = preprocessing
"""
Low-level utilities for preprocessing. Should be functions that apply
to NumPy arrays, not preprocessor classes (though preprocessor classes
should reuse these).
"""
__author__ = "David Warde-Farley"
__copyright__ = "Copyright 2012, Universite de Montreal"
__credits__ = ["David Warde-Farley"]
__license__ = "3-clause BSD"
__email__ = "wardefar@iro"
__maintainer__ = "David Warde-Farley"

import numpy


def global_contrast_normalize(X, scale=1., subtract_mean=True, use_std=False,
                              sqrt_bias=0., min_divisor=1e-8):
    """
    Global contrast normalizes by (optionally) subtracting the mean
    across features and then normalizes by either the vector norm
    or the standard deviation (across features, for each example).

    Parameters
    ----------
    X : ndarray, 2-dimensional
        Design matrix with examples indexed on the first axis and \
        features indexed on the second.

    scale : float, optional
        Multiply features by this const.

    subtract_mean : bool, optional
        Remove the mean across features/pixels before normalizing. \
        Defaults to `True`.

    use_std : bool, optional
        Normalize by the per-example standard deviation across features \
        instead of the vector norm. Defaults to `False`.

    sqrt_bias : float, optional
        Fudge factor added inside the square root. Defaults to 0.

    min_divisor : float, optional
        If the divisor for an example is less than this value, \
        do not apply it. Defaults to `1e-8`.

    Returns
    -------
    Xp : ndarray, 2-dimensional
        The contrast-normalized features.

    Notes
    -----
    `sqrt_bias` = 10 and `use_std = True` (and defaults for all other
    parameters) corresponds to the preprocessing used in [1].

    References
    ----------
    .. [1] A. Coates, H. Lee and A. Ng. "An Analysis of Single-Layer
       Networks in Unsupervised Feature Learning". AISTATS 14, 2011.
       http://www.stanford.edu/~acoates/papers/coatesleeng_aistats_2011.pdf
    """
    assert X.ndim == 2, "X.ndim must be 2"
    scale = float(scale)
    assert scale >= min_divisor

    # Note: this is per-example mean across pixels, not the
    # per-pixel mean across examples. So it is perfectly fine
    # to subtract this without worrying about whether the current
    # object is the train, valid, or test set.
    mean = X.mean(axis=1)
    if subtract_mean:
        X = X - mean[:, numpy.newaxis]  # Makes a copy.
    else:
        X = X.copy()

    if use_std:
        # ddof=1 simulates MATLAB's var() behaviour, which is what Adam
        # Coates' code does.
        ddof = 1

        # If we don't do this, X.var will return nan.
        if X.shape[1] == 1:
            ddof = 0

        normalizers = numpy.sqrt(sqrt_bias + X.var(axis=1, ddof=ddof)) / scale
    else:
        normalizers = numpy.sqrt(sqrt_bias + (X ** 2).sum(axis=1)) / scale

    # Don't normalize by anything too small.
    normalizers[normalizers < min_divisor] = 1.

    X /= normalizers[:, numpy.newaxis]  # Does not make a copy.
    return X

########NEW FILE########
__FILENAME__ = probabilistic_max_pooling
"""
An implementation of probabilistic max-pooling, based on

"Convolutional Deep Belief Networks for Scalable
Unsupervised Learning of Hierarchical Representations"
Honglak Lee, Roger Grosse, Rajesh Ranganath, and Andrew Y. Ng
ICML 2009


This paper defines probabilistic max-pooling in the context
of a Convolutional Deep Belief Network (its energy function is
more like a DBM than a DBN but it is trained like a DBN). Here
we define probabilistic max pooling as a general layer for
use in an energy-based model regardless of how the rest of the
model is assembled.
"""

__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

import logging
import theano.tensor as T
import numpy as np
from theano import config
from theano import function
import time
from pylearn2.utils import sharedX
from theano.sandbox.rng_mrg import MRG_RandomStreams
from theano.gof.op import get_debug_values

logger = logging.getLogger(__name__)


def max_pool(z, pool_shape, top_down=None, theano_rng=None):
    """
    Probabilistic max-pooling

    Parameters
    ----------
    z : theano 4-tensor
        a theano 4-tensor representing input from below
    pool_shape : tuple
        tuple of ints. the shape of regions to be pooled
    top_down : theano 4-tensor, optional
        a theano 4-tensor representing input from above
        if None, assumes top-down input is 0
    theano_rng : MRG_RandomStreams, optional
        Used for random numbers for sampling

    Returns
    -------
    p : theano 4-tensor
        the expected value of the pooling layer p
    h : theano 4-tensor
        the expected value of the detector layer h
    p_samples : theano 4-tensor, only returned if theano_rng is not None
        samples of the pooling layer
    h_samples : theano 4-tensor, only returned if theano_rng is not None
        samples of the detector layer

    Notes
    ------

    all 4-tensors are formatted with axes ('b', 'c', 0, 1).
    This is for maximum speed when using theano's conv2d
    to generate z and top_down, or when using it to infer conditionals of
    other layers using the return values.

    Detailed description:

    Suppose you have a variable h that lives in a Conv2DSpace h_space and
    you want to pool it down to a variable p that lives in a smaller
    Conv2DSpace p.

    This function does that, using non-overlapping pools.

    Specifically, consider one channel of h. h must have a height that is a
    multiple of pool_shape[0] and a width that is a multiple of pool_shape[1].
    A channel of h can thus be broken down into non-overlapping rectangles
    of shape pool_shape.

    Now consider one rectangular pooled region within one channel of h.
    I now use 'h' to refer just to this rectangle, and 'p' to refer to
    just the one pooling unit associated with that rectangle.
    We assume that the space that h and p live in is constrained such
    that h and p are both binary and p = max(h). To reduce the state-space
    in order to make probabilistic computations cheaper we also
    constrain sum(h) <= 1.
    Suppose h contains k different units. Suppose that the only term
    in the model's energy function involving h is -(z*h).sum()
    (elemwise multiplication) and the only term in
    the model's energy function involving p is -(top_down*p).sum().

    Then P(h[i] = 1) = softmax( [ z[1], z[2], ..., z[k], -top_down] )[i]
    and P(p = 1) = 1-softmax( [z[1], z[2], ..., z[k], -top_down])[k]

    This variation of the function assumes that z, top_down, and all
    return values use Conv2D axes ('b', 'c', 0, 1).
    This variation of the function implements the softmax using a
    theano graph of exp, maximum, sub, and div operations.

    Performance notes:
    It might be possible to make a faster implementation with different
    theano ops. rather than using set_subtensor, it might be possible
    to use the stuff in theano.sandbox.neighbours. Probably not possible,
    or at least nasty, because that code isn't written with multiple
    channels in mind, and I don't think just a reshape can fix it.
    Some work on this in galatea.cond.neighbs.py
    At some point images2neighbs' gradient was broken so check that
    it has been fixed before sinking too much time into this.

    Stabilizing the softmax is also another source of slowness.
    Here it is stabilized with several calls to maximum and sub.
    It might also be possible to stabilize it with
    T.maximum(-top_down,T.signal.downsample.max_pool(z)).
    Don't know if that would be faster or slower.

    Elsewhere in this file I implemented the softmax with a reshape
    and call to Softmax / SoftmaxWithBias.
    This is slower, even though Softmax is faster on the GPU than the
    equivalent max/sub/exp/div graph. Maybe the reshape is too expensive.

    Benchmarks show that most of the time is spent in GpuIncSubtensor
    when running on gpu. So it is mostly that which needs a faster
    implementation. One other way to implement this would be with
    a linear.Conv2D.lmul_T, where the convolution stride is equal to
    the pool width, and the thing to multiply with is the hparts stacked
    along the channel axis. Unfortunately, conv2D doesn't work right
    with stride > 2 and is pretty slow for stride 2. Conv3D is used to
    mitigate some of this, but only has CPU code.
    """

    z_name = z.name
    if z_name is None:
        z_name = 'anon_z'

    batch_size, ch, zr, zc = z.shape

    r, c = pool_shape

    zpart = []

    mx = None

    if top_down is None:
        t = 0.
    else:
        t = - top_down
        t.name = 'neg_top_down'

    for i in xrange(r):
        zpart.append([])
        for j in xrange(c):
            cur_part = z[:, :, i:zr:r, j:zc:c]
            if z_name is not None:
                cur_part.name = z_name + '[%d,%d]' % (i, j)
            zpart[i].append(cur_part)
            if mx is None:
                mx = T.maximum(t, cur_part)
                if cur_part.name is not None:
                    mx.name = 'max(-top_down,' + cur_part.name + ')'
            else:
                max_name = None
                if cur_part.name is not None:
                    mx_name = 'max(' + cur_part.name + ',' + mx.name + ')'
                mx = T.maximum(mx, cur_part)
                mx.name = mx_name
    mx.name = 'local_max(' + z_name + ')'

    pt = []

    for i in xrange(r):
        pt.append([])
        for j in xrange(c):
            z_ij = zpart[i][j]
            safe = z_ij - mx
            safe.name = 'safe_z(%s)' % z_ij.name
            cur_pt = T.exp(safe)
            cur_pt.name = 'pt(%s)' % z_ij.name
            pt[-1].append(cur_pt)

    off_pt = T.exp(t - mx)
    off_pt.name = 'p_tilde_off(%s)' % z_name
    denom = off_pt

    for i in xrange(r):
        for j in xrange(c):
            denom = denom + pt[i][j]
    denom.name = 'denom(%s)' % z_name

    off_prob = off_pt / denom
    p = 1. - off_prob
    p.name = 'p(%s)' % z_name

    hpart = []
    for i in xrange(r):
        hpart.append([pt_ij / denom for pt_ij in pt[i]])

    h = T.alloc(0., batch_size, ch, zr, zc)

    for i in xrange(r):
        for j in xrange(c):
            h.name = 'h_interm'
            h = T.set_subtensor(h[:, :, i:zr:r, j:zc:c], hpart[i][j])

    h.name = 'h(%s)' % z_name

    if theano_rng is None:
        return p, h
    else:
        events = []
        for i in xrange(r):
            for j in xrange(c):
                events.append(hpart[i][j])
        events.append(off_prob)

        events = [event.dimshuffle(0, 1, 2, 3, 'x') for event in events]

        events = tuple(events)

        stacked_events = T.concatenate(events, axis=4)

        rows = zr // pool_shape[0]
        cols = zc // pool_shape[1]
        outcomes = pool_shape[0] * pool_shape[1] + 1
        assert stacked_events.ndim == 5
        for se, bs, r, c, chv in get_debug_values(stacked_events, batch_size,
                                                  rows, cols, ch):
            assert se.shape[0] == bs
            assert se.shape[1] == r
            assert se.shape[2] == c
            assert se.shape[3] == chv
            assert se.shape[4] == outcomes
        reshaped_events = stacked_events.reshape((
            batch_size * rows * cols * ch, outcomes))

        multinomial = theano_rng.multinomial(pvals=reshaped_events,
                                             dtype=p.dtype)

        reshaped_multinomial = multinomial.reshape((batch_size, ch, rows,
                                                    cols, outcomes))

        h_sample = T.alloc(0., batch_size, ch, zr, zc)

        idx = 0
        for i in xrange(r):
            for j in xrange(c):
                h_sample = T.set_subtensor(h_sample[:, :, i:zr:r, j:zc:c],
                                           reshaped_multinomial[:, :, :, :,
                                           idx])
                idx += 1

        p_sample = 1 - reshaped_multinomial[:, :, :, :, -1]

        return p, h, p_sample, h_sample


def max_pool_c01b(z, pool_shape, top_down=None, theano_rng=None):
    """
    .. todo::

        WRITEME properly

    Like max_pool but with all 4-tensors formatted with axes ('c', 0, 1, 'b').
    This is for maximum speed when using-cuda convnet.

    Notes
    -----
    Performance notes:
    Stabilizing the softmax is one source of slowness. Here it is stabilized
    with several calls to maximum and sub. It might also be possible to
    stabilize it with T.maximum(-top_down,<cuda convnet max pooling>).
    Don't know if that would be faster or slower.

    Benchmarks show that most of the time is spent in GpuIncSubtensor
    when running on gpu. So it is mostly that which needs a faster
    implementation. One other way to implement this would be with cuda
    convnet convolution, where the convolution stride is equal to the
    pool width, and the thing to multiply with is the hparts stacked
    along the channel axis. This isn't a feasible solution for max_pool
    because of theano convolution's poor support for strides, but for cuda
    convnet it could give a speedup.
    """

    z_name = z.name
    if z_name is None:
        z_name = 'anon_z'

    ch, zr, zc, batch_size = z.shape

    r, c = pool_shape

    zpart = []

    mx = None

    if top_down is None:
        t = 0.
    else:
        t = - top_down
        t.name = 'neg_top_down'

    for i in xrange(r):
        zpart.append([])
        for j in xrange(c):
            cur_part = z[:, i:zr:r, j:zc:c, :]
            if z_name is not None:
                cur_part.name = z_name + '[%d, %d]' % (i, j)
            zpart[i].append(cur_part)
            if mx is None:
                mx = T.maximum(t, cur_part)
                if cur_part.name is not None:
                    mx.name = 'max(-top_down,' + cur_part.name + ')'
            else:
                max_name = None
                if cur_part.name is not None:
                    mx_name = 'max(' + cur_part.name + ',' + mx.name + ')'
                mx = T.maximum(mx, cur_part)
                mx.name = mx_name
    mx.name = 'local_max(' + z_name + ')'

    pt = []

    for i in xrange(r):
        pt.append([])
        for j in xrange(c):
            z_ij = zpart[i][j]
            safe = z_ij - mx
            safe.name = 'safe_z(%s)' % z_ij.name
            cur_pt = T.exp(safe)
            cur_pt.name = 'pt(%s)' % z_ij.name
            pt[-1].append(cur_pt)

    off_pt = T.exp(t - mx)
    off_pt.name = 'p_tilde_off(%s)' % z_name
    denom = off_pt

    for i in xrange(r):
        for j in xrange(c):
            denom = denom + pt[i][j]
    denom.name = 'denom(%s)' % z_name

    off_prob = off_pt / denom
    p = 1. - off_prob
    p.name = 'p(%s)' % z_name

    hpart = []
    for i in xrange(r):
        hpart.append([pt_ij / denom for pt_ij in pt[i]])

    h = T.alloc(0., ch, zr, zc, batch_size)

    for i in xrange(r):
        for j in xrange(c):
            h.name = 'h_interm'
            h = T.set_subtensor(h[:, i:zr:r, j:zc:c, :], hpart[i][j])

    h.name = 'h(%s)' % z_name

    if theano_rng is None:
        return p, h
    else:
        events = []
        for i in xrange(r):
            for j in xrange(c):
                events.append(hpart[i][j])
        events.append(off_prob)

        events = [event.dimshuffle(0, 1, 2, 3, 'x') for event in events]

        events = tuple(events)

        stacked_events = T.concatenate(events, axis=4)

        ch, rows, cols, batch_size, outcomes = stacked_events.shape
        reshaped_events = stacked_events.reshape((ch * rows * cols *
                                                  batch_size, outcomes))

        multinomial = theano_rng.multinomial(pvals=reshaped_events,
                                             dtype=p.dtype)

        reshaped_multinomial = multinomial.reshape((ch, rows, cols, batch_size,
                                                    outcomes))

        h_sample = T.alloc(0., ch, zr, zc, batch_size)

        idx = 0
        for i in xrange(r):
            for j in xrange(c):
                h_sample = T.set_subtensor(h_sample[:, i:zr:r, j:zc:c, :],
                                           reshaped_multinomial[:, :, :, :,
                                           idx])
                idx += 1

        p_sample = 1 - reshaped_multinomial[:, :, :, :, -1]

        return p, h, p_sample, h_sample


def max_pool_channels(z, pool_size, top_down=None, theano_rng=None):
    """
    Unlike Honglak's convolutional max pooling, which pools over spatial
    locations within each channels, this does max pooling in a densely
    connected model. Here we pool groups of channels together.

    Parameters
    ----------
    z : theano matrix
        representings a batch of input from below
    pool_size : int
        the number of features to combine into one pooled unit
    top_down : theano matrix, optional
        a theano matrix representing input from above
        if None, assumes top-down input is 0
    theano_rng : MRG_RandomStreams, optional
        For random numbers for sampling

    Returns
    -------
    h : theano matrix
        a theano matrix for the expected value of the detector layer h
    p : theano matrix
        a theano matrix for the expected value of the pooling layer p
    h_samples : theano matrix, only returned if theano_rng is not None
        a theano matrix of samples of the detector layer
    p_samples: theano matrix, only returned if theano_rng is not None
        a theano matrix of samples of the pooling layer

    Notes
    -----
    All matrices are formatted as (num_example, num_features)
    """

    z_name = z.name
    if z_name is None:
        z_name = 'anon_z'

    if pool_size == 1:
        if top_down is None:
            top_down = 0.
        total_input = z + top_down
        p = T.nnet.sigmoid(total_input)
        h = p

        if theano_rng is None:
            return p, h
        else:
            p_samples = theano_rng.binomial(p=p, size=p.shape,
                                            dtype=p.dtype, n=1)
            h_samples = p_samples
            return p_samples, h_samples, p_samples, h_samples
    else:
        batch_size, n = z.shape

        mx = None

        if top_down is None:
            t = 0.
        else:
            t = - top_down
            t.name = 'neg_top_down'

        zpart = []
        for i in xrange(pool_size):
            cur_part = z[:, i:n:pool_size]
            if z_name is not None:
                cur_part.name = z_name + '[%d]' % (i)
            zpart.append(cur_part)
            if mx is None:
                mx = T.maximum(t, cur_part)
                if cur_part.name is not None:
                    mx.name = 'max(-top_down,' + cur_part.name + ')'
            else:
                max_name = None
                if cur_part.name is not None:
                    mx_name = 'max(' + cur_part.name + ',' + mx.name + ')'
                mx = T.maximum(mx, cur_part)
                mx.name = mx_name
        mx.name = 'local_max(' + z_name + ')'

        pt = []

        for i in xrange(pool_size):
            z_i = zpart[i]
            safe = z_i - mx
            safe.name = 'safe_z(%s)' % z_i.name
            cur_pt = T.exp(safe)
            cur_pt.name = 'pt(%s)' % z_i.name
            assert cur_pt.ndim == 2
            pt.append(cur_pt)

        off_pt = T.exp(t - mx)
        assert off_pt.ndim == 2
        off_pt.name = 'p_tilde_off(%s)' % z_name

        denom = off_pt
        for i in xrange(pool_size):
            denom = denom + pt[i]
        assert denom.ndim == 2
        denom.name = 'denom(%s)' % z_name

        off_prob = off_pt / denom
        p = 1. - off_prob
        assert p.dtype == z.dtype

        hpart = [pt_i / denom for pt_i in pt]

        h = T.alloc(0., batch_size, n)

        for i in xrange(pool_size):
            h.name = 'h_interm'
            hp = hpart[i]
            sub_h = h[:, i:n:pool_size]
            assert sub_h.ndim == 2
            assert hp.ndim == 2
            for hv, hsv, hpartv in get_debug_values(h, sub_h, hp):
                logger.info(hv.shape)
                logger.info(hsv.shape)
                logger.info(hpartv.shape)
            h = T.set_subtensor(sub_h, hp)

    p.name = 'p(%s)' % z_name
    h.name = 'h(%s)' % z_name

    if theano_rng is None:
        return p, h
    else:
        events = []
        for i in xrange(pool_size):
            events.append(hpart[i])
        events.append(off_prob)

        events = [event.dimshuffle(0, 1, 'x') for event in events]

        events = tuple(events)

        stacked_events = T.concatenate(events, axis=2)

        outcomes = pool_size + 1
        reshaped_events = stacked_events.reshape((batch_size * n // pool_size,
                                                  outcomes))

        multinomial = theano_rng.multinomial(pvals=reshaped_events,
                                             dtype=p.dtype)

        reshaped_multinomial = multinomial.reshape((batch_size,
                                                    n // pool_size,
                                                    outcomes))

        h_sample = T.zeros_like(z)

        idx = 0
        for i in xrange(pool_size):
            h_sample = T.set_subtensor(h_sample[:, i:n:pool_size],
                                       reshaped_multinomial[:, :, idx])
            idx += 1

        p_sample = 1 - reshaped_multinomial[:, :, -1]

        assert h_sample.dtype == z.dtype

        return p, h, p_sample, h_sample


def max_pool_python(z, pool_shape, top_down=None):
    """
    .. todo::

        WRITEME properly

    Slow python implementation of max_pool
    for unit tests.
    Also, this uses the ('b', 0, 1, 'c') format.
    """

    batch_size, zr, zc, ch = z.shape

    r, c = pool_shape

    assert zr % r == 0
    assert zc % c == 0

    h = np.zeros(z.shape, dtype=z.dtype)
    p = np.zeros((batch_size, zr / r, zc / c, ch), dtype=z.dtype)
    if top_down is None:
        top_down = p.copy()

    for u in xrange(0, zr, r):
        for l in xrange(0, zc, c):
            pt = np.exp(z[:, u:u+r, l:l+c, :])
            off_pt = np.exp(-top_down[:, u/r, l/c, :])
            denom = pt.sum(axis=1).sum(axis=1) + off_pt
            p[:, u/r, l/c, :] = 1. - off_pt / denom
            for i in xrange(batch_size):
                for j in xrange(ch):
                    pt[i, :, :, j] /= denom[i, j]
            h[:, u:u+r, l:l+c, :] = pt

    return p, h


def max_pool_channels_python(z, pool_size, top_down=None):
    """
    .. todo::

        WRITEME properly

    Slow python implementation of max_pool_channels
    for unit tests.
    Also, this uses the ('b', 0, 1, 'c') format.
    """

    batch_size, n = z.shape

    assert n % pool_size == 0

    h = np.zeros(z.shape, dtype=z.dtype)
    p = np.zeros((batch_size, n / pool_size), dtype=z.dtype)
    if top_down is None:
        top_down = p.copy()

    for i in xrange(0, n / pool_size):
        pt = np.exp(z[:, i*pool_size:(i+1)*pool_size])
        off_pt = np.exp(-top_down[:, i])
        denom = pt.sum(axis=1) + off_pt
        assert denom.ndim == 1
        p[:, i] = 1. - off_pt / denom
        for j in xrange(batch_size):
            for k in xrange(pool_size):
                h[j, i*pool_size+k] = pt[j, k] / denom[j]

    return p, h


def max_pool_unstable(z, pool_shape):
    """
    .. todo::

        WRITEME properly

    A version of max_pool that does not numerically stabilize the softmax.
    This is faster, but prone to both overflow and underflow in the
    intermediate computations.
    Mostly useful for benchmarking, to determine how much speedup we
    could hope to get by using a better stabilization method.
    Also, this uses the ('b', 0, 1, 'c') format.
    """

    batch_size, zr, zc, ch = z.shape

    r, c = pool_shape

    zpart = []

    for i in xrange(r):
        zpart.append([])
        for j in xrange(c):
            zpart[i].append(z[:, i:zr:r, j:zc:c, :])

    pt = []

    for i in xrange(r):
        pt.append([T.exp(z_ij) for z_ij in zpart[i]])

    denom = 1.

    for i in xrange(r):
        for j in xrange(c):
            denom = denom + pt[i][j]

    p = 1. - 1. / denom

    hpart = []
    for i in xrange(r):
        hpart.append([pt_ij / denom for pt_ij in pt[i]])

    h = T.alloc(0., batch_size, zr, zc, ch)

    for i in xrange(r):
        for j in xrange(c):
            h = T.set_subtensor(h[:, i:zr:r, j:zc:c, :], hpart[i][j])

    return p, h


def max_pool_b01c(z, pool_shape, top_down=None, theano_rng=None):
    """
    .. todo::

        WRITEME properly

    An implementation of max_pool but where all 4-tensors use the
    ('b', 0, 1, 'c') format.
    """

    z_name = z.name
    if z_name is None:
        z_name = 'anon_z'

    batch_size, zr, zc, ch = z.shape

    r, c = pool_shape

    zpart = []

    mx = None

    if top_down is None:
        t = 0.
    else:
        t = - top_down

    for i in xrange(r):
        zpart.append([])
        for j in xrange(c):
            cur_part = z[:, i:zr:r, j:zc:c, :]
            if z_name is not None:
                cur_part.name = z_name + '[%d, %d]' % (i, j)
            zpart[i].append(cur_part)
            if mx is None:
                mx = T.maximum(t, cur_part)
                if cur_part.name is not None:
                    mx.name = 'max(-top_down,' + cur_part.name + ')'
            else:
                max_name = None
                if cur_part.name is not None:
                    mx_name = 'max(' + cur_part.name + ','+mx.name + ')'
                mx = T.maximum(mx, cur_part)
                mx.name = mx_name
    mx.name = 'local_max('+z_name+')'

    pt = []

    for i in xrange(r):
        pt.append([])
        for j in xrange(c):
            z_ij = zpart[i][j]
            safe = z_ij - mx
            safe.name = 'safe_z(%s)' % z_ij.name
            cur_pt = T.exp(safe)
            cur_pt.name = 'pt(%s)' % z_ij.name
            pt[-1].append(cur_pt)

    off_pt = T.exp(t - mx)
    off_pt.name = 'p_tilde_off(%s)' % z_name
    denom = off_pt

    for i in xrange(r):
        for j in xrange(c):
            denom = denom + pt[i][j]
    denom.name = 'denom(%s)' % z_name

    off_prob = off_pt / denom
    p = 1. - off_prob
    p.name = 'p(%s)' % z_name

    hpart = []
    for i in xrange(r):
        hpart.append([pt_ij / denom for pt_ij in pt[i]])

    h = T.alloc(0., batch_size, zr, zc, ch)

    for i in xrange(r):
        for j in xrange(c):
            h = T.set_subtensor(h[:, i:zr:r, j:zc:c, :], hpart[i][j])

    h.name = 'h(%s)' % z_name

    if theano_rng is None:
        return p, h
    else:
        events = []
        for i in xrange(r):
            for j in xrange(c):
                events.append(hpart[i][j])
        events.append(off_prob)

        events = [event.dimshuffle(0, 1, 2, 3, 'x') for event in events]

        events = tuple(events)

        stacked_events = T.concatenate(events, axis=4)

        batch_size, rows, cols, channels, outcomes = stacked_events.shape
        reshaped_events = stacked_events.reshape((batch_size * rows * cols *
                                                  channels, outcomes))

        multinomial = theano_rng.multinomial(pvals=reshaped_events,
                                             dtype=p.dtype)

        reshaped_multinomial = multinomial.reshape((batch_size, rows, cols,
                                                    channels, outcomes))

        h_sample = T.alloc(0., batch_size, zr, zc, ch)

        idx = 0
        for i in xrange(r):
            for j in xrange(c):
                h_sample = T.set_subtensor(h_sample[:, i:zr:r, j:zc:c, :],
                                           reshaped_multinomial[:, :, :, :,
                                           idx])
                idx += 1

        p_sample = 1 - reshaped_multinomial[:, :, :, :, -1]

        return p, h, p_sample, h_sample


def max_pool_softmax_with_bias_op(z, pool_shape):
    """
    .. todo::

        WRITEME properly

    An implementation of max_pool that uses the SoftmaxWithBias op.
    Mostly kept around for comparison benchmarking purposes.
    Also, this uses the ('b', 0, 1, 'c') format.
    """

    z_name = z.name
    if z_name is None:
        z_name = 'anon_z'

    batch_size, zr, zc, ch = z.shape

    r, c = pool_shape

    flat_z = []

    for i in xrange(r):
        for j in xrange(c):
            cur_part = z[:, i:zr:r, j:zc:c, :]
            assert cur_part.ndim == 4
            if z_name is not None:
                cur_part.name = z_name + '[%d,%d]' % (i, j)
            flat_z.append(cur_part.dimshuffle(0, 1, 2, 3, 'x'))

    flat_z.append(T.zeros_like(flat_z[-1]))

    stacked_z = T.concatenate(flat_z, axis=4)

    batch_size, rows, cols, channels, outcomes = stacked_z.shape
    reshaped_z = stacked_z.reshape((batch_size * rows * cols * channels,
                                    outcomes))

    dist = T.nnet.softmax_with_bias(reshaped_z, T.zeros_like(reshaped_z[0, :]))

    dist = dist.reshape((batch_size, rows, cols, channels, outcomes))

    p = 1. - dist[:, :, :, :, -1]
    p.name = 'p(%s)' % z_name

    h = T.alloc(0., batch_size, zr, zc, ch)

    idx = 0
    for i in xrange(r):
        for j in xrange(c):
            h = T.set_subtensor(h[:, i:zr:r, j:zc:c, :],
                                dist[:, :, :, :, idx])
            idx += 1

    h.name = 'h(%s)' % z_name

    return p, h


def max_pool_softmax_op(z, pool_shape):
    """
    .. todo::

        WRITEME properly

    An implementation of max_pool that uses the SoftmaxWithBias op.
    Mostly kept around for comparison benchmarking purposes.
    Also, this uses the ('b', 0, 1, 'c') format.
    """

    z_name = z.name
    if z_name is None:
        z_name = 'anon_z'

    batch_size, zr, zc, ch = z.shape

    r, c = pool_shape

    flat_z = []

    for i in xrange(r):
        for j in xrange(c):
            cur_part = z[:, i:zr:r, j:zc:c, :]
            assert cur_part.ndim == 4
            if z_name is not None:
                cur_part.name = z_name + '[%d,%d]' % (i, j)
            flat_z.append(cur_part.dimshuffle(0, 1, 2, 3, 'x'))

    flat_z.append(T.zeros_like(flat_z[-1]))

    stacked_z = T.concatenate(flat_z, axis=4)

    batch_size, rows, cols, channels, outcomes = stacked_z.shape
    reshaped_z = stacked_z.reshape((batch_size * rows * cols * channels,
                                    outcomes))

    dist = T.nnet.softmax(reshaped_z)

    dist = dist.reshape((batch_size, rows, cols, channels, outcomes))

    p = 1. - dist[:, :, :, :, len(flat_z)-1]
    p.name = 'p(%s)' % z_name

    h = T.alloc(0., batch_size, zr, zc, ch)

    idx = 0
    for i in xrange(r):
        for j in xrange(c):
            h = T.set_subtensor(h[:, i:zr:r, j:zc:c, :],
                                dist[:, :, :, :, idx])
            idx += 1

    h.name = 'h(%s)' % z_name

    return p, h


def profile(f):
    """
    .. todo::

        WRITEME
    """
    logger.info('profiling {0}'.format(f))
    rng = np.random.RandomState([2012, 7, 19])
    batch_size = 80
    rows = 26
    cols = 27
    channels = 30
    pool_rows = 2
    pool_cols = 3
    zv = rng.randn(batch_size, rows, cols, channels).astype(config.floatX)

    #put the inputs + outputs in shared variables so we don't pay GPU
    # transfer during test
    p_shared = sharedX(zv[:, 0:rows:pool_rows, 0:cols:pool_cols, :])
    h_shared = sharedX(zv)
    z_shared = sharedX(zv)

    p_th, h_th = f(z_shared, (pool_rows, pool_cols))

    func = function([], updates={p_shared: p_th, h_shared: h_th})

    logger.info('warming up')
    for i in xrange(10):
        func()

    trials = 10
    results = []

    for i in xrange(trials):
        t1 = time.time()
        for j in xrange(10):
            func()
        t2 = time.time()
        logger.info(t2 - t1)
        results.append(t2-t1)
    logger.info('final: {0}'.format(sum(results)/float(trials)))


def profile_bc01(f):
    """
    .. todo::

        WRITEME
    """
    logger.info('profiling {0}'.format(f))
    rng = np.random.RandomState([2012, 7, 19])
    batch_size = 80
    rows = 26
    cols = 27
    channels = 30
    pool_rows = 2
    pool_cols = 3
    zv = rng.randn(batch_size, channels, rows, cols).astype(config.floatX)

    # put the inputs + outputs in shared variables so we don't pay GPU
    # transfer during test
    p_shared = sharedX(zv[:, :, 0:rows:pool_rows, 0:cols:pool_cols])
    h_shared = sharedX(zv)
    z_shared = sharedX(zv)

    p_th, h_th = f(z_shared, (pool_rows, pool_cols))

    func = function([], updates={p_shared: p_th, h_shared: h_th})

    logger.info('warming up')
    for i in xrange(10):
        func()

    trials = 10
    results = []

    for i in xrange(trials):
        t1 = time.time()
        for j in xrange(10):
            func()
        t2 = time.time()
        logger.info(t2 - t1)
        results.append(t2-t1)
    logger.info('final: {0}'.format(sum(results)/float(trials)))


def profile_samples(f):
    """
    .. todo::

        WRITEME
    """
    logger.info('profiling samples {0}'.format(f))
    rng = np.random.RandomState([2012, 7, 19])
    theano_rng = MRG_RandomStreams(rng.randint(2147462579))
    batch_size = 80
    rows = 26
    cols = 27
    channels = 30
    pool_rows = 2
    pool_cols = 3
    zv = rng.randn(batch_size, rows, cols, channels).astype(config.floatX)

    #put the inputs + outputs in shared variables so we don't pay GPU
    # transfer during test
    p_shared = sharedX(zv[:, 0:rows:pool_rows, 0:cols:pool_cols, :])
    h_shared = sharedX(zv)
    z_shared = sharedX(zv)

    p_th, h_th, ps_th, hs_th = f(z_shared, (pool_rows, pool_cols), theano_rng)

    func = function([], updates={p_shared: ps_th, h_shared: hs_th})

    logger.info('warming up')
    for i in xrange(10):
        func()

    trials = 10
    results = []

    for i in xrange(trials):
        t1 = time.time()
        for j in xrange(10):
            func()
        t2 = time.time()
        logger.info(t2 - t1)
        results.append(t2-t1)
    logger.info('final: {0}'.format(sum(results)/float(trials)))


def profile_grad(f):
    """
    .. todo::

        WRITEME
    """
    logger.info('profiling gradient of {0}'.format(f))
    rng = np.random.RandomState([2012, 7, 19])
    batch_size = 80
    rows = 26
    cols = 27
    channels = 30
    pool_rows = 2
    pool_cols = 3
    zv = rng.randn(batch_size, rows, cols, channels).astype(config.floatX)

    #put the inputs + outputs in shared variables so we don't pay GPU
    # transfer during test
    grad_shared = sharedX(zv)
    z_shared = sharedX(zv)

    p_th, h_th = f(z_shared, (pool_rows, pool_cols))

    func = function([], updates={grad_shared: T.grad(p_th.sum() + h_th.sum(),
                                 z_shared)})

    logger.info('warming up')
    for i in xrange(10):
        func()

    trials = 10
    results = []

    for i in xrange(trials):
        t1 = time.time()
        for j in xrange(10):
            func()
        t2 = time.time()
        logger.info(t2 - t1)
        results.append(t2-t1)
    logger.info('final: {0}'.format(sum(results)/float(trials)))


def profile_grad_bc01(f):
    """
    .. todo::

        WRITEME
    """
    logger.info('profiling gradient of {0}'.format(f))
    rng = np.random.RandomState([2012, 7, 19])
    batch_size = 80
    rows = 26
    cols = 27
    channels = 30
    pool_rows = 2
    pool_cols = 3
    zv = rng.randn(batch_size, channels, rows, cols).astype(config.floatX)

    # put the inputs + outputs in shared variables so we don't pay GPU
    # transfer during test
    grad_shared = sharedX(zv)
    z_shared = sharedX(zv)

    p_th, h_th = f(z_shared, (pool_rows, pool_cols))

    func = function([], updates={grad_shared: T.grad(p_th.sum() + h_th.sum(),
                                 z_shared)})

    logger.info('warming up')
    for i in xrange(10):
        func()

    trials = 10
    results = []

    for i in xrange(trials):
        t1 = time.time()
        for j in xrange(10):
            func()
        t2 = time.time()
        logger.info(t2 - t1)
        results.append(t2-t1)
    logger.info('final: {0}'.format(sum(results)/float(trials)))


if __name__ == '__main__':
    profile_bc01(max_pool)
    profile_grad_bc01(max_pool)
    """
    profile(max_pool_unstable)
    profile_samples(max_pool_b01c)
    profile(max_pool_softmax_op)
    profile(max_pool_softmax_with_bias_op)
    profile_grad(max_pool_unstable)
    profile_grad(max_pool_b01c)
    profile_grad(max_pool_softmax_op)
    profile_grad(max_pool_softmax_with_bias_op)
    """

########NEW FILE########
__FILENAME__ = sampling
"""
.. todo::

    WRITEME
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

from theano.sandbox.rng_mrg import MRG_RandomStreams

from pylearn2.blocks import Block
from pylearn2.utils.rng import make_theano_rng


class SampleBernoulli(Block):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    theano_rng : WRITEME
    seed : WRITEME
    input_space : WRITEME
    """
    def __init__(self, theano_rng = None, seed=None, input_space=None):
        super(SampleBernoulli, self).__init__()
        assert theano_rng is None or seed is None
        theano_rng = make_theano_rng(theano_rng if theano_rng is not None else seed,
                                     2012+11+22, which_method='binomial')
        self.__dict__.update(locals())
        del self.self

    def __call__(self, inputs):
        """
        .. todo::

            WRITEME
        """
        if self.input_space:
            self.input_space.validate(inputs)
        return self.theano_rng.binomial(p=inputs, size=inputs.shape, dtype=inputs.dtype)

    def set_input_space(self, space):
        """
        .. todo::

            WRITEME
        """
        self.input_space = space

    def get_input_space(self):
        """
        .. todo::

            WRITEME
        """
        if self.input_space is not None:
            return self.input_space
        raise ValueError("No input space was specified for this Block (%s). "
                "You can call set_input_space to correct that." % str(self))

    def get_output_space(self):
        """
        .. todo::

            WRITEME
        """
        return self.get_input_space()

########NEW FILE########
__FILENAME__ = stochastic_pool
"""
An implementation of stochastic max-pooling, based on

Stochastic Pooling for Regularization of Deep Convolutional Neural Networks
Matthew D. Zeiler, Rob Fergus, ICLR 2013
"""

__authors__ = "Mehdi Mirza"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Mehdi Mirza", "Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "Mehdi Mirza"
__email__ = "mirzamom@iro"

import numpy
import theano
from theano import tensor
from theano.gof.op import get_debug_values
from pylearn2.utils.rng import make_theano_rng

def stochastic_max_pool_bc01(bc01, pool_shape, pool_stride, image_shape, rng = None):
    """
    .. todo::

        WRITEME properly

    Stochastic max pooling for training as defined in:

    Stochastic Pooling for Regularization of Deep Convolutional Neural Networks
    Matthew D. Zeiler, Rob Fergus

    Parameters
    ----------
    bc01 : theano 4-tensor
        in format (batch size, channels, rows, cols),
        IMPORTANT: All values should be positive
    pool_shape : tuple
        shape of the pool region (rows, cols)
    pool_stride : tuple
        strides between pooling regions (row stride, col stride)
    image_shape : tuple
        avoid doing some of the arithmetic in theano
    rng : theano random stream
    """
    r, c = image_shape
    pr, pc = pool_shape
    rs, cs = pool_stride

    batch = bc01.shape[0]
    channel = bc01.shape[1]

    rng = make_theano_rng(rng, 2022, which_method='multinomial')

    # Compute index in pooled space of last needed pool
    # (needed = each input pixel must appear in at least one pool)
    def last_pool(im_shp, p_shp, p_strd):
        rval = int(numpy.ceil(float(im_shp - p_shp) / p_strd))
        assert p_strd * rval + p_shp >= im_shp
        assert p_strd * (rval - 1) + p_shp < im_shp
        return rval
    # Compute starting row of the last pool
    last_pool_r = last_pool(image_shape[0] ,pool_shape[0], pool_stride[0]) * pool_stride[0]
    # Compute number of rows needed in image for all indexes to work out
    required_r = last_pool_r + pr

    last_pool_c = last_pool(image_shape[1] ,pool_shape[1], pool_stride[1]) * pool_stride[1]
    required_c = last_pool_c + pc

    # final result shape
    res_r = int(numpy.floor(last_pool_r/rs)) + 1
    res_c = int(numpy.floor(last_pool_c/cs)) + 1

    for bc01v in get_debug_values(bc01):
        assert not numpy.any(numpy.isinf(bc01v))
        assert bc01v.shape[2] == image_shape[0]
        assert bc01v.shape[3] == image_shape[1]

    # padding
    padded = tensor.alloc(0.0, batch, channel, required_r, required_c)
    name = bc01.name
    if name is None:
        name = 'anon_bc01'
    bc01 = tensor.set_subtensor(padded[:,:, 0:r, 0:c], bc01)
    bc01.name = 'zero_padded_' + name

    # unraveling
    window = tensor.alloc(0.0, batch, channel, res_r, res_c, pr, pc)
    window.name = 'unravlled_winodows_' + name

    for row_within_pool in xrange(pool_shape[0]):
        row_stop = last_pool_r + row_within_pool + 1
        for col_within_pool in xrange(pool_shape[1]):
            col_stop = last_pool_c + col_within_pool + 1
            win_cell = bc01[:,:,row_within_pool:row_stop:rs, col_within_pool:col_stop:cs]
            window  =  tensor.set_subtensor(window[:,:,:,:, row_within_pool, col_within_pool], win_cell)

    # find the norm
    norm = window.sum(axis = [4, 5])
    norm = tensor.switch(tensor.eq(norm, 0.0), 1.0, norm)
    norm = window / norm.dimshuffle(0, 1, 2, 3, 'x', 'x')
    # get prob
    prob = rng.multinomial(pvals = norm.reshape((batch * channel * res_r * res_c, pr * pc)), dtype='float32')
    # select
    res = (window * prob.reshape((batch, channel, res_r, res_c,  pr, pc))).max(axis=5).max(axis=4)
    res.name = 'pooled_' + name

    return tensor.cast(res, theano.config.floatX)

def weighted_max_pool_bc01(bc01, pool_shape, pool_stride, image_shape, rng = None):
    """
    This implements test time probability weighted pooling defined in:

    Stochastic Pooling for Regularization of Deep Convolutional Neural Networks
    Matthew D. Zeiler, Rob Fergus

    Parameters
    ----------
    bc01 : theano 4-tensor
        minibatch in format (batch size, channels, rows, cols),
        IMPORTANT: All values should be poitivie
    pool_shape : theano 4-tensor
        shape of the pool region (rows, cols)
    pool_stride : tuple
        strides between pooling regions (row stride, col stride)
    image_shape : tuple
        avoid doing some of the arithmetic in theano
    """
    r, c = image_shape
    pr, pc = pool_shape
    rs, cs = pool_stride

    batch = bc01.shape[0]
    channel = bc01.shape[1]

    rng = make_theano_rng(rng, 2022, which_method='multinomial')

    # Compute index in pooled space of last needed pool
    # (needed = each input pixel must appear in at least one pool)
    def last_pool(im_shp, p_shp, p_strd):
        rval = int(numpy.ceil(float(im_shp - p_shp) / p_strd))
        assert p_strd * rval + p_shp >= im_shp
        assert p_strd * (rval - 1) + p_shp < im_shp
        return rval
    # Compute starting row of the last pool
    last_pool_r = last_pool(image_shape[0] ,pool_shape[0], pool_stride[0]) * pool_stride[0]
    # Compute number of rows needed in image for all indexes to work out
    required_r = last_pool_r + pr

    last_pool_c = last_pool(image_shape[1] ,pool_shape[1], pool_stride[1]) * pool_stride[1]
    required_c = last_pool_c + pc

    # final result shape
    res_r = int(numpy.floor(last_pool_r/rs)) + 1
    res_c = int(numpy.floor(last_pool_c/cs)) + 1

    for bc01v in get_debug_values(bc01):
        assert not numpy.any(numpy.isinf(bc01v))
        assert bc01v.shape[2] == image_shape[0]
        assert bc01v.shape[3] == image_shape[1]

    # padding
    padded = tensor.alloc(0.0, batch, channel, required_r, required_c)
    name = bc01.name
    if name is None:
        name = 'anon_bc01'
    bc01 = tensor.set_subtensor(padded[:,:, 0:r, 0:c], bc01)
    bc01.name = 'zero_padded_' + name

    # unraveling
    window = tensor.alloc(0.0, batch, channel, res_r, res_c, pr, pc)
    window.name = 'unravlled_winodows_' + name

    for row_within_pool in xrange(pool_shape[0]):
        row_stop = last_pool_r + row_within_pool + 1
        for col_within_pool in xrange(pool_shape[1]):
            col_stop = last_pool_c + col_within_pool + 1
            win_cell = bc01[:,:,row_within_pool:row_stop:rs, col_within_pool:col_stop:cs]
            window  =  tensor.set_subtensor(window[:,:,:,:, row_within_pool, col_within_pool], win_cell)

    # find the norm
    norm = window.sum(axis = [4, 5])
    norm = tensor.switch(tensor.eq(norm, 0.0), 1.0, norm)
    norm = window / norm.dimshuffle(0, 1, 2, 3, 'x', 'x')
    # average
    res = (window * norm).sum(axis=[4,5])
    res.name = 'pooled_' + name

    return res.reshape((batch, channel, res_r, res_c))



########NEW FILE########
__FILENAME__ = test_coding
from pylearn2.expr.coding import triangle_code
import numpy as np
import theano.tensor as T
from theano import function
from pylearn2.utils import as_floatX

def test_triangle_code():
    rng = np.random.RandomState([20,18,9])

    m = 5
    n = 6
    k = 7

    X = as_floatX(rng.randn(m,n))
    D = as_floatX(rng.randn(k,n))

    D_norm_squared = np.sum(D**2,axis=1)
    X_norm_squared = np.sum(X**2,axis=1)
    sq_distance = -2.0 * np.dot(X,D.T) + D_norm_squared + np.atleast_2d(X_norm_squared).T
    distance = np.sqrt(sq_distance)

    mu = np.mean(distance, axis = 1)
    expected = np.maximum(0.0,mu.reshape(mu.size,1)-distance)

    Xv = T.matrix()
    Dv = T.matrix()

    code = triangle_code(X = Xv, centroids = Dv)
    actual = function([Xv,Dv],code)(X,D)

    assert np.allclose(expected, actual)

########NEW FILE########
__FILENAME__ = test_nnet
"""
Useful expressions common to many neural network applications.
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2013, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

import numpy as np
from theano import tensor as T

from pylearn2.expr.nnet import pseudoinverse_softmax_numpy
from pylearn2.expr.nnet import softmax_numpy
from pylearn2.expr.nnet import softmax_ratio
from pylearn2.expr.nnet import compute_recall
from pylearn2.utils import sharedX


def test_softmax_ratio():
    # Tests that the numerically stabilized version of the softmax ratio
    # matches the naive implementation, for small input values

    n = 3
    m = 4

    rng = np.random.RandomState([2013, 3, 23])

    Z_numer = sharedX(rng.randn(m, n))
    Z_denom = sharedX(rng.randn(m, n))

    numer = T.nnet.softmax(Z_numer)
    denom = T.nnet.softmax(Z_denom)

    naive = numer / denom
    stable = softmax_ratio(numer, denom)

    naive = naive.eval()
    stable = stable.eval()

    assert np.allclose(naive, stable)


def test_pseudoinverse_softmax_numpy():
    rng = np.random.RandomState([2013, 3, 28])

    p = np.abs(rng.randn(5))
    p /= p.sum()

    z = pseudoinverse_softmax_numpy(p)
    zbroad = z.reshape(1, z.size)
    p2 = softmax_numpy(zbroad)
    p2 = p2[0, :]

    assert np.allclose(p, p2)


def test_compute_recall():
    """
    Tests whether compute_recall function works as
    expected.
    """
    tp_pyval = 4
    ys_pyval = np.asarray([0, 1, 1, 0, 1, 1, 0])

    tp = sharedX(tp_pyval, name="tp")
    ys = sharedX(ys_pyval, name="ys_pyval")
    recall_py = tp_pyval / ys_pyval.sum()
    recall = compute_recall(ys, tp)
    assert np.allclose(recall.eval(),
                       recall_py)

########NEW FILE########
__FILENAME__ = test_normalize
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2013, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

import numpy as np
import warnings

from theano import config
from theano import function
import theano.tensor as T

from pylearn2.expr.normalize import (CrossChannelNormalization,
                                     CrossChannelNormalizationBC01)

def ground_truth_normalizer(c01b, k, n, alpha, beta):
    out = np.zeros(c01b.shape)

    for r in xrange(out.shape[1]):
        for c in xrange(out.shape[2]):
            for x in xrange(out.shape[3]):
                out[:,r,c,x] = ground_truth_normalize_row(row=c01b[:,r,c,x], k=k, n=n, alpha=alpha, beta=beta)
    return out

def ground_truth_normalize_row(row, k, n, alpha, beta):
    assert row.ndim == 1
    out = np.zeros(row.shape)
    for i in xrange(row.shape[0]):
        s = k
        tot = 0
        for j in xrange(max(0,i-n/2), min(row.shape[0],i+n/2+1)):
            tot += 1
            sq = row[j] ** 2.
            assert sq > 0.
            assert s >= k
            assert alpha > 0.
            s += alpha * sq
            assert s >= k
        assert tot <= n
        assert s >= k
        s = s ** beta
        out[i] = row[i] / s
    return out


def basic_test():

    channels = 15
    rows = 3
    cols = 4
    batch_size = 2

    shape = [channels, rows, cols, batch_size]

    k = 2
    n = 5
    # use a big value of alpha so mistakes involving alpha show up strong
    alpha = 1.5
    beta = 0.75
    
    # Perform test for C01B

    rng = np.random.RandomState([2013,2])

    c01b = rng.randn(*shape).astype(config.floatX)
    
    normalizer = CrossChannelNormalization(k=k, n=n, alpha=alpha, beta=beta)
    warnings.warn("TODO: add test for the CudaConvnet version.")

    X = T.TensorType(dtype=config.floatX, broadcastable=tuple([False]*4))()

    out = normalizer(X)

    out = function([X], out)(c01b)

    ground_out = ground_truth_normalizer(c01b, n=n, k=k, alpha=alpha, beta=beta)

    assert out.shape == ground_out.shape

    diff = out - ground_out
    err = np.abs(diff)
    max_err = err.max()

    if not np.allclose(out, ground_out):
        print 'C01B test failed'
        print 'error range: ',(err.min(), err.max())
        print 'output: '
        print out
        print 'expected output: '
        print ground_out
        assert False
        
    # Perform test for BC01
    
    bc01 = np.transpose(c01b, [3,0,1,2])
    
    normalizerBC01 = CrossChannelNormalizationBC01(k=k, n=n, alpha=alpha, beta=beta)
    
    X = T.TensorType(dtype=config.floatX, broadcastable=tuple([False]*4))()

    out = normalizerBC01(X)

    out = function([X], out)(bc01)

    ground_out_BC01 = np.transpose(ground_out, [3,0,1,2])

    assert out.shape == ground_out_BC01.shape

    diff = out - ground_out_BC01
    err = np.abs(diff)
    max_err = err.max()

    if not np.allclose(out, ground_out_BC01):
        print 'BC01 test failed'
        print 'error range: ',(err.min(), err.max())
        print 'output: '
        print out
        print 'expected output: '
        print ground_out
        assert False

########NEW FILE########
__FILENAME__ = test_preprocessing
import numpy
from pylearn2.expr.preprocessing import global_contrast_normalize


def test_basic():
    rng = numpy.random.RandomState(0)
    X = abs(rng.randn(50, 70))
    Y = global_contrast_normalize(X)
    numpy.testing.assert_allclose((Y ** 2).sum(axis=1), 1)
    numpy.testing.assert_allclose(Y.mean(axis=1), 0, atol=1e-10)


def test_scale():
    rng = numpy.random.RandomState(0)
    X = abs(rng.randn(50, 70))
    Y = global_contrast_normalize(X, scale=5)
    numpy.testing.assert_allclose(numpy.sqrt((Y ** 2).sum(axis=1)), 5)
    numpy.testing.assert_allclose(Y.mean(axis=1), 0, atol=1e-10)


def test_subtract_mean_false():
    rng = numpy.random.RandomState(0)
    X = abs(rng.randn(50, 70))
    Y = global_contrast_normalize(X, subtract_mean=False, scale=5)
    numpy.testing.assert_allclose(numpy.sqrt((Y ** 2).sum(axis=1)), 5)
    numpy.testing.assert_raises(AssertionError,
                                numpy.testing.assert_allclose,
                                Y.mean(axis=1), 0, atol=1e-10)


def test_std_norm():
    rng = numpy.random.RandomState(0)
    X = abs(rng.randn(50, 70))
    Y = global_contrast_normalize(X, use_std=True, scale=5)
    numpy.testing.assert_allclose(Y.std(axis=1, ddof=1), 5)


def test_min_divisor():
    rng = numpy.random.RandomState(0)
    X = abs(rng.randn(50, 70))
    X[0] *= 1e-15
    Y = global_contrast_normalize(X, subtract_mean=False, use_std=True)
    numpy.testing.assert_array_equal(X[0], Y[0])

########NEW FILE########
__FILENAME__ = test_probabilistic_max_pooling
import numpy as np
import warnings

from theano import config
from theano import function
import theano.tensor as T
from theano.sandbox.rng_mrg import MRG_RandomStreams

from pylearn2.expr.probabilistic_max_pooling import max_pool_python
from pylearn2.expr.probabilistic_max_pooling import max_pool_channels_python
from pylearn2.expr.probabilistic_max_pooling import max_pool
from pylearn2.expr.probabilistic_max_pooling import max_pool_channels
from pylearn2.expr.probabilistic_max_pooling import max_pool_b01c
from pylearn2.expr.probabilistic_max_pooling import max_pool_c01b
from pylearn2.expr.probabilistic_max_pooling import max_pool_unstable
from pylearn2.expr.probabilistic_max_pooling import max_pool_softmax_op
from pylearn2.expr.probabilistic_max_pooling import \
    max_pool_softmax_with_bias_op
from pylearn2.testing import no_debug_mode


def check_correctness_channelwise(f):
    """
    Tests that the theano expression emitted by f computes the same values
    as the ground truth python function
    Note: to keep the python version as dead simple as possible (i.e., to make
    sure there are not bugs in the ground truth) it uses the numerically
    unstable verison of softmax. So this test does not work with too big of
    numbers.
    """

    rng = np.random.RandomState([2012, 7, 19])
    batch_size = 5
    pool_size = 4
    n = 3 * pool_size
    zv = rng.randn(batch_size, n).astype(config.floatX) * 1. - 1.5
    top_down_v = rng.randn(batch_size,  n / pool_size).astype(config.floatX)

    p_np, h_np = max_pool_channels_python(zv, pool_size, top_down_v)

    z_th = T.matrix()
    z_th.name = 'z_th'

    top_down_th = T.matrix()
    top_down_th.name = 'top_down_th'

    p_th, h_th = f(z_th, pool_size, top_down_th)

    func = function([z_th, top_down_th], [p_th, h_th])

    pv, hv = func(zv, top_down_v)

    assert p_np.shape == pv.shape
    assert h_np.shape == hv.shape
    if not np.allclose(h_np, hv):
        print (h_np.min(), h_np.max())
        print (hv.min(), hv.max())
        assert False
    if not np.allclose(p_np, pv):
        diff = abs(p_np - pv)
        print 'max diff ', diff.max()
        print 'min diff ', diff.min()
        print 'ave diff ', diff.mean()
        assert False


def check_correctness_sigmoid_channelwise(f):
    """
    Tests that f is equivalent to the sigmoid function when the pool size is 1
    """

    rng = np.random.RandomState([2012, 7, 19])
    batch_size = 5
    pool_size = 1
    n = 3 * pool_size
    zv = rng.randn(batch_size, n).astype(config.floatX) * 1. - 1.5
    top_down_v = rng.randn(batch_size,  n / pool_size).astype(config.floatX)

    z_th = T.matrix()
    z_th.name = 'z_th'

    top_down_th = T.matrix()
    top_down_th.name = 'top_down_th'

    p_th, h_th = f(z_th, pool_size, top_down_th)
    h_s = T.nnet.sigmoid(z_th + top_down_th)

    func = function([z_th, top_down_th], [p_th, h_th, h_s])

    pv, hv, h_s = func(zv, top_down_v)
    p_s = h_s

    assert p_s.shape == pv.shape
    assert h_s.shape == hv.shape
    if not np.allclose(h_s, hv):
        print (h_s.min(), h_s.max())
        print (hv.min(), hv.max())
        assert False
    if not np.allclose(p_s, pv):
        diff = abs(p_s - pv)
        print 'max diff ', diff.max()
        print 'min diff ', diff.min()
        print 'ave diff ', diff.mean()
        assert False


def check_correctness(f):
    rng = np.random.RandomState([2012, 7, 19])
    batch_size = 5
    rows = 32
    cols = 30
    channels = 3
    pool_rows = 2
    pool_cols = 3
    zv = rng.randn(batch_size, rows, cols,
                   channels).astype(config.floatX) * 2. - 3.

    p_np, h_np = max_pool_python(zv, (pool_rows, pool_cols))

    z_th = T.TensorType(broadcastable=(False, False, False, False),
                        dtype=config.floatX)()
    z_th.name = 'z_th'

    p_th, h_th = f(z_th, (pool_rows, pool_cols))

    func = function([z_th], [p_th, h_th])

    pv, hv = func(zv)

    assert p_np.shape == pv.shape
    assert h_np.shape == hv.shape
    if not np.allclose(h_np, hv):
        print (h_np.min(), h_np.max())
        print (hv.min(), hv.max())
        assert False
    assert np.allclose(p_np, pv)


def check_correctness_bc01(f):
    """
    Tests that the theano expression emitted by f computes the same values
    as the ground truth python function
    Note: to keep the python version as dead simple as possible (i.e., to make
    sure there are not bugs in the ground truth) it uses the numerically
    unstable verison of softmax. So this test does not work with too big of
    numbers.
    """

    rng = np.random.RandomState([2012, 7, 19])
    batch_size = 5
    rows = 32
    cols = 30
    channels = 3
    pool_rows = 2
    pool_cols = 3
    zv = rng.randn(batch_size,  rows, cols,
                   channels).astype(config.floatX) * 1. - 1.5
    top_down_v = rng.randn(batch_size, rows / pool_rows, cols / pool_cols,
                           channels).astype(config.floatX)

    p_np, h_np = max_pool_python(zv, (pool_rows, pool_cols), top_down_v)

    z_th = T.TensorType(broadcastable=(False, False, False, False),
                        dtype = config.floatX)()
    z_th.name = 'z_th'
    zr = z_th.dimshuffle(0, 3, 1, 2)

    top_down_th = T.TensorType(broadcastable=(False, False, False, False),
                               dtype = config.floatX)()
    top_down_th.name = 'top_down_th'
    top_down_r = top_down_th.dimshuffle(0, 3, 1, 2)

    p_th, h_th = f(zr, (pool_rows, pool_cols), top_down_r)

    func = function([z_th, top_down_th], [p_th.dimshuffle(0, 2, 3, 1),
                                          h_th.dimshuffle(0, 2, 3, 1)])

    pv, hv = func(zv, top_down_v)

    assert p_np.shape == pv.shape
    assert h_np.shape == hv.shape
    if not np.allclose(h_np, hv):
        print (h_np.min(), h_np.max())
        print (hv.min(), hv.max())
        assert False
    if not np.allclose(p_np, pv):
        diff = abs(p_np - pv)
        print 'max diff ', diff.max()
        print 'min diff ', diff.min()
        print 'ave diff ', diff.mean()
        assert False


def check_correctness_c01b(f):
    """
    Tests that the theano expression emitted by f computes the same values
    as the ground truth python function
    Note: to keep the python version as dead simple as possible (i.e., to make
    sure there are not bugs in the ground truth) it uses the numerically
    unstable version of softmax. So this test does not work with too big of
    numbers.
    """

    rng = np.random.RandomState([2013, 5, 6])
    batch_size = 5
    rows = 32
    cols = 30
    channels = 3
    pool_rows = 2
    pool_cols = 3

    # Do the python ground truth in b01c format
    zv = rng.randn(batch_size,  rows, cols,
                   channels).astype(config.floatX) * 1. - 1.5
    top_down_v = rng.randn(batch_size, rows / pool_rows, cols / pool_cols,
                           channels).astype(config.floatX)

    p_np, h_np = max_pool_python(zv, (pool_rows, pool_cols), top_down_v)

    # Dimshuffle the inputs into c01b for the theano implementation
    z_th = T.TensorType(broadcastable=(False, False, False, False),
                        dtype = config.floatX)()
    z_th.tag.test_value = zv
    z_th.name = 'z_th'
    zr = z_th.dimshuffle(3, 1, 2, 0)

    top_down_th = T.TensorType(broadcastable=(False, False, False, False),
                               dtype = config.floatX)()
    top_down_th.name = 'top_down_th'
    top_down_th.tag.test_value = top_down_v
    top_down_r = top_down_th.dimshuffle(3, 1, 2, 0)

    p_th, h_th = f(zr, (pool_rows, pool_cols), top_down_r)

    func = function([z_th, top_down_th], [p_th.dimshuffle(3, 1, 2, 0),
                                          h_th.dimshuffle(3, 1, 2, 0)])

    pv, hv = func(zv, top_down_v)

    if not p_np.shape == pv.shape:
        raise AssertionError(str((p_np.shape, pv.shape)))
    assert h_np.shape == hv.shape
    if not np.allclose(h_np, hv):
        print (h_np.min(), h_np.max())
        print (hv.min(), hv.max())
        assert False
    if not np.allclose(p_np, pv):
        diff = abs(p_np - pv)
        print 'max diff ', diff.max()
        print 'min diff ', diff.min()
        print 'ave diff ', diff.mean()
        assert False
    warnings.warn("TODO: make sampling tests run on c01b format of pooling.")


@no_debug_mode
def check_sample_correctishness_b01c(f):
    batch_size = 5
    rows = 32
    cols = 30
    channels = 3
    pool_rows = 2
    pool_cols = 3
    rng = np.random.RandomState([2012, 9, 26])
    zv = rng.randn(batch_size, rows, cols,
                   channels).astype(config.floatX) * 2. - 3.
    top_down_v = rng.randn(batch_size, rows / pool_rows, cols / pool_cols,
                           channels).astype(config.floatX)

    z_th = T.TensorType(broadcastable=(False, False, False, False),
                        dtype = config.floatX)()
    z_th.name = 'z_th'

    top_down_th = T.TensorType(broadcastable=(False, False, False, False),
                               dtype = config.floatX)()
    top_down_th.name = 'top_down_th'

    theano_rng = MRG_RandomStreams(rng.randint(2147462579))
    p_th, h_th, p_sth, h_sth = f(z_th, (pool_rows, pool_cols), top_down_th,
                                 theano_rng)

    prob_func = function([z_th, top_down_th], [p_th, h_th])
    pv, hv = prob_func(zv, top_down_v)

    sample_func = function([z_th, top_down_th], [p_sth, h_sth])

    acc_p = 0. * pv
    acc_h = 0. * hv

    # make sure the test gets good coverage, ie, that it includes many
    # different activation probs for both detector and pooling layer
    buckets = 10
    bucket_width = 1. / float(buckets)
    for i in xrange(buckets):
        lower_lim = i * bucket_width
        upper_lim = (i+1) * bucket_width

        assert np.any((pv >= lower_lim) * (pv < upper_lim))
        assert np.any((hv >= lower_lim) * (hv < upper_lim))

    assert upper_lim == 1.

    for i in xrange(10000):
        ps, hs = sample_func(zv, top_down_v)

        assert ps.shape == pv.shape
        assert hs.shape == hv.shape

        acc_p += ps
        acc_h += hs

    est_p = acc_p / float(i+1)
    est_h = acc_h / float(i+1)

    pd = np.abs(est_p-pv)
    hd = np.abs(est_h-hv)

    """
    # plot maps of the estimation error, this is to see if it has
    # some spatial pattern this is useful for detecting bugs like
    # not handling the border correctly, etc.
    from pylearn2.gui.patch_viewer import PatchViewer

    pv = PatchViewer((pd.shape[0],pd.shape[3]),(pd.shape[1],pd.shape[2]),
                      is_color = False)
    for i in xrange(pd.shape[0]):
    for j in xrange(pd.shape[3]):
    pv.add_patch((pd[i,:,:,j] / pd.max() )* 2.0 - 1.0, rescale = False)
    pv.show()

    pv = PatchViewer((hd.shape[0],hd.shape[3]),(hd.shape[1],hd.shape[2]),
                      is_color = False)
    for i in xrange(hd.shape[0]):
    for j in xrange(hd.shape[3]):
    pv.add_patch( (hd[i,:,:,j] / hd.max() )* 2.0 - 1.0, rescale = False)
    pv.show()
    """

    """
    plot expectation to estimate versus error in estimation
    expect bigger errors for values closer to 0.5

    from matplotlib import pyplot as plt

    #nelem = reduce( lambda x, y : x*y, pd.shape)
    #plt.scatter( pv.reshape(nelem), pd.reshape(nelem))
    #plt.show()

    nelem = reduce( lambda x, y : x*y, hd.shape)
    plt.scatter( hv.reshape(nelem), hd.reshape(nelem))
    plt.show()
    """

    # don't really know how tight this should be
    # but you can try to pose an equivalent problem
    # and implement it in another way
    # using a numpy implementation in softmax_acc.py
    # I got a max error of .17
    assert max(pd.max(), hd.max()) < .17

    # Do exhaustive checks on just the last sample
    assert np.all((ps == 0) + (ps == 1))
    assert np.all((hs == 0) + (hs == 1))

    for k in xrange(batch_size):
        for i in xrange(ps.shape[1]):
            for j in xrange(ps.shape[2]):
                for l in xrange(channels):
                    p = ps[k, i, j, l]
                    h = hs[k, i*pool_rows:(i+1)*pool_rows,
                           j*pool_cols:(j+1)*pool_cols, l]
                    assert h.shape == (pool_rows, pool_cols)
                    assert p == h.max()

    """ If you made it to here, it's correctish
     (cant tell if samples are perfectly "correct") """


@no_debug_mode
def check_sample_correctishness_c01b(f):
    batch_size = 5
    rows = 32
    cols = 30
    channels = 3
    pool_rows = 2
    pool_cols = 3
    rng = np.random.RandomState([2012, 9, 26])
    zv = rng.randn(channels, rows, cols,
                   batch_size).astype(config.floatX) * 2. - 3.
    top_down_v = rng.randn(channels, rows / pool_rows, cols / pool_cols,
                           batch_size).astype(config.floatX)

    z_th = T.TensorType(broadcastable=(False, False, False, False),
                        dtype = config.floatX)()
    z_th.name = 'z_th'
    z_th.tag.test_value = zv

    top_down_th = T.TensorType(broadcastable=(False, False, False, False),
                               dtype = config.floatX)()
    top_down_th.name = 'top_down_th'
    top_down_th.tag.test_value = top_down_v

    theano_rng = MRG_RandomStreams(rng.randint(2147462579))
    p_th, h_th, p_sth, h_sth = f(z_th, (pool_rows, pool_cols), top_down_th,
                                 theano_rng)

    prob_func = function([z_th, top_down_th], [p_th, h_th])
    pv, hv = prob_func(zv, top_down_v)

    sample_func = function([z_th, top_down_th], [p_sth, h_sth])

    acc_p = 0. * pv
    acc_h = 0. * hv

    # make sure the test gets good coverage, ie, that it includes
    # many different activation probs for both detector and pooling layer
    buckets = 10
    bucket_width = 1. / float(buckets)
    for i in xrange(buckets):
        lower_lim = i * bucket_width
        upper_lim = (i+1) * bucket_width

        assert np.any((pv >= lower_lim) * (pv < upper_lim))
        assert np.any((hv >= lower_lim) * (hv < upper_lim))

    assert upper_lim == 1.

    for i in xrange(10000):
        ps, hs = sample_func(zv, top_down_v)

        assert ps.shape == pv.shape
        assert hs.shape == hv.shape

        acc_p += ps
        acc_h += hs

    est_p = acc_p / float(i+1)
    est_h = acc_h / float(i+1)

    pd = np.abs(est_p-pv)
    hd = np.abs(est_h-hv)

    # don't really know how tight this should be
    # but you can try to pose an equivalent problem
    # and implement it in another way
    # using a numpy implementation in softmax_acc.py
    # I got a max error of .17
    assert max(pd.max(), hd.max()) < .17

    # Do exhaustive checks on just the last sample
    assert np.all((ps == 0) + (ps == 1))
    assert np.all((hs == 0) + (hs == 1))

    for k in xrange(batch_size):
        for i in xrange(ps.shape[1]):
            for j in xrange(ps.shape[2]):
                for l in xrange(channels):
                    p = ps[l, i, j, k]
                    h = hs[l, i*pool_rows:(i+1)*pool_rows,
                           j*pool_cols:(j+1)*pool_cols, k]
                    assert h.shape == (pool_rows, pool_cols)
                    assert p == h.max()

    """ If you made it to here, it's correctish
     (cant tell if samples are perfectly "correct") """


@no_debug_mode
def check_sample_correctishness_bc01(f):
    """
    Tests that the sample mean converges to the conditional
    expectation given by the function
    Tests that p really is the max of the samples
    Tests that at most one h in a group is on
    """

    batch_size = 5
    rows = 32
    cols = 30
    channels = 3
    pool_rows = 2
    pool_cols = 3

    rng = np.random.RandomState([2012, 9, 26])
    zv = rng.randn(batch_size, channels, rows,
                   cols).astype(config.floatX) * 2. - 3.
    top_down_v = rng.randn(batch_size, channels, rows / pool_rows,
                           cols / pool_cols).astype(config.floatX)

    z_th = T.TensorType(broadcastable=(False, False, False, False),
                        dtype = config.floatX)()
    z_th.tag.test_value = zv
    z_th.name = 'z_th'

    top_down_th = T.TensorType(broadcastable=(False, False, False, False),
                               dtype = config.floatX)()
    top_down_th.tag.test_value = top_down_v
    top_down_th.name = 'top_down_th'

    theano_rng = MRG_RandomStreams(rng.randint(2147462579))
    p_th, h_th, p_sth, h_sth = f(z_th, (pool_rows, pool_cols), top_down_th,
                                 theano_rng)

    prob_func = function([z_th, top_down_th], [p_th, h_th])
    pv, hv = prob_func(zv, top_down_v)

    sample_func = function([z_th, top_down_th], [p_sth, h_sth])

    acc_p = 0. * pv
    acc_h = 0. * hv

    # make sure the test gets good coverage, ie, that it includes many
    # different activation probs for both detector and pooling layer
    buckets = 10
    bucket_width = 1. / float(buckets)
    for i in xrange(buckets):
        lower_lim = i * bucket_width
        upper_lim = (i+1) * bucket_width

        assert np.any((pv >= lower_lim) * (pv < upper_lim))
        assert np.any((hv >= lower_lim) * (hv < upper_lim))

    assert upper_lim == 1.

    for i in xrange(10000):
        ps, hs = sample_func(zv, top_down_v)

        assert ps.shape == pv.shape
        assert hs.shape == hv.shape

        acc_p += ps
        acc_h += hs

    est_p = acc_p / float(i+1)
    est_h = acc_h / float(i+1)

    pd = np.abs(est_p-pv)
    hd = np.abs(est_h-hv)

    """
    # plot maps of the estimation error, this is to see if it has some
    # spatial pattern this is useful for detecting bugs like not handling
    # the border correctly, etc.
    from pylearn2.gui.patch_viewer import PatchViewer

    pv = PatchViewer((pd.shape[0],pd.shape[3]),(pd.shape[1],pd.shape[2]),
                                                            is_color = False)
    for i in xrange(pd.shape[0]):
    for j in xrange(pd.shape[3]):
    pv.add_patch( (pd[i,:,:,j] / pd.max() )* 2.0 - 1.0, rescale = False)
    pv.show()

    pv = PatchViewer((hd.shape[0],hd.shape[3]), (hd.shape[1],hd.shape[2]),
                                                        is_color = False)
    for i in xrange(hd.shape[0]):
    for j in xrange(hd.shape[3]):
    pv.add_patch( (hd[i,:,:,j] / hd.max() )* 2.0 - 1.0, rescale = False)
    pv.show()
    """

    """
    plot expectation to estimate versus error in estimation
    expect bigger errors for values closer to 0.5

    from matplotlib import pyplot as plt

    #nelem = reduce( lambda x, y : x*y, pd.shape)
    #plt.scatter( pv.reshape(nelem), pd.reshape(nelem))
    #plt.show()

    nelem = reduce( lambda x, y : x*y, hd.shape)
    plt.scatter( hv.reshape(nelem), hd.reshape(nelem))
    plt.show()
    """

    # don't really know how tight this should be
    # but you can try to pose an equivalent problem
    # and implement it in another way
    # using a numpy implementation in softmax_acc.py
    # I got a max error of .17
    assert max(pd.max(), hd.max()) < .17

    # Do exhaustive checks on just the last sample
    assert np.all((ps == 0) + (ps == 1))
    assert np.all((hs == 0) + (hs == 1))

    for k in xrange(batch_size):
        for i in xrange(ps.shape[2]):
            for j in xrange(ps.shape[3]):
                for l in xrange(channels):
                    p = ps[k, l, i, j]
                    h = hs[k, l, i*pool_rows:(i+1)*pool_rows,
                           j*pool_cols:(j+1)*pool_cols]
                    assert h.shape == (pool_rows, pool_cols)
                    assert p == h.max()
                    assert h.sum() <= 1

    """ If you made it to here, it's correctish
     (cant tell if samples are perfectly "correct") """


@no_debug_mode
def check_sample_correctishness_channelwise(f):
    """
    Tests that the sample mean converges to the conditional expectation given
    by the function Tests that p really is the max of the samples tests that
    at most one h in a group is on
    """

    batch_size = 27
    pool_size = 4
    n = pool_size * 21

    rng = np.random.RandomState([2012, 9, 26])
    zv = rng.randn(batch_size, n).astype(config.floatX) * 3.5 - 5.
    top_down_v = rng.randn(batch_size, n / pool_size).astype(config.floatX)

    z_th = T.matrix()
    z_th.tag.test_value = zv
    z_th.name = 'z_th'

    top_down_th = T.matrix()
    top_down_th.tag.test_value = top_down_v
    top_down_th.name = 'top_down_th'

    theano_rng = MRG_RandomStreams(rng.randint(2147462579))
    p_th, h_th, p_sth, h_sth = f(z_th, pool_size, top_down_th, theano_rng)

    prob_func = function([z_th, top_down_th], [p_th, h_th])
    pv, hv = prob_func(zv, top_down_v)

    sample_func = function([z_th, top_down_th], [p_sth, h_sth])

    acc_p = 0. * pv
    acc_h = 0. * hv

    # make sure the test gets good coverage, ie, that it includes
    # many different activation probs for both detector and pooling layer
    buckets = 10
    bucket_width = 1. / float(buckets)
    print pv.min(), pv.max()
    print hv.min(), hv.max()
    for i in xrange(buckets):
        lower_lim = i * bucket_width
        upper_lim = (i+1) * bucket_width
        print lower_lim, upper_lim

        assert np.any((pv >= lower_lim) * (pv < upper_lim))
        assert np.any((hv >= lower_lim) * (hv < upper_lim))

    assert upper_lim == 1.

    for i in xrange(10000):
        ps, hs = sample_func(zv, top_down_v)

        assert ps.shape == pv.shape
        assert hs.shape == hv.shape

        acc_p += ps
        acc_h += hs

    est_p = acc_p / float(i+1)
    est_h = acc_h / float(i+1)

    pd = np.abs(est_p-pv)
    hd = np.abs(est_h-hv)

    """
    # plot maps of the estimation error, this is to see if it has some
    # spatial pattern this is useful for detecting bugs like not handling
    # the border correctly, etc.
    # from pylearn2.gui.patch_viewer import PatchViewer

    pv = PatchViewer((pd.shape[0],pd.shape[3]),(pd.shape[1],pd.shape[2]),
                                                            is_color = False)
    for i in xrange(pd.shape[0]):
    for j in xrange(pd.shape[3]):
    pv.add_patch( (pd[i,:,:,j] / pd.max() )* 2.0 - 1.0, rescale = False)
    pv.show()

    pv = PatchViewer((hd.shape[0],hd.shape[3]),(hd.shape[1],hd.shape[2]),
                                                            is_color = False)
    for i in xrange(hd.shape[0]):
    for j in xrange(hd.shape[3]):
    pv.add_patch( (hd[i,:,:,j] / hd.max() )* 2.0 - 1.0, rescale = False)
    pv.show()
    """

    """
    plot expectation to estimate versus error in estimation
    expect bigger errors for values closer to 0.5

    from matplotlib import pyplot as plt

    #nelem = reduce( lambda x, y : x*y, pd.shape)
    #plt.scatter( pv.reshape(nelem), pd.reshape(nelem))
    #plt.show()

    nelem = reduce( lambda x, y : x*y, hd.shape)
    plt.scatter( hv.reshape(nelem), hd.reshape(nelem))
    plt.show()
    """

    # don't really know how tight this should be
    # but you can try to pose an equivalent problem
    # and implement it in another way
    # using a numpy implementation in softmax_acc.py
    # I got a max error of .17
    assert max(pd.max(), hd.max()) < .17

    # Do exhaustive checks on just the last sample
    assert np.all((ps == 0) + (ps == 1))
    assert np.all((hs == 0) + (hs == 1))

    for k in xrange(batch_size):
        for i in xrange(ps.shape[1]):
            p = ps[k, i]
            h = hs[k, i*pool_size:(i+1)*pool_size]
            assert h.shape == (pool_size,)
            assert p == h.max()
            assert h.sum() <= 1

    """ If you made it to here, it's correctish
     (cant tell if samples are perfectly "correct") """


def test_max_pool_channels():
    check_correctness_channelwise(max_pool_channels)


def test_max_pool_channels_sigmoid():
    check_correctness_sigmoid_channelwise(max_pool_channels)


def test_max_pool_channels_samples():
    check_sample_correctishness_channelwise(max_pool_channels)


def test_max_pool():
    check_correctness_bc01(max_pool)


def test_max_pool_c01b():
    check_correctness_c01b(max_pool_c01b)


def test_max_pool_samples():
    check_sample_correctishness_bc01(max_pool)


def test_max_pool_b01c_samples():
    check_sample_correctishness_b01c(max_pool_b01c)


def test_max_pool_c01b_samples():
    check_sample_correctishness_c01b(max_pool_c01b)


def test_max_pool_b01c():
    check_correctness(max_pool_b01c)


def test_max_pool_unstable():
    check_correctness(max_pool_unstable)


def test_max_pool_softmax_op():
    check_correctness(max_pool_softmax_op)


def test_max_pool_softmax_with_bias_op():
    check_correctness(max_pool_softmax_with_bias_op)

########NEW FILE########
__FILENAME__ = test_stochastic_pool
import numpy
import theano
from theano.compat.python2x import Counter
from pylearn2.expr.stochastic_pool import stochastic_max_pool_bc01, weighted_max_pool_bc01

# TODO add unit tests for: differnt shape, stide, batch and channel size

def test_stochasatic_pool_samples():
    """
    check if the order of frequency of samples from stochastic max pool
    are same as the order of input values.
    """

    batch = 1
    channel = 1
    pool_shape = (2, 2)
    pool_stride = (2, 2)
    image_shape = (2, 2)
    rng = numpy.random.RandomState(12345)
    data = rng.uniform(0, 10, size=(batch, channel, image_shape[0], image_shape[1])).astype('float32')

    x = theano.tensor.tensor4()
    s_max = stochastic_max_pool_bc01(x, pool_shape, pool_stride, image_shape)
    f = theano.function([x], s_max)

    samples = []
    for i in xrange(300):
        samples.append(numpy.asarray(f(data))[0,0,0,0])
    counts = Counter(samples)
    data = data.reshape(numpy.prod(image_shape))
    data.sort()
    data = data[::-1]
    for i in range(len(data) -1):
        assert counts[data[i]] >= counts[data[i+1]]

def test_weighted_pool():
    """
    Test weighted pooling theano implementation against numpy implementation
    """

    rng = numpy.random.RandomState(220)

    for ds in [3]:
        for batch in [2]:
            for ch in [2]:
                data = rng.uniform(size=(batch, ch, ds, ds)).astype('float32')

                # op
                x = theano.tensor.tensor4()
                w_max = weighted_max_pool_bc01(x, (ds,ds), (ds,ds), (ds,ds))
                f = theano.function([x], w_max)
                op_val = numpy.asarray(f(data))

                # python
                norm = data / data.sum(3).sum(2)[:, :, numpy.newaxis, numpy.newaxis]
                py_val = (data * norm).sum(3).sum(2)[:, :, numpy.newaxis, numpy.newaxis]

                assert numpy.allclose(op_val, py_val)


########NEW FILE########
__FILENAME__ = target_format
"""Code for reformatting supervised learning targets."""
from operator import mul

import numpy as np
import scipy
import scipy.sparse
import theano.sparse
from theano import tensor, config


class OneHotFormatter(object):
    """
    A target formatter that transforms labels from integers in both single
    and batch mode.

    Parameters
    ----------
    max_labels : int
        The number of possible classes/labels. This means that all labels
        should be < max_labels. Example: For MNIST there are 10 numbers
        and hence max_labels = 10.
    dtype : dtype, optional
        The desired dtype for the converted one-hot vectors. Defaults to
        `config.floatX` if not given.
    """
    def __init__(self, max_labels, dtype=None):
        """
        Initializes the formatter given the number of max labels.
        """
        try:
            np.empty(max_labels)
        except (ValueError, TypeError):
            raise ValueError("%s got bad max_labels argument '%s'" %
                             (self.__class__.__name__, str(max_labels)))
        self._max_labels = max_labels
        if dtype is None:
            self._dtype = config.floatX
        else:
            try:
                np.dtype(dtype)
            except TypeError:
                raise TypeError("%s got bad dtype identifier %s" %
                                (self.__class__.__name__, str(dtype)))
            self._dtype = dtype

    def format(self, targets, mode='stack', sparse=False):
        """
        Formats a given array of target labels into a one-hot
        vector. If labels appear multiple times, their value
        in the one-hot vector is incremented.

        Parameters
        ----------
        targets : ndarray
            A 1D array of targets, or a batch (2D array) where
            each row is a list of targets.
        mode : string
            The way in which to convert the labels to arrays. Takes
            three different options:

              - "concatenate" : concatenates the one-hot vectors from
                multiple labels
              - "stack" : returns a matrix where each row is the
                one-hot vector of a label
              - "merge" : merges the one-hot vectors together to
                form a vector where the elements are
                the result of an indicator function
                NB: As the result of an indicator function
                the result is the same in case a label
                is duplicated in the input.
        sparse : bool
            If true then the return value is sparse matrix. Note that
            if sparse is True, then mode cannot be 'stack' because
            sparse matrices need to be 2D

        Returns
        -------
        one_hot : a NumPy array (can be 1D-3D depending on settings)
            where normally the first axis are the different batch items,
            the second axis the labels, the third axis the one_hot
            vectors. Can be dense or sparse.
        """
        if mode not in ('concatenate', 'stack', 'merge'):
            raise ValueError("%s got bad mode argument '%s'" %
                             (self.__class__.__name__, str(self._max_labels)))
        elif mode == 'stack' and sparse:
            raise ValueError("Sparse matrices need to be 2D, hence they"
                             "cannot be stacked")
        if targets.ndim > 2:
            raise ValueError("Targets needs to be 1D or 2D, but received %d "
                             "dimensions" % targets.ndim)
        if 'int' not in str(targets.dtype):
            raise TypeError("need an integer array for targets")
        if sparse:
            if mode == 'concatenate':
                one_hot = scipy.sparse.csr_matrix(
                    (np.ones(targets.size, dtype=self._dtype),
                     (targets.flatten() + np.arange(targets.size)
                      * self._max_labels)
                     % (self._max_labels * targets.shape[1]),
                     np.arange(targets.shape[0] + 1) * targets.shape[1]),
                    (targets.shape[0], self._max_labels * targets.shape[1])
                )
            elif mode == 'merge':
                one_hot = scipy.sparse.csr_matrix(
                    (np.ones(targets.size), targets.flatten(),
                     np.arange(targets.shape[0] + 1) * targets.shape[1]),
                    (targets.shape[0], self._max_labels)
                )
        else:
            one_hot = np.zeros(targets.shape + (self._max_labels,),
                               dtype=self._dtype)
            shape = (np.prod(one_hot.shape[:-1]), one_hot.shape[-1])
            one_hot.reshape(shape)[np.arange(shape[0]), targets.flatten()] = 1
            if mode == 'concatenate':
                shape = one_hot.shape[-3:-2] + (reduce(mul,
                                                       one_hot.shape[-2:], 1),)
                one_hot = one_hot.reshape(shape)
            elif mode == 'merge':
                one_hot = np.minimum(one_hot.sum(axis=one_hot.ndim - 2), 1)
        return one_hot

    def theano_expr(self, targets, mode='stack', sparse=False):
        """
        Return the one-hot transformation as a symbolic expression.
        If labels appear multiple times, their value in the one-hot
        vector is incremented.

        Parameters
        ----------
        targets : tensor_like, 1- or 2-dimensional, integer dtype
            A symbolic tensor representing labels as integers
            between 0 and `max_labels` - 1, `max_labels` supplied
            at formatter construction.
        mode : string
            The way in which to convert the labels to arrays. Takes
            three different options:

              - "concatenate" : concatenates the one-hot vectors from
                multiple labels
              - "stack" : returns a matrix where each row is the
                one-hot vector of a label
              - "merge" : merges the one-hot vectors together to
                form a vector where the elements are
                the result of an indicator function
                NB: As the result of an indicator function
                the result is the same in case a label
                is duplicated in the input.
        sparse : bool
            If true then the return value is sparse matrix. Note that
            if sparse is True, then mode cannot be 'stack' because
            sparse matrices need to be 2D

        Returns
        -------
        one_hot : TensorVariable, 1, 2 or 3-dimensional, sparse or dense
            A symbolic tensor representing a one-hot encoding of the
            supplied labels.
        """
        if mode not in ('concatenate', 'stack', 'merge'):
            raise ValueError("%s got bad mode argument '%s'" %
                             (self.__class__.__name__, str(self._max_labels)))
        elif mode == 'stack' and sparse:
            raise ValueError("Sparse matrices need to be 2D, hence they"
                             "cannot be stacked")
        squeeze_required = False
        if targets.ndim != 2:
            if targets.ndim == 1:
                squeeze_required = True
                targets = targets.dimshuffle('x', 0)
            else:
                raise ValueError("targets tensor must be 1 or 2-dimensional")
        if 'int' not in str(targets.dtype):
            raise TypeError("need an integer tensor for targets")
        if sparse:
            if mode == 'concatenate':
                one_hot = theano.sparse.CSR(
                    tensor.ones_like(targets, dtype=self._dtype).flatten(),
                    (targets.flatten() + tensor.arange(targets.size) *
                     self._max_labels) % (self._max_labels * targets.shape[1]),
                    tensor.arange(targets.shape[0] + 1) * targets.shape[1],
                    tensor.stack(targets.shape[0],
                                 self._max_labels * targets.shape[1])
                )
            else:
                one_hot = theano.sparse.CSR(
                    tensor.ones_like(targets, dtype=self._dtype).flatten(),
                    targets.flatten(),
                    tensor.arange(targets.shape[0] + 1) * targets.shape[1],
                    tensor.stack(targets.shape[0], self._max_labels)
                )
        else:
            if mode == 'concatenate':
                one_hot = tensor.zeros((targets.shape[0] * targets.shape[1],
                                        self._max_labels))
                one_hot = tensor.set_subtensor(
                    one_hot[tensor.arange(targets.size),
                            targets.flatten()], 1)
                one_hot = one_hot.reshape(
                    (targets.shape[0], targets.shape[1] * self._max_labels)
                )
            elif mode == 'merge':
                one_hot = tensor.zeros((targets.shape[0], self._max_labels))
                one_hot = tensor.set_subtensor(
                    one_hot[tensor.arange(targets.size) % targets.shape[0],
                            targets.T.flatten()], 1)
            else:
                one_hot = tensor.zeros((targets.shape[0], targets.shape[1],
                                        self._max_labels))
                one_hot = tensor.set_subtensor(one_hot[
                    tensor.arange(targets.shape[0]).reshape((targets.shape[0],
                                                             1)),
                    tensor.arange(targets.shape[1]),
                    targets
                ], 1)
            if squeeze_required:
                if one_hot.ndim == 2:
                    one_hot = one_hot.reshape((one_hot.shape[1],))
                if one_hot.ndim == 3:
                    one_hot = one_hot.reshape((one_hot.shape[1],
                                               one_hot.shape[2]))
        return one_hot


def convert_to_one_hot(integer_vector, dtype=None, max_labels=None,
                       mode='stack', sparse=False):
    """
    Formats a given array of target labels into a one-hot
    vector.

    Parameters
    ----------
    max_labels : int, optional
        The number of possible classes/labels. This means that
        all labels should be < max_labels. Example: For MNIST
        there are 10 numbers and hence max_labels = 10. If not
        given it defaults to max(integer_vector) + 1.
    dtype : dtype, optional
        The desired dtype for the converted one-hot vectors.
        Defaults to config.floatX if not given.
    integer_vector : ndarray
        A 1D array of targets, or a batch (2D array) where
        each row is a list of targets.
    mode : string
        The way in which to convert the labels to arrays. Takes
        three different options:

          - "concatenate" : concatenates the one-hot vectors from
            multiple labels
          - "stack" : returns a matrix where each row is the
            one-hot vector of a label
          - "merge" : merges the one-hot vectors together to
            form a vector where the elements are
            the result of an indicator function
    sparse : bool
        If true then the return value is sparse matrix. Note that
        if sparse is True, then mode cannot be 'stack' because
        sparse matrices need to be 2D

    Returns
    -------
    one_hot : NumPy array
       Can be 1D-3D depending on settings. Normally, the first axis are
       the different batch items, the second axis the labels, the third
       axis the one_hot vectors. Can be dense or sparse.
    """
    if dtype is None:
        dtype = config.floatX
    if isinstance(integer_vector, list):
        integer_vector = np.array(integer_vector)
    assert np.min(integer_vector) >= 0
    assert integer_vector.ndim <= 2
    if max_labels is None:
        max_labels = max(integer_vector) + 1
    return OneHotFormatter(max_labels, dtype=dtype).format(
        integer_vector, mode=mode, sparse=sparse
    )

########NEW FILE########
__FILENAME__ = test_target_format
import numpy
import theano
from pylearn2.format.target_format import OneHotFormatter
from theano.scalar.basic import all_types


def test_one_hot_formatter_simple():
    def check_one_hot_formatter(seed, max_labels, dtype, ncases):
        rng = numpy.random.RandomState(seed)
        fmt = OneHotFormatter(max_labels=max_labels, dtype=dtype)
        integer_labels = rng.random_integers(0, max_labels - 1, size=ncases)
        one_hot_labels = fmt.format(integer_labels)
        assert len(zip(*one_hot_labels.nonzero())) == ncases
        for case, label in enumerate(integer_labels):
            assert one_hot_labels[case, label] == 1
    rng = numpy.random.RandomState(0)
    for seed, dtype in enumerate(all_types):
        yield (check_one_hot_formatter, seed, rng.random_integers(1, 30),
               dtype, rng.random_integers(1, 100))
    fmt = OneHotFormatter(max_labels=10)
    assert fmt.format(numpy.zeros((1, 1), dtype='uint8')).shape == (1, 1, 10)


def test_one_hot_formatter_symbolic():
    def check_one_hot_formatter_symbolic(seed, max_labels, dtype, ncases):
        rng = numpy.random.RandomState(seed)
        fmt = OneHotFormatter(max_labels=max_labels, dtype=dtype)
        integer_labels = rng.random_integers(0, max_labels - 1, size=ncases)
        x = theano.tensor.vector(dtype='int64')
        y = fmt.theano_expr(x)
        f = theano.function([x], y)
        one_hot_labels = f(integer_labels)
        assert len(zip(*one_hot_labels.nonzero())) == ncases
        for case, label in enumerate(integer_labels):
            assert one_hot_labels[case, label] == 1

    rng = numpy.random.RandomState(0)
    for seed, dtype in enumerate(all_types):
        yield (check_one_hot_formatter_symbolic, seed,
               rng.random_integers(1, 30), dtype,
               rng.random_integers(1, 100))


def test_dtype_errors():
    # Try to call theano_expr with a bad label dtype.
    raised = False
    fmt = OneHotFormatter(max_labels=50)
    try:
        fmt.theano_expr(theano.tensor.vector(dtype=theano.config.floatX))
    except TypeError:
        raised = True
    assert raised

    # Try to call format with a bad label dtype.
    raised = False
    try:
        fmt.format(numpy.zeros(10, dtype='float64'))
    except TypeError:
        raised = True
    assert raised


def test_bad_arguments():
    # Make sure an invalid max_labels raises an error.
    raised = False
    try:
        fmt = OneHotFormatter(max_labels=-10)
    except ValueError:
        raised = True
    assert raised

    raised = False
    try:
        fmt = OneHotFormatter(max_labels='10')
    except ValueError:
        raised = True
    assert raised

    # Make sure an invalid dtype identifier raises an error.
    raised = False
    try:
        fmt = OneHotFormatter(max_labels=10, dtype='invalid')
    except TypeError:
        raised = True
    assert raised

    # Make sure an invalid ndim raises an error for format().
    fmt = OneHotFormatter(max_labels=10)
    raised = False
    try:
        fmt.format(numpy.zeros((2, 3, 4), dtype='int32'))
    except ValueError:
        raised = True
    assert raised

    # Make sure an invalid ndim raises an error for theano_expr().
    raised = False
    try:
        fmt.theano_expr(theano.tensor.itensor3())
    except ValueError:
        raised = True
    assert raised

########NEW FILE########
__FILENAME__ = get_weights_report
"""
.. todo::

    WRITEME
"""
import logging
from pylearn2.utils import serial
from pylearn2.gui import patch_viewer
from pylearn2.config import yaml_parse
from pylearn2.datasets import control
import numpy as np


logger = logging.getLogger(__name__)


def get_weights_report(model_path=None,
                       model=None,
                       rescale='individual',
                       border=False,
                       norm_sort=False,
                       dataset=None):
    """
    Returns a PatchViewer displaying a grid of filter weights

    Parameters
    ----------
    model_path : str
        Filepath of the model to make the report on.
    rescale : str
        A string specifying how to rescale the filter images:
            - 'individual' (default) : scale each filter so that it
                  uses as much as possible of the dynamic range
                  of the display under the constraint that 0
                  is gray and no value gets clipped
            - 'global' : scale the whole ensemble of weights
            - 'none' :   don't rescale
    dataset : pylearn2.datasets.dataset.Dataset
        Dataset object to do view conversion for displaying the weights. If
        not provided one will be loaded from the model's dataset_yaml_src.

    Returns
    -------
    WRITEME
    """

    if model is None:
        logger.info('making weights report')
        logger.info('loading model')
        model = serial.load(model_path)
        logger.info('loading done')
    else:
        assert model_path is None
    assert model is not None

    if rescale == 'none':
        global_rescale = False
        patch_rescale = False
    elif rescale == 'global':
        global_rescale = True
        patch_rescale = False
    elif rescale == 'individual':
        global_rescale = False
        patch_rescale = True
    else:
        raise ValueError('rescale=' + rescale +
                         ", must be 'none', 'global', or 'individual'")


    if isinstance(model, dict):
        #assume this was a saved matlab dictionary
        del model['__version__']
        del model['__header__']
        del model['__globals__']
        keys = [key for key in model \
                if hasattr(model[key], 'ndim') and model[key].ndim == 2]
        if len(keys) > 2:
            key = None
            while key not in keys:
                logger.info('Which is the weights?')
                for key in keys:
                    logger.info('\t{0}'.format(key))
                key = raw_input()
        else:
            key, = keys
        weights = model[key]

        norms = np.sqrt(np.square(weights).sum(axis=1))
        logger.info('min norm: {0}'.format(norms.min()))
        logger.info('mean norm: {0}'.format(norms.mean()))
        logger.info('max norm: {0}'.format(norms.max()))

        return patch_viewer.make_viewer(weights,
                                        is_color=weights.shape[1] % 3 == 0)

    weights_view = None
    W = None

    try:
        weights_view = model.get_weights_topo()
        h = weights_view.shape[0]
    except NotImplementedError:

        if dataset is None:
            logger.info('loading dataset...')
            control.push_load_data(False)
            dataset = yaml_parse.load(model.dataset_yaml_src)
            control.pop_load_data()
            logger.info('...done')

        try:
            W = model.get_weights()
        except AttributeError, e:
            raise AttributeError("""
Encountered an AttributeError while trying to call get_weights on a model.
This probably means you need to implement get_weights for this model class,
but look at the original exception to be sure.
If this is an older model class, it may have weights stored as weightsShared,
etc.
Original exception: """+str(e))

    if W is None and weights_view is None:
        raise ValueError("model doesn't support any weights interfaces")

    if weights_view is None:
        weights_format = model.get_weights_format()
        assert hasattr(weights_format,'__iter__')
        assert len(weights_format) == 2
        assert weights_format[0] in ['v','h']
        assert weights_format[1] in ['v','h']
        assert weights_format[0] != weights_format[1]

        if weights_format[0] == 'v':
            W = W.T
        h = W.shape[0]

        if norm_sort:
            norms = np.sqrt(1e-8+np.square(W).sum(axis=1))
            norm_prop = norms / norms.max()


        weights_view = dataset.get_weights_view(W)
        assert weights_view.shape[0] == h
    try:
        hr, hc = model.get_weights_view_shape()
    except NotImplementedError:
        hr = int(np.ceil(np.sqrt(h)))
        hc = hr
        if 'hidShape' in dir(model):
            hr, hc = model.hidShape

    pv = patch_viewer.PatchViewer(grid_shape=(hr, hc),
                                  patch_shape=weights_view.shape[1:3],
            is_color = weights_view.shape[-1] == 3)

    if global_rescale:
        weights_view /= np.abs(weights_view).max()

    if norm_sort:
        logger.info('sorting weights by decreasing norm')
        idx = sorted( range(h), key=lambda l : - norm_prop[l] )
    else:
        idx = range(h)

    if border:
        act = 0
    else:
        act = None

    for i in range(0,h):
        patch = weights_view[idx[i],...]
        pv.add_patch(patch, rescale=patch_rescale, activation=act)

    abs_weights = np.abs(weights_view)
    logger.info('smallest enc weight magnitude: {0}'.format(abs_weights.min()))
    logger.info('mean enc weight magnitude: {0}'.format(abs_weights.mean()))
    logger.info('max enc weight magnitude: {0}'.format(abs_weights.max()))


    if W is not None:
        norms = np.sqrt(np.square(W).sum(axis=1))
        assert norms.shape == (h,)
        logger.info('min norm: {0}'.format(norms.min()))
        logger.info('mean norm: {0}'.format(norms.mean()))
        logger.info('max norm: {0}'.format(norms.max()))

    return pv


def get_binocular_greyscale_weights_report(model_path=None,
                                           model=None,
                                           rescale='individual',
                                           border=False,
                                           norm_sort=False,
                                           dataset=None):
    """
    Returns a PatchViewer displaying a grid of filter weights

    Parameters
    ----------
    model_path : str
        Filepath of the model to make the report on.
    rescale : str
        A string specifying how to rescale the filter images:

          - 'individual' (default) : scale each filter so that it
            uses as much as possible of the dynamic range
            of the display under the constraint that 0
            is gray and no value gets clipped
          - 'global' : scale the whole ensemble of weights
          - 'none' : don't rescale
    dataset : pylearn2.datasets.dataset.Dataset
        Dataset object to do view conversion for displaying the weights. If \
        not provided one will be loaded from the model's dataset_yaml_src.

    Returns
    -------
    WRITEME
    """

    if model is None:
        logger.info('making weights report')
        logger.info('loading model')
        model = serial.load(model_path)
        logger.info('loading done')
    else:
        assert model_path is None
    assert model is not None

    if rescale == 'none':
        global_rescale = False
        patch_rescale = False
    elif rescale == 'global':
        global_rescale = True
        patch_rescale = False
    elif rescale == 'individual':
        global_rescale = False
        patch_rescale = True
    else:
        raise ValueError('rescale=' + rescale +
                         ", must be 'none', 'global', or 'individual'")


    if isinstance(model, dict):
        #assume this was a saved matlab dictionary
        del model['__version__']
        del model['__header__']
        del model['__globals__']
        weights ,= model.values()

        norms = np.sqrt(np.square(weights).sum(axis=1))
        logger.info('min norm: {0}'.format(norms.min()))
        logger.info('mean norm: {0}'.format(norms.mean()))
        logger.info('max norm: {0}'.format(norms.max()))

        return patch_viewer.make_viewer(weights,
                                        is_color=weights.shape[1] % 3 == 0)

    weights_view = None
    W = None

    try:
        weights_view = model.get_weights_topo()
        h = weights_view.shape[0]
    except NotImplementedError:

        if dataset is None:
            logger.info('loading dataset...')
            control.push_load_data(False)
            dataset = yaml_parse.load(model.dataset_yaml_src)
            control.pop_load_data()
            logger.info('...done')

        try:
            W = model.get_weights()
        except AttributeError, e:
            raise AttributeError("""
Encountered an AttributeError while trying to call get_weights on a model.
This probably means you need to implement get_weights for this model class,
but look at the original exception to be sure.
If this is an older model class, it may have weights stored as weightsShared,
etc.
Original exception: """+str(e))

    if W is None and weights_view is None:
        raise ValueError("model doesn't support any weights interfaces")

    if weights_view is None:
        if hasattr(model,'get_weights_format'):
            weights_format = model.get_weights_format()
        elif hasattr(model, 'weights_format'):
            weights_format = model.weights_format
        else:
            # assume default
            weights_format = ('v', 'h')


        assert hasattr(weights_format,'__iter__')
        assert len(weights_format) == 2
        assert weights_format[0] in ['v','h']
        assert weights_format[1] in ['v','h']
        assert weights_format[0] != weights_format[1]

        if weights_format[0] == 'v':
            W = W.T
        h = W.shape[0]

        if norm_sort:
            norms = np.sqrt(1e-8+np.square(W).sum(axis=1))
            norm_prop = norms / norms.max()


        weights_view = dataset.get_weights_view(W)
        assert weights_view.shape[0] == h
    try:
        hr, hc = model.get_weights_view_shape()
    except NotImplementedError:
        hr = int(np.ceil(np.sqrt(h)))
        hc = hr
        if 'hidShape' in dir(model):
            hr, hc = model.hidShape

    pv = patch_viewer.PatchViewer(grid_shape=(hr, hc * 2),
                                  patch_shape=weights_view.shape[1:3],
                                  is_color=weights_view.shape[-1] == 3)

    if global_rescale:
        weights_view /= np.abs(weights_view).max()

    if norm_sort:
        logger.info('sorting weights by decreasing norm')
        idx = sorted(range(h), key=lambda l : - norm_prop[l])
    else:
        idx = range(h)

    if border:
        act = 0
    else:
        act = None

    for i in range(0,h):
        patch = weights_view[idx[i],...]
        if patch_rescale:
            patch = patch / np.abs(patch).max()
        pv.add_patch(patch[:,:,1], rescale=False, activation=act)
        pv.add_patch(patch[:,:,0], rescale=False, activation=act)

    abs_weights = np.abs(weights_view)
    logger.info('smallest enc weight magnitude: {0}'.format(abs_weights.min()))
    logger.info('mean enc weight magnitude: {0}'.format(abs_weights.mean()))
    logger.info('max enc weight magnitude: {0}'.format(abs_weights.max()))


    if W is not None:
        norms = np.sqrt(np.square(W).sum(axis=1))
        assert norms.shape == (h,)
        logger.info('min norm: {0}'.format(norms.min()))
        logger.info('mean norm: {0}'.format(norms.mean()))
        logger.info('max norm: {0}'.format(norms.max()))

    return pv

########NEW FILE########
__FILENAME__ = graph_2D
"""
.. todo::

    WRITEME
"""
import numpy as N
from theano import config


class Graph2D:
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    shape : WRITEME
    xlim : WRITEME
    ycenter : WRITEME
    """
    def __init__(self, shape, xlim, ycenter):
        self.xmin = 0.
        self.xmax = 0.
        self.set_shape(shape)
        self.set_xlim(xlim)
        self.set_ycenter(ycenter)

        self.components = []

    def set_shape(self, shape):
        """
        .. todo::

            WRITEME
        """
        self.rows = shape[0]
        self.cols = shape[1]



    def set_xlim(self, xlim):
        """
        .. todo::

            WRITEME
        """
        #x coordinate of center of leftmost pixel
        self.xmin = xlim[0]
        #x coordinate of center of rightmost pixel
        self.xmax = xlim[1]
        self.delta_x = (self.xmax-self.xmin)/float(self.cols-1)

    def set_ycenter(self, ycenter):
        """
        .. todo::

            WRITEME
        """
        self.delta_y = self.delta_x
        self.ymin = ycenter - (self.rows / 2) * self.delta_y
        self.ymax = self.ymin + (self.rows -1) * self.delta_y

    def render(self):
        """
        .. todo::

            WRITEME
        """
        rval = N.zeros((self.rows, self.cols, 3))

        for component in self.components:
            rval = component.render( prev_layer = rval, parent = self )
            assert rval is not None

        return rval

    def get_coords_for_col(self, i):
        """
        .. todo::

            WRITEME
        """
        X = N.zeros((self.rows,2),dtype=config.floatX)
        X[:,0] = self.xmin + float(i) * self.delta_x
        X[:,1] = self.ymin + N.cast[config.floatX](N.asarray(range(self.rows-1,-1,-1))) * self.delta_y


        return X

class HeatMap:
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    f : WRITEME
        A callable that takes a design matrix of 2D coordinates and returns a
        vector containing the function value at those coordinates
    normalizer : WRITEME
        None or a callable that takes a 2D numpy array and returns a 2D numpy
        array
    render_mode : WRITEME
        * 'o' : opaque.
        * 'r' : render only to the (r)ed channel
    """
    def __init__(self, f, normalizer, render_mode = 'o'):
        self.f = f
        self.normalizer = normalizer
        self.render_mode = render_mode

    def render(self, prev_layer, parent):
        """
        .. todo::

            WRITEME
        """
        my_img = prev_layer * 0.0

        for i in xrange(prev_layer.shape[1]):
            X = parent.get_coords_for_col(i)
            f = self.f(X)
            if len(f.shape) == 1:
                for j in xrange(3):
                    my_img[:,i,j] = f
            else:
                my_img[:,i,:] = f
            #end if
        #end for i

        if self.normalizer is not None:
            my_img = self.normalizer(my_img)
            assert my_img is not None

        if self.render_mode == 'r':
            my_img[:,:,1:] = prev_layer[:,:,1:]
        elif self.render_mode == 'o':
            pass
        else:
            raise NotImplementedError()

        return my_img

########NEW FILE########
__FILENAME__ = patch_viewer
"""
Functionality for display and saving images of collections of images patches.
"""
import numpy as np
from pylearn2.datasets.dense_design_matrix import DefaultViewConverter
from pylearn2.utils.image import Image, ensure_Image
from pylearn2.utils.image import show
from pylearn2.utils import py_integer_types
import warnings


def make_viewer(mat, grid_shape=None, patch_shape=None,
                activation=None, pad=None, is_color = False, rescale = True):
    """
    Given filters in rows, guesses dimensions of patches
    and nice dimensions for the PatchViewer and returns a PatchViewer
    containing visualizations of the filters.

    Parameters
    ----------
    mat : ndarray
        Values should lie in [-1, 1] if `rescale` is False.
        0. always indicates medium gray, with negative values drawn as
        blacker and positive values drawn as whiter.
        A matrix with each row being a different image patch, OR
        a 4D tensor in ('b', 0, 1, 'c') format.
        If matrix, we assume it was flattened using the same procedure as a
        ('b', 0, 1, 'c') DefaultViewConverter uses.
    grid_shape : tuple, optional
        A tuple of two ints specifying the shape of the grad in the
        PatchViewer, in (rows, cols) format. If not specified, this
        function does its best to choose an aesthetically pleasing
        value.
    patch_shape : tupe, optional
        A tuple of two ints specifying the shape of the patch.
        If `mat` is 4D, this function gets the patch shape from the shape of
        `mat`. If `mat` is 2D and patch_shape is not specified, this function
        assumes the patches are perfectly square.
    activation : iterable
        An iterable collection describing some kind of activation value
        associated with each patch. This is indicated with a border around the
        patch whose color intensity increases with activation value.
        The individual activation values may be single floats to draw one
        border or iterable collections of floats to draw multiple borders with
        differing intensities around the patch.
    pad : int, optional
        The amount of padding to add between patches in the displayed image.
    is_color : int
        If True, assume the images are in color.
        Note needed if `mat` is in ('b', 0, 1, 'c') format since we can just
        look at its shape[-1].
    rescale : bool
        If True, rescale each patch so that its highest magnitude pixel
        reaches a value of either 0 or 1 depending on the sign of that pixel.

    Returns
    -------
    patch_viewer : PatchViewer
        A PatchViewer containing the patches stored in `mat`.
    """

    num_channels = 1
    if is_color:
        num_channels = 3

    if grid_shape is None:
        grid_shape = PatchViewer.pick_shape(mat.shape[0] )
    if mat.ndim > 2:
        patch_shape = mat.shape[1:3]
        topo_view = mat
        num_channels = mat.shape[3]
        is_color = num_channels > 1
    else:
        if patch_shape is None:
            assert mat.shape[1] % num_channels == 0
            patch_shape = PatchViewer.pick_shape(mat.shape[1] / num_channels,
                                                 exact = True)
            assert mat.shape[1] == (patch_shape[0] *
                                    patch_shape[1] *
                                    num_channels)
        topo_shape = (patch_shape[0], patch_shape[1], num_channels)
        view_converter = DefaultViewConverter(topo_shape)
        topo_view = view_converter.design_mat_to_topo_view(mat)
    rval = PatchViewer(grid_shape, patch_shape, pad=pad, is_color = is_color)
    for i in xrange(mat.shape[0]):
        if activation is not None:
            if hasattr(activation[0], '__iter__'):
                act = [a[i] for a in activation]
            else:
                act = activation[i]
        else:
            act = None

        patch = topo_view[i, :]

        rval.add_patch(patch, rescale=rescale,
                       activation=act)
    return rval


class PatchViewer(object):
    """
    A class for viewing collections of image patches arranged in a grid.

    Parameters
    ----------
    grid_shape : tuple
        A tuple in format (rows, cols), in units of patches. This determines
        the size of the display. e.g. if you want to display 100 patches at
        a time you might use (10, 10).
    patch_shape : tuple
        A tuple in format (rows, cols), in units of pixels. The patches must
        be at most this large. It will be possible to display smaller patches
        in this `PatchViewer`, but each patch will appear in the center of a
        rectangle of this size.
    is_color : bool
        Whether the PatchViewer should be color (True) or grayscale (False)
    pad : tuple
        Tuple of ints in the form (pad vertical, pad horizontal). Number of
        pixels to put between each patch in each direction.
    background : float or 3-tuple
        The color of the background of the display. Either a float in [0, 1]
        if `is_color` is `False` or a 3-tuple/3d ndarray array of floats in
        [0, 1] for RGB color if `is_color` is `True`.
    """
    def __init__(self, grid_shape, patch_shape, is_color=False, pad = None,
                 background = None ):
        if background is None:
            if is_color:
                background = np.zeros((3,))
            else:
                background = 0.
        self.background = background
        assert len(grid_shape) == 2
        assert len(patch_shape) == 2
        for shape in [grid_shape, patch_shape]:
            for elem in shape:
                if not isinstance(elem, py_integer_types):
                    raise ValueError("Expected grid_shape and patch_shape to"
                                     "be pairs of ints, but they are %s and "
                                     "%s respectively." % (str(grid_shape),
                                                         str(patch_shape)))
        self.is_color = is_color
        if pad is None:
            self.pad = (5, 5)
        else:
            self.pad = pad
        # these are the colors of the activation shells
        self.colors = [np.asarray([1, 1, 0]),
                       np.asarray([1, 0, 1]),
                       np.asarray([0, 1, 0])]

        height = (self.pad[0] * (1 + grid_shape[0]) + grid_shape[0] *
                  patch_shape[0])
        width = (self.pad[1] * (1 + grid_shape[1]) + grid_shape[1] *
                 patch_shape[1])
        self.patch_shape = patch_shape
        self.grid_shape = grid_shape

        image_shape = (height, width, 3)

        self.image = np.zeros(image_shape)
        assert self.image.shape[1] == (self.pad[1] *
                                       (1 + self.grid_shape[1]) +
                                       self.grid_shape[1] *
                                       self.patch_shape[1])
        self.cur_pos = (0, 0)
        #needed to render in the background color
        self.clear()


    def clear(self):
        """
        .. todo::

            WRITEME
        """
        if self.is_color:
            for i in xrange(3):
                self.image[:, :, i] = self.background[i] * .5 + .5
        else:
            self.image[:] = self.background * .5 + .5
        self.cur_pos = (0, 0)

    #0 is perfect gray. If not rescale, assumes images are in [-1,1]
    def add_patch(self, patch, rescale=True, recenter=True, activation=None,
                  warn_blank_patch = True):
        """
        Adds an image patch to the `PatchViewer`.

        Patches are added left to right, top to bottom. If this method is
        called when the `PatchViewer` is already full, it will clear the
        viewer and start adding patches at the upper left again.

        Parameters
        ----------
        patch : ndarray
            If this `PatchViewer` is in color (controlled by the `is_color`
            parameter of the constructor) `patch` should be a 3D ndarray, with
            the first axis being the rows of the image, the second axis
            being the columsn of the image, and the third being RGB color
            channels.
            If this `PatchViewer` is grayscale, `patch` should be either a
            3D ndarray with the third axis having length 1, or a 2D ndarray.

            The values of the ndarray should be floating point. 0 is displayed
            as gray. Negative numbers are displayed as blacker. Positive
            numbers are displayed as whiter. See the `rescale` parameter for
            more detail. This color convention was chosen because it is useful
            for displaying weight matrices.
        rescale : bool
            If True, the maximum absolute value of a pixel in `patch` sets the
            scale, so that abs(patch).max() is absolute white and
            -abs(patch).max() is absolute black.
            If False, `patch` should lie in [-1, 1].
        recenter : bool
            If True (default), if `patch` has smaller dimensions than were
            specified to the constructor's `patch_shape` argument, we will
            display the patch in the center of the area allocated to it in
            the display grid.
            If False, we will raise an exception if `patch` is not exactly
            the specified shape.
        activation : WRITEME
            WRITEME
        warn_blank_patch : WRITEME
            WRITEME
        """
        if warn_blank_patch and \
               (patch.min() == patch.max()) and \
               (rescale or patch.min() == 0.0):
            warnings.warn("displaying totally blank patch")


        if self.is_color:
            assert patch.ndim == 3
            if not (patch.shape[-1] == 3):
                raise ValueError("Expected color image to have shape[-1]=3, "
                                 "but shape[-1] is " + str(patch.shape[-1]))
        else:
            assert patch.ndim in [2, 3]
            if patch.ndim == 3:
                if patch.shape[-1] != 1:
                    raise ValueError("Expected 2D patch or 3D patch with 1 "
                                     "channel, but got patch with shape " + \
                                     str(patch.shape))

        if recenter:
            assert patch.shape[0] <= self.patch_shape[0]
            if patch.shape[1] > self.patch_shape[1]:
                raise ValueError("Given patch of width %d but only patches up"
                                 " to width %d fit" \
                                 % (patch.shape[1], self.patch_shape[1]))
            rs_pad = (self.patch_shape[0] - patch.shape[0]) / 2
            re_pad = self.patch_shape[0] - rs_pad - patch.shape[0]
            cs_pad = (self.patch_shape[1] - patch.shape[1]) / 2
            ce_pad = self.patch_shape[1] - cs_pad - patch.shape[1]
        else:
            if patch.shape[0:2] != self.patch_shape:
                raise ValueError('Expected patch with shape %s, got %s' %
                                 (str(self.patch_shape), str(patch.shape)))
            rs_pad = 0
            re_pad = 0
            cs_pad = 0
            ce_pad = 0

        temp = patch.copy()

        assert (not np.any(np.isnan(temp))) and (not np.any(np.isinf(temp)))

        if rescale:
            scale = np.abs(temp).max()
            if scale > 0:
                temp /= scale
        else:
            if temp.min() < -1.0 or temp.max() > 1.0:
                raise ValueError('When rescale is set to False, pixel values '
                                 'must lie in [-1,1]. Got [%f, %f].'
                                 % (temp.min(), temp.max()))
        temp *= 0.5
        temp += 0.5

        assert temp.min() >= 0.0
        assert temp.max() <= 1.0

        if self.cur_pos == (0, 0):
            self.clear()

        rs = self.pad[0] + (self.cur_pos[0] *
                            (self.patch_shape[0] + self.pad[0]))
        re = rs + self.patch_shape[0]

        assert self.cur_pos[1] <= self.grid_shape[1]
        cs = self.pad[1] + (self.cur_pos[1] *
                            (self.patch_shape[1] + self.pad[1]))
        ce = cs + self.patch_shape[1]

        assert ce <= self.image.shape[1], (ce, self.image.shape[1])

        temp *= (temp > 0)

        if len(temp.shape) == 2:
            temp = temp[:, :, np.newaxis]

        assert ce-ce_pad <= self.image.shape[1]
        self.image[rs + rs_pad:re - re_pad, cs + cs_pad:ce - ce_pad, :] = temp

        if activation is not None:
            if (not isinstance(activation, tuple) and
               not isinstance(activation, list)):
                activation = (activation,)

            for shell, amt in enumerate(activation):
                assert 2 * shell + 2 < self.pad[0]
                assert 2 * shell + 2 < self.pad[1]
                if amt >= 0:
                    act = amt * np.asarray(self.colors[shell])
                    self.image[rs + rs_pad - shell - 1,
                               cs + cs_pad - shell - 1:
                               ce - ce_pad + 1 + shell,
                               :] = act
                    self.image[re - re_pad + shell,
                               cs + cs_pad - 1 - shell:
                               ce - ce_pad + 1 + shell,
                               :] = act
                    self.image[rs + rs_pad - 1 - shell:
                               re - re_pad + 1 + shell,
                               cs + cs_pad - 1 - shell,
                               :] = act
                    self.image[rs + rs_pad - shell - 1:
                               re - re_pad + shell + 1,
                               ce - ce_pad + shell,
                               :] = act

        self.cur_pos = (self.cur_pos[0], self.cur_pos[1] + 1)
        if self.cur_pos[1] == self.grid_shape[1]:
            self.cur_pos = (self.cur_pos[0] + 1, 0)
            if self.cur_pos[0] == self.grid_shape[0]:
                self.cur_pos = (0, 0)

    def addVid(self, vid, rescale=False, subtract_mean=False, recenter=False):
        myvid = vid.copy()
        """
        .. todo::

            WRITEME
        """
        if subtract_mean:
            myvid -= vid.mean()
        if rescale:
            scale = np.abs(myvid).max()
            if scale == 0:
                scale = 1
            myvid /= scale
        for i in xrange(vid.shape[2]):
            self.add_patch(myvid[:, :, i], rescale=False, recenter=recenter)

    def show(self):
        """
        .. todo::

            WRITEME
        """
        #image.imview_async(self.image)
        show(self.image)

    def get_img(self):
        """
        .. todo::

            WRITEME
        """
        #print 'image range '+str((self.image.min(), self.image.max()))
        x = np.cast['uint8'](self.image * 255.0)
        if x.shape[2] == 1:
            x = x[:, :, 0]
        ensure_Image()
        img = Image.fromarray(x)
        return img

    def save(self, path):
        """
        .. todo::

            WRITEME
        """
        self.get_img().save(path)

    def pick_shape(n, exact = False):
        """
        .. todo::

            WRITEME properly

        Returns a shape that fits n elements. If exact, fits exactly n elements
        """

        if not isinstance(n, py_integer_types):
            raise TypeError("n must be an integer, but is "+str(type(n)))

        if exact:

            best_r = -1
            best_c = -1
            best_ratio = 0

            for r in xrange(1,int(np.sqrt(n))+1):
                if n % r != 0:
                    continue
                c = n / r

                ratio = min( float(r)/float(c), float(c)/float(r) )

                if ratio > best_ratio:
                    best_ratio = ratio
                    best_r = r
                    best_c = c

            return (best_r, best_c)

        sqrt = np.sqrt(n)
        r = c = int(np.floor(sqrt))
        while r * c < n:
            c += 1
        return (r, c)
    pick_shape = staticmethod(pick_shape)


########NEW FILE########
__FILENAME__ = tangent_plot
"""
Code for plotting curves with tangent lines.
"""

__author__ = "Ian Goodfellow"

try:
    from matplotlib import pyplot
except Exception:
    pyplot = None


def tangent_plot(x, y, s):
    """
    Plots a curve with tangent lines.

    Parameters
    ----------
    x : list
        List of x coordinates.
        Assumed to be sorted into ascending order, so that the tangent
        lines occupy 80 percent of the horizontal space between each pair
        of points.
    y : list
        List of y coordinates
    s : list
        List of slopes
    """

    assert isinstance(x, list)
    assert isinstance(y, list)
    assert isinstance(s, list)
    n = len(x)
    assert len(y) == n
    assert len(s) == n

    if pyplot is None:
        raise RuntimeError("Could not import pyplot, can't run this code.")

    pyplot.plot(x, y, color='b')

    if n == 0:
        pyplot.show()
        return

    pyplot.hold(True)

    # Add dummy entries so that the for loop can use the same code on every
    # entry
    if n == 1:
        x = [x[0] - 1] + x[0] + [x[0] + 1.]
    else:
        x = [x[0] - (x[1] - x[0])] + x + [x[-2] + (x[-1] - x[-2])]

    y = [0.] + y + [0]
    s = [0.] + s + [0]

    for i in xrange(1, n + 1):
        ld = 0.4 * (x[i] - x[i - 1])
        lx = x[i] - ld
        ly = y[i] - ld * s[i]
        rd = 0.4 * (x[i + 1] - x[i])
        rx = x[i] + rd
        ry = y[i] + rd * s[i]
        pyplot.plot([lx, rx], [ly, ry], color='g')

    pyplot.show()

if __name__ == "__main__":
    # Demo by plotting a quadratic function
    import numpy as np
    x = np.arange(-5., 5., .1)
    y = 0.5 * (x ** 2)
    x = list(x)
    y = list(y)
    tangent_plot(x, y, x)

########NEW FILE########
__FILENAME__ = conv2d
"""
.. todo::

    WRITEME
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

import functools
import numpy as np

import theano
import theano.tensor as T
from theano.tensor.nnet.conv import conv2d

from pylearn2.packaged_dependencies.theano_linear.conv2d \
    import Conv2d as OrigConv2D

from pylearn2.linear.linear_transform import LinearTransform as P2LT
from pylearn2.utils import sharedX
from pylearn2.utils.rng import make_np_rng


default_seed = [2012, 11, 6, 9]
default_sparse_seed = [2012, 11, 6]


class Conv2D(OrigConv2D):
    """
    Extend the TheanoLinear Conv2d class to support everything
    needed for a pylearn2 linear operator.

    Also extend it to handle different axis semantics.

    Parameters
    ----------
    filters : Theano shared variable
        4-tensor of shape (out channels, in channels, rows, cols)
    batch_size : WRITEME
    input_space : WRITEME
    output_axes : WRITEME
    subsample : WRITEME
    border_mode : WRITEME
    filters_shape : WRITEME
    message : WRITEME
    """

    def __init__(self, filters, batch_size, input_space,
                 output_axes=('b', 0, 1, 'c'), subsample=(1, 1),
                 border_mode='valid', filters_shape=None, message=''):
        assert batch_size is None or batch_size > 0
        self.input_space = input_space
        self.output_axes = output_axes

        super(Conv2D, self).__init__(
            filters=filters,
            img_shape=(batch_size, input_space.num_channels,
                       input_space.shape[0], input_space.shape[1]),
            subsample=subsample,
            border_mode=border_mode,
            filters_shape=filters.get_value(borrow=True).shape,
            message=message
        )

    @functools.wraps(P2LT.get_params)
    def get_params(self):
        """
        .. todo::

            WRITEME
        """
        return [self._filters]

    @functools.wraps(P2LT.get_weights_topo)
    def get_weights_topo(self, borrow):
        """
        .. todo::

            WRITEME
        """
        return np.transpose(self._filters.get_value(borrow=borrow),
                            (0, 2, 3, 1))

    def lmul(self, x):
        """
        .. todo::

            WRITEME properly

        dot(x, A)

        This method overrides the original Conv2D lmul to make it work
        with arbitrary axis orders
        """

        # x must be formatted as batch index, channel, topo dim 0, topo dim 1
        # for use with conv2d, so check what the current input space format is
        assert x.ndim == 4
        axes = self.input_space.axes
        assert len(axes) == 4

        op_axes = ('b', 'c', 0, 1)

        if tuple(axes) != op_axes:
            x = x.dimshuffle(
                axes.index('b'),
                axes.index('c'),
                axes.index(0),
                axes.index(1))

        rval = conv2d(
            x, self._filters,
            image_shape=self._img_shape,
            filter_shape=self._filters_shape,
            subsample=self._subsample,
            border_mode=self._border_mode,
        )

        # Format the output based on the output space
        axes = self.output_axes
        assert len(axes) == 4

        if tuple(axes) != op_axes:
            rval = rval.dimshuffle(
                op_axes.index(axes[0]),
                op_axes.index(axes[1]),
                op_axes.index(axes[2]),
                op_axes.index(axes[3])
            )

        return rval

    def lmul_T(self, x):
        """
        .. todo::

            WRITEME properly

        Override the original Conv2D lmul_T to make it work
        with pylearn format of topological data using dimshuffles
        """
        assert x.dtype == self._filters.dtype

        op_axes = ('b', 'c', 0, 1)
        axes = self.output_axes
        if tuple(axes) != op_axes:
            x = x.dimshuffle(
                axes.index('b'),
                axes.index('c'),
                axes.index(0),
                axes.index(1)
            )

        # dot(x, A.T)
        dummy_v = T.tensor4()
        dummy_v.name = 'dummy_v'

        # Since we made this variable, we need to put a tag on it
        if theano.config.compute_test_value == 'raise':
            dummy_v.tag.test_value = np.zeros(
                (x.tag.test_value.shape[0],
                 self.input_space.num_channels,
                 self.input_space.shape[0],
                 self.input_space.shape[1]),
                dtype=dummy_v.dtype
            )

        z_hs = conv2d(
            dummy_v, self._filters,
            image_shape=self._img_shape,
            filter_shape=self._filters_shape,
            subsample=self._subsample,
            border_mode=self._border_mode,
        )

        rval, xdummy = z_hs.owner.op.grad((dummy_v, self._filters), (x,))

        # Format the output based on the input space
        axes = self.input_space.axes
        assert len(axes) == 4

        if tuple(axes) != op_axes:
            rval = rval.dimshuffle(
                op_axes.index(axes[0]),
                op_axes.index(axes[1]),
                op_axes.index(axes[2]),
                op_axes.index(axes[3])
            )

        return rval

    def lmul_sq_T(self, x):
        """
        .. todo::

            WRITEME properly

        Kind of a stupid hacky method used to support convolutional score
        matching. Ought to find a way to make _filters symbolic rather
        than shared.
        """
        assert x.dtype == self._filters.dtype

        op_axes = ('b', 'c', 0, 1)
        axes = self.output_axes
        if tuple(axes) != op_axes:
            x = x.dimshuffle(
                axes.index('b'),
                axes.index('c'),
                axes.index(0),
                axes.index(1)
            )

        # dot(x, sq(A).T)
        dummy_v = T.tensor4()
        sqfilt = T.square(self._filters)
        z_hs = conv2d(
            dummy_v, sqfilt,
            image_shape=self._img_shape,
            filter_shape=self._filters_shape,
            subsample=self._subsample,
            border_mode=self._border_mode,
        )
        rval, xdummy = z_hs.owner.op.grad((dummy_v, sqfilt), (x,))

        # Format the output based on the input space
        axes = self.input_space.axes
        assert len(axes) == 4

        if tuple(axes) != op_axes:
            rval = rval.dimshuffle(
                op_axes.index(axes[0]),
                op_axes.index(axes[1]),
                op_axes.index(axes[2]),
                op_axes.index(axes[3])
            )

        return rval

    def set_batch_size(self, batch_size):
        """
        .. todo::

            WRITEME
        """
        self._img_shape = tuple([batch_size] + list(self._img_shape[1:]))


def make_random_conv2D(irange, input_space, output_space,
                       kernel_shape, batch_size=None, \
                       subsample = (1,1), border_mode = 'valid',
                       message = "", rng = None):
    """
    .. todo::

        WRITEME properly

    Creates a Conv2D with random kernels
    """

    rng = make_np_rng(rng, default_seed, which_method='uniform')

    W = sharedX(rng.uniform(
        -irange, irange,
        (output_space.num_channels, input_space.num_channels,
         kernel_shape[0], kernel_shape[1])
    ))

    return Conv2D(
        filters=W,
        batch_size=batch_size,
        input_space=input_space,
        output_axes=output_space.axes,
        subsample=subsample, border_mode=border_mode,
        filters_shape=W.get_value(borrow=True).shape, message=message
    )


def make_sparse_random_conv2D(num_nonzero, input_space, output_space,
                              kernel_shape, batch_size, subsample=(1, 1),
                              border_mode='valid', message="", rng=None):
    """
    .. todo::

        WRITEME properly

    Creates a Conv2D with random kernels, where the randomly initialized
    values are sparse
    """

    raise AssertionError(
        "TODO: I think this is a bug--num_nonzero "
        "determines the total number of nonzero elements in the "
        "whole kernel stack, not the number of non-zero elements per "
        "kernel. Investigate what it's meant to do."
    )

    rng = make_np_rng(rng, default_sparse_seed,
                      which_method=['randn', 'randint'])

    W = np.zeros((output_space.num_channels, input_space.num_channels,
                  kernel_shape[0], kernel_shape[1]))

    def random_coord():
        return [rng.randint(dim) for dim in W.shape]

    for i in xrange(num_nonzero):
        o, ch, r, c = random_coord()
        while W[o, ch, r, c] != 0:
            o, ch, r, c = random_coord()
        W[o, ch, r, c] = rng.randn()

    W = sharedX(W)

    return Conv2D(
        filters=W,
        batch_size=batch_size,
        input_space=input_space,
        output_axes=output_space.axes,
        subsample=subsample, border_mode=border_mode,
        filters_shape=W.get_value(borrow=True).shape, message=message
    )

########NEW FILE########
__FILENAME__ = conv2d_c01b
"""
The functionality in this module is very similar to that in
pylearn2.linear.conv2d. The difference is that this module is
based on Alex Krizhevsky's cuda-convnet convolution, while
pylearn2.linear.conv2d is based on theano's 2D convolution.
This module therefore uses the axis format ('c', 0, 1, 'b')
as its native format, while the other uses ('b', 'c', 0, 1).
This module also requires the use of GPU, while the other
supports CPU.
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

import functools
import logging
import numpy as np
import warnings

from theano.compat.python2x import OrderedDict
from theano.sandbox import cuda
import theano.tensor as T

if cuda.cuda_available:
    from theano.sandbox.cuda.basic_ops import gpu_contiguous
    from theano.sandbox.cuda import gpu_from_host
    from theano.sandbox.cuda import host_from_gpu

from pylearn2.linear.conv2d import default_seed, default_sparse_seed
from pylearn2.linear.linear_transform import LinearTransform
from pylearn2.sandbox.cuda_convnet import check_cuda
from pylearn2.sandbox.cuda_convnet.filter_acts import FilterActs
from pylearn2.sandbox.cuda_convnet.filter_acts import ImageActs
from pylearn2.space import Conv2DSpace
from pylearn2.utils.call_check import checked_call
from pylearn2.utils import sharedX
from pylearn2.utils.rng import make_np_rng


logger = logging.getLogger(__name__)


class Conv2D(LinearTransform):
    """
    A pylearn2 linear operator based on 2D convolution,
    implemented using Alex Krizhevsky's cuda-convnet library.

    Parameters
    ----------
    filters : Theano shared variable
        4-tensor of shape (in channels, rows, cols, out channels)
    input_axes : WRITEME
    batch_size : WRITEME
    output_axes : WRITEME
    kernel_stride : WRITEME
    pad : WRITEME
    message : WRITEME
    partial_sum : WRITEME
    """

    def __init__(self, filters, input_axes=('c', 0, 1, 'b'),
                 batch_size=None, output_axes=('c', 0, 1, 'b'),
                 kernel_stride=(1, 1), pad=0, message='',
                 partial_sum=None):
        if len(kernel_stride) != 2:
            raise ValueError("kernel_stride must have length 2")
        elif kernel_stride[0] != kernel_stride[1]:
            raise ValueError("only values of kernel_stride with both "
                             "elements equal are supported currently")
        if message != '':
            raise NotImplementedError()

        if batch_size is not None:
            raise NotImplementedError()

        self.input_axes = input_axes
        self.output_axes = output_axes

        # filters should be a GPU shared variable.
        # I guess you could GpuFromHost them every time,
        # but if you're using this class you probably care
        # about performance and want to be at least warned
        # that this is happening
        assert hasattr(filters, 'get_value')
        assert 'Cuda' in str(type(filters))
        self._filters = filters
        self.pad = pad
        self.partial_sum = partial_sum
        self.kernel_stride = kernel_stride

    @functools.wraps(LinearTransform.get_params)
    def get_params(self):
        """
        .. todo::

            WRITEME
        """
        return [self._filters]

    @functools.wraps(LinearTransform.get_weights_topo)
    def get_weights_topo(self, borrow=False):
        """
        .. todo::

            WRITEME
        """
        inp, rows, cols, outp = range(4)
        raw = self._filters.get_value(borrow=borrow)
        return np.transpose(raw, (outp, rows, cols, inp))

    def lmul(self, x):
        """
        .. todo::

            WRITEME properly

        dot(x, A)
        aka, do convolution with input image x
        """

        check_cuda(str(type(self)) + ".lmul")

        cpu = 'Cuda' not in str(type(x))

        if cpu:
            x = gpu_from_host(x)

        # x must be formatted as channel, topo dim 0, topo dim 1, batch_index
        # for use with FilterActs
        assert x.ndim == 4
        x_axes = self.input_axes
        assert len(x_axes) == 4

        op_axes = ('c', 0, 1, 'b')

        if tuple(x_axes) != op_axes:
            x = x.dimshuffle(*[x_axes.index(axis) for axis in x_axes])

        x = gpu_contiguous(x)

        # Patch old pickle files.
        if not hasattr(self, 'kernel_stride'):
            self.kernel_stride = (1, 1)
        rval = FilterActs(self.pad, self.partial_sum, self.kernel_stride[0])(
            x,
            self._filters
        )

        # Format the output based on the output space
        rval_axes = self.output_axes
        assert len(rval_axes) == 4

        if cpu:
            rval = host_from_gpu(rval)

        if tuple(rval_axes) != op_axes:
            rval = rval.dimshuffle(*[op_axes.index(axis)
                                     for axis in rval_axes])

        return rval

    def lmul_T(self, x):
        """
        .. todo::

            WRITEME
        """

        check_cuda(str(type(self)) + ".lmul_T")

        assert x.dtype == self._filters.dtype

        op_axes = ('c', 0, 1, 'b')
        axes = self.output_axes
        if tuple(axes) != op_axes:
            x = x.dimshuffle(*[axes.index(ax) for ax in op_axes])

        x = gpu_contiguous(x)

        rval = ImageActs(pad=self.pad, partial_sum=self.partial_sum,
                         stride=self.kernel_stride[0])(x, self._filters)

        # Format the output based on the input space
        axes = self.input_axes
        assert len(axes) == 4

        if tuple(axes) != op_axes:
            rval = rval.dimshuffle(op_axes.index(axes[0]),
                                   op_axes.index(axes[1]),
                                   op_axes.index(axes[2]),
                                   op_axes.index(axes[3]))

        return rval

    def lmul_sq_T(self, x):
        """
        .. todo::

            WRITEME
        """
        raise NotImplementedError("This method is not yet modified since "
                                  "copy-pasting from pylearn2.linear.conv2d")
        """ Kind of a stupid hacky method used to support convolutional score
        matching. Ought to find a way to make _filters symbolic rather than
        shared.
        """
        assert x.dtype == self._filters.dtype

        op_axes = ('b', 'c', 0, 1)
        axes = self.output_axes
        if tuple(axes) != op_axes:
            x = x.dimshuffle(axes.index('b'), axes.index('c'),
                             axes.index(0), axes.index(1))

        # dot(x, sq(A).T)
        dummy_v = T.tensor4()
        sqfilt = T.square(self._filters)
        z_hs = 0.  # conv2d(dummy_v, sqfilt,
                   # image_shape=self._img_shape,
                   # filter_shape=self._filters_shape,
                   # kernel_stride=self._kernel_stride,
                   # pad = self.pad
                   # )
        rval, xdummy = z_hs.owner.op.grad((dummy_v, sqfilt), (x,))

        # Format the output based on the input space
        axes = self.input_space.axes
        assert len(axes) == 4

        if tuple(axes) != op_axes:
            rval = rval.dimshuffle(op_axes.index(axes[0]),
                                   op_axes.index(axes[1]),
                                   op_axes.index(axes[2]),
                                   op_axes.index(axes[3]))

        return rval

    def set_batch_size(self, batch_size):
        """
        .. todo::

            WRITEME
        """
        pass


def make_random_conv2D(irange, input_channels, input_axes, output_axes,
                       output_channels, kernel_shape, kernel_stride=(1, 1),
                       pad=0, message="", rng=None, partial_sum=None,
                       sparse_init=None):
    """
    .. todo::

        WRITEME properly

    Creates a Conv2D with random kernels.
    Should be functionally equivalent to
    pylearn2.linear.conv2d.make_random_conv2D
    """

    rng = make_np_rng(rng, default_seed, which_method='uniform')

    W = sharedX(rng.uniform(-irange, irange,
                            (input_channels, kernel_shape[0],
                             kernel_shape[1], output_channels)))

    return Conv2D(filters=W, input_axes=input_axes, output_axes=output_axes,
                  kernel_stride=kernel_stride, pad=pad, message=message,
                  partial_sum=partial_sum)


def make_sparse_random_conv2D(num_nonzero, input_space, output_space,
                              kernel_shape, pad=0, kernel_stride=(1, 1),
                              border_mode='valid', message="", rng=None,
                              partial_sum=None):
    """
    .. todo::

        WRITEME properly

    Creates a Conv2D with random kernels, where the randomly initialized
    values are sparse
    """

    rng = make_np_rng(rng, default_sparse_seed,
                      which_method=['randn', 'randint'])

    W = np.zeros((input_space.num_channels, kernel_shape[0],
                  kernel_shape[1], output_space.num_channels))

    def random_coord():
        return [rng.randint(dim) for dim in W.shape[0:3]]

    for o in xrange(output_space.num_channels):
        for i in xrange(num_nonzero):
            ch, r, c = random_coord()
            while W[ch, r, c, o] != 0:
                ch, r, c = random_coord()
            W[ch, r, c, o] = rng.randn()

    W = sharedX(W)

    return Conv2D(filters=W, input_axes=input_space.axes,
                  output_axes=output_space.axes, kernel_stride=kernel_stride,
                  pad=pad, message=message, partial_sum=partial_sum)


def setup_detector_layer_c01b(layer, input_space, rng, irange="not specified"):
    """
    .. todo::

        WRITEME properly

    Takes steps to set up an object for use as being some kind of convolutional
    layer. This function sets up only the detector layer.

    Does the following:

    * raises a RuntimeError if cuda is not available
    * sets layer.input_space to input_space
    * sets up addition of dummy channels for compatibility with cuda-convnet:

      - layer.dummy_channels: # of dummy channels that need to be added
        (You might want to check this and raise an Exception if it's not 0)
      - layer.dummy_space: The Conv2DSpace representing the input with dummy
        channels added

    * sets layer.detector_space to the space for the detector layer
    * sets layer.transformer to be a Conv2D instance
    * sets layer.b to the right value

    Parameters
    ----------
    layer : object
        Any python object that allows the modifications described below and
        has the following attributes:

          * pad : int describing amount of zero padding to add
          * kernel_shape : 2-element tuple or list describing spatial shape of
            kernel
          * fix_kernel_shape : bool, if true, will shrink the kernel shape to
            make it feasible, as needed (useful for hyperparameter searchers)
          * detector_channels : The number of channels in the detector layer
          * init_bias : numeric constant added to a tensor of zeros to
            initialize the bias
          * tied_b : If true, biases are shared across all spatial locations
    input_space : WRITEME
        A Conv2DSpace to be used as input to the layer
    rng : WRITEME
        A numpy RandomState or equivalent
    """

    if irange != "not specified":
        raise AssertionError(
            "There was a bug in setup_detector_layer_c01b."
            "It uses layer.irange instead of the irange parameter to the "
            "function. The irange parameter is now disabled by this "
            "AssertionError, so that this error message can alert you that "
            "the bug affected your code and explain why the interface is "
            "changing. The irange parameter to the function and this "
            "error message may be removed after April 21, 2014."
        )

    # Use "self" to refer to layer from now on, so we can pretend we're
    # just running in the set_input_space method of the layer
    self = layer

    # Make sure cuda is available
    check_cuda(str(type(self)))

    # Validate input
    if not isinstance(input_space, Conv2DSpace):
        raise TypeError("The input to a convolutional layer should be a "
                        "Conv2DSpace, but layer " + self.layer_name + " got " +
                        str(type(self.input_space)))

    if not hasattr(self, 'detector_channels'):
        raise ValueError("layer argument must have a 'detector_channels' "
                         "attribute specifying how many channels to put in "
                         "the convolution kernel stack.")

    # Store the input space
    self.input_space = input_space

    # Make sure number of channels is supported by cuda-convnet
    # (multiple of 4 or <= 3)
    # If not supported, pad the input with dummy channels
    ch = self.input_space.num_channels
    rem = ch % 4
    if ch > 3 and rem != 0:
        self.dummy_channels = 4 - rem
    else:
        self.dummy_channels = 0
    self.dummy_space = Conv2DSpace(
        shape=input_space.shape,
        channels=input_space.num_channels + self.dummy_channels,
        axes=('c', 0, 1, 'b')
    )

    if hasattr(self, 'kernel_stride'):
        kernel_stride = self.kernel_stride
    else:
        kernel_stride = [1, 1]

    output_shape = \
        [int(np.ceil((i_sh + 2. * self.pad - k_sh) / float(k_st))) + 1
         for i_sh, k_sh, k_st in zip(self.input_space.shape,
                                     self.kernel_shape, kernel_stride)]

    def handle_kernel_shape(idx):
        if self.kernel_shape[idx] < 1:
            raise ValueError("kernel must have strictly positive size on all "
                             "axes but has shape: " + str(self.kernel_shape))
        if output_shape[idx] <= 0:
            if self.fix_kernel_shape:
                self.kernel_shape[idx] = \
                    self.input_space.shape[idx] + 2 * self.pad
                assert self.kernel_shape[idx] != 0
                output_shape[idx] = 1
                warnings.warn("Had to change the kernel shape to make "
                              "network feasible")
            else:
                raise ValueError("kernel too big for input "
                                 "(even with zero padding)")

    map(handle_kernel_shape, [0, 1])

    if self.detector_channels < 16:
        raise ValueError("Cuda-convnet requires the detector layer to have "
                         "at least 16 channels.")

    self.detector_space = Conv2DSpace(shape=output_shape,
                                      num_channels=self.detector_channels,
                                      axes=('c', 0, 1, 'b'))

    if hasattr(self, 'partial_sum'):
        partial_sum = self.partial_sum
    else:
        partial_sum = 1

    if hasattr(self, 'sparse_init') and self.sparse_init is not None:
        self.transformer = \
            checked_call(make_sparse_random_conv2D,
                         OrderedDict([('num_nonzero', self.sparse_init),
                                      ('input_space', self.input_space),
                                      ('output_space', self.detector_space),
                                      ('kernel_shape', self.kernel_shape),
                                      ('pad', self.pad),
                                      ('partial_sum', partial_sum),
                                      ('kernel_stride', kernel_stride),
                                      ('rng', rng)]))
    else:
        self.transformer = make_random_conv2D(
            irange=self.irange,
            input_axes=self.input_space.axes,
            output_axes=self.detector_space.axes,
            input_channels=self.dummy_space.num_channels,
            output_channels=self.detector_space.num_channels,
            kernel_shape=self.kernel_shape,
            pad=self.pad,
            partial_sum=partial_sum,
            kernel_stride=kernel_stride,
            rng=rng
        )

    W, = self.transformer.get_params()
    W.name = self.layer_name + '_W'

    if self.tied_b:
        self.b = sharedX(np.zeros(self.detector_space.num_channels) +
                         self.init_bias)
    else:
        self.b = sharedX(self.detector_space.get_origin() + self.init_bias)
    self.b.name = self.layer_name + '_b'

    logger.info('Input shape: {0}'.format(self.input_space.shape))
    logger.info('Detector space: {0}'.format(self.detector_space.shape))

########NEW FILE########
__FILENAME__ = linear_transform
"""
.. todo::

    WRITEME
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

class LinearTransform(object):
    """
    A generic class describing a LinearTransform. Derived classes may implement linear
    transformation as a dense matrix multiply, a convolution, etc.

    Classes inheriting from this should also inherit from TheanoLinear's LinearTransform
    This class does not directly inherit from TheanoLinear's LinearTransform because
    most LinearTransform classes in pylearn2 will inherit from a TheanoLinear derived
    class and don't want to end up inheriting from TheanoLinear by two paths

    This class is basically just here as a placeholder to show you what extra methods you
    need to add to make a TheanoLinear LinearTransform work with pylearn2
    """

    def get_params(self):
        """
        Return a list of parameters that govern the linear transformation
        """

        raise NotImplementedError()

    def get_weights_topo(self):
        """
        Return a batch of filters, formatted topologically.
        This only really makes sense if you are working with a topological space,
        such as for a convolution operator.

        If your transformation is defined on a VectorSpace then some other class
        like a ViewConverter will need to transform your vector into a topological
        space; you are not responsible for doing so here.
        """

        raise NotImplementedError()

    def set_batch_size(self, batch_size):
        """
        Some transformers such as Conv2D have a fixed batch size.
        Use this method to change the batch size.

        Parameters
        ----------
        batch_size : int
            The size of the batch
        """
        pass

########NEW FILE########
__FILENAME__ = local_c01b
"""
This module is based on Alex Krizhevsky's cuda-convnet locally connected layers.
pylearn2.linear.conv2d is based on theano's 2D convolution.
This module therefore uses images with axis format ('c', 0, 1, 'b')
as its native format.
Unlike the other cuda-convnet functionality in pylearn2, this linear transform
has CPU support, provided by TheanoLinear.
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

import logging
import numpy as np
import warnings

from pylearn2.packaged_dependencies.theano_linear.unshared_conv.localdot import LocalDot

from pylearn2.utils import sharedX
from pylearn2.utils.rng import make_np_rng
from pylearn2.linear.conv2d import default_seed, default_sparse_seed
from pylearn2.linear.linear_transform import LinearTransform


logger = logging.getLogger(__name__)


class Local(LinearTransform, LocalDot):
    """
    A pylearn2 linear operator based on local receptive fields (convolution
    without parameter sharing) implemented using Alex Krizhevsky's cuda-convnet
    library + James Bergstra's TheanoLinear module

    Parameters
    ----------
    filters : WRITEME
    image_shape : WRITEME
    input_groups : WRITEME
    input_axes : WRITEME
    batch_size : WRITEME
    output_axes : WRITEME
    kernel_stride : WRITEME
    pad : WRITEME
    message : WRITEME
    partial_sum : WRITEME
    """

    def __init__(self, filters, image_shape, input_groups,
                 input_axes=('c', 0, 1, 'b'), batch_size=None,
                 output_axes=('c', 0, 1, 'b'), kernel_stride=(1, 1), pad=0,
                 message='', partial_sum=None):
        self.input_groups = input_groups

        """TODO: Local ignores partial_sum argument,
                 figure out how James' code controls it"""

        logger.warning("partial_sum argument ignored")

        LocalDot.__init__(self,
            filters=filters,
            irows=image_shape[0],
            icols=image_shape[1],
            subsample=kernel_stride,
            padding_start=pad,
            message='')


    def lmul(self, x):
        """
        .. todo::

            WRITEME
        """

        reshaped = x.reshape(( self.input_groups, x.shape[0] / self.input_groups, x.shape[1], x.shape[2], x.shape[3]))

        out = LocalDot.rmul(self, reshaped)

        return out.reshape((out.shape[0] * out.shape[1], out.shape[2], out.shape[3], out.shape[4]))

    def get_params(self):
        """
        .. todo::

            WRITEME
        """
        return [self._filters]


def make_random_local(irange, input_channels, input_axes, input_groups,
        image_shape,
        output_channels,
        output_axes,
        kernel_shape,
        kernel_stride = (1,1), pad=0, message = "", rng = None,
        partial_sum = None):
    """
    .. todo::

        WRITEME properly

    Creates a Local with random weights.
    """

    rng = make_np_rng(rng, default_seed, which_method='uniform')

    def num_pos(img, stride, kwidth):
        img = img + 2 * pad
        return (img - kwidth) // stride + 1

    num_row_pos = num_pos(image_shape[0], kernel_stride[0], kernel_shape[0])
    num_col_pos = num_pos(image_shape[1], kernel_stride[1], kernel_shape[1])

    assert input_channels % input_groups == 0
    colors_per_group = input_channels // input_groups
    assert output_channels % input_groups == 0
    filters_per_group = output_channels // input_groups

    W = sharedX( rng.uniform(-irange,irange,
        (num_row_pos, num_col_pos, colors_per_group, kernel_shape[0], kernel_shape[1], input_groups,
            filters_per_group)))

    return Local(filters = W,
        image_shape = image_shape,
        input_groups = input_groups,
        input_axes = input_axes,
        output_axes = output_axes,
        kernel_stride = kernel_stride, pad=pad,
        message = message, partial_sum=partial_sum)


def make_sparse_random_local(num_nonzero, input_space, output_space,
        kernel_shape, batch_size, \
        kernel_stride = (1,1), border_mode = 'valid', message = "", rng=None):
    """
    .. todo::

        WRITEME
    """
    raise NotImplementedError("Not yet modified after copy-paste from "
            "pylearn2.linear.conv2d_c01b")
    """ Creates a Conv2D with random kernels, where the randomly initialized
    values are sparse"""

    rng = make_np_rng(rng, default_sparse_seed, which_method=['randn','randint'])

    W = np.zeros(( output_space.num_channels, input_space.num_channels, \
            kernel_shape[0], kernel_shape[1]))

    def random_coord():
        return [ rng.randint(dim) for dim in W.shape ]

    for i in xrange(num_nonzero):
        o, ch, r, c = random_coord()
        while W[o, ch, r, c] != 0:
            o, ch, r, c = random_coord()
        W[o, ch, r, c] = rng.randn()


    W = sharedX( W)

    #return Conv2D(filters = W,
    #    batch_size = batch_size,
    #    input_space = input_space,
    #    output_axes = output_space.axes,
    #    kernel_stride = kernel_stride, border_mode = border_mode,
    #    filters_shape = W.get_value(borrow=True).shape, message = message)

########NEW FILE########
__FILENAME__ = matrixmul
"""
.. todo::

    WRITEME
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

from theano import tensor as T

from pylearn2.linear.linear_transform import LinearTransform
import functools
import numpy as np
from pylearn2.utils import sharedX
from pylearn2.utils.rng import make_np_rng


class MatrixMul(LinearTransform):
    """
    The most basic LinearTransform: matrix multiplication. See TheanoLinear
    for more documentation.

    Note: this does not inherit from the TheanoLinear's MatrixMul.

    The TheanoLinear version does a bunch of extra undocumented reshaping,
    concatenating, etc. that looks like it's probably meant to allow converting
    between Spaces without warning. Since the reshape and concatenate
    operations
    are always inserted whether they're needed or not, this can cause annoying
    things like the reshape breaking if you change the shape of W, bugs in
    Theano's optimization system being harder to avoid, etc.

    Parameters
    ----------
    W : WRITEME
    """

    def __init__(self, W):
        """
        Sets the initial values of the matrix
        """
        self._W = W

    @functools.wraps(LinearTransform.get_params)
    def get_params(self):
        """
        .. todo::

            WRITEME
        """
        return [self._W]

    def lmul(self, x):
        """
        .. todo::

            WRITEME

        Parameters
        ----------
        x : ndarray, 1d or 2d
            The input data
        """

        return T.dot(x, self._W)

    def lmul_T(self, x):
        """
        .. todo::

            WRITEME

        Parameters
        ----------
        x : ndarray, 1d or 2d
            The input data
        """
        return T.dot(x, self._W.T)


def make_local_rfs(dataset, nhid, rf_shape, stride, irange = .05,
        draw_patches = False, rng = None):
    """
    Initializes a weight matrix with local receptive fields

    Parameters
    ----------
    dataset : pylearn2.datasets.dataset.Dataset
        Dataset defining the topology of the space (needed to convert 2D
        patches into subsets of pixels in a 1D filter vector)
    nhid : int
        Number of hidden units to make filters for
    rf_shape : list or tuple (2 elements)
        Gives topological shape of a receptive field
    stride : list or tuple (2 elements)
        Gives offset between receptive fields
    irange : float
        If draw_patches is False, weights are initialized in U(-irange,irange)
    draw_patches : bool
        If True, weights are drawn from random examples

    Returns
    -------
    weights : ndarray
        2D ndarray containing the desired weights.
    """
    s = dataset.view_shape()
    height, width, channels = s
    W_img = np.zeros( (nhid, height, width, channels) )

    last_row = s[0] - rf_shape[0]
    last_col = s[1] - rf_shape[1]

    rng = make_np_rng(rng, [2012,07,18], which_method='uniform')


    if stride is not None:
        # local_rf_stride specified, make local_rfs on a grid
        assert last_row % stride[0] == 0
        num_row_steps = last_row / stride[0] + 1

        assert last_col % stride[1] == 0
        num_col_steps = last_col /stride[1] + 1

        total_rfs = num_row_steps * num_col_steps

        if nhid % total_rfs != 0:
            raise ValueError('nhid modulo total_rfs should be 0, but we get '
                    '%d modulo %d = %d' % (nhid, total_rfs, nhid % total_rfs))

        filters_per_rf = nhid / total_rfs

        idx = 0
        for r in xrange(num_row_steps):
            rc = r * stride[0]
            for c in xrange(num_col_steps):
                cc = c * stride[1]

                for i in xrange(filters_per_rf):

                    if draw_patches:
                        img = dataset.get_batch_topo(1)[0]
                        local_rf = img[rc:rc+rf_shape[0],
                                       cc:cc+rf_shape[1],
                                       :]
                    else:
                        local_rf = rng.uniform(-irange,
                                    irange,
                                    (rf_shape[0], rf_shape[1], s[2]) )



                    W_img[idx,rc:rc+rf_shape[0],
                      cc:cc+rf_shape[1],:] = local_rf
                    idx += 1
        assert idx == nhid
    else:
        raise NotImplementedError()
        #the case below is copy-pasted from s3c and not generalized yet
        #no stride specified, use random shaped patches
        """
        assert local_rf_max_shape is not None

        for idx in xrange(nhid):
            shape = [ self.rng.randint(min_shape,max_shape+1) for
                    min_shape, max_shape in zip(
                        local_rf_shape,
                        local_rf_max_shape) ]
            loc = [ self.rng.randint(0, bound - width + 1) for
                    bound, width in zip(s, shape) ]

            rc, cc = loc

            if local_rf_draw_patches:
                img = local_rf_src.get_batch_topo(1)[0]
                local_rf = img[rc:rc+shape[0],
                               cc:cc+shape[1],
                               :]
            else:
                local_rf = self.rng.uniform(-self.irange,
                            self.irange,
                            (shape[0], shape[1], s[2]) )

            W_img[idx,rc:rc+shape[0],
                      cc:cc+shape[1],:] = local_rf
        """


    W = dataset.view_converter.topo_view_to_design_mat(W_img).T

    rval = MatrixMul(W = sharedX(W))

    return rval

########NEW FILE########
__FILENAME__ = test_conv2d
import theano
from theano import tensor
import numpy
from pylearn2.linear.conv2d import Conv2D, make_random_conv2D
from pylearn2.space import Conv2DSpace
from pylearn2.utils import sharedX
import unittest
try:
    scipy_available = True
    import scipy.ndimage
except:
    scipy_available = False


class TestConv2D(unittest.TestCase):
    """
    Tests for Conv2D code
    """
    def setUp(self):
        """
        Set up a test image and filter to re-use
        """
        self.image = numpy.random.rand(1, 3, 3, 1).astype(theano.config.floatX)
        self.image_tensor = tensor.tensor4()
        self.input_space = Conv2DSpace((3, 3), 1)
        self.filters_values = numpy.ones(
            (1, 1, 2, 2), dtype=theano.config.floatX
        )
        self.filters = sharedX(self.filters_values, name='filters')
        self.conv2d = Conv2D(self.filters, 1, self.input_space)

    def test_value_errors(self):
        """
        Check correct errors are raised when bad input is given
        """
        bad_filters = sharedX(numpy.zeros((1, 3, 2)))
        self.assertRaises(ValueError, Conv2D, bad_filters, 1, self.input_space)
        self.assertRaises(AssertionError, Conv2D, self.filters, 0,
                          self.input_space)

    def test_get_params(self):
        """
        Check whether the conv2d has stored the correct filters
        """
        assert self.conv2d.get_params() == [self.filters]

    def test_lmul(self):
        """
        Use SciPy's ndimage to check whether the convolution worked
        correctly
        """
        f = theano.function([self.image_tensor],
                            self.conv2d.lmul(self.image_tensor))
        if scipy_available:
            numpy.allclose(
                f(self.image).reshape((2, 2)),
                scipy.ndimage.filters.convolve(
                    self.image.reshape((3, 3)),
                    self.filters_values.reshape((2, 2))
                )[:2, :2]
            )

    def test_lmul_T(self):
        """
        Check whether this function outputs the right shape
        """
        conv2d = self.conv2d.lmul(self.image_tensor)
        f = theano.function([self.image_tensor],
                            self.conv2d.lmul_T(conv2d))
        assert f(self.image).shape == self.image.shape

    def test_lmul_sq_T(self):
        """
        Check whether this function outputs the same values as when
        taking the square manually
        """
        conv2d_sq = Conv2D(sharedX(numpy.square(self.filters_values)),
            1, self.input_space
        ).lmul(self.image_tensor)
        conv2d = self.conv2d.lmul(self.image_tensor)
        f = theano.function([self.image_tensor],
                            self.conv2d.lmul_T(conv2d_sq))
        f2 = theano.function([self.image_tensor],
                             self.conv2d.lmul_sq_T(conv2d))

        numpy.testing.assert_allclose(f(self.image), f2(self.image))

    def test_set_batch_size(self):
        """
        Make sure that setting the batch size actually changes the property
        """
        cur_img_shape = self.conv2d._img_shape
        cur_batch_size = self.conv2d._img_shape[0]
        self.conv2d.set_batch_size(cur_batch_size + 10)
        assert self.conv2d._img_shape[0] == cur_batch_size + 10
        assert self.conv2d._img_shape[1:] == cur_img_shape[1:]

    def test_axes(self):
        """
        Use different output axes and see whether the output is what we
        expect
        """
        default_axes = ('b', 0, 1, 'c')
        axes = (0, 'b', 1, 'c')
        mapping = tuple(axes.index(axis) for axis in default_axes)
        input_space = Conv2DSpace((3, 3), num_channels=1, axes=axes)
        conv2d = Conv2D(self.filters, 1, input_space, output_axes=axes)
        f_axes = theano.function([self.image_tensor],
                                 conv2d.lmul(self.image_tensor))
        f = theano.function([self.image_tensor],
                            self.conv2d.lmul(self.image_tensor))
        output_axes = f_axes(numpy.transpose(self.image, mapping))
        output = f(self.image)
        output_axes = numpy.transpose(output_axes, mapping)
        numpy.testing.assert_allclose(output, output_axes)
        assert output.shape == output_axes.shape

    def test_channels(self):
        """
        Go from 2 to 3 channels and see whether the shape is correct
        """
        input_space = Conv2DSpace((3, 3), num_channels=3)
        filters_values = numpy.ones(
            (2, 3, 2, 2), dtype=theano.config.floatX
        )
        filters = sharedX(filters_values)
        image = numpy.random.rand(1, 3, 3, 3).astype(theano.config.floatX)
        conv2d = Conv2D(filters, 1, input_space)
        f = theano.function([self.image_tensor],
                            conv2d.lmul(self.image_tensor))
        assert f(image).shape == (1, 2, 2, 2)

    def test_make_random_conv2D(self):
        """
        Create a random convolution and check whether the shape, axes and
        input space are all what we expect
        """
        output_space = Conv2DSpace((2, 2), 1)
        conv2d = make_random_conv2D(1, self.input_space, output_space,
                                    (2, 2), 1)
        f = theano.function([self.image_tensor],
                            conv2d.lmul(self.image_tensor))
        assert f(self.image).shape == (1, 2, 2, 1)
        assert conv2d.input_space == self.input_space
        assert conv2d.output_axes == output_space.axes

########NEW FILE########
__FILENAME__ = test_conv2d_c01b
import theano
from theano import tensor
import numpy
from pylearn2.linear.conv2d_c01b import (Conv2D, make_random_conv2D,
    make_sparse_random_conv2D, setup_detector_layer_c01b)
from pylearn2.space import Conv2DSpace
from pylearn2.utils import sharedX
from pylearn2.testing.skip import skip_if_no_gpu
from pylearn2.models.maxout import MaxoutConvC01B
from pylearn2.models.mlp import MLP
skip_if_no_gpu()
import unittest
try:
    scipy_available = True
    import scipy.ndimage
except:
    scipy_available = False


class TestConv2DC01b(unittest.TestCase):
    """
    Tests for Alex Krizhevsky's Conv2D code
    """
    def setUp(self):
        """
        Set up a test image and filter to re-use
        """
        self.image = \
            numpy.random.rand(16, 3, 3, 1).astype(theano.config.floatX)
        self.image_tensor = tensor.tensor4()
        self.filters_values = numpy.ones(
            (16, 2, 2, 16), dtype=theano.config.floatX
        )
        self.filters = sharedX(self.filters_values, name='filters')
        self.conv2d = Conv2D(self.filters)

    def test_get_params(self):
        """
        Check whether the conv2d has stored the correct filters
        """
        assert self.conv2d.get_params() == [self.filters]

    def test_lmul(self):
        """
        Use SciPy's ndimage to check whether the convolution worked
        correctly
        """
        f = theano.function([self.image_tensor],
                            self.conv2d.lmul(self.image_tensor))
        if scipy_available:
            numpy.allclose(
                f(self.image).reshape((16, 2, 2)),
                scipy.ndimage.filters.convolve(
                    self.image.reshape((16, 3, 3, 1)),
                    self.filters_values.reshape((16, 2, 2, 16))
                )[:2, :2]
            )

    def test_lmul_T(self):
        """
        Check whether this function outputs the right shape
        """
        conv2d = self.conv2d.lmul(self.image_tensor)
        f = theano.function([self.image_tensor],
                            self.conv2d.lmul_T(conv2d))
        assert f(self.image).shape == self.image.shape

    def test_axes(self):
        """
        Use custom output axes and check whether it worked
        """
        default_axes = ('c', 0, 1, 'b')
        axes = (0, 'b', 1, 'c')
        mapping = tuple(axes.index(axis) for axis in default_axes)
        conv2d = Conv2D(self.filters, output_axes=axes)
        f_axes = theano.function([self.image_tensor],
                                 conv2d.lmul(self.image_tensor))
        f = theano.function([self.image_tensor],
                            self.conv2d.lmul(self.image_tensor))
        output_axes = f_axes(self.image)
        output = f(self.image)
        output_axes = numpy.transpose(output_axes, mapping)
        numpy.testing.assert_allclose(output, output_axes)
        assert output.shape == output_axes.shape

    def test_channels(self):
        """
        Go from 32 to 16 channels and see whether that works without error
        """
        filters_values = numpy.ones(
            (32, 2, 2, 16), dtype=theano.config.floatX
        )
        filters = sharedX(filters_values)
        image = numpy.random.rand(32, 3, 3, 1).astype(theano.config.floatX)
        conv2d = Conv2D(filters)
        f = theano.function([self.image_tensor],
                            conv2d.lmul(self.image_tensor))
        assert f(image).shape == (16, 2, 2, 1)

    def test_make_random_conv2D(self):
        """
        Make random filters
        """
        default_axes = ('c', 0, 1, 'b')
        conv2d = make_random_conv2D(1, 16, default_axes, default_axes,
                                    16, (2, 2))
        f = theano.function([self.image_tensor],
                            conv2d.lmul(self.image_tensor))
        assert f(self.image).shape == (16, 2, 2, 1)
        assert conv2d.output_axes == default_axes

    def test_make_sparse_random_conv2D(self):
        """
        Make random sparse filters, count whether the number of
        non-zero elements is sensible
        """
        axes = ('c', 0, 1, 'b')
        input_space = Conv2DSpace((3, 3), 16, axes=axes)
        output_space = Conv2DSpace((3, 3), 16, axes=axes)
        num_nonzero = 2
        kernel_shape = (2, 2)

        conv2d = make_sparse_random_conv2D(num_nonzero, input_space,
                                           output_space, kernel_shape)
        f = theano.function([self.image_tensor],
                            conv2d.lmul(self.image_tensor))
        assert f(self.image).shape == (16, 2, 2, 1)
        assert conv2d.output_axes == axes
        assert numpy.count_nonzero(conv2d._filters.get_value()) >= 32

    def test_setup_detector_layer_c01b(self):
        """
        Very basic test to see whether a detector layer can be set up
        without error. Not checking much for the actual output.
        """
        axes = ('c', 0, 1, 'b')
        layer = MaxoutConvC01B(16, 2, (2, 2), (2, 2),
                               (1, 1), 'maxout', irange=1.)
        input_space = Conv2DSpace((3, 3), 16, axes=axes)
        MLP(layers=[layer], input_space=input_space)
        layer.set_input_space(input_space)
        assert isinstance(layer.input_space, Conv2DSpace)
        input = theano.tensor.tensor4()
        f = theano.function([input], layer.fprop(input))
        f(numpy.random.rand(16, 3, 3, 1).astype(theano.config.floatX))

########NEW FILE########
__FILENAME__ = test_local_c01b
import theano
from theano import tensor
import numpy
from pylearn2.linear.local_c01b import Local, make_random_local
from pylearn2.utils import sharedX
from pylearn2.testing.skip import skip_if_no_gpu
import unittest


class TestConv2DC01b(unittest.TestCase):
    """
    Test for local receptive fields
    """
    def setUp(self):
        """
        Set up a test image and filter to re-use
        """
        skip_if_no_gpu()
        self.image = \
            numpy.random.rand(16, 3, 3, 1).astype(theano.config.floatX)
        self.image_tensor = tensor.tensor4()
        self.filters_values = numpy.ones(
            (2, 2, 16, 2, 2, 1, 16), dtype=theano.config.floatX
        )
        self.filters = sharedX(self.filters_values, name='filters')
        self.local = Local(self.filters, (3, 3), 1)

    def test_get_params(self):
        """
        Check whether the local receptive field has stored the correct filters
        """
        assert self.local.get_params() == [self.filters]

    def test_lmul(self):
        """
        Make sure the shape of the output is correct
        """
        f = theano.function([self.image_tensor],
                            self.local.lmul(self.image_tensor))
        assert f(self.image).shape == (16, 2, 2, 1)

    def test_make_random_local(self):
        """
        Create random local receptive fields and check whether they can be
        applied and give a sensible output shape
        """
        local = make_random_local(1, 16, ('c', 0, 1, 'b'), 1, (3, 3),
                                  16, ('c', 0, 1, 'b'), (2, 2))
        f = theano.function([self.image_tensor],
                            local.lmul(self.image_tensor))
        assert f(self.image).shape == (16, 2, 2, 1)

########NEW FILE########
__FILENAME__ = test_matrixmul
from pylearn2.linear.matrixmul import MatrixMul, make_local_rfs
from pylearn2.datasets.dense_design_matrix import DefaultViewConverter
from pylearn2.datasets.dense_design_matrix import DenseDesignMatrix
import theano
from theano import tensor
import numpy as np


def test_matrixmul():
    """
    Tests matrix multiplication for a range of different
    dtypes. Checks both normal and transpose multiplication
    using randomly generated matrices.
    """
    rng = np.random.RandomState(222)
    dtypes = [
        'int16', 'int32', 'int64', 'float64', 'float32'
    ]
    tensor_x = [
        tensor.wmatrix(),
        tensor.imatrix(),
        tensor.lmatrix(),
        tensor.dmatrix(),
        tensor.fmatrix()
    ]
    np_W, np_x, np_x_T = [], [], []
    for dtype in dtypes:
        if 'int' in dtype:
            np_W.append(rng.randint(
                -10, 10, rng.random_integers(5, size=2)
            ).astype(dtype))
            np_x.append(rng.randint(
                -10, 10, (rng.random_integers(5),
                          np_W[-1].shape[0])
            ).astype(dtype))
            np_x_T.append(rng.randint(
                -10, 10, (rng.random_integers(5),
                          np_W[-1].shape[1])
            ).astype(dtype))
        elif 'float' in dtype:
            np_W.append(rng.uniform(
                -1, 1, rng.random_integers(5, size=2)
            ).astype(dtype))
            np_x.append(rng.uniform(
                -10, 10, (rng.random_integers(5),
                          np_W[-1].shape[0])
            ).astype(dtype))
            np_x.append(rng.uniform(
                -10, 10, (rng.random_integers(5),
                          np_W[-1].shape[1])
            ).astype(dtype))
        else:
            assert False

    def sharedW(value, dtype):
        return theano.shared(theano._asarray(value, dtype=dtype))
    tensor_W = [sharedW(W, dtype) for W in np_W]
    matrixmul = [MatrixMul(W) for W in tensor_W]
    assert all(mm.get_params()[0] == W for mm, W in zip(matrixmul, tensor_W))

    fn = [theano.function([x], mm.lmul(x))
          for x, mm in zip(tensor_x, matrixmul)]
    fn_T = [theano.function([x], mm.lmul_T(x))
            for x, mm in zip(tensor_x, matrixmul)]
    for W, x, x_T, f, f_T in zip(np_W, np_x, np_x_T, fn, fn_T):
        np.testing.assert_allclose(f(x), np.dot(x, W))
        np.testing.assert_allclose(f_T(x_T), np.dot(x_T, W.T))


def test_make_local_rfs():
    view_converter = DefaultViewConverter((10, 10, 3))
    test_dataset = DenseDesignMatrix(np.ones((10, 300)),
                                     view_converter=view_converter)
    matrixmul = make_local_rfs(test_dataset, 4, (5, 5), (5, 5),
                               draw_patches=True)
    W = matrixmul.get_params()[0].get_value()
    assert W.shape == (300, 4)
    np.testing.assert_allclose(W.sum(axis=0), 75 * np.ones(4))
    np.testing.assert_allclose(W.sum(axis=1), np.ones(300))

    matrixmul = make_local_rfs(test_dataset, 4, (5, 5), (5, 5))
    W = matrixmul.get_params()[0].get_value()
    assert W.shape == (300, 4)
    np.testing.assert_raises(ValueError, make_local_rfs,
                             test_dataset, 2, (5, 5), (5, 5))

########NEW FILE########
__FILENAME__ = autoencoder
"""
Autoencoders, denoising autoencoders, and stacked DAEs.
"""
# Standard library imports
import functools
from itertools import izip
import operator

# Third-party imports
import numpy
import theano
from theano import tensor

# Local imports
from pylearn2.blocks import Block, StackedBlocks
from pylearn2.models import Model
from pylearn2.utils import sharedX
from pylearn2.utils.theano_graph import is_pure_elemwise
from pylearn2.utils.rng import make_np_rng, make_theano_rng
from pylearn2.space import VectorSpace

theano.config.warn.sum_div_dimshuffle_bug = False


class Autoencoder(Block, Model):
    """
    Base class implementing ordinary autoencoders.

    More exotic variants (denoising, contracting autoencoders) can inherit
    much of the necessary functionality and override what they need.

    Parameters
    ----------
    nvis : int
        Number of visible units (input dimensions) in this model.
        A value of 0 indicates that this block will be left partially
        initialized until later (e.g., when the dataset is loaded and
        its dimensionality is known).  Note: There is currently a bug
        when nvis is set to 0. For now, you should not set nvis to 0.
    nhid : int
        Number of hidden units in this model.
    act_enc : callable or string
        Activation function (elementwise nonlinearity) to use for the
        encoder. Strings (e.g. 'tanh' or 'sigmoid') will be looked up as
        functions in `theano.tensor.nnet` and `theano.tensor`. Use `None`
        for linear units.
    act_dec : callable or string
        Activation function (elementwise nonlinearity) to use for the
        decoder. Strings (e.g. 'tanh' or 'sigmoid') will be looked up as
        functions in `theano.tensor.nnet` and `theano.tensor`. Use `None`
        for linear units.
    tied_weights : bool, optional
        If `False` (default), a separate set of weights will be allocated
        (and learned) for the encoder and the decoder function. If
        `True`, the decoder weight matrix will be constrained to be equal
        to the transpose of the encoder weight matrix.
    irange : float, optional
        Width of the initial range around 0 from which to sample initial
        values for the weights.
    rng : RandomState object or seed, optional
        NumPy random number generator object (or seed to create one) used
        to initialize the model parameters.
    """

    def __init__(self, nvis, nhid, act_enc, act_dec,
                 tied_weights=False, irange=1e-3, rng=9001):
        super(Autoencoder, self).__init__()
        assert nvis > 0, "Number of visible units must be non-negative"
        assert nhid > 0, "Number of hidden units must be positive"

        self.input_space = VectorSpace(nvis)
        self.output_space = VectorSpace(nhid)

        # Save a few parameters needed for resizing
        self.nhid = nhid
        self.irange = irange
        self.tied_weights = tied_weights
        self.rng = make_np_rng(rng, which_method="randn")
        self._initialize_hidbias()
        if nvis > 0:
            self._initialize_visbias(nvis)
            self._initialize_weights(nvis)
        else:
            self.visbias = None
            self.weights = None

        seed = int(self.rng.randint(2 ** 30))

        # why a theano rng? should we remove it?
        self.s_rng = make_theano_rng(seed, which_method="uniform")

        if tied_weights and self.weights is not None:
            self.w_prime = self.weights.T
        else:
            self._initialize_w_prime(nvis)

        def _resolve_callable(conf, conf_attr):
            """
            .. todo::

                WRITEME
            """
            if conf[conf_attr] is None or conf[conf_attr] == "linear":
                return None
            # If it's a callable, use it directly.
            if hasattr(conf[conf_attr], '__call__'):
                return conf[conf_attr]
            elif (conf[conf_attr] in globals()
                  and hasattr(globals()[conf[conf_attr]], '__call__')):
                return globals()[conf[conf_attr]]
            elif hasattr(tensor.nnet, conf[conf_attr]):
                return getattr(tensor.nnet, conf[conf_attr])
            elif hasattr(tensor, conf[conf_attr]):
                return getattr(tensor, conf[conf_attr])
            else:
                raise ValueError("Couldn't interpret %s value: '%s'" %
                                    (conf_attr, conf[conf_attr]))

        self.act_enc = _resolve_callable(locals(), 'act_enc')
        self.act_dec = _resolve_callable(locals(), 'act_dec')
        self._params = [
            self.visbias,
            self.hidbias,
            self.weights,
        ]
        if not self.tied_weights:
            self._params.append(self.w_prime)

    def _initialize_weights(self, nvis, rng=None, irange=None):
        """
        .. todo::

            WRITEME
        """
        if rng is None:
            rng = self.rng
        if irange is None:
            irange = self.irange
        # TODO: use weight scaling factor if provided, Xavier's default else
        self.weights = sharedX(
            (.5 - rng.rand(nvis, self.nhid)) * irange,
            name='W',
            borrow=True
        )

    def _initialize_hidbias(self):
        """
        .. todo::

            WRITEME
        """
        self.hidbias = sharedX(
            numpy.zeros(self.nhid),
            name='hb',
            borrow=True
        )

    def _initialize_visbias(self, nvis):
        """
        .. todo::

            WRITEME
        """
        self.visbias = sharedX(
            numpy.zeros(nvis),
            name='vb',
            borrow=True
        )

    def _initialize_w_prime(self, nvis, rng=None, irange=None):
        """
        .. todo::

            WRITEME
        """
        assert not self.tied_weights, (
            "Can't initialize w_prime in tied weights model; "
            "this method shouldn't have been called"
        )
        if rng is None:
            rng = self.rng
        if irange is None:
            irange = self.irange
        self.w_prime = sharedX(
            (.5 - rng.rand(self.nhid, nvis)) * irange,
            name='Wprime',
            borrow=True
        )

    def set_visible_size(self, nvis, rng=None):
        """
        Create and initialize the necessary parameters to accept
        `nvis` sized inputs.

        Parameters
        ----------
        nvis : int
            Number of visible units for the model.
        rng : RandomState object or seed, optional
            NumPy random number generator object (or seed to create one) used \
            to initialize the model parameters. If not provided, the stored \
            rng object (from the time of construction) will be used.
        """
        if self.weights is not None:
            raise ValueError('parameters of this model already initialized; '
                             'create a new object instead')
        if rng is not None:
            self.rng = rng
        else:
            rng = self.rng
        self._initialize_visbias(nvis)
        self._initialize_weights(nvis, rng)
        if not self.tied_weights:
            self._initialize_w_prime(nvis, rng)
        self._set_params()

    def _hidden_activation(self, x):
        """
        Single minibatch activation function.

        Parameters
        ----------
        x : tensor_like
            Theano symbolic representing the input minibatch.

        Returns
        -------
        y : tensor_like
            (Symbolic) hidden unit activations given the input.
        """
        if self.act_enc is None:
            act_enc = lambda x: x
        else:
            act_enc = self.act_enc
        return act_enc(self._hidden_input(x))

    def _hidden_input(self, x):
        """
        Given a single minibatch, computes the input to the
        activation nonlinearity without applying it.

        Parameters
        ----------
        x : tensor_like
            Theano symbolic representing the input minibatch.

        Returns
        -------
        y : tensor_like
            (Symbolic) input flowing into the hidden layer nonlinearity.
        """
        return self.hidbias + tensor.dot(x, self.weights)

    def upward_pass(self, inputs):
        """
        Wrapper to Autoencoder encode function. Called when autoencoder
        is accessed by mlp.PretrainedLayer

        Parameters
        ----------
        inputs : WRITEME

        Returns
        -------
        WRITEME
        """
        return self.encode(inputs)

    def encode(self, inputs):
        """
        Map inputs through the encoder function.

        Parameters
        ----------
        inputs : tensor_like or list of tensor_likes
            Theano symbolic (or list thereof) representing the input
            minibatch(es) to be encoded. Assumed to be 2-tensors, with the
            first dimension indexing training examples and the second
            indexing data dimensions.

        Returns
        -------
        encoded : tensor_like or list of tensor_like
            Theano symbolic (or list thereof) representing the corresponding
            minibatch(es) after encoding.
        """
        if isinstance(inputs, tensor.Variable):
            return self._hidden_activation(inputs)
        else:
            return [self.encode(v) for v in inputs]

    def decode(self, hiddens):
        """
        Map inputs through the encoder function.

        Parameters
        ----------
        hiddens : tensor_like or list of tensor_likes
            Theano symbolic (or list thereof) representing the input
            minibatch(es) to be encoded. Assumed to be 2-tensors, with the
            first dimension indexing training examples and the second
            indexing data dimensions.

        Returns
        -------
        decoded : tensor_like or list of tensor_like
            Theano symbolic (or list thereof) representing the corresponding
            minibatch(es) after decoding.
        """
        if self.act_dec is None:
            act_dec = lambda x: x
        else:
            act_dec = self.act_dec
        if isinstance(hiddens, tensor.Variable):
            return act_dec(self.visbias + tensor.dot(hiddens, self.w_prime))
        else:
            return [self.decode(v) for v in hiddens]

    def reconstruct(self, inputs):
        """
        Reconstruct (decode) the inputs after mapping through the encoder.

        Parameters
        ----------
        inputs : tensor_like or list of tensor_likes
            Theano symbolic (or list thereof) representing the input
            minibatch(es) to be encoded and reconstructed. Assumed to be
            2-tensors, with the first dimension indexing training examples
            and the second indexing data dimensions.

        Returns
        -------
        reconstructed : tensor_like or list of tensor_like
            Theano symbolic (or list thereof) representing the corresponding
            reconstructed minibatch(es) after encoding/decoding.
        """
        return self.decode(self.encode(inputs))

    def __call__(self, inputs):
        """
        Forward propagate (symbolic) input through this module, obtaining
        a representation to pass on to layers above.

        This just aliases the `encode()` function for syntactic
        sugar/convenience.
        """
        return self.encode(inputs)

    def get_weights(self, borrow=False):
        """
        .. todo::

            WRITEME
        """

        return self.weights.get_value(borrow = borrow)

    def get_weights_format(self):
        """
        .. todo::

            WRITEME
        """

        return ['v', 'h']

    # Use version defined in Model, rather than Block (which raises
    # NotImplementedError).
    get_input_space = Model.get_input_space
    get_output_space = Model.get_output_space


class DenoisingAutoencoder(Autoencoder):
    """
    A denoising autoencoder learns a representation of the input by
    reconstructing a noisy version of it.

    Parameters
    ----------
    corruptor : object
        Instance of a corruptor object to use for corrupting the
        input.
    nvis : int
        WRITEME
    nhid : int
        WRITEME
    act_enc : WRITEME
    act_dec : WRITEME
    tied_weights : bool, optional
        WRITEME
    irange : WRITEME
    rng : WRITEME

    Notes
    -----
    The remaining parameters are identical to those of the constructor
    for the Autoencoder class; see the `Autoencoder.__init__` docstring
    for details.
    """
    def __init__(self, corruptor, nvis, nhid, act_enc, act_dec,
                 tied_weights=False, irange=1e-3, rng=9001):
        super(DenoisingAutoencoder, self).__init__(
            nvis,
            nhid,
            act_enc,
            act_dec,
            tied_weights,
            irange,
            rng
        )
        self.corruptor = corruptor

    def reconstruct(self, inputs):
        """
        Reconstruct the inputs after corrupting and mapping through the
        encoder and decoder.

        Parameters
        ----------
        inputs : tensor_like or list of tensor_likes
            Theano symbolic (or list thereof) representing the input
            minibatch(es) to be corrupted and reconstructed. Assumed to be
            2-tensors, with the first dimension indexing training examples
            and the second indexing data dimensions.

        Returns
        -------
        reconstructed : tensor_like or list of tensor_like
            Theano symbolic (or list thereof) representing the corresponding
            reconstructed minibatch(es) after corruption and encoding/decoding.
        """
        corrupted = self.corruptor(inputs)
        return super(DenoisingAutoencoder, self).reconstruct(corrupted)


class ContractiveAutoencoder(Autoencoder):
    """
    A contracting autoencoder works like a regular autoencoder, and adds an
    extra term to its cost function.
    """

    @functools.wraps(Autoencoder.__init__)
    def __init__(self, *args, **kwargs):
        super(ContractiveAutoencoder, self).__init__(*args, **kwargs)
        dummyinput = tensor.matrix()
        if not is_pure_elemwise(self.act_enc(dummyinput), [dummyinput]):
            raise ValueError("Invalid encoder activation function: "
                             "not an elementwise function of its input")

    def _activation_grad(self, inputs):
        """
        Calculate (symbolically) the contracting autoencoder penalty term.

        Parameters
        ----------
        inputs : tensor_like or list of tensor_likes
            Theano symbolic (or list thereof) representing the input \
            minibatch(es) on which the penalty is calculated. Assumed to be \
            2-tensors, with the first dimension indexing training examples \
            and the second indexing data dimensions.

        Returns
        -------
        act_grad : tensor_like
            2-dimensional tensor representing, dh/da for every \
            pre/postsynaptic pair, which we can easily do by taking the \
            gradient of the sum of the hidden units activations w.r.t the \
            presynaptic activity, since the gradient of hiddens.sum() with \
            respect to hiddens is a matrix of ones!

        Notes
        -----
        Theano's differentiation capabilities do not currently allow
        (efficient) automatic evaluation of the Jacobian, mainly because
        of the immature state of the `scan` operator. Here we use a
        "semi-automatic" hack that works for hidden layers of the for
        :math:`s(Wx + b)`, where `s` is the activation function, :math:`W`
        is `self.weights`, and :math:`b` is `self.hidbias`, by only taking
        the derivative of :math:`s` with respect :math:`a = Wx + b` and
        manually constructing the Jacobian from there.

        Because of this implementation depends *critically* on the
        _hidden_inputs() method implementing only an affine transformation
        by the weights (i.e. :math:`Wx + b`), and the activation function
        `self.act_enc` applying an independent, elementwise operation.
        """

        # Compute the input flowing into the hidden units, i.e. the
        # value before applying the nonlinearity/activation function
        acts = self._hidden_input(inputs)
        # Apply the activating nonlinearity.
        hiddens = self.act_enc(acts)
        act_grad = tensor.grad(hiddens.sum(), acts)
        return act_grad

    def jacobian_h_x(self, inputs):
        """
        Calculate (symbolically) the contracting autoencoder penalty term.

        Parameters
        ----------
        inputs : tensor_like or list of tensor_likes
            Theano symbolic (or list thereof) representing the input
            minibatch(es) on which the penalty is calculated. Assumed to be
            2-tensors, with the first dimension indexing training examples
            and the second indexing data dimensions.

        Returns
        -------
        jacobian : tensor_like
            3-dimensional tensor representing, for each mini-batch example,
            the Jacobian matrix of the encoder transformation. You can then
            apply the penalty you want on it, or use the contraction_penalty
            method to have a default one.
        """
        # As long as act_enc is an elementwise operator, the Jacobian
        # of a act_enc(Wx + b) hidden layer has a Jacobian of the
        # following form.
        act_grad = self._activation_grad(inputs)
        jacobian = self.weights * act_grad.dimshuffle(0, 'x', 1)
        return jacobian

    def contraction_penalty(self, data):
        """
        Calculate (symbolically) the contracting autoencoder penalty term.

        Parameters
        ----------
        data : tuple containing one tensor_like or list of tensor_likes
            Theano symbolic (or list thereof) representing the input
            minibatch(es) on which the penalty is calculated. Assumed to be
            2-tensors, with the first dimension indexing training examples
            and the second indexing data dimensions.

        Returns
        -------
        jacobian : tensor_like
            1-dimensional tensor representing, for each mini-batch
            example, the penalty of the encoder transformation. Add this to
            the output of a Cost object, such as SquaredError, to penalize it.
        """
        X = data
        act_grad = self._activation_grad(X)
        frob_norm = tensor.dot(tensor.sqr(act_grad), tensor.sqr(self.weights).sum(axis=0))
        contract_penalty = frob_norm.sum() / X.shape[0]
        return tensor.cast(contract_penalty, X .dtype)

    def contraction_penalty_data_specs(self):
        """
        .. todo::

            WRITEME
        """
        return (self.get_input_space(), self.get_input_source())


class HigherOrderContractiveAutoencoder(ContractiveAutoencoder):
    """
    Higher order contractive autoencoder. Adds higher orders regularization

    Parameters
    ----------
    corruptor : object
        Instance of a corruptor object to use for corrupting the input.
    num_corruptions : integer
        number of corrupted inputs to use
    nvis : int
        WRITEME
    nhid : int
        WRITEME
    act_enc : WRITEME
    act_dec : WRITEME
    tied_weights : WRITEME
    irange : WRITEME
    rng : WRITEME

    Notes
    -----
    The remaining parameters are identical to those of the constructor
    for the Autoencoder class; see the `ContractiveAutoEncoder.__init__`
    docstring for details.
    """
    def __init__(self, corruptor, num_corruptions, nvis, nhid, act_enc,
                    act_dec, tied_weights=False, irange=1e-3, rng=9001):
        super(HigherOrderContractiveAutoencoder, self).__init__(
            nvis,
            nhid,
            act_enc,
            act_dec,
            tied_weights,
            irange,
            rng
        )
        self.corruptor = corruptor
        self.num_corruptions = num_corruptions


    def higher_order_penalty(self, data):
        """
        Stochastic approximation of Hessian Frobenius norm

        Parameters
        ----------
        data : WRITEME

        Returns
        -------
        WRITEME
        """
        X = data

        corrupted_inputs = [self.corruptor(X) for times in\
                            range(self.num_corruptions)]

        hessian = tensor.concatenate([self.jacobian_h_x(X) - \
                                self.jacobian_h_x(corrupted) for\
                                corrupted in corrupted_inputs])

        return (hessian ** 2).mean()

    def higher_order_penalty_data_specs(self):
        """
        .. todo::

            WRITEME
        """
        return (self.get_input_space(), self.get_input_source())


class UntiedAutoencoder(Autoencoder):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    base : WRITEME
    """

    def __init__(self, base):
        if not base.tied_weights:
            raise ValueError("%s is not a tied-weights autoencoder" %
                             str(base))
        self.weights = tensor.shared(base.weights.get_value(borrow=False),
                                     name='weights')
        self.visbias = tensor.shared(base.visbias.get_value(borrow=False),
                                     name='vb')
        self.hidbias = tensor.shared(base.visbias.get_value(borrow=False),
                                     name='hb')
        self.w_prime = tensor.shared(base.weights.get_value(borrow=False).T,
                                     name='w_prime')
        self._params = [self.visbias, self.hidbias, self.weights, self.w_prime]


class DeepComposedAutoencoder(Autoencoder):
    """
    A deep autoencoder composed of several single-layer
    autoencoders.

    Parameters
    ----------
    autoencoders : list
        A list of autoencoder objects.
    """
    def __init__(self, autoencoders):
        self.fn = None
        self.cpu_only = False

        assert all([autoencoders[i].get_output_space().dim == autoencoders[i+1].get_input_space().dim for i in range(len(autoencoders)-1)])
        self.autoencoders = list(autoencoders)
        self.input_space = autoencoders[0].get_input_space()
        self.output_space = autoencoders[-1].get_output_space()

    @functools.wraps(Autoencoder.encode)
    def encode(self, inputs):
        """
        .. todo::

            WRITEME
        """
        current = inputs
        for encoder in self.autoencoders:
            current = encoder.encode(current)
        return current

    @functools.wraps(Autoencoder.decode)
    def decode(self, hiddens):
        """
        .. todo::

            WRITEME
        """
        current = hiddens
        for decoder in self.autoencoders[::-1]:
            current = decoder.decode(current)
        return current

    @functools.wraps(Model.get_params)
    def get_params(self):
        """
        .. todo::

            WRITEME
        """
        return reduce(operator.add,
                      [ae.get_params() for ae in self.autoencoders])

    def _modify_updates(self, updates):
        """
        .. todo::

            WRITEME
        """
        for autoencoder in self.autoencoders:
            autoencoder.modify_updates(updates)


def build_stacked_ae(nvis, nhids, act_enc, act_dec,
                     tied_weights=False, irange=1e-3, rng=None,
                     corruptor=None, contracting=False):
    """
    .. todo::

        WRITEME properly

    Allocate a stack of autoencoders.
    """
    rng = make_np_rng(rng, which_method='randn')
    layers = []
    final = {}
    # "Broadcast" arguments if they are singular, or accept sequences if
    # they are the same length as nhids
    for c in ['corruptor', 'contracting', 'act_enc', 'act_dec',
              'tied_weights', 'irange']:
        if type(locals()[c]) is not str and hasattr(locals()[c], '__len__'):
            assert len(nhids) == len(locals()[c])
            final[c] = locals()[c]
        else:
            final[c] = [locals()[c]] * len(nhids)
    # The number of visible units in each layer is the initial input
    # size and the first k-1 hidden unit sizes.
    nviss = [nvis] + nhids[:-1]
    seq = izip(nhids, nviss,
        final['act_enc'],
        final['act_dec'],
        final['corruptor'],
        final['contracting'],
        final['tied_weights'],
        final['irange'],
    )
    # Create each layer.
    for (nhid, nvis, act_enc, act_dec, corr, cae, tied, ir) in seq:
        args = (nvis, nhid, act_enc, act_dec, tied, ir, rng)
        if cae and corr is not None:
            raise ValueError("Can't specify denoising and contracting "
                             "objectives simultaneously")
        elif cae:
            autoenc = ContractiveAutoencoder(*args)
        elif corr is not None:
            autoenc = DenoisingAutoencoder(corr, *args)
        else:
            autoenc = Autoencoder(*args)
        layers.append(autoenc)

    # Create the stack
    return StackedBlocks(layers)

########NEW FILE########
__FILENAME__ = dbm
"""
The main DBM class
"""
__authors__ = ["Ian Goodfellow", "Vincent Dumoulin"]
__copyright__ = "Copyright 2012-2013, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"

import functools
import logging
import numpy as np
import warnings

from theano.compat import OrderedDict
from theano import tensor as T, config

from pylearn2.models import Model
from pylearn2.models.dbm import flatten
from pylearn2.models.dbm.inference_procedure import WeightDoubling
from pylearn2.models.dbm.sampling_procedure import GibbsEvenOdd
from pylearn2.utils import safe_zip, safe_izip
from pylearn2.utils.rng import make_np_rng


logger = logging.getLogger(__name__)


class DBM(Model):
    """
    A deep Boltzmann machine.

    See "Deep Boltzmann Machines" by Ruslan Salakhutdinov and Geoffrey Hinton
    for details.

    Parameters
    ----------
    batch_size : int
        The batch size the model should use. Some convolutional
        LinearTransforms require a compile-time hardcoded batch size,
        otherwise this would not be part of the model specification.
    visible_layer : WRITEME
        The visible layer of the DBM.
    hidden_layers : list
        The hidden layers. A list of HiddenLayer objects. The first
        layer in the list is connected to the visible layer.
    niter : int
        Number of mean field iterations for variational inference
        for the positive phase.
    sampling_procedure : WRITEME
    inference_procedure : WRITEME
    """

    def __init__(self, batch_size, visible_layer, hidden_layers, niter,
                 sampling_procedure=None, inference_procedure=None):
        self.__dict__.update(locals())
        del self.self
        assert len(hidden_layers) >= 1

        if len(hidden_layers) > 1 and niter <= 1:
            raise ValueError("with more than one hidden layer, niter needs to "
                             "be greater than 1; otherwise mean field won't "
                             "work properly.")

        self.setup_rng()
        self.layer_names = set()
        self.visible_layer.set_dbm(self)
        for layer in hidden_layers:
            assert layer.get_dbm() is None
            layer.set_dbm(self)
            assert layer.layer_name not in self.layer_names
            self.layer_names.add(layer.layer_name)
        self._update_layer_input_spaces()
        self.force_batch_size = batch_size
        self.freeze_set = set([])
        if inference_procedure is None:
            self.setup_inference_procedure()
        self.inference_procedure.set_dbm(self)
        if sampling_procedure is None:
            self.setup_sampling_procedure()
        self.sampling_procedure.set_dbm(self)

    def get_all_layers(self):
        """
        .. todo::

            WRITEME
        """
        return [self.visible_layer] + self.hidden_layers

    def energy(self, V, hidden):
        """
        .. todo::

            WRITEME

        Parameters
        ----------
        V : tensor_like
            Theano batch of visible unit observations (must be SAMPLES, not
            mean field parameters)
        hidden : list
            List, one element per hidden layer, of batches of samples (must
            be SAMPLES, not mean field parameters)

        Returns
        -------
        rval : tensor_like
            Vector containing the energy of each sample

        Notes
        -----
        Applying this function to non-sample theano variables is not guaranteed
        to give you an expected energy in general, so don't use this that way.
        """

        terms = []

        terms.append(self.visible_layer.expected_energy_term(state=V,
                     average=False))

        # This condition could be relaxed, but current code assumes it
        assert len(self.hidden_layers) > 0

        terms.append(self.hidden_layers[0].expected_energy_term(
            state_below=self.visible_layer.upward_state(V),
            state=hidden[0], average_below=False, average=False))

        for i in xrange(1, len(self.hidden_layers)):
            layer = self.hidden_layers[i]
            samples_below = hidden[i-1]
            layer_below = self.hidden_layers[i-1]
            samples_below = layer_below.upward_state(samples_below)
            samples = hidden[i]
            terms.append(layer.expected_energy_term(state_below=samples_below,
                         state=samples, average_below=False, average=False))

        assert len(terms) > 0

        rval = reduce(lambda x, y: x + y, terms)

        assert rval.ndim == 1
        return rval

    def mf(self, *args, **kwargs):
        """
        .. todo::

            WRITEME
        """
        self.setup_inference_procedure()
        return self.inference_procedure.mf(*args, **kwargs)

    def expected_energy(self, V, mf_hidden):
        """
        WRITEME

        Parameters
        ----------
        V : tensor_like
            Theano batch of visible unit observations (must be SAMPLES, not
            mean field parameters: the random variables in the expectation
            are the hiddens only)
        mf_hidden : list
            List, one element per hidden layer, of batches of variational
            parameters (must be VARIATIONAL PARAMETERS, not samples. Layers
            with analytically determined variance parameters for their mean
            field parameters will use those to integrate over the variational
            distribution, so it's not generally the same thing as measuring
            the energy at a point.)

        Returns
        -------
        rval : tensor_like
            Vector containing the expected energy of each example under the
            corresponding variational distribution.
        """

        self.visible_layer.space.validate(V)
        assert isinstance(mf_hidden, (list, tuple))
        assert len(mf_hidden) == len(self.hidden_layers)

        terms = []

        terms.append(self.visible_layer.expected_energy_term(state=V,
                     average=False))

        # This condition could be relaxed, but current code assumes it
        assert len(self.hidden_layers) > 0

        terms.append(self.hidden_layers[0].expected_energy_term(
            state_below=self.visible_layer.upward_state(V),
            average_below=False, state=mf_hidden[0], average=True))

        for i in xrange(1, len(self.hidden_layers)):
            layer = self.hidden_layers[i]
            layer_below = self.hidden_layers[i-1]
            mf_below = mf_hidden[i-1]
            mf_below = layer_below.upward_state(mf_below)
            mf = mf_hidden[i]
            terms.append(layer.expected_energy_term(state_below=mf_below,
                         state=mf, average_below=True, average=True))

        assert len(terms) > 0

        rval = reduce(lambda x, y: x + y, terms)

        assert rval.ndim == 1
        return rval

    def setup_rng(self):
        """
        .. todo::

            WRITEME
        """
        self.rng = make_np_rng(None, [2012, 10, 17], which_method="uniform")

    def setup_inference_procedure(self):
        """
        .. todo::

            WRITEME
        """
        if not hasattr(self, 'inference_procedure') or \
                self.inference_procedure is None:
            self.inference_procedure = WeightDoubling()
            self.inference_procedure.set_dbm(self)

    def setup_sampling_procedure(self):
        """
        .. todo::

            WRITEME
        """
        if not hasattr(self, 'sampling_procedure') or \
                self.sampling_procedure is None:
            self.sampling_procedure = GibbsEvenOdd()
            self.sampling_procedure.set_dbm(self)

    def get_output_space(self):
        """
        .. todo::

            WRITEME
        """
        return self.hidden_layers[-1].get_output_space()

    def _update_layer_input_spaces(self):
        """
        Tells each layer what its input space should be.

        Notes
        -----
        This usually resets the layer's parameters!
        """
        visible_layer = self.visible_layer
        hidden_layers = self.hidden_layers

        self.hidden_layers[0].set_input_space(visible_layer.space)
        for i in xrange(1, len(hidden_layers)):
            hidden_layers[i].set_input_space(
                hidden_layers[i-1].get_output_space())

        for layer in self.get_all_layers():
            layer.finalize_initialization()

    def add_layers(self, layers):
        """
        Add new layers on top of the existing hidden layers

        Parameters
        ----------
        layers : WRITEME
        """

        # Patch old pickle files
        if not hasattr(self, 'rng'):
            self.setup_rng()

        hidden_layers = self.hidden_layers
        assert len(hidden_layers) > 0
        for layer in layers:
            assert layer.get_dbm() is None
            layer.set_dbm(self)
            layer.set_input_space(hidden_layers[-1].get_output_space())
            hidden_layers.append(layer)
            assert layer.layer_name not in self.layer_names
            self.layer_names.add(layer.layer_name)

    def freeze(self, parameter_set):
        """
        .. todo::

            WRITEME
        """
        # patch old pickle files
        if not hasattr(self, 'freeze_set'):
            self.freeze_set = set([])

        self.freeze_set = self.freeze_set.union(parameter_set)

    def get_params(self):
        """
        .. todo::

            WRITEME
        """

        rval = []
        for param in self.visible_layer.get_params():
            assert param.name is not None
        rval = self.visible_layer.get_params()
        for layer in self.hidden_layers:
            for param in layer.get_params():
                if param.name is None:
                    raise ValueError("All of your parameters should have "
                                     "names, but one of " + layer.layer_name +
                                     "'s doesn't")
            layer_params = layer.get_params()
            assert not isinstance(layer_params, set)
            for param in layer_params:
                if param not in rval:
                    rval.append(param)

        # Patch pickle files that predate the freeze_set feature
        if not hasattr(self, 'freeze_set'):
            self.freeze_set = set([])

        rval = [elem for elem in rval if elem not in self.freeze_set]

        assert all([elem.name is not None for elem in rval])

        return rval

    def set_batch_size(self, batch_size):
        """
        .. todo::

            WRITEME
        """
        self.batch_size = batch_size
        self.force_batch_size = batch_size

        for layer in self.hidden_layers:
            layer.set_batch_size(batch_size)

        if not hasattr(self, 'inference_procedure'):
            self.setup_inference_procedure()
        self.inference_procedure.set_batch_size(batch_size)

    @functools.wraps(Model._modify_updates)
    def _modify_updates(self, updates):
        self.visible_layer.modify_updates(updates)
        for layer in self.hidden_layers:
            layer.modify_updates(updates)

    def get_input_space(self):
        """
        .. todo::

            WRITEME
        """
        return self.visible_layer.space

    def get_lr_scalers(self):
        """
        .. todo::

            WRITEME
        """
        rval = OrderedDict()

        params = self.get_params()

        for layer in self.hidden_layers + [self.visible_layer]:
            contrib = layer.get_lr_scalers()

            # No two layers can contend to scale a parameter
            assert not any([key in rval for key in contrib])
            # Don't try to scale anything that's not a parameter
            assert all([key in params for key in contrib])

            rval.update(contrib)
        assert all([isinstance(val, float) for val in rval.values()])

        return rval

    def get_weights(self):
        """
        .. todo::

            WRITEME
        """
        return self.hidden_layers[0].get_weights()

    def get_weights_view_shape(self):
        """
        .. todo::

            WRITEME
        """
        return self.hidden_layers[0].get_weights_view_shape()

    def get_weights_format(self):
        """
        .. todo::

            WRITEME
        """
        return self.hidden_layers[0].get_weights_format()

    def get_weights_topo(self):
        """
        .. todo::

            WRITEME
        """
        return self.hidden_layers[0].get_weights_topo()

    def make_layer_to_state(self, num_examples, rng=None):
        """
        Makes and returns a dictionary mapping layers to states.

        By states, we mean here a real assignment, not a mean field
        state. For example, for a layer containing binary random
        variables, the state will be a shared variable containing
        values in {0,1}, not [0,1]. The visible layer will be included.

        Uses a dictionary so it is easy to unambiguously index a layer
        without needing to remember rules like vis layer = 0, hiddens
        start at 1, etc.

        Parameters
        ----------
        num_examples : int
            WRITEME
        rng : WRITEME
        """

        # Make a list of all layers
        layers = [self.visible_layer] + self.hidden_layers

        if rng is None:
            rng = self.rng

        states = [layer.make_state(num_examples, rng) for layer in layers]

        zipped = safe_zip(layers, states)

        def recurse_check(layer, state):
            if isinstance(state, (list, tuple)):
                for elem in state:
                    recurse_check(layer, elem)
            else:
                val = state.get_value()
                m = val.shape[0]
                if m != num_examples:
                    raise ValueError(layer.layer_name + " gave state with " +
                                     str(m) + " examples in some component."
                                     "We requested " + str(num_examples))

        for layer, state in zipped:
            recurse_check(layer, state)

        rval = OrderedDict(zipped)

        return rval

    def make_layer_to_symbolic_state(self, num_examples, rng=None):
        """
        .. todo::

            Explain the difference with `make_layer_to_state`

        Makes and returns a dictionary mapping layers to states.

        By states, we mean here a real assignment, not a mean field
        state. For example, for a layer containing binary random
        variables, the state will be a shared variable containing
        values in {0,1}, not [0,1]. The visible layer will be included.

        Uses a dictionary so it is easy to unambiguously index a layer
        without needing to remember rules like vis layer = 0, hiddens
        start at 1, etc.

        Parameters
        ----------
        num_examples : int
            WRITEME
        rng : WRITEME
        """

        # Make a list of all layers
        layers = [self.visible_layer] + self.hidden_layers

        assert rng is not None

        states = [layer.make_symbolic_state(num_examples, rng)
                  for layer in layers]

        zipped = safe_zip(layers, states)

        rval = OrderedDict(zipped)

        return rval

    def mcmc_steps(self, layer_to_state, theano_rng, layer_to_clamp=None,
                   num_steps=1):
        """
        .. todo::

            WRITEME
        """
        warnings.warn("DBM.mcmc_steps is deprecated. You should instead " +
                      "call DBM.sampling_procedure.sample, which defaults " +
                      "to what DBM.mcmc_steps used to do. This method will " +
                      "be removed on or after July 31, 2014.")
        return self.sampling_procedure.sample(layer_to_state, theano_rng,
                                              layer_to_clamp, num_steps)

    def get_sampling_updates(self, layer_to_state, theano_rng,
                             layer_to_clamp=None, num_steps=1,
                             return_layer_to_updated=False):
        """
        This method is for getting an updates dictionary for a theano function.

        It thus implies that the samples are represented as shared variables.
        If you want an expression for a sampling step applied to arbitrary
        theano variables, use the 'mcmc_steps' method. This is a wrapper around
        that method.

        Parameters
        ----------
        layer_to_state : dict
            Dictionary mapping the SuperDBM_Layer instances contained in
            self to shared variables representing batches of samples of them.
            (you can allocate one by calling self.make_layer_to_state)
        theano_rng : MRG_RandomStreams
            WRITEME
        layer_to_clamp : dict, optional
            Dictionary mapping layers to bools. If a layer is not in the
            dictionary, defaults to False. True indicates that this layer
            should be clamped, so we are sampling from a conditional
            distribution rather than the joint distribution
        num_steps : int, optional
            WRITEME
        return_layer_to_updated : bool, optional
            WRITEME

        Returns
        -------
        rval : dict
            Dictionary mapping each shared variable to an expression to
            update it. Repeatedly applying these updates does MCMC sampling.

        Notes
        -----
        The specific sampling schedule used by default is to sample all of the
        even-idexed layers of model.hidden_layers, then the visible layer and
        all the odd-indexed layers.
        """

        updated = self.sampling_procedure.sample(layer_to_state, theano_rng,
                                                 layer_to_clamp, num_steps)

        rval = OrderedDict()

        def add_updates(old, new):
            if isinstance(old, (list, tuple)):
                for old_elem, new_elem in safe_izip(old, new):
                    add_updates(old_elem, new_elem)
            else:
                rval[old] = new

        # Validate layer_to_clamp / make sure layer_to_clamp is a fully
        # populated dictionary
        if layer_to_clamp is None:
            layer_to_clamp = OrderedDict()

        for key in layer_to_clamp:
            assert key is self.visible_layer or key in self.hidden_layers

        for layer in [self.visible_layer] + self.hidden_layers:
            if layer not in layer_to_clamp:
                layer_to_clamp[layer] = False

        # Translate update expressions into theano updates
        for layer in layer_to_state:
            old = layer_to_state[layer]
            new = updated[layer]
            if layer_to_clamp[layer]:
                assert new is old
            else:
                add_updates(old, new)

        assert isinstance(self.hidden_layers, list)

        if return_layer_to_updated:
            return rval, updated

        return rval

    def get_monitoring_channels(self, data):
        """
        .. todo::

            WRITEME
        """
        space, source = self.get_monitoring_data_specs()
        space.validate(data)
        X = data
        history = self.mf(X, return_history=True)
        q = history[-1]

        rval = OrderedDict()

        ch = self.visible_layer.get_monitoring_channels()
        for key in ch:
            rval['vis_' + key] = ch[key]

        for state, layer in safe_zip(q, self.hidden_layers):
            ch = layer.get_monitoring_channels()
            for key in ch:
                rval[layer.layer_name + '_' + key] = ch[key]
            ch = layer.get_monitoring_channels_from_state(state)
            for key in ch:
                rval['mf_' + layer.layer_name + '_' + key] = ch[key]

        if len(history) > 1:
            prev_q = history[-2]

            flat_q = flatten(q)
            flat_prev_q = flatten(prev_q)

            mx = None
            for new, old in safe_zip(flat_q, flat_prev_q):
                cur_mx = abs(new - old).max()
                if new is old:
                    logger.error('{0} is {1}'.format(new, old))
                    assert False
                if mx is None:
                    mx = cur_mx
                else:
                    mx = T.maximum(mx, cur_mx)

            rval['max_var_param_diff'] = mx

            for layer, new, old in safe_zip(self.hidden_layers,
                                            q, prev_q):
                sum_diff = 0.
                for sub_new, sub_old in safe_zip(flatten(new), flatten(old)):
                    sum_diff += abs(sub_new - sub_old).sum()
                denom = self.batch_size * \
                    layer.get_total_state_space().get_total_dimension()
                denom = np.cast[config.floatX](denom)
                rval['mean_'+layer.layer_name+'_var_param_diff'] = \
                    sum_diff / denom

        return rval

    def get_monitoring_data_specs(self):
        """
        Get the data_specs describing the data for get_monitoring_channel.

        This implementation returns specification corresponding to unlabeled
        inputs.
        """
        return (self.get_input_space(), self.get_input_source())

    def get_test_batch_size(self):
        """
        .. todo::

            WRITEME
        """
        return self.batch_size

    def reconstruct(self, V):
        """
        .. todo::

            WRITEME
        """

        H = self.mf(V)[0]

        downward_state = self.hidden_layers[0].downward_state(H)

        recons = self.visible_layer.inpaint_update(
            layer_above=self.hidden_layers[0],
            state_above=downward_state,
            drop_mask=None, V=None)

        return recons

    def do_inpainting(self, *args, **kwargs):
        """
        .. todo::

            WRITEME
        """
        self.setup_inference_procedure()
        return self.inference_procedure.do_inpainting(*args, **kwargs)

########NEW FILE########
__FILENAME__ = inference_procedure
"""
.. todo::

    WRITEME
"""
__authors__ = ["Ian Goodfellow", "Vincent Dumoulin"]
__copyright__ = "Copyright 2012-2013, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"

import logging
from theano import gof
import theano.tensor as T
import theano
from theano.gof.op import get_debug_values
from pylearn2.models.dbm import block, flatten
from pylearn2.models.dbm.layer import Softmax
from pylearn2.utils import safe_izip, block_gradient, safe_zip


logger = logging.getLogger(__name__)


class InferenceProcedure(object):
    """
    .. todo::

        WRITEME
    """

    def set_dbm(self, dbm):
        """
        .. todo::

            WRITEME
        """
        self.dbm = dbm

    def mf(self, V, Y = None, return_history = False, niter = None, block_grad = None):
        """
        .. todo::

            WRITEME
        """
        raise NotImplementedError(str(type(self))+" does not implement mf.")

    def set_batch_size(self, batch_size):
        """
        If the inference procedure is dependent on a batch size at all, makes
        the necessary internal configurations to work with that batch size.
        """
        # TODO : was this supposed to be implemented?


class WeightDoubling(InferenceProcedure):
    """
    .. todo::

        WRITEME
    """

    def mf(self, V, Y = None, return_history = False, niter = None, block_grad = None):
        """
        .. todo::

            WRITEME
        """

        dbm = self.dbm

        assert Y not in [True, False, 0, 1]
        assert return_history in [True, False, 0, 1]

        if Y is not None:
            dbm.hidden_layers[-1].get_output_space().validate(Y)

        if niter is None:
            niter = dbm.niter

        H_hat = []
        for i in xrange(0,len(dbm.hidden_layers)-1):
            #do double weights update for_layer_i
            if i == 0:
                H_hat.append(dbm.hidden_layers[i].mf_update(
                    state_above = None,
                    double_weights = True,
                    state_below = dbm.visible_layer.upward_state(V),
                    iter_name = '0'))
            else:
                H_hat.append(dbm.hidden_layers[i].mf_update(
                    state_above = None,
                    double_weights = True,
                    state_below = dbm.hidden_layers[i-1].upward_state(H_hat[i-1]),
                    iter_name = '0'))

        #last layer does not need its weights doubled, even on the first pass
        if len(dbm.hidden_layers) > 1:
            H_hat.append(dbm.hidden_layers[-1].mf_update(
                state_above = None,
                state_below = dbm.hidden_layers[-2].upward_state(H_hat[-1])))
        else:
            H_hat.append(dbm.hidden_layers[-1].mf_update(
                state_above = None,
                state_below = dbm.visible_layer.upward_state(V)))

        # Make corrections for if we're also running inference on Y
        if Y is not None:
            state_above = dbm.hidden_layers[-1].downward_state(Y)
            layer_above = dbm.hidden_layers[-1]
            assert len(dbm.hidden_layers) > 1

            # Last layer before Y does not need its weights doubled
            # because it already has top down input
            if len(dbm.hidden_layers) > 2:
                state_below = dbm.hidden_layers[-3].upward_state(H_hat[-3])
            else:
                state_below = dbm.visible_layer.upward_state(V)

            H_hat[-2] = dbm.hidden_layers[-2].mf_update(
                            state_below = state_below,
                            state_above = state_above,
                            layer_above = layer_above)

            # Last layer is clamped to Y
            H_hat[-1] = Y



        if block_grad == 1:
            H_hat = block(H_hat)

        history = [ list(H_hat) ]


        #we only need recurrent inference if there are multiple layers
        if len(H_hat) > 1:
            for i in xrange(1, niter):
                for j in xrange(0,len(H_hat),2):
                    if j == 0:
                        state_below = dbm.visible_layer.upward_state(V)
                    else:
                        state_below = dbm.hidden_layers[j-1].upward_state(H_hat[j-1])
                    if j == len(H_hat) - 1:
                        state_above = None
                        layer_above = None
                    else:
                        state_above = dbm.hidden_layers[j+1].downward_state(H_hat[j+1])
                        layer_above = dbm.hidden_layers[j+1]
                    H_hat[j] = dbm.hidden_layers[j].mf_update(
                            state_below = state_below,
                            state_above = state_above,
                            layer_above = layer_above)

                if Y is not None:
                    H_hat[-1] = Y

                for j in xrange(1,len(H_hat),2):
                    state_below = dbm.hidden_layers[j-1].upward_state(H_hat[j-1])
                    if j == len(H_hat) - 1:
                        state_above = None
                        state_above = None
                    else:
                        state_above = dbm.hidden_layers[j+1].downward_state(H_hat[j+1])
                        layer_above = dbm.hidden_layers[j+1]
                    H_hat[j] = dbm.hidden_layers[j].mf_update(
                            state_below = state_below,
                            state_above = state_above,
                            layer_above = layer_above)
                    #end ifelse
                #end for odd layer

                if Y is not None:
                    H_hat[-1] = Y

                if block_grad == i:
                    H_hat = block(H_hat)

                history.append(list(H_hat))
            # end for mf iter
        # end if recurrent

        # Run some checks on the output
        for layer, state in safe_izip(dbm.hidden_layers, H_hat):
            upward_state = layer.upward_state(state)
            layer.get_output_space().validate(upward_state)
        if Y is not None:
            inferred = H_hat[:-1]
        else:
            inferred = H_hat
        for elem in flatten(inferred):
            # This check doesn't work with ('c', 0, 1, 'b') because 'b' is no longer axis 0
            # for value in get_debug_values(elem):
            #    assert value.shape[0] == dbm.batch_size
            assert V in gof.graph.ancestors([elem])
            if Y is not None:
                assert Y in gof.graph.ancestors([elem])
        if Y is not None:
            assert all([elem[-1] is Y for elem in history])
            assert H_hat[-1] is Y

        if return_history:
            return history
        else:
            return H_hat


class SuperWeightDoubling(WeightDoubling):
    """
    .. todo::

        WRITEME
    """

    def multi_infer(self, V, return_history = False, niter = None, block_grad = None):
        """
        .. todo::

            WRITEME
        """

        dbm = self.dbm

        assert return_history in [True, False, 0, 1]

        if niter is None:
            niter = dbm.niter

        new_V = 0.5 * V + 0.5 * dbm.visible_layer.init_inpainting_state(V,drop_mask = None,noise = False, return_unmasked = False)

        H_hat = []
        for i in xrange(0,len(dbm.hidden_layers)-1):
            #do double weights update for_layer_i
            if i == 0:
                H_hat.append(dbm.hidden_layers[i].mf_update(
                                                            state_above = None,
                                                            double_weights = True,
                                                            state_below = dbm.visible_layer.upward_state(new_V),
                                                            iter_name = '0'))
            else:
                H_hat.append(dbm.hidden_layers[i].mf_update(
                                                            state_above = None,
                                                            double_weights = True,
                                                            state_below = dbm.hidden_layers[i-1].upward_state(H_hat[i-1]),
                                                            iter_name = '0'))

        #last layer does not need its weights doubled, even on the first pass
        if len(dbm.hidden_layers) > 1:
            H_hat.append(dbm.hidden_layers[-1].mf_update(
                                                         state_above = None,
                                                         state_below = dbm.hidden_layers[-2].upward_state(H_hat[-1])))
        else:
            H_hat.append(dbm.hidden_layers[-1].mf_update(
                                                         state_above = None,
                                                         state_below = dbm.visible_layer.upward_state(V)))

        if block_grad == 1:
            H_hat = block(H_hat)

        history = [ (new_V, list(H_hat)) ]


        #we only need recurrent inference if there are multiple layers
        if len(H_hat) > 1:
            for i in xrange(1, niter):
                for j in xrange(0,len(H_hat),2):
                    if j == 0:
                        state_below = dbm.visible_layer.upward_state(new_V)
                    else:
                        state_below = dbm.hidden_layers[j-1].upward_state(H_hat[j-1])
                    if j == len(H_hat) - 1:
                        state_above = None
                        layer_above = None
                    else:
                        state_above = dbm.hidden_layers[j+1].downward_state(H_hat[j+1])
                        layer_above = dbm.hidden_layers[j+1]
                    H_hat[j] = dbm.hidden_layers[j].mf_update(
                                                              state_below = state_below,
                                                              state_above = state_above,
                                                              layer_above = layer_above)
                V_hat = dbm.visible_layer.inpaint_update(
                                                                                 state_above = dbm.hidden_layers[0].downward_state(H_hat[0]),
                                                                                 layer_above = dbm.hidden_layers[0],
                                                                                 V = V,
                                                                                 drop_mask = None)
                new_V = 0.5 * V_hat + 0.5 * V

                for j in xrange(1,len(H_hat),2):
                    state_below = dbm.hidden_layers[j-1].upward_state(H_hat[j-1])
                    if j == len(H_hat) - 1:
                        state_above = None
                        state_above = None
                    else:
                        state_above = dbm.hidden_layers[j+1].downward_state(H_hat[j+1])
                        layer_above = dbm.hidden_layers[j+1]
                    H_hat[j] = dbm.hidden_layers[j].mf_update(
                                                              state_below = state_below,
                                                              state_above = state_above,
                                                              layer_above = layer_above)
                #end ifelse
                #end for odd layer

                if block_grad == i:
                    H_hat = block(H_hat)
                    V_hat = block_gradient(V_hat)

                history.append((new_V, list(H_hat)))
        # end for mf iter
        # end if recurrent
        # Run some checks on the output
        for layer, state in safe_izip(dbm.hidden_layers, H_hat):
            upward_state = layer.upward_state(state)
            layer.get_output_space().validate(upward_state)

        inferred = H_hat
        for elem in flatten(inferred):
            for value in get_debug_values(elem):
                assert value.shape[0] == dbm.batch_size
            assert V in gof.graph.ancestors([elem])

        if return_history:
            return history
        else:
            return H_hat[-1]

    def do_inpainting(self, V, Y = None, drop_mask = None, drop_mask_Y = None,
            return_history = False, noise = False, niter = None, block_grad = None):
        """
        .. todo::

            WRITEME properly

        Gives the mean field expression for units masked out by drop_mask.
        Uses self.niter mean field updates.

        If you use this method in your research work, please cite:

            Multi-prediction deep Boltzmann machines. Ian J. Goodfellow,
            Mehdi Mirza, Aaron Courville, and Yoshua Bengio. NIPS 2013.


        Comes in two variants, unsupervised and supervised:

        * unsupervised: Y and drop_mask_Y are not passed to the method. The
          method produces V_hat, an inpainted version of V.
        * supervised: Y and drop_mask_Y are passed to the method. The method
          produces V_hat and Y_hat

        Parameters
        ----------
        V : tensor_like
            Theano batch in `model.input_space`
        Y : tensor_like
            Theano batch in `model.output_space`, i.e. in the output space of
            the last hidden layer. (It's not really a hidden layer anymore,
            but oh well. It's convenient to code it this way because the
            labels are sort of "on top" of everything else.) *** Y is always
            assumed to be a matrix of one-hot category labels. ***
        drop_mask : tensor_like
            Theano batch in `model.input_space`. Should be all binary, with
            1s indicating that the corresponding element of X should be
            "dropped", i.e. hidden from the algorithm and filled in as part
            of the inpainting process
        drop_mask_Y : tensor_like
            Theano vector. Since we assume Y is a one-hot matrix, each row is
            a single categorical variable. `drop_mask_Y` is a binary mask
            specifying which *rows* to drop.
        return_history : bool, optional
            WRITEME
        noise : bool, optional
            WRITEME
        niter : int, optional
            WRITEME
        block_grad : WRITEME

        Returns
        -------
        WRITEME
        """

        dbm = self.dbm

        """TODO: Should add unit test that calling this with a batch of
                 different inputs should yield the same output for each
                 if noise is False and drop_mask is all 1s"""

        if niter is None:
            niter = dbm.niter

        assert drop_mask is not None
        assert return_history in [True, False]
        assert noise in [True, False]
        if Y is None:
            if drop_mask_Y is not None:
                raise ValueError("do_inpainting got drop_mask_Y but not Y.")
        else:
            if drop_mask_Y is None:
                raise ValueError("do_inpainting got Y but not drop_mask_Y.")

        if Y is not None:
            assert isinstance(dbm.hidden_layers[-1], Softmax)
            if drop_mask_Y.ndim != 1:
                raise ValueError("do_inpainting assumes Y is a matrix of one-hot labels,"
    "so each example is only one variable. drop_mask_Y should "
    "therefore be a vector, but we got something with ndim " +
                        str(drop_mask_Y.ndim))
            drop_mask_Y = drop_mask_Y.dimshuffle(0, 'x')

        orig_V = V
        orig_drop_mask = drop_mask

        history = []

        V_hat, V_hat_unmasked = dbm.visible_layer.init_inpainting_state(V,drop_mask,noise, return_unmasked = True)
        assert V_hat_unmasked.ndim > 1

        H_hat = []
        for i in xrange(0,len(dbm.hidden_layers)-1):
            #do double weights update for_layer_i
            if i == 0:
                H_hat.append(dbm.hidden_layers[i].mf_update(
                    state_above = None,
                    double_weights = True,
                    state_below = dbm.visible_layer.upward_state(V_hat),
                    iter_name = '0'))
            else:
                H_hat.append(dbm.hidden_layers[i].mf_update(
                    state_above = None,
                    double_weights = True,
                    state_below = dbm.hidden_layers[i-1].upward_state(H_hat[i-1]),
                    iter_name = '0'))
        # Last layer does not need its weights doubled, even on the first pass
        if len(dbm.hidden_layers) > 1:
            H_hat.append(dbm.hidden_layers[-1].mf_update(
                state_above = None,
                #layer_above = None,
                state_below = dbm.hidden_layers[-2].upward_state(H_hat[-1])))
        else:
            H_hat.append(dbm.hidden_layers[-1].mf_update(
                state_above = None,
                state_below = dbm.visible_layer.upward_state(V_hat)))

        if Y is not None:
            Y_hat_unmasked = dbm.hidden_layers[-1].init_inpainting_state(Y, noise)
            dirty_term = drop_mask_Y * Y_hat_unmasked
            clean_term = (1 - drop_mask_Y) * Y
            Y_hat = dirty_term + clean_term
            H_hat[-1] = Y_hat
            if len(dbm.hidden_layers) > 1:
                i = len(dbm.hidden_layers) - 2
                if i == 0:
                    H_hat[i] = dbm.hidden_layers[i].mf_update(
                        state_above = Y_hat,
                        layer_above = dbm.hidden_layers[-1],
                        state_below = dbm.visible_layer.upward_state(V_hat),
                        iter_name = '0')
                else:
                    H_hat[i] = dbm.hidden_layers[i].mf_update(
                        state_above = Y_hat,
                        layer_above = dbm.hidden_layers[-1],
                        state_below = dbm.hidden_layers[i-1].upward_state(H_hat[i-1]),
                        iter_name = '0')


        def update_history():
            assert V_hat_unmasked.ndim > 1
            d =  { 'V_hat' :  V_hat, 'H_hat' : list(H_hat), 'V_hat_unmasked' : V_hat_unmasked }
            if Y is not None:
                d['Y_hat_unmasked'] = Y_hat_unmasked
                d['Y_hat'] = H_hat[-1]
            history.append(d)

        if block_grad == 1:
            V_hat = block_gradient(V_hat)
            V_hat_unmasked = block_gradient(V_hat_unmasked)
            H_hat = block(H_hat)
        update_history()

        for i in xrange(niter-1):
            for j in xrange(0, len(H_hat), 2):
                if j == 0:
                    state_below = dbm.visible_layer.upward_state(V_hat)
                else:
                    state_below = dbm.hidden_layers[j-1].upward_state(H_hat[j-1])
                if j == len(H_hat) - 1:
                    state_above = None
                    layer_above = None
                else:
                    state_above = dbm.hidden_layers[j+1].downward_state(H_hat[j+1])
                    layer_above = dbm.hidden_layers[j+1]
                H_hat[j] = dbm.hidden_layers[j].mf_update(
                        state_below = state_below,
                        state_above = state_above,
                        layer_above = layer_above)
                if Y is not None and j == len(dbm.hidden_layers) - 1:
                    Y_hat_unmasked = H_hat[j]
                    H_hat[j] = drop_mask_Y * H_hat[j] + (1 - drop_mask_Y) * Y

            V_hat, V_hat_unmasked = dbm.visible_layer.inpaint_update(
                    state_above = dbm.hidden_layers[0].downward_state(H_hat[0]),
                    layer_above = dbm.hidden_layers[0],
                    V = V,
                    drop_mask = drop_mask, return_unmasked = True)
            V_hat.name = 'V_hat[%d](V_hat = %s)' % (i, V_hat.name)

            for j in xrange(1,len(H_hat),2):
                state_below = dbm.hidden_layers[j-1].upward_state(H_hat[j-1])
                if j == len(H_hat) - 1:
                    state_above = None
                    layer_above = None
                else:
                    state_above = dbm.hidden_layers[j+1].downward_state(H_hat[j+1])
                    layer_above = dbm.hidden_layers[j+1]
                #end if j
                H_hat[j] = dbm.hidden_layers[j].mf_update(
                        state_below = state_below,
                        state_above = state_above,
                        layer_above = layer_above)
                if Y is not None and j == len(dbm.hidden_layers) - 1:
                    Y_hat_unmasked = H_hat[j]
                    H_hat[j] = drop_mask_Y * H_hat[j] + (1 - drop_mask_Y) * Y
                #end if y
            #end for j
            if block_grad == i:
                V_hat = block_gradient(V_hat)
                V_hat_unmasked = block_gradient(V_hat_unmasked)
                H_hat = block(H_hat)
            update_history()
        #end for i

        # debugging, make sure V didn't get changed in this function
        assert V is orig_V
        assert drop_mask is orig_drop_mask

        Y_hat = H_hat[-1]

        assert V in theano.gof.graph.ancestors([V_hat])
        if Y is not None:
            assert V in theano.gof.graph.ancestors([Y_hat])

        if return_history:
            return history
        else:
            if Y is not None:
                return V_hat, Y_hat
            return V_hat


class MoreConsistent(SuperWeightDoubling):
    """
    There's an oddity in SuperWeightDoubling where during the inpainting, we
    initialize Y_hat to sigmoid(biases) if a clean Y is passed in and 2 * weights
    otherwise. I believe but ought to check that mf always does weight doubling.
    This class makes the two more consistent by just implementing mf as calling
    inpainting with Y masked out.
    """

    def mf(self, V, Y = None, return_history = False, niter = None, block_grad = None):
        """
        .. todo::

            WRITEME
        """

        drop_mask = T.zeros_like(V)

        if Y is not None:
            # Y is observed, specify that it's fully observed
            drop_mask_Y = T.zeros_like(Y)
        else:
            # Y is not observed
            last_layer = self.dbm.hidden_layers[-1]
            if isinstance(last_layer, Softmax):
                # Y is not observed, the model has a Y variable, fill in a dummy one
                # and specify that no element of it is observed
                batch_size = self.dbm.get_input_space().batch_size(V)
                num_classes = self.dbm.hidden_layers[-1].n_classes
                assert isinstance(num_classes, int)
                Y = T.alloc(1., batch_size, num_classes)
                drop_mask_Y = T.alloc(1., batch_size)
            else:
                # Y is not observed because the model has no Y variable
                drop_mask_Y = None

        history = self.do_inpainting(V=V,
            Y=Y,
            return_history=True,
            drop_mask=drop_mask,
            drop_mask_Y=drop_mask_Y,
            noise=False,
            niter=niter,
            block_grad=block_grad)

        assert history[-1]['H_hat'][0] is not history[-2]['H_hat'][0] # rm

        if return_history:
            return [elem['H_hat'] for elem in history]

        rval =  history[-1]['H_hat']

        if 'Y_hat_unmasked' in history[-1]:
            rval[-1] = history[-1]['Y_hat_unmasked']

        return rval


class MoreConsistent2(WeightDoubling):
    """
    .. todo::

        WRITEME
    """

    def do_inpainting(self, V, Y = None, drop_mask = None, drop_mask_Y = None,
            return_history = False, noise = False, niter = None, block_grad = None):
        """
        .. todo::

            WRITEME properly

        If you use this method in your research work, please cite:

            Multi-prediction deep Boltzmann machines. Ian J. Goodfellow,
            Mehdi Mirza, Aaron Courville, and Yoshua Bengio. NIPS 2013.


        Gives the mean field expression for units masked out by drop_mask.
        Uses self.niter mean field updates.

        Comes in two variants, unsupervised and supervised:

        * unsupervised: Y and drop_mask_Y are not passed to the method. The
          method produces V_hat, an inpainted version of V
        * supervised: Y and drop_mask_Y are passed to the method. The method
          produces V_hat and Y_hat

        Parameters
        ----------
        V : tensor_like
            Theano batch in `model.input_space`
        Y : tensor_like
            Theano batch in `model.output_space`, i.e. in the output space of
            the last hidden layer. (It's not really a hidden layer anymore,
            but oh well. It's convenient to code it this way because the
            labels are sort of "on top" of everything else.) *** Y is always
            assumed to be a matrix of one-hot category labels. ***
        drop_mask : tensor_like
            Theano batch in `model.input_space`. Should be all binary, with
            1s indicating that the corresponding element of X should be
            "dropped", i.e. hidden from the algorithm and filled in as part
            of the inpainting process
        drop_mask_Y : tensor_like
            Theano vector. Since we assume Y is a one-hot matrix, each row is
            a single categorical variable. `drop_mask_Y` is a binary mask
            specifying which *rows* to drop.
        return_history : bool, optional
            WRITEME
        noise : bool, optional
            WRITEME
        niter : int, optional
            WRITEME
        block_grad : WRITEME

        Returns
        -------
        WRITEME
        """

        dbm = self.dbm
        """TODO: Should add unit test that calling this with a batch of
                 different inputs should yield the same output for each
                 if noise is False and drop_mask is all 1s"""

        if niter is None:
            niter = dbm.niter

        assert drop_mask is not None
        assert return_history in [True, False]
        assert noise in [True, False]
        if Y is None:
            if drop_mask_Y is not None:
                raise ValueError("do_inpainting got drop_mask_Y but not Y.")
        else:
            if drop_mask_Y is None:
                raise ValueError("do_inpainting got Y but not drop_mask_Y.")

        if Y is not None:
            assert isinstance(dbm.hidden_layers[-1], Softmax)
            if drop_mask_Y.ndim != 1:
                raise ValueError("do_inpainting assumes Y is a matrix of one-hot labels,"
    "so each example is only one variable. drop_mask_Y should "
    "therefore be a vector, but we got something with ndim " +
                        str(drop_mask_Y.ndim))
            drop_mask_Y = drop_mask_Y.dimshuffle(0, 'x')

        orig_V = V
        orig_drop_mask = drop_mask

        history = []

        V_hat, V_hat_unmasked = dbm.visible_layer.init_inpainting_state(V,drop_mask,noise, return_unmasked = True)
        assert V_hat_unmasked.ndim > 1

        H_hat = []
        for i in xrange(0,len(dbm.hidden_layers)-1):
            #do double weights update for_layer_i
            if i == 0:
                H_hat.append(dbm.hidden_layers[i].mf_update(
                    state_above = None,
                    double_weights = True,
                    state_below = dbm.visible_layer.upward_state(V_hat),
                    iter_name = '0'))
            else:
                H_hat.append(dbm.hidden_layers[i].mf_update(
                    state_above = None,
                    double_weights = True,
                    state_below = dbm.hidden_layers[i-1].upward_state(H_hat[i-1]),
                    iter_name = '0'))
        # Last layer does not need its weights doubled, even on the first pass
        if len(dbm.hidden_layers) > 1:
            H_hat.append(dbm.hidden_layers[-1].mf_update(
                state_above = None,
                #layer_above = None,
                state_below = dbm.hidden_layers[-2].upward_state(H_hat[-1])))
        else:
            H_hat.append(dbm.hidden_layers[-1].mf_update(
                state_above = None,
                state_below = dbm.visible_layer.upward_state(V_hat)))

        if Y is not None:
            Y_hat_unmasked = H_hat[-1]
            dirty_term = drop_mask_Y * Y_hat_unmasked
            clean_term = (1 - drop_mask_Y) * Y
            Y_hat = dirty_term + clean_term
            H_hat[-1] = Y_hat
            """
            if len(dbm.hidden_layers) > 1:
                i = len(dbm.hidden_layers) - 2
                if i == 0:
                    H_hat[i] = dbm.hidden_layers[i].mf_update(
                        state_above = Y_hat,
                        layer_above = dbm.hidden_layers[-1],
                        state_below = dbm.visible_layer.upward_state(V_hat),
                        iter_name = '0')
                else:
                    H_hat[i] = dbm.hidden_layers[i].mf_update(
                        state_above = Y_hat,
                        layer_above = dbm.hidden_layers[-1],
                        state_below = dbm.hidden_layers[i-1].upward_state(H_hat[i-1]),
                        iter_name = '0')
            """


        def update_history():
            assert V_hat_unmasked.ndim > 1
            d =  { 'V_hat' :  V_hat, 'H_hat' : list(H_hat), 'V_hat_unmasked' : V_hat_unmasked }
            if Y is not None:
                d['Y_hat_unmasked'] = Y_hat_unmasked
                d['Y_hat'] = H_hat[-1]
            history.append(d)

        if block_grad == 1:
            V_hat = block_gradient(V_hat)
            V_hat_unmasked = block_gradient(V_hat_unmasked)
            H_hat = block(H_hat)
        update_history()

        for i in xrange(niter-1):
            for j in xrange(0, len(H_hat), 2):
                if j == 0:
                    state_below = dbm.visible_layer.upward_state(V_hat)
                else:
                    state_below = dbm.hidden_layers[j-1].upward_state(H_hat[j-1])
                if j == len(H_hat) - 1:
                    state_above = None
                    layer_above = None
                else:
                    state_above = dbm.hidden_layers[j+1].downward_state(H_hat[j+1])
                    layer_above = dbm.hidden_layers[j+1]
                H_hat[j] = dbm.hidden_layers[j].mf_update(
                        state_below = state_below,
                        state_above = state_above,
                        layer_above = layer_above)
                if Y is not None and j == len(dbm.hidden_layers) - 1:
                    Y_hat_unmasked = H_hat[j]
                    H_hat[j] = drop_mask_Y * H_hat[j] + (1 - drop_mask_Y) * Y

            V_hat, V_hat_unmasked = dbm.visible_layer.inpaint_update(
                    state_above = dbm.hidden_layers[0].downward_state(H_hat[0]),
                    layer_above = dbm.hidden_layers[0],
                    V = V,
                    drop_mask = drop_mask, return_unmasked = True)
            V_hat.name = 'V_hat[%d](V_hat = %s)' % (i, V_hat.name)

            for j in xrange(1,len(H_hat),2):
                state_below = dbm.hidden_layers[j-1].upward_state(H_hat[j-1])
                if j == len(H_hat) - 1:
                    state_above = None
                    layer_above = None
                else:
                    state_above = dbm.hidden_layers[j+1].downward_state(H_hat[j+1])
                    layer_above = dbm.hidden_layers[j+1]
                #end if j
                H_hat[j] = dbm.hidden_layers[j].mf_update(
                        state_below = state_below,
                        state_above = state_above,
                        layer_above = layer_above)
                if Y is not None and j == len(dbm.hidden_layers) - 1:
                    Y_hat_unmasked = H_hat[j]
                    H_hat[j] = drop_mask_Y * H_hat[j] + (1 - drop_mask_Y) * Y
                #end if y
            #end for j
            if block_grad == i:
                V_hat = block_gradient(V_hat)
                V_hat_unmasked = block_gradient(V_hat_unmasked)
                H_hat = block(H_hat)
            update_history()
        #end for i

        # debugging, make sure V didn't get changed in this function
        assert V is orig_V
        assert drop_mask is orig_drop_mask

        Y_hat = H_hat[-1]

        assert V in theano.gof.graph.ancestors([V_hat])
        if Y is not None:
            assert V in theano.gof.graph.ancestors([Y_hat])

        if return_history:
            return history
        else:
            if Y is not None:
                return V_hat, Y_hat
            return V_hat


class BiasInit(InferenceProcedure):
    """
    An InferenceProcedure that initializes the mean field parameters based on the
    biases in the model. This InferenceProcedure uses the same weights at every
    iteration, rather than doubling the weights on the first pass.
    """

    def mf(self, V, Y = None, return_history = False, niter = None, block_grad = None):
        """
        .. todo::

            WRITEME
        """

        dbm = self.dbm

        assert Y not in [True, False, 0, 1]
        assert return_history in [True, False, 0, 1]

        if Y is not None:
            dbm.hidden_layers[-1].get_output_space().validate(Y)

        if niter is None:
            niter = dbm.niter

        H_hat = [None] + [layer.init_mf_state() for layer in dbm.hidden_layers[1:]]

        # Make corrections for if we're also running inference on Y
        if Y is not None:
            # Last layer is clamped to Y
            H_hat[-1] = Y

        history = [ list(H_hat) ]

        #we only need recurrent inference if there are multiple layers
        assert (niter > 1) == (len(dbm.hidden_layers) > 1)

        for i in xrange(niter):
            for j in xrange(0,len(H_hat),2):
                if j == 0:
                    state_below = dbm.visible_layer.upward_state(V)
                else:
                    state_below = dbm.hidden_layers[j-1].upward_state(H_hat[j-1])
                if j == len(H_hat) - 1:
                    state_above = None
                    layer_above = None
                else:
                    state_above = dbm.hidden_layers[j+1].downward_state(H_hat[j+1])
                    layer_above = dbm.hidden_layers[j+1]
                H_hat[j] = dbm.hidden_layers[j].mf_update(
                        state_below = state_below,
                        state_above = state_above,
                        layer_above = layer_above)

            if Y is not None:
                H_hat[-1] = Y

            for j in xrange(1,len(H_hat),2):
                state_below = dbm.hidden_layers[j-1].upward_state(H_hat[j-1])
                if j == len(H_hat) - 1:
                    state_above = None
                    state_above = None
                else:
                    state_above = dbm.hidden_layers[j+1].downward_state(H_hat[j+1])
                    layer_above = dbm.hidden_layers[j+1]
                H_hat[j] = dbm.hidden_layers[j].mf_update(
                        state_below = state_below,
                        state_above = state_above,
                        layer_above = layer_above)
                #end ifelse
            #end for odd layer

            if Y is not None:
                H_hat[-1] = Y

            for i, elem in enumerate(H_hat):
                if elem is Y:
                    assert i == len(H_hat) -1
                    continue
                else:
                    assert elem not in history[-1]


            if block_grad == i + 1:
                H_hat = block(H_hat)

            history.append(list(H_hat))
        # end for mf iter

        # Run some checks on the output
        for layer, state in safe_izip(dbm.hidden_layers, H_hat):
            upward_state = layer.upward_state(state)
            layer.get_output_space().validate(upward_state)

        if Y is not None:
            assert H_hat[-1] is Y
            inferred = H_hat[:-1]
        else:
            inferred = H_hat
        for elem in flatten(inferred):
            for value in get_debug_values(elem):
                assert value.shape[0] == dbm.batch_size
            if V not in theano.gof.graph.ancestors([elem]):
                logger.error("{0} "
                             "does not have V as an ancestor!".format(elem))
                logger.error(theano.printing.min_informative_str(V))
                if elem is V:
                    logger.error("this variational parameter *is* V")
                else:
                    logger.error("this variational parameter "
                                 "is not the same as V")
                logger.error("V is {0}".format(V))
                assert False
            if Y is not None:
                assert Y in theano.gof.graph.ancestors([elem])

        if Y is not None:
            assert all([elem[-1] is Y for elem in history])
            assert H_hat[-1] is Y

        for elem in history:
            assert len(elem) == len(dbm.hidden_layers)

        if return_history:
            for hist_elem, H_elem in safe_zip(history[-1], H_hat):
                assert hist_elem is H_elem
            return history
        else:
            return H_hat

    def do_inpainting(self, V, Y = None, drop_mask = None, drop_mask_Y = None,
            return_history = False, noise = False, niter = None, block_grad = None):
        """
        .. todo::

            WRITEME properly

        Gives the mean field expression for units masked out by drop_mask.
        Uses self.niter mean field updates.

        Comes in two variants, unsupervised and supervised:

        * unsupervised: Y and drop_mask_Y are not passed to the method. The
          method produces V_hat, an inpainted version of V.
        * supervised: Y and drop_mask_Y are passed to the method. The method
          produces V_hat and Y_hat.

        If you use this method in your research work, please cite:

            Multi-prediction deep Boltzmann machines. Ian J. Goodfellow,
            Mehdi Mirza, Aaron Courville, and Yoshua Bengio. NIPS 2013.


        Parameters
        ----------
        V : tensor_like
            Theano batch in `model.input_space`
        Y : tensor_like
            Theano batch in model.output_space, ie, in the output space of
            the last hidden layer (it's not really a hidden layer anymore,
            but oh well. It's convenient to code it this way because the
            labels are sort of "on top" of everything else). *** Y is always
            assumed to be a matrix of one-hot category labels. ***
        drop_mask : tensor_like
            A theano batch in `model.input_space`. Should be all binary, with
            1s indicating that the corresponding element of X should be
            "dropped", ie, hidden from the algorithm and filled in as part of
            the inpainting process
        drop_mask_Y : tensor_like
            Theano vector. Since we assume Y is a one-hot matrix, each row is
            a single categorical variable. `drop_mask_Y` is a binary mask
            specifying which *rows* to drop.
        """

        dbm = self.dbm

        """TODO: Should add unit test that calling this with a batch of
                 different inputs should yield the same output for each
                 if noise is False and drop_mask is all 1s"""

        if niter is None:
            niter = dbm.niter


        assert drop_mask is not None
        assert return_history in [True, False]
        assert noise in [True, False]
        if Y is None:
            if drop_mask_Y is not None:
                raise ValueError("do_inpainting got drop_mask_Y but not Y.")
        else:
            if drop_mask_Y is None:
                raise ValueError("do_inpainting got Y but not drop_mask_Y.")

        if Y is not None:
            assert isinstance(dbm.hidden_layers[-1], Softmax)
            if drop_mask_Y.ndim != 1:
                raise ValueError("do_inpainting assumes Y is a matrix of one-hot labels,"
                        "so each example is only one variable. drop_mask_Y should "
                        "therefore be a vector, but we got something with ndim " +
                        str(drop_mask_Y.ndim))
            drop_mask_Y = drop_mask_Y.dimshuffle(0, 'x')

        orig_V = V
        orig_drop_mask = drop_mask

        history = []

        V_hat, V_hat_unmasked = dbm.visible_layer.init_inpainting_state(V,drop_mask,noise, return_unmasked = True)
        assert V_hat_unmasked.ndim > 1

        H_hat = [None] + [layer.init_mf_state() for layer in dbm.hidden_layers[1:]]

        if Y is not None:
            Y_hat_unmasked = dbm.hidden_layers[-1].init_inpainting_state(Y, noise)
            Y_hat = drop_mask_Y * Y_hat_unmasked + (1 - drop_mask_Y) * Y
            H_hat[-1] = Y_hat

        def update_history():
            assert V_hat_unmasked.ndim > 1
            d =  { 'V_hat' :  V_hat, 'H_hat' : H_hat, 'V_hat_unmasked' : V_hat_unmasked }
            if Y is not None:
                d['Y_hat_unmasked'] = Y_hat_unmasked
                d['Y_hat'] = H_hat[-1]
            history.append( d )

        update_history()

        for i in xrange(niter):
            for j in xrange(0, len(H_hat), 2):
                if j == 0:
                    state_below = dbm.visible_layer.upward_state(V_hat)
                else:
                    state_below = dbm.hidden_layers[j-1].upward_state(H_hat[j-1])
                if j == len(H_hat) - 1:
                    state_above = None
                    layer_above = None
                else:
                    state_above = dbm.hidden_layers[j+1].downward_state(H_hat[j+1])
                    layer_above = dbm.hidden_layers[j+1]
                H_hat[j] = dbm.hidden_layers[j].mf_update(
                        state_below = state_below,
                        state_above = state_above,
                        layer_above = layer_above)
                if Y is not None and j == len(dbm.hidden_layers) - 1:
                    Y_hat_unmasked = H_hat[j]
                    H_hat[j] = drop_mask_Y * H_hat[j] + (1 - drop_mask_Y) * Y

            V_hat, V_hat_unmasked = dbm.visible_layer.inpaint_update(
                    state_above = dbm.hidden_layers[0].downward_state(H_hat[0]),
                    layer_above = dbm.hidden_layers[0],
                    V = V,
                    drop_mask = drop_mask, return_unmasked = True)
            V_hat.name = 'V_hat[%d](V_hat = %s)' % (i, V_hat.name)

            for j in xrange(1,len(H_hat),2):
                state_below = dbm.hidden_layers[j-1].upward_state(H_hat[j-1])
                if j == len(H_hat) - 1:
                    state_above = None
                    layer_above = None
                else:
                    state_above = dbm.hidden_layers[j+1].downward_state(H_hat[j+1])
                    layer_above = dbm.hidden_layers[j+1]
                #end if j
                H_hat[j] = dbm.hidden_layers[j].mf_update(
                        state_below = state_below,
                        state_above = state_above,
                        layer_above = layer_above)
                if Y is not None and j == len(dbm.hidden_layers) - 1:
                    Y_hat_unmasked = H_hat[j]
                    H_hat[j] = drop_mask_Y * H_hat[j] + (1 - drop_mask_Y) * Y
                #end if y
            #end for j
            if block_grad == i + 1:
                V_hat = block_gradient(V_hat)
                V_hat_unmasked = block_gradient(V_hat_unmasked)
                H_hat = block(H_hat)
            update_history()
        #end for i

        # debugging, make sure V didn't get changed in this function
        assert V is orig_V
        assert drop_mask is orig_drop_mask

        Y_hat = H_hat[-1]

        assert V in theano.gof.graph.ancestors([V_hat])
        if Y is not None:
            assert V in theano.gof.graph.ancestors([Y_hat])

        if return_history:
            return history
        else:
            if Y is not None:
                return V_hat, Y_hat
            return V_hat


class UpDown(InferenceProcedure):
    """
    An InferenceProcedure that initializes the mean field parameters based on the
    biases in the model, then alternates between updating each of the layers bottom-to-top
    and updating each of the layers top-to-bottom.
    """

    def mf(self, V, Y = None, return_history = False, niter = None, block_grad = None):
        """
        .. todo::

            WRITEME
        """

        dbm = self.dbm

        assert Y not in [True, False, 0, 1]
        assert return_history in [True, False, 0, 1]

        if Y is not None:
            dbm.hidden_layers[-1].get_output_space().validate(Y)

        if niter is None:
            niter = dbm.niter

        H_hat = [None] + [layer.init_mf_state() for layer in dbm.hidden_layers[1:]]

        # Make corrections for if we're also running inference on Y
        if Y is not None:
            # Last layer is clamped to Y
            H_hat[-1] = Y

        history = [ list(H_hat) ]

        #we only need recurrent inference if there are multiple layers
        assert (niter > 1) == (len(dbm.hidden_layers) > 1)

        for i in xrange(niter):
            # Determine whether to go up or down on this iteration
            if i % 2 == 0:
                start = 0
                stop = len(H_hat)
                inc = 1
            else:
                start = len(H_hat) - 1
                stop = -1
                inc = -1
            # Do the mean field updates
            for j in xrange(start, stop, inc):
                if j == 0:
                    state_below = dbm.visible_layer.upward_state(V)
                else:
                    state_below = dbm.hidden_layers[j-1].upward_state(H_hat[j-1])
                if j == len(H_hat) - 1:
                    state_above = None
                    layer_above = None
                else:
                    state_above = dbm.hidden_layers[j+1].downward_state(H_hat[j+1])
                    layer_above = dbm.hidden_layers[j+1]
                H_hat[j] = dbm.hidden_layers[j].mf_update(
                        state_below = state_below,
                        state_above = state_above,
                        layer_above = layer_above)
                if Y is not None:
                    H_hat[-1] = Y

            for j in xrange(1,len(H_hat),2):
                state_below = dbm.hidden_layers[j-1].upward_state(H_hat[j-1])
                if j == len(H_hat) - 1:
                    state_above = None
                    state_above = None
                else:
                    state_above = dbm.hidden_layers[j+1].downward_state(H_hat[j+1])
                    layer_above = dbm.hidden_layers[j+1]
                H_hat[j] = dbm.hidden_layers[j].mf_update(
                        state_below = state_below,
                        state_above = state_above,
                        layer_above = layer_above)
                #end ifelse
            #end for odd layer

            if Y is not None:
                H_hat[-1] = Y

            if block_grad == i + 1:
                H_hat = block(H_hat)

            history.append(list(H_hat))
        # end for mf iter

        # Run some checks on the output
        for layer, state in safe_izip(dbm.hidden_layers, H_hat):
            upward_state = layer.upward_state(state)
            layer.get_output_space().validate(upward_state)
        if Y is not None:
            assert all([elem[-1] is Y for elem in history])
            assert H_hat[-1] is Y

        if return_history:
            return history
        else:
            return H_hat

    def do_inpainting(self, V, Y = None, drop_mask = None, drop_mask_Y = None,
            return_history = False, noise = False, niter = None, block_grad = None):
        """
        .. todo::

            WRITEME properly

        Gives the mean field expression for units masked out by drop_mask.
        Uses self.niter mean field updates.

        Comes in two variants, unsupervised and supervised:

        * unsupervised: Y and drop_mask_Y are not passed to the method. The
          method produces V_hat, an inpainted version of V.
        * supervised: Y and drop_mask_Y are passed to the method. The method
          produces V_hat and Y_hat.

        If you use this method in your research work, please cite:

            Multi-prediction deep Boltzmann machines. Ian J. Goodfellow,
            Mehdi Mirza, Aaron Courville, and Yoshua Bengio. NIPS 2013.


        Parameters
        ----------
        V : tensor_like
            Theano batch in `model.input_space`
        Y : tensor_like
            Theano batch in model.output_space, ie, in the output space of
            the last hidden layer (it's not really a hidden layer anymore,
            but oh well. It's convenient to code it this way because the
            labels are sort of "on top" of everything else). *** Y is always
            assumed to be a matrix of one-hot category labels. ***
        drop_mask : tensor_like
            A theano batch in `model.input_space`. Should be all binary, with
            1s indicating that the corresponding element of X should be
            "dropped", ie, hidden from the algorithm and filled in as part of
            the inpainting process
        drop_mask_Y : tensor_like
            Theano vector. Since we assume Y is a one-hot matrix, each row is
            a single categorical variable. `drop_mask_Y` is a binary mask
            specifying which *rows* to drop.
        """

        if Y is not None:
            assert isinstance(self.hidden_layers[-1], Softmax)

        model = self.dbm

        """TODO: Should add unit test that calling this with a batch of
                 different inputs should yield the same output for each
                 if noise is False and drop_mask is all 1s"""

        if niter is None:
            niter = model.niter


        assert drop_mask is not None
        assert return_history in [True, False]
        assert noise in [True, False]
        if Y is None:
            if drop_mask_Y is not None:
                raise ValueError("do_inpainting got drop_mask_Y but not Y.")
        else:
            if drop_mask_Y is None:
                raise ValueError("do_inpainting got Y but not drop_mask_Y.")

        if Y is not None:
            assert isinstance(model.hidden_layers[-1], Softmax)
            if drop_mask_Y.ndim != 1:
                raise ValueError("do_inpainting assumes Y is a matrix of one-hot labels,"
                        "so each example is only one variable. drop_mask_Y should "
                        "therefore be a vector, but we got something with ndim " +
                        str(drop_mask_Y.ndim))
            drop_mask_Y = drop_mask_Y.dimshuffle(0, 'x')

        orig_V = V
        orig_drop_mask = drop_mask

        history = []

        V_hat, V_hat_unmasked = model.visible_layer.init_inpainting_state(V,drop_mask,noise, return_unmasked = True)
        assert V_hat_unmasked.ndim > 1

        H_hat = [None] + [layer.init_mf_state() for layer in model.hidden_layers[1:]]

        if Y is not None:
            Y_hat_unmasked = model.hidden_layers[-1].init_inpainting_state(Y, noise)
            Y_hat = drop_mask_Y * Y_hat_unmasked + (1 - drop_mask_Y) * Y
            H_hat[-1] = Y_hat

        def update_history():
            assert V_hat_unmasked.ndim > 1
            d =  { 'V_hat' :  V_hat, 'H_hat' : H_hat, 'V_hat_unmasked' : V_hat_unmasked }
            if Y is not None:
                d['Y_hat_unmasked'] = Y_hat_unmasked
                d['Y_hat'] = H_hat[-1]
            history.append( d )

        update_history()

        for i in xrange(niter):

            if i % 2 == 0:
                start = 0
                stop = len(H_hat)
                inc = 1
                if i > 0:
                    # Don't start by updating V_hat on iteration 0 or this will throw out the
                    # noise
                    V_hat, V_hat_unmasked = model.visible_layer.inpaint_update(
                            state_above = model.hidden_layers[0].downward_state(H_hat[0]),
                            layer_above = model.hidden_layers[0],
                            V = V,
                            drop_mask = drop_mask, return_unmasked = True)
                    V_hat.name = 'V_hat[%d](V_hat = %s)' % (i, V_hat.name)
            else:
                start = len(H_hat) - 1
                stop = -1
                inc = -1
            for j in xrange(start, stop, inc):
                if j == 0:
                    state_below = model.visible_layer.upward_state(V_hat)
                else:
                    state_below = model.hidden_layers[j-1].upward_state(H_hat[j-1])
                if j == len(H_hat) - 1:
                    state_above = None
                    layer_above = None
                else:
                    state_above = model.hidden_layers[j+1].downward_state(H_hat[j+1])
                    layer_above = model.hidden_layers[j+1]
                H_hat[j] = model.hidden_layers[j].mf_update(
                        state_below = state_below,
                        state_above = state_above,
                        layer_above = layer_above)
                if Y is not None and j == len(model.hidden_layers) - 1:
                    Y_hat_unmasked = H_hat[j]
                    H_hat[j] = drop_mask_Y * H_hat[j] + (1 - drop_mask_Y) * Y

            if i % 2 == 1:
                V_hat, V_hat_unmasked = model.visible_layer.inpaint_update(
                        state_above = model.hidden_layers[0].downward_state(H_hat[0]),
                        layer_above = model.hidden_layers[0],
                        V = V,
                        drop_mask = drop_mask, return_unmasked = True)
                V_hat.name = 'V_hat[%d](V_hat = %s)' % (i, V_hat.name)

            if block_grad == i + 1:
                V_hat = block_gradient(V_hat)
                V_hat_unmasked = block_gradient(V_hat_unmasked)
                H_hat = block(H_hat)
            update_history()
        #end for i

        # debugging, make sure V didn't get changed in this function
        assert V is orig_V
        assert drop_mask is orig_drop_mask

        Y_hat = H_hat[-1]

        assert V in theano.gof.graph.ancestors([V_hat])
        if Y is not None:
            assert V in theano.gof.graph.ancestors([Y_hat])

        if return_history:
            return history
        else:
            if Y is not None:
                return V_hat, Y_hat
            return V_hat

########NEW FILE########
__FILENAME__ = ising
"""
Implementation of a densely connected Ising model in the
pylearn2.models.dbm framework

Notes
-----
If :math:`h` can be -1 or 1, and

.. math::

    p(h) = \exp(T\dot z \dot h),

then the expected value of :math:`h` is given by

.. math::

    \\tanh(T \dot z),

and the probability that :math:`h` is 1 is given by

.. math::

    \sigma(2T \dot z)
"""

__authors__ = ["Ian Goodfellow", "Vincent Dumoulin"]
__copyright__ = "Copyright 2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"

import numpy as np

from theano.compat.python2x import OrderedDict

from theano import function
from theano.gof.op import get_debug_values
from theano.compile.sharedvalue import SharedVariable
import theano.tensor as T
import warnings

from pylearn2.expr.nnet import sigmoid_numpy
from pylearn2.linear.matrixmul import MatrixMul
from pylearn2.models.dbm import init_sigmoid_bias_from_array
from pylearn2.models.dbm.layer import HiddenLayer, VisibleLayer
from pylearn2.space import Conv2DSpace
from pylearn2.space import VectorSpace
from pylearn2.utils import sharedX
from pylearn2.utils.rng import make_theano_rng


def init_tanh_bias_from_marginals(dataset, use_y=False):
    """
    .. todo::

        WRITEME
    """
    if use_y:
        X = dataset.y
    else:
        X = dataset.get_design_matrix()
    if not (X.max() == 1):
        raise ValueError("Expected design matrix to consist entirely "
                         "of 0s and 1s, but maximum value is "+str(X.max()))
    assert X.min() == -1.

    mean = X.mean(axis=0)

    mean = np.clip(mean, 1e-7, 1-1e-7)

    init_bias = np.arctanh(mean)

    return init_bias


class IsingVisible(VisibleLayer):
    """
    A DBM visible layer consisting of random variables living
    in a `VectorSpace`, with values in {-1, 1}.

    Implements the energy function term :math:`-\mathbf{b}^T \mathbf{h}`.

    Parameters
    ----------
    nvis : int
        The dimension of the space
    beta : theano shared variable
        Shared variable representing a multiplicative factor of the
        energy function (the inverse temperature)
    learn_beta : boolean, optional
        Whether or not the inverse temperature should be considered as a
        learned parameter
    bias_from_marginals : `pylearn2.datasets.dataset.Dataset`, optional
        A dataset whose marginals are used to initialize the visible
        biases
    """

    def __init__(self, nvis, beta, learn_beta=False, bias_from_marginals=None):
        if not isinstance(beta, SharedVariable):
            raise ValueError("beta needs to be a theano shared variable.")
        self.__dict__.update(locals())
        del self.self
        # Don't serialize the dataset
        del self.bias_from_marginals

        self.space = VectorSpace(nvis)
        self.input_space = self.space

        origin = self.space.get_origin()

        if bias_from_marginals is None:
            init_bias = np.zeros((nvis,))
        else:
            init_bias = init_tanh_bias_from_marginals(bias_from_marginals)

        self.bias = sharedX(init_bias, 'visible_bias')

    def get_biases(self):
        """
        .. todo::

            WRITEME
        """
        return self.bias.get_value()

    def set_biases(self, biases, recenter=False):
        """
        .. todo::

            WRITEME
        """
        self.bias.set_value(biases)
        if recenter:
            assert self.center
            self.offset.set_value(sigmoid_numpy(self.bias.get_value()))

    def upward_state(self, total_state):
        """
        .. todo::

            WRITEME
        """
        return total_state

    def get_params(self):
        """
        .. todo::

            WRITEME
        """
        rval = [self.bias]
        if self.learn_beta:
            rval.append(self.beta)
        return rval

    def mf_update(self, state_above, layer_above):
        """
        .. todo::

            WRITEME
        """
        msg = layer_above.downward_message(state_above)

        bias = self.bias

        z = msg + bias

        rval = T.tanh(self.beta * z)

        return rval

    def sample(self, state_below=None, state_above=None, layer_above=None,
               theano_rng=None):
        """
        .. todo::

            WRITEME
        """

        assert state_below is None

        msg = layer_above.downward_message(state_above)

        bias = self.bias

        z = msg + bias

        phi = T.nnet.sigmoid(2. * self.beta * z)

        rval = theano_rng.binomial(size=phi.shape, p=phi, dtype=phi.dtype, n=1)

        return rval * 2. - 1.

    def make_state(self, num_examples, numpy_rng):
        """
        .. todo::

            WRITEME
        """
        driver = numpy_rng.uniform(0., 1., (num_examples, self.nvis))
        on_prob = sigmoid_numpy(2. * self.beta.get_value() *
                                self.bias.get_value())
        sample = 2. * (driver < on_prob) - 1.

        rval = sharedX(sample, name='v_sample_shared')

        return rval

    def make_symbolic_state(self, num_examples, theano_rng):
        """
        .. todo::

            WRITEME
        """
        mean = T.nnet.sigmoid(2. * self.beta * self.b)
        rval = theano_rng.binomial(size=(num_examples, self.nvis), p=mean)
        rval = 2. * (rval) - 1.

        return rval

    def expected_energy_term(self, state, average, state_below=None,
                             average_below=None):
        """
        .. todo::

            WRITEME
        """
        assert state_below is None
        assert average_below is None
        assert average in [True, False]
        self.space.validate(state)

        # Energy function is linear so it doesn't matter if we're averaging
        # or not
        rval = -(self.beta * T.dot(state, self.bias))

        assert rval.ndim == 1

        return rval


class IsingHidden(HiddenLayer):
    """
    A hidden layer with :math:`\mathbf{h}` being a vector in {-1, 1},
    implementing the energy function term

    .. math::

        -\mathbf{v}^T \mathbf{W}\mathbf{h} -\mathbf{b}^T \mathbf{h}

    where :math:`\mathbf{W}` and :math:`\mathbf{b}` are parameters of this
    layer, and :math:`\mathbf{v}` is the upward state of the layer below.

    Parameters
    ----------
    dim : WRITEME
    layer_name : WRITEME
    beta : theano shared variable
        Shared variable representing a multiplicative factor of the energy
        function (the inverse temperature)
    learn_beta : boolean, optional
        Whether or not the inverse temperature should be considered as a
        learned parameter
    irange : WRITEME
    sparse_init : WRITEME
    sparse_stdev : WRITEME
    include_prob : float, optional
        Probability of including a weight element in the set of weights
        initialized to U(-irange, irange). If not included it is
        initialized to 0.
    init_bias : WRITEME
    W_lr_scale : WRITEME
    b_lr_scale : WRITEME
    max_col_norm : WRITEME
    """

    def __init__(self,
                 dim,
                 layer_name,
                 beta,
                 learn_beta=False,
                 irange=None,
                 sparse_init=None,
                 sparse_stdev=1.,
                 include_prob=1.0,
                 init_bias=0.,
                 W_lr_scale=None,
                 b_lr_scale=None,
                 max_col_norm=None):
        if not isinstance(beta, SharedVariable):
            raise ValueError("beta needs to be a theano shared variable.")
        self.__dict__.update(locals())
        del self.self

        self.b = sharedX(np.zeros((self.dim,)) + init_bias,
                         name=layer_name + '_b')

    def get_lr_scalers(self):
        """
        .. todo::

            WRITEME
        """

        if not hasattr(self, 'W_lr_scale'):
            self.W_lr_scale = None

        if not hasattr(self, 'b_lr_scale'):
            self.b_lr_scale = None

        rval = OrderedDict()

        if self.W_lr_scale is not None:
            W, = self.transformer.get_params()
            rval[W] = self.W_lr_scale

        if self.b_lr_scale is not None:
            rval[self.b] = self.b_lr_scale

        return rval

    def set_input_space(self, space):
        """
        .. todo::

            WRITEME properly

        Notes
        -----
        Note: this resets parameters!
        """

        self.input_space = space

        if isinstance(space, VectorSpace):
            self.requires_reformat = False
            self.input_dim = space.dim
        else:
            self.requires_reformat = True
            self.input_dim = space.get_total_dimension()
            self.desired_space = VectorSpace(self.input_dim)

        self.output_space = VectorSpace(self.dim)

        rng = self.dbm.rng
        if self.irange is not None:
            assert self.sparse_init is None
            W = rng.uniform(-self.irange, self.irange,
                            (self.input_dim, self.dim)) * \
                (rng.uniform(0., 1., (self.input_dim, self.dim))
                    < self.include_prob)
        else:
            assert self.sparse_init is not None
            W = np.zeros((self.input_dim, self.dim))
            W *= self.sparse_stdev

        W = sharedX(W)
        W.name = self.layer_name + '_W'

        self.transformer = MatrixMul(W)

        W, = self.transformer.get_params()
        assert W.name is not None

    def _modify_updates(self, updates):
        """
        .. todo::

            WRITEME
        """

        if self.max_col_norm is not None:
            W, = self.transformer.get_params()
            if W in updates:
                updated_W = updates[W]
                col_norms = T.sqrt(T.sum(T.sqr(updated_W), axis=0))
                desired_norms = T.clip(col_norms, 0, self.max_col_norm)
                updates[W] = updated_W * (desired_norms / (1e-7 + col_norms))

    def get_total_state_space(self):
        """
        .. todo::

            WRITEME
        """
        return VectorSpace(self.dim)

    def get_params(self):
        """
        .. todo::

            WRITEME
        """
        assert self.b.name is not None
        W, = self.transformer.get_params()
        assert W.name is not None
        rval = self.transformer.get_params()
        assert not isinstance(rval, set)
        rval = list(rval)
        assert self.b not in rval
        rval.append(self.b)
        if self.learn_beta:
            rval.append(self.beta)
        return rval

    def get_weight_decay(self, coeff):
        """
        .. todo::

            WRITEME
        """
        if isinstance(coeff, str):
            coeff = float(coeff)
        assert isinstance(coeff, float) or hasattr(coeff, 'dtype')
        W, = self.transformer.get_params()
        return coeff * T.sqr(W).sum()

    def get_weights(self):
        """
        .. todo::

            WRITEME
        """
        if self.requires_reformat:
            # This is not really an unimplemented case.
            # We actually don't know how to format the weights
            # in design space. We got the data in topo space
            # and we don't have access to the dataset
            raise NotImplementedError()
        W, = self.transformer.get_params()
        return W.get_value()

    def set_weights(self, weights):
        """
        .. todo::

            WRITEME
        """
        W, = self.transformer.get_params()
        W.set_value(weights)

    def set_biases(self, biases, recenter=False):
        """
        .. todo::

            WRITEME
        """
        self.b.set_value(biases)
        if recenter:
            assert self.center
            if self.pool_size != 1:
                raise NotImplementedError()
            self.offset.set_value(sigmoid_numpy(self.b.get_value()))

    def get_biases(self):
        """
        .. todo::

            WRITEME
        """
        return self.b.get_value()

    def get_weights_format(self):
        """
        .. todo::

            WRITEME
        """
        return ('v', 'h')

    def get_weights_topo(self):
        """
        .. todo::

            WRITEME
        """

        if not isinstance(self.input_space, Conv2DSpace):
            raise NotImplementedError()

        W, = self.transformer.get_params()

        W = W.T

        W = W.reshape(
            (self.detector_layer_dim, self.input_space.shape[0],
             self.input_space.shape[1], self.input_space.nchannels)
        )

        W = Conv2DSpace.convert(W, self.input_space.axes, ('b', 0, 1, 'c'))

        return function([], W)()

    def upward_state(self, total_state):
        """
        .. todo::

            WRITEME
        """
        return total_state

    def downward_state(self, total_state):
        """
        .. todo::

            WRITEME
        """
        return total_state

    def get_monitoring_channels(self):
        """
        .. todo::

            WRITEME
        """

        W, = self.transformer.get_params()

        assert W.ndim == 2

        sq_W = T.sqr(W)

        row_norms = T.sqrt(sq_W.sum(axis=1))
        col_norms = T.sqrt(sq_W.sum(axis=0))

        return OrderedDict([
            ('row_norms_min', row_norms.min()),
            ('row_norms_mean', row_norms.mean()),
            ('row_norms_max', row_norms.max()),
            ('col_norms_min', col_norms.min()),
            ('col_norms_mean', col_norms.mean()),
            ('col_norms_max', col_norms.max()),
        ])

    def get_monitoring_channels_from_state(self, state):
        """
        .. todo::

            WRITEME
        """

        P = state

        rval = OrderedDict()

        vars_and_prefixes = [(P, '')]

        for var, prefix in vars_and_prefixes:
            v_max = var.max(axis=0)
            v_min = var.min(axis=0)
            v_mean = var.mean(axis=0)
            v_range = v_max - v_min

            # max_x.mean_u is "the mean over *u*nits of the max over
            # e*x*amples". The x and u are included in the name because
            # otherwise its hard to remember which axis is which when reading
            # the monitor I use inner.outer rather than outer_of_inner or
            # something like that because I want mean_x.* to appear next to
            # each other in the alphabetical list, as these are commonly
            # plotted together
            for key, val in [
                ('max_x.max_u', v_max.max()),
                ('max_x.mean_u', v_max.mean()),
                ('max_x.min_u', v_max.min()),
                ('min_x.max_u', v_min.max()),
                ('min_x.mean_u', v_min.mean()),
                ('min_x.min_u', v_min.min()),
                ('range_x.max_u', v_range.max()),
                ('range_x.mean_u', v_range.mean()),
                ('range_x.min_u', v_range.min()),
                ('mean_x.max_u', v_mean.max()),
                ('mean_x.mean_u', v_mean.mean()),
                ('mean_x.min_u', v_mean.min()),
            ]:
                rval[prefix+key] = val

        return rval

    def sample(self, state_below=None, state_above=None, layer_above=None,
               theano_rng=None):
        """
        .. todo::

            WRITEME
        """

        if theano_rng is None:
            raise ValueError("theano_rng is required; it just defaults to " +
                             "None so that it may appear after layer_above " +
                             "/ state_above in the list.")

        if state_above is not None:
            msg = layer_above.downward_message(state_above)
        else:
            msg = None

        if self.requires_reformat:
            state_below = self.input_space.format_as(state_below,
                                                     self.desired_space)

        z = self.transformer.lmul(state_below) + self.b

        if msg is not None:
            z = z + msg

        on_prob = T.nnet.sigmoid(2. * self.beta * z)

        samples = theano_rng.binomial(p=on_prob, n=1, size=on_prob.shape,
                                      dtype=on_prob.dtype) * 2. - 1.

        return samples

    def downward_message(self, downward_state):
        """
        .. todo::

            WRITEME
        """
        rval = self.transformer.lmul_T(downward_state)

        if self.requires_reformat:
            rval = self.desired_space.format_as(rval, self.input_space)

        return rval

    def init_mf_state(self):
        """
        .. todo::

            WRITEME
        """
        # work around theano bug with broadcasted vectors
        z = T.alloc(0., self.dbm.batch_size,
                    self.dim).astype(self.b.dtype) + \
            self.b.dimshuffle('x', 0)
        rval = T.tanh(self.beta * z)
        return rval

    def make_state(self, num_examples, numpy_rng):
        """
        .. todo::

            WRITEME properly

        Returns a shared variable containing an actual state
        (not a mean field state) for this variable.
        """
        driver = numpy_rng.uniform(0., 1., (num_examples, self.dim))
        on_prob = sigmoid_numpy(2. * self.beta.get_value() *
                                self.b.get_value())
        sample = 2. * (driver < on_prob) - 1.

        rval = sharedX(sample, name='v_sample_shared')

        return rval

    def make_symbolic_state(self, num_examples, theano_rng):
        """
        .. todo::

            WRITEME
        """
        mean = T.nnet.sigmoid(2. * self.beta * self.b)
        rval = theano_rng.binomial(size=(num_examples, self.nvis), p=mean)
        rval = 2. * (rval) - 1.

        return rval

    def expected_energy_term(self, state, average, state_below, average_below):
        """
        .. todo::

            WRITEME
        """

        # state = Print('h_state', attrs=['min', 'max'])(state)

        self.input_space.validate(state_below)

        if self.requires_reformat:
            if not isinstance(state_below, tuple):
                for sb in get_debug_values(state_below):
                    if sb.shape[0] != self.dbm.batch_size:
                        raise ValueError("self.dbm.batch_size is %d but got " +
                                         "shape of %d" % (self.dbm.batch_size,
                                                          sb.shape[0]))
                    assert reduce(lambda x, y: x * y, sb.shape[1:]) == self.input_dim

            state_below = self.input_space.format_as(state_below,
                                                     self.desired_space)

        # Energy function is linear so it doesn't matter if we're averaging or
        # not. Specifically, our terms are -u^T W d - b^T d where u is the
        # upward state of layer below and d is the downward state of this layer

        bias_term = T.dot(state, self.b)
        weights_term = (self.transformer.lmul(state_below) * state).sum(axis=1)

        rval = -bias_term - weights_term

        rval *= self.beta

        assert rval.ndim == 1

        return rval

    def linear_feed_forward_approximation(self, state_below):
        """
        .. todo::

            WRITEME properly

        Used to implement TorontoSparsity. Unclear exactly what properties of
        it are important or how to implement it for other layers.

        Properties it must have:
            output is same kind of data structure (ie, tuple of theano
            2-tensors) as mf_update

        Properties it probably should have for other layer types:
            An infinitesimal change in state_below or the parameters should
            cause the same sign of change in the output of
            linear_feed_forward_approximation and in mf_update

            Should not have any non-linearities that cause the gradient to
            shrink

            Should disregard top-down feedback
        """

        z = self.beta * (self.transformer.lmul(state_below) + self.b)

        if self.pool_size != 1:
            # Should probably implement sum pooling for the non-pooled version,
            # but in reality it's not totally clear what the right answer is
            raise NotImplementedError()

        return z, z

    def mf_update(self, state_below, state_above, layer_above=None,
                  double_weights=False, iter_name=None):
        """
        .. todo::

            WRITEME
        """

        self.input_space.validate(state_below)

        if self.requires_reformat:
            if not isinstance(state_below, tuple):
                for sb in get_debug_values(state_below):
                    if sb.shape[0] != self.dbm.batch_size:
                        raise ValueError("self.dbm.batch_size is %d but got " +
                                         "shape of %d" % (self.dbm.batch_size,
                                                          sb.shape[0]))
                    assert reduce(lambda x, y: x * y, sb.shape[1:]) == self.input_dim

            state_below = self.input_space.format_as(state_below,
                                                     self.desired_space)

        if iter_name is None:
            iter_name = 'anon'

        if state_above is not None:
            assert layer_above is not None
            msg = layer_above.downward_message(state_above)
            msg.name = 'msg_from_' + layer_above.layer_name + '_to_' + \
                       self.layer_name + '[' + iter_name + ']'
        else:
            msg = None

        if double_weights:
            state_below = 2. * state_below
            state_below.name = self.layer_name + '_'+iter_name + '_2state'
        z = self.transformer.lmul(state_below) + self.b
        if self.layer_name is not None and iter_name is not None:
            z.name = self.layer_name + '_' + iter_name + '_z'
        if msg is not None:
            z = z + msg
        h = T.tanh(self.beta * z)

        return h


class BoltzmannIsingVisible(VisibleLayer):
    """
    An IsingVisible whose parameters are defined in Boltzmann machine space.

    Notes
    -----
    All parameter noise/clipping is handled by BoltzmannIsingHidden.

    .. todo::

        WRITEME properly

    Parameters
    ----------
    nvis : int
        Number of visible units
    beta : theano shared variable
        Shared variable representing a multiplicative factor of the energy
        function (the inverse temperature)
    learn_beta : boolean, optional
        Whether or not the inverse temperature should be considered
            as a learned parameter
    bias_from_marginals : `pylearn2.datasets.dataset.Dataset`, optional
        A dataset whose marginals are used to initialize the visible
        biases
    sampling_b_stdev : WRITEME
    min_ising_b : WRITEME
    max_ising_b : WRITEME
    """

    def __init__(self, nvis, beta, learn_beta=False, bias_from_marginals=None,
                 sampling_b_stdev=None, min_ising_b=None, max_ising_b=None):

        if not isinstance(beta, SharedVariable):
            raise ValueError("beta needs to be a theano shared " +
                             "variable.")
        self.__dict__.update(locals())
        del self.self
        # Don't serialize the dataset
        del self.bias_from_marginals

        self.space = VectorSpace(nvis)
        self.input_space = self.space

        if bias_from_marginals is None:
            init_bias = np.zeros((nvis,))
        else:
            # data is in [-1, 1], but want biases for a sigmoid
            init_bias = \
                init_sigmoid_bias_from_array(bias_from_marginals.X / 2. + 0.5)
            # init_bias =
        self.boltzmann_bias = sharedX(init_bias, 'visible_bias')

        self.resample_fn = None

    def finalize_initialization(self):
        """
        .. todo::

            WRITEME
        """
        if self.sampling_b_stdev is not None:
            self.noisy_sampling_b = \
                sharedX(np.zeros((self.layer_above.dbm.batch_size, self.nvis)))

        updates = OrderedDict()
        updates[self.boltzmann_bias] = self.boltzmann_bias
        updates[self.layer_above.W] = self.layer_above.W
        self.enforce_constraints()

    def _modify_updates(self, updates):
        """
        .. todo::

            WRITEME
        """
        beta = self.beta
        if beta in updates:
            updated_beta = updates[beta]
            updates[beta] = T.clip(updated_beta, 1., 1000.)

        if any(constraint is not None for constraint in [self.min_ising_b,
                                                         self.max_ising_b]):
            bmn = self.min_ising_b
            if bmn is None:
                bmn = - 1e6
            bmx = self.max_ising_b
            if bmx is None:
                bmx = 1e6
            wmn_above = self.layer_above.min_ising_W
            if wmn_above is None:
                wmn_above = - 1e6
            wmx_above = self.layer_above.max_ising_W
            if wmx_above is None:
                wmx_above = 1e6

            b = updates[self.boltzmann_bias]
            W_above = updates[self.layer_above.W]
            ising_b = 0.5 * b + 0.25 * W_above.sum(axis=1)
            ising_b = T.clip(ising_b, bmn, bmx)

            ising_W_above = 0.25 * W_above
            ising_W_above = T.clip(ising_W_above, wmn_above, wmx_above)
            bhn = 2. * (ising_b - ising_W_above.sum(axis=1))

            updates[self.boltzmann_bias] = bhn

        if self.noisy_sampling_b is not None:
            theano_rng = make_theano_rng(None, self.dbm.rng.randint(2**16), which_method="normal")

            b = updates[self.boltzmann_bias]
            W_above = updates[self.layer_above.W]
            ising_b = 0.5 * b + 0.25 * W_above.sum(axis=1)

            noisy_sampling_b = \
                theano_rng.normal(avg=ising_b.dimshuffle('x', 0),
                                  std=self.sampling_b_stdev,
                                  size=self.noisy_sampling_b.shape,
                                  dtype=ising_b.dtype)
            updates[self.noisy_sampling_b] = noisy_sampling_b

    def resample_bias_noise(self, batch_size_changed=False):
        """
        .. todo::

            WRITEME
        """
        if batch_size_changed:
            self.resample_fn = None

        if self.resample_fn is None:
            updates = OrderedDict()

            if self.sampling_b_stdev is not None:
                self.noisy_sampling_b = \
                    sharedX(np.zeros((self.dbm.batch_size, self.nvis)))

            if self.noisy_sampling_b is not None:
                theano_rng = make_theano_rng(None, self.dbm.rng.randint(2**16), which_method="normal")

                b = self.boltzmann_bias
                W_above = self.layer_above.W
                ising_b = 0.5 * b + 0.25 * W_above.sum(axis=1)

                noisy_sampling_b = \
                    theano_rng.normal(avg=ising_b.dimshuffle('x', 0),
                                      std=self.sampling_b_stdev,
                                      size=self.noisy_sampling_b.shape,
                                      dtype=ising_b.dtype)
                updates[self.noisy_sampling_b] = noisy_sampling_b

            self.resample_fn = function([], updates=updates)

        self.resample_fn()

    def get_biases(self):
        """
        .. todo::

            WRITEME
        """
        warnings.warn("BoltzmannIsingVisible.get_biases returns the " +
                      "BOLTZMANN biases, is that what we want?")
        return self.boltzmann_bias.get_value()

    def set_biases(self, biases, recenter=False):
        """
        .. todo::

            WRITEME
        """
        assert False  # not really sure what this should do for this layer

    def ising_bias(self, for_sampling=False):
        """
        .. todo::

            WRITEME
        """
        if for_sampling and self.layer_above.sampling_b_stdev is not None:
            return self.noisy_sampling_b
        return \
            0.5 * self.boltzmann_bias + 0.25 * self.layer_above.W.sum(axis=1)

    def ising_bias_numpy(self):
        """
        .. todo::

            WRITEME
        """
        return 0.5 * self.boltzmann_bias.get_value() + \
            0.25 * self.layer_above.W.get_value().sum(axis=1)

    def upward_state(self, total_state):
        """
        .. todo::

            WRITEME
        """
        return total_state

    def get_params(self):
        """
        .. todo::

            WRITEME
        """
        rval = [self.boltzmann_bias]
        if self.learn_beta:
            rval.append(self.beta)
        return rval

    def sample(self, state_below=None, state_above=None, layer_above=None,
               theano_rng=None):
        """
        .. todo::

            WRITEME
        """

        assert state_below is None

        msg = layer_above.downward_message(state_above, for_sampling=True)

        bias = self.ising_bias(for_sampling=True)

        z = msg + bias

        phi = T.nnet.sigmoid(2. * self.beta * z)

        rval = theano_rng.binomial(size=phi.shape, p=phi, dtype=phi.dtype, n=1)

        return rval * 2. - 1.

    def make_state(self, num_examples, numpy_rng):
        """
        .. todo::

            WRITEME
        """
        driver = numpy_rng.uniform(0., 1., (num_examples, self.nvis))
        on_prob = sigmoid_numpy(2. * self.beta.get_value() *
                                self.ising_bias_numpy())
        sample = 2. * (driver < on_prob) - 1.

        rval = sharedX(sample, name='v_sample_shared')

        return rval

    def make_symbolic_state(self, num_examples, theano_rng):
        """
        .. todo::

            WRITEME
        """
        mean = T.nnet.sigmoid(2. * self.beta * self.ising_bias())
        rval = theano_rng.binomial(size=(num_examples, self.nvis), p=mean)
        rval = 2. * (rval) - 1.

        return rval

    def mf_update(self, state_above, layer_above):
        """
        .. todo::

            WRITEME
        """
        msg = layer_above.downward_message(state_above, for_sampling=True)

        bias = self.ising_bias(for_sampling=True)

        z = msg + bias

        rval = T.tanh(self.beta * z)

        return rval

    def expected_energy_term(self, state, average, state_below=None,
                             average_below=None):
        """
        .. todo::

            WRITEME
        """

        # state = Print('v_state', attrs=['min', 'max'])(state)

        assert state_below is None
        assert average_below is None
        assert average in [True, False]
        self.space.validate(state)

        # Energy function is linear so it doesn't matter if we're averaging
        # or not
        rval = -(self.beta * T.dot(state, self.ising_bias()))

        assert rval.ndim == 1

        return rval

    def get_monitoring_channels(self):
        """
        .. todo::

            WRITEME
        """
        rval = OrderedDict()

        ising_b = self.ising_bias()

        rval['ising_b_min'] = ising_b.min()
        rval['ising_b_max'] = ising_b.max()
        rval['beta'] = self.beta

        if hasattr(self, 'noisy_sampling_b'):
            rval['noisy_sampling_b_min'] = self.noisy_sampling_b.min()
            rval['noisy_sampling_b_max'] = self.noisy_sampling_b.max()

        return rval


class BoltzmannIsingHidden(HiddenLayer):
    """
    An IsingHidden whose parameters are defined in Boltzmann machine space.

    .. todo::

        WRITEME properly

    Parameters
    ----------
    dim : WRITEME
    layer_name : WRITEME
    layer_below : WRITEME
    beta : theano shared variable
        Shared variable representing a multiplicative factor of the energy
        function (the inverse temperature)
    learn_beta : boolean, optional
        Whether or not the inverse temperature should be considered as a
        learned parameter
    irange : WRITEME
    sparse_init : WRITEME
    sparse_stdev : WRITEME
    include_prob : WRITEME
    init_bias : WRITEME
    W_lr_scale : WRITEME
    b_lr_scale : WRITEME
    beta_lr_scale : WRITEME
    max_col_norm : WRITEME
    min_ising_b : WRITEME
    max_ising_b : WRITEME
    min_ising_W : WRITEME
    max_ising_W : WRITEME
    sampling_W_stdev : WRITEME
    sampling_b_stdev : WRITEME
    """
    def __init__(self,
                 dim,
                 layer_name,
                 layer_below,
                 beta,
                 learn_beta=False,
                 irange=None,
                 sparse_init=None,
                 sparse_stdev=1.,
                 include_prob=1.0,
                 init_bias=0.,
                 W_lr_scale=None,
                 b_lr_scale=None,
                 beta_lr_scale=None,
                 max_col_norm=None,
                 min_ising_b=None,
                 max_ising_b=None,
                 min_ising_W=None,
                 max_ising_W=None,
                 sampling_W_stdev=None,
                 sampling_b_stdev=None):
        if not isinstance(beta, SharedVariable):
            raise ValueError("beta needs to be a theano shared variable.")
        self.__dict__.update(locals())
        del self.self

        layer_below.layer_above = self
        self.layer_above = None
        self.resample_fn = None

    def get_lr_scalers(self):
        """
        .. todo::

            WRITEME
        """

        if not hasattr(self, 'W_lr_scale'):
            self.W_lr_scale = None

        if not hasattr(self, 'b_lr_scale'):
            self.b_lr_scale = None

        if not hasattr(self, 'beta_lr_scale'):
            self.beta_lr_scale = None

        rval = OrderedDict()

        if self.W_lr_scale is not None:
            W = self.W
            rval[W] = self.W_lr_scale

        if self.b_lr_scale is not None:
            rval[self.boltzmann_b] = self.b_lr_scale

        if self.beta_lr_scale is not None:
            rval[self.beta] = self.beta_lr_scale

        return rval

    def set_input_space(self, space):
        """
        .. todo::

            WRITEME properly

        Note: this resets parameters!
        """

        self.input_space = space

        if isinstance(space, VectorSpace):
            self.requires_reformat = False
            self.input_dim = space.dim
        else:
            self.requires_reformat = True
            self.input_dim = space.get_total_dimension()
            self.desired_space = VectorSpace(self.input_dim)

        self.output_space = VectorSpace(self.dim)

        rng = self.dbm.rng

        if self.irange is not None:
            assert self.sparse_init is None
            W = rng.uniform(-self.irange, self.irange,
                            (self.input_dim, self.dim)) * \
                (rng.uniform(0., 1., (self.input_dim, self.dim))
                    < self.include_prob)
        else:
            assert self.sparse_init is not None
            W = np.zeros((self.input_dim, self.dim))
            W *= self.sparse_stdev
        W = sharedX(W)
        W.name = self.layer_name + '_W'
        self.W = W

        self.boltzmann_b = sharedX(np.zeros((self.dim,)) + self.init_bias,
                                   name=self.layer_name + '_b')

    def finalize_initialization(self):
        """
        .. todo::

            WRITEME
        """
        if self.sampling_b_stdev is not None:
            self.noisy_sampling_b = \
                sharedX(np.zeros((self.dbm.batch_size, self.dim)))
        if self.sampling_W_stdev is not None:
            self.noisy_sampling_W = \
                sharedX(np.zeros((self.input_dim, self.dim)),
                        'noisy_sampling_W')

        updates = OrderedDict()
        updates[self.boltzmann_b] = self.boltzmann_b
        updates[self.W] = self.W
        if self.layer_above is not None:
            updates[self.layer_above.W] = self.layer_above.W
        self.enforce_constraints()

    def _modify_updates(self, updates):
        """
        .. todo::

            WRITEME
        """
        beta = self.beta
        if beta in updates:
            updated_beta = updates[beta]
            updates[beta] = T.clip(updated_beta, 1., 1000.)

        if self.max_col_norm is not None:
            W = self.W
            if W in updates:
                updated_W = updates[W]
                col_norms = T.sqrt(T.sum(T.sqr(updated_W), axis=0))
                desired_norms = T.clip(col_norms, 0, self.max_col_norm)
                updates[W] = updated_W * (desired_norms / (1e-7 + col_norms))

        if any(constraint is not None for constraint in [self.min_ising_b,
                                                         self.max_ising_b,
                                                         self.min_ising_W,
                                                         self.max_ising_W]):
            bmn = self.min_ising_b
            if bmn is None:
                bmn = - 1e6
            bmx = self.max_ising_b
            if bmx is None:
                bmx = 1e6
            wmn = self.min_ising_W
            if wmn is None:
                wmn = - 1e6
            wmx = self.max_ising_W
            if wmx is None:
                wmx = 1e6
            if self.layer_above is not None:
                wmn_above = self.layer_above.min_ising_W
                if wmn_above is None:
                    wmn_above = - 1e6
                wmx_above = self.layer_above.max_ising_W
                if wmx_above is None:
                    wmx_above = 1e6

            W = updates[self.W]
            ising_W = 0.25 * W
            ising_W = T.clip(ising_W, wmn, wmx)

            b = updates[self.boltzmann_b]
            if self.layer_above is not None:
                W_above = updates[self.layer_above.W]
                ising_b = 0.5 * b + 0.25 * W.sum(axis=0) \
                                  + 0.25 * W_above.sum(axis=1)
            else:
                ising_b = 0.5 * b + 0.25 * W.sum(axis=0)
            ising_b = T.clip(ising_b, bmn, bmx)

            if self.layer_above is not None:
                ising_W_above = 0.25 * W_above
                ising_W_above = T.clip(ising_W_above, wmn_above, wmx_above)
                bhn = 2. * (ising_b - ising_W.sum(axis=0)
                                    - ising_W_above.sum(axis=1))
            else:
                bhn = 2. * (ising_b - ising_W.sum(axis=0))
            Wn = 4. * ising_W

            updates[self.W] = Wn
            updates[self.boltzmann_b] = bhn

        if self.noisy_sampling_W is not None:
            theano_rng = make_theano_rng(None, self.dbm.rng.randint(2**16), which_method="normal")

            W = updates[self.W]
            ising_W = 0.25 * W

            noisy_sampling_W = \
                theano_rng.normal(avg=ising_W, std=self.sampling_W_stdev,
                                  size=ising_W.shape, dtype=ising_W.dtype)
            updates[self.noisy_sampling_W] = noisy_sampling_W

            b = updates[self.boltzmann_b]
            if self.layer_above is not None:
                W_above = updates[self.layer_above.W]
                ising_b = 0.5 * b + 0.25 * W.sum(axis=0) \
                                  + 0.25 * W_above.sum(axis=1)
            else:
                ising_b = 0.5 * b + 0.25 * W.sum(axis=0)

            noisy_sampling_b = \
                theano_rng.normal(avg=ising_b.dimshuffle('x', 0),
                                  std=self.sampling_b_stdev,
                                  size=self.noisy_sampling_b.shape,
                                  dtype=ising_b.dtype)
            updates[self.noisy_sampling_b] = noisy_sampling_b

    def resample_bias_noise(self, batch_size_changed=False):
        """
        .. todo::

            WRITEME
        """
        if batch_size_changed:
            self.resample_fn = None

        if self.resample_fn is None:
            updates = OrderedDict()

            if self.sampling_b_stdev is not None:
                self.noisy_sampling_b = \
                    sharedX(np.zeros((self.dbm.batch_size, self.dim)))

            if self.noisy_sampling_b is not None:
                theano_rng = make_theano_rng(None, self.dbm.rng.randint(2**16), which_method="normal")

                b = self.boltzmann_b
                if self.layer_above is not None:
                    W_above = self.layer_above.W
                    ising_b = 0.5 * b + 0.25 * self.W.sum(axis=0) \
                                      + 0.25 * W_above.sum(axis=1)
                else:
                    ising_b = 0.5 * b + 0.25 * self.W.sum(axis=0)

                noisy_sampling_b = \
                    theano_rng.normal(avg=ising_b.dimshuffle('x', 0),
                                      std=self.sampling_b_stdev,
                                      size=self.noisy_sampling_b.shape,
                                      dtype=ising_b.dtype)
                updates[self.noisy_sampling_b] = noisy_sampling_b

            self.resample_fn = function([], updates=updates)

        self.resample_fn()

    def get_total_state_space(self):
        """
        .. todo::

            WRITEME
        """
        return VectorSpace(self.dim)

    def get_params(self):
        """
        .. todo::

            WRITEME
        """
        assert self.boltzmann_b.name is not None
        W = self.W
        assert W.name is not None
        rval = [W]
        assert not isinstance(rval, set)
        rval = list(rval)
        assert self.boltzmann_b not in rval
        rval.append(self.boltzmann_b)
        if self.learn_beta:
            rval.append(self.beta)
        return rval

    def ising_weights(self, for_sampling=False):
        """
        .. todo::

            WRITEME
        """
        if not hasattr(self, 'sampling_W_stdev'):
            self.sampling_W_stdev = None
        if for_sampling and self.sampling_W_stdev is not None:
            return self.noisy_sampling_W
        return 0.25 * self.W

    def ising_b(self, for_sampling=False):
        """
        .. todo::

            WRITEME
        """
        if not hasattr(self, 'sampling_b_stdev'):
            self.sampling_b_stdev = None
        if for_sampling and self.sampling_b_stdev is not None:
            return self.noisy_sampling_b
        else:
            if self.layer_above is not None:
                return 0.5 * self.boltzmann_b + \
                    0.25 * self.W.sum(axis=0) + \
                    0.25 * self.layer_above.W.sum(axis=1)
            else:
                return 0.5 * self.boltzmann_b + 0.25 * self.W.sum(axis=0)

    def ising_b_numpy(self):
        """
        .. todo::

            WRITEME
        """
        if self.layer_above is not None:
            return 0.5 * self.boltzmann_b.get_value() + \
                0.25 * self.W.get_value().sum(axis=0) + \
                0.25 * self.layer_above.W.get_value().sum(axis=1)
        else:
            return 0.5 * self.boltzmann_b.get_value() + \
                0.25 * self.W.get_value().sum(axis=0)

    def get_weight_decay(self, coeff):
        """
        .. todo::

            WRITEME
        """
        if isinstance(coeff, str):
            coeff = float(coeff)
        assert isinstance(coeff, float) or hasattr(coeff, 'dtype')
        W = self.W
        return coeff * T.sqr(W).sum()

    def get_weights(self):
        """
        .. todo::

            WRITEME
        """
        warnings.warn("BoltzmannIsingHidden.get_weights returns the " +
                      "BOLTZMANN weights, is that what we want?")
        W = self.W
        return W.get_value()

    def set_weights(self, weights):
        """
        .. todo::

            WRITEME
        """
        warnings.warn("BoltzmannIsingHidden.set_weights sets the BOLTZMANN " +
                      "weights, is that what we want?")
        W = self.W
        W.set_value(weights)

    def set_biases(self, biases, recenter=False):
        """
        .. todo::

            WRITEME
        """
        self.boltzmann_b.set_value(biases)
        assert not recenter  # not really sure what this should do if True

    def get_biases(self):
        """
        .. todo::

            WRITEME
        """
        warnings.warn("BoltzmannIsingHidden.get_biases returns the " +
                      "BOLTZMANN biases, is that what we want?")
        return self.boltzmann_b.get_value()

    def get_weights_format(self):
        """
        .. todo::

            WRITEME
        """
        return ('v', 'h')

    def get_weights_topo(self):
        """
        .. todo::

            WRITEME
        """
        warnings.warn("BoltzmannIsingHidden.get_weights_topo returns the " +
                      "BOLTZMANN weights, is that what we want?")

        if not isinstance(self.input_space, Conv2DSpace):
            raise NotImplementedError()

        W = self.W

        W = W.T

        W = W.reshape((self.detector_layer_dim, self.input_space.shape[0],
                       self.input_space.shape[1], self.input_space.nchannels))

        W = Conv2DSpace.convert(W, self.input_space.axes, ('b', 0, 1, 'c'))

        return function([], W)()

    def upward_state(self, total_state):
        """
        .. todo::

            WRITEME
        """
        return total_state

    def downward_state(self, total_state):
        """
        .. todo::

            WRITEME
        """
        return total_state

    def get_monitoring_channels(self):
        """
        .. todo::

            WRITEME
        """

        W = self.W

        assert W.ndim == 2

        sq_W = T.sqr(W)

        row_norms = T.sqrt(sq_W.sum(axis=1))
        col_norms = T.sqrt(sq_W.sum(axis=0))

        rval = OrderedDict([
            ('boltzmann_row_norms_min', row_norms.min()),
            ('boltzmann_row_norms_mean', row_norms.mean()),
            ('boltzmann_row_norms_max', row_norms.max()),
            ('boltzmann_col_norms_min', col_norms.min()),
            ('boltzmann_col_norms_mean', col_norms.mean()),
            ('boltzmann_col_norms_max', col_norms.max()),
        ])

        ising_W = self.ising_weights()

        rval['ising_W_min'] = ising_W.min()
        rval['ising_W_max'] = ising_W.max()

        ising_b = self.ising_b()

        rval['ising_b_min'] = ising_b.min()
        rval['ising_b_max'] = ising_b.max()

        if hasattr(self, 'noisy_sampling_W'):
            rval['noisy_sampling_W_min'] = self.noisy_sampling_W.min()
            rval['noisy_sampling_W_max'] = self.noisy_sampling_W.max()
            rval['noisy_sampling_b_min'] = self.noisy_sampling_b.min()
            rval['noisy_sampling_b_max'] = self.noisy_sampling_b.max()

        return rval

    def get_monitoring_channels_from_state(self, state):
        """
        .. todo::

            WRITEME
        """

        P = state

        rval = OrderedDict()

        vars_and_prefixes = [(P, '')]

        for var, prefix in vars_and_prefixes:
            v_max = var.max(axis=0)
            v_min = var.min(axis=0)
            v_mean = var.mean(axis=0)
            v_range = v_max - v_min

            # max_x.mean_u is "the mean over *u*nits of the max over
            # e*x*amples". The x and u are included in the name because
            # otherwise its hard to remember which axis is which when reading
            # the monitor I use inner.outer rather than outer_of_inner or
            # something like that because I want mean_x.* to appear next to
            # each other in the alphabetical list, as these are commonly
            # plotted together
            for key, val in [
                    ('max_x.max_u', v_max.max()),
                    ('max_x.mean_u', v_max.mean()),
                    ('max_x.min_u', v_max.min()),
                    ('min_x.max_u', v_min.max()),
                    ('min_x.mean_u', v_min.mean()),
                    ('min_x.min_u', v_min.min()),
                    ('range_x.max_u', v_range.max()),
                    ('range_x.mean_u', v_range.mean()),
                    ('range_x.min_u', v_range.min()),
                    ('mean_x.max_u', v_mean.max()),
                    ('mean_x.mean_u', v_mean.mean()),
                    ('mean_x.min_u', v_mean.min())
            ]:
                rval[prefix+key] = val

        return rval

    def sample(self, state_below=None, state_above=None, layer_above=None,
               theano_rng=None):
        """
        .. todo::

            WRITEME
        """

        if theano_rng is None:
            raise ValueError("theano_rng is required; it just defaults to " +
                             "None so that it may appear after layer_above " +
                             "/ state_above in the list.")

        if state_above is not None:
            msg = layer_above.downward_message(state_above, for_sampling=True)
        else:
            msg = None

        if self.requires_reformat:
            state_below = self.input_space.format_as(state_below,
                                                     self.desired_space)

        z = T.dot(state_below, self.ising_weights(for_sampling=True)) + \
            self.ising_b(for_sampling=True)

        if msg is not None:
            z = z + msg

        on_prob = T.nnet.sigmoid(2. * self.beta * z)

        samples = theano_rng.binomial(p=on_prob, n=1, size=on_prob.shape,
                                      dtype=on_prob.dtype) * 2. - 1.

        return samples

    def downward_message(self, downward_state, for_sampling=False):
        """
        .. todo::

            WRITEME
        """
        rval = T.dot(downward_state,
                     self.ising_weights(for_sampling=for_sampling).T)

        if self.requires_reformat:
            rval = self.desired_space.format_as(rval, self.input_space)

        return rval

    def init_mf_state(self):
        """
        .. todo::

            WRITEME
        """
        # work around theano bug with broadcasted vectors
        z = T.alloc(0., self.dbm.batch_size,
                    self.dim).astype(self.boltzmann_b.dtype) + \
            self.ising_b().dimshuffle('x', 0)
        rval = T.tanh(self.beta * z)
        return rval

    def make_state(self, num_examples, numpy_rng):
        """
        .. todo::

            WRITEME properly

        Returns a shared variable containing an actual state
        (not a mean field state) for this variable.
        """
        driver = numpy_rng.uniform(0., 1., (num_examples, self.dim))
        on_prob = sigmoid_numpy(2. * self.beta.get_value() *
                                self.ising_b_numpy())
        sample = 2. * (driver < on_prob) - 1.

        rval = sharedX(sample, name='v_sample_shared')

        return rval

    def make_symbolic_state(self, num_examples, theano_rng):
        """
        .. todo::

            WRITEME
        """
        mean = T.nnet.sigmoid(2. * self.beta * self.ising_b())
        rval = theano_rng.binomial(size=(num_examples, self.dim), p=mean)
        rval = 2. * (rval) - 1.

        return rval

    def expected_energy_term(self, state, average, state_below, average_below):
        """
        .. todo::

            WRITEME
        """

        # state = Print('h_state', attrs=['min', 'max'])(state)

        self.input_space.validate(state_below)

        if self.requires_reformat:
            if not isinstance(state_below, tuple):
                for sb in get_debug_values(state_below):
                    if sb.shape[0] != self.dbm.batch_size:
                        raise ValueError("self.dbm.batch_size is %d but got " +
                                         "shape of %d" % (self.dbm.batch_size,
                                                          sb.shape[0]))
                    assert reduce(lambda x, y: x * y, sb.shape[1:]) == self.input_dim

            state_below = self.input_space.format_as(state_below,
                                                     self.desired_space)

        # Energy function is linear so it doesn't matter if we're averaging or
        # not. Specifically, our terms are -u^T W d - b^T d where u is the
        # upward state of layer below and d is the downward state of this layer

        bias_term = T.dot(state, self.ising_b())
        weights_term = \
            (T.dot(state_below, self.ising_weights()) * state).sum(axis=1)

        rval = -bias_term - weights_term

        rval *= self.beta

        assert rval.ndim == 1

        return rval

    def linear_feed_forward_approximation(self, state_below):
        """
        .. todo::

            WRITEME properly

        Used to implement TorontoSparsity. Unclear exactly what properties of
        it are important or how to implement it for other layers.

        Properties it must have:
            output is same kind of data structure (ie, tuple of theano
            2-tensors) as mf_update

        Properties it probably should have for other layer types:
            An infinitesimal change in state_below or the parameters should
            cause the same sign of change in the output of
            linear_feed_forward_approximation and in mf_update

            Should not have any non-linearities that cause the gradient to
            shrink

            Should disregard top-down feedback
        """

        z = self.beta * (T.dot(state_below, self.ising_weights()) + self.ising_b())

        return z

    def mf_update(self, state_below, state_above, layer_above=None,
                  double_weights=False, iter_name=None):
        """
        .. todo::

            WRITEME
        """

        self.input_space.validate(state_below)

        if self.requires_reformat:
            if not isinstance(state_below, tuple):
                for sb in get_debug_values(state_below):
                    if sb.shape[0] != self.dbm.batch_size:
                        raise ValueError("self.dbm.batch_size is %d but got " +
                                         "shape of %d" % (self.dbm.batch_size,
                                                          sb.shape[0]))
                    assert reduce(lambda x, y: x * y, sb.shape[1:]) == self.input_dim

            state_below = self.input_space.format_as(state_below,
                                                     self.desired_space)

        if iter_name is None:
            iter_name = 'anon'

        if state_above is not None:
            assert layer_above is not None
            msg = layer_above.downward_message(state_above)
            msg.name = 'msg_from_' + layer_above.layer_name + '_to_' + \
                       self.layer_name + '[' + iter_name+']'
        else:
            msg = None

        if double_weights:
            state_below = 2. * state_below
            state_below.name = self.layer_name + '_'+iter_name + '_2state'
        z = T.dot(state_below, self.ising_weights()) + self.ising_b()
        if self.layer_name is not None and iter_name is not None:
            z.name = self.layer_name + '_' + iter_name + '_z'
        if msg is not None:
            z = z + msg
        h = T.tanh(self.beta * z)

        return h

    def get_l2_act_cost(self, state, target, coeff):
        """
        .. todo::

            WRITEME
        """
        avg = state.mean(axis=0)
        diff = avg - target
        return coeff * T.sqr(diff).mean()

########NEW FILE########
__FILENAME__ = layer
"""
Common DBM Layer classes
"""
__authors__ = ["Ian Goodfellow", "Vincent Dumoulin"]
__copyright__ = "Copyright 2012-2013, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"

import functools
import logging
import numpy as np
import time
import warnings

from theano import tensor as T, function, config
import theano
from theano.compat import OrderedDict
from theano.gof.op import get_debug_values
from theano.printing import Print

from pylearn2.expr.nnet import sigmoid_numpy
from pylearn2.expr.probabilistic_max_pooling import max_pool_channels, max_pool_b01c, max_pool, max_pool_c01b
from pylearn2.linear.conv2d import make_random_conv2D, make_sparse_random_conv2D
from pylearn2.linear.conv2d_c01b import setup_detector_layer_c01b
from pylearn2.linear.matrixmul import MatrixMul
from pylearn2.models import Model
from pylearn2.models.dbm import init_sigmoid_bias_from_marginals
from pylearn2.space import VectorSpace, CompositeSpace, Conv2DSpace, Space
from pylearn2.utils import is_block_gradient
from pylearn2.utils import sharedX, safe_zip, py_integer_types, block_gradient
from pylearn2.utils.rng import make_theano_rng
"""
.. todo::

    WRITEME
"""
from pylearn2.utils import safe_union


logger = logging.getLogger(__name__)


class Layer(Model):
    """
    Abstract class.
    A layer of a DBM.
    May only belong to one DBM.

    Each layer has a state ("total state") that can be split into
    the piece that is visible to the layer above ("upward state")
    and the piece that is visible to the layer below ("downward state").
    (Since visible layers don't have a downward state, the downward_state
    method only appears in the DBM_HiddenLayer subclass)

    For simple layers, all three of these are the same thing.
    """

    def get_dbm(self):
        """
        Returns the DBM that this layer belongs to, or None
        if it has not been assigned to a DBM yet.
        """

        if hasattr(self, 'dbm'):
            return self.dbm

        return None

    def set_dbm(self, dbm):
        """
        Assigns this layer to a DBM.

        Parameters
        ----------
        dbm : WRITEME
        """
        assert self.get_dbm() is None
        self.dbm = dbm

    def get_total_state_space(self):
        """
        Returns the Space that the layer's total state lives in.
        """
        raise NotImplementedError(str(type(self))+" does not implement " +\
                "get_total_state_space()")


    def get_monitoring_channels(self):
        """
        .. todo::

            WRITEME
        """
        return OrderedDict()

    def get_monitoring_channels_from_state(self, state):
        """
        .. todo::

            WRITEME
        """
        return OrderedDict()

    def upward_state(self, total_state):
        """
        Takes total_state and turns it into the state that layer_above should
        see when computing P( layer_above | this_layer).

        So far this has two uses:

        * If this layer consists of a detector sub-layer h that is pooled
          into a pooling layer p, then total_state = (p,h) but layer_above
          should only see p.
        * If the conditional P( layer_above | this_layer) depends on
          parameters of this_layer, sometimes you can play games with
          the state to avoid needing the layers to communicate. So far
          the only instance of this usage is when the visible layer
          is N( Wh, beta). This makes the hidden layer be
          sigmoid( v beta W + b). Rather than having the hidden layer
          explicitly know about beta, we can just pass v beta as
          the upward state.

        Parameters
        ----------
        total_state : WRITEME

        Notes
        -----
        This method should work both for computing sampling updates
        and for computing mean field updates. So far I haven't encountered
        a case where it needs to do different things for those two
        contexts.
        """
        return total_state

    def make_state(self, num_examples, numpy_rng):
        """
        Returns a shared variable containing an actual state (not a mean field
        state) for this variable.

        Parameters
        ----------
        num_examples : WRITEME
        numpy_rng : WRITEME

        Returns
        -------
        WRITEME
        """

        raise NotImplementedError("%s doesn't implement make_state" %
                type(self))

    def make_symbolic_state(self, num_examples, theano_rng):
        """
        Returns a theano symbolic variable containing an actual state (not a
        mean field state) for this variable.

        Parameters
        ----------
        num_examples : WRITEME
        numpy_rng : WRITEME

        Returns
        -------
        WRITEME
        """

        raise NotImplementedError("%s doesn't implement make_symbolic_state" %
                                  type(self))

    def sample(self, state_below = None, state_above = None,
            layer_above = None,
            theano_rng = None):
        """
        Returns an expression for samples of this layer's state, conditioned on
        the layers above and below Should be valid as an update to the shared
        variable returned by self.make_state

        Parameters
        ----------
        state_below : WRITEME
            Corresponds to layer_below.upward_state(full_state_below),
            where full_state_below is the same kind of object as you get
            out of layer_below.make_state
        state_above : WRITEME
            Corresponds to layer_above.downward_state(full_state_above)

        theano_rng : WRITEME
            An MRG_RandomStreams instance

        Returns
        -------
        WRITEME

        Notes
        -----
        This can return multiple expressions if this layer's total state
        consists of more than one shared variable.
        """

        if hasattr(self, 'get_sampling_updates'):
            raise AssertionError("Looks like "+str(type(self))+" needs to rename get_sampling_updates to sample.")

        raise NotImplementedError("%s doesn't implement sample" %
                type(self))

    def expected_energy_term(self, state,
                                   average,
                                   state_below,
                                   average_below):
        """
        Returns a term of the expected energy of the entire model.
        This term should correspond to the expected value of terms
        of the energy function that:

        - involve this layer only
        - if there is a layer below, include terms that involve both this layer
          and the layer below

        Do not include terms that involve the layer below only.
        Do not include any terms that involve the layer above, if it
        exists, in any way (the interface doesn't let you see the layer
        above anyway).

        Parameters
        ----------
        state_below : WRITEME
            Upward state of the layer below.
        state : WRITEME
            Total state of this layer
        average_below : bool
            If True, the layer below is one of the variables to integrate
            over in the expectation, and state_below gives its variational
            parameters. If False, that layer is to be held constant and
            state_below gives a set of assignments to it average: like
            average_below, but for 'state' rather than 'state_below'

        Returns
        -------
        rval : tensor_like
            A 1D theano tensor giving the expected energy term for each example
        """
        raise NotImplementedError(str(type(self))+" does not implement expected_energy_term.")

    def finalize_initialization(self):
        """
        Some layers' initialization depends on layer above being initialized,
        which is why this method is called after `set_input_space` has been
        called.
        """
        pass


class VisibleLayer(Layer):
    """
    Abstract class.
    A layer of a DBM that may be used as a visible layer.
    Currently, all implemented layer classes may be either visible
    or hidden but not both. It may be worth making classes that can
    play both roles though. This would allow getting rid of the BinaryVector
    class.
    """

    def get_total_state_space(self):
        """
        .. todo::

            WRITEME
        """
        return self.get_input_space()


class HiddenLayer(Layer):
    """
    Abstract class.
    A layer of a DBM that may be used as a hidden layer.
    """

    def downward_state(self, total_state):
        """
        .. todo::

            WRITEME
        """
        return total_state

    def get_stdev_rewards(self, state, coeffs):
        """
        .. todo::

            WRITEME
        """
        raise NotImplementedError(str(type(self))+" does not implement get_stdev_rewards")

    def get_range_rewards(self, state, coeffs):
        """
        .. todo::

            WRITEME
        """
        raise NotImplementedError(str(type(self))+" does not implement get_range_rewards")

    def get_l1_act_cost(self, state, target, coeff, eps):
        """
        .. todo::

            WRITEME
        """
        raise NotImplementedError(str(type(self))+" does not implement get_l1_act_cost")

    def get_l2_act_cost(self, state, target, coeff):
        """
        .. todo::

            WRITEME
        """
        raise NotImplementedError(str(type(self))+" does not implement get_l2_act_cost")


class BinaryVector(VisibleLayer):
    """
    A DBM visible layer consisting of binary random variables living
    in a VectorSpace.

    Parameters
    ----------
    nvis : int
        Dimension of the space
    bias_from_marginals : pylearn2.datasets.dataset.Dataset
        Dataset, whose marginals are used to initialize the visible biases
    center : bool
        WRITEME
    copies : int
        WRITEME
    """
    def __init__(self,
            nvis,
            bias_from_marginals = None,
            center = False,
            copies = 1, learn_init_inpainting_state = False):

        self.__dict__.update(locals())
        del self.self
        # Don't serialize the dataset
        del self.bias_from_marginals

        self.space = VectorSpace(nvis)
        self.input_space = self.space

        origin = self.space.get_origin()

        if bias_from_marginals is None:
            init_bias = np.zeros((nvis,))
        else:
            init_bias = init_sigmoid_bias_from_marginals(bias_from_marginals)

        self.bias = sharedX(init_bias, 'visible_bias')

        if center:
            self.offset = sharedX(sigmoid_numpy(init_bias))

    def get_biases(self):
        """
        Returns
        -------
        biases : ndarray
            The numpy value of the biases
        """
        return self.bias.get_value()

    def set_biases(self, biases, recenter=False):
        """
        .. todo::

            WRITEME
        """
        self.bias.set_value(biases)
        if recenter:
            assert self.center
            self.offset.set_value(sigmoid_numpy(self.bias.get_value()))

    def upward_state(self, total_state):
        """
        .. todo::

            WRITEME
        """

        if not hasattr(self, 'center'):
            self.center = False

        if self.center:
            rval = total_state - self.offset
        else:
            rval = total_state

        if not hasattr(self, 'copies'):
            self.copies = 1

        return rval * self.copies


    def get_params(self):
        """
        .. todo::

            WRITEME
        """
        return [self.bias]

    def sample(self, state_below = None, state_above = None,
            layer_above = None,
            theano_rng = None):
        """
        .. todo::

            WRITEME
        """


        assert state_below is None
        if self.copies != 1:
            raise NotImplementedError()

        msg = layer_above.downward_message(state_above)

        bias = self.bias

        z = msg + bias

        phi = T.nnet.sigmoid(z)

        rval = theano_rng.binomial(size = phi.shape, p = phi, dtype = phi.dtype,
                       n = 1 )

        return rval

    def mf_update(self, state_above, layer_above):
        """
        .. todo::

            WRITEME
        """
        msg = layer_above.downward_message(state_above)
        mu = self.bias

        z = msg + mu

        rval = T.nnet.sigmoid(z)

        return rval


    def make_state(self, num_examples, numpy_rng):
        """
        .. todo::

            WRITEME
        """
        if not hasattr(self, 'copies'):
            self.copies = 1
        if self.copies != 1:
            raise NotImplementedError()
        driver = numpy_rng.uniform(0.,1., (num_examples, self.nvis))
        mean = sigmoid_numpy(self.bias.get_value())
        sample = driver < mean

        rval = sharedX(sample, name = 'v_sample_shared')

        return rval

    def make_symbolic_state(self, num_examples, theano_rng):
        """
        .. todo::

            WRITEME
        """
        if not hasattr(self, 'copies'):
            self.copies = 1
        if self.copies != 1:
            raise NotImplementedError()
        mean = T.nnet.sigmoid(self.bias)
        rval = theano_rng.binomial(size=(num_examples, self.nvis), p=mean,
                                   dtype=theano.config.floatX)

        return rval

    def expected_energy_term(self, state, average, state_below = None, average_below = None):
        """
        .. todo::

            WRITEME
        """

        if self.center:
            state = state - self.offset

        assert state_below is None
        assert average_below is None
        assert average in [True, False]
        self.space.validate(state)

        # Energy function is linear so it doesn't matter if we're averaging or not
        rval = -T.dot(state, self.bias)

        assert rval.ndim == 1

        return rval * self.copies

    def init_inpainting_state(self, V, drop_mask, noise = False, return_unmasked = False):
        """
        .. todo::

            WRITEME
        """
        assert drop_mask is None or drop_mask.ndim > 1

        unmasked = T.nnet.sigmoid(self.bias.dimshuffle('x',0))
        # this condition is needed later if unmasked is used as V_hat
        assert unmasked.ndim == 2
        # this condition is also needed later if unmasked is used as V_hat
        assert hasattr(unmasked.owner.op, 'scalar_op')
        if drop_mask is not None:
            masked_mean = unmasked * drop_mask
        else:
            masked_mean = unmasked
        if not hasattr(self, 'learn_init_inpainting_state'):
            self.learn_init_inpainting_state = 0
        if not self.learn_init_inpainting_state:
            masked_mean = block_gradient(masked_mean)
        masked_mean.name = 'masked_mean'

        if noise:
            theano_rng = theano.sandbox.rng_mrg.MRG_RandomStreams(42)
            # we want a set of random mean field parameters, not binary samples
            unmasked = T.nnet.sigmoid(theano_rng.normal(avg = 0.,
                    std = 1., size = masked_mean.shape,
                    dtype = masked_mean.dtype))
            masked_mean = unmasked * drop_mask
            masked_mean.name = 'masked_noise'

        if drop_mask is None:
            rval = masked_mean
        else:
            masked_V  = V  * (1-drop_mask)
            rval = masked_mean + masked_V
        rval.name = 'init_inpainting_state'

        if return_unmasked:
            assert unmasked.ndim > 1
            return rval, unmasked

        return rval


    def inpaint_update(self, state_above, layer_above, drop_mask = None, V = None, return_unmasked = False):
        """
        .. todo::

            WRITEME
        """
        msg = layer_above.downward_message(state_above)
        mu = self.bias

        z = msg + mu
        z.name = 'inpainting_z_[unknown_iter]'

        unmasked = T.nnet.sigmoid(z)

        if drop_mask is not None:
            rval = drop_mask * unmasked + (1-drop_mask) * V
        else:
            rval = unmasked

        rval.name = 'inpainted_V[unknown_iter]'

        if return_unmasked:
            owner = unmasked.owner
            assert owner is not None
            op = owner.op
            assert hasattr(op, 'scalar_op')
            assert isinstance(op.scalar_op, T.nnet.sigm.ScalarSigmoid)
            return rval, unmasked

        return rval


    def recons_cost(self, V, V_hat_unmasked, drop_mask = None, use_sum=False):
        """
        .. todo::

            WRITEME
        """
        if use_sum:
            raise NotImplementedError()

        V_hat = V_hat_unmasked

        assert hasattr(V_hat, 'owner')
        owner = V_hat.owner
        assert owner is not None
        op = owner.op
        block_grad = False
        if is_block_gradient(op):
            assert isinstance(op.scalar_op, theano.scalar.Identity)
            block_grad = True
            real, = owner.inputs
            owner = real.owner
            op = owner.op

        if not hasattr(op, 'scalar_op'):
            raise ValueError("Expected V_hat_unmasked to be generated by an Elemwise op, got "+str(op)+" of type "+str(type(op)))
        assert isinstance(op.scalar_op, T.nnet.sigm.ScalarSigmoid)
        z ,= owner.inputs
        if block_grad:
            z = block_gradient(z)

        if V.ndim != V_hat.ndim:
            raise ValueError("V and V_hat_unmasked should have same ndim, but are %d and %d." % (V.ndim, V_hat.ndim))
        unmasked_cost = V * T.nnet.softplus(-z) + (1 - V) * T.nnet.softplus(z)
        assert unmasked_cost.ndim == V_hat.ndim

        if drop_mask is None:
            masked_cost = unmasked_cost
        else:
            masked_cost = drop_mask * unmasked_cost

        return masked_cost.mean()

class BinaryVectorMaxPool(HiddenLayer):
    """
    A hidden layer that does max-pooling on binary vectors.
    It has two sublayers, the detector layer and the pooling
    layer. The detector layer is its downward state and the pooling
    layer is its upward state.

    Parameters
    ----------
    detector_layer_dim : WRITEME
    pool_size : WRITEME
    layer_name : WRITEME
    irange : WRITEME
    sparse_init : WRITEME
    sparse_stdev : WRITEME
    include_prob : , optional
        Probability of including a weight element in the set of weights
        initialized to U(-irange, irange). If not included it is
        initialized to 0.
    init_bias : WRITEME
    W_lr_scale : WRITEME
    b_lr_scale : WRITEME
    center : WRITEME
    mask_weights : WRITEME
    max_col_norm : WRITEME
    copies : WRITEME
    """
    # TODO: this layer uses (pooled, detector) as its total state,
    #       which can be confusing when listing all the states in
    #       the network left to right. Change this and
    #       pylearn2.expr.probabilistic_max_pooling to use
    #       (detector, pooled)

    def __init__(self,
            detector_layer_dim,
            pool_size,
            layer_name,
            irange = None,
            sparse_init = None,
            sparse_stdev = 1.,
            include_prob = 1.0,
            init_bias = 0.,
            W_lr_scale = None,
            b_lr_scale = None,
            center = False,
            mask_weights = None,
            max_col_norm = None,
            copies = 1):
        self.__dict__.update(locals())
        del self.self

        self.b = sharedX( np.zeros((self.detector_layer_dim,)) + init_bias, name = layer_name + '_b')

        if self.center:
            if self.pool_size != 1:
                raise NotImplementedError()
            self.offset = sharedX(sigmoid_numpy(self.b.get_value()))

    def get_lr_scalers(self):
        """
        .. todo::

            WRITEME
        """

        if not hasattr(self, 'W_lr_scale'):
            self.W_lr_scale = None

        if not hasattr(self, 'b_lr_scale'):
            self.b_lr_scale = None

        rval = OrderedDict()

        if self.W_lr_scale is not None:
            W, = self.transformer.get_params()
            rval[W] = self.W_lr_scale

        if self.b_lr_scale is not None:
            rval[self.b] = self.b_lr_scale

        return rval

    def set_input_space(self, space):
        """
        .. todo::

            WRITEME

        Notes
        -----
        This resets parameters!
        """

        self.input_space = space

        if isinstance(space, VectorSpace):
            self.requires_reformat = False
            self.input_dim = space.dim
        else:
            self.requires_reformat = True
            self.input_dim = space.get_total_dimension()
            self.desired_space = VectorSpace(self.input_dim)


        if not (self.detector_layer_dim % self.pool_size == 0):
            raise ValueError("detector_layer_dim = %d, pool_size = %d. Should be divisible but remainder is %d" %
                    (self.detector_layer_dim, self.pool_size, self.detector_layer_dim % self.pool_size))

        self.h_space = VectorSpace(self.detector_layer_dim)
        self.pool_layer_dim = self.detector_layer_dim / self.pool_size
        self.output_space = VectorSpace(self.pool_layer_dim)

        rng = self.dbm.rng
        if self.irange is not None:
            assert self.sparse_init is None
            W = rng.uniform(-self.irange,
                                 self.irange,
                                 (self.input_dim, self.detector_layer_dim)) * \
                    (rng.uniform(0.,1., (self.input_dim, self.detector_layer_dim))
                     < self.include_prob)
        else:
            assert self.sparse_init is not None
            W = np.zeros((self.input_dim, self.detector_layer_dim))
            def mask_rejects(idx, i):
                if self.mask_weights is None:
                    return False
                return self.mask_weights[idx, i] == 0.
            for i in xrange(self.detector_layer_dim):
                assert self.sparse_init <= self.input_dim
                for j in xrange(self.sparse_init):
                    idx = rng.randint(0, self.input_dim)
                    while W[idx, i] != 0 or mask_rejects(idx, i):
                        idx = rng.randint(0, self.input_dim)
                    W[idx, i] = rng.randn()
            W *= self.sparse_stdev

        W = sharedX(W)
        W.name = self.layer_name + '_W'

        self.transformer = MatrixMul(W)

        W ,= self.transformer.get_params()
        assert W.name is not None

        if self.mask_weights is not None:
            expected_shape =  (self.input_dim, self.detector_layer_dim)
            if expected_shape != self.mask_weights.shape:
                raise ValueError("Expected mask with shape "+str(expected_shape)+" but got "+str(self.mask_weights.shape))
            self.mask = sharedX(self.mask_weights)

    @functools.wraps(Model._modify_updates)
    def _modify_updates(self, updates):

        # Patch old pickle files
        if not hasattr(self, 'mask_weights'):
            self.mask_weights = None
        if not hasattr(self, 'max_col_norm'):
            self.max_col_norm = None

        if self.mask_weights is not None:
            W ,= self.transformer.get_params()
            if W in updates:
                updates[W] = updates[W] * self.mask

        if self.max_col_norm is not None:
            W, = self.transformer.get_params()
            if W in updates:
                updated_W = updates[W]
                col_norms = T.sqrt(T.sum(T.sqr(updated_W), axis=0))
                desired_norms = T.clip(col_norms, 0, self.max_col_norm)
                updates[W] = updated_W * (desired_norms / (1e-7 + col_norms))


    def get_total_state_space(self):
        """
        .. todo::

            WRITEME
        """
        return CompositeSpace((self.output_space, self.h_space))

    def get_params(self):
        """
        .. todo::

            WRITEME
        """
        assert self.b.name is not None
        W ,= self.transformer.get_params()
        assert W.name is not None
        rval = self.transformer.get_params()
        assert not isinstance(rval, set)
        rval = list(rval)
        assert self.b not in rval
        rval.append(self.b)
        return rval

    def get_weight_decay(self, coeff):
        """
        .. todo::

            WRITEME
        """
        if isinstance(coeff, str):
            coeff = float(coeff)
        assert isinstance(coeff, float) or hasattr(coeff, 'dtype')
        W ,= self.transformer.get_params()
        return coeff * T.sqr(W).sum()

    def get_weights(self):
        """
        .. todo::

            WRITEME
        """
        if self.requires_reformat:
            # This is not really an unimplemented case.
            # We actually don't know how to format the weights
            # in design space. We got the data in topo space
            # and we don't have access to the dataset
            raise NotImplementedError()
        W ,= self.transformer.get_params()
        return W.get_value()

    def set_weights(self, weights):
        """
        .. todo::

            WRITEME
        """
        W, = self.transformer.get_params()
        W.set_value(weights)

    def set_biases(self, biases, recenter = False):
        """
        .. todo::

            WRITEME
        """
        self.b.set_value(biases)
        if recenter:
            assert self.center
            if self.pool_size != 1:
                raise NotImplementedError()
            self.offset.set_value(sigmoid_numpy(self.b.get_value()))

    def get_biases(self):
        """
        .. todo::

            WRITEME
        """
        return self.b.get_value()

    def get_weights_format(self):
        """
        .. todo::

            WRITEME
        """
        return ('v', 'h')

    def get_weights_view_shape(self):
        """
        .. todo::

            WRITEME
        """
        total = self.detector_layer_dim
        cols = self.pool_size
        if cols == 1:
            # Let the PatchViewer decidew how to arrange the units
            # when they're not pooled
            raise NotImplementedError()
        # When they are pooled, make each pooling unit have one row
        rows = total / cols
        return rows, cols


    def get_weights_topo(self):
        """
        .. todo::

            WRITEME
        """

        if not isinstance(self.input_space, Conv2DSpace):
            raise NotImplementedError()

        W ,= self.transformer.get_params()

        W = W.T

        W = W.reshape((self.detector_layer_dim, self.input_space.shape[0],
            self.input_space.shape[1], self.input_space.num_channels))

        W = Conv2DSpace.convert(W, self.input_space.axes, ('b', 0, 1, 'c'))

        return function([], W)()

    def upward_state(self, total_state):
        """
        .. todo::

            WRITEME
        """
        p,h = total_state
        self.h_space.validate(h)
        self.output_space.validate(p)

        if not hasattr(self, 'center'):
            self.center = False

        if self.center:
            return p - self.offset

        if not hasattr(self, 'copies'):
            self.copies = 1

        return p * self.copies

    def downward_state(self, total_state):
        """
        .. todo::

            WRITEME
        """
        p,h = total_state

        if not hasattr(self, 'center'):
            self.center = False

        if self.center:
            return h - self.offset

        return h * self.copies

    def get_monitoring_channels(self):
        """
        .. todo::

            WRITEME
        """

        W ,= self.transformer.get_params()

        assert W.ndim == 2

        sq_W = T.sqr(W)

        row_norms = T.sqrt(sq_W.sum(axis=1))
        col_norms = T.sqrt(sq_W.sum(axis=0))

        return OrderedDict([
              ('row_norms_min'  , row_norms.min()),
              ('row_norms_mean' , row_norms.mean()),
              ('row_norms_max'  , row_norms.max()),
              ('col_norms_min'  , col_norms.min()),
              ('col_norms_mean' , col_norms.mean()),
              ('col_norms_max'  , col_norms.max()),
            ])


    def get_monitoring_channels_from_state(self, state):
        """
        .. todo::

            WRITEME
        """

        P, H = state

        rval = OrderedDict()

        if self.pool_size == 1:
            vars_and_prefixes = [ (P,'') ]
        else:
            vars_and_prefixes = [ (P, 'p_'), (H, 'h_') ]

        for var, prefix in vars_and_prefixes:
            v_max = var.max(axis=0)
            v_min = var.min(axis=0)
            v_mean = var.mean(axis=0)
            v_range = v_max - v_min

            # max_x.mean_u is "the mean over *u*nits of the max over e*x*amples"
            # The x and u are included in the name because otherwise its hard
            # to remember which axis is which when reading the monitor
            # I use inner.outer rather than outer_of_inner or something like that
            # because I want mean_x.* to appear next to each other in the alphabetical
            # list, as these are commonly plotted together
            for key, val in [
                    ('max_x.max_u', v_max.max()),
                    ('max_x.mean_u', v_max.mean()),
                    ('max_x.min_u', v_max.min()),
                    ('min_x.max_u', v_min.max()),
                    ('min_x.mean_u', v_min.mean()),
                    ('min_x.min_u', v_min.min()),
                    ('range_x.max_u', v_range.max()),
                    ('range_x.mean_u', v_range.mean()),
                    ('range_x.min_u', v_range.min()),
                    ('mean_x.max_u', v_mean.max()),
                    ('mean_x.mean_u', v_mean.mean()),
                    ('mean_x.min_u', v_mean.min())
                    ]:
                rval[prefix+key] = val

        return rval

    def get_stdev_rewards(self, state, coeffs):
        """
        .. todo::

            WRITEME
        """
        rval = 0.

        P, H = state
        self.output_space.validate(P)
        self.h_space.validate(H)


        if self.pool_size == 1:
            # If the pool size is 1 then pools = detectors
            # and we should not penalize pools and detectors separately
            assert len(state) == 2
            if isinstance(coeffs, str):
                coeffs = float(coeffs)
            assert isinstance(coeffs, float)
            _, state = state
            state = [state]
            coeffs = [coeffs]
        else:
            assert all([len(elem) == 2 for elem in [state, coeffs]])

        for s, c in safe_zip(state, coeffs):
            assert all([isinstance(elem, float) for elem in [c]])
            if c == 0.:
                continue
            mn = s.mean(axis=0)
            dev = s - mn
            stdev = T.sqrt(T.sqr(dev).mean(axis=0))
            rval += (0.5 - stdev).mean()*c

        return rval
    def get_range_rewards(self, state, coeffs):
        """
        .. todo::

            WRITEME
        """
        rval = 0.

        P, H = state
        self.output_space.validate(P)
        self.h_space.validate(H)


        if self.pool_size == 1:
            # If the pool size is 1 then pools = detectors
            # and we should not penalize pools and detectors separately
            assert len(state) == 2
            if isinstance(coeffs, str):
                coeffs = float(coeffs)
            assert isinstance(coeffs, float)
            _, state = state
            state = [state]
            coeffs = [coeffs]
        else:
            assert all([len(elem) == 2 for elem in [state, coeffs]])

        for s, c in safe_zip(state, coeffs):
            assert all([isinstance(elem, float) for elem in [c]])
            if c == 0.:
                continue
            mx = s.max(axis=0)
            assert hasattr(mx.owner.op, 'grad')
            assert mx.ndim == 1
            mn = s.min(axis=0)
            assert hasattr(mn.owner.op, 'grad')
            assert mn.ndim == 1
            r = mx - mn
            rval += (1 - r).mean()*c

        return rval

    def get_l1_act_cost(self, state, target, coeff, eps = None):
        """
        .. todo::

            WRITEME
        """
        rval = 0.

        P, H = state
        self.output_space.validate(P)
        self.h_space.validate(H)


        if self.pool_size == 1:
            # If the pool size is 1 then pools = detectors
            # and we should not penalize pools and detectors separately
            assert len(state) == 2
            if not isinstance(target, float):
                raise TypeError("BinaryVectorMaxPool.get_l1_act_cost expected target of type float " + \
                        " but an instance named "+self.layer_name + " got target "+str(target) + " of type "+str(type(target)))
            assert isinstance(coeff, float) or hasattr(coeff, 'dtype')
            _, state = state
            state = [state]
            target = [target]
            coeff = [coeff]
            if eps is None:
                eps = [0.]
            else:
                eps = [eps]
        else:
            assert all([len(elem) == 2 for elem in [state, target, coeff]])
            if eps is None:
                eps = [0., 0.]
            if target[1] > target[0]:
                warnings.warn("Do you really want to regularize the detector units to be more active than the pooling units?")

        for s, t, c, e in safe_zip(state, target, coeff, eps):
            assert all([isinstance(elem, float) or hasattr(elem, 'dtype') for elem in [t, c, e]])
            if c == 0.:
                continue
            m = s.mean(axis=0)
            assert m.ndim == 1
            rval += T.maximum(abs(m-t)-e,0.).mean()*c

        return rval

    def get_l2_act_cost(self, state, target, coeff):
        """
        .. todo::

            WRITEME
        """
        rval = 0.

        P, H = state
        self.output_space.validate(P)
        self.h_space.validate(H)


        if self.pool_size == 1:
            # If the pool size is 1 then pools = detectors
            # and we should not penalize pools and detectors separately
            assert len(state) == 2
            if not isinstance(target, float):
                raise TypeError("BinaryVectorMaxPool.get_l1_act_cost expected target of type float " + \
                        " but an instance named "+self.layer_name + " got target "+str(target) + " of type "+str(type(target)))
            assert isinstance(coeff, float) or hasattr(coeff, 'dtype')
            _, state = state
            state = [state]
            target = [target]
            coeff = [coeff]
        else:
            assert all([len(elem) == 2 for elem in [state, target, coeff]])
            if target[1] > target[0]:
                warnings.warn("Do you really want to regularize the detector units to be more active than the pooling units?")

        for s, t, c in safe_zip(state, target, coeff):
            assert all([isinstance(elem, float) or hasattr(elem, 'dtype') for elem in [t, c]])
            if c == 0.:
                continue
            m = s.mean(axis=0)
            assert m.ndim == 1
            rval += T.square(m-t).mean()*c

        return rval

    def sample(self, state_below = None, state_above = None,
            layer_above = None,
            theano_rng = None):
        """
        .. todo::

            WRITEME
        """
        if self.copies != 1:
            raise NotImplementedError()

        if theano_rng is None:
            raise ValueError("theano_rng is required; it just defaults to None so that it may appear after layer_above / state_above in the list.")

        if state_above is not None:
            msg = layer_above.downward_message(state_above)
        else:
            msg = None

        if self.requires_reformat:
            state_below = self.input_space.format_as(state_below, self.desired_space)

        z = self.transformer.lmul(state_below) + self.b
        p, h, p_sample, h_sample = max_pool_channels(z,
                self.pool_size, msg, theano_rng)

        return p_sample, h_sample

    def downward_message(self, downward_state):
        """
        .. todo::

            WRITEME
        """
        self.h_space.validate(downward_state)
        rval = self.transformer.lmul_T(downward_state)

        if self.requires_reformat:
            rval = self.desired_space.format_as(rval, self.input_space)

        return rval * self.copies

    def init_mf_state(self):
        """
        .. todo::

            WRITEME
        """
        # work around theano bug with broadcasted vectors
        z = T.alloc(0., self.dbm.batch_size, self.detector_layer_dim).astype(self.b.dtype) + \
                self.b.dimshuffle('x', 0)
        rval = max_pool_channels(z = z,
                pool_size = self.pool_size)
        return rval

    def make_state(self, num_examples, numpy_rng):
        """
        .. todo::

            WRITEME
        """
        """ Returns a shared variable containing an actual state
           (not a mean field state) for this variable.
        """

        if not hasattr(self, 'copies'):
            self.copies = 1

        if self.copies != 1:
            raise NotImplementedError()


        empty_input = self.h_space.get_origin_batch(num_examples)
        empty_output = self.output_space.get_origin_batch(num_examples)

        h_state = sharedX(empty_input)
        p_state = sharedX(empty_output)

        theano_rng = make_theano_rng(None, numpy_rng.randint(2 ** 16), which_method="binomial")

        default_z = T.zeros_like(h_state) + self.b

        p_exp, h_exp, p_sample, h_sample = max_pool_channels(
                z = default_z,
                pool_size = self.pool_size,
                theano_rng = theano_rng)

        assert h_sample.dtype == default_z.dtype

        f = function([], updates = [
            (p_state , p_sample),
            (h_state , h_sample)
            ])

        f()

        p_state.name = 'p_sample_shared'
        h_state.name = 'h_sample_shared'

        return p_state, h_state

    def make_symbolic_state(self, num_examples, theano_rng):
        """
        .. todo::

            WRITEME
        """
        """
        Returns a theano symbolic variable containing an actual state
        (not a mean field state) for this variable.
        """

        if not hasattr(self, 'copies'):
            self.copies = 1

        if self.copies != 1:
            raise NotImplementedError()

        default_z = T.alloc(self.b, num_examples, self.detector_layer_dim)

        p_exp, h_exp, p_sample, h_sample = max_pool_channels(z=default_z,
                                                             pool_size=self.pool_size,
                                                             theano_rng=theano_rng)

        assert h_sample.dtype == default_z.dtype

        return p_sample, h_sample

    def expected_energy_term(self, state, average, state_below, average_below):
        """
        .. todo::

            WRITEME
        """

        # Don't need to do anything special for centering, upward_state / downward state
        # make it all just work

        self.input_space.validate(state_below)

        if self.requires_reformat:
            if not isinstance(state_below, tuple):
                for sb in get_debug_values(state_below):
                    if sb.shape[0] != self.dbm.batch_size:
                        raise ValueError("self.dbm.batch_size is %d but got shape of %d" % (self.dbm.batch_size, sb.shape[0]))
                    assert reduce(lambda x,y: x * y, sb.shape[1:]) == self.input_dim

            state_below = self.input_space.format_as(state_below, self.desired_space)

        downward_state = self.downward_state(state)
        self.h_space.validate(downward_state)

        # Energy function is linear so it doesn't matter if we're averaging or not
        # Specifically, our terms are -u^T W d - b^T d where u is the upward state of layer below
        # and d is the downward state of this layer

        bias_term = T.dot(downward_state, self.b)
        weights_term = (self.transformer.lmul(state_below) * downward_state).sum(axis=1)

        rval = -bias_term - weights_term

        assert rval.ndim == 1

        return rval * self.copies

    def linear_feed_forward_approximation(self, state_below):
        """
        Used to implement TorontoSparsity. Unclear exactly what properties of
        it are important or how to implement it for other layers.

        Properties it must have: output is same kind of data structure (ie,
        tuple of theano 2-tensors) as mf_update.

        Properties it probably should have for other layer types: an
        infinitesimal change in state_below or the parameters should cause the
        same sign of change in the output of linear_feed_forward_approximation
        and in mf_update

        Should not have any non-linearities that cause the gradient to shrink

        Should disregard top-down feedback

        Parameters
        ----------
        state_below : WRITEME
        """

        z = self.transformer.lmul(state_below) + self.b

        if self.pool_size != 1:
            # Should probably implement sum pooling for the non-pooled version,
            # but in reality it's not totally clear what the right answer is
            raise NotImplementedError()

        return z, z

    def mf_update(self, state_below, state_above, layer_above = None, double_weights = False, iter_name = None):
        """
        .. todo::

            WRITEME
        """

        self.input_space.validate(state_below)

        if self.requires_reformat:
            if not isinstance(state_below, tuple):
                for sb in get_debug_values(state_below):
                    if sb.shape[0] != self.dbm.batch_size:
                        raise ValueError("self.dbm.batch_size is %d but got shape of %d" % (self.dbm.batch_size, sb.shape[0]))
                    assert reduce(lambda x,y: x * y, sb.shape[1:]) == self.input_dim

            state_below = self.input_space.format_as(state_below, self.desired_space)

        if iter_name is None:
            iter_name = 'anon'

        if state_above is not None:
            assert layer_above is not None
            msg = layer_above.downward_message(state_above)
            msg.name = 'msg_from_'+layer_above.layer_name+'_to_'+self.layer_name+'['+iter_name+']'
        else:
            msg = None

        if double_weights:
            state_below = 2. * state_below
            state_below.name = self.layer_name + '_'+iter_name + '_2state'
        z = self.transformer.lmul(state_below) + self.b
        if self.layer_name is not None and iter_name is not None:
            z.name = self.layer_name + '_' + iter_name + '_z'
        p,h = max_pool_channels(z, self.pool_size, msg)

        p.name = self.layer_name + '_p_' + iter_name
        h.name = self.layer_name + '_h_' + iter_name

        return p, h


class Softmax(HiddenLayer):
    """
    .. todo::

        WRITEME
    """

    presynaptic_name = "presynaptic_Y_hat"

    def __init__(self, n_classes, layer_name, irange = None,
                 sparse_init = None, sparse_istdev = 1., W_lr_scale = None,
                 b_lr_scale = None,
                 max_col_norm = None,
                 copies = 1, center = False,
                 learn_init_inpainting_state = True):
        if isinstance(W_lr_scale, str):
            W_lr_scale = float(W_lr_scale)

        self.__dict__.update(locals())
        del self.self

        assert isinstance(n_classes, py_integer_types)

        self.output_space = VectorSpace(n_classes)
        self.b = sharedX( np.zeros((n_classes,)), name = 'softmax_b')

        if self.center:
            b = self.b.get_value()
            self.offset = sharedX(np.exp(b) / np.exp(b).sum())

    @functools.wraps(Model._modify_updates)
    def _modify_updates(self, updates):

        if not hasattr(self, 'max_col_norm'):
            self.max_col_norm = None

        if self.max_col_norm is not None:
            W = self.W
            if W in updates:
                updated_W = updates[W]
                col_norms = T.sqrt(T.sum(T.sqr(updated_W), axis=0))
                desired_norms = T.clip(col_norms, 0, self.max_col_norm)
                updates[W] = updated_W * (desired_norms / (1e-7 + col_norms))

    def get_lr_scalers(self):
        """
        .. todo::

            WRITEME
        """

        rval = OrderedDict()

        # Patch old pickle files
        if not hasattr(self, 'W_lr_scale'):
            self.W_lr_scale = None

        if self.W_lr_scale is not None:
            assert isinstance(self.W_lr_scale, float)
            rval[self.W] = self.W_lr_scale

        if not hasattr(self, 'b_lr_scale'):
            self.b_lr_scale = None

        if self.b_lr_scale is not None:
            assert isinstance(self.b_lr_scale, float)
            rval[self.b] = self.b_lr_scale

        return rval

    def get_total_state_space(self):
        """
        .. todo::

            WRITEME
        """
        return self.output_space

    def get_monitoring_channels_from_state(self, state):
        """
        .. todo::

            WRITEME
        """

        mx = state.max(axis=1)

        return OrderedDict([
                ('mean_max_class' , mx.mean()),
                ('max_max_class' , mx.max()),
                ('min_max_class' , mx.min())
        ])

    def set_input_space(self, space):
        """
        .. todo::

            WRITEME
        """
        self.input_space = space

        if not isinstance(space, Space):
            raise TypeError("Expected Space, got "+
                    str(space)+" of type "+str(type(space)))

        self.input_dim = space.get_total_dimension()
        self.needs_reformat = not isinstance(space, VectorSpace)

        self.desired_space = VectorSpace(self.input_dim)

        if not self.needs_reformat:
            assert self.desired_space == self.input_space

        rng = self.dbm.rng

        if self.irange is not None:
            assert self.sparse_init is None
            W = rng.uniform(-self.irange,self.irange, (self.input_dim,self.n_classes))
        else:
            assert self.sparse_init is not None
            W = np.zeros((self.input_dim, self.n_classes))
            for i in xrange(self.n_classes):
                for j in xrange(self.sparse_init):
                    idx = rng.randint(0, self.input_dim)
                    while W[idx, i] != 0.:
                        idx = rng.randint(0, self.input_dim)
                    W[idx, i] = rng.randn() * self.sparse_istdev

        self.W = sharedX(W,  'softmax_W' )

        self._params = [ self.b, self.W ]

    def get_weights_topo(self):
        """
        .. todo::

            WRITEME
        """
        if not isinstance(self.input_space, Conv2DSpace):
            raise NotImplementedError()
        desired = self.W.get_value().T
        ipt = self.desired_space.format_as(desired, self.input_space)
        rval = Conv2DSpace.convert_numpy(ipt, self.input_space.axes, ('b', 0, 1, 'c'))
        return rval

    def get_weights(self):
        """
        .. todo::

            WRITEME
        """
        if not isinstance(self.input_space, VectorSpace):
            raise NotImplementedError()

        return self.W.get_value()

    def set_weights(self, weights):
        """
        .. todo::

            WRITEME
        """
        self.W.set_value(weights)

    def set_biases(self, biases, recenter=False):
        """
        .. todo::

            WRITEME
        """
        self.b.set_value(biases)
        if recenter:
            assert self.center
            self.offset.set_value( (np.exp(biases) / np.exp(biases).sum()).astype(self.offset.dtype))

    def get_biases(self):
        """
        .. todo::

            WRITEME
        """
        return self.b.get_value()

    def get_weights_format(self):
        """
        .. todo::

            WRITEME
        """
        return ('v', 'h')

    def sample(self, state_below = None, state_above = None,
            layer_above = None,
            theano_rng = None):
        """
        .. todo::

            WRITEME
        """


        if self.copies != 1:
            raise NotImplementedError("need to draw self.copies samples and average them together.")

        if state_above is not None:
            # If you implement this case, also add a unit test for it.
            # Or at least add a warning that it is not tested.
            raise NotImplementedError()

        if theano_rng is None:
            raise ValueError("theano_rng is required; it just defaults to None so that it may appear after layer_above / state_above in the list.")

        self.input_space.validate(state_below)

        # patch old pickle files
        if not hasattr(self, 'needs_reformat'):
            self.needs_reformat = self.needs_reshape
            del self.needs_reshape

        if self.needs_reformat:
            state_below = self.input_space.format_as(state_below, self.desired_space)

        self.desired_space.validate(state_below)


        z = T.dot(state_below, self.W) + self.b
        h_exp = T.nnet.softmax(z)
        h_sample = theano_rng.multinomial(pvals = h_exp, dtype = h_exp.dtype)

        return h_sample

    def mf_update(self, state_below, state_above = None, layer_above = None, double_weights = False, iter_name = None):
        """
        .. todo::

            WRITEME
        """
        if state_above is not None:
            raise NotImplementedError()

        if double_weights:
            raise NotImplementedError()

        self.input_space.validate(state_below)

        # patch old pickle files
        if not hasattr(self, 'needs_reformat'):
            self.needs_reformat = self.needs_reshape
            del self.needs_reshape

        if self.needs_reformat:
            state_below = self.input_space.format_as(state_below, self.desired_space)

        for value in get_debug_values(state_below):
            if value.shape[0] != self.dbm.batch_size:
                raise ValueError("state_below should have batch size "+str(self.dbm.batch_size)+" but has "+str(value.shape[0]))

        self.desired_space.validate(state_below)

        assert self.W.ndim == 2
        assert state_below.ndim == 2

        b = self.b

        Z = T.dot(state_below, self.W) + b

        rval = T.nnet.softmax(Z)

        for value in get_debug_values(rval):
            assert value.shape[0] == self.dbm.batch_size

        return rval

    def downward_message(self, downward_state):
        """
        .. todo::

            WRITEME
        """

        if not hasattr(self, 'copies'):
            self.copies = 1

        rval =  T.dot(downward_state, self.W.T) * self.copies

        rval = self.desired_space.format_as(rval, self.input_space)

        return rval

    def recons_cost(self, Y, Y_hat_unmasked, drop_mask_Y, scale):
        """
        .. todo::

            WRITEME
        """
        """
            scale is because the visible layer also goes into the
            cost. it uses the mean over units and examples, so that
            the scale of the cost doesn't change too much with batch
            size or example size.
            we need to multiply this cost by scale to make sure that
            it is put on the same scale as the reconstruction cost
            for the visible units. ie, scale should be 1/nvis
        """


        Y_hat = Y_hat_unmasked
        assert hasattr(Y_hat, 'owner')
        owner = Y_hat.owner
        assert owner is not None
        op = owner.op
        if isinstance(op, Print):
            assert len(owner.inputs) == 1
            Y_hat, = owner.inputs
            owner = Y_hat.owner
            op = owner.op
        assert isinstance(op, T.nnet.Softmax)
        z ,= owner.inputs
        assert z.ndim == 2

        z = z - z.max(axis=1).dimshuffle(0, 'x')
        log_prob = z - T.log(T.exp(z).sum(axis=1).dimshuffle(0, 'x'))
        # we use sum and not mean because this is really one variable per row
        log_prob_of = (Y * log_prob).sum(axis=1)
        masked = log_prob_of * drop_mask_Y
        assert masked.ndim == 1

        rval = masked.mean() * scale * self.copies

        return - rval

    def init_mf_state(self):
        """
        .. todo::

            WRITEME
        """
        rval =  T.nnet.softmax(self.b.dimshuffle('x', 0)) + T.alloc(0., self.dbm.batch_size, self.n_classes).astype(config.floatX)
        return rval

    def make_state(self, num_examples, numpy_rng):
        """
        .. todo::

            WRITEME
        """
        """ Returns a shared variable containing an actual state
           (not a mean field state) for this variable.
        """

        if self.copies != 1:
            raise NotImplementedError("need to make self.copies samples and average them together.")

        t1 = time.time()

        empty_input = self.output_space.get_origin_batch(num_examples)
        h_state = sharedX(empty_input)

        default_z = T.zeros_like(h_state) + self.b

        theano_rng = make_theano_rng(None, numpy_rng.randint(2 ** 16),
                                     which_method="binomial")

        h_exp = T.nnet.softmax(default_z)

        h_sample = theano_rng.multinomial(pvals = h_exp, dtype = h_exp.dtype)

        h_state = sharedX( self.output_space.get_origin_batch(
            num_examples))


        t2 = time.time()

        f = function([], updates = [(
            h_state , h_sample
            )])

        t3 = time.time()

        f()

        t4 = time.time()

        logger.info('{0}.make_state took {1}'.format(self, t4-t1))
        logger.info('\tcompose time: {0}'.format(t2-t1))
        logger.info('\tcompile time: {0}'.format(t3-t2))
        logger.info('\texecute time: {0}'.format(t4-t3))

        h_state.name = 'softmax_sample_shared'

        return h_state

    def make_symbolic_state(self, num_examples, theano_rng):
        """
        .. todo::

            WRITEME
        """
        """
        Returns a symbolic variable containing an actual state
        (not a mean field state) for this variable.
        """

        if self.copies != 1:
            raise NotImplementedError("need to make self.copies samples and average them together.")

        default_z = T.alloc(self.b, num_examples, self.n_classes)

        h_exp = T.nnet.softmax(default_z)

        h_sample = theano_rng.multinomial(pvals=h_exp, dtype=h_exp.dtype)

        return h_sample

    def get_weight_decay(self, coeff):
        """
        .. todo::

            WRITEME
        """
        if isinstance(coeff, str):
            coeff = float(coeff)
        assert isinstance(coeff, float) or hasattr(coeff, 'dtype')
        return coeff * T.sqr(self.W).sum()

    def upward_state(self, state):
        """
        .. todo::

            WRITEME
        """
        if self.center:
            return state - self.offset
        return state

    def downward_state(self, state):
        """
        .. todo::

            WRITEME
        """
        if not hasattr(self, 'center'):
            self.center = False
        if self.center:
            """TODO: write a unit test verifying that inference or sampling
                     below a centered Softmax layer works"""
            return state - self.offset
        return state

    def expected_energy_term(self, state, average, state_below, average_below):
        """
        .. todo::

            WRITEME
        """

        if self.center:
            state = state - self.offset

        self.input_space.validate(state_below)
        if self.needs_reformat:
            state_below = self.input_space.format_as(state_below, self.desired_space)
        self.desired_space.validate(state_below)

        # Energy function is linear so it doesn't matter if we're averaging or not
        # Specifically, our terms are -u^T W d - b^T d where u is the upward state of layer below
        # and d is the downward state of this layer

        bias_term = T.dot(state, self.b)
        weights_term = (T.dot(state_below, self.W) * state).sum(axis=1)

        rval = -bias_term - weights_term

        rval *= self.copies

        assert rval.ndim == 1

        return rval

    def init_inpainting_state(self, Y, noise):
        """
        .. todo::

            WRITEME
        """
        if noise:
            theano_rng = make_theano_rng(None, 2012+10+30, which_method="binomial")
            return T.nnet.softmax(theano_rng.normal(avg=0., size=Y.shape, std=1., dtype='float32'))
        rval =  T.nnet.softmax(self.b)
        if not hasattr(self, 'learn_init_inpainting_state'):
            self.learn_init_inpainting_state = 1
        if not self.learn_init_inpainting_state:
            rval = block_gradient(rval)
        return rval

    def install_presynaptic_outputs(self, outputs_dict, batch_size):
        """
        .. todo::

            WRITEME
        """

        assert self.presynaptic_name not in outputs_dict
        outputs_dict[self.presynaptic_name] = self.output_space.make_shared_batch(batch_size, self.presynaptic_name)


class GaussianVisLayer(VisibleLayer):
    """
    Implements a visible layer that is conditionally gaussian with
    diagonal variance. The layer lives in a Conv2DSpace.

    Parameters
    ----------
    rows, cols, channels : WRITEME
        the shape of the space
    learn_init_inpainting : bool, optional
        WRITEME
    nvis : WRITEME
    init_beta : WRITEME
        the initial value of the precision parameter
    min_beta : WRITEME
        clip beta so it is at least this big (default 1)
    init_mu : WRITEME
        the initial value of the mean parameter
    tie_beta : WRITEME
        None or a string specifying how to tie beta 'locations' = tie beta
        across locations, ie beta should be a vector with one elem per channel
    tie_mu : WRITEME
        None or a string specifying how to tie mu 'locations' = tie mu across
        locations, ie mu should be a vector with one elem per channel
    bias_from_marginals : WRITEME
    beta_lr_scale : WRITEME
    axes : tuple
        WRITEME
    """
    def __init__(self,
            rows = None,
            cols = None,
            learn_init_inpainting_state=True,
            channels = None,
            nvis = None,
            init_beta = 1.,
            min_beta = 1.,
            init_mu = None,
            tie_beta = None,
            tie_mu = None,
            bias_from_marginals = None,
            beta_lr_scale = 'by_sharing',
            axes = ('b', 0, 1, 'c')):

        warnings.warn("GaussianVisLayer math very faith based, need to finish working through gaussian.lyx")

        self.__dict__.update(locals())
        del self.self

        if bias_from_marginals is not None:
            del self.bias_from_marginals
            if self.nvis is None:
                raise NotImplementedError()
            assert init_mu is None
            init_mu = bias_from_marginals.X.mean(axis=0)

        if init_mu is None:
            init_mu = 0.
        if nvis is None:
            assert rows is not None
            assert cols is not None
            assert channels is not None
            self.space = Conv2DSpace(shape=[rows,cols], num_channels=channels, axes=axes)
            # To make GaussianVisLayer compatible with any axis ordering
            self.batch_axis=list(axes).index('b')
            self.axes_to_sum = range(len(axes))
            self.axes_to_sum.remove(self.batch_axis)
        else:
            assert rows is None
            assert cols is None
            assert channels is None
            self.space = VectorSpace(nvis)
            self.axes_to_sum = 1
            self.batch_axis = None
        self.input_space = self.space

        origin = self.space.get_origin()

        beta_origin = origin.copy()
        assert tie_beta in [ None, 'locations']
        if tie_beta == 'locations':
            assert nvis is None
            beta_origin = np.zeros((self.space.num_channels,))
        self.beta = sharedX(beta_origin + init_beta,name = 'beta')
        assert self.beta.ndim == beta_origin.ndim

        mu_origin = origin.copy()
        assert tie_mu in [None, 'locations']
        if tie_mu == 'locations':
            assert nvis is None
            mu_origin = np.zeros((self.space.num_channels,))
        self.mu = sharedX( mu_origin + init_mu, name = 'mu')
        assert self.mu.ndim == mu_origin.ndim



    def get_monitoring_channels(self):
        """
        .. todo::

            WRITEME
        """
        rval = OrderedDict()

        rval['beta_min'] = self.beta.min()
        rval['beta_mean'] = self.beta.mean()
        rval['beta_max'] = self.beta.max()

        return rval


    def get_params(self):
        """
        .. todo::

            WRITEME
        """
        if self.mu is None:
            return [self.beta]
        return [self.beta, self.mu]

    def get_lr_scalers(self):
        """
        .. todo::

            WRITEME
        """
        rval = OrderedDict()

        if self.nvis is None:
            rows, cols = self.space.shape
            num_loc = float(rows * cols)

        assert self.tie_beta in [None, 'locations']
        if self.beta_lr_scale == 'by_sharing':
            if self.tie_beta == 'locations':
                assert self.nvis is None
                rval[self.beta] = 1. / num_loc
        elif self.beta_lr_scale == None:
            pass
        else:
            rval[self.beta] = self.beta_lr_scale

        assert self.tie_mu in [None, 'locations']
        if self.tie_mu == 'locations':
            warn = True
            assert self.nvis is None
            rval[self.mu] = 1./num_loc
            logger.warning("mu lr_scaler hardcoded to 1/sharing")

        return rval

    @functools.wraps(Model._modify_updates)
    def _modify_updates(self, updates):
        if self.beta in updates:
            updated_beta = updates[self.beta]
            updates[self.beta] = T.clip(updated_beta,
                    self.min_beta,1e6)

    def set_biases(self, bias):
        """
        Set mean parameter

        Parameters
        ----------
        bias: WRITEME
            Vector of size nvis
        """
        self.mu = sharedX(bias, name = 'mu')

    def broadcasted_mu(self):
        """
        Returns mu, broadcasted to have the same shape as a batch of data
        """

        if self.tie_mu == 'locations':
            def f(x):
                if x == 'c':
                    return 0
                return 'x'
            axes = [f(ax) for ax in self.axes]
            rval = self.mu.dimshuffle(*axes)
        else:
            assert self.tie_mu is None
            if self.nvis is None:
                axes = [0, 1, 2]
                axes.insert(self.axes.index('b'), 'x')
                rval = self.mu.dimshuffle(*axes)
            else:
                rval = self.mu.dimshuffle('x', 0)

        self.input_space.validate(rval)

        return rval

    def broadcasted_beta(self):
        """
        Returns beta, broadcasted to have the same shape as a batch of data
        """
        return self.broadcast_beta(self.beta)

    def broadcast_beta(self, beta):
        """
        .. todo::

            WRITEME
        """
        """
        Returns beta, broadcasted to have the same shape as a batch of data
        """

        if self.tie_beta == 'locations':
            def f(x):
                if x == 'c':
                    return 0
                return 'x'
            axes = [f(ax) for ax in self.axes]
            rval = beta.dimshuffle(*axes)
        else:
            assert self.tie_beta is None
            if self.nvis is None:
                axes = [0, 1, 2]
                axes.insert(self.axes.index('b'), 'x')
                rval = beta.dimshuffle(*axes)
            else:
                rval = beta.dimshuffle('x', 0)

        self.input_space.validate(rval)

        return rval

    def init_inpainting_state(self, V, drop_mask, noise = False, return_unmasked = False):
        """
        .. todo::

            WRITEME
        """

        """for Vv, drop_mask_v in get_debug_values(V, drop_mask):
            assert Vv.ndim == 4
            assert drop_mask_v.ndim in [3,4]
            for i in xrange(drop_mask.ndim):
                if Vv.shape[i] != drop_mask_v.shape[i]:
                    print Vv.shape
                    print drop_mask_v.shape
                    assert False
        """

        unmasked = self.broadcasted_mu()

        if drop_mask is None:
            assert not noise
            assert not return_unmasked
            return unmasked
        masked_mu = unmasked * drop_mask
        if not hasattr(self, 'learn_init_inpainting_state'):
            self.learn_init_inpainting_state = True
        if not self.learn_init_inpainting_state:
            masked_mu = block_gradient(masked_mu)
        masked_mu.name = 'masked_mu'

        if noise:
            theano_rng = make_theano_rng(None, 42, which_method="binomial")
            unmasked = theano_rng.normal(avg = 0.,
                    std = 1., size = masked_mu.shape,
                    dtype = masked_mu.dtype)
            masked_mu = unmasked * drop_mask
            masked_mu.name = 'masked_noise'


        masked_V  = V  * (1-drop_mask)
        rval = masked_mu + masked_V
        rval.name = 'init_inpainting_state'

        if return_unmasked:
            return rval, unmasked
        return rval


    def expected_energy_term(self, state, average, state_below = None, average_below = None):
        """
        .. todo::

            WRITEME
        """
        assert state_below is None
        assert average_below is None
        self.space.validate(state)
        if average:
            raise NotImplementedError(str(type(self))+" doesn't support integrating out variational parameters yet.")
        else:
            rval =  0.5 * (self.beta * T.sqr(state - self.mu)).sum(axis=self.axes_to_sum)
        assert rval.ndim == 1
        return rval


    def inpaint_update(self, state_above, layer_above, drop_mask = None, V = None,
                        return_unmasked = False):
        """
        .. todo::

            WRITEME
        """

        msg = layer_above.downward_message(state_above)
        mu = self.broadcasted_mu()

        z = msg + mu
        z.name = 'inpainting_z_[unknown_iter]'

        if drop_mask is not None:
            rval = drop_mask * z + (1-drop_mask) * V
        else:
            rval = z

        rval.name = 'inpainted_V[unknown_iter]'

        if return_unmasked:
            return rval, z

        return rval

    def sample(self, state_below = None, state_above = None,
            layer_above = None,
            theano_rng = None):
        """
        .. todo::

            WRITEME
        """

        assert state_below is None
        msg = layer_above.downward_message(state_above)
        mu = self.mu

        z = msg + mu
        rval = theano_rng.normal(size = z.shape, avg = z, dtype = z.dtype,
                       std = 1. / T.sqrt(self.beta))
        return rval

    def recons_cost(self, V, V_hat_unmasked, drop_mask = None, use_sum=False):
        """
        .. todo::

            WRITEME
        """

        return self._recons_cost(V=V, V_hat_unmasked=V_hat_unmasked, drop_mask=drop_mask, use_sum=use_sum, beta=self.beta)


    def _recons_cost(self, V, V_hat_unmasked, beta, drop_mask=None, use_sum=False):
        """
        .. todo::

            WRITEME
        """
        V_hat = V_hat_unmasked

        assert V.ndim == V_hat.ndim
        beta = self.broadcasted_beta()
        unmasked_cost = 0.5 * beta * T.sqr(V-V_hat) - 0.5*T.log(beta / (2*np.pi))
        assert unmasked_cost.ndim == V_hat.ndim

        if drop_mask is None:
            masked_cost = unmasked_cost
        else:
            masked_cost = drop_mask * unmasked_cost

        if use_sum:
            return masked_cost.mean(axis=0).sum()

        return masked_cost.mean()

        return masked_cost.mean()

    def upward_state(self, total_state):
        """
        .. todo::

            WRITEME
        """
        if self.nvis is None and total_state.ndim != 4:
            raise ValueError("total_state should have 4 dimensions, has "+str(total_state.ndim))
        assert total_state is not None
        V = total_state
        self.input_space.validate(V)
        upward_state = (V - self.broadcasted_mu()) * self.broadcasted_beta()
        return upward_state

    def make_state(self, num_examples, numpy_rng):
        """
        .. todo::

            WRITEME
        """

        shape = [num_examples]

        if self.nvis is None:
            rows, cols = self.space.shape
            channels = self.space.num_channels
            shape.append(rows)
            shape.append(cols)
            shape.append(channels)
        else:
            shape.append(self.nvis)

        sample = numpy_rng.randn(*shape)

        sample *= 1./np.sqrt(self.beta.get_value())
        sample += self.mu.get_value()
        rval = sharedX(sample, name = 'v_sample_shared')

        return rval

    def install_presynaptic_outputs(self, outputs_dict, batch_size):
        """
        .. todo::

            WRITEME
        """

        outputs_dict['output_V_weighted_pred_sum'] = self.space.make_shared_batch(batch_size)

    def ensemble_prediction(self, symbolic, outputs_dict, ensemble):
        """
        .. todo::

            WRITEME
        """
        """
        Output a symbolic expression for V_hat_unmasked based on taking the
        geometric mean over the ensemble and renormalizing.
        n - 1 members of the ensemble have modified outputs_dict and the nth
        gives its prediction in "symbolic". The parameters for the nth one
        are currently loaded in the model.
        """

        weighted_pred_sum = outputs_dict['output_V_weighted_pred_sum'] \
                + self.broadcasted_beta() * symbolic

        beta_sum = sum(ensemble.get_ensemble_variants(self.beta))

        unmasked_V_hat = weighted_pred_sum / self.broadcast_beta(beta_sum)

        return unmasked_V_hat

    def ensemble_recons_cost(self, V, V_hat_unmasked, drop_mask=None,
            use_sum=False, ensemble=None):
        """
        .. todo::

            WRITEME
        """

        beta = sum(ensemble.get_ensemble_variants(self.beta)) / ensemble.num_copies

        return self._recons_cost(V=V, V_hat_unmasked=V_hat_unmasked, beta=beta, drop_mask=drop_mask,
            use_sum=use_sum)


class ConvMaxPool(HiddenLayer):
    """
    .. todo::

        WRITEME
    """

    def __init__(self,
             output_channels,
            kernel_rows,
            kernel_cols,
            pool_rows,
            pool_cols,
            layer_name,
            center = False,
            irange = None,
            sparse_init = None,
            scale_by_sharing = True,
            init_bias = 0.,
            border_mode = 'valid',
            output_axes = ('b', 'c', 0, 1)):
        self.__dict__.update(locals())
        del self.self

        assert (irange is None) != (sparse_init is None)

        self.b = sharedX( np.zeros((output_channels,)) + init_bias, name = layer_name + '_b')
        assert border_mode in ['full','valid']

    def broadcasted_bias(self):
        """
        .. todo::

            WRITEME
        """

        assert self.b.ndim == 1

        shuffle = [ 'x' ] * 4
        shuffle[self.output_axes.index('c')] = 0

        return self.b.dimshuffle(*shuffle)


    def get_total_state_space(self):
        """
        .. todo::

            WRITEME
        """
        return CompositeSpace((self.h_space, self.output_space))

    def set_input_space(self, space):
        """
        .. todo::

            WRITEME
        """
        """ Note: this resets parameters!"""
        if not isinstance(space, Conv2DSpace):
            raise TypeError("ConvMaxPool can only act on a Conv2DSpace, but received " +
                    str(type(space))+" as input.")
        self.input_space = space
        self.input_rows, self.input_cols = space.shape
        self.input_channels = space.num_channels

        if self.border_mode == 'valid':
            self.h_rows = self.input_rows - self.kernel_rows + 1
            self.h_cols = self.input_cols - self.kernel_cols + 1
        else:
            assert self.border_mode == 'full'
            self.h_rows = self.input_rows + self.kernel_rows - 1
            self.h_cols = self.input_cols + self.kernel_cols - 1


        if not( self.h_rows % self.pool_rows == 0):
            raise ValueError("h_rows = %d, pool_rows = %d. Should be divisible but remainder is %d" %
                    (self.h_rows, self.pool_rows, self.h_rows % self.pool_rows))
        assert self.h_cols % self.pool_cols == 0

        self.h_space = Conv2DSpace(shape = (self.h_rows, self.h_cols), num_channels = self.output_channels,
                axes = self.output_axes)
        self.output_space = Conv2DSpace(shape = (self.h_rows / self.pool_rows,
                                                self.h_cols / self.pool_cols),
                                                num_channels = self.output_channels,
                axes = self.output_axes)

        logger.info('{0}: detector shape: {1} '
                    'pool shape: {2}'.format(self.layer_name,
                                             self.h_space.shape,
                                             self.output_space.shape))

        if tuple(self.output_axes) == ('b', 0, 1, 'c'):
            self.max_pool = max_pool_b01c
        elif tuple(self.output_axes) == ('b', 'c', 0, 1):
            self.max_pool = max_pool
        else:
            raise NotImplementedError()

        if self.irange is not None:
            self.transformer = make_random_conv2D(self.irange, input_space = space,
                    output_space = self.h_space, kernel_shape = (self.kernel_rows, self.kernel_cols),
                    batch_size = self.dbm.batch_size, border_mode = self.border_mode, rng = self.dbm.rng)
        else:
            self.transformer = make_sparse_random_conv2D(self.sparse_init, input_space = space,
                    output_space = self.h_space, kernel_shape = (self.kernel_rows, self.kernel_cols),
                    batch_size = self.dbm.batch_size, border_mode = self.border_mode, rng = self.dbm.rng)
        self.transformer._filters.name = self.layer_name + '_W'


        W ,= self.transformer.get_params()
        assert W.name is not None

        if self.center:
            p_ofs, h_ofs = self.init_mf_state()
            self.p_offset = sharedX(self.output_space.get_origin(), 'p_offset')
            self.h_offset = sharedX(self.h_space.get_origin(), 'h_offset')
            f = function([], updates={self.p_offset: p_ofs[0,:,:,:], self.h_offset: h_ofs[0,:,:,:]})
            f()


    def get_params(self):
        """
        .. todo::

            WRITEME
        """
        assert self.b.name is not None
        W ,= self.transformer.get_params()
        assert W.name is not None

        return [ W, self.b]

    def state_to_b01c(self, state):
        """
        .. todo::

            WRITEME
        """

        if tuple(self.output_axes) == ('b',0,1,'c'):
            return state
        return [ Conv2DSpace.convert(elem, self.output_axes, ('b', 0, 1, 'c'))
                for elem in state ]

    def get_range_rewards(self, state, coeffs):
        """
        .. todo::

            WRITEME
        """
        rval = 0.

        if self.pool_rows == 1 and self.pool_cols == 1:
            # If the pool size is 1 then pools = detectors
            # and we should not penalize pools and detectors separately
            assert len(state) == 2
            assert isinstance(coeffs, float)
            _, state = state
            state = [state]
            coeffs = [coeffs]
        else:
            assert all([len(elem) == 2 for elem in [state, coeffs]])

        for s, c in safe_zip(state, coeffs):
            if c == 0.:
                continue
            # Range over everything but the channel index
            # theano can only take gradient through max if the max is over 1 axis or all axes
            # so I manually unroll the max for the case I use here
            assert self.h_space.axes == ('b', 'c', 0, 1)
            assert self.output_space.axes == ('b', 'c', 0, 1)
            mx = s.max(axis=3).max(axis=2).max(axis=0)
            assert hasattr(mx.owner.op, 'grad')
            mn = s.min(axis=3).max(axis=2).max(axis=0)
            assert hasattr(mn.owner.op, 'grad')
            assert mx.ndim == 1
            assert mn.ndim == 1
            r = mx - mn
            rval += (1. - r).mean() * c

        return rval

    def get_l1_act_cost(self, state, target, coeff, eps):
        """
        .. todo::

            WRITEME
        """
        """

            target: if pools contain more than one element, should be a list with
                    two elements. the first element is for the pooling units and
                    the second for the detector units.

        """
        rval = 0.


        if self.pool_rows == 1 and self.pool_cols == 1:
            # If the pool size is 1 then pools = detectors
            # and we should not penalize pools and detectors separately
            assert len(state) == 2
            assert isinstance(target, float)
            assert isinstance(coeff, float)
            _, state = state
            state = [state]
            target = [target]
            coeff = [coeff]
            if eps is None:
                eps = 0.
            eps = [eps]
        else:
            if eps is None:
                eps = [0., 0.]
            assert all([len(elem) == 2 for elem in [state, target, coeff]])
            p_target, h_target = target
            if h_target > p_target and (coeff[0] != 0. and coeff[1] != 0.):
                # note that, within each group, E[p] is the sum of E[h]
                warnings.warn("Do you really want to regularize the detector units to be more active than the pooling units?")

        for s, t, c, e in safe_zip(state, target, coeff, eps):
            if c == 0.:
                continue
            # Average over everything but the channel index
            m = s.mean(axis= [ ax for ax in range(4) if self.output_axes[ax] != 'c' ])
            assert m.ndim == 1
            rval += T.maximum(abs(m-t)-e,0.).mean()*c

        return rval

    def get_lr_scalers(self):
        """
        .. todo::

            WRITEME
        """
        if self.scale_by_sharing:
            # scale each learning rate by 1 / # times param is reused
            h_rows, h_cols = self.h_space.shape
            num_h = float(h_rows * h_cols)
            return OrderedDict([(self.transformer._filters, 1./num_h),
                     (self.b, 1. / num_h)])
        else:
            return OrderedDict()

    def upward_state(self, total_state):
        """
        .. todo::

            WRITEME
        """
        p,h = total_state

        if not hasattr(self, 'center'):
            self.center = False

        if self.center:
            p -= self.p_offset
            h -= self.h_offset

        return p

    def downward_state(self, total_state):
        """
        .. todo::

            WRITEME
        """
        p,h = total_state

        if not hasattr(self, 'center'):
            self.center = False

        if self.center:
            p -= self.p_offset
            h -= self.h_offset

        return h

    def get_monitoring_channels_from_state(self, state):
        """
        .. todo::

            WRITEME
        """

        P, H = state

        if tuple(self.output_axes) == ('b',0,1,'c'):
            p_max = P.max(axis=(0,1,2))
            p_min = P.min(axis=(0,1,2))
            p_mean = P.mean(axis=(0,1,2))
        else:
            assert tuple(self.output_axes) == ('b','c',0,1)
            p_max = P.max(axis=(0,2,3))
            p_min = P.min(axis=(0,2,3))
            p_mean = P.mean(axis=(0,2,3))
        p_range = p_max - p_min

        rval = {
                'p_max_max' : p_max.max(),
                'p_max_mean' : p_max.mean(),
                'p_max_min' : p_max.min(),
                'p_min_max' : p_min.max(),
                'p_min_mean' : p_min.mean(),
                'p_min_max' : p_min.max(),
                'p_range_max' : p_range.max(),
                'p_range_mean' : p_range.mean(),
                'p_range_min' : p_range.min(),
                'p_mean_max' : p_mean.max(),
                'p_mean_mean' : p_mean.mean(),
                'p_mean_min' : p_mean.min()
                }

        return rval

    def get_weight_decay(self, coeffs):
        """
        .. todo::

            WRITEME
        """
        W , = self.transformer.get_params()
        return coeffs * T.sqr(W).sum()



    def mf_update(self, state_below, state_above, layer_above = None, double_weights = False, iter_name = None):
        """
        .. todo::

            WRITEME
        """

        self.input_space.validate(state_below)

        if iter_name is None:
            iter_name = 'anon'

        if state_above is not None:
            assert layer_above is not None
            msg = layer_above.downward_message(state_above)
            msg.name = 'msg_from_'+layer_above.layer_name+'_to_'+self.layer_name+'['+iter_name+']'
        else:
            msg = None

        if not hasattr(state_below, 'ndim'):
            raise TypeError("state_below should be a TensorType, got " +
                    str(state_below) + " of type " + str(type(state_below)))
        if state_below.ndim != 4:
            raise ValueError("state_below should have ndim 4, has "+str(state_below.ndim))

        if double_weights:
            state_below = 2. * state_below
            state_below.name = self.layer_name + '_'+iter_name + '_2state'
        z = self.transformer.lmul(state_below) + self.broadcasted_bias()
        if self.layer_name is not None and iter_name is not None:
            z.name = self.layer_name + '_' + iter_name + '_z'
        p,h = self.max_pool(z, (self.pool_rows, self.pool_cols), msg)

        p.name = self.layer_name + '_p_' + iter_name
        h.name = self.layer_name + '_h_' + iter_name

        return p, h

    def sample(self, state_below = None, state_above = None,
            layer_above = None,
            theano_rng = None):
        """
        .. todo::

            WRITEME
        """

        if state_above is not None:
            msg = layer_above.downward_message(state_above)
            try:
                self.output_space.validate(msg)
            except TypeError, e:
                raise TypeError(str(type(layer_above))+".downward_message gave something that was not the right type: "+str(e))
        else:
            msg = None

        z = self.transformer.lmul(state_below) + self.broadcasted_bias()
        p, h, p_sample, h_sample = self.max_pool(z,
                (self.pool_rows, self.pool_cols), msg, theano_rng)

        return p_sample, h_sample

    def downward_message(self, downward_state):
        """
        .. todo::

            WRITEME
        """
        self.h_space.validate(downward_state)
        return self.transformer.lmul_T(downward_state)

    def set_batch_size(self, batch_size):
        """
        .. todo::

            WRITEME
        """
        self.transformer.set_batch_size(batch_size)

    def get_weights_topo(self):
        """
        .. todo::

            WRITEME
        """
        outp, inp, rows, cols = range(4)
        raw = self.transformer._filters.get_value()

        return np.transpose(raw,(outp,rows,cols,inp))


    def init_mf_state(self):
        """
        .. todo::

            WRITEME
        """
        default_z = self.broadcasted_bias()
        shape = {
                'b': self.dbm.batch_size,
                0: self.h_space.shape[0],
                1: self.h_space.shape[1],
                'c': self.h_space.num_channels
                }
        # work around theano bug with broadcasted stuff
        default_z += T.alloc(*([0.]+[shape[elem] for elem in self.h_space.axes])).astype(default_z.dtype)
        assert default_z.ndim == 4

        p, h = self.max_pool(
                z = default_z,
                pool_shape = (self.pool_rows, self.pool_cols))

        return p, h

    def make_state(self, num_examples, numpy_rng):
        """
        .. todo::

            WRITEME
        """
        """ Returns a shared variable containing an actual state
           (not a mean field state) for this variable.
        """

        t1 = time.time()

        empty_input = self.h_space.get_origin_batch(self.dbm.batch_size)
        h_state = sharedX(empty_input)

        default_z = T.zeros_like(h_state) + self.broadcasted_bias()

        theano_rng = make_theano_rng(None, numpy_rng.randint(2 ** 16),
                                     which_method="binomial")

        p_exp, h_exp, p_sample, h_sample = self.max_pool(
                z = default_z,
                pool_shape = (self.pool_rows, self.pool_cols),
                theano_rng = theano_rng)

        p_state = sharedX( self.output_space.get_origin_batch(
            self.dbm.batch_size))


        t2 = time.time()

        f = function([], updates = [
            (p_state, p_sample),
            (h_state, h_sample)
            ])

        t3 = time.time()

        f()

        t4 = time.time()

        logger.info('{0}.make_state took'.format(self, t4-t1))
        logger.info('\tcompose time: {0}'.format(t2-t1))
        logger.info('\tcompile time: {0}'.format(t3-t2))
        logger.info('\texecute time: {0}'.format(t4-t3))

        p_state.name = 'p_sample_shared'
        h_state.name = 'h_sample_shared'

        return p_state, h_state

    def expected_energy_term(self, state, average, state_below, average_below):
        """
        .. todo::

            WRITEME
        """

        self.input_space.validate(state_below)

        downward_state = self.downward_state(state)
        self.h_space.validate(downward_state)

        # Energy function is linear so it doesn't matter if we're averaging or not
        # Specifically, our terms are -u^T W d - b^T d where u is the upward state of layer below
        # and d is the downward state of this layer

        bias_term = (downward_state * self.broadcasted_bias()).sum(axis=(1,2,3))
        weights_term = (self.transformer.lmul(state_below) * downward_state).sum(axis=(1,2,3))

        rval = -bias_term - weights_term

        assert rval.ndim == 1

        return rval


class ConvC01B_MaxPool(HiddenLayer):
    """
    .. todo::

        WRITEME
    """

    def __init__(self,
             output_channels,
            kernel_shape,
            pool_rows,
            pool_cols,
            layer_name,
            center = False,
            irange = None,
            sparse_init = None,
            scale_by_sharing = True,
            init_bias = 0.,
            pad = 0,
            partial_sum = 1):
        self.__dict__.update(locals())
        del self.self

        assert (irange is None) != (sparse_init is None)
        self.output_axes = ('c', 0, 1, 'b')
        self.detector_channels = output_channels
        self.tied_b = 1

    def broadcasted_bias(self):
        """
        .. todo::

            WRITEME
        """

        if self.b.ndim != 1:
            raise NotImplementedError()

        shuffle = [ 'x' ] * 4
        shuffle[self.output_axes.index('c')] = 0

        return self.b.dimshuffle(*shuffle)


    def get_total_state_space(self):
        """
        .. todo::

            WRITEME
        """
        return CompositeSpace((self.h_space, self.output_space))

    def set_input_space(self, space):
        """
        .. todo::

            WRITEME
        """
        """ Note: this resets parameters!"""

        setup_detector_layer_c01b(layer=self,
                input_space=space, rng=self.dbm.rng,
                irange=self.irange)

        if not tuple(space.axes) == ('c', 0, 1, 'b'):
            raise AssertionError("You're not using c01b inputs. Ian is enforcing c01b inputs while developing his pipeline to make sure it runs at maximal speed. If you really don't want to use c01b inputs, you can remove this check and things should work. If they don't work it's only because they're not tested.")
        if self.dummy_channels != 0:
            raise NotImplementedError(str(type(self))+" does not support adding dummy channels for cuda-convnet compatibility yet, you must implement that feature or use inputs with <=3 channels or a multiple of 4 channels")

        self.input_rows = self.input_space.shape[0]
        self.input_cols = self.input_space.shape[1]
        self.h_rows = self.detector_space.shape[0]
        self.h_cols = self.detector_space.shape[1]

        if not(self.h_rows % self.pool_rows == 0):
            raise ValueError(self.layer_name + ": h_rows = %d, pool_rows = %d. Should be divisible but remainder is %d" %
                    (self.h_rows, self.pool_rows, self.h_rows % self.pool_rows))
        assert self.h_cols % self.pool_cols == 0

        self.h_space = Conv2DSpace(shape = (self.h_rows, self.h_cols), num_channels = self.output_channels,
                axes = self.output_axes)
        self.output_space = Conv2DSpace(shape = (self.h_rows / self.pool_rows,
                                                self.h_cols / self.pool_cols),
                                                num_channels = self.output_channels,
                axes = self.output_axes)

        logger.info('{0} : detector shape: {1} '
                    'pool shape: {2}'.format(self.layer_name,
                                             self.h_space.shape,
                                             self.output_space.shape))

        assert tuple(self.output_axes) == ('c', 0, 1, 'b')
        self.max_pool = max_pool_c01b

        if self.center:
            p_ofs, h_ofs = self.init_mf_state()
            self.p_offset = sharedX(self.output_space.get_origin(), 'p_offset')
            self.h_offset = sharedX(self.h_space.get_origin(), 'h_offset')
            f = function([], updates={self.p_offset: p_ofs[:,:,:,0], self.h_offset: h_ofs[:,:,:,0]})
            f()


    def get_params(self):
        """
        .. todo::

            WRITEME
        """
        assert self.b.name is not None
        W ,= self.transformer.get_params()
        assert W.name is not None

        return [ W, self.b]

    def state_to_b01c(self, state):
        """
        .. todo::

            WRITEME
        """

        if tuple(self.output_axes) == ('b',0,1,'c'):
            return state
        return [ Conv2DSpace.convert(elem, self.output_axes, ('b', 0, 1, 'c'))
                for elem in state ]

    def get_range_rewards(self, state, coeffs):
        """
        .. todo::

            WRITEME
        """
        rval = 0.

        if self.pool_rows == 1 and self.pool_cols == 1:
            # If the pool size is 1 then pools = detectors
            # and we should not penalize pools and detectors separately
            assert len(state) == 2
            assert isinstance(coeffs, float)
            _, state = state
            state = [state]
            coeffs = [coeffs]
        else:
            assert all([len(elem) == 2 for elem in [state, coeffs]])

        for s, c in safe_zip(state, coeffs):
            if c == 0.:
                continue
            # Range over everything but the channel index
            # theano can only take gradient through max if the max is over 1 axis or all axes
            # so I manually unroll the max for the case I use here
            assert self.h_space.axes == ('b', 'c', 0, 1)
            assert self.output_space.axes == ('b', 'c', 0, 1)
            mx = s.max(axis=3).max(axis=2).max(axis=0)
            assert hasattr(mx.owner.op, 'grad')
            mn = s.min(axis=3).max(axis=2).max(axis=0)
            assert hasattr(mn.owner.op, 'grad')
            assert mx.ndim == 1
            assert mn.ndim == 1
            r = mx - mn
            rval += (1. - r).mean() * c

        return rval

    def get_l1_act_cost(self, state, target, coeff, eps):
        """
        .. todo::

            WRITEME properly

        Parameters
        ----------
        state : WRITEME
        target : WRITEME
            if pools contain more than one element, should be a list
            with two elements. the first element is for the pooling
            units and the second for the detector units.
        coeff : WRITEME
        eps : WRITEME
        """
        rval = 0.


        if self.pool_rows == 1 and self.pool_cols == 1:
            # If the pool size is 1 then pools = detectors
            # and we should not penalize pools and detectors separately
            assert len(state) == 2
            assert isinstance(target, float)
            assert isinstance(coeff, float)
            _, state = state
            state = [state]
            target = [target]
            coeff = [coeff]
            if eps is None:
                eps = 0.
            eps = [eps]
        else:
            if eps is None:
                eps = [0., 0.]
            assert all([len(elem) == 2 for elem in [state, target, coeff]])
            p_target, h_target = target
            if h_target > p_target and (coeff[0] != 0. and coeff[1] != 0.):
                # note that, within each group, E[p] is the sum of E[h]
                warnings.warn("Do you really want to regularize the detector units to be more active than the pooling units?")

        for s, t, c, e in safe_zip(state, target, coeff, eps):
            if c == 0.:
                continue
            # Average over everything but the channel index
            m = s.mean(axis= [ ax for ax in range(4) if self.output_axes[ax] != 'c' ])
            assert m.ndim == 1
            rval += T.maximum(abs(m-t)-e,0.).mean()*c

        return rval

    def get_lr_scalers(self):
        """
        .. todo::

            WRITEME
        """

        rval = OrderedDict()

        if self.scale_by_sharing:
            # scale each learning rate by 1 / # times param is reused
            h_rows, h_cols = self.h_space.shape
            num_h = float(h_rows * h_cols)
            rval[self.transformer._filters] = 1. /num_h
            rval[self.b] = 1. / num_h

        return rval

    def upward_state(self, total_state):
        """
        .. todo::

            WRITEME
        """
        p,h = total_state

        if not hasattr(self, 'center'):
            self.center = False

        if self.center:
            p -= self.p_offset
            h -= self.h_offset

        return p

    def downward_state(self, total_state):
        """
        .. todo::

            WRITEME
        """
        p,h = total_state

        if not hasattr(self, 'center'):
            self.center = False

        if self.center:
            p -= self.p_offset
            h -= self.h_offset

        return h

    def get_monitoring_channels_from_state(self, state):
        """
        .. todo::

            WRITEME
        """

        P, H = state

        axes = tuple([i for i, ax in enumerate(self.output_axes) if ax != 'c'])
        p_max = P.max(axis=(0,1,2))
        p_min = P.min(axis=(0,1,2))
        p_mean = P.mean(axis=(0,1,2))

        p_range = p_max - p_min

        rval = {
                'p_max_max' : p_max.max(),
                'p_max_mean' : p_max.mean(),
                'p_max_min' : p_max.min(),
                'p_min_max' : p_min.max(),
                'p_min_mean' : p_min.mean(),
                'p_min_max' : p_min.max(),
                'p_range_max' : p_range.max(),
                'p_range_mean' : p_range.mean(),
                'p_range_min' : p_range.min(),
                'p_mean_max' : p_mean.max(),
                'p_mean_mean' : p_mean.mean(),
                'p_mean_min' : p_mean.min()
                }

        return rval

    def get_weight_decay(self, coeffs):
        """
        .. todo::

            WRITEME
        """
        W , = self.transformer.get_params()
        return coeffs * T.sqr(W).sum()

    def mf_update(self, state_below, state_above, layer_above = None, double_weights = False, iter_name = None):
        """
        .. todo::

            WRITEME
        """

        self.input_space.validate(state_below)

        if iter_name is None:
            iter_name = 'anon'

        if state_above is not None:
            assert layer_above is not None
            msg = layer_above.downward_message(state_above)
            msg.name = 'msg_from_'+layer_above.layer_name+'_to_'+self.layer_name+'['+iter_name+']'
        else:
            msg = None

        if not hasattr(state_below, 'ndim'):
            raise TypeError("state_below should be a TensorType, got " +
                    str(state_below) + " of type " + str(type(state_below)))
        if state_below.ndim != 4:
            raise ValueError("state_below should have ndim 4, has "+str(state_below.ndim))

        if double_weights:
            state_below = 2. * state_below
            state_below.name = self.layer_name + '_'+iter_name + '_2state'
        z = self.transformer.lmul(state_below) + self.broadcasted_bias()
        if self.layer_name is not None and iter_name is not None:
            z.name = self.layer_name + '_' + iter_name + '_z'
        p,h = self.max_pool(z, (self.pool_rows, self.pool_cols), msg)

        p.name = self.layer_name + '_p_' + iter_name
        h.name = self.layer_name + '_h_' + iter_name

        return p, h

    def sample(self, state_below = None, state_above = None,
            layer_above = None,
            theano_rng = None):
        """
        .. todo::

            WRITEME
        """
        raise NotImplementedError("Need to update for C01B")

        if state_above is not None:
            msg = layer_above.downward_message(state_above)
            try:
                self.output_space.validate(msg)
            except TypeError, e:
                raise TypeError(str(type(layer_above))+".downward_message gave something that was not the right type: "+str(e))
        else:
            msg = None

        z = self.transformer.lmul(state_below) + self.broadcasted_bias()
        p, h, p_sample, h_sample = self.max_pool(z,
                (self.pool_rows, self.pool_cols), msg, theano_rng)

        return p_sample, h_sample

    def downward_message(self, downward_state):
        """
        .. todo::

            WRITEME
        """
        self.h_space.validate(downward_state)
        return self.transformer.lmul_T(downward_state)

    def set_batch_size(self, batch_size):
        """
        .. todo::

            WRITEME
        """
        self.transformer.set_batch_size(batch_size)

    def get_weights_topo(self):
        """
        .. todo::

            WRITEME
        """
        return self.transformer.get_weights_topo()

    def init_mf_state(self):
        """
        .. todo::

            WRITEME
        """
        default_z = self.broadcasted_bias()
        shape = {
                'b': self.dbm.batch_size,
                0: self.h_space.shape[0],
                1: self.h_space.shape[1],
                'c': self.h_space.num_channels
                }
        # work around theano bug with broadcasted stuff
        default_z += T.alloc(*([0.]+[shape[elem] for elem in self.h_space.axes])).astype(default_z.dtype)
        assert default_z.ndim == 4

        p, h = self.max_pool(
                z = default_z,
                pool_shape = (self.pool_rows, self.pool_cols))

        return p, h

    def make_state(self, num_examples, numpy_rng):
        """
        .. todo::

            WRITEME properly

        Returns a shared variable containing an actual state
        (not a mean field state) for this variable.
        """
        raise NotImplementedError("Need to update for C01B")

        t1 = time.time()

        empty_input = self.h_space.get_origin_batch(self.dbm.batch_size)
        h_state = sharedX(empty_input)

        default_z = T.zeros_like(h_state) + self.broadcasted_bias()

        theano_rng = make_theano_rng(None, numpy_rng.randint(2 ** 16),
                                     which_method="binomial")

        p_exp, h_exp, p_sample, h_sample = self.max_pool(
                z = default_z,
                pool_shape = (self.pool_rows, self.pool_cols),
                theano_rng = theano_rng)

        p_state = sharedX( self.output_space.get_origin_batch(
            self.dbm.batch_size))


        t2 = time.time()

        f = function([], updates = [
            (p_state, p_sample),
            (h_state, h_sample)
            ])

        t3 = time.time()

        f()

        t4 = time.time()

        logger.info('{0}.make_state took {1}'.format(self, t4-t1))
        logger.info('\tcompose time: {0}'.format(t2-t1))
        logger.info('\tcompile time: {0}'.format(t3-t2))
        logger.info('\texecute time: {0}'.format(t4-t3))

        p_state.name = 'p_sample_shared'
        h_state.name = 'h_sample_shared'

        return p_state, h_state

    def expected_energy_term(self, state, average, state_below, average_below):
        """
        .. todo::

            WRITEME
        """

        raise NotImplementedError("Need to update for C01B")
        self.input_space.validate(state_below)

        downward_state = self.downward_state(state)
        self.h_space.validate(downward_state)

        # Energy function is linear so it doesn't matter if we're averaging or not
        # Specifically, our terms are -u^T W d - b^T d where u is the upward state of layer below
        # and d is the downward state of this layer

        bias_term = (downward_state * self.broadcasted_bias()).sum(axis=(1,2,3))
        weights_term = (self.transformer.lmul(state_below) * downward_state).sum(axis=(1,2,3))

        rval = -bias_term - weights_term

        assert rval.ndim == 1

        return rval


class BVMP_Gaussian(BinaryVectorMaxPool):
    """
    Like BinaryVectorMaxPool, but must have GaussianVisLayer
    as its input. Uses its beta to bias the hidden units appropriately.
    See gaussian.lyx

    beta is *not* considered a parameter of this layer, it's just an
    external factor influencing how this layer behaves.
    Gradient can still flow to beta, but it will only be included in
    the parameters list if some class other than this layer includes it.

    .. todo::

        WRITEME : parameter list
    """

    def __init__(self,
            input_layer,
            detector_layer_dim,
            pool_size,
            layer_name,
            irange = None,
            sparse_init = None,
            sparse_stdev = 1.,
            include_prob = 1.0,
            init_bias = 0.,
            W_lr_scale = None,
            b_lr_scale = None,
            center = False,
            mask_weights = None,
            max_col_norm = None,
            copies = 1):
        warnings.warn("BVMP_Gaussian math is very faith-based, need to complete gaussian.lyx")

        args = locals()

        del args['input_layer']
        del args['self']
        super(BVMP_Gaussian, self).__init__(**args)
        self.input_layer = input_layer

    def get_weights(self):
        """
        .. todo::

            WRITEME
        """
        if self.requires_reformat:
            # This is not really an unimplemented case.
            # We actually don't know how to format the weights
            # in design space. We got the data in topo space
            # and we don't have access to the dataset
            raise NotImplementedError()
        W ,= self.transformer.get_params()
        W = W.get_value()

        x = raw_input("multiply by beta?")
        if x == 'y':
            beta = self.input_layer.beta.get_value()
            return (W.T * beta).T
        assert x == 'n'
        return W

    def set_weights(self, weights):
        """
        .. todo::

            WRITEME
        """
        raise NotImplementedError("beta would make get_weights for visualization not correspond to set_weights")
        W, = self.transformer.get_params()
        W.set_value(weights)

    def set_biases(self, biases, recenter = False):
        """
        .. todo::

            WRITEME
        """
        self.b.set_value(biases)
        if recenter:
            assert self.center
            if self.pool_size != 1:
                raise NotImplementedError()
            self.offset.set_value(sigmoid_numpy(self.b.get_value()))

    def get_biases(self):
        """
        .. todo::

            WRITEME
        """
        return self.b.get_value() - self.beta_bias().eval()


    def sample(self, state_below = None, state_above = None,
            layer_above = None,
            theano_rng = None):
        """
        .. todo::

            WRITEME
        """
        raise NotImplementedError("need to account for beta")
        if self.copies != 1:
            raise NotImplementedError()

        if theano_rng is None:
            raise ValueError("theano_rng is required; it just defaults to None so that it may appear after layer_above / state_above in the list.")

        if state_above is not None:
            msg = layer_above.downward_message(state_above)
        else:
            msg = None

        if self.requires_reformat:
            state_below = self.input_space.format_as(state_below, self.desired_space)

        z = self.transformer.lmul(state_below) + self.b
        p, h, p_sample, h_sample = max_pool_channels(z,
                self.pool_size, msg, theano_rng)

        return p_sample, h_sample

    def downward_message(self, downward_state):
        """
        .. todo::

            WRITEME
        """
        rval = self.transformer.lmul_T(downward_state)

        if self.requires_reformat:
            rval = self.desired_space.format_as(rval, self.input_space)

        return rval * self.copies

    def init_mf_state(self):
        """
        .. todo::

            WRITEME
        """
        # work around theano bug with broadcasted vectors
        z = T.alloc(0., self.dbm.batch_size, self.detector_layer_dim).astype(self.b.dtype) + \
                self.b.dimshuffle('x', 0) + self.beta_bias()
        rval = max_pool_channels(z = z,
                pool_size = self.pool_size)
        return rval

    def make_state(self, num_examples, numpy_rng):
        """
        .. todo::

            WRITEME properly

        Returns a shared variable containing an actual state
        (not a mean field state) for this variable.
        """
        raise NotImplementedError("need to account for beta")

        if not hasattr(self, 'copies'):
            self.copies = 1

        if self.copies != 1:
            raise NotImplementedError()


        empty_input = self.h_space.get_origin_batch(num_examples)
        empty_output = self.output_space.get_origin_batch(num_examples)

        h_state = sharedX(empty_input)
        p_state = sharedX(empty_output)

        theano_rng = make_theano_rng(None, numpy_rng.randint(2 ** 16),
                                     which_method="binomial")

        default_z = T.zeros_like(h_state) + self.b

        p_exp, h_exp, p_sample, h_sample = max_pool_channels(
                z = default_z,
                pool_size = self.pool_size,
                theano_rng = theano_rng)

        assert h_sample.dtype == default_z.dtype

        f = function([], updates = [
            (p_state , p_sample),
            (h_state , h_sample)
            ])

        f()

        p_state.name = 'p_sample_shared'
        h_state.name = 'h_sample_shared'

        return p_state, h_state

    def expected_energy_term(self, state, average, state_below, average_below):
        """
        .. todo::

            WRITEME
        """
        raise NotImplementedError("need to account for beta, and maybe some oether stuff")

        # Don't need to do anything special for centering, upward_state / downward state
        # make it all just work

        self.input_space.validate(state_below)

        if self.requires_reformat:
            if not isinstance(state_below, tuple):
                for sb in get_debug_values(state_below):
                    if sb.shape[0] != self.dbm.batch_size:
                        raise ValueError("self.dbm.batch_size is %d but got shape of %d" % (self.dbm.batch_size, sb.shape[0]))
                    assert reduce(lambda x,y: x * y, sb.shape[1:]) == self.input_dim

            state_below = self.input_space.format_as(state_below, self.desired_space)

        downward_state = self.downward_state(state)
        self.h_space.validate(downward_state)

        # Energy function is linear so it doesn't matter if we're averaging or not
        # Specifically, our terms are -u^T W d - b^T d where u is the upward state of layer below
        # and d is the downward state of this layer

        bias_term = T.dot(downward_state, self.b)
        weights_term = (self.transformer.lmul(state_below) * downward_state).sum(axis=1)

        rval = -bias_term - weights_term

        assert rval.ndim == 1

        return rval * self.copies

    def linear_feed_forward_approximation(self, state_below):
        """
        .. todo::

            WRITEME properly

        Used to implement TorontoSparsity. Unclear exactly what properties of it are
        important or how to implement it for other layers.

        Properties it must have:
            output is same kind of data structure (ie, tuple of theano 2-tensors)
            as mf_update

        Properties it probably should have for other layer types:
            An infinitesimal change in state_below or the parameters should cause the same sign of change
            in the output of linear_feed_forward_approximation and in mf_update

            Should not have any non-linearities that cause the gradient to shrink

            Should disregard top-down feedback
        """
        raise NotImplementedError("need to account for beta")

        z = self.transformer.lmul(state_below) + self.b

        if self.pool_size != 1:
            # Should probably implement sum pooling for the non-pooled version,
            # but in reality it's not totally clear what the right answer is
            raise NotImplementedError()

        return z, z

    def beta_bias(self):
        """
        .. todo::

            WRITEME
        """
        W, = self.transformer.get_params()
        beta = self.input_layer.beta
        assert beta.ndim == 1
        return - 0.5 * T.dot(beta, T.sqr(W))

    def mf_update(self, state_below, state_above, layer_above = None, double_weights = False, iter_name = None):
        """
        .. todo::

            WRITEME
        """

        self.input_space.validate(state_below)

        if self.requires_reformat:
            if not isinstance(state_below, tuple):
                for sb in get_debug_values(state_below):
                    if sb.shape[0] != self.dbm.batch_size:
                        raise ValueError("self.dbm.batch_size is %d but got shape of %d" % (self.dbm.batch_size, sb.shape[0]))
                    assert reduce(lambda x,y: x * y, sb.shape[1:]) == self.input_dim

            state_below = self.input_space.format_as(state_below, self.desired_space)

        if iter_name is None:
            iter_name = 'anon'

        if state_above is not None:
            assert layer_above is not None
            msg = layer_above.downward_message(state_above)
            msg.name = 'msg_from_'+layer_above.layer_name+'_to_'+self.layer_name+'['+iter_name+']'
        else:
            msg = None

        if double_weights:
            state_below = 2. * state_below
            state_below.name = self.layer_name + '_'+iter_name + '_2state'
        z = self.transformer.lmul(state_below) + self.b + self.beta_bias()
        if self.layer_name is not None and iter_name is not None:
            z.name = self.layer_name + '_' + iter_name + '_z'
        p,h = max_pool_channels(z, self.pool_size, msg)

        p.name = self.layer_name + '_p_' + iter_name
        h.name = self.layer_name + '_h_' + iter_name

        return p, h

class CompositeLayer(HiddenLayer):
    """
        A Layer constructing by aligning several other Layer
        objects side by side

        Parameters
        ----------
        components : WRITEME
            A list of layers that are combined to form this layer
        inputs_to_components : None or dict mapping int to list of int
            Should be None unless the input space is a CompositeSpace
            If inputs_to_components[i] contains j, it means input i will
            be given as input to component j.
            If an input dodes not appear in the dictionary, it will be given
            to all components.

            This field allows one CompositeLayer to have another as input
            without forcing each component to connect to all members
            of the CompositeLayer below. For example, you might want to
            have both densely connected and convolutional units in all
            layers, but a convolutional unit is incapable of taking a
            non-topological input space.
    """


    def __init__(self, layer_name, components, inputs_to_components = None):
        self.layer_name = layer_name

        self.components = list(components)
        assert isinstance(components, list)
        for component in components:
            assert isinstance(component, HiddenLayer)
        self.num_components = len(components)
        self.components = list(components)

        if inputs_to_components is None:
            self.inputs_to_components = None
        else:
            if not isinstance(inputs_to_components, dict):
                raise TypeError("CompositeLayer expected inputs_to_components to be a dict, got "+str(type(inputs_to_components)))
            self.inputs_to_components = OrderedDict()
            for key in inputs_to_components:
                assert isinstance(key, int)
                assert key >= 0
                value = inputs_to_components[key]
                assert isinstance(value, list)
                assert all([isinstance(elem, int) for elem in value])
                assert min(value) >= 0
                assert max(value) < self.num_components
                self.inputs_to_components[key] = list(value)

    def set_input_space(self, space):
        """
        .. todo::

            WRITEME
        """
        self.input_space = space

        if not isinstance(space, CompositeSpace):
            assert self.inputs_to_components is None
            self.routing_needed = False
        else:
            if self.inputs_to_components is None:
                self.routing_needed = False
            else:
                self.routing_needed = True
                assert max(self.inputs_to_components) < space.num_components
                # Invert the dictionary
                self.components_to_inputs = OrderedDict()
                for i in xrange(self.num_components):
                    inputs = []
                    for j in xrange(space.num_components):
                        if i in self.inputs_to_components[j]:
                            inputs.append(i)
                    if len(inputs) < space.num_components:
                        self.components_to_inputs[i] = inputs

        for i, component in enumerate(self.components):
            if self.routing_needed and i in self.components_to_inputs:
                cur_space = space.restrict(self.components_to_inputs[i])
            else:
                cur_space = space

            component.set_input_space(cur_space)

        self.output_space = CompositeSpace([ component.get_output_space() for component in self.components ])

    def make_state(self, num_examples, numpy_rng):
        """
        .. todo::

            WRITEME
        """
        return tuple(component.make_state(num_examples, numpy_rng) for
                component in self.components)

    def get_total_state_space(self):
        """
        .. todo::

            WRITEME
        """
        return CompositeSpace([component.get_total_state_space() for component in self.components])

    def set_batch_size(self, batch_size):
        """
        .. todo::

            WRITEME
        """
        for component in self.components:
            component.set_batch_size(batch_size)

    def set_dbm(self, dbm):
        """
        .. todo::

            WRITEME
        """
        for component in self.components:
            component.set_dbm(dbm)

    def mf_update(self, state_below, state_above, layer_above = None, double_weights = False, iter_name = None):
        """
        .. todo::

            WRITEME
        """
        rval = []

        for i, component in enumerate(self.components):
            if self.routing_needed and i in self.components_to_inputs:
                cur_state_below =self.input_space.restrict_batch(state_below, self.components_to_inputs[i])
            else:
                cur_state_below = state_below

            class RoutingLayer(object):
                def __init__(self, idx, layer):
                    self.__dict__.update(locals())
                    del self.self
                    self.layer_name = 'route_'+str(idx)+'_'+layer.layer_name

                def downward_message(self, state):
                    return self.layer.downward_message(state)[self.idx]

            if layer_above is not None:
                cur_layer_above = RoutingLayer(i, layer_above)
            else:
                cur_layer_above = None

            mf_update = component.mf_update(state_below = cur_state_below,
                                            state_above = state_above,
                                            layer_above = cur_layer_above,
                                            double_weights = double_weights,
                                            iter_name = iter_name)

            rval.append(mf_update)

        return tuple(rval)

    def init_mf_state(self):
        """
        .. todo::

            WRITEME
        """
        return tuple([component.init_mf_state() for component in self.components])


    def get_weight_decay(self, coeffs):
        """
        .. todo::

            WRITEME
        """
        return sum([component.get_weight_decay(coeff) for component, coeff
            in safe_zip(self.components, coeffs)])

    def upward_state(self, total_state):
        """
        .. todo::

            WRITEME
        """
        return tuple([component.upward_state(elem)
            for component, elem in
            safe_zip(self.components, total_state)])

    def downward_state(self, total_state):
        """
        .. todo::

            WRITEME
        """
        return tuple([component.downward_state(elem)
            for component, elem in
            safe_zip(self.components, total_state)])

    def downward_message(self, downward_state):
        """
        .. todo::

            WRITEME
        """
        if isinstance(self.input_space, CompositeSpace):
            num_input_components = self.input_space.num_components
        else:
            num_input_components = 1

        rval = [ None ] * num_input_components

        def add(x, y):
            if x is None:
                return y
            if y is None:
                return x
            return x + y

        for i, packed in enumerate(safe_zip(self.components, downward_state)):
            component, state = packed
            if self.routing_needed and i in self.components_to_inputs:
                input_idx = self.components_to_inputs[i]
            else:
                input_idx = range(num_input_components)

            partial_message = component.downward_message(state)

            if len(input_idx) == 1:
                partial_message = [ partial_message ]

            assert len(input_idx) == len(partial_message)

            for idx, msg in safe_zip(input_idx, partial_message):
                rval[idx] = add(rval[idx], msg)

        if len(rval) == 1:
            rval = rval[0]
        else:
            rval = tuple(rval)

        self.input_space.validate(rval)

        return rval

    def get_l1_act_cost(self, state, target, coeff, eps):
        """
        .. todo::

            WRITEME
        """
        return sum([ comp.get_l1_act_cost(s, t, c, e) \
            for comp, s, t, c, e in safe_zip(self.components, state, target, coeff, eps)])

    def get_range_rewards(self, state, coeffs):
        """
        .. todo::

            WRITEME
        """
        return sum([comp.get_range_rewards(s, c)
            for comp, s, c in safe_zip(self.components, state, coeffs)])

    def get_params(self):
        """
        .. todo::

            WRITEME
        """
        return reduce(lambda x, y: safe_union(x, y),
                [component.get_params() for component in self.components])

    def get_weights_topo(self):
        """
        .. todo::

            WRITEME
        """
        logger.info('Get topological weights for which layer?')
        for i, component in enumerate(self.components):
            logger.info('{0} {1}'.format(i, component.layer_name))
        x = raw_input()
        return self.components[int(x)].get_weights_topo()

    def get_monitoring_channels_from_state(self, state):
        """
        .. todo::

            WRITEME
        """
        rval = OrderedDict()

        for layer, s in safe_zip(self.components, state):
            d = layer.get_monitoring_channels_from_state(s)
            for key in d:
                rval[layer.layer_name+'_'+key] = d[key]

        return rval

    def sample(self, state_below = None, state_above = None,
            layer_above = None,
            theano_rng = None):
        """
        .. todo::

            WRITEME
        """
        rval = []

        for i, component in enumerate(self.components):
            if self.routing_needed and i in self.components_to_inputs:
                cur_state_below =self.input_space.restrict_batch(state_below, self.components_to_inputs[i])
            else:
                cur_state_below = state_below

            class RoutingLayer(object):
                def __init__(self, idx, layer):
                    self.__dict__.update(locals())
                    del self.self
                    self.layer_name = 'route_'+str(idx)+'_'+layer.layer_name

                def downward_message(self, state):
                    return self.layer.downward_message(state)[self.idx]

            if layer_above is not None:
                cur_layer_above = RoutingLayer(i, layer_above)
            else:
                cur_layer_above = None

            sample = component.sample(state_below = cur_state_below,
                                            state_above = state_above,
                                            layer_above = cur_layer_above,
                                            theano_rng = theano_rng)

            rval.append(sample)

        return tuple(rval)

########NEW FILE########
__FILENAME__ = sampling_procedure
"""
.. todo::

    WRITEME
"""
__authors__ = ["Ian Goodfellow", "Vincent Dumoulin"]
__copyright__ = "Copyright 2012-2013, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"

from theano.compat import OrderedDict
from pylearn2.utils import py_integer_types


class SamplingProcedure(object):
    """
    Procedure for sampling from a DBM.
    """

    def set_dbm(self, dbm):
        """
        .. todo::

            WRITEME
        """
        self.dbm = dbm

    def sample(self, layer_to_state, theano_rng, layer_to_clamp=None,
               num_steps=1):
        """
        Samples from self.dbm using `layer_to_state` as starting values.

        Parameters
        ----------
        layer_to_state : dict
            Maps the DBM's Layer instances to theano variables representing
            batches of samples of them.
        theano_rng : theano.sandbox.rng_mrg.MRG_RandomStreams
            WRITEME
        layer_to_clamp : dict, optional
            Maps Layers to bools. If a layer is not in the dictionary,
            defaults to False. True indicates that this layer should be
            clamped, so we are sampling from a conditional distribution
            rather than the joint distribution.
        num_steps : int, optional
            WRITEME

        Returns
        -------
        layer_to_updated_state : dict
            Maps the DBM's Layer instances to theano variables representing
            batches of updated samples of them.
        """
        raise NotImplementedError(str(type(self))+" does not implement " +
                                  "sample.")


class GibbsEvenOdd(SamplingProcedure):
    """
    The specific sampling schedule used to sample all of the even-idexed
    layers of model.hidden_layers, then the visible layer and all the
    odd-indexed layers.
    """

    def sample(self, layer_to_state, theano_rng, layer_to_clamp=None,
               num_steps=1):
        """
        .. todo::

            WRITEME
        """
        # Validate num_steps
        assert isinstance(num_steps, py_integer_types)
        assert num_steps > 0

        # Implement the num_steps > 1 case by repeatedly calling the
        # num_steps == 1 case
        if num_steps != 1:
            for i in xrange(num_steps):
                layer_to_state = self.sample(layer_to_state, theano_rng,
                                             layer_to_clamp, num_steps=1)
            return layer_to_state

        # The rest of the function is the num_steps = 1 case
        # Current code assumes this, though we could certainly relax this
        # constraint
        assert len(self.dbm.hidden_layers) > 0

        # Validate layer_to_clamp / make sure layer_to_clamp is a fully
        # populated dictionary
        if layer_to_clamp is None:
            layer_to_clamp = OrderedDict()

        for key in layer_to_clamp:
            assert (key is self.dbm.visible_layer or
                    key in self.dbm.hidden_layers)

        for layer in [self.dbm.visible_layer] + self.dbm.hidden_layers:
            if layer not in layer_to_clamp:
                layer_to_clamp[layer] = False

        # Assemble the return value
        layer_to_updated = OrderedDict()

        for i, this_layer in list(enumerate(self.dbm.hidden_layers))[::2]:
            # Iteration i does the Gibbs step for hidden_layers[i]

            # Get the sampled state of the layer below so we can condition
            # on it in our Gibbs update
            if i == 0:
                layer_below = self.dbm.visible_layer
            else:
                layer_below = self.dbm.hidden_layers[i-1]
            state_below = layer_to_state[layer_below]
            state_below = layer_below.upward_state(state_below)

            # Get the sampled state of the layer above so we can condition
            # on it in our Gibbs step
            if i + 1 < len(self.dbm.hidden_layers):
                layer_above = self.dbm.hidden_layers[i + 1]
                state_above = layer_to_state[layer_above]
                state_above = layer_above.downward_state(state_above)
            else:
                state_above = None
                layer_above = None

            if layer_to_clamp[this_layer]:
                this_state = layer_to_state[this_layer]
                this_sample = this_state
            else:
                # Compute the Gibbs sampling update
                # Sample the state of this layer conditioned
                # on its Markov blanket (the layer above and
                # layer below)
                this_sample = this_layer.sample(state_below=state_below,
                                                state_above=state_above,
                                                layer_above=layer_above,
                                                theano_rng=theano_rng)

            layer_to_updated[this_layer] = this_sample

        #Sample the visible layer
        vis_state = layer_to_state[self.dbm.visible_layer]
        if layer_to_clamp[self.dbm.visible_layer]:
            vis_sample = vis_state
        else:
            first_hid = self.dbm.hidden_layers[0]
            state_above = layer_to_updated[first_hid]
            state_above = first_hid.downward_state(state_above)

            vis_sample = self.dbm.visible_layer.sample(state_above=state_above,
                                                       layer_above=first_hid,
                                                       theano_rng=theano_rng)
        layer_to_updated[self.dbm.visible_layer] = vis_sample

        # Sample the odd-numbered layers
        for i, this_layer in list(enumerate(self.dbm.hidden_layers))[1::2]:

            # Get the sampled state of the layer below so we can condition
            # on it in our Gibbs update
            layer_below = self.dbm.hidden_layers[i-1]

            # We want to sample from each conditional distribution
            # ***sequentially*** so we must use the updated version
            # of the state for the layers whose updates we have
            # calculcated already, in layer_to_updated.
            # If we used the original value from
            # layer_to_state
            # then we would sample from each conditional
            # ***simultaneously*** which does not implement MCMC
            # sampling.
            state_below = layer_to_updated[layer_below]

            state_below = layer_below.upward_state(state_below)

            # Get the sampled state of the layer above so we can condition
            # on it in our Gibbs step
            if i + 1 < len(self.dbm.hidden_layers):
                layer_above = self.dbm.hidden_layers[i + 1]
                state_above = layer_to_updated[layer_above]
                state_above = layer_above.downward_state(state_above)
            else:
                state_above = None
                layer_above = None

            if layer_to_clamp[this_layer]:
                this_state = layer_to_state[this_layer]
                this_sample = this_state
            else:
                # Compute the Gibbs sampling update
                # Sample the state of this layer conditioned
                # on its Markov blanket (the layer above and
                # layer below)
                this_sample = this_layer.sample(state_below=state_below,
                                                state_above=state_above,
                                                layer_above=layer_above,
                                                theano_rng=theano_rng)

            layer_to_updated[this_layer] = this_sample

        # Check that all layers were updated
        assert all([layer in layer_to_updated for layer in layer_to_state])
        # Check that we didn't accidentally treat any other object as a layer
        assert all([layer in layer_to_state for layer in layer_to_updated])
        # Check that clamping worked
        assert all([(layer_to_state[layer] is layer_to_updated[layer]) ==
                    layer_to_clamp[layer] for layer in layer_to_state])

        return layer_to_updated

########NEW FILE########
__FILENAME__ = differentiable_sparse_coding
"""
An implementation of the model described in "Differentiable Sparse Coding" by
Bradley and Bagnell
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

import logging
import numpy as N
import theano.tensor as T

import theano
from theano import function, shared, config
floatX = config.floatX
from pylearn2.utils.rng import make_np_rng


logger = logging.getLogger(__name__)


class DifferentiableSparseCoding(object):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    nvis : WRITEME
    nhid : WRITEME
    init_lambda : WRITEME
    init_p : WRITEME
    init_alpha : WRITEME
    learning_rate : WRITEME
    """

    def __init__(self, nvis, nhid,
            init_lambda,
            init_p, init_alpha, learning_rate):
        self.nvis = int(nvis)
        self.nhid = int(nhid)
        self.init_lambda = float(init_lambda)
        self.init_p = float(init_p)
        self.init_alpha = N.cast[config.floatX](init_alpha)
        self.tol = 1e-6
        self.time_constant = 1e-2
        self.learning_rate = N.cast[config.floatX](learning_rate)

        self.predictor_learning_rate = self.learning_rate

        self.rng = make_np_rng(None, [1,2,3], which_method="randn")

        self.error_record = []
        self.ERROR_RECORD_MODE_MONITORING = 0
        self.error_record_mode = self.ERROR_RECORD_MODE_MONITORING

        self.instrumented = False

        self.redo_everything()

    def get_output_dim(self):
        """
        .. todo::

            WRITEME
        """
        return self.nhid

    def get_output_channels(self):
        """
        .. todo::

            WRITEME
        """
        return self.nhid

    def normalize_W(self):
        """
        .. todo::

            WRITEME
        """
        W = self.W.get_value(borrow=True)
        norms = N.sqrt(N.square(W).sum(axis=0))
        self.W.set_value(W/norms, borrow=True)

    def redo_everything(self):
        """
        .. todo::

            WRITEME
        """
        self.W = shared(N.cast[floatX](self.rng.randn(self.nvis,self.nhid)), name='W')

        self.pred_W = shared(self.W.get_value(borrow=False),name='pred_W')
        self.pred_b = shared(N.zeros(self.nhid,dtype=floatX),name='pred_b')
        self.pred_g = shared(N.ones(self.nhid,dtype=floatX),name='pred_g')


        self.normalize_W()
        self.p = shared(N.zeros(self.nhid, dtype=floatX)+N.cast[floatX](self.init_p), name='p')

        #mispelling lambda because python is too dumb to know that self.lambda isn't a lambda function
        self.lamda = shared(
                            N.zeros(
                                    self.nhid, dtype=floatX)+
                            N.cast[floatX](self.init_lambda), name='lambda')


        self.alpha = self.init_alpha
        self.failure_rate = .5

        self.examples_seen = 0
        self.batches_seen = 0

        self.redo_theano()

    def recons_error(self, v, h):
        """
        .. todo::

            WRITEME
        """
        recons = T.dot(self.W,h)
        diffs = recons - v
        rval = T.dot(diffs,diffs) / N.cast[floatX](self.nvis)
        return rval

    def recons_error_batch(self, V, H):
        """
        .. todo::

            WRITEME
        """
        recons = T.dot(H,self.W.T)
        diffs = recons - V
        rval = T.mean(T.sqr(diffs))
        return rval

    def sparsity_penalty(self, v, h):
        """
        .. todo::

            WRITEME
        """
        sparsity_measure = h * T.log(h) - h * T.log(self.p) - h + self.p
        rval = T.dot(self.lamda, sparsity_measure) / N.cast[floatX](self.nhid)
        return rval

    def sparsity_penalty_batch(self, V, H):
        """
        .. todo::

            WRITEME
        """
        sparsity_measure = H * T.log(H) - H * T.log(self.p) - H + self.p
        sparsity_measure_exp = T.mean(sparsity_measure, axis=0)
        rval = T.dot(self.lamda, sparsity_measure_exp) / N.cast[floatX](self.nhid)
        return rval


    def coding_obj(self, v, h):
        """
        .. todo::

            WRITEME
        """
        return self.recons_error(v,h) + self.sparsity_penalty(v,h)

    def coding_obj_batch(self, V, H):
        """
        .. todo::

            WRITEME
        """
        return self.recons_error_batch(V,H) + self.sparsity_penalty_batch(V,H)

    def predict(self, V):
        """
        .. todo::

            WRITEME
        """
        rval =  T.nnet.sigmoid(T.dot(V,self.pred_W)+self.pred_b)*self.pred_g
        assert rval.type.dtype == V.type.dtype
        return rval

    def redo_theano(self):
        """
        .. todo::

            WRITEME
        """

        self.h = shared(N.zeros(self.nhid, dtype=floatX), name='h')
        self.v = shared(N.zeros(self.nvis, dtype=floatX), name='v')

        input_v = T.vector()
        assert input_v.type.dtype == floatX

        self.init_h_v = function([input_v], updates = { self.h : self.predict(input_v),
                                                 self.v : input_v } )


        coding_obj = self.coding_obj(self.v, self.h)
        assert len(coding_obj.type.broadcastable) == 0

        coding_grad = T.grad(coding_obj, self.h)
        assert len(coding_grad.type.broadcastable) == 1

        self.coding_obj_grad = function([], [coding_obj, coding_grad] )


        self.new_h = shared(N.zeros(self.nhid, dtype=floatX), name='new_h')

        alpha = T.scalar(name='alpha')

        outside_grad = T.vector(name='outside_grad')

        new_h = T.clip(self.h * T.exp(-alpha * outside_grad), 1e-10, 1e4)

        new_obj = self.coding_obj(self.v, new_h)

        self.try_step = function( [alpha, outside_grad], updates = { self.new_h : new_h }, outputs = new_obj )

        self.accept_h = function( [], updates = { self.h : self.new_h } )

        self.get_h = function( [] , self.h )


        V = T.matrix(name='V')
        H = T.matrix(name='H')

        coding_obj_batch = self.coding_obj_batch(V,H)

        self.code_learning_obj = function( [V,H], coding_obj_batch)

        learning_grad = T.grad( coding_obj_batch, self.W )
        self.code_learning_step = function( [V,H,alpha], updates = { self.W : self.W - alpha * learning_grad } )



        pred_obj = T.mean(T.sqr(self.predict(V)-H))

        predictor_params = [ self.pred_W, self.pred_b, self.pred_g ]

        pred_grads = T.grad(pred_obj, wrt = predictor_params )

        predictor_updates = {}

        for param, grad in zip(predictor_params, pred_grads):
            predictor_updates[param] = param - alpha * grad

        predictor_updates[self.pred_g ] = T.clip(predictor_updates[self.pred_g], N.cast[floatX](0.5), N.cast[floatX](1000.))

        self.train_predictor = function([V,H,alpha] , updates = predictor_updates )

    def weights_format(self):
        """
        .. todo::

            WRITEME
        """
        return ['v','h']

    def error_func(self, x):
        """
        .. todo::

            WRITEME
        """
        batch_size = x.shape[0]

        H = N.zeros((batch_size,self.nhid),dtype=floatX)

        for i in xrange(batch_size):
            assert self.alpha > 9e-8
            H[i,:] = self.optimize_h(x[i,:])
            assert self.alpha > 9e-8

        return self.code_learning_obj(x,H)

    def record_monitoring_error(self, dataset, batch_size, batches):
        """
        .. todo::

            WRITEME
        """
        logger.info('running on monitoring set')
        assert self.error_record_mode == self.ERROR_RECORD_MODE_MONITORING

        w = self.W.get_value(borrow=True)
        logger.info('weights summary: '
                    '({0}, {1}, {2})'.format(w.min(), w.mean(), w.max()))

        errors = []

        if self.instrumented:
            self.clear_instruments()

        for i in xrange(batches):
            x = dataset.get_batch_design(batch_size)
            error = self.error_func(x)
            errors.append( error )
            if self.instrumented:
                self.update_instruments(x)


        self.error_record.append( (self.examples_seen, self.batches_seen, N.asarray(errors).mean() ) )


        if self.instrumented:
            self.instrument_record.begin_report(examples_seen = self.examples_seen, batches_seen = self.batches_seen)
            self.make_instrument_report()
            self.instrument_record.end_report()
            self.clear_instruments()
        logger.info('monitoring set done')

    def infer_h(self, v):
        """
        .. todo::

            WRITEME
        """
        return self.optimize_h(v)

    def optimize_h(self, v):
        """
        .. todo::

            WRITEME
        """
        assert self.alpha > 9e-8

        self.init_h_v(v)

        first = True

        while True:
            obj, grad = self.coding_obj_grad()

            if first:
                #print 'orig_obj: ', obj
                first = False

            assert not N.any(N.isnan(obj))
            assert not N.any(N.isnan(grad))

            if N.abs(grad).max() < self.tol:
                break

            #print 'max gradient ',N.abs(grad).max()

            cur_alpha = N.cast[floatX] ( self.alpha  + 0.0 )

            new_obj = self.try_step(cur_alpha, grad)

            assert not N.isnan(new_obj)

            self.failure_rate =  (1. - self.time_constant ) * self.failure_rate + self.time_constant * float(new_obj > obj )

            assert self.alpha > 9e-8

            if self.failure_rate  > .6 and self.alpha > 1e-7:
                self.alpha *= .9
                #print '!!!!!!!!!!!!!!!!!!!!!!shrank alpha to ',self.alpha
            elif self.failure_rate < .3:
                self.alpha *= 1.1
                #print '**********************grew alpha to ',self.alpha

            assert self.alpha > 9e-8

            while new_obj >= obj:
                cur_alpha *= .9

                if cur_alpha < 1e-12:
                    self.accept_h()
                    #print 'failing final obj ',new_obj
                    return self.get_h()

                new_obj = self.try_step(cur_alpha, grad)

                assert not N.isnan(new_obj)

            self.accept_h()

        #print 'final obj ',new_obj
        return self.get_h()

    def train_batch(self, dataset, batch_size):
        """
        .. todo::

            WRITEME
        """
        self.learn_mini_batch(dataset.get_batch_design(batch_size))
        return True

    def learn_mini_batch(self, x):
        """
        .. todo::

            WRITEME
        """
        assert self.alpha > 9e-8

        batch_size = x.shape[0]

        H = N.zeros((batch_size,self.nhid),dtype=floatX)

        for i in xrange(batch_size):
            assert self.alpha > 9e-8
            H[i,:] = self.optimize_h(x[i,:])
            assert self.alpha > 9e-8

        self.code_learning_step(x,H,self.learning_rate)
        self.normalize_W()

        self.train_predictor(x,H,self.predictor_learning_rate)

        self.examples_seen += x.shape[0]
        self.batches_seen += 1

########NEW FILE########
__FILENAME__ = gsn
"""
Generative Stochastic Networks

This is described in:

- "Generalized Denoising Auto-Encoders as Generative Models" Bengio, Yao, Alain,
   Vincent. arXiv:1305.6663
- "Deep Generative Stochastic Networks Trainable by Backprop" Bengio,
   Thibodeau-Laufer. arXiv:1306.1091

There is an example of training both unsupervised and supervised GSNs on MNIST
in pylearn2/scripts/gsn_example.py
"""
__authors__ = "Eric Martin"
__copyright__ = "Copyright 2013, Universite de Montreal"
__license__ = "3-clause BSD"

import copy
import functools
import warnings

import numpy as np
import theano
T = theano.tensor

from pylearn2.base import StackedBlocks
from pylearn2.expr.activations import identity
from pylearn2.models.autoencoder import Autoencoder
from pylearn2.models.model import Model
from pylearn2.utils import safe_zip

# Enforce correct restructured text list format.
# Be sure to re-run docgen.py and make sure there are no warnings if you
# modify the module-level docstring.
assert """:

- """ in __doc__

class GSN(StackedBlocks, Model):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    autoencoders : list
        A list of autoencoder objects. As of now, only the functionality
        from the base Autoencoder class is used.
    preact_cors : list
        A list of length len(autoencoders) + 1 where each element is a
        callable (which includes Corruptor objects). The callable at
        index i is called before activating the ith layer. Name stands
        for "preactivation corruptors".
    postact_cors : list
        A list of length len(autoencoders) + 1 where each element is a
        callable (which includes Corruptor objects). The callable at
        index i is called directly after activating the ith layer. Name
        stands for "postactivation corruptors". The valid values for this
        parameter are the same as that for preact_cors.
    layer_samplers: list
        Describes how to sample from each layer. Sampling occurs directly
        before the post activation corruption is applied. Valid values
        for this argument are of the same form as valid parameters for
        preact_cor and postact_cor (and if an element in the list is
        None, no sampling will be applied at that layer). Note: as of
        right now, we've only experimented with sampling at the visible
        layer.

    Notes
    -----
    Most of the time it will be much easier to construct a GSN using
    GSN.new rather than GSN.__init__. This method exists to make the GSN
    class very easy to modify.

    The activation function for the visible layer is the "act_dec" function
    on the first autoencoder, and the activation function for the i_th
    hidden layer is the "act_enc" function on the (i - 1)th autoencoder.
    """

    def __init__(self, autoencoders, preact_cors=None, postact_cors=None,
                 layer_samplers=None):
        super(GSN, self).__init__(autoencoders)

        # only for convenience
        self.aes = self._layers

        # easy way to turn off corruption (True => corrupt, False => don't)
        self._corrupt_switch = True

        # easy way to turn off sampling
        self._sample_switch = True

        # easy way to not use bias (True => use bias, False => don't)
        self._bias_switch = True

        # check that autoencoders are the correct sizes by looking at previous
        # layer. We can't do this for the first ae, so we skip it.
        for i in xrange(1, len(self.aes)):
            assert (self.aes[i].weights.get_value().shape[0] ==
                    self.aes[i - 1].nhid)


        # do some type checking and convert None's to identity function
        def _make_callable_list(previous):
            """
            .. todo::

                WRITEME
            """
            if len(previous) != self.nlayers:
                raise ValueError("Need same number of corruptors/samplers as layers")

            if not all(map(lambda x: callable(x) or x is None, previous)):
                raise ValueError("All elements must either be None or be a callable")

            return map(lambda x: identity if x is None else x, previous)

        self._preact_cors = _make_callable_list(preact_cors)
        self._postact_cors = _make_callable_list(postact_cors)
        self._layer_samplers = _make_callable_list(layer_samplers)

    @staticmethod
    def _make_aes(layer_sizes, activation_funcs, tied=True):
        """
        Creates the Autoencoder objects needed by the GSN.

        Parameters
        ----------
        layer_sizes : WRITEME
        activation_funcs : WRITEME
        tied : WRITEME
        """
        aes = []
        assert len(activation_funcs) == len(layer_sizes)

        for i in xrange(len(layer_sizes) - 1):
            # activation for visible layer is aes[0].act_dec
            act_enc = activation_funcs[i + 1]
            act_dec = act_enc if i != 0 else activation_funcs[0]
            aes.append(
                Autoencoder(layer_sizes[i], layer_sizes[i + 1],
                            act_enc, act_dec, tied_weights=tied)
            )

        return aes

    @classmethod
    def new(cls,
            layer_sizes,
            activation_funcs,
            pre_corruptors,
            post_corruptors,
            layer_samplers,
            tied=True):
        """
        An easy (and recommended) way to initialize a GSN.

        Parameters
        ----------
        layer_sizes : list
            A list of integers. The i_th element in the list is the size of
            the i_th layer of the network, and the network will have
            len(layer_sizes) layers.
        activation_funcs : list
            activation_funcs must be a list of the same length as layer_sizes
            where the i_th element is the activation function for the i_th
            layer. Each component of the list must refer to an activation
            function in such a way that the Autoencoder class recognizes the
            function. Valid values include a callable (which takes a symbolic
            tensor), a string that refers to a Theano activation function, or
            None (which gives the identity function).
        preact_corruptors : list
            preact_corruptors follows exactly the same format as the
            activations_func argument.
        postact_corruptors : list
            postact_corruptors follows exactly the same format as the
            activations_func argument.
        layer_samplers : list
            layer_samplers follows exactly the same format as the
            activations_func argument.
        tied : bool
            Indicates whether the network should use tied weights.

        Notes
        -----
        The GSN classes applies functions in the following order:
          - pre-activation corruption
          - activation
          - clamping applied
          - sampling
          - post-activation corruption

        All setting and returning of values occurs after applying the
        activation function (or clamping if clamping is used) but before
        applying sampling.
        """
        args = [layer_sizes, pre_corruptors, post_corruptors, layer_samplers]
        if not all(isinstance(arg, list) for arg in args):
            raise TypeError("All arguments except for tied must be lists")
        if not all(len(arg) == len(args[0]) for arg in args):
            lengths = map(len, args)
            raise ValueError("All list arguments must be of the same length. " +
                             "Current lengths are %s" % lengths)

        aes = cls._make_aes(layer_sizes, activation_funcs, tied=tied)

        return cls(aes,
                   preact_cors=pre_corruptors,
                   postact_cors=post_corruptors,
                   layer_samplers=layer_samplers)

    @functools.wraps(Model.get_params)
    def get_params(self):
        """
        .. todo::

            WRITEME
        """
        params = set()
        for ae in self.aes:
            params.update(ae.get_params())
        return list(params)

    @property
    def nlayers(self):
        """
        Returns how many layers the GSN has.
        """
        return len(self.aes) + 1

    def _run(self, minibatch, walkback=0, clamped=None):
        """
        This runs the GSN on input 'minibatch' and returns all of the activations
        at every time step.

        Parameters
        ----------
        minibatch : see parameter description in _set_activations
        walkback : int
            How many walkback steps to perform.
        clamped : list of theano tensors or None.
            clamped must be None or a list of len(minibatch) where each
            element is a Theano tensor or None. Each Theano tensor should be
            1 for indices where the value should be clamped and 0 for where
            the value should not be clamped.

        Returns
        -------
        steps : list of list of tensor_likes
            A list of the activations at each time step. The activations
            themselves are lists of tensor_like symbolic variables.
            A time step consists of a call to the _update function
            (so updating both the even and odd layers). When there is no
            walkback, the GSN runs long enough for signal from the bottom
            layer to propogate to the top layer and then back to the bottom.
            The walkback parameter adds single steps on top of the default.
        """
        # the indices which are being set
        set_idxs = safe_zip(*minibatch)[0]

        if self.nlayers == 2 and len(set_idxs) == 2:
            if clamped is None:
                raise ValueError("Setting both layers of 2 layer GSN without " +
                                 "clamping causes one layer to overwrite the " +
                                 "other. The value for layer 0 will not be used.")
            else:
                warnings.warn("Setting both layers of 2 layer GSN with clamping " +
                              "may not be valid, depending on what clamping is " +
                              "done")

        diff = lambda L: [L[i] - L[i - 1] for i in xrange(1, len(L))]
        if 1 in diff(sorted(set_idxs)):
            # currently doing an odd step at first. If this warning appears, you
            # should remember that the odd step (ie calculating the odd activations)
            # is done first (so all setting of odd layers is valid) and that for
            # an even layer to have an effect it must be used to compute either the
            # (odd) layer below or above it.
            warnings.warn("Adjacent layers in the GSN are being set. There is a" +
                          " significant possibility that some of the set values" +
                          " are not being used and are just overwriting each " +
                          "other. This is dependent on both the ordering of the " +
                          "even and odd steps as well as the proximity to the " +
                          "edge of the network.\n It is recommended to read the " +
                          "source to ensure the behavior is understood if setting " +
                          "adjacent layers.")

        self._set_activations(minibatch)

        # intialize steps
        steps = [self.activations[:]]

        self.apply_postact_corruption(self.activations,
                                      xrange(self.nlayers))

        if clamped is not None:
            vals = safe_zip(*minibatch)[1]
            clamped = safe_zip(set_idxs, vals, clamped)

        # main loop
        for _ in xrange(len(self.aes) + walkback):
            steps.append(self._update(self.activations, clamped=clamped))

        return steps

    def _make_or_get_compiled(self, indices, clamped=False):
        """
        Compiles, wraps, and caches Theano functions for non-symbolic calls
        to get_samples.

        Parameters
        ----------
        indices : WRITEME
        clamped : WRITEME
        """
        def compile_f_init():
            mb = T.matrices(len(indices))
            zipped = safe_zip(indices, mb)
            f_init = theano.function(mb,
                                     self._set_activations(zipped, corrupt=True),
                                     allow_input_downcast=True)
            # handle splitting of concatenated data
            def wrap_f_init(*args):
                data = f_init(*args)
                length = len(data) / 2
                return data[:length], data[length:]
            return wrap_f_init

        def compile_f_step():
            prev = T.matrices(self.nlayers)
            if clamped:
                _initial = T.matrices(len(indices))
                _clamps = T.matrices(len(indices))

                z = self._update(copy.copy(prev),
                                 clamped=safe_zip(indices, _initial, _clamps),
                                 return_activations=True)
                f = theano.function(prev + _initial + _clamps, z,
                                    on_unused_input='ignore',
                                    allow_input_downcast=True)
            else:
                z = self._update(copy.copy(prev), return_activations=True)
                f = theano.function(prev, z, on_unused_input='ignore',
                                    allow_input_downcast=True)

            def wrapped(*args):
                data = f(*args)
                length = len(data) / 2
                return data[:length], data[length:]

            return wrapped

        # things that require re-compiling everything
        state = (self._corrupt_switch, self._sample_switch, self._bias_switch)

        if hasattr(self, '_compiled_cache') and state == self._compiled_cache[0]:
            # already have some cached functions

            if indices == self._compiled_cache[1]:
                # everything is cached, return all but state and indices
                return self._compiled_cache[2:]
            else:
                # indices have changed, need to recompile f_init
                f_init = compile_f_init()
                cc = self._compiled_cache
                self._compiled_cache = (state, indices, f_init, cc[3])
                return self._compiled_cache[2:]
        else:
            # have no cached function (or incorrect state)
            f_init = compile_f_init()
            f_step = compile_f_step()
            self._compiled_cache = (state, indices, f_init, f_step)
            return self._compiled_cache[2:]

    def get_samples(self, minibatch, walkback=0, indices=None, symbolic=True,
                    include_first=False, clamped=None):
        """
        Runs minibatch through GSN and returns reconstructed data.

        Parameters
        ----------
        minibatch : see parameter description in _set_activations
            In addition to the description in get_samples, the tensor_likes
            in the list should be replaced by numpy matrices if symbolic=False.
        walkback : int
            How many walkback steps to perform. This is both how many extra
            samples to take as well as how many extra reconstructed points
            to train off of. See description in _run.
            This parameter controls how many samples you get back.
        indices : None or list of ints, optional
            Indices of the layers that should be returned for each time step.
            If indices is None, then get_samples returns the values for all
            of the layers which were initially specified (by minibatch).
        symbolic : bool, optional
            Whether the input (minibatch) contains a Theano (symbolic)
            tensors or actual (numpy) arrays. This flag is needed because
            Theano cannot compile the large computational graphs that
            walkback creates.
        include_first : bool, optional
            Whether to include the initial activations (ie just the input) in
            the output. This is useful for visualization, but can screw up
            training due to some cost functions failing on perfect
            reconstruction.
        clamped : list of tensor_likes, optional
            See description on _run. Theano symbolics should be replaced by
            numpy matrices if symbolic=False. Length must be the same as
            length of minibatch.

        Returns
        -------
        reconstructions : list of tensor_likes
            A list of length 1 + number of layers + walkback that contains
            the samples generated by the GSN. The layers returned at each
            time step is decided by the indices parameter (and defaults to
            the layers specified in minibatch). If include_first is True,
            then the list will be 1 element longer (inserted at beginning)
            than specified above.
        """
        if walkback > 8 and symbolic:
            warnings.warn(("Running GSN in symbolic mode (needed for training) " +
                           "with a lot of walkback. Theano may take a very long " +
                           "time to compile this computational graph. If " +
                           "compiling is taking too long, then reduce the amount " +
                           "of walkback."))

        input_idxs = safe_zip(*minibatch)[0]
        if indices is None:
            indices = input_idxs

        if not symbolic:
            vals = safe_zip(*minibatch)[1]
            f_init, f_step = self._make_or_get_compiled(input_idxs,
                                                        clamped=clamped is not None)

            if clamped is None:
                get_args = lambda x: x
            else:
                mb_values = [mb[1] for mb in minibatch]
                get_args = lambda x: x + mb_values + clamped

            precor, activations = f_init(*vals)
            results = [precor]
            for _ in xrange(len(self.aes) + walkback):
                precor, activations = f_step(*get_args(activations))
                results.append(precor)
        else:
            results = self._run(minibatch, walkback=walkback, clamped=clamped)

        # leave out the first time step
        if not include_first:
            results = results[1:]

        return [[step[i] for i in indices] for step in results]

    @functools.wraps(Autoencoder.reconstruct)
    def reconstruct(self, minibatch):
        """
        .. todo::

            WRITEME
        """
        # included for compatibility with cost functions for autoencoders,
        # so assumes model is in unsupervised mode

        assert len(minibatch) == 1
        idx = minibatch[0][0]
        return self.get_samples(minibatch, walkback=0, indices=[idx])

    def __call__(self, minibatch):
        """
        As specified by StackedBlocks, this returns the output representation of
        all layers. This occurs at the final time step.

        Parameters
        ----------
        minibatch : WRITEME

        Returns
        -------
        WRITEME
        """
        return self._run(minibatch)[-1]

    """
    NOTE: The following methods contain the algorithmic content of the GSN class.
    All of these methods are written in a way such that they can be run without
    modifying the state of the GSN object. This primary visible consequence of this
    is that the methods take an "activations parameter", which is generally just
    self.activations.
    Although this style is a bit odd, it is completely necessary. Theano can handle
    small amounts of walkback (which allows us to train for walkback), but for
    many sampling iterations (ie more than 10) Theano struggles to compile these
    large computational graphs. Making all of these methods below take
    activations as an explicit parameter (which they then modify in place,
    which allows calling with self.activations) allows one to create smaller
    external Theano functions that allow many sampling iterations.
    See pylearn2.models.tests.test_gsn.sampling_test for an example.
    """

    def _set_activations(self, minibatch, set_val=True, corrupt=False):
        """
        Initializes the GSN as specified by minibatch.

        Parameters
        ----------
        minibatch : list of (int, tensor_like)
            The minibatch parameter must be a list of tuples of form
            (int, tensor_like), where the int component represents the index
            of the layer (so 0 for visible, -1 for top/last layer) and the
            tensor_like represents the activation at that level. Layer
            indices not included in the minibatch will be set to 0. For
            tuples included in the minibatch, the tensor_like component can
            actually be None; this will result in that layer getting set to 0
            initially.
        set_val : bool, optional
            Determines whether the method sets self.activations.
        corrupt : bool, optional
            Instructs the method to return both a non-corrupted and corrupted
            set of activations rather than just non-corrupted.

        Notes
        -----
        This method creates a new list, not modifying an existing list.
        This method also does the first odd step in the network.
        """
        activations = [None] * self.nlayers

        mb_size = minibatch[0][1].shape[0]
        first_layer_size = self.aes[0].weights.shape[0]

        # zero out activations to start
        activations[0] = T.alloc(0, mb_size, first_layer_size)
        for i in xrange(1, len(activations)):
            activations[i] = T.zeros_like(
                T.dot(activations[i - 1], self.aes[i - 1].weights)
            )

        # set minibatch
        for i, val in minibatch:
            if val is not None:
                activations[i] = val

        indices = [t[0] for t in minibatch if t[1] is not None]
        self._update_odds(activations, skip_idxs=indices, corrupt=False)

        if set_val:
            self.activations = activations

        if corrupt:
            return (activations +
                    self.apply_postact_corruption(activations[:],
                                                  xrange(len(activations))))
        else:
            return activations

    def _update_odds(self, activations, skip_idxs=frozenset(), corrupt=True,
                     clamped=None):
        """
        Updates just the odd layers of the network.

        Parameters
        ----------
        activations : list
            List of symbolic tensors representing the current activations.
        skip_idxs : list
            List of integers representing which odd indices should not be
            updated. This parameter exists so that _set_activations can solve
            the tricky problem of initializing the network when both even and
            odd layers are being assigned.
        corrupt : bool, optional
            Whether or not to apply post-activation corruption to the odd
            layers. This parameter does not alter the return value of this
            method but does modify the activations parameter in place.
        clamped : list, optional
            See description for _apply_clamping.
        """
        # Update and corrupt all of the odd layers (which we aren't skipping)
        odds = filter(lambda i: i not in skip_idxs,
                      range(1, len(activations), 2))

        self._update_activations(activations, odds)

        if clamped is not None:
            self._apply_clamping(activations, clamped)

        odds_copy = [(i, activations[i]) for i in xrange(1, len(activations), 2)]

        if corrupt:
            self.apply_postact_corruption(activations, odds)

        return odds_copy

    def _update_evens(self, activations, clamped=None):
        """
        Updates just the even layers of the network.

        Parameters
        ----------
        See all of the descriptions for _update_evens.
        """
        evens = xrange(0, len(activations), 2)

        self._update_activations(activations, evens)
        if clamped is not None:
            self._apply_clamping(activations, clamped)

        evens_copy = [(i, activations[i]) for i in evens]
        self.apply_postact_corruption(activations, evens)

        return evens_copy

    def _update(self, activations, clamped=None, return_activations=False):
        """
        See Figure 1 in "Deep Generative Stochastic Networks as Generative
        Models" by Bengio, Thibodeau-Laufer.
        This and _update_activations implement exactly that, which is essentially
        forward propogating the neural network in both directions.

        Parameters
        ----------
        activations : list of tensors
            List of activations at time step t - 1.
        clamped : list
            See description on _apply_clamping
        return_activations : bool
            If true, then this method returns both the activation values
            after the activation function has been applied and the values
            after the sampling + post-activation corruption has been applied.
            If false, then only return the values after the activation
            function has been applied (no corrupted version).
            This parameter is only set to True when compiling the functions
            needed by get_samples. Regardless of this parameter setting, the
            sampling/post-activation corruption noise is still added in-place
            to activations.

        Returns
        -------
        y : list of tensors
            List of activations at time step t (prior to adding postact noise).

        Notes
        -----
        The return value is generally not equal to the value of activations at
        the the end of this method. The return value contains all layers
        without sampling/post-activation noise, but the activations value
        contains noise on the odd layers (necessary to compute the even
        layers).
        """
        evens_copy = self._update_evens(activations, clamped=clamped)
        odds_copy = self._update_odds(activations, clamped=clamped)

        # precor is before sampling + postactivation corruption (after preactivation
        # corruption and activation)
        precor = [None] * len(self.activations)
        for idx, val in evens_copy + odds_copy:
            assert precor[idx] is None
            precor[idx] = val
        assert None not in precor

        if return_activations:
            return precor + activations
        else:
            return precor

    @staticmethod
    def _apply_clamping(activations, clamped, symbolic=True):
        """
        Resets the value of some layers within the network.

        Parameters
        ----------
        activations : list
            List of symbolic tensors representing the current activations.
        clamped : list of (int, matrix, matrix or None) tuples
            The first component of each tuple is an int representing the
            index of the layer to clamp.
            The second component is a matrix of the initial values for that
            layer (ie what we are resetting the values to).
            The third component is a matrix mask indicated which indices in
            the minibatch to clamp (1 indicates clamping, 0 indicates not).
            The value of None is equivalent to the 0 matrix (so no clamping).
            If symbolic is true then matrices are Theano tensors, otherwise
            they should be numpy matrices.
        symbolic : bool, optional
            Whether to execute with symbolic Theano tensors or numpy matrices.
        """
        for idx, initial, clamp in clamped:
            if clamp is None:
                continue

            # take values from initial
            clamped_val = clamp * initial

            # zero out values in activations
            if symbolic:
                activations[idx] = T.switch(clamp, initial, activations[idx])
            else:
                activations[idx] = np.switch(clamp, initial, activations[idx])
        return activations

    @staticmethod
    def _apply_corruption(activations, corruptors, idx_iter):
        """
        Applies a list of corruptor functions to all layers.

        Parameters
        ----------
        activations : list of tensor_likes
            Generally gsn.activations
        corruptors : list of callables
            Generally gsn.postact_cors or gsn.preact_cors
        idx_iter : iterable
            An iterable of indices into self.activations. The indexes
            indicate which layers the post activation corruptors should be
            applied to.
        """
        assert len(corruptors) == len(activations)
        for i in idx_iter:
            activations[i] = corruptors[i](activations[i])
        return activations

    def apply_sampling(self, activations, idx_iter):
        """
        .. todo::

            WRITEME
        """
        # using _apply_corruption to apply samplers
        if self._sample_switch:
            self._apply_corruption(activations, self._layer_samplers,
                                   idx_iter)
        return activations

    def apply_postact_corruption(self, activations, idx_iter, sample=True):
        """
        .. todo::

            WRITEME
        """
        if sample:
            self.apply_sampling(activations, idx_iter)
        if self._corrupt_switch:
            self._apply_corruption(activations, self._postact_cors,
                                   idx_iter)
        return activations

    def apply_preact_corruption(self, activations, idx_iter):
        """
        .. todo::

            WRITEME
        """
        if self._corrupt_switch:
            self._apply_corruption(activations, self._preact_cors,
                                   idx_iter)
        return activations

    def _update_activations(self, activations, idx_iter):
        """
        Actually computes the activations for all indices in idx_iters.

        This method computes the values for a layer by computing a linear
        combination of the neighboring layers (dictated by the weight matrices),
        applying the pre-activation corruption, and then applying the layer's
        activation function.

        Parameters
        ----------
        activations : list of tensor_likes
            The activations to update (could be self.activations). Updates
            in-place.
        idx_iter : iterable
            An iterable of indices into self.activations. The indexes
            indicate which layers should be updated.
            Must be able to iterate over idx_iter multiple times.
        """
        from_above = lambda i: ((self.aes[i].visbias if self._bias_switch else 0) +
                                T.dot(activations[i + 1],
                                      self.aes[i].w_prime))

        from_below = lambda i: ((self.aes[i - 1].hidbias if self._bias_switch else 0) +
                                T.dot(activations[i - 1],
                                     self.aes[i - 1].weights))

        for i in idx_iter:
            # first compute the hidden activation
            if i == 0:
                activations[i] = from_above(i)
            elif i == len(activations) - 1:
                activations[i] = from_below(i)
            else:
                activations[i] = from_below(i) + from_above(i)

        self.apply_preact_corruption(activations, idx_iter)

        for i in idx_iter:
            # Using the activation function from lower autoencoder
            act_func = None
            if i == 0:
                act_func = self.aes[0].act_dec
            else:
                act_func = self.aes[i - 1].act_enc

            # ACTIVATION
            # None implies linear
            if act_func is not None:
                activations[i] = act_func(activations[i])


class JointGSN(GSN):
    """
    This class only provides a few convenient methods on top of the GSN class
    above. This class should be used when learning the joint distribution between
    2 vectors.
    """
    @classmethod
    def convert(cls, gsn, input_idx=0, label_idx=None):
        """
        'convert' essentially serves as the constructor for JointGSN.

        Parameters
        ----------
        gsn : GSN
        input_idx : int
            The index of the layer which serves as the "input" to the
            network. During classification, this layer will be given.
            Defaults to 0.
        label_idx : int
            The index of the layer which serves as the "output" of the
            network. This label is predicted during classification.
            Defaults to top layer of network.
        """
        gsn = copy.copy(gsn)
        gsn.__class__ = cls
        gsn.input_idx = input_idx
        gsn.label_idx = label_idx or (gsn.nlayers - 1)
        return gsn

    def calc_walkback(self, trials):
        """
        Utility method that calculates how much walkback is needed to get at
        at least 'trials' samples.

        Parameters
        ----------
        trials : WRITEME
        """
        wb = trials - len(self.aes)
        if wb <= 0:
            return 0
        else:
            return wb

    def _get_aggregate_classification(self, minibatch, trials=10, skip=0):
        """
        See classify method.

        Returns the prediction vector aggregated over all time steps where
        axis 0 is the minibatch item and axis 1 is the output for the label.
        """
        clamped = np.ones(minibatch.shape, dtype=np.float32)

        data = self.get_samples([(self.input_idx, minibatch)],
                                walkback=self.calc_walkback(trials + skip),
                                indices=[self.label_idx],
                                clamped=[clamped],
                                symbolic=False)

        # 3d tensor: axis 0 is time step, axis 1 is minibatch item,
        # axis 2 is softmax output for label (after slicing)
        data = np.asarray(data[skip:skip+trials])[:, 0, :, :]

        return data.mean(axis=0)

    def classify(self, minibatch, trials=10, skip=0):
        """
        Classifies a minibatch.

        This method clamps minibatch at self.input_idx and then runs the GSN.
        The first 'skip' predictions are skipped and the next 'trials'
        predictions are averaged and then arg-maxed to make a final prediction.
        The prediction vectors are the activations at self.label_idx.

        Parameters
        ----------
        minibatch : numpy matrix
            WRITEME
        trials : int
            WRITEME
        skip : int
            WRITEME

        Notes
        -----
        A fairly large 3D tensor during classification, so one should watch
        their memory use. The easiest way to limit memory consumption is to
        classify just minibatches rather than the whole test set at once.
        The large tensor is of size (skip + trials) * mb_size * num labels.

        .. warning::

            This method does not directly control whether or not
            corruption and sampling is applied during classification.
            These are decided by self._corrupt_switch and 
            self._sample_switch.
        """
        mean = self._get_aggregate_classification(minibatch, trials=trials,
                                                  skip=skip)
        am = np.argmax(mean, axis=1)

        # convert argmax's to one-hot format
        labels = np.zeros_like(mean)
        labels[np.arange(labels.shape[0]), am] = 1.0

        return labels

    def get_samples_from_labels(self, labels, trials=5):
        """
        Clamps labels and generates samples.

        Parameters
        ----------
        labels : WRITEME
        trials : WRITEME
        """
        clamped = np.ones(labels.shape, dtype=np.float32)
        data = self.get_samples([(self.label_idx, labels)],
                                walkback=self.calc_walkback(trials),
                                indices=[self.input_idx],
                                clamped=[clamped],
                                symbolic=False)

        return np.array(data)[:, 0, :, :]

########NEW FILE########
__FILENAME__ = independent_multiclass_logistic
"""
Multiclass-classification by taking the max over a set of one-against-rest
logistic classifiers.
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

import logging
try:
    from sklearn.linear_model import LogisticRegression
except ImportError:
    LogisticRegression = None
import numpy as np

logger = logging.getLogger(__name__)


class IndependentMulticlassLogistic:
    """
    Fits a separate logistic regression classifier for each class, makes
    predictions based on the max output: during training, views a one-hot label
    vector as a vector of independent binary labels, rather than correctly
    modeling them as one-hot like softmax would do.

    This is what Jia+Huang used to get state of the art on CIFAR-100

    Parameters
    ----------
    C : WRITEME
    """

    def __init__(self, C):
        self.C = C

    def fit(self, X, y):
        """
        Fits the model to the given training data.

        Parameters
        ----------
        X : ndarray
            2D array, each row is one example
        y : ndarray
            vector of integer class labels
        """

        if LogisticRegression is None:
            raise RuntimeError("sklearn not available.")

        min_y = y.min()
        max_y = y.max()

        assert min_y == 0

        num_classes = max_y + 1
        assert num_classes > 1

        logistics = []

        for c in xrange(num_classes):

            logger.info('fitting class {0}'.format(c))
            cur_y = (y == c).astype('int32')

            logistics.append(LogisticRegression(C = self.C).fit(X,cur_y))

        return Classifier(logistics)

class Classifier:
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    logistics : WRITEME
    """
    def __init__(self, logistics):
        assert len(logistics) > 1

        num_classes = len(logistics)
        num_features = logistics[0].coef_.shape[1]

        self.W = np.zeros((num_features, num_classes))
        self.b = np.zeros((num_classes,))

        for i in xrange(num_classes):
            self.W[:,i] = logistics[i].coef_
            self.b[i] = logistics[i].intercept_

    def predict(self, X):
        """
        .. todo::

            WRITEME
        """

        return np.argmax(self.b + np.dot(X,self.W), 1)

########NEW FILE########
__FILENAME__ = kmeans
"""K-means as a postprocessing Block subclass."""

import logging
import numpy
from pylearn2.blocks import Block
from pylearn2.models.model import Model
from pylearn2.space import VectorSpace
from pylearn2.utils import sharedX
from pylearn2.utils.mem import TypicalMemoryError
from pylearn2.utils import wraps
import warnings

try:
    import milk
except:
    milk = None
    warnings.warn(""" Install milk ( http://packages.python.org/milk/ )
                    It has a better k-means implementation. Falling back to
                    our own really slow implementation. """)

logger = logging.getLogger(__name__)


class KMeans(Block, Model):
    """
    Block that outputs a vector of probabilities that a sample belong
    to means computed during training.

    Parameters
    ----------
    k : int
        Number of clusters
    nvis : int
        Dimension of input
    convergence_th : float, optional
        Threshold of distance to clusters under which k-means stops
        iterating.
    max_iter : int, optional
        Maximum number of iterations. Defaults to infinity.
    verbose : bool
        WRITEME
    """

    def __init__(self, k, nvis, convergence_th=1e-6, max_iter=None,
                 verbose=False):
        Block.__init__(self)
        Model.__init__(self)

        self.input_space = VectorSpace(nvis)

        self.k = k
        self.mu = None
        self.convergence_th = convergence_th
        if max_iter:
            if max_iter < 0:
                raise Exception('KMeans init: max_iter should be positive.')
            self.max_iter = max_iter
        else:
            self.max_iter = float('inf')

        self.verbose = verbose

    def train_all(self, dataset, mu=None):
        """
        Process kmeans algorithm on the input to localize clusters.

        Parameters
        ----------
        dataset : WRITEME
        mu : WRITEME

        Returns
        -------
        rval : bool
            WRITEME
        """

        # TODO-- why does this sometimes return X and sometimes return nothing?

        X = dataset.get_design_matrix()

        n, m = X.shape
        k = self.k

        if milk is not None:
            # use the milk implementation of k-means if it's available
            cluster_ids, mu = milk.kmeans(X, k)
        else:
            # our own implementation

            # taking random inputs as initial clusters if user does not provide
            # them.
            if mu is not None:
                if not len(mu) == k:
                    raise Exception("You gave %i clusters"
                                    ", but k=%i were expected"
                                    % (len(mu), k))
            else:
                indices = numpy.random.randint(X.shape[0], size=k)
                mu = X[indices]

            try:
                dists = numpy.zeros((n, k))
            except MemoryError:
                raise TypicalMemoryError("dying trying to allocate dists "
                                         "matrix for {0} examples and {1} "
                                         "means".format(n, k))

            old_kills = {}

            iter = 0
            mmd = prev_mmd = float('inf')
            while True:
                if self.verbose:
                    logger.info('kmeans iter {0}'.format(iter))

                # print 'iter:',iter,' conv crit:',abs(mmd-prev_mmd)
                # if numpy.sum(numpy.isnan(mu)) > 0:
                if numpy.any(numpy.isnan(mu)):
                    logger.info('nan found')
                    return X

                # computing distances
                for i in xrange(k):
                    dists[:, i] = numpy.square((X - mu[i, :])).sum(axis=1)

                if iter > 0:
                    prev_mmd = mmd

                min_dists = dists.min(axis=1)

                # mean minimum distance:
                mmd = min_dists.mean()

                logger.info('cost: {0}'.format(mmd))

                if iter > 0 and (iter >= self.max_iter or
                                 abs(mmd - prev_mmd) < self.convergence_th):
                    # converged
                    break

                # finding minimum distances
                min_dist_inds = dists.argmin(axis=1)

                # computing means
                i = 0
                blacklist = []
                new_kills = {}
                while i < k:
                    b = min_dist_inds == i
                    if not numpy.any(b):
                        killed_on_prev_iter = True
                        # initializes empty cluster to be the mean of the d
                        # data points farthest from their corresponding means
                        if i in old_kills:
                            d = old_kills[i] - 1
                            if d == 0:
                                d = 50
                            new_kills[i] = d
                        else:
                            d = 5
                        mu[i, :] = 0
                        for j in xrange(d):
                            idx = numpy.argmax(min_dists)
                            min_dists[idx] = 0
                            # chose point idx
                            mu[i, :] += X[idx, :]
                            blacklist.append(idx)
                        mu[i, :] /= float(d)
                        # cluster i was empty, reset it to d far out data
                        # points recomputing distances for this cluster
                        dists[:, i] = numpy.square((X - mu[i, :])).sum(axis=1)
                        min_dists = dists.min(axis=1)
                        for idx in blacklist:
                            min_dists[idx] = 0
                        min_dist_inds = dists.argmin(axis=1)
                        # done
                        i += 1
                    else:
                        mu[i, :] = numpy.mean(X[b, :], axis=0)
                        if numpy.any(numpy.isnan(mu)):
                            logger.info('nan found at {0}'.format(i))
                            return X
                        i += 1

                old_kills = new_kills

                iter += 1

        self.mu = sharedX(mu)
        self._params = [self.mu]
        return True

    @wraps(Model.continue_learning)
    def continue_learning(self):
        # One call to train_all currently trains the model fully,
        # so return False immediately.
        return False

    def get_params(self):
        """
        .. todo::

            WRITEME
        """
        # patch older pkls
        if not hasattr(self.mu, 'get_value'):
            self.mu = sharedX(self.mu)
        if not hasattr(self, '_params'):
            self._params = [self.mu]

        return [param for param in self._params]

    def __call__(self, X):
        """
        Compute for each sample its probability to belong to a cluster.

        Parameters
        ----------
        X : numpy.ndarray
            Matrix of sampless of shape (n, d)

        Returns
        -------
        WRITEME
        """
        n, m = X.shape
        k = self.k
        mu = self.mu
        dists = numpy.zeros((n, k))
        for i in xrange(k):
            dists[:, i] = numpy.square((X - mu[i, :])).sum(axis=1)
        return dists / dists.sum(axis=1).reshape(-1, 1)

    def get_weights(self):
        """
        .. todo::

            WRITEME
        """
        return self.mu

    def get_weights_format(self):
        """
        .. todo::

            WRITEME
        """
        return ['h', 'v']

    # Use version defined in Model, rather than Block (which raises
    # NotImplementedError).
    get_input_space = Model.get_input_space
    get_output_space = Model.get_output_space

########NEW FILE########
__FILENAME__ = local_coordinate_coding
"""
.. todo::

    WRITEME
"""
import logging
from theano import function, shared
from pylearn2.optimization import linear_cg as cg
from pylearn2.optimization.feature_sign import feature_sign_search
import numpy as N
import theano.tensor as T
from pylearn2.utils.rng import make_np_rng


logger = logging.getLogger(__name__)


class LocalCoordinateCoding(object):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    nvis : WRITEME
    nhid : WRITEME
    coeff : WRITEME
    """

    def __init__(self, nvis, nhid, coeff):
        self.nvis = nvis
        self.nhid = nhid
        self.coeff = float(coeff)
        self.rng = make_np_rng(None, [1, 2, 3], which_method="randn")

        self.redo_everything()

    def get_output_channels(self):
        """
        .. todo::

            WRITEME
        """
        return self.nhid

    def redo_everything(self):
        """
        .. todo::

            WRITEME
        """
        self.W = shared(self.rng.randn(self.nhid, self.nvis), name='W')
        self.W.T.name = 'W.T'

    def weights_format(self):
        """
        .. todo::

            WRITEME
        """
        return ['h', 'v']

    def optimize_gamma(self, example):
        """
        .. todo::

            WRITEME
        """

        #variable names chosen to follow the arguments to l1ls_featuresign

        Y = N.zeros((self.nvis,))
        Y[:] = example
        c = (1e-10 + N.square(self.W.get_value(borrow=True) -
                              example).sum(axis=1))
        A = self.W.get_value(borrow=True).T / c
        x = feature_sign_search(A, Y, self.coeff)
        g = x / c
        return g

    def train_batch(self, dataset, batch_size):
        """
        .. todo::

            WRITEME
        """
        #TODO-- this results in compilation happening every time learn is
        # called should cache the compilation results, including those
        # inside cg
        X = dataset.get_design_matrix()
        m = X.shape[0]
        assert X.shape[1] == self.nvis

        gamma = N.zeros((batch_size, self.nhid))
        cur_gamma = T.vector(name='cur_gamma')
        cur_v = T.vector(name='cur_v')
        recons = T.dot(cur_gamma, self.W)
        recons.name = 'recons'

        recons_diffs = cur_v - recons
        recons_diffs.name = 'recons_diffs'

        recons_diff_sq = T.sqr(recons_diffs)
        recons_diff_sq.name = 'recons_diff'

        recons_error = T.sum(recons_diff_sq)
        recons_error.name = 'recons_error'

        dict_dists = T.sum(T.sqr(self.W - cur_v), axis=1)
        dict_dists.name = 'dict_dists'

        abs_gamma = abs(cur_gamma)
        abs_gamma.name = 'abs_gamma'

        weighted_dists = T.dot(abs_gamma, dict_dists)
        weighted_dists.name = 'weighted_dists'

        penalty = self.coeff * weighted_dists
        penalty.name = 'penalty'

        #prevent directions of absolute flatness in the hessian
        #W_sq = T.sqr(self.W)
        #W_sq.name = 'W_sq'
        #debug =  T.sum(W_sq)
        debug = 1e-10 * T.sum(dict_dists)
        debug.name = 'debug'

        #J = debug
        J = recons_error + penalty + debug
        J.name = 'J'

        Jf = function([cur_v, cur_gamma], J)

        start = self.rng.randint(m - batch_size + 1)
        batch_X = X[start:start + batch_size, :]

        #TODO-- optimize gamma
        logger.info('optimizing gamma')
        for i in xrange(batch_size):
            #print str(i+1)+'/'+str(batch_size)
            gamma[i, :] = self.optimize_gamma(batch_X[i, :])

        logger.info('max min')
        logger.info(N.abs(gamma).min(axis=0).max())
        logger.info('min max')
        logger.info(N.abs(gamma).max(axis=0).max())

        #Optimize W
        logger.info('optimizing W')
        logger.warning("not tested since switching to Razvan's all-theano "
                       "implementation of linear cg")
        cg.linear_cg(J, [self.W], max_iters=3)

        err = 0.

        for i in xrange(batch_size):
            err += Jf(batch_X[i, :], gamma[i, :])
        assert not N.isnan(err)
        assert not N.isinf(err)
        logger.info('err: {0}'.format(err))
        return True

########NEW FILE########
__FILENAME__ = maxout
"""
MLP Layer objects related to the paper

Maxout Networks. Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron
Courville, and Yoshua Bengio. ICML 2013.

If you use this code in your research, please cite this paper.

The objects in this module are Layer objects for use with
pylearn2.models.mlp.MLP. You need to make an MLP object in
order for these to do anything. For an example of how to build
an MLP with maxout hidden layers, see pylearn2/scripts/papers/maxout.

Note that maxout is designed for use with dropout, so you probably should
use dropout in your MLP when using these layers. If not using dropout, it
is best to use only 2 pieces per unit.

Note to developers / maintainers: when making changes to this module,
ensure that the changes do not break the examples in
pylearn2/scripts/papers/maxout.
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2012-2013, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"

import functools
import logging
import numpy as np
import warnings
from itertools import izip

from theano.compat.python2x import OrderedDict
from theano.sandbox import cuda
from theano import tensor as T

from pylearn2.linear.matrixmul import MatrixMul
from pylearn2.models.mlp import Layer
from pylearn2.models.model import Model
from pylearn2.space import Conv2DSpace
from pylearn2.space import VectorSpace
from pylearn2.utils import py_integer_types
from pylearn2.utils import sharedX

from pylearn2.linear.conv2d_c01b import setup_detector_layer_c01b
from pylearn2.linear import local_c01b
if cuda.cuda_available:
    from pylearn2.sandbox.cuda_convnet.pool import max_pool_c01b
else:
    max_pool_c01b = None
from pylearn2.sandbox.cuda_convnet import check_cuda


logger = logging.getLogger(__name__)


class Maxout(Layer):
    """
    A hidden layer that does max pooling over groups of linear
    units. If you use this code in a research project, please
    cite

    "Maxout Networks" Ian J. Goodfellow, David Warde-Farley,
    Mehdi Mirza, Aaron Courville, and Yoshua Bengio. ICML 2013


    Parameters
    ----------
    layer_name : str
        A name for this layer that will be prepended to monitoring channels
        related to this layer. Each layer in an MLP must have a unique
        name.
    num_units : int
        The number of maxout units to use in this layer.
    num_pieces: int
        The number of linear pieces to use in each maxout unit.
    pool_stride : int, optional
        The distance between the start of each max pooling region. Defaults
        to num_pieces, which makes the pooling regions disjoint. If set to
        a smaller number, can do overlapping pools.
    randomize_pools : bool, optional
        If True, does max pooling over randomized subsets of the linear
        responses, rather than over sequential subsets.
    irange : float, optional
        If specified, initializes each weight randomly in
        U(-irange, irange)
    sparse_init : int, optional
        if specified, irange must not be specified.
        This is an integer specifying how many weights to make non-zero.
        All non-zero weights will be initialized randomly in
        N(0, sparse_stdev^2)
    sparse_stdev : float, optional
        WRITEME
    include_prob : float, optional
        probability of including a weight element in the set
        of weights initialized to U(-irange, irange). If not included
        a weight is initialized to 0. This defaults to 1.
    init_bias : float or ndarray, optional
        A value that can be broadcasted to a numpy vector.
        All biases are initialized to this number.
    W_lr_scale: float, optional
        The learning rate on the weights for this layer is multiplied by
        this scaling factor
    b_lr_scale: float, optional
        The learning rate on the biases for this layer is multiplied by
        this scaling factor
    max_col_norm: float, optional
        The norm of each column of the weight matrix is constrained to
        have at most this norm. If unspecified, no constraint. Constraint
        is enforced by re-projection (if necessary) at the end of each
        update.
    max_row_norm: float, optional
        Like max_col_norm, but applied to the rows.
    mask_weights: ndarray, optional
        A binary matrix multiplied by the weights after each update,
        allowing you to restrict their connectivity.
    min_zero: bool, optional
        If true, includes a zero in the set we take a max over for each
        maxout unit. This is equivalent to pooling over rectified
        linear units.
    """

    def __str__(self):
        """
        Returns
        -------
        rval : str
            A string representation of the object. In this case, just the
            class name.
        """
        return "Maxout"

    def __init__(self,
                 layer_name,
                 num_units,
                 num_pieces,
                 pool_stride=None,
                 randomize_pools=False,
                 irange=None,
                 sparse_init=None,
                 sparse_stdev=1.,
                 include_prob=1.0,
                 init_bias=0.,
                 W_lr_scale=None,
                 b_lr_scale=None,
                 max_col_norm=None,
                 max_row_norm=None,
                 mask_weights=None,
                 min_zero=False):

        super(Maxout, self).__init__()

        detector_layer_dim = num_units * num_pieces
        pool_size = num_pieces

        if pool_stride is None:
            pool_stride = pool_size

        self.__dict__.update(locals())
        del self.self

        self.b = sharedX(np.zeros((self.detector_layer_dim,)) + init_bias,
                         name=(layer_name + '_b'))

        if max_row_norm is not None:
            raise NotImplementedError()

    @functools.wraps(Model.get_lr_scalers)
    def get_lr_scalers(self):

        if not hasattr(self, 'W_lr_scale'):
            self.W_lr_scale = None

        if not hasattr(self, 'b_lr_scale'):
            self.b_lr_scale = None

        rval = OrderedDict()

        if self.W_lr_scale is not None:
            W, = self.transformer.get_params()
            rval[W] = self.W_lr_scale

        if self.b_lr_scale is not None:
            rval[self.b] = self.b_lr_scale

        return rval

    def set_input_space(self, space):
        """
        Tells the layer to use the specified input space.

        This resets parameters! The weight matrix is initialized with the
        size needed to receive input from this space.

        Parameters
        ----------
        space : Space
            The Space that the input will lie in.
        """

        self.input_space = space

        if isinstance(space, VectorSpace):
            self.requires_reformat = False
            self.input_dim = space.dim
        else:
            self.requires_reformat = True
            self.input_dim = space.get_total_dimension()
            self.desired_space = VectorSpace(self.input_dim)

        if not (0 == ((self.detector_layer_dim - self.pool_size) %
                      self.pool_stride)):
            if self.pool_stride == self.pool_size:
                raise ValueError("detector_layer_dim = %d, pool_size = %d. "
                                 "Should be divisible but remainder is %d" %
                                 (self.detector_layer_dim,
                                  self.pool_size,
                                  self.detector_layer_dim % self.pool_size))
            raise ValueError()

        self.h_space = VectorSpace(self.detector_layer_dim)
        self.pool_layer_dim = ((self.detector_layer_dim - self.pool_size) /
                               self.pool_stride + 1)
        self.output_space = VectorSpace(self.pool_layer_dim)

        rng = self.mlp.rng
        if self.irange is not None:
            assert self.sparse_init is None
            W = rng.uniform(-self.irange,
                            self.irange,
                            (self.input_dim, self.detector_layer_dim)) * \
                (rng.uniform(0., 1., (self.input_dim, self.detector_layer_dim))
                 < self.include_prob)
        else:
            assert self.sparse_init is not None
            W = np.zeros((self.input_dim, self.detector_layer_dim))

            def mask_rejects(idx, i):
                if self.mask_weights is None:
                    return False
                return self.mask_weights[idx, i] == 0.

            for i in xrange(self.detector_layer_dim):
                assert self.sparse_init <= self.input_dim
                for j in xrange(self.sparse_init):
                    idx = rng.randint(0, self.input_dim)
                    while W[idx, i] != 0 or mask_rejects(idx, i):
                        idx = rng.randint(0, self.input_dim)
                    W[idx, i] = rng.randn()
            W *= self.sparse_stdev

        W = sharedX(W)
        W.name = self.layer_name + '_W'

        self.transformer = MatrixMul(W)

        W, = self.transformer.get_params()
        assert W.name is not None

        if not hasattr(self, 'randomize_pools'):
            self.randomize_pools = False

        if self.randomize_pools:
            permute = np.zeros((self.detector_layer_dim,
                                self.detector_layer_dim))
            for j in xrange(self.detector_layer_dim):
                i = rng.randint(self.detector_layer_dim)
                permute[i, j] = 1
            self.permute = sharedX(permute)

        if self.mask_weights is not None:
            expected_shape = (self.input_dim, self.detector_layer_dim)
            if expected_shape != self.mask_weights.shape:
                raise ValueError("Expected mask with shape " +
                                 str(expected_shape) +
                                 " but got " +
                                 str(self.mask_weights.shape))
            self.mask = sharedX(self.mask_weights)

    def _modify_updates(self, updates):
        """
        Replaces the values in `updates` if needed to enforce the options set
        in the __init__ method, including `mask_weights` and `max_col_norm`.

        Parameters
        ----------
        updates : OrderedDict
            A dictionary mapping parameters (including parameters not
            belonging to this model) to updated values of those parameters.
            The dictionary passed in contains the updates proposed by the
            learning algorithm. This function modifies the dictionary
            directly. The modified version will be compiled and executed
            by the learning algorithm.
        """

        # Patch old pickle files
        if not hasattr(self, 'mask_weights'):
            self.mask_weights = None

        if self.mask_weights is not None:
            W, = self.transformer.get_params()
            if W in updates:
                updates[W] = updates[W] * self.mask

        if self.max_col_norm is not None:
            assert self.max_row_norm is None
            W, = self.transformer.get_params()
            if W in updates:
                updated_W = updates[W]
                col_norms = T.sqrt(T.sum(T.sqr(updated_W), axis=0))
                desired_norms = T.clip(col_norms, 0, self.max_col_norm)
                updates[W] = updated_W * (desired_norms / (1e-7 + col_norms))

    @functools.wraps(Model.get_params)
    def get_params(self):
        assert self.b.name is not None
        W, = self.transformer.get_params()
        assert W.name is not None
        rval = self.transformer.get_params()
        assert not isinstance(rval, set)
        rval = list(rval)
        assert self.b not in rval
        rval.append(self.b)
        return rval

    @functools.wraps(Layer.get_weight_decay)
    def get_weight_decay(self, coeff):
        if isinstance(coeff, str):
            coeff = float(coeff)
        assert isinstance(coeff, float) or hasattr(coeff, 'dtype')
        W, = self.transformer.get_params()
        return coeff * T.sqr(W).sum()

    @functools.wraps(Layer.get_l1_weight_decay)
    def get_l1_weight_decay(self, coeff):
        if isinstance(coeff, str):
            coeff = float(coeff)
        assert isinstance(coeff, float) or hasattr(coeff, 'dtype')
        W, = self.transformer.get_params()
        return coeff * T.abs(W).sum()

    @functools.wraps(Model.get_weights)
    def get_weights(self):
        if self.requires_reformat:
            # This is not really an unimplemented case.
            # We actually don't know how to format the weights
            # in design space. We got the data in topo space
            # and we don't have access to the dataset
            raise NotImplementedError()
        W, = self.transformer.get_params()
        W = W.get_value()

        if not hasattr(self, 'randomize_pools'):
            self.randomize_pools = False

        if self.randomize_pools:
            warnings.warn("randomize_pools makes get_weights multiply by the "
                          "permutation matrix. If you call set_weights(W) and "
                          "then call get_weights(), the return value will "
                          "WP not W.")
            P = self.permute.get_value()
            return np.dot(W, P)

        return W

    @functools.wraps(Layer.set_weights)
    def set_weights(self, weights):
        W, = self.transformer.get_params()
        W.set_value(weights)

    @functools.wraps(Layer.set_biases)
    def set_biases(self, biases):
        self.b.set_value(biases)

    @functools.wraps(Layer.get_biases)
    def get_biases(self):
        return self.b.get_value()

    @functools.wraps(Model.get_weights_format)
    def get_weights_format(self):
        return ('v', 'h')

    @functools.wraps(Model.get_weights_view_shape)
    def get_weights_view_shape(self):
        total = self.detector_layer_dim
        cols = self.pool_size
        if cols == 1:
            # Let the PatchViewer decide how to arrange the units
            # when they're not pooled
            raise NotImplementedError()
        # When they are pooled, make each pooling unit have one row
        rows = total // cols
        if rows * cols < total:
            rows = rows + 1
        return rows, cols

    @functools.wraps(Model.get_weights_topo)
    def get_weights_topo(self):

        if not isinstance(self.input_space, Conv2DSpace):
            raise NotImplementedError()

        # There was an implementation of this, but it was broken
        raise NotImplementedError()

    @functools.wraps(Layer.get_monitoring_channels)
    def get_monitoring_channels(self):
        warnings.warn("Layer.get_monitoring_channels is " +
                      "deprecated. Use get_layer_monitoring_channels " +
                      "instead. Layer.get_monitoring_channels " +
                      "will be removed on or after september 24th 2014",
                      stacklevel=2)

        W, = self.transformer.get_params()

        assert W.ndim == 2

        sq_W = T.sqr(W)

        row_norms = T.sqrt(sq_W.sum(axis=1))
        col_norms = T.sqrt(sq_W.sum(axis=0))

        row_norms_min = row_norms.min()
        row_norms_min.__doc__ = ("The smallest norm of any row of the "
                                 "weight matrix W. This is a measure of the "
                                 "least influence any visible unit has.")

        return OrderedDict([('row_norms_min',  row_norms_min),
                            ('row_norms_mean', row_norms.mean()),
                            ('row_norms_max',  row_norms.max()),
                            ('col_norms_min',  col_norms.min()),
                            ('col_norms_mean', col_norms.mean()),
                            ('col_norms_max',  col_norms.max()), ])

    @functools.wraps(Layer.get_monitoring_channels_from_state)
    def get_monitoring_channels_from_state(self, state):
        warnings.warn("Layer.get_monitoring_channels_from_state is " +
                      "deprecated. Use get_layer_monitoring_channels " +
                      "instead. Layer.get_monitoring_channels_from_state " +
                      "will be removed on or after september 24th 2014",
                      stacklevel=2)

        P = state

        rval = OrderedDict()

        if self.pool_size == 1:
            vars_and_prefixes = [(P, '')]
        else:
            vars_and_prefixes = [(P, 'p_')]

        for var, prefix in vars_and_prefixes:
            v_max = var.max(axis=0)
            v_min = var.min(axis=0)
            v_mean = var.mean(axis=0)
            v_range = v_max - v_min

            # max_x.mean_u is "the mean over *u*nits of the max over
            # e*x*amples" The x and u are included in the name because
            # otherwise its hard to remember which axis is which when reading
            # the monitor I use inner.outer rather than outer_of_inner or
            # something like that because I want mean_x.* to appear next to
            # each other in the alphabetical list, as these are commonly
            # plotted together
            for key, val in [('max_x.max_u', v_max.max()),
                             ('max_x.mean_u', v_max.mean()),
                             ('max_x.min_u', v_max.min()),
                             ('min_x.max_u', v_min.max()),
                             ('min_x.mean_u', v_min.mean()),
                             ('min_x.min_u', v_min.min()),
                             ('range_x.max_u', v_range.max()),
                             ('range_x.mean_u', v_range.mean()),
                             ('range_x.min_u', v_range.min()),
                             ('mean_x.max_u', v_mean.max()),
                             ('mean_x.mean_u', v_mean.mean()),
                             ('mean_x.min_u', v_mean.min())]:
                rval[prefix+key] = val

        return rval

    @functools.wraps(Layer.get_layer_monitoring_channels)
    def get_layer_monitoring_channels(self, state_below=None,
                                      state=None, targets=None):

        W, = self.transformer.get_params()

        assert W.ndim == 2

        sq_W = T.sqr(W)

        row_norms = T.sqrt(sq_W.sum(axis=1))
        col_norms = T.sqrt(sq_W.sum(axis=0))

        row_norms_min = row_norms.min()
        row_norms_min.__doc__ = ("The smallest norm of any row of the "
                                 "weight matrix W. This is a measure of the "
                                 "least influence any visible unit has.")

        rval = OrderedDict([('row_norms_min',  row_norms_min),
                            ('row_norms_mean', row_norms.mean()),
                            ('row_norms_max',  row_norms.max()),
                            ('col_norms_min',  col_norms.min()),
                            ('col_norms_mean', col_norms.mean()),
                            ('col_norms_max',  col_norms.max()), ])

        if (state is not None) or (state_below is not None):
            if state is None:
                state = self.fprop(state_below)

            P = state
            if self.pool_size == 1:
                vars_and_prefixes = [(P, '')]
            else:
                vars_and_prefixes = [(P, 'p_')]

            for var, prefix in vars_and_prefixes:
                v_max = var.max(axis=0)
                v_min = var.min(axis=0)
                v_mean = var.mean(axis=0)
                v_range = v_max - v_min

                # max_x.mean_u is "the mean over *u*nits of the max over
                # e*x*amples" The x and u are included in the name because
                # otherwise its hard to remember which axis is which when
                # reading the monitor I use inner.outer
                # rather than outer_of_inner or
                # something like that because I want mean_x.* to appear next to
                # each other in the alphabetical list, as these are commonly
                # plotted together
                for key, val in [('max_x.max_u', v_max.max()),
                                 ('max_x.mean_u', v_max.mean()),
                                 ('max_x.min_u', v_max.min()),
                                 ('min_x.max_u', v_min.max()),
                                 ('min_x.mean_u', v_min.mean()),
                                 ('min_x.min_u', v_min.min()),
                                 ('range_x.max_u', v_range.max()),
                                 ('range_x.mean_u', v_range.mean()),
                                 ('range_x.min_u', v_range.min()),
                                 ('mean_x.max_u', v_mean.max()),
                                 ('mean_x.mean_u', v_mean.mean()),
                                 ('mean_x.min_u', v_mean.min())]:
                    rval[prefix+key] = val

        return rval

    @functools.wraps(Layer.fprop)
    def fprop(self, state_below):

        self.input_space.validate(state_below)

        if self.requires_reformat:
            state_below = self.input_space.format_as(state_below,
                                                     self.desired_space)

        z = self.transformer.lmul(state_below) + self.b

        if not hasattr(self, 'randomize_pools'):
            self.randomize_pools = False

        if not hasattr(self, 'pool_stride'):
            self.pool_stride = self.pool_size

        if self.randomize_pools:
            z = T.dot(z, self.permute)

        if not hasattr(self, 'min_zero'):
            self.min_zero = False

        if self.min_zero:
            p = 0.
        else:
            p = None

        last_start = self.detector_layer_dim - self.pool_size
        for i in xrange(self.pool_size):
            cur = z[:, i:last_start+i+1:self.pool_stride]
            if p is None:
                p = cur
            else:
                p = T.maximum(cur, p)

        p.name = self.layer_name + '_p_'

        return p


class MaxoutConvC01B(Layer):
    """
    Maxout units arranged in a convolutional layer, with
    spatial max pooling on top of the maxout. If you use this
    code in a research project, please cite

    "Maxout Networks" Ian J. Goodfellow, David Warde-Farley,
    Mehdi Mirza, Aaron Courville, and Yoshua Bengio. ICML 2013


    This uses the C01B ("channels", topological axis 0,
    topological axis 1, "batch") format of tensors for input
    and output.

    The back-end is Alex Krizhevsky's cuda-convnet library,
    so it is extremely fast, but requires a GPU.

    Parameters
    ----------
    num_channels : int
        The number of output channels the layer should have.
        Note that it must internally compute num_channels * num_pieces
        convolution channels.
    num_pieces : int
        The number of linear pieces used to make each maxout unit.
    kernel_shape : tuple
        The shape of the convolution kernel.
    pool_shape : tuple
        The shape of the spatial max pooling. A two-tuple of ints.
        This is redundant as cuda-convnet requires the pool shape to
        be square.
    pool_stride : tuple
        The stride of the spatial max pooling. Also must be square.
    layer_name : str
        A name for this layer that will be prepended to
        monitoring channels related to this layer.
    irange : float, optional
        if specified, initializes each weight randomly in
        U(-irange, irange)
    init_bias : float, optional
        All biases are initialized to this number
    W_lr_scale : float, optional
        The learning rate on the weights for this layer is
        multiplied by this scaling factor
    b_lr_scale : float, optional
        The learning rate on the biases for this layer is
        multiplied by this scaling factor
    pad : int, optional
        The amount of zero-padding to implicitly add to the boundary of the
        image when computing the convolution. Useful for making sure pixels
        at the edge still get to influence multiple hidden units.
    fix_pool_shape : bool, optional
        If True, will modify self.pool_shape to avoid having
        pool shape bigger than the entire detector layer.
        If you have this on, you should probably also have
        fix_pool_stride on, since the pool shape might shrink
        smaller than the stride, even if the stride was initially
        valid.
        The "fix" parameters are useful for working with a hyperparameter
        optimization package, which might often propose sets of
        hyperparameters that are not feasible, but can easily be projected
        back into the feasible set.
    fix_pool_stride : bool, optional
        WRITEME
    fix_kernel_shape : bool, optional
        if True, will modify self.kernel_shape to avoid having the kernel
        shape bigger than the implicitly zero padded input layer
    partial_sum : int, optional
        a parameter that controls whether to prefer runtime savings
        or memory savings when computing the gradient with respect to
        the kernels. See pylearn2.sandbox.cuda_convnet.weight_acts.py
        for details. The default is to prefer high speed.
        Note that changing this setting may change the value of computed
        results slightly due to different rounding error.
    tied_b : bool, optional
        If true, all biases in the same channel are constrained to be the
        same as each other. Otherwise, each bias at each location is
        learned independently.
    max_kernel_norm : float, optional
        If specified, each kernel is constrained to have at most this norm.
    input_normalization : callable, optional
        see output normalization
    detector_normalization : callable, optional
        see output normalization
    min_zero : bool, optional
        WRITEME
    output_normalization : callable, optional
        if specified, should be a callable object. the state of the
        network is optionally replaced with normalization(state) at each
        of the 3 points in processing:

          - input: the input the layer receives can be normalized right
            away
          - detector: the maxout units can be normalized prior to the
            spatial pooling
          - output: the output of the layer, after sptial pooling,
            can be normalized as well
    kernel_stride : tuple, optional
        vertical and horizontal pixel stride between each detector.
    """

    def __init__(self,
                 num_channels,
                 num_pieces,
                 kernel_shape,
                 pool_shape,
                 pool_stride,
                 layer_name,
                 irange=None,
                 init_bias=0.,
                 W_lr_scale=None,
                 b_lr_scale=None,
                 pad=0,
                 fix_pool_shape=False,
                 fix_pool_stride=False,
                 fix_kernel_shape=False,
                 partial_sum=1,
                 tied_b=False,
                 max_kernel_norm=None,
                 input_normalization=None,
                 detector_normalization=None,
                 min_zero=False,
                 output_normalization=None,
                 kernel_stride=(1, 1)):
        check_cuda(str(type(self)))
        super(MaxoutConvC01B, self).__init__()

        detector_channels = num_channels * num_pieces

        self.__dict__.update(locals())
        del self.self

    @functools.wraps(Model.get_lr_scalers)
    def get_lr_scalers(self):

        if not hasattr(self, 'W_lr_scale'):
            self.W_lr_scale = None

        if not hasattr(self, 'b_lr_scale'):
            self.b_lr_scale = None

        rval = OrderedDict()

        if self.W_lr_scale is not None:
            W, = self.transformer.get_params()
            rval[W] = self.W_lr_scale

        if self.b_lr_scale is not None:
            rval[self.b] = self.b_lr_scale

        return rval

    def set_input_space(self, space):
        """
        Tells the layer to use the specified input space.

        This resets parameters! The kernel tensor is initialized with the
        size needed to receive input from this space.

        Parameters
        ----------
        space : Space
            The Space that the input will lie in.
        """

        setup_detector_layer_c01b(layer=self,
                                  input_space=space,
                                  rng=self.mlp.rng)

        rng = self.mlp.rng

        detector_shape = self.detector_space.shape

        def handle_pool_shape(idx):
            if self.pool_shape[idx] < 1:
                raise ValueError("bad pool shape: " + str(self.pool_shape))
            if self.pool_shape[idx] > detector_shape[idx]:
                if self.fix_pool_shape:
                    assert detector_shape[idx] > 0
                    self.pool_shape[idx] = detector_shape[idx]
                else:
                    raise ValueError("Pool shape exceeds detector layer shape "
                                     "on axis %d" % idx)

        map(handle_pool_shape, [0, 1])

        assert self.pool_shape[0] == self.pool_shape[1]
        assert self.pool_stride[0] == self.pool_stride[1]
        assert all(isinstance(elem, py_integer_types)
                   for elem in self.pool_stride)
        if self.pool_stride[0] > self.pool_shape[0]:
            if self.fix_pool_stride:
                warnings.warn("Fixing the pool stride")
                ps = self.pool_shape[0]
                assert isinstance(ps, py_integer_types)
                self.pool_stride = [ps, ps]
            else:
                raise ValueError("Stride too big.")
        assert all(isinstance(elem, py_integer_types)
                   for elem in self.pool_stride)

        dummy_detector = sharedX(self.detector_space.get_origin_batch(2)[0:16,
                                                                         :,
                                                                         :,
                                                                         :])

        dummy_p = max_pool_c01b(c01b=dummy_detector,
                                pool_shape=self.pool_shape,
                                pool_stride=self.pool_stride,
                                image_shape=self.detector_space.shape)
        dummy_p = dummy_p.eval()
        self.output_space = Conv2DSpace(shape=[dummy_p.shape[1],
                                               dummy_p.shape[2]],
                                        num_channels=self.num_channels,
                                        axes=('c', 0, 1, 'b'))

        logger.info('Output space: {0}'.format(self.output_space.shape))

    def _modify_updates(self, updates):
        """
        Replaces the values in `updates` if needed to enforce the options set
        in the __init__ method, including `max_kernel_norm`.

        Parameters
        ----------
        updates : OrderedDict
            A dictionary mapping parameters (including parameters not
            belonging to this model) to updated values of those parameters.
            The dictionary passed in contains the updates proposed by the
            learning algorithm. This function modifies the dictionary
            directly. The modified version will be compiled and executed
            by the learning algorithm.
        """

        if self.max_kernel_norm is not None:
            W, = self.transformer.get_params()
            if W in updates:
                updated_W = updates[W]
                row_norms = T.sqrt(T.sum(T.sqr(updated_W), axis=(0, 1, 2)))
                desired_norms = T.clip(row_norms, 0, self.max_kernel_norm)
                scales = desired_norms / (1e-7 + row_norms)
                updates[W] = (updated_W * scales.dimshuffle('x', 'x', 'x', 0))

    @functools.wraps(Model.get_params)
    def get_params(self):
        assert self.b.name is not None
        W, = self.transformer.get_params()
        assert W.name is not None
        rval = self.transformer.get_params()
        assert not isinstance(rval, set)
        rval = list(rval)
        assert self.b not in rval
        rval.append(self.b)
        return rval

    @functools.wraps(Layer.get_weight_decay)
    def get_weight_decay(self, coeff):
        if isinstance(coeff, str):
            coeff = float(coeff)
        assert isinstance(coeff, float) or hasattr(coeff, 'dtype')
        W, = self.transformer.get_params()
        return coeff * T.sqr(W).sum()

    @functools.wraps(Layer.set_weights)
    def set_weights(self, weights):
        W, = self.transformer.get_params()
        W.set_value(weights)

    @functools.wraps(Layer.set_biases)
    def set_biases(self, biases):
        self.b.set_value(biases)

    @functools.wraps(Layer.get_biases)
    def get_biases(self):
        return self.b.get_value()

    @functools.wraps(Model.get_weights_topo)
    def get_weights_topo(self):
        return self.transformer.get_weights_topo()

    @functools.wraps(Layer.get_monitoring_channels)
    def get_monitoring_channels(self):

        W, = self.transformer.get_params()

        assert W.ndim == 4

        sq_W = T.sqr(W)

        row_norms = T.sqrt(sq_W.sum(axis=(0, 1, 2)))

        return OrderedDict([('kernel_norms_min',  row_norms.min()),
                            ('kernel_norms_mean', row_norms.mean()),
                            ('kernel_norms_max',  row_norms.max()), ])

    @functools.wraps(Layer.fprop)
    def fprop(self, state_below):
        check_cuda(str(type(self)))

        self.input_space.validate(state_below)

        if not hasattr(self, 'input_normalization'):
            self.input_normalization = None

        if self.input_normalization:
            state_below = self.input_normalization(state_below)

        # Alex's code requires # input channels to be <= 3 or a multiple of 4
        # so we add dummy channels if necessary
        if not hasattr(self, 'dummy_channels'):
            self.dummy_channels = 0
        if self.dummy_channels > 0:
            zeros = T.zeros_like(state_below[0:self.dummy_channels, :, :, :])
            state_below = T.concatenate((state_below, zeros), axis=0)

        z = self.transformer.lmul(state_below)
        if not hasattr(self, 'tied_b'):
            self.tied_b = False
        if self.tied_b:
            b = self.b.dimshuffle(0, 'x', 'x', 'x')
        else:
            b = self.b.dimshuffle(0, 1, 2, 'x')

        z = z + b
        if self.layer_name is not None:
            z.name = self.layer_name + '_z'

        self.detector_space.validate(z)

        assert self.detector_space.num_channels % 16 == 0

        if self.output_space.num_channels % 16 == 0:
            # alex's max pool op only works when the number of channels
            # is divisible by 16. we can only do the cross-channel pooling
            # first if the cross-channel pooling preserves that property
            if self.num_pieces != 1:
                s = None
                for i in xrange(self.num_pieces):
                    t = z[i::self.num_pieces, :, :, :]
                    if s is None:
                        s = t
                    else:
                        s = T.maximum(s, t)
                z = s

            if self.detector_normalization:
                z = self.detector_normalization(z)

            p = max_pool_c01b(c01b=z, pool_shape=self.pool_shape,
                              pool_stride=self.pool_stride,
                              image_shape=self.detector_space.shape)
        else:

            if self.detector_normalization is not None:
                raise NotImplementedError("We can't normalize the detector "
                                          "layer because the detector layer "
                                          "never exists as a stage of "
                                          "processing in this implementation.")
            z = max_pool_c01b(c01b=z, pool_shape=self.pool_shape,
                              pool_stride=self.pool_stride,
                              image_shape=self.detector_space.shape)
            if self.num_pieces != 1:
                s = None
                for i in xrange(self.num_pieces):
                    t = z[i::self.num_pieces, :, :, :]
                    if s is None:
                        s = t
                    else:
                        s = T.maximum(s, t)
                z = s
            p = z

        self.output_space.validate(p)

        if hasattr(self, 'min_zero') and self.min_zero:
            p = p * (p > 0.)

        if not hasattr(self, 'output_normalization'):
            self.output_normalization = None

        if self.output_normalization:
            p = self.output_normalization(p)

        return p

    @functools.wraps(Model.get_weights_view_shape)
    def get_weights_view_shape(self):
        total = self.detector_channels
        cols = self.num_pieces
        if cols == 1:
            # Let the PatchViewer decide how to arrange the units
            # when they're not pooled
            raise NotImplementedError()
        # When they are pooled, make each pooling unit have one row
        rows = total // cols
        if rows * cols < total:
            rows = rows + 1
        return rows, cols

    @functools.wraps(Layer.get_monitoring_channels_from_state)
    def get_monitoring_channels_from_state(self, state):

        P = state

        rval = OrderedDict()

        vars_and_prefixes = [(P, '')]

        for var, prefix in vars_and_prefixes:
            assert var.ndim == 4
            v_max = var.max(axis=(1, 2, 3))
            v_min = var.min(axis=(1, 2, 3))
            v_mean = var.mean(axis=(1, 2, 3))
            v_range = v_max - v_min

            # max_x.mean_u is "the mean over *u*nits of the max over
            # e*x*amples" The x and u are included in the name because
            # otherwise its hard to remember which axis is which when reading
            # the monitor I use inner.outer rather than outer_of_inner or
            # something like that because I want mean_x.* to appear next to
            # each other in the alphabetical list, as these are commonly
            # plotted together
            for key, val in [('max_x.max_u',    v_max.max()),
                             ('max_x.mean_u',   v_max.mean()),
                             ('max_x.min_u',    v_max.min()),
                             ('min_x.max_u',    v_min.max()),
                             ('min_x.mean_u',   v_min.mean()),
                             ('min_x.min_u',    v_min.min()),
                             ('range_x.max_u',  v_range.max()),
                             ('range_x.mean_u', v_range.mean()),
                             ('range_x.min_u',  v_range.min()),
                             ('mean_x.max_u',   v_mean.max()),
                             ('mean_x.mean_u',  v_mean.mean()),
                             ('mean_x.min_u',   v_mean.min())]:
                rval[prefix+key] = val

        return rval


class MaxoutLocalC01B(Layer):
    """
    Maxout units arranged in a convolutional layer, with
    spatial max pooling on top of the maxout. If you use this
    code in a research project, please cite

    "Maxout Networks" Ian J. Goodfellow, David Warde-Farley,
    Mehdi Mirza, Aaron Courville, and Yoshua Bengio. ICML 2013

    This uses the C01B ("channels", topological axis 0,
    topological axis 1, "batch") format of tensors for input
    and output.

    Unlike MaxoutConvC01B, this class supports operation on CPU,
    thanks to James Bergstra's TheanoLinear library, which
    pylearn2 has forked. The GPU code is still based on Alex
    Krizvhevsky's cuda_convnet library.

    Parameters
    ----------
    num_channels : int
        The number of output channels the layer should have.
        Note that it must internally compute num_channels * num_pieces
        convolution channels.
    num_pieces : int
        The number of linear pieces used to make each maxout unit.
    kernel_shape : tuple
        The shape of the convolution kernel.
    layer_name : str
        A name for this layer that will be prepended to
        monitoring channels related to this layer.
    pool_shape : tuple, optional
        The shape of the spatial max pooling. A two-tuple of ints.
        This is redundant as cuda-convnet requires the pool shape to
        be square.
        Defaults to None, which means no spatial pooling
    pool_stride : tuple, optional
        The stride of the spatial max pooling. Also must be square.
        Defaults to None, which means no spatial pooling.
    irange : float, optional
        if specified, initializes each weight randomly in
        U(-irange, irange)
    init_bias : float, optional
        All biases are initialized to this number
    W_lr_scale : float, optional
        The learning rate on the weights for this layer is
        multiplied by this scaling factor
    b_lr_scale : float, optional
        The learning rate on the biases for this layer is
        multiplied by this scaling factor
    pad : int, optional
        The amount of zero-padding to implicitly add to the boundary of the
        image when computing the convolution. Useful for making sure pixels
        at the edge still get to influence multiple hidden units.
    fix_pool_shape : bool, optional
        If True, will modify self.pool_shape to avoid having
        pool shape bigger than the entire detector layer.
        If you have this on, you should probably also have
        fix_pool_stride on, since the pool shape might shrink
        smaller than the stride, even if the stride was initially
        valid.
        The "fix" parameters are useful for working with a hyperparameter
        optimization package, which might often propose sets of
        hyperparameters that are not feasible, but can easily be projected
        back into the feasible set.
    fix_pool_stride : bool, optional
        WRITEME
    fix_kernel_shape : bool, optional
        if True, will modify self.kernel_shape to avoid
        having the kernel shape bigger than the implicitly
        zero padded input layer
    partial_sum : int, optional
        a parameter that controls whether to prefer runtime savings
        or memory savings when computing the gradient with respect to
        the kernels. See pylearn2.sandbox.cuda_convnet.weight_acts.py
        for details. The default is to prefer high speed.
        Note that changing this setting may change the value of computed
        results slightly due to different rounding error.
    tied_b : bool, optional
        If true, all biases in the same channel are constrained to be the
        same as each other. Otherwise, each bias at each location is
        learned independently.
    max_filter_norm : float, optional
        DEPRECATED, use max_kernel_norm instead.
    max_kernel_norm : float, optional
        If specified, each kernel is constrained to have at most this norm.
    input_normalization : callable
        see output_normalization
    detector_normalization : callable
        see output_normalization
    min_zero : bool, optional
        WRITEME
    output_normalization : callable
        if specified, should be a callable object. the state of the network
        is optionally replaced with normalization(state) at each of the 3
        points in processing:

          - input: the input the layer receives can be normalized right
            away
          - detector: the maxout units can be normalized prior to the
            spatial pooling
          - output: the output of the layer, after sptial pooling, can be
            normalized as well
    kernel_stride : tuple, optional
        Vertical and horizontal pixel stride between each detector.
    input_groups : int, optional
        WRITEME
    """

    def __init__(self,
                 num_channels,
                 num_pieces,
                 kernel_shape,
                 layer_name,
                 pool_shape=None,
                 pool_stride=None,
                 irange=None,
                 init_bias=0.,
                 W_lr_scale=None,
                 b_lr_scale=None,
                 pad=0,
                 fix_pool_shape=False,
                 fix_pool_stride=False,
                 fix_kernel_shape=False,
                 partial_sum=1,
                 tied_b=False,
                 max_filter_norm=None,
                 max_kernel_norm=None,
                 input_normalization=None,
                 detector_normalization=None,
                 min_zero=False,
                 output_normalization=None,
                 input_groups=1,
                 kernel_stride=(1, 1)):

        if max_filter_norm is not None:
            max_kernel_norm = max_filter_norm
            warnings.warn("max_filter_norm argument is deprecated, use "
                          "max_kernel_norm instead. max_filter_norm "
                          "will be removed on or after 2014-10-02.",
                          stacklevel=2)

        assert (pool_shape is None) == (pool_stride is None)

        detector_channels = num_channels * num_pieces

        self.__dict__.update(locals())
        del self.self

    @functools.wraps(Model.get_lr_scalers)
    def get_lr_scalers(self):

        if not hasattr(self, 'W_lr_scale'):
            self.W_lr_scale = None

        if not hasattr(self, 'b_lr_scale'):
            self.b_lr_scale = None

        rval = OrderedDict()

        if self.W_lr_scale is not None:
            W, = self.transformer.get_params()
            rval[W] = self.W_lr_scale

        if self.b_lr_scale is not None:
            rval[self.b] = self.b_lr_scale

        return rval

    def set_input_space(self, space):
        """
        Tells the layer to use the specified input space.

        This resets parameters! The weight tensor is initialized with the
        size needed to receive input from this space.

        Parameters
        ----------
        space : Space
            The Space that the input will lie in.
        """

        self.input_space = space

        if not isinstance(self.input_space, Conv2DSpace):
            raise TypeError("The input to a convolutional layer should be a "
                            "Conv2DSpace,  but layer " + self.layer_name +
                            " got " + str(type(self.input_space)))
        # note: I think the desired space thing is actually redundant,
        # since LinearTransform will also dimshuffle the axes if needed
        # It's not hurting anything to have it here but we could reduce
        # code complexity by removing it
        self.desired_space = Conv2DSpace(shape=space.shape,
                                         channels=space.num_channels,
                                         axes=('c', 0, 1, 'b'))

        ch = self.desired_space.num_channels
        rem = ch % 4
        if ch > 3 and rem != 0:
            self.dummy_channels = 4 - rem
        else:
            self.dummy_channels = 0
        self.dummy_space = Conv2DSpace(shape=space.shape,
                                       channels=(space.num_channels +
                                                 self.dummy_channels),
                                       axes=('c', 0, 1, 'b'))

        rng = self.mlp.rng

        output_shape = \
            [int(np.ceil((i_sh + 2. * self.pad - k_sh) / float(k_st))) + 1
             for i_sh, k_sh, k_st in izip(self.input_space.shape,
                                          self.kernel_shape,
                                          self.kernel_stride)]

        def handle_kernel_shape(idx):
            if self.kernel_shape[idx] < 1:
                raise ValueError("kernel must have strictly positive size on "
                                 "all axes but has shape: " +
                                 str(self.kernel_shape))
            if output_shape[idx] <= 0:
                if self.fix_kernel_shape:
                    self.kernel_shape[idx] = (self.input_space.shape[idx] +
                                              2 * self.pad)
                    assert self.kernel_shape[idx] != 0
                    output_shape[idx] = 1
                    warnings.warn("Had to change the kernel shape to make "
                                  "network feasible")
                else:
                    raise ValueError("kernel too big for input (even with "
                                     "zero padding)")

        map(handle_kernel_shape, [0, 1])

        self.detector_space = Conv2DSpace(shape=output_shape,
                                          num_channels=self.detector_channels,
                                          axes=('c', 0, 1, 'b'))

        if self.pool_shape is not None:
            def handle_pool_shape(idx):
                if self.pool_shape[idx] < 1:
                    raise ValueError("bad pool shape: " + str(self.pool_shape))
                if self.pool_shape[idx] > output_shape[idx]:
                    if self.fix_pool_shape:
                        assert output_shape[idx] > 0
                        self.pool_shape[idx] = output_shape[idx]
                    else:
                        raise ValueError("Pool shape exceeds detector layer "
                                         "shape on axis %d" % idx)

            map(handle_pool_shape, [0, 1])

            assert self.pool_shape[0] == self.pool_shape[1]
            assert self.pool_stride[0] == self.pool_stride[1]
            assert all(isinstance(elem, py_integer_types)
                       for elem in self.pool_stride)
            if self.pool_stride[0] > self.pool_shape[0]:
                if self.fix_pool_stride:
                    warnings.warn("Fixing the pool stride")
                    ps = self.pool_shape[0]
                    assert isinstance(ps, py_integer_types)
                    self.pool_stride = [ps, ps]
                else:
                    raise ValueError("Stride too big.")
            assert all(isinstance(elem, py_integer_types)
                       for elem in self.pool_stride)

        if self.irange is not None:
            self.transformer = local_c01b.make_random_local(
                input_groups=self.input_groups,
                irange=self.irange,
                input_axes=self.desired_space.axes,
                image_shape=self.desired_space.shape,
                output_axes=self.detector_space.axes,
                input_channels=self.dummy_space.num_channels,
                output_channels=self.detector_space.num_channels,
                kernel_shape=self.kernel_shape,
                kernel_stride=self.kernel_stride,
                pad=self.pad,
                partial_sum=self.partial_sum,
                rng=rng)
        W, = self.transformer.get_params()
        W.name = 'W'

        if self.tied_b:
            self.b = sharedX(np.zeros((self.detector_space.num_channels)) +
                             self.init_bias)
        else:
            self.b = sharedX(self.detector_space.get_origin() + self.init_bias)
        self.b.name = 'b'

        logger.info('Input shape: {0}'.format(self.input_space.shape))
        logger.info('Detector space: {0}'.format(self.detector_space.shape))

        assert self.detector_space.num_channels >= 16

        if self.pool_shape is None or np.prod(self.pool_shape) == 1:
            self.output_space = Conv2DSpace(shape=self.detector_space.shape,
                                            num_channels=self.num_channels,
                                            axes=('c', 0, 1, 'b'))
        elif max_pool_c01b is not None:
            ds = self.detector_space
            dummy_detector = sharedX(ds.get_origin_batch(2)[0:16, :, :, :])

            dummy_p = max_pool_c01b(c01b=dummy_detector,
                                    pool_shape=self.pool_shape,
                                    pool_stride=self.pool_stride,
                                    image_shape=self.detector_space.shape)
            dummy_p = dummy_p.eval()
            self.output_space = Conv2DSpace(shape=[dummy_p.shape[1],
                                                   dummy_p.shape[2]],
                                            num_channels=self.num_channels,
                                            axes=('c', 0, 1, 'b'))
        else:
            raise NotImplementedError("Pooling is not implemented for CPU")

        logger.info('Output space: {0}'.format(self.output_space.shape))

    def _modify_updates(self, updates):
        """
        Replaces the values in `updates` if needed to enforce the options set
        in the __init__ method, including `max_kernel_norm`.

        Parameters
        ----------
        updates : OrderedDict
            A dictionary mapping parameters (including parameters not
            belonging to this model) to updated values of those parameters.
            The dictionary passed in contains the updates proposed by the
            learning algorithm. This function modifies the dictionary
            directly. The modified version will be compiled and executed
            by the learning algorithm.
        """

        if self.max_kernel_norm is not None:
            W, = self.transformer.get_params()
            if W in updates:
                # TODO:    push some of this into the transformer itself
                updated_W = updates[W]
                updated_norms = self.get_filter_norms(updated_W)
                desired_norms = T.clip(updated_norms, 0, self.max_kernel_norm)
                scales = desired_norms / (1e-7 + updated_norms)
                updates[W] = (updated_W *
                              scales.dimshuffle(0, 1, 'x', 'x', 'x', 2, 3))

    @functools.wraps(Model.get_params)
    def get_params(self):
        assert self.b.name is not None
        W, = self.transformer.get_params()
        assert W.name is not None
        rval = self.transformer.get_params()
        assert not isinstance(rval, set)
        rval = list(rval)
        assert self.b not in rval
        rval.append(self.b)
        return rval

    @functools.wraps(Layer.get_weight_decay)
    def get_weight_decay(self, coeff):
        if isinstance(coeff, str):
            coeff = float(coeff)
        assert isinstance(coeff, float) or hasattr(coeff, 'dtype')
        W, = self.transformer.get_params()
        return coeff * T.sqr(W).sum()

    @functools.wraps(Layer.set_weights)
    def set_weights(self, weights):
        W, = self.transformer.get_params()
        W.set_value(weights)

    @functools.wraps(Layer.set_biases)
    def set_biases(self, biases):
        self.b.set_value(biases)

    @functools.wraps(Layer.get_biases)
    def get_biases(self):
        return self.b.get_value()

    @functools.wraps(Layer.get_weights_topo)
    def get_weights_topo(self):
        return self.transformer.get_weights_topo()

    def get_filter_norms(self, W=None):
        """
        Returns
        -------
        norms : theano 4 tensor
            A theano expression for the norms of the different filters in
            the layer.
            TODO: explain significance of each of the 4 axes, and what
            order they'll be in.
        """

        # TODO: push this into the transformer class itself

        if W is None:
            W, = self.transformer.get_params()

        assert W.ndim == 7

        sq_W = T.sqr(W)

        norms = T.sqrt(sq_W.sum(axis=(2, 3, 4)))

        return norms

    @functools.wraps(Layer.get_monitoring_channels)
    def get_monitoring_channels(self):

        filter_norms = self.get_filter_norms()

        return OrderedDict([('filter_norms_min',  filter_norms.min()),
                            ('filter_norms_mean', filter_norms.mean()),
                            ('filter_norms_max',  filter_norms.max()), ])

    @functools.wraps(Layer.fprop)
    def fprop(self, state_below):

        self.input_space.validate(state_below)

        state_below = self.input_space.format_as(state_below,
                                                 self.desired_space)

        if not hasattr(self, 'input_normalization'):
            self.input_normalization = None

        if self.input_normalization:
            state_below = self.input_normalization(state_below)

        # Alex's code requires # input channels to be <= 3 or a multiple of 4
        # so we add dummy channels if necessary
        if not hasattr(self, 'dummy_channels'):
            self.dummy_channels = 0
        if self.dummy_channels > 0:
            zeros = T.zeros_like(state_below[0:self.dummy_channels, :, :, :])
            state_below = T.concatenate((state_below, zeros), axis=0)

        z = self.transformer.lmul(state_below)
        if not hasattr(self, 'tied_b'):
            self.tied_b = False
        if self.tied_b:
            b = self.b.dimshuffle(0, 'x', 'x', 'x')
        else:
            b = self.b.dimshuffle(0, 1, 2, 'x')

        z = z + b
        if self.layer_name is not None:
            z.name = self.layer_name + '_z'

        self.detector_space.validate(z)

        assert self.detector_space.num_channels % 16 == 0

        if self.output_space.num_channels % 16 == 0:
            # alex's max pool op only works when the number of channels
            # is divisible by 16. we can only do the cross-channel pooling
            # first if the cross-channel pooling preserves that property
            if self.num_pieces != 1:
                s = None
                for i in xrange(self.num_pieces):
                    t = z[i::self.num_pieces, :, :, :]
                    if s is None:
                        s = t
                    else:
                        s = T.maximum(s, t)
                z = s

            if self.detector_normalization:
                z = self.detector_normalization(z)

            if self.pool_shape is None or np.prod(self.pool_shape) == 1:
                p = z
            else:
                p = max_pool_c01b(c01b=z,
                                  pool_shape=self.pool_shape,
                                  pool_stride=self.pool_stride,
                                  image_shape=self.detector_space.shape)
        else:

            if self.detector_normalization is not None:
                raise NotImplementedError("We can't normalize the detector "
                                          "layer because the detector layer "
                                          "never exists as a stage of "
                                          "processing in this "
                                          "implementation.")
            if self.pool_shape is not None or np.prod(self.pool_shape) > 1:
                z = max_pool_c01b(c01b=z,
                                  pool_shape=self.pool_shape,
                                  pool_stride=self.pool_stride,
                                  image_shape=self.detector_space.shape)
            if self.num_pieces != 1:
                s = None
                for i in xrange(self.num_pieces):
                    t = z[i::self.num_pieces, :, :, :]
                    if s is None:
                        s = t
                    else:
                        s = T.maximum(s, t)
                z = s
            p = z

        self.output_space.validate(p)

        if hasattr(self, 'min_zero') and self.min_zero:
            p = p * (p > 0.)

        if not hasattr(self, 'output_normalization'):
            self.output_normalization = None

        if self.output_normalization:
            p = self.output_normalization(p)

        return p

    @functools.wraps(Model.get_weights_view_shape)
    def get_weights_view_shape(self):
        total = self.detector_channels
        cols = self.num_pieces
        if cols == 1:
            # Let the PatchViewer decide how to arrange the units
            # when they're not pooled
            raise NotImplementedError()
        # When they are pooled, make each pooling unit have one row
        rows = total // cols
        if rows * cols < total:
            rows = rows + 1
        return rows, cols

    @functools.wraps(Layer.get_monitoring_channels_from_state)
    def get_monitoring_channels_from_state(self, state):

        P = state

        rval = OrderedDict()

        vars_and_prefixes = [(P, '')]

        for var, prefix in vars_and_prefixes:
            assert var.ndim == 4
            v_max = var.max(axis=(1, 2, 3))
            v_min = var.min(axis=(1, 2, 3))
            v_mean = var.mean(axis=(1, 2, 3))
            v_range = v_max - v_min

            # max_x.mean_u is "the mean over *u*nits of the max over
            # e*x*amples" The x and u are included in the name because
            # otherwise its hard to remember which axis is which when reading
            # the monitor I use inner.outer rather than outer_of_inner or
            # something like that because I want mean_x.* to appear next to
            # each other in the alphabetical list, as these are commonly
            # plotted together
            for key, val in [('max_x.max_u',    v_max.max()),
                             ('max_x.mean_u',   v_max.mean()),
                             ('max_x.min_u',    v_max.min()),
                             ('min_x.max_u',    v_min.max()),
                             ('min_x.mean_u',   v_min.mean()),
                             ('min_x.min_u',    v_min.min()),
                             ('range_x.max_u',  v_range.max()),
                             ('range_x.mean_u', v_range.mean()),
                             ('range_x.min_u',  v_range.min()),
                             ('mean_x.max_u',   v_mean.max()),
                             ('mean_x.mean_u',  v_mean.mean()),
                             ('mean_x.min_u',   v_mean.min())]:
                rval[prefix+key] = val

        return rval

########NEW FILE########
__FILENAME__ = mlp
"""
Multilayer Perceptron
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2012-2013, Universite de Montreal"
__credits__ = ["Ian Goodfellow", "David Warde-Farley"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"

import logging
import math
import sys
import warnings

import numpy as np
from theano import config
from theano.compat.python2x import OrderedDict
from theano.gof.op import get_debug_values
from theano.printing import Print
from theano.sandbox.rng_mrg import MRG_RandomStreams
import theano.tensor as T

from pylearn2.costs.mlp import Default
from pylearn2.expr.probabilistic_max_pooling import max_pool_channels
from pylearn2.linear import conv2d
from pylearn2.linear.matrixmul import MatrixMul
from pylearn2.models.model import Model
from pylearn2.monitor import get_monitor_doc
from pylearn2.expr.nnet import pseudoinverse_softmax_numpy
from pylearn2.space import CompositeSpace
from pylearn2.space import Conv2DSpace
from pylearn2.space import Space
from pylearn2.space import VectorSpace
from pylearn2.utils import function
from pylearn2.utils import is_iterable
from pylearn2.utils import py_float_types
from pylearn2.utils import py_integer_types
from pylearn2.utils import safe_union
from pylearn2.utils import safe_zip
from pylearn2.utils import safe_izip
from pylearn2.utils import sharedX
from pylearn2.utils import wraps

from pylearn2.expr.nnet import (kl, compute_precision,
                                    compute_recall, compute_f1)

# Only to be used by the deprecation warning wrapper functions
from pylearn2.costs.mlp import L1WeightDecay as _L1WD
from pylearn2.costs.mlp import WeightDecay as _WD


logger = logging.getLogger(__name__)

logger.debug("MLP changing the recursion limit.")
# We need this to be high enough that the big theano graphs we make
# when doing max pooling via subtensors don't cause python to complain.
# python intentionally declares stack overflow well before the stack
# segment is actually exceeded. But we can't make this value too big
# either, or we'll get seg faults when the python interpreter really
# does go over the stack segment.
# IG encountered seg faults on eos3 (a machine at LISA labo) when using
# 50000 so for now it is set to 40000.
# I think the actual safe recursion limit can't be predicted in advance
# because you don't know how big of a stack frame each function will
# make, so there is not really a "correct" way to do this. Really the
# python interpreter should provide an option to raise the error
# precisely when you're going to exceed the stack segment.
sys.setrecursionlimit(40000)


class Layer(Model):
    """
    Abstract class. A Layer of an MLP.

    May only belong to one MLP.

    Parameters
    ----------
    kwargs : dict
        Passed on to the superclass.

    Notes
    -----
    This is not currently a Block because as far as I know the Block interface
    assumes every input is a single matrix. It doesn't support using Spaces to
    work with composite inputs, stacked multichannel image inputs, etc. If the
    Block interface were upgraded to be that flexible, then we could make this
    a block.
    """


    # When applying dropout to a layer's input, use this for masked values.
    # Usually this will be 0, but certain kinds of layers may want to override
    # this behaviour.
    dropout_input_mask_value = 0.

    def get_mlp(self):
        """
        Returns the MLP that this layer belongs to.

        Returns
        -------
        mlp : MLP
            The MLP that this layer belongs to, or None if it has not been
            assigned to an MLP yet.
        """

        if hasattr(self, 'mlp'):
            return self.mlp

        return None

    def set_mlp(self, mlp):
        """
        Assigns this layer to an MLP. This layer will then use the MLP's
        random number generator, batch size, etc. This layer's name must
        be unique within the MLP.

        Parameters
        ----------
        mlp : MLP
        """
        assert self.get_mlp() is None
        self.mlp = mlp

    def get_monitoring_channels_from_state(self, state, target=None):
        """
        Returns monitoring channels based on the values computed by
        `fprop`.

        Parameters
        ----------
        state : member of self.output_space
            A minibatch of states that this Layer took on during fprop.
            Provided externally so that we don't need to make a second
            expression for it. This helps keep the Theano graph smaller
            so that function compilation runs faster.
        target : member of self.output_space
            Should be None unless this is the last layer.
            If specified, it should be a minibatch of targets for the
            last layer.

        Returns
        -------
        channels : OrderedDict
            A dictionary mapping channel names to monitoring channels of
            interest for this layer.
        """
        warnings.warn("Layer.get_monitoring_channels is " + \
                    "deprecated. Use get_layer_monitoring_channels " + \
                    "instead. Layer.get_monitoring_channels " + \
                    "will be removed on or after september 24th 2014",
                    stacklevel=2)

        return OrderedDict()

    def get_layer_monitoring_channels(self, state_below=None,
                                    state=None, targets=None):
        """
        Returns monitoring channels.

        Parameters
        ----------
        state_below : member of self.input_space
            A minibatch of states that this Layer took as input.
            Most of the time providing state_blow is unnecessary when
            state is given.
        state : member of self.output_space
            A minibatch of states that this Layer took on during fprop.
            Provided externally so that we don't need to make a second
            expression for it. This helps keep the Theano graph smaller
            so that function compilation runs faster.
        targets : member of self.output_space
            Should be None unless this is the last layer.
            If specified, it should be a minibatch of targets for the
            last layer.

        Returns
        -------
        channels : OrderedDict
            A dictionary mapping channel names to monitoring channels of
            interest for this layer.
        """

        return OrderedDict()

    def fprop(self, state_below):
        """
        Does the forward prop transformation for this layer.

        Parameters
        ----------
        state_below : member of self.input_space
            A minibatch of states of the layer below.

        Returns
        -------
        state : member of self.output_space
            A minibatch of states of this layer.
        """

        raise NotImplementedError(str(type(self))+" does not implement fprop.")

    def cost(self, Y, Y_hat):
        """
        The cost of outputting Y_hat when the true output is Y.

        Parameters
        ----------
        Y : theano.gof.Variable
            The targets
        Y_hat : theano.gof.Variable
            The predictions.
            Assumed to be the output of the layer's `fprop` method.
            The implmentation is permitted to do things like look at the
            ancestors of `Y_hat` in the theano graph. This is useful for
            e.g. computing numerically stable *log* probabilities when
            `Y_hat` is the *probability*.

        Returns
        -------
        cost : theano.gof.Variable
            A Theano scalar describing the cost.
        """

        raise NotImplementedError(str(type(self)) +
                                  " does not implement mlp.Layer.cost.")

    def cost_from_cost_matrix(self, cost_matrix):
        """
        The cost final scalar cost computed from the cost matrix

        Parameters
        ----------
        cost_matrix : WRITEME

        Examples
        --------
        >>> # C = model.cost_matrix(Y, Y_hat)
        >>> # Do something with C like setting some values to 0
        >>> # cost = model.cost_from_cost_matrix(C)
        """

        raise NotImplementedError(str(type(self)) +
                                  " does not implement "
                                  "mlp.Layer.cost_from_cost_matrix.")

    def cost_matrix(self, Y, Y_hat):
        """
        The element wise cost of outputting Y_hat when the true output is Y.

        Parameters
        ----------
        Y : WRITEME
        Y_hat : WRITEME

        Returns
        -------
        WRITEME
        """
        raise NotImplementedError(str(type(self)) +
                                  " does not implement mlp.Layer.cost_matrix")

    def get_weights(self):
        """
        .. todo::

            WRITEME
        """
        raise NotImplementedError

    def set_weights(self, weights):
        """
        Sets the weights of the layer.

        Parameters
        ----------
        weights : ndarray
            A numpy ndarray containing the desired weights of the layer. This
            docstring is provided by the Layer base class. Layer subclasses
            should add their own docstring explaining the subclass-specific
            format of the ndarray.
        """
        raise NotImplementedError(str(type(self)) + " does not implement "
                "set_weights.")

    def get_biases(self):
        """
        Returns the value of the biases of the layer.

        Returns
        -------
        biases : ndarray
            A numpy ndarray containing the biases of the layer. This docstring
            is provided by the Layer base class. Layer subclasses should add
            their own docstring explaining the subclass-specific format of the
            ndarray.
        """
        raise NotImplementedError(str(type(self)) + " does not implement "
                "get_biases (perhaps because the class has no biases).")

    def set_biases(self, biases):
        """
        Sets the biases of the layer.

        Parameters
        ----------
        biases : ndarray
            A numpy ndarray containing the desired biases of the layer. This
            docstring is provided by the Layer base class. Layer subclasses
            should add their own docstring explaining the subclass-specific
            format of the ndarray.
        """
        raise NotImplementedError(str(type(self)) + " does not implement "
                "set_biases (perhaps because the class has no biases).")

    def get_weights_format(self):
        """
        .. todo::

            WRITEME
        """
        raise NotImplementedError

    def get_weight_decay(self, coeff):
        """
        Provides an expresion for a squared L2 penalty on the weights.

        Parameters
        ----------
        coeff : float or tuple
            The coefficient on the weight decay penalty for this layer.
            This docstring is provided by the Layer base class. Individual
            Layer subclasses should add their own docstring explaining the
            format of `coeff` for that particular layer. For most ordinary
            layers, `coeff` is a single float to multiply by the weight
            decay term. Layers containing many pieces may take a tuple or
            nested tuple of floats, and should explain the semantics of
            the different elements of the tuple.

        Returns
        -------
        weight_decay : theano.gof.Variable
            An expression for the weight decay penalty term for this
            layer.
        """
        raise NotImplementedError(str(type(self)) + " does not implement "
                "get_weight_decay.")

    def get_l1_weight_decay(self, coeff):
        """
        Provides an expresion for an L1 penalty on the weights.

        Parameters
        ----------
        coeff : float or tuple
            The coefficient on the L1 weight decay penalty for this layer.
            This docstring is provided by the Layer base class. Individual
            Layer subclasses should add their own docstring explaining the
            format of `coeff` for that particular layer. For most ordinary
            layers, `coeff` is a single float to multiply by the weight
            decay term. Layers containing many pieces may take a tuple or
            nested tuple of floats, and should explain the semantics of
            the different elements of the tuple.

        Returns
        -------
        weight_decay : theano.gof.Variable
            An expression for the L1 weight decay penalty term for this
            layer.
        """
        raise NotImplementedError(str(type(self)) + " does not implement "
                "get_l1_weight_decay.")

    def set_input_space(self, space):
        """
        Tells the layer to prepare for input formatted according to the
        given space.

        Parameters
        ----------
        space : Space
            The Space the input to this layer will lie in.

        Notes
        -----
        This usually resets parameters.
        """
        raise NotImplementedError(str(type(self)) + " does not implement "
                "set_input_space.")


class MLP(Layer):
    """
    A multilayer perceptron.

    Note that it's possible for an entire MLP to be a single layer of a larger
    MLP.

    Parameters
    ----------
    layers : list
        A list of Layer objects. The final layer specifies the output space
        of this MLP.
    batch_size : int, optional
        If not specified then must be a positive integer. Mostly useful if
        one of your layers involves a Theano op like convolution that
        requires a hard-coded batch size.
    nvis : int, optional
        Number of "visible units" (input units). Equivalent to specifying
        `input_space=VectorSpace(dim=nvis)`. Note that certain methods require
        a different type of input space (e.g. a Conv2Dspace in the case of
        convnets). Use the input_space parameter in such cases. Should be 
        None if the MLP is part of another MLP.
    input_space : Space object, optional
        A Space specifying the kind of input the MLP accepts. If None,
        input space is specified by nvis. Should be None if the MLP is
        part of another MLP.
    layer_name : name of the MLP layer. Should be None if the MLP is
        not part of another MLP.
    seed : WRITEME
    kwargs : dict
        Passed on to the superclass.
    """

    def __init__(self, layers, batch_size=None, input_space=None,
                 nvis=None, seed=None, layer_name=None, **kwargs):
        super(MLP, self).__init__(**kwargs)

        self.seed = seed

        assert isinstance(layers, list)
        assert all(isinstance(layer, Layer) for layer in layers)
        assert len(layers) >= 1

        self.layer_name = layer_name

        self.layer_names = set()
        for layer in layers:
            assert layer.get_mlp() is None
            if layer.layer_name in self.layer_names:
                raise ValueError("MLP.__init__ given two or more layers "
                                 "with same name: " + layer.layer_name)

            layer.set_mlp(self)

            self.layer_names.add(layer.layer_name)



        self.layers = layers

        self.batch_size = batch_size
        self.force_batch_size = batch_size

        if input_space is not None or nvis is not None:
            self.setup_rng()

            # check if the layer_name is None (the MLP is the outer MLP)
            assert layer_name is None

            if nvis is not None:
                input_space = VectorSpace(nvis)

            self.input_space = input_space

            self._update_layer_input_spaces()

        self.freeze_set = set([])

        def f(x):
            if x is None:
                return None
            return 1. / x

    def setup_rng(self):
        """
        .. todo::

            WRITEME
        """
        if self.seed is None:
            self.seed = [2013, 1, 4]

        self.rng = np.random.RandomState(self.seed)

    @wraps(Layer.get_default_cost)
    def get_default_cost(self):

        return Default()

    @wraps(Layer.get_output_space)
    def get_output_space(self):

        return self.layers[-1].get_output_space()

    @wraps(Layer.set_input_space)
    def set_input_space(self, space):

        if hasattr(self, "mlp"):
            self.rng = self.mlp.rng
            self.batch_size = self.mlp.batch_size

        self.input_space = space

        self._update_layer_input_spaces()

    def _update_layer_input_spaces(self):
        """
        Tells each layer what its input space should be.

        Notes
        -----
        This usually resets the layer's parameters!
        """
        layers = self.layers
        try:
            layers[0].set_input_space(self.get_input_space())
        except BadInputSpaceError, e:
            raise TypeError("Layer 0 (" + str(layers[0]) + " of type " +
                            str(type(layers[0])) +
                            ") does not support the MLP's "
                            + "specified input space (" +
                            str(self.get_input_space()) +
                            " of type " + str(type(self.get_input_space())) +
                            "). Original exception: " + str(e))
        for i in xrange(1, len(layers)):
            layers[i].set_input_space(layers[i-1].get_output_space())

    def add_layers(self, layers):
        """
        Add new layers on top of the existing hidden layers

        Parameters
        ----------
        layers : WRITEME
        """

        existing_layers = self.layers
        assert len(existing_layers) > 0
        for layer in layers:
            assert layer.get_mlp() is None
            layer.set_mlp(self)
            layer.set_input_space(existing_layers[-1].get_output_space())
            existing_layers.append(layer)
            assert layer.layer_name not in self.layer_names
            self.layer_names.add(layer.layer_name)

    def freeze(self, parameter_set):
        """
        Freezes some of the parameters (new theano functions that implement
        learning will not use them; existing theano functions will continue
        to modify them).

        Parameters
        ----------
        parameter_set : set
            Set of parameters to freeze.
        """

        self.freeze_set = self.freeze_set.union(parameter_set)

    @wraps(Layer.get_monitoring_channels)
    def get_monitoring_channels(self, data):
        # if the MLP is the outer MLP \
        # (ie MLP is not contained in another structure)

        X, Y = data
        state = X
        rval = self.get_layer_monitoring_channels(state_below=X,
                                                    targets=Y)

        return rval

    @wraps(Layer.get_monitoring_channels_from_state)
    def get_monitoring_channels_from_state(self, state, target=None):
        #
        # Notes
        # -----
        # We are only monitoring the last layer for data dependent channels.
        # If you want to monitor every inner layer you should change the
        # get_monitoring_channels_from_state method.

        warnings.warn("Layer.get_monitoring_channels_from_state is " + \
                    "deprecated. Use get_layer_monitoring_channels " + \
                    "instead. Layer.get_monitoring_channels_from_state " + \
                    "will be removed on or after september 24th 2014",
                    stacklevel=2)
        rval = OrderedDict()

        for layer in self.layers:
            ch = layer.get_monitoring_channels()
            for key in ch:
                value = ch[key]
                doc = get_monitor_doc(value)
                if doc is None:
                    doc = str(type(layer)) + ".get_monitoring_channels did" + \
                            " not provide any further documentation for" + \
                            " this channel."
                doc = 'This channel came from a layer called "' + \
                        layer.layer_name + '" of an MLP.\n' + doc
                value.__doc__ = doc
                rval[layer.layer_name+'_'+key] = value


        args = [state]
        if target is not None:
            args.append(target)
        ch = self.layers[-1].get_monitoring_channels_from_state(*args)
        if not isinstance(ch, OrderedDict):
            raise TypeError(str((type(ch), self.layers[-1].layer_name)))
        for key in ch:
            value = ch[key]
            doc = get_monitor_doc(value)
            if doc is None:
                doc = str(type(self.layers[-1])) + \
                        ".get_monitoring_channels_from_state did" + \
                        " not provide any further documentation for" + \
                        " this channel."
            doc = 'This channel came from a layer called "' + \
                    self.layers[-1].layer_name + '" of an MLP.\n' + doc
            value.__doc__ = doc
            rval[self.layers[-1].layer_name+'_'+key] = value

        return rval


    @wraps(Layer.get_layer_monitoring_channels)
    def get_layer_monitoring_channels(self, state_below=None,
                                        state=None, targets=None):

        rval = OrderedDict()
        if state_below is not None:
            state = state_below

            for layer in self.layers:
                # We don't go through all the inner layers recursively
                state = layer.fprop(state)
                args = [None, state]
                if layer is self.layers[-1] and targets is not None:
                    args.append(targets)
                ch = layer.get_layer_monitoring_channels(*args)
                if not isinstance(ch, OrderedDict):
                    raise TypeError(str((type(ch), layer.layer_name)))
                for key in ch:
                    value = ch[key]
                    doc = get_monitor_doc(value)
                    if doc is None:
                        doc = str(type(layer)) + \
                            ".get_monitoring_channels_from_state did" + \
                            " not provide any further documentation for" + \
                            " this channel."
                    doc = 'This channel came from a layer called "' + \
                            layer.layer_name + '" of an MLP.\n' + doc
                    value.__doc__ = doc
                    rval[layer.layer_name+'_'+key] = value


        elif state is not None:

            for layer in self.layers:
                if layer is self.layers[-1]:
                    args = [None, state]
                    if targets is not None:
                        args.append(targets)
                    ch = layer.get_layer_monitoring_channels(*args)
                else:
                    ch = layer.get_layer_monitoring_channels()
                for key in ch:
                    value = ch[key]
                    doc = get_monitor_doc(value)
                    if doc is None:
                        doc = str(type(layer)) + \
                            ".get_monitoring_channels did" + \
                            " not provide any further documentation for" + \
                            " this channel."
                    doc = 'This channel came from a layer called "' + \
                            layer.layer_name + '" of an MLP.\n' + doc
                    value.__doc__ = doc
                    rval[layer.layer_name+'_'+key] = value

        else:
            for layer in self.layers:
                ch = layer.get_layer_monitoring_channels()
                if not isinstance(ch, OrderedDict):
                    raise TypeError(str((type(ch), layer.layer_name)))
                for key in ch:
                    value = ch[key]
                    doc = get_monitor_doc(value)
                    if doc is None:
                        doc = str(type(layer)) + \
                            ".get_monitoring_channels_from_state did" + \
                            " not provide any further documentation for" + \
                            " this channel."
                    doc = 'This channel came from a layer called "' + \
                            layer.layer_name + '" of an MLP.\n' + doc
                    value.__doc__ = doc
                    rval[layer.layer_name+'_'+key] = value

        return rval

    def get_monitoring_data_specs(self):
        """
        Returns data specs requiring both inputs and targets.

        Returns
        -------
        data_specs: TODO
            The data specifications for both inputs and targets.
        """
        space = CompositeSpace((self.get_input_space(),
                                self.get_output_space()))
        source = (self.get_input_source(), self.get_target_source())
        return (space, source)

    @wraps(Layer.get_params)
    def get_params(self):

        if not hasattr(self, "input_space"):
            raise AttributeError("Input space has not been provided.")


        rval = []
        for layer in self.layers:
            for param in layer.get_params():
                if param.name is None:
                    logger.info(type(layer))
            layer_params = layer.get_params()
            assert not isinstance(layer_params, set)
            for param in layer_params:
                if param not in rval:
                    rval.append(param)

        rval = [elem for elem in rval if elem not in self.freeze_set]

        assert all([elem.name is not None for elem in rval])

        return rval

    @wraps(Layer.get_weight_decay)
    def get_weight_decay(self, coeffs):

        # check the case where coeffs is a scalar
        if not hasattr(coeffs, '__iter__'):
            coeffs = [coeffs]*len(self.layers)

        layer_costs = []
        for layer, coeff in safe_izip(self.layers, coeffs):
            if coeff != 0.:
                layer_costs += [layer.get_weight_decay(coeff)]
            else:
                layer_costs += [0.]

        total_cost = reduce(lambda x, y: x + y, layer_costs)

        return total_cost

    @wraps(Layer.get_l1_weight_decay)
    def get_l1_weight_decay(self, coeffs):

        # check the case where coeffs is a scalar
        if not hasattr(coeffs, '__iter__'):
            coeffs = [coeffs]*len(self.layers)

        layer_costs = []
        for layer, coeff in safe_izip(self.layers, coeffs):
            if coeff != 0.:
                layer_costs += [layer.get_l1_weight_decay(coeff)]
            else:
                layer_costs += [0.]

        total_cost = reduce(lambda x, y: x + y, layer_costs)

        return total_cost

    @wraps(Model.set_batch_size)
    def set_batch_size(self, batch_size):

        self.batch_size = batch_size
        self.force_batch_size = batch_size

        for layer in self.layers:
            layer.set_batch_size(batch_size)

    @wraps(Layer._modify_updates)
    def _modify_updates(self, updates):

        for layer in self.layers:
            layer.modify_updates(updates)

    @wraps(Layer.get_lr_scalers)
    def get_lr_scalers(self):

        rval = OrderedDict()

        params = self.get_params()

        for layer in self.layers:
            contrib = layer.get_lr_scalers()

            assert isinstance(contrib, OrderedDict)
            # No two layers can contend to scale a parameter
            assert not any([key in rval for key in contrib])
            # Don't try to scale anything that's not a parameter
            assert all([key in params for key in contrib])

            rval.update(contrib)
        assert all([isinstance(val, float) for val in rval.values()])

        return rval

    @wraps(Layer.get_weights)
    def get_weights(self):

        if not hasattr(self, "input_space"):
            raise AttributeError("Input space has not been provided.")


        return self.layers[0].get_weights()

    @wraps(Layer.get_weights_view_shape)
    def get_weights_view_shape(self):

        if not hasattr(self, "input_space"):
            raise AttributeError("Input space has not been provided.")


        return self.layers[0].get_weights_view_shape()

    @wraps(Layer.get_weights_format)
    def get_weights_format(self):

        if not hasattr(self, "input_space"):
            raise AttributeError("Input space has not been provided.")


        return self.layers[0].get_weights_format()

    @wraps(Layer.get_weights_topo)
    def get_weights_topo(self):

        if not hasattr(self, "input_space"):
            raise AttributeError("Input space has not been provided.")


        return self.layers[0].get_weights_topo()

    def dropout_fprop(self, state_below, default_input_include_prob=0.5,
                      input_include_probs=None, default_input_scale=2.,
                      input_scales=None, per_example=True):
        """
        Returns the output of the MLP, when applying dropout to the input and
        intermediate layers.


        Parameters
        ----------
        state_below : WRITEME
            The input to the MLP
        default_input_include_prob : WRITEME
        input_include_probs : WRITEME
        default_input_scale : WRITEME
        input_scales : WRITEME
        per_example : bool, optional
            Sample a different mask value for every example in a batch.
            Defaults to `True`. If `False`, sample one mask per mini-batch.


        Notes
        -----
        Each input to each layer is randomly included or
        excluded for each example. The probability of inclusion is independent
        for each input and each example. Each layer uses
        `default_input_include_prob` unless that layer's name appears as a key
        in input_include_probs, in which case the input inclusion probability
        is given by the corresponding value.

        Each feature is also multiplied by a scale factor. The scale factor for
        each layer's input scale is determined by the same scheme as the input
        probabilities.
        """

        warnings.warn("dropout doesn't use fixed_var_descr so it won't work "
                      "with algorithms that make more than one theano "
                      "function call per batch, such as BGD. Implementing "
                      "fixed_var descr could increase the memory usage "
                      "though.")

        if input_include_probs is None:
            input_include_probs = {}

        if input_scales is None:
            input_scales = {}

        self._validate_layer_names(list(input_include_probs.keys()))
        self._validate_layer_names(list(input_scales.keys()))

        theano_rng = MRG_RandomStreams(max(self.rng.randint(2 ** 15), 1))

        for layer in self.layers:
            layer_name = layer.layer_name

            if layer_name in input_include_probs:
                include_prob = input_include_probs[layer_name]
            else:
                include_prob = default_input_include_prob

            if layer_name in input_scales:
                scale = input_scales[layer_name]
            else:
                scale = default_input_scale

            state_below = self.apply_dropout(
                state=state_below,
                include_prob=include_prob,
                theano_rng=theano_rng,
                scale=scale,
                mask_value=layer.dropout_input_mask_value,
                input_space=layer.get_input_space(),
                per_example=per_example
            )
            state_below = layer.fprop(state_below)

        return state_below

    def masked_fprop(self, state_below, mask, masked_input_layers=None,
                     default_input_scale=2., input_scales=None):
        """
        Forward propagate through the network with a dropout mask
        determined by an integer (the binary representation of
        which is used to generate the mask).

        Parameters
        ----------
        state_below : tensor_like
            The (symbolic) output state of the layer below.
        mask : int
            An integer indexing possible binary masks. It should be
            < 2 ** get_total_input_dimension(masked_input_layers)
            and greater than or equal to 0.
        masked_input_layers : list, optional
            A list of layer names to mask. If `None`, the input to all layers
            (including the first hidden layer) is masked.
        default_input_scale : float, optional
            The amount to scale inputs in masked layers that do not appear in
            `input_scales`. Defaults to 2.
        input_scales : dict, optional
            A dictionary mapping layer names to floating point numbers
            indicating how much to scale input to a given layer.

        Returns
        -------
        masked_output : tensor_like
            The output of the forward propagation of the masked network.
        """
        if input_scales is not None:
            self._validate_layer_names(input_scales)
        else:
            input_scales = {}
        if any(n not in masked_input_layers for n in input_scales):
            layers = [n for n in input_scales if n not in masked_input_layers]
            raise ValueError("input scales provided for layer not masked: " %
                             ", ".join(layers))
        if masked_input_layers is not None:
            self._validate_layer_names(masked_input_layers)
        else:
            masked_input_layers = self.layer_names
        num_inputs = self.get_total_input_dimension(masked_input_layers)
        assert mask >= 0, "Mask must be a non-negative integer."
        if mask > 0 and math.log(mask, 2) > num_inputs:
            raise ValueError("mask value of %d too large; only %d "
                             "inputs to layers (%s)" %
                             (mask, num_inputs,
                              ", ".join(masked_input_layers)))

        def binary_string(x, length, dtype):
            """
            Create the binary representation of an integer `x`, padded to
            `length`, with dtype `dtype`.

            Parameters
            ----------
            length : WRITEME
            dtype : WRITEME

            Returns
            -------
            WRITEME
            """
            s = np.empty(length, dtype=dtype)
            for i in range(length - 1, -1, -1):
                if x // (2 ** i) == 1:
                    s[i] = 1
                else:
                    s[i] = 0
                x = x % (2 ** i)
            return s

        remaining_mask = mask
        for layer in self.layers:
            if layer.layer_name in masked_input_layers:
                scale = input_scales.get(layer.layer_name,
                                         default_input_scale)
                n_inputs = layer.get_input_space().get_total_dimension()
                layer_dropout_mask = remaining_mask & (2 ** n_inputs - 1)
                remaining_mask >>= n_inputs
                mask = binary_string(layer_dropout_mask, n_inputs,
                                     'uint8')
                shape = layer.get_input_space().get_origin_batch(1).shape
                s_mask = T.as_tensor_variable(mask).reshape(shape)
                if layer.dropout_input_mask_value == 0:
                    state_below = state_below * s_mask * scale
                else:
                    state_below = T.switch(s_mask, state_below * scale,
                                           layer.dropout_input_mask_value)
            state_below = layer.fprop(state_below)

        return state_below

    def _validate_layer_names(self, layers):
        """
        .. todo::

            WRITEME
        """
        if any(layer not in self.layer_names for layer in layers):
            unknown_names = [layer for layer in layers
                             if layer not in self.layer_names]
            raise ValueError("MLP has no layer(s) named %s" %
                             ", ".join(unknown_names))

    def get_total_input_dimension(self, layers):
        """
        Get the total number of inputs to the layers whose
        names are listed in `layers`. Used for computing the
        total number of dropout masks.

        Parameters
        ----------
        layers : WRITEME

        Returns
        -------
        WRITEME
        """
        self._validate_layer_names(layers)
        total = 0
        for layer in self.layers:
            if layer.layer_name in layers:
                total += layer.get_input_space().get_total_dimension()
        return total

    @wraps(Layer.fprop)
    def fprop(self, state_below, return_all=False):

        if not hasattr(self, "input_space"):
            raise AttributeError("Input space has not been provided.")


        rval = self.layers[0].fprop(state_below)

        rlist = [rval]

        for layer in self.layers[1:]:
            rval = layer.fprop(rval)
            rlist.append(rval)

        if return_all:
            return rlist
        return rval

    def apply_dropout(self, state, include_prob, scale, theano_rng,
                      input_space, mask_value=0, per_example=True):
        """
        .. todo::

            WRITEME

        Parameters
        ----------
        state: WRITEME
        include_prob : WRITEME
        scale : WRITEME
        theano_rng : WRITEME
        input_space : WRITEME
        mask_value : WRITEME
        per_example : bool, optional
            Sample a different mask value for every example in a batch.
            Defaults to `True`. If `False`, sample one mask per mini-batch.
        """
        if include_prob in [None, 1.0, 1]:
            return state
        assert scale is not None
        if isinstance(state, tuple):
            return tuple(self.apply_dropout(substate, include_prob,
                                            scale, theano_rng, mask_value)
                         for substate in state)
        # TODO: all of this assumes that if it's not a tuple, it's
        # a dense tensor. It hasn't been tested with sparse types.
        # A method to format the mask (or any other values) as
        # the given symbolic type should be added to the Spaces
        # interface.
        if per_example:
            mask = theano_rng.binomial(p=include_prob, size=state.shape,
                                       dtype=state.dtype)
        else:
            batch = input_space.get_origin_batch(1)
            mask = theano_rng.binomial(p=include_prob, size=batch.shape,
                                       dtype=state.dtype)
            rebroadcast = T.Rebroadcast(*zip(xrange(batch.ndim),
                                             [s == 1 for s in batch.shape]))
            mask = rebroadcast(mask)
        if mask_value == 0:
            rval = state * mask * scale
        else:
            rval = T.switch(mask, state * scale, mask_value)
        return T.cast(rval, state.dtype)

    @wraps(Layer.cost)
    def cost(self, Y, Y_hat):

        return self.layers[-1].cost(Y, Y_hat)

    @wraps(Layer.cost_matrix)
    def cost_matrix(self, Y, Y_hat):

        return self.layers[-1].cost_matrix(Y, Y_hat)

    @wraps(Layer.cost_from_cost_matrix)
    def cost_from_cost_matrix(self, cost_matrix):

        return self.layers[-1].cost_from_cost_matrix(cost_matrix)

    def cost_from_X(self, data):
        """
        Computes self.cost, but takes data=(X, Y) rather than Y_hat as an
        argument.

        This is just a wrapper around self.cost that computes Y_hat by
        calling Y_hat = self.fprop(X)

        Parameters
        ----------
        data : WRITEME
        """
        self.cost_from_X_data_specs()[0].validate(data)
        X, Y = data
        Y_hat = self.fprop(X)
        return self.cost(Y, Y_hat)

    def cost_from_X_data_specs(self):
        """
        Returns the data specs needed by cost_from_X.

        This is useful if cost_from_X is used in a MethodCost.
        """
        space = CompositeSpace((self.get_input_space(),
                                self.get_output_space()))
        source = (self.get_input_source(), self.get_target_source())
        return (space, source)

    def __str__(self):
        """
        Summarizes the MLP by printing the size and format of the input to all
        layers. Feel free to add reasonably concise info as needed.
        """
        rval = []
        for layer in self.layers:
            rval.append(layer.layer_name)
            input_space = layer.get_input_space()
            rval.append('\tInput space: ' + str(input_space))
            rval.append('\tTotal input dimension: ' +
                        str(input_space.get_total_dimension()))
        rval = '\n'.join(rval)
        return rval


class Softmax(Layer):
    """
    .. todo::

        WRITEME (including parameters list)

    Parameters
    ----------
    n_classes : WRITEME
    layer_name : WRITEME
    irange : WRITEME
    istdev : WRITEME
    sparse_init : WRITEME
    W_lr_scale : WRITEME
    b_lr_scale : WRITEME
    max_row_norm : WRITEME
    no_affine : WRITEME
    max_col_norm : WRITEME
    init_bias_target_marginals : WRITEME
    """

    def __init__(self, n_classes, layer_name, irange=None,
                 istdev=None,
                 sparse_init=None, W_lr_scale=None,
                 b_lr_scale=None, max_row_norm=None,
                 no_affine=False,
                 max_col_norm=None, init_bias_target_marginals=None):

        super(Softmax, self).__init__()

        if isinstance(W_lr_scale, str):
            W_lr_scale = float(W_lr_scale)

        self.__dict__.update(locals())
        del self.self
        del self.init_bias_target_marginals

        assert isinstance(n_classes, py_integer_types)

        self.output_space = VectorSpace(n_classes)
        if not no_affine:
            self.b = sharedX(np.zeros((n_classes,)), name='softmax_b')
            if init_bias_target_marginals:
                marginals = init_bias_target_marginals.y.mean(axis=0)
                assert marginals.ndim == 1
                b = pseudoinverse_softmax_numpy(marginals).astype(self.b.dtype)
                assert b.ndim == 1
                assert b.dtype == self.b.dtype
                self.b.set_value(b)
        else:
            assert init_bias_target_marginals is None

    @wraps(Layer.get_lr_scalers)
    def get_lr_scalers(self):

        rval = OrderedDict()

        if self.W_lr_scale is not None:
            assert isinstance(self.W_lr_scale, float)
            rval[self.W] = self.W_lr_scale

        if not hasattr(self, 'b_lr_scale'):
            self.b_lr_scale = None

        if self.b_lr_scale is not None:
            assert isinstance(self.b_lr_scale, float)
            rval[self.b] = self.b_lr_scale

        return rval

    @wraps(Layer.get_monitoring_channels)
    def get_monitoring_channels(self):
        warnings.warn("Layer.get_monitoring_channels is " + \
                    "deprecated. Use get_layer_monitoring_channels " + \
                    "instead. Layer.get_monitoring_channels " + \
                    "will be removed on or after september 24th 2014",
                    stacklevel=2)

        if self.no_affine:
            return OrderedDict()

        W = self.W

        assert W.ndim == 2

        sq_W = T.sqr(W)

        row_norms = T.sqrt(sq_W.sum(axis=1))
        col_norms = T.sqrt(sq_W.sum(axis=0))

        return OrderedDict([('row_norms_min',  row_norms.min()),
                            ('row_norms_mean', row_norms.mean()),
                            ('row_norms_max',  row_norms.max()),
                            ('col_norms_min',  col_norms.min()),
                            ('col_norms_mean', col_norms.mean()),
                            ('col_norms_max',  col_norms.max()), ])

    @wraps(Layer.get_monitoring_channels_from_state)
    def get_monitoring_channels_from_state(self, state, target=None):
        warnings.warn("Layer.get_monitoring_channels_from_state is " + \
                    "deprecated. Use get_layer_monitoring_channels " + \
                    "instead. Layer.get_monitoring_channels_from_state " + \
                    "will be removed on or after september 24th 2014",
                    stacklevel=2)

        # channels that does not require state information
        if self.no_affine:
            rval = OrderedDict()

        W = self.W

        assert W.ndim == 2

        sq_W = T.sqr(W)

        row_norms = T.sqrt(sq_W.sum(axis=1))
        col_norms = T.sqrt(sq_W.sum(axis=0))

        rval = OrderedDict([('row_norms_min',  row_norms.min()),
                            ('row_norms_mean', row_norms.mean()),
                            ('row_norms_max',  row_norms.max()),
                            ('col_norms_min',  col_norms.min()),
                            ('col_norms_mean', col_norms.mean()),
                            ('col_norms_max',  col_norms.max()), ])

        mx = state.max(axis=1)

        rval.update(OrderedDict([('mean_max_class', mx.mean()),
                            ('max_max_class', mx.max()),
                            ('min_max_class', mx.min())]))

        if target is not None:
            y_hat = T.argmax(state, axis=1)
            y = T.argmax(target, axis=1)
            misclass = T.neq(y, y_hat).mean()
            misclass = T.cast(misclass, config.floatX)
            rval['misclass'] = misclass
            rval['nll'] = self.cost(Y_hat=state, Y=target)

        return rval

    @wraps(Layer.get_layer_monitoring_channels)
    def get_layer_monitoring_channels(self, state_below=None,
                                    state=None, targets=None):

        # channels that does not require state information
        if self.no_affine:
            rval = OrderedDict()

        W = self.W

        assert W.ndim == 2

        sq_W = T.sqr(W)

        row_norms = T.sqrt(sq_W.sum(axis=1))
        col_norms = T.sqrt(sq_W.sum(axis=0))

        rval = OrderedDict([('row_norms_min',  row_norms.min()),
                            ('row_norms_mean', row_norms.mean()),
                            ('row_norms_max',  row_norms.max()),
                            ('col_norms_min',  col_norms.min()),
                            ('col_norms_mean', col_norms.mean()),
                            ('col_norms_max',  col_norms.max()), ])

        if (state_below is not None) or (state is not None):
            if state is None:
                state = self.fprop(state_below)

            mx = state.max(axis=1)

            rval.update(OrderedDict([('mean_max_class', mx.mean()),
                                ('max_max_class', mx.max()),
                                ('min_max_class', mx.min())]))

            if targets is not None:
                y_hat = T.argmax(state, axis=1)
                y = T.argmax(targets, axis=1)
                misclass = T.neq(y, y_hat).mean()
                misclass = T.cast(misclass, config.floatX)
                rval['misclass'] = misclass
                rval['nll'] = self.cost(Y_hat=state, Y=targets)

        return rval

    @wraps(Layer.set_input_space)
    def set_input_space(self, space):

        self.input_space = space

        if not isinstance(space, Space):
            raise TypeError("Expected Space, got " +
                            str(space)+" of type "+str(type(space)))

        self.input_dim = space.get_total_dimension()
        self.needs_reformat = not isinstance(space, VectorSpace)

        if self.no_affine:
            desired_dim = self.n_classes
            assert self.input_dim == desired_dim
        else:
            desired_dim = self.input_dim
        self.desired_space = VectorSpace(desired_dim)

        if not self.needs_reformat:
            assert self.desired_space == self.input_space

        rng = self.mlp.rng

        if self.no_affine:
            self._params = []
        else:
            if self.irange is not None:
                assert self.istdev is None
                assert self.sparse_init is None
                W = rng.uniform(-self.irange,
                                self.irange,
                                (self.input_dim, self.n_classes))
            elif self.istdev is not None:
                assert self.sparse_init is None
                W = rng.randn(self.input_dim, self.n_classes) * self.istdev
            else:
                assert self.sparse_init is not None
                W = np.zeros((self.input_dim, self.n_classes))
                for i in xrange(self.n_classes):
                    for j in xrange(self.sparse_init):
                        idx = rng.randint(0, self.input_dim)
                        while W[idx, i] != 0.:
                            idx = rng.randint(0, self.input_dim)
                        W[idx, i] = rng.randn()

            self.W = sharedX(W,  'softmax_W')

            self._params = [self.b, self.W]

    @wraps(Layer.get_weights_topo)
    def get_weights_topo(self):

        if not isinstance(self.input_space, Conv2DSpace):
            raise NotImplementedError()
        desired = self.W.get_value().T
        ipt = self.desired_space.format_as(desired, self.input_space)
        rval = Conv2DSpace.convert_numpy(ipt,
                                         self.input_space.axes,
                                         ('b', 0, 1, 'c'))
        return rval

    @wraps(Layer.get_weights)
    def get_weights(self):

        if not isinstance(self.input_space, VectorSpace):
            raise NotImplementedError()

        return self.W.get_value()

    @wraps(Layer.set_weights)
    def set_weights(self, weights):

        self.W.set_value(weights)

    @wraps(Layer.set_biases)
    def set_biases(self, biases):

        self.b.set_value(biases)

    @wraps(Layer.get_biases)
    def get_biases(self):

        return self.b.get_value()

    @wraps(Layer.get_weights_format)
    def get_weights_format(self):

        return ('v', 'h')

    @wraps(Layer.fprop)
    def fprop(self, state_below):

        self.input_space.validate(state_below)

        if self.needs_reformat:
            state_below = self.input_space.format_as(state_below,
                                                     self.desired_space)

        self.desired_space.validate(state_below)
        assert state_below.ndim == 2

        if not hasattr(self, 'no_affine'):
            self.no_affine = False

        if self.no_affine:
            Z = state_below
        else:
            assert self.W.ndim == 2
            b = self.b

            Z = T.dot(state_below, self.W) + b

        rval = T.nnet.softmax(Z)

        for value in get_debug_values(rval):
            if self.mlp.batch_size is not None:
                assert value.shape[0] == self.mlp.batch_size

        return rval

    @wraps(Layer.cost)
    def cost(self, Y, Y_hat):

        assert hasattr(Y_hat, 'owner')
        owner = Y_hat.owner
        assert owner is not None
        op = owner.op
        if isinstance(op, Print):
            assert len(owner.inputs) == 1
            Y_hat, = owner.inputs
            owner = Y_hat.owner
            op = owner.op
        assert isinstance(op, T.nnet.Softmax)
        z, = owner.inputs
        assert z.ndim == 2

        z = z - z.max(axis=1).dimshuffle(0, 'x')
        log_prob = z - T.log(T.exp(z).sum(axis=1).dimshuffle(0, 'x'))
        # we use sum and not mean because this is really one variable per row
        log_prob_of = (Y * log_prob).sum(axis=1)
        assert log_prob_of.ndim == 1

        rval = log_prob_of.mean()

        return - rval

    @wraps(Layer.cost_matrix)
    def cost_matrix(self, Y, Y_hat):

        assert hasattr(Y_hat, 'owner')
        owner = Y_hat.owner
        assert owner is not None
        op = owner.op
        if isinstance(op, Print):
            assert len(owner.inputs) == 1
            Y_hat, = owner.inputs
            owner = Y_hat.owner
            op = owner.op
        assert isinstance(op, T.nnet.Softmax)
        z, = owner.inputs
        assert z.ndim == 2

        z = z - z.max(axis=1).dimshuffle(0, 'x')
        log_prob = z - T.log(T.exp(z).sum(axis=1).dimshuffle(0, 'x'))
        # we use sum and not mean because this is really one variable per row
        log_prob_of = (Y * log_prob)

        return -log_prob_of

    @wraps(Layer.get_weight_decay)
    def get_weight_decay(self, coeff):

        if isinstance(coeff, str):
            coeff = float(coeff)
        assert isinstance(coeff, float) or hasattr(coeff, 'dtype')
        return coeff * T.sqr(self.W).sum()

    @wraps(Layer.get_l1_weight_decay)
    def get_l1_weight_decay(self, coeff):

        if isinstance(coeff, str):
            coeff = float(coeff)
        assert isinstance(coeff, float) or hasattr(coeff, 'dtype')
        W = self.W
        return coeff * abs(W).sum()

    @wraps(Layer._modify_updates)
    def _modify_updates(self, updates):

        if self.no_affine:
            return
        if self.max_row_norm is not None:
            W = self.W
            if W in updates:
                updated_W = updates[W]
                row_norms = T.sqrt(T.sum(T.sqr(updated_W), axis=1))
                desired_norms = T.clip(row_norms, 0, self.max_row_norm)
                scales = desired_norms / (1e-7 + row_norms)
                updates[W] = updated_W * scales.dimshuffle(0, 'x')
        if self.max_col_norm is not None:
            assert self.max_row_norm is None
            W = self.W
            if W in updates:
                updated_W = updates[W]
                col_norms = T.sqrt(T.sum(T.sqr(updated_W), axis=0))
                desired_norms = T.clip(col_norms, 0, self.max_col_norm)
                updates[W] = updated_W * (desired_norms / (1e-7 + col_norms))


class SoftmaxPool(Layer):
    """
    A hidden layer that uses the softmax function to do max pooling over groups
    of units. When the pooling size is 1, this reduces to a standard sigmoidal
    MLP layer.

    Parameters
    ----------
    detector_layer_dim : WRITEME
    layer_name : WRITEME
    pool_size : WRITEME
    irange : WRITEME
    sparse_init : WRITEME
    sparse_stdev : WRITEME
    include_prob : float, optional
        Probability of including a weight element in the set of weights
        initialized to U(-irange, irange). If not included it is
        initialized to 0.
    init_bias : WRITEME
    W_lr_scale : WRITEME
    b_lr_scale : WRITEME
    mask_weights : WRITEME
    max_col_norm : WRITEME
    """

    def __init__(self,
                 detector_layer_dim,
                 layer_name,
                 pool_size=1,
                 irange=None,
                 sparse_init=None,
                 sparse_stdev=1.,
                 include_prob=1.0,
                 init_bias=0.,
                 W_lr_scale=None,
                 b_lr_scale=None,
                 mask_weights=None,
                 max_col_norm=None):
        super(SoftmaxPool, self).__init__()
        self.__dict__.update(locals())
        del self.self

        self.b = sharedX(np.zeros((self.detector_layer_dim,)) + init_bias,
                         name=(layer_name + '_b'))

    @wraps(Layer.get_lr_scalers)
    def get_lr_scalers(self):

        if not hasattr(self, 'W_lr_scale'):
            self.W_lr_scale = None

        if not hasattr(self, 'b_lr_scale'):
            self.b_lr_scale = None

        rval = OrderedDict()

        if self.W_lr_scale is not None:
            W, = self.transformer.get_params()
            rval[W] = self.W_lr_scale

        if self.b_lr_scale is not None:
            rval[self.b] = self.b_lr_scale

        return rval

    @wraps(Layer.set_input_space)
    def set_input_space(self, space):

        self.input_space = space

        if isinstance(space, VectorSpace):
            self.requires_reformat = False
            self.input_dim = space.dim
        else:
            self.requires_reformat = True
            self.input_dim = space.get_total_dimension()
            self.desired_space = VectorSpace(self.input_dim)

        if not (self.detector_layer_dim % self.pool_size == 0):
            raise ValueError("detector_layer_dim = %d, pool_size = %d. "
                             "Should be divisible but remainder is %d" %
                             (self.detector_layer_dim,
                              self.pool_size,
                              self.detector_layer_dim % self.pool_size))

        self.h_space = VectorSpace(self.detector_layer_dim)
        self.pool_layer_dim = self.detector_layer_dim / self.pool_size
        self.output_space = VectorSpace(self.pool_layer_dim)

        rng = self.mlp.rng
        if self.irange is not None:
            assert self.sparse_init is None
            W = rng.uniform(-self.irange,
                            self.irange,
                            (self.input_dim, self.detector_layer_dim)) * \
                (rng.uniform(0., 1., (self.input_dim, self.detector_layer_dim))
                 < self.include_prob)
        else:
            assert self.sparse_init is not None
            W = np.zeros((self.input_dim, self.detector_layer_dim))

            def mask_rejects(idx, i):
                if self.mask_weights is None:
                    return False
                return self.mask_weights[idx, i] == 0.

            for i in xrange(self.detector_layer_dim):
                assert self.sparse_init <= self.input_dim
                for j in xrange(self.sparse_init):
                    idx = rng.randint(0, self.input_dim)
                    while W[idx, i] != 0 or mask_rejects(idx, i):
                        idx = rng.randint(0, self.input_dim)
                    W[idx, i] = rng.randn()
            W *= self.sparse_stdev

        W = sharedX(W)
        W.name = self.layer_name + '_W'

        self.transformer = MatrixMul(W)

        W, = self.transformer.get_params()
        assert W.name is not None

        if self.mask_weights is not None:
            expected_shape = (self.input_dim, self.detector_layer_dim)
            if expected_shape != self.mask_weights.shape:
                raise ValueError("Expected mask with shape " +
                                 str(expected_shape) +
                                 " but got " +
                                 str(self.mask_weights.shape))
            self.mask = sharedX(self.mask_weights)

    @wraps(Layer._modify_updates)
    def _modify_updates(self, updates):

        # Patch old pickle files
        if not hasattr(self, 'mask_weights'):
            self.mask_weights = None

        if self.mask_weights is not None:
            W, = self.transformer.get_params()
            if W in updates:
                updates[W] = updates[W] * self.mask

        if self.max_col_norm is not None:
            W, = self.transformer.get_params()
            if W in updates:
                updated_W = updates[W]
                col_norms = T.sqrt(T.sum(T.sqr(updated_W), axis=0))
                desired_norms = T.clip(col_norms, 0, self.max_col_norm)
                updates[W] = updated_W * (desired_norms / (1e-7 + col_norms))

    @wraps(Layer.get_params)
    def get_params(self):

        assert self.b.name is not None
        W, = self.transformer.get_params()
        assert W.name is not None
        rval = self.transformer.get_params()
        assert not isinstance(rval, set)
        rval = list(rval)
        assert self.b not in rval
        rval.append(self.b)
        return rval

    @wraps(Layer.get_weight_decay)
    def get_weight_decay(self, coeff):

        if isinstance(coeff, str):
            coeff = float(coeff)
        assert isinstance(coeff, float) or hasattr(coeff, 'dtype')
        W, = self.transformer.get_params()
        return coeff * T.sqr(W).sum()

    @wraps(Layer.get_l1_weight_decay)
    def get_l1_weight_decay(self, coeff):

        if isinstance(coeff, str):
            coeff = float(coeff)
        assert isinstance(coeff, float) or hasattr(coeff, 'dtype')
        W, = self.transformer.get_params()
        return coeff * abs(W).sum()

    @wraps(Layer.get_weights)
    def get_weights(self):

        if self.requires_reformat:
            # This is not really an unimplemented case.
            # We actually don't know how to format the weights
            # in design space. We got the data in topo space
            # and we don't have access to the dataset
            raise NotImplementedError()
        W, = self.transformer.get_params()
        return W.get_value()

    @wraps(Layer.set_weights)
    def set_weights(self, weights):

        W, = self.transformer.get_params()
        W.set_value(weights)

    @wraps(Layer.set_biases)
    def set_biases(self, biases):
        """
        .. todo::

            WRITEME
        """
        self.b.set_value(biases)

    @wraps(Layer.get_biases)
    def get_biases(self):

        return self.b.get_value()

    @wraps(Layer.get_weights_format)
    def get_weights_format(self):

        return ('v', 'h')

    @wraps(Layer.get_weights_view_shape)
    def get_weights_view_shape(self):

        total = self.detector_layer_dim
        cols = self.pool_size
        if cols == 1:
            # Let the PatchViewer decide how to arrange the units
            # when they're not pooled
            raise NotImplementedError()
        # When they are pooled, make each pooling unit have one row
        rows = total / cols
        return rows, cols

    @wraps(Layer.get_weights_topo)
    def get_weights_topo(self):

        if not isinstance(self.input_space, Conv2DSpace):
            raise NotImplementedError()

        W, = self.transformer.get_params()

        W = W.T

        W = W.reshape((self.detector_layer_dim,
                       self.input_space.shape[0],
                       self.input_space.shape[1],
                       self.input_space.num_channels))

        W = Conv2DSpace.convert(W, self.input_space.axes, ('b', 0, 1, 'c'))

        return function([], W)()

    @wraps(Layer.get_monitoring_channels)
    def get_monitoring_channels(self):
        warnings.warn("Layer.get_monitoring_channels is " + \
                    "deprecated. Use get_layer_monitoring_channels " + \
                    "instead. Layer.get_monitoring_channels " + \
                    "will be removed on or after september 24th 2014",
                    stacklevel=2)

        W, = self.transformer.get_params()

        assert W.ndim == 2

        sq_W = T.sqr(W)

        row_norms = T.sqrt(sq_W.sum(axis=1))
        col_norms = T.sqrt(sq_W.sum(axis=0))

        return OrderedDict([('row_norms_min',  row_norms.min()),
                            ('row_norms_mean', row_norms.mean()),
                            ('row_norms_max',  row_norms.max()),
                            ('col_norms_min',  col_norms.min()),
                            ('col_norms_mean', col_norms.mean()),
                            ('col_norms_max',  col_norms.max()), ])

    @wraps(Layer.get_monitoring_channels_from_state)
    def get_monitoring_channels_from_state(self, state):
        warnings.warn("Layer.get_monitoring_channels_from_state is " + \
                    "deprecated. Use get_layer_monitoring_channels " + \
                    "instead. Layer.get_monitoring_channels_from_state " + \
                    "will be removed on or after september 24th 2014",
                    stacklevel=2)

        W, = self.transformer.get_params()

        assert W.ndim == 2

        sq_W = T.sqr(W)

        row_norms = T.sqrt(sq_W.sum(axis=1))
        col_norms = T.sqrt(sq_W.sum(axis=0))

        rval = OrderedDict([('row_norms_min',  row_norms.min()),
                            ('row_norms_mean', row_norms.mean()),
                            ('row_norms_max',  row_norms.max()),
                            ('col_norms_min',  col_norms.min()),
                            ('col_norms_mean', col_norms.mean()),
                            ('col_norms_max',  col_norms.max()), ])

        P = state


        if self.pool_size == 1:
            vars_and_prefixes = [(P, '')]
        else:
            vars_and_prefixes = [(P, 'p_')]

        for var, prefix in vars_and_prefixes:
            v_max = var.max(axis=0)
            v_min = var.min(axis=0)
            v_mean = var.mean(axis=0)
            v_range = v_max - v_min

            # max_x.mean_u is "the mean over *u*nits of the max over
            # e*x*amples" The x and u are included in the name because
            # otherwise its hard to remember which axis is which when reading
            # the monitor I use inner.outer rather than outer_of_inner or
            # something like that because I want mean_x.* to appear next to
            # each other in the alphabetical list, as these are commonly
            # plotted together
            for key, val in [('max_x.max_u', v_max.max()),
                             ('max_x.mean_u', v_max.mean()),
                             ('max_x.min_u', v_max.min()),
                             ('min_x.max_u', v_min.max()),
                             ('min_x.mean_u', v_min.mean()),
                             ('min_x.min_u', v_min.min()),
                             ('range_x.max_u', v_range.max()),
                             ('range_x.mean_u', v_range.mean()),
                             ('range_x.min_u', v_range.min()),
                             ('mean_x.max_u', v_mean.max()),
                             ('mean_x.mean_u', v_mean.mean()),
                             ('mean_x.min_u', v_mean.min())]:
                rval[prefix+key] = val

        return rval

    @wraps(Layer.get_layer_monitoring_channels)
    def get_layer_monitoring_channels(self, state_below=None,
                                    state=None, **kwargs):

        W, = self.transformer.get_params()

        assert W.ndim == 2

        sq_W = T.sqr(W)

        row_norms = T.sqrt(sq_W.sum(axis=1))
        col_norms = T.sqrt(sq_W.sum(axis=0))

        rval = OrderedDict([('row_norms_min',  row_norms.min()),
                            ('row_norms_mean', row_norms.mean()),
                            ('row_norms_max',  row_norms.max()),
                            ('col_norms_min',  col_norms.min()),
                            ('col_norms_mean', col_norms.mean()),
                            ('col_norms_max',  col_norms.max()), ])

        if (state_below is not None) or (state is not None):
            if state is None:
                P = self.fprop(state_below)
            else:
                P = state


            if self.pool_size == 1:
                vars_and_prefixes = [(P, '')]
            else:
                vars_and_prefixes = [(P, 'p_')]

            for var, prefix in vars_and_prefixes:
                v_max = var.max(axis=0)
                v_min = var.min(axis=0)
                v_mean = var.mean(axis=0)
                v_range = v_max - v_min

                # max_x.mean_u is "the mean over *u*nits of the max over
                # e*x*amples" The x and u are included in the name because
                # otherwise its hard to remember which axis is which when
                # reading the monitor I use inner.outer rather than
                # outer_of_inner or something like that because I want
                # mean_x.* to appear next to each other in the alphabetical
                # list, as these are commonly plotted together
                for key, val in [('max_x.max_u', v_max.max()),
                                 ('max_x.mean_u', v_max.mean()),
                                 ('max_x.min_u', v_max.min()),
                                 ('min_x.max_u', v_min.max()),
                                 ('min_x.mean_u', v_min.mean()),
                                 ('min_x.min_u', v_min.min()),
                                 ('range_x.max_u', v_range.max()),
                                 ('range_x.mean_u', v_range.mean()),
                                 ('range_x.min_u', v_range.min()),
                                 ('mean_x.max_u', v_mean.max()),
                                 ('mean_x.mean_u', v_mean.mean()),
                                 ('mean_x.min_u', v_mean.min())]:
                    rval[prefix+key] = val

        return rval


    @wraps(Layer.fprop)
    def fprop(self, state_below):

        self.input_space.validate(state_below)

        if self.requires_reformat:
            state_below = self.input_space.format_as(state_below,
                                                     self.desired_space)

        z = self.transformer.lmul(state_below) + self.b
        if self.layer_name is not None:
            z.name = self.layer_name + '_z'
        p, h = max_pool_channels(z, self.pool_size)

        p.name = self.layer_name + '_p_'

        return p


class Linear(Layer):
    """
    A "linear model" in machine learning terminology. This would be more
    accurately described as an affine model because it adds an offset to
    the output as well as doing a matrix multiplication. The output is:

    output = T.dot(weights, input) + biases

    This class may be used as the output layer of an MLP for regression.
    It may also be used as a hidden layer. Most hidden layers classes are
    subclasses of this class that add apply a fixed nonlinearity to the
    output of the affine transformation provided by this class.

    One notable use of this class is to provide "bottleneck" layers.
    By using a Linear layer with few hidden units followed by a nonlinear
    layer such as RectifiedLinear with many hidden units, one essentially
    gets a RectifiedLinear layer with a factored weight matrix, which can
    reduce the number of parameters in the model (by making the effective
    weight matrix low rank).

    Parameters
    ----------
    dim : int
        The number of elements in the output of the layer.
    layer_name : str
        The name of the layer. All layers in an MLP must have a unique name.
    irange : WRITEME
    istdev : WRITEME
    sparse_init : WRITEME
    sparse_stdev : WRITEME
    include_prob : float
        Probability of including a weight element in the set of weights
        initialized to U(-irange, irange). If not included it is
        initialized to 0.
        Anything that can be broadcasted to a numpy vector.
        Provides the initial value of the biases of the model.
        When using this class as an output layer (specifically the Linear
        class, or subclasses that don't change the output like
        LinearGaussian, but not subclasses that change the output, like
        Softmax) it can be a good idea to set this to the return value of
        the `mean_of_targets` function. This provides the mean value of
        all the targets in the training set, so the model is initialized
        to a dummy model that predicts the expected value of each output
        variable.
    W_lr_scale : float, optional
        Multiply the learning rate on the weights by this constant.
    b_lr_scale : float, optional
        Multiply the learning rate on the biases by this constant.
    mask_weights : ndarray, optional
        If provided, the weights will be multiplied by this mask after each
        learning update.
    max_row_norm : WRITEME
    max_col_norm : WRITEME
    min_col_norm : WRITEME
    softmax_columns : DEPRECATED
    copy_input : REMOVED
    use_abs_loss : bool, optional
        If True, the cost function will be mean absolute error rather
        than mean squared error.
        You can think of mean squared error as fitting a Gaussian
        distribution with variance 1, or as learning to predict the mean
        of the data.
        You can think of mean absolute error as fitting a Laplace
        distribution with variance 1, or as learning to predict the
        median of the data.
    use_bias : bool, optional
        If False, does not add the bias term to the output.
    """

    def __init__(self,
                 dim,
                 layer_name,
                 irange=None,
                 istdev=None,
                 sparse_init=None,
                 sparse_stdev=1.,
                 include_prob=1.0,
                 init_bias=0.,
                 W_lr_scale=None,
                 b_lr_scale=None,
                 mask_weights=None,
                 max_row_norm=None,
                 max_col_norm=None,
                 min_col_norm=None,
                 softmax_columns=None,
                 copy_input=None,
                 use_abs_loss=False,
                 use_bias=True):

        if copy_input is not None:
            raise AssertionError("The copy_input option had a bug and has "
                    "been removed from the library.")

        super(Linear, self).__init__()

        if softmax_columns is None:
            softmax_columns = False
        else:
            warnings.warn("The softmax_columns argument is deprecated, and "
                    "will be removed on or after 2014-08-27.", stacklevel=2)

        if use_bias and init_bias is None:
            init_bias = 0.

        self.__dict__.update(locals())
        del self.self

        if use_bias:
            self.b = sharedX(np.zeros((self.dim,)) + init_bias,
                             name=(layer_name + '_b'))
        else:
            assert b_lr_scale is None
            init_bias is None

    @wraps(Layer.get_lr_scalers)
    def get_lr_scalers(self):

        if not hasattr(self, 'W_lr_scale'):
            self.W_lr_scale = None

        if not hasattr(self, 'b_lr_scale'):
            self.b_lr_scale = None

        rval = OrderedDict()

        if self.W_lr_scale is not None:
            W, = self.transformer.get_params()
            rval[W] = self.W_lr_scale

        if self.b_lr_scale is not None:
            rval[self.b] = self.b_lr_scale

        return rval

    @wraps(Layer.set_input_space)
    def set_input_space(self, space):

        self.input_space = space

        if isinstance(space, VectorSpace):
            self.requires_reformat = False
            self.input_dim = space.dim
        else:
            self.requires_reformat = True
            self.input_dim = space.get_total_dimension()
            self.desired_space = VectorSpace(self.input_dim)

        self.output_space = VectorSpace(self.dim)

        rng = self.mlp.rng
        if self.irange is not None:
            assert self.istdev is None
            assert self.sparse_init is None
            W = rng.uniform(-self.irange,
                            self.irange,
                            (self.input_dim, self.dim)) * \
                (rng.uniform(0., 1., (self.input_dim, self.dim))
                 < self.include_prob)
        elif self.istdev is not None:
            assert self.sparse_init is None
            W = rng.randn(self.input_dim, self.dim) * self.istdev
        else:
            assert self.sparse_init is not None
            W = np.zeros((self.input_dim, self.dim))

            def mask_rejects(idx, i):
                if self.mask_weights is None:
                    return False
                return self.mask_weights[idx, i] == 0.

            for i in xrange(self.dim):
                assert self.sparse_init <= self.input_dim
                for j in xrange(self.sparse_init):
                    idx = rng.randint(0, self.input_dim)
                    while W[idx, i] != 0 or mask_rejects(idx, i):
                        idx = rng.randint(0, self.input_dim)
                    W[idx, i] = rng.randn()
            W *= self.sparse_stdev

        W = sharedX(W)
        W.name = self.layer_name + '_W'

        self.transformer = MatrixMul(W)

        W, = self.transformer.get_params()
        assert W.name is not None

        if self.mask_weights is not None:
            expected_shape = (self.input_dim, self.dim)
            if expected_shape != self.mask_weights.shape:
                raise ValueError("Expected mask with shape " +
                                 str(expected_shape)+" but got " +
                                 str(self.mask_weights.shape))
            self.mask = sharedX(self.mask_weights)

    @wraps(Layer._modify_updates)
    def _modify_updates(self, updates):

        if self.mask_weights is not None:
            W, = self.transformer.get_params()
            if W in updates:
                updates[W] = updates[W] * self.mask

        if self.max_row_norm is not None:
            W, = self.transformer.get_params()
            if W in updates:
                updated_W = updates[W]
                row_norms = T.sqrt(T.sum(T.sqr(updated_W), axis=1))
                desired_norms = T.clip(row_norms, 0, self.max_row_norm)
                scales = desired_norms / (1e-7 + row_norms)
                updates[W] = updated_W * scales.dimshuffle(0, 'x')

        if self.max_col_norm is not None or self.min_col_norm is not None:
            assert self.max_row_norm is None
            if self.max_col_norm is not None:
                max_col_norm = self.max_col_norm
            if self.min_col_norm is None:
                self.min_col_norm = 0
            W, = self.transformer.get_params()
            if W in updates:
                updated_W = updates[W]
                col_norms = T.sqrt(T.sum(T.sqr(updated_W), axis=0))
                if self.max_col_norm is None:
                    max_col_norm = col_norms.max()
                desired_norms = T.clip(col_norms,
                                       self.min_col_norm,
                                       max_col_norm)
                updates[W] = updated_W * desired_norms / (1e-7 + col_norms)

    @wraps(Layer.get_params)
    def get_params(self):

        W, = self.transformer.get_params()
        assert W.name is not None
        rval = self.transformer.get_params()
        assert not isinstance(rval, set)
        rval = list(rval)
        if self.use_bias:
            assert self.b.name is not None
            assert self.b not in rval
            rval.append(self.b)
        return rval

    @wraps(Layer.get_weight_decay)
    def get_weight_decay(self, coeff):

        if isinstance(coeff, str):
            coeff = float(coeff)
        assert isinstance(coeff, float) or hasattr(coeff, 'dtype')
        W, = self.transformer.get_params()
        return coeff * T.sqr(W).sum()

    @wraps(Layer.get_l1_weight_decay)
    def get_l1_weight_decay(self, coeff):

        if isinstance(coeff, str):
            coeff = float(coeff)
        assert isinstance(coeff, float) or hasattr(coeff, 'dtype')
        W, = self.transformer.get_params()
        return coeff * abs(W).sum()

    @wraps(Layer.get_weights)
    def get_weights(self):

        if self.requires_reformat:
            # This is not really an unimplemented case.
            # We actually don't know how to format the weights
            # in design space. We got the data in topo space
            # and we don't have access to the dataset
            raise NotImplementedError()
        W, = self.transformer.get_params()

        W = W.get_value()

        if self.softmax_columns:
            P = np.exp(W)
            Z = np.exp(W).sum(axis=0)
            rval = P / Z
            return rval
        return W

    @wraps(Layer.set_weights)
    def set_weights(self, weights):

        W, = self.transformer.get_params()
        W.set_value(weights)

    @wraps(Layer.set_biases)
    def set_biases(self, biases):

        self.b.set_value(biases)

    @wraps(Layer.get_biases)
    def get_biases(self):
        """
        .. todo::

            WRITEME
        """
        return self.b.get_value()

    @wraps(Layer.get_weights_format)
    def get_weights_format(self):

        return ('v', 'h')

    @wraps(Layer.get_weights_topo)
    def get_weights_topo(self):

        if not isinstance(self.input_space, Conv2DSpace):
            raise NotImplementedError()

        W, = self.transformer.get_params()

        W = W.T

        W = W.reshape((self.dim, self.input_space.shape[0],
                       self.input_space.shape[1],
                       self.input_space.num_channels))

        W = Conv2DSpace.convert(W, self.input_space.axes, ('b', 0, 1, 'c'))

        return function([], W)()

    @wraps(Layer.get_monitoring_channels)
    def get_monitoring_channels(self):
        warnings.warn("Layer.get_monitoring_channels is " + \
                    "deprecated. Use get_layer_monitoring_channels " + \
                    "instead. Layer.get_monitoring_channels " + \
                    "will be removed on or after september 24th 2014",
                    stacklevel=2)

        W, = self.transformer.get_params()

        assert W.ndim == 2

        sq_W = T.sqr(W)

        row_norms = T.sqrt(sq_W.sum(axis=1))
        col_norms = T.sqrt(sq_W.sum(axis=0))

        return OrderedDict([('row_norms_min',  row_norms.min()),
                            ('row_norms_mean', row_norms.mean()),
                            ('row_norms_max',  row_norms.max()),
                            ('col_norms_min',  col_norms.min()),
                            ('col_norms_mean', col_norms.mean()),
                            ('col_norms_max',  col_norms.max()), ])

    @wraps(Layer.get_monitoring_channels_from_state)
    def get_monitoring_channels_from_state(self, state, target=None):
        warnings.warn("Layer.get_monitoring_channels_from_state is " + \
                    "deprecated. Use get_layer_monitoring_channels " + \
                    "instead. Layer.get_monitoring_channels_from_state " + \
                    "will be removed on or after september 24th 2014",
                    stacklevel=2)

        W, = self.transformer.get_params()

        assert W.ndim == 2

        sq_W = T.sqr(W)

        row_norms = T.sqrt(sq_W.sum(axis=1))
        col_norms = T.sqrt(sq_W.sum(axis=0))

        rval = OrderedDict([('row_norms_min',  row_norms.min()),
                            ('row_norms_mean', row_norms.mean()),
                            ('row_norms_max',  row_norms.max()),
                            ('col_norms_min',  col_norms.min()),
                            ('col_norms_mean', col_norms.mean()),
                            ('col_norms_max',  col_norms.max()), ])

        mx = state.max(axis=0)
        mean = state.mean(axis=0)
        mn = state.min(axis=0)
        rg = mx - mn

        rval['range_x_max_u'] = rg.max()
        rval['range_x_mean_u'] = rg.mean()
        rval['range_x_min_u'] = rg.min()

        rval['max_x_max_u'] = mx.max()
        rval['max_x_mean_u'] = mx.mean()
        rval['max_x_min_u'] = mx.min()

        rval['mean_x_max_u'] = mean.max()
        rval['mean_x_mean_u'] = mean.mean()
        rval['mean_x_min_u'] = mean.min()

        rval['min_x_max_u'] = mn.max()
        rval['min_x_mean_u'] = mn.mean()
        rval['min_x_min_u'] = mn.min()

        return rval


    @wraps(Layer.get_layer_monitoring_channels)
    def get_layer_monitoring_channels(self, state_below=None,
                                    state=None, targets=None):
        W, = self.transformer.get_params()

        assert W.ndim == 2

        sq_W = T.sqr(W)

        row_norms = T.sqrt(sq_W.sum(axis=1))
        col_norms = T.sqrt(sq_W.sum(axis=0))

        rval = OrderedDict([('row_norms_min',  row_norms.min()),
                            ('row_norms_mean', row_norms.mean()),
                            ('row_norms_max',  row_norms.max()),
                            ('col_norms_min',  col_norms.min()),
                            ('col_norms_mean', col_norms.mean()),
                            ('col_norms_max',  col_norms.max()), ])

        if (state is not None) or (state_below is not None):
            if state is None:
                state = self.fprop(state_below)

            mx = state.max(axis=0)
            mean = state.mean(axis=0)
            mn = state.min(axis=0)
            rg = mx - mn

            rval['range_x_max_u'] = rg.max()
            rval['range_x_mean_u'] = rg.mean()
            rval['range_x_min_u'] = rg.min()

            rval['max_x_max_u'] = mx.max()
            rval['max_x_mean_u'] = mx.mean()
            rval['max_x_min_u'] = mx.min()

            rval['mean_x_max_u'] = mean.max()
            rval['mean_x_mean_u'] = mean.mean()
            rval['mean_x_min_u'] = mean.min()

            rval['min_x_max_u'] = mn.max()
            rval['min_x_mean_u'] = mn.mean()
            rval['min_x_min_u'] = mn.min()

        return rval

    def _linear_part(self, state_below):
        """
        Parameters
        ----------
        state_below : member of input_space

        Returns
        -------
        output : theano matrix
            Affine transformation of state_below
        """
        self.input_space.validate(state_below)

        if self.requires_reformat:
            state_below = self.input_space.format_as(state_below,
                                                     self.desired_space)

        # Support old pickle files
        if not hasattr(self, 'softmax_columns'):
            self.softmax_columns = False

        if self.softmax_columns:
            W, = self.transformer.get_params()
            W = W.T
            W = T.nnet.softmax(W)
            W = W.T
            z = T.dot(state_below, W)
            if self.use_bias:
                z += self.b
        else:
            z = self.transformer.lmul(state_below)
            if self.use_bias:
                z += self.b

        if self.layer_name is not None:
            z.name = self.layer_name + '_z'

        return z

    @wraps(Layer.fprop)
    def fprop(self, state_below):
        p = self._linear_part(state_below)
        return p

    @wraps(Layer.cost)
    def cost(self, Y, Y_hat):

        return self.cost_from_cost_matrix(self.cost_matrix(Y, Y_hat))

    @wraps(Layer.cost_from_cost_matrix)
    def cost_from_cost_matrix(self, cost_matrix):

        return cost_matrix.sum(axis=1).mean()

    @wraps(Layer.cost_matrix)
    def cost_matrix(self, Y, Y_hat):

        if(self.use_abs_loss):
            return T.abs_(Y - Y_hat)
        else:
            return T.sqr(Y - Y_hat)


class Tanh(Linear):
    """
    A layer that performs an affine transformation of its (vectorial)
    input followed by a hyperbolic tangent elementwise nonlinearity.

    Parameters
    ----------
    kwargs : dict
        Keyword arguments to pass through to `Linear` class constructor.
    """

    @wraps(Layer.fprop)
    def fprop(self, state_below):

        p = self._linear_part(state_below)
        p = T.tanh(p)
        return p

    @wraps(Layer.cost)
    def cost(self, *args, **kwargs):

        raise NotImplementedError()


class Sigmoid(Linear):
    """
    A layer that performs an affine transformation of its (vectorial)
    input followed by a logistic sigmoid elementwise nonlinearity.

    .. todo::

        WRITEME properly

    Parameters
    ----------
    monitor_style : string
        Values can be either 'detection' or 'classification'.
        'detection' is the default.

          - 'detection' : get_monitor_from_state makes no assumptions about
            target, reports info about how good model is at
            detecting positive bits.
            This will monitor precision, recall, and F1 score
            based on a detection threshold of 0.5. Note that
            these quantities are computed *per-minibatch* and
            averaged together. Unless your entire monitoring
            dataset fits in one minibatch, this is not the same
            as the true F1 score, etc., and will usually
            seriously overestimate your performance.
          - 'classification' : get_monitor_from_state assumes target is one-hot
            class indicator, even though you're training the
            model as k independent sigmoids. gives info on how
            good the argmax is as a classifier
    kwargs : dict
        WRITEME
    """

    def __init__(self, monitor_style='detection', **kwargs):
        super(Sigmoid, self).__init__(**kwargs)
        assert monitor_style in ['classification', 'detection']
        self.monitor_style = monitor_style

    @wraps(Layer.fprop)
    def fprop(self, state_below):

        p = self._linear_part(state_below)
        p = T.nnet.sigmoid(p)
        return p

    @wraps(Layer.cost)
    def cost(self, Y, Y_hat):
        """
        Returns a batch (vector) of
        mean across units of KL divergence for each example.

        Parameters
        ----------
        Y : theano.gof.Variable
            Targets
        Y_hat : theano.gof.Variable
            Output of `fprop`

        mean across units, mean across batch of KL divergence
        Notes
        -----
        Uses KL(P || Q) where P is defined by Y and Q is defined by Y_hat
        Currently Y must be purely binary. If it's not, you'll still
        get the right gradient, but the value in the monitoring channel
        will be wrong.
        Y_hat must be generated by fprop, i.e., it must be a symbolic
        sigmoid.

        p log p - p log q + (1-p) log (1-p) - (1-p) log (1-q)
        For binary p, some terms drop out:
        - p log q - (1-p) log (1-q)
        - p log sigmoid(z) - (1-p) log sigmoid(-z)
        p softplus(-z) + (1-p) softplus(z)
        """
        total = self.kl(Y=Y, Y_hat=Y_hat)

        ave = total.mean()

        return ave

    def kl(self, Y, Y_hat):
        """
        Computes the KL divergence.


        Parameters
        ----------
        Y : Variable
            targets for the sigmoid outputs. Currently Y must be purely binary.
            If it's not, you'll still get the right gradient, but the
            value in the monitoring channel will be wrong.
        Y_hat : Variable
            predictions made by the sigmoid layer. Y_hat must be generated by
            fprop, i.e., it must be a symbolic sigmoid.
        batch_axis : list
            list of axes to compute average kl divergence across.

        Returns
        -------
        ave : Variable
            average kl divergence between Y and Y_hat.

        Notes
        -----
        Warning: This function expects a sigmoid nonlinearity in the
        output layer and it uses kl function under pylearn2/expr/nnet/.
        Returns a batch (vector) of mean across units of KL
        divergence for each example,
        KL(P || Q) where P is defined by Y and Q is defined by Y_hat:

        p log p - p log q + (1-p) log (1-p) - (1-p) log (1-q)
        For binary p, some terms drop out:
        - p log q - (1-p) log (1-q)
        - p log sigmoid(z) - (1-p) log sigmoid(-z)
        p softplus(-z) + (1-p) softplus(z)
        """
        batch_axis = self.output_space.get_batch_axis()
        div = kl(Y=Y, Y_hat=Y_hat, batch_axis=batch_axis)
        return div

    def get_detection_channels_from_state(self, state, target):
        """
        Returns monitoring channels when using the layer to do detection
        of binary events.

        Parameters
        ----------
        state : theano.gof.Variable
            Output of `fprop`
        target : theano.gof.Variable
            The targets from the dataset

        Returns
        -------
        channels : OrderedDict
            Dictionary mapping channel names to Theano channel values.
        """

        rval = OrderedDict()
        y_hat = state > 0.5
        y = target > 0.5
        wrong_bit = T.cast(T.neq(y, y_hat), state.dtype)
        rval['01_loss'] = wrong_bit.mean()
        rval['kl'] = self.cost(Y_hat=state, Y=target)

        y = T.cast(y, state.dtype)
        y_hat = T.cast(y_hat, state.dtype)
        tp = (y * y_hat).sum()
        fp = ((1-y) * y_hat).sum()

        precision = compute_precision(tp, fp)
        recall = compute_recall(y, fp)
        f1 = compute_f1(precision, recall)

        rval['precision'] = precision
        rval['recall'] = recall
        rval['f1'] = f1

        tp = (y * y_hat).sum(axis=0)
        fp = ((1-y) * y_hat).sum(axis=0)

        precision = compute_precision(tp, fp)

        rval['per_output_precision_max'] = precision.max()
        rval['per_output_precision_mean'] = precision.mean()
        rval['per_output_precision_min'] = precision.min()

        recall = compute_recall(y, tp)

        rval['per_output_recall_max'] = recall.max()
        rval['per_output_recall_mean'] = recall.mean()
        rval['per_output_recall_min'] = recall.min()

        f1 = compute_f1(precision, recall)

        rval['per_output_f1_max'] = f1.max()
        rval['per_output_f1_mean'] = f1.mean()
        rval['per_output_f1_min'] = f1.min()

        return rval

    @wraps(Layer.get_monitoring_channels_from_state)
    def get_monitoring_channels_from_state(self, state, target=None):
        warnings.warn("Layer.get_monitoring_channels_from_state is " + \
                    "deprecated. Use get_layer_monitoring_channels " + \
                    "instead. Layer.get_monitoring_channels_from_state " + \
                    "will be removed on or after september 24th 2014",
                    stacklevel=2)

        rval = super(Sigmoid, self).get_monitoring_channels_from_state(state,
                                                                       target)

        if target is not None:
            if self.monitor_style == 'detection':
                rval.update(self.get_detection_channels_from_state(state,
                                                                   target))
            else:
                assert self.monitor_style == 'classification'
                # Threshold Y_hat at 0.5.
                prediction = T.gt(state, 0.5)
                # If even one feature is wrong for a given training example,
                # it's considered incorrect, so we max over columns.
                incorrect = T.neq(target, prediction).max(axis=1)
                rval['misclass'] = T.cast(incorrect, config.floatX).mean()

        return rval

    @wraps(Layer.get_layer_monitoring_channels)
    def get_layer_monitoring_channels(self, state_below=None,
                                    state=None, targets=None):

        rval = super(Sigmoid, self).get_layer_monitoring_channels(state=state,
                                                        targets=targets)

        if (targets is not None) and \
                ((state_below is not None) or (state is not None)):
            if state is None:
                state = self.fprop(state_below)
            if self.monitor_style == 'detection':
                rval.update(self.get_detection_channels_from_state(state,
                                                                   targets))
            else:
                assert self.monitor_style == 'classification'
                # Threshold Y_hat at 0.5.
                prediction = T.gt(state, 0.5)
                # If even one feature is wrong for a given training example,
                # it's considered incorrect, so we max over columns.
                incorrect = T.neq(targets, prediction).max(axis=1)
                rval['misclass'] = T.cast(incorrect, config.floatX).mean()
        return rval


class RectifiedLinear(Linear):
    """
    Rectified linear MLP layer (Glorot and Bengio 2011).

    Parameters
    ----------
    left_slope : float
        The slope the line should have left of 0.
    kwargs : dict
        Keyword arguments to pass to `Linear` class constructor.
    """

    def __init__(self, left_slope=0.0, **kwargs):
        super(RectifiedLinear, self).__init__(**kwargs)
        self.left_slope = left_slope

    @wraps(Layer.fprop)
    def fprop(self, state_below):

        p = self._linear_part(state_below)
        # Original: p = p * (p > 0.) + self.left_slope * p * (p < 0.)
        # T.switch is faster.
        # For details, see benchmarks in
        # pylearn2/scripts/benchmark/time_relu.py
        p = T.switch(p > 0., p, self.left_slope * p)
        return p

    @wraps(Layer.cost)
    def cost(self, *args, **kwargs):

        raise NotImplementedError()


class Softplus(Linear):
    """
    An MLP layer using the softplus nonlinearity
    h = log(1 + exp(Wx + b))

    Parameters
    ----------
    kwargs : dict
        Keyword arguments to `Linear` constructor.
    """

    def __init__(self, **kwargs):
        super(Softplus, self).__init__(**kwargs)

    @wraps(Layer.fprop)
    def fprop(self, state_below):

        p = self._linear_part(state_below)
        p = T.nnet.softplus(p)
        return p

    @wraps(Layer.cost)
    def cost(self, *args, **kwargs):

        raise NotImplementedError()


class SpaceConverter(Layer):
    """
    A Layer with no parameters that converts the input from
    one space to another.

    Parameters
    ----------
    layer_name : str
        Name of the layer.
    output_space : Space
        The space to convert to.
    """

    def __init__(self, layer_name, output_space):
        super(SpaceConverter, self).__init__()
        self.__dict__.update(locals())
        del self.self
        self._params = []

    @wraps(Layer.set_input_space)
    def set_input_space(self, space):

        self.input_space = space

    @wraps(Layer.fprop)
    def fprop(self, state_below):

        return self.input_space.format_as(state_below, self.output_space)


class ConvNonlinearity(object):
    """
    Abstract convolutional nonlinearity class.
    """
    def apply(self, linear_response):
        """
        Applies the nonlinearity over the convolutional layer.

        Parameters
        ----------
        linear_response: Variable
            linear response of the layer.

        Returns
        -------
        p: Variable
            the response of the layer after the activation function
            is applied over.
        """
        p = linear_response
        return p

    def _get_monitoring_channels_for_activations(self, state):
        """
        Computes the monitoring channels which does not require targets.

        Parameters
        ----------
        state : member of self.output_space
            A minibatch of states that this Layer took on during fprop.
            Provided externally so that we don't need to make a second
            expression for it. This helps keep the Theano graph smaller
            so that function compilation runs faster.

        Returns
        -------
        rval : OrderedDict
            A dictionary mapping channel names to monitoring channels of
            interest for this layer.
        """
        rval = OrderedDict({})

        mx = state.max(axis=0)
        mean = state.mean(axis=0)
        mn = state.min(axis=0)
        rg = mx - mn

        rval['range_x_max_u'] = rg.max()
        rval['range_x_mean_u'] = rg.mean()
        rval['range_x_min_u'] = rg.min()

        rval['max_x_max_u'] = mx.max()
        rval['max_x_mean_u'] = mx.mean()
        rval['max_x_min_u'] = mx.min()

        rval['mean_x_max_u'] = mean.max()
        rval['mean_x_mean_u'] = mean.mean()
        rval['mean_x_min_u'] = mean.min()

        rval['min_x_max_u'] = mn.max()
        rval['min_x_mean_u'] = mn.mean()
        rval['min_x_min_u'] = mn.min()

        return rval

    def get_monitoring_channels_from_state(self, state, target,
                                           cost_fn=None):
        """
        Override the default get_monitoring_channels_from_state function.

        Parameters
        ----------
        state : member of self.output_space
            A minibatch of states that this Layer took on during fprop.
            Provided externally so that we don't need to make a second
            expression for it. This helps keep the Theano graph smaller
            so that function compilation runs faster.
        target : member of self.output_space
            Should be None unless this is the last layer.
            If specified, it should be a minibatch of targets for the
            last layer.
        cost_fn : theano computational graph or None
            This is the theano computational graph of a cost function.

        Returns
        -------
        rval : OrderedDict
            A dictionary mapping channel names to monitoring channels of
            interest for this layer.
        """

        rval = self._get_monitoring_channels_for_activations(state)

        return rval


class IdentityConvNonlinearity(ConvNonlinearity):
    """
    Linear convolutional nonlinearity class.
    """
    def __init__(self):
        self.non_lin_name = "linear"

    @wraps(ConvNonlinearity.get_monitoring_channels_from_state)
    def get_monitoring_channels_from_state(self,
                                           state,
                                           target,
                                           cost_fn=False):

        rval = super(IdentityConvNonlinearity,
                     self).get_monitoring_channels_from_state(state,
                                                              target,
                                                              cost_fn)

        if target is not None:
            prediction = T.gt(state, 0.5)
            incorrect = T.new(target, prediction).max(axis=1)
            rval["misclass"] = T.cast(incorrect, config.floatX).mean()

        return rval


class RectifierConvNonlinearity(ConvNonlinearity):
    """
    A simple rectifier nonlinearity class for convolutional layers.

    Parameters
    ----------
    left_slope : float
        The slope of the left half of the activation function.
    """
    def __init__(self, left_slope=0.0):
        """
        Parameters
        ----------
        left_slope : float, optional
            left slope for the linear response of the rectifier function.
            default is 0.0.
        """
        self.non_lin_name = "rectifier"
        self.left_slope = left_slope

    @wraps(ConvNonlinearity.apply)
    def apply(self, linear_response):
        """
        Applies the rectifier nonlinearity over the convolutional layer.
        """
        p = linear_response * (linear_response > 0.) + self.left_slope *\
            linear_response * (linear_response < 0.)
        return p


class SigmoidConvNonlinearity(ConvNonlinearity):
    """
    Sigmoid nonlinearity class for convolutional layers.

    Parameters
    ----------
    monitor_style : str, optional
        default monitor_style is "classification".
        This determines whether to do classification or detection.
    """

    def __init__(self, monitor_style="classification"):
        assert monitor_style in ['classification', 'detection']
        self.monitor_style = monitor_style
        self.non_lin_name = "sigmoid"

    @wraps(ConvNonlinearity.apply)
    def apply(self, linear_response):
        """
        Applies the sigmoid nonlinearity over the convolutional layer.
        """
        rval = OrderedDict()
        p = T.nnet.sigmoid(linear_response)
        return p

    @wraps(ConvNonlinearity.get_monitoring_channels_from_state)
    def get_monitoring_channels_from_state(self, state, target,
                                           cost_fn=None):

        rval = super(SigmoidConvNonlinearity,
                     self).get_monitoring_channels_from_state(state,
                                                              target,
                                                              cost_fn)

        if target is not None:
            y_hat = state > 0.5
            y = target > 0.5

            wrong_bit = T.cast(T.neq(y, y_hat), state.dtype)

            rval['01_loss'] = wrong_bit.mean()
            rval['kl'] = cost_fn(Y_hat=state, Y=target)

            y = T.cast(y, state.dtype)
            y_hat = T.cast(y_hat, state.dtype)
            tp = (y * y_hat).sum()
            fp = ((1-y) * y_hat).sum()

            precision = compute_precision(tp, fp)
            recall = compute_recall(y, tp)
            f1 = compute_f1(precision, recall)

            rval['precision'] = precision
            rval['recall'] = recall
            rval['f1'] = f1

            tp = (y * y_hat).sum(axis=[0, 1])
            fp = ((1-y) * y_hat).sum(axis=[0, 1])

            precision = compute_precision(tp, fp)

            rval['per_output_precision_max'] = precision.max()
            rval['per_output_precision_mean'] = precision.mean()
            rval['per_output_precision_min'] = precision.min()

            recall = compute_recall(y, tp)

            rval['per_output_recall_max'] = recall.max()
            rval['per_output_recall_mean'] = recall.mean()
            rval['per_output_recall_min'] = recall.min()

            f1 = compute_f1(precision, recall)

            rval['per_output_f1_max'] = f1.max()
            rval['per_output_f1_mean'] = f1.mean()
            rval['per_output_f1_min'] = f1.min()

        return rval


class TanhConvNonlinearity(ConvNonlinearity):
    """
    Tanh nonlinearity class for convolutional layers.
    """
    def __init__(self):
        self.non_lin_name = "tanh"

    @wraps(ConvNonlinearity.apply)
    def apply(self, linear_response):
        """
        Applies the tanh nonlinearity over the convolutional layer.
        """
        p = T.tanh(linear_response)
        return p


class ConvElemwise(Layer):
    """
    Generic convolutional elemwise layer.
    Takes the ConvNonlinearity object as an argument and implements
    convolutional layer with the specified nonlinearity.

    This function can implement:

    * Linear convolutional layer
    * Rectifier convolutional layer
    * Sigmoid convolutional layer
    * Tanh convolutional layer

    based on the nonlinearity argument that it recieves.

    Parameters
    ----------
    output_channels : int
        The number of output channels the layer should have.
    kernel_shape : tuple
        The shape of the convolution kernel.
    pool_shape : tuple
        The shape of the spatial max pooling. A two-tuple of ints.
    pool_stride : tuple
        The stride of the spatial max pooling. Also must be square.
    layer_name : str
        A name for this layer that will be prepended to monitoring channels
        related to this layer.
    nonlinearity : object
        An instance of a nonlinearity object which might be inherited
        from the ConvNonlinearity class.
    irange : float, optional
        if specified, initializes each weight randomly in
        U(-irange, irange)
    border_mode : str, optional
        A string indicating the size of the output:

          - "full" : The output is the full discrete linear convolution of the
            inputs.
          - "valid" : The output consists only of those elements that do not
            rely on the zero-padding. (Default)
    sparse_init : WRITEME
    include_prob : float, optional
        probability of including a weight element in the set of weights
        initialized to U(-irange, irange). If not included it is initialized
        to 1.0.
    init_bias : float, optional
        All biases are initialized to this number. Default is 0.
    W_lr_scale : float or None
        The learning rate on the weights for this layer is multiplied by this
        scaling factor
    b_lr_scale : float or None
        The learning rate on the biases for this layer is multiplied by this
        scaling factor
    max_kernel_norm : float or None
        If specified, each kernel is constrained to have at most this norm.
    pool_type : str or None
        The type of the pooling operation performed the convolution.
        Default pooling type is max-pooling.
    tied_b : bool, optional
        If true, all biases in the same channel are constrained to be the
        same as each other. Otherwise, each bias at each location is
        learned independently. Default is true.
    detector_normalization : callable or None
        See `output_normalization`.
        If pooling argument is not provided, detector_normalization
        is not applied on the layer.
    output_normalization : callable  or None
        if specified, should be a callable object. the state of the
        network is optionally replaced with normalization(state) at each
        of the 3 points in processing:

          - detector: the maxout units can be normalized prior to the
            spatial pooling
          - output: the output of the layer, after sptial pooling, can
            be normalized as well
    kernel_stride : 2-tuple of ints, optional
        The stride of the convolution kernel. Default is (1, 1).
    """
    def __init__(self,
                 output_channels,
                 kernel_shape,
                 layer_name,
                 nonlinearity,
                 irange=None,
                 border_mode='valid',
                 sparse_init=None,
                 include_prob=1.0,
                 init_bias=0.,
                 W_lr_scale=None,
                 b_lr_scale=None,
                 max_kernel_norm=None,
                 pool_type=None,
                 pool_shape=None,
                 pool_stride=None,
                 tied_b=None,
                 detector_normalization=None,
                 output_normalization=None,
                 kernel_stride=(1, 1),
                 monitor_style="classification"):

        if (irange is None) and (sparse_init is None):
            raise AssertionError("You should specify either irange or "
                                 "sparse_init when calling the constructor of "
                                 "ConvElemwise.")
        elif (irange is not None) and (sparse_init is not None):
            raise AssertionError("You should specify either irange or "
                                 "sparse_init when calling the constructor of "
                                 "ConvElemwise and not both.")

        if pool_type is not None:
            assert pool_shape is not None, ("You should specify the shape of "
                                           "the spatial %s-pooling." % pool_type)
            assert pool_stride is not None, ("You should specify the strides of "
                                            "the spatial %s-pooling." % pool_type)

        assert nonlinearity is not None

        self.nonlin = nonlinearity
        self.__dict__.update(locals())
        assert monitor_style in ['classification',
                            'detection'], ("%s.monitor_style"
                            "should be either detection or classification"
                            % self.__class__.__name__)
        del self.self

    def initialize_transformer(self, rng):
        """
        This function initializes the transformer of the class. Re-running
        this function will reset the transformer.

        Parameters
        ----------
        rng : object
            random number generator object.
        """
        if self.irange is not None:
            assert self.sparse_init is None
            self.transformer = conv2d.make_random_conv2D(
                    irange=self.irange,
                    input_space=self.input_space,
                    output_space=self.detector_space,
                    kernel_shape=self.kernel_shape,
                    subsample=self.kernel_stride,
                    border_mode=self.border_mode,
                    rng=rng)
        elif self.sparse_init is not None:
            self.transformer = conv2d.make_sparse_random_conv2D(
                    num_nonzero=self.sparse_init,
                    input_space=self.input_space,
                    output_space=self.detector_space,
                    kernel_shape=self.kernel_shape,
                    subsample=self.kernel_stride,
                    border_mode=self.border_mode,
                    rng=rng)

    def initialize_output_space(self):
        """
        Initializes the output space of the ConvElemwise layer by taking
        pooling operator and the hyperparameters of the convolutional layer
        into consideration as well.
        """
        dummy_batch_size = self.mlp.batch_size

        if dummy_batch_size is None:
            dummy_batch_size = 2
        dummy_detector =\
                sharedX(self.detector_space.get_origin_batch(dummy_batch_size))

        if self.pool_type is not None:
            assert self.pool_type in ['max', 'mean']
            if self.pool_type == 'max':
                dummy_p = max_pool(bc01=dummy_detector,
                                   pool_shape=self.pool_shape,
                                   pool_stride=self.pool_stride,
                                   image_shape=self.detector_space.shape)
            elif self.pool_type == 'mean':
                dummy_p = mean_pool(bc01=dummy_detector,
                                    pool_shape=self.pool_shape,
                                    pool_stride=self.pool_stride,
                                    image_shape=self.detector_space.shape)
            dummy_p = dummy_p.eval()
            self.output_space = Conv2DSpace(shape=[dummy_p.shape[2],
                                                   dummy_p.shape[3]],
                                            num_channels=
                                                self.output_channels,
                                            axes=('b', 'c', 0, 1))
        else:
            dummy_detector = dummy_detector.eval()
            self.output_space = Conv2DSpace(shape=[dummy_detector.shape[2],
                                            dummy_detector.shape[3]],
                                            num_channels=self.output_channels,
                                            axes=('b', 'c', 0, 1))

        logger.info('Output space: {0}'.format(self.output_space.shape))

    @wraps(Layer.set_input_space)
    def set_input_space(self, space):
        """ Note: this function will reset the parameters! """

        self.input_space = space

        if not isinstance(space, Conv2DSpace):
            raise BadInputSpaceError(self.__class__.__name__ +
                                     ".set_input_space "
                                     "expected a Conv2DSpace, got " +
                                     str(space) + " of type " +
                                     str(type(space)))

        rng = self.mlp.rng

        if self.border_mode == 'valid':
            output_shape = [(self.input_space.shape[0] - self.kernel_shape[0])
                            / self.kernel_stride[0] + 1,
                            (self.input_space.shape[1] - self.kernel_shape[1])
                            / self.kernel_stride[1] + 1]
        elif self.border_mode == 'full':
            output_shape = [(self.input_space.shape[0] + self.kernel_shape[0])
                            / self.kernel_stride[0] - 1,
                            (self.input_space.shape[1] + self.kernel_shape[1])
                            / self.kernel_stride_stride[1] - 1]

        self.detector_space = Conv2DSpace(shape=output_shape,
                                          num_channels=self.output_channels,
                                          axes=('b', 'c', 0, 1))

        self.initialize_transformer(rng)

        W, = self.transformer.get_params()
        W.name = self.layer_name + '_W'

        if self.tied_b:
            self.b = sharedX(np.zeros((self.detector_space.num_channels)) +
                             self.init_bias)
        else:
            self.b = sharedX(self.detector_space.get_origin() + self.init_bias)

        self.b.name = self.layer_name + '_b'

        logger.info('Input shape: {0}'.format(self.input_space.shape))
        logger.info('Detector space: {0}'.format(self.detector_space.shape))

        self.initialize_output_space()


    @wraps(Layer._modify_updates)
    def _modify_updates(self, updates):
        if self.max_kernel_norm is not None:
            W, = self.transformer.get_params()
            if W in updates:
                updated_W = updates[W]
                row_norms = T.sqrt(T.sum(T.sqr(updated_W), axis=(1, 2, 3)))
                desired_norms = T.clip(row_norms, 0, self.max_kernel_norm)
                updates[W] = updated_W * (desired_norms /
                        (1e-7 + row_norms)).dimshuffle(0, 'x', 'x', 'x')

    @wraps(Layer.get_params)
    def get_params(self):
        assert self.b.name is not None
        W, = self.transformer.get_params()
        assert W.name is not None
        rval = self.transformer.get_params()
        assert not isinstance(rval, set)
        rval = list(rval)
        assert self.b not in rval
        rval.append(self.b)
        return rval

    @wraps(Layer.get_weight_decay)
    def get_weight_decay(self, coeff):

        if isinstance(coeff, str):
            coeff = float(coeff)
        assert isinstance(coeff, float) or hasattr(coeff, 'dtype')
        W, = self.transformer.get_params()
        return coeff * T.sqr(W).sum()

    @wraps(Layer.get_l1_weight_decay)
    def get_l1_weight_decay(self, coeff):

        if isinstance(coeff, str):
            coeff = float(coeff)
        assert isinstance(coeff, float) or hasattr(coeff, 'dtype')
        W, = self.transformer.get_params()
        return coeff * abs(W).sum()

    @wraps(Layer.set_weights)
    def set_weights(self, weights):

        W, = self.transformer.get_params()
        W.set_value(weights)

    @wraps(Layer.set_biases)
    def set_biases(self, biases):

        self.b.set_value(biases)

    @wraps(Layer.get_biases)
    def get_biases(self):

        return self.b.get_value()

    @wraps(Layer.get_weights_format)
    def get_weights_format(self):

        return ('v', 'h')

    @wraps(Layer.get_lr_scalers)
    def get_lr_scalers(self):
        if not hasattr(self, 'W_lr_scale'):
            self.W_lr_scale = None

        if not hasattr(self, 'b_lr_scale'):
            self.b_lr_scale = None

        rval = OrderedDict()

        if self.W_lr_scale is not None:
            W, = self.transformer.get_params()
            rval[W] = self.W_lr_scale

        if self.b_lr_scale is not None:
            rval[self.b] = self.b_lr_scale

        return rval

    @wraps(Layer.get_weights_topo)
    def get_weights_topo(self):

        outp, inp, rows, cols = range(4)
        raw = self.transformer._filters.get_value()

        return np.transpose(raw, (outp, rows, cols, inp))


    @wraps(Layer.get_monitoring_channels_from_state)
    def get_monitoring_channels_from_state(self, state, target=None):

        rval = super(ConvElemwise, self).get_monitoring_channels_from_state(state,
                                                                            target)

        cst = self.cost
        orval = self.nonlin.get_monitoring_channels_from_state(state,
                                                               target,
                                                               cost_fn=cst)

        rval.update(orval)

        return rval

    @wraps(Layer.get_monitoring_channels)
    def get_monitoring_channels(self):
        warnings.warn("Layer.get_monitoring_channels is deprecated. " + \
                    "Use get_layer_monitoring_channels instead. " + \
                    "Layer.get_monitoring_channels will be removed " + \
                    "on or after september 24th 2014", stacklevel=2)

        W, = self.transformer.get_params()

        assert W.ndim == 4

        sq_W = T.sqr(W)

        row_norms = T.sqrt(sq_W.sum(axis=(1, 2, 3)))

        return OrderedDict([('kernel_norms_min',  row_norms.min()),
                            ('kernel_norms_mean', row_norms.mean()),
                            ('kernel_norms_max',  row_norms.max()), ])

    @wraps(Layer.get_layer_monitoring_channels)
    def get_layer_monitoring_channels(self, state_below=None,
                                    state=None, targets=None):

        W, = self.transformer.get_params()

        assert W.ndim == 4

        sq_W = T.sqr(W)

        row_norms = T.sqrt(sq_W.sum(axis=(1, 2, 3)))

        return OrderedDict([
                           ('kernel_norms_min', row_norms.min()),
                           ('kernel_norms_mean', row_norms.mean()),
                           ('kernel_norms_max', row_norms.max()),
                           ])

    @wraps(Layer.fprop)
    def fprop(self, state_below):

        self.input_space.validate(state_below)

        z = self.transformer.lmul(state_below)
        if not hasattr(self, 'tied_b'):
            self.tied_b = False

        if self.tied_b:
            b = self.b.dimshuffle('x', 0, 'x', 'x')
        else:
            b = self.b.dimshuffle('x', 0, 1, 2)

        z = z + b
        d = self.nonlin.apply(z)

        if self.layer_name is not None:
            d.name = self.layer_name + '_z'
            self.detector_space.validate(d)

        if self.pool_type is not None:
            if not hasattr(self, 'detector_normalization'):
                self.detector_normalization = None

            if self.detector_normalization:
                d = self.detector_normalization(d)

            assert self.pool_type in ['max', 'mean'], ("pool_type should be"
                                                      "either max or mean"
                                                      "pooling.")

            if self.pool_type == 'max':
                p = max_pool(bc01=d, pool_shape=self.pool_shape,
                        pool_stride=self.pool_stride,
                        image_shape=self.detector_space.shape)
            elif self.pool_type == 'mean':
                p = mean_pool(bc01=d, pool_shape=self.pool_shape,
                        pool_stride=self.pool_stride,
                        image_shape=self.detector_space.shape)

            self.output_space.validate(p)
        else:
            p = d

        if not hasattr(self, 'output_normalization'):
           self.output_normalization = None

        if self.output_normalization:
           p = self.output_normalization(p)

        return p

    def cost(self, Y, Y_hat):
        """
        Cost for convnets is hardcoded to be the cost for sigmoids.
        TODO: move the cost into the non-linearity class.

        Parameters
        ----------
        Y : theano.gof.Variable
            Output of `fprop`
        Y_hat : theano.gof.Variable
            Targets

        Returns
        -------
        cost : theano.gof.Variable
            0-D tensor describing the cost

        Notes
        -----
        Cost mean across units, mean across batch of KL divergence
        KL(P || Q) where P is defined by Y and Q is defined by Y_hat
        KL(P || Q) = p log p - p log q + (1-p) log (1-p) - (1-p) log (1-q)
        """
        assert self.nonlin.non_lin_name == "sigmoid", ("ConvElemwise "
                                                       "supports "
                                                       "cost function "
                                                       "for only "
                                                       "sigmoid layer "
                                                       "for now.")
        batch_axis = self.output_space.get_batch_axis()
        ave_total = kl(Y=Y, Y_hat=Y_hat, batch_axis=batch_axis)
        ave = ave_total.mean()
        return ave


class ConvRectifiedLinear(ConvElemwise):
    """
    A convolutional rectified linear layer, based on theano's B01C
    formatted convolution.

    Parameters
    ----------
    output_channels : int
        The number of output channels the layer should have.
    kernel_shape : tuple
        The shape of the convolution kernel.
    pool_shape : tuple
        The shape of the spatial max pooling. A two-tuple of ints.
    pool_stride : tuple
        The stride of the spatial max pooling. Also must be square.
    layer_name : str
        A name for this layer that will be prepended to monitoring channels
        related to this layer.
    irange : float
        if specified, initializes each weight randomly in
        U(-irange, irange)
    border_mode : str
        A string indicating the size of the output:

        - "full" : The output is the full discrete linear convolution of the
            inputs.
        - "valid" : The output consists only of those elements that do not
            rely on the zero-padding. (Default)

    include_prob : float
        probability of including a weight element in the set of weights
        initialized to U(-irange, irange). If not included it is initialized
        to 0.
    init_bias : float
        All biases are initialized to this number
    W_lr_scale : float
        The learning rate on the weights for this layer is multiplied by this
        scaling factor
    b_lr_scale : float
        The learning rate on the biases for this layer is multiplied by this
        scaling factor
    left_slope : float
        The slope of the left half of the activation function
    max_kernel_norm : float
        If specifed, each kernel is constrained to have at most this norm.
    pool_type :
        The type of the pooling operation performed the the convolution.
        Default pooling type is max-pooling.
    tied_b : bool
        If true, all biases in the same channel are constrained to be the
        same as each other. Otherwise, each bias at each location is
        learned independently.
    detector_normalization : callable
        See `output_normalization`
    output_normalization : callable
        if specified, should be a callable object. the state of the
        network is optionally replaced with normalization(state) at each
        of the 3 points in processing:

        - detector: the rectifier units can be normalized prior to the
            spatial pooling
        - output: the output of the layer, after spatial pooling, can
            be normalized as well

    kernel_stride : tuple
        The stride of the convolution kernel. A two-tuple of ints.
    """
    def __init__(self,
                 output_channels,
                 kernel_shape,
                 pool_shape,
                 pool_stride,
                 layer_name,
                 irange=None,
                 border_mode='valid',
                 sparse_init=None,
                 include_prob=1.0,
                 init_bias=0.,
                 W_lr_scale=None,
                 b_lr_scale=None,
                 left_slope=0.0,
                 max_kernel_norm=None,
                 pool_type='max',
                 tied_b=False,
                 detector_normalization=None,
                 output_normalization=None,
                 kernel_stride=(1, 1),
                 monitor_style="classification"):

        nonlinearity = RectifierConvNonlinearity(left_slope)

        if (irange is None) and (sparse_init is None):
            raise AssertionError("You should specify either irange or "
                                 "sparse_init when calling the constructor of "
                                 "ConvRectifiedLinear.")
        elif (irange is not None) and (sparse_init is not None):
            raise AssertionError("You should specify either irange or "
                                 "sparse_init when calling the constructor of "
                                 "ConvRectifiedLinear and not both.")

        #Alias the variables for pep8
        mkn = max_kernel_norm
        dn = detector_normalization
        on = output_normalization

        super(ConvRectifiedLinear, self).__init__(output_channels,
                                                  kernel_shape,
                                                  layer_name,
                                                  nonlinearity,
                                                  irange=irange,
                                                  border_mode=border_mode,
                                                  sparse_init=sparse_init,
                                                  include_prob=include_prob,
                                                  init_bias=init_bias,
                                                  W_lr_scale=W_lr_scale,
                                                  b_lr_scale=b_lr_scale,
                                                  pool_shape=pool_shape,
                                                  pool_stride=pool_stride,
                                                  max_kernel_norm=mkn,
                                                  pool_type=pool_type,
                                                  tied_b=tied_b,
                                                  detector_normalization=dn,
                                                  output_normalization=on,
                                                  kernel_stride=kernel_stride,
                                                  monitor_style=monitor_style)


def max_pool(bc01, pool_shape, pool_stride, image_shape):
    """
    Theano's max pooling op only supports pool_stride = pool_shape
    so here we have a graph that does max pooling with strides

    Parameters
    ----------
    bc01 : theano tensor
        minibatch in format (batch size, channels, rows, cols)
    pool_shape : tuple
        shape of the pool region (rows, cols)
    pool_stride : tuple
        strides between pooling regions (row stride, col stride)
    image_shape : tuple
        avoid doing some of the arithmetic in theano

    Returns
    -------
    pooled : theano tensor
        The output of pooling applied to `bc01`

    See Also
    --------
    max_pool_c01b : Same functionality but with ('c', 0, 1, 'b') axes
    sandbox.cuda_convnet.pool.max_pool_c01b : Same functionality as
        `max_pool_c01b` but GPU-only and considerably faster.
    mean_pool : Mean pooling instead of max pooling
    """
    mx = None
    r, c = image_shape
    pr, pc = pool_shape
    rs, cs = pool_stride

    assert pr <= r
    assert pc <= c

    # Compute index in pooled space of last needed pool
    # (needed = each input pixel must appear in at least one pool)
    def last_pool(im_shp, p_shp, p_strd):
        rval = int(np.ceil(float(im_shp - p_shp) / p_strd))
        assert p_strd * rval + p_shp >= im_shp
        assert p_strd * (rval - 1) + p_shp < im_shp
        return rval
    # Compute starting row of the last pool
    last_pool_r = last_pool(image_shape[0],
                            pool_shape[0],
                            pool_stride[0]) * pool_stride[0]
    # Compute number of rows needed in image for all indexes to work out
    required_r = last_pool_r + pr

    last_pool_c = last_pool(image_shape[1],
                            pool_shape[1],
                            pool_stride[1]) * pool_stride[1]
    required_c = last_pool_c + pc

    for bc01v in get_debug_values(bc01):
        assert not np.any(np.isinf(bc01v))
        assert bc01v.shape[2] == image_shape[0]
        assert bc01v.shape[3] == image_shape[1]

    wide_infinity = T.alloc(T.constant(-np.inf, dtype=config.floatX),
                            bc01.shape[0],
                            bc01.shape[1],
                            required_r,
                            required_c)

    name = bc01.name
    if name is None:
        name = 'anon_bc01'

    bc01 = T.set_subtensor(wide_infinity[:, :, 0:r, 0:c], bc01)
    bc01.name = 'infinite_padded_' + name

    for row_within_pool in xrange(pool_shape[0]):
        row_stop = last_pool_r + row_within_pool + 1
        for col_within_pool in xrange(pool_shape[1]):
            col_stop = last_pool_c + col_within_pool + 1
            cur = bc01[:,
                       :,
                       row_within_pool:row_stop:rs,
                       col_within_pool:col_stop:cs]
            cur.name = ('max_pool_cur_' + bc01.name + '_' +
                        str(row_within_pool) + '_' + str(col_within_pool))
            if mx is None:
                mx = cur
            else:
                mx = T.maximum(mx, cur)
                mx.name = ('max_pool_mx_' + bc01.name + '_' +
                           str(row_within_pool) + '_' + str(col_within_pool))

    mx.name = 'max_pool('+name+')'

    for mxv in get_debug_values(mx):
        assert not np.any(np.isnan(mxv))
        assert not np.any(np.isinf(mxv))

    return mx


def max_pool_c01b(c01b, pool_shape, pool_stride, image_shape):
    """
    Theano's max pooling op only supports pool_stride = pool_shape
    so here we have a graph that does max pooling with strides

    Parameters
    ----------
    c01b : theano tensor
        minibatch in format (channels, rows, cols, batch size)
    pool_shape : tuple
        shape of the pool region (rows, cols)
    pool_stride : tuple
        strides between pooling regions (row stride, col stride)
    image_shape : tuple
        avoid doing some of the arithmetic in theano

    Returns
    -------
    pooled : theano tensor
        The output of pooling applied to `c01b`

    See Also
    --------
    sandbox.cuda_convnet.pool.max_pool_c01b : Same functionality but GPU-only
        and considerably faster.
    max_pool : Same functionality but with ('b', 0, 1, 'c') axes
    """
    mx = None
    r, c = image_shape
    pr, pc = pool_shape
    rs, cs = pool_stride
    assert pr > 0
    assert pc > 0
    assert pr <= r
    assert pc <= c

    # Compute index in pooled space of last needed pool
    # (needed = each input pixel must appear in at least one pool)
    def last_pool(im_shp, p_shp, p_strd):
        rval = int(np.ceil(float(im_shp - p_shp) / p_strd))
        assert p_strd * rval + p_shp >= im_shp
        assert p_strd * (rval - 1) + p_shp < im_shp
        return rval
    # Compute starting row of the last pool
    last_pool_r = last_pool(image_shape[0],
                            pool_shape[0],
                            pool_stride[0]) * pool_stride[0]
    # Compute number of rows needed in image for all indexes to work out
    required_r = last_pool_r + pr

    last_pool_c = last_pool(image_shape[1],
                            pool_shape[1],
                            pool_stride[1]) * pool_stride[1]
    required_c = last_pool_c + pc

    for c01bv in get_debug_values(c01b):
        assert not np.any(np.isinf(c01bv))
        assert c01bv.shape[1] == r
        assert c01bv.shape[2] == c

    wide_infinity = T.alloc(-np.inf,
                            c01b.shape[0],
                            required_r,
                            required_c,
                            c01b.shape[3])

    name = c01b.name
    if name is None:
        name = 'anon_bc01'
    c01b = T.set_subtensor(wide_infinity[:, 0:r, 0:c, :], c01b)
    c01b.name = 'infinite_padded_' + name

    for row_within_pool in xrange(pool_shape[0]):
        row_stop = last_pool_r + row_within_pool + 1
        for col_within_pool in xrange(pool_shape[1]):
            col_stop = last_pool_c + col_within_pool + 1
            cur = c01b[:,
                       row_within_pool:row_stop:rs,
                       col_within_pool:col_stop:cs,
                       :]
            cur.name = ('max_pool_cur_' + c01b.name + '_' +
                        str(row_within_pool) + '_' + str(col_within_pool))
            if mx is None:
                mx = cur
            else:
                mx = T.maximum(mx, cur)
                mx.name = ('max_pool_mx_' + c01b.name + '_' +
                           str(row_within_pool)+'_'+str(col_within_pool))

    mx.name = 'max_pool('+name+')'

    for mxv in get_debug_values(mx):
        assert not np.any(np.isnan(mxv))
        assert not np.any(np.isinf(mxv))

    return mx


def mean_pool(bc01, pool_shape, pool_stride, image_shape):
    """
    Does mean pooling (aka average pooling) via a Theano graph.

    Parameters
    ----------
    bc01 : theano tensor
        minibatch in format (batch size, channels, rows, cols)
    pool_shape : tuple
        shape of the pool region (rows, cols)
    pool_stride : tuple
        strides between pooling regions (row stride, col stride)
    image_shape : tuple
        avoid doing some of the arithmetic in theano

    Returns
    -------
    pooled : theano tensor
        The output of pooling applied to `bc01`

    See Also
    --------
    max_pool : Same thing but with max pooling
    """
    mx = None
    r, c = image_shape
    pr, pc = pool_shape
    rs, cs = pool_stride

    # Compute index in pooled space of last needed pool
    # (needed = each input pixel must appear in at least one pool)
    def last_pool(im_shp, p_shp, p_strd):
        rval = int(np.ceil(float(im_shp - p_shp) / p_strd))
        assert p_strd * rval + p_shp >= im_shp
        assert p_strd * (rval - 1) + p_shp < im_shp
        return rval
    # Compute starting row of the last pool
    last_pool_r = last_pool(image_shape[0],
                            pool_shape[0],
                            pool_stride[0]) * pool_stride[0]
    # Compute number of rows needed in image for all indexes to work out
    required_r = last_pool_r + pr

    last_pool_c = last_pool(image_shape[1],
                            pool_shape[1],
                            pool_stride[1]) * pool_stride[1]
    required_c = last_pool_c + pc

    for bc01v in get_debug_values(bc01):
        assert not np.any(np.isinf(bc01v))
        assert bc01v.shape[2] == image_shape[0]
        assert bc01v.shape[3] == image_shape[1]

    wide_infinity = T.alloc(-np.inf,
                            bc01.shape[0],
                            bc01.shape[1],
                            required_r,
                            required_c)

    name = bc01.name
    if name is None:
        name = 'anon_bc01'
    bc01 = T.set_subtensor(wide_infinity[:, :, 0:r, 0:c], bc01)
    bc01.name = 'infinite_padded_' + name

    # Create a 'mask' used to keep count of the number of elements summed for
    # each position
    wide_infinity_count = T.alloc(0, bc01.shape[0], bc01.shape[1], required_r,
                                  required_c)
    bc01_count = T.set_subtensor(wide_infinity_count[:, :, 0:r, 0:c], 1)

    for row_within_pool in xrange(pool_shape[0]):
        row_stop = last_pool_r + row_within_pool + 1
        for col_within_pool in xrange(pool_shape[1]):
            col_stop = last_pool_c + col_within_pool + 1
            cur = bc01[:,
                       :,
                       row_within_pool:row_stop:rs,
                       col_within_pool:col_stop:cs]
            cur.name = ('mean_pool_cur_' + bc01.name + '_' +
                        str(row_within_pool) + '_' + str(col_within_pool))
            cur_count = bc01_count[:,
                                   :,
                                   row_within_pool:row_stop:rs,
                                   col_within_pool:col_stop:cs]
            if mx is None:
                mx = cur
                count = cur_count
            else:
                mx = mx + cur
                count = count + cur_count
                mx.name = ('mean_pool_mx_' + bc01.name + '_' +
                           str(row_within_pool) + '_' + str(col_within_pool))

    mx /= count
    mx.name = 'mean_pool('+name+')'

    for mxv in get_debug_values(mx):
        assert not np.any(np.isnan(mxv))
        assert not np.any(np.isinf(mxv))

    return mx


@wraps(_WD)
def WeightDecay(*args, **kwargs):
    warnings.warn("pylearn2.models.mlp.WeightDecay has moved to "
                  "pylearn2.costs.mlp.WeightDecay")
    return _WD(*args, **kwargs)


@wraps(_L1WD)
def L1WeightDecay(*args, **kwargs):
    warnings.warn("pylearn2.models.mlp.L1WeightDecay has moved to "
                  "pylearn2.costs.mlp.WeightDecay")
    return _L1WD(*args, **kwargs)


class LinearGaussian(Linear):
    """
    A Linear layer augmented with a precision vector, for modeling
    conditionally Gaussian data.

    Specifically, given an input x, this layer models the distrbution over
    the output as

    y ~ p(y | x) = N(y | Wx + b, beta^-1)

    i.e., y is conditionally Gaussian with mean Wx + b and variance
    beta^-1.

    beta is a diagonal precision matrix so beta^-1 is a diagonal covariance
    matrix.

    Internally, beta is stored as the vector of diagonal values on this
    matrix.

    Since the output covariance is not a function of the input, this does
    not provide an example-specific estimate of the error in the mean.
    However, the vector-valued beta does mean that maximizing log p(y | x)
    will reweight the mean squared error so that variables that can be
    estimated easier will receive a higher penalty. This is one way of
    adapting the model better to heterogenous data.

    Parameters
    ----------
    init_beta : float or ndarray
        Any value > 0 that can be broadcasted to a vector of shape (dim, ).
        The elements of beta are initialized to this value.
        A good value is often the precision (inverse variance) of the target
        variables in the training set, as provided by the
        `beta_from_targets` function. This is the optimal beta for a dummy
        model that just predicts the mean target value from the training set.
    min_beta : float
        The elements of beta are constrained to be >= this value.
        This value must be > 0., otherwise the output conditional is not
        constrained to be a valid probability distribution.
        A good value is often the precision (inverse variance) of the target
        variables in the training set, as provided by the
        `beta_from_targets` function. This is the optimal beta for a dummy
        model that just predicts the mean target value from the training set.
        A trained model should always be able to obtain at least this much
        precision, at least on the training set.
    max_beta : float
        The elements of beta are constrained to be <= this value.
        We impose this constraint because for problems
        where the training set values can be predicted
        exactly, beta can grow without bound, which also makes the
        gradients grow without bound, resulting in numerical problems.
    kwargs : dict
        Arguments to the `Linear` superclass.
    """

    def __init__(self, init_beta, min_beta, max_beta, beta_lr_scale, **kwargs):
        super(LinearGaussian, self).__init__(**kwargs)
        self.__dict__.update(locals())
        del self.self
        del self.kwargs

    @wraps(Layer.set_input_space)
    def set_input_space(self, space):

        super(LinearGaussian, self).set_input_space(space)
        assert isinstance(self.output_space, VectorSpace)
        self.beta = sharedX(self.output_space.get_origin() + self.init_beta,
                            'beta')

    @wraps(Linear.get_monitoring_channels)
    def get_monitoring_channels(self):
        warnings.warn("Layer.get_monitoring_channels is " + \
                    "deprecated. Use get_layer_monitoring_channels " + \
                    "instead. Layer.get_monitoring_channels " + \
                    "will be removed on or after september 24th 2014",
                    stacklevel=2)

        rval = super(LinearGaussian, self).get_monitoring_channels()
        assert isinstance(rval, OrderedDict)
        rval['beta_min'] = self.beta.min()
        rval['beta_mean'] = self.beta.mean()
        rval['beta_max'] = self.beta.max()
        return rval

    @wraps(Linear.get_monitoring_channels_from_state)
    def get_monitoring_channels_from_state(self, state, target=None):
        warnings.warn("Layer.get_monitoring_channels_from_state is " + \
                    "deprecated. Use get_layer_monitoring_channels " + \
                    "instead. Layer.get_monitoring_channels_from_state " + \
                    "will be removed on or after september 24th 2014",
                    stacklevel=2)

        rval = super(LinearGaussian, self).get_monitoring_channels()
        assert isinstance(rval, OrderedDict)
        rval['beta_min'] = self.beta.min()
        rval['beta_mean'] = self.beta.mean()
        rval['beta_max'] = self.beta.max()

        if target:
            rval['mse'] = T.sqr(state - target).mean()
        return rval

    @wraps(Layer.get_layer_monitoring_channels)
    def get_layer_monitoring_channels(self, state_below=None,
                                    state=None, targets=None):

        rval = super(LinearGaussian,
                self).get_layer_monitoring_channels(state_below, \
                                                    state, \
                                                    targets)
        assert isinstance(rval, OrderedDict)
        rval['beta_min'] = self.beta.min()
        rval['beta_mean'] = self.beta.mean()
        rval['beta_max'] = self.beta.max()

        if targets:
            rval['mse'] = T.sqr(state - targets).mean()
        return rval

    @wraps(Linear.cost)
    def cost(self, Y, Y_hat):
        return (0.5 * T.dot(T.sqr(Y-Y_hat), self.beta).mean() -
                0.5 * T.log(self.beta).sum())

    @wraps(Layer._modify_updates)
    def _modify_updates(self, updates):

        super(LinearGaussian, self)._modify_updates(updates)

        if self.beta in updates:
            updates[self.beta] = T.clip(updates[self.beta],
                                        self.min_beta,
                                        self.max_beta)

    @wraps(Layer.get_lr_scalers)
    def get_lr_scalers(self):

        rval = super(LinearGaussian, self).get_lr_scalers()
        if self.beta_lr_scale is not None:
            rval[self.beta] = self.beta_lr_scale
        return rval

    @wraps(Layer.get_params)
    def get_params(self):

        return super(LinearGaussian, self).get_params() + [self.beta]


def beta_from_design(design, min_var=1e-6, max_var=1e6):
    """
    Returns the marginal precision of a design matrix.

    Parameters
    ----------
    design : ndarray
        A numpy ndarray containing a design matrix
    min_var : float
    max_var : float
        All variances are constrained to lie in the range [min_var, max_var]
        to avoid numerical issues like infinite precision.

    Returns
    -------
    beta : ndarray
        A 1D vector containing the marginal precision of each variable in the
        design matrix.
    """
    return 1. / np.clip(design.var(axis=0), min_var, max_var)


def beta_from_targets(dataset, **kwargs):
    """
    Returns the marginal precision of the targets in a dataset.

    Parameters
    ----------
    dataset : DenseDesignMatrix
        A DenseDesignMatrix with a targets field `y`
    kwargs : dict
        Extra arguments to `beta_from_design`

    Returns
    -------
    beta : ndarray
        A 1-D vector containing the marginal precision of the *targets* in
        `dataset`.
    """
    return beta_from_design(dataset.y, **kwargs)


def beta_from_features(dataset, **kwargs):
    """
    Returns the marginal precision of the features in a dataset.

    Parameters
    ----------
    dataset : DenseDesignMatrix
        The dataset to compute the precision on.
    kwargs : dict
        Passed through to `beta_from_design`

    Returns
    -------
    beta : ndarray
        Vector of precision values for each feature in `dataset`
    """
    return beta_from_design(dataset.X, **kwargs)


def mean_of_targets(dataset):
    """
    Returns the mean of the targets in a dataset.

    Parameters
    ----------
    dataset : DenseDesignMatrix

    Returns
    -------
    mn : ndarray
        A 1-D vector with entry i giving the mean of target i
    """
    return dataset.y.mean(axis=0)


class PretrainedLayer(Layer):
    """
    A layer whose weights are initialized, and optionally fixed,
    based on prior training.

    Parameters
    ----------
    layer_content : Model
        Should implement "upward_pass" (RBM and Autoencoder do this)
    freeze_params: bool
        If True, regard layer_conent's parameters as fixed
        If False, they become parameters of this layer and can be
        fine-tuned to optimize the MLP's cost function.
    """

    def __init__(self, layer_name, layer_content, freeze_params=False):
        super(PretrainedLayer, self).__init__()
        self.__dict__.update(locals())
        del self.self

    @wraps(Layer.set_input_space)
    def set_input_space(self, space):

        assert self.get_input_space() == space

    @wraps(Layer.get_params)
    def get_params(self):

        if self.freeze_params:
            return []
        return self.layer_content.get_params()

    @wraps(Layer.get_input_space)
    def get_input_space(self):

        return self.layer_content.get_input_space()

    @wraps(Layer.get_output_space)
    def get_output_space(self):

        return self.layer_content.get_output_space()

    @wraps(Layer.get_monitoring_channels)
    def get_monitoring_channels(self):
        warnings.warn("Layer.get_monitoring_channels is " + \
                    "deprecated. Use get_layer_monitoring_channels " + \
                    "instead. Layer.get_monitoring_channels " + \
                    "will be removed on or after september 24th 2014",
                    stacklevel=2)

        return OrderedDict([])

    @wraps(Layer.get_layer_monitoring_channels)
    def get_layer_monitoring_channels(self, state_below=None,
                                    state=None, targets=None):
        return OrderedDict([])

    @wraps(Layer.fprop)
    def fprop(self, state_below):

        return self.layer_content.upward_pass(state_below)


class CompositeLayer(Layer):
    """
    A Layer that runs several layers in parallel. Its default behavior
    is to pass the layer's input to each of the components.
    Alternatively, it can take a CompositeSpace as an input and a mapping
    from inputs to layers i.e. providing each component layer with a
    subset of the inputs.

    Parameters
    ----------
    layer_name : str
        The name of this layer
    layers : tuple or list
        The component layers to run in parallel.
    inputs_to_components : dict mapping int to list of ints, optional
        Can only be used if the input space is a CompositeSpace.
        If inputs_to_components[i] contains j, it means input i will
        be given as input to component j. Note that if multiple inputs are
        passed on to e.g. an inner CompositeLayer, the same order will
        be maintained. If the list is empty, the input will be discarded.
        If an input does not appear in the dictionary, it will be given to
        all components.

    Examples
    --------
    >>> composite_layer = CompositeLayer(
    ...     layer_name='composite_layer',
    ...     layers=[Tanh(7, 'h0', 0.1), Sigmoid(5, 'h1', 0.1)],
    ...     inputs_to_components={
    ...         0: [1],
    ...         1: [0]
    ...     })

    This CompositeLayer has a CompositeSpace with 2 subspaces as its
    input space. The first input is given to the Sigmoid layer, the second
    input is given to the Tanh layer.

    >>> wrapper_layer = CompositeLayer(
    ...     layer_name='wrapper_layer',
    ...     layers=[Linear(9, 'h2', 0.1),
    ...             composite_layer,
    ...             Tanh(7, 'h3', 0.1)],
    ...     inputs_to_components={
    ...         0: [0],
    ...         2: []
    ...     })

    This CompositeLayer takes 3 inputs. The first one is given to the
    inner CompositeLayer. The second input is passed on to each component
    layer i.e. to the Tanh, Linear as well as CompositeLayer. The third
    input is discarded. Note that the inner CompositeLayer wil receive
    the inputs with the same ordering i.e. [0, 1], and never [1, 0].
    """
    def __init__(self, layer_name, layers, inputs_to_layers=None):
        self.num_layers = len(layers)
        if inputs_to_layers is not None:
            if not isinstance(inputs_to_layers, dict):
                raise TypeError("CompositeLayer expected inputs_to_layers to "
                                "be dict, got " + str(type(inputs_to_layers)))
            self.inputs_to_layers = OrderedDict()
            for key in sorted(inputs_to_layers):
                assert isinstance(key, py_integer_types)
                assert 0 <= key < self.num_layers
                value = inputs_to_layers[key]
                assert is_iterable(value)
                assert all(isinstance(v, py_integer_types) for v in value)
                # Check 'not value' to support case of empty list
                assert not value or all(0 <= v < self.num_layers
                                        for v in value)
                self.inputs_to_layers[key] = sorted(value)
        super(CompositeLayer, self).__init__()
        self.__dict__.update(locals())
        del self.self

    @property
    def routing_needed(self):
        return self.inputs_to_layers is not None

    @wraps(Layer.set_input_space)
    def set_input_space(self, space):
        if not isinstance(space, CompositeSpace):
            if self.inputs_to_layers is not None:
                raise ValueError("CompositeLayer received an inputs_to_layers "
                                 "mapping, but does not have a CompositeSpace "
                                 "as its input space, so there is nothing to "
                                 "map. Received " + str(space) + " as input "
                                 "space.")
        elif self.routing_needed:
            if not max(self.inputs_to_layers) < len(space.components):
                raise ValueError("The inputs_to_layers mapping of "
                                 "CompositeSpace contains they key " +
                                 str(max(self.inputs_to_layers)) + " "
                                 "(0-based) but the input space only "
                                 "contains " + str(self.num_layers) + " "
                                 "layers.")
            # Invert the dictionary
            self.layers_to_inputs = OrderedDict()
            for i in xrange(self.num_layers):
                inputs = []
                for j in xrange(len(space.components)):
                    if j in self.inputs_to_layers:
                        if i in self.inputs_to_layers[j]:
                            inputs.append(j)
                    else:
                        inputs.append(j)
                self.layers_to_inputs[i] = inputs
        for i, layer in enumerate(self.layers):
            if self.routing_needed and i in self.layers_to_inputs:
                cur_space = space.restrict(self.layers_to_inputs[i])
            else:
                cur_space = space
            layer.set_input_space(cur_space)

        self.input_space = space
        self.output_space = CompositeSpace(tuple(layer.get_output_space()
                                                 for layer in self.layers))

    @wraps(Layer.get_params)
    def get_params(self):
        rval = []
        for layer in self.layers:
            rval = safe_union(layer.get_params(), rval)
        return rval

    @wraps(Layer.fprop)
    def fprop(self, state_below):
        rvals = []
        for i, layer in enumerate(self.layers):
            if self.routing_needed and i in self.layers_to_inputs:
                cur_state_below = [state_below[j]
                                   for j in self.layers_to_inputs[i]]
                # This is to mimic the behavior of CompositeSpace's restrict
                # method, which only returns a CompositeSpace when the number
                # of components is greater than 1
                if len(cur_state_below) == 1:
                    cur_state_below, = cur_state_below
            else:
                cur_state_below = state_below
            rvals.append(layer.fprop(cur_state_below))
        return tuple(rvals)

    def _weight_decay_aggregate(self, method_name, coeff):
        if isinstance(coeff, py_float_types):
            return T.sum([getattr(layer, method_name)(coeff)
                          for layer in self.layers])
        elif is_iterable(coeff):
            assert all(layer_coeff >= 0 for layer_coeff in coeff)
            return T.sum([getattr(layer, method_name)(layer_coeff) for
                          layer, layer_coeff in safe_zip(self.layers, coeff)
                          if layer_coeff > 0])
        else:
            raise TypeError("CompositeLayer's " + method_name + " received "
                            "coefficients of type " + str(type(coeff)) + " "
                            "but must be provided with a float or list/tuple")

    def get_weight_decay(self, coeff):
        """
        Provides an expression for a squared L2 penalty on the weights,
        which is the weighted sum of the squared L2 penalties of the layer
        components.

        Parameters
        ----------
        coeff : float or tuple/list
            The coefficient on the squared L2 weight decay penalty for
            this layer. If a single value is provided, this coefficient is
            used for each component layer. If a list of tuple of
            coefficients is given they are passed on to the component
            layers in the given order.

        Returns
        -------
        weight_decay : theano.gof.Variable
            An expression for the squared L2 weight decay penalty term for
            this layer.
        """
        return self._weight_decay_aggregate('get_weight_decay', coeff)

    def get_l1_weight_decay(self, coeff):
        """
        Provides an expression for a squared L1 penalty on the weights,
        which is the weighted sum of the squared L1 penalties of the layer
        components.

        Parameters
        ----------
        coeff : float or tuple/list
            The coefficient on the L1 weight decay penalty for this layer.
            If a single value is provided, this coefficient is used for
            each component layer. If a list of tuple of coefficients is
            given they are passed on to the component layers in the
            given order.

        Returns
        -------
        weight_decay : theano.gof.Variable
            An expression for the L1 weight decay penalty term for this
            layer.
        """
        return self._weight_decay_aggregate('get_l1_weight_decay', coeff)

    @wraps(Layer.cost)
    def cost(self, Y, Y_hat):
        return sum(layer.cost(Y_elem, Y_hat_elem)
                   for layer, Y_elem, Y_hat_elem in
                   safe_zip(self.layers, Y, Y_hat))

    @wraps(Layer.set_mlp)
    def set_mlp(self, mlp):
        super(CompositeLayer, self).set_mlp(mlp)
        for layer in self.layers:
            layer.set_mlp(mlp)


class FlattenerLayer(Layer):
    """
    A wrapper around a different layer that flattens
    the original layer's output.

    The cost works by unflattening the target and then
    calling the wrapped Layer's cost.

    This is mostly intended for use with CompositeLayer as the wrapped
    Layer, and is mostly useful as a workaround for theano not having
    a TupleVariable with which to represent a composite target.

    There are obvious memory, performance, and readability issues with doing
    this, so really it would be better for theano to support TupleTypes.

    See pylearn2.sandbox.tuple_var and the theano-dev e-mail thread
    "TupleType".

    Parameters
    ----------
    raw_layer : WRITEME
        WRITEME
    """

    def __init__(self, raw_layer):
        super(FlattenerLayer, self).__init__()
        self.__dict__.update(locals())
        del self.self
        self.layer_name = raw_layer.layer_name

    @wraps(Layer.set_input_space)
    def set_input_space(self, space):

        self.raw_layer.set_input_space(space)
        total_dim = self.raw_layer.get_output_space().get_total_dimension()
        self.output_space = VectorSpace(total_dim)

    @wraps(Layer.get_input_space)
    def get_input_space(self):
        return self.raw_layer.get_input_space()
    @wraps(Layer.get_monitoring_channels)
    def get_monitoring_channels(self, data):
        return self.raw_layer.get_monitoring_channels(data)

    @wraps(Layer.get_layer_monitoring_channels)
    def get_layer_monitoring_channels(self, state_below=None,
                                      state=None, targets=None):
        return self.raw_layer.get_layer_monitoring_channels(
            state_below=state_below,
            state=state,
            targets=targets
            )

    @wraps(Layer.get_monitoring_data_specs)
    def get_monitoring_data_specs(self):
        return self.raw_layer.get_monitoring_data_specs()

    @wraps(Layer.get_params)
    def get_params(self):
        return self.raw_layer.get_params()

    @wraps(Layer.get_weights)
    def get_weights(self):
        return self.raw_layer.get_weights()

    @wraps(Layer.get_weight_decay)
    def get_weight_decay(self, coeffs):
        return self.raw_layer.get_weight_decay(coeffs)

    @wraps(Layer.get_l1_weight_decay)
    def get_l1_weight_decay(self, coeffs):
        return self.raw_layer.get_l1_weight_decay(coeffs)

    @wraps(Layer.set_batch_size)
    def set_batch_size(self, batch_size):
        self.raw_layer.set_batch_size(batch_size)

    @wraps(Layer.censor_updates)
    def censor_updates(self, updates):
        self.raw_layer.censor_updates(updates)

    @wraps(Layer.get_lr_scalers)
    def get_lr_scalers(self):
        return self.raw_layer.get_lr_scalers()

    @wraps(Layer.fprop)
    def fprop(self, state_below):

        raw = self.raw_layer.fprop(state_below)

        return self.raw_layer.get_output_space().format_as(raw,
                                                           self.output_space)

    @wraps(Layer.cost)
    def cost(self, Y, Y_hat):

        raw_space = self.raw_layer.get_output_space()
        target_space = self.output_space
        raw_Y = target_space.format_as(Y, raw_space)

        if isinstance(raw_space, CompositeSpace):
            # Pick apart the Join that our fprop used to make Y_hat
            assert hasattr(Y_hat, 'owner')
            owner = Y_hat.owner
            assert owner is not None
            assert str(owner.op) == 'Join'
            # first input to join op is the axis
            raw_Y_hat = tuple(owner.inputs[1:])
        else:
            # To implement this generally, we'll need to give Spaces an
            # undo_format or something. You can't do it with format_as
            # in the opposite direction because Layer.cost needs to be
            # able to assume that Y_hat is the output of fprop
            raise NotImplementedError()
        raw_space.validate(raw_Y_hat)

        return self.raw_layer.cost(raw_Y, raw_Y_hat)

    @wraps(Layer.set_mlp)
    def set_mlp(self, mlp):

        super(FlattenerLayer, self).set_mlp(mlp)
        self.raw_layer.set_mlp(mlp)

    @wraps(Layer.get_weights)
    def get_weights(self):

        return self.raw_layer.get_weights()


class WindowLayer(Layer):
    """
    Layer used to select a window of an image input.
    The input of the layer must be Conv2DSpace.

    Parameters
    ----------
    layer_name : str
        A name for this layer.
    window : tuple
        A four-tuple of ints indicating respectively
        the top left x and y position, and
        the bottom right x and y position of the window.
    """

    def __init__(self, layer_name, window):
        super(WindowLayer, self).__init__()
        self.__dict__.update(locals())
        del self.self
        if window[0] < 0 or window[0] > window[2] or \
           window[1] < 0 or window[1] > window[3]:
            raise ValueError("WindowLayer: bad window parameter")

    @wraps(Layer.fprop)
    def fprop(self, state_below):
        extracts = [slice(None), slice(None), slice(None), slice(None)]
        extracts[self.rows] = slice(self.window[0], self.window[2] + 1)
        extracts[self.cols] = slice(self.window[1], self.window[3] + 1)
        extracts = tuple(extracts)

        return state_below[extracts]

    @wraps(Layer.set_input_space)
    def set_input_space(self, space):
        self.input_space = space

        if not isinstance(space, Conv2DSpace):
            raise TypeError("The input to a Window layer should be a "
                            "Conv2DSpace,  but layer " + self.layer_name +
                            " got " + str(type(self.input_space)))
        axes = space.axes
        self.rows = axes.index(0)
        self.cols = axes.index(1)

        nrows = space.shape[0]
        ncols = space.shape[1]

        if self.window[2] + 1 > nrows or self.window[3] + 1 > ncols:
            raise ValueError("WindowLayer: bad window shape. "
                             "Input is [" + str(nrows)  + ", " +
                             str(ncols) + "], "
                             "but layer " + self.layer_name + " has window "
                             + str(self.window))
        self.output_space = Conv2DSpace(
                                shape=[self.window[2] - self.window[0] + 1,
                                       self.window[3] - self.window[1] + 1],
                                num_channels=space.num_channels,
                                axes=axes
                                )

    @wraps(Layer.get_params)
    def get_params(self):
        return []

    @wraps(Layer.get_monitoring_channels)
    def get_monitoring_channels(self):
        return []


def generate_dropout_mask(mlp, default_include_prob=0.5,
                          input_include_probs=None, rng=(2013, 5, 17)):
    """
    Generate a dropout mask (as an integer) given inclusion
    probabilities.

    Parameters
    ----------
    mlp : object
        An MLP object.
    default_include_prob : float, optional
        The probability of including an input to a hidden
        layer, for layers not listed in `input_include_probs`.
        Default is 0.5.
    input_include_probs : dict, optional
        A dictionary  mapping layer names to probabilities
        of input inclusion for that layer. Default is `None`,
        in which `default_include_prob` is used for all
        layers.
    rng : RandomState object or seed, optional
        A `numpy.random.RandomState` object or a seed used to
        create one.

    Returns
    -------
    mask : int
        An integer indexing a dropout mask for the network,
        drawn with the appropriate probability given the
        inclusion probabilities.
    """
    if input_include_probs is None:
        input_include_probs = {}
    if not hasattr(rng, 'uniform'):
        rng = np.random.RandomState(rng)
    total_units = 0
    mask = 0
    for layer in mlp.layers:
        if layer.layer_name in input_include_probs:
            p = input_include_probs[layer.layer_name]
        else:
            p = default_include_prob
        for _ in xrange(layer.get_input_space().get_total_dimension()):
            mask |= int(rng.uniform() < p) << total_units
            total_units += 1
    return mask


def sampled_dropout_average(mlp, inputs, num_masks,
                            default_input_include_prob=0.5,
                            input_include_probs=None,
                            default_input_scale=2.,
                            input_scales=None,
                            rng=(2013, 05, 17),
                            per_example=False):
    """
    Take the geometric mean over a number of randomly sampled
    dropout masks for an MLP with softmax outputs.

    Parameters
    ----------
    mlp : object
        An MLP object.
    inputs : tensor_like
        A Theano variable representing a minibatch appropriate
        for fpropping through the MLP.
    num_masks : int
        The number of masks to sample.
    default_input_include_prob : float, optional
        The probability of including an input to a hidden
        layer, for layers not listed in `input_include_probs`.
        Default is 0.5.
    input_include_probs : dict, optional
        A dictionary  mapping layer names to probabilities
        of input inclusion for that layer. Default is `None`,
        in which `default_include_prob` is used for all
        layers.
    default_input_scale : float, optional
        The amount to scale input in dropped out layers.
    input_scales : dict, optional
        A dictionary  mapping layer names to constants by
        which to scale the input.
    rng : RandomState object or seed, optional
        A `numpy.random.RandomState` object or a seed used to
        create one.
    per_example : bool, optional
        If `True`, generate a different mask for every single
        test example, so you have `num_masks` per example
        instead of `num_mask` networks total. If `False`,
        `num_masks` masks are fixed in the graph.

    Returns
    -------
    geo_mean : tensor_like
        A symbolic graph for the geometric mean prediction of
        all the networks.
    """
    if input_include_probs is None:
        input_include_probs = {}

    if input_scales is None:
        input_scales = {}

    if not hasattr(rng, 'uniform'):
        rng = np.random.RandomState(rng)

    mlp._validate_layer_names(list(input_include_probs.keys()))
    mlp._validate_layer_names(list(input_scales.keys()))

    if per_example:
        outputs = [mlp.dropout_fprop(inputs, default_input_include_prob,
                                     input_include_probs,
                                     default_input_scale,
                                     input_scales)
                   for _ in xrange(num_masks)]

    else:
        masks = [generate_dropout_mask(mlp, default_input_include_prob,
                                       input_include_probs, rng)
                 for _ in xrange(num_masks)]

        outputs = [mlp.masked_fprop(inputs, mask, None,
                                    default_input_scale, input_scales)
                   for mask in masks]

    return geometric_mean_prediction(outputs)


def exhaustive_dropout_average(mlp, inputs, masked_input_layers=None,
                               default_input_scale=2., input_scales=None):
    """
    Take the geometric mean over all dropout masks of an
    MLP with softmax outputs.

    Parameters
    ----------
    mlp : object
        An MLP object.
    inputs : tensor_like
        A Theano variable representing a minibatch appropriate
        for fpropping through the MLP.
    masked_input_layers : list, optional
        A list of layer names whose input should be masked.
        Default is all layers (including the first hidden
        layer, i.e. mask the input).
    default_input_scale : float, optional
        The amount to scale input in dropped out layers.
    input_scales : dict, optional
        A dictionary  mapping layer names to constants by
        which to scale the input.

    Returns
    -------
    geo_mean : tensor_like
        A symbolic graph for the geometric mean prediction
        of all exponentially many masked subnetworks.

    Notes
    -----
    This is obviously exponential in the size of the network,
    don't do this except for tiny toy networks.
    """
    if masked_input_layers is None:
        masked_input_layers = mlp.layer_names
    mlp._validate_layer_names(masked_input_layers)

    if input_scales is None:
        input_scales = {}
    mlp._validate_layer_names(input_scales.keys())

    if any(key not in masked_input_layers for key in input_scales):
        not_in = [key for key in input_scales
                  if key not in mlp.layer_names]
        raise ValueError(", ".join(not_in) + " in input_scales"
                         " but not masked")

    num_inputs = mlp.get_total_input_dimension(masked_input_layers)
    outputs = [mlp.masked_fprop(inputs, mask, masked_input_layers,
                                default_input_scale, input_scales)
               for mask in xrange(2 ** num_inputs)]
    return geometric_mean_prediction(outputs)


def geometric_mean_prediction(forward_props):
    """
    Take the geometric mean over all dropout masks of an
    MLP with softmax outputs.

    Parameters
    ----------
    forward_props : list
        A list of Theano graphs corresponding to forward
        propagations through the network with different
        dropout masks.

    Returns
    -------
    geo_mean : tensor_like
        A symbolic graph for the geometric mean prediction
        of all exponentially many masked subnetworks.

    Notes
    -----
    This is obviously exponential in the size of the network,
    don't do this except for tiny toy networks.
    """
    presoftmax = []
    for out in forward_props:
        assert isinstance(out.owner.op, T.nnet.Softmax)
        assert len(out.owner.inputs) == 1
        presoftmax.append(out.owner.inputs[0])
    average = reduce(lambda x, y: x + y, presoftmax) / float(len(presoftmax))
    return T.nnet.softmax(average)


class BadInputSpaceError(TypeError):
    """
    An error raised by an MLP layer when set_input_space is given an
    object that is not one of the Spaces that layer supports.
    """

########NEW FILE########
__FILENAME__ = mnd
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"
from pylearn2.models.model import Model
from pylearn2.utils import sharedX
import numpy as np
import theano.tensor as T

class DiagonalMND(Model):
    """
    A model based on the multivariate normal distribution. This variant is
    constrained to have diagonal covariance.

    Parameters
    ----------
    nvis : WRITEME
    init_beta : WRITEME
    init_mu : WRITEME
    min_beta : WRITEME
    max_beta : WRITEME
    """
    # TODO: unify this with distribution.mnd
    def __init__(self, nvis,
            init_beta,
            init_mu,
            min_beta,
            max_beta):
        #copy all arguments to the object
        self.__dict__.update( locals() )
        del self.self

        super(DiagonalMND,self).__init__()

        #build the object
        self.redo_everything()

    def redo_everything(self):
        """
        .. todo::

            WRITEME
        """

        self.beta = sharedX(np.ones((self.nvis,))*self.init_beta,'beta')
        self.mu = sharedX(np.ones((self.nvis,))*self.init_mu,'mu')
        self.redo_theano()


    def free_energy(self, X):
        """
        .. todo::

            WRITEME
        """

        diff = X-self.mu
        sq = T.sqr(diff)

        return  0.5 * T.dot( sq, self.beta )


    def log_prob(self, X):
        """
        .. todo::

            WRITEME
        """

        return -self.free_energy(X) - self.log_partition_function()

    def log_partition_function(self):
        """
        .. todo::

            WRITEME
        """
        # Z^-1 = (2pi)^{-nvis/2} det( beta^-1 )^{-1/2}
        # Z = (2pi)^(nvis/2) sqrt( det( beta^-1) )
        # log Z = (nvis/2) log 2pi - (1/2) sum(log(beta))

        return float(self.nvis)/2. * np.log(2*np.pi) - 0.5 * T.sum(T.log(self.beta))

    def redo_theano(self):
        """
        .. todo::

            WRITEME
        """

        init_names = dir(self)

        self.censored_updates = {}
        for param in self.get_params():
            self.censored_updates[param] = set([])

        final_names = dir(self)

        self.register_names_to_del( [name for name in final_names if name not in init_names])


    def _modify_updates(self, updates):
        """
        .. todo::

            WRITEME
        """

        if self.beta in updates and updates[self.beta] not in self.censored_updates[self.beta]:
            updates[self.beta] = T.clip(updates[self.beta], self.min_beta, self.max_beta )

        params = self.get_params()
        for param in updates:
            if param in params:
                self.censored_updates[param] = self.censored_updates[param].union(set([updates[param]]))

    def get_params(self):
        """
        .. todo::

            WRITEME
        """
        return [self.mu, self.beta ]


def kl_divergence(q,p):
    """
    .. todo::

        WRITEME
    """
    #KL divergence of two DiagonalMNDs
    #http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#KL_divergence_for_Normal_Distributions
    #D_KL(q||p) = 0.5 ( beta_q^T beta_p^-1 + beta_p^T sq(mu_p - mu_q) - log(det Siga_q / det Sigma_p) - k)
    assert isinstance(q,DiagonalMND)
    assert isinstance(p,DiagonalMND)

    assert q.nvis == p.nvis
    k = q.nvis

    beta_q = q.beta
    beta_p = p.beta
    beta_q_inv = 1./beta_q

    trace_term = T.dot(beta_q_inv,beta_p)
    assert trace_term.ndim == 0

    mu_p = p.mu
    mu_q = q.mu

    quad_term = T.dot(beta_p, T.sqr(mu_p-mu_q))
    assert quad_term.ndim == 0

    # - log ( det Sigma_q / det Sigma_p)
    # = log det Sigma_p - log det Sigma_q
    # = log det Beta_p_inv - log det Beta_q_inv
    # = sum(log(beta_p_inv)) - sum(log(beta_q_inv))
    # = sum(log(beta_q)) - sum(log(beta_p))
    log_term = T.sum(T.log(beta_q)) - T.sum(T.log(beta_p))
    assert log_term.ndim == 0

    inside_parens = trace_term + quad_term + log_term - k
    assert inside_parens.ndim == 0

    rval = 0.5 * inside_parens

    return rval

########NEW FILE########
__FILENAME__ = model
"""Generic "model" class."""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

from collections import defaultdict
from itertools import izip as izip_no_length_check
import numpy as np
import warnings

from theano.compat.python2x import OrderedDict
from theano import tensor as T

from pylearn2.model_extensions.model_extension import ModelExtension
from pylearn2.space import NullSpace
from pylearn2.utils import function
from pylearn2.utils import safe_zip
from pylearn2.utils.track_version import MetaLibVersion


class Model(object):
    """
    A class representing a model with learnable parameters.

    Parameters
    ----------
    extensions : list of ModelExtension
        Plugins to extend the model's functionality
    """

    __metaclass__ = MetaLibVersion
    _test_batch_size = 2

    def __init__(self, extensions=None):
        if extensions is None:
            extensions = []
        else:
            assert isinstance(extensions, list)
            assert all(isinstance(extensions, ModelExtension) for extension in
                       extensions)

        self.__dict__.update(locals())
        del self.self

        self._disallow_censor_updates()

        self.names_to_del = set()

    def _disallow_censor_updates(self):
        """
        Don't let subclasses use censor_updates.
        """
        if hasattr(self, '_censor_updates_message_shown'):
            return
        if self._overrides_censor_updates():
            self._censor_updates_message_shown = True
            warnings.warn(str(type(self)) + " overrides "
                          "Model.censor_updates, which is deprecated. Change "
                          "this to _modify_updates. censor_updates will no "
                          "longer be called on or after 2014-11-01.")

    def _ensure_extensions(self):
        """
        Makes sure the model has an "extensions" field.
        """

        if not hasattr(self, "extensions"):
            warnings.warn("The " + str(type(self)) + " Model subclass "
                          "seems not to call the Model constructor. This "
                          "behavior may be considered an error on or after "
                          "2014-11-01.")
            self.extensions = []

    def __setstate__(self, d):
        """
        An implementation of __setstate__ that patches old pickle files.
        """

        self._disallow_censor_updates()

        self.__dict__.update(d)

        # Patch old pickle files
        if 'extensions' not in d:
            self.extensions = []

    def get_default_cost(self):
        """
        Returns the default cost to use with this model.

        Returns
        -------
        default_cost : Cost
            The default cost to use with this model.
        """

        raise NotImplementedError(str(type(self)) +
                                  " does not implement get_default_cost.")

    def train_all(self, dataset):
        """
        If implemented, performs one epoch of training.


        Parameters
        ----------
        dataset : pylearn2.datasets.dataset.Dataset
            Dataset object to draw training data from

        Notes
        -----
        This method is useful
        for models with highly specialized training algorithms for which is
        does not make much sense to factor the training code into a separate
        class. It is also useful for implementors that want to make their model
        trainable without enforcing compatibility with pylearn2
        TrainingAlgorithms.
        """
        raise NotImplementedError(str(type(self)) +
                                  " does not implement train_all.")

    def continue_learning(self):
        """
        If train_all is used to train the model, this method is used to
        determine when the training process has converged. This method is
        called after the monitor has been run on the latest parameters.

        Returns
        -------
        rval : bool
            True if training should continue
        """

        raise NotImplementedError(str(type(self)) +
                                  " does not implement continue_learning.")

    def train_batch(self, dataset, batch_size):
        """
        If implemented, performs an update on a single minibatch.

        Parameters
        ----------
        dataset: pylearn2.datasets.dataset.Dataset
            The object to draw training data from.
        batch_size: int
            Size of the minibatch to draw from dataset.

        Returns
        -------
        rval : bool
            True if the method should be called again for another update.
            False if convergence has been reached.
        """
        raise NotImplementedError()

    def get_weights_view_shape(self):
        """
        Returns the shape `PatchViewer` should use to display the
        weights.

        Returns
        -------
        shape : tuple
            A tuple containing two ints. These are used as the
            `grid_shape` argument to `PatchViewer` when
            displaying the weights of this model.

        Notes
        -----
        This can be useful when there is some geometric
        significance to the order of your weight
        vectors. For example, the `Maxout` model makes sure that all of
        the filters for the same hidden unit appear on the same row
        of the display.
        """
        raise NotImplementedError(str(type(self)) + " does not implement "
                                  "get_weights_view_shape (perhaps by design)")

    def get_monitoring_channels(self, data):
        """
        Get monitoring channels for this model.

        Parameters
        ----------
        data : tensor_like, or (possibly nested) tuple of tensor_likes,
            This is data on which the monitoring quantities will be
            calculated (e.g., a validation set). See
            `self.get_monitoring_data_specs()`.

        Returns
        -------
        channels : OrderedDict
            A dictionary with strings as keys, mapping channel names to
            symbolic values that depend on the variables in `data`.

        Notes
        -----
        You can make any channel names you want, just try to make sure they
        won't collide with names made by the training Cost, etc. Anything you
        think is worth monitoring during training can be added here. You
        probably want to control which channels get added with some config
        option for your model.
        """
        space, source = self.get_monitoring_data_specs()
        space.validate(data)
        return OrderedDict()

    def get_monitoring_data_specs(self):
        """
        Get the data_specs describing the data for get_monitoring_channels.

        This implementation returns an empty data_specs, appropriate for
        when no monitoring channels are defined, or when none of the channels
        actually need data (for instance, if they only monitor functions
        of the model's parameters).

        Returns
        -------
        data_specs : TODO WRITEME
            TODO WRITEME

        """
        return (NullSpace(), '')

    def set_batch_size(self, batch_size):
        """
        Sets the batch size used by the model.

        Parameters
        ----------
        batch_size : int
            If None, allows the model to use any batch size.
        """
        pass

    def get_weights(self):
        """
        Returns the weights (of the first layer if more than one layer is
        present).

        Returns
        -------
        weights : ndarray
            Returns any matrix that is analogous to the weights of the first
            layer of an MLP, such as the dictionary of a sparse coding model.
            This implementation raises NotImplementedError. For models where
            this method is not conceptually applicable, do not override it.
            Format should be compatible with the return value of
            self.get_weights_format.
        """

        raise NotImplementedError(str(type(self)) + " does not implement "
                                  "get_weights (perhaps by design)")

    def get_weights_format(self):
        """
        Returns a description of how to interpret the return value of
        `get_weights`.

        Returns
        -------
        format : tuple
            Either ('v', 'h') or ('h', 'v'). ('v', 'h') means self.get_weights
            returns a matrix of shape (num visible units, num hidden units),
            while ('h', 'v') means it returns the transpose of this.
        """

        return ('v', 'h')

    def get_weights_topo(self):
        """
        Returns a topological view of the weights.

        Returns
        -------
        weights : ndarray
            Same as the return value of `get_weights` but formatted as a 4D
            tensor with the axes being (hidden units, rows, columns,
            channels). Only applicable for models where the weights can be
            viewed as 2D-multichannel, and the number of channels is either
            1 or 3 (because they will be visualized as grayscale or RGB color).
        """

        raise NotImplementedError(str(type(self)) + " does not implement "
                                  "get_weights_topo (perhaps by design)")

    def score(self, V):
        """
        Compute a "score function" for this model, if this model has
        probabilistic semantics.

        Parameters
        ----------
        V : tensor_like, 2-dimensional
            A batch of i.i.d. examples with examples indexed along the
            first axis and features along the second. This is data on which
            the monitoring quantities will be calculated (e.g., a validation
            set).

        Returns
        -------
        score : tensor_like
            The gradient of the negative log probability of the model
            on the given datal.

        Notes
        -----
        If the model implements a probability distribution on R^n,
        this method should return the gradient of the log probability
        of the batch with respect to V, or raise an exception explaining
        why this is not possible.
        """
        return T.grad(-self.free_energy(V).sum(), V)

    def get_lr_scalers(self):
        """
        Specify how to rescale the learning rate on each parameter.

        Returns
        -------
        lr_scalers : OrderedDict
            A dictionary mapping the parameters of the model to floats. The
            learning rate will be multiplied by the float for each parameter.
            If a parameter does not appear in the dictionary, it will use
            the global learning rate with no scaling.
        """
        return OrderedDict()

    def _overrides_censor_updates(self):
        """
        Returns true if the model overrides censor_updates.
        (It shouldn't do so because it's deprecated, and we have
        to take special action to handle this case)
        """

        return type(self).censor_updates != Model.censor_updates

    def censor_updates(self, updates):
        """
        Deprecated method. Callers should call modify_updates instead.
        Subclasses should override _modify_updates instead.

        Parameters
        ----------
        updates : dict
            A dictionary mapping shared variables to symbolic values they
            will be updated to.
        """

        warnings.warn("censor_updates is deprecated, call modify_updates "
                      "instead. This will become an error on or after "
                      "2014-11-01.", stacklevel=2)

        self.modify_updates(updates)

    def modify_updates(self, updates):
        """"
        Modifies the parameters before a learning update is applied. Behavior
        is defined by subclass's implementation of _modify_updates and any
        ModelExtension's implementation of post_modify_updates.

        Parameters
        ----------
        updates : dict
            A dictionary mapping shared variables to symbolic values they
            will be updated to

        Notes
        -----
        For example, if a given parameter is not meant to be learned, a
        subclass or extension
        should remove it from the dictionary. If a parameter has a restricted
        range, e.g.. if it is the precision of a normal distribution,
        a subclass or extension should clip its update to that range. If a
        parameter
        has any other special properties, its updates should be modified
        to respect that here, e.g. a matrix that must be orthogonal should
        have its update value modified to be orthogonal here.

        This is the main mechanism used to make sure that generic training
        algorithms such as those found in pylearn2.training_algorithms
        respect the specific properties of the models passed to them.
        """

        self._modify_updates(updates)

        self._ensure_extensions()
        for extension in self.extensions:
            extension.post_modify_updates(updates)

    def _modify_updates(self, updates):
        """
        Subclasses may override this method to add functionality to
        modify_updates.

        Parameters
        ----------
        updates : dict
            A dictionary mapping shared variables to symbolic values they
            will be updated to.
        """

        # Support subclasses that use the deprecated interface.
        if self._overrides_censor_updates():
            self.censor_updates(updates)

    def get_input_space(self):
        """
        Returns an instance of pylearn2.space.Space describing the format of
        the vector space that the model operates on (this is a generalization
        of get_input_dim)
        """

        return self.input_space

    def get_output_space(self):
        """
        Returns an instance of pylearn2.space.Space describing the format of
        the vector space that the model outputs (this is a generalization
        of get_output_dim)
        """

        return self.output_space

    def get_input_source(self):
        """
        Returns a string, stating the source for the input. By default the
        input source (when is the only one) is called 'features'.
        """
        return 'features'

    def get_target_source(self):
        """
        Returns a string, stating the source for the output. By default the
        output source (when is the only one) is called 'targets'.
        """
        return 'targets'

    def free_energy(self, V):
        """
        Compute the free energy of data examples, if this model has
        probabilistic semantics.

        Parameters
        ----------
        V : tensor_like, 2-dimensional
            A batch of i.i.d. examples with examples indexed along the
            first axis and features along the second. This is data on which
            the monitoring quantities will be calculated (e.g., a validation
            set).

        Returns
        -------
        free_energy : tensor, 1-dimensional
            A (symbolic) vector of free energies for each data example in
            `V`, i.e.  `free_energy[i] = F(V[i])`.
        """
        raise NotImplementedError()

    def get_params(self):
        """
        Returns the parameters that define the model.

        Returns
        -------
        params : list
            A list of (Theano shared variable) parameters of the model.

        Notes
        -----
        By default, this returns a copy of the _params attribute, which
        individual models can simply fill with the list of model parameters.
        Alternatively, models may override `get_params`, so this should
        be considered the public interface to model parameters -- directly
        accessing or modifying _params is at-your-own-risk, as it may
        or may not exist.

        This is the main mechanism by which generic training algorithms
        like SGD know which values to update, however, even model
        parameters that should not be learned ought to be included here,
        so that the model's parameter set is more predictable.

        Parameters may be included here but held constant during
        learning via the `modify_updates` method.
        """
        return list(self._params)

    def get_param_values(self, borrow=False):
        """
        Returns numerical values for the parameters that define the model.

        Parameters
        ----------
        borrow : bool, optional
            Flag to be passed to the `.get_value()` method of the
            shared variable. If `False`, a copy will always be returned.

        Returns
        -------
        params : list
            A list of `numpy.ndarray` objects containing the current
            parameters of the model.
        """
        assert not isinstance(self.get_params(), set)
        return [param.get_value(borrow=borrow) for param in self.get_params()]

    def set_param_values(self, values, borrow=False):
        """
        Sets the values of the parameters that define the model

        Parameters
        ----------
        values : list
            list of ndarrays
        borrow : bool
            The `borrow` flag to use with `set_value`.
        """
        for param, value in zip(self.get_params(), values):
            param.set_value(value, borrow=borrow)

    def get_param_vector(self):
        """
        Returns all parameters flattened into a single vector.

        Returns
        -------
        params : ndarray
            1-D array of all parameter values.
        """

        values = self.get_param_values()
        values = [value.reshape(value.size) for value in values]
        return np.concatenate(values, axis=0)

    def set_param_vector(self, vector):
        """
        Sets all parameters from a single flat vector. Format is consistent
        with `get_param_vector`.

        Parameters
        ----------
        vector : ndarray
            1-D array of all parameter values.
        """

        params = self.get_params()
        cur_values = self.get_param_values()

        pos = 0
        for param, value in safe_zip(params, cur_values):
            size = value.size
            new_value = vector[pos:pos+size]
            param.set_value(new_value.reshape(*value.shape))
            pos += size
        assert pos == vector.size

    def redo_theano(self):
        """
        Re-compiles all Theano functions used internally by the model.

        Notes
        -----
        This function is often called after a model is unpickled from
        disk, since Theano functions are not pickled. However, it is
        not always called. This allows scripts like show_weights.py
        to rapidly unpickle a model and inspect its weights without
        needing to recompile all of its learning machinery.

        All Theano functions compiled by this method should be registered
        with the register_names_to_del method.
        """
        pass

    def get_input_dim(self):
        """
        Returns the number of visible units of the model.
        Deprecated; this assumes the model operates on a vector.
        Use get_input_space instead.
        """
        raise NotImplementedError()

    def get_output_dim(self):
        """
        Returns the number of visible units of the model.
        Deprecated; this assumes the model operates on a vector.
        Use get_input_space instead.
        """
        raise NotImplementedError()

    def __getstate__(self):
        """
        This is the method that pickle/cPickle uses to determine what
        portion of the model to serialize. We remove all fields listed in
        `self.fields_to_del`. In particular, this should include all Theano
        functions, since they do not play nice with pickling.
        """

        self._disallow_censor_updates()

        d = OrderedDict()
        names_to_del = getattr(self, 'names_to_del', set())
        names_to_keep = set(self.__dict__.keys()).difference(names_to_del)
        for name in names_to_keep:
            d[name] = self.__dict__[name]

        return d

    def get_test_batch_size(self):
        """
        Specifies the batch size to use with compute.test_value

        Returns
        -------
        test_batch_size : int
            Number of examples to use in batches with compute.test_value

        Notes
        -----
        The model specifies
        the number of examples in case it needs a fixed batch size or to
        keep
        the memory usage of testing under control.
        """
        return self._test_batch_size

    def print_versions(self, print_theano_config=False):
        """
        Print version of the various Python packages and basic information
        about the experiment setup (e.g. cpu, os)

        Parameters
        ----------
        print_theano_config : bool
            TODO WRITEME

        Notes
        -----

        Example output:

        .. code-block::  none

             numpy:1.6.1 | pylearn:a6e634b83d | pylearn2:57a156beb0
             CPU: x86_64
             OS: Linux-2.6.35.14-106.fc14.x86_64-x86_64-with-fedora-14-Laughlin
        """
        self.libv.print_versions()
        self.libv.print_exp_env_info(print_theano_config)

    def register_names_to_del(self, names):
        """
        Register names of fields that should not be pickled.

        Parameters
        ----------
        names : iterable
            A collection of strings indicating names of fields on ts
            object that should not be pickled.

        Notes
        -----
        All names registered will be deleted from the dictionary returned
        by the model's `__getstate__` method (unless a particular model
        overrides this method).
        """
        if isinstance(names, basestring):
            names = [names]
        try:
            assert all(isinstance(n, basestring) for n in iter(names))
        except (TypeError, AssertionError):
            raise ValueError('Invalid names argument')
        # Quick check in case __init__ was never called, e.g. by a derived
        # class.
        if not hasattr(self, 'names_to_del'):
            self.names_to_del = set()
        self.names_to_del = self.names_to_del.union(names)

    def enforce_constraints(self):
        """
        Enforces all constraints encoded by self.modify_updates.
        """
        params = self.get_params()
        updates = OrderedDict(izip_no_length_check(params, params))
        self.modify_updates(updates)
        f = function([], updates=updates)
        f()

    @property
    def tag(self):
        """
        A "scratch-space" for storing model metadata.

        Returns
        -------
        tag : defaultdict
            A defaultdict with "dict" as the default constructor. This
            lets you do things like `model.tag[ext_name][quantity_name]`
            without the annoyance of first initializing the dict
            `model.tag[ext_name]`.

        Notes
        -----
        Nothing critical to the implementation of a particular model or
        training algorithm in the library should get stored in `tag`. This
        is mainly for extensions or user code to take advantage of, and
        segregate such things from actual model implementation attributes.
        """
        if not hasattr(self, '_tag'):
            self._tag = defaultdict(dict)
        return self._tag

########NEW FILE########
__FILENAME__ = normalized_ebm
"""
The NormalizedEBM class.
"""

import functools
import numpy as np

from theano import config, shared

from pylearn2.models import Model

__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"


class NormalizedEBM(Model):
    """
    An Energy-Based Model with an additional parameter representing log Z.

    In practice, this parameter is only approximately correct, though it
    can be learned through methods such as noise-contrastive estimation.

    Parameters
    ----------
    ebm : WRITEME
        The underlying EBM to learn.
    init_logZ : float
        Initial estimate of log Z
    learn_logZ : bool
        If true, learn log Z. Otherwise, continue to assume init_logZ is
        accurate.
    logZ_lr_scale : float
        How much to scale the learning rate on the log Z estimate
    """
    def __init__(self, ebm, init_logZ, learn_logZ, logZ_lr_scale=1.0):
        super(NormalizedEBM, self).__init__()
        self.ebm = ebm
        self.logZ_driver = shared(np.cast[config.floatX](
            init_logZ / logZ_lr_scale
        ))
        self.learn_logZ = learn_logZ
        self.logZ_lr_scale = logZ_lr_scale

        self.batches_seen = 0
        self.examples_seen = 0

    def log_prob(self, X):
        """
        Returns the log probability of a batch of examples.

        Parameters
        ----------
        X : WRITEME
            The examples whose log probability should be computed.

        Returns
        -------
        log_prob : WRITEME
            The log probability of the examples.
        """
        return -self.ebm.free_energy(X) - self.logZ_driver * self.logZ_lr_scale

    def free_energy(self, X):
        """
        Returns the free energy of a batch of examples.

        Parameters
        ----------
        X : WRITEME
            The examples whose free energy should be computed.

        Returns
        -------
        free_energy : WRITEME
            The free energy of the examples.
        """
        return self.ebm.free_energy(X)

    @functools.wraps(Model.get_params)
    def get_params(self):
        params = self.ebm.get_params()
        if self.learn_logZ:
            params.append(self.logZ_driver)
        return params

    @functools.wraps(Model._modify_updates)
    def _modify_updates(self, updates):
        self.ebm.modify_updates(updates)

    @functools.wraps(Model.redo_theano)
    def redo_theano(self):
        self.ebm.redo_theano()
        self.E_X_batch_func = self.ebm.E_X_batch_func

    @functools.wraps(Model.get_weights_format)
    def get_weights(self):
        return self.ebm.get_weights()

    @functools.wraps(Model.get_weights_format)
    def get_weights_format(self):
        return self.ebm.get_weights_format()

########NEW FILE########
__FILENAME__ = pca
"""
.. todo::

    WRITEME
"""
# Standard library imports
import logging
import sys

# Third-party imports
import numpy
N = numpy
import warnings
from scipy import linalg, sparse
# Warning: ridiculous.
try:
    # scipy 0.9
    from scipy.sparse.linalg import eigsh as eigen_symmetric
except ImportError:
    try:
        # scipy 0.8
        from scipy.sparse.linalg import eigen_symmetric
    except ImportError:
        try:
            # scipy 0.7
            from scipy.sparse.linalg.eigen.arpack import eigen_symmetric
        except ImportError:
            warnings.warn('Cannot import any kind of symmetric eigen' \
                ' decomposition function from scipy.sparse.linalg')
from scipy.sparse.csr import csr_matrix
import theano
from theano import tensor
from theano.sparse import SparseType, structured_dot
from scipy import linalg
from scipy.sparse.csr import csr_matrix

try:
    from scipy.sparse.linalg import eigen_symmetric
except ImportError:
    #this was renamed to eigsh in scipy 0.9
    try:
        from scipy.sparse.linalg import eigsh as eigen_symmetric
    except ImportError:
        warnings.warn("couldn't import eigsh / eigen_symmetric from "
                      "scipy.linalg.sparse, some of your pca functions "
                      "may randomly fail later")
        warnings.warn("the fact that somebody is using this doesn't bode well "
                      "since it's unlikely "
                      "that the covariance matrix is sparse")


# Local imports
from pylearn2.blocks import Block
from pylearn2.utils import sharedX


logger = logging.getLogger()


class _PCABase(Block):
    """
    Block which transforms its input via Principal Component Analysis.

    This class is not intended to be instantiated directly. Use a subclass to
    select a particular PCA implementation.

    Parameters
    ----------
    num_components : int, optional
        This many components will be preserved, in decreasing order of variance
        (default None keeps all)
    min_variance : float, optional
        Components with normalized variance [0-1] below this threshold will be
        discarded
    whiten : bool, optional
        Whether or not to divide projected features by their standard deviation
    """

    def __init__(self, num_components=None, min_variance=0.0, whiten=False):
        super(_PCABase, self).__init__()

        self.num_components = num_components
        self.min_variance = min_variance
        self.whiten = whiten

        self.W = None
        self.v = None
        self.mean = None

        self.component_cutoff = theano.shared(
                                    theano._asarray(0, dtype='int64'),
                                    name='component_cutoff')

        # This module really has no adjustable parameters -- once train()
        # is called once, they are frozen, and are not modified via gradient
        # descent.
        self._params = []

    def train(self, X, mean=None):
        """
        Compute the PCA transformation matrix.

        Given a rectangular matrix :math:`X = USV` such that :math:`S` is a
        diagonal matrix with :math:`X`'s singular values along its diagonal,
        returns :math:`W = V^{-1}`.

        If mean is provided, :math:`X` will not be centered first.

        Parameters
        ----------
        X : numpy.ndarray
            Matrix of shape (n, d) on which to train PCA
        mean : numpy.ndarray, optional
            Feature means of shape (d,)
        """

        if self.num_components is None:
            self.num_components = X.shape[1]

        # Center each feature.
        if mean is None:
            mean = X.mean(axis=0)
            X = X - mean

        # Compute eigen{values,vectors} of the covariance matrix.
        v, W = self._cov_eigen(X)

        # Build Theano shared variables
        # For the moment, I do not use borrow=True because W and v are
        # subtensors, and I want the original memory to be freed
        self.W = sharedX(W, name='W')
        self.v = sharedX(v, name='v')
        self.mean = sharedX(mean, name='mean')

        # Filter out unwanted components, permanently.
        self._update_cutoff()
        component_cutoff = self.component_cutoff.get_value(borrow=True)
        self.v.set_value(self.v.get_value(borrow=True)[:component_cutoff])
        self.W.set_value(self.W.get_value(borrow=True)[:, :component_cutoff])

    def __call__(self, inputs):
        """
        Compute and return the PCA transformation of the current data.

        Parameters
        ----------
        inputs : numpy.ndarray
            Matrix of shape (n, d) on which to compute PCA

        Returns
        -------
        WRITEME
        """

        # Update component cutoff, in case min_variance or num_components has
        # changed (or both).

        #TODO: Looks like the person who wrote this function didn't know what
        #      they were doing
        # component_cutoff is a shared variable, so updating its value here has
        # NO EFFECT on the symbolic expression returned by this call (and what
        # this expression evalutes to can be modified by subsequent calls to
        # _update_cutoff)
        self._update_cutoff()

        normalized_mean = inputs - self.mean
        normalized_mean.name = 'normalized_mean'

        W = self.W[:, :self.component_cutoff]

        #TODO: this is inefficient, should make another shared variable where
        # this proprocessing is already done
        if self.whiten:
            W = W / tensor.sqrt(self.v[:self.component_cutoff])

        Y = tensor.dot(normalized_mean, W)

        return Y

    def get_weights(self):
        """
        Compute and return the matrix one should multiply with to get the
        PCA/whitened data

        Returns
        -------
        WRITEME
        """

        self._update_cutoff()

        component_cutoff = self.component_cutoff.get_value()

        W = self.W.get_value(borrow=False)
        W = W[:, :component_cutoff]

        if self.whiten:
            W /= N.sqrt(self.v.get_value(borrow=False)[:component_cutoff])

        return W

    def reconstruct(self, inputs, add_mean=True):
        """
        Given a PCA transformation of the current data, compute and return
        the reconstruction of the original input

        Parameters
        ----------
        inputs : WRITEME
        add_mean : bool, optional
            WRITEME

        Returns
        -------
        WRITEME
        """
        self._update_cutoff()
        if self.whiten:
            inputs *= tensor.sqrt(self.v[:self.component_cutoff])
        X = tensor.dot(inputs, self.W[:, :self.component_cutoff].T)
        if add_mean:
            X = X + self.mean
        return X

    def _update_cutoff(self):
        """
        Update component cutoff shared var, based on current parameters.
        """

        assert self.num_components is not None and self.num_components > 0, \
            'Number of components requested must be >= 1'

        v = self.v.get_value(borrow=True)
        var_mask = v / v.sum() > self.min_variance
        assert numpy.any(var_mask), \
            'No components exceed the given min. variance'
        var_cutoff = 1 + numpy.where(var_mask)[0].max()

        self.component_cutoff.set_value(min(var_cutoff, self.num_components))

    def _cov_eigen(self, X):
        """
        Compute and return eigen{values,vectors} of X's covariance matrix.

        Parameters
        ----------
        X : WRITEME

        Returns
        -------
        All eigenvalues in decreasing order matrix containing corresponding
        eigenvectors in its columns
        """
        raise NotImplementedError('Not implemented in _PCABase. Use a ' +
                                  'subclass (and implement it there).')


class SparseMatPCA(_PCABase):
    """
    Does PCA on sparse  matrices. Does not do online PCA. This is for the case
    where `X - X.mean()` does not fit in memory (because it's dense) but
    `N.dot((X-X.mean()).T, X-X.mean())` does.

    Parameters
    ----------
    batch_size : WRITEME
    kwargs : dict
        WRITEME
    """
    def __init__(self, batch_size=50, **kwargs):
        super(SparseMatPCA, self).__init__(**kwargs)
        self.minibatch_size = batch_size

    def get_input_type(self):
        """
        .. todo::

            WRITEME
        """
        return csr_matrix

    def _cov_eigen(self, X):
        """
        .. todo::

            WRITEME
        """
        n, d = X.shape

        cov = numpy.zeros((d, d))
        batch_size = self.minibatch_size

        for i in xrange(0, n, batch_size):
            logger.info('\tprocessing example {0}'.format(i))
            end = min(n, i + batch_size)
            x = X[i:end, :].todense() - self.mean_
            assert x.shape[0] == end - i

            prod = numpy.dot(x.T, x)
            assert prod.shape == (d, d)

            cov += prod

        cov /= n

        logger.info('computing eigens')
        v, W = linalg.eigh(cov, eigvals=(d - self.num_components, d - 1))

        # The resulting components are in *ascending* order of eigenvalue, and
        # W contains eigenvectors in its *columns*, so we simply reverse both.
        v, W = v[::-1], W[:, ::-1]
        return v, W

    def train(self, X):
        """
        Compute the PCA transformation matrix.

        Given a rectangular matrix :math:`X = USV` such that :math:`S` is a
        diagonal matrix with :math:`X`'s singular values along its diagonal,
        returns :math:`W = V^{-1}`.

        If mean is provided, :math:`X` will not be centered first.

        Parameters
        ----------
        X : numpy.ndarray
            Matrix of shape (n, d) on which to train PCA
        """

        assert sparse.issparse(X)

        # Compute feature means.
        logger.info('computing mean')
        self.mean_ = numpy.asarray(X.mean(axis=0))[0, :]

        super(SparseMatPCA, self).train(X, mean=self.mean_)

    def __call__(self, inputs):
        """
        .. todo::

            WRITEME
        """

        self._update_cutoff()

        Y = structured_dot(inputs, self.W[:, :self.component_cutoff])
        Z = Y - tensor.dot(self.mean, self.W[:, :self.component_cutoff])

        #TODO-- this is inefficient, should work by modifying W not Z
        if self.whiten:
            Z /= tensor.sqrt(self.v[:self.component_cutoff])
        return Z

    def function(self, name=None):
        """
        Returns a compiled theano function to compute a representation

        Parameters
        ----------
        name : str
            WRITEME
        """
        inputs = SparseType('csr', dtype=theano.config.floatX)()
        return theano.function([inputs], self(inputs), name=name)


class OnlinePCA(_PCABase):
    """
    Online PCA implementation.

    Parameters
    ----------
    minibatch_size : WRITEME
    """
    def __init__(self, minibatch_size=500, **kwargs):
        super(OnlinePCA, self).__init__(**kwargs)
        self.minibatch_size = minibatch_size

    def _cov_eigen(self, X):
        """
        Perform online computation of covariance matrix eigen{values,vectors}.

        Parameters
        ----------
        X : WRITEME

        Returns
        -------
        WRITEME
        """
        num_components = min(self.num_components, X.shape[1])

        pca_estimator = PcaOnlineEstimator(X.shape[1],
            n_eigen=num_components,
            minibatch_size=self.minibatch_size,
            centering=False
        )

        logger.debug('*' * 50)
        for i in range(X.shape[0]):
            if (i + 1) % (X.shape[0] / 50) == 0:
                logger.debug('|')  # suppresses newline/whitespace.
            pca_estimator.observe(X[i, :])

        v, W = pca_estimator.getLeadingEigen()

        # The resulting components are in *ascending* order of eigenvalue,
        # and W contains eigenvectors in its *rows*, so we reverse both and
        # transpose W.
        return v[::-1], W.T[:, ::-1]


class Cov:
    """
    Covariance estimator

    It computes the covariance in small batches instead of with one
    huge matrix multiply, in order to prevent memory problems. Its call
    method has the same functionality as `numpy.cov`.

    Parameters
    ----------
    batch_size : WRITEME
    """

    def __init__(self, batch_size):
        self.batch_size = batch_size

    def __call__(self, X):
        """
        .. todo::

            WRITEME
        """
        X = X.T
        m, n = X.shape
        mean = X.mean(axis=0)
        rval = N.zeros((n, n))
        for i in xrange(0, m, self.batch_size):
            B = X[i:i + self.batch_size, :] - mean
            rval += N.dot(B.T, B)
        return rval / float(m - 1)


class CovEigPCA(_PCABase):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    cov_batch_size : WRITEME
    """

    def __init__(self, cov_batch_size=None, **kwargs):
        super(CovEigPCA, self).__init__(**kwargs)
        if cov_batch_size is not None:
            self.cov = Cov(cov_batch_size)
        else:
            self.cov = numpy.cov

    def _cov_eigen(self, X):
        """
        Perform direct computation of covariance matrix eigen{values,vectors}.

        Parameters
        ----------
        X : WRITEME

        Returns
        -------
        WRITEME
        """
        v, W = linalg.eigh(self.cov(X.T))
        # The resulting components are in *ascending* order of eigenvalue, and
        # W contains eigenvectors in its *columns*, so we simply reverse both.
        return v[::-1], W[:, ::-1]


class SVDPCA(_PCABase):
    """
    .. todo::

        WRITEME
    """

    def _cov_eigen(self, X):
        """
        Compute covariance matrix eigen{values,vectors} via Singular Value
        Decomposition (SVD).

        Parameters
        ----------
        X : WRITEME

        Returns
        -------
        WRITEME
        """
        U, s, Vh = linalg.svd(X, full_matrices=False)
        # Vh contains eigenvectors in its *rows*, thus we transpose it.
        # s contains X's singular values in *decreasing* order, thus (noting
        # that X's singular values are the sqrt of cov(X'X)'s eigenvalues), we
        # simply square it.
        return s ** 2, Vh.T


class SparsePCA(_PCABase):
    """
    .. todo::

        WRITEME
    """

    def train(self, X, mean=None):
        """
        .. todo::

            WRITEME
        """
        warnings.warn('You should probably be using SparseMatPCA, '
                      'unless your design matrix fits in memory.')

        n, d = X.shape
        # Can't subtract a sparse vector from a sparse matrix, apparently,
        # so here I repeat the vector to construct a matrix.
        mean = X.mean(axis=0)
        mean_matrix = csr_matrix(mean.repeat(n).reshape((d, n))).T
        X = X - mean_matrix

        super(SparsePCA, self).train(X, mean=numpy.asarray(mean).squeeze())

    def _cov_eigen(self, X):
        """
        Perform direct computation of covariance matrix eigen{values,vectors},
        given a scipy.sparse matrix.

        Parameters
        ----------
        X : WRITEME

        Returns
        -------
        WRITEME
        """

        v, W = eigen_symmetric(X.T.dot(X) / X.shape[0], k=self.num_components)

        # The resulting components are in *ascending* order of eigenvalue, and
        # W contains eigenvectors in its *columns*, so we simply reverse both.
        return v[::-1], W[:, ::-1]

    def __call__(self, inputs):
        """
        Compute and return the PCA transformation of sparse data.

        Precondition: `self.mean` has been subtracted from inputs. The reason
        for this is that, as far as I can tell, there is no way to subtract a
        vector from a sparse matrix without constructing an intermediary dense
        matrix, in theano; even the hack used in `train()` won't do, because
        there is no way to symbolically construct a sparse matrix by repeating
        a vector (again, as far as I can tell).

        Parameters
        ----------
        inputs : scipy.sparse matrix object
            Sparse matrix of shape (n, d) on which to compute PCA

        Returns
        -------
        WRITEME
        """

        # Update component cutoff, in case min_variance or num_components has
        # changed (or both).
        self._update_cutoff()

        Y = structured_dot(inputs, self.W[:, :self.component_cutoff])
        if self.whiten:
            Y /= tensor.sqrt(self.v[:self.component_cutoff])
        return Y

    def function(self, name=None):
        """
        Returns a compiled theano function to compute a representation

        Parameters
        ----------
        name : str
            WRITEME

        Returns
        -------
        WRITEME
        """
        inputs = SparseType('csr', dtype=theano.config.floatX)()
        return theano.function([inputs], self(inputs), name=name)



#############################################################################

class PcaOnlineEstimator(object):
    """
    Online estimation of the leading eigen values/vectors of the covariance of
    some samples.

    Maintains a moving (with discount) low rank (n_eigen) estimate of the
    covariance matrix of some observations. New observations are accumulated
    until the batch is complete, at which point the low rank estimate is
    reevaluated.

    Example:

      pca_esti = \
              pca_online_estimator.PcaOnlineEstimator(dimension_of_the_samples)

      for i in range(number_of_samples):
        pca_esti.observe(samples[i])

      [eigvals, eigvecs] = pca_esti.getLeadingEigen()

    Parameters
    ----------
    n_dim : WRITEME
    n_eigen : WRITEME
    minibatch_size : WRITEME
    gamma : WRITEME
    regularizer : WRITEME
    centering : WRITEME
    """


    def __init__(self, n_dim, n_eigen=10, minibatch_size=25, gamma=0.999,
                 regularizer=1e-6, centering=True):
        # dimension of the observations
        self.n_dim = n_dim
        # rank of the low-rank estimate
        self.n_eigen = n_eigen
        # how many observations between reevaluations of the low rank estimate
        self.minibatch_size = minibatch_size
        # the discount factor in the moving estimate
        self.gamma = gamma
        # regularizer of the covariance estimate
        self.regularizer = regularizer
        # wether we center the observations or not: obtain leading eigen of
        # covariance (centering = True) vs second moment (centering = False)
        self.centering = centering

        # Total number of observations: to compute the normalizer for the mean
        # and the covariance.
        self.n_observations = 0
        # Index in the current minibatch
        self.minibatch_index = 0

        # Matrix containing on its *rows*:
        # - the current unnormalized eigen vector estimates
        # - the observations since the last reevaluation
        self.Xt = numpy.zeros([self.n_eigen + self.minibatch_size, self.n_dim])

        # The discounted sum of the observations.
        self.x_sum = numpy.zeros([self.n_dim])

        # The Gram matrix of the observations, ie Xt Xt' (since Xt is rowwise)
        self.G = numpy.zeros([self.n_eigen + self.minibatch_size,
                              self.n_eigen + self.minibatch_size])
        for i in range(self.n_eigen):
            self.G[i,i] = self.regularizer

        # I don't think it's worth "allocating" these 3 next (though they need
        # to be # declared). I don't know how to do in place operations...

        # Hold the results of the eigendecomposition of the Gram matrix G
        # (eigen vectors on columns of V).
        self.d = numpy.zeros([self.n_eigen + self.minibatch_size])
        self.V = numpy.zeros([self.n_eigen + self.minibatch_size,
                              self.n_eigen + self.minibatch_size])

        # Holds the unnormalized eigenvectors of the covariance matrix before
        # they're copied back to Xt.
        self.Ut = numpy.zeros([self.n_eigen, self.n_dim])

    def observe(self, x):
        """
        .. todo::

            WRITEME
        """
        assert(numpy.size(x) == self.n_dim)

        self.n_observations += 1

        # Add the *non-centered* observation to Xt.
        row = self.n_eigen + self.minibatch_index
        self.Xt[row] = x

        # Update the discounted sum of the observations.
        self.x_sum *= self.gamma
        self.x_sum += x

        # To get the mean, we must normalize the sum by:
        # \gamma^(n_observations-1) + /gamma^(n_observations-2) + ... + 1
        normalizer = (1.0 - pow(self.gamma, self.n_observations)) / \
                     (1.0 - self.gamma)

        # Now center the observation.
        # We will lose the first observation as it is the only one in the mean.
        if self.centering:
            self.Xt[row] -= self.x_sum / normalizer

        # Multiply the observation by the discount compensator. Basically we
        # make this observation look "younger" than the previous ones. The
        # actual discount is applied in the reevaluation (and when solving the
        # equations in the case of TONGA) by multiplying every direction with
        # the same aging factor.
        rn = pow(self.gamma, -0.5*(self.minibatch_index+1));
        self.Xt[row] *= rn

        # Update the Gram Matrix.
        # The column.
        self.G[:row+1,row] = numpy.dot(self.Xt[:row+1,:],
                                       self.Xt[row,:].transpose())
        # The symetric row.
        # There are row+1 values, but the diag doesn't need to get copied.
        self.G[row,:row] = self.G[:row,row].transpose()

        self.minibatch_index += 1

        if self.minibatch_index == self.minibatch_size:
            self.reevaluate()


    def reevaluate(self):
        """
        .. todo::

            WRITEME
        """
        # TODO do the modifications to handle when this is not true.
        assert(self.minibatch_index == self.minibatch_size);

        # Regularize - not necessary but in case
        for i in range(self.n_eigen + self.minibatch_size):
            self.G[i,i] += self.regularizer

        # The Gram matrix is up to date. Get its low rank eigendecomposition.
        # NOTE: the eigenvalues are in ASCENDING order and the vectors are on
        # the columns.
        # With scipy 0.7, you can ask for only some eigenvalues (the n_eigen
        # top ones) but it doesn't look loke it for scipy 0.6.
        self.d, self.V = linalg.eigh(self.G)

        # Convert the n_eigen LAST eigenvectors of the Gram matrix contained in
        # V into *unnormalized* eigenvectors U of the covariance (unnormalized
        # wrt the eigen values, not the moving average).
        self.Ut = numpy.dot(self.V[:,-self.n_eigen:].transpose(), self.Xt)

        # Take into account the discount factor.
        # Here, minibatch index is minibatch_size. We age everyone. Because of
        # the previous multiplications to make some observations "younger" we
        # multiply everyone by the same factor.
        # TODO VERIFY THIS!
        rn = pow(self.gamma, -0.5*(self.minibatch_index+1))
        inv_rn2 = 1.0/(rn*rn)
        self.Ut *= 1.0/rn
        self.d *= inv_rn2;

        # Update Xt, G and minibatch_index
        self.Xt[:self.n_eigen,:] = self.Ut

        for i in range(self.n_eigen):
            self.G[i,i] = self.d[-self.n_eigen+i]

        self.minibatch_index = 0

    # Returns a copy of the current estimate of the eigen values and vectors
    # (normalized vectors on rows), normalized by the discounted number of
    # observations.
    def getLeadingEigen(self):
        """
        .. todo::

            WRITEME
        """
        # We subtract self.minibatch_index in case this call is not right
        # after a reevaluate call.
        normalizer = (1.0 - pow(self.gamma,
                                self.n_observations - self.minibatch_index)) /\
                     (1.0 - self.gamma)

        eigvals = self.d[-self.n_eigen:] / normalizer
        eigvecs = numpy.zeros([self.n_eigen, self.n_dim])
        for i in range(self.n_eigen):
            eigvecs[i] = self.Ut[-self.n_eigen+i] / \
                         numpy.sqrt(numpy.dot(self.Ut[-self.n_eigen+i],
                                              self.Ut[-self.n_eigen+i]))

        return [eigvals, eigvecs]

#############################################################################

if __name__ == "__main__":
    """
    Load a dataset; compute a PCA transformation matrix from the training
    subset and pickle it (or load a previously computed one); apply said
    transformation to the test and valid subsets.
    """

    import argparse
    from pylearn2.utils import load_data

    parser = argparse.ArgumentParser(
        description="Transform the output of a model by Principal Component"
                    " Analysis"
    )
    parser.add_argument('dataset', action='store',
                        type=str,
                        choices=['avicenna', 'harry', 'rita', 'sylvester',
                                 'ule'],
                        help='Dataset on which to compute and apply the PCA')
    parser.add_argument('-i', '--load-file', action='store',
                        type=str,
                        default=None,
                        required=False,
                        help='File containing precomputed PCA (if any)')
    parser.add_argument('-o', '--save-file', action='store',
                        type=str,
                        default='model-pca.pkl',
                        required=False,
                        help='File where the PCA pickle will be saved')
    parser.add_argument('-a', '--algorithm', action='store',
                        type=str,
                        choices=['cov_eig', 'svd', 'online'],
                        default='cov_eig',
                        required=False,
                        help='Which algorithm to use to compute the PCA')
    parser.add_argument('-m', '--minibatch-size', action='store',
                        type=int,
                        default=500,
                        required=False,
                        help='Size of minibatches used in online algorithm')
    parser.add_argument('-n', '--num-components', action='store',
                        type=int,
                        default=None,
                        required=False,
                        help='This many most components will be preserved')
    parser.add_argument('-v', '--min-variance', action='store',
                        type=float,
                        default=0.0,
                        required=False,
                        help="Components with variance below this threshold"
                            " will be discarded")
    parser.add_argument('-w', '--whiten', action='store_const',
                        default=False,
                        const=True,
                        required=False,
                        help='Divide projected features by their '
                             'standard deviation')
    args = parser.parse_args()
    # Load dataset.
    data = load_data({'dataset': args.dataset})
    # TODO: this can be done more efficiently and readably by list
    # comprehensions
    train_data, valid_data, test_data = map(lambda(x):
                                            x.get_value(borrow=True), data)
    logger.info("Dataset shapes: {0}".format(map(lambda(x):
                                             x.get_value().shape, data)))
    # PCA base-class constructor arguments.
    conf = {
        'num_components': args.num_components,
        'min_variance': args.min_variance,
        'whiten': args.whiten
    }

    # Set PCA subclass from argument.
    if args.algorithm == 'cov_eig':
        PCAImpl = CovEigPCA
    elif args.algorithm == 'svd':
        PCAImpl = SVDPCA
    elif args.algorithm == 'online':
        PCAImpl = OnlinePCA
        conf['minibatch_size'] = args.minibatch_size
    else:
        # This should never happen.
        raise NotImplementedError(args.algorithm)

    # Load precomputed PCA transformation if requested; otherwise compute it.
    if args.load_file:
        pca = Block.load(args.load_file)
    else:
        logger.info("... computing PCA")
        pca = PCAImpl(**conf)
        pca.train(train_data)
        # Save the computed transformation.
        pca.save(args.save_file)

    # Apply the transformation to test and valid subsets.
    inputs = tensor.matrix()
    pca_transform = theano.function([inputs], pca(inputs))
    valid_pca = pca_transform(valid_data)
    test_pca = pca_transform(test_data)
    logger.info("New shapes: {0}".format(map(numpy.shape,
                                         [valid_pca, test_pca])))

    # TODO: Compute ALC here when the code using the labels is ready.

########NEW FILE########
__FILENAME__ = rbm
"""
Implementations of Restricted Boltzmann Machines and associated sampling
strategies.
"""
# Standard library imports
from itertools import izip
import logging

# Third-party imports
import numpy
N = numpy
np = numpy
import theano
from theano import tensor
T = tensor
from theano.tensor import nnet
from pylearn2.costs.cost import Cost

# Local imports
from pylearn2.blocks import Block, StackedBlocks
from pylearn2.utils import as_floatX, safe_update, sharedX
from pylearn2.models import Model
from pylearn2.expr.nnet import inverse_sigmoid_numpy
from pylearn2.linear.matrixmul import MatrixMul
from pylearn2.space import VectorSpace
from pylearn2.utils import safe_union
from pylearn2.utils.rng import make_np_rng, make_theano_rng
theano.config.warn.sum_div_dimshuffle_bug = False

logger = logging.getLogger(__name__)

if 0:
    logger.warning('using SLOW rng')
    RandomStreams = tensor.shared_randomstreams.RandomStreams
else:
    import theano.sandbox.rng_mrg
    RandomStreams = theano.sandbox.rng_mrg.MRG_RandomStreams


def training_updates(visible_batch, model, sampler, optimizer):
    """
    Combine together updates from various sources for RBM training.

    Parameters
    ----------
    visible_batch : tensor_like
        Theano symbolic representing a minibatch on the visible units,
        with the first dimension indexing training examples and the second
        indexing data dimensions.
    model : object
        An instance of `RBM` or a derived class, or one implementing
        the RBM interface.
    sampler : object
        An instance of `Sampler` or a derived class, or one implementing
        the sampler interface.
    optimizer : object
        An instance of `_Optimizer` or a derived class, or one implementing
        the optimizer interface (typically an `_SGDOptimizer`).

    Returns
    -------
    WRITEME
    """
    # TODO: the Optimizer object got deprecated, and this is the only
    #         functionality that requires it. We moved the Optimizer
    #         here with an _ before its name.
    #         We should figure out how best to refactor the code.
    #         Optimizer was problematic because people kept using SGDOptimizer
    #         instead of training_algorithms.sgd.
    # Compute negative phase updates.
    sampler_updates = sampler.updates()
    # Compute SML gradients.
    pos_v = visible_batch
    #neg_v = sampler_updates[sampler.particles]
    neg_v = sampler.particles
    grads = model.ml_gradients(pos_v, neg_v)
    # Build updates dictionary combining (gradient, sampler) updates.
    ups = optimizer.updates(gradients=grads)
    safe_update(ups, sampler_updates)
    return ups


class Sampler(object):
    """
    A sampler is responsible for implementing a sampling strategy on top of
    an RBM, which may include retaining state e.g. the negative particles for
    Persistent Contrastive Divergence.

    Parameters
    ----------
    rbm : object
        An instance of `RBM` or a derived class, or one implementing
        the `gibbs_step_for_v` interface.
    particles : numpy.ndarray
        An initial state for the set of persistent Narkov chain particles
        that will be updated at every step of learning.
    rng : RandomState object
        NumPy random number generator object used to initialize a
        RandomStreams object used in training.
    """
    def __init__(self, rbm, particles, rng):
        self.__dict__.update(rbm=rbm)

        rng = make_np_rng(rng, which_method="randn")
        seed = int(rng.randint(2 ** 30))
        self.s_rng = make_theano_rng(seed, which_method="binomial")
        self.particles = sharedX(particles, name='particles')

    def updates(self):
        """
        Get the dictionary of updates for the sampler's persistent state
        at each step.

        Returns
        -------
        updates : dict
            Dictionary with shared variable instances as keys and symbolic
            expressions indicating how they should be updated as values.

        Notes
        -----
        In the `Sampler` base class, this is simply a stub.
        """
        raise NotImplementedError()


class BlockGibbsSampler(Sampler):
    """
    Implements a persistent Markov chain based on block gibbs sampling
    for use with Persistent Contrastive
    Divergence, a.k.a. stochastic maximum likelhiood, as described in [1].

    .. [1] T. Tieleman. "Training Restricted Boltzmann Machines using
       approximations to the likelihood gradient". Proceedings of the 25th
       International Conference on Machine Learning, Helsinki, Finland,
       2008. http://www.cs.toronto.edu/~tijmen/pcd/pcd.pdf

    Parameters
    ----------
    rbm : object
        An instance of `RBM` or a derived class, or one implementing
        the `gibbs_step_for_v` interface.
    particles : ndarray
        An initial state for the set of persistent Markov chain particles
        that will be updated at every step of learning.
    rng : RandomState object
        NumPy random number generator object used to initialize a
        RandomStreams object used in training.
    steps : int, optional
        Number of Gibbs steps to run the Markov chain for at each
        iteration.
    particles_clip : None or (min, max) pair, optional
        The values of the returned particles will be clipped between
        min and max.
    """
    def __init__(self, rbm, particles, rng, steps=1, particles_clip=None):
        super(BlockGibbsSampler, self).__init__(rbm, particles, rng)
        self.steps = steps
        self.particles_clip = particles_clip

    def updates(self, particles_clip=None):
        """
        Get the dictionary of updates for the sampler's persistent state
        at each step.

        Parameters
        ----------
        particles_clip : WRITEME

        Returns
        -------
        updates : dict
            Dictionary with shared variable instances as keys and symbolic
            expressions indicating how they should be updated as values.
        """
        steps = self.steps
        particles = self.particles
        # TODO: do this with scan?
        for i in xrange(steps):
            particles, _locals = self.rbm.gibbs_step_for_v(
                particles,
                self.s_rng
            )
            assert particles.type.dtype == self.particles.type.dtype
            if self.particles_clip is not None:
                p_min, p_max = self.particles_clip
                # The clipped values should still have the same type
                dtype = particles.dtype
                p_min = tensor.as_tensor_variable(p_min)
                if p_min.dtype != dtype:
                    p_min = tensor.cast(p_min, dtype)
                p_max = tensor.as_tensor_variable(p_max)
                if p_max.dtype != dtype:
                    p_max = tensor.cast(p_max, dtype)
                particles = tensor.clip(particles, p_min, p_max)
        if not hasattr(self.rbm, 'h_sample'):
            self.rbm.h_sample = sharedX(numpy.zeros((0, 0)), 'h_sample')
        return {
            self.particles: particles,
            # TODO: self.rbm.h_sample is never used, why is that here?
            # Moreover, it does not make sense for things like ssRBM.
            self.rbm.h_sample: _locals['h_mean']
        }


class RBM(Block, Model):
    """
    A base interface for RBMs, implementing the binary-binary case.

    Parameters
    ----------
    nvis : int, optional
        Number of visible units in the model.
        (Specifying this implies that the model acts on a vector,
        i.e. it sets vis_space = pylearn2.space.VectorSpace(nvis) )
    nhid : int, optional
        Number of hidden units in the model.
        (Specifying this implies that the model acts on a vector)
    vis_space : pylearn2.space.Space, optional
        Space object describing what kind of vector space the RBM acts
        on. Don't specify if you used nvis / hid
    hid_space: pylearn2.space.Space, optional
        Space object describing what kind of vector space the RBM's
        hidden units live in. Don't specify if you used nvis / nhid
    transformer : WRITEME
    irange : float, optional
        The size of the initial interval around 0 for weights.
    rng : RandomState object or seed, optional
        NumPy RandomState object to use when initializing parameters
        of the model, or (integer) seed to use to create one.
    init_bias_vis : array_like, optional
        Initial value of the visible biases, broadcasted as necessary.
    init_bias_vis_marginals : pylearn2.datasets.dataset.Dataset or None
        Optional. Dataset used to initialize the visible biases to the
        inverse sigmoid of the data marginals
    init_bias_hid : array_like, optional
        initial value of the hidden biases, broadcasted as necessary.
    base_lr : float, optional
        The base learning rate
    anneal_start : int, optional
        Number of steps after which to start annealing on a 1/t schedule
    nchains : int, optional
        Number of negative chains
    sml_gibbs_steps : int, optional
        Number of gibbs steps to take per update
    random_patches_src : pylearn2.datasets.dataset.Dataset or None
        Optional. Dataset from which to draw random patches in order to
        initialize the weights. Patches will be multiplied by irange.
    monitor_reconstruction : bool, optional
        If True, will request a monitoring channel to monitor
        reconstruction error

    Notes
    -----
    The `RBM` class is redundant now that we have a `DBM` class, since
    an RBM is just a DBM with one hidden layer. Users of pylearn2 should
    use single-layer DBMs when possible. Not all RBM functionality has
    been ported to the DBM framework yet, so this is not always possible.
    (Examples: spike-and-slab RBMs, score matching, denoising score matching)
    pylearn2 developers should not add new features to the RBM class or
    add new RBM subclasses. pylearn2 developers should only add documentation
    and bug fixes to the RBM class and subclasses. pylearn2 developers should
    finish porting all RBM functionality to the DBM framework, then turn
    the RBM class into a thin wrapper around the DBM class that allocates
    a single layer DBM.
    """

    def __init__(self, nvis = None, nhid = None,
            vis_space = None,
            hid_space = None,
            transformer = None,
            irange=0.5, rng=None, init_bias_vis = None,
            init_bias_vis_marginals = None, init_bias_hid=0.0,
            base_lr = 1e-3, anneal_start = None, nchains = 100,
            sml_gibbs_steps = 1,
            random_patches_src = None,
            monitor_reconstruction = False):

        Model.__init__(self)
        Block.__init__(self)

        if init_bias_vis_marginals is not None:
            assert init_bias_vis is None
            X = init_bias_vis_marginals.X
            assert X.min() >= 0.0
            assert X.max() <= 1.0

            marginals = X.mean(axis=0)

            #rescale the marginals a bit to avoid NaNs
            init_bias_vis = inverse_sigmoid_numpy(.01 + .98 * marginals)


        if init_bias_vis is None:
            init_bias_vis = 0.0

        rng = make_np_rng(rng, 1001, which_method="uniform")
        self.rng = rng

        if vis_space is None:
            #if we don't specify things in terms of spaces and a transformer,
            #assume dense matrix multiplication and work off of nvis, nhid
            assert hid_space is None
            assert transformer is None or isinstance(transformer,MatrixMul)
            assert nvis is not None
            assert nhid is not None

            if transformer is None:
                if random_patches_src is None:
                    W = rng.uniform(-irange, irange, (nvis, nhid))
                else:
                    if hasattr(random_patches_src, '__array__'):
                        W = irange * random_patches_src.T
                        assert W.shape == (nvis, nhid)
                    else:
                        W = irange * random_patches_src.get_batch_design(
                                nhid).T

                self.transformer = MatrixMul(  sharedX(
                        W,
                        name='W',
                        borrow=True
                    )
                )
            else:
                self.transformer = transformer

            self.vis_space = VectorSpace(nvis)
            self.hid_space = VectorSpace(nhid)
        else:
            assert hid_space is not None
            assert transformer is not None
            assert nvis is None
            assert nhid is None

            self.vis_space = vis_space
            self.hid_space = hid_space
            self.transformer = transformer


        try:
            b_vis = self.vis_space.get_origin()
            b_vis += init_bias_vis
        except ValueError:
            raise ValueError("bad shape or value for init_bias_vis")
        self.bias_vis = sharedX(b_vis, name='bias_vis', borrow=True)

        try:
            b_hid = self.hid_space.get_origin()
            b_hid += init_bias_hid
        except ValueError:
            raise ValueError('bad shape or value for init_bias_hid')
        self.bias_hid = sharedX(b_hid, name='bias_hid', borrow=True)

        self.random_patches_src = random_patches_src
        self.register_names_to_del(['random_patches_src'])


        self.__dict__.update(nhid=nhid, nvis=nvis)
        self._params = safe_union(self.transformer.get_params(),
                [self.bias_vis, self.bias_hid])

        self.base_lr = base_lr
        self.anneal_start = anneal_start
        self.nchains = nchains
        self.sml_gibbs_steps = sml_gibbs_steps

    def get_default_cost(self):
        """
        .. todo::

            WRITEME
        """
        raise NotImplementedError("The RBM class predates the current "
                "Cost-based training algorithms (SGD and BGD). To train "
                "the RBM with PCD, use DefaultTrainingAlgorithm rather "
                "than SGD or BGD. Some RBM subclassess may also be "
                "trained with SGD or BGD by using the "
                "Cost classes defined in pylearn2.costs.ebm_estimation. "
                "Note that it is also possible to make an RBM by allocating "
                "a DBM with only one hidden layer. The DBM class is newer "
                "and supports training with SGD / BGD. In the long run we "
                "should remove the old RBM class and turn it into a wrapper "
                "around the DBM class that makes a 1-layer DBM.")

    def get_input_dim(self):
        """
        Returns
        -------
        dim : int
            The number of elements in the input, if the input is a vector.
        """
        if not isinstance(self.vis_space, VectorSpace):
            raise TypeError("Can't describe " + str(type(self.vis_space))
                    + " as a dimensionality number.")

        return self.vis_space.dim

    def get_output_dim(self):
        """
        Returns
        -------
        dim : int
            The number of elements in the output, if the output is a vector.
        """
        if not isinstance(self.hid_space, VectorSpace):
            raise TypeError("Can't describe " + str(type(self.hid_space))
                    + " as a dimensionality number.")
        return self.hid_space.dim

    def get_input_space(self):
        """
        .. todo::

            WRITEME
        """
        return self.vis_space

    def get_output_space(self):
        """
        .. todo::

            WRITEME
        """
        return self.hid_space

    def get_params(self):
        """
        .. todo::

            WRITEME
        """
        return [param for param in self._params]

    def get_weights(self, borrow=False):
        """
        .. todo::

            WRITEME
        """

        weights ,= self.transformer.get_params()

        return weights.get_value(borrow=borrow)

    def get_weights_topo(self):
        """
        .. todo::

            WRITEME
        """
        return self.transformer.get_weights_topo()

    def get_weights_format(self):
        """
        .. todo::

            WRITEME
        """
        return ['v', 'h']


    def get_monitoring_channels(self, data):
        """
        .. todo::

            WRITEME
        """
        V = data
        theano_rng = make_theano_rng(None, 42, which_method="binomial")

        H = self.mean_h_given_v(V)

        h = H.mean(axis=0)

        return { 'bias_hid_min' : T.min(self.bias_hid),
                 'bias_hid_mean' : T.mean(self.bias_hid),
                 'bias_hid_max' : T.max(self.bias_hid),
                 'bias_vis_min' : T.min(self.bias_vis),
                 'bias_vis_mean' : T.mean(self.bias_vis),
                 'bias_vis_max': T.max(self.bias_vis),
                 'h_min' : T.min(h),
                 'h_mean': T.mean(h),
                 'h_max' : T.max(h),
                'reconstruction_error' : self.reconstruction_error(V,
                    theano_rng) }

    def get_monitoring_data_specs(self):
        """
        Get the data_specs describing the data for get_monitoring_channel.

        This implementation returns specification corresponding to unlabeled
        inputs.

        Returns
        -------
        WRITEME
        """
        return (self.get_input_space(), self.get_input_source())

    def ml_gradients(self, pos_v, neg_v):
        """
        Get the contrastive gradients given positive and negative phase
        visible units.

        Parameters
        ----------
        pos_v : tensor_like
            Theano symbolic representing a minibatch on the visible units,
            with the first dimension indexing training examples and the
            second indexing data dimensions (usually actual training data).
        neg_v : tensor_like
            Theano symbolic representing a minibatch on the visible units,
            with the first dimension indexing training examples and the
            second indexing data dimensions (usually reconstructions of the
            data or sampler particles from a persistent Markov chain).

        Returns
        -------
        grads : list
            List of Theano symbolic variables representing gradients with
            respect to model parameters, in the same order as returned by
            `params()`.

        Notes
        -----
        `pos_v` and `neg_v` need not have the same first dimension, i.e.
        minibatch size.
        """

        # taking the mean over each term independently allows for different
        # mini-batch sizes in the positive and negative phase.
        ml_cost = (self.free_energy_given_v(pos_v).mean() -
                   self.free_energy_given_v(neg_v).mean())

        grads = tensor.grad(ml_cost, self.get_params(),
                            consider_constant=[pos_v, neg_v])

        return grads


    def train_batch(self, dataset, batch_size):
        """
        .. todo::

            WRITEME properly

        A default learning rule based on SML
        """
        self.learn_mini_batch(dataset.get_batch_design(batch_size))
        return True

    def learn_mini_batch(self, X):
        """
        .. todo::

            WRITEME

        A default learning rule based on SML
        """

        if not hasattr(self, 'learn_func'):
            self.redo_theano()

        rval =  self.learn_func(X)

        return rval

    def redo_theano(self):
        """
        Compiles the theano function for the default learning rule
        """

        init_names = dir(self)

        minibatch = tensor.matrix()

        optimizer = _SGDOptimizer(self, self.base_lr, self.anneal_start)

        sampler = sampler = BlockGibbsSampler(self, 0.5 + np.zeros((
            self.nchains, self.get_input_dim())), self.rng,
            steps= self.sml_gibbs_steps)


        updates = training_updates(visible_batch=minibatch, model=self,
                                   sampler=sampler, optimizer=optimizer)

        self.learn_func = theano.function([minibatch], updates=updates)

        final_names = dir(self)

        self.register_names_to_del([name for name in final_names
            if name not in init_names])

    def gibbs_step_for_v(self, v, rng):
        """
        Do a round of block Gibbs sampling given visible configuration

        Parameters
        ----------
        v : tensor_like
            Theano symbolic representing the hidden unit states for a batch
            of training examples (or negative phase particles), with the
            first dimension indexing training examples and the second
            indexing data dimensions.
        rng : RandomStreams object
            Random number generator to use for sampling the hidden and
            visible units.

        Returns
        -------
        v_sample : tensor_like
            Theano symbolic representing the new visible unit state after one
            round of Gibbs sampling.
        locals : dict
            Contains the following auxiliary state as keys (all symbolics
            except shape tuples):

              * `h_mean`: the returned value from `mean_h_given_v`
              * `h_mean_shape`: shape tuple indicating the size of
                `h_mean` and `h_sample`
              * `h_sample`: the stochastically sampled hidden units
              * `v_mean_shape`: shape tuple indicating the shape of
                `v_mean` and `v_sample`
              * `v_mean`: the returned value from `mean_v_given_h`
              * `v_sample`: the stochastically sampled visible units
        """
        h_mean = self.mean_h_given_v(v)
        assert h_mean.type.dtype == v.type.dtype
        # For binary hidden units
        # TODO: factor further to extend to other kinds of hidden units
        #       (e.g. spike-and-slab)
        h_sample = rng.binomial(size = h_mean.shape, n = 1 , p = h_mean,
            dtype=h_mean.type.dtype)
        assert h_sample.type.dtype == v.type.dtype
        # v_mean is always based on h_sample, not h_mean, because we don't
        # want h transmitting more than one bit of information per unit.
        v_mean = self.mean_v_given_h(h_sample)
        assert v_mean.type.dtype == v.type.dtype
        v_sample = self.sample_visibles([v_mean], v_mean.shape, rng)
        assert v_sample.type.dtype == v.type.dtype
        return v_sample, locals()

    def sample_visibles(self, params, shape, rng):
        """
        Stochastically sample the visible units given hidden unit
        configurations for a set of training examples.

        Parameters
        ----------
        params : list
            List of the necessary parameters to sample :math:`p(v|h)`. In the
            case of a binary-binary RBM this is a single-element list
            containing the symbolic representing :math:`p(v|h)`, as returned
            by `mean_v_given_h`.

        Returns
        -------
        vprime : tensor_like
            Theano symbolic representing stochastic samples from :math:`p(v|h)`
        """
        v_mean = params[0]
        return as_floatX(rng.uniform(size=shape) < v_mean)

    def input_to_h_from_v(self, v):
        """
        Compute the affine function (linear map plus bias) that serves as
        input to the hidden layer in an RBM.

        Parameters
        ----------
        v : tensor_like or list of tensor_likes
            Theano symbolic (or list thereof) representing the one or several
            minibatches on the visible units, with the first dimension
            indexing training examples and the second indexing data dimensions.

        Returns
        -------
        a : tensor_like or list of tensor_likes
            Theano symbolic (or list thereof) representing the input to each
            hidden unit for each training example.
        """

        if isinstance(v, tensor.Variable):
            return self.bias_hid + self.transformer.lmul(v)
        else:
            return [self.input_to_h_from_v(vis) for vis in v]

    def input_to_v_from_h(self, h):
        """
        Compute the affine function (linear map plus bias) that serves as
        input to the visible layer in an RBM.

        Parameters
        ----------
        h : tensor_like or list of tensor_likes
            Theano symbolic (or list thereof) representing the one or several
            minibatches on the hidden units, with the first dimension
            indexing training examples and the second indexing data dimensions.

        Returns
        -------
        a : tensor_like or list of tensor_likes
            Theano symbolic (or list thereof) representing the input to each
            visible unit for each row of h.
        """
        if isinstance(h, tensor.Variable):
            return self.bias_vis + self.transformer.lmul_T(h)
        else:
            return [self.input_to_v_from_h(hid) for hid in h]

    def upward_pass(self, v):
        """
        Wrapper around mean_h_given_v method.  Called when RBM is accessed
        by mlp.HiddenLayer.
        """
        return self.mean_h_given_v(v)

    def mean_h_given_v(self, v):
        """
        Compute the mean activation of the hidden units given visible unit
        configurations for a set of training examples.

        Parameters
        ----------
        v : tensor_like or list of tensor_likes
            Theano symbolic (or list thereof) representing the hidden unit
            states for a batch (or several) of training examples, with the
            first dimension indexing training examples and the second
            indexing data dimensions.

        Returns
        -------
        h : tensor_like or list of tensor_likes
            Theano symbolic (or list thereof) representing the mean
            (deterministic) hidden unit activations given the visible units.
        """
        if isinstance(v, tensor.Variable):
            return nnet.sigmoid(self.input_to_h_from_v(v))
        else:
            return [self.mean_h_given_v(vis) for vis in v]

    def mean_v_given_h(self, h):
        """
        Compute the mean activation of the visibles given hidden unit
        configurations for a set of training examples.

        Parameters
        ----------
        h : tensor_like or list of tensor_likes
            Theano symbolic (or list thereof) representing the hidden unit
            states for a batch (or several) of training examples, with the
            first dimension indexing training examples and the second
            indexing hidden units.

        Returns
        -------
        vprime : tensor_like or list of tensor_likes
            Theano symbolic (or list thereof) representing the mean
            (deterministic) reconstruction of the visible units given the
            hidden units.
        """
        if isinstance(h, tensor.Variable):
            return nnet.sigmoid(self.input_to_v_from_h(h))
        else:
            return [self.mean_v_given_h(hid) for hid in h]

    def free_energy_given_v(self, v):
        """
        Calculate the free energy of a visible unit configuration by
        marginalizing over the hidden units.

        Parameters
        ----------
        v : tensor_like
            Theano symbolic representing the hidden unit states for a batch
            of training examples, with the first dimension indexing training
            examples and the second indexing data dimensions.

        Returns
        -------
        f : tensor_like
            1-dimensional tensor (vector) representing the free energy
            associated with each row of v.
        """
        sigmoid_arg = self.input_to_h_from_v(v)
        return (-tensor.dot(v, self.bias_vis) -
                 nnet.softplus(sigmoid_arg).sum(axis=1))

    def free_energy(self, V):
        return self.free_energy_given_v(V)


    def free_energy_given_h(self, h):
        """
        Calculate the free energy of a hidden unit configuration by
        marginalizing over the visible units.

        Parameters
        ----------
        h : tensor_like
            Theano symbolic representing the hidden unit states, with the
            first dimension indexing training examples and the second
            indexing data dimensions.

        Returns
        -------
        f : tensor_like
            1-dimensional tensor (vector) representing the free energy
            associated with each row of v.
        """
        sigmoid_arg = self.input_to_v_from_h(h)
        return (-tensor.dot(h, self.bias_hid) -
                nnet.softplus(sigmoid_arg).sum(axis=1))

    def __call__(self, v):
        """
        Forward propagate (symbolic) input through this module, obtaining
        a representation to pass on to layers above.

        This just aliases the `mean_h_given_v()` function for syntactic
        sugar/convenience.
        """
        return self.mean_h_given_v(v)

    def reconstruction_error(self, v, rng):
        """
        Compute the mean-squared error (mean over examples, sum over units)
        across a minibatch after a Gibbs step starting from the training data.

        Parameters
        ----------
        v : tensor_like
            Theano symbolic representing the hidden unit states for a batch
            of training examples, with the first dimension indexing training
            examples and the second indexing data dimensions.
        rng : RandomStreams object
            Random number generator to use for sampling the hidden and
            visible units.

        Returns
        -------
        mse : tensor_like
            0-dimensional tensor (essentially a scalar) indicating the mean
            reconstruction error across the minibatch.

        Notes
        -----
        The reconstruction used to assess error samples only the hidden
        units. For the visible units, it uses the conditional mean. No sampling
        of the visible units is done, to reduce noise in the estimate.
        """
        sample, _locals = self.gibbs_step_for_v(v, rng)
        return ((_locals['v_mean'] - v) ** 2).sum(axis=1).mean()


class GaussianBinaryRBM(RBM):
    """
    An RBM with Gaussian visible units and binary hidden units.

    Parameters
    ----------
    energy_function_class : WRITEME
    nvis : int, optional
        Number of visible units in the model.
    nhid : int, optional
        Number of hidden units in the model.
    vis_space : WRITEME
    hid_space : WRITEME
    irange : float, optional
        The size of the initial interval around 0 for weights.
    rng : RandomState object or seed, optional
        NumPy RandomState object to use when initializing parameters
        of the model, or (integer) seed to use to create one.
    mean_vis : bool, optional
        Don't actually sample visibles; make sample method simply return
        mean.
    init_sigma : float or numpy.ndarray, optional
        Initial value of the sigma variable. If init_sigma is a scalar
        and sigma is not, will be broadcasted.
    learn_sigma : bool, optional
        WRITEME
    sigma_lr_scale : float, optional
        WRITEME
    init_bias_hid : scalar or 1-d array of length `nhid`
        Initial value for the biases on hidden units.
    min_sigma, max_sigma : float, float, optional
        Elements of sigma are clipped to this range during learning
    """

    def __init__(self, energy_function_class,
            nvis = None,
            nhid = None,
            vis_space = None,
            hid_space = None,
            transformer = None,
            irange=0.5, rng=None,
                 mean_vis=False, init_sigma=2., learn_sigma=False,
                 sigma_lr_scale=1., init_bias_hid=0.0,
                 min_sigma = .1, max_sigma = 10.):
        super(GaussianBinaryRBM, self).__init__(nvis = nvis, nhid = nhid,
                                                transformer = transformer,
                                                vis_space = vis_space,
                                                hid_space = hid_space,
                                                irange = irange, rng = rng,
                                                init_bias_hid = init_bias_hid)

        self.learn_sigma = learn_sigma
        self.init_sigma = init_sigma
        self.sigma_lr_scale = float(sigma_lr_scale)

        if energy_function_class.supports_vector_sigma():
            base = N.ones(nvis)
        else:
            base = 1

        self.sigma_driver = sharedX(
            base * init_sigma / self.sigma_lr_scale,
            name='sigma_driver',
            borrow=True
        )

        self.sigma = self.sigma_driver * self.sigma_lr_scale
        self.min_sigma = min_sigma
        self.max_sigma = max_sigma

        if self.learn_sigma:
            self._params.append(self.sigma_driver)

        self.mean_vis = mean_vis

        self.energy_function = energy_function_class(
                    transformer = self.transformer,
                    sigma=self.sigma,
                    bias_vis=self.bias_vis,
                    bias_hid=self.bias_hid
                )

    def _modify_updates(self, updates):
        """
        .. todo::

            WRITEME
        """
        if self.sigma_driver in updates:
            assert self.learn_sigma
            updates[self.sigma_driver] = T.clip(
                updates[self.sigma_driver],
                self.min_sigma / self.sigma_lr_scale,
                self.max_sigma / self.sigma_lr_scale
            )

    def score(self, V):
        """
        .. todo::

            WRITEME
        """
        return self.energy_function.score(V)

    def P_H_given_V(self, V):
        """
        .. todo::

            WRITEME
        """
        return self.energy_function.mean_H_given_V(V)

    def mean_h_given_v(self, v):
        """
        .. todo::

            WRITEME
        """
        return self.P_H_given_V(v)

    def mean_v_given_h(self, h):
        """
        Compute the mean activation of the visibles given hidden unit
        configurations for a set of training examples.

        Parameters
        ----------
        h : tensor_like
            Theano symbolic representing the hidden unit states for a batch
            of training examples, with the first dimension indexing training
            examples and the second indexing hidden units.

        Returns
        -------
        vprime : tensor_like
            Theano symbolic representing the mean (deterministic)
            reconstruction of the visible units given the hidden units.
        """

        return self.energy_function.mean_V_given_H(h)
        #return self.bias_vis + self.sigma * tensor.dot(h, self.weights.T)

    def free_energy_given_v(self, V):
        """
        Calculate the free energy of a visible unit configuration by
        marginalizing over the hidden units.

        Parameters
        ----------
        v : tensor_like
            Theano symbolic representing the hidden unit states for a batch
            of training examples, with the first dimension indexing training
            examples and the second indexing data dimensions.

        Returns
        -------
        f : tensor_like
            1-dimensional tensor representing the free energy of the visible
            unit configuration for each example in the batch
        """

        """hid_inp = self.input_to_h_from_v(v)
        squared_term = ((self.bias_vis - v) ** 2.) / (2. * self.sigma)
        rval =  squared_term.sum(axis=1) - nnet.softplus(hid_inp).sum(axis=1)
        assert len(rval.type.broadcastable) == 1"""

        return self.energy_function.free_energy(V)

    def free_energy(self, V):
        """
        .. todo::

            WRITEME
        """
        return self.energy_function.free_energy(V)

    def sample_visibles(self, params, shape, rng):
        """
        Stochastically sample the visible units given hidden unit
        configurations for a set of training examples.

        Parameters
        ----------
        params : list
            List of the necessary parameters to sample :math:`p(v|h)`.
            In the case of a Gaussian-binary RBM this is a single-element
            list containing the conditional mean.
        shape : WRITEME
        rng : WRITEME

        Returns
        -------
        vprime : tensor_like
            Theano symbolic representing stochastic samples from
            :math:`p(v|h)`

        Notes
        -----
        If `mean_vis` is specified as `True` in the constructor, this is
        equivalent to a call to `mean_v_given_h`.
        """
        v_mean = params[0]
        if self.mean_vis:
            return v_mean
        else:
            # zero mean, std sigma noise
            zero_mean = rng.normal(size=shape) * self.sigma
            return zero_mean + v_mean


class mu_pooled_ssRBM(RBM):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    alpha : WRITEME
        Vector of length nslab, diagonal precision term on s.
    b : WRITEME
        Vector of length nhid, hidden unit bias.
    B : WRITEME
        Vector of length nvis, diagonal precision on v.  Lambda in ICML2011
        paper.
    Lambda : WRITEME
        Matrix of shape nvis x nhid, whose i-th column encodes a diagonal
        precision on v, conditioned on h_i.  phi in ICML2011 paper.
    log_alpha : WRITEME
        Vector of length nslab, precision on s.
    mu : WRITEME
        Vector of length nslab, mean parameter on s.
    W : WRITEME
        Matrix of shape nvis x nslab, weights of the nslab linear filters s.
    """

    def __init__(self, nvis, nhid, n_s_per_h,
            batch_size,
            alpha0, alpha_irange,
            b0,
            B0,
            Lambda0, Lambda_irange,
            mu0,
            W_irange=None,
            rng=None):

        rng = make_np_rng(rng, 1001, which_method="rand")

        self.nhid = nhid
        self.nslab = nhid * n_s_per_h
        self.n_s_per_h = n_s_per_h
        self.nvis = nvis

        self.batch_size = batch_size

        # configure \alpha: precision parameter on s
        alpha_init = numpy.zeros(self.nslab) + alpha0
        if alpha_irange > 0:
            alpha_init += (2 * rng.rand(self.nslab) - 1) * alpha_irange
        self.log_alpha = sharedX(numpy.log(alpha_init), name='log_alpha')
        self.alpha = tensor.exp(self.log_alpha)
        self.alpha.name = 'alpha'

        self.mu = sharedX(
                numpy.zeros(self.nslab) + mu0,
                name='mu', borrow=True)
        self.b = sharedX(
                numpy.zeros(self.nhid) + b0,
                name='b', borrow=True)

        if W_irange is None:
            # Derived closed to Xavier Glorot's magic formula
            W_irange = 2 / numpy.sqrt(nvis * nhid)
        self.W = sharedX(
                (.5 - rng.rand(self.nvis, self.nslab)) * 2 * W_irange,
                name='W', borrow=True)

        # THE BETA IS IGNORED DURING TRAINING - FIXED AT MARGINAL DISTRIBUTION
        self.B = sharedX(numpy.zeros(self.nvis) + B0, name='B', borrow=True)

        if Lambda_irange > 0:
            L = (rng.rand(self.nvis, self.nhid) * Lambda_irange
                    + Lambda0)
        else:
            L = numpy.zeros((self.nvis, self.nhid)) + Lambda0
        self.Lambda = sharedX(L, name='Lambda', borrow=True)

        self._params = [
                self.mu,
                self.B,
                self.Lambda,
                self.W,
                self.b,
                self.log_alpha]

    #def ml_gradients(self, pos_v, neg_v):
    #    inherited version is OK.

    def gibbs_step_for_v(self, v, rng):
        """
        .. todo::

            WRITEME
        """
        # Sometimes, the number of examples in the data set is not a
        # multiple of self.batch_size.
        batch_size = v.shape[0]

        # sample h given v
        h_mean = self.mean_h_given_v(v)
        h_mean_shape = (batch_size, self.nhid)
        h_sample = rng.binomial(size=h_mean_shape,
                n = 1, p = h_mean, dtype = h_mean.dtype)

        # sample s given (v,h)
        s_mu, s_var = self.mean_var_s_given_v_h1(v)
        s_mu_shape = (batch_size, self.nslab)
        s_sample = s_mu + rng.normal(size=s_mu_shape) * tensor.sqrt(s_var)
        #s_sample=(s_sample.reshape()*h_sample.dimshuffle(0,1,'x')).flatten(2)

        # sample v given (s,h)
        v_mean, v_var = self.mean_var_v_given_h_s(h_sample, s_sample)
        v_mean_shape = (batch_size, self.nvis)
        v_sample = rng.normal(size=v_mean_shape) * tensor.sqrt(v_var) + v_mean

        del batch_size
        return v_sample, locals()

    ## TODO?
    def sample_visibles(self, params, shape, rng):
        """
        .. todo::

            WRITEME
        """
        raise NotImplementedError('mu_pooled_ssRBM.sample_visibles')

    def input_to_h_from_v(self, v):
        """
        .. todo::

            WRITEME
        """
        D = self.Lambda
        alpha = self.alpha

        def sum_s(x):
            return x.reshape((
                -1,
                self.nhid,
                self.n_s_per_h)).sum(axis=2)

        return tensor.add(
                self.b,
                -0.5 * tensor.dot(v * v, D),
                sum_s(self.mu * tensor.dot(v, self.W)),
                sum_s(0.5 * tensor.sqr(tensor.dot(v, self.W)) / alpha))

    #def mean_h_given_v(self, v):
    #    inherited version is OK:
    #    return nnet.sigmoid(self.input_to_h_from_v(v))

    def mean_var_v_given_h_s(self, h, s):
        """
        .. todo::

            WRITEME
        """
        v_var = 1 / (self.B + tensor.dot(h, self.Lambda.T))
        s3 = s.reshape((
                -1,
                self.nhid,
                self.n_s_per_h))
        hs = h.dimshuffle(0, 1, 'x') * s3
        v_mu = tensor.dot(hs.flatten(2), self.W.T) * v_var
        return v_mu, v_var

    def mean_var_s_given_v_h1(self, v):
        """
        .. todo::

            WRITEME
        """
        alpha = self.alpha
        return (self.mu + tensor.dot(v, self.W) / alpha,
                1.0 / alpha)

    ## TODO?
    def mean_v_given_h(self, h):
        """
        .. todo::

            WRITEME
        """
        raise NotImplementedError('mu_pooled_ssRBM.mean_v_given_h')

    def free_energy_given_v(self, v):
        """
        .. todo::

            WRITEME
        """
        sigmoid_arg = self.input_to_h_from_v(v)
        return tensor.add(
                0.5 * (self.B * (v ** 2)).sum(axis=1),
                -tensor.nnet.softplus(sigmoid_arg).sum(axis=1))

    #def __call__(self, v):
    #    inherited version is OK

    #def reconstruction_error:
    #    inherited version should be OK

    #def params(self):
    #    inherited version is OK.


def build_stacked_RBM(nvis, nhids, batch_size, vis_type='binary',
        input_mean_vis=None, irange=1e-3, rng=None):
    """
    .. todo::

        WRITEME properly

    Note from IG:
        This method doesn't seem to work correctly with Gaussian RBMs.
        In general, this is a difficult function to support, because it
        needs to pass the write arguments to the constructor of many kinds
        of RBMs. It would probably be better to just construct an instance
        of pylearn2.models.mlp.MLP with its hidden layers set to instances
        of pylearn2.models.mlp.RBM_Layer. If anyone is working on this kind
        of problem, a PR replacing this function with a helper function to
        make such an MLP would be very welcome.


    Allocate a StackedBlocks containing RBMs.

    The visible units of the input RBM can be either binary or gaussian,
    the other ones are all binary.
    """
    #TODO: not sure this is the right way of dealing with mean_vis.
    layers = []
    assert vis_type in ['binary', 'gaussian']
    if vis_type == 'binary':
        assert input_mean_vis is None
    elif vis_type == 'gaussian':
        assert input_mean_vis in (True, False)

    # The number of visible units in each layer is the initial input
    # size and the first k-1 hidden unit sizes.
    nviss = [nvis] + nhids[:-1]
    seq = izip(
            xrange(len(nhids)),
            nhids,
            nviss,
            )
    for k, nhid, nvis in seq:
        if k == 0 and vis_type == 'gaussian':
            rbm = GaussianBinaryRBM(nvis=nvis, nhid=nhid,
                    batch_size=batch_size,
                    irange=irange,
                    rng=rng,
                    mean_vis=input_mean_vis)
        else:
            rbm = RBM(nvis - nvis, nhid=nhid,
                    batch_size=batch_size,
                    irange=irange,
                    rng=rng)
        layers.append(rbm)

    # Create the stack
    return StackedBlocks(layers)


class L1_ActivationCost(Cost):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    target : WRITEME
    eps : WRITEME
    coeff : WRITEME
    """
    def __init__(self, target, eps, coeff):
        self.__dict__.update(locals())
        del self.self

    def expr(self, model, data, ** kwargs):
        """
        .. todo::

            WRITEME
        """
        self.get_data_specs(model)[0].validate(data)
        X = data
        H = model.P_H_given_V(X)
        h = H.mean(axis=0)
        err = abs(h - self.target)
        dead = T.maximum(err - self.eps, 0.)
        assert dead.ndim == 1
        rval = self.coeff * dead.mean()
        return rval

    def get_data_specs(self, model):
        """
        .. todo::

            WRITEME
        """
        return (model.get_input_space(), model.get_input_source())


# The following functionality was deprecated, but is evidently
# still needed to make the RBM work

class _Optimizer(object):
    """
    Basic abstract class for computing parameter updates of a model.
    """

    def updates(self):
        """Return symbolic updates to apply."""
        raise NotImplementedError()


class _SGDOptimizer(_Optimizer):
    """
    Compute updates by stochastic gradient descent on mini-batches.

    Supports constant learning rates, or decreasing like 1/t after an initial
    period.

    Parameters
    ----------
    params : object or list
        Either a Model object with a .get_params() method, or a list of
        parameters to be optimized.
    base_lr : float
        The base learning rate before annealing or parameter-specific
        scaling.
    anneal_start : int, optional
        Number of steps after which to start annealing the learning
        rate at a 1/t schedule, where t is the number of stochastic
        gradient updates.
    use_adagrad : bool, optional
        'adagrad' adaptive learning rate scheme is used. If set to True,
        base_lr is used as e0.
    kwargs : dict
        WRITEME

    Notes
    -----
    The formula to compute the effective learning rate on a parameter is:
    <paramname>_lr * max(0.0, min(base_lr, lr_anneal_start/(iteration+1)))

    Parameter-specific learning rates can be set by passing keyword
    arguments <name>_lr, where name is the .name attribute of a given
    parameter.

    Parameter-specific bounding values can be specified by passing
    keyword arguments <param>_clip, which should be a (min, max) pair.

    Adagrad is recommended with sparse inputs. It normalizes the base
    learning rate of a parameter theta_i by the accumulated 2-norm of its
    gradient: e{ti} = e0 / sqrt( sum_t (dL_t / dtheta_i)^2 )
    """
    def __init__(self, params, base_lr, anneal_start=None, use_adagrad=False,
                 ** kwargs):
        if hasattr(params, '__iter__'):
            self.params = params
        elif hasattr(params, 'get_params') and hasattr(
                params.get_params, '__call__'):
            self.params = params.get_params()
        else:
            raise ValueError("SGDOptimizer couldn't figure out what to do "
                             "with first argument: '%s'" % str(params))
        if anneal_start == None:
            self.anneal_start = None
        else:
            self.anneal_start = as_floatX(anneal_start)

        # Create accumulators and epsilon0's
        self.use_adagrad = use_adagrad
        if self.use_adagrad:
            self.accumulators = {}
            self.e0s = {}
            for param in self.params:
                self.accumulators[param] = theano.shared(
                        value=as_floatX(0.), name='acc_%s' % param.name)
                self.e0s[param] = as_floatX(base_lr)

        # Set up the clipping values
        self.clipping_values = {}
        # Keep track of names already seen
        clip_names_seen = set()
        for parameter in self.params:
            clip_name = '%s_clip' % parameter.name
            if clip_name in kwargs:
                if clip_name in clip_names_seen:
                    logger.warning('In SGDOptimizer, at least two parameters '
                                   'have the same name. Both will be affected '
                                   'by the keyword argument '
                                   '{0}.'.format(clip_name))
                clip_names_seen.add(clip_name)
                p_min, p_max = kwargs[clip_name]
                assert p_min <= p_max
                self.clipping_values[parameter] = (p_min, p_max)

        # Check that no ..._clip keyword is being ignored
        for clip_name in clip_names_seen:
            kwargs.pop(clip_name)
        for kw in kwargs.iterkeys():
            if kw[-5:] == '_clip':
                logger.warning('In SGDOptimizer, keyword argument {0} '
                               'will be ignored, because no parameter '
                               'was found with name {1}.'.format(kw, kw[:-5]))

        self.learning_rates_setup(base_lr, **kwargs)

    def learning_rates_setup(self, base_lr, **kwargs):
        """
        Initializes parameter-specific learning rate dictionary and shared
        variables for the annealed base learning rate and iteration number.

        Parameters
        ----------
        base_lr : float
            The base learning rate before annealing or parameter-specific
            scaling.
        kwargs : dict
            WRITEME

        Notes
        -----
        Parameter-specific learning rates can be set by passing keyword
        arguments <name>_lr, where name is the .name attribute of a given
        parameter.
        """
        # Take care of learning rate scales for individual parameters
        self.learning_rates = {}
        # Base learning rate per example.
        self.base_lr = theano._asarray(base_lr, dtype=theano.config.floatX)

        # Keep track of names already seen
        lr_names_seen = set()
        for parameter in self.params:
            lr_name = '%s_lr' % parameter.name
            if lr_name in lr_names_seen:
                logger.warning('In SGDOptimizer, '
                               'at least two parameters have the same name. '
                               'Both will be affected by the keyword argument '
                               '{0}.'.format(lr_name))
            lr_names_seen.add(lr_name)

            thislr = kwargs.get(lr_name, 1.)
            self.learning_rates[parameter] = sharedX(thislr, lr_name)

        # Verify that no ..._lr keyword argument is ignored
        for lr_name in lr_names_seen:
            if lr_name in kwargs:
                kwargs.pop(lr_name)
        for kw in kwargs.iterkeys():
            if kw[-3:] == '_lr':
                logger.warning('In SGDOptimizer, keyword argument {0} '
                               'will be ignored, because no parameter '
                               'was found with name {1}.'.format(kw, kw[:-3]))

        # A shared variable for storing the iteration number.
        self.iteration = sharedX(theano._asarray(0, dtype='int32'),
                                 name='iter')

        # A shared variable for storing the annealed base learning rate, used
        # to lower the learning rate gradually after a certain amount of time.
        self.annealed = sharedX(base_lr, 'annealed')

    def learning_rate_updates(self, gradients):
        """
        Compute a dictionary of shared variable updates related to annealing
        the learning rate.

        Parameters
        ----------
        gradients : WRITEME

        Returns
        -------
        updates : dict
            A dictionary with the shared variables representing SGD metadata
            as keys and a symbolic expression of how they are to be updated as
            values.
        """
        ups = {}

        if self.use_adagrad:
            learn_rates = []
            for param, gp in zip(self.params, gradients):
                acc = self.accumulators[param]
                ups[acc] = acc + (gp ** 2).sum()
                learn_rates.append(self.e0s[param] / (ups[acc] ** .5))
        else:
            # Annealing coefficient. Here we're using a formula of
            # min(base_lr, anneal_start / (iteration + 1))
            if self.anneal_start is None:
                annealed = sharedX(self.base_lr)
            else:
                frac = self.anneal_start / (self.iteration + 1.)
                annealed = tensor.minimum(
                                          as_floatX(frac),
                                          self.base_lr  # maximum learning rate
                                          )

            # Update the shared variable for the annealed learning rate.
            ups[self.annealed] = annealed
            ups[self.iteration] = self.iteration + 1

            # Calculate the learning rates for each parameter, in the order
            # they appear in self.params
            learn_rates = [annealed * self.learning_rates[p] for p in
                    self.params]
        return ups, learn_rates

    def updates(self, gradients):
        """
        Return symbolic updates to apply given a set of gradients
        on the parameters being optimized.

        Parameters
        ----------
        gradients : list of tensor_likes
            List of symbolic gradients for the parameters contained
            in self.params, in the same order as in self.params.

        Returns
        -------
        updates : dict
            A dictionary with the shared variables in self.params as keys
            and a symbolic expression of how they are to be updated each
            SGD step as values.

        Notes
        -----
        `cost_updates` is a convenient helper function that takes all
        necessary gradients with respect to a given symbolic cost.
        """
        ups = {}
        # Add the learning rate/iteration updates
        l_ups, learn_rates = self.learning_rate_updates(gradients)
        safe_update(ups, l_ups)

        # Get the updates from sgd_updates, a PyLearn library function.
        p_up = dict(self.sgd_updates(self.params, gradients, learn_rates))

        # Add the things in p_up to ups
        safe_update(ups, p_up)

        # Clip the values if needed.
        # We do not want the clipping values to force an upcast
        # of the update: updates should have the same type as params
        for param, (p_min, p_max) in self.clipping_values.iteritems():
            p_min = tensor.as_tensor(p_min)
            p_max = tensor.as_tensor(p_max)
            dtype = param.dtype
            if p_min.dtype != dtype:
                p_min = tensor.cast(p_min, dtype)
            if p_max.dtype != dtype:
                p_max = tensor.cast(p_max, dtype)
            ups[param] = tensor.clip(ups[param], p_min, p_max)

        # Return the updates dictionary.
        return ups

    def cost_updates(self, cost):
        """
        Return symbolic updates to apply given a cost function.

        Parameters
        ----------
        cost : tensor_like
            Symbolic cost with respect to which the gradients of
            the parameters should be taken. Should be 0-dimensional
            (scalar valued).

        Returns
        -------
        updates : dict
            A dictionary with the shared variables in self.params as keys
            and a symbolic expression of how they are to be updated each
            SGD step as values.
        """
        grads = [tensor.grad(cost, p) for p in self.params]
        return self.updates(gradients=grads)

    def sgd_updates(self, params, grads, stepsizes):
        """
        Return a list of (pairs) that can be used
        as updates in theano.function to
        implement stochastic gradient descent.

        Parameters
        ----------
        params : list of Variable
            variables to adjust in order to minimize some cost
        grads : list of Variable
            the gradient on each param (with respect to some cost)
        stepsizes : symbolic scalar or list of one symbolic scalar per param
            step by this amount times the negative gradient on each iteration
        """
        try:
            iter(stepsizes)
        except Exception:
            stepsizes = [stepsizes for p in params]
        if len(params) != len(grads):
            raise ValueError('params and grads have different lens')
        updates = [(p, p - step * gp) for (step, p, gp)
                in zip(stepsizes, params, grads)]
        return updates

    def sgd_momentum_updates(self, params, grads, stepsizes, momentum=0.9):
        """
        .. todo::

            WRITEME
        """
        # if stepsizes is just a scalar, expand it to match params
        try:
            iter(stepsizes)
        except Exception:
            stepsizes = [stepsizes for p in params]
        try:
            iter(momentum)
        except Exception:
            momentum = [momentum for p in params]
        if len(params) != len(grads):
            raise ValueError('params and grads have different lens')
        headings = [theano.shared(numpy.zeros_like(p.get_value(borrow=True)))
                for p in params]
        updates = []
        for s, p, gp, m, h in zip(stepsizes, params, grads, momentum,
                headings):
            updates.append((p, p + s * h))
            updates.append((h, m * h - (1.0 - m) * gp))
        return updates

########NEW FILE########
__FILENAME__ = s3c
"""
Spike-and-slab sparse coding (S3C)
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2011, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"


import logging
import time
import warnings

import numpy as np
from theano import config, function
from theano import scan
from theano.compat.python2x import OrderedDict
from theano.gof.op import get_debug_values, debug_error_message, debug_assert
import theano.tensor as T

from pylearn2.utils import make_name, sharedX, as_floatX
from pylearn2.blocks import Block
from pylearn2.expr.information_theory import entropy_binary_vector
from pylearn2.models import Model
from pylearn2.space import VectorSpace
from pylearn2.utils.rng import make_np_rng
from pylearn2.expr.basic import (full_min,
        full_max, numpy_norms, theano_norms)


logger = logging.getLogger(__name__)

logger.debug('s3c changing the recursion limit')
import sys
sys.setrecursionlimit(50000)


def rotate_towards(old_W, new_W, new_coeff):
    """
    .. todo::

        WRITEME properly

    For each column, rotates old_w toward new_w by new_coeff * theta,
    where theta is the angle between them

    Parameters
    ----------
    old_W : WRITEME
        every column is a unit vector
    new_W : WRITEME
    new_coeff : WRITEME
    """

    norms = theano_norms(new_W)

    # update, scaled back onto unit sphere
    scal_points = new_W / norms.dimshuffle('x',0)

    # dot product between scaled update and current W
    dot_update = (old_W * scal_points).sum(axis=0)

    theta = T.arccos(dot_update)

    rot_amt = new_coeff * theta

    new_basis_dir = scal_points - dot_update * old_W

    new_basis_norms = theano_norms(new_basis_dir)

    new_basis = new_basis_dir / new_basis_norms

    rval = T.cos(rot_amt) * old_W + T.sin(rot_amt) * new_basis

    return rval



class SufficientStatistics:
    """
    The SufficientStatistics class computes several sufficient
    statistics of a minibatch of examples / variational parameters.

    This is mostly for convenience since several expressions are easy
    to express in terms of these same sufficient statistics. Also,
    re-using the same expression for the sufficient statistics in
    multiple code locations can reduce theano compilation time. The
    current version of the S3C code no longer supports features like
    decaying sufficient statistics since these were not found to be
    particularly beneficial relative to the burden of computing the
    O(nhid^2) second moment matrix. The current version of the code
    merely computes the sufficient statistics apart from the second
    moment matrix as a notational convenience. Expressions that most
    naturally are expressed in terms of the second moment matrix are
    now written with a different order of operations that avoids
    O(nhid^2) operations but whose dependence on the dataset cannot be
    expressed in terms only of sufficient statistics.

    Parameters
    ----------
    d : WRITEME
    """

    def __init__(self, d):
        self. d = {}
        for key in d:
            self.d[key] = d[key]

    @classmethod
    def from_observations(cls, needed_stats, V, H_hat, S_hat, var_s0_hat, var_s1_hat):
        """
        Returns a SufficientStatistics

        .. todo::

            WRITEME properly

        Parameters
        ----------
        needed_stats : WRITEME
            a set of string names of the statistics to include
        V : WRITEME
            a num_examples x nvis matrix of input examples
        H_hat : WRITEME
            a num_examples x nhid matrix of \hat{h} variational parameters
        S_hat : WRITEME
            variational parameters for expectation of s given h=1
        var_s0_hat : WRITEME
            variational parameters for variance of s given h=0
            (only a vector of length nhid, since this is the same for
            all inputs)
        var_s1_hat : WRITEME
            variational parameters for variance of s given h=1
            (again, a vector of length nhid)
        """

        m = T.cast(V.shape[0],config.floatX)

        H_name = make_name(H_hat, 'anon_H_hat')
        S_name = make_name(S_hat, 'anon_S_hat')

        #mean_h
        assert H_hat.dtype == config.floatX
        mean_h = T.mean(H_hat, axis=0)
        assert H_hat.dtype == mean_h.dtype
        assert mean_h.dtype == config.floatX
        mean_h.name = 'mean_h('+H_name+')'

        #mean_v
        mean_v = T.mean(V,axis=0)

        #mean_sq_v
        mean_sq_v = T.mean(T.sqr(V),axis=0)

        #mean_s1
        mean_s1 = T.mean(S_hat,axis=0)

        #mean_sq_s
        mean_sq_S = H_hat * (var_s1_hat + T.sqr(S_hat)) + (1. - H_hat)*(var_s0_hat)
        mean_sq_s = T.mean(mean_sq_S,axis=0)

        #mean_hs
        mean_HS = H_hat * S_hat
        mean_hs = T.mean(mean_HS,axis=0)
        mean_hs.name = 'mean_hs(%s,%s)' % (H_name, S_name)
        mean_s = mean_hs #this here refers to the expectation of the s variable, not s_hat
        mean_D_sq_mean_Q_hs = T.mean(T.sqr(mean_HS), axis=0)

        #mean_sq_hs
        mean_sq_HS = H_hat * (var_s1_hat + T.sqr(S_hat))
        mean_sq_hs = T.mean(mean_sq_HS, axis=0)
        mean_sq_hs.name = 'mean_sq_hs(%s,%s)' % (H_name, S_name)

        #mean_sq_mean_hs
        mean_sq_mean_hs = T.mean(T.sqr(mean_HS), axis=0)
        mean_sq_mean_hs.name = 'mean_sq_mean_hs(%s,%s)' % (H_name, S_name)

        #mean_hsv
        sum_hsv = T.dot(mean_HS.T,V)
        sum_hsv.name = 'sum_hsv<from_observations>'
        mean_hsv = sum_hsv / m


        d = {
                    "mean_h"                :   mean_h,
                    "mean_v"                :   mean_v,
                    "mean_sq_v"             :   mean_sq_v,
                    "mean_s"                :   mean_s,
                    "mean_s1"               :   mean_s1,
                    "mean_sq_s"             :   mean_sq_s,
                    "mean_hs"               :   mean_hs,
                    "mean_sq_hs"            :   mean_sq_hs,
                    "mean_sq_mean_hs"       :   mean_sq_mean_hs,
                    "mean_hsv"              :   mean_hsv,
                }


        final_d = {}

        for stat in needed_stats:
            final_d[stat] = d[stat]
            final_d[stat].name = 'observed_'+stat

        return SufficientStatistics(final_d)


class S3C(Model, Block):
    """
    If you use S3C in published work, please cite:

    Large-Scale Feature Learning With Spike-and-Slab Sparse Coding.
    Goodfellow, I., Courville, A., & Bengio, Y. ICML 2012.

    Parameters
    ----------
    nvis : WRITEME
        # of visible units
    nhid : WRITEME
        # of hidden units
    irange : WRITEME
        (scalar) weights are initinialized ~U( [-irange,irange] )
    init_bias_hid : WRITEME
        initial value of hidden biases (scalar or vector)
    init_B : WRITEME
        initial value of B (scalar or vector)
    min_B : WRITEME
        See `max_B`
    max_B : WRITEME
        (scalar) learning updates to B are clipped to [min_B, max_B]
    init_alpha : WRITEME
        initial value of alpha (scalar or vector)
    min_alpha : WRITEME
        See `max_alpha`
    max_alpha : WRITEME
        (scalar) learning updates to alpha are clipped to [min_alpha, max_alpha]
    init_mu : WRITEME
        initial value of mu (scalar or vector)
    min_mu : WRITEME
        See `max_mu`
    max_mu : WRITEME
        clip mu updates to this range.
    e_step : WRITEME
        An E_Step object that determines what kind of E-step to do
        if None, assumes that the S3C model is being driven by
        a larger model, and does not generate theano functions
        necessary for autonomous operation
    m_step : WRITEME
        An M_Step object that determines what kind of M-step to do
    tied_B : WRITEME
        if True, use a scalar times identity for the precision on visible units.
        otherwise use a diagonal matrix for the precision on visible units
    constrain_W_norm : bool
        if true, norm of each column of W must be 1 at all times
    init_unit_W : bool
        if true, each column of W is initialized to have unit norm
    monitor_stats : WRITEME
        a list of sufficient statistics to monitor on the monitoring dataset
    monitor_params : WRITEME
        a list of parameters to monitor TODO: push this into Model base class
    monitor_functional : WRITEME
        if true, monitors the EM functional on the monitoring dataset
    monitor_norms : bool
        if true, monitors the norm of W at the end of each solve step, but before
        blending with old W by new_coeff
        This lets us see how much distortion is introduced by norm clipping
        Note that unless new_coeff = 1, the post-solve norm monitored by this
        flag will not be equal to the norm of the final parameter value, even
        if no norm clipping is activated.
    recycle_q : WRITEME
        if nonzero, initializes the e-step with the output of the previous iteration's
        e-step. obviously this should only be used if you are using the same data
        in each batch. when recycle_q is nonzero, it should be set to the batch size.
    disable_W_update : WRITEME
        if true, doesn't update W (useful for experiments where you only learn the prior)
    random_patches_src : WRITEME
        if not None, should be a dataset
        will set W to a batch
    local_rf_src : Dataset, optional
        if not None, it should be a dataset.
        it requires the following other params:

        - local_rf_shape : a 2 tuple
        - One of:

            - local_rf_stride: a 2 tuple or None
                if specified, pull out patches on a regular grid
            - local_rf_max_shape: a 2 tuple or None
                if specified, pull out patches of random shape and
                location
            - local_rf_draw_patches : WRITEME
                if true, local receptive fields are patches from
                local_rf_src. otherwise, they're random patches.
                will initialize the weights to have only local
                receptive fields. (won't make a sparse matrix or
                anything like that)

        incompatible with random_patches_src for now
    init_unit_W : bool
        if True, initializes weights with unit norm
    """

    def __init__(self, nvis, nhid, irange, init_bias_hid,
                       init_B, min_B, max_B,
                       init_alpha, min_alpha, max_alpha, init_mu,
                       m_step,
                       min_bias_hid=-1e30,
                       max_bias_hid=1e30,
                       min_mu=-1e30,
                       max_mu=1e30,
                       e_step=None,
                       tied_B=False,
                       monitor_stats=None,
                       monitor_params=None,
                       monitor_functional=False,
                       recycle_q=0,
                       seed=None,
                       disable_W_update=False,
                       constrain_W_norm=False,
                       monitor_norms=False,
                       random_patches_src=None,
                       local_rf_src=None,
                       local_rf_shape=None,
                       local_rf_max_shape=None,
                       local_rf_stride=None,
                       local_rf_draw_patches=False,
                       init_unit_W=None,
                       debug_m_step=False,
                       print_interval=10000,
                       stop_after_hack=None,
                       set_B_to_marginal_precision=False,
                       init_momentum=None,
                       final_momentum=None,
                       momentum_saturation_example=None):
        Model.__init__(self)
        Block.__init__(self)

        self.debug_m_step = debug_m_step
        self.set_B_to_marginal_precision = set_B_to_marginal_precision

        self.monitoring_channel_prefix = ''

        if init_unit_W is not None and not init_unit_W:
            assert not constrain_W_norm

        self.init_momentum = init_momentum
        self.final_momentum = final_momentum
        self.momentum_saturation_example = momentum_saturation_example

        self.seed = seed
        self.reset_rng()
        self.irange = irange
        self.nvis = nvis
        self.input_space = VectorSpace(nvis)
        self.nhid = nhid

        if random_patches_src is not None:
            self.init_W = random_patches_src.get_batch_design(nhid).T
            assert local_rf_src is None
        elif local_rf_src is not None:
            s = local_rf_src.view_shape()
            height, width, channels = s
            W_img = np.zeros( (self.nhid, height, width, channels) )

            last_row = s[0] - local_rf_shape[0]
            last_col = s[1] - local_rf_shape[1]


            if local_rf_stride is not None:
                #local_rf_stride specified, make local_rfs on a grid
                assert last_row % local_rf_stride[0] == 0
                num_row_steps = last_row / local_rf_stride[0] + 1

                assert last_col % local_rf_stride[1] == 0
                num_col_steps = last_col /local_rf_stride[1] + 1

                total_rfs = num_row_steps * num_col_steps

                if self.nhid % total_rfs != 0:
                    raise ValueError('nhid modulo total_rfs should be 0, but we get %d modulo %d = %d' % (self.nhid, total_rfs, self.nhid % total_rfs))

                filters_per_rf = self.nhid / total_rfs

                idx = 0
                for r in xrange(num_row_steps):
                    rc = r * local_rf_stride[0]
                    for c in xrange(num_col_steps):
                        cc = c * local_rf_stride[1]

                        for i in xrange(filters_per_rf):

                            if local_rf_draw_patches:
                                img = local_rf_src.get_batch_topo(1)[0]
                                local_rf = img[rc:rc+local_rf_shape[0],
                                               cc:cc+local_rf_shape[1],
                                               :]
                            else:
                                local_rf = self.rng.uniform(-self.irange,
                                            self.irange,
                                            (local_rf_shape[0], local_rf_shape[1], s[2]) )



                            W_img[idx,rc:rc+local_rf_shape[0],
                              cc:cc+local_rf_shape[1],:] = local_rf
                            idx += 1
                assert idx == self.nhid
            else:
                #no stride specified, use random shaped patches
                assert local_rf_max_shape is not None

                for idx in xrange(nhid):
                    shape = [ self.rng.randint(min_shape,max_shape+1) for
                            min_shape, max_shape in zip(
                                local_rf_shape,
                                local_rf_max_shape) ]
                    loc = [self.rng.randint(0, bound - cur_width + 1) for
                            bound, cur_width in zip(s, shape)]

                    rc, cc = loc

                    if local_rf_draw_patches:
                        img = local_rf_src.get_batch_topo(1)[0]
                        local_rf = img[rc:rc+shape[0],
                                       cc:cc+shape[1],
                                       :]
                    else:
                        local_rf = self.rng.uniform(-self.irange,
                                    self.irange,
                                    (shape[0], shape[1], s[2]) )

                    W_img[idx,rc:rc+shape[0],
                              cc:cc+shape[1],:] = local_rf


            self.init_W = local_rf_src.view_converter.topo_view_to_design_mat(W_img).T


        else:
            self.init_W = None

        self.register_names_to_del(['init_W'])

        if monitor_stats is None:
            self.monitor_stats = []
        else:
            self.monitor_stats = [ elem for elem in monitor_stats ]

        if monitor_params is None:
            self.monitor_params = []
        else:
            self.monitor_params = [ elem for elem in monitor_params ]

        self.init_unit_W = init_unit_W


        self.print_interval = print_interval

        self.constrain_W_norm = constrain_W_norm

        self.stop_after_hack = stop_after_hack
        self.monitor_norms = monitor_norms
        self.disable_W_update = disable_W_update
        self.monitor_functional = monitor_functional
        self.init_bias_hid = init_bias_hid
        def nostrings(x):
            if isinstance(x,str):
                return float(x)
            return x
        self.init_alpha = nostrings(init_alpha)
        self.min_alpha = nostrings(min_alpha)
        self.max_alpha = nostrings(max_alpha)
        self.init_B = nostrings(init_B)
        self.min_B = nostrings(min_B)
        self.max_B = nostrings(max_B)
        self.m_step = m_step
        self.e_step = e_step
        if e_step is None:
            self.autonomous = False
            assert not self.m_step.autonomous
            #create a non-autonomous E step
            self.e_step = E_Step(h_new_coeff_schedule = None,
                                rho = None,
                                monitor_kl = None,
                                monitor_energy_functional = None,
                                clip_reflections = None)
            assert not self.e_step.autonomous
        else:
            self.autonomous = True
            assert e_step.autonomous
            assert self.m_step.autonomous
        self.init_mu = init_mu
        self.min_mu = np.cast[config.floatX](float(min_mu))
        self.max_mu = np.cast[config.floatX](float(max_mu))
        self.min_bias_hid = float(min_bias_hid)
        self.max_bias_hid = float(max_bias_hid)
        self.recycle_q = recycle_q
        self.tied_B = tied_B

        self.redo_everything()

    def reset_rng(self):
        """
        .. todo::

            WRITEME
        """

        self.rng = make_np_rng(self.seed, [1,2,3], which_method="uniform")

    def redo_everything(self):
        """
        .. todo::

            WRITEME
        """

        if self.init_W is not None:
            W = self.init_W.copy()
        else:
            W = self.rng.uniform(-self.irange, self.irange, (self.nvis, self.nhid))

        if self.constrain_W_norm or self.init_unit_W:
            norms = numpy_norms(W)
            W /= norms

        self.W = sharedX(W, name = 'W')

        self.bias_hid = sharedX(np.zeros(self.nhid)+self.init_bias_hid, name='bias_hid')
        self.alpha = sharedX(np.zeros(self.nhid)+self.init_alpha, name = 'alpha')
        self.mu = sharedX(np.zeros(self.nhid)+self.init_mu, name='mu')
        if self.tied_B:
            self.B_driver = sharedX(0.0+self.init_B, name='B')
        else:
            self.B_driver = sharedX(np.zeros(self.nvis)+self.init_B, name='B')

        if self.recycle_q:
            self.prev_H = sharedX(np.zeros((self.recycle_q,self.nhid)), name="prev_H")
            self.prev_S = sharedX(np.zeros((self.recycle_q,self.nhid)), name="prev_S")

        if self.debug_m_step:
            warnings.warn('M step debugging activated-- this is only valid for certain settings, and causes a performance slowdown.')
            self.energy_functional_diff = sharedX(0.)

        if self.momentum_saturation_example is not None:
            self.params_to_incs = {}

            for param in self.get_params():
                self.params_to_incs[param] = sharedX(np.zeros(param.get_value().shape), name = param.name + '_inc')

            self.momentum = sharedX(self.init_momentum, name='momentum')

        if self.monitor_norms:
            self.debug_norms = sharedX(np.zeros(self.nhid))

        self.redo_theano()

    @classmethod
    def energy_functional_needed_stats(cls):
        """
        .. todo::

            WRITEME
        """
        return S3C.expected_log_prob_vhs_needed_stats()

    def energy_functional(self, H_hat, S_hat, var_s0_hat, var_s1_hat, stats):
        """
        .. todo::

            WRITEME

        Returns the energy_functional for a single batch of data stats is
        assumed to be computed from and only from the same data points that
        yielded H
        """

        entropy_term = self.entropy_hs(H_hat = H_hat, var_s0_hat = var_s0_hat, var_s1_hat = var_s1_hat).mean()
        likelihood_term = self.expected_log_prob_vhs(stats, H_hat = H_hat, S_hat = S_hat)

        energy_functional = likelihood_term + entropy_term
        assert len(energy_functional.type.broadcastable) == 0

        return energy_functional

    def energy_functional_batch(self, V, H_hat, S_hat, var_s0_hat, var_s1_hat):
        """
        .. todo::

            WRITEME

        Returns the energy_functional for a single batch of data stats is
        assumed to be computed from and only from the same data points that
        yielded H
        """

        entropy_term = self.entropy_hs(H_hat = H_hat, var_s0_hat = var_s0_hat, var_s1_hat = var_s1_hat)
        assert len(entropy_term.type.broadcastable) == 1
        likelihood_term = self.expected_log_prob_vhs_batch(V = V, H_hat = H_hat, S_hat = S_hat, var_s0_hat = var_s0_hat, var_s1_hat = var_s1_hat)
        assert len(likelihood_term.type.broadcastable) == 1

        energy_functional = likelihood_term + entropy_term
        assert len(energy_functional.type.broadcastable) == 1

        return energy_functional

    def set_monitoring_channel_prefix(self, prefix):
        """
        .. todo::

            WRITEME
        """
        self.monitoring_channel_prefix = prefix

    def get_monitoring_channels(self, data):
        """
        .. todo::

            WRITEME
        """
        space, source = self.get_monitoring_data_specs()
        space.validate(data)
        V = data
        try:
            self.compile_mode()

            if self.m_step != None:
                rval = self.m_step.get_monitoring_channels(V, self)
            else:
                rval = {}

            if self.momentum_saturation_example is not None:
                rval['momentum'] = self.momentum

            from_e_step = self.e_step.get_monitoring_channels(V)

            rval.update(from_e_step)

            if self.debug_m_step:
                rval['m_step_diff'] = self.energy_functional_diff

            monitor_stats = len(self.monitor_stats) > 0

            if monitor_stats or self.monitor_functional:

                obs = self.infer(V)

                needed_stats = set(self.monitor_stats)

                if self.monitor_functional:
                    needed_stats = needed_stats.union(S3C.expected_log_prob_vhs_needed_stats())

                stats = SufficientStatistics.from_observations( needed_stats = needed_stats,
                                                            V = V, ** obs )

                H_hat = obs['H_hat']
                S_hat = obs['S_hat']
                var_s0_hat = obs['var_s0_hat']
                var_s1_hat = obs['var_s1_hat']

                if self.monitor_functional:
                    energy_functional = self.energy_functional(H_hat = H_hat, S_hat = S_hat, var_s0_hat = var_s0_hat,
                            var_s1_hat = var_s1_hat, stats = stats)

                    rval['energy_functional'] = energy_functional

                if monitor_stats:
                    for stat in self.monitor_stats:
                        stat_val = stats.d[stat]

                        rval[stat+'_min'] = T.min(stat_val)
                        rval[stat+'_mean'] = T.mean(stat_val)
                        rval[stat+'_max'] = T.max(stat_val)
                    #end for stat
                #end if monitor_stats
            #end if monitor_stats or monitor_functional

            if len(self.monitor_params) > 0:
                for param in self.monitor_params:
                    param_val = getattr(self, param)


                    rval[param+'_min'] = full_min(param_val)
                    rval[param+'_mean'] = T.mean(param_val)

                    mx = full_max(param_val)
                    assert len(mx.type.broadcastable) == 0
                    rval[param+'_max'] = mx

                    if param == 'mu':
                        abs_mu = abs(self.mu)
                        rval['mu_abs_min'] = full_min(abs_mu)
                        rval['mu_abs_mean'] = T.mean(abs_mu)
                        rval['mu_abs_max'] = full_max(abs_mu)

                    if param == 'W':
                        norms = theano_norms(self.W)
                        rval['W_norm_min'] = full_min(norms)
                        rval['W_norm_mean'] = T.mean(norms)
                        rval['W_norm_max'] = T.max(norms)

            if self.monitor_norms:
                rval['post_solve_norms_min'] = T.min(self.debug_norms)
                rval['post_solve_norms_max'] = T.max(self.debug_norms)
                rval['post_solve_norms_mean'] = T.mean(self.debug_norms)

            new_rval = {}

            for key in rval:
                new_rval[self.monitoring_channel_prefix+key] = rval[key]

            rval = new_rval

            return rval
        finally:
            self.deploy_mode()

    def get_monitoring_data_specs(self):
        """
        Get the data_specs describing the data for get_monitoring_channel.

        This implementation returns specification corresponding to unlabeled
        inputs.

        WRITEME: Returns section
        """
        return (self.get_input_space(), self.get_input_source())

    def __call__(self, V):
        """
        .. todo::

            WRITEME

        This is the symbolic transformation for the Block class
        """
        if not hasattr(self,'w'):
            self.make_pseudoparams()
        obs = self.infer(V)
        return obs['H_hat']

    def compile_mode(self):
        """
        If any shared variables need to have batch-size dependent sizes, sets
        them all to the sizes used for interactive debugging during graph
        construction
        """
        if self.recycle_q:
            self.prev_H.set_value(
                    np.cast[self.prev_H.dtype](
                        np.zeros((self._test_batch_size, self.nhid)) \
                                + 1./(1.+np.exp(-self.bias_hid.get_value()))))
            self.prev_S.set_value(
                    np.cast[self.prev_S.dtype](
                        np.zeros((self._test_batch_size, self.nhid)) + self.mu.get_value() ) )

    def deploy_mode(self):
        """
        If any shared variables need to have batch-size dependent sizes, sets
        them all to their runtime sizes
        """
        if self.recycle_q:
            self.prev_H.set_value( np.cast[self.prev_H.dtype]( np.zeros((self.recycle_q, self.nhid)) + 1./(1.+np.exp(-self.bias_hid.get_value()))))
            self.prev_S.set_value( np.cast[self.prev_S.dtype]( np.zeros((self.recycle_q, self.nhid)) + self.mu.get_value() ) )

    def get_params(self):
        """
        .. todo::

            WRITEME
        """
        return [self.W, self.bias_hid, self.alpha, self.mu, self.B_driver ]

    def energy_vhs(self, V, H, S):
        """
        .. todo::

            WRITEME

        H MUST be binary
        """

        h_term = - T.dot(H, self.bias_hid)
        assert len(h_term.type.broadcastable) == 1

        s_term_1 = T.dot(T.sqr(S), self.alpha)/2.
        s_term_2 = -T.dot(S * self.mu * H , self.alpha)
        #s_term_3 = T.dot(T.sqr(self.mu * H), self.alpha)/2.
        s_term_3 = T.dot(T.sqr(self.mu) * H, self.alpha) / 2.

        s_term = s_term_1 + s_term_2 + s_term_3
        #s_term = T.dot( T.sqr( S - self.mu * H) , self.alpha) / 2.
        assert len(s_term.type.broadcastable) == 1


        recons = T.dot(H*S, self.W.T)

        v_term_1 = T.dot( T.sqr(V), self.B) / 2.
        v_term_2 = T.dot( - V * recons, self.B)
        v_term_3 = T.dot( T.sqr(recons), self.B) / 2.

        v_term = v_term_1 + v_term_2 + v_term_3

        #v_term = T.dot( T.sqr( V - recons), self. B) / 2.
        assert len(v_term.type.broadcastable) == 1

        rval = h_term + s_term + v_term
        assert len(rval.type.broadcastable) == 1

        return rval

    def expected_energy_vhs(self, V, H_hat, S_hat, var_s0_hat, var_s1_hat):
        """
        .. todo::

            WRITEME

        This is not the same as negative expected log prob,
        which includes the constant term for the log partition function
        """

        var_HS = H_hat * var_s1_hat + (1.-H_hat) * var_s0_hat

        half = as_floatX(.5)

        HS = H_hat * S_hat

        sq_HS = H_hat * ( var_s1_hat + T.sqr(S_hat))

        sq_S = sq_HS + (1.-H_hat)*(var_s0_hat)

        presign = T.dot(H_hat, self.bias_hid)
        presign.name = 'presign'
        h_term = - presign
        assert len(h_term.type.broadcastable) == 1

        precoeff =  T.dot(sq_S, self.alpha)
        precoeff.name = 'precoeff'
        s_term_1 = half * precoeff
        assert len(s_term_1.type.broadcastable) == 1

        presign2 = T.dot(HS, self.alpha * self.mu)
        presign2.name = 'presign2'
        s_term_2 = - presign2
        assert len(s_term_2.type.broadcastable) == 1

        s_term_3 = half * T.dot(H_hat, T.sqr(self.mu) * self.alpha)
        assert len(s_term_3.type.broadcastable) == 1

        s_term = s_term_1 + s_term_2 + s_term_3

        v_term_1 = half * T.dot(T.sqr(V),self.B)
        assert len(v_term_1.type.broadcastable) == 1

        term6_factor1 = V * self.B
        term6_factor2 = T.dot(HS, self.W.T)
        v_term_2 = - (term6_factor1 * term6_factor2).sum(axis=1)
        assert len(v_term_2.type.broadcastable) == 1

        term7_subterm1 = T.dot(T.sqr(T.dot(HS, self.W.T)), self.B)
        assert len(term7_subterm1.type.broadcastable) == 1
        term7_subterm2 = - T.dot( T.dot(T.sqr(HS), T.sqr(self.W.T)), self.B)
        term7_subterm3 = T.dot( T.dot(sq_HS, T.sqr(self.W.T)), self.B )

        v_term_3 = half * (term7_subterm1  + term7_subterm2 + term7_subterm3)
        assert len(v_term_3.type.broadcastable) == 1

        v_term = v_term_1 + v_term_2 + v_term_3

        rval = h_term + s_term + v_term

        return rval

    def entropy_h(self, H_hat):
        """
        .. todo::

            WRITEME
        """

        for H_hat_v in get_debug_values(H_hat):
            assert H_hat_v.min() >= 0.0
            assert H_hat_v.max() <= 1.0

        return entropy_binary_vector(H_hat)

    def entropy_hs(self, H_hat, var_s0_hat, var_s1_hat):
        """
        .. todo::

            WRITEME
        """

        half = as_floatX(.5)

        one = as_floatX(1.)

        two = as_floatX(2.)

        pi = as_floatX(np.pi)

        for H_hat_v in get_debug_values(H_hat):
            assert H_hat_v.min() >= 0.0
            assert H_hat_v.max() <= 1.0

        term1_plus_term2 = self.entropy_h(H_hat)
        assert len(term1_plus_term2.type.broadcastable) == 1

        term3 = T.sum( H_hat * ( half * (T.log(var_s1_hat) +  T.log(two*pi) + one )  ) , axis= 1)
        assert len(term3.type.broadcastable) == 1

        term4 = T.dot( 1.-H_hat, half * (T.log(var_s0_hat) +  T.log(two*pi) + one ))
        assert len(term4.type.broadcastable) == 1


        for t12, t3, t4 in get_debug_values(term1_plus_term2, term3, term4):
            debug_assert(not np.any(np.isnan(t12)))
            debug_assert(not np.any(np.isnan(t3)))
            debug_assert(not np.any(np.isnan(t4)))

        rval = term1_plus_term2 + term3 + term4

        assert len(rval.type.broadcastable) == 1

        return rval

    def infer(self, V, return_history=False):
        """
        .. todo::

            WRITEME
        """
        return self.e_step.infer(V, return_history)

    def make_learn_func(self, V):
        """
        WRITEME

        Parameters
        ----------
        V : tensor_like
            A symbolic design matrix

        WRITEME: Returns section
        """

        #E step
        hidden_obs = self.infer(V)

        stats = SufficientStatistics.from_observations(needed_stats = self.m_step.needed_stats(),
                V = V, **hidden_obs)

        H_hat = hidden_obs['H_hat']
        S_hat = hidden_obs['S_hat']

        learning_updates = self.m_step.get_updates(self, stats, H_hat, S_hat)

        if self.recycle_q:
            learning_updates[self.prev_H] = H_hat
            learning_updates[self.prev_S] = S_hat

        self.modify_updates(learning_updates)

        if self.debug_m_step:
            energy_functional_before = self.energy_functional(H = hidden_obs['H'],
                                                      var_s0_hat = hidden_obs['var_s0_hat'],
                                                      var_s1_hat = hidden_obs['var_s1_hat'],
                                                      stats = stats)

            tmp_bias_hid = self.bias_hid
            tmp_mu = self.mu
            tmp_alpha = self.alpha
            tmp_W = self.W
            tmp_B_driver = self.B_driver

            self.bias_hid = learning_updates[self.bias_hid]
            self.mu = learning_updates[self.mu]
            self.alpha = learning_updates[self.alpha]
            if self.W in learning_updates:
                self.W = learning_updates[self.W]
            self.B_driver = learning_updates[self.B_driver]
            self.make_pseudoparams()

            try:
                energy_functional_after  = self.energy_functional(H_hat = hidden_obs['H_hat'],
                                                          var_s0_hat = hidden_obs['var_s0_hat'],
                                                          var_s1_hat = hidden_obs['var_s1_hat'],
                                                          stats = stats)
            finally:
                self.bias_hid = tmp_bias_hid
                self.mu = tmp_mu
                self.alpha = tmp_alpha
                self.W = tmp_W
                self.B_driver = tmp_B_driver
                self.make_pseudoparams()

            energy_functional_diff = energy_functional_after - energy_functional_before

            learning_updates[self.energy_functional_diff] = energy_functional_diff



        logger.info("compiling s3c learning function...")
        t1 = time.time()
        rval = function([V], updates = learning_updates)
        t2 = time.time()
        logger.debug("... compilation took {0} seconds".format(t2-t1))
        logger.debug("graph size: "
                    "{0}".format(len(rval.maker.fgraph.toposort())))

        return rval

    def _modify_updates(self, updates):
        """
        .. todo::

            WRITEME
        """

        assert self.bias_hid in self.censored_updates

        def should_censor(param):
            return param in updates and updates[param] not in self.censored_updates[param]

        if should_censor(self.W):
            if self.disable_W_update:
                del updates[self.W]
            elif self.constrain_W_norm:
                norms = theano_norms(updates[self.W])
                updates[self.W] /= norms.dimshuffle('x',0)

        if should_censor(self.alpha):
            updates[self.alpha] = T.clip(updates[self.alpha],self.min_alpha,self.max_alpha)

        if should_censor(self.mu):
            updates[self.mu] = T.clip(updates[self.mu],self.min_mu,self.max_mu)

        if should_censor(self.B_driver):
            updates[self.B_driver] = T.clip(updates[self.B_driver],self.min_B,self.max_B)

        if should_censor(self.bias_hid):
            updates[self.bias_hid] = T.clip(updates[self.bias_hid],self.min_bias_hid,self.max_bias_hid)

        model_params = self.get_params()
        for param in updates:
            if param in model_params:
                self.censored_updates[param] = self.censored_updates[param].union(set([updates[param]]))


    def random_design_matrix(self, batch_size, theano_rng = None,
                            H_sample = None, S_sample = None,
                            full_sample = True, return_all = False):
        """
        .. todo::

            WRITEME

        Parameters
        ----------
        H_sample: a matrix of values of H, optional
            if none is provided, samples one from the prior
            (H_sample is used if you want to see what samples due
            to specific hidden units look like, or when sampling
            from a larger model that s3c is part of)
        """

        if theano_rng is None:
            assert H_sample is not None
            assert S_sample is not None
            assert full_sample == False

        if not hasattr(self,'p'):
            self.make_pseudoparams()

        hid_shape = (batch_size, self.nhid)


        if H_sample is None:
            H_sample = theano_rng.binomial( size = hid_shape, n = 1, p = self.p, dtype = self.W.dtype)
        assert H_sample.dtype == 'float32' # rm

        if hasattr(H_sample,'__array__'):
            assert len(H_sample.shape) == 2
        else:
            assert len(H_sample.type.broadcastable) == 2

        if S_sample is None:
            S_sample = theano_rng.normal( size = hid_shape, avg = self.mu, std = T.sqrt(1./self.alpha) )
        assert S_sample.dtype == 'float32' # rm

        final_hs_sample = H_sample * S_sample

        assert len(final_hs_sample.type.broadcastable) == 2

        V_mean = T.dot(final_hs_sample, self.W.T)


        if not full_sample:
            warnings.warn('showing conditional means (given sampled h and s) on visible units rather than true samples')
            if return_all:
                return H_sample, S_sample, V_mean
            return V_mean

        V_sample = theano_rng.normal( size = V_mean.shape, avg = V_mean, std = T.sqrt(1./self.B))
        assert V_sample.dtype == 'float32' # rm

        if return_all:
            assert H_sample is not None
            assert S_sample is not None
            assert V_sample is not None
            return H_sample, S_sample, V_sample
        return V_sample

    @classmethod
    def expected_log_prob_vhs_needed_stats(cls):
        """
        .. todo::

            WRITEME
        """
        h = S3C.expected_log_prob_h_needed_stats()
        s = S3C.expected_log_prob_s_given_h_needed_stats()
        v = S3C.expected_log_prob_v_given_hs_needed_stats()

        union = h.union(s).union(v)

        return union


    def expected_log_prob_vhs(self, stats, H_hat, S_hat):
        """
        .. todo::

            WRITEME
        """

        expected_log_prob_v_given_hs = self.expected_log_prob_v_given_hs(stats, H_hat = H_hat, S_hat = S_hat)
        expected_log_prob_s_given_h  = self.expected_log_prob_s_given_h(stats)
        expected_log_prob_h          = self.expected_log_prob_h(stats)

        rval = expected_log_prob_v_given_hs + expected_log_prob_s_given_h + expected_log_prob_h

        assert len(rval.type.broadcastable) == 0

        return rval


    def log_partition_function(self):
        """
        .. todo::

            WRITEME
        """

        half = as_floatX(0.5)
        two = as_floatX(2.)
        pi = as_floatX(np.pi)
        N = as_floatX(self.nhid)

        #log partition function terms
        term1 = -half * T.sum(T.log(self.B))
        term2 = half * N * T.log(two * pi)
        term3 = - half * T.log( self.alpha ).sum()
        term4 = half * N * T.log(two*pi)
        term5 = T.nnet.softplus(self.bias_hid).sum()

        return term1 + term2 + term3 + term4 + term5


    def expected_log_prob_vhs_batch(self, V, H_hat, S_hat, var_s0_hat, var_s1_hat):
        """
        .. todo::

            WRITEME
        """

        half = as_floatX(0.5)
        two = as_floatX(2.)
        pi = as_floatX(np.pi)
        N = as_floatX(self.nhid)

        negative_log_partition_function = - self.log_partition_function()
        assert len(negative_log_partition_function.type.broadcastable) == 0

        #energy term
        negative_energy = - self.expected_energy_vhs(V = V, H_hat = H_hat, S_hat = S_hat, var_s0_hat = var_s0_hat, var_s1_hat = var_s1_hat)
        assert len(negative_energy.type.broadcastable) == 1

        rval = negative_log_partition_function + negative_energy

        return rval


    def log_prob_v_given_hs(self, V, H, S):
        """
        V, H, S are SAMPLES   (i.e., H must be LITERALLY BINARY)
        Return value is a vector, of length batch size

        Parameters
        ----------
        V : WRITEME
        H : WRITEME
        S : WRITEME

        Returns
        -------
        WRITEME
        """

        half = as_floatX(0.5)
        two = as_floatX(2.)
        pi = as_floatX(np.pi)
        N = as_floatX(self.nhid)

        term1 = half * T.sum(T.log(self.B))
        term2 = - half * N * T.log(two * pi)

        mean_HS = H * S
        recons = T.dot(H*S, self.W.T)
        residuals = V - recons


        term3 = - half * T.dot(T.sqr(residuals), self.B)

        rval = term1 + term2 + term3

        assert len(rval.type.broadcastable) == 1

        return rval

    @classmethod
    def expected_log_prob_v_given_hs_needed_stats(cls):
        """
        .. todo::

            WRITEME
        """
        return set(['mean_sq_v','mean_hsv', 'mean_sq_hs', 'mean_sq_mean_hs'])

    def expected_log_prob_v_given_hs(self, stats, H_hat, S_hat):
        """
        Return value is a SCALAR-- expectation taken across batch index too

        Parameters
        ----------
        stats : WRITEME
        H_hat : WRITEME
        S_hat : WRITEME

        Returns
        -------
        WRITEME
        """


        """
        E_v,h,s \sim Q log P( v | h, s)
        = sum_k [  E_v,h,s \sim Q log sqrt(B/2 pi) exp( - 0.5 B (v- W[v,:] (h*s) )^2)   ]
        = sum_k [ E_v,h,s \sim Q 0.5 log B_k - 0.5 log 2 pi - 0.5 B_k v_k^2 + v_k B_k W[k,:] (h*s) - 0.5 B_k sum_i sum_j W[k,i] W[k,j] h_i s_i h_j s_j ]
        = sum_k [ 0.5 log B_k - 0.5 log 2 pi - 0.5 B_k v_k^2 + v_k B_k W[k,:] (h*s) ] - 0.5  sum_k B_k sum_i,j W[k,i] W[k,j]  < h_i s_i  h_j s_j >
        = sum_k [ 0.5 log B_k - 0.5 log 2 pi - 0.5 B_k v_k^2 + v_k B_k W[k,:] (h*s) ] - (1/2T)  sum_k B_k sum_i,j W[k,i] W[k,j]  sum_t <h_it s_it  h_jt s_t>
        = sum_k [ 0.5 log B_k - 0.5 log 2 pi - 0.5 B_k v_k^2 + v_k B_k W[k,:] (h*s) ] - (1/2T)  sum_k B_k sum_t sum_i,j W[k,i] W[k,j] <h_it s_it  h_jt s_t>
        = sum_k [ 0.5 log B_k - 0.5 log 2 pi - 0.5 B_k v_k^2 + v_k B_k W[k,:] (h*s) ]
          - (1/2T)  sum_k B_k sum_t sum_i W[k,i]  sum_{j\neq i} W[k,j] <h_it s_it>  <h_jt s_t>
          - (1/2T) sum_k B_k sum_t sum_i W[k,i]^2 <h_it s_it^2>
        = sum_k [ 0.5 log B_k - 0.5 log 2 pi - 0.5 B_k v_k^2 + v_k B_k W[k,:] (h*s) ]
          - (1/2T)  sum_k B_k sum_t sum_i W[k,i] <h_it s_it> sum_j W[k,j]  <h_jt s_t>
          + (1/2T) sum_k B_k sum_t sum_i W[k,i]^2 <h_it s_it>^2
          - (1/2T) sum_k B_k sum_t sum_i W[k,i]^2 <h_it s_it^2>
        = sum_k [ 0.5 log B_k - 0.5 log 2 pi - 0.5 B_k v_k^2 + v_k B_k W[k,:] (h*s) ]
          - (1/2T)  sum_k B_k sum_t sum_i W[k,i] <h_it s_it> sum_j W[k,j]  <h_jt s_t>
          + (1/2T) sum_k B_k sum_t sum_i W[k,i]^2 (<h_it s_it>^2 - <h_it s_it^2>)
        = sum_k [ 0.5 log B_k - 0.5 log 2 pi - 0.5 B_k v_k^2 + v_k B_k W[k,:] (h*s) ]
          - (1/2T)  sum_k B_k sum_t sum_i W_ki HS_it sum_j W_kj  HS_tj
          + (1/2T) sum_k B_k sum_t sum_i sq(W)_ki ( sq(HS)-sq_HS)_it
        = sum_k [ 0.5 log B_k - 0.5 log 2 pi - 0.5 B_k v_k^2 + v_k B_k W[k,:] (h*s) ]
          - (1/2T)  sum_k B_k sum_t sum_i W_ki HS_it sum_j W_kj  HS_tj
          + (1/2T) sum_k B_k sum_t sum_i sq(W)_ki ( sq(HS)-sq_HS)_it
        = sum_k [ 0.5 log B_k - 0.5 log 2 pi - 0.5 B_k v_k^2 + v_k B_k W[k,:] (h*s) ]
          - (1/2T)  sum_k B_k sum_t sum_i W_ki HS_it sum_j W_kj  HS_tj
          + (1/2T) sum_k B_k sum_t sum_i sq(W)_ki ( sq(HS)-sq_HS)_it
        = sum_k [ 0.5 log B_k - 0.5 log 2 pi - 0.5 B_k v_k^2 + v_k B_k W[k,:] (h*s) ]
          - (1/2T)  sum_k B_k sum_t (HS_t: W_k:^T)  (HS_t:  W_k:^T)
          + (1/2) sum_k B_k  sum_i sq(W)_ki ( mean_sq_mean_hs-mean_sq_hs)_i
        = sum_k [ 0.5 log B_k - 0.5 log 2 pi - 0.5 B_k v_k^2 + v_k B_k W[k,:] (h*s) ]
          - (1/2T)  sum_t sum_k B_k  (HS_t: W_k:^T)^2
          + (1/2) sum_k B_k  sum_i sq(W)_ki ( mean_sq_mean_hs-mean_sq_hs)_i
        = sum_k [ 0.5 log B_k - 0.5 log 2 pi - 0.5 B_k v_k^2 + v_k B_k W[k,:] (h*s) ]
          - (1/2)  mean(   (HS W^T)^2 B )
          + (1/2) sum_k B_k  sum_i sq(W)_ki ( mean_sq_mean_hs-mean_sq_hs)_i
        """


        half = as_floatX(0.5)
        two = as_floatX(2.)
        pi = as_floatX(np.pi)
        N = as_floatX(self.nhid)

        mean_sq_v = stats.d['mean_sq_v']
        mean_hsv  = stats.d['mean_hsv']
        mean_sq_mean_hs = stats.d['mean_sq_mean_hs']
        mean_sq_hs = stats.d['mean_sq_hs']

        term1 = half * T.sum(T.log(self.B))
        term2 = - half * N * T.log(two * pi)
        term3 = - half * T.dot(self.B, mean_sq_v)
        term4 = T.dot(self.B , (self.W * mean_hsv.T).sum(axis=1))

        HS = H_hat * S_hat
        recons = T.dot(HS, self.W.T)
        sq_recons = T.sqr(recons)
        weighted = T.dot(sq_recons, self.B)
        assert len(weighted.type.broadcastable) == 1
        term5 = - half * T.mean( weighted)

        term6 = half * T.dot(self.B, T.dot(T.sqr(self.W), mean_sq_mean_hs - mean_sq_hs))

        rval = term1 + term2 + term3 + term4 + term5 + term6

        assert len(rval.type.broadcastable) == 0

        return rval


    @classmethod
    def expected_log_prob_s_given_h_needed_stats(cls):
        """
        .. todo::

            WRITEME
        """
        return set(['mean_h','mean_hs','mean_sq_s'])

    def expected_log_prob_s_given_h(self, stats):
        """
        .. todo::

            WRITEME

        E_h,s\sim Q log P(s|h)
        = E_h,s\sim Q log sqrt( alpha / 2pi) exp(- 0.5 alpha (s-mu h)^2)
        = E_h,s\sim Q log sqrt( alpha / 2pi) - 0.5 alpha (s-mu h)^2
        = E_h,s\sim Q  0.5 log alpha - 0.5 log 2 pi - 0.5 alpha s^2 + alpha s mu h + 0.5 alpha mu^2 h^2
        = E_h,s\sim Q 0.5 log alpha - 0.5 log 2 pi - 0.5 alpha s^2 + alpha mu h s + 0.5 alpha mu^2 h
        = 0.5 log alpha - 0.5 log 2 pi - 0.5 alpha mean_sq_s + alpha mu mean_hs - 0.5 alpha mu^2 mean_h
        """

        mean_h = stats.d['mean_h']
        mean_sq_s = stats.d['mean_sq_s']
        mean_hs = stats.d['mean_hs']

        half = as_floatX(0.5)
        two = as_floatX(2.)
        N = as_floatX(self.nhid)
        pi = as_floatX(np.pi)

        term1 = half * T.log( self.alpha ).sum()
        term2 = - half * N * T.log(two*pi)
        term3 = - half * T.dot( self.alpha , mean_sq_s )
        term4 = T.dot(self.mu*self.alpha,mean_hs)
        term5 = - half * T.dot(T.sqr(self.mu), self.alpha * mean_h)

        rval = term1 + term2 + term3 + term4 + term5

        assert len(rval.type.broadcastable) == 0

        return rval

    @classmethod
    def expected_log_prob_h_needed_stats(cls):
        """
        .. todo::

            WRITEME
        """
        return set(['mean_h'])

    def expected_log_prob_h(self, stats):
        """
        Returns the expected log probability of the vector h
        under the model when the data is drawn according to
        stats:

            E_h\sim Q log P(h)
            = E_h\sim Q log exp( bh) / (1+exp(b))
            = E_h\sim Q bh - softplus(b)

        Parameters
        ----------
        stats : WRITEME

        Returns
        -------
        WRITEME
        """

        mean_h = stats.d['mean_h']

        term1 = T.dot(self.bias_hid, mean_h)
        term2 = - T.nnet.softplus(self.bias_hid).sum()

        rval = term1 + term2

        assert len(rval.type.broadcastable) == 0

        return rval


    def make_pseudoparams(self):
        """
        .. todo::

            WRITEME
        """
        if self.tied_B:
            #can't just use a dimshuffle; dot products involving B won't work
            #and because doing it this way makes the partition function multiply by nvis automatically
            self.B = self.B_driver + as_floatX(np.zeros(self.nvis))
            self.B.name = 'S3C.tied_B'
        else:
            self.B = self.B_driver

        self.w = T.dot(self.B, T.sqr(self.W))
        self.w.name = 'S3C.w'

        self.p = T.nnet.sigmoid(self.bias_hid)
        self.p.name = 'S3C.p'

    def reset_censorship_cache(self):
        """
        .. todo::

            WRITEME
        """

        self.censored_updates = {}
        self.register_names_to_del(['censored_updates'])
        for param in self.get_params():
            self.censored_updates[param] = set([])

    def redo_theano(self):
        """
        .. todo::

            WRITEME
        """

        self.reset_censorship_cache()

        if not self.autonomous:
            return

        try:
            self.compile_mode()
            init_names = dir(self)

            self.make_pseudoparams()

            self.e_step.register_model(self)

            self.get_B_value = function([], self.B)

            X = T.matrix(name='V')
            X.tag.test_value = np.cast[config.floatX](self.rng.randn(self._test_batch_size,self.nvis))

            self.learn_func = self.make_learn_func(X)

            final_names = dir(self)

            self.register_names_to_del([name for name in final_names if name not in init_names])
        finally:
            self.deploy_mode()

    def train_batch(self, dataset, batch_size):
        """
        .. todo::

            WRITEME
        """

        if self.set_B_to_marginal_precision:
            assert not self.tied_B

            var = dataset.X.var(axis=0)

            self.B_driver.set_value( 1. / (var + .01))

        if self.stop_after_hack is not None:
            if self.monitor.examples_seen > self.stop_after_hack:
                logger.error('stopping due to too many examples seen')
                quit(-1)

        self.learn_mini_batch(dataset.get_batch_design(batch_size))
        return True

    def print_status(self):
        """
        .. todo::

            WRITEME
        """
        b = self.bias_hid.get_value(borrow=True)
        assert not np.any(np.isnan(b))
        p = 1./(1.+np.exp(-b))
        logger.info('p: ({0}, {1}, {2})'.format(p.min(), p.mean(), p.max()))
        B = self.B_driver.get_value(borrow=True)
        assert not np.any(np.isnan(B))
        logger.info('B: ({0}, {1}, {2})'.format(B.min(), B.mean(), B.max()))
        mu = self.mu.get_value(borrow=True)
        assert not np.any(np.isnan(mu))
        logger.info('mu: ({0}, {1}, {2})'.format(mu.min(), mu.mean(),
                                                 mu.max()))
        alpha = self.alpha.get_value(borrow=True)
        assert not np.any(np.isnan(alpha))
        logger.info('alpha: ({0}, {1}, {2})'.format(alpha.min(), alpha.mean(),
                                                    alpha.max()))
        W = self.W.get_value(borrow=True)
        assert not np.any(np.isnan(W))
        assert not np.any(np.isinf(W))
        logger.info('W: ({0}, {1}, {2})'.format(W.min(), W.mean(), W.max()))
        norms = numpy_norms(W)
        logger.info('W norms: '
                    '({0}, {1}, {2})'.format(norms.min(), norms.mean(),
                                             norms.max()))

    def learn_mini_batch(self, X):
        """
        .. todo::

            WRITEME
        """

        self.learn_func(X)

        if self.momentum_saturation_example is not None:
            alpha = float(self.monitor.get_examples_seen()) / float(self.momentum_saturation_example)
            alpha = min(alpha, 1.0)
            self.momentum.set_value(np.cast[config.floatX]( (1.-alpha) * self.init_momentum + alpha * self.final_momentum))

        if self.monitor.get_examples_seen() % self.print_interval == 0:
            self.print_status()

        if self.debug_m_step:
            if self.energy_functional_diff.get_value() < 0.0:
                warnings.warn( "m step decreased the em functional" )
                if self.debug_m_step != 'warn':
                    quit(-1)

    def get_weights_format(self):
        """
        .. todo::

            WRITEME
        """
        return ['v','h']

    def get_weights(self):
        """
        .. todo::

            WRITEME
        """

        W = self.W.get_value()

        x = raw_input('multiply weights by mu? (y/n) ')

        if x == 'y':
            return W * self.mu.get_value()
        elif x == 'n':
            return W
        assert False

def reflection_clip(S_hat, new_S_hat, rho = 0.5):
    """
    .. todo::

        WRITEME
    """
    rho = np.cast[config.floatX](rho)

    ceiling = full_max(abs(new_S_hat))

    positives = S_hat > 0
    non_positives = 1. - positives


    negatives = S_hat < 0
    non_negatives = 1. - negatives


    low = - rho * positives * S_hat - non_positives * ceiling
    high = non_negatives * ceiling - rho * negatives * S_hat

    rval = T.clip(new_S_hat, low,  high )

    S_name = make_name(S_hat, 'anon_S_hat')
    new_S_name = make_name(new_S_hat, 'anon_new_S_hat')

    rval.name = 'reflection_clip(%s, %s)' % (S_name, new_S_name)

    return rval

def damp(old, new, new_coeff):
    """
    .. todo::

        WRITEME
    """

    if new_coeff == 1.0:
        return new

    old_coeff = as_floatX(1.) - new_coeff

    new_scaled = new_coeff * new

    old_scaled = old_coeff * old

    rval =  new_scaled + old_scaled

    old_name = make_name(old, anon='anon_old')
    new_name = make_name(new, anon='anon_new')

    rval.name = 'damp( %s, %s)' % (old_name, new_name)

    return rval

class E_Step(object):
    """
    A variational E_step that works by running damped fixed point updates on a
    structured variation approximation to P(v,h,s) (i.e., we do not use any
    auxiliary variable).

    The structured variational approximation is:

    P(v,h,s) = \Pi_i Q_i (h_i, s_i)

    We alternate between updating the Q parameters over s in parallel and
    updating the q parameters over h in parallel.

    The h parameters are updated with a damping coefficient that is the same
    for all units but changes each time step, specified by the yaml file. The
    slab variables are updated with:

    - optionally: a unit-specific damping designed to ensure stability
        by preventing reflections from going too far away from the origin.
    - optionally: additional damping that is the same for all units but
        changes each time step, specified by the yaml file

    The update equations were derived based on updating h_i independently,
    then updating s_i independently, even though it is possible to solve for
    a simultaneous update to h_i and s_I.

    This is because the damping is necessary for parallel updates to be
    reasonable, but damping the solution to s_i from the joint update makes the
    solution to h_i from the joint update no longer optimal.

    Parameters
    ----------
    h_new_coeff_schedule : list
        List of coefficients to put on the new value of h on each damped
        fixed point step (coefficients on s are driven by a special
        formula). Length of this list determines the number of fixed point
        steps. If None, assumes that the model is not meant to run on its
        own (ie a larger model will specify how to do inference in this
        layer)
    s_new_coeff_schedule : list
        List of coefficients to put on the new value of s on each damped
        fixed point step. These are applied AFTER the reflection
        clipping, which can be seen as a form of per-unit damping.
        s_new_coeff_schedule must have same length as
        h_new_coeff_schedule. If s_new_coeff_schedule is not provided, it
        will be filled in with all ones, i.e. it will default to no
        damping beyond the reflection clipping
    clip_reflections, rho : bool, float
        If clip_reflections is True, the update to S_hat[i,j] is bounded
        on one side by - rho * S_hat[i,j] and unbounded on the other side
    monitor_ranges : bool
        If True, adds the channels h_range_<min,mean,max> and
        hs_range_<min,mean_max>  showing the amounts that different h_hat
        and s_hat variational parameters change across the monitoring
        dataset
    """

    def get_monitoring_channels(self, V):
        """
        .. todo::

            WRITEME
        """

        #TODO: update this to show updates to h_i and s_i in correct sequence

        rval = {}

        if self.autonomous:
            if self.monitor_kl or self.monitor_energy_functional or self.monitor_s_mag \
                    or self.monitor_ranges:
                obs_history = self.model.infer(V, return_history = True)
                assert isinstance(obs_history, list)


                final_vals = obs_history[-1]
                S_hat = final_vals['S_hat']
                H_hat = final_vals['H_hat']
                HS = H_hat * S_hat

                hs_max = T.max(HS,axis=0)
                hs_min = T.min(HS,axis=0)

                hs_range = hs_max - hs_min

                rval['hs_range_min'] = T.min(hs_range)
                rval['hs_range_mean'] = T.mean(hs_range)
                rval['hs_range_max'] = T.max(hs_range)

                h_max = T.max(H_hat,axis=0)
                h_min = T.min(H_hat,axis=0)

                h_range = h_max - h_min

                rval['h_range_min'] = T.min(h_range)
                rval['h_range_mean'] = T.mean(h_range)
                rval['h_range_max'] = T.max(h_range)




                for i in xrange(1, 2 + len(self.h_new_coeff_schedule)):
                    obs = obs_history[i-1]
                    if self.monitor_kl:
                        if i == 1:
                            rval['trunc_KL_'+str(i)] = self.truncated_KL(V, obs =  obs).mean()
                        else:
                            coeff = self.h_new_coeff_schedule[i-2]
                            rval['trunc_KL_'+str(i)+'.2(h '+str(coeff)+')'] = self.truncated_KL(V,obs = obs).mean()
                            obs = {}
                            for key in obs_history[i-1]:
                                obs[key] = obs_history[i-1][key]
                            obs['H_hat'] = obs_history[i-2]['H_hat']
                            coeff = self.s_new_coeff_schedule[i-2]
                            rval['trunc_KL_'+str(i)+'.1(s '+str(coeff)+')'] = self.truncated_KL(V,obs = obs).mean()
                            obs = obs_history[i-1]
                    if self.monitor_energy_functional:
                        rval['energy_functional_'+str(i)] = self.energy_functional(V, self.model, obs).mean()
                    if self.monitor_s_mag:
                        rval['s_mag_'+str(i)] = T.sqrt(T.sum(T.sqr(obs['S_hat'])))

        return rval

    def __init__(self, h_new_coeff_schedule,
                       s_new_coeff_schedule = None,
                       clip_reflections = False,
                       monitor_kl = False,
                       monitor_energy_functional = False,
                       monitor_s_mag = False,
                       rho = 0.5,
                       monitor_ranges = False):
        self.autonomous = True

        if h_new_coeff_schedule is None:
            self.autonomous = False
            assert s_new_coeff_schedule is None
            assert rho is None
            assert clip_reflections is None
            assert monitor_energy_functional is None
        else:
            if s_new_coeff_schedule is None:
                s_new_coeff_schedule = [ 1.0 for cur_rho in h_new_coeff_schedule ]
            else:
                if len(s_new_coeff_schedule) != len(h_new_coeff_schedule):
                    raise ValueError('s_new_coeff_schedule has %d elems ' % (len(s_new_coeff_schedule),) + \
                            'but h_new_coeff_schedule has %d elems' % (len(h_new_coeff_schedule),) )


        if s_new_coeff_schedule is not None:
            assert isinstance(s_new_coeff_schedule, (list, tuple))
        if h_new_coeff_schedule is not None:
            assert isinstance(h_new_coeff_schedule, (list, tuple))

        self.s_new_coeff_schedule = s_new_coeff_schedule

        self.clip_reflections = clip_reflections
        self.h_new_coeff_schedule = h_new_coeff_schedule
        self.monitor_kl = monitor_kl
        self.monitor_energy_functional = monitor_energy_functional
        self.monitor_ranges = monitor_ranges

        if self.autonomous:
            self.rho = as_floatX(rho)

        self.monitor_s_mag = monitor_s_mag

        self.model = None

    def energy_functional(self, V, model, obs):
        """
        Return value is a scalar

        Parameters
        ----------
        V : WRITEME
        model : WRITEME
        obs : WRITEME

        Returns
        -------
        WRITEME
        """
        #TODO: refactor so that this is shared between E-steps

        needed_stats = S3C.expected_log_prob_vhs_needed_stats()

        stats = SufficientStatistics.from_observations( needed_stats = needed_stats,
                                                        V = V, ** obs )

        H_hat = obs['H_hat']
        S_hat = obs['S_hat']
        var_s0_hat = obs['var_s0_hat']
        var_s1_hat = obs['var_s1_hat']

        entropy_term = (model.entropy_hs(H_hat = H_hat, var_s0_hat = var_s0_hat, var_s1_hat = var_s1_hat)).mean()
        likelihood_term = model.expected_log_prob_vhs(stats, H_hat = H_hat, S_hat = S_hat)

        energy_functional = entropy_term + likelihood_term

        return energy_functional

    def register_model(self, model):
        """
        .. todo::

            WRITEME
        """
        self.model = model

    def truncated_KL(self, V, Y = None, obs = None):
        """
        KL divergence between variation and true posterior, dropping terms that
        don't depend on the variational parameters

        Parameters
        ----------
        V : WRITEME
        Y : WRITEME
        obs : WRITEME

        Returns
        -------
        WRITEME
        """

        assert Y is None
        assert obs is not None

        H_hat = obs['H_hat']
        var_s0_hat = obs['var_s0_hat']
        var_s1_hat = obs['var_s1_hat']
        S_hat = obs['S_hat']
        model = self.model

        for H_hat_v in get_debug_values(H_hat):
            assert H_hat_v.min() >= 0.0
            assert H_hat_v.max() <= 1.0

        entropy_term = - model.entropy_hs(H_hat = H_hat, var_s0_hat = var_s0_hat, var_s1_hat = var_s1_hat)
        energy_term = model.expected_energy_vhs(V, H_hat = H_hat, S_hat = S_hat,
                                        var_s0_hat = var_s0_hat, var_s1_hat = var_s1_hat)


        for entropy, energy in get_debug_values(entropy_term, energy_term):
            debug_assert(not np.any(np.isnan(entropy)))
            debug_assert(not np.any(np.isnan(energy)))

        KL = entropy_term + energy_term

        return KL

    def init_H_hat(self, V):
        """
        .. todo::

            WRITEME
        """

        if self.model.recycle_q:
            rval = self.model.prev_H

            if config.compute_test_value != 'off':
                if rval.get_value().shape[0] != V.tag.test_value.shape[0]:
                    raise Exception('E step given wrong test batch size', rval.get_value().shape, V.tag.test_value.shape)
        else:
            #just use the prior
            value =  T.nnet.sigmoid(self.model.bias_hid)
            rval = T.alloc(value, V.shape[0], value.shape[0])

            for rval_value, V_value in get_debug_values(rval, V):
                if rval_value.shape[0] != V_value.shape[0]:
                    debug_error_message("rval.shape = %s, V.shape = %s, element 0 should match but doesn't", str(rval_value.shape), str(V_value.shape))

        return rval

    def init_S_hat(self, V):
        """
        .. todo::

            WRITEME
        """
        if self.model.recycle_q:
            rval = self.model.prev_S_hat
        else:
            #just use the prior
            value = self.model.mu
            assert self.model.mu.get_value(borrow=True).shape[0] == self.model.nhid
            rval = T.alloc(value, V.shape[0], value.shape[0])

        return rval

    def infer_S_hat(self, V, H_hat, S_hat):
        """
        .. todo::

            WRITEME
        """

        for Vv, Hv in get_debug_values(V, H_hat):
            if Vv.shape != (self.model._test_batch_size,self.model.nvis):
                raise Exception('Well this is awkward. We require visible input test tags to be of shape '+str((self.model._test_batch_size,self.model.nvis))+' but the monitor gave us something of shape '+str(Vv.shape)+". The batch index part is probably only important if recycle_q is enabled. It's also probably not all that realistic to plan on telling the monitor what size of batch we need for test tags. the best thing to do is probably change self.model._test_batch_size to match what the monitor does")

            assert Vv.shape[0] == Hv.shape[0]
            if not (Hv.shape[1] == self.model.nhid):
                raise AssertionError("Hv.shape[1] is %d, does not match self.model.nhid, %d" \
                        % ( Hv.shape[1], self.model.nhid) )


        mu = self.model.mu
        alpha = self.model.alpha
        W = self.model.W
        B = self.model.B
        w = self.model.w

        BW = B.dimshuffle(0,'x') * W
        BW.name = 'infer_S_hat:BW'

        HS = H_hat * S_hat
        HS.name = 'infer_S_hat:HS'

        mean_term = mu * alpha
        mean_term.name = 'infer_S_hat:mean_term'

        assert V.dtype == config.floatX
        assert BW.dtype == config.floatX, \
            "Expected %s, got %s" % (config.floatX, BW.dtype)
        data_term = T.dot(V, BW)
        data_term.name = 'infer_S_hat:data_term'

        iterm_part_1 = - T.dot(T.dot(HS, W.T), BW)
        iterm_part_1.name = 'infer_S_hat:iterm_part_1'
        assert w.name is not None
        iterm_part_2 = w * HS
        iterm_part_2.name = 'infer_S_hat:iterm_part_2'

        interaction_term = iterm_part_1 + iterm_part_2
        interaction_term.name = 'infer_S_hat:interaction_term'

        for i1v, Vv in get_debug_values(iterm_part_1, V):
            assert i1v.shape[0] == Vv.shape[0]


        assert mean_term.dtype == config.floatX
        assert data_term.dtype == config.floatX
        assert interaction_term.dtype == config.floatX

        debug_interm = mean_term + data_term
        debug_interm.name = 'infer_S_hat:debug_interm'

        numer = debug_interm + interaction_term
        numer.name = 'infer_S_hat:numer'
        assert numer.dtype == config.floatX

        alpha = self.model.alpha
        w = self.model.w

        denom = alpha + w
        assert denom.dtype == config.floatX
        denom.name = 'infer_S_hat:denom'

        S_hat =  numer / denom


        return S_hat


    def infer_var_s0_hat(self):
        """
        .. todo::

            WRITEME
        """

        return 1. / self.model.alpha

    def infer_var_s1_hat(self):
        """
        Returns the variational parameter for the variance of s given h=1. This
        is data-independent so its just a vector of size (nhid,) and doesn't
        take any arguments

        Returns
        -------
        WRITEME
        """

        rval =  1./ (self.model.alpha + self.model.w )

        rval.name = 'var_s1'

        return rval

    def infer_H_hat_presigmoid(self, V, H_hat, S_hat):
        """
        Computes the value of H_hat prior to the application of the sigmoid
        function. This is a useful quantity to compute for larger models that
        influence h with top-down terms. Such models can apply the sigmoid
        themselves after adding the top-down interactions

        Parameters
        ----------
        V : WRITEME
        H_hat : WRITEME
        S_hat : WRITEME

        Returns
        -------
        WRITEME
        """

        half = as_floatX(.5)
        alpha = self.model.alpha
        w = self.model.w
        mu = self.model.mu
        W = self.model.W
        B = self.model.B
        BW = B.dimshuffle(0,'x') * W

        HS = H_hat * S_hat

        t1f1t1 = V

        t1f1t2 = -T.dot(HS,W.T)
        iterm_corrective = w * H_hat *T.sqr(S_hat)

        t1f1t3_effect = - half * w * T.sqr(S_hat)

        term_1_factor_1 = t1f1t1 + t1f1t2

        term_1 = T.dot(term_1_factor_1, BW) * S_hat + iterm_corrective + t1f1t3_effect

        term_2_subterm_1 = - half * alpha * T.sqr(S_hat)

        term_2_subterm_2 = alpha * S_hat * mu

        term_2_subterm_3 = - half * alpha * T.sqr(mu)

        term_2 = term_2_subterm_1 + term_2_subterm_2 + term_2_subterm_3

        term_3 = self.model.bias_hid

        term_4 = -half * T.log(alpha + self.model.w)

        term_5 = half * T.log(alpha)

        arg_to_sigmoid = term_1 + term_2 + term_3 + term_4 + term_5

        return arg_to_sigmoid


    def infer_H_hat(self, V, H_hat, S_hat, count = None):
        """
        .. todo::

            WRITEME
        """

        arg_to_sigmoid = self.infer_H_hat_presigmoid(V, H_hat, S_hat)

        H = T.nnet.sigmoid(arg_to_sigmoid)

        V_name = make_name(V, anon = 'anon_V')

        if count is not None:
            H.name = 'H_hat(%s, %d)' % ( V_name, count)

        return H

    def variational_inference(self, V, return_history = False):
        """
        .. todo::

            WRITEME

        TODO: rename to infer (for now, infer exists as a synonym)
        """

        warnings.warn("E_Step.variational_inference is deprecated. It has been renamd to E_step.infer", stacklevel = 2)

        return self.infer( V, return_history)

    def infer(self, V, return_history = False):
        """
        ... todo::

            WRITEME

        Parameters
        ----------
        V : WRITEME
        return_history : bool
            If True, returns a list of dictionaries with showing the history
            of the variational parameters throughout fixed point updates
            If False, returns a dictionary containing the final variational
            parameters

        Returns
        -------
        WRITEME
        """
        if not self.autonomous:
            raise ValueError("Non-autonomous model asked to perform inference on its own")

        alpha = self.model.alpha


        var_s0_hat = 1. / alpha
        var_s1_hat = self.infer_var_s1_hat()


        H_hat = self.init_H_hat(V)
        S_hat = self.init_S_hat(V)

        def check_H(my_H, my_V):
            if my_H.dtype != config.floatX:
                raise AssertionError('my_H.dtype should be config.floatX, but they are '
                        ' %s and %s, respectively' % (my_H.dtype, config.floatX))

            allowed_v_types = ['float32']

            if config.floatX == 'float64':
                allowed_v_types.append('float64')

            assert my_V.dtype in allowed_v_types

            if config.compute_test_value != 'off':
                from theano.gof.op import PureOp
                Hv = PureOp._get_test_value(my_H)

                Vv = my_V.tag.test_value

                assert Hv.shape[0] == Vv.shape[0]

        check_H(H_hat,V)

        def make_dict():

            return {
                    'H_hat' : H_hat,
                    'S_hat' : S_hat,
                    'var_s0_hat' : var_s0_hat,
                    'var_s1_hat': var_s1_hat,
                    }


        history = [ make_dict() ]

        count = 2

        h_new_coeff_schedule = self.h_new_coeff_schedule
        s_new_coeff_schedule = self.s_new_coeff_schedule

        assert isinstance(s_new_coeff_schedule, (list, tuple))
        assert isinstance(h_new_coeff_schedule, (list, tuple))

        for new_H_coeff, new_S_coeff in zip(h_new_coeff_schedule, s_new_coeff_schedule):
            new_H_coeff = as_floatX(new_H_coeff)
            new_S_coeff = as_floatX(new_S_coeff)

            assert V.dtype == config.floatX
            assert H_hat.dtype == config.floatX
            assert S_hat.dtype == config.floatX
            new_S_hat = self.infer_S_hat(V, H_hat, S_hat)
            assert new_S_hat.type.dtype == config.floatX

            if self.clip_reflections:
                clipped_S_hat = reflection_clip(S_hat = S_hat, new_S_hat = new_S_hat, rho = self.rho)
            else:
                clipped_S_hat = new_S_hat
            assert clipped_S_hat.dtype == config.floatX
            assert S_hat.type.dtype == config.floatX
            assert new_S_coeff.dtype == config.floatX
            S_hat = damp(old = S_hat, new = clipped_S_hat, new_coeff = new_S_coeff)
            S_hat.name = 'S_hat_'+str(count)
            assert  S_hat.type.dtype == config.floatX
            new_H = self.infer_H_hat(V, H_hat, S_hat, count)
            assert new_H.type.dtype == config.floatX
            count += 1

            H_hat = damp(old = H_hat, new = new_H, new_coeff = new_H_coeff)

            check_H(H_hat,V)

            history.append(make_dict())


        if return_history:
            return history
        else:
            return history[-1]

    def __setstate__(self,d):
        """
        .. todo::

            WRITEME
        """
        #patch pkls made before autonomous flag
        if 'autonomous' not in d:
            d['autonomous'] = True

        self.__dict__.update(d)

class Grad_M_Step:
    """
    A partial M-step based on gradient ascent. More aggressive M-steps are
    possible but didn't work particularly well in practice on STL-10/CIFAR-10

    .. todo::

        WRITEME : parameter list
    """

    def __init__(self, learning_rate = None, B_learning_rate_scale  = 1,
            alpha_learning_rate_scale = 1.,
            W_learning_rate_scale = 1, p_penalty = 0.0, B_penalty = 0.0, alpha_penalty = 0.0):
        self.autonomous = True

        if learning_rate is None:
            self.autonomous = False
        else:
            self.learning_rate = np.cast[config.floatX](float(learning_rate))


        self.B_learning_rate_scale = np.cast[config.floatX](float(B_learning_rate_scale))
        self.W_learning_rate_scale = np.cast[config.floatX](float(W_learning_rate_scale))
        self.alpha_learning_rate_scale = np.cast[config.floatX](float(alpha_learning_rate_scale))
        self.p_penalty = as_floatX(p_penalty)
        self.B_penalty = as_floatX(B_penalty)
        self.alpha_penalty = as_floatX(alpha_penalty)

    def get_updates(self, model, stats, H_hat, S_hat):
        """
        .. todo::

            WRITEME
        """

        assert self.autonomous

        params = model.get_params()

        obj = model.expected_log_prob_vhs(stats, H_hat, S_hat) - T.mean(model.p) * self.p_penalty - T.mean(model.B)*self.B_penalty-T.mean(model.alpha)*self.alpha_penalty


        constants = set(stats.d.values()).union([H_hat, S_hat])

        grads = T.grad(obj, params, consider_constant = constants)

        updates = OrderedDict()

        for param, grad in zip(params, grads):
            learning_rate = self.learning_rate

            if param is model.W:
                learning_rate = learning_rate * self.W_learning_rate_scale

            if param is model.B_driver:
                #can't use *= since this is a numpy ndarray now
                learning_rate = learning_rate * self.B_learning_rate_scale

            if param is model.alpha:
                learning_rate = learning_rate * self.alpha_learning_rate_scale

            if model.momentum_saturation_example is None:
                if param is model.W and model.constrain_W_norm:
                    #project the gradient into the tangent space of the unit hypersphere
                    #see "On Gradient Adaptation With Unit Norm Constraints"
                    #this is the "true gradient" method on a sphere
                    #it computes the gradient, projects the gradient into the tangent space of the sphere,
                    #then moves a certain distance along a geodesic in that direction

                    g_k = learning_rate * grad

                    h_k = g_k -  (g_k*model.W).sum(axis=0) * model.W

                    theta_k = T.sqrt(1e-8+T.sqr(h_k).sum(axis=0))

                    u_k = h_k / theta_k

                    updates[model.W] = T.cos(theta_k) * model.W + T.sin(theta_k) * u_k

                else:
                    pparam = param

                    inc = learning_rate * grad

                    updated_param = pparam + inc

                    updates[param] = updated_param
            else:
                #use momentum
                inc = model.params_to_incs[param]
                updates[inc] = model.momentum * inc + learning_rate * grad
                updates[param] = param + inc

        return updates

    def needed_stats(self):
        """
        .. todo::

            WRITEME
        """
        return S3C.expected_log_prob_vhs_needed_stats()

    def get_monitoring_channels(self, V, model):
        """
        .. todo::

            WRITEME
        """
        hid_observations = model.infer(V)

        stats = SufficientStatistics.from_observations(needed_stats = S3C.expected_log_prob_vhs_needed_stats(),
                V = V, **hid_observations)

        H_hat = hid_observations['H_hat']
        S_hat = hid_observations['S_hat']

        obj = model.expected_log_prob_vhs(stats, H_hat, S_hat)

        return { 'expected_log_prob_vhs' : obj }


class E_Step_Scan(E_Step):
    """
    The heuristic E step implemented using scan rather than unrolled loops
    """

    def __init__(self, ** kwargs):
        super(E_Step_Scan, self).__init__( ** kwargs )

        self.h_new_coeff_schedule = sharedX( self.h_new_coeff_schedule)
        self.s_new_coeff_schedule = sharedX( self.s_new_coeff_schedule)

    def infer(self, V, return_history = False):
        """
        WRITEME

        Parameters
        ----------
        V : WRITEME
        return_history : bool
            If True, returns a list of dictionaries with showing the history
            of the variational parameters throughout fixed point updates
            If False, returns a dictionary containing the final variational
            parameters

        Returns
        -------
        WRITEME
        """
        if not self.autonomous:
            raise ValueError("Non-autonomous model asked to perform inference on its own")

        alpha = self.model.alpha


        var_s0_hat = 1. / alpha
        var_s1_hat = self.infer_var_s1_hat()


        H_hat   =    self.init_H_hat(V)
        S_hat =    self.init_S_hat(V)

        def inner_function(new_H_coeff, new_S_coeff, H_hat, S_hat):

            orig_H_dtype = H_hat.dtype
            orig_S_dtype = S_hat.dtype

            new_S_hat = self.infer_S_hat(V, H_hat,S_hat)
            if self.clip_reflections:
                clipped_S_hat = reflection_clip(S_hat = S_hat, new_S_hat = new_S_hat, rho = self.rho)
            else:
                clipped_S_hat = new_S_hat
            S_hat = damp(old = S_hat, new = clipped_S_hat, new_coeff = new_S_coeff)
            new_H = self.infer_H_hat(V, H_hat, S_hat)

            H_hat = damp(old = H_hat, new = new_H, new_coeff = new_H_coeff)

            assert H_hat.dtype == orig_H_dtype
            assert S_hat.dtype == orig_S_dtype

            return H_hat, S_hat


        (H_hats, S_hats), _ = scan( fn = inner_function, sequences =
                [self.h_new_coeff_schedule,
                 self.s_new_coeff_schedule],
                                        outputs_info = [ H_hat, S_hat ] )

        if  return_history:
            hist =  [
                    {'H_hat' : H_hats[i],
                     'S_hat' : S_hats[i],
                     'var_s0_hat' : var_s0_hat,
                     'var_s1_hat' : var_s1_hat
                    } for i in xrange(self.h_new_coeff_schedule.get_value().shape[0]) ]

            hist.insert(0, { 'H_hat' : H_hat,
                             'S_hat' : S_hat,
                             'var_s0_hat' : var_s0_hat,
                             'var_s1_hat' : var_s1_hat
                            } )

            return hist

        return {
                'H_hat' : H_hats[-1],
                'S_hat' : S_hats[-1],
                'var_s0_hat' : var_s0_hat,
                'var_s1_hat': var_s1_hat,
                }

########NEW FILE########
__FILENAME__ = softmax_regression
"""
Softmax regression

Note: softmax regression is implemented as a special case of the MLP.
    It is an MLP with no hidden layers, and with the output layer
    always set to be softmax.
"""

__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2012-2013, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"

from pylearn2.models import mlp

class SoftmaxRegression(mlp.MLP):
    """
    A softmax regression model. TODO: add reference.
    This is just a convenience class making a logistic regression model
    out of an MLP.

    Parameters
    ----------
    n_classes : int
        WRITEME
    batch_size : int, optional
        If not None, then should be a positive integer. Mostly useful if
        one of your layers involves a theano op like convolution that
        requires a hard-coded batch size.
    input_space : pylearn2.space.Space, optional
        A Space specifying the kind of input the MLP acts on. If None,
        input space is specified by nvis.
    irange : WRITEME
    istdev : WRITEME
    W_lr_scale : WRITEME
    b_lr_scale : WRITEME
    max_row_norm : WRITEME
    max_col_norm : WRITEME
    sparse_init : WRITEME
    init_bias_target_marginals : WRITEME
    nvis : WRITEME
    seed : WRITEME
    """

    def __init__(self,
                 n_classes,
                 batch_size=None,
                 input_space=None,
                 irange=None,
                 istdev=None,
                 W_lr_scale=None,
                 b_lr_scale=None,
                 max_row_norm=None,
                 max_col_norm=None,
                 sparse_init=None,
                 init_bias_target_marginals=None,
                 nvis=None,
                 seed=None):

        super(SoftmaxRegression, self).__init__(
                layers=[mlp.Softmax(n_classes=n_classes, layer_name='y',
                    irange=irange, istdev=istdev, sparse_init=sparse_init,
                    W_lr_scale=W_lr_scale, b_lr_scale=b_lr_scale,
                    max_row_norm=max_row_norm, max_col_norm=max_col_norm,
                    init_bias_target_marginals=init_bias_target_marginals)],
                batch_size=batch_size,
                input_space=input_space,
                nvis=nvis,
                seed=seed)

########NEW FILE########
__FILENAME__ = sparse_autoencoder
"""
.. todo::

    WRITEME
"""
__authors__ = "Li Yao"

import theano
import theano.sparse
from theano import tensor
from pylearn2.models.autoencoder import DenoisingAutoencoder
from pylearn2.space import VectorSpace
from theano.sparse.sandbox.sp2 import sampling_dot

from pylearn2.expr.basic import theano_norms


class Linear(object):
    """
    .. todo::

        WRITEME
    """

    def __call__(self, X_before_activation):
        """
        .. todo::

            WRITEME
        """
        # X_before_activation is linear inputs of hidden units, dense
        return X_before_activation

class Rectify(object):
    """
    .. todo::

        WRITEME
    """

    def __call__(self, X_before_activation):
        # X_before_activation is linear inputs of hidden units, dense
        return X_before_activation * (X_before_activation > 0)

class SparseDenoisingAutoencoder(DenoisingAutoencoder):
    """
    Denoising autoencoder working with only sparse inputs and efficient
    reconstruction sampling

    Parameters
    ----------
    corruptor : WRITEME
    nvis : WRITEME
    nhid : WRITEME
    act_enc : WRITEME
    act_dec : WRITEME
    tied_weights : WRITEME
    irange : WRITEME
    rng : WRITEME

    References
    ----------
    Y. Dauphin, X. Glorot, Y. Bengio. Large-Scale Learning of Embeddings with
    Reconstruction Sampling. In Proceedings of the 28th International
    Conference on Machine Learning (ICML 2011).
    """

    def __init__(self, corruptor, nvis, nhid, act_enc, act_dec,
                 tied_weights=False, irange=1e-3, rng=9001):
        # sampling dot only supports tied weights
        assert tied_weights == True

        self.names_to_del = set()

        super(SparseDenoisingAutoencoder, self).__init__(corruptor,
                                    nvis, nhid, act_enc, act_dec,
                                    tied_weights=tied_weights, irange=irange, rng=rng)

        # this step is crucial to save loads of space because w_prime is never used in
        # training the sparse da.
        del self.w_prime

        self.input_space = VectorSpace(nvis, sparse=True)

    def get_params(self):
        """
        .. todo::

            WRITEME
        """
        # this is needed because sgd complains when not w_prime is not used in grad
        # so delete w_prime from the params list
        params = super(SparseDenoisingAutoencoder, self).get_params()
        return params[0:3]

    def encode(self, inputs):
        """
        .. todo::

            WRITEME
        """
        if (isinstance(inputs, theano.sparse.basic.SparseVariable)):
            return self._hidden_activation(inputs)
        else:
            raise TypeError
            #return [self.encode(v) for v in inputs]

    def decode(self, hiddens, pattern):
        """
        Map inputs through the encoder function.

        Parameters
        ----------
        hiddens : tensor_like or list of tensor_likes
            Theano symbolic (or list thereof) representing the input
            minibatch(es) to be encoded. Assumed to be 2-tensors, with the
            first dimension indexing training examples and the second
            indexing data dimensions.
        pattern : dense matrix, the same shape of the minibatch inputs
            0/1 like matrix specifying how to reconstruct inputs.

        Returns
        -------
        decoded : tensor_like or list of tensor_like
            Theano symbolic (or list thereof) representing the corresponding
            minibatch(es) after decoding.
        """
        if self.act_dec is None:
            act_dec = lambda x: x
        else:
            act_dec = self.act_dec
            if isinstance(hiddens, tensor.Variable):
                pattern = theano.sparse.csr_from_dense(pattern)
                return act_dec(self.visbias + theano.sparse.dense_from_sparse(sampling_dot(hiddens, self.weights, pattern)))
            else:
                return [self.decode(v, pattern) for v in hiddens]

    def reconstruct(self, inputs, pattern):
        """
        Parameters
        ----------
        inputs : theano sparse variable
        pattern : binary dense matrix
            Specifies which part of inputs should be reconstructed
        """
        # corrupt the inputs
        inputs_dense = theano.sparse.dense_from_sparse(inputs)
        corrupted = self.corruptor(inputs_dense)
        inputs = theano.sparse.csc_from_dense(corrupted)

        return self.decode(self.encode(inputs), pattern)

    def reconstruct_without_dec_acti(self, inputs, pattern):
        """
        .. todo::

            WRITEME
        """
        # return results before applying the decoding activation function
        inputs_dense = theano.sparse.dense_from_sparse(inputs)
        corrupted = self.corruptor(inputs_dense)
        inputs = theano.sparse.csc_from_dense(corrupted)

        hiddens = self.encode(inputs)

        outputs = self.visbias + sampling_dot.sampling_dot(hiddens, self.weights, pattern)

        return outputs

    def _hidden_input(self, x):
        """
        Given a single minibatch, computes the input to the activation
        nonlinearity without applying it.

        Parameters
        ----------
        x : theano sparse variable
            Theano symbolic representing the corrupted input minibatch.

        Returns
        -------
        y : tensor_like
            (Symbolic) input flowing into the hidden layer nonlinearity.
        """

        return self.hidbias + theano.sparse.dot(x, self.weights)

    def get_monitoring_channels(self, V):
        """
        .. todo::

            WRITEME
        """
        vb, hb, weights = self.get_params()
        norms = theano_norms(weights)
        return {'W_min': tensor.min(weights),
                'W_max': tensor.max(weights),
                'W_norm_mean': tensor.mean(norms),
                'bias_hid_min' : tensor.min(hb),
                'bias_hid_mean' : tensor.mean(hb),
                'bias_hid_max' : tensor.max(hb),
                'bias_vis_min' : tensor.min(vb),
                'bias_vis_mean' : tensor.mean(vb),
                'bias_vis_max': tensor.max(vb),
        }

########NEW FILE########
__FILENAME__ = svm
"""Wrappers for SVM models."""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

import numpy as np
import warnings

try:
    from sklearn.multiclass import OneVsRestClassifier
    from sklearn.svm import SVC
except ImportError:
    warnings.warn("Could not import sklearn.")

    class OneVsRestClassifier(object):
        """
        See `sklearn.multiclass.OneVsRestClassifier`.

        Notes
        -----
        This class is a dummy class included so that sphinx
        can import DenseMulticlassSVM and document it even
        when sklearn is not installed.
        """

        def __init__(self, estimator):
            raise RuntimeError("sklearn not available.")

class DenseMulticlassSVM(OneVsRestClassifier):
    """
    sklearn does very different things behind the scenes depending
    upon the exact identity of the class you use. The only way to
    get an SVM implementation that works with dense data is to use
    the `SVC` class, which implements one-against-one
    classification. This wrapper uses it to implement one-against-
    rest classification, which generally works better in my
    experiments.

    To avoid duplicating the training data, use only numpy ndarrays
    whose tags.c_contigous flag is true, and which are in float64
    format.

    Parameters
    ----------
    C : float
        SVM regularization parameter.
        See SVC.__init__ for details.
    kernel : str
        Type of kernel to use.
        See SVC.__init__ for details.
    gamma : float
        Optional parameter of kernel.
        See SVC.__init__ for details.
    coef0 : float
        Optional parameter of kernel.
        See SVC.__init__ for details.
    degree : int
        Degree of kernel, if kernel is polynomial.
        See SVC.__init__ for details.
    """

    def __init__(self, C, kernel='rbf', gamma = 1.0, coef0 = 1.0, degree = 3):
        estimator = SVC(C=C, kernel=kernel, gamma = gamma, coef0 = coef0,
                degree = degree)
        super(DenseMulticlassSVM,self).__init__(estimator)

    def fit(self, X, y):
        """
        .. todo::

            WRITEME
        """
        super(DenseMulticlassSVM,self).fit(X,y)

        return self

    def decision_function(self, X):
        """
        X : ndarray
            A 2D ndarray with each row containing the input features for one
            example.
        """
        return np.concatenate([estimator.decision_function(X) for estimator in
            self.estimators_ ], axis = 1)


########NEW FILE########
__FILENAME__ = test_autoencoder
"""
Tests for the pylearn2 autoencoder module.
"""
import numpy as np
import theano
import theano.tensor as tensor
from theano import config
from pylearn2.models.autoencoder import Autoencoder, HigherOrderContractiveAutoencoder
from pylearn2.corruption import BinomialCorruptor
from pylearn2.config import yaml_parse
from theano.tensor.basic import _allclose

def test_autoencoder_logistic_linear_tied():
    data = np.random.randn(10, 5).astype(config.floatX)
    ae = Autoencoder(5, 7, act_enc='sigmoid', act_dec='linear',
                     tied_weights=True)
    w = ae.weights.get_value()
    ae.hidbias.set_value(np.random.randn(7).astype(config.floatX))
    hb = ae.hidbias.get_value()
    ae.visbias.set_value(np.random.randn(5).astype(config.floatX))
    vb = ae.visbias.get_value()
    d = tensor.matrix()
    result = np.dot(1. / (1 + np.exp(-hb - np.dot(data,  w))), w.T) + vb
    ff = theano.function([d], ae.reconstruct(d))
    assert _allclose(ff(data), result)


def test_autoencoder_tanh_cos_untied():
    data = np.random.randn(10, 5).astype(config.floatX)
    ae = Autoencoder(5, 7, act_enc='tanh', act_dec='cos',
                     tied_weights=False)
    w = ae.weights.get_value()
    w_prime = ae.w_prime.get_value()
    ae.hidbias.set_value(np.random.randn(7).astype(config.floatX))
    hb = ae.hidbias.get_value()
    ae.visbias.set_value(np.random.randn(5).astype(config.floatX))
    vb = ae.visbias.get_value()
    d = tensor.matrix()
    result = np.cos(np.dot(np.tanh(hb + np.dot(data,  w)), w_prime) + vb)
    ff = theano.function([d], ae.reconstruct(d))
    assert _allclose(ff(data), result)


def test_high_order_autoencoder_init():
    """
    Just test that model initialize and return
    the penalty without error.
    """
    corruptor = BinomialCorruptor(corruption_level = 0.5)
    model = HigherOrderContractiveAutoencoder(
            corruptor = corruptor,
            num_corruptions = 2,
            nvis = 5,
            nhid = 7,
            act_enc = 'sigmoid',
            act_dec = 'sigmoid')

    X = tensor.matrix()
    data = np.random.randn(10, 5).astype(config.floatX)
    ff = theano.function([X], model.higher_order_penalty(X))
    assert type(ff(data)) == np.ndarray


def test_cae_basic():
    """
    Tests that we can load a contractive autoencoder
    and train it for a few epochs (without saving) on a dummy
    dataset-- tiny model and dataset
    """

    yaml_string = """
    !obj:pylearn2.train.Train {
        dataset: &train !obj:pylearn2.testing.datasets.random_one_hot_dense_design_matrix {
            rng: !obj:numpy.random.RandomState { seed: [2013, 3, 16] },
            num_examples: 10,
            dim: 5,
            num_classes: 5
        },
        model: !obj:pylearn2.models.autoencoder.ContractiveAutoencoder {
            nvis: 5,
            nhid: 5,
            irange: 0.05,
            act_enc: "sigmoid",
            act_dec: "sigmoid"
        },
        algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {
            batch_size: 10,
            learning_rate: .1,
            monitoring_dataset:
                {
                    'train' : *train
                },
            cost: !obj:pylearn2.costs.cost.SumOfCosts {
            costs: [
                !obj:pylearn2.costs.autoencoder.MeanBinaryCrossEntropy {},
                [0.1, !obj:pylearn2.costs.cost.MethodCost { method: contraction_penalty }]
            ]
        },
           termination_criterion: !obj:pylearn2.termination_criteria.EpochCounter {
                max_epochs: 1,
            },
        },
    }
    """

    train = yaml_parse.load(yaml_string)
    train.main_loop()

def test_hcae_basic():
    """
    Tests that we can load a higher order contractive autoencoder
    and train it for a few epochs (without saving) on a dummy
    dataset-- tiny model and dataset
    """

    yaml_string = """
    !obj:pylearn2.train.Train {
        dataset: &train !obj:pylearn2.testing.datasets.random_one_hot_dense_design_matrix {
            rng: !obj:numpy.random.RandomState { seed: [2013, 3, 16] },
            num_examples: 10,
            dim: 5,
            num_classes: 5
        },
        model: !obj:pylearn2.models.autoencoder.HigherOrderContractiveAutoencoder {
            nvis: 5,
            nhid: 5,
            irange: 0.05,
            act_enc: "sigmoid",
            act_dec: "sigmoid",
            num_corruptions: 2,
            corruptor: !obj:pylearn2.corruption.BinomialCorruptor {
                corruption_level: 0.5
            }
        },
        algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {
            batch_size: 10,
            learning_rate: .1,
            monitoring_dataset:
                {
                    'train' : *train
                },
            cost: !obj:pylearn2.costs.cost.SumOfCosts {
            costs: [
                !obj:pylearn2.costs.autoencoder.MeanBinaryCrossEntropy {},
                [0.1, !obj:pylearn2.costs.cost.MethodCost { method: higher_order_penalty }]
            ]
        },
            termination_criterion: !obj:pylearn2.termination_criteria.EpochCounter {
                max_epochs: 1,
            },
        },
    }
    """

    train = yaml_parse.load(yaml_string)
    train.main_loop()

########NEW FILE########
__FILENAME__ = test_convelemwise_rect
"""
Test for rectifier convolutional layer.
"""

import os

from theano import config
from theano.sandbox import cuda

from pylearn2.config import yaml_parse
import pylearn2


def test_conv_rectifier_basic():
    """
    Tests that we can load a convolutional rectifier model
    and train it for a few epochs (without saving) on a dummy
    dataset-- tiny model and dataset.

    """
    yaml_file = os.path.join(pylearn2.__path__[0],
                             "models/tests/conv_elemwise_rect.yaml")
    with open(yaml_file) as yamlh:
        yaml_lines = yamlh.readlines()
        yaml_str = "".join(yaml_lines)

    train = yaml_parse.load(yaml_str)
    train.main_loop()

if __name__ == "__main__":
    test_conv_rectifier_basic()

########NEW FILE########
__FILENAME__ = test_convelemwise_sigm
"""
Test for convolutional sigmoid layer.
"""

import os

import theano
from theano import config

from pylearn2.config import yaml_parse
import pylearn2

from pylearn2.models.mlp import (MLP, ConvElemwise,
                                 SigmoidConvNonlinearity)

from pylearn2.training_algorithms.sgd import SGD
from pylearn2.termination_criteria import EpochCounter
import numpy as np


def test_conv_sigmoid_basic():
    """
    Tests that we can load a convolutional sigmoid model
    and train it for a few epochs (without saving) on a dummy
    dataset-- tiny model and dataset
    """
    yaml_file = os.path.join(pylearn2.__path__[0],
                             "models/tests/conv_elemwise_sigm.yaml")
    with open(yaml_file) as yamlh:
        yaml_lines = yamlh.readlines()
        yaml_str = "".join(yaml_lines)

    train = yaml_parse.load(yaml_str)
    train.main_loop()


def test_sigmoid_detection_cost():
    """
    Tests whether the sigmoid convolutional layer returns the right value.
    """

    rng = np.random.RandomState(0)
    sigmoid_nonlin = SigmoidConvNonlinearity(monitor_style="detection")
    (rows, cols) = (10, 10)
    axes = ('c', 0, 1, 'b')
    nchs = 1

    space_shp = (nchs, rows, cols, 1)
    X_vals = np.random.uniform(-0.01, 0.01,
                               size=space_shp).astype(config.floatX)
    X = theano.shared(X_vals, name="X")

    Y_vals = (np.random.uniform(-0.01, 0.01,
                                size=(rows, cols)) > 0.005).astype("uint8")
    Y = theano.shared(Y_vals, name="y_vals")

    conv_elemwise = ConvElemwise(layer_name="h0",
                                 output_channels=1,
                                 irange=.005,
                                 kernel_shape=(1, 1),
                                 max_kernel_norm=0.9,
                                 nonlinearity=sigmoid_nonlin)

    input_space = pylearn2.space.Conv2DSpace(shape=(rows, cols),
                                             num_channels=nchs,
                                             axes=axes)
    model = MLP(batch_size=1,
                layers=[conv_elemwise],
                input_space=input_space)
    Y_hat = model.fprop(X)
    cost = model.cost(Y, Y_hat).eval()

    assert not(np.isnan(cost) or np.isinf(cost) or (cost < 0.0)
               or (cost is None)), ("cost returns illegal "
                                    "value.")

def test_conv_pooling_nonlin():
    """
    Tests whether the nonlinearity is applied before the pooling.
    """

    rng = np.random.RandomState(0)
    sigm_nonlin = SigmoidConvNonlinearity(monitor_style="detection")
    (rows, cols) = (5, 5)
    axes = ('c', 0, 1, 'b')
    nchs = 1

    space_shp = (nchs, rows, cols, 1)
    X_vals = np.random.uniform(-0.01, 0.01,
                               size=space_shp).astype(config.floatX)
    X = theano.shared(X_vals, name="X")

    conv_elemwise = ConvElemwise(layer_name="h0",
                                 output_channels=1,
                                 pool_type="max",
                                 irange=.005,
                                 kernel_shape=(1, 1),
                                 pool_shape=(1, 1),
                                 pool_stride=(1, 1),
                                 nonlinearity=sigm_nonlin)

    input_space = pylearn2.space.Conv2DSpace(shape=(rows, cols),
                                             num_channels=nchs,
                                             axes=axes)
    model = MLP(batch_size=1,
                layers=[conv_elemwise],
                input_space=input_space)

    Y_hat = model.fprop(X)
    assert "max" in str(Y_hat.name)
    ancestors = theano.gof.graph.ancestors([Y_hat])
    lcond = ["sigm" in str(anc.owner) for anc in ancestors]
    assert np.array(lcond).nonzero()[0].shape[0] > 0, ("Nonlinearity should be "
                                                       "applied before pooling.")


if __name__ == "__main__":
    test_conv_sigmoid_basic()
    test_sigmoid_detection_cost()
    test_conv_pooling_nonlin()

########NEW FILE########
__FILENAME__ = test_convelemwise_tanh
"""
Test for Tanh convolutional layer.
"""

import os

from theano import config
from theano.sandbox import cuda

from pylearn2.config import yaml_parse
import pylearn2


def test_conv_tanh_basic():
    """
    Tests that we can load a convolutional tanh model
    and train it for a few epochs (without saving) on a dummy
    dataset-- tiny model and dataset
    """
    yaml_file = os.path.join(pylearn2.__path__[0],
                             "models/tests/conv_elemwise_tanh.yaml")

    with open(yaml_file) as yamlh:
        yaml_lines = yamlh.readlines()
        yaml_str = "".join(yaml_lines)

    train = yaml_parse.load(yaml_str)
    train.main_loop()

if __name__ == "__main__":
    test_conv_tanh_basic()

########NEW FILE########
__FILENAME__ = test_dbm
from pylearn2.models.dbm.dbm import DBM
from pylearn2.models.dbm.layer import BinaryVector, BinaryVectorMaxPool, Softmax, GaussianVisLayer

__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"

import numpy as np
import random
assert hasattr(np, 'exp')

from theano import config
from theano import function
from theano.sandbox.rng_mrg import MRG_RandomStreams
from theano import tensor as T

from pylearn2.expr.basic import is_binary
from pylearn2.expr.nnet import inverse_sigmoid_numpy
from pylearn2.costs.dbm import VariationalCD
import pylearn2.testing.datasets as datasets
from pylearn2.space import VectorSpace
from pylearn2.utils import sharedX
from pylearn2.utils import safe_zip
from pylearn2.utils.data_specs import DataSpecsMapping

def check_binary_samples(value, expected_shape, expected_mean, tol):
    """
    Tests that a matrix of binary samples (observations in rows, variables
        in columns)
    1) Has the right shape
    2) Is binary
    3) Converges to the right mean
    """
    assert value.shape == expected_shape
    assert is_binary(value)
    mean = value.mean(axis=0)
    max_error = np.abs(mean-expected_mean).max()
    print 'Actual mean:'
    print mean
    print 'Expected mean:'
    print expected_mean
    print 'Maximal error:', max_error
    if max_error > tol:
        raise ValueError("Samples don't seem to have the right mean.")

def test_binary_vis_layer_make_state():

    # Verifies that BinaryVector.make_state creates
    # a shared variable whose value passes check_binary_samples

    n = 5
    num_samples = 1000
    tol = .04

    layer = BinaryVector(nvis = n)

    rng = np.random.RandomState([2012,11,1])

    mean = rng.uniform(1e-6, 1. - 1e-6, (n,))

    z = inverse_sigmoid_numpy(mean)

    layer.set_biases(z.astype(config.floatX))

    init_state = layer.make_state(num_examples=num_samples,
            numpy_rng=rng)

    value = init_state.get_value()

    check_binary_samples(value, (num_samples, n), mean, tol)

def test_binary_vis_layer_sample():

    # Verifies that BinaryVector.sample returns an expression
    # whose value passes check_binary_samples

    assert hasattr(np, 'exp')

    n = 5
    num_samples = 1000
    tol = .04

    class DummyLayer(object):
        """
        A layer that we build for the test that just uses a state
        as its downward message.
        """

        def downward_state(self, state):
            return state

        def downward_message(self, state):
            return state

    vis = BinaryVector(nvis=n)
    hid = DummyLayer()

    rng = np.random.RandomState([2012,11,1,259])

    mean = rng.uniform(1e-6, 1. - 1e-6, (n,))

    ofs = rng.randn(n)

    vis.set_biases(ofs.astype(config.floatX))

    z = inverse_sigmoid_numpy(mean) - ofs

    z_var = sharedX(np.zeros((num_samples, n)) + z)

    theano_rng = MRG_RandomStreams(2012+11+1)

    sample = vis.sample(state_above=z_var, layer_above=hid,
            theano_rng=theano_rng)

    sample = sample.eval()

    check_binary_samples(sample, (num_samples, n), mean, tol)


def check_gaussian_samples(value, nsamples, nvis, rows, cols, channels, expected_mean, tol):
    """
    Tests that a matrix of Gaussian samples (observations in rows, variables
     in columns)
    1) Has the right shape
    2) Is not binary
    3) Converges to the right mean

    """
    if nvis:
        expected_shape = (nsamples, nvis)
    else:
        expected_shape = (nsamples,rows,cols,channels)
    assert value.shape == expected_shape
    assert not is_binary(value)
    mean = value.mean(axis=0)
    max_error = np.abs(mean-expected_mean).max()
    print 'Actual mean:'
    print mean
    print 'Expected mean:'
    print expected_mean
    print 'Maximal error:', max_error
    print 'Tolerable variance:', tol
    if max_error > tol:
        raise ValueError("Samples don't seem to have the right mean.")
    else:
        print 'Mean is within expected range'


def test_gaussian_vis_layer_make_state():
    """
    Verifies that GaussianVisLayer.make_state creates
    a shared variable whose value passes check_gaussian_samples

    In this case the layer lives in a VectorSpace

    """
    n = 5
    rows = None
    cols = None
    channels = None
    num_samples = 1000
    tol = .042 # tolerated variance
    beta = 1/tol # precision parameter

    layer = GaussianVisLayer(nvis = n, init_beta=beta)

    rng = np.random.RandomState([2012,11,1])

    mean = rng.uniform(1e-6, 1. - 1e-6, (n,))

    z= mean

    layer.set_biases(z.astype(config.floatX))

    init_state = layer.make_state(num_examples=num_samples,
            numpy_rng=rng)

    value = init_state.get_value()

    check_gaussian_samples(value, num_samples, n, rows, cols, channels, mean, tol)

def test_gaussian_vis_layer_make_state_conv():
    """
    Verifies that GaussianVisLayer.make_state creates
    a shared variable whose value passes check_gaussian_samples

    In this case the layer lives in a Conv2DSpace

    """
    n = None
    rows = 3
    cols = 3
    channels = 3
    num_samples = 1000
    tol = .042  # tolerated variance
    beta = 1/tol  # precision parameter
    # axes for batch, rows, cols, channels, can be given in any order
    axes = ['b', 0, 1, 'c']
    random.shuffle(axes)
    axes = tuple(axes)
    print 'axes:', axes

    layer = GaussianVisLayer(rows=rows, cols=cols, channels=channels, init_beta=beta, axes=axes)

    # rng = np.random.RandomState([2012,11,1])
    rng = np.random.RandomState()
    mean = rng.uniform(1e-6, 1. - 1e-6, (rows, cols, channels))

    #z = inverse_sigmoid_numpy(mean)
    z= mean

    layer.set_biases(z.astype(config.floatX))

    init_state = layer.make_state(num_examples=num_samples,
            numpy_rng=rng)

    value = init_state.get_value()

    check_gaussian_samples(value, num_samples, n, rows, cols, channels, mean, tol)

def test_gaussian_vis_layer_sample():
    """
    Verifies that GaussianVisLayer.sample returns an expression
    whose value passes check_gaussian_samples

    In this case the layer lives in a VectorSpace

    """
    assert hasattr(np, 'exp')

    n = 5
    num_samples = 1000
    tol = .042  # tolerated variance
    beta = 1/tol  # precision parameter
    rows = None
    cols = None
    channels = None

    class DummyLayer(object):
        """
        A layer that we build for the test that just uses a state
        as its downward message.
        """

        def downward_state(self, state):
            return state

        def downward_message(self, state):
            return state

    vis = GaussianVisLayer(nvis=n, init_beta=beta)
    hid = DummyLayer()

    rng = np.random.RandomState([2012,11,1,259])

    mean = rng.uniform(1e-6, 1. - 1e-6, (n,))

    ofs = rng.randn(n)

    vis.set_biases(ofs.astype(config.floatX))

    #z = inverse_sigmoid_numpy(mean) - ofs
    z=mean -ofs # linear activation function
    z_var = sharedX(np.zeros((num_samples, n)) + z)
    # mean will be z_var + mu

    theano_rng = MRG_RandomStreams(2012+11+1)

    sample = vis.sample(state_above=z_var, layer_above=hid,
            theano_rng=theano_rng)

    sample = sample.eval()

    check_gaussian_samples(sample, num_samples, n, rows, cols, channels, mean, tol)

def test_gaussian_vis_layer_sample_conv():
    """
    Verifies that GaussianVisLayer.sample returns an expression
    whose value passes check_gaussian_samples.

    In this case the layer lives in a Conv2DSpace

    """
    assert hasattr(np, 'exp')

    n = None
    num_samples = 1000
    tol = .042  # tolerated variance
    beta = 1/tol  # precision parameter
    rows = 3
    cols = 3
    channels = 3
    # axes for batch, rows, cols, channels, can be given in any order
    axes = ['b', 0, 1, 'c']
    random.shuffle(axes)
    axes = tuple(axes)
    print 'axes:', axes

    class DummyLayer(object):
        """
        A layer that we build for the test that just uses a state
        as its downward message.
        """

        def downward_state(self, state):
            return state

        def downward_message(self, state):
            return state

    vis = GaussianVisLayer(nvis=None,rows=rows, cols=cols, channels=channels, init_beta=beta, axes=axes)
    hid = DummyLayer()

    rng = np.random.RandomState([2012,11,1,259])

    mean = rng.uniform(1e-6, 1. - 1e-6, (rows, cols, channels))

    ofs = rng.randn(rows,cols,channels)

    vis.set_biases(ofs.astype(config.floatX))

    #z = inverse_sigmoid_numpy(mean) - ofs
    z = mean -ofs

    z_var = sharedX(np.zeros((num_samples, rows, cols, channels)) + z)

    theano_rng = MRG_RandomStreams(2012+11+1)

    sample = vis.sample(state_above=z_var, layer_above=hid,
            theano_rng=theano_rng)

    sample = sample.eval()

    check_gaussian_samples(sample, num_samples, n, rows, cols, channels, mean, tol)

def check_bvmp_samples(value, num_samples, n, pool_size, mean, tol):
    """
    bvmp=BinaryVectorMaxPool
    value: a tuple giving (pooled batch, detector batch)   (all made with same params)
    num_samples: number of samples there should be in the batch
    n: detector layer dimension
    pool_size: size of each pool region
    mean: (expected value of pool unit, expected value of detector units)
    tol: amount the emprical mean is allowed to deviate from the analytical expectation

    checks that:
        1) all values are binary
        2) detector layer units are mutually exclusive
        3) pooled unit is max of the detector units
        4) correct number of samples is present
        5) variables are of the right shapes
        6) samples converge to the right expected value
    """

    pv, hv = value

    assert n % pool_size == 0
    num_pools = n // pool_size

    assert pv.ndim == 2
    assert pv.shape[0] == num_samples
    assert pv.shape[1] == num_pools

    assert hv.ndim == 2
    assert hv.shape[0] == num_samples
    assert hv.shape[1] == n

    assert is_binary(pv)
    assert is_binary(hv)

    for i in xrange(num_pools):
        sub_p = pv[:,i]
        assert sub_p.shape == (num_samples,)
        sub_h = hv[:,i*pool_size:(i+1)*pool_size]
        assert sub_h.shape == (num_samples, pool_size)
        if not np.all(sub_p == sub_h.max(axis=1)):
            for j in xrange(num_samples):
                print sub_p[j], sub_h[j,:]
                assert sub_p[j] == sub_h[j,:]
            assert False
        assert np.max(sub_h.sum(axis=1)) == 1

    p, h = mean
    assert p.ndim == 1
    assert h.ndim == 1
    emp_p = pv.mean(axis=0)
    emp_h = hv.mean(axis=0)

    max_diff = np.abs(p - emp_p).max()
    if max_diff > tol:
        print 'expected value of pooling units: ',p
        print 'empirical expectation: ',emp_p
        print 'maximum difference: ',max_diff
        raise ValueError("Pooling unit samples have an unlikely mean.")
    max_diff = np.abs(h - emp_h).max()
    if max_diff > tol:
        assert False

def test_bvmp_make_state():

    # Verifies that BinaryVector.make_state creates
    # a shared variable whose value passes check_binary_samples

    num_pools = 3
    num_samples = 1000
    tol = .04
    rng = np.random.RandomState([2012,11,1,9])
    # pool_size=1 is an important corner case
    for pool_size in [1, 2, 5]:
        n = num_pools * pool_size

        layer = BinaryVectorMaxPool(
                detector_layer_dim=n,
                layer_name='h',
                irange=1.,
                pool_size=pool_size)

        # This is just to placate mf_update below
        input_space = VectorSpace(1)
        class DummyDBM(object):
            def __init__(self):
                self.rng = rng
        layer.set_dbm(DummyDBM())
        layer.set_input_space(input_space)

        layer.set_biases(rng.uniform(-pool_size, 1., (n,)).astype(config.floatX))

        # To find the mean of the samples, we use mean field with an input of 0
        mean = layer.mf_update(
                state_below=T.alloc(0., 1, 1),
                state_above=None,
                layer_above=None)

        mean = function([], mean)()

        mean = [ mn[0,:] for mn in mean ]

        state = layer.make_state(num_examples=num_samples,
                numpy_rng=rng)

        value = [elem.get_value() for elem in state]

        check_bvmp_samples(value, num_samples, n, pool_size, mean, tol)


def make_random_basic_binary_dbm(
        rng,
        pool_size_1,
        num_vis = None,
        num_pool_1 = None,
        num_pool_2 = None,
        pool_size_2 = None,
        center = False
        ):
    """
    Makes a DBM with BinaryVector for the visible layer,
    and two hidden layers of type BinaryVectorMaxPool.
    The weights and biases are initialized randomly with
    somewhat large values (i.e., not what you'd want to
    use for learning)

    rng: A numpy RandomState.
    pool_size_1: The size of the pools to use in the first
                 layer.
    """

    if num_vis is None:
        num_vis = rng.randint(1,11)
    if num_pool_1 is None:
        num_pool_1 = rng.randint(1,11)
    if num_pool_2 is None:
        num_pool_2 = rng.randint(1,11)
    if pool_size_2 is None:
        pool_size_2 = rng.randint(1,6)

    num_h1 = num_pool_1 * pool_size_1
    num_h2 = num_pool_2 * pool_size_2

    v = BinaryVector(num_vis, center=center)
    v.set_biases(rng.uniform(-1., 1., (num_vis,)).astype(config.floatX), recenter=center)

    h1 = BinaryVectorMaxPool(
            detector_layer_dim = num_h1,
            pool_size = pool_size_1,
            layer_name = 'h1',
            center = center,
            irange = 1.)
    h1.set_biases(rng.uniform(-1., 1., (num_h1,)).astype(config.floatX), recenter=center)

    h2 = BinaryVectorMaxPool(
            center = center,
            detector_layer_dim = num_h2,
            pool_size = pool_size_2,
            layer_name = 'h2',
            irange = 1.)
    h2.set_biases(rng.uniform(-1., 1., (num_h2,)).astype(config.floatX), recenter=center)

    dbm = DBM(visible_layer = v,
            hidden_layers = [h1, h2],
            batch_size = 1,
            niter = 50)

    return dbm


def test_bvmp_mf_energy_consistent():

    # A test of the BinaryVectorMaxPool class
    # Verifies that the mean field update is consistent with
    # the energy function

    # Specifically, in a DBM consisting of (v, h1, h2), the
    # lack of intra-layer connections means that
    # P(h1|v, h2) is factorial so mf_update tells us the true
    # conditional.
    # We also know P(h1[i] | h1[-i], v)
    #  = P(h, v) / P(h[-i], v)
    #  = P(h, v) / sum_h[i] P(h, v)
    #  = exp(-E(h, v)) / sum_h[i] exp(-E(h, v))
    # So we can check that computing P(h[i] | v) with both
    # methods works the same way

    rng = np.random.RandomState([2012,11,1,613])

    def do_test(pool_size_1):

        # Make DBM and read out its pieces
        dbm = make_random_basic_binary_dbm(
                rng = rng,
                pool_size_1 = pool_size_1,
                )

        v = dbm.visible_layer
        h1, h2 = dbm.hidden_layers

        num_p = h1.get_output_space().dim

        # Choose which unit we will test
        p_idx = rng.randint(num_p)

        # Randomly pick a v, h1[-p_idx], and h2 to condition on
        # (Random numbers are generated via dbm.rng)
        layer_to_state = dbm.make_layer_to_state(1)
        v_state = layer_to_state[v]
        h1_state = layer_to_state[h1]
        h2_state = layer_to_state[h2]

        # Debugging checks
        num_h = h1.detector_layer_dim
        assert num_p * pool_size_1 == num_h
        pv, hv = h1_state
        assert pv.get_value().shape == (1, num_p)
        assert hv.get_value().shape == (1, num_h)

        # Infer P(h1[i] | h2, v) using mean field
        expected_p, expected_h = h1.mf_update(
                state_below = v.upward_state(v_state),
                state_above = h2.downward_state(h2_state),
                layer_above = h2)

        expected_p = expected_p[0, p_idx]
        expected_h = expected_h[0, p_idx * pool_size : (p_idx + 1) * pool_size]

        expected_p, expected_h = function([], [expected_p, expected_h])()

        # Infer P(h1[i] | h2, v) using the energy function
        energy = dbm.energy(V = v_state,
                hidden = [h1_state, h2_state])
        unnormalized_prob = T.exp(-energy)
        assert unnormalized_prob.ndim == 1
        unnormalized_prob = unnormalized_prob[0]
        unnormalized_prob = function([], unnormalized_prob)

        p_state, h_state = h1_state

        def compute_unnormalized_prob(which_detector):
            write_h = np.zeros((pool_size_1,))
            if which_detector is None:
                write_p = 0.
            else:
                write_p = 1.
                write_h[which_detector] = 1.

            h_value = h_state.get_value()
            p_value = p_state.get_value()

            h_value[0, p_idx * pool_size : (p_idx + 1) * pool_size] = write_h
            p_value[0, p_idx] = write_p

            h_state.set_value(h_value)
            p_state.set_value(p_value)

            return unnormalized_prob()

        off_prob = compute_unnormalized_prob(None)
        on_probs = [compute_unnormalized_prob(idx) for idx in xrange(pool_size)]
        denom = off_prob + sum(on_probs)
        off_prob /= denom
        on_probs = [on_prob / denom for on_prob in on_probs]
        assert np.allclose(1., off_prob + sum(on_probs))

        # np.asarray(on_probs) doesn't make a numpy vector, so I do it manually
        wtf_numpy = np.zeros((pool_size_1,))
        for i in xrange(pool_size_1):
            wtf_numpy[i] = on_probs[i]
        on_probs = wtf_numpy

        # Check that they match
        if not np.allclose(expected_p, 1. - off_prob):
            print 'mean field expectation of p:',expected_p
            print 'expectation of p based on enumerating energy function values:',1. - off_prob
            print 'pool_size_1:',pool_size_1

            assert False
        if not np.allclose(expected_h, on_probs):
            print 'mean field expectation of h:',expected_h
            print 'expectation of h based on enumerating energy function values:',on_probs
            assert False

    # 1 is an important corner case
    # We must also run with a larger number to test the general case
    for pool_size in [1, 2, 5]:
        do_test(pool_size)


def test_bvmp_mf_energy_consistent_center():
    """
    A test of the BinaryVectorMaxPool class
    Verifies that the mean field update is consistent with
    the energy function when using Gregoire Montavon's centering
    trick.

    Specifically, in a DBM consisting of (v, h1, h2), the
    lack of intra-layer connections means that
    P(h1|v, h2) is factorial so mf_update tells us the true
    conditional.
    We also know P(h1[i] | h1[-i], v)
    = P(h, v) / P(h[-i], v)
    = P(h, v) / sum_h[i] P(h, v)
    = exp(-E(h, v)) / sum_h[i] exp(-E(h, v))
    So we can check that computing P(h[i] | v) with both
    methods works the same way

    :return:
    """
    rng = np.random.RandomState([2012,11,1,613])

    def do_test(pool_size_1):

        # Make DBM and read out its pieces
        dbm = make_random_basic_binary_dbm(
                rng = rng,
                pool_size_1 = pool_size_1,
                pool_size_2 = 1, # centering is only updated for pool size 1
                center = True
                )

        v = dbm.visible_layer
        h1, h2 = dbm.hidden_layers

        num_p = h1.get_output_space().dim

        # Choose which unit we will test
        p_idx = rng.randint(num_p)

        # Randomly pick a v, h1[-p_idx], and h2 to condition on
        # (Random numbers are generated via dbm.rng)
        layer_to_state = dbm.make_layer_to_state(1)
        v_state = layer_to_state[v]
        h1_state = layer_to_state[h1]
        h2_state = layer_to_state[h2]

        # Debugging checks
        num_h = h1.detector_layer_dim
        assert num_p * pool_size_1 == num_h
        pv, hv = h1_state
        assert pv.get_value().shape == (1, num_p)
        assert hv.get_value().shape == (1, num_h)

        # Infer P(h1[i] | h2, v) using mean field
        expected_p, expected_h = h1.mf_update(
                state_below = v.upward_state(v_state),
                state_above = h2.downward_state(h2_state),
                layer_above = h2)

        expected_p = expected_p[0, p_idx]
        expected_h = expected_h[0, p_idx * pool_size_1 : (p_idx + 1) * pool_size_1]

        expected_p, expected_h = function([], [expected_p, expected_h])()

        # Infer P(h1[i] | h2, v) using the energy function
        energy = dbm.energy(V = v_state,
                hidden = [h1_state, h2_state])
        unnormalized_prob = T.exp(-energy)
        assert unnormalized_prob.ndim == 1
        unnormalized_prob = unnormalized_prob[0]
        unnormalized_prob = function([], unnormalized_prob)

        p_state, h_state = h1_state

        def compute_unnormalized_prob(which_detector):
            write_h = np.zeros((pool_size_1,))
            if which_detector is None:
                write_p = 0.
            else:
                write_p = 1.
                write_h[which_detector] = 1.

            h_value = h_state.get_value()
            p_value = p_state.get_value()

            h_value[0, p_idx * pool_size_1 : (p_idx + 1) * pool_size_1] = write_h
            p_value[0, p_idx] = write_p

            h_state.set_value(h_value)
            p_state.set_value(p_value)

            return unnormalized_prob()

        off_prob = compute_unnormalized_prob(None)
        on_probs = [compute_unnormalized_prob(idx) for idx in xrange(pool_size_1)]
        denom = off_prob + sum(on_probs)
        off_prob /= denom
        on_probs = [on_prob / denom for on_prob in on_probs]
        assert np.allclose(1., off_prob + sum(on_probs))

        # np.asarray(on_probs) doesn't make a numpy vector, so I do it manually
        wtf_numpy = np.zeros((pool_size_1,))
        for i in xrange(pool_size_1):
            wtf_numpy[i] = on_probs[i]
        on_probs = wtf_numpy

        # Check that they match
        if not np.allclose(expected_p, 1. - off_prob):
            print 'mean field expectation of p:',expected_p
            print 'expectation of p based on enumerating energy function values:',1. - off_prob
            print 'pool_size_1:',pool_size_1

            assert False
        if not np.allclose(expected_h, on_probs):
            print 'mean field expectation of h:',expected_h
            print 'expectation of h based on enumerating energy function values:',on_probs
            assert False

    # 1 is the only pool size for which centering is implemented
    do_test(1)

def test_bvmp_mf_sample_consistent():

    # A test of the BinaryVectorMaxPool class
    # Verifies that the mean field update is consistent with
    # the sampling function

    # Specifically, in a DBM consisting of (v, h1, h2), the
    # lack of intra-layer connections means that
    # P(h1|v, h2) is factorial so mf_update tells us the true
    # conditional.
    # We can thus use mf_update to compute the expected value
    # of a sample of h1 from v and h2, and check that samples
    # drawn using the layer's sample method convert to that
    # value.

    rng = np.random.RandomState([2012,11,1,1016])
    theano_rng = MRG_RandomStreams(2012+11+1+1036)
    num_samples = 1000
    tol = .042

    def do_test(pool_size_1):

        # Make DBM and read out its pieces
        dbm = make_random_basic_binary_dbm(
                rng = rng,
                pool_size_1 = pool_size_1,
                )

        v = dbm.visible_layer
        h1, h2 = dbm.hidden_layers

        num_p = h1.get_output_space().dim

        # Choose which unit we will test
        p_idx = rng.randint(num_p)

        # Randomly pick a v, h1[-p_idx], and h2 to condition on
        # (Random numbers are generated via dbm.rng)
        layer_to_state = dbm.make_layer_to_state(1)
        v_state = layer_to_state[v]
        h1_state = layer_to_state[h1]
        h2_state = layer_to_state[h2]

        # Debugging checks
        num_h = h1.detector_layer_dim
        assert num_p * pool_size_1 == num_h
        pv, hv = h1_state
        assert pv.get_value().shape == (1, num_p)
        assert hv.get_value().shape == (1, num_h)

        # Infer P(h1[i] | h2, v) using mean field
        expected_p, expected_h = h1.mf_update(
                state_below = v.upward_state(v_state),
                state_above = h2.downward_state(h2_state),
                layer_above = h2)

        expected_p = expected_p[0, :]
        expected_h = expected_h[0, :]

        expected_p, expected_h = function([], [expected_p, expected_h])()

        # copy all the states out into a batch size of num_samples
        cause_copy = sharedX(np.zeros((num_samples,))).dimshuffle(0,'x')
        v_state = v_state[0,:] + cause_copy
        p, h = h1_state
        h1_state = (p[0,:] + cause_copy, h[0,:] + cause_copy)
        p, h = h2_state
        h2_state = (p[0,:] + cause_copy, h[0,:] + cause_copy)

        h1_samples = h1.sample(state_below = v.upward_state(v_state),
                            state_above = h2.downward_state(h2_state),
                            layer_above = h2, theano_rng = theano_rng)

        h1_samples = function([], h1_samples)()


        check_bvmp_samples(h1_samples, num_samples, num_h, pool_size, (expected_p, expected_h), tol)


    # 1 is an important corner case
    # We must also run with a larger number to test the general case
    for pool_size in [1, 2, 5]:
        do_test(pool_size)

def check_multinomial_samples(value, expected_shape, expected_mean, tol):
    """
    Tests that a matrix of multinomial samples (observations in rows, variables
        in columns)
    1) Has the right shape
    2) Is binary
    3) Has one 1 per row
    4) Converges to the right mean
    """
    assert value.shape == expected_shape
    assert is_binary(value)
    assert np.all(value.sum(axis=1) == 1)
    mean = value.mean(axis=0)
    max_error = np.abs(mean-expected_mean).max()
    if max_error > tol:
        print 'Actual mean:'
        print mean
        print 'Expected mean:'
        print expected_mean
        print 'Maximal error:', max_error
        raise ValueError("Samples don't seem to have the right mean.")

def test_softmax_make_state():

    # Verifies that BinaryVector.make_state creates
    # a shared variable whose value passes check_multinomial_samples

    n = 5
    num_samples = 1000
    tol = .04

    layer = Softmax(n_classes = n, layer_name = 'y')

    rng = np.random.RandomState([2012, 11, 1, 11])

    z = 3 * rng.randn(n)

    mean = np.exp(z)
    mean /= mean.sum()

    layer.set_biases(z.astype(config.floatX))

    state = layer.make_state(num_examples=num_samples,
            numpy_rng=rng)

    value = state.get_value()

    check_multinomial_samples(value, (num_samples, n), mean, tol)

def test_softmax_mf_energy_consistent():

    # A test of the Softmax class
    # Verifies that the mean field update is consistent with
    # the energy function

    # Since a Softmax layer contains only one random variable
    # (with n_classes possible values) the mean field assumption
    # does not impose any restriction so mf_update simply gives
    # the true expected value of h given v.
    # We also know P(h |  v)
    #  = P(h, v) / P( v)
    #  = P(h, v) / sum_h P(h, v)
    #  = exp(-E(h, v)) / sum_h exp(-E(h, v))
    # So we can check that computing P(h | v) with both
    # methods works the same way

    rng = np.random.RandomState([2012,11,1,1131])

    # Make DBM
    num_vis = rng.randint(1,11)
    n_classes = rng.randint(1, 11)

    v = BinaryVector(num_vis)
    v.set_biases(rng.uniform(-1., 1., (num_vis,)).astype(config.floatX))

    y = Softmax(
            n_classes = n_classes,
            layer_name = 'y',
            irange = 1.)
    y.set_biases(rng.uniform(-1., 1., (n_classes,)).astype(config.floatX))

    dbm = DBM(visible_layer = v,
            hidden_layers = [y],
            batch_size = 1,
            niter = 50)

    # Randomly pick a v to condition on
    # (Random numbers are generated via dbm.rng)
    layer_to_state = dbm.make_layer_to_state(1)
    v_state = layer_to_state[v]
    y_state = layer_to_state[y]

    # Infer P(y | v) using mean field
    expected_y = y.mf_update(
            state_below = v.upward_state(v_state))

    expected_y = expected_y[0, :]

    expected_y = expected_y.eval()

    # Infer P(y | v) using the energy function
    energy = dbm.energy(V = v_state,
            hidden = [y_state])
    unnormalized_prob = T.exp(-energy)
    assert unnormalized_prob.ndim == 1
    unnormalized_prob = unnormalized_prob[0]
    unnormalized_prob = function([], unnormalized_prob)

    def compute_unnormalized_prob(which):
        write_y = np.zeros((n_classes,))
        write_y[which] = 1.

        y_value = y_state.get_value()

        y_value[0, :] = write_y

        y_state.set_value(y_value)

        return unnormalized_prob()

    probs = [compute_unnormalized_prob(idx) for idx in xrange(n_classes)]
    denom = sum(probs)
    probs = [on_prob / denom for on_prob in probs]

    # np.asarray(probs) doesn't make a numpy vector, so I do it manually
    wtf_numpy = np.zeros((n_classes,))
    for i in xrange(n_classes):
        wtf_numpy[i] = probs[i]
    probs = wtf_numpy

    if not np.allclose(expected_y, probs):
        print 'mean field expectation of h:',expected_y
        print 'expectation of h based on enumerating energy function values:',probs
        assert False

def test_softmax_mf_energy_consistent_centering():

    # A test of the Softmax class
    # Verifies that the mean field update is consistent with
    # the energy function when using the centering trick

    # Since a Softmax layer contains only one random variable
    # (with n_classes possible values) the mean field assumption
    # does not impose any restriction so mf_update simply gives
    # the true expected value of h given v.
    # We also know P(h |  v)
    #  = P(h, v) / P( v)
    #  = P(h, v) / sum_h P(h, v)
    #  = exp(-E(h, v)) / sum_h exp(-E(h, v))
    # So we can check that computing P(h | v) with both
    # methods works the same way

    rng = np.random.RandomState([2012,11,1,1131])

    # Make DBM
    num_vis = rng.randint(1,11)
    n_classes = rng.randint(1, 11)

    v = BinaryVector(num_vis, center=True)
    v.set_biases(rng.uniform(-1., 1., (num_vis,)).astype(config.floatX), recenter=True)

    y = Softmax(
            n_classes = n_classes,
            layer_name = 'y',
            irange = 1., center=True)
    y.set_biases(rng.uniform(-1., 1., (n_classes,)).astype(config.floatX), recenter=True)

    dbm = DBM(visible_layer = v,
            hidden_layers = [y],
            batch_size = 1,
            niter = 50)

    # Randomly pick a v to condition on
    # (Random numbers are generated via dbm.rng)
    layer_to_state = dbm.make_layer_to_state(1)
    v_state = layer_to_state[v]
    y_state = layer_to_state[y]

    # Infer P(y | v) using mean field
    expected_y = y.mf_update(
            state_below = v.upward_state(v_state))

    expected_y = expected_y[0, :]

    expected_y = expected_y.eval()

    # Infer P(y | v) using the energy function
    energy = dbm.energy(V = v_state,
            hidden = [y_state])
    unnormalized_prob = T.exp(-energy)
    assert unnormalized_prob.ndim == 1
    unnormalized_prob = unnormalized_prob[0]
    unnormalized_prob = function([], unnormalized_prob)

    def compute_unnormalized_prob(which):
        write_y = np.zeros((n_classes,))
        write_y[which] = 1.

        y_value = y_state.get_value()

        y_value[0, :] = write_y

        y_state.set_value(y_value)

        return unnormalized_prob()

    probs = [compute_unnormalized_prob(idx) for idx in xrange(n_classes)]
    denom = sum(probs)
    probs = [on_prob / denom for on_prob in probs]

    # np.asarray(probs) doesn't make a numpy vector, so I do it manually
    wtf_numpy = np.zeros((n_classes,))
    for i in xrange(n_classes):
        wtf_numpy[i] = probs[i]
    probs = wtf_numpy

    if not np.allclose(expected_y, probs):
        print 'mean field expectation of h:',expected_y
        print 'expectation of h based on enumerating energy function values:',probs
        assert False

def test_softmax_mf_sample_consistent():

    # A test of the Softmax class
    # Verifies that the mean field update is consistent with
    # the sampling function

    # Since a Softmax layer contains only one random variable
    # (with n_classes possible values) the mean field assumption
    # does not impose any restriction so mf_update simply gives
    # the true expected value of h given v.
    # We can thus use mf_update to compute the expected value
    # of a sample of y conditioned on v, and check that samples
    # drawn using the layer's sample method convert to that
    # value.

    rng = np.random.RandomState([2012,11,1,1154])
    theano_rng = MRG_RandomStreams(2012+11+1+1154)
    num_samples = 1000
    tol = .042

    # Make DBM
    num_vis = rng.randint(1,11)
    n_classes = rng.randint(1, 11)

    v = BinaryVector(num_vis)
    v.set_biases(rng.uniform(-1., 1., (num_vis,)).astype(config.floatX))

    y = Softmax(
            n_classes = n_classes,
            layer_name = 'y',
            irange = 1.)
    y.set_biases(rng.uniform(-1., 1., (n_classes,)).astype(config.floatX))

    dbm = DBM(visible_layer = v,
            hidden_layers = [y],
            batch_size = 1,
            niter = 50)

    # Randomly pick a v to condition on
    # (Random numbers are generated via dbm.rng)
    layer_to_state = dbm.make_layer_to_state(1)
    v_state = layer_to_state[v]
    y_state = layer_to_state[y]

    # Infer P(y | v) using mean field
    expected_y = y.mf_update(
            state_below = v.upward_state(v_state))

    expected_y = expected_y[0, :]

    expected_y = expected_y.eval()

    # copy all the states out into a batch size of num_samples
    cause_copy = sharedX(np.zeros((num_samples,))).dimshuffle(0,'x')
    v_state = v_state[0,:] + cause_copy
    y_state = y_state[0,:] + cause_copy

    y_samples = y.sample(state_below = v.upward_state(v_state), theano_rng=theano_rng)

    y_samples = function([], y_samples)()

    check_multinomial_samples(y_samples, (num_samples, n_classes), expected_y, tol)


def test_make_symbolic_state():
    # Tests whether the returned p_sample and h_sample have the right
    # dimensions
    num_examples = 40
    theano_rng = MRG_RandomStreams(2012+11+1)

    visible_layer = BinaryVector(nvis=100)
    rval = visible_layer.make_symbolic_state(num_examples=num_examples,
                                             theano_rng=theano_rng)

    hidden_layer = BinaryVectorMaxPool(detector_layer_dim=500,
                                       pool_size=1,
                                       layer_name='h',
                                       irange=0.05,
                                       init_bias=-2.0)
    p_sample, h_sample = hidden_layer.make_symbolic_state(num_examples=num_examples,
                                                          theano_rng=theano_rng)

    softmax_layer = Softmax(n_classes=10, layer_name='s', irange=0.05)
    h_sample_s = softmax_layer.make_symbolic_state(num_examples=num_examples,
                                                   theano_rng=theano_rng)

    required_shapes = [(40, 100), (40, 500), (40, 500), (40, 10)]
    f = function(inputs=[],
                 outputs=[rval, p_sample, h_sample, h_sample_s])

    for s, r in zip(f(), required_shapes):
        assert s.shape == r


def test_variational_cd():

    # Verifies that VariationalCD works well with make_layer_to_symbolic_state
    visible_layer = BinaryVector(nvis=100)
    hidden_layer = BinaryVectorMaxPool(detector_layer_dim=500,
                                       pool_size=1,
                                       layer_name='h',
                                       irange=0.05,
                                       init_bias=-2.0)
    model = DBM(visible_layer=visible_layer,
                hidden_layers=[hidden_layer],
                batch_size=100,
                niter=1)

    cost = VariationalCD(num_chains=100, num_gibbs_steps=2)

    data_specs = cost.get_data_specs(model)
    mapping = DataSpecsMapping(data_specs)
    space_tuple = mapping.flatten(data_specs[0], return_tuple=True)
    source_tuple = mapping.flatten(data_specs[1], return_tuple=True)

    theano_args = []
    for space, source in safe_zip(space_tuple, source_tuple):
        name = '%s' % (source)
        arg = space.make_theano_batch(name=name)
        theano_args.append(arg)
    theano_args = tuple(theano_args)
    nested_args = mapping.nest(theano_args)

    grads, updates = cost.get_gradients(model, nested_args)


def test_extra():
    """
    Test functionality that remains private, if available.
    """

    try:
        import galatea
    except ImportError:
        return
    from galatea.dbm.pylearn2_bridge import run_unit_tests
    run_unit_tests()

########NEW FILE########
__FILENAME__ = test_dropout
"""
Tests of the dropout functionality.
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2013, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"

import warnings

warnings.warn("""
TODO: add test that dropout_fprop with all include probabilities and scales set
    to 1 is equivalent to fprop.
TODO: add a test file to the corresponding cost module and make sure that the
    dropout cost with everything set to 1 is equivalent to the Default cost
""")

########NEW FILE########
__FILENAME__ = test_gsn
"""
No tests for now, but there is an example of use in
pylearn2/scripts/gsn_example.py
"""

########NEW FILE########
__FILENAME__ = test_maxout
"""
Tests of the maxout functionality.
So far these don't test correctness, just that you can
run the objects.
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2013, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"

import numpy as np
import unittest


# Skip test if cuda_ndarray is not available.
from nose.plugins.skip import SkipTest
from theano import config
from theano import function
from theano.sandbox import cuda
from theano import tensor as T

from pylearn2.config import yaml_parse
from pylearn2.datasets.exc import NoDataPathError
from pylearn2.models.mlp import MLP
from pylearn2.models.maxout import Maxout
from pylearn2.space import VectorSpace

def test_min_zero():
    """
    This test guards against a bug where the size of the zero buffer used with
    the min_zero flag was specified to have the wrong size. The bug only
    manifested when compiled with optimizations off, because the optimizations
    discard information about the size of the zero buffer.
    """
    mlp = MLP(input_space=VectorSpace(1),
            layers= [Maxout(layer_name="test_layer", num_units=1,
                num_pieces = 2,
            irange=.05, min_zero=True)])
    X = T.matrix()
    output = mlp.fprop(X)
    # Compile in debug mode so we don't optimize out the size of the buffer
    # of zeros
    f = function([X], output, mode="DEBUG_MODE")
    f(np.zeros((1, 1)).astype(X.dtype))


def test_maxout_basic():

    # Tests that we can load a densely connected maxout model
    # and train it for a few epochs (without saving) on a dummy
    # dataset-- tiny model and dataset

    yaml_string = """
    !obj:pylearn2.train.Train {
        dataset: &train !obj:pylearn2.testing.datasets.random_one_hot_dense_d\
esign_matrix {
            rng: !obj:numpy.random.RandomState { seed: [2013, 3, 16] },
            num_examples: 12,
            dim: 2,
            num_classes: 10
        },
        model: !obj:pylearn2.models.mlp.MLP {
            layers: [
                     !obj:pylearn2.models.maxout.Maxout {
                         layer_name: 'h0',
                         num_units: 3,
                         num_pieces: 2,
                         irange: .005,
                         max_col_norm: 1.9365,
                     },
                     !obj:pylearn2.models.maxout.Maxout {
                         layer_name: 'h1',
                         num_units: 2,
                         num_pieces: 3,
                         irange: .005,
                         max_col_norm: 1.9365,
                     },
                     !obj:pylearn2.models.mlp.Softmax {
                         max_col_norm: 1.9365,
                         layer_name: 'y',
                         n_classes: 10,
                         irange: .005
                     }
                    ],
            nvis: 2,
        },
        algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {
            batch_size: 6,
            learning_rate: .1,
            init_momentum: .5,
            monitoring_dataset:
                {
                    'train' : *train
                },
            cost: !obj:pylearn2.costs.mlp.dropout.Dropout {
                input_include_probs: { 'h0' : .8 },
                input_scales: { 'h0': 1. }
            },
            termination_criterion: !obj:pylearn2.termination_criteria.EpochCo\
unter {
                max_epochs: 3,
            },
            update_callbacks: !obj:pylearn2.training_algorithms.sgd.Exponenti\
alDecay {
                decay_factor: 1.000004,
                min_lr: .000001
            }
        },
        extensions: [
            !obj:pylearn2.training_algorithms.sgd.MomentumAdjustor {
                start: 1,
                saturate: 250,
                final_momentum: .7
            }
        ],
    }
    """

    train = yaml_parse.load(yaml_string)

    train.main_loop()

yaml_string_maxout_conv_c01b_basic = """
    !obj:pylearn2.train.Train {
        dataset: &train !obj:pylearn2.testing.datasets.random_one_hot_topolog\
ical_dense_design_matrix {
            rng: !obj:numpy.random.RandomState { seed: [2013, 3, 16] },
            shape: &input_shape [10, 10],
            channels: 1,
            axes: ['c', 0, 1, 'b'],
            num_examples: 12,
            num_classes: 10
        },
        model: !obj:pylearn2.models.mlp.MLP {
            batch_size: 2,
            layers: [
                     !obj:pylearn2.models.maxout.MaxoutConvC01B {
                         layer_name: 'h0',
                         pad: 0,
                         num_channels: 8,
                         num_pieces: 2,
                         kernel_shape: [2, 2],
                         pool_shape: [2, 2],
                         pool_stride: [2, 2],
                         irange: .005,
                         max_kernel_norm: .9,
                     },
                     # The following layers are commented out to make this
                     # test pass on a GTX 285.
                     # cuda-convnet isn't really meant to run on such an old
                     # graphics card but that's what we use for the buildbot.
                     # In the long run, we should move the buildbot to a newer
                     # graphics card and uncomment the remaining layers.
                     # !obj:pylearn2.models.maxout.MaxoutConvC01B {
                     #    layer_name: 'h1',
                     #    pad: 3,
                     #    num_channels: 4,
                     #    num_pieces: 4,
                     #    kernel_shape: [3, 3],
                     #    pool_shape: [2, 2],
                     #    pool_stride: [2, 2],
                     #    irange: .005,
                     #    max_kernel_norm: 1.9365,
                     # },
                     #!obj:pylearn2.models.maxout.MaxoutConvC01B {
                     #    pad: 3,
                     #    layer_name: 'h2',
                     #    num_channels: 16,
                     #    num_pieces: 2,
                     #    kernel_shape: [2, 2],
                     #    pool_shape: [2, 2],
                     #    pool_stride: [2, 2],
                     #    irange: .005,
                     #    max_kernel_norm: 1.9365,
                     # },
                     !obj:pylearn2.models.mlp.Softmax {
                         max_col_norm: 1.9365,
                         layer_name: 'y',
                         n_classes: 10,
                         irange: .005
                     }
                    ],
            input_space: !obj:pylearn2.space.Conv2DSpace {
                shape: *input_shape,
                num_channels: 1,
                axes: ['c', 0, 1, 'b'],
            },
        },
        algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {
            learning_rate: .05,
            init_momentum: .5,
            monitoring_dataset:
                {
                    'train': *train
                },
            cost: !obj:pylearn2.costs.mlp.dropout.Dropout {
                input_include_probs: { 'h0' : .8 },
                input_scales: { 'h0': 1. }
            },
            termination_criterion: !obj:pylearn2.termination_criteria.EpochCo\
unter {
                max_epochs: 3
            },
            update_callbacks: !obj:pylearn2.training_algorithms.sgd.Exponenti\
alDecay {
                decay_factor: 1.00004,
                min_lr: .000001
            }
        },
        extensions: [
            !obj:pylearn2.training_algorithms.sgd.MomentumAdjustor {
                start: 1,
                saturate: 250,
                final_momentum: .7
            }
        ]
    }
    """

yaml_string_maxout_conv_c01b_cifar10 = """
    !obj:pylearn2.train.Train {
        dataset: &train !obj:pylearn2.datasets.cifar10.CIFAR10 {
            toronto_prepro: True,
            which_set: 'train',
            one_hot: 1,
            axes: ['c', 0, 1, 'b'],
            start: 0,
            stop: 50000
        },
        model: !obj:pylearn2.models.mlp.MLP {
            batch_size: 100,
            input_space: !obj:pylearn2.space.Conv2DSpace {
                shape: [32, 32],
                num_channels: 3,
                axes: ['c', 0, 1, 'b'],
            },
            layers: [
                     !obj:pylearn2.models.maxout.MaxoutConvC01B {
                         layer_name: 'conv1',
                         pad: 0,
                         num_channels: 32,
                         num_pieces: 1,
                         kernel_shape: [5, 5],
                         pool_shape: [3, 3],
                         pool_stride: [2, 2],
                         irange: .01,
                         min_zero: True,
                         W_lr_scale: 1.,
                         b_lr_scale: 2.,
                         tied_b: True,
                         max_kernel_norm: 9.9,
                     },
                     !obj:pylearn2.models.mlp.Softmax {
                         layer_name: 'y',
                         n_classes: 10,
                         istdev: .01,
                         W_lr_scale: 1.,
                         b_lr_scale: 2.,
                         max_col_norm: 9.9365
                     }
                    ],
        },
        algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {
            batch_size: 100,
            learning_rate: .01,
            init_momentum: .9,
            monitoring_dataset:
                {
                    'valid' : !obj:pylearn2.datasets.cifar10.CIFAR10 {
                                  toronto_prepro: True,
                                  axes: ['c', 0, 1, 'b'],
                                  which_set: 'train',
                                  one_hot: 1,
                                  start: 40000,
                                  stop:  50000
                              },
                    'test'  : !obj:pylearn2.datasets.cifar10.CIFAR10 {
                                  toronto_prepro: True,
                                  axes: ['c', 0, 1, 'b'],
                                  which_set: 'test',
                                  one_hot: 1,
                              }
                },
            termination_criterion: !obj:pylearn2.termination_criteria.EpochCo\
unter {
                max_epochs: 5
            }
        }
    }

    """


class TestMaxout(unittest.TestCase):
    def test_maxout_conv_c01b_basic_err(self):
        assert cuda.cuda_enabled is False
        self.assertRaises(RuntimeError,
                          yaml_parse.load,
                          yaml_string_maxout_conv_c01b_basic)

    def test_maxout_conv_c01b_basic(self):
        if cuda.cuda_available is False:
            raise SkipTest('Optional package cuda disabled')
        if not hasattr(cuda, 'unuse'):
            raise Exception("Theano version too old to run this test!")
        # Tests that we can run a small convolutional model on GPU,
        assert cuda.cuda_enabled is False
        # Even if there is a GPU, but the user didn't specify device=gpu
        # we want to run this test.
        try:
            old_floatX = config.floatX
            cuda.use('gpu')
            config.floatX = 'float32'
            train = yaml_parse.load(yaml_string_maxout_conv_c01b_basic)
            train.main_loop()
        finally:
            config.floatX = old_floatX
            cuda.unuse()
        assert cuda.cuda_enabled is False

    def test_maxout_conv_c01b_cifar10(self):
        if cuda.cuda_available is False:
            raise SkipTest('Optional package cuda disabled')
        if not hasattr(cuda, 'unuse'):
            raise Exception("Theano version too old to run this test!")
        # Tests that we can run a small convolutional model on GPU,
        assert cuda.cuda_enabled is False
        # Even if there is a GPU, but the user didn't specify device=gpu
        # we want to run this test.
        try:
            old_floatX = config.floatX
            cuda.use('gpu')
            config.floatX = 'float32'
            try:
                train = yaml_parse.load(yaml_string_maxout_conv_c01b_cifar10)
            except NoDataPathError:
                raise SkipTest("PYLEARN2_DATA_PATH environment variable "
                               "not defined")
            train.main_loop()
            # Check that the performance is close to the expected one:
            # test_y_misclass: 0.3777000308036804
            misclass_chan = train.algorithm.monitor.channels['test_y_misclass']
            assert misclass_chan.val_record[-1] < 0.38, \
                ("misclass_chan.val_record[-1] = %g" %
                 misclass_chan.val_record[-1])
            # test_y_nll: 1.0978516340255737
            nll_chan = train.algorithm.monitor.channels['test_y_nll']
            assert nll_chan.val_record[-1] < 1.1
        finally:
            config.floatX = old_floatX
            cuda.unuse()
        assert cuda.cuda_enabled is False


if __name__ == '__main__':

    t = TestMaxout('setUp')
    t.setUp()
    t.test_maxout_conv_c01b_basic()

    if 0:
        unittest.main()

########NEW FILE########
__FILENAME__ = test_mlp
from itertools import product

import numpy as np
import theano
from theano import tensor

from pylearn2.models.mlp import (MLP, Linear, Softmax, Sigmoid,
                                 exhaustive_dropout_average,
                                 sampled_dropout_average, CompositeLayer)
from pylearn2.space import VectorSpace, CompositeSpace
from pylearn2.utils import is_iterable


class IdentityLayer(Linear):
    dropout_input_mask_value = -np.inf

    def fprop(self, state_below):
        return state_below


def test_masked_fprop():
    # Construct a dirt-simple linear network with identity weights.
    mlp = MLP(nvis=2, layers=[Linear(2, 'h0', irange=0),
                              Linear(2, 'h1', irange=0)])
    mlp.layers[0].set_weights(np.eye(2, dtype=mlp.get_weights().dtype))
    mlp.layers[1].set_weights(np.eye(2, dtype=mlp.get_weights().dtype))
    mlp.layers[0].set_biases(np.arange(1, 3, dtype=mlp.get_weights().dtype))
    mlp.layers[1].set_biases(np.arange(3, 5, dtype=mlp.get_weights().dtype))

    # Verify that get_total_input_dimension works.
    np.testing.assert_equal(mlp.get_total_input_dimension(['h0', 'h1']), 4)
    inp = theano.tensor.matrix()

    # Accumulate the sum of output of all masked networks.
    l = []
    for mask in xrange(16):
        l.append(mlp.masked_fprop(inp, mask))
    outsum = reduce(lambda x, y: x + y, l)

    f = theano.function([inp], outsum, allow_input_downcast=True)
    np.testing.assert_equal(f([[5, 3]]), [[144., 144.]])
    np.testing.assert_equal(f([[2, 7]]), [[96., 208.]])

    np.testing.assert_raises(ValueError, mlp.masked_fprop, inp, 22)
    np.testing.assert_raises(ValueError, mlp.masked_fprop, inp, 2,
                             ['h3'])
    np.testing.assert_raises(ValueError, mlp.masked_fprop, inp, 2,
                             None, 2., {'h3': 4})


def test_sampled_dropout_average():
    # This is only a smoke test: verifies that it compiles and runs,
    # not any particular value.
    inp = theano.tensor.matrix()
    mlp = MLP(nvis=2, layers=[Linear(2, 'h0', irange=0.8),
                              Linear(2, 'h1', irange=0.8),
                              Softmax(3, 'out', irange=0.8)])
    out = sampled_dropout_average(mlp, inp, 5)
    f = theano.function([inp], out, allow_input_downcast=True)
    f([[2.3, 4.9]])


def test_exhaustive_dropout_average():
    # This is only a smoke test: verifies that it compiles and runs,
    # not any particular value.
    inp = theano.tensor.matrix()
    mlp = MLP(nvis=2, layers=[Linear(2, 'h0', irange=0.8),
                              Linear(2, 'h1', irange=0.8),
                              Softmax(3, 'out', irange=0.8)])
    out = exhaustive_dropout_average(mlp, inp)
    f = theano.function([inp], out, allow_input_downcast=True)
    f([[2.3, 4.9]])

    out = exhaustive_dropout_average(mlp, inp, input_scales={'h0': 3})
    f = theano.function([inp], out, allow_input_downcast=True)
    f([[2.3, 4.9]])

    out = exhaustive_dropout_average(mlp, inp, masked_input_layers=['h1'])
    f = theano.function([inp], out, allow_input_downcast=True)
    f([[2.3, 4.9]])

    np.testing.assert_raises(ValueError, exhaustive_dropout_average, mlp,
                             inp, ['h5'])

    np.testing.assert_raises(ValueError, exhaustive_dropout_average, mlp,
                             inp, ['h0'], 2., {'h5': 3.})


def test_dropout_input_mask_value():
    # Construct a dirt-simple linear network with identity weights.
    mlp = MLP(nvis=2, layers=[IdentityLayer(2, 'h0', irange=0)])
    mlp.layers[0].set_weights(np.eye(2, dtype=mlp.get_weights().dtype))
    mlp.layers[0].set_biases(np.arange(1, 3, dtype=mlp.get_weights().dtype))
    mlp.layers[0].dropout_input_mask_value = -np.inf
    inp = theano.tensor.matrix()
    f = theano.function([inp], mlp.masked_fprop(inp, 1, default_input_scale=1),
                        allow_input_downcast=True)
    np.testing.assert_equal(f([[4., 3.]]), [[4., -np.inf]])


def test_sigmoid_layer_misclass_reporting():
    mlp = MLP(nvis=3, layers=[Sigmoid(layer_name='h0', dim=1, irange=0.005,
                                      monitor_style='classification')])
    target = theano.tensor.matrix(dtype=theano.config.floatX)
    batch = theano.tensor.matrix(dtype=theano.config.floatX)
    rval = mlp.layers[0].get_monitoring_channels_from_state(mlp.fprop(batch),
                                                            target)

    f = theano.function([batch, target], [tensor.gt(mlp.fprop(batch), 0.5),
                                          rval['misclass']],
                        allow_input_downcast=True)
    rng = np.random.RandomState(0)

    for _ in range(10):  # repeat a few times for statistical strength
        targets = (rng.uniform(size=(30, 1)) > 0.5).astype('uint8')
        out, misclass = f(rng.normal(size=(30, 3)), targets)
        np.testing.assert_allclose((targets != out).mean(), misclass)


def test_batchwise_dropout():
    mlp = MLP(nvis=2, layers=[IdentityLayer(2, 'h0', irange=0)])
    mlp.layers[0].set_weights(np.eye(2, dtype=mlp.get_weights().dtype))
    mlp.layers[0].set_biases(np.arange(1, 3, dtype=mlp.get_weights().dtype))
    mlp.layers[0].dropout_input_mask_value = 0
    inp = theano.tensor.matrix()
    f = theano.function([inp], mlp.dropout_fprop(inp, per_example=False),
                        allow_input_downcast=True)
    for _ in range(10):
        d = f([[3.0, 4.5]] * 3)
        np.testing.assert_equal(d[0], d[1])
        np.testing.assert_equal(d[0], d[2])

    f = theano.function([inp], mlp.dropout_fprop(inp, per_example=True),
                        allow_input_downcast=True)
    d = f([[3.0, 4.5]] * 3)
    print d
    np.testing.assert_(np.any(d[0] != d[1]) or np.any(d[0] != d[2]))

def test_str():
    """
    Make sure the __str__ method returns a string
    """

    mlp = MLP(nvis=2, layers=[Linear(2, 'h0', irange=0),
                              Linear(2, 'h1', irange=0)])

    s = str(mlp)

    assert isinstance(s, basestring)

def test_sigmoid_detection_cost():
    # This is only a smoke test: verifies that it compiles and runs,
    # not any particular value.
    rng = np.random.RandomState(0)
    y = (rng.uniform(size=(4, 3)) > 0.5).astype('uint8')
    X = theano.shared(rng.uniform(size=(4, 2)))
    model = MLP(nvis=2, layers=[Sigmoid(monitor_style='detection', dim=3,
                layer_name='y', irange=0.8)])
    y_hat = model.fprop(X)
    model.cost(y, y_hat).eval()

if __name__ == "__main__":
    test_masked_fprop()
    test_sampled_dropout_average()
    test_exhaustive_dropout_average()
    test_dropout_input_mask_value()
    test_sigmoid_layer_misclass_reporting()
    test_batchwise_dropout()
    test_sigmoid_detection_cost()


def test_composite_layer():
    """
    Test the routing functionality of the CompositeLayer
    """
    # Without routing
    composite_layer = CompositeLayer('composite_layer',
                                     [Linear(2, 'h0', irange=0),
                                      Linear(2, 'h1', irange=0),
                                      Linear(2, 'h2', irange=0)])
    mlp = MLP(nvis=2, layers=[composite_layer])
    for i in range(3):
        composite_layer.layers[i].set_weights(
            np.eye(2, dtype=theano.config.floatX)
        )
        composite_layer.layers[i].set_biases(
            np.zeros(2, dtype=theano.config.floatX)
        )
    X = tensor.matrix()
    y = mlp.fprop(X)
    funs = [theano.function([X], y_elem) for y_elem in y]
    x_numeric = np.random.rand(2, 2).astype('float32')
    y_numeric = [f(x_numeric) for f in funs]
    assert np.all(x_numeric == y_numeric)

    # With routing
    for inputs_to_layers in [{0: [1], 1: [2], 2: [0]},
                             {0: [1], 1: [0, 2], 2: []},
                             {0: [], 1: []}]:
        composite_layer = CompositeLayer('composite_layer',
                                         [Linear(2, 'h0', irange=0),
                                          Linear(2, 'h1', irange=0),
                                          Linear(2, 'h2', irange=0)],
                                         inputs_to_layers)
        input_space = CompositeSpace([VectorSpace(dim=2),
                                      VectorSpace(dim=2),
                                      VectorSpace(dim=2)])
        mlp = MLP(input_space=input_space, layers=[composite_layer])
        for i in range(3):
            composite_layer.layers[i].set_weights(
                np.eye(2, dtype=theano.config.floatX)
            )
            composite_layer.layers[i].set_biases(
                np.zeros(2, dtype=theano.config.floatX)
            )
        X = [tensor.matrix() for _ in range(3)]
        y = mlp.fprop(X)
        funs = [theano.function(X, y_elem, on_unused_input='ignore')
                for y_elem in y]
        x_numeric = [np.random.rand(2, 2).astype(theano.config.floatX)
                     for _ in range(3)]
        y_numeric = [f(*x_numeric) for f in funs]
        assert all([all([np.all(x_numeric[i] == y_numeric[j])
                         for j in inputs_to_layers[i]])
                    for i in inputs_to_layers])

    # Get the weight decay expressions from a composite layer
    composite_layer = CompositeLayer('composite_layer',
                                     [Linear(2, 'h0', irange=0.1),
                                      Linear(2, 'h1', irange=0.1)])
    input_space = VectorSpace(dim=10)
    mlp = MLP(input_space=input_space, layers=[composite_layer])
    for attr, coeff in product(['get_weight_decay', 'get_l1_weight_decay'],
                               [[0.7, 0.3], 0.5]):
        f = theano.function([], getattr(composite_layer, attr)(coeff))
        if is_iterable(coeff):
            g = theano.function(
                [], tensor.sum([getattr(layer, attr)(c) for c, layer
                                in zip(coeff, composite_layer.layers)])
            )
            assert np.allclose(f(), g())
        else:
            g = theano.function(
                [], tensor.sum([getattr(layer, attr)(coeff) for layer
                                in composite_layer.layers])
            )
            assert np.allclose(f(), g())

########NEW FILE########
__FILENAME__ = test_mnd
import numpy as np
from pylearn2.models.mnd import DiagonalMND
from pylearn2.models.mnd import kl_divergence
from pylearn2.optimization.batch_gradient_descent import BatchGradientDescent
from theano import config
from theano import function
import warnings
floatX = config.floatX

class Test_DiagonalMND:
    """ Class for testing DiagonalMND """

    def __init__(self):
        dim = 3

        self.dim = dim

        self.p = DiagonalMND( nvis = dim,
                init_beta = 1.,
                init_mu = 0.,
                min_beta = 1e-6,
                max_beta = 1e6)

        self.q = DiagonalMND( nvis = dim,
                init_beta = 1.,
                init_mu = 0.,
                min_beta = 1e-6,
                max_beta = 1e6)

    def test_same_zero(self):
        """ checks that two models with the same parameters
        have zero KL divergence """

        rng = np.random.RandomState([1,2,3])

        dim = self.dim

        num_trials = 3

        for trial in xrange(num_trials):
            mu = rng.randn(dim).astype(floatX)
            beta = rng.uniform(.1,10.,(dim,)).astype(floatX)

            self.p.mu.set_value(mu)
            self.q.mu.set_value(mu)
            self.p.beta.set_value(beta)
            self.q.beta.set_value(beta)

            kl = kl_divergence(self.q,self.p)

            kl = function([],kl)()

            tol = 1e-7
            # Second part of the check handles cases where kl is None, etc.
            if kl > tol or not (kl <= tol):
                raise AssertionError("KL divergence between two "
                        "equivalent models should be 0 but is "+
                        str(kl))

    def test_nonnegative_samples(self):
        """ checks that the kl divergence is non-negative
            at sampled parameter values for q and p"""

        rng = np.random.RandomState([1,2,3])

        dim = self.dim

        num_trials = 3

        for trial in xrange(num_trials):
            mu = rng.randn(dim).astype(floatX)
            beta = rng.uniform(.1,10.,(dim,)).astype(floatX)
            self.p.mu.set_value(mu)
            mu = rng.randn(dim).astype(floatX)
            self.q.mu.set_value(mu)
            self.p.beta.set_value(beta)
            beta = rng.uniform(.1,10.,(dim,)).astype(floatX)
            self.q.beta.set_value(beta)

            kl = kl_divergence(self.q,self.p)

            kl = function([],kl)()

            if kl < 0.:
                raise AssertionError("KL divergence should "
                        "be non-negative but is "+
                        str(kl))

    def test_zero_optimal(self):
        """ minimizes the kl divergence between q and p
            using batch gradient descent and checks that
            the result is zero"""

        rng = np.random.RandomState([1,2,3])

        dim = self.dim

        num_trials = 3

        mu = rng.randn(dim).astype(floatX)
        beta = rng.uniform(.1,10.,(dim,)).astype(floatX)
        self.p.mu.set_value(mu)
        mu = rng.randn(dim).astype(floatX)
        self.q.mu.set_value(mu)
        self.p.beta.set_value(beta)
        beta = rng.uniform(.1,10.,(dim,)).astype(floatX)
        self.q.beta.set_value(beta)

        kl = kl_divergence(self.q,self.p)

        p = self.p
        q = self.q

        optimizer = BatchGradientDescent(
                    max_iter = 100,
                    line_search_mode = 'exhaustive',
                    verbose = True,
                    objective = kl,
                    conjugate = True,
                    params = [ p.mu, p.beta, q.mu, q.beta ],
                    param_constrainers = [ p.modify_updates,
                        q.modify_updates ])

        kl = optimizer.minimize()

        if kl < 0.:

            if config.floatX == 'float32':
                neg_tol = 4.8e-7
            else:
                neg_tol = 0.

            if kl < - neg_tol:
                raise AssertionError("KL divergence should "
                    "be non-negative but is "+
                    str(kl))

            warnings.warn("KL divergence is not very numerically stable, evidently")

        tol = 6e-5
        if kl > tol:
            print 'kl:',kl
            print 'tol:',tol
        assert kl <= tol
        assert not (kl > tol )


def test_log_partition_function():
    """ tests that the log partition function is right in the simple 1D case"""

    sigma = 2.3
    model = DiagonalMND(nvis=1,init_beta=1/np.square(sigma),
            min_beta = 1e-6, max_beta = 1e6, init_mu = 17.)

    log_Z = model.log_partition_function()

    log_Z = function([],log_Z)()

    ground = np.log( sigma * np.sqrt(2.*np.pi))

    print ground
    print log_Z

    assert np.allclose(ground, log_Z)

if __name__ == '__main__':
    tester = Test_DiagonalMND()
    tester.test_zero_optimal()

########NEW FILE########
__FILENAME__ = test_model
"""
Tests for functionality from model.py
"""

import numpy as np

from pylearn2.models import Model
from pylearn2.utils import sharedX


def test_get_set_vector():
    """
    Tests that get_vector and set_vector use the same
    format.
    """

    rng = np.random.RandomState([2014, 5, 8])

    class DummyModel(Model):
        """
        A Model that exercises this test by having a few different
        parameters with different shapes and dimensionalities.

        Don't instantiate more than one of these because the parameters
        are class-level attributes.
        """

        _params = [sharedX(rng.randn(5)), sharedX(rng.randn(5, 3)),
                   sharedX(rng.randn(4, 4, 4))]

    model = DummyModel()

    vector = model.get_param_vector()
    model.set_param_vector(0. * vector)
    assert np.allclose(0. * vector, model.get_param_vector())
    model.set_param_vector(vector)
    assert np.allclose(model.get_param_vector(), vector)


def test_tag():
    """Test that the tag attribute works correctly."""
    class DummyModel(Model):
        """The simplest instance of Model possible."""
    x = DummyModel()
    x.tag['foo']['bar'] = 5

    assert len(x.tag.keys()) == 1
    assert len(x.tag['foo'].keys()) == 1
    assert x.tag['foo']['bar'] == 5

    assert 'bar' not in x.tag
    x.tag['bar']['baz'] = 3
    assert 'bar' in x.tag
    assert 'baz' in x.tag['bar']
    assert len(x.tag.keys()) == 2

########NEW FILE########
__FILENAME__ = test_rbm
import numpy as np

import theano.sandbox.rng_mrg
RandomStreams = theano.sandbox.rng_mrg.MRG_RandomStreams
from theano import tensor as T

from pylearn2.datasets.dense_design_matrix import DenseDesignMatrix
from pylearn2.models.rbm import RBM
from pylearn2.training_algorithms.default import DefaultTrainingAlgorithm
from pylearn2.utils.rng import make_theano_rng

def test_get_weights():
    # Tests that the RBM, when constructed
    # with nvis and nhid arguments, supports the
    # weights interface

    model = RBM(nvis = 2, nhid = 3)
    W = model.get_weights()

def test_get_input_space():
    # Tests that the RBM supports
    # the Space interface

    model = RBM(nvis = 2, nhid = 3)
    space = model.get_input_space()

def test_gibbs_step_for_v():
    # Just tests that gibbs_step_for_v can be called
    # without crashing

    model = RBM(nvis = 2, nhid = 3)

    theano_rng = make_theano_rng(17, which_method='binomial')

    X = T.matrix()

    Y = model.gibbs_step_for_v(X, theano_rng)

def test_train_batch():
    # Just tests that train_batch can be called without crashing

    m = 1
    dim = 2
    rng = np.random.RandomState([2014, 03, 17])
    X = rng.randn(m, dim)
    train = DenseDesignMatrix(X=X)

    rbm = RBM(nvis=dim, nhid=3)
    trainer = DefaultTrainingAlgorithm(batch_size=1)
    trainer.setup(rbm, train)
    trainer.train(train)

########NEW FILE########
__FILENAME__ = test_reflection_clip
import numpy as np
from pylearn2.models.s3c import reflection_clip
from theano import function
from theano import shared
from pylearn2.utils.rng import make_np_rng

def test_reflection_clip():
    N = 5
    m = 10

    rng = make_np_rng([1,2,3], which_method='randn')

    Mu1_old = rng.randn(m,N)
    Mu1_new = rng.randn(m,N)

    rho = .6

    Mu1_clipped = function([],reflection_clip( \
            shared(Mu1_old), shared(Mu1_new), rho))()

    case1 = False
    case2 = False
    case3 = False
    case4 = False

    for i in xrange(m):
        for j in xrange(N):
            old = Mu1_old[i,j]
            new = Mu1_new[i,j]
            clipped = Mu1_clipped[i,j]

            if old > 0.:
                if new < - rho * old:
                    case1 = True
                    assert abs(clipped-(-rho*old)) < 1e-6
                else:
                    case2 = True
                    assert new == clipped
            elif old < 0.:
                if new > - rho * old:
                    case3 = True
                    assert abs(clipped-(-rho*old)) < 1e-6
                else:
                    case4 = True
                    assert new == clipped
            else:
                assert new == clipped

    #if any of the following fail, it doesn't necessarily mean
    #that reflection_clip is broken, just that the test needs
    #to be adjusted to have better coverage
    assert case1
    assert case2
    assert case3
    assert case4

########NEW FILE########
__FILENAME__ = test_s3c_inference
from pylearn2.models.s3c import S3C
from pylearn2.models.s3c import E_Step_Scan
from pylearn2.models.s3c import Grad_M_Step
from pylearn2.models.s3c import E_Step
from theano import function
import numpy as np
import theano.tensor as T
from theano import config
#from pylearn2.utils import serial
import warnings


def broadcast(mat, shape_0):
    rval = mat
    if mat.shape[0] != shape_0:
        assert mat.shape[0] == 1

        rval = np.zeros((shape_0, mat.shape[1]),dtype=mat.dtype)

        for i in xrange(shape_0):
            rval[i,:] = mat[0,:]

    return rval


class Test_S3C_Inference:
    def setUp(self):
        # Temporarily change config.floatX to float64, as s3c inference
        # tests currently fail due to numerical issues for float32.
        self.prev_floatX = config.floatX
        config.floatX = 'float64'

    def tearDown(self):
        # Restore previous value of floatX
        config.floatX = self.prev_floatX

    def __init__(self):
        """ gets a small batch of data
            sets up an S3C model
        """
        # We also have to change the value of config.floatX in __init__.
        self.prev_floatX = config.floatX
        config.floatX = 'float64'

        try:
            self.tol = 1e-5

            #dataset = serial.load('${PYLEARN2_DATA_PATH}/stl10/stl10_patches/data.pkl')

            #X = dataset.get_batch_design(1000)
            #X = X[:,0:5]

            X = np.random.RandomState([1,2,3]).randn(1000,5)

            X -= X.mean()
            X /= X.std()
            m, D = X.shape
            N = 5

            #don't give the model an e_step or learning rate so it won't spend years compiling a learn_func
            self.model = S3C(nvis = D,
                             nhid = N,
                             irange = .1,
                             init_bias_hid = 0.,
                             init_B = 3.,
                             min_B = 1e-8,
                             max_B = 1000.,
                             init_alpha = 1., min_alpha = 1e-8, max_alpha = 1000.,
                             init_mu = 1., e_step = None,
                             m_step = Grad_M_Step(),
                             min_bias_hid = -1e30, max_bias_hid = 1e30,
                            )

            self.model.make_pseudoparams()

            self.h_new_coeff_schedule = [.1, .2, .3, .4, .5, .6, .7, .8, .9, 1. ]

            self.e_step = E_Step_Scan(h_new_coeff_schedule = self.h_new_coeff_schedule)
            self.e_step.register_model(self.model)

            self.X = X
            self.N = N
            self.m = m

        finally:
            config.floatX = self.prev_floatX

    def test_match_unrolled(self):
        """ tests that inference with scan matches result using unrolled loops """

        unrolled_e_step = E_Step(h_new_coeff_schedule = self.h_new_coeff_schedule)
        unrolled_e_step.register_model(self.model)

        V = T.matrix()

        scan_result = self.e_step.infer(V)
        unrolled_result = unrolled_e_step.infer(V)

        outputs = []

        for key in scan_result:
            outputs.append(scan_result[key])
            outputs.append(unrolled_result[key])

        f = function([V], outputs)

        outputs = f(self.X)

        assert len(outputs) % 2 == 0

        for i in xrange(0,len(outputs),2):
            assert np.allclose(outputs[i],outputs[i+1])


    def test_grad_s(self):

        "tests that the gradients with respect to s_i are 0 after doing a mean field update of s_i "

        model = self.model
        e_step = self.e_step
        X = self.X

        assert X.shape[0] == self.m

        model.test_batch_size = X.shape[0]

        init_H = e_step.init_H_hat(V = X)
        init_Mu1 = e_step.init_S_hat(V = X)

        prev_setting = config.compute_test_value
        config.compute_test_value= 'off'
        H, Mu1 = function([], outputs=[init_H, init_Mu1])()
        config.compute_test_value = prev_setting

        H = broadcast(H, self.m)
        Mu1 = broadcast(Mu1, self.m)

        H = np.cast[config.floatX](self.model.rng.uniform(0.,1.,H.shape))
        Mu1 = np.cast[config.floatX](self.model.rng.uniform(-5.,5.,Mu1.shape))



        H_var = T.matrix(name='H_var')
        H_var.tag.test_value = H
        Mu1_var = T.matrix(name='Mu1_var')
        Mu1_var.tag.test_value = Mu1
        idx = T.iscalar()
        idx.tag.test_value = 0


        S = e_step.infer_S_hat(V = X, H_hat = H_var, S_hat = Mu1_var)

        s_idx = S[:,idx]

        s_i_func = function([H_var,Mu1_var,idx],s_idx)

        sigma0 = 1. / model.alpha
        Sigma1 = e_step.infer_var_s1_hat()
        mu0 = T.zeros_like(model.mu)

        #by truncated KL, I mean that I am dropping terms that don't depend on H and Mu1
        # (they don't affect the outcome of this test and some of them are intractable )
        trunc_kl = - model.entropy_hs(H_hat = H_var, var_s0_hat = sigma0, var_s1_hat = Sigma1) + \
                     model.expected_energy_vhs(V = X, H_hat = H_var, S_hat = Mu1_var, var_s0_hat = sigma0, var_s1_hat = Sigma1)

        grad_Mu1 = T.grad(trunc_kl.sum(), Mu1_var)

        grad_Mu1_idx = grad_Mu1[:,idx]

        grad_func = function([H_var, Mu1_var, idx], grad_Mu1_idx)

        for i in xrange(self.N):
            Mu1[:,i] = s_i_func(H, Mu1, i)

            g = grad_func(H,Mu1,i)

            assert not np.any(np.isnan(g))

            g_abs_max = np.abs(g).max()


            if g_abs_max > self.tol:
                raise Exception('after mean field step, gradient of kl divergence wrt mean field parameter should be 0, but here the max magnitude of a gradient element is '+str(g_abs_max)+' after updating s_'+str(i))

    def test_value_s(self):

        "tests that the value of the kl divergence decreases with each update to s_i "

        model = self.model
        e_step = self.e_step
        X = self.X

        assert X.shape[0] == self.m

        init_H = e_step.init_H_hat(V = X)
        init_Mu1 = e_step.init_S_hat(V = X)

        prev_setting = config.compute_test_value
        config.compute_test_value= 'off'
        H, Mu1 = function([], outputs=[init_H, init_Mu1])()
        config.compute_test_value = prev_setting

        H = broadcast(H, self.m)
        Mu1 = broadcast(Mu1, self.m)

        H = np.cast[config.floatX](self.model.rng.uniform(0.,1.,H.shape))
        Mu1 = np.cast[config.floatX](self.model.rng.uniform(-5.,5.,Mu1.shape))


        H_var = T.matrix(name='H_var')
        H_var.tag.test_value = H
        Mu1_var = T.matrix(name='Mu1_var')
        Mu1_var.tag.test_value = Mu1
        idx = T.iscalar()
        idx.tag.test_value = 0

        S = e_step.infer_S_hat( V = X, H_hat = H_var, S_hat = Mu1_var)

        s_idx = S[:,idx]

        s_i_func = function([H_var,Mu1_var,idx],s_idx)

        sigma0 = 1. / model.alpha
        Sigma1 = e_step.infer_var_s1_hat()
        mu0 = T.zeros_like(model.mu)

        #by truncated KL, I mean that I am dropping terms that don't depend on H and Mu1
        # (they don't affect the outcome of this test and some of them are intractable )
        trunc_kl = - model.entropy_hs(H_hat = H_var, var_s0_hat = sigma0, var_s1_hat = Sigma1) + \
                     model.expected_energy_vhs(V = X, H_hat = H_var, S_hat = Mu1_var, var_s0_hat = sigma0, var_s1_hat = Sigma1)

        trunc_kl_func = function([H_var, Mu1_var], trunc_kl)

        for i in xrange(self.N):
            prev_kl = trunc_kl_func(H,Mu1)

            Mu1[:,i] = s_i_func(H, Mu1, i)

            new_kl = trunc_kl_func(H,Mu1)


            increase = new_kl - prev_kl


            mx = increase.max()

            if mx > 1e-3:
                raise Exception('after mean field step in s, kl divergence should decrease, but some elements increased by as much as '+str(mx)+' after updating s_'+str(i))

    def test_grad_h(self):

        "tests that the gradients with respect to h_i are 0 after doing a mean field update of h_i "

        model = self.model
        e_step = self.e_step
        X = self.X

        assert X.shape[0] == self.m

        init_H = e_step.init_H_hat(V = X)
        init_Mu1 = e_step.init_S_hat(V = X)

        prev_setting = config.compute_test_value
        config.compute_test_value= 'off'
        H, Mu1 = function([], outputs=[init_H, init_Mu1])()
        config.compute_test_value = prev_setting

        H = broadcast(H, self.m)
        Mu1 = broadcast(Mu1, self.m)

        H = np.cast[config.floatX](self.model.rng.uniform(0.,1.,H.shape))
        Mu1 = np.cast[config.floatX](self.model.rng.uniform(-5.,5.,Mu1.shape))


        H_var = T.matrix(name='H_var')
        H_var.tag.test_value = H
        Mu1_var = T.matrix(name='Mu1_var')
        Mu1_var.tag.test_value = Mu1
        idx = T.iscalar()
        idx.tag.test_value = 0


        new_H = e_step.infer_H_hat(V = X, H_hat = H_var, S_hat = Mu1_var)
        h_idx = new_H[:,idx]

        updates_func = function([H_var,Mu1_var,idx], h_idx)

        sigma0 = 1. / model.alpha
        Sigma1 = e_step.infer_var_s1_hat()
        mu0 = T.zeros_like(model.mu)

        #by truncated KL, I mean that I am dropping terms that don't depend on H and Mu1
        # (they don't affect the outcome of this test and some of them are intractable )
        trunc_kl = - model.entropy_hs(H_hat = H_var, var_s0_hat = sigma0, var_s1_hat = Sigma1) + \
                     model.expected_energy_vhs(V = X, H_hat = H_var, S_hat = Mu1_var,  var_s0_hat = sigma0,
                             var_s1_hat = Sigma1)

        grad_H = T.grad(trunc_kl.sum(), H_var)

        assert len(grad_H.type.broadcastable) == 2

        #from theano.printing import min_informative_str
        #print min_informative_str(grad_H)

        #grad_H = Print('grad_H')(grad_H)

        #grad_H_idx = grad_H[:,idx]

        grad_func = function([H_var, Mu1_var], grad_H)

        failed = False

        for i in xrange(self.N):
            rval = updates_func(H, Mu1, i)
            H[:,i] = rval

            g = grad_func(H,Mu1)[:,i]

            assert not np.any(np.isnan(g))

            g_abs_max = np.abs(g).max()

            if g_abs_max > self.tol:
                #print "new values of H"
                #print H[:,i]
                #print "gradient on new values of H"
                #print g

                failed = True

                print 'iteration ',i
                #print 'max value of new H: ',H[:,i].max()
                #print 'H for failing g: '
                failing_h = H[np.abs(g) > self.tol, i]
                #print failing_h

                #from matplotlib import pyplot as plt
                #plt.scatter(H[:,i],g)
                #plt.show()

                #ignore failures extremely close to h=1

                high_mask = failing_h > .001
                low_mask = failing_h < .999

                mask = high_mask * low_mask

                print 'masked failures: ',mask.shape[0],' err ',g_abs_max

                if mask.sum() > 0:
                    print 'failing h passing the range mask'
                    print failing_h[ mask.astype(bool) ]
                    raise Exception('after mean field step, gradient of kl divergence'
                            ' wrt freshly updated variational parameter should be 0, '
                            'but here the max magnitude of a gradient element is '
                            +str(g_abs_max)+' after updating h_'+str(i))


        #assert not failed


    def test_value_h(self):

        "tests that the value of the kl divergence decreases with each update to h_i "

        model = self.model
        e_step = self.e_step
        X = self.X

        assert X.shape[0] == self.m

        init_H = e_step.init_H_hat(V = X)
        init_Mu1 = e_step.init_S_hat(V = X)

        prev_setting = config.compute_test_value
        config.compute_test_value= 'off'
        H, Mu1 = function([], outputs=[init_H, init_Mu1])()
        config.compute_test_value = prev_setting

        H = broadcast(H, self.m)
        Mu1 = broadcast(Mu1, self.m)

        H = np.cast[config.floatX](self.model.rng.uniform(0.,1.,H.shape))
        Mu1 = np.cast[config.floatX](self.model.rng.uniform(-5.,5.,Mu1.shape))


        H_var = T.matrix(name='H_var')
        H_var.tag.test_value = H
        Mu1_var = T.matrix(name='Mu1_var')
        Mu1_var.tag.test_value = Mu1
        idx = T.iscalar()
        idx.tag.test_value = 0

        newH = e_step.infer_H_hat(V = X, H_hat = H_var, S_hat = Mu1_var)


        h_idx = newH[:,idx]


        h_i_func = function([H_var,Mu1_var,idx],h_idx)

        sigma0 = 1. / model.alpha
        Sigma1 = e_step.infer_var_s1_hat()
        mu0 = T.zeros_like(model.mu)

        #by truncated KL, I mean that I am dropping terms that don't depend on H and Mu1
        # (they don't affect the outcome of this test and some of them are intractable )
        trunc_kl = - model.entropy_hs(H_hat = H_var, var_s0_hat = sigma0, var_s1_hat = Sigma1) + \
                     model.expected_energy_vhs(V = X, H_hat = H_var, S_hat = Mu1_var, var_s0_hat = sigma0, var_s1_hat = Sigma1)

        trunc_kl_func = function([H_var, Mu1_var], trunc_kl)

        for i in xrange(self.N):
            prev_kl = trunc_kl_func(H,Mu1)

            H[:,i] = h_i_func(H, Mu1, i)
            #we don't update mu, the whole point of the split e step is we don't have to

            new_kl = trunc_kl_func(H,Mu1)


            increase = new_kl - prev_kl


            print 'failures after iteration ',i,': ',(increase > self.tol).sum()

            mx = increase.max()

            if mx > 1e-4:
                print 'increase amounts of failing examples:'
                print increase[increase > self.tol]
                print 'failing H:'
                print H[increase > self.tol,:]
                print 'failing Mu1:'
                print Mu1[increase > self.tol,:]
                print 'failing V:'
                print X[increase > self.tol,:]


                raise Exception('after mean field step in h, kl divergence should decrease, but some elements increased by as much as '+str(mx)+' after updating h_'+str(i))

if __name__ == '__main__':
    obj = Test_S3C_Inference()

    #obj.test_grad_h()
    #obj.test_grad_s()
    #obj.test_value_s()
    obj.test_value_h()

########NEW FILE########
__FILENAME__ = test_s3c_misc
from theano.sandbox.linalg.ops import alloc_diag
from pylearn2.models.s3c import S3C
from pylearn2.models.s3c import SufficientStatistics
from pylearn2.models.s3c import E_Step_Scan
from pylearn2.models.s3c import Grad_M_Step
from pylearn2.utils import as_floatX
from theano import function
import numpy as np
import theano.tensor as T
from theano import config

class TestS3C_Misc:
    def setUp(self):
        # Temporarily change config.floatX to float64, as s3c these
        # tests currently fail with float32.
        self.prev_floatX = config.floatX
        config.floatX = 'float64'

    def tearDown(self):
        # Restore previous value of floatX
        config.floatX = self.prev_floatX

    def __init__(self):
        """ gets a small batch of data
            sets up an S3C model and learns on the data
            creates an expression for the log likelihood of the data
        """

        # We also have to change the value of config.floatX in __init__.
        self.prev_floatX = config.floatX
        config.floatX = 'float64'

        try:
            self.tol = 1e-5


            X = np.random.RandomState([1,2,3]).randn(1000,108)
            m, D = X.shape
            N = 300

            self.model = S3C(nvis = D,
                             nhid = N,
                             irange = .5,
                             init_bias_hid = -.1,
                             init_B = 1.,
                             min_B = 1e-8,
                             max_B = 1e8,
                             tied_B = 1,
                             e_step = E_Step_Scan(
                                 h_new_coeff_schedule = [ .01 ]
                             ),
                             init_alpha = 1.,
                             min_alpha = 1e-8, max_alpha = 1e8,
                             init_mu = 1.,
                             m_step = Grad_M_Step( learning_rate = 1.0 ),
                            )

            self.orig_params = self.model.get_param_values()

            model = self.model
            self.mf_obs = model.e_step.infer(X)

            self.stats = SufficientStatistics.from_observations(needed_stats =
                    model.m_step.needed_stats(), V =X,
                    ** self.mf_obs)

            self.prob = self.model.expected_log_prob_vhs( self.stats , H_hat = self.mf_obs['H_hat'], S_hat = self.mf_obs['S_hat'])
            self.X = X
            self.m = m
            self.D = D
            self.N = N

        finally:
            config.floatX = self.prev_floatX

    def test_expected_log_prob_vhs_batch_match(self):
        """ verifies that expected_log_prob_vhs = mean(expected_log_prob_vhs_batch)
            expected_log_prob_vhs_batch is implemented in terms of expected_energy_vhs
            so this verifies that as well """

        scalar = self.model.expected_log_prob_vhs( stats = self.stats, H_hat = self.mf_obs['H_hat'], S_hat = self.mf_obs['S_hat'])
        batch  = self.model.expected_log_prob_vhs_batch( V = self.X, H_hat = self.mf_obs['H_hat'], S_hat = self.mf_obs['S_hat'], var_s0_hat = self.mf_obs['var_s0_hat'], var_s1_hat = self.mf_obs['var_s1_hat'])

        f = function([], [scalar, batch] )

        res1, res2 = f()

        res2 = res2.mean(dtype='float64')

        print res1, res2

        assert np.allclose(res1, res2)



    def test_grad_alpha(self):
        """tests that the gradient of the log probability with respect to alpha
        matches my analytical derivation """

        #self.model.set_param_values(self.new_params)

        g = T.grad(self.prob, self.model.alpha, consider_constant = self.mf_obs.values())

        mu = self.model.mu
        alpha = self.model.alpha
        half = as_floatX(.5)

        mean_sq_s = self.stats.d['mean_sq_s']
        mean_hs = self.stats.d['mean_hs']
        mean_h = self.stats.d['mean_h']

        term1 = - half * mean_sq_s

        term2 = mu * mean_hs

        term3 = - half * T.sqr(mu) * mean_h

        term4 = half / alpha

        analytical = term1 + term2 + term3 + term4

        f = function([],(g,analytical))

        gv, av = f()

        assert gv.shape == av.shape

        max_diff = np.abs(gv-av).max()

        if max_diff > self.tol:
            print "gv"
            print gv
            print "av"
            print av
            raise Exception("analytical gradient on alpha deviates from theano gradient on alpha by up to "+str(max_diff))

    def test_grad_W(self):
        """tests that the gradient of the log probability with respect to W
        matches my analytical derivation """

        #self.model.set_param_values(self.new_params)

        g = T.grad(self.prob, self.model.W, consider_constant = self.mf_obs.values())

        B = self.model.B
        W = self.model.W
        mean_hsv = self.stats.d['mean_hsv']

        mean_sq_hs = self.stats.d['mean_sq_hs']

        mean_HS = self.mf_obs['H_hat'] * self.mf_obs['S_hat']

        m = mean_HS.shape[0]

        outer_prod = T.dot(mean_HS.T,mean_HS)
        outer_prod.name = 'outer_prod<from_observations>'
        outer = outer_prod/m
        mask = T.identity_like(outer)
        second_hs = (1.-mask) * outer + alloc_diag(mean_sq_hs)


        term1 = (B * mean_hsv).T
        term2 = - B.dimshuffle(0,'x') * T.dot(W, second_hs)

        analytical = term1 + term2

        f = function([],(g,analytical))

        gv, av = f()

        assert gv.shape == av.shape

        max_diff = np.abs(gv-av).max()

        if max_diff > self.tol:
            print "gv"
            print gv
            print "av"
            print av
            raise Exception("analytical gradient on W deviates from theano gradient on W by up to "+str(max_diff))


    def test_d_kl_d_h(self):

        "tests that the gradient of the kl with respect to h matches my analytical version of it "

        model = self.model
        ip = self.model.e_step
        X = self.X

        assert X.shape[0] == self.m

        H = np.cast[config.floatX](self.model.rng.uniform(0.001,.999,(self.m, self.N)))
        S = np.cast[config.floatX](self.model.rng.uniform(-5.,5.,(self.m, self.N)))

        H_var = T.matrix(name='H_var')
        H_var.tag.test_value = H
        S_var = T.matrix(name='S_var')
        S_var.tag.test_value = S


        sigma0 = ip.infer_var_s0_hat()
        Sigma1 = ip.infer_var_s1_hat()
        mu0 = T.zeros_like(model.mu)

        trunc_kl = ip.truncated_KL( V = X, obs = { 'H_hat' : H_var,
                                                 'S_hat' : S_var,
                                                 'var_s0_hat' : sigma0,
                                                 'var_s1_hat' : Sigma1 } ).sum()

        assert len(trunc_kl.type.broadcastable) == 0

        grad_H = T.grad(trunc_kl, H_var)

        grad_func = function([H_var, S_var], grad_H)

        grad_theano = grad_func(H,S)


        half = as_floatX(0.5)
        one = as_floatX(1.)
        two = as_floatX(2.)
        pi = as_floatX(np.pi)
        e = as_floatX(np.e)
        mu = self.model.mu
        alpha = self.model.alpha
        W = self.model.W
        B = self.model.B
        w = self.model.w

        term1 = T.log(H_var)
        term2 = -T.log(one - H_var)
        term3 = - half * T.log( Sigma1 *  two * pi * e )
        term4 = half * T.log(sigma0 *  two * pi * e )
        term5 = - self.model.bias_hid
        term6 = half * ( - sigma0 + Sigma1 + T.sqr(S_var) )
        term7 = - mu * alpha * S_var
        term8 = half * T.sqr(mu) * alpha
        term9 = - T.dot(X * self.model.B, self.model.W) * S_var
        term10 = S_var * T.dot(T.dot(H_var * S_var, W.T * B),W)
        term11 = - w * T.sqr(S_var) * H_var
        term12 = half * (Sigma1 + T.sqr(S_var)) * T.dot(B,T.sqr(W))

        analytical = term1 + term2 + term3 + term4 + term5 + term6 + term7 + term8 + term9 + term10 + term11 + term12

        grad_analytical = function([H_var, S_var], analytical)(H,S)

        if not np.allclose(grad_theano, grad_analytical):
            print 'grad theano: ',(grad_theano.min(), grad_theano.mean(), grad_theano.max())
            print 'grad analytical: ',(grad_analytical.min(), grad_analytical.mean(), grad_analytical.max())
            ad = np.abs(grad_theano-grad_analytical)
            print 'abs diff: ',(ad.min(),ad.mean(),ad.max())
            assert False

    def test_d_negent_d_h(self):

        "tests that the gradient of the negative entropy with respect to h matches my analytical version of it "

        model = self.model
        ip = self.model.e_step
        X = self.X

        assert X.shape[0] == self.m

        H = np.cast[config.floatX](self.model.rng.uniform(0.001,.999,(self.m, self.N)))
        S = np.cast[config.floatX](self.model.rng.uniform(-5.,5.,(self.m, self.N)))

        H_var = T.matrix(name='H_var')
        H_var.tag.test_value = H
        S_var = T.matrix(name='S_var')
        S_var.tag.test_value = S


        sigma0 = ip.infer_var_s0_hat()
        Sigma1 = ip.infer_var_s1_hat()
        mu0 = T.zeros_like(model.mu)

        negent = - self.model.entropy_hs( H_hat =  H_var,
                                                 var_s0_hat =  sigma0,
                                                 var_s1_hat = Sigma1 ).sum()

        assert len(negent.type.broadcastable) == 0

        grad_H = T.grad(negent, H_var)

        grad_func = function([H_var, S_var], grad_H, on_unused_input = 'ignore' )

        grad_theano = grad_func(H,S)


        half = as_floatX(0.5)
        one = as_floatX(1.)
        two = as_floatX(2.)
        pi = as_floatX(np.pi)
        e = as_floatX(np.e)
        mu = self.model.mu
        alpha = self.model.alpha
        W = self.model.W
        B = self.model.B
        w = self.model.w

        term1 = T.log(H_var)
        term2 = -T.log(one - H_var)
        term3 = - half * T.log( Sigma1 * two * pi * e )
        term4 = half * T.log(  sigma0 * two * pi * e )

        analytical = term1 + term2 + term3 + term4

        grad_analytical = function([H_var, S_var], analytical, on_unused_input = 'ignore')(H,S)

        if not np.allclose(grad_theano, grad_analytical):
            print 'grad theano: ',(grad_theano.min(), grad_theano.mean(), grad_theano.max())
            print 'grad analytical: ',(grad_analytical.min(), grad_analytical.mean(), grad_analytical.max())
            ad = np.abs(grad_theano-grad_analytical)
            print 'abs diff: ',(ad.min(),ad.mean(),ad.max())
            assert False

    def test_d_negent_h_d_h(self):

        "tests that the gradient of the negative entropy of h with respect to \hat{h} matches my analytical version of it "

        model = self.model
        ip = self.model.e_step
        X = self.X

        assert X.shape[0] == self.m

        H = np.cast[config.floatX](self.model.rng.uniform(0.001,.999,(self.m, self.N)))
        S = np.cast[config.floatX](self.model.rng.uniform(-5.,5.,(self.m, self.N)))

        H_var = T.matrix(name='H_var')
        H_var.tag.test_value = H
        S_var = T.matrix(name='S_var')
        S_var.tag.test_value = S


        sigma0 = ip.infer_var_s0_hat()
        Sigma1 = ip.infer_var_s1_hat()
        mu0 = T.zeros_like(model.mu)

        negent = - self.model.entropy_h( H_hat =  H_var  ).sum()

        assert len(negent.type.broadcastable) == 0

        grad_H = T.grad(negent, H_var)

        grad_func = function([H_var, S_var], grad_H, on_unused_input = 'ignore')

        grad_theano = grad_func(H,S)


        half = as_floatX(0.5)
        one = as_floatX(1.)
        two = as_floatX(2.)
        pi = as_floatX(np.pi)
        e = as_floatX(np.e)
        mu = self.model.mu
        alpha = self.model.alpha
        W = self.model.W
        B = self.model.B
        w = self.model.w

        term1 = T.log(H_var)
        term2 = -T.log(one - H_var)

        analytical = term1 + term2

        grad_analytical = function([H_var, S_var], analytical, on_unused_input = 'ignore')(H,S)

        if not np.allclose(grad_theano, grad_analytical):
            print 'grad theano: ',(grad_theano.min(), grad_theano.mean(), grad_theano.max())
            print 'grad analytical: ',(grad_analytical.min(), grad_analytical.mean(), grad_analytical.max())
            ad = np.abs(grad_theano-grad_analytical)
            print 'abs diff: ',(ad.min(),ad.mean(),ad.max())
            assert False


    def test_d_ee_d_h(self):

        "tests that the gradient of the expected energy with respect to h matches my analytical version of it "

        model = self.model
        ip = self.model.e_step
        X = self.X

        assert X.shape[0] == self.m

        H = np.cast[config.floatX](self.model.rng.uniform(0.001,.999,(self.m, self.N)))
        S = np.cast[config.floatX](self.model.rng.uniform(-5.,5.,(self.m, self.N)))

        H_var = T.matrix(name='H_var')
        H_var.tag.test_value = H
        S_var = T.matrix(name='S_var')
        S_var.tag.test_value = S


        sigma0 = ip.infer_var_s0_hat()
        Sigma1 = ip.infer_var_s1_hat()
        mu0 = T.zeros_like(model.mu)

        ee = self.model.expected_energy_vhs( V = X, H_hat = H_var,
                                                 S_hat =  S_var,
                                                 var_s0_hat = sigma0,
                                                 var_s1_hat = Sigma1 ).sum()

        assert len(ee.type.broadcastable) == 0

        grad_H = T.grad(ee, H_var)

        grad_func = function([H_var, S_var], grad_H)

        grad_theano = grad_func(H,S)


        half = as_floatX(0.5)
        one = as_floatX(1.)
        two = as_floatX(2.)
        pi = as_floatX(np.pi)
        e = as_floatX(np.e)
        mu = self.model.mu
        alpha = self.model.alpha
        W = self.model.W
        B = self.model.B
        w = self.model.w

        term1 = - self.model.bias_hid
        term2 = half * ( - sigma0 + Sigma1 + T.sqr(S_var) )
        term3 = - mu * alpha * S_var
        term4 = half * T.sqr(mu) * alpha
        term5 = - T.dot(X * self.model.B, self.model.W) * S_var
        term6 = S_var * T.dot(T.dot(H_var * S_var, W.T * B),W)
        term7 = - w * T.sqr(S_var) * H_var
        term8 = half * (Sigma1 + T.sqr(S_var)) * T.dot(B,T.sqr(W))

        analytical = term1 + term2 + term3 + term4 + term5 + term6 + term7 + term8

        grad_analytical = function([H_var, S_var], analytical, on_unused_input = 'ignore')(H,S)

        if not np.allclose(grad_theano, grad_analytical):
            print 'grad theano: ',(grad_theano.min(), grad_theano.mean(), grad_theano.max())
            print 'grad analytical: ',(grad_analytical.min(), grad_analytical.mean(), grad_analytical.max())
            ad = np.abs(grad_theano-grad_analytical)
            print 'abs diff: ',(ad.min(),ad.mean(),ad.max())
            assert False

if __name__ == '__main__':
    obj = TestS3C_Misc()
    obj.test_d_ee_d_h()

########NEW FILE########
__FILENAME__ = test_svm
from pylearn2.datasets.mnist import MNIST
from pylearn2.testing.skip import skip_if_no_sklearn
import numpy as np
import unittest
DenseMulticlassSVM = None

class TestSVM(unittest.TestCase):
    def setUp(self):
        global DenseMulticlassSVM
        skip_if_no_sklearn()
        import pylearn2.models.svm
        DenseMulticlassSVM = pylearn2.models.svm.DenseMulticlassSVM


    def test_decision_function(self):
        dataset = MNIST(which_set = 'train')

        X = dataset.X[0:20,:]
        y = dataset.y[0:20]

        for i in xrange(10):
            assert (y == i).sum() > 0

        model = DenseMulticlassSVM(kernel = 'poly', C = 1.0).fit(X,y)

        f = model.decision_function(X)

        print f

        yhat_f = np.argmax(f,axis=1)

        yhat = np.cast[yhat_f.dtype](model.predict(X))

        print yhat_f
        print yhat

        assert (yhat_f != yhat).sum() == 0

########NEW FILE########
__FILENAME__ = test_windowlayer
"""
Test for WindowLayer
"""
__authors__ = "Axel Davy"
__copyright__ = "Copyright 2014, Universite de Montreal"
__credits__ = ["Axel Davy"]
__license__ = "3-clause BSD"
__maintainer__ = "Axel Davy"

import numpy as np
import theano
from pylearn2.models.mlp import MLP, WindowLayer
from pylearn2.space import Conv2DSpace


def build_mlp_fn(x0, y0, x1, y1, s0, s1, c, axes):
    """
    Creates an theano function to test the WindowLayer

    Parameters
    ----------
    x0: x coordinate of the left of the window
    y0: y coordinate of the top of the window
    x1: x coordinate of the right of the window
    y1: y coordinate of the bottom of the window
    s0: x shape of the images of the input space
    s1: y shape of the images of the input space
    c: number of channels of the input space
    axes: description of the axes of the input space

    Returns
    -------
    f: a theano function applicating the window layer
    of window (x0, y0, x1, y1).
    """
    mlp = MLP(layers=[WindowLayer('h0', window=(x0, y0, x1, y1))],
              input_space=Conv2DSpace(shape=(s0, s1),
                                      num_channels=c, axes=axes))
    X = mlp.get_input_space().make_batch_theano()
    f = theano.function([X], mlp.fprop(X))
    return f


def test_windowlayer():
    """
    Tests that WindowLayer reacts correctly to the error in window
    settings, and that the window gives the correct results.

    Parameters
    ----------
    No Parameter
    """
    np.testing.assert_raises(ValueError, build_mlp_fn,
                             0, 0, 20, 20, 20, 20, 3, ('c', 0, 1, 'b'))
    np.testing.assert_raises(ValueError, build_mlp_fn,
                             -1, -1, 19, 19, 20, 20, 3, ('c', 0, 1, 'b'))
    fprop = build_mlp_fn(5, 5, 10, 15, 20, 20, 2, ('b', 'c', 0, 1))
    n = np.random.rand(3, 2, 20, 20).astype(theano.config.floatX)
    r = fprop(n)
    assert r.shape == (3, 2, 6, 11)
    assert r[0, 0, 0, 0] == n[0, 0, 5, 5]
    assert r[2, 1, 5, 10] == n[2, 1, 10, 15]
    assert r[1, 1, 3, 3] == n[1, 1, 8, 8]

if __name__ == "__main__":
    test_windowlayer()

########NEW FILE########
__FILENAME__ = model_extension
"""
Base class for model extensions
"""


class ModelExtension(object):
    """
    An object that may be plugged into a model to add some functionality
    to it.
    """

    def post_modify_updates(self, updates):
        """"
        Modifies the parameters before a learning update is applied.
        This method acts *after* the model subclass' _modify_updates
        method and any ModelExtensions that come earlier in the
        extensions list.

        Parameters
        ----------
        updates : dict
            A dictionary mapping shared variables to symbolic values they
            will be updated to.
        """

        pass

########NEW FILE########
__FILENAME__ = monitor
"""
The module defining the Monitor and MonitorChannel objects used for
tracking the changes in values of various quantities throughout training
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

import copy, time, warnings, logging
import numpy as np

from theano.compat.python2x import OrderedDict
import theano.sparse
from theano import config
from theano import tensor as T
from theano.printing import var_descriptor

from pylearn2.config import yaml_parse
from pylearn2.datasets.dataset import Dataset
from pylearn2.space import Space, CompositeSpace, NullSpace
from pylearn2.utils import function, sharedX, safe_zip, safe_izip
from pylearn2.utils.iteration import is_stochastic
from pylearn2.utils.data_specs import DataSpecsMapping
from pylearn2.utils.string_utils import number_aware_alphabetical_key
from pylearn2.utils.timing import log_timing

log = logging.getLogger(__name__)


class Monitor(object):
    """
    A class for monitoring Models while they are being trained.

    A monitor object records the number of minibatches and number of
    examples the model has trained, as well as any number of "channels"
    that track quantities of interest (examples: the objective
    function, measures of hidden unit activity, reconstruction error,
    sum of squared second derivatives, average norm of the weight
    vectors, etc.)

    Parameters
    ----------
    model : `pylearn2.models.model.Model`
    """

    def __init__(self, model):
        self.training_succeeded = False
        self.model = model
        self.channels = OrderedDict()
        self._num_batches_seen = 0
        self._examples_seen = 0
        self._epochs_seen = 0
        self._datasets = []
        self._iteration_mode = []
        self._batch_size = []
        self._num_batches = []
        self._dirty = True
        self._rng_seed = []
        self.names_to_del = ['theano_function_mode']
        self.t0 = time.time()
        self.theano_function_mode = None

        # Initialize self._nested_data_specs, self._data_specs_mapping,
        # and self._flat_data_specs
        self._build_data_specs()

    def _build_data_specs(self):
        """
        Computes a nested data_specs for input and all channels

        Also computes the mapping to flatten it. This function is
        called from redo_theano.
        """
        # Ask the model what it needs
        m_space, m_source = self.model.get_monitoring_data_specs()
        input_spaces = [m_space]
        input_sources = [m_source]
        for channel in self.channels.values():
            space = channel.data_specs[0]
            assert isinstance(space, Space)
            input_spaces.append(space)
            input_sources.append(channel.data_specs[1])

        nested_space = CompositeSpace(input_spaces)
        nested_source = tuple(input_sources)

        self._nested_data_specs = (nested_space, nested_source)
        self._data_specs_mapping = DataSpecsMapping(self._nested_data_specs)

        flat_space = self._data_specs_mapping.flatten(nested_space,
                                                      return_tuple=True)
        flat_source = self._data_specs_mapping.flatten(nested_source,
                                                       return_tuple=True)
        self._flat_data_specs = (CompositeSpace(flat_space), flat_source)

    def set_theano_function_mode(self, mode):
        """
        .. todo::

            WRITEME

        Parameters
        ----------
        mode : theano.compile.Mode
            Theano functions for the monitoring channels will be
            compiled and run using this mode.
        """
        if self.theano_function_mode != mode:
            self._dirty = True
            self.theano_function_mode = mode

    def add_dataset(self, dataset, mode='sequential', batch_size=None,
                    num_batches=None, seed=None):
        """
        Determines the data used to calculate the values of each channel.

        Parameters
        ----------
        dataset : object
            A `pylearn2.datasets.Dataset` object.
        mode : str or object, optional
            Iteration mode; see the docstring of the `iterator` method
            on `pylearn2.datasets.Dataset` for details.
        batch_size : int, optional
            The size of an individual batch. Optional if `mode` is
            'sequential' and `num_batches` is specified (batch size
            will be calculated based on full dataset size).
        num_batches : int, optional
            The total number of batches. Unnecessary if `mode` is
            'sequential' and `batch_size` is specified (number of
            batches will be calculated based on full dataset size).
        seed : int, optional
            Optional. The seed to be used for random iteration modes.
        """
        # The user can ommit using lists if only one dataset is set
        if not isinstance(dataset, list):
            dataset = [dataset]
        if not isinstance(mode, list):
            mode = [mode]
        if not isinstance(batch_size, list):
            batch_size = [batch_size]
        if not isinstance(num_batches, list):
            num_batches = [num_batches]
        if seed is None:
            seed = [None] * len(dataset)
        if not isinstance(seed, list):
            seed = [seed]
        if len(mode) != len(dataset):
            raise ValueError("Received " + str(len(dataset)) +
                             " dataset but " + str(len(mode)) + " modes.")
        if any([len(l) != len(dataset) for l in [batch_size, seed]]):
            raise ValueError("make sure each dataset has its iteration " +
                             "batch size and number of batches.")
        for (d, m, b, n, sd) in safe_izip(dataset, mode, batch_size,
                                          num_batches, seed):
            try:
                it = d.iterator(mode=m,
                                batch_size=b,
                                num_batches=n,
                                data_specs=self._flat_data_specs,
                                return_tuple=True,
                                rng=sd)
            except ValueError as exc:
                raise ValueError("invalid iteration parameters in " +
                                 "Monitor.add_dataset: " + str(exc))
            if it.stochastic:
                # Must be a seed, not a random number generator. If it were a
                # random number generator, different iterators using it would
                # update its state, so we would not get the same iterator
                # each time. Also, must not be None, because this makes the
                # iterator pick a seed based on the clock
                if sd is None:
                    raise TypeError("Monitor requires a seed when using " +
                                    "stochastic iteration modes.")
                if not isinstance(sd, (list, tuple, int)):
                    raise TypeError("Monitor requires a seed (not a random " +
                                    "number generator) when using " +
                                    "stochastic iteration modes.")
            else:
                # The iterator should catch this, but let's double-check
                assert sd is None

            if not d in self._datasets:
                self._datasets.append(d)
                self._iteration_mode.append(m)
                self._batch_size.append(b)
                self._num_batches.append(n)
                self._rng_seed.append(sd)

    def __call__(self):
        """
        Runs the model on the monitoring dataset in order to add one
        data point to each of the channels.
        """

        # If the channels have changed at all, we need to recompile the theano
        # functions used to compute them
        if self._dirty:
            self.redo_theano()

        datasets = self._datasets

        # Set all channels' val_shared to 0
        self.begin_record_entry()
        for d, i, b, n, a, sd, ne in safe_izip(datasets,
                                               self._iteration_mode,
                                               self._batch_size,
                                               self._num_batches,
                                               self.accum,
                                               self._rng_seed,
                                               self.num_examples):
            if isinstance(d, basestring):
                d = yaml_parse.load(d)
                raise NotImplementedError()

            # need to put d back into self._datasets
            myiterator = d.iterator(mode=i,
                                    batch_size=b,
                                    num_batches=n,
                                    data_specs=self._flat_data_specs,
                                    return_tuple=True,
                                    rng=sd)

            # If self._flat_data_specs is empty, no channel needs data,
            # so we do not need to call the iterator in order to average
            # the monitored values across different batches, we only
            # have to call them once.
            if len(self._flat_data_specs[1]) == 0:
                X = ()
                self.run_prereqs(X, d)
                a(*X)

            else:
                actual_ne = 0
                for X in myiterator:
                    # X is a flat (not nested) tuple
                    self.run_prereqs(X, d)
                    a(*X)
                    actual_ne += self._flat_data_specs[0].np_batch_size(X)
                # end for X
                if actual_ne != ne:
                    raise RuntimeError("At compile time, your iterator said "
                                       "it had %d examples total, but at "
                                       "runtime it gave us %d." %
                                       (ne, actual_ne))
        # end for d

        log.info("Monitoring step:")
        log.info("\tEpochs seen: %d" % self._epochs_seen)
        log.info("\tBatches seen: %d" % self._num_batches_seen)
        log.info("\tExamples seen: %d" % self._examples_seen)
        t = time.time() - self.t0
        for channel_name in sorted(self.channels.keys(),
                                   key=number_aware_alphabetical_key):
            channel = self.channels[channel_name]
            channel.time_record.append(t)
            channel.batch_record.append(self._num_batches_seen)
            channel.example_record.append(self._examples_seen)
            channel.epoch_record.append(self._epochs_seen)
            val = channel.val_shared.get_value()
            channel.val_record.append(val)
            # TODO: use logging infrastructure so that user can configure
            # formatting
            if abs(val) < 1e4:
                val_str = str(val)
            else:
                val_str = '%.3e' % val

            log.info("\t%s: %s" % (channel_name, val_str))

    def run_prereqs(self, data, dataset):
        """
        Runs all "prerequistie functions" on a batch of data. Always
        called right before computing the monitoring channels on that
        batch.

        Parameters
        ----------
        data : tuple or Variable
            a member of the Space used as input to the monitoring
            functions
        dataset : Dataset
            the Dataset the data was drawn from
        """
        if dataset not in self.prereqs:
            return
        for prereq in self.prereqs[dataset]:
            prereq(*data)

    def get_batches_seen(self):
        """
        Returns the number of batches the model has learned on
        (assuming that the learning code has been calling
        Monitor.report_batch correctly).
        """
        return self._num_batches_seen

    def get_epochs_seen(self):
        """
        .. todo::

            WRITEME

        Returns
        -------
        epochs_seen : int
            The number of epochs the model has been trained on.
            One "epoch" is one pass through Dataset.iterator.
        """
        return self._epochs_seen

    def get_examples_seen(self):
        """
        .. todo::

            WRITEME

        Returns
        -------
        examples_seen : int
            The number of examples the model has learned on (assuming
            that the learning code has been calling Monitor.report_batch
            correctly)
        """
        return self._examples_seen

    def report_batch(self, num_examples):
        """
        Call this whenever the model has learned on another batch of
        examples. Report how many examples were learned on.

        Parameters
        ----------
        num_examples : int
            The number of examples learned on in this minibatch.
        """
        self._examples_seen += num_examples
        self._num_batches_seen += 1

    def report_epoch(self):
        """
        Call this whenever the model has completed another "epoch" of
        learning. We regard one pass through Dataset.iterator as one
        epoch.
        """
        self._epochs_seen += 1

    def redo_theano(self):
        """
        Recompiles Theano functions used by this monitor.

        This is called any time we need to evaluate the channels and
        the channel definitions have changed since last we called it,
        or if the theano functions are unavailable for any other reason
        (first time they are needed after construction or
        deserialization, etc.)

        All channels are compiled as part of the same theano function
        so that the theano optimizations can eliminate subexpressions
        that are shared between multiple channels.
        """
        self._dirty = False

        # Recompute the data specs, since the channels may have changed.
        self._build_data_specs()

        init_names = dir(self)
        self.prereqs = OrderedDict()
        for channel in self.channels.values():
            if channel.prereqs is not None:
                dataset = channel.dataset
                if dataset not in self.prereqs:
                    self.prereqs[dataset] = []
                prereqs = self.prereqs[dataset]
                for prereq in channel.prereqs:
                    if prereq not in prereqs:
                        prereqs.append(prereq)

        updates = OrderedDict()
        for channel in self.channels.values():
            updates[channel.val_shared] = np.cast[config.floatX](0.0)
        with log_timing(log, "compiling begin_record_entry"):
            self.begin_record_entry = function(
                inputs=[],
                updates=updates,
                mode=self.theano_function_mode,
                name='Monitor.begin_record_entry'
            )
        updates = OrderedDict()
        givens = OrderedDict()
        # Get the appropriate kind of theano variable to represent the data
        # the model acts on
        batch_names = ['monitoring_%s' % s for s in self._flat_data_specs[1]]
        theano_args = self._flat_data_specs[0].make_theano_batch(batch_names)

        # Get a symbolic expression of the batch size
        # We do it here, rather than for each channel, because channels with an
        # empty data_specs do not use data, and are unable to extract the batch
        # size. The case where the whole data specs is empty is not supported.
        batch_size = self._flat_data_specs[0].batch_size(theano_args)

        # Also get a nested representation, for joint iteration
        # with each of channel.graph_input
        nested_theano_args = self._data_specs_mapping.nest(theano_args)
        if not isinstance(nested_theano_args, tuple):
            nested_theano_args = (nested_theano_args,)
        assert len(nested_theano_args) == (len(self.channels) + 1)

        log.info('Monitored channels: ')
        for key in sorted(self.channels.keys()):
            mode = self.theano_function_mode
            if mode is not None and hasattr(mode, 'record'):
                mode.record.handle_line('compiling monitor including ' +
                                        'channel ' + key + '\n')
            log.info('\t%s' % key)
        it = [d.iterator(mode=i, num_batches=n, batch_size=b,
                         data_specs=self._flat_data_specs,
                         return_tuple=True)
              for d, i, n, b in safe_izip(self._datasets, self._iteration_mode,
                                          self._num_batches, self._batch_size)]
        self.num_examples = [np.cast[config.floatX](float(i.num_examples))
                             for i in it]
        givens = [OrderedDict() for d in self._datasets]
        updates = [OrderedDict() for d in self._datasets]
        for i, channel in enumerate(self.channels.values()):
            index = self._datasets.index(channel.dataset)
            d = self._datasets[index]
            g = givens[index]
            cur_num_examples = self.num_examples[index]
            u = updates[index]

            # Flatten channel.graph_input and the appropriate part of
            # nested_theano_args, to iterate jointly over them.
            c_mapping = DataSpecsMapping(channel.data_specs)
            channel_inputs = c_mapping.flatten(channel.graph_input,
                                               return_tuple=True)
            inputs = c_mapping.flatten(nested_theano_args[i + 1],
                                       return_tuple=True)

            for (channel_X, X) in safe_izip(channel_inputs, inputs):
                assert channel_X not in g or g[channel_X] is X
                assert channel_X.type == X.type, (channel_X.type, X.type)
                g[channel_X] = X

            if batch_size == 0:
                # No channel does need any data, so there is not need to
                # average results, and we will call the accum functions only
                # once.
                # TODO: better handling of channels not needing data when
                # some other channels need data.
                assert len(self._flat_data_specs[1]) == 0
                val = channel.val
            else:
                if n == 0:
                    raise ValueError("Iterating over 0 examples results in " +
                                     "divide by 0")
                val = (channel.val * T.cast(batch_size, config.floatX)
                       / cur_num_examples)
            u[channel.val_shared] = channel.val_shared + val

        with log_timing(log, "Compiling accum"):
            # Check type of update expressions
            for up in updates:
                for key in up:
                    if key.dtype != up[key].dtype:
                        raise TypeError('Monitoring channel shared variable ' +
                                        key.name + ' has dtype ' + key.dtype +
                                        ' but is driven by an expression ' +
                                        'with type ' + up[key].dtype)

            self.accum = []
            for idx, packed in enumerate(safe_izip(givens, updates)):
                g, u = packed
                mode = self.theano_function_mode
                if mode is not None and hasattr(mode, 'record'):
                    for elem in g:
                        mode.record.handle_line('g key ' +
                                                var_descriptor(elem) + '\n')
                        mode.record.handle_line('g val ' +
                                                var_descriptor(g[elem]) + '\n')
                    for elem in u:
                        mode.record.handle_line('u key ' +
                                                var_descriptor(elem) + '\n')
                        mode.record.handle_line('u val ' +
                                                var_descriptor(u[elem]) + '\n')
                function_name = 'Monitor.accum[%d]' % idx
                if mode is not None and hasattr(mode, 'record'):
                    mode.record.handle_line('compiling supervised accum\n')
                # Some channels may not depend on the data, ie, they might just
                # monitor the model parameters, or some shared variable updated
                # by the training algorithm, so we need to ignore the unused
                # input error
                self.accum.append(function(theano_args,
                                           givens=g,
                                           updates=u,
                                           mode=self.theano_function_mode,
                                           name=function_name))
            for a in self.accum:
                if mode is not None and hasattr(mode, 'record'):
                    for elem in a.maker.fgraph.outputs:
                        mode.record.handle_line('accum output ' +
                                                var_descriptor(elem) + '\n')
                log.info("graph size: %d" % len(a.maker.fgraph.toposort()))
        final_names = dir(self)
        self.register_names_to_del([name for name in final_names
                                    if name not in init_names])

    def register_names_to_del(self, names):
        """
        Register names of fields that should be deleted before pickling.

        Parameters
        ----------
        names : list
            A list of attribute names as strings.
        """
        for name in names:
            if name not in self.names_to_del:
                self.names_to_del.append(name)

    def __getstate__(self):
        """
        In order to avoid pickling a copy of the dataset whenever a
        monitor is saved, the __getstate__ method replaces the dataset
        field with the dataset's yaml source. This is not a perfect
        solution because it won't work with job resuming, which would
        require saving the state of the dataset's random number
        generator.

        Like in the Model class, we also need to avoid saving any
        Theano functions, so we delete everything that can be
        regenerated with `redo_theano` by deleting the fields in
        `self.names_to_del`
        """

        # Patch old pickled monitors
        if not hasattr(self, '_datasets'):
            self._datasets = [self._dataset]
            del self._dataset

        temp = self._datasets

        if self._datasets:
            self._datasets = []
            for dataset in temp:
                if isinstance(dataset, basestring):
                    self._datasets.append(dataset)
                else:
                    try:
                        self._datasets.append(dataset.yaml_src)
                    except AttributeError:
                        warnings.warn('Trained model saved without ' +
                                      'indicating yaml_src')
        d = copy.copy(self.__dict__)
        self._datasets = temp
        for name in self.names_to_del:
            if name in d:
                del d[name]

        return d

    def __setstate__(self, d):
        """
        Sets the object to have the state described by `d`.

        Parameters
        ----------
        d : dict
            A dictionary mapping string names of fields to values for
            these fields.
        """
        # patch old pkl files
        if '_dataset' in d:
            d['_datasets'] = [d['_dataset']]
            del d['_dataset']

        self.__dict__.update(d)

    def add_channel(self, name, ipt, val, dataset=None, prereqs=None,
                    data_specs=None):
        """
        Asks the monitor to start tracking a new value.  Can be called
        even after the monitor is already in use.

        Parameters
        ----------
        name : str
            The display name in the monitor.
        ipt : tensor_like
            The symbolic tensor which should be clamped to the data.
            (or a list/tuple containing symbolic tensors, following the
            data_specs)
        val : tensor_like
            The value (function of `ipt`) to be tracked.
        dataset : pylearn2.datasets.Dataset
            Which dataset to compute this channel on
        prereqs : list of callables that take a list of numpy tensors
            Each prereq must be called exactly once per each new batch
            of data drawn *from dataset* before the channel value is
            computed if two channels provide a prereq with exactly the
            same id, that prereq will only be called once
        data_specs : (space, source) pair
            Identifies the order, format and semantics of ipt
        """
        if isinstance(val, (float, int, long)):
            val = np.cast[theano.config.floatX](val)

        val = T.as_tensor_variable(val)

        if data_specs is None:
            warnings.warn("parameter 'data_specs' should be provided when " +
                          "calling add_channel. We will build a default one.",
                          stacklevel=2)
            if isinstance(ipt, list):
                ipt = tuple(ipt)
            if ipt is not None and not isinstance(ipt, tuple):
                ipt = (ipt,)

            if ipt is None:
                data_specs = (NullSpace(), '')
            elif len(ipt) == 0:
                data_specs = (CompositeSpace([]), ())
            elif hasattr(dataset, 'get_data_specs'):
                dataset_space, dataset_source = dataset.get_data_specs()
                if (len(ipt) == 1 and
                        dataset_source is not None and
                        (not isinstance(dataset_source, tuple) or
                            len(dataset_source) == 1) and
                        'features' in dataset_source):
                    data_specs = (dataset_space, dataset_source)
                elif (len(ipt) == 2 and
                        dataset_source == ('features', 'targets')):
                    data_specs = (dataset_space, dataset_source)
                else:
                    raise ValueError("Cannot infer default data_specs for " +
                                     "the following input points and " +
                                     "dataset: ipt = %s, dataset = %s"
                                     % (ipt, dataset))

        data_specs[0].validate(ipt)

        mapping = DataSpecsMapping(data_specs)
        flat_ipt = mapping.flatten(ipt)
        if not isinstance(flat_ipt, tuple):
            flat_ipt = (flat_ipt,)
        inputs = theano.gof.graph.inputs([val])
        for elem in inputs:
            if not hasattr(elem, 'get_value') and \
               not isinstance(elem, theano.gof.graph.Constant):
                if elem not in flat_ipt:
                    raise ValueError("Unspecified input: " + str(elem) +
                                     ". This may be due to an incorrect " +
                                     "implementation of a cost's " +
                                     "get_data_specs() method, or of a " +
                                     "model's get_monitoring_data_specs() " +
                                     "method.")

        mode = self.theano_function_mode
        if mode is not None and hasattr(mode, 'record'):
            mode.record.handle_line('Adding monitor channel '+name+'\n')
            assert isinstance(flat_ipt, tuple)
            if len(flat_ipt) != 1:
                for elem in flat_ipt:
                    mode.record.handle_line('Includes input var ' +
                                            var_descriptor(elem) + '\n')
            else:
                mode.record.handle_line(name + ' input var is ' +
                                        var_descriptor(flat_ipt[0]) + '\n')
            mode.record.handle_line('channel ' + name + ' is ' +
                                    var_descriptor(val) + '\n')

        if dataset is None:
            if len(self._datasets) == 1:
                dataset = self._datasets[0]
            elif len(self._datasets) == 0:
                raise ValueError(_err_no_data)
            else:
                raise ValueError(_err_ambig_data)

        try:
            self._datasets.index(dataset)
        except ValueError:
            raise ValueError("The dataset specified is not one of the " +
                             "monitor's datasets")

        if name in self.channels:
            raise ValueError("Tried to create the same channel twice (%s)" %
                             name)

        self.channels[name] = MonitorChannel(ipt, val, name, data_specs,
                                             dataset, prereqs)
        self._dirty = True

    def _sanity_check(self):
        """
        Sometimes we serialize models and then load them somewhere else
        but still try to use their Monitor, and the Monitor is in a
        mangled state. I've added some calls to _sanity_check to try to
        catch when that happens. Not sure what to do for a long term
        fix. I think it requires making theano graphs serializable
        first.
        """
        for name in self.channels:
            channel = self.channels[name]
            assert hasattr(channel, 'prereqs')

    @classmethod
    def get_monitor(cls, model):
        """
        Returns a model's monitor. If the model doesn't have a monitor
        yet, installs one and returns that.

        Parameters
        ----------
        model : object
            An object that implements the `Model` interface specified
            in `pylearn2.models`.
        """

        if hasattr(model, 'monitor'):
            rval = model.monitor
            rval._sanity_check()
        else:
            rval = Monitor(model)
            model.monitor = rval

        return rval

    # TODO: find out if this method is used anywhere, remove if not.
    @property
    def batch_size(self):
        """
        .. todo::

            WRITEME

        Returns
        -------
        batch_size : int
            The size of the batches used for monitoring
        """
        return self._batch_size

    # TODO: find out if this method is used anywhere, remove if not.
    @property
    def num_batches(self):
        """
        .. todo::

            WRITEME

        Returns
        -------
        num_batches : int
            The number of batches used for monitoring
        """
        return self._num_batches

    def setup(self, dataset, cost, batch_size, num_batches=None,
              extra_costs=None, mode='sequential', obj_prereqs=None,
              cost_monitoring_args=None):
        """
        Sets up the monitor for a cost minimization problem.
        Adds channels defined by both the model and the cost for
        the specified dataset(s), as well as a channel called
        'objective' defined by the costs' __call__ method.

        Parameters
        ----------
        dataset : pylearn2.datasets.Dataset
            Dataset or dictionary mapping string names to Datasets.
            If string names are used, then for every dataset, each
            channel defined by the model or cost will be replicated
            with that dataset's name followed by an underscore as the
            prefix. For example, if your cost defines a channel called
            'misclass', and datasets is
            {'train' : train_dataset, 'valid' : valid_dataset},
            you will get channels called 'train_misclass' and
            'valid_misclass'.
        cost : pylearn2.costs.Cost
            The cost being optimized by training. The value of the cost
            will appear as the `objective` channel. Its
            `get_monitoring_channels` method will also be used to
            supply other channels.
        extra_costs : OrderedDict, optional
            A dictionary mapping channel names to Cost objects.
            Their value will appear as the specified channel name.
            They will also provide more monitoring channels via their
            `get_monitoring_channels` method.
        obj_prereqs : None, or list of functions
            Functions to pass as prerequisites to the `objective` channel.
        cost_monitoring_args : dict
            Dictionary of kwargs that will be passed to
            `cost.get_monitoring_channels()`
            (but not for the extra_costs).
        """

        if dataset is None:
            return
        if isinstance(dataset, Dataset):
            dataset = {'': dataset}
        else:
            assert isinstance(dataset, dict)
            assert all(isinstance(key, str) for key in dataset)
            assert all(isinstance(dataset[key], Dataset) for key in dataset)

        if extra_costs is None:
            costs = {}
        else:
            costs = extra_costs
        assert '' not in costs
        costs[''] = cost

        if cost_monitoring_args is None:
            cost_monitoring_args = {}

        model = self.model

        # Build a composite data_specs containing the specs for all costs,
        # then the specs of the model
        cost_names = sorted(costs.keys())
        spaces = []
        sources = []
        for c in cost_names:
            c_space, c_source = costs[c].get_data_specs(model)
            spaces.append(c_space)
            sources.append(c_source)

        # Ask the model for the data_specs needed
        m_space, m_source = model.get_monitoring_data_specs()
        spaces.append(m_space)
        sources.append(m_source)

        nested_space = CompositeSpace(spaces)
        nested_sources = tuple(sources)

        # Flatten this data_specs, so we build only one symbolic Theano
        # variable for each of the unique (space, source) pairs.
        mapping = DataSpecsMapping((nested_space, nested_sources))
        space_tuple = mapping.flatten(nested_space, return_tuple=True)
        source_tuple = mapping.flatten(nested_sources, return_tuple=True)
        ipt = tuple(space.make_theano_batch(name='monitor_%s' % source,
                                            batch_size=None)
                    for (space, source) in safe_zip(space_tuple, source_tuple))

        # Build a nested tuple from ipt, to dispatch the appropriate parts
        # of the ipt batch to each cost
        nested_ipt = mapping.nest(ipt)

        custom_channels = {}
        for i, cost_name in enumerate(cost_names):
            if cost_name == '':
                prefix = ''
            else:
                prefix = cost_name + '_'
            cost = costs[cost_name]
            cost_ipt = nested_ipt[i]
            raw_channels = cost.get_monitoring_channels(model, cost_ipt)
            channels = {}
            for name in raw_channels:
                # We need three things: the value itself (raw_channels[name]),
                # the input variables (cost_ipt), and the data_specs for
                # these input variables ((spaces[i], sources[i]))
                channels[prefix + name] = (raw_channels[name],
                                           cost_ipt,
                                           (spaces[i], sources[i]))
            custom_channels.update(channels)

        # Use the last inputs from nested_ipt for the model
        model_channels = model.get_monitoring_channels(nested_ipt[-1])
        channels = {}
        for name in model_channels:
            # Note: some code used to consider that model_channels[name]
            # could be a a (channel, prereqs) pair, this is not supported.
            channels[name] = (model_channels[name],
                              nested_ipt[-1],
                              (spaces[-1], sources[-1]))
        custom_channels.update(channels)

        if is_stochastic(mode):
            seed = [[2013, 02, 22]]
        else:
            seed = None

        for dataset_name in dataset:
            cur_dataset = dataset[dataset_name]
            self.add_dataset(dataset=cur_dataset,
                             mode=mode,
                             batch_size=batch_size,
                             num_batches=num_batches,
                             seed=seed)
            if dataset_name == '':
                dprefix = ''
            else:
                dprefix = dataset_name + '_'
            # These channel name 'objective' must not vary, since callbacks
            # that respond to the values in the monitor use the name to find
            # it.
            for i, cost_name in enumerate(cost_names):
                cost = costs[cost_name]
                cost_ipt = nested_ipt[i]
                cost_value = cost.expr(model, cost_ipt)
                if cost_value is not None:
                    if cost_name == '':
                        name = dprefix + 'objective'
                        prereqs = obj_prereqs
                    else:
                        name = dprefix + cost_name
                        prereqs = None

                    cost.get_data_specs(model)[0].validate(cost_ipt)
                    self.add_channel(name=name,
                                     ipt=cost_ipt,
                                     val=cost_value,
                                     data_specs=cost.get_data_specs(model),
                                     dataset=cur_dataset,
                                     prereqs=prereqs)

            for key in custom_channels:
                val, ipt, data_specs = custom_channels[key]
                data_specs[0].validate(ipt)
                self.add_channel(name=dprefix + key,
                                 ipt=ipt,
                                 val=val,
                                 data_specs=data_specs,
                                 dataset=cur_dataset)


class MonitorChannel(object):
    """
    A class representing a specific quantity to be monitored.

    Parameters
    ----------
    graph_input : tensor_like
        The symbolic tensor which should be clamped to the data.
    val : tensor_like
        The value (symbolic function of `graph_input`) to be evaluated
        and recorded.
    name : str
        The display name in the monitor.
    data_specs : (space, source) pair
        Identifies the order, format and semantics of graph_input
    prereqs : list of callables
        Callables that take numpy tensors each prereq must be called
        exactly once per each new batch of data before the channel
        value is computed if two channels provide a prereq with exactly
        the same id, that prereq will only be called once
    """

    def __init__(self, graph_input, val, name, data_specs, dataset,
                 prereqs=None):
        self.name = name
        self.prereqs = prereqs
        self.graph_input = graph_input
        self.data_specs = data_specs
        if isinstance(val, float):
            val = T.constant(np.cast[config.floatX](val))
        self.val = val
        self.val_shared = sharedX(0.0, name + "_tracker")
        assert self.val_shared.dtype == config.floatX, \
            "expected %s, got %s" % (config.floatX, self.val_shared.dtype)
        if not hasattr(val, 'dtype'):
            raise TypeError('Monitor channel ' + name + ' has value of type ' +
                            str(type(val)))
        if val.dtype != self.val_shared.dtype:
            raise ValueError('monitor channels are expected to have dtype ' +
                             str(self.val_shared.dtype) + ' but "' + name +
                             '" has dtype ' + str(val.dtype))
        if val.ndim != 0:
            raise ValueError('monitor channels are supposed to have zero ' +
                             'dimensions but "' + name + '" has ' +
                             str(val.ndim))
        # Dataset monitored by this channel
        self.dataset = dataset
        # Value of the desired quantity at measurement time.
        self.val_record = []
        # Number of batches seen at measurement time.
        self.batch_record = []
        # Number of examples seen at measurement time (batch sizes may
        # fluctuate).
        self.example_record = []
        self.epoch_record = []
        self.time_record = []

    def __str__(self):
        """
        .. todo::

            WRITEME

        Returns
        -------
        s : str
            A reasonably human-readable string representation of the object.
        """
        try:
            graph_input_str = str(self.graph_input)
        except:
            graph_input_str = '<bad graph input>'

        try:
            val_str = str(self.val)
        except:
            val_str = '<bad val>'

        try:
            name_str = str(self.name)
        except:
            name_str = '<bad name>'

        try:
            prereqs_str = str(self.prereqs)
        except:
            prereqs_str = '<bad prereqs>'

        return "MonitorChannel(%s,%s,%s,%s)" % (graph_input_str,
                                                val_str,
                                                name_str,
                                                prereqs_str)

    def __getstate__(self):
        """
        .. todo::

            WRITEME

        Returns
        -------
        d : dict
            A dictionary mapping the string names of the fields of the class
            to values appropriate for pickling.
        """
        # We need to figure out a good way of saving the other fields. In the
        # current setup, since there's no good way of coordinating with the
        # model/training algorithm, the theano based fields might be invalid
        # after a repickle. This means we can't, for instance, resume a job with
        # monitoring after a crash. For now, to make sure no one erroneously
        # depends on these bad values, I exclude them from the pickle.

        if hasattr(self, 'val'):
            doc = get_monitor_doc(self.val)
        else:
            # Hack to deal with Theano expressions not being serializable.
            # If this is channel that has been serialized and then
            # deserialized, the expression is gone, but we should have
            # stored the doc
            if hasattr(self, "doc"):
                doc = self.doc
            else:
                # Support pickle files that are older than the doc system
                doc = None

        return {
            'doc' : doc,
            'example_record' : self.example_record,
            'batch_record' : self.batch_record,
            'time_record' : self.time_record,
            'epoch_record' : self.epoch_record,
            'val_record': self.val_record
        }

    def __setstate__(self, d):
        """
        Sets the object to have the state described by `d`.

        Parameters
        ----------
        d : dict
            A dictionary mapping string names of fields to values for
            these fields.
        """
        self.__dict__.update(d)
        if 'batch_record' not in d:
            self.batch_record = [None] * len(self.val_record)
        # Patch old pickle files that don't have the "epoch_record" field
        if 'epoch_record' not in d:
            # This is not necessarily correct but it is in the most common use
            # case where you don't add monitoring channels over time.
            self.epoch_record = range(len(self.val_record))
        if 'time_record' not in d:
            self.time_record = [None] * len(self.val_record)


def push_monitor(model, name, transfer_experience=False):
    """
    When you load a model in a yaml file and you want to store its
    old monitor under a different name and start a new monitor, wrap
    the model in this function call.


    Parameters
    ----------
    model : pylearn2.models.model.Model
        The model you loaded
    name : str
        Will save the old monitor to model.name
    transfer_experience : bool
        If True, the new monitor will start with its epochs seen,
        batches seen, and examples seen set to where the old monitor
        left off. This is nice for stitching together learning curves
        across multiple stages of learning.

    Returns
    -------
    model : WRITEME
        Returns the model itself so you can use an !obj:push_monitor
        call as the definition of a model in a YAML file.
    """

    assert hasattr(model, 'monitor')
    old_monitor = model.monitor
    setattr(model, name, old_monitor)
    del model.monitor

    if transfer_experience:
        monitor = Monitor.get_monitor(model)
        assert monitor is not old_monitor
        monitor._num_batches_seen = old_monitor._num_batches_seen
        monitor._examples_seen = old_monitor._examples_seen
        monitor._epochs_seen = old_monitor._epochs_seen

    return model


def read_channel(model, channel_name, monitor_name='monitor'):
    """
    Returns the last value recorded in a channel.

    Parameters
    ----------
    model : Model
        The model to read the channel from
    channel_name : str
        The name of the channel to read from
    monitor_name : str, optional
        The name of the Monitor to read from
        (In case you want to read from an old Monitor moved by
        `push_monitor`)

    Returns
    -------
    value : float
        The last value recorded in this monitoring channel
    """
    return getattr(model, monitor_name).channels[channel_name].val_record[-1]

def get_channel(model, dataset, channel, cost, batch_size):
    """
    Make a temporary monitor and return the value of a channel in it.

    Parameters
    ----------
    model : pylearn2.models.model.Model
        Will evaluate the channel for this Model.
    dataset : pylearn2.datasets.Dataset
        The Dataset to run on
    channel : str
        A string identifying the channel name to evaluate
    cost : pylearn2.costs.Cost
        The Cost to setup for monitoring
    batch_size : int
        The size of the batch to use when running the monitor

    Returns
    -------
    value : WRITEME
        The value of the requested channel.

    Notes
    -----
    This doesn't modify the model (unless some of the channel prereqs do).
    In particular, it does not change model.monitor.
    """
    monitor = Monitor(model)
    monitor.setup(dataset=dataset, cost=cost, batch_size=batch_size)
    monitor()
    channels = monitor.channels
    channel = channels[channel]
    val_record = channel.val_record
    value, = val_record
    return value


def get_monitor_doc(var):
    """
    Returns the __doc__ field of var or None. This field is used on
    theano Variables to document the meaning of monitor channels.

    Parameters
    ----------
    var : theano.gof.Variable
        The variable to get the documentation of

    Returns
    -------
    doc : str or None
        var.__doc__ if var has an instance-level doc, otherwise None
    """

    doc = None

    if var.__doc__ is not var.__class__.__doc__:
        doc = var.__doc__

    return doc

_err_no_data = "You tried to add a channel to a Monitor that has no dataset."
_err_ambig_data = ("You added a channel to a Monitor that has multiple " +
                   "datasets, and did not specify which dataset to use it " +
                   "with.")

########NEW FILE########
__FILENAME__ = batch_gradient_descent
"""
.. todo::

    WRITEME
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

import logging
import time

import numpy as np

from theano import config
from theano.compat.python2x import OrderedDict
from theano.printing import var_descriptor
import theano.tensor as T

from pylearn2.utils import function
from pylearn2.utils import grad
from pylearn2.utils import safe_zip
from pylearn2.utils import sharedX


logger = logging.getLogger(__name__)


class BatchGradientDescent(object):
    """
    A class for minimizing a function via the method of steepest descent.

    Parameters
    ----------
    objective : tensor_like
        A theano expression to be minimized should be a function of params and,
        if provided, inputs
    params : list
        A list of theano shared variables. These are the optimization variables
    inputs : list, optional
        A list of theano variables to serve as inputs to the graph.
    param_constrainers : list
        A list of callables to be called on all updates dictionaries to be
        applied to params. This is how you implement constrained
        optimization.
    reset_alpha : bool
        If True, reverts to using init_alpha after each call. If False, the
        final set of alphas is used at the start of the next call to minimize.
    conjugate : bool
        If True, tries to pick conjugate gradient directions. For the
        directions to be truly conjugate, you must use line_search_mode =
        'exhaustive' and the objective function must be quadratic. Using
        line_search_mode = 'exhaustive' on a non-quadratic objective function
        implements nonlinear conjugate gradient descent.
    reset_conjugate : bool
        Has no effect unless conjugate == True. If reset_conjugate ==
        True, reverts to direction of steepest descent for the first
        step in each call to minimize. Otherwise, tries to make the new
        search direction conjugate to the last one (even though the
        objective function might be totally different on each call to
        minimize)
    gradients : WRITEME
        If None, compute the gradients of obj using T.grad otherwise, a
        dictionary mapping from params to expressions for their gradients
        (this allows you to use approximate gradients computed with
        something other than T.grad)
    gradient_updates : dict
        A dictionary of shared variable updates to run each time the
        gradient is computed

    Notes
    -----
    Calling the `minimize` method with values for for `inputs` will
    update `params` to minimize `objective`.
    """
    def __init__(self, objective, params, inputs = None,
            param_constrainers = None, max_iter = -1,
            lr_scalers = None, verbose = 0, tol = None,
            init_alpha = None, min_init_alpha = 1e-3,
            reset_alpha = True, conjugate = False,
            reset_conjugate = True, gradients = None,
            gradient_updates = None, line_search_mode = None,
            accumulate = False, theano_function_mode=None):

        self.__dict__.update(locals())
        del self.self

        if line_search_mode is None:
            if init_alpha is None:
                init_alpha  = (.001, .005, .01, .05, .1)
        else:
            assert line_search_mode == 'exhaustive'
            if init_alpha is None:
                init_alpha = (.5, 1.)

        self.init_alpha = tuple([float(elem) for elem in init_alpha])

        if inputs is None:
            inputs = []

        if param_constrainers is None:
            param_constrainers = []

        obj = objective

        self.verbose = verbose

        param_to_grad_sym = OrderedDict()
        param_to_grad_shared = OrderedDict()
        updates = OrderedDict()
        if self.gradient_updates is not None:
            updates.update(self.gradient_updates)

        self.params = [ param for param in params ]

        for param in params:
            if self.gradients is not None and param in self.gradients:
                g = self.gradients[param]
            else:
                g = grad(objective, param)
            param_to_grad_sym[param] = g
            if param.name is not None:
                param_name = param.name
            else:
                param_name = 'anon_param'
            grad_name = 'BatchGradientDescent.grad_' + param_name
            grad_shared = sharedX( param.get_value() * 0., name=grad_name)
            param_to_grad_shared[param] = grad_shared
            updates[grad_shared] = g

        self.param_to_grad_shared = param_to_grad_shared

        if self.verbose:
            logger.info('batch gradient class compiling gradient function')
        t1 = time.time()
        if self.accumulate:
            self._compute_grad = Accumulator(inputs, updates = updates)
        else:
            self._compute_grad = function(inputs, updates = updates,
                    mode=self.theano_function_mode,
                    name='BatchGradientDescent._compute_grad')
        if self.verbose:
            t2 = time.time()
            logger.info('done. Took {0}'.format(t2-t1))

        if self.verbose:
            logger.info('batch gradient class compiling objective function')
        if self.accumulate:
            self.obj = Accumulator(inputs, obj)
        else:
            self.obj = function(inputs, obj, mode=self.theano_function_mode,
                    name='BatchGradientDescent.obj')

        if self.verbose:
            logger.info('done')

        self.param_to_cache = OrderedDict()
        alpha = T.scalar(name = 'alpha')
        alpha.tag.test_value = np.cast[alpha.dtype](.01)
        cache_updates = OrderedDict()
        goto_updates = OrderedDict()
        for param in params:
            if param.name is None:
                param_name = 'anon_param'
            else:
                param_name = param.name
            cache_name = 'BatchGradientDescent.param_to_cache[%s]' % param_name
            self.param_to_cache[param] = sharedX(param.get_value(borrow=False), name=cache_name)
            cache_updates[self.param_to_cache[param]] = param
            cached = self.param_to_cache[param]
            g = self.param_to_grad_shared[param]
            if lr_scalers is not None and param in lr_scalers:
                scaled_alpha = alpha * lr_scalers[param]
            else:
                scaled_alpha = alpha
            mul = scaled_alpha * g
            diff = cached - mul
            goto_updates[param] = diff
        self._cache_values = function([], updates = cache_updates, mode=self.theano_function_mode, name='BatchGradientDescent._cache_values')
        assert isinstance(param_constrainers, (list, tuple))
        for param_constrainer in param_constrainers:
            param_constrainer(goto_updates)
        self._goto_alpha = function([alpha], updates=goto_updates,
                mode=self.theano_function_mode, name='BatchGradientDescent._goto_alpha')

        norm = T.sqrt(sum([T.sqr(elem).sum() for elem in self.param_to_grad_shared.values()]))
        norm.name = 'BatchGradientDescent.norm'
        normalize_grad_updates = OrderedDict()
        for grad_shared in self.param_to_grad_shared.values():
            normalize_grad_updates[grad_shared] = grad_shared / norm

        # useful for monitoring
        self.ave_grad_size = sharedX(0.)
        self.new_weight = sharedX(1.)
        normalize_grad_updates[self.ave_grad_size] = self.new_weight * norm + (1.-self.new_weight) * self.ave_grad_size

        self._normalize_grad = function([], norm, updates=normalize_grad_updates, mode=self.theano_function_mode,
                name='BatchGradientDescent._normalize_grad')

        if self.conjugate:
            grad_shared = self.param_to_grad_shared.values()

            grad_to_old_grad = OrderedDict()
            for elem in grad_shared:
                grad_to_old_grad[elem] = sharedX(elem.get_value(), 'old_'+elem.name)

            self._store_old_grad = function([norm], updates = OrderedDict([(grad_to_old_grad[g_], g_ * norm)
                for g_ in grad_to_old_grad]), mode=self.theano_function_mode,
                name='BatchGradientDescent._store_old_grad')

            grad_ordered = list(grad_to_old_grad.keys())
            old_grad_ordered = [grad_to_old_grad[g_] for g_ in grad_ordered]

            def dot_product(x, y):
                return sum([ (x_elem * y_elem).sum() for x_elem, y_elem in safe_zip(x, y) ])

            beta_pr = (dot_product(grad_ordered, grad_ordered) - dot_product(grad_ordered, old_grad_ordered)) / \
                    (1e-7+dot_product(old_grad_ordered, old_grad_ordered))
            assert beta_pr.ndim == 0

            beta = T.maximum(beta_pr, 0.)

            #beta_pr is the Polak-Ribiere formula for beta.
            #According to wikipedia, the beta to use for NCG is "a matter of heuristics or taste"
            #but max(0, beta_pr) is "a popular choice... which provides direction reset automatically."
            #(ie, it is meant to revert to steepest descent when you have traveled far enough that
            #the objective function is behaving non-quadratically enough that the conjugate gradient
            #formulas aren't working anymore)

            #http://en.wikipedia.org/wiki/Nonlinear_conjugate_gradient_method

            assert grad not in grad_to_old_grad

            make_conjugate_updates = [(g_, g_ + beta * grad_to_old_grad[g_]) for g_ in grad_ordered]

            mode = self.theano_function_mode
            if mode is not None and hasattr(mode, 'record'):
                for v, u in make_conjugate_updates:
                    mode.record.handle_line('BatchGradientDescent._make_conjugate var ' \
                            + var_descriptor(v) + '\n')
                    mode.record.handle_line('BatchGradientDescent._make_conjugate update ' \
                            + var_descriptor(u) + '\n')

            self._make_conjugate = function([], updates=make_conjugate_updates,
                    mode=self.theano_function_mode, name='BatchGradientDescent._make_conjugate')

            if mode is not None and hasattr(mode, 'record'):
                for output in self._make_conjugate.maker.fgraph.outputs:
                    mode.record.handle_line('BatchGradientDescent._make_conjugate output ' \
                            + var_descriptor(output) + '\n')


        if tol is None:
            if objective.dtype == "float32":
                self.tol = 1e-6
            else:
                self.tol = 3e-7
        else:
            self.tol = tol

        self.ave_step_size = sharedX(0.)
        self.ave_grad_mult = sharedX(0.)

    def minimize(self, * inputs ):
        """
        .. todo::

            WRITEME
        """

        if self.verbose:
            logger.info('minimizing')
        alpha_list = list( self.init_alpha )

        orig_obj = self.obj(*inputs)

        if self.verbose:
            logger.info(orig_obj)

        iters = 0

        # A bit of a hack here: we multiply by norm
        # when calling store_old_grad below. This is mostly
        # so we store the non-normalized version of the gradient,
        # but we can also exploit it to either clear the old grad
        # on the first iteration by setting norm = 0 initially.
        # This makes the algorithm reset to steepest descent on
        # each call to minimize. Or we can set the norm to 1 to
        # save the previous gradient, so we can try to maintain
        # conjugacy across several calls to minimize.
        # If self.conjugate is False none of this matters
        # since store_old_grad is never called anyway.
        if self.reset_conjugate:
            norm = 0.
        else:
            norm = 1.

        while iters != self.max_iter:
            if self.verbose:
                logger.info('batch gradient descent iteration '
                            '{0}'.format(iters))
            iters += 1
            self._cache_values()
            if self.conjugate:
                self._store_old_grad(norm)
            self._compute_grad(*inputs)
            if self.conjugate:
                self._make_conjugate()
            norm = self._normalize_grad()

            if self.line_search_mode is None:
                best_obj, best_alpha, best_alpha_ind = self.obj( * inputs), 0., -1
                prev_best_obj = best_obj

                for ind, alpha in enumerate(alpha_list):
                    self._goto_alpha(alpha)
                    obj = self.obj(*inputs)
                    if self.verbose:
                        logger.info('\t{0} {1}'.format(alpha, obj))

                    #Use <= rather than = so if there are ties
                    #the bigger step size wins
                    if obj <= best_obj:
                        best_obj = obj
                        best_alpha = alpha
                        best_alpha_ind = ind
                    #end if obj
                #end for ind, alpha


                if self.verbose:
                    logger.info(best_obj)

                assert not np.isnan(best_obj)
                assert best_obj <= prev_best_obj
                self._goto_alpha(best_alpha)

                step_size = best_alpha

                #if best_obj == prev_best_obj and alpha_list[0] < 1e-5:
                #    break
                if best_alpha_ind < 1 and alpha_list[0] > self.tol:
                    alpha_list = [ alpha / 3. for alpha in alpha_list ]
                    if self.verbose:
                        logger.info('shrinking the step size')
                elif best_alpha_ind > len(alpha_list) -2:
                    alpha_list = [ alpha * 2. for alpha in alpha_list ]
                    if self.verbose:
                        logger.info('growing the step size')
                elif best_alpha_ind == -1 and alpha_list[0] <= self.tol:
                    if alpha_list[-1] > 1:
                        if self.verbose:
                            logger.info('converged')
                        break
                    if self.verbose:
                        logger.info('expanding the range of step sizes')
                    for i in xrange(len(alpha_list)):
                        for j in xrange(i,len(alpha_list)):
                            alpha_list[j] *= 1.5
                        #end for j
                    #end for i
                else:
                    # if a step succeeded and didn't result in growing or shrinking
                    # the step size then we can probably benefit from more fine-grained
                    # exploration of the middle ranges of step size
                    # (this is especially necessary if we've executed the
                    # 'expanding the range of step sizes' case multiple times)
                    a = np.asarray(alpha_list)
                    s = a[1:]/a[:-1]
                    max_gap = 5.
                    if s.max() > max_gap:
                        weight = .99
                        if self.verbose:
                            logger.info('shrinking the range of step sizes')
                        alpha_list = [ (alpha ** weight) * (best_alpha ** (1.-weight)) for alpha in alpha_list ]
                        assert all([second > first for first, second in safe_zip(alpha_list[:-1], alpha_list[1:])])
                        # y^(weight) best^(1-weight) / x^(weight) best^(1-weight) = (y/x)^weight
                        # so this shrinks the ratio between each successive pair of alphas by raising it to weight
                        # weight = .99 -> a gap of 5 is shrunk to 4.92


                #end check on alpha_ind
            else:
                assert self.line_search_mode == 'exhaustive'

                # In exhaustive mode, we search until we get very little
                # improvement (or have tried over ten points)
                # and we dynamically pick the search points to try to
                # maximize the improvement.
                # The points we pick are kind of dumb; it's just a binary
                # search. We could probably do better by fitting a function
                # and jumping to its local minima at each step

                if self.verbose > 1:
                    logger.info('Exhaustive line search')


                obj = self.obj(*inputs)
                if np.isnan(obj):
                    logger.warning("Objective is NaN for these parameters.")
                results = [ (0., obj ) ]
                for alpha in alpha_list:
                    if not (alpha > results[-1][0]):
                        logger.error('alpha: {0}'.format(alpha))
                        logger.error('most recent alpha (should be smaller): '
                                     '{0}'.format(results[-1][0]))
                        assert False
                    self._goto_alpha(alpha)
                    obj = self.obj(*inputs)
                    if np.isnan(obj):
                        obj = np.inf
                    results.append( (alpha, obj) )
                if self.verbose > 1:
                    for alpha, obj in results:
                        logger.info('\t{0} {1}'.format(alpha, obj))

                    logger.info('\t-------')

                prev_improvement = 0.
                while True:
                    alpha_list = [alpha for alpha, obj in results]
                    obj = [ obj for alpha, obj in results]
                    mn = min(obj)
                    idx = obj.index(mn)

                    def do_point(x):
                        self._goto_alpha(x)
                        res = self.obj(*inputs)
                        if self.verbose > 1:
                            logger.info('\t{0} {1}'.format(x, res))
                        # Regard NaN results as infinitely bad so they won't be picked as the min objective
                        if np.isnan(res):
                            res = np.inf
                        for i in xrange(len(results)):
                            elem = results[i]
                            ex = elem[0]
                            if x == ex:
                                raise AssertionError(str(ex)+" is already in the list.")
                            if x > ex:
                                if i + 1 == len(results) or x < results[i+1][0]:
                                    results.insert(i+1, (x, res))
                                    return mn - res
                        assert False # should be unreached

                    if idx == 0:
                        x = (alpha_list[0] + alpha_list[1]) / 2.
                    elif idx == len(alpha_list) - 1:
                        x = 2 * alpha_list[-1]
                    else:
                        if obj[idx+1] < obj[idx-1]:
                            x = (alpha_list[idx] + alpha_list[idx+1])/2.
                        else:
                            x = (alpha_list[idx] + alpha_list[idx-1])/2.

                    if x < 1e-20:
                        break

                    improvement = do_point(x)

                    if (improvement > 0 and improvement < .01 * prev_improvement) or len(obj) > 10:
                        break
                    prev_improvement = improvement

                alpha_list = [alpha for alpha, ignore_obj in results]
                obj = [obj_elem for alpha, obj_elem in results]
                mn = min(obj)
                idx = obj.index(mn)
                x = alpha_list[idx]
                self._goto_alpha(x)
                # used for statistics gathering
                step_size = x
                if self.verbose:
                    logger.info('best objective: {0}'.format(mn))
                assert not np.isnan(mn)

                if idx == 0:
                    x = alpha_list[1]

                if self.min_init_alpha is not None:
                    x = max(x, 2. * self.min_init_alpha)

                alpha_list = [ x/2., x ]
                best_obj = mn
            # end if branching on type of line search

            new_weight = self.new_weight.get_value()
            old = self.ave_step_size.get_value()
            update = new_weight * step_size + (1-new_weight) * old
            update = np.cast[config.floatX](update)
            if self.ave_step_size.dtype == 'float32':
                assert update.dtype == 'float32'
            self.ave_step_size.set_value(update)

            old = self.ave_grad_mult.get_value()
            update = new_weight * (step_size / norm) + (1. - new_weight) * old
            update = np.cast[config.floatX](update)
            self.ave_grad_mult.set_value(update)
            # it is initialized to 1 to get all the means started at data points,
            # but then we turn it into a running average
            if new_weight == 1.:
                self.new_weight.set_value(.01)


        # end while


        if not self.reset_alpha:
            self.init_alpha = alpha_list

        # The way this optimizer is used (with max_iters set to 3 or 5 so it doesn't go too
        # far on one minibatch) this warning doesn't make a lot of sense, but we might want
        # a switch to turn it on if you really are trying to absolutely minimize the objective
        # for the current inputs.
        # if norm > 1e-2:
        #    warnings.warn(str(norm)+" seems pretty big for a gradient at convergence...")

        return best_obj

class Accumulator(object):
    """
    Standin for a theano function with the given inputs, outputs, updates.

    Here in the __init__ method you give the same expression as usual.
    However, instead of passing __call__ the input variables directly, you
    pass it batches, where each batch is a list containing the inputs for
    that batch. It returns the average value of the function, averaged
    across batches, taking batch size into account. The average of all
    updates is also applied.

    One extra change: if any of the inputs is a shared variable, then this
    can assign to that variable, while theano.function would refuse to.
    Those shared variables will be left with the value of the last batch
    when __call__ returns.

    Parameters
    ----------
    inputs : WRITEME
    outputs : WRITEME
    updates : WRITEME
    """

    def __init__(self, inputs, outputs = None, updates = None):
        batch_size = T.cast(inputs[0].shape[0], 'float32')
        total_examples = T.scalar()
        transformed_updates = OrderedDict()
        self.has_updates = updates is not None
        if self.has_updates:
            self._clear = function([], updates = [ (var, 0. * var) for var in updates])
            for var in updates:
                update = updates[var]
                transformed_updates[var] = var + (batch_size / total_examples) * update
        self._shared_mask = [ hasattr(elem, 'get_value') for elem in inputs]
        true_inputs = self._true_inputs(inputs)
        self._shared = self._shared_inputs(inputs)
        if outputs is not None:
            if not isinstance(outputs, list):
                outputs = [ outputs ]
            outputs = [ output * (batch_size / total_examples) for output in outputs]
        self._func = function(true_inputs + [total_examples], outputs=outputs, updates=transformed_updates)

    def _true_inputs(self, inputs):
        """
        .. todo::

            WRITEME
        """
        return [elem for elem, shared in safe_zip(inputs, self._shared_mask) if not shared ]

    def _shared_inputs(self, inputs):
        """
        .. todo::

            WRITEME
        """
        return [elem for elem, shared in safe_zip(inputs, self._shared_mask) if shared ]

    def _set_shared(self, inputs):
        """
        .. todo::

            WRITEME
        """
        for elem, mask, shared in safe_zip(inputs, self._shared_mask, self._shared):
            if mask:
                shared.set_value(elem)

    def __call__(self, * batches ):
        """
        .. todo::

            WRITEME
        """
        for batch in batches:
            if not isinstance(batch, list):
                raise TypeError("Expected each argument to be a list, but one argument is " + \
                        str(batch) + " of type "+str(type(batch)))
        total_examples = np.cast[config.floatX](sum([batch[0].shape[0] for batch in batches]))
        if self.has_updates:
            self._clear()
        augmented = self._true_inputs(batches[0]) + [total_examples]
        self._set_shared(batches[0])
        rval = self._func(*augmented)
        for batch in batches[1:]:
            augmented = self._true_inputs(batch) + [total_examples]
            self._set_shared(batch)
            # This works if there is no output, because the output is an empty list
            cur_out = self._func(*augmented)
            rval = [ x + y for x, y in safe_zip(rval, cur_out)]
        if len(rval) == 1:
            return rval[0]
        return rval


def norm_sq(s):
    """
    .. todo::

        WRITEME
    """
    return np.square(s.get_value()).sum()


def scale(s, a):
    """
    .. todo::

        WRITEME
    """
    s.set_value(s.get_value() * np.cast[config.floatX](a))

########NEW FILE########
__FILENAME__ = feature_sign
"""
L1-penalized minimization using the feature sign search algorithm.
"""

__authors__ = "David Warde-Farley"
__copyright__ = "Copyright 2010-2011, Universite de Montreal"
__credits__ = ["David Warde-Farley"]
__license__ = "3-clause BSD"
__maintainer__ = "David Warde-Farley"
__email__ = "wardefar@iro"

__all__ = ["feature_sign_search"]

from itertools import izip, count
import logging
import numpy as np
log = logging.getLogger(__name__)
log.setLevel(logging.INFO)


def _feature_sign_checkargs(dictionary, signals, sparsity, max_iter,
                            solution):
    """
    .. todo::

        WRITEME
    """
    if solution is not None:
        assert solution.ndim == signals.ndim, (
            "if provided, solutions must be same number of dimensions as "
            "signals"
        )
    if signals.ndim == 1:
        assert signals.shape[0] == dictionary.shape[0], (
            "signals.ndim == 1, but signals.shape[0] !=  dictionary.shape[0]"
        )
        if solution is not None:
            assert solution.shape[0] == dictionary.shape[1], (
                "solutions array is wrong shape (ndim=1, should have first "
                "dimension %d given dictionary)" % dictionary.shape[1]
            )
    elif signals.ndim == 2:
        assert signals.shape[1] == dictionary.shape[0], (
            "signals.ndim == 2, but signals.shape[1] !=  dictionary.shape[0]"
        )
        if solution is not None:
            assert solution.shape[0] == signals.shape[0], (
                "solutions array is wrong shape (ndim=2, should have first "
                "dimension %d given signals)" % signals.shape[0]
            )
            assert solution.shape[1] == dictionary.shape[1], (
                "solutions array is wrong shape (ndim=1, should have second "
                "dimension %d given dictionary)" % dictionary.shape[1]
            )


def _feature_sign_search_single(dictionary, signal, sparsity, max_iter,
                                solution=None):
    """
    Solve a single L1-penalized minimization problem with
    feature-sign search.

    Parameters
    ----------
    dictionary : array_like, 2-dimensional
        The dictionary of basis functions from which to form the
        sparse linear combination.
    signal : array_like, 1-dimensional
        The signal being decomposed as a sparse linear combination
        of the columns of the dictionary.
    sparsity : float
        The coefficient on the L1 penalty term of the cost function.
    max_iter : int
        The maximum number of iterations to run.
    solution : ndarray, 1-dimensional, optional
        Pre-allocated vector to use to store the solution.

    Returns
    -------
    solution : ndarray, 1-dimensional
        Vector containing the solution. If an array was passed in
        as the argument `solution`, it will be updated in place
        and the same object will be returned.
    count : int
        The number of iterations of the algorithm that were run.

    Notes
    -----
    See the docstring of `feature_sign_search` for details on the
    algorithm.
    """
    # This prevents the sparsity penalty scalar from upcasting the entire
    # rhs vector sent to linalg.solve().
    sparsity = np.array(sparsity).astype(dictionary.dtype)
    effective_zero = 1e-18
    # precompute matrices for speed.
    gram_matrix = np.dot(dictionary.T, dictionary)
    target_correlation = np.dot(dictionary.T, signal)
    # initialization goes here.
    if solution is None:
        solution = np.zeros(gram_matrix.shape[0], dtype=dictionary.dtype)
    else:
        assert solution.ndim == 1, "solution must be 1-dimensional"
        assert solution.shape[0] == dictionary.shape[1], (
            "solution.shape[0] does not match dictionary.shape[1]"
        )
        # Initialize all elements to be zero.
        solution[...] = 0.
    signs = np.zeros(gram_matrix.shape[0], dtype=np.int8)
    active_set = set()
    z_opt = np.inf
    # Used to store whether max(abs(grad[nzidx] + sparsity * signs[nzidx]))
    # is approximately 0.
    # Set to True here to trigger a new feature activation on first iteration.
    nz_optimal = True
    # second term is zero on initialization.
    grad = - 2 * target_correlation  # + 2 * np.dot(gram_matrix, solution)
    max_grad_zero = np.argmax(np.abs(grad))
    # Just used to compute exact cost function.
    sds = np.dot(signal.T, signal)
    counter = count(0)
    while z_opt > sparsity or not nz_optimal:
        if counter.next() == max_iter:
            break
        if nz_optimal:
            candidate = np.argmax(np.abs(grad) * (signs == 0))
            log.debug("candidate feature: %d" % candidate)
            if grad[candidate] > sparsity:
                signs[candidate] = -1.
                solution[candidate] = 0.
                log.debug("added feature %d with negative sign" %
                          candidate)
                active_set.add(candidate)
            elif grad[candidate] < -sparsity:
                signs[candidate] = 1.
                solution[candidate] = 0.
                log.debug("added feature %d with positive sign" %
                          candidate)
                active_set.add(candidate)
            if len(active_set) == 0:
                break
        else:
            log.debug("Non-zero coefficient optimality not satisfied, "
                      "skipping new feature activation")
        indices = np.array(sorted(active_set))
        restr_gram = gram_matrix[np.ix_(indices, indices)]
        restr_corr = target_correlation[indices]
        restr_sign = signs[indices]
        rhs = restr_corr - sparsity * restr_sign / 2
        # TODO: implement footnote 3.
        #
        # If restr_gram becomes singular, check if rhs is in the column
        # space of restr_gram.
        #
        # If so, use the pseudoinverse instead; if not, update to first
        # zero-crossing along any direction in the null space of restr_gram
        # such that it has non-zero dot product with rhs (how to choose
        # this direction?).
        new_solution = np.linalg.solve(np.atleast_2d(restr_gram), rhs)
        new_signs = np.sign(new_solution)
        restr_oldsol = solution[indices]
        sign_flips = np.where(abs(new_signs - restr_sign) > 1)[0]
        if len(sign_flips) > 0:
            best_obj = np.inf
            best_curr = None
            best_curr = new_solution
            best_obj = (sds + (np.dot(new_solution,
                                      np.dot(restr_gram, new_solution))
                        - 2 * np.dot(new_solution, restr_corr))
                        + sparsity * abs(new_solution).sum())
            if log.isEnabledFor(logging.DEBUG):
                # Extra computation only done if the log-level is
                # sufficient.
                ocost = (sds + (np.dot(restr_oldsol,
                                       np.dot(restr_gram, restr_oldsol))
                        - 2 * np.dot(restr_oldsol, restr_corr))
                        + sparsity * abs(restr_oldsol).sum())
                cost = (sds + np.dot(new_solution,
                                     np.dot(restr_gram, new_solution))
                        - 2 * np.dot(new_solution, restr_corr)
                        + sparsity * abs(new_solution).sum())
                log.debug("Cost before linesearch (old)\t: %e" % ocost)
                log.debug("Cost before linesearch (new)\t: %e" % cost)
            else:
                ocost = None
            for idx in sign_flips:
                a = new_solution[idx]
                b = restr_oldsol[idx]
                prop = b / (b - a)
                curr = restr_oldsol - prop * (restr_oldsol - new_solution)
                cost = sds + (np.dot(curr, np.dot(restr_gram, curr))
                              - 2 * np.dot(curr, restr_corr)
                              + sparsity * abs(curr).sum())
                log.debug("Line search coefficient: %.5f cost = %e "
                          "zero-crossing coefficient's value = %e" %
                          (prop, cost, curr[idx]))
                if cost < best_obj:
                    best_obj = cost
                    best_prop = prop
                    best_curr = curr
            log.debug("Lowest cost after linesearch\t: %e" % best_obj)
            if ocost is not None:
                if ocost < best_obj and not np.allclose(ocost, best_obj):
                    log.debug("Warning: objective decreased from %e to %e" %
                              (ocost, best_obj))
        else:
            log.debug("No sign flips, not doing line search")
            best_curr = new_solution
        solution[indices] = best_curr
        zeros = indices[np.abs(solution[indices]) < effective_zero]
        solution[zeros] = 0.
        signs[indices] = np.int8(np.sign(solution[indices]))
        active_set.difference_update(zeros)
        grad = - 2 * target_correlation + 2 * np.dot(gram_matrix, solution)
        z_opt = np.max(abs(grad[signs == 0]))
        nz_opt = np.max(abs(grad[signs != 0] + sparsity * signs[signs != 0]))
        nz_optimal = np.allclose(nz_opt, 0)

    return solution, min(counter.next(), max_iter)


def feature_sign_search(dictionary, signals, sparsity, max_iter=1000,
                        solution=None):
    """
    Solve L1-penalized quadratic minimization problems with
    feature-sign search.

    Employs the feature sign search algorithm of Lee et al (2006)
    to solve an L1-penalized quadratic optimization problem as a
    sequence of unconstrained quadratic minimization problems over
    subsets of the variables, with candidates for non-zero elements
    chosen by means of a gradient-based criterion.

    Parameters
    ----------
    dictionary : array_like, 2-dimensional
        The dictionary of basis functions from which to form the
        sparse linear combination. Each column constitutes a basis
        vector for the sparse code. There should be as many rows as
        input dimensions in the signal.
    signals : array_like, 1- or 2-dimensional
        The signal(s) to be decomposed as a sparse linear combination
        of the columns of the dictionary. If 2-dimensional, each
        different signal (training case) should be a row of this matrix.
    sparsity : float
        The coefficient on the L1 penalty term of the cost function.
    max_iter : int, optional
        The maximum number of iterations to run, per code vector, if
        the optimization has still not converged. Default is 1000.
    solution : ndarray, 1- or 2-dimensional, optional
        Pre-allocated vector or matrix used to store the solution(s).
        If provided, it should have the same rank as `signals`. If
        2-dimensional, it should have as many rows as `signals`.

    Returns
    -------
    solution : ndarray, 1- or 2-dimensional
        Matrix where each row contains the solution corresponding to a
        row of `signals`. If an array was passed in as the argument
        `solution`, it  will be updated in place and the same object
        will be returned.

    Notes
    -----
    It might seem more natural, from a linear-algebraic point of
    view, to think of both `signals` and `solution` as matrices with
    training examples contained as column vectors; then the overall
    cost function being minimized is

    .. math::
        (Y - AX)^2 + \gamma \sum_{i,j} |X_{ij}|

    with :math:`$A$` representing the dictionary, :math:`Y` being
    `signals.T` and math:`X` being `solutions.T`. However, in order
    to maintain the convention of training examples being indexed
    along the first dimension in the case of 2-dimensional `signals`
    input (as well as provide faster computation via memory locality
    in the case of C-contiguous inputs), this function expects and
    returns input with training examples as rows of a matrix.

    References
    ----------
    .. [1] H. Lee, A. Battle, R. Raina, and A. Y. Ng. "Efficient
       sparse coding algorithms". Advances in Neural Information
       Processing Systems 19, 2007.
    """
    dictionary = np.asarray(dictionary)
    _feature_sign_checkargs(dictionary, signals, sparsity, max_iter, solution)
    # Make things the code a bit simpler by always forcing the
    # 2-dimensional case.
    signals_ndim = signals.ndim
    signals = np.atleast_2d(signals)
    if solution is None:
        solution = np.zeros((signals.shape[0], dictionary.shape[1]),
                            dtype=signals.dtype)
        orig_sol = None
    else:
        orig_sol = solution
        solution = np.atleast_2d(solution)
    # Solve each minimization in sequence.
    for row, (signal, sol) in enumerate(izip(signals, solution)):
        _, iters = _feature_sign_search_single(dictionary, signal, sparsity,
                                               max_iter, sol)
        if iters >= max_iter:
            log.warning("maximum number of iterations reached when "
                        "optimizing code for training case %d; solution "
                        "may not be optimal" % iters)
    # Attempt to return the exact same object reference.
    if orig_sol is not None and orig_sol.ndim == 1:
        solution = orig_sol
    # Return a vector with the same rank as the input `signals`.
    elif orig_sol is None and signals_ndim == 1:
        solution = solution.squeeze()
    return solution

########NEW FILE########
__FILENAME__ = linear_cg
"""
.. todo::

    WRITEME
"""
import theano
from theano import tensor
from theano.ifelse import ifelse

def linear_cg(fn, params, tol=1e-3, max_iters=1000, floatX=None):
    """
    Minimizes a POSITIVE DEFINITE quadratic function via linear conjugate
    gradient using the R operator to avoid explicitly representing the Hessian.

    If you have several variables, this is cheaper than Newton's method, which
    would need to invert the Hessian. It is also cheaper than standard linear
    conjugate gradient, which works with an explicit representation of the
    Hessian. It is also cheaper than nonlinear conjugate gradient which does a
    line search by repeatedly evaluating f.

    For more information about linear conjugate gradient, you may look at
    http://en.wikipedia.org/wiki/Conjugate_gradient_method .

    (This reference describes linear CG but not converting it to use
    the R operator instead of an explicit representation of the Hessian)

    Parameters
    ----------
    params : WRITEME
    f : theano_like
        A theano expression which is quadratic with POSITIVE DEFINITE hessian
        in x
    x : list
        List of theano shared variables that influence f
    tol : float
        Minimization halts when the norm of the gradient is smaller than tol

    Returns
    -------
    rval : theano_like
        The solution in form of a symbolic expression (or list of
        symbolic expressions)
    """
    provided_as_list = True
    if not isinstance(params, (list,tuple)):
        params = [params]
        provided_as_list = False

    n_params = len(params)
    def loop(rsold, *args):
        ps = args[:n_params]
        rs = args[n_params:2*n_params]
        xs = args[2*n_params:]

        Aps = []
        for param in params:
            rval = tensor.Rop(tensor.grad(fn,param), params, ps)
            if isinstance(rval, (list, tuple)):
                Aps.append(rval[0])
            else:
                Aps.append(rval)
        alpha = rsold/ sum( (x*y).sum() for x,y in zip(Aps, ps) )
        xs = [ x - alpha*p for x,p in zip(xs,ps)]
        rs = [ r - alpha*Ap for r,Ap in zip(rs,Aps)]
        rsnew = sum( (r*r).sum() for r in rs)
        ps = [ r + rsnew/rsold*p for r,p in zip(rs,ps)]
        return [rsnew]+ps+rs+xs, theano.scan_module.until(rsnew < tol)

    r0s = tensor.grad(fn, params)
    if not isinstance(r0s, (list,tuple)):
        r0s = [r0s]
    p0s = [x for x in r0s]
    x0s = params
    rsold = sum( (r*r).sum() for r in r0s)
    outs, updates = theano.scan( loop,
                                outputs_info = [rsold] + p0s+r0s+x0s,
                                n_steps = max_iters,
                                name = 'linear_conjugate_gradient')
    fxs = outs[1+2*n_params:]
    fxs = [ifelse(rsold <tol, x0, x[-1]) for x0,x in zip(x0s, fxs)]
    if not provided_as_list:
        return fxs[0]
    else:
        return fxs

########NEW FILE########
__FILENAME__ = linesearch
"""
Note: this code is a Theano translation of the linesearch implemented in
scipy.optimize.linesearch

See :
    https://github.com/scipy/scipy/blob/master/scipy/optimize/linesearch.py
"""

import theano
import theano.tensor as TT
from theano.ifelse import ifelse
from theano.sandbox.scan import scan
import numpy

one = TT.constant(numpy.asarray(1, dtype=theano.config.floatX))
zero = TT.constant(numpy.asarray(0, dtype=theano.config.floatX))
nan = TT.constant(numpy.asarray(numpy.nan, dtype=theano.config.floatX))

true = TT.constant(numpy.asarray(1, dtype='int8'))
false = TT.constant(numpy.asarray(0, dtype='int8'))


def lazy_or(name='none', *args):
    """
    .. todo::

        WRITEME
    """
    def apply_me(args):
        if len(args) == 1:
            return args[0]
        else:
            rval = ifelse(args[0], true, apply_me(args[1:]),
                          name=name + str(len(args)))
            return rval
    return apply_me(args)


def lazy_and(name='node', *args):
    """
    .. todo::

        WRITEME
    """
    def apply_me(args):
        if len(args) == 1:
            return args[0]
        else:
            rval = ifelse(TT.eq(args[0], zero), false, apply_me(args[1:]),
                         name=name + str(len(args)))
            return rval
    return apply_me(args)


def my_not(arg):
    """
    .. todo::

        WRITEME
    """
    return TT.eq(arg, zero)

def constant(value):
    """
    .. todo::

        WRITEME
    """
    return TT.constant(numpy.asarray(value, dtype=theano.config.floatX))

def scalar_armijo_search(phi, phi0, derphi0, c1=constant(1e-4),
                         n_iters=10, profile=0):
    """
    .. todo::

        WRITEME
    """
    alpha0 = one
    phi_a0 = phi(alpha0)
    alpha1 = -(derphi0) * alpha0 ** 2 / 2.0 /\
            (phi_a0 - phi0 - derphi0 * alpha0)
    phi_a1 = phi(alpha1)

    csol1 = phi_a0 <= phi0 + c1 * derphi0
    csol2 = phi_a1 <= phi0 + c1 * alpha1 * derphi0

    def armijo(alpha0, alpha1, phi_a0, phi_a1):
        factor = alpha0 ** 2 * alpha1 ** 2 * (alpha1 - alpha0)
        a = alpha0 ** 2 * (phi_a1 - phi0 - derphi0 * alpha1) - \
            alpha1 ** 2 * (phi_a0 - phi0 - derphi0 * alpha0)
        a = a / factor
        b = -alpha0 ** 3 * (phi_a1 - phi0 - derphi0 * alpha1) + \
            alpha1 ** 3 * (phi_a0 - phi0 - derphi0 * alpha0)
        b = b / factor

        alpha2 = (-b + TT.sqrt(abs(b ** 2 - 3 * a * derphi0))) / (3.0 * a)
        phi_a2 = phi(alpha2)

        end_condition = phi_a2 <= phi0 + c1 * alpha2 * derphi0
        end_condition = TT.bitwise_or(
            TT.isnan(alpha2), end_condition)
        end_condition = TT.bitwise_or(
            TT.isinf(alpha2), end_condition)
        alpha2 = TT.switch(
            TT.bitwise_or(alpha1 - alpha2 > alpha1 / constant(2.),
                  one - alpha2 / alpha1 < 0.96),
            alpha1 / constant(2.),
            alpha2)
        return [alpha1, alpha2, phi_a1, phi_a2], \
                theano.scan_module.until(end_condition)

    states = []
    states += [TT.unbroadcast(TT.shape_padleft(alpha0), 0)]
    states += [TT.unbroadcast(TT.shape_padleft(alpha1), 0)]
    states += [TT.unbroadcast(TT.shape_padleft(phi_a0), 0)]
    states += [TT.unbroadcast(TT.shape_padleft(phi_a1), 0)]
    # print 'armijo'
    rvals, _ = scan(
                armijo,
                states=states,
                n_steps=n_iters,
                name='armijo',
                mode=theano.Mode(linker='cvm'),
                profile=profile)

    sol_scan = rvals[1][0]
    a_opt = ifelse(csol1, one,
                ifelse(csol2, alpha1,
                    sol_scan))
    score = ifelse(csol1, phi_a0,
                   ifelse(csol2, phi_a1,
                          rvals[2][0]))
    return a_opt, score


def scalar_search_wolfe2(phi,
                         derphi,
                         phi0=None,
                         old_phi0=None,
                         derphi0=None,
                         n_iters=20,
                         c1=1e-4,
                         c2=0.9,
                        profile=False):
    """
    Find alpha that satisfies strong Wolfe conditions.

    alpha > 0 is assumed to be a descent direction.

    Parameters
    ----------
    phi : callable f(x)
        Objective scalar function.
    derphi : callable f'(x)
        Objective function derivative (can be None)
    phi0 : float, optional
        Value of phi at s=0
    old_phi0 : float, optional
        Value of phi at previous point
    derphi0 : float, optional
        Value of derphi at s=0
    c1 : float
        Parameter for Armijo condition rule.
    c2 : float
        Parameter for curvature condition rule.
    profile : flag (boolean)
        True if you want printouts of profiling information

    Returns
    -------
    alpha_star : float
        Best alpha
    phi_star : WRITEME
        phi at alpha_star
    phi0 : WRITEME
        phi at 0
    derphi_star : WRITEME
        derphi at alpha_star

    Notes
    -----
    Uses the line search algorithm to enforce strong Wolfe
    conditions.  See Wright and Nocedal, 'Numerical Optimization',
    1999, pg. 59-60.

    For the zoom phase it uses an algorithm by [...].

    """

    if phi0 is None:
        phi0 = phi(zero)
    else:
        phi0 = phi0

    if derphi0 is None and derphi is not None:
        derphi0 = derphi(zero)
    else:
        derphi0 = derphi0

    alpha0 = zero
    alpha0.name = 'alpha0'
    if old_phi0 is not None:
        alpha1 = TT.minimum(one,
                            numpy.asarray(1.01, dtype=theano.config.floatX) *
                            numpy.asarray(2, dtype=theano.config.floatX) * \
                            (phi0 - old_phi0) / derphi0)
    else:
        old_phi0 = nan
        alpha1 = one

    alpha1 = TT.switch(alpha1 < zero, one, alpha1)
    alpha1.name = 'alpha1'

    # This shouldn't happen. Perhaps the increment has slipped below
    # machine precision?  For now, set the return variables skip the
    # useless while loop, and raise warnflag=2 due to possible imprecision.
    phi0 = TT.switch(TT.eq(alpha1, zero), old_phi0, phi0)
    # I need a lazyif for alpha1 == 0 !!!
    phi_a1 = ifelse(TT.eq(alpha1, zero), phi0,
                    phi(alpha1), name='phi_a1')
    phi_a1.name = 'phi_a1'

    phi_a0 = phi0
    phi_a0.name = 'phi_a0'
    derphi_a0 = derphi0
    derphi_a0.name = 'derphi_a0'
    # Make sure variables are tensors otherwise strange things happen
    c1 = TT.as_tensor_variable(c1)
    c2 = TT.as_tensor_variable(c2)
    maxiter = n_iters

    def while_search(alpha0, alpha1, phi_a0, phi_a1, derphi_a0, i_t,
                    alpha_star, phi_star, derphi_star):
        derphi_a1 = derphi(alpha1)
        cond1 = TT.bitwise_or(phi_a1 > phi0 + c1 * alpha1 * derphi0,
                              TT.bitwise_and(phi_a1 >= phi_a0, i_t > zero))
        cond2 = abs(derphi_a1) <= -c2 * derphi0
        cond3 = derphi_a1 >= zero
        alpha_star_c1, phi_star_c1, derphi_star_c1 = \
                _zoom(alpha0, alpha1, phi_a0, phi_a1, derphi_a0,
                      phi, derphi, phi0, derphi0, c1, c2,
                     profile=profile)
        alpha_star_c3, phi_star_c3, derphi_star_c3 = \
                _zoom(alpha1, alpha0, phi_a1, phi_a0, derphi_a1, phi,
                      derphi, phi0, derphi0, c1, c2,
                     profile=profile)
        nw_alpha1 = alpha1 * numpy.asarray(2, dtype=theano.config.floatX)
        nw_phi = phi(nw_alpha1)
        alpha_star, phi_star, derphi_star = \
                ifelse(cond1,
                          (alpha_star_c1, phi_star_c1, derphi_star_c1),
                ifelse(cond2,
                          (alpha1, phi_a1, derphi_a1),
                ifelse(cond3,
                          (alpha_star_c3, phi_star_c3, derphi_star_c3),
                           (nw_alpha1, nw_phi, nan),
                      name='alphastar_c3'),
                      name='alphastar_c2'),
                      name='alphastar_c1')

        return ([alpha1,
                 nw_alpha1,
                 phi_a1,
                 ifelse(lazy_or('allconds',
                                cond1,
                                cond2,
                                cond3),
                        phi_a1,
                        nw_phi,
                        name='nwphi1'),
                 ifelse(cond1, derphi_a0, derphi_a1, name='derphi'),
                 i_t + one,
                 alpha_star,
                 phi_star,
                 derphi_star],
                theano.scan_module.scan_utils.until(
                    lazy_or('until_cond_',
                            TT.eq(nw_alpha1, zero),
                            cond1,
                            cond2,
                            cond3)))
    states = []
    states += [TT.unbroadcast(TT.shape_padleft(alpha0), 0)]
    states += [TT.unbroadcast(TT.shape_padleft(alpha1), 0)]
    states += [TT.unbroadcast(TT.shape_padleft(phi_a0), 0)]
    states += [TT.unbroadcast(TT.shape_padleft(phi_a1), 0)]
    states += [TT.unbroadcast(TT.shape_padleft(derphi_a0), 0)]
    # i_t
    states += [TT.unbroadcast(TT.shape_padleft(zero), 0)]
    # alpha_star
    states += [TT.unbroadcast(TT.shape_padleft(zero), 0)]
    # phi_star
    states += [TT.unbroadcast(TT.shape_padleft(zero), 0)]
    # derphi_star
    states += [TT.unbroadcast(TT.shape_padleft(zero), 0)]
    # print 'while_search'
    outs, updates = scan(while_search,
                         states=states,
                         n_steps=maxiter,
                         name='while_search',
                         mode=theano.Mode(linker='cvm_nogc'),
                         profile=profile)
    # print 'done_while_search'
    out3 = outs[-3][0]
    out2 = outs[-2][0]
    out1 = outs[-1][0]
    alpha_star, phi_star, derphi_star = \
            ifelse(TT.eq(alpha1, zero),
                        (nan, phi0, nan),
                        (out3, out2, out1), name='main_alphastar')
    return alpha_star, phi_star,  phi0, derphi_star


def _cubicmin(a, fa, fpa, b, fb, c, fc):
    """
    Finds the minimizer for a cubic polynomial that goes through the
    points (a,fa), (b,fb), and (c,fc) with derivative at a of fpa.

    If no minimizer can be found return None

    Parameters
    ----------
    a : WRITEME
    fa : WRITEME
    fpa : WRITEME
    b : WRITEME
    fb : WRITEME
    c : WRITEME
    fc : WRITEME

    Returns
    -------
    WRITEME
    """
    # f(x) = A *(x-a)^3 + B*(x-a)^2 + C*(x-a) + D
    a.name = 'a'
    fa.name = 'fa'
    fpa.name = 'fpa'
    fb.name = 'fb'
    fc.name = 'fc'
    C = fpa
    D = fa
    db = b - a
    dc = c - a

    denom = (db * dc) ** 2 * (db - dc)
    d1_00 = dc ** 2
    d1_01 = -db ** 2
    d1_10 = -dc ** 3
    d1_11 = db ** 3
    t1_0 = fb - fa - C * db
    t1_1 = fc - fa - C * dc
    A = d1_00 * t1_0 + d1_01 * t1_1
    B = d1_10 * t1_0 + d1_11 * t1_1
    A /= denom
    B /= denom
    radical = B * B - 3 * A * C
    radical.name = 'radical'
    db.name = 'db'
    dc.name = 'dc'
    b.name = 'b'
    c.name = 'c'
    A.name = 'A'
    #cond = TT.bitwise_or(radical < zero,
    #       TT.bitwise_or(TT.eq(db,zero),
    #       TT.bitwise_or(TT.eq(dc,zero),
    #       TT.bitwise_or(TT.eq(b, c),
    #                    TT.eq(A, zero)))))

    cond = lazy_or('cubicmin',
                   radical < zero,
                   TT.eq(db, zero),
                   TT.eq(dc, zero),
                   TT.eq(b, c),
                   TT.eq(A, zero))
    # Note: `lazy if` would make more sense, but it is not
    #       implemented in C right now
    xmin = TT.switch(cond, constant(numpy.nan),
                         a + (-B + TT.sqrt(radical)) / (3 * A))
    return xmin


def _quadmin(a, fa, fpa, b, fb):
    """
    Finds the minimizer for a quadratic polynomial that goes through
    the points (a,fa), (b,fb) with derivative at a of fpa.

    Parameters
    ----------
    a : WRITEME
    fa : WRITEME
    fpa : WRITEME
    b : WRITEME
    fb : WRITEME

    Returns
    -------
    WRITEME
    """
    # f(x) = B*(x-a)^2 + C*(x-a) + D
    D = fa
    C = fpa
    db = b - a * one

    B = (fb - D - C * db) / (db * db)
    # Note : `lazy if` would make more sense, but it is not
    #        implemented in C right now
    # lazy_or('quadmin',TT.eq(db , zero), (B <= zero)),
    # xmin = TT.switch(TT.bitwise_or(TT.eq(db,zero), B <= zero),
    xmin = TT.switch(lazy_or(TT.eq(db, zero), B <= zero),
                     nan,
                     a - C /\
                     (numpy.asarray(2, dtype=theano.config.floatX) * B))
    return xmin


def _zoom(a_lo, a_hi, phi_lo, phi_hi, derphi_lo,
          phi, derphi, phi0, derphi0, c1, c2,
          n_iters=10,
          profile=False):
    """
    WRITEME

    Part of the optimization algorithm in `scalar_search_wolfe2`.

    Parameters
    ----------
    a_lo : float
        Step size
    a_hi : float
        Step size
    phi_lo : float
        Value of f at a_lo
    phi_hi : float
        Value of f at a_hi
    derphi_lo : float
        Value of derivative at a_lo
    phi : callable
        Generates computational graph
    derphi : callable
        Generates computational graph
    phi0 : float
        Value of f at 0
    derphi0 : float
        Value of the derivative at 0
    c1 : float
        Wolfe parameter
    c2 : float
        Wolfe parameter
    profile : bool
        True if you want printouts of profiling information
    """
    # Function reprensenting the computations of one step of the while loop
    def while_zoom(phi_rec, a_rec, a_lo, a_hi, phi_hi,
                   phi_lo, derphi_lo, a_star, val_star, valprime):
        # interpolate to find a trial step length between a_lo and
        # a_hi Need to choose interpolation here.  Use cubic
        # interpolation and then if the result is within delta *
        # dalpha or outside of the interval bounded by a_lo or a_hi
        # then use quadratic interpolation, if the result is still too
        # close, then use bisection
        dalpha = a_hi - a_lo
        a = TT.switch(dalpha < zero, a_hi, a_lo)
        b = TT.switch(dalpha < zero, a_lo, a_hi)

        # minimizer of cubic interpolant
        # (uses phi_lo, derphi_lo, phi_hi, and the most recent value of phi)
        #
        # if the result is too close to the end points (or out of the
        # interval) then use quadratic interpolation with phi_lo,
        # derphi_lo and phi_hi if the result is stil too close to the
        # end points (or out of the interval) then use bisection

        # cubic interpolation
        cchk = delta1 * dalpha
        a_j_cubic = _cubicmin(a_lo, phi_lo, derphi_lo,
                              a_hi, phi_hi, a_rec, phi_rec)
        # quadric interpolation
        qchk = delta2 * dalpha
        a_j_quad = _quadmin(a_lo, phi_lo, derphi_lo, a_hi, phi_hi)
        cond_q = lazy_or('condq',
                         TT.isnan(a_j_quad),
                         a_j_quad > b - qchk,
                         a_j_quad < a + qchk)
        a_j_quad = TT.switch(cond_q, a_lo +
                             numpy.asarray(0.5, dtype=theano.config.floatX) * \
                             dalpha, a_j_quad)

        # pick between the two ..
        cond_c = lazy_or('condc',
                         TT.isnan(a_j_cubic),
                         TT.bitwise_or(a_j_cubic > b - cchk,
                                       a_j_cubic < a + cchk))
        # this lazy if actually decides if we need to run the quadric
        # interpolation
        a_j = TT.switch(cond_c, a_j_quad, a_j_cubic)
        #a_j = ifelse(cond_c, a_j_quad,  a_j_cubic)

        # Check new value of a_j
        phi_aj = phi(a_j)
        derphi_aj = derphi(a_j)

        stop = lazy_and('stop',
                        TT.bitwise_and(phi_aj <= phi0 + c1 * a_j * derphi0,
                                       phi_aj < phi_lo),
                        abs(derphi_aj) <= -c2 * derphi0)

        cond1 = TT.bitwise_or(phi_aj > phi0 + c1 * a_j * derphi0,
                              phi_aj >= phi_lo)
        cond2 = derphi_aj * (a_hi - a_lo) >= zero

        # Switches just make more sense here because they have a C
        # implementation and they get composed
        phi_rec = ifelse(cond1,
                         phi_hi,
                         TT.switch(cond2, phi_hi, phi_lo),
                         name='phi_rec')
        a_rec = ifelse(cond1,
                       a_hi,
                       TT.switch(cond2, a_hi, a_lo),
                         name='a_rec')
        a_hi = ifelse(cond1, a_j,
                      TT.switch(cond2, a_lo, a_hi),
                      name='a_hi')
        phi_hi = ifelse(cond1, phi_aj,
                        TT.switch(cond2, phi_lo, phi_hi),
                        name='phi_hi')

        a_lo = TT.switch(cond1, a_lo, a_j)
        phi_lo = TT.switch(cond1, phi_lo, phi_aj)
        derphi_lo = ifelse(cond1, derphi_lo, derphi_aj, name='derphi_lo')

        a_star = a_j
        val_star = phi_aj
        valprime = ifelse(cond1, nan,
                          TT.switch(cond2, derphi_aj, nan), name='valprime')

        return ([phi_rec,
                 a_rec,
                 a_lo,
                 a_hi,
                 phi_hi,
                 phi_lo,
                 derphi_lo,
                 a_star,
                 val_star,
                 valprime],
                theano.scan_module.scan_utils.until(stop))

    maxiter = n_iters
    # cubic interpolant check
    delta1 = TT.constant(numpy.asarray(0.2,
                                       dtype=theano.config.floatX))
    # quadratic interpolant check
    delta2 = TT.constant(numpy.asarray(0.1,
                                       dtype=theano.config.floatX))
    phi_rec = phi0
    a_rec = zero

    # Initial iteration

    dalpha = a_hi - a_lo
    a = TT.switch(dalpha < zero, a_hi, a_lo)
    b = TT.switch(dalpha < zero, a_lo, a_hi)
    #a = ifelse(dalpha < 0, a_hi, a_lo)
    #b = ifelse(dalpha < 0, a_lo, a_hi)

    # minimizer of cubic interpolant
    # (uses phi_lo, derphi_lo, phi_hi, and the most recent value of phi)
    #
    # if the result is too close to the end points (or out of the
    # interval) then use quadratic interpolation with phi_lo,
    # derphi_lo and phi_hi if the result is stil too close to the
    # end points (or out of the interval) then use bisection

    # quadric interpolation
    qchk = delta2 * dalpha
    a_j = _quadmin(a_lo, phi_lo, derphi_lo, a_hi, phi_hi)
    cond_q = lazy_or('mcond_q',
                     TT.isnan(a_j),
                     TT.bitwise_or(a_j > b - qchk,
                                   a_j < a + qchk))

    a_j = TT.switch(cond_q, a_lo +
                    numpy.asarray(0.5, dtype=theano.config.floatX) * \
                    dalpha, a_j)

    # Check new value of a_j
    phi_aj = phi(a_j)
    derphi_aj = derphi(a_j)

    cond1 = TT.bitwise_or(phi_aj > phi0 + c1 * a_j * derphi0,
                          phi_aj >= phi_lo)
    cond2 = derphi_aj * (a_hi - a_lo) >= zero

    # Switches just make more sense here because they have a C
    # implementation and they get composed
    phi_rec = ifelse(cond1,
                     phi_hi,
                     TT.switch(cond2, phi_hi, phi_lo),
                     name='mphirec')
    a_rec = ifelse(cond1,
                   a_hi,
                   TT.switch(cond2, a_hi, a_lo),
                   name='marec')
    a_hi = ifelse(cond1,
                  a_j,
                  TT.switch(cond2, a_lo, a_hi),
                  name='mahi')
    phi_hi = ifelse(cond1,
                    phi_aj,
                    TT.switch(cond2, phi_lo, phi_hi),
                    name='mphihi')

    onlyif = lazy_and('only_if',
                      TT.bitwise_and(phi_aj <= phi0 + c1 * a_j * derphi0,
                                     phi_aj < phi_lo),
                      abs(derphi_aj) <= -c2 * derphi0)

    a_lo = TT.switch(cond1, a_lo, a_j)
    phi_lo = TT.switch(cond1, phi_lo, phi_aj)
    derphi_lo = ifelse(cond1, derphi_lo, derphi_aj, name='derphi_lo_main')
    phi_rec.name = 'phi_rec'
    a_rec.name = 'a_rec'
    a_lo.name = 'a_lo'
    a_hi.name = 'a_hi'
    phi_hi.name = 'phi_hi'
    phi_lo.name = 'phi_lo'
    derphi_lo.name = 'derphi_lo'
    vderphi_aj = ifelse(cond1, nan, TT.switch(cond2, derphi_aj, nan),
                        name='vderphi_aj')
    states = []
    states += [TT.unbroadcast(TT.shape_padleft(phi_rec), 0)]
    states += [TT.unbroadcast(TT.shape_padleft(a_rec), 0)]
    states += [TT.unbroadcast(TT.shape_padleft(a_lo), 0)]
    states += [TT.unbroadcast(TT.shape_padleft(a_hi), 0)]
    states += [TT.unbroadcast(TT.shape_padleft(phi_hi), 0)]
    states += [TT.unbroadcast(TT.shape_padleft(phi_lo), 0)]
    states += [TT.unbroadcast(TT.shape_padleft(derphi_lo), 0)]
    states += [TT.unbroadcast(TT.shape_padleft(zero), 0)]
    states += [TT.unbroadcast(TT.shape_padleft(zero), 0)]
    states += [TT.unbroadcast(TT.shape_padleft(zero), 0)]
    # print'while_zoom'
    outs, updates = scan(while_zoom,
                         states=states,
                         n_steps=maxiter,
                         name='while_zoom',
                         mode=theano.Mode(linker='cvm_nogc'),
                         profile=profile)
    # print 'done_while'
    a_star = ifelse(onlyif, a_j, outs[7][0], name='astar')
    val_star = ifelse(onlyif, phi_aj, outs[8][0], name='valstar')
    valprime = ifelse(onlyif, vderphi_aj, outs[9][0], name='valprime')

    ## WARNING !! I ignore updates given by scan which I should not do !!!
    return a_star, val_star, valprime

########NEW FILE########
__FILENAME__ = minres
"""
Note: this code is inspired from the following matlab source :
    http://www.stanford.edu/group/SOL/software/minres.html
"""

import theano
import theano.tensor as TT
from theano.sandbox.scan import scan
import numpy
from pylearn2.utils import constantX
from pylearn2.expr.basic import multiple_switch, symGivens2, \
        sqrt_inner_product, inner_product

# Messages that matches the flag value returned by the method
messages = [
    ' beta1 = 0.  The exact solution is  x = 0.                    ',  # 0
    ' A solution to (poss. singular) Ax = b found, given rtol.     ',  # 1
    ' A least-squares solution was found, given rtol.              ',  # 2
    ' A solution to (poss. singular) Ax = b found, given eps.      ',  # 3
    ' A least-squares solution was found, given eps.               ',  # 4
    ' x has converged to an eigenvector.                           ',  # 5
    ' xnorm has exceeded maxxnorm.                                 ',  # 6
    ' Acond has exceeded Acondlim.                                 ',  # 7
    ' The iteration limit was reached.                             ',  # 8
    ' A least-squares solution for singular LS problem, given eps. ',  # 9
    ' A least-squares solution for singular LS problem, given rtol.',  # 10
    ' A null vector obtained, given rtol.                          ',  # 11
    ' Numbers are too small to continue computation                ']  # 12


def minres(compute_Av,
           bs,
           rtol=constantX(1e-6),
           maxit=20,
           Ms=None,
           shift=constantX(0.),
           maxxnorm=constantX(1e15),
           Acondlim=constantX(1e16),
           profile=0):
    """
    Attempts to find the minimum-length and minimum-residual-norm
    solution :math:`x` to the system of linear equations :math:`A*x = b`
    or least squares problem :math:`\\min||Ax-b||`. 

    The n-by-n coefficient matrix A must be symmetric (but need not be
    positive definite or invertible). The right-hand-side column vector
    b must have length n.

    .. note:: This code is inspired from
        http://www.stanford.edu/group/SOL/software/minres.html .

    Parameters
    ----------
    compute_Av : callable
        Callable returing the symbolic expression for
        `Av` (the product of matrix A with some vector v).
        `v` should be a list of tensors, where the vector v means
        the vector obtain by concatenating and flattening all tensors in v
    bs : list
        List of Theano expressions. We are looking to compute `A^-1\dot bs`.
    rtol : float, optional
        Specifies the tolerance of the method.  Default is 1e-6.
    maxit : int, positive, optional
        Specifies the maximum number of iterations. Default is 20.
    Ms : list
        List of theano expression of same shape as `bs`. The method uses
        these to precondition with diag(Ms)
    shift : float, optional
        Default is 0.  Effectively solve the system (A - shift I) * x = b.
    maxxnorm : float, positive, optional
        Maximum bound on NORM(x). Default is 1e14.
    Acondlim : float, positive, optional
        Maximum bound on COND(A). Default is 1e15.
    show : bool
        If True, show iterations, otherwise suppress outputs. Default is
        False.

    Returns
    -------
    x : list
        List of Theano tensor representing the solution
    flag : tensor_like
        Theano int scalar - convergence flag

            0. beta1 = 0.  The exact solution is  x = 0.
            1. A solution to (poss. singular) Ax = b found, given rtol.
            2. Pseudoinverse solution for singular LS problem, given rtol.
            3. A solution to (poss. singular) Ax = b found, given eps.
            4. Pseudoinverse solution for singular LS problem, given eps.
            5. x has converged to an eigenvector.
            6. xnorm has exceeded maxxnorm.
            7. Acond has exceeded Acondlim.
            8. The iteration limit was reached.
            9. 10. It is a least squares problem but no converged
               solution yet.
    iter : int
        Iteration number at which x was computed: `0 <= iter <= maxit`.
    relres : float
        Real positive, the relative residual is defined as
        NORM(b-A*x)/(NORM(A) * NORM(x) + NORM(b)),
        computed recurrently here.  If flag is 1 or 3,  relres <= TOL.
    relAres : float
        Real positive, the relative-NORM(Ar) := NORM(Ar) / NORM(A)
        computed recurrently here. If flag is 2 or 4, relAres <= TOL.
    Anorm : float
        Real positive, estimate of matrix 2-norm of A.
    Acond : float
        Real positive, estimate of condition number of A with respect to
        2-norm.
    xnorm : float
        Non-negative positive, recurrently computed NORM(x)
    Axnorm : float
        Non-negative positive, recurrently computed NORM(A * x).

    References
    ----------
    .. [1] Choi, Sou-Cheng. Iterative Methods for Singular Linear 
           Equations and Least-Squares Problems, PhD Dissertation,
           Stanford University, 2006.
    """

    if not isinstance(bs, (tuple, list)):
        bs = [bs]
        return_as_list = False
    else:
        bs = list(bs)
        return_as_list = True

    eps = constantX(1e-23)

    # Initialise
    beta1 = sqrt_inner_product(bs)

    #------------------------------------------------------------------
    # Set up p and v for the first Lanczos vector v1.
    # p  =  beta1 P' v1,  where  P = C**(-1).
    # v is really P' v1.
    #------------------------------------------------------------------
    r3s = [b for b in bs]
    r2s = [b for b in bs]
    r1s = [b for b in bs]
    if Ms is not None:
        r3s = [b / m for b, m in zip(bs, Ms)]
        beta1 = sqrt_inner_product(r3s, bs)
    #------------------------------------------------------------------
    ## Initialize other quantities.
    # Note that Anorm has been initialized by IsOpSym6.
    # ------------------------------------------------------------------
    bnorm = beta1
    n_params = len(bs)

    def loop(niter,
             beta,
             betan,
             phi,
             Acond,
             cs,
             dbarn,
             eplnn,
             rnorm,
             sn,
             Tnorm,
             rnorml,
             xnorm,
             Dnorm,
             gamma,
             pnorm,
             gammal,
             Axnorm,
             relrnorm,
             relArnorml,
             Anorm,
             flag,
             *args):
        #-----------------------------------------------------------------
        ## Obtain quantities for the next Lanczos vector vk+1, k = 1, 2,...
        # The general iteration is similar to the case k = 1 with v0 = 0:
        #
        #   p1      = Operator * v1  -  beta1 * v0,
        #   alpha1  = v1'p1,
        #   q2      = p2  -  alpha1 * v1,
        #   beta2^2 = q2'q2,
        #   v2      = (1/beta2) q2.
        #
        # Again, p = betak P vk,  where  P = C**(-1).
        # .... more description needed.
        #-----------------------------------------------------------------
        xs = args[0 * n_params: 1 * n_params]
        r1s = args[1 * n_params: 2 * n_params]
        r2s = args[2 * n_params: 3 * n_params]
        r3s = args[3 * n_params: 4 * n_params]
        dls = args[4 * n_params: 5 * n_params]
        ds = args[5 * n_params: 6 * n_params]
        betal = beta
        beta = betan
        vs = [r3 / beta for r3 in r3s]
        r3s, upds = compute_Av(*vs)

        r3s = [r3 - shift * v for r3, v in zip(r3s, vs)]
        r3s = [TT.switch(TT.ge(niter, constantX(1.)),
                         r3 - (beta / betal) * r1,
                         r3) for r3, r1 in zip(r3s, r1s)]

        alpha = inner_product(r3s, vs)
        r3s = [r3 - (alpha / beta) * r2 for r3, r2 in zip(r3s, r2s)]
        r1s = [r2 for r2 in r2s]
        r2s = [r3 for r3 in r3s]
        if Ms is not None:
            r3s = [r3 / M for r3, M in zip(r3s, Ms)]
            betan = sqrt_inner_product(r2s, r3s)
        else:
            betan = sqrt_inner_product(r3s)
        pnorml = pnorm
        pnorm = TT.switch(TT.eq(niter, constantX(0.)),
                          TT.sqrt(TT.sqr(alpha) + TT.sqr(betan)),
                          TT.sqrt(TT.sqr(alpha) + TT.sqr(betan) +
                                  TT.sqr(beta)))

        #-----------------------------------------------------------------
        ## Apply previous rotation Qk-1 to get
        #   [dlta_k epln_{k+1}] = [cs  sn][dbar_k    0      ]
        #   [gbar_k  dbar_{k+1} ]   [sn -cs][alpha_k beta_{k+1}].
        #-----------------------------------------------------------------
        dbar = dbarn
        epln = eplnn
        dlta = cs * dbar + sn * alpha
        gbar = sn * dbar - cs * alpha

        eplnn = sn * betan
        dbarn = -cs * betan

        ## Compute the current plane rotation Qk
        gammal2 = gammal
        gammal = gamma
        cs, sn, gamma = symGivens2(gbar, betan)
        tau = cs * phi
        phi = sn * phi
        Axnorm = TT.sqrt(TT.sqr(Axnorm) + TT.sqr(tau))
        # Update d

        dl2s = [dl for dl in dls]
        dls = [d for d in ds]
        ds = [TT.switch(TT.neq(gamma, constantX(0.)),
                        (v - epln * dl2 - dlta * dl) / gamma,
                        v)
              for v, dl2, dl in zip(vs, dl2s, dls)]
        d_norm = TT.switch(TT.neq(gamma, constantX(0.)),
                           sqrt_inner_product(ds),
                           constantX(numpy.inf))

        # Update x except if it will become too big
        xnorml = xnorm
        dl2s = [x for x in xs]
        xs = [x + tau * d for x, d in zip(xs, ds)]

        xnorm = sqrt_inner_product(xs)
        xs = [TT.switch(TT.ge(xnorm, maxxnorm),
                        dl2, x)
              for dl2, x in zip(dl2s, xs)]

        flag = TT.switch(TT.ge(xnorm, maxxnorm),
                         constantX(6.), flag)
        # Estimate various norms
        rnorml = rnorm  # ||r_{k-1}||
        Anorml = Anorm
        Acondl = Acond
        relrnorml = relrnorm
        flag_no_6 = TT.neq(flag, constantX(6.))
        Dnorm = TT.switch(flag_no_6,
                          TT.sqrt(TT.sqr(Dnorm) + TT.sqr(d_norm)),
                          Dnorm)
        xnorm = TT.switch(flag_no_6, sqrt_inner_product(xs), xnorm)
        rnorm = TT.switch(flag_no_6, phi, rnorm)
        relrnorm = TT.switch(flag_no_6,
                             rnorm / (Anorm * xnorm + bnorm),
                             relrnorm)
        Tnorm = TT.switch(flag_no_6,
                          TT.switch(TT.eq(niter, constantX(0.)),
                                    TT.sqrt(TT.sqr(alpha) + TT.sqr(betan)),
                                    TT.sqrt(TT.sqr(Tnorm) +
                                            TT.sqr(beta) +
                                            TT.sqr(alpha) +
                                            TT.sqr(betan))),
                          Tnorm)
        Anorm = TT.maximum(Anorm, pnorm)
        Acond = Anorm * Dnorm
        rootl = TT.sqrt(TT.sqr(gbar) + TT.sqr(dbarn))
        Anorml = rnorml * rootl
        relArnorml = rootl / Anorm

        #---------------------------------------------------------------
        # See if any of the stopping criteria are satisfied.
        # In rare cases, flag is already -1 from above (Abar = const*I).
        #---------------------------------------------------------------
        epsx = Anorm * xnorm * eps
        epsr = Anorm * xnorm * rtol
        #Test for singular Hk (hence singular A)
        # or x is already an LS solution (so again A must be singular).
        t1 = constantX(1) + relrnorm
        t2 = constantX(1) + relArnorml

        flag = TT.switch(
            TT.bitwise_or(TT.eq(flag, constantX(0)),
                          TT.eq(flag, constantX(6))),
            multiple_switch(TT.le(t1, constantX(1)),
                            constantX(3),
                            TT.le(t2, constantX(1)),
                            constantX(4),
                            TT.le(relrnorm, rtol),
                            constantX(1),
                            TT.le(Anorm, constantX(1e-20)),
                            constantX(12),
                            TT.le(relArnorml, rtol),
                            constantX(10),
                            TT.ge(epsx, beta1),
                            constantX(5),
                            TT.ge(xnorm, maxxnorm),
                            constantX(6),
                            TT.ge(niter, TT.cast(maxit,
                                                 theano.config.floatX)),
                            constantX(8),
                            flag),
            flag)

        flag = TT.switch(TT.lt(Axnorm, rtol * Anorm * xnorm),
                         constantX(11.),
                         flag)
        return [niter + constantX(1.),
                beta,
                betan,
                phi,
                Acond,
                cs,
                dbarn,
                eplnn,
                rnorm,
                sn,
                Tnorm,
                rnorml,
                xnorm,
                Dnorm,
                gamma,
                pnorm,
                gammal,
                Axnorm,
                relrnorm,
                relArnorml,
                Anorm,
                flag] + xs + r1s + r2s + r3s + dls + ds, upds, \
                theano.scan_module.scan_utils.until(TT.neq(flag, 0))

    states = []
    # 0 niter
    states.append(constantX([0]))
    # 1 beta
    states.append(constantX([0]))
    # 2 betan
    states.append(TT.unbroadcast(TT.shape_padleft(beta1), 0))
    # 3 phi
    states.append(TT.unbroadcast(TT.shape_padleft(beta1), 0))
    # 4 Acond
    states.append(constantX([1]))
    # 5 cs
    states.append(constantX([-1]))
    # 6 dbarn
    states.append(constantX([0]))
    # 7 eplnn
    states.append(constantX([0]))
    # 8 rnorm
    states.append(TT.unbroadcast(TT.shape_padleft(beta1), 0))
    # 9 sn
    states.append(constantX([0]))
    # 10 Tnorm
    states.append(constantX([0]))
    # 11 rnorml
    states.append(TT.unbroadcast(TT.shape_padleft(beta1), 0))
    # 12 xnorm
    states.append(constantX([0]))
    # 13 Dnorm
    states.append(constantX([0]))
    # 14 gamma
    states.append(constantX([0]))
    # 15 pnorm
    states.append(constantX([0]))
    # 16 gammal
    states.append(constantX([0]))
    # 17 Axnorm
    states.append(constantX([0]))
    # 18 relrnorm
    states.append(constantX([1]))
    # 19 relArnorml
    states.append(constantX([1]))
    # 20 Anorm
    states.append(constantX([0]))
    # 21 flag
    states.append(constantX([0]))
    xs = [TT.unbroadcast(TT.shape_padleft(TT.zeros_like(b)), 0) for b in bs]
    ds = [TT.unbroadcast(TT.shape_padleft(TT.zeros_like(b)), 0) for b in bs]
    dls = [TT.unbroadcast(TT.shape_padleft(TT.zeros_like(b)), 0) for b in bs]
    r1s = [TT.unbroadcast(TT.shape_padleft(r1), 0) for r1 in r1s]
    r2s = [TT.unbroadcast(TT.shape_padleft(r2), 0) for r2 in r2s]
    r3s = [TT.unbroadcast(TT.shape_padleft(r3), 0) for r3 in r3s]

    rvals, loc_updates = scan(
        loop,
        states=states + xs + r1s + r2s + r3s + dls + ds,
        n_steps=maxit + numpy.int32(1),
        name='minres',
        profile=profile,
        mode=theano.Mode(linker='cvm'))
    assert isinstance(loc_updates, dict) and 'Ordered' in str(type(loc_updates))

    niters = TT.cast(rvals[0][0], 'int32')
    flag = TT.cast(rvals[21][0], 'int32')
    relres = rvals[18][0]
    relAres = rvals[19][0]
    Anorm = rvals[20][0]
    Acond = rvals[4][0]
    xnorm = rvals[12][0]
    Axnorm = rvals[17][0]
    sol = [x[0] for x in rvals[22: 22 + n_params]]
    return (sol,
            flag,
            niters,
            relres,
            relAres,
            Anorm,
            Acond,
            xnorm,
            Axnorm,
            loc_updates)

########NEW FILE########
__FILENAME__ = test_batch_gradient_descent
from pylearn2.optimization.batch_gradient_descent import BatchGradientDescent
import theano.tensor as T
from pylearn2.utils import sharedX
import numpy as np
from theano import config
from theano.printing import min_informative_str

def test_batch_gradient_descent():
        """ Verify that batch gradient descent works by checking that
        it minimizes a quadratic function f(x) = x^T A x + b^T x + c
        correctly for several sampled values of A, b, and c.
        The ground truth minimizer is x = np.linalg.solve(A,-b)"""

        n = 3

        A = T.matrix(name = 'A')
        b = T.vector(name = 'b')
        c = T.scalar(name = 'c')

        x = sharedX( np.zeros((n,)) , name = 'x')

        half = np.cast[config.floatX](0.5)

        obj = half * T.dot(T.dot(x,A),x)+T.dot(b,x)+c

        minimizer = BatchGradientDescent(
                        objective = obj,
                        params = [ x],
                        inputs = [ A, b, c])

        num_samples = 3

        rng = np.random.RandomState([1,2,3])

        for i in xrange(num_samples):
            A = np.cast[config.floatX](rng.randn(1.5*n,n))
            A = np.cast[config.floatX](np.dot(A.T,A))
            A += np.cast[config.floatX](np.identity(n) * .02)
            b = np.cast[config.floatX](rng.randn(n))
            c = np.cast[config.floatX](rng.randn())
            x.set_value(np.cast[config.floatX](rng.randn(n)))

            analytical_x = np.linalg.solve(A,-b)

            actual_obj = minimizer.minimize(A,b,c)
            actual_x = x.get_value()

            #Check that the value returned by the minimize method
            #is the objective function value at the parameters
            #chosen by the minimize method
            cur_obj = minimizer.obj(A,b,c)
            assert np.allclose(actual_obj, cur_obj)

            x.set_value(analytical_x)
            analytical_obj = minimizer.obj(A,b,c)

            #make sure the objective function is accurate to first 4 digits
            condition1 = not np.allclose(analytical_obj, actual_obj)
            condition2 = np.abs(analytical_obj-actual_obj) >= 1e-4 * \
                    np.abs(analytical_obj)

            if (config.floatX == 'float64' and condition1) \
                    or (config.floatX == 'float32' and condition2):
                print 'objective function value came out wrong on sample ',i
                print 'analytical obj', analytical_obj
                print 'actual obj',actual_obj

                """
                The following section of code was used to verify that numerical
                error can make the objective function look non-convex

                print 'Checking for numerically induced non-convex behavior'
                def f(x):
                    return 0.5 * np.dot(x,np.dot(A,x)) + np.dot(b,x) + c

                x.set_value(actual_x)
                minimizer._compute_grad(A,b,c)
                minimizer._normalize_grad()
                d = minimizer.param_to_grad_shared[x].get_value()

                x = actual_x.copy()
                prev = f(x)
                print prev
                step_size = 1e-4
                x += step_size * d
                cur = f(x)
                print cur
                cur_sgn = np.sign(cur-prev)
                flip_cnt = 0
                for i in xrange(10000):
                    x += step_size * d
                    prev = cur
                    cur = f(x)
                    print cur
                    prev_sgn = cur_sgn
                    cur_sgn = np.sign(cur-prev)
                    if cur_sgn != prev_sgn:
                        print 'flip'
                        flip_cnt += 1
                        if flip_cnt > 1:
                            print "Non-convex!"

                            from matplotlib import pyplot as plt
                            y = []

                            x = actual_x.copy()
                            for j in xrange(10000):
                                y.append(f(x))
                                x += step_size * d

                            plt.plot(y)
                            plt.show()

                            assert False

                print 'None found'
                """

                #print 'actual x',actual_x
                #print 'A:'
                #print A
                #print 'b:'
                #print b
                #print 'c:'
                #print c
                x.set_value(actual_x)
                minimizer._compute_grad(A,b,c)
                x_grad = minimizer.param_to_grad_shared[x]
                actual_grad =  x_grad.get_value()
                correct_grad = 0.5 * np.dot(A,x.get_value())+ 0.5 * \
                        np.dot(A.T, x.get_value()) +b
                if not np.allclose(actual_grad, correct_grad):
                    print 'gradient was wrong at convergence point'
                    print 'actual grad: '
                    print actual_grad
                    print 'correct grad: '
                    print correct_grad
                    print 'max difference: ',
                    np.abs(actual_grad-correct_grad).max()
                    assert False


                minimizer._normalize_grad()
                d = minimizer.param_to_grad_shared[x].get_value()
                step_len = ( np.dot(b,d) + 0.5 * np.dot(d,np.dot(A,actual_x)) \
                        + 0.5 * np.dot(actual_x,np.dot(A,d)) ) \
                        / np.dot(d, np.dot(A,d))

                g = np.dot(A,actual_x)+b
                deriv = np.dot(g,d)

                print 'directional deriv at actual', deriv
                print 'optimal step_len', step_len
                optimal_x = actual_x - d * step_len
                g = np.dot(A,optimal_x) + b
                deriv = np.dot(g,d)

                print 'directional deriv at optimal: ',deriv
                x.set_value(optimal_x)
                print 'obj at optimal: ',minimizer.obj(A,b,c)



                print 'eigenvalue range:'
                val, vec = np.linalg.eig(A)
                print (val.min(),val.max())
                print 'condition number: ',(val.max()/val.min())
                assert False


if __name__ == '__main__':
    test_batch_gradient_descent()

########NEW FILE########
__FILENAME__ = test_feature_sign
"""
Tests for the implementation of feature sign search.
"""

__authors__ = "David Warde-Farley"
__copyright__ = "Copyright 2010-2011, Universite de Montreal"
__credits__ = ["David Warde-Farley"]
__license__ = "3-clause BSD"
__maintainer__ = "David Warde-Farley"
__email__ = "wardefar@iro"

import numpy as np
from pylearn2.optimization.feature_sign import feature_sign_search


class TestFeatureSign(object):
    @classmethod
    def setup_class(cls):
        rng = np.random.RandomState(0)
        cls.dictionary = rng.normal(size=(100, 500)) / 1000
        cls.dictionary /= np.sqrt((cls.dictionary ** 2).sum(axis=0))
        cls.gram = np.dot(cls.dictionary.T, cls.dictionary)
        cls.signal = rng.normal(size=100) / 1000
        cls.corr = np.dot(cls.dictionary.T, cls.signal)
        cls.nzi = [np.array([0, 6, 15, 30, 31, 34,
                             36, 48, 52, 53, 62, 94,
                             99, 103, 105, 107, 124, 131,
                             133, 137, 142, 145, 171, 178,
                             190, 206, 207, 214, 221, 226,
                             228, 229, 231, 234, 246, 255,
                             257, 258, 261, 265, 268, 269,
                             279, 281, 282, 288, 289, 293,
                             294, 296, 297, 302, 303, 306,
                             319, 322, 328, 329, 331, 335,
                             337, 351, 353, 355, 362, 368,
                             369, 372, 375, 380, 390, 392,
                             394, 397, 405, 410, 412, 420,
                             421, 422, 425, 430, 432, 435,
                             439, 443, 449, 451, 455, 456,
                             457, 458, 463, 479, 484, 485,
                             492, 493, 496]),
                   np.array([0, 6, 15, 30, 31, 34,
                             40, 52, 62, 84, 99, 103,
                             107, 131, 133, 137, 142, 145,
                             171, 190, 207, 221, 226, 228,
                             229, 231, 234, 243, 255, 257,
                             258, 261, 265, 268, 279, 281,
                             284, 288, 289, 293, 294, 296,
                             297, 302, 303, 306, 308, 319,
                             322, 329, 331, 335, 337, 351,
                             353, 355, 357, 362, 369, 371,
                             372, 375, 380, 390, 392, 394,
                             397, 405, 412, 420, 422, 430,
                             435, 439, 443, 451, 455, 456,
                             457, 463, 479, 483, 484, 485,
                             492, 493, 497]),
                   np.array([0, 6, 15, 21, 28, 31,
                             34, 40, 52, 62, 75, 103,
                             133, 164, 171, 190, 226, 228,
                             229, 231, 234, 255, 258, 261,
                             265, 268, 279, 284, 289, 293,
                             294, 296, 303, 306, 308, 319,
                             322, 335, 351, 355, 360, 369,
                             371, 372, 375, 380, 392, 394,
                             397, 420, 422, 430, 435, 439,
                             443, 451, 455, 456, 457, 463,
                             479, 483, 485, 492, 493, 497]),
                   np.array([0, 15, 21, 28, 31, 34,
                             40, 52, 62, 103, 164, 171,
                             190, 206, 226, 228, 229, 231,
                             234, 255, 258, 261, 265, 268,
                             279, 294, 303, 308, 319, 322,
                             335, 355, 360, 369, 371, 372,
                             375, 380, 392, 420, 422, 430,
                             439, 443, 446, 451, 456, 457,
                             479, 483, 485, 492, 497]),
                   np.array([0, 15, 31, 32, 34, 40,
                             52, 62, 103, 164, 190, 206,
                             226, 228, 229, 231, 234, 261,
                             268, 279, 294, 303, 308, 319,
                             322, 335, 355, 360, 363, 369,
                             371, 372, 375, 392, 422, 430,
                             439, 451, 457, 479, 485, 492, 497]),
                   np.array([0, 32, 40, 52, 103, 164,
                             182, 190, 229, 234, 268, 294,
                             303, 319, 335, 355, 360, 363,
                             369, 371, 372, 375, 392, 422,
                             430, 439, 479, 485, 492, 497]),
                   np.array([0, 32, 52, 103, 190, 229,
                             234, 268, 303, 319, 335, 363,
                             369, 371, 372, 375, 392, 422,
                             430, 439, 479]),
                   np.array([52, 190, 229, 234, 319, 335,
                             369, 371, 372, 375, 392, 422,
                             430, 479]),
                   np.array([52, 229, 234, 335, 369, 371,
                             372, 375, 422, 430, 479]),
                   np.array([229, 234, 369, 372, 375, 422, 479]),
                   np.array([234, 369, 375, 479]),
                   np.array([234, 369, 375, 479]),
                   np.array([234, 369]),
                   np.array([], dtype=np.int32)]
        cls.nze = [np.array([-1.9348249481460072e-03, -2.6733387284447153e-04,
                             -1.3132617311436409e-03, -4.8160077424667838e-04,
                             -7.5839249350565386e-04, -1.5909426774904003e-03,
                              5.7527969676995383e-05, 7.9447260531266320e-05,
                              1.0451809037644966e-03, -4.1613884795362038e-04,
                              1.5212190467793459e-03, 1.7787664944475309e-04,
                             -4.8836751622554329e-04, 1.6494321148399042e-03,
                             -2.7677800129602731e-06, -5.3707951770584682e-04,
                             -6.5444024170409589e-05, -1.5828515693151556e-04,
                             -7.2127417409642656e-04, -5.1401896787202958e-04,
                              3.7758489731919223e-05, 6.6680708523377810e-04,
                              5.2900822039974530e-04, 3.0975349526476766e-05,
                              1.2917361992321301e-03, -2.6589905870352928e-04,
                              4.5817183084630225e-05, 1.3754030475796234e-04,
                             -1.6320053069891164e-03, 4.5131596808813552e-04,
                             -2.5421398951701307e-04, 1.8433162492285078e-03,
                             -9.1173071359616081e-04, 1.7471082488497108e-03,
                             -5.3360585337509928e-06, 9.1283752277065869e-04,
                              2.6051969664101067e-05, -2.4057039933035281e-04,
                              4.1276967786097276e-04, -7.2185718488784083e-04,
                              7.3222222810349295e-04, -1.1258619069166660e-03,
                             -5.9539175662470837e-05, 1.2020202196530964e-04,
                             -6.4852933217228329e-05, -3.6136499613693474e-04,
                             -2.7509451425501420e-04, 7.8402482743400567e-04,
                             -1.6375047820679251e-03, -7.6328448464493817e-05,
                              2.5201048678107204e-04, -5.3343888345009510e-04,
                              1.3637930948889019e-03, 4.1072719924889650e-05,
                             -1.5727527265822143e-03, -1.5471429922949904e-03,
                             -5.6998443448416484e-04, -3.1120566643703262e-04,
                             -5.6386207057422957e-05, 5.7371833193473369e-04,
                              3.6188962138560557e-04, -4.7316772908796121e-04,
                              2.9484545185290238e-04, -6.6535357340597704e-04,
                              8.4119874640662382e-04, -2.8329017840323988e-05,
                             -2.5306839397483431e-03, 2.4715013439992202e-03,
                             -1.1910480261999676e-03, -2.9709479220933098e-04,
                              6.8431258887171015e-04, -2.7295761244718773e-03,
                             -2.4093971793902772e-05, 6.2201228574189945e-04,
                              5.1780698320989707e-04, 2.6733781310107836e-04,
                              5.6336177344038440e-04, -1.0616591760754300e-03,
                              3.6947848006075820e-04, 6.7141992520493947e-05,
                              2.9771817071157870e-04, 5.9797094277252722e-04,
                             -3.9933204819632510e-04, 8.6517244244875048e-04,
                              4.4920353680412715e-04, -6.6808772499122302e-04,
                             -5.3361944478563209e-04, 1.5242227367637563e-03,
                              5.2896850426062208e-04, 8.9662346856869842e-04,
                              5.5085221933474243e-04, 6.6736148020986167e-05,
                              8.1560784413259606e-04, 2.6011480255750508e-04,
                              6.6830208202212017e-04, 4.1085940128486484e-04,
                             -1.7682961625122970e-03, 8.3934219900091750e-04,
                             -2.0873017058309116e-05]),
                  np.array([-1.1035294455088696e-03, -2.2000890007574505e-04,
                            -7.8903889419539659e-04, -2.1119535900836997e-04,
                            -5.8279291450617342e-04, -1.7080583717419850e-03,
                            -2.6627718634094304e-04, 1.2487012342411459e-03,
                             1.2674860805574550e-03, 1.0316196854953281e-04,
                            -3.8989827573487992e-05, 1.3959573167727619e-03,
                            -2.9263690772758253e-04, -7.5516160142673238e-05,
                            -3.2027802224615773e-04, -4.0194333313972751e-05,
                             5.2792602866442842e-05, 1.0030885855889520e-04,
                             3.4359859807725127e-04, 1.4202578828080572e-03,
                             4.9435053415793168e-05, -2.9342239639478933e-04,
                             3.7342261630825908e-04, -1.1357718570444337e-04,
                             1.4693020881404192e-03, -8.1038826473469011e-04,
                             1.8665829767200055e-03, -9.6714718311873344e-05,
                             5.6412327489350621e-04, 7.0701526303205728e-05,
                            -3.9516191942322022e-04, 6.3432608784393975e-04,
                            -6.3954164541396520e-04, 4.5235975549458074e-04,
                            -8.5992159621133923e-05, 1.4804790922793453e-05,
                             4.0644969429524014e-05, -5.2722126671811232e-05,
                            -3.6077541732844342e-04, 4.2575453908280664e-04,
                            -1.1487697489090171e-03, -4.2018807922097055e-04,
                             3.4790809594425012e-05, -1.6160032947381991e-04,
                             1.2440480492768469e-03, 1.1231144781693225e-04,
                            -3.1889994757268988e-05, -1.4839438754208341e-03,
                            -1.4511548747212302e-03, -1.3590898496856349e-04,
                            -1.1193045928555190e-04, 1.0102752291013409e-03,
                             8.5488972321372484e-05, -4.5029116166219244e-04,
                             1.2167805925053460e-04, -9.8493755243442174e-04,
                             1.2491091198674258e-04, 3.7378349690722683e-04,
                            -2.7075691573085138e-03, -4.8129161730357662e-05,
                             2.6410719309027152e-03, -1.4938221269370467e-03,
                            -4.7855585389856826e-04, 1.6361688366860866e-04,
                            -2.1767953133433049e-03, -3.0577449369473172e-04,
                             1.9800957374812715e-04, 2.1713584774933896e-04,
                             1.2547286780562931e-04, -8.8978093970981957e-04,
                             1.8019125328437374e-04, 7.3691032485438852e-04,
                             6.7509982879728641e-04, 2.7311153047371414e-04,
                            -6.8500802556878382e-04, 1.3273193785611063e-03,
                             3.1869297588771220e-04, 8.6925548887159651e-04,
                             9.4018360395236385e-04, 3.0819494676812361e-04,
                             2.6271523986771711e-04, 3.0870274205584814e-04,
                             3.0289953214768245e-04, 1.3749791740379300e-04,
                            -1.1786735752273350e-03, 5.0250477725954619e-04,
                            -8.6070034229997804e-05]),
                  np.array([-8.3437579213689074e-04, -3.0015619460217073e-05,
                            -5.7897541676280300e-04, 1.6551471495779046e-05,
                            -2.2393605800632093e-04, -5.2001242860563247e-04,
                            -1.3870942715484462e-03, -5.9567578646616199e-04,
                             1.0534240091407204e-03, 9.1585112462395880e-04,
                            -7.4399294769562441e-05, 1.0651860600130367e-03,
                            -1.1115812798157547e-04, -1.2644694854842305e-04,
                             8.0859909014213599e-05, 9.5258814604793707e-04,
                             4.8694577190044154e-04, -1.2857867793752289e-04,
                             1.3623765533842772e-03, -6.3098386281712981e-04,
                             1.8709346040428150e-03, 2.4869691048019343e-04,
                            -4.4258285202598450e-04, 6.2641533196780173e-04,
                            -2.1070150676115288e-04, 5.5018802760393299e-04,
                            -8.6507017547800483e-05, 2.5113468709088080e-05,
                            -5.9572458159247166e-05, 2.7174990026237772e-04,
                            -6.8299151173242584e-04, -6.9141524100211209e-05,
                             9.9677709958487432e-04, 2.1455469032139378e-05,
                            -1.0931081545008502e-04, -1.4288398610195221e-03,
                            -9.2641619187052728e-04, 1.1631696831111123e-03,
                            -2.5008597010047157e-04, -4.9109092205391425e-04,
                             1.4573367169579149e-05, -2.4556619817285266e-03,
                            -2.2737442063480292e-04, 2.0873563680032670e-03,
                            -1.7221092085957831e-03, -3.9149748509506707e-04,
                            -1.9083890283104121e-03, -4.5179869240156825e-05,
                             3.1550562053295335e-08, -6.5469640615284299e-04,
                             2.3552832526085873e-04, 5.9969305766932981e-04,
                             2.2278964442494262e-04, 3.2359079954438926e-04,
                            -4.0264229418057637e-04, 7.7866637053198141e-04,
                             1.1841428769164845e-04, 3.8661136419945486e-04,
                             6.7602096499568042e-04, 3.1878480353931328e-04,
                             5.1411896426637957e-04, 2.5156200586072758e-04,
                             2.2088078507041245e-04, -1.1117185648776493e-03,
                             2.1497044788086782e-04, -1.0188107377535942e-04
                           ]),
                  np.array([-6.4713254630098830e-04, -2.7713153029343456e-04,
                             1.0757385692728924e-04, -1.2262501147393618e-04,
                            -2.3059082888739592e-04, -7.3161012508945306e-04,
                            -5.2413912135354621e-04, 6.2111326346937247e-04,
                             4.3335174642597019e-04, 8.9356967523135681e-04,
                            -3.3763242273813105e-04, 3.3430657447490773e-05,
                             6.2191414452732641e-04, -7.4132022612954130e-06,
                             5.3662340320133003e-04, -1.4178212558739250e-04,
                             1.3095988871201910e-03, -3.1597818931136630e-04,
                             2.0378913237038515e-03, 6.6562531139041254e-05,
                            -1.6868955601997661e-04, 2.8713705809760455e-04,
                            -1.2304693935549571e-04, 4.4141928313035041e-04,
                            -1.2253018622955899e-04, -4.6171051458478255e-04,
                             7.3723710785185957e-04, -1.9032537033656971e-04,
                            -1.2697562952863858e-03, -5.2482737659127472e-04,
                             1.2061124796596517e-03, -4.0817504185725420e-04,
                             2.5998882856128638e-05, -2.0966819569876341e-03,
                            -2.7608734302861750e-04, 1.6726478730511611e-03,
                            -1.6663463119142058e-03, -1.0586294396151170e-04,
                            -1.6941836184160486e-03, -2.5946770642156560e-04,
                             3.5633343009712243e-04, 6.9737388074824142e-04,
                             3.5050766641770261e-04, -6.2751827095089259e-05,
                             1.2404656899160754e-04, 3.5941380874020181e-04,
                             3.4446046190986252e-05, 4.2103096984376170e-04,
                             5.3026111408425991e-04, 5.1273187361432225e-05,
                             3.5432933330489038e-04, -7.4663640180765406e-04,
                            -3.8335424804281977e-04]),
                  np.array([-5.1836610661442424e-04, -1.0477410668018961e-04,
                            -5.7495834485112783e-05, 1.7900407122394070e-04,
                            -2.6703148129370935e-04, -3.9176848759322184e-04,
                             5.8091871319957496e-04, 1.6462922992537709e-04,
                             6.5002017969994672e-04, -3.2338128176255906e-04,
                             3.9479040414653807e-04, -2.7188461309840551e-05,
                             3.1135232326379012e-04, -2.8133206599649399e-05,
                             1.2641601901411529e-03, -1.0536469984737687e-04,
                             2.0205599857680634e-03, 1.0750521985466104e-05,
                             3.3091039446384973e-04, -6.4895294462220010e-05,
                            -2.1369409832986251e-04, 4.5576174900652586e-04,
                            -3.1012129723400486e-05, -1.1405083021739002e-03,
                            -8.5555280955904731e-05, 1.1637843624565714e-03,
                            -2.3051364523934005e-04, 3.4620317476115259e-05,
                             1.3934400265841262e-04, -1.6160179207995579e-03,
                            -3.5463876387457341e-04, 1.2476670438728542e-03,
                            -1.5272790887738484e-03, -1.2188452493555049e-03,
                             4.1391946602114946e-04, 5.7720967689920983e-04,
                             3.9160014684697103e-04, 7.9576153790110988e-05,
                             1.2980545083543301e-04, 5.7890412759707829e-04,
                             2.5677418172492382e-04, -3.4102744376433380e-04,
                            -3.8079926681336754e-04]),
                  np.array([-2.9393635354609123e-04, 2.8338417294713878e-04,
                            -7.1322391135519907e-05, 6.0928102145436805e-04,
                             4.1117051502882046e-04, -1.3597922568600674e-04,
                             3.5892355459974317e-06, 2.5647272365454443e-04,
                             1.0960456734638818e-03, 1.8670751666828435e-03,
                             1.7805877172321683e-04, -4.9436006980503034e-05,
                             2.8396916362562023e-04, -9.6659831830055534e-04,
                             9.8937911969777977e-04, -5.9732960363989054e-05,
                             6.0131277001765439e-06, 2.0633055937450479e-04,
                            -1.3238747581454256e-03, -4.4239264759708273e-04,
                             9.3309879025114137e-04, -1.3254818250397289e-03,
                            -7.4066565997860437e-04, 3.8820311348479301e-04,
                             4.3545224753910783e-04, 2.9649776286609744e-04,
                             6.5031628085758639e-04, 1.0771789715400958e-04,
                            -1.6633242528597823e-05, -1.2235122833187349e-04
                           ]),
                  np.array([-4.6467519143475876e-05, 1.0821123640127927e-04,
                             5.2011630846811980e-04, 1.5331854905217915e-04,
                             1.3866548495350116e-04, 8.2075898345678074e-04,
                             1.6343894091814725e-03, 1.0471892103365427e-04,
                             9.4287873260011381e-05, -6.0720057514918534e-04,
                             7.3927770528263870e-04, 7.3587103118350852e-05,
                            -1.2461689301653100e-03, -3.7588254684182553e-04,
                             7.0300087714795709e-04, -1.1703586963490017e-03,
                            -3.4761993323616724e-04, 3.2959907503653533e-04,
                             3.7357318166526934e-04, 1.2210075043889726e-04,
                             6.8778984271716630e-04]),
                  np.array([3.8272388058376711e-04, 1.9899166301244989e-05,
                             5.6189540105860936e-04, 1.3899178409636488e-03,
                            -2.5900269633353400e-04, 4.6538791234591143e-04,
                            -1.1071682569346324e-03, -2.6088142077321339e-04,
                             4.6133971870889563e-04, -9.8984072841656542e-04,
                            -3.7539187986192293e-06, 2.6845217976317101e-04,
                             2.4946606382563613e-04, 6.8309969710756292e-04
                           ]),
                  np.array([1.6472832434094874e-04, 3.4060230114089002e-04,
                            1.1427152271565732e-03, 1.9920279319872669e-04,
                            -9.6452219060174412e-04, -1.0812607675017613e-04,
                            2.7900553513436648e-04, -8.0628617478484157e-04,
                            1.5476496796440077e-04, 4.9104287902516419e-05,
                            6.1965472101718983e-04]),
                  np.array([1.7454378937657987e-04, 9.4360910010814669e-04,
                            -7.7576992971679156e-04, 8.8628243235214528e-05,
                            -6.1000343573593921e-04, 1.0084201597356237e-05,
                            5.3306901959571291e-04]),
                  np.array([0.000713547849065, -0.0005803708454116,
                            -0.0003480663897498, 0.000378159050198]),
                  np.array([4.6907206821826032e-04, -3.5326757239539523e-04,
                            -9.6124150391964280e-05, 1.9113349844253737e-04
                           ]),
                  np.array([0.000230238402802, -0.0001260715414588]),
                  np.array([], dtype=np.float64)]
        cls.penalties = [0.0001, 0.0006, 0.0011, 0.0016,
                         0.0021, 0.0026, 0.0031, 0.0036,
                         0.0041, 0.0046, 0.0051, 0.0056,
                         0.0061, 0.0066]

    @classmethod
    def teardown_class(cls):
        del cls.signal
        del cls.dictionary
        del cls.nzi
        del cls.nze
        del cls.penalties

    def test_driver(self):
        for index in range(len(self.penalties)):
            solution_vector = np.zeros(self.dictionary.shape[1])
            feature_sign_search(self.dictionary, self.signal,
                                self.penalties[index],
                                solution=solution_vector)
            yield self.check_against_reference, solution_vector, index
            yield self.check_zerocoef_optimality_cond, solution_vector, index
            yield self.check_nonzero_optimality_cond, solution_vector, index
            yield self.check_zeros_against_reference, solution_vector, index
            yield self.check_nonzeros_against_reference, solution_vector, index

    def check_zeros_against_reference(self, solution, index):
        z_ref = np.setdiff1d(np.arange(solution.shape[0]), self.nzi[index])
        z_ind = np.where(solution == 0)[0]
        assert z_ref.shape == z_ind.shape
        assert np.all(z_ref == z_ind)

    def check_nonzeros_against_reference(self, solution, index):
        nz_ref = self.nzi[index]
        nz_ind = np.where(solution != 0)[0]
        assert nz_ref.shape == nz_ind.shape
        assert np.all(nz_ref == nz_ind)

    def check_against_reference(self, solution, index):
        reference = np.zeros(self.dictionary.shape[1])
        reference[self.nzi[index]] = self.nze[index]
        assert np.allclose(solution, reference)

    def check_zerocoef_optimality_cond(self, solution, index):
        sparsity = self.penalties[index]
        grad = - 2 * self.corr + 2 * np.dot(self.gram, solution)
        grad[solution == 0]
        signs = np.sign(solution)
        assert np.all(abs(grad[signs == 0]) <= sparsity)

    def check_nonzero_optimality_cond(self, solution, index):
        sparsity = self.penalties[index]
        grad = - 2 * self.corr + 2 * np.dot(self.gram, solution)
        grad[solution == 0]
        signs = np.sign(solution)
        nzgrad = grad[signs != 0] + sparsity * signs[signs != 0]
        np.testing.assert_almost_equal(nzgrad, np.zeros(nzgrad.shape))

    def test_shape_rank_matches_1d_generated(self):
        sparsity = self.penalties[0]
        solution = feature_sign_search(self.dictionary, self.signal, sparsity)
        assert solution.ndim == 1
        assert solution.shape[0] == self.dictionary.shape[1]

    def test_shape_rank_matches_2d_generated(self):
        sparsity = self.penalties[0]
        signal = self.signal.reshape(1, -1)
        solution = feature_sign_search(self.dictionary, signal, sparsity)
        assert solution.ndim == 2
        assert solution.shape[0] == 1
        assert solution.shape[1] == self.dictionary.shape[1]

    def test_solution_identity_1d_provided(self):
        sparsity = self.penalties[0]
        solution = np.zeros(self.dictionary.shape[1])
        newsol = feature_sign_search(self.dictionary, self.signal, sparsity,
                                     solution=solution)
        assert solution is newsol

    def test_solution_identity_2d_provided(self):
        sparsity = self.penalties[0]
        solution = np.zeros((1, self.dictionary.shape[1]))
        signal = self.signal.reshape(1, -1)
        newsol = feature_sign_search(self.dictionary, signal, sparsity,
                                     solution=solution)
        assert solution is newsol

########NEW FILE########
__FILENAME__ = test_linear_cg
import theano
from theano import tensor, config
import numpy
import linear_cg
import warnings
from pylearn2.testing.skip import skip_if_no_scipy
try:
    import scipy.linalg
except ImportError:
    warnings.warn("Could not import scipy.linalg")
import time

def test_linear_cg():
    rng = numpy.random.RandomState([1,2,3])
    n = 5
    M = rng.randn(2*n,n)
    M = numpy.dot(M.T,M).astype(config.floatX)
    b = rng.randn(n).astype(config.floatX)
    c = rng.randn(1).astype(config.floatX)[0]
    x = theano.tensor.vector('x')
    f = 0.5 * tensor.dot(x,tensor.dot(M,x)) - tensor.dot(b,x) + c
    sol = linear_cg.linear_cg(f,[x])

    fn_sol = theano.function([x], sol)

    start = time.time()
    sol  = fn_sol( rng.randn(n).astype(config.floatX))[0]
    my_lcg = time.time() -start

    eval_f = theano.function([x],f)
    cgf = eval_f(sol)
    print "conjugate gradient's value of f:", str(cgf), 'time (s)', my_lcg
    skip_if_no_scipy()
    spf = eval_f( scipy.linalg.solve(M,b) )
    print "scipy.linalg.solve's value of f: "+str(spf)

    abs_diff = abs(cgf - spf)
    if not (abs_diff < 1e-5):
        raise AssertionError("Expected abs_diff < 1e-5, got abs_diff of " +
                str(abs_diff))


if __name__ == '__main__':
    test_linear_cg()

########NEW FILE########
__FILENAME__ = test_linesearch
import time
import warnings

import theano
import theano.tensor as TT
import numpy
from linesearch import scalar_armijo_search
from linesearch import scalar_search_wolfe2


def line_search_armijo(ftemp, derphi0, old_fval, args=(), c1=1e-4, alpha0=1,
                      minAlpha=1e-20):
    fc = [0]

    def phi(alpha1):
        fc[0] += 1
        return ftemp(alpha1, *args)

    if old_fval is None:
        phi0 = phi(0.)
    else:
        phi0 = old_fval  # compute f(xk) -- done in past loop

    alpha, phi1 = scalar_search_armijo(phi,
                                       phi0,
                                       derphi0,
                                       c1=c1,
                                       alpha0=alpha0,
                                       minAlpha=minAlpha)
    return alpha, fc[0], phi1


def scalar_search_armijo(phi,
                         phi0,
                         derphi0,
                         c1=1e-4,
                         alpha0=1,
                         amin=0,
                         minAlpha=1e-20):
    phi_a0 = phi(alpha0)
    if phi_a0 <= phi0 + c1 * alpha0 * derphi0:
        return alpha0, phi_a0

    # Otherwise compute the minimizer of a quadratic interpolant:

    alpha1 = -(derphi0) * alpha0 ** 2 / 2.0 / \
            (phi_a0 - phi0 - derphi0 * alpha0)
    phi_a1 = phi(alpha1)
    if (phi_a1 <= phi0 + c1 * alpha1 * derphi0):
        return alpha1, phi_a1

    # Otherwise loop with cubic interpolation until we find an alpha which
    # satifies the first Wolfe condition (since we are backtracking, we will
    # assume that the value of alpha is not too small and satisfies the second
    # condition.
    while alpha1 > amin:       # we are assuming alpha>0 is a descent direction
        factor = alpha0 ** 2 * alpha1 ** 2 * (alpha1 - alpha0)
        a = alpha0 ** 2 * (phi_a1 - phi0 - derphi0 * alpha1) - \
            alpha1 ** 2 * (phi_a0 - phi0 - derphi0 * alpha0)
        a = a / factor
        b = -alpha0 ** 3 * (phi_a1 - phi0 - derphi0 * alpha1) + \
            alpha1 ** 3 * (phi_a0 - phi0 - derphi0 * alpha0)
        b = b / factor

        alpha2 = (-b + numpy.sqrt(abs(b ** 2 - 3 * a * derphi0))) / (3.0 * a)
        phi_a2 = phi(alpha2)

        if (phi_a2 <= phi0 + c1 * alpha2 * derphi0):
            return alpha2, phi_a2

        if (alpha1 - alpha2) > alpha1 / 2.0 or (1 - alpha2 / alpha1) < 0.96:
            alpha2 = alpha1 / 2.0
        if alpha2 < minAlpha:
            return alpha2, phi_a2

        alpha0 = alpha1
        alpha1 = alpha2
        phi_a0 = phi_a1
        phi_a1 = phi_a2

    # Failed to find a suitable step length
    return None, phi_a1


def test():
    ## TEST ME
    def random_tensor(name, *args):
        return theano.shared(
            (numpy.random.uniform(size=args) * 1e-1).astype(
                theano.config.floatX), name=name)
    W = random_tensor('W', 784, 500)
    v = random_tensor('v', 40, 784)
    y = random_tensor('y', 40, 500)

    gv = TT.grad(((TT.tanh(TT.dot(v, W)) - y) ** 2).sum(), v)

    def phi(x):
        return ((TT.tanh(TT.dot(v - gv * x, W)) - y) ** 2).sum()

    def derphi(x):
        return TT.grad(((TT.tanh(TT.dot(v - gv * x, W)) - y) ** 2).sum(), x)

    x = TT.scalar('x')
    func = theano.function([x],
                           phi(x),
                           name='func',
                           profile=0,
                           mode=theano.Mode(linker='cvm'),
                           allow_input_downcast=True,
                           on_unused_input='ignore')
    grad = theano.function([x],
                           TT.sum(-gv * v),
                           allow_input_downcast=True,
                           name='grad',
                           profile=0,
                           mode=theano.Mode(linker='cvm'),
                           on_unused_input='ignore')
    phi0 = theano.shared(numpy.asarray(func(0),
                                       dtype=theano.config.floatX),
                         name='phi0')
    derphi0 = theano.shared(numpy.asarray(grad(0),
                                          dtype=theano.config.floatX),
                                  name='derphi0')

    outs = scalar_armijo_search(phi, phi0, derphi0, profile=0)

    f = theano.function([],
                        outs,
                        profile=0,
                        name='test_scalar_search',
                        mode=theano.Mode(linker='cvm'))

    rvals = scalar_search_wolfe2(phi,
                                 derphi,
                                 phi0,
                                 derphi0,
                                 profile=0)

    f2 = theano.function([],
                         rvals,
                         profile=0,
                         name='test_wolfe',
                         mode=theano.Mode(linker='cvm'))

    t_py = 0
    f0 = func(0)
    g0 = grad(0)
    for k in xrange(10):
        t0 = time.time()
        rval = scalar_search_armijo(func, f0, g0)
        t_py += time.time() - t0

    t_th = 0
    thrval = []
    for k in xrange(10):
        t0 = time.time()
        thrval = f()
        t_th += time.time() - t0

    t_th2 = 0
    for k in xrange(10):
        t0 = time.time()
        thrval2 = f2()
        t_th2 += time.time() - t0

    print 'THEANO (armijo) output :: ', thrval
    print 'THEANO (wolfe)  output :: ', thrval2
    print 'NUMPY  (armijo) output :: ', rval
    print
    print 'Timings'
    print
    print 'theano (armijo)---------> time %e' % t_th
    print 'theano (wolfe) ---------> time %e' % t_th2
    print 'numpy  (armijo)---------> time %e' % t_py


if __name__ == '__main__':
    test()

########NEW FILE########
__FILENAME__ = test_minres
import numpy

import theano
from theano.compat.python2x import OrderedDict
import theano.tensor as TT

import minres


def test_1():
    n = 100
    on = numpy.ones((n, 1), dtype=theano.config.floatX)
    A = numpy.zeros((n, n), dtype=theano.config.floatX)
    for k in xrange(n):
        A[k, k] = 4.
        if k > 0:
            A[k - 1, k] = -2.
            A[k, k - 1] = -2.
    b = A.sum(axis=1)
    rtol = numpy.asarray(1e-10, dtype=theano.config.floatX)
    maxit = 50
    M = numpy.ones((n,), dtype=theano.config.floatX) * 4.
    tA = theano.shared(A.astype(theano.config.floatX))
    tb = theano.shared(b.astype(theano.config.floatX))
    tM = theano.shared(M.astype(theano.config.floatX))
    compute_Av = lambda x: ([TT.dot(tA, x)], OrderedDict())
    xs, flag, iters, relres, relAres, Anorm, Acond, xnorm, Axnorm, updates = \
            minres.minres(compute_Av,
                   [tb],
                   rtol=rtol,
                   maxit=maxit,
                   Ms=[tM],
                   profile=0)

    func = theano.function([],
                           xs + [flag, iters, relres, relAres, Anorm, Acond,
                                 xnorm, Axnorm],
                          name='func',
                          profile=0,
                          updates = updates,
                          mode=theano.Mode(linker='cvm'))
    rvals = func()
    print 'flag', rvals[1]
    print minres.messages[int(rvals[1])]
    print 'iters', rvals[2]
    print 'relres', rvals[3]
    print 'relAres', rvals[4]
    print 'Anorm', rvals[5]
    print 'Acond', rvals[6]
    print 'xnorm', rvals[7]
    print 'Axnorm', rvals[8]
    print 'error', numpy.sqrt(numpy.sum((numpy.dot(rvals[0], A) - b) ** 2))
    print


def test_2():
    h = 1
    a = -10
    b = -a
    n = 2 * b // h + 1
    A = numpy.zeros((n, n), dtype=theano.config.floatX)
    A = numpy.zeros((n, n), dtype=theano.config.floatX)
    v = a
    for k in xrange(n):
        A[k, k] = v
        v += h
    b = numpy.ones((n,), dtype=theano.config.floatX)
    rtol = numpy.asarray(1e-6, theano.config.floatX)
    maxxnorm = 1e8
    maxit = 50
    tA = theano.shared(A.astype(theano.config.floatX))
    tb = theano.shared(b.astype(theano.config.floatX))
    compute_Av = lambda x: ([TT.dot(tA, x)], OrderedDict())
    xs, flag, iters, relres, relAres, Anorm, Acond, xnorm, Axnorm, updates = \
            minres.minres(compute_Av,
                   [tb],
                   rtol=rtol,
                   maxit=maxit,
                   maxxnorm=maxxnorm,
                   profile=0)

    func = theano.function([],
                           xs + [flag, iters, relres, relAres, Anorm, Acond,
                                 xnorm, Axnorm],
                          name='func',
                          profile=0,
                          updates = updates,
                          mode=theano.Mode(linker='cvm'))
    rvals = func()
    print 'flag', rvals[1]
    print minres.messages[int(rvals[1])]
    print 'iters', rvals[2]
    print 'relres', rvals[3]
    print 'relAres', rvals[4]
    print 'Anorm', rvals[5]
    print 'Acond', rvals[6]
    print 'xnorm', rvals[7]
    print 'Axnorm', rvals[8]
    print rvals[0]

if __name__ == '__main__':
    test_1()
    test_2()

########NEW FILE########
__FILENAME__ = conv2d
from theano.tensor.nnet.conv import conv2d, ConvOp

from .imaging import tile_slices_to_image, most_square_shape
from .linear import LinearTransform
import numpy
from theano import tensor

def tile_conv_weights(w, flip=False, scale_each=False):
    """
    Return something that can be rendered as an image to visualize the filters.

    Parameters
    ----------
    w : WRITEME
    scale_each : bool
        WRITEME
    flip : bool
        WRITEME

    Returns
    -------
    WRITEME
    """
    if w.shape[1] != 3:
        raise NotImplementedError('not rgb', w.shape)
    if w.shape[2] != w.shape[3]:
        raise NotImplementedError('not square', w.shape)
    wmin, wmax = w.min(), w.max()
    if not scale_each:
        w = numpy.asarray(255 * (w - wmin) / (wmax - wmin + 1e-6), dtype='uint8')
    trows, tcols= most_square_shape(w.shape[0])
    outrows = trows * w.shape[2] + trows-1
    outcols = tcols * w.shape[3] + tcols-1
    out = numpy.zeros((outrows, outcols,3), dtype='uint8')

    tr_stride= 1+w.shape[1]
    for tr in range(trows):
        for tc in range(tcols):
            # this is supposed to flip the filters back into the image
            # coordinates as well as put the channels in the right place, but I
            # don't know if it really does that
            tmp = w[tr*tcols+tc].transpose(1,2,0)[
                             ::-1 if flip else 1,
                             ::-1 if flip else 1]
            if scale_each:
                tmp = numpy.asarray(255*(tmp - tmp.min()) / (tmp.max() - tmp.min() + 1e-6),
                        dtype='uint8')
            out[tr*(1+w.shape[2]):tr*(1+w.shape[2])+w.shape[2],
                    tc*(1+w.shape[3]):tc*(1+w.shape[3])+w.shape[3]] = tmp
    return out


class Conv2d(LinearTransform):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    filters : WRITEME
    img_shape : WRITEME
    subsample : WRITEME
    border_mode : WRITEME
    filters_shape : WRITEME
    message : WRITEME
    """
    def __init__(self, filters, img_shape, subsample=(1,1), border_mode='valid',
            filters_shape=None, message=""):
        super(Conv2d, self).__init__([filters])
        self._filters = filters
        if filters_shape is None:
            self._filters_shape = tuple(filters.get_value().shape)
        else:
            self._filters_shape = tuple(filters_shape)
        self._img_shape = tuple(img_shape)
        self._subsample = tuple(subsample)
        self._border_mode = border_mode
        if message:
            self._message = message
        else:
            self._message = filters.name
        if not len(self._img_shape)==4:
            raise ValueError('need 4-tuple shape', self._img_shape)
        if not len(self._filters_shape)==4:
            raise ValueError('need 4-tuple shape', self._filters_shape)

    def lmul(self, x):
        """
        .. todo::

            WRITEME
        """
        # dot(x, A)
        return conv2d(
                x, self._filters,
                image_shape=self._img_shape,
                filter_shape=self._filters_shape,
                subsample=self._subsample,
                border_mode=self._border_mode,
                )

    def lmul_T(self, x):
        """
        .. todo::

            WRITEME
        """
        # dot(x, A.T)
        dummy_v = tensor.tensor4()
        z_hs = conv2d(dummy_v, self._filters,
                image_shape=self._img_shape,
                filter_shape=self._filters_shape,
                subsample=self._subsample,
                border_mode=self._border_mode,
                )
        xfilters, xdummy = z_hs.owner.op.grad((dummy_v, self._filters), (x,))
        return xfilters

    def row_shape(self):
        """
        .. todo::

            WRITEME
        """
        return self._img_shape[1:]

    def col_shape(self):
        """
        .. todo::

            WRITEME
        """
        rows_cols = ConvOp.getOutputShape(
                self._img_shape[2:],
                self._filters_shape[2:],
                self._subsample,
                self._border_mode)
        rval = (self._filters_shape[0],)+tuple(rows_cols)
        return rval

    def tile_columns(self, scale_each=True, **kwargs):
        """
        .. todo::

            WRITEME
        """
        return tile_slices_to_image(
                self._filters.get_value()[:,:,::-1,::-1].transpose(0,2,3,1),
                scale_each=scale_each,
                **kwargs)

    def print_status(self):
        """
        .. todo::

            WRITEME
        """
        raise NotImplementedError('TODO fix broken method')
        #print ndarray_status(
        #        self._filters.get_value(borrow=True),
        #        msg='Conv2d{%s}'%self._message)

########NEW FILE########
__FILENAME__ = imaging
"""
.. todo::

    WRITEME
"""
import logging
import sys
import numpy
from pylearn2.utils.image import Image, ensure_Image


logger = logging.getLogger(__name__)


def scale_to_unit_interval(ndar,eps=1e-8):
    """
    .. todo::

        WRITEME
    """
    ndar = ndar.copy()
    ndar -= ndar.min()
    ndar *= 1.0 / max(ndar.max(),eps)
    return ndar


def tile_raster_images(X, img_shape,
        tile_shape=None, tile_spacing=(1,1),
        scale_rows_to_unit_interval=True,
        output_pixel_vals=True,
        min_dynamic_range=1e-4,
        ):
    """
    Transform an array with one flattened image per row, into an array in which
    images are reshaped and layed out like tiles on a floor.

    This function is useful for visualizing datasets whose rows are images, and
    also columns of matrices for transforming those rows (such as the first
    layer of a neural net).

    Parameters
    ----------
    X : numpy.ndarray or tuple of 4 channels or None
        A 2-D array in which every row is a flattened image.
    img_shape : tuple
        The original shape of each image
    tile_shape: tuple
        The number of images to tile (rows, cols). Defaults to a square-ish \
        shape with the right area for the number of images.
    min_dynamic_range: float, positive
        Dynamic range of each image is used in scaling to the unit interval, \
        but images with less dynamic range than this will be scaled as if \
        this were the dynamic range.

    Returns
    -------
    out_array : 2D array with same dtype as X
        Array suitable for viewing as an image (See:`PIL.Image.fromarray`).
    """
    # This is premature when tile_slices_to_image is not documented at all yet,
    # but ultimately true:
    #print >> sys.stderr, "WARN: tile_raster_images sucks, use tile_slices_to_image"
    if len(img_shape)==3 and img_shape[2]==3:
        # make this save an rgb image
        if scale_rows_to_unit_interval:
            logger.warning("tile_raster_images' scaling routine "
                           "messes up colour - try tile_slices_to_image")
        return tile_raster_images(
                (X[:,0::3], X[:,1::3], X[:,2::3], None),
                img_shape=img_shape[:2],
                tile_shape=tile_shape,
                tile_spacing=tile_spacing,
                scale_rows_to_unit_interval=scale_rows_to_unit_interval,
                output_pixel_vals=output_pixel_vals,
                min_dynamic_range=min_dynamic_range)

    if isinstance(X, tuple):
        n_images_in_x = X[0].shape[0]
    else:
        n_images_in_x = X.shape[0]

    if tile_shape is None:
        tile_shape = most_square_shape(n_images_in_x)

    assert len(img_shape) == 2
    assert len(tile_shape) == 2
    assert len(tile_spacing) == 2

    #out_shape is the shape in pixels of the returned image array
    out_shape = [(ishp + tsp) * tshp - tsp for ishp, tshp, tsp
        in zip(img_shape, tile_shape, tile_spacing)]

    if isinstance(X, tuple):
        if scale_rows_to_unit_interval:
            raise NotImplementedError()
        assert len(X) == 4
        if output_pixel_vals:
            out_array = numpy.zeros((out_shape[0], out_shape[1], 4), dtype='uint8')
        else:
            out_array = numpy.zeros((out_shape[0], out_shape[1], 4), dtype=X.dtype)

        #colors default to 0, alpha defaults to 1 (opaque)
        if output_pixel_vals:
            channel_defaults = [0,0,0,255]
        else:
            channel_defaults = [0.,0.,0.,1.]

        for i in xrange(4):
            if X[i] is None:
                out_array[:,:,i] = numpy.zeros(out_shape,
                        dtype='uint8' if output_pixel_vals else out_array.dtype
                        )+channel_defaults[i]
            else:
                out_array[:,:,i] = tile_raster_images(X[i], img_shape, tile_shape, tile_spacing, scale_rows_to_unit_interval, output_pixel_vals)
        return out_array

    else:
        H, W = img_shape
        Hs, Ws = tile_spacing

        out_scaling = 1
        if output_pixel_vals and str(X.dtype).startswith('float'):
            out_scaling = 255

        out_array = numpy.zeros(out_shape, dtype='uint8' if output_pixel_vals else X.dtype)
        for tile_row in xrange(tile_shape[0]):
            for tile_col in xrange(tile_shape[1]):
                if tile_row * tile_shape[1] + tile_col < X.shape[0]:
                    if scale_rows_to_unit_interval:
                        try:
                            this_img = scale_to_unit_interval(
                                    X[tile_row * tile_shape[1] + tile_col].reshape(img_shape),
                                    eps=min_dynamic_range)
                        except ValueError:
                            raise ValueError('Failed to reshape array of shape %s to shape %s'
                                    % (
                                        X[tile_row*tile_shape[1] + tile_col].shape
                                        , img_shape
                                        ))
                    else:
                        this_img = X[tile_row * tile_shape[1] + tile_col].reshape(img_shape)
                    out_array[
                        tile_row * (H+Hs):tile_row*(H+Hs)+H,
                        tile_col * (W+Ws):tile_col*(W+Ws)+W
                        ] \
                        = this_img * out_scaling
        return out_array


def most_square_shape(N):
    """
    Return a rectangle (height, width) with area N that is closest to square.

    Parameters
    ----------
    N : int
        WRITEME

    Returns
    -------
    WRITEME
    """
    for i in xrange(int(numpy.sqrt(N)),0, -1):
        if 0 == N % i:
            return (i, N/i)


def save_tiled_raster_images(tiled_img, filename):
    """
    Save a a return value from `tile_raster_images` to `filename`.

    Returns
    -------
    img : WRITEME
        The PIL image that was saved
    """
    if tiled_img.ndim==2:
        ensure_Image()
        img = Image.fromarray( tiled_img, 'L')
    elif tiled_img.ndim==3:
        ensure_Image()
        img = Image.fromarray(tiled_img, 'RGBA')
    else:
        raise TypeError('bad ndim', tiled_img)

    img.save(filename)
    return img


def tile_slices_to_image_uint8(X, tile_shape=None):
    """
    .. todo::

        WRITEME
    """
    if str(X.dtype) != 'uint8':
        raise TypeError(X)
    if tile_shape is None:
        #how many tile rows and cols
        (TR, TC) = most_square_shape(X.shape[0])
    H, W = X.shape[1], X.shape[2]

    Hs = H+1 #spacing between tiles
    Ws = W+1 #spacing between tiles

    trows, tcols= most_square_shape(X.shape[0])
    outrows = trows * Hs - 1
    outcols = tcols * Ws - 1
    out = numpy.zeros((outrows, outcols,3), dtype='uint8')
    tr_stride= 1+X.shape[1]
    for tr in range(trows):
        for tc in range(tcols):
            Xrc = X[tr*tcols+tc]
            if Xrc.ndim==2: # if no color channel make it broadcast
                Xrc=Xrc[:,:,None]
            #print Xrc.shape
            #print out[tr*Hs:tr*Hs+H,tc*Ws:tc*Ws+W].shape
            out[tr*Hs:tr*Hs+H,tc*Ws:tc*Ws+W] = Xrc
    ensure_Image()
    img = Image.fromarray(out, 'RGB')
    return img


def tile_slices_to_image(X,
        tile_shape=None,
        scale_each=True,
        min_dynamic_range=1e-4):
    """
    .. todo::

        WRITEME
    """
    #always returns an RGB image
    def scale_0_255(x):
        xmin = x.min()
        xmax = x.max()
        return numpy.asarray(
                255 * (x - xmin) / max(xmax - xmin, min_dynamic_range),
                dtype='uint8')

    if scale_each:
        uintX = numpy.empty(X.shape, dtype='uint8')
        for i, Xi in enumerate(X):
            uintX[i] = scale_0_255(Xi)
        X = uintX
    else:
        X = scale_0_255(X)
    return tile_slices_to_image_uint8(X, tile_shape=tile_shape)

########NEW FILE########
__FILENAME__ = linear
"""
.. todo::

    WRITEME
"""
import numpy
import theano
from theano import tensor

prod = numpy.prod

def dot(x, y):
    """
    Return the linear transformation of `y` by `x` or `x` by `y` when one
    or both of `x` and `y` is a LinearTransform instance

    Parameters
    ----------
    x : WRITEME
    y : WRITEME

    Returns
    -------
    WRITEME
    """
    if isinstance(x, LinearTransform):
        return x.rmul(y)
    elif isinstance(y, LinearTransform):
        return y.lmul(x)
    else:
        return theano.dot(x,y)


def dot_shape_from_shape(x, y):
    """
    Compute `dot(x, y).shape` from the shape of the non-LinearTransform

    Parameters
    ----------
    x : WRITEME
    y : WRITEME

    Returns
    -------
    WRITEME
    """
    if isinstance(x, LinearTransform):
        if type(y) != tuple:
            raise TypeError('y should be tuple', y)
        return x.col_shape() + x.split_right_shape(y, False)[1]
    elif isinstance(y, LinearTransform):
        if type(x) != tuple:
            raise TypeError('x should be tuple', x)
        return y.split_left_shape(x, False)[0] + y.row_shape()
    else:
        raise TypeError('One of x or y should be a LinearTransform')


def dot_shape(x, y):
    """
    Return the linear transformation of `y` by `x` or `x` by `y` when one
    or both of `x` and `y` is a LinearTransform instance

    Parameters
    ----------
    x : WRITEME
    y : WRITEME

    Returns
    -------
    WRITEME
    """
    if isinstance(x, LinearTransform):
        return dot_shape_from_shape(x, tuple(y.shape))
    elif isinstance(y, LinearTransform):
        return dot_shape_from_shape(tuple(x.shape), y)
    else:
        raise TypeError('One of x or y should be a LinearTransform')


class LinearTransform(object):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    params : list
        List of theano shared variables that parametrize the linear
        transformation
    """
    def __init__(self, params):
        self.set_params(params)

    def set_params(self, params):
        """
        .. todo::

            WRITEME
        """
        self._params = list(params)

    def params(self):
        """
        .. todo::

            WRITEME
        """
        return list(self._params)

    def __str__(self):
        """
        .. todo::

            WRITEME
        """
        return self.__class__.__name__ +'{}'

    # N.B. Don't implement __mul__ and __lmul__ because these mean
    # element-wise multiplication in numpy land.

    def __add__(self, other):
        """
        .. todo::

            WRITEME
        """
        return Sum([self, other])

    def __radd__(self, other):
        """
        .. todo::

            WRITEME
        """
        return Sum([other, self])

    # OVER-RIDE THIS (or rmul)
    def lmul(self, x):
        """
        .. todo::

            WRITEME
        """
        # this is a circular definition with rmul so that they are both
        # implemented as soon as one of them is overridden by a base class.

        try:
            # dot(x, A)
            # = dot(A.T, x.T).T
            AT_xT = self.rmul_T(self.transpose_left(x, False))
            rval = self.transpose_right(AT_xT, True)
            return rval
        except RuntimeError, e:
            if 'ecursion' in str(e):
                raise TypeError('either lmul or rmul_T must be implemented')
            raise
        except TypeError, e:
            if 'either lmul' in str(e):
                raise TypeError('either lmul or rmul_T must be implemented')

    def lmul_T(self, x):
        """
        .. todo::

            WRITEME
        """
        # this is a circular definition with rmul so that they are both
        # implemented as soon as one of them is overridden by a base class.

        # dot(x, A.T)
        # = dot(A, x.T).T
        A_xT = self.rmul(self.transpose_right(x, True))
        rval = self.transpose_left(A_xT, True)
        return rval

    # OVER-RIDE THIS (or lmul)
    def rmul(self, x):
        """
        .. todo::

            WRITEME
        """
        # this is a circular definition with rmul so that they are both
        # implemented as soon as one of them is overridden by a base class.

        try:
            # dot(A, x)
            # = dot(x.T, A.T).T
            xT_AT = self.lmul_T(self.transpose_right(x, False))
            rval = self.transpose_left(xT_AT, False)
            return rval
        except RuntimeError, e:
            if 'ecursion' in str(e):
                raise TypeError('either rmul or lmul_T must be implemented')
            raise
        except TypeError, e:
            if 'either lmul' in str(e):
                raise TypeError('either rmul or lmul_T must be implemented')

    def rmul_T(self, x):
        """
        .. todo::

            WRITEME
        """
        # this is a circular definition with rmul so that they are both
        # implemented as soon as one of them is overridden by a base class.

        # dot (A.T, x)
        # = dot(x.T, A).T
        xT_A = self.lmul(self.transpose_left(x, True))
        rval = self.transpose_right(xT_A, True)
        return rval

    def transpose_left(self, x, T):
        """
        .. todo::

            WRITEME
        """
        # supposing self.row_shape is (R1,)...
        cshp = self.col_shape()
        if T:
            # C1 C2 C3 R1 R2 -> R1 R2 C1 C2 C3
            ss = len(cshp)
        else:
            # R1 R2 C1 C2 C3 -> C1 C2 C3 R1 R2
            ss = x.ndim - len(cshp)
        pattern = range(ss, x.ndim) + range(ss)
        return x.transpose(pattern)

    def transpose_right(self, x, T):
        """
        .. todo::

            WRITEME
        """
        # supposing self.row_shape is (R1,)...
        rshp = self.row_shape()
        if T:
            # C1 C2 R1 -> R1 C1 C2
            ss = len(rshp)
        else:
            # R1 C1 C2 -> C1 C2 R1
            ss = x.ndim - len(rshp)
        pattern = range(ss, x.ndim) + range(ss)
        return x.transpose(pattern)

    def split_left_shape(self, xshp, T):
        """
        .. todo::

            WRITEME
        """
        if type(xshp) != tuple:
            raise TypeError('need tuple', xshp)
        # supposing self.col_shape is (C1, C2, C3) ...
        cshp = self.col_shape()
        assert type(cshp) == tuple
        if T:
            # C1 C2 C3 R1 R2
            ss = len(cshp)
            RR, CC = xshp[ss:], xshp[:ss]
        else:
            # R1 R2 C1 C2 C3
            ss = len(xshp) - len(cshp)
            RR, CC = xshp[:ss], xshp[ss:]
        if len(CC) != len(cshp) or (
                not all((isinstance(cc, theano.Variable) or cc == ci)
                    for cc, ci in zip(CC, cshp))):
            raise ValueError('invalid left shape',
                    dict(xshp=xshp, col_shape=cshp, xcols=CC, T=T))
        if T:
            return CC, RR
        else:
            return RR, CC

    def split_right_shape(self, xshp, T):
        """
        .. todo::

            WRITEME
        """
        if type(xshp) != tuple:
            raise TypeError('need tuple', xshp)
        # supposing self.row_shape is (R1, R2) ...
        rshp = self.row_shape()
        assert type(rshp) == tuple
        if T:
            # C1 C2 C3 R1 R2
            ss = len(xshp) - len(rshp)
            RR, CC = xshp[ss:], xshp[:ss]
        else:
            # R1 R2 C1 C2 C3
            ss = len(rshp)
            RR, CC = xshp[:ss], xshp[ss:]
        if len(RR) != len(rshp) or (
                not all((isinstance(rr, theano.Variable) or rr == ri)
                    for rr, ri in zip(RR, rshp))):
            raise ValueError('invalid left shape',
                    dict(xshp=xshp, row_shape=rshp, xrows=RR, T=T))
        if T:
            return CC, RR
        else:
            return RR, CC

    def transpose_left_shape(self, xshp, T):
        """
        .. todo::

            WRITEME
        """
        RR, CC = self.split_left_shape(xshp, T)
        return CC + RR

    def transpose_right_shape(self, xshp, T):
        """
        .. todo::

            WRITEME
        """
        RR, CC = self.split_right_shape(xshp, T)
        return CC + RR

    def is_valid_left_shape(self, xshp, T):
        """
        .. todo::

            WRITEME
        """
        try:
            self.split_left_shape(xshp, T)
            return True
        except ValueError:
            return False

    def is_valid_right_shape(self, xshp, T):
        """
        .. todo::

            WRITEME
        """
        try:
            self.split_right_shape(xshp, T)
            return True
        except ValueError:
            return False

    # OVER-RIDE THIS
    def row_shape(self):
        """
        .. todo::

            WRITEME
        """
        raise NotImplementedError('override me')

    # OVER-RIDE THIS
    def col_shape(self):
        """
        .. todo::

            WRITEME
        """
        raise NotImplementedError('override me')

    def transpose(self):
        """
        .. todo::

            WRITEME
        """
        return TransposeTransform(self)

    T = property(lambda self: self.transpose())

    # OVER-RIDE THIS
    def tile_columns(self, **kwargs):
        raise NotImplementedError('override me')


class TransposeTransform(LinearTransform):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    base : WRITEMe
    """
    def __init__(self, base):
        super(TransposeTransform, self).__init__([])
        self.base = base

    def transpose(self):
        """
        .. todo::

            WRITEME
        """
        return self.base

    def params(self):
        """
        .. todo::

            WRITEME
        """
        return self.base.params()

    def lmul(self, x):
        """
        .. todo::

            WRITEME
        """
        return self.base.lmul_T(x)

    def lmul_T(self, x):
        """
        .. todo::

            WRITEME
        """
        return self.base.lmul(x)

    def rmul(self, x):
        """
        .. todo::

            WRITEME
        """
        return self.base.rmul_T(x)

    def rmul_T(self, x):
        """
        .. todo::

            WRITEME
        """
        return self.base.rmul(x)

    def transpose_left(self, x, T):
        """
        .. todo::

            WRITEME
        """
        return self.base.transpose_right(x, not T)

    def transpose_right(self, x, T):
        """
        .. todo::

            WRITEME
        """
        return self.base.transpose_left(x, not T)

    def transpose_left_shape(self, x, T):
        """
        .. todo::

            WRITEME
        """
        return self.base.transpose_right_shape(x, not T)

    def transpose_right_shape(self, x, T):
        """
        .. todo::

            WRITEME
        """
        return self.base.transpose_left_shape(x, not T)

    def split_left_shape(self, x, T):
        """
        .. todo::

            WRITEME
        """
        return self.base.split_right_shape(x, not T)

    def split_right_shape(self, x, T):
        """
        .. todo::

            WRITEME
        """
        return self.base.split_left_shape(x, not T)

    def is_valid_left_shape(self, x, T):
        """
        .. todo::

            WRITEME
        """
        return self.base.is_valid_right_shape(x, not T)

    def is_valid_right_shape(self, x, T):
        """
        .. todo::

            WRITEME
        """
        return self.base.is_valid_left_shape(x, not T)

    def row_shape(self):
        """
        .. todo::

            WRITEME
        """
        return self.base.col_shape()

    def col_shape(self):
        """
        .. todo::

            WRITEME
        """
        return self.base.row_shape()

    def print_status(self):
        """
        .. todo::

            WRITEME
        """
        return self.base.print_status()

    def tile_columns(self):
        """
        .. todo::

            WRITEME
        """
        # yes, it would be nice to do rows, but since this is a visualization
        # and there *is* no tile_rows, we fall back on this.
        return self.base.tile_columns()

use_concat_class = 0
if use_concat_class: # needs to be brought up to date with LinearTransform method names
    class Concat(LinearTransform):
        """
        Form a linear map of the form [A B ... Z].

        For this to be valid, A,B...Z must have identical row_shape.

        The col_shape defaults to being the concatenation of flattened output from
        each of A,B,...Z, but a col_shape tuple specified via the constructor will
        reshape that vector.

        Parameters
        ----------
        Wlist : WRITEME
        col_shape : WRITEME
        """
        def __init__(self, Wlist, col_shape=None):
            super(Concat, self).__init__([])
            self._Wlist = list(Wlist)
            if not isinstance(col_shape, (int, long, numpy.integer, tuple, type(None))):
                raise TypeError('col_shape must be int or int tuple')
            self._col_sizes = [prod(w.col_shape()) for w in Wlist]
            if col_shape is None:
                self.__col_shape = sum(self._col_sizes),
            elif isinstance(col_shape, (int, long, numpy.integer)):
                self.__col_shape = col_shape,
            else:
                self.__col_shape = tuple(col_shape)
            assert prod(self.__col_shape) == sum(self._col_sizes)
            self.__row_shape = Wlist[0].row_shape()
            for W in Wlist[1:]:
                if W.row_shape() != self.row_shape():
                    raise ValueError('Transforms has different row_shape',
                            W.row_shape())

        def params(self):
            rval = []
            for W in self._Wlist:
                rval.extend(W.params())
            return rval
        def _lmul(self, x, T):
            if T:
                if len(self.col_shape())>1:
                    x2 = x.flatten(2)
                else:
                    x2 = x
                n_rows = x2.shape[0]
                offset = 0
                xWlist = []
                assert len(self._col_sizes) == len(self._Wlist)
                for size, W in zip(self._col_sizes, self._Wlist):
                    # split the output rows into pieces
                    x_s = x2[:,offset:offset+size]
                    # multiply each piece by one transform
                    xWlist.append(
                            W.lmul(
                                x_s.reshape(
                                    (n_rows,)+W.col_shape()),
                                T))
                    offset += size
                # sum the results
                rval = tensor.add(*xWlist)
            else:
                # multiply the input by each transform
                xWlist = [W.lmul(x,T).flatten(2) for W in self._Wlist]
                # join the resuls
                rval = tensor.join(1, *xWlist)
            return rval
        def _col_shape(self):
            return self.__col_shape
        def _row_shape(self):
            return self.__row_shape
        def _tile_columns(self):
            # hard-coded to produce RGB images
            arrays = [W._tile_columns() for W in self._Wlist]
            o_rows = sum([a.shape[0]+10 for a in arrays]) - 10
            o_cols = max([a.shape[1] for a in arrays])
            rval = numpy.zeros(
                    (o_rows, o_cols, 3),
                    dtype=arrays[0].dtype)
            offset = 0
            for a in arrays:
                if a.ndim==2:
                    a = a[:,:,None] #make greyscale broadcast over colors
                rval[offset:offset+a.shape[0], 0:a.shape[1],:] = a
                offset += a.shape[0] + 10
            return rval
        def print_status(self):
            for W in self._Wlist:
                W.print_status()

use_sum_class = 0
if use_sum_class: # needs to be brought up to date with LinearTransform method names
    class Sum(LinearTransform):
        def __init__(self, terms):
            self.terms = terms
            for t in terms[1:]:
                assert t.row_shape() == terms[0].row_shape()
                assert t.col_shape() == terms[0].col_shape()
        def params(self):
            rval = []
            for t in self.terms:
                rval.extend(t.params())
            return rval
        def _lmul(self, x, T):
            raise NotImplementedError()
            #results = [t._lmul(x, T)]
            #return tensor.add(*results)
        def _row_shape(self):
            return self.terms[0].col_shape()
        def _col_shape(self):
            return self.terms[0].row_shape()
        def print_status(self):
            raise NotImplementedError('TODO: fix old broken implementation')
            #for t in terms:
            #    t.print_status()
        def _tile_columns(self):
            raise NotImplementedError('TODO')

use_compose_class = 0
if use_compose_class: # This is incomplete
    class Compose(LinearTransform):
        """ For linear transformations [A,B,C]
        this represents the linear transformation A(B(C(x))).
        """
        def __init__(self, linear_transformations):
            self._linear_transformations = linear_transformations
        def dot(self, x):
            return reduce(
                    lambda t,a:t.dot(a),
                    self._linear_transformations,
                    x)
        def transpose_dot(self, x):
            return reduce(
                    lambda t, a: t.transpose_dot(a),
                    reversed(self._linear_transformations),
                    x)
        def params(self):
            return reduce(
                    lambda t, a: a + t.params(),
                    self._linear_transformations,
                    [])


########NEW FILE########
__FILENAME__ = linearmixin
"""
.. todo::

    WRITEME
"""
class LinearMixin(object):
    """
    This class can be mixed in to an Op that is linear in its first input.

    Supposing this Op takes two inputs: x, y.  It can be written as a
    linear operation in x: z = x W(y)
    """

    def transpose(zlike, *inputs_1_to_n):
        """
        This function returns (zlike) transpose(W(y))

        Parameters
        ----------
        zlike : WRITEME
        *inputs_1_to_n : WRITEME

        Returns
        -------
        WRITEME
        """
        raise NotImplementedError('override-me')

    def grads_1_to_n(inputs, gzlist):
        """
        .. todo::

            WRITEME
        """
        raise NotImplementedError('override-me')

    def grad(self, inputs, gzlist):
        """
        .. todo::

            WRITEME
        """
        if len(gzlist) > 1:
            raise NotImplementedError()
        g_input0 = self.transpose(gzlist[0], *inputs[1:])
        return [g_input0] + self.grads_1_to_n(inputs, gzlist)



########NEW FILE########
__FILENAME__ = pyramid
"""
.. todo::

    WRITEME
"""
import logging
import sys
import numpy
import theano
import warnings
from theano.gof import Variable, Op, utils, Type, Constant,  Value, Apply
from theano.tensor import as_tensor_variable


logger = logging.getLogger(__name__)

try:
    import cv
except ImportError:
    warnings.warn("cv not available")


def cv_available():
    """
    .. todo::

        WRITEME
    """
    return 'cv' in globals()

class GaussianPyramid(Op):
    """
    Returns `n_levels` images

    Parameters
    ----------
    n_levels : WRITEME
    """
    default_output = slice(0,None,1) #always return a list, even when there's only one element in it

    def __init__(self, n_levels):
        self.n_levels = n_levels

    def props(self):
        """
        .. todo::

            WRITEME
        """
        return (self.n_levels,)

    def __hash__(self):
        """
        .. todo::

            WRITEME
        """
        return hash((type(self), self.props()))

    def __eq__(self, other):
        """
        .. todo::

            WRITEME
        """
        return (type(self)==type(other) and self.props() == other.props())

    def __repr__(self):
        """
        .. todo::

            WRITEME
        """
        return '%s{n_levels=%s}' %(self.__class__.__name__, self.n_levels)

    def infer_shape(self, node, input_shapes):
        """
        .. todo::

            WRITEME
        """
        xshp, = input_shapes
        out_shapes = [xshp]
        while len(out_shapes) < self.n_levels:
            s = out_shapes[-1]
            out_shapes.append((s[0], s[1]//2, s[2]//2,s[3]))
        return out_shapes

    def make_node(self, x):
        """
        .. todo::

            WRITEME
        """
        if self.n_levels < 1:
            raise ValueError(('It does not make sense for'
                ' GaussianPyramid to generate %i levels'),
                self.n_levels)
        x = as_tensor_variable(x)
        return Apply(self, [x], [x.type() for i in range(self.n_levels)])

    def perform(self, node, ins, outs):
        """
        .. todo::

            WRITEME
        """
        x, = ins
        outs[0][0] = z = x.copy()
        B,M,N,K = x.shape
        for level in range(1,self.n_levels):
            # z is the whole pyramid at level `level-1`
            # loop body builds `out` which is the pyramid at `level`
            z0 = z[0]
            if z0.shape[0] <=2 or z0.shape[1] <= 2:
                raise ValueError('Cannot downsample an image smaller than 3x3',
                        z0.shape)
            logger.info('{0} {1} {2}'.format(z0.shape, z0.dtype, z0.strides))
            out0 = cv.pyrDown(z0)
            assert out0.dtype == x.dtype
            if out0.ndim ==3:
                assert out0.shape[2] == x.shape[3] # assert same # channels
            else:
                assert K==1
            out = numpy.empty(
                    (x.shape[0],
                        out0.shape[0],
                        out0.shape[1],
                        K),
                    dtype=out0.dtype)
            if K==1:
                out[0][:,:,0] = out0
            else:
                out[0] = out0
            for i, zi in enumerate(z[1:]):
                if K==1:
                    out[i][:,:,0] = cv.pyrDown(z[i])
                else:
                    out[i] = cv.pyrDown(z[i])
            outs[level][0] = out
            z = out


# test infer shape

# test non power-of-two shapes

# test different numbers of channels

def test_gaussian_pyramid_shapes():
    for dtype in ('float32', 'float64'):
        x = theano.tensor.tensor4(dtype=dtype)
        f = theano.function([x], GaussianPyramid(3)(x))

        xval = numpy.ones((1, 64, 64, 1), dtype=dtype)
        a,b,c = f(xval)
        assert a.shape == (1,64,64,1)
        assert b.shape == (1,32,32,1)
        assert c.shape == (1,16,16,1)

        xval = numpy.ones((1, 12, 12, 10), dtype=dtype)
        a,b,c = f(xval)
        assert a.shape == (1,12,12,10)
        assert b.shape == (1,6,6,10)
        assert c.shape == (1,3,3,10)

        p = GaussianPyramid(1)(x)
        f = theano.function([x], p)
        a, = f(xval)
        assert a.shape  == xval.shape
        #print a.max(), a.min()
        #print x.max(), x.min()
        #assert numpy.allclose(a, x)

########NEW FILE########
__FILENAME__ = spconv
"""
Convolution-like operations with sparse matrix multiplication.

To read about different sparse formats, see U{http://www-users.cs.umn.edu/~saad/software/SPARSKIT/paper.ps}.

@todo: Automatic methods for determining best sparse format?
"""
#### COPIED FROM hpu/icml09/sp.py

import logging
import numpy
from scipy import sparse as scipy_sparse

import theano
import theano.sparse
from theano import sparse, gof, Op, tensor
from theano.printing import Print

raise ImportError("THIS OLD CODE'S TESTS ARE BIT-ROTTEN")

logger = logging.getLogger(__name__)


class RasterOrders(object):
    """
    .. todo::

        WRITEME
    """
    @staticmethod
    def row_col_channel(row, col, channel, n_rows, n_cols, n_channels):
        """
        .. todo::

            WRITEME
        """
        return row * n_cols * n_channels + col * n_channels + channel
    @staticmethod
    def channel_row_col(row, col, channel, n_rows, n_cols, n_channels):
        """
        .. todo::

            WRITEME
        """
        return channel * n_rows * n_cols + row * n_cols + col

def conv_out_shp(IR, IC, KR, KC, border_mode, subsample):
    """
    .. todo::

        WRITEME
    """
    ssR, ssC = subsample
    def ceildiv(x, y):
        r = x // y
        if r * y < x:
            return r + 1
        return r
    if border_mode == 'valid':
        OR, OC = ceildiv(IR - KR + 1,ssR), ceildiv(IC - KC + 1,ssC)
    elif border_mode == 'full':
        OR, OC = ceildiv(IR + KR - 1,ssR), ceildiv(IC + KC - 1,ssC)
    else:
        raise NotImplementedError(border_mode)
    return OR, OC

def sp_extract_patches(IR, IC, KR, KC, CH,
        input_raster_order,
        output_raster_order,
        subsample,
        border_mode,
        flip_patches):
    """
    Construct a sparse matrix such that multiplication with a rasterized image
    produces the concatenation of rasterized image patches.

    The original image is presumed to be in row-major, channel-minor order:
    R(0,0) G(0,0) B(0,0) R(0,1), G(0,1), B(0,1), ...

    Parameters
    ----------
    IR : WRITEME
    IC : WRITEME
    KR : WRITEME
    KC : WRITEME
    CH : WRITEME
    input_raster_order : WRITEME
    output_raster_order : WRITEME
    subsample : WRITEME
    border_mode : WRITEME
    flip_patches : WRITEME

    Returns
    -------
    WRITEME
    """

    ssR, ssC = subsample
    OR, OC = conv_out_shp(IR, IC, KR, KC, border_mode, subsample)

    rval = scipy_sparse.lil_matrix((IR*IC*CH, OR*OC*KR*KC*CH))

    if not flip_patches:
        raise NotImplementedError()

    def in_pos(i, j, k):
        return input_raster_order(i, j, k, IR, IC, CH)
    def out_pos(i, j, k):
        return output_raster_order(i, j, k, KR, KC, CH)

    for orow in range(OR):
        for ocol in range(OC):
            for krow in range(KR):
                for kcol in range(KC):
                    assert flip_patches
                    if border_mode == 'valid':
                        irow = orow*ssR + KR - krow - 1
                        icol = ocol*ssC + KC - kcol - 1
                    else:
                        irow = orow*ssR - krow
                        icol = ocol*ssC - kcol
                    if (0 <= irow < IR) and (0 <= icol < IC):
                        for ch in range(CH):
                            t = out_pos(krow,kcol,ch)
                            T = KR * KC * CH
                            try:
                                i = in_pos(irow, icol, ch)
                                j = orow*OC*T + ocol*T + t
                                rval[i, j] = 1
                            except IndexError:
                                logger.error('{0} {1} {2} {3} {4} {5} {6} {7} '
                                             '{8}'.format(rval.shape, i, j, IR,
                                                          IC, KR, KC, OR, OC))
                                raise
    return rval

def conv2d_channel_minor(images, kerns, ishp4, kshp4, subsample=(1,1),
             border_mode='valid'):
    """
    .. todo::

        WRITEME
    """
    # start by computing output dimensions, size, etc
    B, IR, IC, C = ishp4
    K, KR, KC, CH = kshp4
    assert C == CH # number of channels must match

    OR, OC = conv_out_shp(IR, IC, KR, KC, border_mode, subsample)
    oshp = (B, OR, OC, K)

    # construct indices and index pointers for sparse matrix, which, when multiplied
    # with input images will generate a stack of image patches
    patch_extractor = sp_extract_patches(IR, IC, KR, KC, CH,
            RasterOrders.row_col_channel,
            RasterOrders.row_col_channel,
            subsample,
            border_mode,
            flip_patches=True).tocsc()

    #print IR, IC, KR, KC, CH, patch_extractor.shape, patch_extractor.nnz
    patches = sparse.structured_dot(
            images.flatten(2),
            patch_extractor)

    # compute output of linear classifier
    patch_stack = patches.reshape((B*OR*OC, KR*KC*CH))

    # kern is of shape: nkern x ksize*number_of_input_features
    # output is thus of shape: bsize*outshp x nkern
    output = tensor.dot(patch_stack, kerns.flatten(2).T).reshape((B, OR, OC, K))

    return output, oshp

def conv2d(images, kerns, ishp4, kshp4, subsample=(1,1),
             border_mode='valid'):
    """
    .. todo::

        WRITEME
    """
    # start by computing output dimensions, size, etc
    B, C, IR, IC = ishp4
    K, CH, KR, KC = kshp4
    assert C == CH # number of channels must match

    OR, OC = conv_out_shp(IR, IC, KR, KC, border_mode, subsample)
    oshp = (B, OR, OC, K)

    # construct indices and index pointers for sparse matrix, which, when multiplied
    # with input images will generate a stack of image patches
    patch_extractor = sp_extract_patches(IR, IC, KR, KC, CH,
            RasterOrders.channel_row_col,
            RasterOrders.channel_row_col,
            subsample,
            border_mode,
            flip_patches=True).tocsc()

    #print IR, IC, KR, KC, CH, patch_extractor.shape, patch_extractor.nnz
    patches = sparse.structured_dot(
            images.flatten(2),
            patch_extractor)

    # compute output of linear classifier
    patch_stack = patches.reshape((B*OR*OC, KR*KC*CH))

    # kern is of shape: nkern x ksize*number_of_input_features
    # output is thus of shape: bsize*outshp x nkern
    output = tensor.dot(patch_stack, kerns.flatten(2).T).reshape((B, OR, OC, K))

    return output, oshp



def register_specialize(lopt, *tags, **kwargs):
    """
    .. todo::

        WRITEME
    """
    theano.compile.optdb['specialize'].register((kwargs and kwargs.pop('name')) or lopt.__name__, lopt, 'fast_run', *tags)


class Remove0(Op):
    """
    Remove explicit zeros from a sparse matrix, and resort indices
    """
    def make_node(self, x):
        """
        .. todo::

            WRITEME
        """
        return gof.Apply(self, [x], [x.type()])

    def perform(self,node, (x,), (z,)):
        """
        .. todo::

            WRITEME
        """
        if x.format != 'csc':
            raise TypeError('Remove0 only works on csc matrices')

        M, N = x.shape

        data = x.data
        indices = x.indices
        indptr = x.indptr

        #TODO: try using ndarrays and then prune() on the result
        new_data = []
        new_indices = []
        new_indptr = [0]

        for j in xrange(0, N):
            for i_idx in xrange(indptr[j], indptr[j+1]):
                if data[i_idx] != 0:
                    new_data.append(data[i_idx])
                    new_indices.append(indices[i_idx])
            new_indptr.append(len(new_indices))

        z[0] = sparse.csc_matrix((new_data, new_indices, new_indptr), (M,N))

    def grad(self, (x,), (gz,)):
        """
        .. todo::

            WRITEME
        """
        return [gz]

remove0 = Remove0()

class EnsureSortedIndices(Op):
    """
    Remove explicit zeros from a sparse matrix, and resort indices

    Parameters
    ----------
    inplace : WRITEME
    """
    inplace=False

    def __init__(self, inplace):
        self.inplace=inplace
        if self.inplace:
            self.view_map = {0:[0]}

    def make_node(self, x):
        """
        .. todo::

            WRITEME
        """
        return gof.Apply(self, [x], [x.type()])

    def perform(self,node, (x,), (z,)):
        """
        .. todo::

            WRITEME
        """
        z[0] = x.ensure_sorted_indices(inplace=self.inplace)

    def grad(self, (x,), (gz,)):
        """
        .. todo::

            WRITEME
        """
        return [gz]

ensure_sorted_indices = EnsureSortedIndices(inplace=False)

def clean(x):
    """
    .. todo::

        WRITEME
    """
    return ensure_sorted_indices(remove0(x))


def max_pool(images, imgshp, maxpoolshp):
    """
    Implements a max pooling layer

    Takes as input a 2D tensor of shape batch_size x img_size and performs max pooling.
    Max pooling downsamples by taking the max value in a given area, here defined by
    maxpoolshp. Outputs a 2D tensor of shape batch_size x output_size.

    Parameters
    ----------
    images : 2D tensor
        Tensorcontaining images on which to apply convolution. Assumed to be \
        of shape `batch_size x img_size`
    imgshp : tuple
        Tuple containing image dimensions
    maxpoolshp : tuple
        Tuple containing shape of area to max pool over

    Returns
    -------
    out1 : WRITEME
        Symbolic result (2D tensor)
    out2 : WRITEME
        Logical shape of the output
    """
    N = numpy
    poolsize = N.int64(N.prod(maxpoolshp))

    # imgshp contains either 2 entries (height,width) or 3 (nfeatures,h,w)
    # in the first case, default nfeatures to 1
    if N.size(imgshp)==2:
        imgshp = (1,)+imgshp

    # construct indices and index pointers for sparse matrix, which, when multiplied
    # with input images will generate a stack of image patches
    indices, indptr, spmat_shape, sptype, outshp = \
            convolution_indices.conv_eval(imgshp, maxpoolshp, maxpoolshp, mode='valid')

    logger.info('XXXXXXXXXXXXXXXX MAX POOLING LAYER XXXXXXXXXXXXXXXXXXXX')
    logger.info('imgshp = {0}'.format(imgshp))
    logger.info('maxpoolshp = {0}'.format(maxpoolshp))
    logger.info('outshp = {0}'.format(outshp))

    # build sparse matrix, then generate stack of image patches
    csc = theano.sparse.CSM(sptype)(N.ones(indices.size), indices, indptr, spmat_shape)
    patches = sparse.structured_dot(csc, images.T).T

    pshape = tensor.stack(images.shape[0]*\
                            tensor.as_tensor(N.prod(outshp)),
                          tensor.as_tensor(imgshp[0]),
                          tensor.as_tensor(poolsize))
    patch_stack = tensor.reshape(patches, pshape, ndim=3);

    out1 = tensor.max(patch_stack, axis=2)

    pshape = tensor.stack(images.shape[0],
                          tensor.as_tensor(N.prod(outshp)),
                          tensor.as_tensor(imgshp[0]))
    out2 = tensor.reshape(out1, pshape, ndim=3);

    out3 = tensor.DimShuffle((False,)*3, (0,2,1))(out2)

    return tensor.flatten(out3,2), outshp
class ConvolutionIndices(Op):
    """
    This generates a sparse matrix M, which generates a stack of image patches
    when computing the dot product of M with image patch. Convolution is then
    simply the dot product of (img x M) and the kernels.
    """

    @staticmethod
    def sparse_eval(inshp, kshp, nkern, (dx,dy)=(1,1), mode='valid'):
        """
        .. todo::

            WRITEME
        """
        # STALE
        return convolution_indices.evaluate(inshp,kshp,(dx,dy),nkern,mode=mode,ws=False)

    @staticmethod
    def conv_eval(IR, IC, KR, KC, C, subsample=(1,1), mode='valid'):
        """
        .. todo::

            WRITEME
        """
        raise NotImplementedError('TODO: fix broken method')
        #return convolution_indices.evaluate(IR, IC, KR, KC, C, (dx,dy), mode=mode, ws=True)

    # img_shape and ker_shape are (height,width)
    @staticmethod
    def evaluate(imshp,kshp, (dx,dy)=(1,1), nkern=1, mode='valid', ws=True):
        """
        Build a sparse matrix which can be used for performing...
        * convolution: in this case, the dot product of this matrix with the
          input images will generate a stack of images patches. Convolution is
          then a tensordot operation of the filters and the patch stack.
        * sparse local connections: in this case, the sparse matrix allows us
          to operate the weight matrix as if it were fully-connected. The
          structured-dot with the input image gives the output for the
          following layer.

        Parameters
        ----------
        ker_shape : tuple
            Shape of kernel to apply (smaller than image)
        img_shape: tuple
            Shape of input images
        mode : str
            'valid' generates output only when kernel and image overlap. \
            'full' full convolution obtained by zero-padding the input
        ws : bool
            True if weight sharing, False otherwise
        (dx,dy) : tuple of int
            Offset parameter. In the case of no weight sharing, gives the \
            pixel offset between two receptive fields. With weight sharing \
            gives the offset between the top-left pixels of the generated \
            patches

        Returns
        -------
        rval : tuple(indices, indptr, logical_shape, sp_type, out_img_shp)
            The structure of a sparse matrix, and the logical dimensions of \
            the image which will be the result of filtering.
        """
        N = numpy

        # inshp contains either 2 entries (height,width) or 3 (nfeatures,h,w)
        # in the first case, default nfeatures to 1
        if N.size(imshp)==2:
            inshp = (1,)+imshp

        inshp = N.array(imshp)
        kshp  = N.array(kshp)
        ksize = N.prod(kshp)

        kern = ksize-1 - N.arange(ksize)

        # size of output image if doing proper convolution (mode='full',dx=dy=0)
        # outshp is the actual output shape given the parameters
        fulloutshp = inshp[1:] + kshp - 1
        s = -1 if mode=='valid' else 1
        outshp = N.int64(N.ceil((inshp[1:] + s*kshp - s*1) \
                 /N.array([dy,dx], dtype='float')))
        if any(outshp <= 0):
            err = 'Invalid kernel', kshp,'and/or step size',(dx,dy),\
                  'for given input shape', inshp
            raise ValueError(err)

        outsize = N.prod(outshp)
        insize = N.prod(inshp)

        # range of output units over which to iterate
        lbound = N.array([kshp[0]-1,kshp[1]-1]) if mode=='valid' else N.zeros(2)
        ubound = lbound + (inshp[1:]-kshp+1) if mode=='valid' else fulloutshp

        # coordinates of image in "fulloutshp" coordinates
        topleft  = N.array([kshp[0]-1,kshp[1]-1])
        botright = topleft + inshp[1:] # bound when counting the receptive field

        # sparse matrix specifics...
        spmatshp = (outsize*N.prod(kshp)*inshp[0],insize) if ws else\
                   (nkern*outsize,insize)
        spmat = scipy_sparse.lil_matrix(spmatshp)

        # loop over output image pixels
        z,zz = 0,0

        # incremented every time we write something to the sparse matrix
        # this is used to track the ordering of filter tap coefficient in sparse
        # column ordering
        tapi, ntaps = 0, 0

        # Note: looping over the number of kernels could've been done more efficiently
        # as the last step (when writing to spmat). However, this messes up the ordering
        # of the column values (order in which you write the values determines how the
        # vectorized data will get used later one)

        for fmapi in range(inshp[0]): # loop over input features
            for n in range(nkern): # loop over number of kernels (nkern=1 for weight sharing)

                # FOR EACH OUTPUT PIXEL...
                for oy in N.arange(lbound[0],ubound[0],dy): # loop over output image height
                    for ox in N.arange(lbound[1],ubound[1],dx): # loop over output image width

                        l = 0 # kern[l] is filter value to apply at (oj,oi) for (iy,ix)

                        # ... ITERATE OVER INPUT UNITS IN RECEPTIVE FIELD
                        for ky in oy+N.arange(kshp[0]):
                            for kx in ox+N.arange(kshp[1]):

                                # verify if we are still within image boundaries. Equivalent to
                                # zero-padding of the input image
                                if all((ky,kx) >= topleft) and all((ky,kx) < botright):

                                    # convert to "valid" input space coords
                                    # used to determine column index to write to in sparse mat
                                    iy,ix = N.array((ky,kx)) - topleft
                                    # determine raster-index of input pixel...
                                    col = iy*inshp[2]+ix +\
                                          fmapi*N.prod(inshp[1:]) # taking into account multiple input features

                                    # convert oy,ox values to output space coordinates
                                    (y,x) = (oy,ox) if mode=='full' else (oy,ox) - topleft
                                    (y,x) = N.array([y,x]) / (dy,dx) # taking into account step size
                                    # convert to row index of sparse matrix
                                    row = (y*outshp[1]+x)*inshp[0]*ksize + l + fmapi*ksize if ws else\
                                          y*outshp[1] + x

                                    # Store something at that location in sparse matrix.
                                    # The written value is only useful for the sparse case. It
                                    # will determine the way kernel taps are mapped onto
                                    # the sparse columns (idea of kernel map)
                                    spmat[row + n*outsize, col] = tapi + 1   # n*... only for sparse

                                    # total number of active taps (used for kmap)
                                    ntaps += 1

                                tapi += 1 # absolute tap index (total number of taps)
                                l+=1 # move on to next filter tap l=(l+1)%ksize

        if spmat.format != 'csc':
            spmat = spmat.tocsc().ensure_sorted_indices()
        else:
            # BUG ALERT: scipy0.6 has bug where data and indices are written in reverse column
            # ordering. Explicit call to ensure_sorted_indices removes this problem
            spmat = spmat.ensure_sorted_indices()

        if ws:
            kmap = None
        else:
            kmap = N.zeros(ntaps, dtype='int')
            k=0
            #print 'TEMPORARY BUGFIX: REMOVE !!!'
            for j in xrange(spmat.shape[1]):
                for i_idx in xrange(spmat.indptr[j], spmat.indptr[j+1]):
                    if spmat.data[i_idx] != 0:
                        kmap[k] = spmat.data[i_idx] -1 # this is == spmat[i,j] - 1
                        k+=1

        # when in valid mode, it is more efficient to store in sparse row
        # TODO: need to implement structured dot for csr matrix
        assert spmat.format == 'csc'
        sptype = 'csc'
        #sptype = 'csr' if mode=='valid' else 'csc'
        use_csr_type = 0
        if use_csr_type and mode=='valid':
            spmat = spmat.tocsr()

        rval = (spmat.indices[:spmat.size],
                spmat.indptr, spmatshp, sptype, outshp)
        rval += (kmap,) if kmap!=None else ()

        return rval

    def perform(self, node, (inshp, kshp),\
                (out_indices, out_indptr, spmat_shape)):
        """
        .. todo::

            WRITEME
        """
        indices, indptr, spmatshp, outshp = self.evaluate(inshp, kshp)
        out_indices[0] = indices
        out_indptr[0] = indptr
        spmat_shape[0] = numpy.asarray(spmatshp)

convolution_indices = ConvolutionIndices()

def applySparseFilter(kerns, kshp, nkern, images, imgshp, step=(1,1), bias=None, mode='valid'):
    """
    WRITEME

    Output feature map will have shape
    `batch_size x number of kernels * output_size`.

    Each filter is applied seperately to consecutive output pixels.

    Parameters
    ----------
    kerns : 1D tensor_like
        `nkern * outsize * ksize` vector containing kernels
    kshp : tuple
        Tuple containing actual dimensions of kernel (not symbolic)
    nkern : int
        Number of kernels to apply at each pixel in the input image. \
        `nkern = 1` will apply a single unique filter for each input pixel.
    images : WRITEME
        `bsize x imgsize` matrix containing images on which to apply filters. \
        Second dimension represents each image in raster order.
    imgshp : tuple
        Tuple containing actual image dimensions (not symbolic)
    step : WRITEME
        Determines number of pixels between adjacent receptive fields \
        (tuple containing dx,dy values)
    mode : str
        'full', 'valid' see `CSM.evaluate` function for details

    Returns
    -------
    out1 : WRITEME
        Symbolic result
    out2 : WRITEME
        Logical shape of the output img (nkern,height,width) \
        (after dot product, not of the sparse matrix!)

    Notes
    -----
    Note that this means that each feature map is contiguous in memory.
    The memory layout will therefore be
    `[ <feature_map_0> <feature_map_1> ... <feature_map_n>]`,
    where `<feature_map>` represents a "feature map" in raster order.

    Also, the concept of feature map doesn't really apply to sparse filters
    without weight sharing. Basically, nkern=1 will generate one output
    img/feature map, nkern=2 a second feature map, etc.
    """

    # inshp contains either 2 entries (height,width) or 3 (nfeatures,h,w)
    # in the first case, default nfeatures to 1
    if numpy.size(imgshp)==2:
        imgshp = (1,)+imgshp

    # construct indices and index pointers for sparse matrix
    indices, indptr, spmat_shape, sptype, outshp, kmap = \
        convolution_indices.sparse_eval(imgshp, kshp, nkern, step, mode)

    # build a sparse weight matrix
    sparsew = theano.sparse.CSM(sptype, kmap)(kerns, indices, indptr, spmat_shape)
    output =  sparse.structured_dot(sparsew, images.T).T
    if bias is not None:
        output += bias

    return output, numpy.hstack((nkern,outshp))

########NEW FILE########
__FILENAME__ = test_linear

import unittest

import numpy

from .linear import LinearTransform
from .linear import dot
from .linear import dot_shape
from .linear import dot_shape_from_shape


class ReshapeBase(LinearTransform):
    def __init__(self, from_shp, to_shp):
        LinearTransform.__init__(self, [])
        self._from_shp = from_shp
        self._to_shp = to_shp

    def row_shape(self):
        return self._to_shp

    def col_shape(self):
        return self._from_shp


class ReshapeL(ReshapeBase):

    def lmul(self, x):
        RR, CC = self.split_left_shape(x.shape, False)
        return x.reshape(RR + self._to_shp)

    def rmul(self, x):
        RR, CC = self.split_right_shape(x.shape, False)
        return x.reshape(self._from_shp + CC)


class ReshapeR(ReshapeBase):

    def lmul_T(self, x):
        CC, RR = self.split_right_shape(x.shape, True)
        return x.reshape(CC + self._from_shp)

    def rmul_T(self, x):
        CC, RR = self.split_left_shape(x.shape, True)
        return x.reshape(self._to_shp + RR)


class NumericSelfTestMixin(object):
    """
    Generic tests that assert the self-consistency of LinearTransform
    implementations that operate on numpy arrays.

    """

    def test_shape_xl_A(self):
        xl_A = dot(self.xl, self.A)
        assert xl_A.shape == dot_shape(self.xl, self.A)

    def test_shape_A_xr(self):
        A_xr = dot(self.A, self.xr)
        A_xr_shape = dot_shape(self.A, self.xr)
        assert A_xr.shape == A_xr_shape, (A_xr.shape, A_xr_shape)

    def test_shape_xrT_AT(self):
        # dot (xr.T, A.T)
        AT = self.A.T
        xrT_AT = dot(AT.transpose_left(self.xr, T=True), AT)
        assert xrT_AT.shape == dot_shape_from_shape(
                AT.transpose_left_shape(self.xr.shape, T=True), AT)

    def test_shape_AT_xlT(self):
        # dot (A.T, xl.T)
        AT = self.A.T
        AT_xlT = dot(AT,
                AT.transpose_right(self.xl, T=True))
        AT_xlt_shape = dot_shape_from_shape(AT,
                AT.transpose_right_shape(self.xl.shape, T=True))
        assert AT_xlT.shape == AT_xlt_shape, (AT_xlT.shape, AT_xlt_shape)


class TestReshapeL(NumericSelfTestMixin):
    def setUp(self):
        self.xl = numpy.random.randn(4, 3, 2)  # for left-mul
        self.xr = numpy.random.randn(6, 5)     # for right-mul
        self.A = ReshapeL((3, 2), (6,))
        self.xl_A_shape = (4, 6)

    def test_xl_A_value(self):
        xl_A = dot(self.xl, self.A)
        assert numpy.all(xl_A == self.xl.reshape(xl_A.shape))

class TestReshapeR(NumericSelfTestMixin):
    def setUp(self):
        self.xl = numpy.random.randn(4, 3, 2)  # for left-mul
        self.xr = numpy.random.randn(6, 5)     # for right-mul
        self.A = ReshapeR((3, 2), (6,))
        self.xl_A_shape = (4, 6)

    def test_xl_A_value(self):
        xl_A = dot(self.xl, self.A)
        assert numpy.all(xl_A == self.xl.reshape(xl_A.shape))


########NEW FILE########
__FILENAME__ = test_matrixmul
import numpy
import unittest
import warnings

import theano
from theano import tensor

from .linear import dot_shape
from .linear import dot_shape_from_shape
from .linear import dot

def assert_compute_equal(outputs, inputs=[]):
    outputs = map(tensor.as_tensor_variable, outputs)
    f = theano.function(inputs, outputs)
    outvals = f()
    assert all(numpy.all(outvals[i] == outvals[0])
            for i in range(1, len(outvals))), (outvals)


def assert_compute_allclose(outputs, inputs=[]):
    outputs = map(tensor.as_tensor_variable, outputs)
    f = theano.function(inputs, outputs)
    outvals = f()
    assert all(numpy.allclose(outvals[i], outvals[0])
            for i in range(1, len(outvals))), (outvals)

class SymbolicSelfTestMixin(object):
    """
    Generic tests that assert the self-consistency of LinearTransform
    implementations that operate on Theano variables.

    """

    def test_shape_xl_A(self):
        xl_A = dot(self.xl, self.A)
        assert_compute_equal([xl_A.shape, dot_shape(self.xl, self.A)])

    def test_shape_A_xr(self):
        A_xr = dot(self.A, self.xr)
        assert_compute_equal([A_xr.shape, dot_shape(self.A, self.xr)])

    def test_shape_xrT_AT(self):
        # dot (xr.T, A.T)
        AT = self.A.T
        xrT_AT = dot(AT.transpose_left(self.xr, T=True), AT)
        assert_compute_equal([
                xrT_AT.shape,
                dot_shape_from_shape(
                    AT.transpose_left_shape(tuple(self.xr.shape), T=True), AT)])

    def test_shape_AT_xlT(self):
        # dot (A.T, xl.T)
        AT = self.A.T
        AT_xlT = dot(AT,
                AT.transpose_right(self.xl, T=True))
        AT_xlt_shape = dot_shape_from_shape(AT,
                AT.transpose_right_shape(tuple(self.xl.shape), T=True))
        assert_compute_equal([
                AT_xlT.shape,
                AT_xlt_shape])

warnings.warn("TODO: port these disabled tests to the new pylearn2 setup")
"""
from .linear import LinearTransform
from .matrixmul import MatrixMul


class TestMatrixMul(unittest.TestCase, SymbolicSelfTestMixin):
    def setUp(self):
        self.xlval = 0.5 + numpy.random.randn(4, 3, 2)
        self.xrval = 0.5 + numpy.random.randn(7, 5)
        self.Wval = numpy.random.rand(6, 7) + 0.5
        self.xl = theano.shared(self.xlval)
        self.xr = theano.shared(self.xrval)
        self.W = theano.shared(self.Wval)
        self.A = MatrixMul(self.W, col_shape = (3, 2))

    def test_xl_A_value(self):
        xl_A = numpy.dot(self.xlval.reshape(4, 6), self.Wval)
        assert_compute_allclose([xl_A, dot(self.xl, self.A)])

    def test_A_xr_value(self):
        val = numpy.dot(self.Wval, self.xrval).reshape(3, 2, 5)
        assert_compute_allclose([val, dot(self.A, self.xr)])

    def test_AT_xlT_value(self):
        val = numpy.dot(self.Wval.T,
                self.xlval.transpose(1, 2, 0).reshape(-1, 4))
        assert_compute_allclose([val,
            dot(self.A.T, self.A.transpose_left(self.xl, T=False))])

    def test_xrT_AT(self):
        val = numpy.dot(
                self.xrval.transpose(),
                self.Wval.T).reshape(5, 3, 2)
        assert_compute_allclose([val,
            dot(self.A.transpose_right(self.xr, T=False), self.A.T)])
"""

########NEW FILE########
__FILENAME__ = test_spconv
activate_test_spconv = 0
if activate_test_spconv:
    import sys
    from theano import function, Mode
    from theano.gof import OpWiseCLinker
    import theano, numpy
    import theano.tensor as T
    import theano.sparse
    import scipy.sparse

    from scipy.signal import convolve2d
    import scipy.sparse as sparse
    import numpy
    import numpy as N
    #from theano.sparse.sandbox import spconv as sp

    import unittest
    import time
    sp = None

    def test_convolution():
        print '\n\n*************************************************'
        print '           TEST CONVOLUTION'
        print '*************************************************'

        # fixed parameters
        channels=3
        bsize = 10     # batch size
        imshp = (32,32)
        kshp = (8,8)
        nkern = 32
        subsample_amounts = ((1,1),(2,2),(3,3),(4,4))
        convmodes = ('full','valid')

        ishp4_channel_major = (bsize, channels) + imshp
        kshp4_channel_major = (nkern, channels) + kshp
        ishp4_channel_minor = (bsize,) + imshp + (channels,)
        kshp4_channel_minor = (nkern,) + kshp + (channels,)

        # symbolic stuff
        kerns = T.tensor4()
        imgs = T.tensor4()
        rng = N.random.RandomState(3423489)
        kern_data = rng.rand(*kshp4_channel_major).astype(kerns.dtype)+1
        img_data = rng.rand(*ishp4_channel_major).astype(imgs.dtype)+1

        # re-arrange these random-images so that the channel data is the minor
        # dimension: (batch rows cols channels)
        kern_data_minor = kern_data.transpose([0,2,3,1]).copy()
        img_data_minor = img_data.transpose([0,2,3,1]).copy()

        assert img_data_minor.shape == (bsize,)+imshp + (channels,)

        for conv_mode in convmodes:
            for subsample in subsample_amounts:
                #print 'Subsample', subsample,
                de_output = theano.tensor.nnet.conv2d(imgs, kerns,
                        ishp4_channel_major,
                        kshp4_channel_major,
                        border_mode=conv_mode,
                        subsample=subsample)

                f_d = function([kerns, imgs], de_output, profile='DENSE')

                t0 = time.time()
                for i in range(5):
                    rval_d = f_d(kern_data, img_data)
                t_d = time.time() - t0
                #print "Conv2D", t_d,
                use_channel_major_ordering = 0
                if use_channel_major_ordering: # sparse with channel_major ordering
                    sp_output, outshp  = sp.conv2d(imgs, kerns,
                            ishp4_channel_major,
                            kshp4_channel_major,
                            subsample=subsample,
                            border_mode=conv_mode)
                    f_s = function([kerns, imgs], sp_output,
                            profile='MAJOR')

                    t0 = time.time()
                    for i in range(5):
                        rval_s = f_s(kern_data, img_data)
                        assert rval_s.size == rval_d.size, (rval_s.shape, rval_d.shape)

                    # put rval_s into channel-submajor format
                    rval_s_major = rval_s.transpose([0,3,1,2])
                    assert numpy.allclose(rval_s_major, rval_d)
                    t_s_major = time.time() - t0
                    #print "spconv_major", t_s_major, 'ratio', t_d / t_s_major

                use_channel_minor_ordering = 1
                if use_channel_minor_ordering: # sparse with channel_minor ordering
                    sp_output, outshp  = sp.conv2d_channel_minor(imgs, kerns,
                            ishp4_channel_minor,
                            kshp4_channel_minor,
                            subsample=subsample,
                            border_mode=conv_mode)
                    f_s = function([kerns, imgs], sp_output,
                            profile='MINOR')

                    t0 = time.time()
                    for i in range(5):
                        rval_s = f_s(kern_data_minor, img_data_minor)
                        assert rval_s.size == rval_d.size, (rval_s.shape, rval_d.shape)

                    # put rval_s into channel-submajor format
                    rval_s_major = rval_s.transpose([0,3,1,2])
                    assert rval_s_major.shape == rval_d.shape
                    assert numpy.allclose(rval_s_major, rval_d)
                    t_s_minor = time.time() - t0
                    #print "spconv_minor", t_s_minor, 'ratio', t_d / t_s_minor
                    #assert rval_d.shape == rval_s.shape

    def test_sparse():

        print '\n\n*************************************************'
        print '           TEST SPARSE'
        print '*************************************************'

        # fixed parameters
        bsize = 10     # batch size
        imshp = (28,28)
        kshp = (5,5)
        nkern = 1 # per output pixel
        ssizes = ((1,1),(2,2))
        convmodes = ('full','valid',)

        # symbolic stuff
        bias = T.dvector()
        kerns = T.dvector()
        input = T.dmatrix()
        rng = N.random.RandomState(3423489)

        import theano.gof as gof
        #Mode(optimizer='fast_run', linker=gof.OpWiseCLinker(allow_gc=False)),):
        ntot, ttot = 0,0
        for conv_mode in convmodes:
            for ss in ssizes:

                output, outshp = sp.applySparseFilter(kerns, kshp,\
                        nkern, input, imshp, ss, bias=bias, mode=conv_mode)
                f = function([kerns, bias, input], output)

                # build actual input images
                img2d = N.arange(bsize*N.prod(imshp)).reshape((bsize,)+imshp)
                img1d = img2d.reshape(bsize,-1)
                zeropad_img = N.zeros((bsize,\
                                       img2d.shape[1]+2*(kshp[0]-1),\
                                       img2d.shape[2]+2*(kshp[1]-1)))
                zeropad_img[:, kshp[0]-1:kshp[0]-1+img2d.shape[1],
                               kshp[1]-1:kshp[1]-1+img2d.shape[2]] = img2d

                # build kernel matrix -- flatten it for theano stuff
                filters = N.arange(N.prod(outshp)*N.prod(kshp)).\
                            reshape(nkern,N.prod(outshp[1:]),N.prod(kshp))
                spfilt = filters.flatten()
                biasvals = N.arange(N.prod(outshp))

                # compute output by hand
                ntime1 = time.time()
                refout = N.zeros((bsize,nkern,outshp[1],outshp[2]))
                patch = N.zeros((kshp[0],kshp[1]))
                for b in xrange(bsize):
                    for k in xrange(nkern):
                        pixi = 0 # pixel index in raster order
                        for j in xrange(outshp[1]):
                            for i in xrange(outshp[2]):
                                n = j * ss[0]
                                m = i * ss[1]
                                patch = zeropad_img[b,n:n+kshp[0],m:m+kshp[1]]
                                refout[b,k,j,i] = N.dot(filters[k,pixi,:],\
                                                        patch.flatten())
                                pixi += 1
                refout = refout.reshape(bsize,-1) + biasvals
                ntot += time.time() - ntime1
                # need to flatten images
                ttime1 = time.time()
                out1 = f(spfilt, biasvals, img1d)
                ttot += time.time() - ttime1
                temp = refout - out1
                assert (temp < 1e-10).all()
                # test downward propagation
                vis = T.grad(output, input, output)
                downprop = function([kerns,output], vis)
                temp1 = time.time()
                for zz in range(100):
                    visval = downprop(spfilt,out1)
                indices, indptr, spmat_shape, sptype, outshp, kmap = \
                        sp.convolution_indices.sparse_eval(imshp,kshp,nkern,ss,conv_mode)
                spmat = sparse.csc_matrix((spfilt[kmap],indices,indptr),spmat_shape)
                visref = N.dot(out1,spmat.todense())
                assert N.all(visref==visval)

            print '**** Sparse Profiling Results ****'
            print 'Numpy processing time: ', ntot
            print 'Theano processing time: ', ttot
        #profmode.print_summary()


    def test_maxpool():
        # generate flatted images
        maxpoolshps = ((2,2),(3,3),(4,4),(5,5),(6,6))
        imval = N.random.rand(4,5,10,10)

        images = T.dmatrix()
        for maxpoolshp in maxpoolshps:

            # symbolic stuff
            output, outshp = sp.max_pool(images, imval.shape[1:], maxpoolshp)
            f = function([images,],[output,])
            output_val = f(imval.reshape(imval.shape[0],-1))

            # numeric verification
            my_output_val = N.zeros((imval.shape[0], imval.shape[1],
                                     imval.shape[2]/maxpoolshp[0],
                                     imval.shape[3]/maxpoolshp[1]))
            assert N.prod(my_output_val.shape[1:]) == N.prod(N.r_[imval.shape[1],outshp])

            for n in range(imval.shape[0]):
                for k in range(imval.shape[1]):
                    for i in range(imval.shape[2]/maxpoolshp[0]):
                        for j in range(imval.shape[3]/maxpoolshp[1]):
                            ii,jj = i*maxpoolshp[0], j*maxpoolshp[1]
                            patch = imval[n,k,ii:ii+maxpoolshp[0],jj:jj+maxpoolshp[1]]
                            my_output_val[n,k,i,j] = N.max(patch)
            my_output_val = my_output_val.reshape(imval.shape[0],-1)
            assert N.all(output_val == my_output_val)

            def mp(input):
                output, outshp = sp.max_pool(input, imval.shape[1:], maxpoolshp)
                return output
            T.verify_grad(None, mp, [imval.reshape(imval.shape[0],-1)])



########NEW FILE########
__FILENAME__ = gpu_unshared_conv
"""
WRITEME
"""
import inspect
import os
import StringIO

import theano
from theano.sandbox.cuda import CudaNdarrayType
from theano.gof import local_optimizer
from theano.sandbox.cuda.opt import register_opt
from theano.sandbox.cuda import gpu_from_host, host_from_gpu

from .unshared_conv import FilterActs
from .unshared_conv import WeightActs
from .unshared_conv import ImgActs

_this_dir = os.path.dirname(inspect.getfile(inspect.currentframe()))


# XXX: move to cuda.opt and refactor there
def any_from_gpu(*vv):
    """
    .. todo::

        WRITEME
    """
    for v in  vv:
        if v.owner and v.owner.op == host_from_gpu:
            return True
    return False


# XXX: move to cuda.opt and refactor there
def any_gpu_client(*vv):
    """
    .. todo::

        WRITEME
    """
    for v in vv:
        for (cl, pos) in v.clients:
            if cl.op == gpu_from_host:
                return True
    return False


class Base(theano.Op):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    module_stride : WRITEME
    partial_sum : WRITEME
    """
    def __init__(self, module_stride, partial_sum):
        self.module_stride = module_stride
        self.partial_sum = partial_sum

    def _attributes(self):
        """
        .. todo::

            WRITEME
        """
        return (
                self.module_stride,
                self.partial_sum,
                )

    def __eq__(self, other):
        """
        .. todo::

            WRITEME
        """
        return (type(self) == type(other)
                and self._attributes() == other._attributes())

    def __hash__(self):
        """
        .. todo::

            WRITEME
        """
        return hash((type(self), self._attributes()))

    def __str__(self):
        """
        .. todo::

            WRITEME
        """
        return '%s{module_stride=%i,partial_sum=%i}' % (
                self.__class__.__name__,
                self.module_stride,
                self.partial_sum,
                )


class GpuFilterActs(Base):
    """
    .. todo::

        WRITEME
    """
    def make_node(self, images, filters):
        """
        .. todo::

            WRITEME
        """
        ibcast = images.broadcastable
        fbcast = filters.broadcastable
        igroups, icolors_per_group, irows, icols, icount = ibcast
        fmodulesR, fmodulesC, fcolors, frows, fcols = fbcast[:-2]
        fgroups, filters_per_group = fbcast[-2:]
        hbcast = (fgroups, filters_per_group, fmodulesR, fmodulesC, icount)
        if not isinstance(images.type, CudaNdarrayType):
            raise TypeError('gpu_filter_acts requires CudaNdarray images',
                    images)
        if not isinstance(filters.type, CudaNdarrayType):
            raise TypeError('gpu_filter_acts requires CudaNdarray filters',
                    filters)
        htype = CudaNdarrayType(broadcastable=hbcast)
        return theano.gof.Apply(self,
                [images, filters],
                [htype()])

    def c_support_code(self):
        """
        .. todo::

            WRITEME
        """
        cufile = open(os.path.join(_this_dir, 'filter_acts.cu'))
        return cufile.read()

    def c_code_cache_version(self):
        """
        .. todo::

            WRITEME
        """
        return ()

    def c_code(self, node, nodename, inputs, outputs, sub):
        """
        .. todo::

            WRITEME
        """
        #z_out = alpha * dot(x,y) + beta * z_in
        #inplace version, set set z_out = z_in
        #not inplace version, we copy z_in to z_out.
        images, filters, = inputs
        responses, = outputs
        fail = sub['fail']
        moduleStride = str(self.module_stride)
        sio = StringIO.StringIO()

        print >> sio, """

        //XXX: actually the rightmost images dimension can be strided
        if (!CudaNdarray_is_c_contiguous(%(images)s))
        {
            PyErr_Format(PyExc_NotImplementedError,
                "images not c contiguous");
            %(fail)s;
        }

        if (!CudaNdarray_is_c_contiguous(%(filters)s))
        {
            PyErr_Format(PyExc_NotImplementedError,
                "filters not c contiguous");
            %(fail)s;
        }

        if (%(images)s->nd != 5)
        {
            PyErr_Format(PyExc_TypeError,
                "images ndim (%%i) must be 5",
                %(images)s->nd);
            %(fail)s;
        }

        if (%(filters)s->nd != 7)
        {
            PyErr_Format(PyExc_TypeError,
                "filters ndim (%%i) must be 7",
                %(filters)s->nd);
            %(fail)s;
        }
        //fprintf(stderr, "really running on GPU\\n");

        { // new scope, new vars

            int igroups           = CudaNdarray_HOST_DIMS(%(images)s)[0];
            int icolors_per_group = CudaNdarray_HOST_DIMS(%(images)s)[1];
            int irows             = CudaNdarray_HOST_DIMS(%(images)s)[2];
            int icols             = CudaNdarray_HOST_DIMS(%(images)s)[3];
            int icount            = CudaNdarray_HOST_DIMS(%(images)s)[4];

            int fmodulesR         = CudaNdarray_HOST_DIMS(%(filters)s)[0];
            int fmodulesC         = CudaNdarray_HOST_DIMS(%(filters)s)[1];
            int fcolors           = CudaNdarray_HOST_DIMS(%(filters)s)[2];
            int frows             = CudaNdarray_HOST_DIMS(%(filters)s)[3];
            int fcols             = CudaNdarray_HOST_DIMS(%(filters)s)[4];
            int fgroups           = CudaNdarray_HOST_DIMS(%(filters)s)[5];
            int filters_per_group = CudaNdarray_HOST_DIMS(%(filters)s)[6];

            // XXX: use this parameter properly
            int paddingStart = 0;
            int imgStride = icount;
            float scaleTargets = 0.0;
            float scaleOutput = 1.0;
            bool conv = false;

            if (igroups != fgroups)
            {
                PyErr_Format(PyExc_ValueError,
                    "igroups != fgroups (%%i != %%i)",
                    igroups, fgroups);
                %(fail)s;
            }

            if (icolors_per_group != fcolors)
            {
                PyErr_Format(PyExc_ValueError,
                    "icolors_per_group != fcolors (%%i != %%i)",
                    icolors_per_group,
                    fcolors);
                %(fail)s;
            }

            if (!%(responses)s)
            {
                Py_XDECREF(%(responses)s);
                int dims[5];
                dims[0] = fgroups;
                dims[1] = filters_per_group;
                dims[2] = fmodulesR;
                dims[3] = fmodulesC;
                dims[4] = icount;
                %(responses)s = (CudaNdarray*)CudaNdarray_NewDims(5, dims);
                if (!%(responses)s)
                {
                    %(fail)s;
                }
            }

            assert(CudaNdarray_is_c_contiguous(%(responses)s));

            if (_filterActs(
                    igroups,
                    icolors_per_group,
                    irows,
                    icols,
                    icount,
                    fmodulesR,
                    fmodulesC,
                    frows,
                    fcols,
                    filters_per_group,
                    CudaNdarray_DEV_DATA(%(images)s),
                    CudaNdarray_DEV_DATA(%(filters)s),
                    CudaNdarray_DEV_DATA(%(responses)s),
                    paddingStart,
                    %(moduleStride)s,
                    imgStride,
                    scaleTargets,
                    scaleOutput,
                    conv))
            {
                %(fail)s;
            }
        } // end bogus scope used for vars

        """

        return sio.getvalue() % locals()


@register_opt()
@local_optimizer([FilterActs])
def insert_gpu_filter_acts(node):
    """
    .. todo::

        WRITEME
    """
    if isinstance(node.op, FilterActs):
        images, filters = node.inputs
        if any_from_gpu(images, filters) or any_gpu_client(*node.outputs):
            gpu_filter_acts = GpuFilterActs(
                    module_stride=node.op.module_stride,
                    partial_sum=1)
            return [host_from_gpu(gpu_filter_acts(
                gpu_from_host(images),
                gpu_from_host(filters)))]

class GpuWeightActs(Base):
    """
    .. todo::

        WRITEME
    """
    def make_node(self, images, hidacts, frows, fcols):
        """
        .. todo::

            WRITEME
        """
        if self.partial_sum != 1:
            # this corresponds to grad when doing convolution
            raise NotImplementedError('partial sum')
        frows = theano.tensor.as_tensor_variable(frows)
        fcols = theano.tensor.as_tensor_variable(fcols)
        if frows.dtype[:3] not in ('int', 'uin'):
            raise TypeError(frows)
        if fcols.dtype[:3] not in ('int', 'uin'):
            raise TypeError(frows)
        if frows.ndim:
            raise TypeError('frows should be scalar', frows)
        if fcols.ndim:
            raise TypeError('fcols should be scalar', fcols)

        igroups, icolors, irows, icols, icount = images.type.broadcastable
        hgroups, hcolors, hrows, hcols, hcount = hidacts.type.broadcastable
        otype = theano.sandbox.cuda.CudaNdarrayType(
                broadcastable=(hrows, hcols, icolors,
                    False, False, hgroups, hcolors))
        return theano.Apply(self,
                [images, hidacts, frows, fcols],
                [otype()])

    def c_support_code(self):
        """
        .. todo::

            WRITEME
        """
        cufile = open(os.path.join(_this_dir, 'weight_acts.cu'))
        return cufile.read()

    def c_code_cache_version(self):
        """
        .. todo::

            WRITEME
        """
        return ()

    def c_code(self, node, nodename, inames, onames, sub):
        """
        .. todo::

            WRITEME
        """
        images, hidacts, frows, fcols = inames
        dweights, = onames
        fail = sub['fail']
        moduleStride = str(self.module_stride)

        sio = StringIO.StringIO()

        print >> sio, """

        if (!CudaNdarray_is_c_contiguous(%(images)s))
        {
            //XXX: Alex's code actually supports the rightmost images
            //     dimension strided
            PyErr_Format(PyExc_NotImplementedError,
                "images not c contiguous");
            %(fail)s;
        }

        if (!CudaNdarray_is_c_contiguous(%(hidacts)s))
        {
            PyErr_Format(PyExc_NotImplementedError,
                "hidacts not c contiguous");
            %(fail)s;
        }

        if (%(images)s->nd != 5)
        {
            PyErr_Format(PyExc_TypeError,
                "images ndim (%%i) must be 5",
                %(images)s->nd);
            %(fail)s;
        }

        if (%(hidacts)s->nd != 5)
        {
            PyErr_Format(PyExc_TypeError,
                "hidacts ndim (%%i) must be 5",
                %(images)s->nd);
            %(fail)s;
        }

        if (%(frows)s->nd != 0)
        {
            PyErr_Format(PyExc_TypeError,
                "frows ndim (%%i) must be 0",
                %(frows)s->nd);
            %(fail)s;
        }

        if (%(fcols)s->nd != 0)
        {
            PyErr_Format(PyExc_TypeError,
                "fcols ndim (%%i) must be 0",
                %(fcols)s->nd);
            %(fail)s;
        }

        { // new scope, new vars

            int igroups           = CudaNdarray_HOST_DIMS(%(images)s)[0];
            int icolors_per_group = CudaNdarray_HOST_DIMS(%(images)s)[1];
            int irows             = CudaNdarray_HOST_DIMS(%(images)s)[2];
            int icols             = CudaNdarray_HOST_DIMS(%(images)s)[3];
            int icount            = CudaNdarray_HOST_DIMS(%(images)s)[4];

            int hgroups           = CudaNdarray_HOST_DIMS(%(hidacts)s)[0];
            int hcolors_per_group = CudaNdarray_HOST_DIMS(%(hidacts)s)[1];
            int hrows             = CudaNdarray_HOST_DIMS(%(hidacts)s)[2];
            int hcols             = CudaNdarray_HOST_DIMS(%(hidacts)s)[3];
            int hcount            = CudaNdarray_HOST_DIMS(%(hidacts)s)[4];

            int fmodulesR = hrows;
            int fmodulesC = hcols;
            int fcolors = icolors_per_group;
            int frows = ((dtype_%(frows)s *) (%(frows)s->data))[0];
            int fcols = ((dtype_%(fcols)s *) (%(fcols)s->data))[0];
            int fgroups = hgroups;
            int filters_per_group = hcolors_per_group;

            // XXX: use this parameter properly
            int paddingStart = 0;
            int imgStride = icount;
            float scaleTargets = 0.0;
            float scaleOutput = 1.0;
            int moduleStride = %(moduleStride)s;
            int partialSum = 1; // set to 0 for convolution.

            if (igroups != hgroups)
            {
                PyErr_Format(PyExc_ValueError,
                    "igroups != hgroups (%%i != %%i)",
                    igroups, hgroups);
                %(fail)s;
            }

            if (icolors_per_group != fcolors)
            {
                PyErr_Format(PyExc_ValueError,
                    "icolors_per_group != fcolors (%%i != %%i)",
                    icolors_per_group,
                    fcolors);
                %(fail)s;
            }

            if (icount != hcount)
            {
                PyErr_Format(PyExc_ValueError,
                    "icount != hcount (%%i != %%i)",
                    icount,
                    hcount);
                %(fail)s;
            }

            // XXX: CHECK SHAPE IS CORRECT
            if (!%(dweights)s)
            {
                Py_XDECREF(%(dweights)s);
                int dims[7];
                dims[0] = fmodulesR;
                dims[1] = fmodulesC;
                dims[2] = fcolors;
                dims[3] = frows;
                dims[4] = fcols;
                dims[5] = fgroups;
                dims[6] = filters_per_group;

                %(dweights)s = (CudaNdarray*)CudaNdarray_NewDims(7, dims);
                if (!%(dweights)s)
                {
                    %(fail)s;
                }
            }

            assert(CudaNdarray_is_c_contiguous(%(dweights)s));

            if (_weightActs(
                    igroups,
                    icolors_per_group,
                    irows,
                    icols,
                    icount,
                    fmodulesR,
                    fmodulesC,
                    frows,
                    fcols,
                    filters_per_group,
                    CudaNdarray_DEV_DATA(%(images)s),
                    CudaNdarray_DEV_DATA(%(hidacts)s),
                    CudaNdarray_DEV_DATA(%(dweights)s),
                    paddingStart,
                    moduleStride,
                    imgStride,
                    scaleTargets,
                    scaleOutput,
                    partialSum))
            {
                %(fail)s;
            }
        } // end bogus scope used for vars

        """

        return sio.getvalue() % locals()


@register_opt()
@local_optimizer([WeightActs])
def insert_gpu_weight_acts(node):
    """
    .. todo::

        WRITEME
    """
    if isinstance(node.op, WeightActs):
        """
        .. todo::

            WRITEME
        """
        images, hidacts, frows, fcols = node.inputs
        if any_from_gpu(images, hidacts) or any_gpu_client(*node.outputs):
            gpu_weight_acts = GpuWeightActs(
                    module_stride=node.op.module_stride,
                    partial_sum=1)
            return [host_from_gpu(gpu_weight_acts(
                gpu_from_host(images),
                gpu_from_host(hidacts),
                frows,
                fcols,
                ))]


class GpuImgActs(Base):
    """
    .. todo::

        WRITEME
    """
    def make_node(self, filters, hidacts, irows, icols):
        """
        .. todo::

            WRITEME
        """
        irows = theano.tensor.as_tensor_variable(irows)
        icols = theano.tensor.as_tensor_variable(icols)
        if irows.dtype[:3] not in ('int', 'uin'):
            raise TypeError(irows)
        if icols.dtype[:3] not in ('int', 'uin'):
            raise TypeError(irows)
        if irows.ndim:
            raise TypeError('irows should be scalar', irows)
        if icols.ndim:
            raise TypeError('icols should be scalar', icols)
        return theano.gof.Apply(self,
                [filters, hidacts, irows, icols],
                [hidacts.type()])


    def c_support_code(self):
        """
        .. todo::

            WRITEME
        """
        cufile = open(os.path.join(_this_dir, 'raw_img_acts.cu'))
        return cufile.read()

    def c_code_cache_version(self):
        """
        .. todo::

            WRITEME
        """
        return ()

    def c_code(self, node, nodename, inames, onames, sub):
        """
        .. todo::

            WRITEME
        """
        filters, hidacts, irows, icols = inames
        dimages, = onames
        fail = sub['fail']
        moduleStride = str(self.module_stride)

        sio = StringIO.StringIO()

        print >> sio, """

        if (!CudaNdarray_is_c_contiguous(%(filters)s))
        {
            //XXX: Alex's code actually supports the rightmost images
            //     dimension strided
            PyErr_Format(PyExc_NotImplementedError,
                "images not c contiguous");
            %(fail)s;
        }

        if (!CudaNdarray_is_c_contiguous(%(hidacts)s))
        {
            PyErr_Format(PyExc_NotImplementedError,
                "hidacts not c contiguous");
            %(fail)s;
        }

        if (%(filters)s->nd != 7)
        {
            PyErr_Format(PyExc_TypeError,
                "images ndim (%%i) must be 7",
                %(filters)s->nd);
            %(fail)s;
        }

        if (%(hidacts)s->nd != 5)
        {
            PyErr_Format(PyExc_TypeError,
                "hidacts ndim (%%i) must be 5",
                %(hidacts)s->nd);
            %(fail)s;
        }

        if (%(irows)s->nd != 0)
        {
            PyErr_Format(PyExc_TypeError,
                "frows ndim (%%i) must be 0",
                %(irows)s->nd);
            %(fail)s;
        }

        if (%(icols)s->nd != 0)
        {
            PyErr_Format(PyExc_TypeError,
                "fcols ndim (%%i) must be 0",
                %(icols)s->nd);
            %(fail)s;
        }

        { // new scope, new vars

            int fmodulesR         = CudaNdarray_HOST_DIMS(%(filters)s)[0];
            int fmodulesC         = CudaNdarray_HOST_DIMS(%(filters)s)[1];
            int fcolors           = CudaNdarray_HOST_DIMS(%(filters)s)[2];
            int frows             = CudaNdarray_HOST_DIMS(%(filters)s)[3];
            int fcols             = CudaNdarray_HOST_DIMS(%(filters)s)[4];
            int fgroups           = CudaNdarray_HOST_DIMS(%(filters)s)[5];
            int filters_per_group = CudaNdarray_HOST_DIMS(%(filters)s)[6];

            int hgroups           = CudaNdarray_HOST_DIMS(%(hidacts)s)[0];
            int hcolors_per_group = CudaNdarray_HOST_DIMS(%(hidacts)s)[1];
            int hrows             = CudaNdarray_HOST_DIMS(%(hidacts)s)[2];
            int hcols             = CudaNdarray_HOST_DIMS(%(hidacts)s)[3];
            int hcount            = CudaNdarray_HOST_DIMS(%(hidacts)s)[4];

            int igroups           = fgroups;
            int icolors_per_group = fcolors;
            int irows             = ((dtype_%(irows)s *) (%(irows)s->data))[0];
            int icols             = ((dtype_%(icols)s *) (%(icols)s->data))[0];
            int icount            = hcount;


            // TODO: use this parameter properly
            int paddingStart = 0;
            float scaleTargets = 0.0;
            float scaleOutput = 1.0;
            int moduleStride = %(moduleStride)s;
            bool conv = 0;

            if (hgroups != fgroups)
            {
                PyErr_Format(PyExc_ValueError,
                    "hgroups != fgroups (%%i != %%i)",
                    hgroups, fgroups);
                %(fail)s;
            }

            if (hcolors_per_group != filters_per_group)
            {
                PyErr_Format(PyExc_ValueError,
                    "hcolors_per_group != filters_per_group (%%i != %%i)",
                    hcolors_per_group,
                    filters_per_group);
                %(fail)s;
            }

            // XXX: CHECK SHAPE IS CORRECT
            if (!%(dimages)s)
            {
                Py_XDECREF(%(dimages)s);
                int dims[5];
                dims[0] = igroups;
                dims[1] = icolors_per_group;
                dims[2] = irows;
                dims[3] = icols;
                dims[4] = icount;

                %(dimages)s = (CudaNdarray*)CudaNdarray_NewDims(5, dims);
                if (!%(dimages)s)
                {
                    %(fail)s;
                }
            }

            assert(CudaNdarray_is_c_contiguous(%(dimages)s));

            if (paddingStart + (fmodulesR - 1) * moduleStride + frows < irows)
            {
                PyErr_Format(PyExc_ValueError,
                    "uhoh123: %%i %%i %%i %%i %%i",
                    paddingStart,
                    fmodulesR,
                    moduleStride,
                    frows,
                    irows);
                %(fail)s;
            }

            if (_imgActs(
                    fgroups,
                    filters_per_group,
                    fcolors,
                    hcount,
                    fmodulesR,
                    fmodulesC,
                    frows,
                    fcols,
                    irows,
                    icols,
                    CudaNdarray_DEV_DATA(%(filters)s),
                    CudaNdarray_DEV_DATA(%(hidacts)s),
                    CudaNdarray_DEV_DATA(%(dimages)s),
                    paddingStart,
                    moduleStride,
                    scaleTargets,
                    scaleOutput,
                    conv))
            {
                %(fail)s;
            }
        } // end bogus scope used for vars

        """

        return sio.getvalue() % locals()



@register_opt()
@local_optimizer([ImgActs])
def insert_gpu_img_acts(node):
    """
    .. todo::

        WRITEME
    """
    if isinstance(node.op, ImgActs):
        filters, hidacts, irows, icols = node.inputs
        if any_from_gpu(filters, hidacts) or any_gpu_client(*node.outputs):
            gpu_img_acts = GpuImgActs(
                    module_stride=node.op.module_stride,
                    partial_sum=1)
            return [host_from_gpu(gpu_img_acts(
                gpu_from_host(filters),
                gpu_from_host(hidacts),
                irows,
                icols,
                ))]

########NEW FILE########
__FILENAME__ = localdot
"""
WRITEME
"""

import logging
from ..linear import LinearTransform
from unshared_conv import FilterActs, ImgActs
from theano.sandbox import cuda
if cuda.cuda_available:
    import gpu_unshared_conv # register optimizations

import numpy as np

try:
    import matplotlib.pyplot as plt
except ImportError:
    pass

logger = logging.getLogger(__name__)


class LocalDot(LinearTransform):
    """
    LocalDot is an linear operation computationally similar to
    convolution in the spatial domain, except that whereas convolution
    applying a single filter or set of filters across an image, the
    LocalDot has different filterbanks for different points in the image.

    Mathematically, this is a general linear transform except for a
    restriction that filters are 0 outside of a spatially localized patch
    within the image.

    Image shape is 5-tuple:
        color_groups
        colors_per_group
        rows
        cols
        images

    Filterbank shape is 7-tuple (!)
        0 row_positions
        1 col_positions
        2 colors_per_group
        3 height
        4 width
        5 color_groups
        6 filters_per_group

    The result of left-multiplication a 5-tuple with shape:
        filter_groups
        filters_per_group
        row_positions
        col_positions
        images

    Parameters
    ----------
    filters : WRITEME
    irows : WRITEME
        Image rows
    icols : WRITEME
        Image columns
    subsample : WRITEME
    padding_start : WRITEME
    filters_shape : WRITEME
    message : WRITEME
    """

    def __init__(self, filters, irows, icols=None,
            subsample=(1, 1),
            padding_start=None,
            filters_shape=None,
            message=""):
        LinearTransform.__init__(self, [filters])
        self._filters = filters
        if filters_shape is None:
            self._filters_shape = tuple(filters.get_value(borrow=True).shape)
        else:
            self._filters_shape = tuple(filters_shape)
        self._irows = irows
        if icols is None:
            self._icols = irows
        else:
            self._icols = icols
        if self._icols != self._irows:
            raise NotImplementedError('GPU code at least needs square imgs')
        self._subsample = tuple(subsample)
        self._padding_start = padding_start

        if len(self._filters_shape) != 7:
            raise TypeError('need 7-tuple filter shape', self._filters_shape)
        if self._subsample[0] != self._subsample[1]:
            raise ValueError('subsampling must be same in rows and cols')

        self._filter_acts = FilterActs(self._subsample[0])
        self._img_acts = ImgActs(module_stride=self._subsample[0])

        if message:
            self._message = message
        else:
            self._message = filters.name

    def rmul(self, x):
        """
        .. todo::

            WRITEME
        """
        assert x.ndim == 5
        return self._filter_acts(x, self._filters)

    def rmul_T(self, x):
        """
        .. todo::

            WRITEME
        """
        return self._img_acts(self._filters, x, self._irows, self._icols)

    def col_shape(self):
        """
        .. todo::

            WRITEME
        """
        ishape = self.row_shape() + (-99,)
        fshape = self._filters_shape
        hshape, = self._filter_acts.infer_shape(None, (ishape, fshape))
        assert hshape[-1] == -99
        return hshape[:-1]

    def row_shape(self):
        """
        .. todo::

            WRITEME
        """
        fshape = self._filters_shape
        fmodulesR, fmodulesC, fcolors, frows, fcols = fshape[:-2]
        fgroups, filters_per_group = fshape[-2:]

        return fgroups, fcolors, self._irows, self._icols


    def print_status(self):
        """
        .. todo::

            WRITEME
        """
        raise NotImplementedError("TODO: fix dependence on non-existent "
                "ndarray_status function")
        """print ndarray_status(
                self._filters.get_value(borrow=True),
                msg='%s{%s}'% (self.__class__.__name__,
                    self._message))
        """

    def imshow_gray(self):
        """
        .. todo::

            WRITEME
        """
        filters = self._filters.get_value()
        modR, modC, colors, rows, cols, grps, fs_per_grp = filters.shape
        logger.info(filters.shape)

        rval = np.zeros((
            modR * (rows + 1) - 1,
            modC * (cols + 1) - 1,
        ))

        for rr, modr in enumerate(xrange(0, rval.shape[0], rows + 1)):
            for cc, modc in enumerate(xrange(0, rval.shape[1], cols + 1)):
                rval[modr:modr + rows, modc:modc + cols] = filters[rr, cc, 0, :, :, 0, 0]

        plt.imshow(rval, cmap='gray')
        return rval

########NEW FILE########
__FILENAME__ = test_gpu_unshared_conv
import unittest
from nose.plugins.skip import SkipTest
import numpy

import theano

# Skip test if cuda_ndarray is not available.
from nose.plugins.skip import SkipTest
import theano.sandbox.cuda as cuda_ndarray
if cuda_ndarray.cuda_available == False:
        raise SkipTest('Optional package cuda disabled')

from theano.sandbox.cuda.var import float32_shared_constructor

from .unshared_conv import FilterActs
from .unshared_conv import WeightActs
from .unshared_conv import ImgActs

from .gpu_unshared_conv import (
        GpuFilterActs,
        GpuWeightActs,
        GpuImgActs,
        )

import test_unshared_conv


if theano.config.mode == 'FAST_COMPILE':
    mode_with_gpu = theano.compile.mode.get_mode('FAST_RUN').including('gpu')
else:
    mode_with_gpu = theano.compile.mode.get_default_mode().including('gpu')


class TestGpuFilterActs(test_unshared_conv.TestFilterActs):
    """
    This class tests GpuWeightActs via the gradient of GpuFilterAct

    The correctness of GpuFilterActs is tested in TestMatchFilterActs
    """
    ishape = (1, 1, 4, 4, 2) # 2 4x4 greyscale images
    fshape = (2, 2, 1, 3, 3, 1, 16) # 5 3x3 filters at each location in a 2x2 grid
    module_stride = 1
    dtype = 'float32'
    mode = theano.compile.get_default_mode().including('gpu_opt',
            'fast_run', 'inplace').including('gpu_after_fusion',
                    'fast_run', 'inplace')

    def setUp(self):
        test_unshared_conv.TestFilterActs.setUp(self)
        self.gpu_op = GpuFilterActs(
                module_stride=self.module_stride,
                partial_sum=1)
        self.s_images = float32_shared_constructor(
                self.s_images.get_value())
        self.s_filters = float32_shared_constructor(
                self.s_filters.get_value())

    def test_gpu_shape(self):
        import theano.sandbox.cuda as cuda_ndarray
        if cuda_ndarray.cuda_available == False:
            raise SkipTest('Optional package cuda disabled')
        gpuout = self.gpu_op(self.s_images, self.s_filters)
        assert 'Cuda' in str(self.s_filters.type)
        f = theano.function([], gpuout, mode=mode_with_gpu)
        outval = f()
        assert outval.shape == (
                self.fshape[-2], self.fshape[-1],
                self.fshape[0], self.fshape[1],
                self.ishape[-1])

    def test_insert_gpu_filter_acts(self):
        out = self.op(self.s_images, self.s_filters)
        f = self.function([], out)
        try:
            fgraph = f.maker.fgraph
        except:
            # this needs to work for older versions of theano too
            fgraph = f.maker.env
        assert isinstance(
                fgraph.toposort()[0].op,
                GpuFilterActs)

    def test_gpu_op_eq(self):
        assert GpuFilterActs(1, 1) == GpuFilterActs(1, 1)
        assert not (GpuFilterActs(1, 1) != GpuFilterActs(1, 1))
        assert (GpuFilterActs(1, 2) != GpuFilterActs(1, 1))
        assert (GpuFilterActs(2, 1) != GpuFilterActs(1, 1))
        assert GpuFilterActs(2, 1) != None

class TestGpuWeightActs(unittest.TestCase):
    """
    """
    ishape = (1, 1, 4, 4, 2) # 2 4x4 greyscale images
    hshape = (1, 16, 2, 2, 2)
    fshape = (2, 2, 1, 3, 3, 1, 16) # 5 3x3 filters at each location in a 2x2 grid
    frows = 3
    fcols = 3
    module_stride = 1
    partial_sum = 1
    dtype = 'float32'

    def setUp(self):
        self.gwa = GpuWeightActs(
                module_stride=self.module_stride,
                partial_sum=self.partial_sum)
        self.gpu_images = float32_shared_constructor(
                numpy.random.rand(*self.ishape).astype(self.dtype))
        self.gpu_hidact = float32_shared_constructor(
                numpy.random.rand(*self.hshape).astype(self.dtype))

    def test_shape(self):
        dfilters = self.gwa(self.gpu_images, self.gpu_hidact,
                self.frows, self.fcols)
        f = theano.function([], dfilters)
        outval = f()
        assert outval.shape == self.fshape

class TestGpuImgActs(unittest.TestCase):
    """
    """
    ishape = (1, 1, 4, 4, 2) # 2 4x4 greyscale images
    hshape = (1, 16, 2, 2, 2)
    fshape = (2, 2, 1, 3, 3, 1, 16) # 5 3x3 filters at each location in a 2x2 grid
    irows = 4
    icols = 4
    module_stride = 1
    partial_sum = 1
    dtype = 'float32'

    def setUp(self):
        self.gia = GpuImgActs(
                module_stride=self.module_stride,
                partial_sum=self.partial_sum)
        self.gpu_images = float32_shared_constructor(
                numpy.random.rand(*self.ishape).astype(self.dtype))
        self.gpu_hidact = float32_shared_constructor(
                numpy.random.rand(*self.hshape).astype(self.dtype))
        self.gpu_filters = float32_shared_constructor(
                numpy.random.rand(*self.fshape).astype(self.dtype))

    def test_shape(self):
        dimages = self.gia(self.gpu_filters, self.gpu_hidact,
                self.irows, self.icols)
        f = theano.function([], dimages)
        outval = f()
        assert outval.shape == self.ishape


if 1:
  class TestMatchFilterActs(unittest.TestCase):
    def setUp(self):
        numpy.random.seed(77)

    def run_match(self, images, filters, module_stride, retvals=False, partial_sum=1):

        gfa = GpuFilterActs(module_stride, partial_sum)
        fa = FilterActs(module_stride)

        gpu_images = float32_shared_constructor(images)
        gpu_filters = float32_shared_constructor(filters)
        cpu_images = theano.shared(images)
        cpu_filters = theano.shared(filters)

        gpu_out = gfa(gpu_images, gpu_filters)
        cpu_out = fa(cpu_images, cpu_filters)

        f = theano.function([], [cpu_out, gpu_out])
        cpuval, gpuval = f()
        gpuval = numpy.asarray(gpuval)

        if retvals:
            return cpuval, gpuval
        else:
            #print 'run_match: cpu shape', cpuval.shape
            #print 'run_match: gpu shape', gpuval.shape
            assert cpuval.shape == gpuval.shape
            assert numpy.allclose(cpuval, gpuval)

    def run_match_shape(self, ishape, fshape, module_stride, dtype='float32'):
        return self.run_match(
            images=numpy.random.rand(*ishape).astype(dtype),
            filters=numpy.random.rand(*fshape).astype(dtype),
            module_stride=module_stride)

    def test_small_random(self):
        self.run_match_shape(
            ishape = (1, 1, 4, 4, 2),
            fshape = (2, 2, 1, 3, 3, 1, 16),
            module_stride = 1)

    def test_small_random_colors(self):
        self.run_match_shape(
            ishape = (1, 6, 4, 4, 2),
            fshape = (2, 2, 6, 3, 3, 1, 16),
            module_stride = 1)

    def test_small_random_groups(self):
        self.run_match_shape(
            ishape = (5, 6, 4, 4, 2),
            fshape = (2, 2, 6, 3, 3, 5, 16),
            module_stride = 1)

    def test_small_random_module_stride(self):
        self.run_match_shape(
            ishape = (4, 6, 5, 5, 1),
            fshape = (2, 2, 6, 3, 3, 4, 16),
            module_stride = 2)

    def test_med_random_module_stride(self):
        self.run_match_shape(
            ishape = (4, 6, 32, 32, 1),
            fshape = (12, 12, 6, 3, 3, 4, 16),
            module_stride = 2)


    def _blah_topcorner_filter1(self):
        ishape = (1, 1, 4, 4, 2)
        fshape = (2, 2, 1, 3, 3, 1, 16)
        images = numpy.random.rand(*ishape).astype('float32')
        filters = numpy.random.rand(*fshape).astype('float32')
        filters *= 0
        filters[0,0,0,0,0,0,0] = 1
        self.run_match(images, filters, 1)

    def _blah_botcorner_filter1(self):
        ishape = (1, 1, 4, 4, 2)
        fshape = (2, 2, 1, 3, 3, 1, 16)
        images = numpy.random.rand(*ishape).astype('float32')
        filters = numpy.random.rand(*fshape).astype('float32')
        filters *= 0
        filters[1,1,0,0,0,0,0] = 1
        cpuval, gpuval = self.run_match(images, filters, 1, retvals=True)
        print images
        print cpuval[:, :, 1, 1, :]
        print gpuval[:, :, 1, 1, :]


########NEW FILE########
__FILENAME__ = test_localdot
import nose
import unittest

import numpy as np

import theano

from localdot import LocalDot

from ..test_matrixmul import SymbolicSelfTestMixin


class TestLocalDot32x32(unittest.TestCase, SymbolicSelfTestMixin):
    channels = 3
    bsize = 10     # batch size
    imshp = (32, 32)
    ksize = 5
    nkern_per_group = 16
    subsample_stride = 1
    ngroups = 1

    def rand(self, shp):
        return np.random.rand(*shp).astype('float32')

    def setUp(self):
        np.random.seed(234)
        assert self.imshp[0] == self.imshp[1]
        fModulesR = (self.imshp[0] - self.ksize + 1) // self.subsample_stride
        #fModulesR += 1 # XXX GpuImgActs crashes w/o this??
        fModulesC = fModulesR
        self.fshape = (fModulesR, fModulesC, self.channels // self.ngroups,
                self.ksize, self.ksize, self.ngroups, self.nkern_per_group)
        self.ishape = (self.ngroups, self.channels // self.ngroups,
                self.imshp[0], self.imshp[1], self.bsize)
        self.hshape = (self.ngroups, self.nkern_per_group, fModulesR, fModulesC,
                self.bsize)

        filters = theano.shared(self.rand(self.fshape))

        self.A = LocalDot(filters, self.imshp[0], self.imshp[1],
                subsample=(self.subsample_stride, self.subsample_stride))

        self.xlval = self.rand((self.hshape[-1],) + self.hshape[:-1])
        self.xrval = self.rand(self.ishape)

        self.xl = theano.shared(self.xlval)
        self.xr = theano.shared(self.xrval)

    # N.B. the tests themselves come from SymbolicSelfTestMixin


class TestLocalDotLargeGray(TestLocalDot32x32):

    channels = 1
    bsize = 128
    imshp = (256, 256)
    ksize = 9
    nkern_per_group = 16
    subsample_stride = 2
    ngroups = 1
    n_patches = 3000

    def rand(self, shp):
        return np.random.rand(*shp).astype('float32')

    # not really a test, but important code to support
    # Currently exposes error, by e.g.:
    #  CUDA_LAUNCH_BLOCKING=1
    #  THEANO_FLAGS=device=gpu,mode=DEBUG_MODE
    #  nosetests -sd test_localdot.py:TestLocalDotLargeGray.run_autoencoder
    def run_autoencoder(
        self,
        n_train_iter=10000,   # -- make this small to be a good unit test
        rf_shape=(9, 9),
        n_filters=1024,
        dtype='float32',
        module_stride=2,
        lr=0.01,
        show_filters=True,
        ):
        if show_filters:
            # import here to fail right away
            import matplotlib.pyplot as plt

        try:
            import skdata.vanhateren.dataset
        except ImportError:
            raise nose.SkipTest()

        # 1. Get a set of image patches from the van Hateren data set
        print 'Loading van Hateren images'
        n_images = 50
        vh = skdata.vanhateren.dataset.Calibrated(n_images)
        patches = vh.raw_patches((self.n_patches,) + self.imshp,
                                 items=vh.meta[:n_images],
                                 rng=np.random.RandomState(123),
                                )
        patches = patches.astype('float32')
        patches /= patches.reshape(self.n_patches, self.imshp[0] * self.imshp[1])\
            .max(axis=1)[:, None, None]
        # TODO: better local contrast normalization

        if 0 and show_filters:
            plt.subplot(2, 2, 1); plt.imshow(patches[0], cmap='gray')
            plt.subplot(2, 2, 2); plt.imshow(patches[1], cmap='gray')
            plt.subplot(2, 2, 3); plt.imshow(patches[2], cmap='gray')
            plt.subplot(2, 2, 4); plt.imshow(patches[3], cmap='gray')
            plt.show()

        # -- Convert patches to localdot format:
        #    groups x colors x rows x cols x images
        patches5 = patches[:, :, :, None, None].transpose(3, 4, 1, 2, 0)
        print 'Patches shape', patches.shape, self.n_patches, patches5.shape

        # 2. Set up an autoencoder
        print 'Setting up autoencoder'
        hid = theano.tensor.tanh(self.A.rmul(self.xl))
        out = self.A.rmul_T(hid)
        cost = ((out - self.xl) ** 2).sum()
        params = self.A.params()
        gparams = theano.tensor.grad(cost, params)
        train_updates = [(p, p - lr / self.bsize * gp)
                         for (p, gp) in zip(params, gparams)]
        if 1:
            train_fn = theano.function([], [cost], updates=train_updates)
        else:
            train_fn = theano.function([], [], updates=train_updates)

        theano.printing.debugprint(train_fn)

        # 3. Train it
        params[0].set_value(0.001 * params[0].get_value())
        for ii in xrange(0, self.n_patches, self.bsize):
            self.xl.set_value(patches5[:, :, :, :, ii:ii + self.bsize], borrow=True)
            cost_ii, = train_fn()
            print 'Cost', ii, cost_ii

        if 0 and show_filters:
            self.A.imshow_gray()
            plt.show()

        assert cost_ii < 0  # TODO: determine a threshold for detecting regression bugs



########NEW FILE########
__FILENAME__ = test_unshared_conv
import sys
import unittest

import numpy

import theano
from theano.sandbox.cuda.var import float32_shared_constructor
from theano.tests.unittest_tools import verify_grad

from .unshared_conv import FilterActs
from .unshared_conv import WeightActs
from .unshared_conv import ImgActs

def rand(shp, dtype):
    return numpy.random.rand(*shp).astype(dtype)

def assert_linear(f, pt, mode=None):
    t = theano.tensor.scalar(dtype=pt.dtype)
    ptlike = theano.shared(rand(
        pt.get_value(borrow=True).shape,
        dtype=pt.dtype))
    out = f(pt)
    out2 = f(pt * t)
    out3 = f(ptlike) + out
    out4 = f(pt + ptlike)

    f = theano.function([t], [out * t, out2, out3, out4],
            allow_input_downcast=True,
            mode=mode)
    outval, out2val, out3val, out4val = f(3.6)
    assert numpy.allclose(outval, out2val)
    assert numpy.allclose(out3val, out4val)


class TestFilterActs(unittest.TestCase):
    # 2 4x4 greyscale images
    ishape = (1, 1, 4, 4, 2)
    # 5 3x3 filters at each location in a 2x2 grid
    fshape = (2, 2, 1, 3, 3, 1, 5)
    module_stride = 1
    dtype = 'float64'
    mode = theano.compile.get_default_mode()

    def function(self, inputs, outputs):
        return theano.function(inputs, outputs, mode=self.mode)

    def setUp(self):
        self.op = FilterActs(self.module_stride)
        self.s_images = theano.shared(rand(self.ishape, self.dtype),
                name = 's_images')
        self.s_filters = theano.shared(
                rand(self.fshape, self.dtype),
                name = 's_filters')

    def test_type(self):
        out = self.op(self.s_images, self.s_filters)
        assert out.dtype == self.dtype
        assert out.ndim == 5

        f = self.function([], out)
        outval = f()
        assert len(outval.shape) == len(self.ishape)
        assert outval.dtype == self.s_images.get_value(borrow=True).dtype

    def test_linearity_images(self):
        assert_linear(
                lambda imgs: self.op(imgs, self.s_filters),
                self.s_images,
                mode=self.mode)

    def test_linearity_filters(self):
        assert_linear(
                lambda fts: self.op(self.s_images, fts),
                self.s_filters,
                mode=self.mode)

    def test_shape(self):
        out = self.op(self.s_images, self.s_filters)
        f = self.function([], out)
        outval = f()
        assert outval.shape == (self.fshape[-2],
                self.fshape[-1],
                self.fshape[0], self.fshape[1],
                self.ishape[-1])

    def test_grad_left(self):
        # test only the left so that the right can be a shared variable,
        # and then TestGpuFilterActs can use a gpu-allocated shared var
        # instead.
        def left_op(imgs):
            return self.op(imgs, self.s_filters)
        try:
            verify_grad(left_op, [self.s_images.get_value()],
                    mode=self.mode)
        except verify_grad.E_grad, e:
            print e.num_grad.gf
            print e.analytic_grad
            raise

    def test_grad_right(self):
        # test only the right so that the left can be a shared variable,
        # and then TestGpuFilterActs can use a gpu-allocated shared var
        # instead.
        def right_op(filters):
            rval =  self.op(self.s_images, filters)
            rval.name = 'right_op(%s, %s)' % (self.s_images.name,
                    filters.name)
            assert rval.dtype == filters.dtype
            return rval
        try:
            verify_grad(right_op, [self.s_filters.get_value()],
                    mode=self.mode)
        except verify_grad.E_grad, e:
            print e.num_grad.gf
            print e.analytic_grad
            raise

    def test_dtype_mismatch(self):
        self.assertRaises(TypeError,
                self.op,
                theano.tensor.cast(self.s_images, 'float32'),
                theano.tensor.cast(self.s_filters, 'float64'))
        self.assertRaises(TypeError,
                self.op,
                theano.tensor.cast(self.s_images, 'float64'),
                theano.tensor.cast(self.s_filters, 'float32'))

    def test_op_eq(self):
        assert FilterActs(1) == FilterActs(1)
        assert not (FilterActs(1) != FilterActs(1))
        assert (FilterActs(2) != FilterActs(1))
        assert FilterActs(1) != None


class TestFilterActsF32(TestFilterActs):
    dtype = 'float32'


class TestWeightActs(unittest.TestCase):
    # 1 5x5 6-channel image (2 groups of 3 channels)
    ishape = (6, 3, 5, 5, 1)
    hshape = (6, 4, 2, 2, 1)
    fshape = (2, 2, 3, 2, 2, 6, 4)
    module_stride = 2
    dtype = 'float64'

    frows = property(lambda s: s.fshape[3])
    fcols = property(lambda s: s.fshape[4])

    def setUp(self):
        self.op = WeightActs(self.module_stride)
        self.s_images = theano.shared(rand(self.ishape, self.dtype))
        self.s_hidacts = theano.shared(rand(self.hshape, self.dtype))

    def test_type(self):
        out = self.op(self.s_images, self.s_hidacts, self.frows, self.fcols)
        assert out.dtype == self.dtype
        assert out.ndim == 7
        f = theano.function([], out)
        outval = f()
        assert outval.shape == self.fshape
        assert outval.dtype == self.dtype

    def test_linearity_images(self):
        def f(images):
            return self.op(images, self.s_hidacts, self.frows, self.fcols)
        assert_linear(f, self.s_images)

    def test_linearity_hidacts(self):
        def f(hidacts):
            return self.op(self.s_images, hidacts, self.frows, self.fcols)
        assert_linear(f, self.s_hidacts)

    def test_grad(self):
        def op2(imgs, hids):
            return self.op(imgs, hids, self.frows, self.fcols)
        try:
            verify_grad(op2,
                    [self.s_images.get_value(),
                        self.s_hidacts.get_value()])
        except verify_grad.E_grad, e:
            print e.num_grad.gf
            print e.analytic_grad
            raise

    def test_dtype_mismatch(self):
        self.assertRaises(TypeError,
                self.op,
                theano.tensor.cast(self.s_images, 'float32'),
                theano.tensor.cast(self.s_hidacts, 'float64'),
                self.frows, self.fcols)
        self.assertRaises(TypeError,
                self.op,
                theano.tensor.cast(self.s_images, 'float64'),
                theano.tensor.cast(self.s_hidacts, 'float32'),
                self.frows, self.fcols)


class TestImgActs(unittest.TestCase):
    # 1 5x5 6-channel image (2 groups of 3 channels)
    ishape = (6, 3, 5, 5, 2)
    hshape = (6, 4, 3, 3, 2)
    fshape = (3, 3, 3, 2, 2, 6, 4)
    module_stride = 1
    dtype = 'float64'

    #frows = property(lambda s: s.fshape[3])
    #fcols = property(lambda s: s.fshape[4])
    irows = property(lambda s: s.ishape[2])
    icols = property(lambda s: s.ishape[3])

    def setUp(self):
        self.op = ImgActs(module_stride=self.module_stride)
        self.s_filters = theano.shared(rand(self.fshape, self.dtype))
        self.s_hidacts = theano.shared(rand(self.hshape, self.dtype))

    def test_type(self):
        out = self.op(self.s_filters, self.s_hidacts, self.irows, self.icols)
        assert out.dtype == self.dtype
        assert out.ndim == 5
        f = theano.function([], out)
        outval = f()
        assert outval.shape == self.ishape
        assert outval.dtype == self.dtype

    def test_linearity_filters(self):
        def f(filts):
            return self.op(filts, self.s_hidacts, self.irows, self.icols)
        assert_linear(f, self.s_filters)

    def test_linearity_hidacts(self):
        def f(hidacts):
            return self.op(self.s_filters, hidacts, self.irows, self.icols)
        assert_linear(f, self.s_hidacts)

    def test_grad(self):
        def op2(imgs, hids):
            return self.op(imgs, hids, self.irows, self.icols)
        try:
            verify_grad(op2,
                    [self.s_filters.get_value(),
                        self.s_hidacts.get_value()])
        except verify_grad.E_grad, e:
            print e.num_grad.gf
            print e.analytic_grad
            raise

    def test_dtype_mismatch(self):
        self.assertRaises(TypeError,
                self.op,
                theano.tensor.cast(self.s_filters, 'float32'),
                theano.tensor.cast(self.s_hidacts, 'float64'),
                self.irows, self.icols)
        self.assertRaises(TypeError,
                self.op,
                theano.tensor.cast(self.s_filters, 'float64'),
                theano.tensor.cast(self.s_hidacts, 'float32'),
                self.irows, self.icols)





########NEW FILE########
__FILENAME__ = unshared_conv
"""
XXX
"""

import numpy
import theano

# Use grad_not_implemented for versions of theano that support it
try:
    grad_not_implemented = theano.gradient.grad_not_implemented
except:
    def grad_not_implemented(op, idx, ipt):
        return None

def any_symbolic(*args):
    """
    Return True iff any a in `args` is a theano Variable
    """
    for a in args:
        if isinstance(a, theano.Variable):
            return True
    return False

def not_symbolic(*args):
    return not any_symbolic(*args)


class Base(theano.Op):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    module_stride : WRITEME
    """
    def __init__(self,
            module_stride=1,
            ):
        self.module_stride = module_stride

    def _attributes(self):
        return (
                self.module_stride,
                )

    def __eq__(self, other):
        return (type(self) == type(other)
                and self._attributes() == other._attributes())

    def __hash__(self):
        return hash((type(self), self._attributes()))

    def __str__(self):
        return '%s{module_stride=%i}' % (
                self.__class__.__name__,
                self.module_stride,
                )


class FilterActs(Base):
    """
    Images of shape: colors x
    Filters are of shape:
        channels
    """

    def make_node(self, images, filters):
        images = theano.tensor.as_tensor_variable(images)
        filters = theano.tensor.as_tensor_variable(filters)
        ibcast = images.broadcastable
        fbcast = filters.broadcastable
        igroups, icolors_per_group, irows, icols, icount = ibcast
        fmodulesR, fmodulesC, fcolors, frows, fcols = fbcast[:-2]
        fgroups, filters_per_group = fbcast[-2:]
        hbcast = (fgroups, filters_per_group, fmodulesR, fmodulesC, icount)
        htype = theano.tensor.TensorType(
                dtype=images.dtype,
                broadcastable=hbcast)
        if images.dtype != filters.dtype:
            raise TypeError('dtype mismatch', (images, images.dtype, filters,
                filters.dtype))
        return theano.gof.Apply(self,
                [images, filters],
                [htype()])

    def perform(self, node, iargs, ostor):
        images, filters = iargs

        igroups, icolors_per_group, irows, icols, icount = images.shape
        fmodulesR, fmodulesC, fcolors, frows, fcols = filters.shape[:-2]
        fgroups, filters_per_group = filters.shape[-2:]

        hshape = self.infer_shape(node, (images.shape, filters.shape))[0]

        hidacts = numpy.zeros(hshape, dtype=images.dtype)

        for mR in xrange(fmodulesR):
            for mC in xrange(fmodulesC):
                for gg in xrange(igroups):
                    img_r_offset = mR * self.module_stride
                    img_c_offset = mC * self.module_stride
                    rc_images = images[gg, :,
                            img_r_offset:img_r_offset + frows,
                            img_c_offset:img_c_offset + fcols,
                            :]
                    rc_filters = filters[mR, mC, :, :, :, gg, :]
                    # rc_images are fcolors x frows x fcols x count
                    # rc_filters are fcolors x frows x fcols x fpg

                    left_arg = rc_filters.reshape(-1, filters_per_group).T
                    right_arg = rc_images.reshape(-1, icount)

                    try:
                        rc_hidacts = numpy.dot(left_arg, right_arg)
                    except ValueError, e:
                        if 'matrices are not aligned' in str(e):
                            raise ValueError("matrices are not aligned: " + \
                                    str(left_arg.shape) + ' vs ' + str(right_arg.shape))
                        else:
                            raise
                    hidacts[gg, :, mR, mC, :] = rc_hidacts
        ostor[0][0] = hidacts
        
        print_sizes = 0
        if print_sizes:
            print 'FilterActs shapes: images', images.shape
            print 'FilterActs shapes: filters', filters.shape
            print 'FilterActs shapes: hidacts', hidacts.shape

    def grad(self, inputs, goutputs):
        images, filters = inputs
        frows = filters.shape[3]
        fcols = filters.shape[4]
        irows = images.shape[2]
        icols = images.shape[3]
        hidacts = goutputs[0]
        # filters and hidacts must have same dtype, upcast if needed
        if filters.dtype == 'float32' and hidacts.dtype == 'float64':
            filters = theano.tensor.cast(filters, 'float64')
        gimages = ImgActs(module_stride=self.module_stride)(
                filters, hidacts, irows, icols)
        # images and hidacts must have same dtype, upcast if needed
        if images.dtype == 'float32' and hidacts.dtype == 'float64':
            images = theano.tensor.cast(images, 'float64')
        gfilters = WeightActs(module_stride=self.module_stride)(
                images, hidacts, frows, fcols)
        return [gimages, gfilters]

    def infer_shape(self, node, shapes):
        ishape, fshape = shapes

        igroups, icolors_per_group, irows, icols, icount = ishape
        fmodulesR, fmodulesC, fcolors, frows, fcols = fshape[:-2]
        fgroups, filters_per_group = fshape[-2:]

        if not any_symbolic(irows, icols) and irows != icols:
            raise ValueError("non-square image argument",
                    (irows, icols))
        if not any_symbolic(frows, fcols) and frows != fcols:
            raise ValueError("non-square filter shape",
                    (frows, fcols))
        if not any_symbolic(fmodulesR, fmodulesC) and fmodulesR != fmodulesC:
            raise ValueError('non-square filter grouping',
                    (fmodulesR, fmodulesC))
        if (not any_symbolic(icolors_per_group, fcolors)
                and icolors_per_group != fcolors):
            raise ValueError("color counts don't match",
                    (icolors_per_group, fcolors))

        hshape = (fgroups, filters_per_group, fmodulesR, fmodulesC, icount)
        return [hshape]


class WeightActs(Base):
    """
    Images of shape: colors x
    Filters are of shape:
        channels
    """

    def make_node(self, images, hidacts, frows, fcols):
        images, hidacts, frows, fcols = map(theano.tensor.as_tensor_variable,
                [images, hidacts, frows, fcols])
        if frows.dtype[:3] not in ('int', 'uin'):
            raise TypeError(frows)
        if fcols.dtype[:3] not in ('int', 'uin'):
            raise TypeError(frows)
        if frows.ndim:
            raise TypeError('frows should be scalar', frows)
        if fcols.ndim:
            raise TypeError('fcols should be scalar', fcols)

        if images.dtype != hidacts.dtype:
            raise TypeError('images and hidacts dtype mismatch',
                    (images.dtype, hidacts.dtype))

        igroups, icolors, irows, icols, icount = images.type.broadcastable
        hgroups, hcolors, hrows, hcols, hcount = hidacts.type.broadcastable
        otype = theano.tensor.TensorType(
                dtype=images.dtype,
                broadcastable=(hrows, hcols, icolors,
                    False, False, hgroups, hcolors))
        return theano.Apply(self,
                [images, hidacts, frows, fcols],
                [otype()])

    def perform(self, node, iargs, ostor):
        images, hidacts, frows, fcols = iargs

        if frows != fcols:
            # this could be implemented, but GPU case doesn't do it
            raise NotImplementedError("non-square filter shape",
                    (frows, fcols))

        #igroups, icolors_per_group, irows, icols, icount = images.shape
        hgroups, hcolors_per_group, hrows, hcols, hcount = hidacts.shape
        fshape = list(self.infer_shape(node,
                (images.shape, hidacts.shape, (), ()))[0])
        fcolors = fshape[2]
        fshape[3] = frows
        fshape[4] = fcols

        filters = numpy.zeros(fshape, dtype=images.dtype)

        for mR in xrange(hrows):
            for mC in xrange(hcols):
                for gg in xrange(hgroups):
                    img_r_offset = mR * self.module_stride
                    img_c_offset = mC * self.module_stride
                    rc_images = images[gg, :,
                            img_r_offset:img_r_offset + frows,
                            img_c_offset:img_c_offset + fcols,
                            :]
                    # rc_images is fcolors x frows x fcols x count

                    rc_hidacts = hidacts[gg, :, mR, mC, :]
                    # rc_hidacts is fpg x count

                    rc_filters = numpy.dot(
                            rc_images.reshape(-1, hcount),
                            rc_hidacts.T)
                    filters[mR, mC, :, :, :, gg, :] = rc_filters.reshape(
                            (fcolors, frows, fcols, hcolors_per_group))
        ostor[0][0] = filters

    def grad(self, inputs, goutputs):
        images, hidacts, frows, fcols = inputs
        gfilters, = goutputs
        irows = images.shape[2]
        icols = images.shape[3]
        gimages = ImgActs(module_stride=self.module_stride)(
                gfilters, hidacts, irows, icols)
        ghidacts = FilterActs(module_stride=self.module_stride)(
                images, gfilters)
        return [gimages, ghidacts, grad_not_implemented(self, 2, inputs[2]),
                grad_not_implemented(self, 3, inputs[3])]

    def infer_shape(self, node, shapes):
        images, hidacts, frows, fcols = node.inputs
        ishape, hshape, frowshp, fcolshp = shapes
        igroups, icolors_per_group, irows, icols, icount = ishape
        hgroups, hcolors_per_group, hrows, hcols, hcount = hshape

        fmodulesR = hrows
        fmodulesC = hcols
        fcolors = icolors_per_group
        # frows already assigned
        # fcols already assigned
        fgroups = hgroups
        filters_per_group = hcolors_per_group

        fshape = (fmodulesR, fmodulesC, fcolors, frows, fcols, fgroups,
                    filters_per_group)

        if not_symbolic(irows, icols) and irows != icols:
            raise NotImplementedError("non-square image argument",
                    (irows, icols))
        if not_symbolic(hrows, hcols) and hrows != hcols:
            raise NotImplementedError("non-square filter shape",
                    (hrows, hcols))
        if not_symbolic(icount, hcount) and icount != hcount:
            raise NotImplementedError("different number of images",
                    (icount, hcount))
        if not_symbolic(igroups, hgroups) and igroups != hgroups:
            raise ValueError("hgroups must match igroups",
                    igroups, hgroups)

        return [fshape]


class ImgActs(Base):
    """
    XXX
    """
    def make_node(self, filters, hidacts, irows, icols):
        filters, hidacts, irows, icols = map(theano.tensor.as_tensor_variable,
                [filters, hidacts, irows, icols])
        if irows.dtype[:3] not in ('int', 'uin'):
            raise TypeError(irows)
        if icols.dtype[:3] not in ('int', 'uin'):
            raise TypeError(irows)
        if irows.ndim:
            raise TypeError('irows should be scalar', irows)
        if icols.ndim:
            raise TypeError('icols should be scalar', icols)
        if filters.ndim != 7:
            raise TypeError('filters must be 7d tensor', filters)
        if hidacts.ndim != 5:
            raise TypeError('hidacts must be 5d tensor', filters)
        if filters.dtype != hidacts.dtype:
            raise TypeError('filters and hidacts must have matching dtype',
                    (filters, filters.dtype, hidacts, hidacts.dtype))
        return theano.gof.Apply(self,
                [filters, hidacts, irows, icols],
                [hidacts.type()])

    def perform(self, node, iargs, ostor):
        filters, hidacts, irows, icols = iargs

        hgroups, hcolors_per_group, hrows, hcols, hcount = hidacts.shape

        fmodulesR, fmodulesC, fcolors, frows, fcols = filters.shape[:-2]
        fgroups, filters_per_group = filters.shape[-2:]

        igroups = fgroups
        icolors_per_group = fcolors
        icount = hcount

        #print 'IMGACTS: NODE OUTPUTS[0]'
        #print theano.printing.debugprint(node.outputs[0])
        #print 'FILTERS SHAPE:', filters.shape
        #print 'HIDACTS SHAPE:', hidacts.shape
        if hrows != hcols:
            raise NotImplementedError("non-square hidacts argument",
                    (hrows, hcols))
        if frows != fcols:
            raise NotImplementedError("non-square filter shape",
                    (frows, fcols))
        if fmodulesR != fmodulesC:
            raise NotImplementedError('non-square filter grouping',
                    (fmodulesR, fmodulesC))
        if hcolors_per_group != filters_per_group:
            raise ValueError("color counts don't match",
                    (hcolors_per_group, filters_per_group))
        if irows != icols:
            raise NotImplementedError("non-square image argument",
                    (irows, icols))

        images = numpy.zeros(
                (igroups, icolors_per_group, irows, icols, icount),
                dtype=hidacts.dtype)

        for mR in xrange(fmodulesR):
            for mC in xrange(fmodulesC):
                for gg in xrange(igroups):
                    rc_filters = filters[mR, mC, :, :, :, gg, :]
                    # rc_filters is fcolors x frows x fcols x fpg

                    rc_hidacts = hidacts[gg, :, mR, mC, :]
                    # rc_hidacts is fpg x icount

                    img_r_offset = mR * self.module_stride
                    img_c_offset = mC * self.module_stride
                    images[gg, :,
                            img_r_offset:img_r_offset + frows,
                            img_c_offset:img_c_offset + fcols,
                            :] += numpy.dot(
                                    rc_filters.reshape(-1, filters_per_group),
                                    rc_hidacts
                                    ).reshape(
                                    (fcolors, frows, fcols, icount))
        ostor[0][0] = images

    def grad(self, inputs, goutputs):
        filters, hidacts, irows, icols = inputs
        gimages, = goutputs
        frows = filters.shape[3]
        fcols = filters.shape[4]
        gfilters = WeightActs(module_stride=self.module_stride)(
                gimages, hidacts, frows, fcols)
        ghidacts = FilterActs(module_stride=self.module_stride)(
                gimages, filters)
        return [gfilters, ghidacts, grad_not_implemented(self, 2, inputs[2]),
                grad_not_implemented(self, 3, inputs[3])]



########NEW FILE########
__FILENAME__ = util

from imaging import tile_slices_to_image

_ndarray_status_fmt='%(msg)s shape=%(shape)s min=%(min)f max=%(max)f'


def ndarray_status(x, fmt=_ndarray_status_fmt, msg="", **kwargs):
    kwargs.update(dict(
            msg=msg,
            min=x.min(),
            max=x.max(),
            mean=x.mean(),
            var = x.var(),
            shape=x.shape))
    return fmt%kwargs


########NEW FILE########
__FILENAME__ = pca
import warnings

warnings.warn("pylearn2.pca has been moved to pylearn2.models.pca and "
    "will be removed from the library on or after Aug 24, 2014.")

# Make sure old import statements still work
from models.pca import SparseMatPCA
from models.pca import OnlinePCA
from models.pca import Cov
from models.pca import CovEigPCA
from models.pca import SVDPCA
from models.pca import SparsePCA
from models.pca import PcaOnlineEstimator

########NEW FILE########
__FILENAME__ = rbm_tools
"""Tools for estimating the partition function of an RBM"""
import numpy
import theano
from theano import tensor, config
from theano.tensor import nnet
from pylearn2.utils.rng import make_np_rng, make_theano_rng
from pylearn2.utils.mem import TypicalMemoryError


def compute_log_z(rbm, free_energy_fn, max_bits=15):
    """
    Compute the log partition function of an (binary-binary) RBM.

    Parameters
    ----------
    rbm : object
        An RBM object from `pylearn2.models`.
    free_energy_fn : callable
        A callable object (e.g. Theano function) that computes the
        free energy of a stack of configuration for this RBM.
    max_bits : int, optional
        The (base-2) log of the number of states to enumerate (and
        compute free energy for) at a time.

    Notes
    -----
    This function enumerates a sum with exponentially many terms, and
    should not be used with more than a small, toy model.
    """
    # Pick whether to iterate over visible or hidden states.
    if rbm.nvis < rbm.nhid:
        width = rbm.nvis
        type = 'vis'
    else:
        width = rbm.nhid
        type = 'hid'

    # Determine in how many steps to compute Z.
    block_bits = width if (not max_bits or width < max_bits) else max_bits
    block_size = 2 ** block_bits

    # Allocate storage for 2**block_bits of the 2**width possible
    # configurations.
    try:
        logz_data_c = numpy.zeros(
            (block_size, width),
            order='C',
            dtype=config.floatX
        )
    except MemoryError:
        raise MemoryError("failed to allocate (%d, %d) matrix of "
                          "type %s in compute_log_z; try a smaller "
                          "value of max_bits" %
                          (block_size, width, str(config.floatX)))

    # fill in the first block_bits, which will remain fixed for all
    # 2**width configs
    tensor_10D_idx = numpy.ndindex(*([2] * block_bits))
    for i, j in enumerate(tensor_10D_idx):
        logz_data_c[i, -block_bits:] = j
    try:
        logz_data = numpy.array(logz_data_c, order='F', dtype=config.floatX)
    except MemoryError:
        raise MemoryError("failed to allocate (%d, %d) matrix of "
                          "type %s in compute_log_z; try a smaller "
                          "value of max_bits" %
                          (block_size, width, str(config.floatX)))

    # Allocate storage for (negative) free-energy of all 2**width
    # configurations.
    try:
        nFE = numpy.zeros(2 ** width, dtype=config.floatX)
    except MemoryError:
        raise TypicalMemoryError("failed to allocate free energy storage "
                                 "array in compute_log_z; your model is too "
                                 "big to use with this function")

    # now loop 2**(width - block_bits) times, filling in the
    # most-significant bits
    for bi, up_bits in enumerate(numpy.ndindex(*([2] * (width - block_bits)))):
        logz_data[:, :width - block_bits] = up_bits
        nFE[bi * block_size:(bi + 1) * block_size] = -free_energy_fn(logz_data)
    alpha = nFE.max()
    # Do the subtraction and exponentiation in-place so as to not incur a copy.
    nFE -= alpha
    numpy.exp(nFE, nFE)
    log_z = numpy.log(nFE.sum()) + alpha
    return log_z


def compute_nll(rbm, data, log_z, free_energy_fn, bufsize=1000, preproc=None):
    """
    .. todo::

        WRITEME
    """
    i = 0.
    nll = 0
    for i in xrange(0, len(data), bufsize):
        # recast data as floatX and apply preprocessing if required
        x = numpy.array(data[i:i + bufsize, :], dtype=config.floatX)
        if preproc:
            x = preproc(x)
        # compute sum of likelihood for current buffer
        x_nll = -numpy.sum(-free_energy_fn(x) - log_z)
        # perform moving average of negative likelihood
        # divide by len(x) and not bufsize, since last buffer might be smaller
        nll = (i * nll + x_nll) / (i + len(x))
    return nll


def rbm_ais(rbm_params, n_runs, visbias_a=None, data=None,
            betas=None, key_betas=None, rng=None, seed=23098):
    """
    Implements Annealed Importance Sampling for Binary-Binary RBMs

    Parameters
    ----------
    rbm_params : list
        list of `numpy.ndarrays` containing model parameters:
        [weights,visbias,hidbias]
    n_runs : int
        Number of particles to use in AIS simulation (size of minibatch)
    visbias_a : numpy.ndarray, optional
        Optional override for visible biases. If both visbias_a and data
        are None, visible biases will be set to the same values of the
        temperature 1 model. For best results, use the `data` parameter.
    data : numpy.ndarray, optional
        Training data used to initialize the visible biases of the
        base-rate model (usually infinite temperature), to the log-mean
        of the distribution (maximum likelihood solution assuming a
        zero-weight matrix). This ensures that the base-rate model is
        the "closest" to the model at temperature 1.
    betas : numpy.ndarray, optional
        Vector specifying inverse temperature of intermediate
        distributions (in increasing order).
        If None, defaults to AIS.dflt_betas
    key_betas : numpy.ndarray, optional
        If not None, AIS.run will save the log AIS weights for all
        temperatures in `key_betas`. This allows the user to estimate
        logZ at several temperatures in a single pass of AIS.
    rng : None or RandomStream, optional
        Random number generator object to use.
    seed : int, optional
        If rng is None, initialize rng with this seed.

    References
    ----------
    .. [1] Neal, R. M. (1998) "Annealed importance sampling",
       Technical Report No. 9805 (revised), Dept. of Statistics,
       University of Toronto, 25 pages
    .. [2] Ruslan Salakhutdinov, Iain Murray. "On the quantitative
           analysis of deep belief networks". Proceedings of the 25th
           International Conference on Machine Learning, p.872-879,
           July 5--9, 2008, Helsinki, Finland
    """
    (weights, visbias, hidbias) = rbm_params

    rng = make_np_rng(rng, seed, ['random_sample', 'rand'])

    if data is None:
        if visbias_a is None:
            # configure base-rate biases to those supplied by user
            visbias_a = visbias
        else:
            visbias_a = visbias_a
    else:
        # set biases of base-rate model to ML solution
        data = numpy.asarray(data, dtype=config.floatX)
        data = numpy.mean(data, axis=0)
        data = numpy.minimum(data, 1 - 1e-5)
        data = numpy.maximum(data, 1e-5)
        visbias_a = -numpy.log(1. / data - 1)
    hidbias_a = numpy.zeros_like(hidbias)
    weights_a = numpy.zeros_like(weights)
    # generate exact sample for the base model
    v0 = numpy.tile(1. / (1 + numpy.exp(-visbias_a)), (n_runs, 1))
    v0 = numpy.array(v0 > rng.random_sample(v0.shape), dtype=config.floatX)
    # we now compute the log AIS weights for the ratio log(Zb/Za)
    ais = rbm_z_ratio((weights_a, visbias_a, hidbias_a),
                      rbm_params, n_runs, v0,
                      betas=betas, key_betas=key_betas, rng=rng)
    dlogz, var_dlogz = ais.estimate_from_weights()
    # log Z = log_za + dlogz
    ais.log_za = weights_a.shape[1] * numpy.log(2) + \
                 numpy.sum(numpy.log(1 + numpy.exp(visbias_a)))
    ais.log_zb = ais.log_za + dlogz
    return (ais.log_zb, var_dlogz), ais


def rbm_z_ratio(rbmA_params, rbmB_params, n_runs, v0=None,
                betas=None, key_betas=None, rng=None, seed=23098):
    """
    Computes the AIS log-weights :math:`log\:w^{(i)}`, such that

    .. math::
        \\log Z_b = \\log Z_a + \\log \\frac{1}{M} \\sum_{i=1}^M
        \\exp(log_{ais} w^{(i)})

    Parameters
    ----------
    rbmA_params : list
        List of `numpy.ndarrays`, corresponding to parameters of RBM
        whose partition :math:`Z_a` is usually known (i.e. baserate
        model at beta=0).  Parameters are given in the order:
        [weights, visbias, hidbias]
    rbmB_params : list
        List of `numpy.ndarrays`, corresponding to parameters of RBM
        whose partition :math:`Z_a` is usually known (i.e. baserate
        model at beta=0).  Parameters are given in the order:
        [weights, visbias, hidbias]
    n_runs : int
        WRITEME
    v0 : WRITEME
    betas : WRITEME
    key_betas : WRITEME
    rng : WRITEME
    seed : int
        WRITEME

    Notes
    -----
    Additional parameters are as described in the docstring for
    `rbm_ais`.
    """
    # check that both models have the same number of hidden units
    assert rbmA_params[0].shape[0] == rbmB_params[0].shape[0]
    # check that both models have the same number of visible units
    assert len(rbmA_params[1]) == len(rbmB_params[1])

    rng = make_np_rng(rng, seed, 'rand')

    # make sure parameters are in floatX format for GPU support
    rbmA_params = [numpy.asarray(q, dtype=config.floatX) for q in rbmA_params]
    rbmB_params = [numpy.asarray(q, dtype=config.floatX) for q in rbmB_params]

    # declare symbolic vars for current sample `v_sample` and temp `beta`
    v_sample = tensor.matrix('ais_v_sample')
    beta = tensor.scalar('ais_beta')

    ### given current sample `v_sample`, generate new samples from inv.
    ### temperature `beta`
    new_v_sample = rbm_ais_gibbs_for_v(rbmA_params, rbmB_params,
                                       beta, v_sample)
    sample_fn = theano.function([beta, v_sample], new_v_sample)

    ### build theano function to compute the free-energy
    fe = rbm_ais_pk_free_energy(rbmA_params, rbmB_params, beta, v_sample)
    free_energy_fn = theano.function([beta, v_sample],
                                     fe, allow_input_downcast=False)

    ### RUN AIS ###
    weights_b = rbmB_params
    v0 = rng.rand(n_runs, weights_b.shape[0]) if v0 is None else v0
    ais = AIS(sample_fn, free_energy_fn, v0, n_runs)
    ais.set_betas(betas, key_betas=key_betas)
    ais.run()

    return ais


def rbm_ais_pk_free_energy(rbmA_params, rbmB_params, beta, v_sample):
    """
    Computes the free-energy of visible unit configuration `v_sample`,
    according to the interpolating distribution at temperature beta.
    The interpolating distributions are given by
    :math:`p_a(v)^{1-\\beta} p_b(v)^\\beta`.
    See equation 10, of Salakhutdinov & Murray 2008.

    Parameters
    ----------
    rbmA_params : list
        See `rbm_z_ratio`
    rbmB_params : list
        See `rbm_z_ratio`
    beta : int
        Inverse temperature at which to compute the free-energy.
    v_sample : tensor.matrix
        Matrix whose rows indexes into the minibatch, and columns into
        the data dimensions.

    Returns
    -------
    f : float (scalar)
       Free-energy of configuration `v_sample` given by the
       interpolating distribution at temperature beta.
    """

    def rbm_fe(rbm_params, v, b):
        (weights, visbias, hidbias) = rbm_params
        vis_term = b * tensor.dot(v, visbias)
        hid_act = b * (tensor.dot(v, weights) + hidbias)
        fe = -vis_term - tensor.sum(tensor.log(1 + tensor.exp(hid_act)),
                                    axis=1)
        return fe

    fe_a = rbm_fe(rbmA_params, v_sample, (1 - beta))
    fe_b = rbm_fe(rbmB_params, v_sample, beta)
    return fe_a + fe_b


def rbm_ais_gibbs_for_v(rbmA_params, rbmB_params, beta, v_sample, seed=23098):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    rbmA_params : list
        Parameters of the baserate model (usually infinite temperature).
        List should be of length 3 and contain numpy.ndarrays
        corresponding to model parameters (weights, visbias, hidbias).

    rbmB_params : list
        Similar to `rbmA_params`, but for model at temperature 1.

    beta : theano.shared
        Scalar, represents inverse temperature at which we wish to sample from.

    v_sample : theano.shared
        Matrix of shape (n_runs, nvis), state of current particles.

    seed : int, optional
        Optional seed parameter for sampling from binomial units.
    """

    (weights_a, visbias_a, hidbias_a) = rbmA_params
    (weights_b, visbias_b, hidbias_b) = rbmB_params

    theano_rng = make_theano_rng(seed, which_method='binomial')

    # equation 15 (Salakhutdinov & Murray 2008)
    ph_a = nnet.sigmoid((1 - beta) * (tensor.dot(v_sample, weights_a) +
                        hidbias_a))
    ha_sample = theano_rng.binomial(size=(v_sample.shape[0], len(hidbias_a)),
                                    n=1, p=ph_a, dtype=config.floatX)

    # equation 16 (Salakhutdinov & Murray 2008)
    ph_b = nnet.sigmoid(beta * (tensor.dot(v_sample, weights_b) + hidbias_b))
    hb_sample = theano_rng.binomial(size=(v_sample.shape[0], len(hidbias_b)),
                                    n=1, p=ph_b, dtype=config.floatX)

    # equation 17 (Salakhutdinov & Murray 2008)
    pv_act = (1 - beta) * (tensor.dot(ha_sample, weights_a.T) + visbias_a) + \
             beta * (tensor.dot(hb_sample, weights_b.T) + visbias_b)
    pv = nnet.sigmoid(pv_act)
    new_v_sample = theano_rng.binomial(
        size=(v_sample.shape[0], len(visbias_b)),
        n=1, p=pv, dtype=config.floatX
    )

    return new_v_sample


class AIS(object):
    """
    Compute the log AIS weights to approximate a ratio of partition functions.

    After being initialized, the user should call the `run()` method which will
    compute the log AIS weights. `estimate_from_weights` will then compute the
    mean & variance of :math:`log(Z_b/Z_a)` (Eq.11).

    The notation used here is slightly different than in Salakhutdinov & Murray
    2008. We write the AIS weights as follows (note that the denominator is
    always of the form :math:`p_i(x_i)` to indicate that
    :math:`x_i \sim p_i`). The free energy is denoted by :math:`\mathcal{F}`.

    .. math::
        w^{(i)} = p_1(v_0)*p_2(v_1)*...*p_k(v_{k-1}) /
                 [p_0(v_0)*p_1(v_1)*...*p_{k-1}(v_{k-1})]

        = p_1(v_0)/p_0(v_0) * p_2(v_1)/p_1(v_1) * ...
          * p_k(v_{k-1})/p_{k-1}(v_{k-1})

        log\:w^{(i)} = \mathcal{F}_0(v_0) - \mathcal{F}_1(v_0) +
                       \mathcal{F}_1(v_1) - \mathcal{F}_2(v_1) + ... +
                       \mathcal{F}_{k-1}(v_{k-1}) - \mathcal{F}_{k}(v_{k-1})

    Parameters
    ----------
    sample_fn : compiled theano function
        `sample_fn(beta, v_sample)` returns new model samples, at
        inverse temperature `beta`. Internally, we do this by
        performing block gibbs sampling using Eq.(15-17) (implemented
        in `rbm_ais_gibbs_for_v`) starting from configuration
        `v_sample`.
    free_energy_fn : theano function
        `free_energy_fn(beta,v_sample)` computes the free-energy of
        configuration `v_sample` at the interpolating distribution
        :math:`p_a^{1-\\beta} p_b^{\\beta}`.
    v_sample0 : numpy.ndarray
        Initial samples from model A.
    n_runs : int
        Number of AIS runs (i.e. minibatch size)
    log_int : int
        Log standard deviation of log ais weights every `log_int`
        temperatures.
    """


    # default configuration for interpolating distributions
    dflt_beta = numpy.hstack((numpy.asarray(numpy.linspace(0, 0.5, 1e3),
                                            dtype=config.floatX),
                              numpy.asarray(numpy.linspace(0.5, 0.9, 1e4),
                                            dtype=config.floatX),
                              numpy.asarray(numpy.linspace(0.9, 1.0, 1e4),
                                            dtype=config.floatX)))

    def __init__(self, sample_fn, free_energy_fn, v_sample0, n_runs,
                 log_int=500):
        self.sample_fn = sample_fn
        self.free_energy_fn = free_energy_fn
        self.v_sample0 = v_sample0
        self.n_runs = n_runs
        self.log_int = log_int

        # initialize log importance weights
        self.log_ais_w = numpy.zeros(n_runs, dtype=config.floatX)

        # utility function for safely computing log-mean of the ais weights
        ais_w = tensor.vector()
        dlogz = (
            tensor.log(tensor.mean(tensor.exp(ais_w - tensor.max(ais_w))))
            + tensor.max(ais_w)
        )
        self.log_mean = theano.function([ais_w], dlogz,
                                        allow_input_downcast=False)

    def set_betas(self, betas=None, key_betas=None):
        """
        Set the inverse temperature parameters of the AIS procedure.

        Parameters
        ----------
        betas : numpy.ndarray, optional
            Vector of temperatures specifying interpolating distributions

        key_betas : numpy.ndarray, optional
            If specified (not None), specifies specific temperatures at
            which we want to compute the AIS estimate. AIS.run will
            then return a vector, containing AIS at each key_beta
            temperature, including the nominal temperature.
        """
        self.key_betas = None if key_betas is None else numpy.sort(key_betas)

        betas = numpy.array(betas, dtype=config.floatX) \
            if betas is not None else self.dflt_beta
        # insert key temperatures within
        if key_betas is not None:
            betas = numpy.hstack((betas, key_betas))
            betas.sort()

        self.betas = betas

    def run(self, n_steps=1):
        """
        Performs the grunt-work, implementing

        .. math::

            log\:w^{(i)} += \mathcal{F}_{k-1}(v_{k-1}) - \mathcal{F}_{k}(v_{k-1})

        recursively for all temperatures.

        Parameters
        ----------
        n_steps : int, optional
            WRITEME
        """
        if not hasattr(self, 'betas'):
            self.set_betas()

        self.std_ais_w = []  # used to log std of log_ais_w regularly
        self.logz_beta = []  # used to log log_ais_w at every `key_beta` value
        self.var_logz_beta = []  # used to log variance of log_ais_w as above

        # initial sample
        state = self.v_sample0
        ki = 0

        # loop over all temperatures from beta=0 to beta=1
        for i in range(len(self.betas) - 1):
            bp, bp1 = self.betas[i], self.betas[i + 1]
            # log-ratio of (free) energies for two nearby temperatures
            self.log_ais_w += \
                self.free_energy_fn(bp, state) - \
                self.free_energy_fn(bp1, state)
            # log standard deviation of AIS weights (kind of deprecated)
            if (i + 1) % self.log_int == 0:
                m = numpy.max(self.log_ais_w)
                std_ais = (numpy.log(numpy.std(numpy.exp(self.log_ais_w - m)))
                           + m - numpy.log(self.n_runs) / 2)
                self.std_ais_w.append(std_ais)

            # whenever we reach a "key" beta value, log log_ais_w and
            # var(log_ais_w) so we can estimate log_Z_{beta=key_betas[i]} after
            # the fact.
            if self.key_betas is not None and \
               ki < len(self.key_betas) and \
               bp1 == self.key_betas[ki]:

                log_ais_w_bi, var_log_ais_w_bi = \
                    self.estimate_from_weights(self.log_ais_w)
                self.logz_beta.insert(0, log_ais_w_bi)
                self.var_logz_beta.insert(0, var_log_ais_w_bi)
                ki += 1

            # generate a new sample at temperature beta_{i+1}
            state = self.sample_fn(bp1, state)

    def estimate_from_weights(self, log_ais_w=None):
        """
        Once run() method has been called, estimates the mean and variance of
        log(Zb/Za).

        Parameters
        ----------
        log_ais_w : None or 1D numpy.ndarray
            optional override for log_ais_w. When None, estimates log(Zb/Za)
            using the log AIS weights computed by AIS.run() method.

        Returns
        -------
        f : float
            Estimated mean of log(Zb/Za), log-ratio of partition functions of
            model B and A.

        v : float
            Estimated variance of log(Zb/Za)
        """

        log_ais_w = self.log_ais_w if log_ais_w is None else log_ais_w

        # estimate the log-mean of the AIS weights
        dlogz = self.log_mean(log_ais_w)

        # estimate log-variance of the AIS weights
        # VAR(log(X)) \approx VAR(X) / E(X)^2 = E(X^2)/E(X)^2 - 1
        m = numpy.max(log_ais_w)
        var_dlogz = (log_ais_w.shape[0] *
                     numpy.sum(numpy.exp(2 * (log_ais_w - m))) /
                     numpy.sum(numpy.exp(log_ais_w - m)) ** 2 - 1.)

        return dlogz, var_dlogz

########NEW FILE########
__FILENAME__ = base_acts
"""
Base class for wrapping
"""
__authors__ = "David Warde-Farley"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["David Warde-Farley", "Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "David Warde-Farley"
__email__ = "wardefar@iro"

"""
This module may contain code copied directly or modified from cuda-convnet.
The copyright and licensing notice for this code is reproduced below:


/*
 * Copyright (c) 2011, Alex Krizhevsky (akrizhevsky@gmail.com)
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without modification,
 * are permitted provided that the following conditions are met:
 *
 * - Redistributions of source code must retain the above copyright notice,
 *   this list of conditions and the following disclaimer.
 *
 * - Redistributions in binary form must reproduce the above copyright notice,
 *   this list of conditions and the following disclaimer in the documentation
 *   and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
 * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
 * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
 * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
 * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
 * NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,
 * EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

"""

import warnings
from theano.sandbox.cuda import GpuOp
from pylearn2.sandbox.cuda_convnet.shared_code import this_dir
from pylearn2.sandbox.cuda_convnet.convnet_compile import convnet_available
from pylearn2.sandbox.cuda_convnet.convnet_compile import cuda_convnet_loc
from pylearn2.utils import py_integer_types

import pylearn2.sandbox.cuda_convnet.pthreads
from theano import config

class BaseActs(GpuOp):
    """
    Shared code for wrapping various convnet operations.
    """
    def __init__(self, pad=0, partial_sum=None, stride=1):

        if not isinstance(pad, py_integer_types):
            raise TypeError("pad must be an int")
        if not (pad >= 0):
            raise ValueError("bad value of pad (must be non-negative): " + str(pad))

        self.partial_sum = partial_sum
        self.pad = pad
        self.stride = stride
        self.copy_non_contiguous = 0
        # TODO: support sparse connectivity pattern
        self.dense_connectivity = True

    def c_header_dirs(self):
        return [this_dir, config.pthreads.inc_dir] if config.pthreads.inc_dir else [this_dir]

    def c_headers(self):
        return ['nvmatrix.cuh', 'cudaconv2.cuh']

    def c_code_cache_version(self):
        warnings.warn("No C-code cache version for %s" %
                      self.__class__.__name__)
        return ()

    def c_lib_dirs(self):
        return [cuda_convnet_loc, config.pthreads.lib_dir] if config.pthreads.lib_dir else [cuda_convnet_loc]

    def c_libraries(self):
        return ['cuda_convnet', config.pthreads.lib] if config.pthreads.lib else ['cuda_convnet']

    def _argument_contiguity_check(self, arg_name):
        return """
        if (!CudaNdarray_is_c_contiguous(%%(%(arg_name)s)s))
        {
            if (!(%(class_name_caps)s_COPY_NON_CONTIGUOUS)) {
                PyErr_SetString(PyExc_ValueError,
                    "%(class)s: %(arg_name)s must be C contiguous");
                %%(fail)s;
            }
        }
        """ % {
            'class': self.__class__.__name__,
            'arg_name': arg_name,
            'class_name_caps': self.__class__.__name__.upper(),
        }

    def _argument_dimension_check(self, arg_name, ndim):
        return """
        if (%%(%(arg_name)s)s->nd != %(ndim)d)
        {
            PyErr_Format(PyExc_ValueError,
                "%(arg_name)s must have ndim=%(ndim)d, got nd=%%%%i",
                %%(%(arg_name)s)s->nd);
            %%(fail)s;
        }
        """ % locals()

    def __eq__(self, other):
        return (type(self) == type(other) and
                self.partial_sum == other.partial_sum and
                self.pad == other.pad and
                self.dense_connectivity == other.dense_connectivity and
                self.stride == other.stride and
                self.copy_non_contiguous == other.copy_non_contiguous)

    def __hash__(self):
        msg = []
        msg.append(self.__class__.__name__)
        for val in (self.partial_sum, self.pad, self.dense_connectivity,
                    self.stride, self.copy_non_contiguous):
            msg.append(str(val))

        return hash(tuple(msg))

    # Make sure the cuda_convnet library is compiled and up-to-date
    def make_thunk(self, node, storage_map, compute_map, no_recycling):
        if not convnet_available():
            raise RuntimeError('Could not compile cuda_convnet')

        return super(BaseActs, self).make_thunk(node, storage_map,
                                                compute_map, no_recycling)


class UnimplementedError(Exception):
    """
    Like NotImplementedError, but designed not to be caught and suppressed
    by theano.
    """

########NEW FILE########
__FILENAME__ = bench
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

from pylearn2.testing.skip import skip_if_no_gpu
skip_if_no_gpu()
import numpy as np
from theano import shared
from pylearn2.sandbox.cuda_convnet.filter_acts import FilterActs
from theano.tensor.nnet.conv import conv2d
from theano import function
import time
import matplotlib.pyplot as plt


def make_funcs(batch_size, rows, cols, channels, filter_rows,
        num_filters):
    rng = np.random.RandomState([2012,10,9])

    filter_cols = filter_rows

    base_image_value = rng.uniform(-1., 1., (channels, rows, cols,
        batch_size)).astype('float32')
    base_filters_value = rng.uniform(-1., 1., (channels, filter_rows,
        filter_cols, num_filters)).astype('float32')
    images = shared(base_image_value)
    filters = shared(base_filters_value, name='filters')

    # bench.py should always be run in gpu mode so we should not need a gpu_from_host here
    output = FilterActs()(images, filters)

    output_shared = shared( output.eval() )

    cuda_convnet = function([], updates = { output_shared : output } )
    cuda_convnet.name = 'cuda_convnet'

    images_bc01v = base_image_value.transpose(3,0,1,2)
    filters_bc01v = base_filters_value.transpose(3,0,1,2)
    filters_bc01v = filters_bc01v[:,:,::-1,::-1]

    images_bc01 = shared(images_bc01v)
    filters_bc01 = shared(filters_bc01v)

    output_conv2d = conv2d(images_bc01, filters_bc01,
            border_mode='valid', image_shape = images_bc01v.shape,
            filter_shape = filters_bc01v.shape)

    output_conv2d_shared = shared(output_conv2d.eval())

    baseline = function([], updates = { output_conv2d_shared : output_conv2d } )
    baseline.name = 'baseline'

    return cuda_convnet, baseline

def bench(f):
    for i in xrange(3):
        f()
    trials = 10
    t1 = time.time()
    for i in xrange(trials):
        f()
    t2 = time.time()
    return (t2-t1)/float(trials)

def get_speedup( *args, **kwargs):
    cuda_convnet, baseline = make_funcs(*args, **kwargs)
    return bench(baseline) / bench(cuda_convnet)

def get_time_per_10k_ex( *args, **kwargs):
    cuda_convnet, baseline = make_funcs(*args, **kwargs)
    batch_size = kwargs['batch_size']
    return 10000 * bench(cuda_convnet) / float(batch_size)

def make_batch_size_plot(yfunc, yname, batch_sizes, rows, cols, channels, filter_rows, num_filters):
    speedups = []
    for batch_size in batch_sizes:
        speedup = yfunc(batch_size = batch_size,
                rows = rows,
                cols = cols,
                channels = channels,
                filter_rows = filter_rows,
                num_filters = num_filters)
        speedups.append(speedup)
    plt.plot(batch_sizes, speedups)
    plt.title("cuda-convnet benchmark")
    plt.xlabel("Batch size")
    plt.ylabel(yname)
    plt.show()

make_batch_size_plot(get_speedup, "Speedup factor", batch_sizes = [1,2,5,25,32,50,63,64,65,96,100,127,128,129,159,160,161,191,192,193,200,255,256,257],
        rows = 32,
        cols = 32,
        channels = 64,
        filter_rows = 7,
        num_filters = 64)

"""
make_batch_size_plot(get_time_per_10k_ex, "Time per 10k examples", batch_sizes = [1,2,5,25,32,50,63,64,65,96,100,127,128,129,159,160,161,191,192,193,200,255,256,257],
        rows = 32,
        cols = 32,
        channels = 3,
        filter_rows = 5,
        num_filters = 64)
"""

########NEW FILE########
__FILENAME__ = code_templates

__authors__ = "David Warde-Farley"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["David Warde-Farley"]
__license__ = "3-clause BSD"
__maintainer__ = "David Warde-Farley"
__email__ = "wardefar@iro"


def output_same_shape(out_arg_name, in_arg_name):
    return """
    const int *%(out_arg_name)s_dims = %(in_arg_name)s_dims;

    if (CudaNdarray_prep_output(& %%(%(out_arg_name)s)s, 4,
                                %(out_arg_name)s_dims))
    {
        %%(fail)s;
    }

    { // setup_nv_%(out_arg_name)s brace #1
    NVMatrix nv_%(out_arg_name)s(%%(%(out_arg_name)s)s,
     %(out_arg_name)s_dims[0] * %(out_arg_name)s_dims[1] *
     %(out_arg_name)s_dims[2],
     %(out_arg_name)s_dims[3], "%%(class_name)s:nv_%(out_arg_name)s");
    """ % locals()


def nv_matrix_create(arg_name):
    return """
    {
    NVMatrix nv_%(arg_name)s(%%(%(arg_name)s)s,
                  %(arg_name)s_dims[0] * %(arg_name)s_dims[1] *
                  %(arg_name)s_dims[2], %(arg_name)s_dims[3],
                  "%%(class_name)s:nv_%(arg_name)s");
    """ % locals()


def ensure_same_shape(new_arg, old_arg):
    return """
    if (%(new_arg)s_dims[0] != %(old_arg)s_dims[0] ||
        %(new_arg)s_dims[1] != %(old_arg)s_dims[1] ||
        %(new_arg)s_dims[2] != %(old_arg)s_dims[2] ||
        %(new_arg)s_dims[3] != %(old_arg)s_dims[3]) {
        PyErr_SetString(PyExc_ValueError, "%%(class_name)s: %(new_arg)s "
                                          "must have same shape as "
                                          "%(old_arg)s");
        %%(fail)s;
    }
    """ % locals()


def contiguity_check(arg_name):
    return """
    if (!CudaNdarray_is_c_contiguous(%%(%(arg_name)s)s))
    {
        PyErr_SetString(PyExc_ValueError,
            "%%(class_name)s: %(arg_name)s must be C contiguous");
        %%(fail)s;
    }
    """ % locals()


def dimension_check(arg_name, ndim):
    return """
    if (%%(%(arg_name)s)s->nd != %(ndim)d)
    {
        PyErr_Format(PyExc_ValueError,
            "%(arg_name)s must have ndim=%(ndim)d, got nd=%%%%i",
            %%(%(arg_name)s)s->nd);
        %%(fail)s;
    }
    """ % locals()


########NEW FILE########
__FILENAME__ = convnet_compile
import errno
import logging
import os
import shutil
import stat
import sys

from theano import config
from theano.gof.cmodule import get_lib_extension
from theano.gof.compilelock import get_lock, release_lock
from theano.sandbox import cuda
from theano.sandbox.cuda import nvcc_compiler

from shared_code import this_dir

import pylearn2.sandbox.cuda_convnet.pthreads

_logger_name = 'pylearn2.sandbox.cuda_convnet.convnet_compile'
_logger = logging.getLogger(_logger_name)
#_logger.addHandler(logging.StreamHandler())
#_logger.setLevel(logging.DEBUG)

_logger.debug('importing')


cuda_convnet_loc = os.path.join(config.compiledir, 'cuda_convnet')
# In partial dependency order: the last ones depend on the first ones
cuda_convnet_file_sources = ('nvmatrix_kernels.cu', 'nvmatrix.cu',
                             'conv_util.cu', 'filter_acts.cu', 'img_acts.cu',
                             'weight_acts.cu')
cuda_convnet_so = os.path.join(cuda_convnet_loc,
        'cuda_convnet.' + get_lib_extension())
libcuda_convnet_so = os.path.join(cuda_convnet_loc,
        'libcuda_convnet.' + get_lib_extension())


def convnet_available():
    # If already compiled, OK
    if convnet_available.compiled:
        _logger.debug('already compiled')
        return True

    # If there was an error, do not try again
    if convnet_available.compile_error:
        _logger.debug('error last time')
        return False

    # Else, we need CUDA
    if not cuda.cuda_available:
        convnet_available.compile_error = True
        _logger.debug('cuda unavailable')
        return False

    # Try to actually compile
    success = convnet_compile()
    if success:
        convnet_available.compiled = True
    else:
        convnet_available.compile_error = False
    _logger.debug('compilation success: %s', success)

    return convnet_available.compiled

# Initialize variables in convnet_available
convnet_available.compiled = False
convnet_available.compile_error = False


def should_recompile():
    """
    Returns True if the .so files are not present or outdated.
    """
    # The following list is in alphabetical order.
    source_files = (
            'conv_util.cu',
            'conv_util.cuh',
            'cudaconv2.cuh',
            'filter_acts.cu',
            'img_acts.cu',
            'nvmatrix.cu',
            'nvmatrix.cuh',
            'nvmatrix_kernels.cu',
            'nvmatrix_kernels.cuh',
            'nvmatrix_operators.cuh',
            'weight_acts.cu')
    stat_times = [
            os.stat(os.path.join(this_dir, source_file))[stat.ST_MTIME]
            for source_file in source_files]
    date = max(stat_times)
    _logger.debug('max date: %f', date)

    if (not os.path.exists(cuda_convnet_so) or
            date >= os.stat(cuda_convnet_so)[stat.ST_MTIME]):
        return True

    return False


def symlink_ok():
    """
    Check if an existing library exists and can be read.
    """
    try:
        open(libcuda_convnet_so).close()
        return True
    except IOError:
        return False


def convnet_compile():
    # Compile .cu files in cuda_convnet
    _logger.debug('nvcc_compiler.rpath_defaults: %s',
            str(nvcc_compiler.rpath_defaults))
    import time
    t1 = time.time()
    if should_recompile():
        _logger.debug('should recompile')

        # Concatenate all .cu files into one big mod.cu
        code = []
        for source_file in cuda_convnet_file_sources:
            code.append(open(os.path.join(this_dir, source_file)).read())
        code = '\n'.join(code)

        get_lock()
        try:
            # Check if the compilation has already been done by another process
            # while we were waiting for the lock
            if should_recompile():
                _logger.debug('recompiling')

                try:
                    compiler = nvcc_compiler.NVCC_compiler()
                    args = compiler.compile_args()

                    # compiler.compile_args() can execute a
                    # compilation This currently will remove empty
                    # directory in the compile dir.  So we must make
                    # destination directory after calling it.
                    if not os.path.exists(cuda_convnet_loc):
                        os.makedirs(cuda_convnet_loc)
                    compiler.compile_str('cuda_convnet',
                            code,
                            location = cuda_convnet_loc,
                            include_dirs = [this_dir, config.pthreads.inc_dir] if config.pthreads.inc_dir else [this_dir],
                            lib_dirs = nvcc_compiler.rpath_defaults + [cuda_convnet_loc] + ([config.pthreads.lib_dir] if config.pthreads.lib_dir else []),
                            libs = ['cublas', config.pthreads.lib] if config.pthreads.lib else ['cublas'],
                            preargs = ['-O3'] + args,
                            py_module=False)
                except Exception, e:
                    _logger.error("Failed to compile %s %s: %s",
                                  os.path.join(cuda_convnet_loc, 'mod.cu'), cuda_convnet_file_sources, str(e))
                    return False
            else:
                _logger.debug('already compiled by another process')

        finally:
            release_lock()
    else:
        _logger.debug('not recompiling')

    # If necessary, create a symlink called libcuda_convnet.so
    if not symlink_ok():
        if sys.platform == "win32":
            # The Python `os` module does not support symlinks on win32.
            shutil.copyfile(cuda_convnet_so, libcuda_convnet_so)
        else:
            try:
                os.symlink(cuda_convnet_so, libcuda_convnet_so)
            except OSError, e:
                # This may happen for instance when running multiple
                # concurrent jobs, if two of them try to create the
                # symlink simultaneously.
                # If that happens, we verify that the existing symlink is
                # indeed working.
                if (getattr(e, 'errno', None) != errno.EEXIST
                        or not symlink_ok()):
                    raise

    # Raise an error if libcuda_convnet_so is still not available
    open(libcuda_convnet_so).close()

    # Add cuda_convnet to the list of places that are hard-coded into
    # compiled modules' runtime library search list.
    nvcc_compiler.add_standard_rpath(cuda_convnet_loc)

    t2 = time.time()
    _logger.debug('successfully imported. Compiled in %fs', t2 - t1)

    return True

########NEW FILE########
__FILENAME__ = debug
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

from pylearn2.testing.skip import skip_if_no_gpu
skip_if_no_gpu()
import logging
import numpy as np
from theano import shared
from pylearn2.sandbox.cuda_convnet.filter_acts import FilterActs
from theano.sandbox.cuda import gpu_from_host
from theano.sandbox.cuda import host_from_gpu
from theano.tensor.nnet.conv import conv2d
from theano import function


logger = logging.getLogger(__name__)

# Tests that running FilterActs with no padding is the same as running
# theano's conv2D in valid mode

rng = np.random.RandomState([2012,10,9])

batch_size = 128
rows = 32
cols = 32
channels = 3
filter_rows = 7
filter_cols = filter_rows
num_filters = 16

images = shared(rng.uniform(-1., 1., (channels, rows, cols,
    batch_size)).astype('float32'), name='images')
filters = shared(rng.uniform(-1., 1., (channels, filter_rows,
    filter_cols, num_filters)).astype('float32'), name='filters')

gpu_images = gpu_from_host(images)
gpu_filters = gpu_from_host(filters)

output = FilterActs()(gpu_images, gpu_filters)
output = host_from_gpu(output)

images_bc01 = images.dimshuffle(3,0,1,2)
filters_bc01 = filters.dimshuffle(3,0,1,2)
filters_bc01 = filters_bc01[:,:,::-1,::-1]

output_conv2d = conv2d(images_bc01, filters_bc01,
        border_mode='valid')

output_conv2d = output_conv2d.dimshuffle(1,2,3,0)

f = function([], [output, output_conv2d])

def err():
    output, output_conv2d = f()
    diff = output - output_conv2d

    return np.abs(diff).max()


prev_err = err()
accepted_steps = 0

while True:
    logger.debug('Current error: {0}'.format(prev_err))
    change_filters =  rng.randint(2)

    if change_filters:
        target = filters
    else:
        target = images

    old_val = target.get_value()

    selector = rng.randint(2)
    if selector == 0:
        new_val = old_val + rng.uniform(-.1, .1, old_val.shape)
    else:
        idx1 = rng.randint(old_val.shape[0])
        idx2 = rng.randint(old_val.shape[1])
        idx3 = rng.randint(old_val.shape[2])
        idx4 = rng.randint(old_val.shape[3])
        new_val = old_val.copy()
        new_val[idx1, idx2, idx3, idx4] += rng.uniform(-1., 1.)
    new_val = new_val.astype(old_val.dtype)

    target.set_value(new_val)

    new_err = err()

    if new_err <= prev_err:
        logger.debug('Failed to move beyond step {0}'.format(accepted_steps))
        target.set_value(old_val)
    else:
        prev_err = new_err
        accepted_steps += 1

########NEW FILE########
__FILENAME__ = filter_acts
"""
A theano / pylearn2 wrapper for cuda-convnet's convFilterActs function.
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow", "David Warde-Farley"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

"""
This module may contain code copied directly or modified from cuda-convnet.
The copyright and licensing notice for this code is reproduced below:


/*
 * Copyright (c) 2011, Alex Krizhevsky (akrizhevsky@gmail.com)
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without modification,
 * are permitted provided that the following conditions are met:
 *
 * - Redistributions of source code must retain the above copyright notice,
 *   this list of conditions and the following disclaimer.
 *
 * - Redistributions in binary form must reproduce the above copyright notice,
 *   this list of conditions and the following disclaimer in the documentation
 *   and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
 * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
 * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
 * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
 * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
 * NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,
 * EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

"""
from theano.sandbox.cuda import CudaNdarrayType
from theano.gof import Apply
from pylearn2.sandbox.cuda_convnet.base_acts import BaseActs
from pylearn2.sandbox.cuda_convnet.base_acts import UnimplementedError
#from pylearn2.sandbox.cuda_convnet.weight_acts import WeightActs
from pylearn2.sandbox.cuda_convnet.img_acts import ImageActs
from pylearn2.sandbox.cuda_convnet.weight_acts import WeightActs
from pylearn2.utils import py_integer_types
from theano.sandbox.cuda.basic_ops import gpu_contiguous


class FilterActs(BaseActs):
    """
    2D convolution implemented on GPU.
    Technically not a true convolution, as it does not flip the kernel.

    This is intended to be a very low-level, performance-oriented op.

    It will not try to fix the input for you. That would slow it down.
    The input must be in the right format. If not, it raises an exception.

    Currently, this op must be inserted manually, not by optimizations.

    * images: (input channels, rows, cols, batch_size). Channels must
      be <=3, or be even. Note: if you want to take the gradient with
      respect to the weights, channels must be divisible by 4. Must be
      C contiguous. You can enforce this by calling
      `theano.sandbox.cuda.basic_ops.gpu_contiguous` on it.
    * filters: (input channels, filter rows, filter cols, output channels).
      Rows must be the same as cols output channels must be a multiple
      of 16. Must be C contiguous. You can enforce this by calling
      `theano.sandbox.cuda.basic_ops.gpu_contiguous` on it.
    * output: (output channels, output rows, output cols, batch size)

    Notes
    -----
    All of these convolution routines are optimized for the case when
    the number of images (i.e. the minibatch size) is a multiple of 128.
    Other batch sizes will work, but Alex made no attempt whatsoever to
    make them work fast.
    """

    # __eq__ and __hash__ are defined in BaseActs.
    # If you add an __init__ method that adds new members to FilterActs,
    # you may need to implement a new version of __eq__ and __hash__
    # in FilterActs, that considers these parameters.

    def make_node(self, images, filters):
        """
        .. todo::

            WRITEME
        """
        if not isinstance(images.type, CudaNdarrayType):
            raise TypeError("FilterActs: expected images.type to be CudaNdarrayType, "
                    "got "+str(images.type))

        if not isinstance(filters.type, CudaNdarrayType):
            raise TypeError("FilterActs: expected filters.type to be CudaNdarrayType, "
                    "got "+str(filters.type))

        assert images.ndim == 4
        assert filters.ndim == 4

        channels_broadcastable = filters.type.broadcastable[3]
        batch_broadcastable = images.type.broadcastable[3]
        # Computing whether the rows and columns are broadcastable requires doing
        # arithmetic on quantities that are known only at runtime, like the specific
        # shape of the image and kernel
        rows_broadcastable = False
        cols_broadcastable = False

        targets_broadcastable = (channels_broadcastable, rows_broadcastable,
                cols_broadcastable, batch_broadcastable)
        targets_type = CudaNdarrayType(broadcastable=targets_broadcastable)
        targets = targets_type()

        return Apply(self, [images, filters], [targets])

    def flops(self, inputs, outputs):
        """ Useful with the hack in profilemode to print the MFlops"""
        images, kerns = inputs
        out, = outputs
        assert images[0] == kerns[0]
        # nb mul and add by output pixed
        flops = kerns[1] * kerns[2] * 2
        #nb flops by output image
        flops *= out[1] * out[2]
        # for all outputs images#n_stack==self.imshp[0]
        flops *= images[0] * kerns[3] * images[3]
        return flops

    def c_code(self, node, name, inputs, outputs, sub):
        """
        .. todo::

            WRITEME
        """
        images, filters = inputs
        targets, = outputs
        fail = sub['fail']

        # convFilterActs will multiply targets by scaleTargets
        # then add scaleOutput * (the convolution value)
        # We could make use of this to implement an inplace
        # addconv op but for this op we just want to compute
        # the convolution so we set them to 0 and 1 respectively
        # Note: there is another version of convFilterActs that
        # does not take these arguments, but it is just a wrapper
        # around the version that does take them, so we save
        # a function call by using the version that we use.
        basic_setup = """
        #define scaleTargets 0
        #define scaleOutput 1
        """

        if self.dense_connectivity:
            basic_setup += """
            #define numGroups 1
            """

        assert isinstance(self.pad, py_integer_types)
        assert self.pad >= 0, "pad must be non-negative"
        basic_setup += """
        #define paddingStart (-%d)
        """ % self.pad

        basic_setup += """
        #define moduleStride %d
        """ % int(self.stride)
        if self.copy_non_contiguous:
            raise UnimplementedError()
        else:
            basic_setup += "#define FILTERACTS_COPY_NON_CONTIGUOUS 0\n"


        # The amount of braces that must be closed at the end
        num_braces = 0

        # Convert images int nv_images, an NVMatrix, for compatibility
        # with the cuda-convnet functions
        setup_nv_images = self._argument_contiguity_check("images") + """
        if (%(images)s->nd != 4)
        {
            PyErr_Format(PyExc_ValueError,
                "images must have nd=4, got nd=%%i", %(images)s->nd);
            %(fail)s;
        }

        { //setup_nv_images brace 1
        const int * images_dims = CudaNdarray_HOST_DIMS(%(images)s);
        const int img_channels = images_dims[0];
        const int imgSizeY = images_dims[1];
        const int imgSizeX = images_dims[2];
        const int batch_size = images_dims[3];
        NVMatrix nv_images(%(images)s, img_channels * imgSizeY * imgSizeX, batch_size,
        "filter_acts:nv_images");
        """
        num_braces += 1

        # Convert filters into nv_filters, an NVMatrix, for compatibility
        # with the cuda-convnet functions
        setup_nv_filters = self._argument_contiguity_check("filters") + """
        if (%(filters)s->nd != 4)
        {
            PyErr_Format(PyExc_ValueError,
            "filters must have nd=4, got nd=%%i", %(filters)s->nd);
            %(fail)s;
        }

        { // setup_nv_filters brace 1
        const int * filters_dims = CudaNdarray_HOST_DIMS(%(filters)s);
        const int filter_channels = filters_dims[0];
        const int filter_rows = filters_dims[1];
        const int filter_cols = filters_dims[2];
        const int num_filters = filters_dims[3];

        if (numGroups * filter_channels != img_channels)
        {
            PyErr_Format(PyExc_ValueError,
            "# input channels mismatch. images have %%d but filters have %%d groups of %%d for a total of %%d.",
            img_channels, numGroups, filter_channels, numGroups * filter_channels);
            %(fail)s;
        }

        if ((num_filters %% (numGroups * 16)) != 0)
        {
            PyErr_Format(PyExc_ValueError,
            "Each group must have a multiple of 16 channels, but num_filters %%%% (numGroups * 16) = %%d %%%% ( %%d * 16) = %%d.",
            num_filters, numGroups, num_filters %% (numGroups * 16));
            %(fail)s;
        }

        if (filter_rows != filter_cols)
        {
            PyErr_Format(PyExc_ValueError,
            "filter must be square, but instead have shape (%%d, %%d)",
            filter_rows, filter_cols);
            %(fail)s;
        }
        else if (moduleStride > filter_rows) {
            PyErr_Format(PyExc_ValueError,
            "stride %%d greater than filter size (%%d, %%d)",
            moduleStride, filter_rows, filter_cols);
            %(fail)s;
        }

        { // setup_nv_filters brace 2


        NVMatrix nv_filters(%(filters)s, filter_channels * filter_rows *
        filter_cols, num_filters, "filter_acts:nv_filters");
        """
        num_braces += 2

        # p + (m_x - 1) * s + f >= i_x
        # p + (m_x - 1) * s >= i_x - f
        # m_x = (i_x - f - p) / s + 1
        div_ms_y = "((imgSizeY - 2*paddingStart - filter_rows) / moduleStride)"
        div_ms_x = "((imgSizeX - 2*paddingStart - filter_cols) / moduleStride)"
        mod_ms_y = "((imgSizeY - 2*paddingStart - filter_rows) % moduleStride)"
        mod_ms_x = "((imgSizeX - 2*paddingStart - filter_cols) % moduleStride)"
        target_rows = "%s + ((%s > 0) ? 1 : 0) + 1" % (div_ms_y, mod_ms_y)
        target_cols = "%s + ((%s > 0) ? 1 : 0) + 1" % (div_ms_x, mod_ms_x)

        setup_nv_targets = """


        int target_dims [] = {
            num_filters,
            %(target_rows)s,
            %(target_cols)s,
            batch_size };

        #define numModulesY target_dims[1]
        #define numModulesX target_dims[2]

        if (CudaNdarray_prep_output(& %(targets)s, 4, target_dims))
        {
            %(fail)s;
        }

        { // setup_nv_filters brace # 1

        NVMatrix nv_targets(%(targets)s, target_dims[0] * target_dims[1]
         * target_dims[2], target_dims[3], "filter_acts:nv_targets");

        """

        num_braces += 1

        # note: imgSizeX is not specified here, it is computed internally
        # (in _filterActsSparse) by the lines:
        # int imgPixels = images.getNumRows() / numImgColors;
        # int imgSizeX = imgPixels / imgSizeY;
        #
        # note: numFilters is not specified here. it is determined by
        # nv_filters.getNumCols()
        #
        # note: the size of the filters is determined by dividing
        # nv_filters.getNumRows() by numFilterColors
        #
        do_convolution = """
        convFilterActs(nv_images, nv_filters, nv_targets,
                       imgSizeY, numModulesY, numModulesX,
                       paddingStart, moduleStride, img_channels,
                       numGroups, scaleTargets, scaleOutput);
        """

        braces = '}' * num_braces

        rval = basic_setup + \
                setup_nv_images + \
                setup_nv_filters + \
                setup_nv_targets + \
                do_convolution + \
                braces

        rval = rval % locals()

        return rval

    def c_code_cache_version(self):
        """
        .. todo::

            WRITEME
        """
        return (10,)

    def R_op(self, inputs, evals):
        """
        .. todo::

            WRITEME
        """
        images, filters = inputs
        images_ev, filters_ev = evals
        if 'Cuda' not in str(type(images)):
            raise TypeError("inputs must be cuda")
        if 'Cuda' not in str(type(filters)):
            raise TypeError("filters must be cuda")

        if filters_ev is not None:
            sol = self(images, filters_ev)
        else:
            sol = None
        if images_ev is not None:
            if sol is not None:
                sol += self(images_ev, filters)
            else:
                sol = self(images_ev, filters)
        return [sol]

    def grad(self, inputs, dout):
        """
        .. todo::

            WRITEME
        """
        images, filters = inputs

        if 'Cuda' not in str(type(images)):
            raise TypeError("inputs must be cuda")
        if 'Cuda' not in str(type(filters)):
            raise TypeError("filters must be cuda")

        dout, = dout
        dout = gpu_contiguous(dout)

        if 'Cuda' not in str(type(dout)):
            raise TypeError("output gradients must be cuda")

        ishape = images.shape[1:3]
        fshape = filters.shape[1:3]
        d_images = ImageActs(self.pad, self.partial_sum, self.stride)(
            dout, filters, ishape)
        d_filters = WeightActs(self.pad, self.partial_sum, self.stride)(
            images, dout, fshape)[0]
        return d_images, d_filters

########NEW FILE########
__FILENAME__ = img_acts
"""
A theano / pylearn2 wrapper for cuda-convnet's convFilterActs function.
"""
__authors__ = "David Warde-Farley and Ian Goodfellow"
__copyright__ = "Copyright 2010-2013, Universite de Montreal"
__credits__ = ["David Warde-Farley and Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

"""
This module may contain code copied directly or modified from cuda-convnet.
The copyright and licensing notice for this code is reproduced below:


/*
 * Copyright (c) 2011, Alex Krizhevsky (akrizhevsky@gmail.com)
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without modification,
 * are permitted provided that the following conditions are met:
 *
 * - Redistributions of source code must retain the above copyright notice,
 *   this list of conditions and the following disclaimer.
 *
 * - Redistributions in binary form must reproduce the above copyright notice,
 *   this list of conditions and the following disclaimer in the documentation
 *   and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
 * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
 * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
 * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
 * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
 * NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,
 * EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

"""

from theano.gradient import DisconnectedType
from theano.gof import Apply
from theano.sandbox.cuda import CudaNdarrayType
from theano.sandbox.cuda.basic_ops import as_cuda_ndarray_variable

from pylearn2.sandbox.cuda_convnet.base_acts import BaseActs
from pylearn2.sandbox.cuda_convnet.base_acts import UnimplementedError

# Must delay import to avoid circular import problem
FilterActs = None
WeightActs = None


class ImageActs(BaseActs):
    """
    Transpose of FilterActs.

    This is intended to be a very low-level, performance-oriented op.

    It will not try to fix the input for you. That would slow it down.
    The input must be in the right format. If not, it raises an exception.

    Currently, this op must be inserted manually, not by optimizations.

    Note that below the term "input" refers to the input to FilterActs.
    This op does the tranpose of that, so its output is sized like
    FilterActs' input.

    * hid_acts: (output channels, rows, cols, batch_size)
    * filters: (input channels, filter rows, filter cols, output channels).
      Rows must be the same as cols. Output channels must be a multiple
      of 16.
    * output: (input channels, input rows, input cols, batch size)

    Notes
    -----
    All of these convolution routines are optimized for the case when
    the number of images (i.e. the minibatch size) is a multiple of 128.
    Other batch sizes will work, but Alex "made no attempt whatsoever
    to make them work fast."
    """

    # __eq__ and __hash__ are defined in BaseActs.
    # If you add an __init__ method that adds new members to ImageActs,
    # you may need to implement a new version of __eq__ and __hash__
    # in ImageActs, that considers these parameters.

    def make_node(self, hid_acts, filters, output_shape=None):
        """
        .. todo::

            WRITEME

        Parameters
        ----------
        hid_acts : WRITEME
        filters : WRITEME
        output_shape : 2-element TensorVariable, optional
            The spatial shape of the image
        """

        if not isinstance(hid_acts.type, CudaNdarrayType):
            raise TypeError("ImageActs: expected hid_acts.type to be CudaNdarrayType, "
                    "got " + str(hid_acts.type))

        if not isinstance(filters.type, CudaNdarrayType):
            raise TypeError("ImageActs: expected filters.type to be CudaNdarrayType, "
                    "got " + str(filters.type))


        if output_shape is None:
            if self.stride != 1:
                raise ValueError("You must specify an output_shape for ImageActs if the stride is not 1.")
            hid_shape = hid_acts.shape[1:3]
            kernel_shape = filters.shape[1:3]
            output_shape = hid_shape + kernel_shape - 2 * self.pad - 1

        assert hid_acts.ndim == 4
        assert filters.ndim == 4

        channels_broadcastable = filters.type.broadcastable[3]
        batch_broadcastable = hid_acts.type.broadcastable[3]
        # Computing whether the rows and columns are broadcastable requires doing
        # arithmetic on quantities that are known only at runtime, like the specific
        # shape of the image and kernel
        rows_broadcastable = False
        cols_broadcastable = False

        targets_broadcastable = (channels_broadcastable, rows_broadcastable,
                cols_broadcastable, batch_broadcastable)
        targets_type = CudaNdarrayType(broadcastable=targets_broadcastable)
        targets = targets_type()

        return Apply(self, [hid_acts, filters, output_shape], [targets])

    def flops(self, inputs, outputs):
        """ Useful with the hack in profilemode to print the MFlops"""
        hid_acts, filters, output_shape = inputs
        out, = outputs
        assert hid_acts[0] == filters[3]
        flops = (hid_acts[3] * filters[0] * hid_acts[0] *
                 filters[1] * filters[2] *
                 hid_acts[1] * hid_acts[2] * 2)
        return flops

    def connection_pattern(self, node):
        """
        .. todo::

            WRITEME
        """
        return [[1], [1], [0]]

    def grad(self, inputs, g_outputs):
        """
        .. todo::

            WRITEME
        """
        hid_acts, filters, output_shape = inputs
        g_images, = g_outputs
        g_images = as_cuda_ndarray_variable(g_images)
        assert not isinstance(g_images, list)

        global FilterActs
        global WeightActs
        if FilterActs is None:
            from pylearn2.sandbox.cuda_convnet.filter_acts import FilterActs
            from pylearn2.sandbox.cuda_convnet.weight_acts import WeightActs

        g_filters = WeightActs(stride=self.stride,
                partial_sum=self.partial_sum, pad=self.pad)(
                        g_images, hid_acts, filters.shape[1:3])[0]
        assert not isinstance(g_filters, list)
        g_hid_acts = FilterActs(stride=self.stride, pad=self.pad,
                partial_sum=self.partial_sum)(g_images, filters)

        return [g_hid_acts, g_filters, DisconnectedType()()]

    def c_code(self, node, name, inputs, outputs, sub):
        """
        .. todo::

            WRITEME
        """
        hid_acts, filters, output_shape = inputs
        targets, = outputs
        fail = sub['fail']

        # convFilterActs will multiply targets by scaleTargets
        # then add scaleOutput * (the convolution value)
        # We could make use of this to implement an inplace
        # addconv op but for this op we just want to compute
        # the convolution so we set them to 0 and 1 respectively
        # Note: there is another version of convFilterActs that
        # does not take these arguments, but it is just a wrapper
        # around the version that does take them, so we save
        # a function call by using the version that we use.
        basic_setup = """
        #define scaleTargets 0
        #define scaleOutput 1
        """

        if self.dense_connectivity:
            basic_setup += """
            #define numGroups 1
            """

        basic_setup += """
        #define paddingStart (-%d)
        """ % self.pad

        basic_setup += """
        #define moduleStride %d
        """ % self.stride

        if self.copy_non_contiguous:
            raise UnimplementedError()
        else:
            basic_setup += "#define IMAGEACTS_COPY_NON_CONTIGUOUS 0\n"

        # The amount of braces that must be closed at the end
        num_braces = 0

        # Convert images int nv_hid_acts, an NVMatrix, for compatibility
        # with the cuda-convnet functions
        setup_nv_hid_acts = self._argument_contiguity_check("hid_acts") + """
        if (%(hid_acts)s->nd != 4)
        {
            PyErr_Format(PyExc_ValueError,
                "hid_acts must have nd=4, got nd=%%i", %(hid_acts)s->nd);
            %(fail)s;
        }

        { //setup_nv_hid_acts brace 1
        const int *hid_act_dims = CudaNdarray_HOST_DIMS(%(hid_acts)s);
        const int numFilters = hid_act_dims[0];
        const int hidActsSizeY = hid_act_dims[1];
        const int hidActsSizeX = hid_act_dims[2];
        //printf("hidActs shape: %%d %%d\\n", hidActsSizeY, hidActsSizeX);
        const int batch_size = hid_act_dims[3];
        NVMatrix nv_hid_acts(%(hid_acts)s, numFilters * hidActsSizeY *
                                           hidActsSizeX, batch_size, "image_acts:nv_hid_acts");
        int img_channels = -1;
        """
        num_braces += 1

        # Convert filters into nv_filters, an NVMatrix, for compatibility
        # with the cuda-convnet functions

        setup_nv_filters = self._argument_contiguity_check("filters") + """
        if (%(filters)s->nd != 4)
        {
            PyErr_Format(PyExc_ValueError,
            "filters must have nd=4, got nd=%%i", %(filters)s->nd);
            %(fail)s;
        }

        { // setup_nv_filters brace 1
        const int * filters_dims = CudaNdarray_HOST_DIMS(%(filters)s);
        const int filter_channels = filters_dims[0];
        const int filter_rows = filters_dims[1];
        const int filter_cols = filters_dims[2];
        const int num_filters = filters_dims[3];

        if ((num_filters %% (numGroups * 16)) != 0)
        {
            PyErr_Format(PyExc_ValueError,
            "Each group must have a multiple of 16 channels, but num_filters %%%% (numGroups * 16) = %%d %%%% ( %%d * 16) = %%d.",
            num_filters, numGroups, num_filters %% (numGroups * 16));
            %(fail)s;
        }

        if (filter_rows != filter_cols)
        {
            PyErr_Format(PyExc_ValueError,
            "filter must be square, but have shape (%%d, %%d).",
            filter_rows, filter_cols);
            %(fail)s;
        }
        else if (moduleStride > filter_rows) {
            PyErr_Format(PyExc_ValueError,
            "stride %%d greater than filter size (%%d, %%d)",
            moduleStride, filter_rows, filter_cols);
            %(fail)s;
        }

        { // setup_nv_filters brace 2


        NVMatrix nv_filters(%(filters)s, filter_channels * filter_rows *
        filter_cols, num_filters, "img_acts:nv_filters");
        """
        num_braces += 2

        #target_rows = "(hidActsSizeY + filter_rows + 2 * paddingStart) * moduleStride - 1"
        #target_cols = "(hidActsSizeX + filter_cols + 2 * paddingStart) * moduleStride - 1"

        setup_nv_targets = """

        #define numModulesY hid_act_dims[1]
        #define numModulesX hid_act_dims[2]
        npy_intp *shape_dims = PyArray_DIMS(%(output_shape)s);
        npy_intp target_rows, target_cols;
        PyArrayObject *casted_shape;
        PyArray_Descr *intp_dtype;
        if (PyArray_NDIM(%(output_shape)s) != 1) {
            PyErr_Format(PyExc_ValueError,
                         "output shape must be a vector, got %%d-tensor",
                         PyArray_NDIM(%(output_shape)s));
            %(fail)s;
        }
        else if (shape_dims[0] != 2)
        {
            PyErr_Format(PyExc_ValueError,
                         "output shape must be length 2, got %%d",
                         (int)shape_dims[0]);
            %(fail)s;
        }
        else if ((PyArray_DESCR(%(output_shape)s))->kind != 'i' &&
                 (PyArray_DESCR(%(output_shape)s))->kind != 'u')
        {
            PyErr_SetString(PyExc_TypeError,
                            "output shape must have integer or uint dtype");
            %(fail)s;
        }
        intp_dtype = PyArray_DescrFromType(NPY_INTP);
        casted_shape = (PyArrayObject *)PyArray_CastToType(%(output_shape)s,
                                                           intp_dtype, 0);
        target_rows = *((npy_intp *)PyArray_GETPTR1(casted_shape, 0));
        target_cols = *((npy_intp *)PyArray_GETPTR1(casted_shape, 1));
        {
        int target_dims [] = {
            filter_channels,
            target_rows,
            target_cols,
            batch_size };
        #define filterSize filter_rows
        #define MAX_ROWS (paddingStart + (numModulesY-1) * moduleStride + filterSize)
        if ((target_rows > MAX_ROWS)
            || (paddingStart + (numModulesX-1) * moduleStride + filterSize < target_cols))
        {
            PyErr_Format(PyExc_ValueError, "pylearn2.sandbox.cuda_convnet.image_acts.ImageActs: incompatible target image size (%%d, %%d), maximum (%%d, %%d)",
                         (int)target_rows, (int)target_cols,
                         (int)MAX_ROWS,
                         (int)(paddingStart + (numModulesX-1) * moduleStride + filterSize));
            %(fail)s;
        }
        if (CudaNdarray_prep_output(& %(targets)s, 4, target_dims))
        {
            %(fail)s;
        }

        { // setup_nv_filters brace # 1
        const int imgSizeY = (int)target_rows;
        const int imgSizeX = (int)target_cols;

        NVMatrix nv_targets(%(targets)s, target_dims[0] * target_dims[1]
         * target_dims[2], target_dims[3], "image_acts: nv_targets");

        """

        num_braces += 2

        # note: numFilters is not specified here. it is determined by
        # nv_filters.getNumCols()
        #
        # note: the size of the filters is determined by dividing
        # nv_filters.getNumRows() by numFilterColors
        #
        do_convolution = """
        convImgActs(nv_hid_acts, nv_filters, nv_targets,
                    imgSizeY, imgSizeX, numModulesY,
                    paddingStart, moduleStride, filter_channels,
                    numGroups);
        """

        braces = '}' * num_braces

        rval = basic_setup + \
                setup_nv_hid_acts + \
                setup_nv_filters + \
                setup_nv_targets + \
                do_convolution + \
                braces

        rval = rval % locals()

        return rval

    def c_code_cache_version(self):
        """
        .. todo::

            WRITEME
        """
        return (9,)

########NEW FILE########
__FILENAME__ = pool
"""
.. todo::

    WRITEME
"""
import warnings

from theano.gof import Apply
from theano.sandbox.cuda import CudaNdarrayType
from theano.sandbox.cuda.basic_ops import as_cuda_ndarray_variable
from theano.sandbox.cuda.basic_ops import gpu_contiguous
from theano.sandbox.cuda import GpuOp
from theano.tensor import get_scalar_constant_value, NotScalarConstantError

from pylearn2.sandbox.cuda_convnet.base_acts import UnimplementedError
from pylearn2.sandbox.cuda_convnet.convnet_compile import convnet_available
from pylearn2.sandbox.cuda_convnet.convnet_compile import cuda_convnet_loc
from pylearn2.sandbox.cuda_convnet.shared_code import this_dir

import pylearn2.sandbox.cuda_convnet.pthreads
from theano import config

def max_pool_c01b(c01b, pool_shape, pool_stride, image_shape = None,  start=0):
    """
    .. todo::

        WRITEME
    """
    assert pool_shape[0] == pool_shape[1]
    assert pool_shape[0] > 0
    assert pool_stride[0] > 0
    assert pool_stride[0] <= pool_shape[0]
    if pool_stride[0] != pool_stride[1]:
        raise ValueError("pool strides must match, but got "+str(pool_stride))
    if image_shape is not None:
        warnings.warn("image_shape argument isn't needed anymore, quit passing it.")
    op = MaxPool(pool_shape[0], pool_stride[0], start)
    c01b = gpu_contiguous(c01b)
    return op(c01b)


class MaxPool(GpuOp):
    """
    This op wrap Alex's MaxPool code on the GPU.
    The input are in the order (channel, image rows, image cols, batch)

    Works only on square images and the grad works only when
    channel % 16 == 0.

    Parameters
    ----------
    ds : int
        Defines the size of the pooling region in the x (equivalently, y)
        dimension. Squares of size (ds)2 get reduced to one value by
        this layer. There are no restrictions on the value of this
        parameter. It's fine for a pooling square to fall off the
        boundary of the image. Named SizeX in Alex's code.
    stride : int
        Defines the stride size between successive pooling squares.
        Setting this parameter smaller than sizeX produces overlapping
        pools. Setting it equal to sizeX gives the usual, non-overlapping
        pools. Values greater than sizeX are not allowed.
    start : int, optional
        Tells the net where in the input image to start the pooling
        (in x,y coordinates). In principle, you can start anywhere you
        want. Setting this to a positive number will cause the net to
        discard some pixels at the top and at the left of the image.
        Setting this to a negative number will cause it to include
        pixels that don't exist (which is fine). start=0 is the usual
        setting.
    outputs : int, optional
        Allows you to control how many output values in the x
        (equivalently, y) dimension this operation will produce. This
        parameter is analogous to the start parameter, in that it
        allows you to discard some portion of the image by setting it
        to a value small enough to leave part of the image uncovered.
        Setting it to zero instructs the net to produce as many outputs
        as is necessary to ensure that the whole image is covered.
        default 0
    """

    def __init__(self, ds, stride, start=0, outputs=0):
        self.ds = ds
        self.stride = stride
        self.start = start
        self.copy_non_contiguous = 0
        assert stride > 0 and stride <= ds, (stride, ds)
        assert ds > 0, ds  # We check in the code if ds <= imgSizeX

    def __eq__(self, other):
        """
        .. todo::

            WRITEME
        """
        #Dont put copy_non_contigous as this doesn't change the output
        return (type(self) == type(other) and
                self.ds == other.ds and
                self.stride == other.stride and
                self.start == other.start)

    def __hash__(self):
        """
        .. todo::

            WRITEME
        """
        #Dont put copy_non_contigous as this doesn't change the output
        return (hash(type(self)) ^ hash(self.ds) ^
                hash(self.stride) ^ hash(self.start))

    def c_header_dirs(self):
        """
        .. todo::

            WRITEME
        """
        return [this_dir, config.pthreads.inc_dir] if config.pthreads.inc_dir else [this_dir]

    def c_headers(self):
        """
        .. todo::

            WRITEME
        """
        return ['nvmatrix.cuh', 'conv_util.cuh']

    def c_lib_dirs(self):
        """
        .. todo::

            WRITEME
        """
        return [cuda_convnet_loc, config.pthreads.lib_dir] if config.pthreads.lib_dir else [cuda_convnet_loc]

    def c_libraries(self):
        """
        .. todo::

            WRITEME
        """
        return ['cuda_convnet', config.pthreads.lib] if config.pthreads.lib else ['cuda_convnet']

    def c_code_cache_version(self):
        """
        .. todo::

            WRITEME
        """
        return (1,)

    def _argument_contiguity_check(self, arg_name):
        """
        .. todo::

            WRITEME
        """
        return """
        if (!CudaNdarray_is_c_contiguous(%%(%(arg_name)s)s))
        {
            if (!(%(class_name_caps)s_COPY_NON_CONTIGUOUS)) {
                PyErr_SetString(PyExc_ValueError,
                    "%(class)s: %(arg_name)s must be C contiguous");
                %%(fail)s;
            }
        }
        """ % {
            'class': self.__class__.__name__,
            'arg_name': arg_name,
            'class_name_caps': self.__class__.__name__.upper(),
        }

    def make_node(self, images):
        """
        .. todo::

            WRITEME
        """
        images = as_cuda_ndarray_variable(images)

        assert images.ndim == 4

        channels_broadcastable = images.type.broadcastable[0]
        batch_broadcastable = images.type.broadcastable[3]

        rows_broadcastable = False
        cols_broadcastable = False

        targets_broadcastable = (channels_broadcastable, rows_broadcastable,
                cols_broadcastable, batch_broadcastable)
        targets_type = CudaNdarrayType(broadcastable=targets_broadcastable)
        targets = targets_type()

        return Apply(self, [images], [targets])

    def c_code(self, node, name, inputs, outputs, sub):
        """
        .. todo::

            WRITEME
        """
        images, = inputs
        targets, = outputs
        fail = sub['fail']

        # The amount of braces that must be closed at the end
        num_braces = 0

        if self.copy_non_contiguous:
            raise UnimplementedError()
        else:
            basic_setup = "#define MAXPOOL_COPY_NON_CONTIGUOUS 0\n"

        # Convert images in nv_images, an NVMatrix, for compatibility
        # with the cuda-convnet functions
        setup_nv_images = self._argument_contiguity_check("images") + """
        if (%(images)s->nd != 4)
        {
            PyErr_Format(PyExc_ValueError,
                "images must have nd=4, got nd=%%i", %(images)s->nd);
            %(fail)s;
        }

        { //setup_nv_images brace 1

        const int * images_dims = CudaNdarray_HOST_DIMS(%(images)s);
        const int img_channels = images_dims[0];
        const int imgSizeY = images_dims[1];
        const int imgSizeX = images_dims[2];
        const int batch_size = images_dims[3];

        if(imgSizeY != imgSizeX){
            PyErr_Format(PyExc_ValueError,
                "images must be square(dims[1] == dims[2]). Shape (%%i,%%i,%%i,%%i)",
                img_channels, imgSizeY, imgSizeX, batch_size);
            %(fail)s;
        }
        if(%(ds)s > imgSizeY){
            PyErr_Format(PyExc_ValueError,
                "ds(%%d) must be <= imgSizeX(%%d) and imgSizeY(%%d).",
                %(ds)s, imgSizeX, imgSizeY);
            %(fail)s;
        }
        if(%(start)s >= imgSizeX){
            PyErr_Format(PyExc_ValueError,
                "start is %%d but must be smaller then the images size of %%d x %%d.",
                %(start)s, imgSizeX, imgSizeY);
            %(fail)s;
        }

        NVMatrix nv_images(%(images)s, img_channels * imgSizeY * imgSizeX, batch_size,
        "MaxPool:nv_images");
        """
        num_braces += 1

        setup_nv_targets = """
        //int _outputsX = int(ceil((dic['imgSize'] - dic['start'] - dic['sizeX']) / float(dic['stride']))) + 1;
        int _outputsX = ((int)(ceil((imgSizeY - %(start)s - %(ds)s) / ((float)%(stride)s)))) + 1;

        int target_dims [] = {
            img_channels,
            _outputsX,
            _outputsX,
            batch_size };

        if (CudaNdarray_prep_output(& %(targets)s, 4, target_dims))
        {
            %(fail)s;
        }

        { // setup_nv_target brace # 1

        NVMatrix nv_targets(%(targets)s, target_dims[0] * target_dims[1] * target_dims[2],
                            target_dims[3], "MaxPool:nv_targets");

        """

        num_braces += 1

        do_pool = """
        convLocalPool(nv_images, nv_targets, img_channels, %(ds)s,
                      %(start)s, %(stride)s, _outputsX, MaxPooler());
        """

        braces = '}' * num_braces

        rval = (basic_setup +
                setup_nv_images +
                setup_nv_targets +
                do_pool +
                braces)
        start = self.start
        stride = self.stride
        ds = self.ds
        rval = rval % locals()

        return rval

    def R_op(self, inp, evals):
        """
        .. todo::

            WRITEME
        """
        x, = inp
        ev, = evals
        if ev is not None:
            ev = gpu_contiguous(ev)
            return [MaxPoolRop(self.ds, self.stride, self.start)(x, ev)]
        else:
            return [None]


    def grad(self, inp, grads):
        """
        .. todo::

            WRITEME
        """
        x, = inp
        gz, = grads
        gz = gpu_contiguous(gz)
        maxout = self(x)
        return [MaxPoolGrad(self.ds, self.stride, self.start)(x, maxout, gz)]

    # Make sure the cuda_convnet library is compiled and up-to-date
    def make_thunk(self, node, storage_map, compute_map, no_recycling):
        """
        .. todo::

            WRITEME
        """
        if not convnet_available():
            raise RuntimeError('Could not compile cuda_convnet')

        return super(MaxPool, self).make_thunk(
                node, storage_map, compute_map, no_recycling)


class MaxPoolRop(GpuOp):
    """
    This op wrap Alex's MaxPool code on the GPU.
    The input are in the order (channel, image rows, image cols, batch)

    Works only on square images and the grad works only when
    channel % 16 == 0.

    Parameters
    ----------
    ds : int
        Defines the size of the pooling region in the x (equivalently, y)
        dimension. Squares of size (ds)2 get reduced to one value by
        this layer. There are no restrictions on the value of this
        parameter. It's fine for a pooling square to fall off the
        boundary of the image. Named SizeX in Alex's code.
    stride : int
        Defines the stride size between successive pooling squares.
        Setting this parameter smaller than sizeX produces overlapping
        pools. Setting it equal to sizeX gives the usual, non-overlapping
        pools. Values greater than sizeX are not allowed.
    start : int, optional
        Tells the net where in the input image to start the pooling
        (in x,y coordinates). In principle, you can start anywhere you
        want. Setting this to a positive number will cause the net to
        discard some pixels at the top and at the left of the image.
        Setting this to a negative number will cause it to include
        pixels that don't exist (which is fine). start=0 is the usual
        setting.
    outputs : int, optional
        Allows you to control how many output values in the x
        (equivalently, y) dimension this operation will produce. This
        parameter is analogous to the start parameter, in that it
        allows you to discard some portion of the image by setting it
        to a value small enough to leave part of the image uncovered.
        Setting it to zero instructs the net to produce as many outputs
        as is necessary to ensure that the whole image is covered.
        default 0
    """

    def __init__(self, ds, stride, start=0, outputs=0):
        self.ds = ds
        self.stride = stride
        self.start = start
        self.copy_non_contiguous = 0
        assert stride > 0 and stride <= ds, (stride, ds)
        assert ds > 0, ds  # We check in the code if ds <= imgSizeX

    def __eq__(self, other):
        """
        .. todo::

            WRITEME
        """
        #Dont put copy_non_contigous as this doesn't change the output
        return (type(self) == type(other) and
                self.ds == other.ds and
                self.stride == other.stride and
                self.start == other.start)

    def __hash__(self):
        """
        .. todo::

            WRITEME
        """
        #Dont put copy_non_contigous as this doesn't change the output
        return (hash(type(self)) ^ hash(self.ds) ^
                hash(self.stride) ^ hash(self.start))

    def c_header_dirs(self):
        """
        .. todo::

            WRITEME
        """
        return [this_dir]

    def c_headers(self):
        """
        .. todo::

            WRITEME
        """
        return ['nvmatrix.cuh', 'conv_util.cuh', 'pool_rop.cuh']

    def c_lib_dirs(self):
        """
        .. todo::

            WRITEME
        """
        return [cuda_convnet_loc]

    def c_libraries(self):
        """
        .. todo::

            WRITEME
        """
        return ['cuda_convnet']

    def c_code_cache_version(self):
        """
        .. todo::

            WRITEME
        """
        return (1,)

    def _argument_contiguity_check(self, arg_name):
        """
        .. todo::

            WRITEME
        """
        return """
        if (!CudaNdarray_is_c_contiguous(%%(%(arg_name)s)s))
        {
            if (!(%(class_name_caps)s_COPY_NON_CONTIGUOUS)) {
                PyErr_SetString(PyExc_ValueError,
                    "%(class)s: %(arg_name)s must be C contiguous");
                %%(fail)s;
            }
        }
        """ % {
            'class': self.__class__.__name__,
            'arg_name': arg_name,
            'class_name_caps': self.__class__.__name__.upper(),
        }

    def make_node(self, images, evals):
        """
        .. todo::

            WRITEME
        """
        images = as_cuda_ndarray_variable(images)
        evals = as_cuda_ndarray_variable(evals)

        assert images.ndim == 4
        assert evals.ndim == 4

        channels_broadcastable = images.type.broadcastable[0]
        batch_broadcastable = images.type.broadcastable[3]

        rows_broadcastable = False
        cols_broadcastable = False

        targets_broadcastable = (channels_broadcastable, rows_broadcastable,
                cols_broadcastable, batch_broadcastable)
        targets_type = CudaNdarrayType(broadcastable=targets_broadcastable)
        targets = targets_type()

        return Apply(self, [images, evals], [targets])

    def c_code(self, node, name, inputs, outputs, sub):
        """
        .. todo::

            WRITEME
        """
        images,evals = inputs
        targets, = outputs
        fail = sub['fail']

        # The amount of braces that must be closed at the end
        num_braces = 0

        if self.copy_non_contiguous:
            raise UnimplementedError()
        else:
            basic_setup = "#define MAXPOOLROP_COPY_NON_CONTIGUOUS 0\n"

        # Convert images in nv_images, an NVMatrix, for compatibility
        # with the cuda-convnet functions
        setup_nv_images = self._argument_contiguity_check("images") + """
        if (%(images)s->nd != 4)
        {
            PyErr_Format(PyExc_ValueError,
                "images must have nd=4, got nd=%%i", %(images)s->nd);
            %(fail)s;
        }

        { //setup_nv_images brace 1

        const int * images_dims = CudaNdarray_HOST_DIMS(%(images)s);
        const int img_channels = images_dims[0];
        const int imgSizeY = images_dims[1];
        const int imgSizeX = images_dims[2];
        const int batch_size = images_dims[3];

        if(imgSizeY != imgSizeX){
            PyErr_Format(PyExc_ValueError,
                "images must be square(dims[1] == dims[2]). Shape (%%i,%%i,%%i,%%i)",
                img_channels, imgSizeY, imgSizeX, batch_size);
            %(fail)s;
        }
        if(%(ds)s > imgSizeY){
            PyErr_Format(PyExc_ValueError,
                "ds(%%d) must be <= imgSizeX(%%d) and imgSizeY(%%d).",
                %(ds)s, imgSizeX, imgSizeY);
            %(fail)s;
        }
        if(%(start)s >= imgSizeX){
            PyErr_Format(PyExc_ValueError,
                "start is %%d but must be smaller then the images size of %%d x %%d.",
                %(start)s, imgSizeX, imgSizeY);
            %(fail)s;
        }

        NVMatrix nv_images(%(images)s, img_channels * imgSizeY * imgSizeX, batch_size,
        "MaxPoolRop:nv_images");
        NVMatrix nv_evals(%(evals)s, img_channels * imgSizeY * imgSizeX,
        batch_size, "MaxPoolRop:nv_evals");
        """
        num_braces += 1

        setup_nv_targets = """
        //int _outputsX = int(ceil((dic['imgSize'] - dic['start'] - dic['sizeX']) / float(dic['stride']))) + 1;
        int _outputsX = ((int)(ceil((imgSizeY - %(start)s - %(ds)s) / ((float)%(stride)s)))) + 1;

        int target_dims [] = {
            img_channels,
            _outputsX,
            _outputsX,
            batch_size };

        if (CudaNdarray_prep_output(& %(targets)s, 4, target_dims))
        {
            %(fail)s;
        }

        { // setup_nv_target brace # 1

        NVMatrix nv_targets(%(targets)s, target_dims[0] * target_dims[1] * target_dims[2],
                            target_dims[3], "MaxPoolRop:nv_targets");

        """

        num_braces += 1

        do_pool = """
        convLocalPoolR(nv_images, nv_evals, nv_targets, img_channels, %(ds)s,
                      %(start)s, %(stride)s, _outputsX, MaxPoolerR());
        """

        braces = '}' * num_braces

        rval = (basic_setup +
                setup_nv_images +
                setup_nv_targets +
                do_pool +
                braces)
        start = self.start
        stride = self.stride
        ds = self.ds
        rval = rval % locals()

        return rval

    # Make sure the cuda_convnet library is compiled and up-to-date
    def make_thunk(self, node, storage_map, compute_map, no_recycling):
        """
        .. todo::

            WRITEME
        """
        if not convnet_available():
            raise RuntimeError('Could not compile cuda_convnet')

        return super(MaxPoolRop, self).make_thunk(
                node, storage_map, storage_map, no_recycling)


class MaxPoolGrad(GpuOp):
    """
    .. todo::

        WRITEME
    """
    def __init__(self, ds, stride, start):
        self.ds = ds
        self.stride = stride
        self.start = start
        self.copy_non_contiguous = 0
        assert stride > 0 and stride <= ds, (stride, ds)
        assert ds > 0, ds #We check in the code if ds <= imgSizeX

    def __eq__(self, other):
        """
        .. todo::

            WRITEME
        """
        #Dont put copy_non_contigous as this doesn't change the output
        return (type(self) == type(other) and
                self.ds == other.ds and
                self.stride == other.stride and
                self.start == other.start)

    def __hash__(self):
        """
        .. todo::

            WRITEME
        """
        #Dont put copy_non_contigous as this doesn't change the output
        return (hash(type(self)) ^ hash(self.ds) ^
                hash(self.stride) ^ hash(self.start))

    def c_header_dirs(self):
        """
        .. todo::

            WRITEME
        """
        return [this_dir, config.pthreads.inc_dir] if config.pthreads.inc_dir else [this_dir]

    def c_headers(self):
        """
        .. todo::

            WRITEME
        """
        return ['nvmatrix.cuh', 'conv_util.cuh']

    def c_lib_dirs(self):
        """
        .. todo::

            WRITEME
        """
        return [cuda_convnet_loc, config.pthreads.lib_dir] if config.pthreads.lib_dir else [cuda_convnet_loc]

    def c_libraries(self):
        """
        .. todo::

            WRITEME
        """
        return ['cuda_convnet', config.pthreads.lib] if config.pthreads.lib else ['cuda_convnet']

    def c_code_cache_version(self):
        """
        .. todo::

            WRITEME
        """
        return (1,)

    def _argument_contiguity_check(self, arg_name):
        return """
        if (!CudaNdarray_is_c_contiguous(%%(%(arg_name)s)s))
        {
            if (!(%(class_name_caps)s_COPY_NON_CONTIGUOUS)) {
                PyErr_SetString(PyExc_ValueError,
                    "%(class)s: %(arg_name)s must be C contiguous");
                %%(fail)s;
            }
        }
        """ % {
            'class': self.__class__.__name__,
            'arg_name': arg_name,
            'class_name_caps': self.__class__.__name__.upper(),
        }

    def make_node(self, images, maxout, gz):
        """
        .. todo::

            WRITEME
        """
        images = as_cuda_ndarray_variable(images)
        maxout = as_cuda_ndarray_variable(maxout)
        gz = as_cuda_ndarray_variable(gz)

        assert images.ndim == 4
        assert maxout.ndim == 4
        assert gz.ndim == 4
        try:
            # Note : `get_scalar_constant_value` returns a ndarray not a
            # int
            nb_channel = int(get_scalar_constant_value(images.shape[0]))
            assert nb_channel % 16 == 0
        except NotScalarConstantError:
                    pass
        return Apply(self, [images, maxout, gz], [images.type()])

    def c_code(self, node, name, inputs, outputs, sub):
        """
        .. todo::

            WRITEME
        """
        images, maxout, gz = inputs
        targets, = outputs
        fail = sub['fail']

        # The amount of braces that must be closed at the end
        num_braces = 0

        if self.copy_non_contiguous:
            raise UnimplementedError()
        else:
            basic_setup = "#define MAXPOOLGRAD_COPY_NON_CONTIGUOUS 0\n"

        # Convert images in nv_images, an NVMatrix, for compatibility
        # with the cuda-convnet functions
        setup_nv_images = self._argument_contiguity_check("images") + """
        if (%(images)s->nd != 4)
        {
            PyErr_Format(PyExc_ValueError,
                "images must have nd=4, got nd=%%i", %(images)s->nd);
            %(fail)s;
        }

        { //setup_nv_images brace 1

        const int * images_dims = CudaNdarray_HOST_DIMS(%(images)s);
        const int img_channels = images_dims[0];
        const int imgSizeY = images_dims[1];
        const int imgSizeX = images_dims[2];
        const int batch_size = images_dims[3];

        if(imgSizeY != imgSizeX){
            PyErr_Format(PyExc_ValueError,
                "images must be square(dims[1] == dims[2]). Shape (%%i,%%i,%%i,%%i)",
                img_channels, imgSizeY, imgSizeX, batch_size);
            %(fail)s;
        }
        if(%(ds)s > imgSizeY){
            PyErr_Format(PyExc_ValueError,
                "ds(%%d) must be <= imgSizeX(%%d) and imgSizeY(%%d).",
                %(ds)s, imgSizeX, imgSizeY);
            %(fail)s;
        }

        NVMatrix nv_images(%(images)s, img_channels * imgSizeY * imgSizeX, batch_size,
        "MaxPool:nv_images");
        """
        num_braces += 1

        # Convert maxout in nv_maxout
        setup_nv_maxout = self._argument_contiguity_check("maxout") + """
        if (%(maxout)s->nd != 4)
        {
            PyErr_Format(PyExc_ValueError,
                "maxout must have nd=4, got nd=%%i", %(maxout)s->nd);
            %(fail)s;
        }

        { //setup_nv_maxout brace 1

        const int * maxout_dims = CudaNdarray_HOST_DIMS(%(maxout)s);
        const int maxout_channels = maxout_dims[0];
        const int maxoutSizeY = maxout_dims[1];
        const int maxoutSizeX = maxout_dims[2];

        if(maxoutSizeY != maxoutSizeX){
            PyErr_Format(PyExc_ValueError,
                "maxout must be square(dims[1] == dims[2])."
                " Shape (%%i,%%i,%%i,%%i)",
                maxout_channels, maxoutSizeY, maxoutSizeX, batch_size);
            %(fail)s;
        }
        if(img_channels != maxout_channels){
            PyErr_Format(PyExc_ValueError,
                "img_channels(%%d) should be equal to maxout_channels(%%d).",
                img_channels, maxout_channels);
            %(fail)s;
        }
        if(maxout_dims[3] != batch_size){
            PyErr_Format(PyExc_ValueError,
                "batch_size(%%d) should be equal to maxout_dims[3](%%d)",
                batch_size, maxout_dims[3]);
            %(fail)s;
        }

       NVMatrix nv_maxout(%(maxout)s, img_channels * maxoutSizeY * maxoutSizeX,
                          batch_size, "MaxPool:nv_maxout");
        """
        num_braces += 1

        # Convert gz in nv_gz
        setup_nv_gz = self._argument_contiguity_check("gz") + """
        if (%(gz)s->nd != 4)
        {
            PyErr_Format(PyExc_ValueError,
                "gz must have nd=4, got nd=%%i", %(gz)s->nd);
            %(fail)s;
        }
        if (CudaNdarray_HOST_DIMS(%(gz)s)[0] %% 16 != 0)
        {
            PyErr_Format(PyExc_ValueError,
                "gz must have a number of channels that is a multiple of 16. Got %%d",
                CudaNdarray_HOST_DIMS(%(gz)s)[0]);
            %(fail)s;
        }

        { //setup_nv_gz brace 1

        const int * gz_dims = CudaNdarray_HOST_DIMS(%(gz)s);
        const int gz_channels = gz_dims[0];
        const int gzSizeY = gz_dims[1];
        const int gzSizeX = gz_dims[2];

        if(maxout_dims[0] != gz_dims[0] ||
           maxout_dims[1] != gz_dims[1] ||
           maxout_dims[2] != gz_dims[2] ||
           maxout_dims[3] != gz_dims[3]){
            PyErr_Format(PyExc_ValueError,
                "gz shape(%%d, %%d, %%d, %%d) must be the same"
                " as maxout(%%d, %%d, %%d, %%d)",
                maxout_dims[0], maxout_dims[1], maxout_dims[2], maxout_dims[3],
                gz_dims[0], gz_dims[1], gz_dims[2], gz_dims[3]);
            %(fail)s;
        }

        NVMatrix nv_gz(%(gz)s, img_channels * maxoutSizeY * maxoutSizeX,
                       batch_size, "MaxPool:nv_gz");
        """
        num_braces += 1

        setup_nv_targets = """
        //int _outputsX = int(ceil((dic['imgSize'] - dic['start'] - dic['sizeX']) / float(dic['stride']))) + 1;
        int _outputsX = ((int)(ceil((imgSizeY - %(start)s - %(ds)s) / ((float)%(stride)s)))) + 1;

        int target_dims [] = {
            img_channels,
            imgSizeX,
            imgSizeY,
            batch_size };

        if (CudaNdarray_prep_output(& %(targets)s, 4, target_dims))
        {
            %(fail)s;
        }

        { // setup_nv_target brace # 1

        NVMatrix nv_targets(%(targets)s,
                            target_dims[0] * target_dims[1] * target_dims[2],
                            target_dims[3], "MaxPool:nv_targets");

        """

        num_braces += 1

        undo_pool = """
        convLocalMaxUndo(nv_images, nv_gz, nv_maxout, nv_targets,
                         %(ds)s, %(start)s, %(stride)s, _outputsX, 0, 1);
        """

        braces = '}' * num_braces

        rval = (basic_setup +
                setup_nv_images +
                setup_nv_maxout +
                setup_nv_gz +
                setup_nv_targets +
                undo_pool +
                braces)
        start = self.start
        stride = self.stride
        ds = self.ds
        rval = rval % locals()

        return rval

    # Make sure the cuda_convnet library is compiled and up-to-date
    def make_thunk(self, node, storage_map, compute_map, no_recycling):
        """
        .. todo::

            WRITEME
        """
        if not convnet_available():
            raise RuntimeError('Could not compile cuda_convnet')

        return super(MaxPoolGrad, self).make_thunk(
                node, storage_map, compute_map, no_recycling)

########NEW FILE########
__FILENAME__ = probabilistic_max_pooling
"""
A GPU implementation of probabilistic max-pooling, based on

"Convolutional Deep Belief Networks for Scalable
Unsupervised Learning of Hierarchical Representations"
Honglak Lee, Roger Grosse, Rajesh Ranganath, and Andrew Y. Ng
ICML 2009


This paper defines probabilistic max-pooling in the context
of a Convolutional Deep Belief Network (its energy function is
more like a DBM than a DBN but it is trained like a DBN). Here
we define probabilistic max pooling as a general layer for
use in an energy-based model regardless of how the rest of the
model is assembled.

The gpu code is written around Alex Krizhevsky's cuda-convnet
library
"""

__authors__ = "Mehdi Mirza"
__copyright__ = "Copyright 2010-2013, Universite de Montreal"
__credits__ = ["Mehdi Mirza", "Ian Goodfellow", "David Warde-Farley"]
__license__ = "3-clause BSD"
__maintainer__ = "Mehdi Mirza"
__email__ = "mirzamom@iro"


import warnings
import theano
import numpy
from theano import tensor
from theano.gof import Apply
from theano.sandbox.cuda import CudaNdarrayType
from theano.sandbox.cuda.basic_ops import as_cuda_ndarray_variable
from theano.sandbox.cuda.basic_ops import gpu_contiguous
from theano.sandbox.cuda import GpuOp
from theano.tensor import get_scalar_constant_value, NotScalarConstantError

from pylearn2.sandbox.cuda_convnet.base_acts import UnimplementedError
from pylearn2.sandbox.cuda_convnet.convnet_compile import convnet_available
from pylearn2.sandbox.cuda_convnet.convnet_compile import cuda_convnet_loc
from pylearn2.sandbox.cuda_convnet.shared_code import this_dir

import pylearn2.sandbox.cuda_convnet.pthreads
from theano import config


def prob_max_pool_c01b(c01b, pool_shape, top_down = None):
    """
    .. todo::

        WRITEME
    """
    if pool_shape[0] != pool_shape[1]:
        raise UnimplementedError("Non sqaure pool shapes are not supported yet")
    assert pool_shape[0] > 0


    ch, zr, zc, batch_size = c01b.shape
    r, c = pool_shape
    if top_down is None:
        top_down = tensor.zeros((ch, zr / r, zc / c, batch_size), dtype = c01b.dtype)

    op = ProbMaxPool(pool_shape[0])
    c01b = gpu_contiguous(c01b)
    top_down = gpu_contiguous(top_down)

    return op(c01b, top_down)

class ProbMaxPool(GpuOp):
    """
    Probabilistic max pooling code on the GPU.
    The input are in the order (channel, image rows, image cols, batch)

    Works only on square images wiht square pooling shape
    and the grad works only when channel % 16 == 0.

    Parameters
    ----------
    ds : int
        defines the size of the pooling region in the x (equivalently, y)
        dimension. Squares of size (ds)2 get reduced to one value by this
        layer.  There are no restrictions on the value of this parameter. It's
        fine for a pooling square to fall off the boundary of the image. Named
        SizeX in Alex's code.
    stride : int
        defines the stride size between successive pooling squares. Setting
        this parameter smaller than sizeX produces overlapping pools. Setting
        it equal to sizeX gives the usual, non-overlapping pools. Values
        greater than sizeX are not allowed.
    start : int, optional
        tells the net where in the input image to start the pooling (in x,y
        coordinates). In principle, you can start anywhere you want. Setting
        this to a positive number will cause the net to discard some pixels at
        the top and at the left of the image. Setting this to a negative number
        will cause it to include pixels that don't exist (which is fine).
        start=0 is the usual setting.
    outputs : int, optional
        allows you to control how many output values in the x (equivalently, y)
        dimension this operation will produce. This parameter is analogous to
        the start parameter, in that it allows you to discard some portion of
        the image by setting it to a value small enough to leave part of the
        image uncovered. Setting it to zero instructs the net to produce as
        many outputs as is necessary to ensure that the whole image is covered.
        default 0
    """
    def __init__(self, ds, start=0, outputs=0):
        self.ds = ds
        self.stride = ds
        self.start = start
        self.copy_non_contiguous = 0
        assert ds > 0, ds  # We check in the code if ds <= imgSizeX
        warnings.warn("non square pool shape and strides different than "
                    "pool shape hasn't been tested and disabled")

    def __eq__(self, other):
        """
        .. todo::

            WRITEME
        """
        #Dont put copy_non_contigous as this doesn't change the output
        return (type(self) == type(other) and
                self.ds == other.ds and
                self.stride == other.stride and
                self.start == other.start)

    def __hash__(self):
        """
        .. todo::

            WRITEME
        """
        #Dont put copy_non_contigous as this doesn't change the output
        return (hash(type(self)) ^ hash(self.ds) ^
                hash(self.stride) ^ hash(self.start))

    def c_header_dirs(self):
        """
        .. todo::

            WRITEME
        """
        return [this_dir, config.pthreads.inc_dir] if config.pthreads.inc_dir else [this_dir]

    def c_headers(self):
        """
        .. todo::

            WRITEME
        """
        return ['nvmatrix.cuh', 'conv_util.cuh']

    def c_lib_dirs(self):
        """
        .. todo::

            WRITEME
        """
        return [cuda_convnet_loc, config.pthreads.lib_dir] if config.pthreads.lib_dir else [cuda_convnet_loc]

    def c_libraries(self):
        """
        .. todo::

            WRITEME
        """
        return ['cuda_convnet', config.pthreads.lib] if config.pthreads.lib else ['cuda_convnet']

    def c_code_cache_version(self):
        """
        .. todo::

            WRITEME
        """
        return (1,)

    def _argument_contiguity_check(self, arg_name):
        """
        .. todo::

            WRITEME
        """
        return """
        if (!CudaNdarray_is_c_contiguous(%%(%(arg_name)s)s))
        {
            if (!(%(class_name_caps)s_COPY_NON_CONTIGUOUS)) {
                PyErr_SetString(PyExc_ValueError,
                    "%(class)s: %(arg_name)s must be C contiguous");
                %%(fail)s;
            }
        }
        """ % {
            'class': self.__class__.__name__,
            'arg_name': arg_name,
            'class_name_caps': self.__class__.__name__.upper(),
        }

    def make_node(self, images, top_down):
        """
        .. todo::

            WRITEME
        """
        images = as_cuda_ndarray_variable(images)
        top_down = as_cuda_ndarray_variable(top_down)

        assert images.ndim == 4
        assert top_down.ndim == 4

        channels_broadcastable = images.type.broadcastable[0]
        batch_broadcastable = images.type.broadcastable[3]

        rows_broadcastable = False
        cols_broadcastable = False

        houtput_broadcastable = (channels_broadcastable, rows_broadcastable,
                cols_broadcastable, batch_broadcastable)
        houtput_type = CudaNdarrayType(broadcastable=houtput_broadcastable)
        houtput = houtput_type()

        poutput_broadcastable = (channels_broadcastable, rows_broadcastable,
                cols_broadcastable, batch_broadcastable)
        poutput_type = CudaNdarrayType(broadcastable=poutput_broadcastable)
        poutput = poutput_type()

        return Apply(self, [images, top_down], [houtput, poutput])

    def c_code(self, node, name, inputs, outputs, sub):
        """
        .. todo::

            WRITEME
        """
        images, top_down = inputs
        ptargets, htargets = outputs
        fail = sub['fail']

        # The amount of braces that must be closed at the end
        num_braces = 0

        if self.copy_non_contiguous:
            raise UnimplementedError()
        else:
            basic_setup = "#define PROBMAXPOOL_COPY_NON_CONTIGUOUS 0\n"

        # Convert images in nv_images, an NVMatrix, for compatibility
        # with the cuda-convnet functions
        setup_nv_images = self._argument_contiguity_check("images") + """
        if (%(images)s->nd != 4)
        {
            PyErr_Format(PyExc_ValueError,
                "images must have nd=4, got nd=%%i", %(images)s->nd);
            %(fail)s;
        }

        { //setup_nv_images brace 1

        const int * images_dims = CudaNdarray_HOST_DIMS(%(images)s);
        const int img_channels = images_dims[0];
        const int imgSizeY = images_dims[1];
        const int imgSizeX = images_dims[2];
        const int batch_size = images_dims[3];

        if(imgSizeY != imgSizeX){
            PyErr_Format(PyExc_ValueError,
                "images must be square(dims[1] == dims[2]). Shape (%%i,%%i,%%i,%%i)",
                img_channels, imgSizeY, imgSizeX, batch_size);
            %(fail)s;
        }
        if(%(ds)s > imgSizeY){
            PyErr_Format(PyExc_ValueError,
                "ds(%%d) must be <= imgSizeX(%%d) and imgSizeY(%%d).",
                %(ds)s, imgSizeX, imgSizeY);
            %(fail)s;
        }
        if(%(start)s >= imgSizeX){
            PyErr_Format(PyExc_ValueError,
                "start is %%d but must be smaller then the images size of %%d x %%d.",
                %(start)s, imgSizeX, imgSizeY);
            %(fail)s;
        }

        NVMatrix nv_images(%(images)s, img_channels * imgSizeY * imgSizeX, batch_size,
        "ProbMaxPool:nv_images");
        """
        num_braces += 1

        # TODO check if stride != pool shape works, if not put error check
        setup_nv_top_down = self._argument_contiguity_check("top_down") + """
        if (%(top_down)s->nd != 4)
        {
            PyErr_Format(PyExc_ValueError,
                "top_down must have nd=4, got nd=%%i", %(images)s->nd);
            %(fail)s;
        }

        { //setup_nv_images brace 1

        int _outputsX = ((int)(ceil((imgSizeY - %(start)s - %(ds)s) / ((float)%(stride)s)))) + 1;


        NVMatrix nv_top_down(%(top_down)s, img_channels * _outputsX * _outputsX, batch_size,
        "ProbMaxPool:nv_top_down");
        """
        num_braces += 1


        setup_nv_ptargets = """
        //int _outputsX = ((int)(ceil((imgSizeY - %(start)s - %(ds)s) / ((float)%(stride)s)))) + 1;

        int target_dims [] = {
            img_channels,
            _outputsX,
            _outputsX,
            batch_size };

        if (CudaNdarray_prep_output(& %(ptargets)s, 4, target_dims))
        {
            %(fail)s;
        }

        { // setup_nv_target brace # 1

        NVMatrix nv_ptargets(%(ptargets)s, target_dims[0] * target_dims[1] * target_dims[2],
                            target_dims[3], "ProbMaxPool:nv_ptargets");

        """
        num_braces += 1

        setup_nv_htargets = """
        int target_dims [] = {
            img_channels,
            imgSizeX,
            imgSizeY,
            batch_size };

        if (CudaNdarray_prep_output(& %(htargets)s, 4, target_dims))
        {
            %(fail)s;
        }

        { // setup_nv_target brace # 1

        NVMatrix nv_htargets(%(htargets)s, target_dims[0] * target_dims[1] * target_dims[2],
                            target_dims[3], "ProbMaxPool:nv_htargets");

        """
        num_braces += 1

        do_pool = """
        probabilisticPool(nv_images, nv_top_down, nv_ptargets, nv_htargets, img_channels, %(ds)s,
                      %(start)s, %(stride)s, _outputsX, MaxPooler());
        """

        braces = '}' * num_braces

        rval = (basic_setup +
                setup_nv_images +
                setup_nv_top_down +
                setup_nv_ptargets +
                setup_nv_htargets +
                do_pool +
                braces)
        start = self.start
        stride = self.stride
        ds = self.ds
        rval = rval % locals()

        return rval

    def grad(self, inp, grads):
        """
        .. todo::

            WRITEME
        """
        x, top_down = inp
        p, h = self(x, top_down)
        gp, gh = grads
        gp_iszero = 0.
        gh_iszero = 0.
        if isinstance(gp.type, theano.gradient.DisconnectedType):
            gp = tensor.zeros_like(p)
            gp_iszero = 1.
        if isinstance(gh.type, theano.gradient.DisconnectedType):
            gh = tensor.zeros_like(h)
            gh_iszero = 1.
        gp = gpu_contiguous(gp)
        gh = gpu_contiguous(gh)
        gp_iszero = as_cuda_ndarray_variable(gp_iszero)
        gh_iszero = as_cuda_ndarray_variable(gh_iszero)
        return ProbMaxPoolGrad(self.ds, self.stride, self.start)(p, h, gp, gh, gp_iszero, gh_iszero)

    # Make sure the cuda_convnet library is compiled and up-to-date
    def make_thunk(self, node, storage_map, compute_map, no_recycling):
        """
        .. todo::

            WRITEME
        """
        if not convnet_available():
            raise RuntimeError('Could not compile cuda_convnet')

        return super(ProbMaxPool, self).make_thunk(
                node, storage_map, compute_map, no_recycling)

class ProbMaxPoolGrad(GpuOp):
    """
    .. todo::

        WRITEME
    """

    def __init__(self, ds, stride, start):
        self.ds = ds
        self.stride = stride
        self.start = start
        self.copy_non_contiguous = 0
        assert stride > 0 and stride <= ds, (stride, ds)
        assert ds > 0, ds #We check in the code if ds <= imgSizeX

    def __eq__(self, other):
        """
        .. todo::

            WRITEME
        """
        #Dont put copy_non_contigous as this doesn't change the output
        return (type(self) == type(other) and
                self.ds == other.ds and
                self.stride == other.stride and
                self.start == other.start)

    def __hash__(self):
        """
        .. todo::

            WRITEME
        """
        #Dont put copy_non_contigous as this doesn't change the output
        return (hash(type(self)) ^ hash(self.ds) ^
                hash(self.stride) ^ hash(self.start))

    def c_header_dirs(self):
        """
        .. todo::

            WRITEME
        """
        return [this_dir, config.pthreads.inc_dir] if config.pthreads.inc_dir else [this_dir]

    def c_headers(self):
        """
        .. todo::

            WRITEME
        """
        return ['nvmatrix.cuh', 'conv_util.cuh']

    def c_lib_dirs(self):
        """
        .. todo::

            WRITEME
        """
        return [cuda_convnet_loc, config.pthreads.lib_dir] if config.pthreads.lib_dir else [cuda_convnet_loc]

    def c_libraries(self):
        """
        .. todo::

            WRITEME
        """
        return ['cuda_convnet', config.pthreads.lib] if config.pthreads.lib else ['cuda_convnet']

    def c_code_cache_version(self):
        """
        .. todo::

            WRITEME
        """
        return (1,)

    def _argument_contiguity_check(self, arg_name):
        """
        .. todo::

            WRITEME
        """
        return """
        if (!CudaNdarray_is_c_contiguous(%%(%(arg_name)s)s))
        {
            if (!(%(class_name_caps)s_COPY_NON_CONTIGUOUS)) {
                PyErr_SetString(PyExc_ValueError,
                    "%(class)s: %(arg_name)s must be C contiguous");
                %%(fail)s;
            }
        }
        """ % {
            'class': self.__class__.__name__,
            'arg_name': arg_name,
            'class_name_caps': self.__class__.__name__.upper(),
        }

    def make_node(self, p, h, gp, gh, gp_iszero, gh_iszero):
        """
        .. todo::

            WRITEME
        """
        p = as_cuda_ndarray_variable(p)
        h = as_cuda_ndarray_variable(h)
        gp = as_cuda_ndarray_variable(gp)
        gh = as_cuda_ndarray_variable(gh)

        assert p.ndim == 4
        assert h.ndim == 4
        assert gp.ndim == 4
        assert gh.ndim == 4
        try:
            nb_channel = int(get_scalar_constant_value(h.shape[0]))
            assert nb_channel % 16 == 0
        except NotScalarConstantError:
                    pass

        return Apply(self, [p, h, gp, gh, gp_iszero, gh_iszero], [p.type(), h.type()])

    def c_code(self, node, name, inputs, outputs, sub):
        """
        .. todo::

            WRITEME
        """
        p, h, gp, gh, gp_iszero, gh_iszero = inputs
        targets_z, targets_t, = outputs
        fail = sub['fail']

        # The amount of braces that must be closed at the end
        num_braces = 0

        if self.copy_non_contiguous:
            raise UnimplementedError()
        else:
            basic_setup = "#define PROBMAXPOOLGRAD_COPY_NON_CONTIGUOUS 0\n"

        # Convert images in nv_images, an NVMatrix, for compatibility
        # with the cuda-convnet functions
        setup_nv_h = self._argument_contiguity_check("h") + """
        if (%(h)s->nd != 4)
        {
            PyErr_Format(PyExc_ValueError,
                "h must have nd=4, got nd=%%i", %(h)s->nd);
            %(fail)s;
        }

        { //setup_nv_images brace 1

        const int * images_dims = CudaNdarray_HOST_DIMS(%(h)s);
        const int img_channels = images_dims[0];
        const int imgSizeY = images_dims[1];
        const int imgSizeX = images_dims[2];
        const int batch_size = images_dims[3];

        if(imgSizeY != imgSizeX){
            PyErr_Format(PyExc_ValueError,
                "images must be square(dims[1] == dims[2]). Shape (%%i,%%i,%%i,%%i)",
                img_channels, imgSizeY, imgSizeX, batch_size);
            %(fail)s;
        }
        if(%(ds)s > imgSizeY){
            PyErr_Format(PyExc_ValueError,
                "ds(%%d) must be <= imgSizeX(%%d) and imgSizeY(%%d).",
                %(ds)s, imgSizeX, imgSizeY);
            %(fail)s;
        }
        if (CudaNdarray_HOST_DIMS(%(h)s)[0] %% 16 != 0)
        {
            PyErr_Format(PyExc_ValueError,
                "h must have a number of channels that is a multiple of 16. Got %%d",
                CudaNdarray_HOST_DIMS(%(gh)s)[0]);
            %(fail)s;
        }


        NVMatrix nv_h(%(h)s, img_channels * imgSizeY * imgSizeX,
                          batch_size, "ProbMaxPool:nv_h");

        """
        num_braces += 1


        setup_nv_p = self._argument_contiguity_check("p") + """
        if (%(p)s->nd != 4)
        {
            PyErr_Format(PyExc_ValueError,
                "P must have nd=4, got nd=%%i", %(p)s->nd);
            %(fail)s;
        }

        { //setup_nv_images brace 1

        int _outputsX = ((int)(ceil((imgSizeY - %(start)s - %(ds)s) / ((float)%(stride)s)))) + 1;


        NVMatrix nv_p(%(p)s, img_channels * _outputsX * _outputsX, batch_size,
        "ProbMaxPool:nv_p");
        """
        num_braces += 1

        # Convert gh in nv_gh
        setup_nv_gh = self._argument_contiguity_check("gh") + """
        if (%(gh)s->nd != 4)
        {
            PyErr_Format(PyExc_ValueError,
                "gh must have nd=4, got nd=%%i", %(gh)s->nd);
            %(fail)s;
        }
        if (CudaNdarray_HOST_DIMS(%(gh)s)[0] %% 16 != 0)
        {
            PyErr_Format(PyExc_ValueError,
                "gh must have a number of channels that is a multiple of 16. Got %%d",
                CudaNdarray_HOST_DIMS(%(gh)s)[0]);
            %(fail)s;
        }

        { //setup_nv_gh brace 1

        const int * gh_dims = CudaNdarray_HOST_DIMS(%(gh)s);
        const int gh_channels = gh_dims[0];
        const int ghSizeY = gh_dims[1];
        const int ghSizeX = gh_dims[2];

        NVMatrix nv_gh(%(gh)s, gh_channels * ghSizeY * ghSizeX,
                       batch_size, "ProbMaxPool:nv_gh");
        """
        num_braces += 1

        setup_nv_gp = self._argument_contiguity_check("gp") + """
        if (%(gp)s->nd != 4)
        {
            PyErr_Format(PyExc_ValueError,
                "gp must have nd=4, got nd=%%i", %(gp)s->nd);
            %(fail)s;
        }

        { //setup_nv_images brace 1

        int _outputsX = ((int)(ceil((imgSizeY - %(start)s - %(ds)s) / ((float)%(stride)s)))) + 1;


        NVMatrix nv_gp(%(gp)s, img_channels * _outputsX * _outputsX, batch_size,
        "ProbMaxPool:nv_gp");
        """
        num_braces += 1


        setup_nv_targets_z = """
        int target_z_dims [] = {
            img_channels,
            imgSizeX,
            imgSizeY,
            batch_size };

        if (CudaNdarray_prep_output(& %(targets_z)s, 4, target_z_dims))
        {
            %(fail)s;
        }

        { // setup_nv_target brace # 1

        NVMatrix nv_targets_z(%(targets_z)s,
                            target_z_dims[0] * target_z_dims[1] * target_z_dims[2],
                            target_z_dims[3], "ProbMaxPool:nv_targets_z");

        """

        num_braces += 1


        setup_nv_targets_t = """
        int target_t_dims [] = {
            img_channels,
            _outputsX,
            _outputsX,
            batch_size };

        if (CudaNdarray_prep_output(& %(targets_t)s, 4, target_t_dims))
        {
            %(fail)s;
        }

        { // setup_nv_target brace # 1

        NVMatrix nv_targets_t(%(targets_t)s, target_t_dims[0] * target_t_dims[1] * target_t_dims[2],
                            target_t_dims[3], "ProbMaxPool:nv_targets_t");


        float * gp_iszero = CudaNdarray_DEV_DATA(%(gp_iszero)s);
        float * gh_iszero = CudaNdarray_DEV_DATA(%(gh_iszero)s);
        """
        num_braces += 1


        undo_pool = """
        localProbMaxUndo(nv_h, nv_p, nv_gh, nv_gp, nv_targets_z, nv_targets_t,
                         %(ds)s, %(start)s, %(stride)s, _outputsX, imgSizeX, gp_iszero, gh_iszero);
        """

        braces = '}' * num_braces

        rval = (basic_setup +
                setup_nv_h +
                setup_nv_p +
                setup_nv_gh +
                setup_nv_gp +
                setup_nv_targets_z +
                setup_nv_targets_t +
                undo_pool +
                braces)
        start = self.start
        stride = self.stride
        ds = self.ds
        rval = rval % locals()

        return rval

    # Make sure the cuda_convnet library is compiled and up-to-date
    def make_thunk(self, node, storage_map, compute_map, no_recycling):
        """
        .. todo::

            WRITEME
        """
        if not convnet_available():
            raise RuntimeError('Could not compile cuda_convnet')

        return super(ProbMaxPoolGrad, self).make_thunk(
                node, storage_map, compute_map, no_recycling)



########NEW FILE########
__FILENAME__ = pthreads
from theano.configparser import AddConfigVar, StrParam

AddConfigVar('pthreads.inc_dir',
        "location of pthread.h",
        StrParam(""))
		
AddConfigVar('pthreads.lib_dir',
        "location of library implementing pthreads",
        StrParam(""))
		
AddConfigVar('pthreads.lib',
        'name of the library that implements pthreads (e.g. "pthreadVC2" if using pthreadVC2.dll/.lib from pthreads-win32)',
        StrParam(""))


########NEW FILE########
__FILENAME__ = response_norm
"""
A theano / pylearn2 wrapper for cuda-convnet's response normalization
functions.
"""
__authors__ = "David Warde-Farley"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["David Warde-Farley", "Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "David Warde-Farley"
__email__ = "wardefar@iro"


__all__ = ["CrossMapNorm", "CrossMapNormUndo"]

"""
This module may contain code copied directly or modified from cuda-convnet.
The copyright and licensing notice for this code is reproduced below:


/*
 * Copyright (c) 2011, Alex Krizhevsky (akrizhevsky@gmail.com)
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without modification,
 * are permitted provided that the following conditions are met:
 *
 * - Redistributions of source code must retain the above copyright notice,
 *   this list of conditions and the following disclaimer.
 *
 * - Redistributions in binary form must reproduce the above copyright notice,
 *   this list of conditions and the following disclaimer in the documentation
 *   and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
 * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
 * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
 * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
 * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
 * NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,
 * EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

"""

import theano
from theano.sandbox.cuda import CudaNdarrayType
from theano.sandbox.cuda.basic_ops import as_cuda_ndarray_variable
from theano.gof import Apply, local_optimizer, TopoOptimizer
from pylearn2.sandbox.cuda_convnet.base_acts import BaseActs
from .code_templates import (
    contiguity_check, dimension_check, output_same_shape,
    ensure_same_shape, nv_matrix_create
)


class CrossMapNorm(BaseActs):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    size_f : int
        Filter neighbourhood size. Must be >= 1 and <= the number
        of filters (I think).

    add_scale : float
        Constant that scales the sum in the denominator (alpha).

    pow_scale : float
        Exponent to which the denominator is raised (beta).

    blocked : bool
        Controls the "block-wise" behaviour in a way I don't quite
        understand.
    """
    _basic_setup = """
        #define %(class_name_upper)s_COPY_NON_CONTIGUOUS 0
        int sizeF = %(size_f)d;
        float addScale = %(add_scale)f;
        float powScale = %(pow_scale)f;
        bool blocked = %(blocked)s;
    """

    _images_setup = """
        { // images_setup brace #1
        const int * images_dims = CudaNdarray_HOST_DIMS(%(images)s);
        const int numFilters = images_dims[0];
        const int imgSizeY = images_dims[1];
        const int imgSizeX = images_dims[2];
        const int batch_size = images_dims[3];
        if (numFilters %% 16 != 0)
        {
            PyErr_Format(PyExc_ValueError, "%(class_name)s: images.shape[0] "
                         "must be a multiple of 16, but got %%d",
                         images_dims[0]);
            %(fail)s;
        }
        if (sizeF > images_dims[0]) {
            PyErr_Format(PyExc_ValueError, "%(class_name)s: size_f "
                         "is %%d but images.shape[0] is %%d", sizeF,
                         images_dims[0]);
            %(fail)s;
        }
        if (imgSizeY != imgSizeX) {
            PyErr_Format(PyExc_ValueError, "%(class_name)s: images "
                         "must be square; got (%%d, %%d)", imgSizeY, imgSizeX);
            %(fail)s;
        }
        { // images_setup brace #2
        NVMatrix nv_images(%(images)s, numFilters * imgSizeY * imgSizeX,
                           batch_size, "%(class_name)s:nv_images");
    """

    def __init__(self, size_f, add_scale, pow_scale, blocked):
        if size_f < 0:
            raise ValueError("size_f must be positive (got %d)" % size_f)
        self._size_f = int(size_f)
        self._add_scale = float(add_scale)
        self._pow_scale = float(pow_scale)
        self._blocked = bool(blocked)

    def __hash__(self):
        """
        .. todo::

            WRITEME
        """
        return hash((self._size_f, self._add_scale, self._pow_scale,
                     self._blocked))

    def __eq__(self, other):
        """
        .. todo::

            WRITEME
        """
        return type(self) == type(other) and hash(self) == hash(other)

    def make_node(self, images):
        """
        .. todo::

            WRITEME
        """
        if not isinstance(images.type, CudaNdarrayType):
            raise TypeError("CrossMapNorm: expected images.type to be CudaNdarrayType, "
                    "got " + str(images.type))

        assert images.ndim == 4

        targets_broadcastable = images.type.broadcastable
        targets_type = CudaNdarrayType(broadcastable=targets_broadcastable)
        denoms = targets_type()
        targets = targets_type()

        return Apply(self, [images], [targets, denoms])

    def c_code(self, node, name, inputs, outputs, sub):
        """
        .. todo::

            WRITEME
        """
        images, = inputs
        targets, denoms = outputs
        fail = sub['fail']
        num_braces = 0

        size_f = self._size_f
        add_scale = self._add_scale
        pow_scale = self._pow_scale
        blocked = "true" if self._blocked else "false"

        class_name = self.__class__.__name__
        class_name_upper = class_name.upper()

        basic_setup = self._basic_setup

        setup_nv_images = (
            contiguity_check("images") +
            dimension_check("images", 4) +
            self._images_setup
        )
        num_braces += 2

        setup_nv_targets = output_same_shape('targets', 'images')
        num_braces += 1

        setup_nv_denoms = output_same_shape('denoms', 'images')
        num_braces += 1

        do_normalize = """
        convResponseNormCrossMap(nv_images, nv_denoms, nv_targets, numFilters, sizeF,
                                 addScale, powScale, blocked);
        """

        braces = '}' * num_braces + "\n"

        rval = (basic_setup +
                setup_nv_images +
                setup_nv_targets +
                setup_nv_denoms +
                do_normalize +
                braces)

        rval = rval % locals()

        return rval

    def grad(self, inputs, dout):
        """
        .. todo::

            WRITEME
        """
        images, = inputs
        acts, denoms = self(images)
        dout, _ = dout  # Ignore the gradient on "denoms"
        dout = as_cuda_ndarray_variable(dout)
        grad_op = CrossMapNormUndo(self._size_f, self._add_scale,
                                   self._pow_scale, self._blocked,
                                   inplace=False)
        return [grad_op(images, acts, denoms, dout)[0]]

    def __str__(self):
        """
        .. todo::

            WRITEME
        """
        return (self.__class__.__name__ +
                "[size_f=%d,add_scale=%f,pow_scale=%f,blocked=%s]"
                % (self._size_f, self._add_scale, self._pow_scale,
                   self._blocked))

    def c_code_cache_version(self):
        """
        .. todo::

            WRITEME
        """
        return (6,)


class CrossMapNormUndo(CrossMapNorm):
    """
    .. todo::

        WRITEME
    """

    def __init__(self, size_f, add_scale, pow_scale, blocked, inplace=False):
        self._scale_targets = 0
        self._scale_outputs = 1
        self._inplace = inplace
        if inplace:
            self.destroy_map = {1: [1]}
        super(CrossMapNormUndo, self).__init__(size_f, add_scale, pow_scale,
                                               blocked)

    def __hash__(self):
        """
        .. todo::

            WRITEME
        """
        super_hash = super(CrossMapNormUndo, self).__hash__()
        return hash((super_hash, self._inplace))

    def make_node(self, images, acts, denoms, dout):
        """
        .. todo::

            WRITEME
        """
        if not isinstance(images.type, CudaNdarrayType):
            inputs = images, acts, denoms, dout
            names = "images", "acts", "denoms", "dout"
            for name, var in zip(names, inputs):
                if not isinstance(var.type, CudaNdarrayType):
                    raise TypeError("CrossMapNormUndo: expected %s.type "
                                    "to be CudaNdarrayType, "
                                    "got %s" (name, str(images.type)))
        assert images.ndim == 4
        assert acts.ndim == 4
        assert denoms.ndim == 4
        assert dout.ndim == 4
        # Not strictly necessary I don't think
        assert images.type.broadcastable == acts.type.broadcastable
        assert images.type.broadcastable == denoms.type.broadcastable
        assert images.type.broadcastable == dout.type.broadcastable

        targets_broadcastable = tuple(images.type.broadcastable)
        targets_type = CudaNdarrayType(broadcastable=targets_broadcastable)
        targets = targets_type()
        out_acts = targets_type()
        return Apply(self, [images, acts, denoms, dout], [targets, out_acts])

    def c_code(self, node, name, inputs, outputs, sub):
        """
        .. todo::

            WRITEME
        """
        images, acts, denoms, dout = inputs
        targets, out_acts = outputs
        fail = sub['fail']
        num_braces = 0
        size_f = self._size_f
        add_scale = self._add_scale
        pow_scale = self._pow_scale
        blocked = "true" if self._blocked else "false"
        inplace = "1" if self._inplace else "0"
        scale_targets = int(self._scale_targets)
        scale_outputs = int(self._scale_outputs)

        class_name = self.__class__.__name__
        class_name_upper = class_name.upper()

        basic_setup = self._basic_setup
        scaling_setup = """
        float scaleTargets = %(scale_targets)s;
        float scaleOutput = %(scale_outputs)s;
        """

        setup_nv_images = (
            contiguity_check("images") +
            dimension_check("images", 4) +
            self._images_setup
        )
        num_braces += 2
        setup_acts = (contiguity_check("acts") +
                      dimension_check("acts", 4) +
        """
        { //setup_nv_images brace 1
        const int * acts_dims = CudaNdarray_HOST_DIMS(%(acts)s);
        """ +
                      ensure_same_shape('acts', 'images') +
        """
        { // setup_nv_acts brace 2
        """)
        num_braces += 2
        setup_nv_denoms = (contiguity_check("denoms") +
                           dimension_check("denoms", 4) +
        """
        {
        const int *denoms_dims = images_dims;
        """ +
                           ensure_same_shape("denoms", "images") +
                           nv_matrix_create("denoms"))
        num_braces += 2

        setup_nv_dout = (contiguity_check("dout") +
                         dimension_check("dout", 4) +
        """
        { // setup_nv_dout brace
        const int *dout_dims = CudaNdarray_HOST_DIMS(%(dout)s);
        """ +
                         ensure_same_shape("dout", "images") +
                         nv_matrix_create("dout"))
        num_braces += 2
        setup_nv_targets = output_same_shape('targets', 'images')
        num_braces += 1

        setup_nv_out_acts = ("""
        const int *out_acts_dims = images_dims;

        #if %(inplace)s
        // XXX: is this right?
        Py_XDECREF(%(out_acts)s);
        %(out_acts)s = %(acts)s;
        Py_INCREF(%(out_acts)s);
        #else
        if (CudaNdarray_prep_output(& %(out_acts)s, 4, out_acts_dims)) {
            Py_DECREF(%(targets)s);
            %(fail)s;
        }
        if (CudaNdarray_CopyFromCudaNdarray(%(out_acts)s, %(acts)s)) {
            Py_DECREF(%(targets)s);
            Py_DECREF(%(out_acts)s);
            %(fail)s;
        }
        #endif
        """ + nv_matrix_create("out_acts"))
        num_braces += 1

        undo_normalize = """
        convResponseNormCrossMapUndo(nv_dout, nv_denoms, nv_images,
                                     nv_out_acts, nv_targets, numFilters,
                                     sizeF, addScale, powScale, blocked,
                                     scaleTargets, scaleOutput);
        """
        rval = "\n".join((basic_setup,
                          scaling_setup,
                          setup_nv_images,
                          setup_acts,
                          setup_nv_denoms,
                          setup_nv_dout,
                          setup_nv_targets,
                          setup_nv_out_acts,
                          undo_normalize,
                          "}" * num_braces))
        return rval % locals()

    def grad(self, inputs, dout):
        """
        .. todo::

            WRITEME
        """
        raise NotImplementedError()

    @property
    def inplace(self):
        """
        .. todo::

            WRITEME
        """
        return self._inplace

    def as_inplace(self):
        """
        .. todo::

            WRITEME
        """
        if self._inplace:
            raise ValueError("%s instance is already inplace, can't convert" %
                             self.__class__.__name__)
        return self.__class__(self._size_f, self._add_scale, self._pow_scale,
                              self._blocked, inplace=True)

    def __str__(self):
        """
        .. todo::

            WRITEME
        """
        return (self.__class__.__name__ +
                "[size_f=%d,add_scale=%.2f,pow_scale=%.2f,blocked=%s,inplace=%s]"
                % (self._size_f, self._add_scale, self._pow_scale,
                   self._blocked, self._inplace))

    def c_code_cache_version(self):
        """
        .. todo::

            WRITEME
        """
        return (8,)


@local_optimizer([CrossMapNormUndo])
def local_crossmapnormundo_inplace(node):
    """
    .. todo::

        WRITEME
    """
    if isinstance(node.op, CrossMapNormUndo) and not node.op.inplace:
        new_op = node.op.as_inplace()
        new_node = new_op(*node.inputs)
        return new_node
    return False


theano.compile.optdb.register('local_crossmapnormundo_inplace',
                              TopoOptimizer(local_crossmapnormundo_inplace,
                                            failure_callback=TopoOptimizer.warn_inplace),
                              80, 'fast_run', 'inplace')

########NEW FILE########
__FILENAME__ = shared_code
"""
Convenience methods for accessing C code shared by the code
generators in different parts of this module.
"""

__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

import pylearn2
pylearn2_path, = pylearn2.__path__
this_dir = pylearn2_path + "/sandbox/cuda_convnet/"

def load_code(local_path):

    path = this_dir + local_path
    f = open(path)
    return f.read()

def get_NVMatrix_code():
    header = '#include "nvmatrix.cuh"'
    source1 = load_code("nvmatrix.cu")
    source2 = load_code("nvmatrix_kernels.cu")

    source = source1 + source2

    rval = header + source

    return rval


########NEW FILE########
__FILENAME__ = specialized_bench
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

from pylearn2.testing.skip import skip_if_no_gpu
skip_if_no_gpu()
import numpy as np
from theano import shared
from pylearn2.sandbox.cuda_convnet.filter_acts import FilterActs
from theano.tensor.nnet.conv import conv2d
from theano import function
import time
import matplotlib.pyplot as plt


def make_funcs(batch_size, rows, cols, channels, filter_rows,
        num_filters):
    rng = np.random.RandomState([2012,10,9])

    filter_cols = filter_rows

    base_image_value = rng.uniform(-1., 1., (channels, rows, cols,
        batch_size)).astype('float32')
    base_filters_value = rng.uniform(-1., 1., (channels, filter_rows,
        filter_cols, num_filters)).astype('float32')
    images = shared(base_image_value)
    filters = shared(base_filters_value, name='filters')

    # bench.py should always be run in gpu mode so we should not need a gpu_from_host here
    layer_1_detector = FilterActs()(images, filters)

    layer_1_pooled_fake = layer_1_detector[:,0:layer_1_detector.shape[0]:2,
            0:layer_1_detector.shape[1]:2, :]

    base_filters2_value = rng.uniform(-1., 1., (num_filters, filter_rows,
        filter_cols, num_filters)).astype('float32')
    filters2 = shared(base_filters_value, name='filters')

    layer_2_detector = FilterActs()(images, filters2)

    output = layer_2_detector

    output_shared = shared( output.eval() )

    cuda_convnet = function([], updates = { output_shared : output } )
    cuda_convnet.name = 'cuda_convnet'

    images_bc01 = base_image_value.transpose(3,0,1,2)
    filters_bc01 = base_filters_value.transpose(3,0,1,2)
    filters_bc01 = filters_bc01[:,:,::-1,::-1]

    images_bc01 = shared(images_bc01)
    filters_bc01 = shared(filters_bc01)

    output_conv2d = conv2d(images_bc01, filters_bc01,
            border_mode='valid')

    output_conv2d_shared = shared(output_conv2d.eval())

    baseline = function([], updates = { output_conv2d_shared : output_conv2d } )
    baseline.name = 'baseline'

    return cuda_convnet, baseline

def bench(f):
    for i in xrange(3):
        f()
    trials = 10
    t1 = time.time()
    for i in xrange(trials):
        f()
    t2 = time.time()
    return (t2-t1)/float(trials)

def get_speedup( *args, **kwargs):
    cuda_convnet, baseline = make_funcs(*args, **kwargs)
    return bench(baseline) / bench(cuda_convnet)

def get_time_per_10k_ex( *args, **kwargs):
    cuda_convnet, baseline = make_funcs(*args, **kwargs)
    batch_size = kwargs['batch_size']
    return 10000 * bench(cuda_convnet) / float(batch_size)

def make_batch_size_plot(yfunc, yname, batch_sizes, rows, cols, channels, filter_rows, num_filters):
    speedups = []
    for batch_size in batch_sizes:
        speedup = yfunc(batch_size = batch_size,
                rows = rows,
                cols = cols,
                channels = channels,
                filter_rows = filter_rows,
                num_filters = num_filters)
        speedups.append(speedup)
    plt.plot(batch_sizes, speedups)
    plt.title("cuda-convnet benchmark")
    plt.xlabel("Batch size")
    plt.ylabel(yname)
    plt.show()

"""
make_batch_size_plot(get_speedup, "Speedup factor", batch_sizes = [1,2,5,25,32,50,63,64,65,96,100,127,128,129,159,160,161,191,192,193,200,255,256,257],
        rows = 32,
        cols = 32,
        channels = 3,
        filter_rows = 7,
        num_filters = 64)
"""

make_batch_size_plot(get_time_per_10k_ex, "Time per 10k examples", batch_sizes = [1,2,5,25,32,50,63,64,65,96,100,127,128,129,159,160,161,191,192,193,200,255,256,257],
        rows = 32,
        cols = 32,
        channels = 3,
        filter_rows = 5,
        num_filters = 64)


########NEW FILE########
__FILENAME__ = stochastic_pool
"""
GPU op for Stochastic max pooling as defined in:

Stochastic Pooling for Regularization of Deep Convolutional Neural Networks
Matthew D. Zeiler, Rob Fergus, ICLR 2013

The code is written around Alex Krizhevsky's cuda-convnet
"""

__authors__ = "Mehdi Mirza"
__copyright__ = "Copyright 2010-2013, Universite de Montreal"
__credits__ = ["Mehdi Mirza", "David Warde-Farley"]
__license__ = "3-clause BSD"
__maintainer__ = "Mehdi Mirza"
__email__ = "mirzamom@iro"

import warnings
import numpy
from theano import shared
from theano.gof import Apply
from theano.sandbox.cuda import CudaNdarrayType
from theano.sandbox.cuda.basic_ops import as_cuda_ndarray_variable
from theano.sandbox.cuda.basic_ops import gpu_contiguous
from theano.sandbox.cuda import GpuOp
from theano.tensor import get_scalar_constant_value, NotScalarConstantError, zeros_like
from pylearn2.sandbox.cuda_convnet.base_acts import UnimplementedError
from pylearn2.sandbox.cuda_convnet.convnet_compile import convnet_available
from pylearn2.sandbox.cuda_convnet.convnet_compile import cuda_convnet_loc
from pylearn2.sandbox.cuda_convnet.shared_code import this_dir
from pylearn2.sandbox.cuda_convnet.pool import MaxPoolGrad

def stochastic_max_pool_c01b(c01b, pool_shape, pool_stride, start=0, seed = 1234):
    """
    .. todo::

        WRITEME
    """
    assert pool_shape[0] == pool_shape[1]
    assert pool_stride[0] == pool_stride[1]
    op = StochasticMaxPool(pool_shape[0], pool_stride[0], start, seed)
    c01b = gpu_contiguous(c01b)
    return op(c01b)

def weighted_max_pool_c01b(c01b, pool_shape, pool_stride, start=0):
    """
    .. todo::

        WRITEME
    """
    assert pool_shape[0] == pool_shape[1]
    assert pool_stride[0] == pool_stride[1]
    op = WeightedMaxPool(pool_shape[0], pool_stride[0], start)
    c01b = gpu_contiguous(c01b)
    return op(c01b)

class StochasticMaxPool(GpuOp):
    """
    Stochastic MaxPool op code on the GPU.
    The input are in the order (channel, image rows, image cols, batch)

    Works only on square images and the grad works only when
    channel % 16 == 0.

    Parameters
    ----------
    ds : int
        defines the size of the pooling region in the x (equivalently, y)
        dimension. Squares of size (ds)2 get reduced to one value by this
        layer.  There are no restrictions on the value of this parameter. It's
        fine for a pooling square to fall off the boundary of the image. Named
        SizeX in Alex's code.
    stride : int
        defines the stride size between successive pooling squares. Setting
        this parameter smaller than sizeX produces overlapping pools. Setting
        it equal to sizeX gives the usual, non-overlapping pools. Values
        greater than sizeX are not allowed.
    start : int, optional
        tells the net where in the input image to start the pooling (in x,y
        coordinates). In principle, you can start anywhere you want. Setting
        this to a positive number will cause the net to discard some pixels at
        the top and at the left of the image. Setting this to a negative number
        will cause it to include pixels that don't exist (which is fine).
        start=0 is the usual setting.
    outputs : int, optional
        allows you to control how many output values in the x (equivalently, y)
        dimension this operation will produce. This parameter is analogous to
        the start parameter, in that it allows you to discard some portion of
        the image by setting it to a value small enough to leave part of the
        image uncovered. Setting it to zero instructs the net to produce as
        many outputs as is necessary to ensure that the whole image is covered.
        default 0
    seed : WRITEME
    """

    def __init__(self, ds, stride, start=0, outputs=0, seed = 1234):
        self.ds = ds
        self.stride = stride
        self.start = start
        self.copy_non_contiguous = 0
        self.seed_state = shared(numpy.asarray(seed).astype('float32'))
        self.seed_state.default_update = self.seed_state + 1
        assert stride > 0 and stride <= ds, (stride, ds)
        assert ds > 0, ds  # We check in the code if ds <= imgSizeX

    def __eq__(self, other):
        """
        .. todo::

            WRITEME
        """
        #Dont put copy_non_contigous as this doesn't change the output
        return (type(self) == type(other) and
                self.ds == other.ds and
                self.stride == other.stride and
                self.start == other.start)

    def __hash__(self):
        """
        .. todo::

            WRITEME
        """
        #Dont put copy_non_contigous as this doesn't change the output
        return (hash(type(self)) ^ hash(self.ds) ^
                hash(self.stride) ^ hash(self.start))

    def c_header_dirs(self):
        """
        .. todo::

            WRITEME
        """
        return [this_dir]

    def c_headers(self):
        """
        .. todo::

            WRITEME
        """
        return ['nvmatrix.cuh', 'conv_util.cuh']

    def c_lib_dirs(self):
        """
        .. todo::

            WRITEME
        """
        return [cuda_convnet_loc]

    def c_libraries(self):
        """
        .. todo::

            WRITEME
        """
        return ['cuda_convnet']

    def c_code_cache_version(self):
        """
        .. todo::

            WRITEME
        """
        return (1,)

    def _argument_contiguity_check(self, arg_name):
        """
        .. todo::

            WRITEME
        """
        return """
        if (!CudaNdarray_is_c_contiguous(%%(%(arg_name)s)s))
        {
            if (!(%(class_name_caps)s_COPY_NON_CONTIGUOUS)) {
                PyErr_SetString(PyExc_ValueError,
                    "%(class)s: %(arg_name)s must be C contiguous");
                %%(fail)s;
            }
        }
        """ % {
            'class': self.__class__.__name__,
            'arg_name': arg_name,
            'class_name_caps': self.__class__.__name__.upper(),
        }

    def make_node(self, images):
        """
        .. todo::

            WRITEME
        """
        images = as_cuda_ndarray_variable(images)

        assert images.ndim == 4

        channels_broadcastable = images.type.broadcastable[0]
        batch_broadcastable = images.type.broadcastable[3]

        rows_broadcastable = False
        cols_broadcastable = False

        targets_broadcastable = (channels_broadcastable, rows_broadcastable,
                cols_broadcastable, batch_broadcastable)
        targets_type = CudaNdarrayType(broadcastable=targets_broadcastable)
        targets = targets_type()
        seed = self.seed_state
        seed = as_cuda_ndarray_variable(seed)
        return Apply(self, [images, seed], [targets])

    def c_code(self, node, name, inputs, outputs, sub):
        """
        .. todo::

            WRITEME
        """
        images, seed = inputs
        targets, = outputs
        fail = sub['fail']

        # The amount of braces that must be closed at the end
        num_braces = 0

        if self.copy_non_contiguous:
            raise UnimplementedError()
        else:
            basic_setup = "#define STOCHASTICMAXPOOL_COPY_NON_CONTIGUOUS 0\n"

        # Convert images in nv_images, an NVMatrix, for compatibility
        # with the cuda-convnet functions
        setup_nv_images = self._argument_contiguity_check("images") + """
        if (%(images)s->nd != 4)
        {
            PyErr_Format(PyExc_ValueError,
                "images must have nd=4, got nd=%%i", %(images)s->nd);
            %(fail)s;
        }

        { //setup_nv_images brace 1

        const int * images_dims = CudaNdarray_HOST_DIMS(%(images)s);
        const int img_channels = images_dims[0];
        const int imgSizeY = images_dims[1];
        const int imgSizeX = images_dims[2];
        const int batch_size = images_dims[3];

        if(imgSizeY != imgSizeX){
            PyErr_Format(PyExc_ValueError,
                "images must be square(dims[1] == dims[2]). Shape (%%i,%%i,%%i,%%i)",
                img_channels, imgSizeY, imgSizeX, batch_size);
            %(fail)s;
        }
        if(%(ds)s > imgSizeY){
            PyErr_Format(PyExc_ValueError,
                "ds(%%d) must be <= imgSizeX(%%d) and imgSizeY(%%d).",
                %(ds)s, imgSizeX, imgSizeY);
            %(fail)s;
        }
        if(%(start)s >= imgSizeX){
            PyErr_Format(PyExc_ValueError,
                "start is %%d but must be smaller then the images size of %%d x %%d.",
                %(start)s, imgSizeX, imgSizeY);
            %(fail)s;
        }

        NVMatrix nv_images(%(images)s, img_channels * imgSizeY * imgSizeX, batch_size,
        "MaxPool:nv_images");

        //int * seed = CudaNdarray_HOST_DIMS%(seed)s;
        float *  seed = CudaNdarray_DEV_DATA(%(seed)s);
        //int * seed = %(seed)s;
        """
        num_braces += 1

        setup_nv_targets = """
        //int _outputsX = int(ceil((dic['imgSize'] - dic['start'] - dic['sizeX']) / float(dic['stride']))) + 1;
        int _outputsX = ((int)(ceil((imgSizeY - %(start)s - %(ds)s) / ((float)%(stride)s)))) + 1;

        int target_dims [] = {
            img_channels,
            _outputsX,
            _outputsX,
            batch_size };

        if (CudaNdarray_prep_output(& %(targets)s, 4, target_dims))
        {
            %(fail)s;
        }

        { // setup_nv_target brace # 1

        NVMatrix nv_targets(%(targets)s, target_dims[0] * target_dims[1] * target_dims[2],
                            target_dims[3], "MaxPool:nv_targets");

        """

        num_braces += 1

        do_pool = """
        convLocalStochasticMaxPool(nv_images, nv_targets, img_channels, %(ds)s,
                      %(start)s, %(stride)s, _outputsX, MaxPooler(), seed);
        """

        braces = '}' * num_braces

        rval = (basic_setup +
                setup_nv_images +
                setup_nv_targets +
                do_pool +
                braces)
        start = self.start
        stride = self.stride
        ds = self.ds
        rval = rval % locals()

        return rval

    def grad(self, inp, grads):
        """
        .. todo::

            WRITEME
        """
        x, seed = inp
        gz, = grads
        gz = gpu_contiguous(gz)
        maxout = self(x)
        return [MaxPoolGrad(self.ds, self.stride, self.start)(x, maxout, gz), zeros_like(seed)]

    # Make sure the cuda_convnet library is compiled and up-to-date
    def make_thunk(self, node, storage_map, compute_map, no_recycling):
        """
        .. todo::

            WRITEME
        """
        if not convnet_available():
            raise RuntimeError('Could not compile cuda_convnet')

        return super(StochasticMaxPool, self).make_thunk(
                node, storage_map, compute_map, no_recycling)

class WeightedMaxPool(GpuOp):
    """
    This op wrap Alex's MaxPool code on the GPU.
    The input are in the order (channel, image rows, image cols, batch)

    Works only on square images and the grad works only when
    channel % 16 == 0.

    Parameters
    ----------
    ds : int
        defines the size of the pooling region in the x (equivalently, y)
        dimension. Squares of size (ds)2 get reduced to one value by this
        layer.  There are no restrictions on the value of this parameter. It's
        fine for a pooling square to fall off the boundary of the image. Named
        SizeX in Alex's code.
    stride : int
        defines the stride size between successive pooling squares. Setting
        this parameter smaller than sizeX produces overlapping pools. Setting
        it equal to sizeX gives the usual, non-overlapping pools. Values
        greater than sizeX are not allowed.
    start : int, optional
        tells the net where in the input image to start the pooling (in x,y
        coordinates). In principle, you can start anywhere you want. Setting
        this to a positive number will cause the net to discard some pixels at
        the top and at the left of the image. Setting this to a negative number
        will cause it to include pixels that don't exist (which is fine).
        start=0 is the usual setting.
    outputs : int, optional
        allows you to control how many output values in the x (equivalently, y)
        dimension this operation will produce. This parameter is analogous to
        the start parameter, in that it allows you to discard some portion of
        the image by setting it to a value small enough to leave part of the
        image uncovered. Setting it to zero instructs the net to produce as
        many outputs as is necessary to ensure that the whole image is covered.
        default 0
    """

    def __init__(self, ds, stride, start=0, outputs=0):
        self.ds = ds
        self.stride = stride
        self.start = start
        self.copy_non_contiguous = 0
        assert stride > 0 and stride <= ds, (stride, ds)
        assert ds > 0, ds  # We check in the code if ds <= imgSizeX

    def __eq__(self, other):
        """
        .. todo::

            WRITEME
        """
        #Dont put copy_non_contigous as this doesn't change the output
        return (type(self) == type(other) and
                self.ds == other.ds and
                self.stride == other.stride and
                self.start == other.start)

    def __hash__(self):
        """
        .. todo::

            WRITEME
        """
        #Dont put copy_non_contigous as this doesn't change the output
        return (hash(type(self)) ^ hash(self.ds) ^
                hash(self.stride) ^ hash(self.start))

    def c_header_dirs(self):
        """
        .. todo::

            WRITEME
        """
        return [this_dir]

    def c_headers(self):
        """
        .. todo::

            WRITEME
        """
        return ['nvmatrix.cuh', 'conv_util.cuh']

    def c_lib_dirs(self):
        """
        .. todo::

            WRITEME
        """
        return [cuda_convnet_loc]

    def c_libraries(self):
        """
        .. todo::

            WRITEME
        """
        return ['cuda_convnet']

    def c_code_cache_version(self):
        """
        .. todo::

            WRITEME
        """
        return (1,)

    def _argument_contiguity_check(self, arg_name):
        """
        .. todo::

            WRITEME
        """
        return """
        if (!CudaNdarray_is_c_contiguous(%%(%(arg_name)s)s))
        {
            if (!(%(class_name_caps)s_COPY_NON_CONTIGUOUS)) {
                PyErr_SetString(PyExc_ValueError,
                    "%(class)s: %(arg_name)s must be C contiguous");
                %%(fail)s;
            }
        }
        """ % {
            'class': self.__class__.__name__,
            'arg_name': arg_name,
            'class_name_caps': self.__class__.__name__.upper(),
        }

    def make_node(self, images):
        """
        .. todo::

            WRITEME
        """
        images = as_cuda_ndarray_variable(images)

        assert images.ndim == 4

        channels_broadcastable = images.type.broadcastable[0]
        batch_broadcastable = images.type.broadcastable[3]

        rows_broadcastable = False
        cols_broadcastable = False

        targets_broadcastable = (channels_broadcastable, rows_broadcastable,
                cols_broadcastable, batch_broadcastable)
        targets_type = CudaNdarrayType(broadcastable=targets_broadcastable)
        targets = targets_type()

        return Apply(self, [images], [targets])

    def c_code(self, node, name, inputs, outputs, sub):
        """
        .. todo::

            WRITEME
        """
        images, = inputs
        targets, = outputs
        fail = sub['fail']

        # The amount of braces that must be closed at the end
        num_braces = 0

        if self.copy_non_contiguous:
            raise UnimplementedError()
        else:
            basic_setup = "#define WEIGHTEDMAXPOOL_COPY_NON_CONTIGUOUS 0\n"

        # Convert images in nv_images, an NVMatrix, for compatibility
        # with the cuda-convnet functions
        setup_nv_images = self._argument_contiguity_check("images") + """
        if (%(images)s->nd != 4)
        {
            PyErr_Format(PyExc_ValueError,
                "images must have nd=4, got nd=%%i", %(images)s->nd);
            %(fail)s;
        }

        { //setup_nv_images brace 1

        const int * images_dims = CudaNdarray_HOST_DIMS(%(images)s);
        const int img_channels = images_dims[0];
        const int imgSizeY = images_dims[1];
        const int imgSizeX = images_dims[2];
        const int batch_size = images_dims[3];

        if(imgSizeY != imgSizeX){
            PyErr_Format(PyExc_ValueError,
                "images must be square(dims[1] == dims[2]). Shape (%%i,%%i,%%i,%%i)",
                img_channels, imgSizeY, imgSizeX, batch_size);
            %(fail)s;
        }
        if(%(ds)s > imgSizeY){
            PyErr_Format(PyExc_ValueError,
                "ds(%%d) must be <= imgSizeX(%%d) and imgSizeY(%%d).",
                %(ds)s, imgSizeX, imgSizeY);
            %(fail)s;
        }
        if(%(start)s >= imgSizeX){
            PyErr_Format(PyExc_ValueError,
                "start is %%d but must be smaller then the images size of %%d x %%d.",
                %(start)s, imgSizeX, imgSizeY);
            %(fail)s;
        }

        NVMatrix nv_images(%(images)s, img_channels * imgSizeY * imgSizeX, batch_size,
        "MaxPool:nv_images");
        """
        num_braces += 1

        setup_nv_targets = """
        //int _outputsX = int(ceil((dic['imgSize'] - dic['start'] - dic['sizeX']) / float(dic['stride']))) + 1;
        int _outputsX = ((int)(ceil((imgSizeY - %(start)s - %(ds)s) / ((float)%(stride)s)))) + 1;

        int target_dims [] = {
            img_channels,
            _outputsX,
            _outputsX,
            batch_size };

        if (CudaNdarray_prep_output(& %(targets)s, 4, target_dims))
        {
            %(fail)s;
        }

        { // setup_nv_target brace # 1

        NVMatrix nv_targets(%(targets)s, target_dims[0] * target_dims[1] * target_dims[2],
                            target_dims[3], "MaxPool:nv_targets");

        """

        num_braces += 1

        do_pool = """
        convLocalWeightedPool(nv_images, nv_targets, img_channels, %(ds)s,
                      %(start)s, %(stride)s, _outputsX, MaxPooler());
        """

        braces = '}' * num_braces

        rval = (basic_setup +
                setup_nv_images +
                setup_nv_targets +
                do_pool +
                braces)
        start = self.start
        stride = self.stride
        ds = self.ds
        rval = rval % locals()

        return rval

    def grad(self, inp, grads):
        """
        .. todo::

            WRITEME
        """
        raise NotImplementedError()

    # Make sure the cuda_convnet library is compiled and up-to-date
    def make_thunk(self, node, storage_map, compute_map, no_recycling):
        """
        .. todo::

            WRITEME
        """
        if not convnet_available():
            raise RuntimeError('Could not compile cuda_convnet')

        return super(WeightedMaxPool, self).make_thunk(
                node, storage_map, compute_map, no_recycling)

########NEW FILE########
__FILENAME__ = profile_probabilistic_max_pooling
import theano.tensor as T
import numpy as np
from theano import config
from theano import function
import time
from pylearn2.utils import sharedX

from pylearn2.sandbox.cuda_convnet.probabilistic_max_pooling import \
        prob_max_pool_c01b
from pylearn2.expr.probabilistic_max_pooling import max_pool_c01b

def profile(f):
    print 'profiling ',f
    rng = np.random.RandomState([2012,7,19])
    batch_size = 128
    rows = 30
    cols = 30
    channels = 16
    pool_rows = 3
    pool_cols = 3
    zv = rng.randn(channels, rows, cols, batch_size).astype(config.floatX)

    # put the inputs + outputs in shared variables so we don't pay GPU
    # transfer during test
    p_shared = sharedX(zv[:,0:rows:pool_rows,0:cols:pool_cols,:])
    h_shared = sharedX(zv)
    z_shared = sharedX(zv)

    p_th, h_th = f( z_shared, (pool_rows, pool_cols) )

    func = function([],updates = { p_shared : p_th, h_shared : h_th} )

    print 'warming up'
    for i in xrange(10):
        func()

    trials = 10
    results = []

    for i in xrange(trials):
        t1 = time.time()
        for j in xrange(10):
            func()
        t2 = time.time()
        print t2 - t1
        results.append(t2-t1)
    print 'final: ',sum(results)/float(trials)

def profile_grad(f):
    print 'profiling gradient of ',f
    rng = np.random.RandomState([2012,7,19])
    batch_size = 128
    rows = 9
    cols = 9
    channels = 16
    pool_rows = 3
    pool_cols = 3
    zv = rng.randn(channels, rows, cols, batch_size).astype(config.floatX)

    # put the inputs + outputs in shared variables so we don't pay GPU
    # transfer during test
    grad_shared = sharedX(zv)
    z_shared = sharedX(zv)

    p_th, h_th = f( z_shared, (pool_rows, pool_cols) )

    func = function([],updates = { grad_shared : T.grad(p_th.sum() +
        h_th.sum(), z_shared)} )

    print 'warming up'
    for i in xrange(10):
        func()

    trials = 10
    results = []

    for i in xrange(trials):
        t1 = time.time()
        for j in xrange(10):
            func()
        t2 = time.time()
        print t2 - t1
        results.append(t2-t1)
    print 'final: ',sum(results)/float(trials)

if __name__ == '__main__':
    profile(prob_max_pool_c01b)
    profile(max_pool_c01b)
    profile_grad(prob_max_pool_c01b)
    profile_grad(max_pool_c01b)


########NEW FILE########
__FILENAME__ = test_common
__authors__ = "Ian Goodfellow, David Warde-Farley"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow, David Warde-Farley"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

from pylearn2.testing.skip import skip_if_no_gpu
skip_if_no_gpu()
import numpy as np
from theano import shared
from pylearn2.sandbox.cuda_convnet.filter_acts import FilterActs
from pylearn2.sandbox.cuda_convnet.img_acts import ImageActs
from theano.sandbox.cuda import gpu_from_host
from theano import function
from theano.tensor import as_tensor_variable


def test_reject_rect():
    for cls in (FilterActs, ImageActs):
        # Tests that running FilterActs with a non-square
        # kernel is an error
        rng = np.random.RandomState([2012, 10, 9])
        batch_size = 5
        rows = 10
        cols = 9
        channels = 3
        filter_rows = 4
        filter_cols = filter_rows + 1
        num_filters = 6

        images = shared(rng.uniform(-1., 1., (channels, rows, cols,
            batch_size)).astype('float32'), name='images')
        filters = shared(rng.uniform(-1., 1., (channels, filter_rows,
            filter_cols, num_filters)).astype('float32'), name='filters')

        gpu_images = gpu_from_host(images)
        gpu_filters = gpu_from_host(filters)

        if cls is ImageActs:
            output = cls()(gpu_images, gpu_filters,
                        as_tensor_variable((rows, cols)))
        else:
            output = cls()(gpu_images, gpu_filters)

        f = function([], output)
        try:
            output = f()
        except ValueError:
            continue
        assert False


def test_reject_bad_filt_number():
    for cls in (FilterActs, ImageActs):
        # Tests that running FilterActs with a # of filters per
        # group that is not 16 is an error
        rng = np.random.RandomState([2012, 10, 9])
        batch_size = 5
        rows = 10
        cols = 9
        channels = 3
        filter_rows = 4
        filter_cols = filter_rows
        num_filters = 6

        images = shared(rng.uniform(-1., 1., (channels, rows, cols,
            batch_size)).astype('float32'), name='images')
        filters = shared(rng.uniform(-1., 1., (channels, filter_rows,
            filter_cols, num_filters)).astype('float32'), name='filters')

        gpu_images = gpu_from_host(images)
        gpu_filters = gpu_from_host(filters)

        if cls is ImageActs:
            output = cls()(gpu_images, gpu_filters,
                           as_tensor_variable((rows, cols)))
        else:
            output = cls()(gpu_images, gpu_filters)
        f = function([], output)
        try:
            output = f()
        except ValueError:
            continue
        assert False

########NEW FILE########
__FILENAME__ = test_filter_acts
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow", "David Warde-Farley"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

from pylearn2.testing.skip import skip_if_no_gpu
skip_if_no_gpu()

import numpy as np
from theano import shared
from theano.tensor import grad, constant
from pylearn2.sandbox.cuda_convnet.filter_acts import FilterActs
from theano.sandbox.cuda import gpu_from_host
from theano.sandbox.cuda import host_from_gpu
from theano.sandbox.rng_mrg import MRG_RandomStreams
from theano.tensor.nnet.conv import conv2d
from theano import function
from theano import tensor as T
import warnings


def test_match_valid_conv():

    # Tests that running FilterActs with no padding is the same as running
    # theano's conv2D in valid mode

    rng = np.random.RandomState([2012,10,9])

    batch_size = 5
    rows = 10
    cols = 9
    channels = 3
    filter_rows = 4
    filter_cols = filter_rows
    num_filters = 16

    images = shared(rng.uniform(-1., 1., (channels, rows, cols,
        batch_size)).astype('float32'), name='images')
    filters = shared(rng.uniform(-1., 1., (channels, filter_rows,
        filter_cols, num_filters)).astype('float32'), name='filters')

    gpu_images = gpu_from_host(images)
    gpu_filters = gpu_from_host(filters)

    output = FilterActs()(gpu_images, gpu_filters)
    output = host_from_gpu(output)

    images_bc01 = images.dimshuffle(3,0,1,2)
    filters_bc01 = filters.dimshuffle(3,0,1,2)
    filters_bc01 = filters_bc01[:,:,::-1,::-1]

    output_conv2d = conv2d(images_bc01, filters_bc01,
            border_mode='valid')

    output_conv2d = output_conv2d.dimshuffle(1,2,3,0)

    f = function([], [output, output_conv2d])

    output, output_conv2d = f()

    warnings.warn("""test_match_valid_conv success criterion is not very strict. Can we verify that this is OK?
                     One possibility is that theano is numerically unstable and Alex's code is better.
                     Probably theano CPU 64 bit is OK but it's worth checking the others.""")
    if np.abs(output - output_conv2d).max() > 2.4e-6:
        assert type(output) == type(output_conv2d)
        assert output.dtype == output_conv2d.dtype
        if output.shape != output_conv2d.shape:
            print 'cuda-convnet shape: ',output.shape
            print 'theano shape: ',output_conv2d.shape
            assert False
        err = np.abs(output - output_conv2d)
        print 'absolute error range: ', (err.min(), err.max())
        print 'mean absolute error: ', err.mean()
        print 'cuda-convnet value range: ', (output.min(), output.max())
        print 'theano value range: ', (output_conv2d.min(), output_conv2d.max())
        assert False


def test_match_valid_conv_strided():

    # Tests that running FilterActs with stride is the same as running
    # theano's conv2D in valid mode and then downsampling

    rng = np.random.RandomState([2012,10,9])

    batch_size = 5
    rows = 9
    cols = 9
    channels = 3
    filter_rows = 3
    filter_cols = filter_rows
    stride = 3
    num_filters = 16

    images = shared(rng.uniform(-1., 1., (channels, rows, cols,
        batch_size)).astype('float32'), name='images')
    filters = shared(rng.uniform(-1., 1., (channels, filter_rows,
        filter_cols, num_filters)).astype('float32'), name='filters')

    gpu_images = gpu_from_host(images)
    gpu_filters = gpu_from_host(filters)

    output = FilterActs(stride=stride)(gpu_images, gpu_filters)
    output = host_from_gpu(output)

    images_bc01 = images.dimshuffle(3,0,1,2)
    filters_bc01 = filters.dimshuffle(3,0,1,2)
    filters_bc01 = filters_bc01[:,:,::-1,::-1]

    output_conv2d = conv2d(images_bc01, filters_bc01,
            border_mode='valid', subsample=(stride, stride))

    output_conv2d_orig = output_conv2d.dimshuffle(1,2,3,0)
    output_conv2d = output_conv2d_orig  # [:, ::stride, ::stride, :]
    f = function([], [output, output_conv2d, output_conv2d_orig])

    output, output_conv2d, output_conv2d_orig = f()

    warnings.warn("""test_match_valid_conv success criterion is not very strict. Can we verify that this is OK?
                     One possibility is that theano is numerically unstable and Alex's code is better.
                     Probably theano CPU 64 bit is OK but it's worth checking the others.""")
    if np.abs(output - output_conv2d).max() > 2.4e-6:
        assert type(output) == type(output_conv2d)
        assert output.dtype == output_conv2d.dtype
        if output.shape != output_conv2d.shape:
            print 'cuda-convnet shape: ',output.shape
            print 'theano shape: ',output_conv2d.shape
            assert False
        err = np.abs(output - output_conv2d)
        print 'absolute error range: ', (err.min(), err.max())
        print 'mean absolute error: ', err.mean()
        print 'cuda-convnet value range: ', (output.min(), output.max())
        print 'theano value range: ', (output_conv2d.min(), output_conv2d.max())
        assert False


def test_match_valid_conv_padded():

    # Tests that running FilterActs with no padding is the same as running
    # theano's conv2D in valid mode

    rng = np.random.RandomState([2012,10,9])

    batch_size = 5
    rows = 10
    cols = 9
    channels = 3
    filter_rows = 4
    filter_cols = filter_rows
    num_filters = 16

    images = shared(rng.uniform(-1., 1., (channels, rows, cols,
        batch_size)).astype('float32'), name='images')
    filters = shared(rng.uniform(-1., 1., (channels, filter_rows,
        filter_cols, num_filters)).astype('float32'), name='filters')

    gpu_images = gpu_from_host(images)
    gpu_filters = gpu_from_host(filters)

    PAD = 3

    output = FilterActs(PAD)(gpu_images, gpu_filters)
    output = host_from_gpu(output)

    images_bc01 = T.alloc(0., batch_size, channels, rows + PAD * 2, cols + PAD * 2)

    images_bc01 = T.set_subtensor(images_bc01[:,:,PAD:-PAD,PAD:-PAD], images.dimshuffle(3,0,1,2))


    filters_bc01 = filters.dimshuffle(3,0,1,2)
    filters_bc01 = filters_bc01[:,:,::-1,::-1]

    output_conv2d = conv2d(images_bc01, filters_bc01,
            border_mode='valid')

    output_conv2d = output_conv2d.dimshuffle(1,2,3,0)

    f = function([], [output, output_conv2d])

    output, output_conv2d = f()

    warnings.warn("""test_match_valid_conv success criterion is not very strict. Can we verify that this is OK?
                     One possibility is that theano is numerically unstable and Alex's code is better.
                     Probably theano CPU 64 bit is OK but it's worth checking the others.""")

    assert output.shape == output_conv2d.shape

    if np.abs(output - output_conv2d).max() > 2.4e-6:
        assert type(output) == type(output_conv2d)
        assert output.dtype == output_conv2d.dtype
        if output.shape != output_conv2d.shape:
            print 'cuda-convnet shape: ',output.shape
            print 'theano shape: ',output_conv2d.shape
            assert False
        err = np.abs(output - output_conv2d)
        print 'absolute error range: ', (err.min(), err.max())
        print 'mean absolute error: ', err.mean()
        print 'cuda-convnet value range: ', (output.min(), output.max())
        print 'theano value range: ', (output_conv2d.min(), output_conv2d.max())
        assert False


def test_grad():
    rng = np.random.RandomState([2012, 10, 9])
    batch_size = 5
    rows = 10
    cols = 9
    channels = 3
    filter_rows = 4
    filter_cols = filter_rows
    num_filters = 16

    images = shared(rng.uniform(-1., 1., (channels, rows, cols,
        batch_size)).astype('float32'), name='images')
    filters = shared(rng.uniform(-1., 1., (channels, filter_rows,
        filter_cols, num_filters)).astype('float32'), name='filters')

    gpu_images = gpu_from_host(images)
    gpu_filters = gpu_from_host(filters)

    output = FilterActs()(gpu_images, gpu_filters)
    output = host_from_gpu(output)

    # Proper random projection, like verify_grad does.
    cost_weights = rng.normal(size=(num_filters, rows - filter_rows + 1,
                                    cols - filter_cols + 1, batch_size))
    cost = (constant(cost_weights) * output).sum()


    images_bc01 = images.dimshuffle(3,0,1,2)
    filters_bc01 = filters.dimshuffle(3,0,1,2)
    filters_bc01 = filters_bc01[:,:,::-1,::-1]

    output_conv2d = conv2d(images_bc01, filters_bc01,
            border_mode='valid')

    output_conv2d = output_conv2d.dimshuffle(1,2,3,0)
    # XXX: use verify_grad
    images_grad, filters_grad = grad(cost.sum(), [images, filters])
    reference_cost = (constant(cost_weights) * output_conv2d).sum()
    images_conv2d_grad, filters_conv2d_grad = grad(reference_cost,
                                                  [images, filters])
    f = function([], [images_grad, filters_grad,
                      images_conv2d_grad,
                      filters_conv2d_grad])

    images_grad, filters_grad, images_conv2d_grad, filters_conv2d_grad = f()

    warnings.warn("""test_match_valid_conv success criterion is not very strict. Can we verify that this is OK?
                     One possibility is that theano is numerically unstable and Alex's code is better.
                     Probably theano CPU 64 bit is OK but it's worth checking the others.""")
    # XXX: Refactor
    if np.abs(images_grad - images_conv2d_grad).max() > 1.15e-5:
        print "=== IMAGES GRADIENT ==="
        assert type(images_grad) == type(images_conv2d_grad)
        assert images_grad.dtype == images_conv2d_grad.dtype
        if images_grad.shape != images_conv2d_grad.shape:
            print 'cuda-convnet shape: ',images_grad.shape
            print 'theano shape: ',images_conv2d_grad.shape
            assert False
        err = np.abs(images_grad - images_conv2d_grad)
        print 'absolute error range: ', (err.min(), err.max())
        print 'mean absolute error: ', err.mean()
        print 'cuda-convnet value range: ', (images_grad.min(),
                                             images_grad.max())
        print 'theano value range: ', (images_conv2d_grad.min(),
                                       images_conv2d_grad.max())
        assert False
    if np.abs(filters_grad - filters_conv2d_grad).max() > 1.15e-5:
        print "=== FILTERS GRADIENT ==="
        assert type(filters_grad) == type(filters_conv2d_grad)
        assert filters_grad.dtype == filters_conv2d_grad.dtype
        if filters_grad.shape != filters_conv2d_grad.shape:
            print 'cuda-convnet shape: ',filters_grad.shape
            print 'theano shape: ',filters_conv2d_grad.shape
            assert False
        err = np.abs(filters_grad - filters_conv2d_grad)
        print 'absolute error range: ', (err.min(), err.max())
        print 'mean absolute error: ', err.mean()
        print 'cuda-convnet value range: ', (filters_grad.min(),
                                             filters_grad.max())
        print 'theano value range: ', (filters_conv2d_grad.min(),
                                       filters_conv2d_grad.max())
        assert False


def test_grad_strided():
    rng = np.random.RandomState([2012, 10, 9])
    batch_size = 5
    rows = 9
    cols = 9
    channels = 3
    filter_rows = 3
    filter_cols = filter_rows
    num_filters = 16
    stride = 3

    images = shared(rng.uniform(-1., 1., (channels, rows, cols,
        batch_size)).astype('float32'), name='images')
    filters = shared(rng.uniform(-1., 1., (channels, filter_rows,
        filter_cols, num_filters)).astype('float32'), name='filters')

    gpu_images = gpu_from_host(images)
    gpu_filters = gpu_from_host(filters)

    output = FilterActs(stride=stride)(gpu_images, gpu_filters)
    output = host_from_gpu(output)

    images_bc01 = images.dimshuffle(3,0,1,2)
    filters_bc01 = filters.dimshuffle(3,0,1,2)
    filters_bc01 = filters_bc01[:,:,::-1,::-1]

    output_conv2d = conv2d(images_bc01, filters_bc01,
            border_mode='valid', subsample=(stride, stride))
    output_conv2d = output_conv2d.dimshuffle(1,2,3,0)


    checker = function([], [output, output_conv2d])
    output_numpy, output_conv2d_numpy = checker()
    if output_numpy.shape != output_conv2d_numpy.shape:
        raise AssertionError("theano and cuda convnet follow different conventions for this input size, so we can't test cuda convnet by matching it against theano for these inputs")

    # Proper random projection, like verify_grad does.
    theano_rng = MRG_RandomStreams(2013*5*4)
    cost_weights = theano_rng.normal(size=output_conv2d.shape, dtype=output_conv2d.dtype)
    cost = (cost_weights * output).sum()


    # XXX: use verify_grad
    images_grad, filters_grad = grad(cost, [images, filters])
    reference_cost = (cost_weights * output_conv2d).sum()
    images_conv2d_grad, filters_conv2d_grad = grad(reference_cost, [images, filters])

    f = function([], [images_grad, filters_grad, images_conv2d_grad,
                      filters_conv2d_grad])

    images_grad, filters_grad, images_conv2d_grad, filters_conv2d_grad = f()

    warnings.warn("""test_match_valid_conv success criterion is not very strict. Can we verify that this is OK?
                     One possibility is that theano is numerically unstable and Alex's code is better.
                     Probably theano CPU 64 bit is OK but it's worth checking the others.""")
    # XXX: Refactor
    if np.abs(images_grad - images_conv2d_grad).max() > 1.15e-5:
        print "=== IMAGES GRADIENT ==="
        assert type(images_grad) == type(images_conv2d_grad)
        assert images_grad.dtype == images_conv2d_grad.dtype
        if images_grad.shape != images_conv2d_grad.shape:
            print 'cuda-convnet shape: ',images_grad.shape
            print 'theano shape: ',images_conv2d_grad.shape
            assert False
        err = np.abs(images_grad - images_conv2d_grad)
        print 'absolute error range: ', (err.min(), err.max())
        print 'mean absolute error: ', err.mean()
        print 'cuda-convnet value range: ', (images_grad.min(),
                                             images_grad.max())
        print 'theano value range: ', (images_conv2d_grad.min(),
                                       images_conv2d_grad.max())
        assert False
    if np.abs(filters_grad - filters_conv2d_grad).max() > 1e-5:
        print "=== FILTERS GRADIENT ==="
        assert type(filters_grad) == type(filters_conv2d_grad)
        assert filters_grad.dtype == filters_conv2d_grad.dtype
        if filters_grad.shape != filters_conv2d_grad.shape:
            print 'cuda-convnet shape: ',filters_grad.shape
            print 'theano shape: ',filters_conv2d_grad.shape
            assert False
        err = np.abs(filters_grad - filters_conv2d_grad)
        print 'absolute error range: ', (err.min(), err.max())
        print 'mean absolute error: ', err.mean()
        print 'cuda-convnet value range: ', (filters_grad.min(),
                                             filters_grad.max())
        print 'theano value range: ', (filters_conv2d_grad.min(),
                                       filters_conv2d_grad.max())
        assert False


if __name__ == '__main__':
    test_match_valid_conv_padded()


########NEW FILE########
__FILENAME__ = test_filter_acts_strided
__authors__ = "Heng Luo"

from pylearn2.testing.skip import skip_if_no_gpu
skip_if_no_gpu()

import numpy as np
from theano import shared
from theano.tensor import grad, constant
from pylearn2.sandbox.cuda_convnet.filter_acts import FilterActs
from theano.sandbox.cuda import gpu_from_host
from theano.sandbox.cuda import host_from_gpu
from theano.sandbox.rng_mrg import MRG_RandomStreams
from theano.tensor.nnet.conv import conv2d
from theano import function
from theano import tensor as T
import warnings
from theano.sandbox import cuda
from theano.sandbox.cuda.var import float32_shared_constructor 

def FilterActs_python(images,
                      filters,
                      stride=1,
                      ):      
    if int(stride) != stride:
        raise TypeError('stride must be an int', stride)
    stride = int(stride)

    channels, rows, cols, batch_size = images.shape    
    _channels, filter_rows, filter_cols, num_filters = filters.shape
    assert rows >= filter_rows
    assert cols >= filter_cols
    assert filter_cols == filter_rows    
    assert channels == _channels
    assert stride <= filter_rows and stride >= 1
    
    if stride > 1:
        if (rows - filter_rows)%stride == 0:
            stride_padding_rows = 0
        else:
            stride_padding_rows = ((rows - filter_rows)/stride + 1)*stride + filter_rows - rows
        idx_rows = (rows + stride_padding_rows - filter_rows)/stride 
        if (cols - filter_cols)%stride == 0:
            stride_padding_cols = 0
        else:
            stride_padding_cols = ((cols - filter_cols)/stride + 1)*stride + filter_cols - cols
        idx_cols = (cols + stride_padding_cols - filter_cols)/stride
                
        new_rows = rows + stride_padding_rows
        new_cols = cols + stride_padding_cols
        idx_rows = (new_rows - filter_rows)/stride 
        idx_cols = (new_cols - filter_cols)/stride
        
        new_images = np.zeros((channels, new_rows, new_cols, batch_size),dtype='float32')
        new_images[:,:rows,:cols,:] = images
        h_shape = (num_filters,
                   idx_rows+1,
                   idx_cols+1,
                   batch_size
                  )
    else:
        new_images = images
        h_shape = (num_filters,
                   rows - filter_rows + 1,
                   cols - filter_cols + 1,
                   batch_size
                  )
    h = np.zeros(h_shape,dtype='float32')
    n_dim_filter = channels*filter_rows*filter_cols
    vector_filters = filters.reshape(n_dim_filter,num_filters).T
    
    for idx_h_rows in xrange(h_shape[1]):
        for idx_h_cols in xrange(h_shape[2]):
                rc_images = new_images[:,
                                       idx_h_rows*stride:idx_h_rows*stride+filter_rows,
                                       idx_h_cols*stride:idx_h_cols*stride+filter_cols,
                                       :]                                  
                rc_hidacts = np.dot(
                        vector_filters,
                        rc_images.reshape(n_dim_filter, batch_size))
                h[:,idx_h_rows,idx_h_cols,:] = rc_hidacts  
                #import pdb;pdb.set_trace()
    return h

def test_filter_acts_strided():

    # Tests that FilterActs with all possible strides 

    rng = np.random.RandomState([2012,10,9])

    #Each list in shape_list : 
    #[img_shape,filter_shape]
    #[(channels, rows, cols, batch_size),(channels, filter_rows, filter_cols, num_filters)]
    shape_list = [[(1, 7, 8, 5),     (1, 2, 2, 16)],
                  [(3, 7, 8, 5),     (3, 3, 3, 16)],
                  [(16, 11, 11, 4),  (16, 4, 4, 16)], 
                  [(3, 20, 20, 3),   (3, 5, 5, 16)],
                  [(3, 21, 21, 3),   (3, 6, 6, 16)],
                  ]

    for test_idx in xrange(len(shape_list)):
        images = rng.uniform(-1., 1., shape_list[test_idx][0]).astype('float32')
        filters = rng.uniform(-1., 1., shape_list[test_idx][1]).astype('float32')
        gpu_images = float32_shared_constructor(images,name='images')
        gpu_filters = float32_shared_constructor(filters,name='filters')
        print "test case %d..."%(test_idx+1) 
        
        for ii in xrange(filters.shape[1]):
            stride = ii + 1
            
            output = FilterActs(stride=stride)(gpu_images, gpu_filters)
            output = host_from_gpu(output)
            f = function([], output)
            output_val = f()
        
            output_python = FilterActs_python(images,filters,stride)
                        
            if np.abs(output_val - output_python).max() > 8.6e-6:
                assert type(output_val) == type(output_python)
                assert output_val.dtype == output_python.dtype
                if output_val.shape != output_python.shape:
                    print 'cuda-convnet shape: ',output_val.shape
                    print 'python conv shape: ',output_python.shape
                    assert False
                err = np.abs(output_val - output_python)
                print 'stride %d'%stride
                print 'absolute error range: ', (err.min(), err.max())
                print 'mean absolute error: ', err.mean()
                print 'cuda-convnet value range: ', (output_val.min(), output_val.max())
                print 'python conv value range: ', (output_python.min(), output_python.max())
                #assert False 
        #print "pass"         
               
if __name__ == '__main__':
    test_filter_acts_strided()







########NEW FILE########
__FILENAME__ = test_image_acts_strided
__authors__ = "Heng Luo"

from pylearn2.testing.skip import skip_if_no_gpu
skip_if_no_gpu()

import numpy as np
from theano import shared
from theano.tensor import grad, constant
from pylearn2.sandbox.cuda_convnet.filter_acts import FilterActs
from pylearn2.sandbox.cuda_convnet.filter_acts import ImageActs
from theano.sandbox.cuda import gpu_from_host
from theano.sandbox.cuda import host_from_gpu
from theano.sandbox.rng_mrg import MRG_RandomStreams
from theano.tensor.nnet.conv import conv2d
from theano.tensor import as_tensor_variable
from theano import function
from theano import tensor as T
import warnings
from theano.sandbox import cuda
from theano.sandbox.cuda.var import float32_shared_constructor 

from test_filter_acts_strided import FilterActs_python

def ImageActs_python(filters,
                     hidacts,
                     stride=1,
                     img_shape=None,
                    ):
    if int(stride) != stride:
        raise TypeError('stride must be an int', stride)
    stride = int(stride)

    num_filters, h_rows, h_cols, batch_size = hidacts.shape    
    channels, filter_rows, filter_cols, _num_filters = filters.shape  
    assert filter_cols == filter_cols
    assert num_filters == _num_filters
    assert stride <= filter_rows and stride >= 1
    
    if stride > 1:
        assert img_shape!= None     
        rows, cols = img_shape 
        if (rows - filter_rows)%stride == 0:
            stride_padding_rows = 0
        else:
            stride_padding_rows = ((rows - filter_rows)/stride + 1)*stride + filter_rows - rows
        idx_rows = (rows + stride_padding_rows - filter_rows)/stride 
        if (cols - filter_cols)%stride == 0:
            stride_padding_cols = 0
        else:
            stride_padding_cols = ((cols - filter_cols)/stride + 1)*stride + filter_cols - cols
        idx_cols = (cols + stride_padding_cols - filter_cols)/stride
                
        new_rows = rows + stride_padding_rows
        new_cols = cols + stride_padding_cols
        idx_rows = (new_rows - filter_rows)/stride 
        idx_cols = (new_cols - filter_cols)/stride
        images = np.zeros((channels,new_rows,new_cols,batch_size),dtype='float32')
    else:
        rows = h_rows+filter_rows-1
        cols = h_cols+filter_cols-1
        img_shape = (channels, 
                     rows, 
                     cols, 
                     batch_size)
        images = np.zeros(img_shape,dtype='float32')
    
    n_dim_filter = channels*filter_rows*filter_cols
    vector_filters = filters.reshape(n_dim_filter,num_filters).T
    
    for idx_h_rows in xrange(h_rows):
        for idx_h_cols in xrange(h_cols):
                rc_hidacts = hidacts[:,idx_h_rows,idx_h_cols,:]
                rc_image = (np.dot(
                                  rc_hidacts.T,
                                  vector_filters).T).reshape(channels,filter_rows,filter_cols,batch_size)
                images[:,
                       idx_h_rows*stride:idx_h_rows*stride+filter_rows,
                       idx_h_cols*stride:idx_h_cols*stride+filter_cols,
                       :] += rc_image  
    rval = images[:,:rows,:cols,:] 
    return rval                
                          

def test_image_acts_strided():

    # Tests that running FilterActs with all possible strides 

    rng = np.random.RandomState([2012,10,9])

    #Each list in shape_list : 
    #[img_shape,filter_shape]
    #[(channels, rows, cols, batch_size),(channels, filter_rows, filter_cols, num_filters)]
    shape_list = [[(1, 7, 8, 5),     (1, 2, 2, 16)],
                  [(3, 7, 8, 5),     (3, 3, 3, 16)],
                  [(16, 11, 11, 4),  (16, 4, 4, 16)], 
                  [(3, 20, 20, 3),   (3, 5, 5, 16)],
                  [(3, 21, 21, 3),   (3, 6, 6, 16)],
                  ]

    for test_idx in xrange(len(shape_list)):
        images = rng.uniform(-1., 1., shape_list[test_idx][0]).astype('float32')
        filters = rng.uniform(-1., 1., shape_list[test_idx][1]).astype('float32')
        gpu_images = float32_shared_constructor(images,name='images')
        gpu_filters = float32_shared_constructor(filters,name='filters')
        print "test case %d..."%(test_idx+1) 
        
        for ii in xrange(filters.shape[1]):
            stride = ii + 1
                   
            output_python = FilterActs_python(images,filters,stride)
            hidacts = rng.uniform(-1., 1., output_python.shape).astype('float32')
            gpu_hidacts = float32_shared_constructor(hidacts,name='hidacts')
            Img_output_python = ImageActs_python(filters,hidacts,stride,(images.shape[1], images.shape[2]))            
            
            Img_output = ImageActs(stride=stride)(gpu_hidacts, gpu_filters, as_tensor_variable((images.shape[1], images.shape[2])))
            Img_output = host_from_gpu(Img_output)
            f = function([], Img_output)
            Img_output_val = f()
            
            warnings.warn("""test_image_acts_strided success criterion is not very strict.""")
            
            if np.abs(Img_output_val - Img_output_python).max() > 2.1e-5:
                assert type(Img_output_val) == type(Img_output_python)
                assert Img_output_val.dtype == Img_output_python.dtype
                if Img_output_val.shape != Img_output_python.shape:
                    print 'cuda-convnet shape: ',Img_output_val.shape
                    print 'python conv shape: ',Img_output_python.shape
                    assert False
                err = np.abs(Img_output_val - Img_output_python)
                print 'stride %d'%stride
                print 'absolute error range: ', (err.min(), err.max())
                print 'mean absolute error: ', err.mean()
                print 'cuda-convnet value range: ', (Img_output_val.min(), Img_output_val.max())
                print 'python conv value range: ', (Img_output_python.min(), Img_output_python.max())    
                #assert False 
        #print "pass"         
               
if __name__ == '__main__':
    test_image_acts_strided()







########NEW FILE########
__FILENAME__ = test_img_acts
__authors__ = "David Warde-Farley, Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["David Warde-Farley", "Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "David Warde-Farley"
__email__ = "wardefar@iro"

from pylearn2.testing.skip import skip_if_no_gpu
skip_if_no_gpu()

import numpy as np
import warnings

from theano import function
from theano.sandbox.cuda import gpu_from_host
from theano.sandbox.cuda import host_from_gpu
from theano.sandbox.rng_mrg import MRG_RandomStreams
from theano import shared
from theano import tensor as T
from theano.tensor import as_tensor_variable
from theano.tensor.nnet.conv import conv2d

from pylearn2.sandbox.cuda_convnet.img_acts import ImageActs

def test_match_full_conv():

    # Tests that running ImageActs with no padding is the same as running
    # theano's conv2D in full mode after flipping the kernel and tranposing
    # the output and input channels
    # In other words, if convolution computes H=XK, we now compute
    # R=HK^T

    rng = np.random.RandomState([2013, 1, 29])

    batch_size = 2
    rows = 6
    cols = 7
    channels = 3
    filter_rows = 5
    filter_cols = filter_rows
    num_filters = 16

    hid_acts = shared(rng.uniform(-1., 1., (num_filters,
                                            rows - filter_rows + 1,
                                            cols - filter_cols + 1,
                                            batch_size)
    ).astype('float32'), name='hidacts')

    filters = shared(rng.uniform(-1., 1., (channels, filter_rows,
        filter_cols, num_filters)).astype('float32'), name='filters')

    gpu_images = gpu_from_host(hid_acts)
    gpu_filters = gpu_from_host(filters)

    output = ImageActs()(gpu_images, gpu_filters, as_tensor_variable((6, 7)))
    output = host_from_gpu(output)

    images_bc01 = hid_acts.dimshuffle(3,0,1,2)
    filters_bc01 = filters.dimshuffle(3,0,1,2)
    # need to tranpose the kernel stack to do imgActs rather than filterActs
    filters_bc01 = filters_bc01.dimshuffle(1, 0, 2, 3)
    # In order to do the transpose operation, we must flip the kernels
    # But in theano's conv2d, the kernels get flipped anyway
    # so in this case, we do not flip the kernel

    output_conv2d = conv2d(images_bc01, filters_bc01, border_mode='full')

    output_conv2d = output_conv2d.dimshuffle(1,2,3,0)

    f = function([], [output, output_conv2d])

    output, output_conv2d = f()

    warnings.warn("""test_match_full_conv success criterion is not very strict. Can we verify that this is OK?
                     One possibility is that theano is numerically unstable and Alex's code is better.
                     Probably theano CPU 64 bit is OK but it's worth checking the others.""")
    if np.abs(output - output_conv2d).max() > 2.4e-6:
        assert type(output) == type(output_conv2d)
        assert output.dtype == output_conv2d.dtype
        if output.shape != output_conv2d.shape:
            print 'cuda-convnet shape: ',output.shape
            print 'theano shape: ',output_conv2d.shape
            assert False
        err = np.abs(output - output_conv2d)
        print 'absolute error range: ', (err.min(), err.max())
        print 'mean absolute error: ', err.mean()
        print 'cuda-convnet value range: ', (output.min(), output.max())
        print 'theano value range: ', (output_conv2d.min(), output_conv2d.max())
        assert False

def test_match_full_conv_grad():

    # Tests that the gradient of ImageActs with no padding is the same as the
    # gradient of
    # theano's conv2D in full mode after flipping the kernel and tranposing
    # the output and input channels

    rng = np.random.RandomState([2013, 1, 29])

    batch_size = 2
    rows = 6
    cols = 7
    channels = 3
    filter_rows = 5
    filter_cols = filter_rows
    num_filters = 16

    hid_acts = shared(rng.uniform(-1., 1., (num_filters,
                                            rows - filter_rows + 1,
                                            cols - filter_cols + 1,
                                            batch_size)
    ).astype('float32'), name='hidacts')

    filters = shared(rng.uniform(-1., 1., (channels, filter_rows,
        filter_cols, num_filters)).astype('float32'), name='filters')

    gpu_images = gpu_from_host(hid_acts)
    gpu_filters = gpu_from_host(filters)

    output = ImageActs()(gpu_images, gpu_filters, as_tensor_variable((6, 7)))
    output = host_from_gpu(output)

    images_bc01 = hid_acts.dimshuffle(3,0,1,2)
    filters_bc01 = filters.dimshuffle(3,0,1,2)
    # need to tranpose the kernel stack to do imgActs rather than filterActs
    filters_bc01 = filters_bc01.dimshuffle(1, 0, 2, 3)
    # In order to do the transpose operation, we must flip the kernels
    # But in theano's conv2d, the kernels get flipped anyway
    # so in this case, we do not flip the kernel

    output_conv2d = conv2d(images_bc01, filters_bc01, border_mode='full')

    output_conv2d = output_conv2d.dimshuffle(1,2,3,0)

    theano_rng = MRG_RandomStreams(5 * 10 * 2013)

    random = theano_rng.normal(size=output_conv2d.shape, dtype=output_conv2d.dtype)

    projected = (output * random).sum()
    projected_conv_2d = (output_conv2d * random).sum()

    grads = T.grad(projected, [hid_acts, filters]) + T.grad(projected_conv_2d, [hid_acts, filters])

    f = function([], grads)

    gi, gf, gi_th, gf_th = f()

    assert gi.shape == gi_th.shape
    diff = np.abs(gi - gi_th).max()
    if diff > 2.9e-6:
        assert False

    diff = np.abs(gf - gf_th).max()
    if diff > 1.5e-6:
        raise AssertionError(diff)




if __name__ == '__main__':
    test_match_full_conv()


########NEW FILE########
__FILENAME__ = test_pool
import copy

import numpy
import theano
from theano.tensor import grad
from theano.tests import unittest_tools
import theano.sandbox.cuda as tcn

if not tcn.cuda_available:
    from nose.plugins.skip import SkipTest
    raise SkipTest('Optional package cuda disabled.')

from pylearn2.sandbox.cuda_convnet.pool import MaxPool, MaxPoolGrad
from pylearn2.models.mlp import max_pool_c01b as gold_max_pool_c01b


if theano.config.mode == 'FAST_COMPILE':
    mode_with_gpu = theano.compile.mode.get_mode('FAST_RUN').including('gpu')
    mode_without_gpu = theano.compile.mode.get_mode(
            'FAST_RUN').excluding('gpu')
else:
    mode_with_gpu = theano.compile.mode.get_default_mode().including('gpu')
    mode_without_gpu = theano.compile.mode.get_default_mode().excluding('gpu')

#The CPU tests already compare C/Py, so we only check C/GPU
mode_with_gpu = copy.copy(mode_with_gpu)
mode_without_gpu = copy.copy(mode_without_gpu)
mode_with_gpu.check_py_code = False
mode_without_gpu.check_py_code = False


def my_rand(*shape):
    return theano._asarray(numpy.random.rand(*shape), dtype='float32')


def test_pool():
    #(batch, channel, x, y)
    shps = [(1, 1, 2, 2),
            (1, 1, 1, 1),
            (1, 1, 4, 4),
            (1, 2, 2, 2),
            (1, 1, 4, 4),
            (3, 1, 4, 4),
            (1, 5, 4, 4),
            (3, 5, 4, 4),
            (25, 1, 7, 7),
            (1, 1, 12, 12),
            (1, 1, 14, 14),
            (1, 1, 16, 16),
            (1, 1, 18, 18),
            (1, 1, 24, 24),
            (1, 6, 24, 24),
            (10, 1, 24, 24),
            (10, 6, 24, 24),
            (30, 6, 12, 12),
            (30, 2, 24, 24),
            (30, 6, 24, 24),
            (65536, 1, 10, 10),
            #(1, 65536, 10, 10),#crash as too much channel
            (30, 3, 40, 40),
             ]
    shps = [(channel, x, y, batch) for (batch, channel, x, y) in shps]

    #numpy.random.RandomState(unittest_tools.fetch_seed()).shuffle(shps)

    for shp in shps:
        for ds in range(1, min(4, shp[2] + 1)):
#            for start in range(shp[2] + 1):
            for start in [0]:
                for stride in range(1, min(shp[2], ds, 4) + 1):
                    print 'test_pool shape=%s, ds=%d, stride=%d start=%d' % (
                        str(shp), ds, stride, start)

                    a = tcn.shared_constructor(my_rand(*shp), 'a')
                    op = MaxPool(ds=ds, stride=stride)
                    f = theano.function([], op(a),
                                        mode=mode_with_gpu)
                    assert any([isinstance(node.op, MaxPool)
                        for node in f.maker.fgraph.toposort()])
                    out = numpy.asarray(f())

                    #Compute the gold version with a Theano graph.
                    gold_out = gold_max_pool_c01b(a, (ds, ds),
                                                  (stride, stride),
                                                  shp[1:3])
                    f2 = theano.function([], gold_out,
                                         mode=mode_without_gpu)
                    assert not any([isinstance(node.op, MaxPool)
                        for node in f2.maker.fgraph.toposort()])
                    out2 = f2()
                    numpy.testing.assert_allclose(out, out2,
                                                  err_msg=str(out - out2))

                    # grad testing
                    # The code support grad only in this case.
                    if shp[0] % 16 != 0:
                        shp2 = list(shp)
                        shp2[0] *= 16
                        # This make it crash due to not enough memory.
                        # On a GPU with 1279M of ram.
                        if numpy.prod(shp2) >= (16 * 10 * 10 * 65536):
                            continue
                        a.set_value(my_rand(*shp2))

                    g = theano.function([],
                                        grad(op(a).sum(), a),
                                        mode=mode_with_gpu)
                    g2 = theano.function([],
                                         grad(gold_out.sum(), a),
                                         mode=mode_without_gpu)
                    assert any([isinstance(node.op, MaxPoolGrad)
                                for node in g.maker.fgraph.toposort()])
                    assert not any([isinstance(node.op, MaxPoolGrad)
                                    for node in g2.maker.fgraph.toposort()])
                    numpy.testing.assert_allclose(g(), g2(), err_msg=str(shp))

                    # Don't call verify_grad. There was problem with
                    # the test and we already assert that 2 version
                    # are equals.  Also, it will be slower to verify
                    # like that then the comparison.
                    continue
                    theano.tests.unittest_tools.verify_grad(op,
                                                            [a.get_value()])

########NEW FILE########
__FILENAME__ = test_probabilistic_max_pooling
import copy
import theano
import numpy as np
import theano.tensor as T
from theano import config
from theano import function
from pylearn2.expr.probabilistic_max_pooling import max_pool_c01b
from pylearn2.sandbox.cuda_convnet.probabilistic_max_pooling import  prob_max_pool_c01b
from pylearn2.utils import float32_floatX
from pylearn2.testing.skip import skip_if_no_gpu

skip_if_no_gpu()

if theano.config.mode == 'FAST_COMPILE':
    mode_with_gpu = theano.compile.mode.get_mode('FAST_RUN').including('gpu')
    mode_without_gpu = theano.compile.mode.get_mode(
            'FAST_RUN').excluding('gpu')
else:
    mode_with_gpu = theano.compile.mode.get_default_mode().including('gpu')
    mode_without_gpu = theano.compile.mode.get_default_mode().excluding('gpu')

#The CPU tests already compare C/Py, so we only check C/GPU
mode_with_gpu = copy.copy(mode_with_gpu)
mode_without_gpu = copy.copy(mode_without_gpu)
mode_with_gpu.check_py_code = False
mode_without_gpu.check_py_code = False

@float32_floatX
def test_correctness():
    """
    Test the forward pass Op against theano graph implementation
    """

    rng = np.random.RandomState([2012,7,19])
    batch_size_list = [1, 5]
    channels = 16
    rows_list = [2, 24]
    pool_rows_list = [2, 3]

    # TODO theano graph version fails with pool shape 1,1,
    # try it with python version

    for batch_size in batch_size_list:
        for rows, pool_rows in zip(rows_list, pool_rows_list):
            cols = rows
            pool_cols = pool_rows

            zv = rng.randn(channels, rows, cols, batch_size).astype(config.floatX)

            z = T.tensor4()

            # gpu op
            p, h = prob_max_pool_c01b(z, (pool_rows, pool_cols) )
            func = function([z], [p, h], mode = mode_with_gpu)

            p_op, h_op = func(zv)

            # theano graph
            p, h = max_pool_c01b(z, (pool_rows, pool_cols) )
            func = function([z], [p, h], mode = mode_without_gpu)

            p_th, h_th = func(zv)

            assert np.allclose(p_op, p_th)
            assert np.allclose(h_op, h_th)

@float32_floatX
def test_top_donw_correctness():
    """
    Test the forward pass Op against theano graph implementation
    """

    rng = np.random.RandomState([2012,7,19])
    batch_size_list = [1]
    channels = 16
    rows_list = [2, 24]
    pool_rows_list = [2, 3]

    # TODO theano graph version fails with pool shape 1,1,
    # try it with python version

    for batch_size in batch_size_list:
        for rows, pool_rows in zip(rows_list, pool_rows_list):
            cols = rows
            pool_cols = pool_rows

            zv = rng.randn(channels, rows, cols, batch_size).astype(config.floatX)
            tv = rng.randn(channels, rows / pool_rows, cols / pool_cols, batch_size).astype(config.floatX)

            z = T.tensor4()
            t = T.tensor4()

            # gpu op
            p, h = prob_max_pool_c01b(z, (pool_rows, pool_cols), top_down = t)
            func = function([z, t], [p, h], mode = mode_with_gpu)

            p_op, h_op = func(zv, tv)

            # theano graph
            p, h = max_pool_c01b(z, (pool_rows, pool_cols), top_down = t)
            func = function([z, t], [p, h], mode = mode_without_gpu)

            p_th, h_th = func(zv, tv)

            assert np.allclose(p_op, p_th)
            assert np.allclose(h_op, h_th)

@float32_floatX
def test_grad():
    """
    Test Op's gradient w.r.t top_down against theano graph implementation
    """

    rng = np.random.RandomState([2012,7,19])
    batch_size_list = [1]
    channels = 16
    rows_list = [2, 24]
    pool_rows_list = [2, 3]

    # TODO theano graph version fails with pool shape 1,1,
    # try it with python version

    for batch_size in batch_size_list:
        for rows, pool_rows in zip(rows_list, pool_rows_list):
            cols = rows
            pool_cols = pool_rows

            zv = rng.randn(channels, rows, cols,
                    batch_size).astype(config.floatX)
            tv = rng.randn(channels, rows / pool_rows, cols / \
                    pool_cols, batch_size).astype(config.floatX)

            z = T.tensor4()
            t = T.tensor4()

            # gpu op
            p, h = prob_max_pool_c01b(z, (pool_rows, pool_cols), top_down = t)
            gh_t = T.grad(h.sum(), t)
            gp_t = T.grad(p.sum(), t)
            gh_z = T.grad(h.sum(), z)
            gp_z = T.grad(p.sum(), z)
            gph_z = T.grad(p.sum() + h.sum(), z)
            gph_t = T.grad(p.sum() + h.sum(), t)

            func = function([z, t], [gh_t, gp_t, gh_z, gp_z, gph_z, gph_t],
                                mode = mode_with_gpu)
            op_rval = func(zv, tv)

            # theano graph
            p, h = max_pool_c01b(z, (pool_rows, pool_cols) , top_down = t)
            gh_t = T.grad(h.sum(), t)
            gp_t = T.grad(p.sum(), t)
            gh_z = T.grad(h.sum(), z)
            gp_z = T.grad(p.sum(), z)
            gph_z = T.grad(p.sum() + h.sum(), z)
            gph_t = T.grad(p.sum() + h.sum(), t)

            func = function([z, t], [gh_t, gp_t, gh_z, gp_z, gph_z, gph_t],
                                mode = mode_without_gpu)
            th_rval = func(zv, tv)

            for op, th in zip (op_rval, th_rval):
                assert np.allclose(op, th, rtol=1e-04, atol=1e-06)


########NEW FILE########
__FILENAME__ = test_response_norm
import numpy
import theano
from nose.plugins.skip import SkipTest
from theano.tests.unittest_tools import verify_grad

try:
    from pylearn2.sandbox.cuda_convnet.response_norm import (
        CrossMapNorm,
        CrossMapNormUndo
    )
    from theano.sandbox.cuda import CudaNdarrayType, CudaNdarray
    from theano.sandbox.cuda import gpu_from_host
except ImportError:
    raise SkipTest('cuda not available')


def test_cross_map_norm_simple():
    op = CrossMapNorm(16, 15. / 16., 1., True)
    x = CudaNdarray(numpy.ones((16, 2, 2, 2), dtype='float32'))
    x_ = theano.tensor.TensorVariable(CudaNdarrayType([False] * 4))
    f = theano.function([x_], op(x_)[0])
    numpy.testing.assert_allclose(f(x), 0.0625)


def test_cross_map_norm_grad_simple():
    rng = numpy.random.RandomState([2013, 02, 10])
    op = CrossMapNorm(16, 15/16., 1, True)
    make_graph = lambda inp: op(gpu_from_host(inp))[0]
    verify = lambda array: verify_grad(make_graph, [array])
    inputs = [numpy.ones((16, 1, 1, 1), dtype='float32'),
              rng.normal(size=(32, 5, 5, 10)).astype('float32')]
    for arr in inputs:
        yield verify, arr

def test_optimization():
    op = CrossMapNorm(16, 15./16., 1, True)
    x_ = theano.tensor.TensorVariable(CudaNdarrayType([False] * 4))
    f = theano.function([x_], theano.grad(op(x_)[0].sum(), x_))
    nodes = [x for x in f.maker.fgraph.apply_nodes
             if type(x.op) == CrossMapNormUndo]
    assert len(nodes) == 1
    assert nodes[0].op.inplace

########NEW FILE########
__FILENAME__ = test_rop_pool
import copy

import numpy
import theano
from theano.tensor import grad
from theano.tests import unittest_tools
import theano.sandbox.cuda as tcn
import warnings

if not tcn.cuda_available:
    from nose.plugins.skip import SkipTest
    raise SkipTest('Optional package cuda disabled.')

from pylearn2.sandbox.cuda_convnet.pool import MaxPool, MaxPoolGrad
from pylearn2.models.mlp import max_pool_c01b as gold_max_pool_c01b


if theano.config.mode == 'FAST_COMPILE':
    mode_with_gpu = theano.compile.mode.get_mode('FAST_RUN').including('gpu')
    mode_without_gpu = theano.compile.mode.get_mode(
            'FAST_RUN').excluding('gpu')
else:
    mode_with_gpu = theano.compile.mode.get_default_mode().including('gpu')
    mode_without_gpu = theano.compile.mode.get_default_mode().excluding('gpu')

#The CPU tests already compare C/Py, so we only check C/GPU
mode_with_gpu = copy.copy(mode_with_gpu)
mode_without_gpu = copy.copy(mode_without_gpu)
mode_with_gpu.check_py_code = False
mode_without_gpu.check_py_code = False


def my_rand(*shape):
    return theano._asarray(numpy.random.rand(*shape), dtype='float32')


def test_pool():
    #(batch, channel, x, y)
    shps = [(1, 1, 2, 2),
             ]
    shps = [(channel, x, y, batch) for (batch, channel, x, y) in shps]

    #numpy.random.RandomState(unittest_tools.fetch_seed()).shuffle(shps)
    warnings.warn("TODO: Razvan needs to finish this")
    for shp in shps:
        for ds in range(1, min(4, shp[2] + 1)):
            for start in [0]:
                for stride in range(1, min(shp[2], ds, 4) + 1):
                    #print 'test_pool shape=%s, ds=%d, stride=%d start=%d' % (
                    #    str(shp), ds, stride, start)

                    va = my_rand(*shp)
                    tva = va.flatten()
                    #print 'va', tva, tva.max(), tva.argmax()

                    vb = my_rand(*shp)
                    tvb = vb.flatten()
                    #print 'vb', tvb, tvb.max(), tvb.argmax(),\
                    #                tvb[tva.argmax()]
                    a = tcn.shared_constructor(va, 'a')
                    b = tcn.shared_constructor(vb, 'b')
                    op = MaxPool(ds=ds, stride=stride)
                    v = op(a)
                    rval = theano.tensor.Rop(v, a, b)
                    f = theano.function([], rval,
                                        mode=mode_with_gpu)
                    print f.maker.fgraph.toposort()
                    #ssert any([isinstance(node.op, MaxPool)
                    #   for node in f.maker.fgraph.toposort()])
                    out = numpy.asarray(f())
                    #print out
                    #print
                    #print


########NEW FILE########
__FILENAME__ = test_stochastic_pool
import copy

import numpy
import theano
from theano.compat.python2x import Counter

from pylearn2.sandbox.cuda_convnet.stochastic_pool import (stochastic_max_pool_c01b,
                                                           weighted_max_pool_c01b)
from pylearn2.testing.skip import skip_if_no_gpu
from pylearn2.utils import float32_floatX

skip_if_no_gpu()

if theano.config.mode == 'FAST_COMPILE':
    mode_with_gpu = theano.compile.mode.get_mode('FAST_RUN').including('gpu')
    mode_without_gpu = theano.compile.mode.get_mode(
        'FAST_RUN').excluding('gpu')
else:
    mode_with_gpu = theano.compile.mode.get_default_mode().including('gpu')
    mode_without_gpu = theano.compile.mode.get_default_mode().excluding('gpu')

#The CPU tests already compare C/Py, so we only check C/GPU
mode_with_gpu = copy.copy(mode_with_gpu)
mode_without_gpu = copy.copy(mode_without_gpu)
mode_with_gpu.check_py_code = False
mode_without_gpu.check_py_code = False

# TODO add unit tests for: seed, differnt shape, stide, batch and channel size


@float32_floatX
def test_stochasatic_pool_samples():
    """
    check if the order of frequency of samples from stochastic max pool
    are same as the order of input values.
    """

    ds = 3
    stride = 3
    rng = numpy.random.RandomState(220)
    data = rng.uniform(0, 10, size=(1, ds, ds, 1)).astype('float32')

    x = theano.tensor.tensor4()
    s_max = stochastic_max_pool_c01b(x, (ds, ds), (stride, stride))
    f = theano.function([x], s_max, mode=mode_with_gpu)

    samples = []
    for i in xrange(300):
        samples.append(numpy.asarray(f(data))[0, 0, 0, 0])

    counts = Counter(samples)
    data = data.reshape(ds*ds)
    data.sort()
    data = data[::-1]
    for i in range(len(data) - 1):
        assert counts[data[i]] >= counts[data[i+1]]


@float32_floatX
def test_weighted_pool():

    # TODO: test with different stride values

    rng = numpy.random.RandomState(220)

    for ds in [9, 2]:
        for batch in [1, 10]:
            for ch in [1, 16]:
                stride = ds
                data = rng.uniform(size=(batch, ds, ds, ch)).astype('float32')

                # op
                x = theano.tensor.tensor4()
                w_max = weighted_max_pool_c01b(x, (ds, ds), (stride, stride))
                f = theano.function([x], w_max, mode=mode_with_gpu)
                op_val = numpy.asarray(f(data))

                # python
                norm = data / data.sum(2).sum(1)[:,
                                                 numpy.newaxis,
                                                 numpy.newaxis, :]
                py_val = (data * norm).sum(2).sum(1)[:, numpy.newaxis,
                                                     numpy.newaxis, :]

                assert numpy.allclose(op_val, py_val)

########NEW FILE########
__FILENAME__ = test_weight_acts
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

from pylearn2.testing.skip import skip_if_no_gpu
skip_if_no_gpu()

import numpy as np
from theano import shared
from pylearn2.sandbox.cuda_convnet.filter_acts import FilterActs
from pylearn2.sandbox.cuda_convnet.weight_acts import WeightActs
from theano.sandbox.cuda import gpu_from_host
from theano.sandbox.cuda import host_from_gpu
from theano.sandbox.rng_mrg import MRG_RandomStreams
import theano.tensor as T
from theano.tensor.nnet.conv import conv2d
from theano.tensor import as_tensor_variable
from theano import function
import warnings

def test_match_grad_valid_conv():

    # Tests that weightActs is the gradient of FilterActs
    # with respect to the weights.

    for partial_sum in [0, 1, 4]:
        rng = np.random.RandomState([2012,10,9])

        batch_size = 3
        rows = 7
        cols = 9
        channels = 8
        filter_rows = 4
        filter_cols = filter_rows
        num_filters = 16

        images = shared(rng.uniform(-1., 1., (channels, rows, cols,
            batch_size)).astype('float32'), name='images')
        filters = shared(rng.uniform(-1., 1., (channels, filter_rows,
            filter_cols, num_filters)).astype('float32'), name='filters')

        gpu_images = gpu_from_host(images)
        gpu_filters = gpu_from_host(filters)

        output = FilterActs(partial_sum=partial_sum)(gpu_images, gpu_filters)
        output = host_from_gpu(output)

        images_bc01 = images.dimshuffle(3,0,1,2)
        filters_bc01 = filters.dimshuffle(3,0,1,2)
        filters_bc01 = filters_bc01[:,:,::-1,::-1]

        output_conv2d = conv2d(images_bc01, filters_bc01,
                border_mode='valid')

        output_conv2d = output_conv2d.dimshuffle(1,2,3,0)

        theano_rng = MRG_RandomStreams(2013 + 1 + 31)

        coeffs = theano_rng.normal(avg=0., std=1., size=output_conv2d.shape, dtype='float32')

        cost_conv2d = (coeffs * output_conv2d).sum()

        weights_grad_conv2d = T.grad(cost_conv2d, filters)

        cost = (coeffs * output).sum()
        hid_acts_grad = T.grad(cost, output)

        weights_grad = WeightActs(partial_sum=partial_sum)(
            gpu_images,
            gpu_from_host(hid_acts_grad),
            as_tensor_variable((4, 4))
        )[0]
        weights_grad = host_from_gpu(weights_grad)

        f = function([], [output, output_conv2d, weights_grad, weights_grad_conv2d])

        output, output_conv2d, weights_grad, weights_grad_conv2d = f()

        if np.abs(output - output_conv2d).max() > 8e-6:
            assert type(output) == type(output_conv2d)
            assert output.dtype == output_conv2d.dtype
            if output.shape != output_conv2d.shape:
                print 'cuda-convnet shape: ',output.shape
                print 'theano shape: ',output_conv2d.shape
                assert False
            err = np.abs(output - output_conv2d)
            print 'absolute error range: ', (err.min(), err.max())
            print 'mean absolute error: ', err.mean()
            print 'cuda-convnet value range: ', (output.min(), output.max())
            print 'theano value range: ', (output_conv2d.min(), output_conv2d.max())
            assert False

        warnings.warn("""test_match_grad_valid_conv success criterion is not very strict. Can we verify that this is OK?
                         One possibility is that theano is numerically unstable and Alex's code is better.
                         Probably theano CPU 64 bit is OK but it's worth checking the others.""")

        if np.abs(weights_grad - weights_grad_conv2d).max() > 8.6e-6:
            if type(weights_grad) != type(weights_grad_conv2d):
                raise AssertionError("weights_grad is of type " + str(weights_grad))
            assert weights_grad.dtype == weights_grad_conv2d.dtype
            if weights_grad.shape != weights_grad_conv2d.shape:
                print 'cuda-convnet shape: ',weights_grad.shape
                print 'theano shape: ',weights_grad_conv2d.shape
                assert False
            err = np.abs(weights_grad - weights_grad_conv2d)
            print 'absolute error range: ', (err.min(), err.max())
            print 'mean absolute error: ', err.mean()
            print 'cuda-convnet value range: ', (weights_grad.min(), weights_grad.max())
            print 'theano value range: ', (weights_grad_conv2d.min(), weights_grad_conv2d.max())
            assert False

if __name__ == '__main__':
    test_match_grad_valid_conv()


########NEW FILE########
__FILENAME__ = test_weight_acts_strided
__authors__ = "Heng Luo"

from pylearn2.testing.skip import skip_if_no_gpu
skip_if_no_gpu()

import numpy as np
from theano import shared
from theano.tensor import grad, constant
from pylearn2.sandbox.cuda_convnet.filter_acts import FilterActs
from pylearn2.sandbox.cuda_convnet.weight_acts import WeightActs
from theano.sandbox.cuda import gpu_from_host
from theano.sandbox.cuda import host_from_gpu
from theano.sandbox.rng_mrg import MRG_RandomStreams
from theano.tensor.nnet.conv import conv2d
from theano.tensor import as_tensor_variable
from theano import function
from theano import tensor as T
import warnings
from theano.sandbox import cuda
from theano.sandbox.cuda.var import float32_shared_constructor 

from test_filter_acts_strided import FilterActs_python

def WeightActs_python(images,
                      hidacts,
                      filter_rows,
                      filter_cols,
                      stride=1,
                      ):
    if int(stride) != stride:
        raise TypeError('stride must be an int', stride)
    stride = int(stride)

    channels, rows, cols, batch_size = images.shape    
    num_filters, hidact_rows, hidact_cols, _batch_size = hidacts.shape
    assert _batch_size == batch_size    
    assert filter_rows == filter_cols
    f_shape = (channels, filter_rows, filter_cols, num_filters)
    f = np.zeros(f_shape,dtype='float32')
    
    if stride > 1:
        if (rows - filter_rows)%stride == 0:
            stride_padding_rows = 0
        else:
            stride_padding_rows = ((rows - filter_rows)/stride + 1)*stride + filter_rows - rows
        idx_rows = (rows + stride_padding_rows - filter_rows)/stride 
        if (cols - filter_cols)%stride == 0:
            stride_padding_cols = 0
        else:
            stride_padding_cols = ((cols - filter_cols)/stride + 1)*stride + filter_cols - cols
        idx_cols = (cols + stride_padding_cols - filter_cols)/stride
                
        new_rows = rows + stride_padding_rows
        new_cols = cols + stride_padding_cols
        idx_rows = (new_rows - filter_rows)/stride 
        idx_cols = (new_cols - filter_cols)/stride
        
        new_images = np.zeros((channels, new_rows, new_cols, batch_size),dtype='float32')
        new_images[:,:rows,:cols,:] = images
    else:
        new_images = images
    
    n_dim_filter = channels*filter_rows*filter_cols
    
    for idx_filters in xrange(num_filters):
        for idx_h_rows in xrange(hidact_rows):
            for idx_h_cols in xrange(hidact_cols):
                rc_images = new_images[:,
                                       idx_h_rows*stride:idx_h_rows*stride+filter_rows,
                                       idx_h_cols*stride:idx_h_cols*stride+filter_cols,
                                      :]                                  
                rc_filters = np.dot(
                                    hidacts[idx_filters,idx_h_rows,idx_h_cols,:].reshape(1,batch_size),
                                    rc_images.reshape(n_dim_filter, batch_size).T)
                f[:,:,:,idx_filters] += rc_filters.reshape(channels, filter_rows, filter_cols)                  
    return f
    
def test_weight_acts_strided():

    # Tests that WeightActs with all possible strides 

    rng = np.random.RandomState([2012,10,9])

    #Each list in shape_list : 
    #[img_shape,filter_shape]
    #[(channels, rows, cols, batch_size),(channels, filter_rows, filter_cols, num_filters)]
    shape_list = [[(1, 7, 8, 5),     (1, 2, 2, 16)],
                  [(3, 7, 8, 5),     (3, 3, 3, 16)],
                  [(16, 11, 11, 4),  (16, 4, 4, 16)], 
                  [(3, 20, 20, 3),   (3, 5, 5, 16)],
                  [(3, 21, 21, 3),   (3, 6, 6, 16)],
                  ]
    for partial_sum in [0, 1, 4]:
        print "partial_sum: %d"%(partial_sum)
        for test_idx in xrange(len(shape_list)):
            images = rng.uniform(-1., 1., shape_list[test_idx][0]).astype('float32')
            filters = rng.uniform(-1., 1., shape_list[test_idx][1]).astype('float32')
            gpu_images = float32_shared_constructor(images,name='images')
            print "test case %d..."%(test_idx+1) 
              
            for ii in xrange(filters.shape[1]):
                stride = ii + 1                            
                output_python = FilterActs_python(images,filters,stride)   
                _, h_rows, h_cols, _ = output_python.shape
                if partial_sum == 4:
                    if (h_rows*h_cols)%partial_sum != 0:
                        print "skip test case %d, stride %d when partial_sum is equal to %d"%(test_idx+1,stride,partial_sum)
                        break
                hidacts = rng.uniform(-1., 1., output_python.shape).astype('float32')
                gpu_hidacts = float32_shared_constructor(hidacts,name='hidacts')
                    
                weights_grad_python = WeightActs_python(images,hidacts,filters.shape[1],filters.shape[2],stride)
                
                weights_grad = WeightActs(partial_sum=partial_sum,stride=stride)(
                                                    gpu_images,
                                                    gpu_hidacts,
                                                    as_tensor_variable((filters.shape[1], filters.shape[2]))
                                                   )[0]
                weights_grad = host_from_gpu(weights_grad)
                f = function([], weights_grad)
                weights_grad_val = f()   
                
                warnings.warn("""test_weight_acts_strided success criterion is not very strict.""")
                
                if np.abs(weights_grad_val - weights_grad_python).max() > 3.4e-5:
                    assert type(weights_grad_val) == type(weights_grad_python)
                    assert weights_grad_val.dtype == weights_grad_python.dtype
                    if weights_grad_val.shape != weights_grad_python.shape:
                        print 'cuda-convnet shape: ',weights_grad_val.shape
                        print 'python conv shape: ',weights_grad_python.shape
                        assert False
                    err = np.abs(weights_grad_val - weights_grad_python)
                    print 'stride %d'%stride
                    print 'absolute error range: ', (err.min(), err.max())
                    print 'mean absolute error: ', err.mean()
                    print 'cuda-convnet value range: ', (weights_grad_val.min(), weights_grad_val.max())
                    print 'python conv value range: ', (weights_grad_python.min(), weights_grad_python.max())
                    #assert False
                #print "stride %d"%stride     
                    
            #print "pass"         
               
if __name__ == '__main__':
    test_weight_acts_strided()







########NEW FILE########
__FILENAME__ = weight_acts
"""
A theano / pylearn2 wrapper for cuda-convnet's convFilterActs function.
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2013, Universite de Montreal"
__credits__ = ["Ian Goodfellow and David Warde-Farley"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

"""
This module may contain code copied directly or modified from cuda-convnet.
The copyright and licensing notice for this code is reproduced below:


/*
 * Copyright (c) 2011, Alex Krizhevsky (akrizhevsky@gmail.com)
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without modification,
 * are permitted provided that the following conditions are met:
 *
 * - Redistributions of source code must retain the above copyright notice,
 *   this list of conditions and the following disclaimer.
 *
 * - Redistributions in binary form must reproduce the above copyright notice,
 *   this list of conditions and the following disclaimer in the documentation
 *   and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
 * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
 * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
 * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
 * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
 * NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,
 * EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

"""

from theano.misc.strutil import render_string
from theano.sandbox.cuda import CudaNdarrayType
from theano.gof import Apply
from pylearn2.sandbox.cuda_convnet.base_acts import BaseActs
from pylearn2.sandbox.cuda_convnet.base_acts import UnimplementedError


class WeightActs(BaseActs):
    """
    Transforms the gradient on the output of FilterActs into the gradient
    on FilterActs' weights.

    This is intended to be a very low-level, performance-oriented op.

    It will not try to fix the input for you. That would slow it down.
    The input must be in the right format. If not, it raises an exception.

    Currently, this op must be inserted manually, not by optimizations.

    Note that the word "input" below refers to the input to FilterActs.

    * images: (input channels, rows, cols, batch_size)
      Input channels must be divisible by 4.
    * hid_grads: (output channels, rows, cols, batch_size)
      Output channels must be a multiple of 16.
    * filters: (input channels, filter rows, filter cols, output channels)
      Filter rows must be the same as filter cols.

    Notes
    -----
    All of these convolution routines are optimized for the case
    when the number of images (i.e. the minibatch size) is a multiple
    of 128. Other batch sizes will work, but Alex "made no attempt
    whatsoever to make them work fast."
    """

    # __eq__ and __hash__ are defined in BaseActs.
    # If you add an __init__ method that adds new members to WeightActs,
    # you may need to implement a new version of __eq__ and __hash__
    # in WeightActs, that considers these parameters.

    def make_node(self, images, hid_grads, output_shape):
        """
        .. todo::

            WRITEME
        """
        if not isinstance(images.type, CudaNdarrayType):
            raise TypeError("WeightActs: expected images.type "
                            "to be CudaNdarrayType, "
                            "got " + str(images.type))

        if not isinstance(hid_grads.type, CudaNdarrayType):
            raise TypeError("WeightActs: expected hid_acts.type "
                            "to be CudaNdarrayType, "
                            "got " + str(hid_grads.type))

        assert images.ndim == 4
        assert hid_grads.ndim == 4

        input_channels_broadcastable = images.type.broadcastable[0]
        # We don't know anything about filter_rows or filter_cols at compile
        # time, so we assume they're not broadcastable.
        filter_rows_broadcastable = False
        filter_cols_broadcastable = False
        output_channels_broadcastable = hid_grads.type.broadcastable[0]

        weights_grads_type = CudaNdarrayType(
                (input_channels_broadcastable,
                 filter_rows_broadcastable,
                 filter_cols_broadcastable,
                 output_channels_broadcastable))

        partial_sums_type = CudaNdarrayType(
            (False,) * 5
        )
        weights_grads = weights_grads_type()
        partial_sums = partial_sums_type()

        return Apply(self, [images, hid_grads, output_shape],
                     [weights_grads, partial_sums])

    def flops(self, inputs, outputs):
        """ Useful with the hack in profilemode to print the MFlops"""
        images, kerns, output_shape = inputs
        out, partial = outputs
        # The partial sum is just a way to specify how to compute
        # stuff inside the op.  It don't change the number of flops.
        assert images[3] == kerns[3]
        # nb mul and add by output pixed
        flops = kerns[1] * kerns[2] * 2
        #nb flops by output image
        flops *= out[1] * out[2]
        # for all outputs images#n_stack==self.imshp[0]
        flops *= images[3] * kerns[0] * images[0]
        return flops

    def c_headers(self):
        """
        .. todo::

            WRITEME
        """
        # For some reason, the function called in the C code (_weightActs)
        # is not defined in cudaconv2.cuh, so I defined it in weight_acts.cuh
        headers = super(WeightActs, self).c_headers()
        headers.append('weight_acts.cuh')
        return headers

    def c_code(self, node, name, inputs, outputs, sub):
        """
        .. todo::

            WRITEME
        """
        partial_sum = self.partial_sum if self.partial_sum is not None else 0
        images, hid_grads, output_shape = inputs
        weights_grads, partialsum_storage = outputs
        fail = sub['fail']
        pad = self.pad

        # convFilterActs will multiply targets by scaleTargets
        # then add scaleOutput * (the convolution value)
        # We could make use of this to implement an inplace
        # addconv op but for this op we just want to compute
        # the convolution so we set them to 0 and 1 respectively
        # Note: there is another version of convFilterActs that
        # does not take these arguments, but it is just a wrapper
        # around the version that does take them, so we save
        # a function call by using the version that we use.
        basic_setup = """
        #define scaleTargets 0
        #define scaleOutput 1
        """

        if self.dense_connectivity:
            basic_setup += """
            #define numGroups 1
            """

        basic_setup += """
        #define paddingStart (-%(pad)d)
        const int *hid_grads_dims = CudaNdarray_HOST_DIMS(%(hid_grads)s);
        const int hidGradsSizeY = hid_grads_dims[1];
        const int hidGradsSizeX = hid_grads_dims[2];
        const int numModules = hidGradsSizeX * hidGradsSizeY;
        int partialSum = %(partial_sum)d > 0 ? %(partial_sum)d : numModules;

        // using this expression instead of numModules %% partialSum
        // because nvcc+msvc9 yield a strange behaviour when using %%
        if ( numModules - (numModules / partialSum) * partialSum != 0) {
            PyErr_Format(PyExc_ValueError,
                "partialSum must divide numModules, but partialSum=%%d and "
                "numModules=%%d", partialSum, numModules);
            %(fail)s;
        }
        """

        basic_setup += """
        #define moduleStride %d
        """ % self.stride
        if self.copy_non_contiguous:
            raise UnimplementedError()
        else:
            basic_setup += "#define WEIGHTACTS_COPY_NON_CONTIGUOUS 0\n"

        # The amount of braces that must be closed at the end
        num_braces = 0

        # Convert images int nv_images, an NVMatrix, for compatibility
        # with the cuda-convnet functions
        setup_nv_images = self._argument_contiguity_check("images") + """
        if (%(images)s->nd != 4)
        {
            PyErr_Format(PyExc_ValueError,
                "images must have nd=4, got nd=%%i", %(images)s->nd);
            %(fail)s;
        }
        { //setup_nv_images brace 1
        const int * images_dims = CudaNdarray_HOST_DIMS(%(images)s);
        const int img_channels = images_dims[0];
        if (img_channels > 3 && img_channels %% 4 != 0)
        {
            PyErr_Format(PyExc_ValueError,
                "images must have 3 or fewer channels, or have a multiple of 4 channels, got %%i",
                img_channels);
            %(fail)s;
        }

        { //setup_nv_images brace 2
        const int * hid_grads_dims = CudaNdarray_HOST_DIMS(%(hid_grads)s);
        const int imgSizeY = images_dims[1];
        const int imgSizeX = images_dims[2];
        const int batch_size = images_dims[3];
        NVMatrix nv_images(%(images)s, img_channels * imgSizeY * imgSizeX, batch_size, "weight_acts: nv_images");
        """
        num_braces += 2

        # Convert hid_grads int nv_hid_grads, an NVMatrix, for compatibility
        # with the cuda-convnet functions
        setup_nv_hid_grads = self._argument_contiguity_check("hid_grads") + """
        if (%(hid_grads)s->nd != 4)
        {
            PyErr_Format(PyExc_ValueError,
                "hid_grads must have nd=4, got nd=%%i", %(hid_grads)s->nd);
            %(fail)s;
        }

        { //setup_nv_hid_grads brace 1
        const int numFilters = hid_grads_dims[0];
        const int batch_size = hid_grads_dims[3];
        NVMatrix nv_hid_grads(%(hid_grads)s, numFilters * hidGradsSizeY *
                                           hidGradsSizeX, batch_size, "weight_acts:nv_hid_grads");
        """
        num_braces += 1

        setup_nv_weights_grads = """
        int filters_dims[4];
        // filters:  (input channels, filter rows, filter cols, output channels)

        npy_intp *shape_dims = PyArray_DIMS(%(output_shape)s);
        npy_intp target_rows, target_cols;
        PyArrayObject *casted_shape;
        PyArray_Descr *intp_dtype;
        if (PyArray_NDIM(%(output_shape)s) != 1) {
            PyErr_Format(PyExc_ValueError,
                         "output shape must be a vector, got %%d-tensor",
                         PyArray_NDIM(%(output_shape)s));
            %(fail)s;
        }
        else if (shape_dims[0] != 2)
        {
            PyErr_Format(PyExc_ValueError,
                         "output shape must be length 2, got %%d",
                         (int)shape_dims[0]);
            %(fail)s;
        }
        else if ((PyArray_DESCR(%(output_shape)s))->kind != 'i' &&
                 (PyArray_DESCR(%(output_shape)s))->kind != 'u')
        {
            PyErr_SetString(PyExc_TypeError,
                            "output shape must have integer or uint dtype");
            %(fail)s;
        }
        intp_dtype = PyArray_DescrFromType(NPY_INTP);
        casted_shape = (PyArrayObject *)PyArray_CastToType(%(output_shape)s,
                                                           intp_dtype, 0);
        target_rows = *((npy_intp *)PyArray_GETPTR1(casted_shape, 0));
        target_cols = *((npy_intp *)PyArray_GETPTR1(casted_shape, 1));
        filters_dims[0] = img_channels;
        filters_dims[1] = target_rows;
        filters_dims[2] = target_cols;
        if (filters_dims[1] != filters_dims[2])
        {
            PyErr_Format(PyExc_ValueError,
            "filter must be square, but have shape (%%d, %%d).",
            filters_dims[1], filters_dims[2]);
            %(fail)s;
        }
        else if (moduleStride > filters_dims[1]) {
            PyErr_Format(PyExc_ValueError,
            "stride %%d greater than filter size (%%d, %%d)",
            moduleStride, filters_dims[1], filters_dims[2]);
            %(fail)s;
        }
        filters_dims[3] = numFilters;
        const int filterSize = filters_dims[1];
        int partialsum_storage_dims[5];
        for (int i = 1; i < 5; i++)
        {
            partialsum_storage_dims[i] = filters_dims[i - 1];
        }
        partialsum_storage_dims[0] = numModules / partialSum;
        if (partialSum != numModules &&
            CudaNdarray_prep_output(&%(partialsum_storage)s, 5,
                                    partialsum_storage_dims))
        {
            %(fail)s;
        }

        for (int i = 0; i < 4; i++)
        {
            if (filters_dims[i] <= 0)
            {
                printf("filters_dims[%%d] = %%d\\n", i, filters_dims[i]);
                assert(false);
            }
        }
        if (CudaNdarray_prep_output(& %(weights_grads)s, 4, filters_dims))
        {
            %(fail)s;
        }

        { // setup_nv_weights_grad brace # 1

        NVMatrix nv_weights_grads(%(weights_grads)s, filters_dims[0] * filterSize
                                  * filterSize, numFilters,
                                  "weight_acts:nv_weights_grads");

        """

        num_braces += 1

        # note: imgSizeX is not specified here, it is computed internally
        # (in _filterActsSparse) by the lines:
        # int imgPixels = images.getNumRows() / numImgColors;
        # int imgSizeX = imgPixels / imgSizeY;
        #
        # note: numFilters is not specified here. it is determined by
        # nv_filters.getNumCols()
        #
        # note: the size of the filters is determined by dividing
        # nv_filters.getNumRows() by numFilterColors
        #
        run_kernel = """

        if (partialSum == numModules)
            _weightActs(nv_images, nv_hid_grads, nv_weights_grads,
                        imgSizeY, hidGradsSizeY, hidGradsSizeX, filterSize,
                        paddingStart, moduleStride, img_channels, numGroups,
                        partialSum, 0, 1);
        else {
            NVMatrix nv_partialsum(%(partialsum_storage)s, (numModules / partialSum) *
                     filters_dims[0] * filterSize * filterSize, numFilters,
                     "weight_acts: nv_partialsum");
            _weightActs(nv_images, nv_hid_grads, nv_partialsum,
                        imgSizeY, hidGradsSizeY, hidGradsSizeX, filterSize,
                        paddingStart, moduleStride, img_channels, numGroups,
                        partialSum, 0, 1);
            nv_partialsum.reshape((numModules / partialSum), filters_dims[0] * filterSize * filterSize * numFilters);

            // sum out axis 0 of nv_partialsum
            #define AXIS 0
            // scale the contents of nv_weights_grads by 0
            // i.e., clear out its pre-existing content
            #define SCALE_THIS 0
            // scale the new sum by 1, i.e., don't do any scaling
            #define SCALE_SUM 1
            nv_weights_grads.addSum(nv_partialsum, AXIS, SCALE_THIS, SCALE_SUM);
        }
        """

        braces = '}' * num_braces

        rval = (basic_setup +
                setup_nv_images +
                setup_nv_hid_grads +
                setup_nv_weights_grads +
                run_kernel +
                braces)

        rval = render_string(rval, locals())

        return rval

    def c_code_cache_version(self):
        """
        .. todo::

            WRITEME
        """
        return (7,)

########NEW FILE########
__FILENAME__ = agent
__author__ = "Ian Goodfellow"

class Agent(object):
    pass

########NEW FILE########
__FILENAME__ = algorithm
__author__ = "Ian Goodfellow"

class Algorithm(object):
    """
    Bare-bones algorithm for driving a bandit learning problem.
    """

    def setup(self, agent, environment):
        self.decide_func = agent.get_decide_func()
        self.action_func = environment.get_action_func()
        self.learn_func = agent.get_learn_func()
        agent.reward_record = []
        self.agent = agent

    def train(self):
        a = self.decide_func()
        r = self.action_func(a)
        self.agent.reward_record.append(r)
        self.learn_func(a, r)

class ContextualBanditAlgorithm(Algorithm):
    """
    Bare-bones algorithm for driving a contextual bandit learning problem.
    """

    def setup(self, agent, environment):
        self.context_func = environment.get_context_func()
        self.decide_func = agent.get_decide_func()
        self.action_func = environment.get_action_func()
        self.learn_func = agent.get_learn_func()
        agent.reward_record = []
        self.agent = agent
        self.agent.dataset_yaml_src = environment.dataset.yaml_src

    def train(self):
        # TODO: this could all be much more efficient on GPU if s, a, and r
        # were stored in shared variables that all the different functions
        # are aware of.
        s = self.context_func()
        a = self.decide_func(s)
        r = self.action_func(a)
        # TODO: figure out how to remove waste here, where forward prop is
        # done a second time in order to do backprop
        self.learn_func(s, a, r)
        if r.ndim > 0:
            r = r.mean()
        self.agent.reward_record.append(r)

########NEW FILE########
__FILENAME__ = average_agent
__author__ = "Ian Goodfellow"

import numpy as np

from theano.compat.python2x import OrderedDict
from theano import function
from theano import tensor as T


from pylearn2.sandbox.lisa_rl.bandit.agent import Agent
from pylearn2.utils import sharedX

class AverageAgent(Agent):
    """
    A simple n-armed bandit playing agent that always plays the
    arm with the highest estimated reward. The estimated reward is just
    based on the average of all observations from that arm. If an arm
    has not been tried, the estimated reward is given by init_reward_estimate.

    .. todo::

        WRITEME : parameter list
    """

    def __init__(self, init_reward_estimate, num_arms):
        self.__dict__.update(locals())
        del self.self
        self.estimated_rewards = sharedX(np.zeros((num_arms,)) \
                + self.init_reward_estimate)
        self.observation_counts = sharedX(np.zeros((num_arms,)))


    def get_decide_func(self):
        """
        Returns a theano function that decides what action to take.
        Since this is a bandit playing agent, there is no input.
        """

        # Cast is for compatibility with default bit depth of T.iscalar
        # (wtf, theano?)
        return function([], T.cast(T.argmax(self.estimated_rewards), 'int32'))

    def get_learn_func(self):
        """
        Returns a theano function that takes an action and a reward,
        and updates the agent based on this experience.
        """

        a = T.iscalar()
        r = T.scalar()

        old_estimated_reward = self.estimated_rewards[a]
        old_observation_count = self.observation_counts[a]
        observation_count = old_observation_count + 1.

        delta = r - old_estimated_reward
        new_estimated_reward = old_estimated_reward + delta / observation_count

        new_estimated_rewards = T.set_subtensor(self.estimated_rewards[a],
            new_estimated_reward)
        new_observation_counts = T.set_subtensor(self.observation_counts[a], observation_count)

        updates = OrderedDict([
            (self.estimated_rewards, new_estimated_rewards),
            (self.observation_counts, new_observation_counts)
            ])

        rval = function([a, r], updates=updates)

        return rval

########NEW FILE########
__FILENAME__ = classifier_agent
__author__ = "Ian Goodfellow"

import logging
import time

from theano import function
import theano.tensor as T

from pylearn2.sandbox.lisa_rl.bandit.agent import Agent
from pylearn2.utils import sharedX
from pylearn2.utils.rng import make_theano_rng


logger = logging.getLogger(__name__)


class ClassifierAgent(Agent):
    """
    A contextual bandit agent that knows a priori that the task is
    classification. Specifically, the actions the agent can take
    are to output k different one-hot vectors. If it outputs the
    correct code for the current input it gets a reward of 1,
    otherwise it gets a reward of 0.

    This hard-coded prior knowledge means that the expected reward
    of taking action a is just the probability of a being the
    correct class.

    This makes the contextual bandit problem as close as possible
    to the classification task, so that any loss in performance
    comes only from needing to explore to discover the correct
    action for for each input.

    .. todo::

        WRITEME : parameter list

    Parameters
    ----------
    stochastic: bool
        If True, samples actions from P(y | x) otherwise, uses argmax_y P(y |x)
    """

    def __init__(self, mlp, learning_rule, init_learning_rate, cost,
            update_callbacks, stochastic=False, epsilon=None, neg_target=False,
            ignore_wrong=False, epsilon_stochastic=None):
        self.__dict__.update(locals())
        del self.self

        self.learning_rate = sharedX(init_learning_rate)

    def get_decide_func(self):
        """
        Returns a theano function that takes a minibatch
        (num_examples, num_features) of contexts and returns
        a minibatch (num_examples, num_classes) of one-hot codes
        for actions.
        """

        X = T.matrix()
        y_hat = self.mlp.fprop(X)

        theano_rng = make_theano_rng(None, 2013+11+20, which_method="multinomial")
        if self.stochastic:
            a = theano_rng.multinomial(pvals=y_hat, dtype='float32')
        else:
            mx = T.max(y_hat, axis=1).dimshuffle(0, 'x')
            a = T.eq(y_hat, mx)

        if self.epsilon is not None:
            a = theano_rng.multinomial(pvals = (1. - self.epsilon) * a +
                    self.epsilon * T.ones_like(y_hat) / y_hat.shape[1],
                    dtype = 'float32')

        if self.epsilon_stochastic is not None:
            a = theano_rng.multinomial(pvals = (1. - self.epsilon_stochastic) * a +
                    self.epsilon_stochastic * y_hat,
                    dtype = 'float32')

        logger.info("Compiling classifier agent learning function")
        t1 = time.time()
        f = function([X], a)
        t2 = time.time()

        logger.info("...done, took {0}".format(t2 - t1))

        return f

    def get_learn_func(self):
        """
        Returns a theano function that does a learning update when passed
        a context, the action that the agent chose, and the reward it got.

        This agent expects the action to be a matrix of one-hot class
        selections and the reward to be a vector of 0 / 1 rewards per example.
        """

        contexts = T.matrix()
        actions = T.matrix()
        rewards = T.vector()

        assert sum([self.neg_target, self.ignore_wrong]) <= 1
        if self.neg_target:
            signed_rewards = 2. * rewards - 1.
            fake_targets = actions * signed_rewards.dimshuffle(0, 'x')
        elif self.ignore_wrong:
            fake_targets = actions * rewards.dimshuffle(0, 'x')
        else:
            correct_actions = actions * rewards.dimshuffle(0, 'x')
            roads_not_taken = (T.ones_like(actions) - actions) / (T.cast(actions.shape[1], 'float32') - 1.)
            #from theano.printing import Print
            #roads_not_taken = Print('roads_not_taken')(roads_not_taken)
            fake_targets = correct_actions + roads_not_taken * (1 - rewards).dimshuffle(0, 'x')

        lr_scalers = self.mlp.get_lr_scalers()

        grads, updates = self.cost.get_gradients(self.mlp, (contexts, fake_targets))

        updates.update(self.learning_rule.get_updates(
                self.learning_rate, grads, lr_scalers))

        self.mlp.modify_updates(updates)

        learn_func = function([contexts, actions, rewards], updates=updates)

        def rval(contexts, actions, rewards):
            learn_func(contexts, actions, rewards)
            for callback in self.update_callbacks:
                callback(self)

        return rval

    def get_weights(self):
        return self.mlp.get_weights()

    def get_weights_format(self):
        return self.mlp.get_weights_format()

    def get_weights_topo(self):
        return self.mlp.get_weights_topo()

    def get_weights_view_shape(self):
        return self.mlp.get_weights_view_shape()

    def get_params(self):
        return self.mlp.get_params()

    def set_batch_size(self, batch_size):
        self.mlp.set_batch_size(batch_size)

    def get_input_space(self):
        return self.mlp.get_input_space()

    def get_output_space(self):
        return self.mlp.get_output_space()

    def fprop(self, state_below):
        return self.mlp.fprop(state_below)

########NEW FILE########
__FILENAME__ = classifier_bandit
__author__ = "Ian Goodfellow"

from pylearn2.sandbox.lisa_rl.bandit.environment import Environment

class ClassifierBandit(Environment):
    """
    An n-armed contextual bandit based on a classification problem.

    Each of the n-arms corresponds to a different class. If the agent
    selects the correct class for the given context, the environment
    gives reward 1. Otherwise, the environment gives reward 0.

    .. todo::

        WRITEME : parameter list
    """

    def __init__(self, dataset, batch_size):
        self.__dict__.update(locals())
        del self.self

    def get_context_func(self):
        """
        Returns a callable that takes no arguments and returns a minibatch
        of contexts. Minibatch should be in VectorSpace(n).
        """

        def rval():
            X, y = self.dataset.get_batch_design(self.batch_size, include_labels=True)
            self.y_cache = y
            return X

        return rval

    def get_action_func(self):
        """
        Returns a callable that takes no arguments and returns a minibatch of
        rewards.
        Assumes that this function has been called after a call to context_func
        that gave the contexts used to choose the actions.
        """

        def rval(a):
            return (a * self.y_cache).sum(axis=1)

        return rval

    def get_learn_func(self):
        """
        Returns a callable that takes a minibatch of contexts, a minibatch of
        actions, and a minibatch of rewards, and updates the model according
        to them.
        """

        raise NotImplementedError()

########NEW FILE########
__FILENAME__ = environment
__author__ = "Ian Goodfellow"

class Environment(object):
    pass

########NEW FILE########
__FILENAME__ = gaussian_bandit
__author__ = "Ian Goodfellow"

import numpy as np

from theano import config
from theano import function
from theano import tensor as T

from pylearn2.sandbox.lisa_rl.bandit.environment import Environment
from pylearn2.utils import sharedX
from pylearn2.utils.rng import make_np_rng, make_theano_rng


class GaussianBandit(Environment):
    """
    An n-armed bandit whose rewards are drawn from a different Gaussian
    distribution for each arm.
    The mean and standard deviation of the reward for each arm is drawn
    at initialization time from N(0, <corresponding std arg>).
    (For the standard deviation we use the absolute value of the Gaussian
    sample)

    .. todo::

        WRITEME : parameter list
    """

    def __init__(self, num_arms, mean_std = 1.0, std_std = 1.0):
        self.rng = make_np_rng(None, [2013, 11, 12], which_method="randn")
        self.means = sharedX(self.rng.randn(num_arms) * mean_std)
        self.stds = sharedX(np.abs(self.rng.randn(num_arms) * std_std))
        self.theano_rng = make_theano_rng(None, self.rng.randint(2 ** 16), which_method="normal")

    def get_action_func(self):
        """
        Returns a theano function that takes an action and returns a reward.
        """

        action = T.iscalar()
        reward_mean = self.means[action]
        reward_std = self.stds[action]
        reward = self.theano_rng.normal(avg=reward_mean, std=reward_std,
                dtype=config.floatX, size=reward_mean.shape)
        rval = function([action], reward)
        return rval

########NEW FILE########
__FILENAME__ = plot_reward
__author__ = "Ian Goodfellow"

from matplotlib import pyplot
import sys
pyplot.hold(True)

from pylearn2.utils import serial

model_paths = sys.argv[1:]

smoothing = 1
try:
    smoothing = int(model_paths[0])
    model_paths = model_paths[1:]
except:
    pass

count = 0
style = '-'
for model_path in model_paths:
    model = serial.load(model_path)
    smoothed_reward_record = []
    count += 1
    if count > 7:
        style = '+'
    for i in xrange(smoothing - 1, len(model.reward_record)):
        smoothed_reward_record.append(sum(model.reward_record[i - smoothing + 1:i + 1]) / float(smoothing))
    pyplot.plot(smoothed_reward_record, style, label=model_path)
pyplot.legend()
pyplot.show()



########NEW FILE########
__FILENAME__ = simulate
__author__ = "Ian Goodfellow"

from pylearn2.config import yaml_parse
import sys

_, path = sys.argv

simulator = yaml_parse.load_path(path)

simulator.main_loop()

########NEW FILE########
__FILENAME__ = simulator
__author__ = "Ian Goodfellow"

import logging
import numpy as np

from pylearn2.utils import serial


logger = logging.getLogger(__name__)


class Simulator(object):
    """
    .. todo::

        WRITEME : parameter list
    """
    def __init__(self, agent, environment, algorithm, save_path):
        self.__dict__.update(locals())
        del self.self

    def main_loop(self):
        self.algorithm.setup(agent=self.agent, environment=self.environment)
        i = 0
        for param in self.agent.get_params():
            assert not np.any(np.isnan(param.get_value())), (i, param.name)
            assert not np.any(np.isinf(param.get_value())), (i, param.name)
        while True:
            rval = self.algorithm.train()
            assert rval is None
            i += 1
            for param in self.agent.get_params():
                assert not np.any(np.isnan(param.get_value())), (i, param.name)
                assert not np.any(np.isinf(param.get_value())), (i, param.name)
            if i % 1000 == 0:
                serial.save(self.save_path, self.agent)
                logger.info('saved!')

########NEW FILE########
__FILENAME__ = penntree
"""
Dataset wrapper for the Penn Treebank dataset

See: http://www.cis.upenn.edu/~treebank/
"""

__authors__ = "Bart van Merrienboer"
__copyright__ = "Copyright 2010-2014, Universite de Montreal"
__license__ = "3-clause BSD"

import warnings

from numpy.lib.stride_tricks import as_strided

from pylearn2.datasets.dense_design_matrix import DenseDesignMatrix
from pylearn2.utils import serial
from pylearn2.utils.iteration import resolve_iterator_class


class PennTreebank(DenseDesignMatrix):
    """
    Loads the Penn Treebank corpus.

    Parameters
    ----------
    which_set : {'train', 'valid', 'test'}
        Choose the set to use
    context_len : int
        The size of the context i.e. the number of words used
        to predict the subsequent word.
    shuffle : bool
        Whether to shuffle the samples or go through the dataset
        linearly
    """
    def __init__(self, which_set, context_len, shuffle=True):
        """
        Loads the data and turns it into n-grams
        """
        path = ("${PYLEARN2_DATA_PATH}/PennTreebankCorpus/" +
                "penntree_char_and_word.npz")
        npz_data = serial.load(path)
        if which_set == 'train':
            self._raw_data = npz_data['train_words']
        elif which_set == 'valid':
            self._raw_data = npz_data['valid_words']
        elif which_set == 'test':
            self._raw_data = npz_data['test_words']
        else:
            raise ValueError("Dataset must be one of 'train', 'valid' "
                             "or 'test'")
        del npz_data  # Free up some memory?

        self._data = as_strided(self._raw_data,
                                shape=(len(self._raw_data) - context_len,
                                       context_len + 1),
                                strides=(self._raw_data.itemsize,
                                         self._raw_data.itemsize))

        super(PennTreebank, self).__init__(
            X=self._data[:, :-1],
            y=self._data[:, -1:],
            X_labels=10000, y_labels=10000
        )

        if shuffle:
            warnings.warn("Note that the PennTreebank samples are only "
                          "shuffled when the iterator method is used to "
                          "retrieve them.")
            self._iter_subset_class = resolve_iterator_class(
                'shuffled_sequential'
            )

########NEW FILE########
__FILENAME__ = matrixmul
"""
Sandbox projection operator for natural language processing (NLP)
"""
from pylearn2.linear import matrixmul


class MatrixMul(matrixmul.MatrixMul):
    """
    Operations which can be represented as matrix multiplications.

    Parameters
    ----------
    W : WRITEME
    """
    def project(self, x):
        """
        Takes a sequence of integers and projects (embeds) these labels
        into a continuous space by concatenating the correspending
        rows in the projection matrix W i.e. [2, 5] -> [W[2] ... W[5]]

        Parameters
        ----------
        x : theano.tensor, int dtype
            A vector of labels (or a matrix where each row is a sample in
            a batch) which will be projected
        """

        assert 'int' in x.dtype

        if x.ndim == 2:
            shape = (x.shape[0], x.shape[1] * self._W.shape[1])
            return self._W[x.flatten()].reshape(shape)
        elif x.ndim == 1:
            return self._W[x].flatten()
        else:
            assert ValueError("project needs 1- or 2-dimensional input")

########NEW FILE########
__FILENAME__ = test_matrixmul
"""
Sandbox tests for the projection operator
"""
import numpy as np
import theano

from pylearn2.sandbox.nlp.linear.matrixmul import MatrixMul
from pylearn2.utils import sharedX
from theano import tensor


def test_matrixmul():
    """
    Tests for projection
    """
    rng = np.random.RandomState(222)
    dtypes = [
        'int16', 'int32', 'int64'
    ]
    tensor_x = [
        tensor.wmatrix(),
        tensor.imatrix(),
        tensor.lmatrix(),
        tensor.wvector(),
        tensor.ivector(),
        tensor.lvector()
    ]
    np_W, np_x = [], []
    for dtype in dtypes:
        np_W.append(rng.rand(10, np.random.randint(1, 10)))
        np_x.append(rng.randint(
            0, 10, (rng.random_integers(5),
                    rng.random_integers(5))
        ).astype(dtype))
    for dtype in dtypes:
        np_W.append(rng.rand(10, np.random.randint(1, 10)))
        np_x.append(
            rng.randint(0, 10, (rng.random_integers(5),)).astype(dtype)
        )

    tensor_W = [sharedX(W) for W in np_W]
    matrixmul = [MatrixMul(W) for W in tensor_W]
    assert all(mm.get_params()[0] == W for mm, W in zip(matrixmul, tensor_W))

    fn = [theano.function([x], mm.project(x))
          for x, mm in zip(tensor_x, matrixmul)]
    for W, x, f in zip(np_W, np_x, fn):
        W_x = W[x]
        if x.ndim == 2:
            W_x = W_x.reshape((W_x.shape[0], np.prod(W_x.shape[1:])))
        else:
            W_x = W_x.flatten()
        np.testing.assert_allclose(f(x), W_x)

########NEW FILE########
__FILENAME__ = mlp
"""
Sandbox multilayer perceptron layers for natural language processing (NLP)
"""
import theano.tensor as T
from theano import config

from pylearn2.models import mlp
from pylearn2.models.mlp import Layer
from pylearn2.space import IndexSpace
from pylearn2.space import VectorSpace
from pylearn2.utils import sharedX
from pylearn2.utils import wraps
from pylearn2.sandbox.nlp.linear.matrixmul import MatrixMul
from theano.compat.python2x import OrderedDict


class Softmax(mlp.Softmax):
    """
    An extension of the MLP's softmax layer which monitors
    the perplexity

    Parameters
    ----------
    n_classes : WRITEME
    layer_name : WRITEME
    irange : WRITEME
    istdev : WRITEME
    sparse_init : WRITEME
    W_lr_scale : WRITEME
    b_lr_scale : WRITEME
    max_row_norm : WRITEME
    no_affine : WRITEME
    max_col_norm : WRITEME
    init_bias_target_marginals : WRITEME
    """
    @wraps(Layer.get_monitoring_channels_from_state)
    def get_monitoring_channels_from_state(self, state, target=None):

        mx = state.max(axis=1)

        rval = OrderedDict([('mean_max_class', mx.mean()),
                            ('max_max_class', mx.max()),
                            ('min_max_class', mx.min())])

        if target is not None:
            y_hat = T.argmax(state, axis=1)
            y = T.argmax(target, axis=1)
            misclass = T.neq(y, y_hat).mean()
            misclass = T.cast(misclass, config.floatX)
            rval['misclass'] = misclass
            rval['nll'] = self.cost(Y_hat=state, Y=target)
            rval['ppl'] = 2 ** (rval['nll'] / T.log(2))

        return rval


class ProjectionLayer(Layer):
    """
    This layer can be used to project discrete labels into a continous space
    as done in e.g. language models. It takes labels as an input (IndexSpace)
    and maps them to their continous embeddings and concatenates them.

    Parameters
        ----------
    dim : int
        The dimension of the embeddings. Note that this means that the
        output dimension is (dim * number of input labels)
    layer_name : string
        Layer name
    irange : numeric
       The range of the uniform distribution used to initialize the
       embeddings. Can't be used with istdev.
    istdev : numeric
        The standard deviation of the normal distribution used to
        initialize the embeddings. Can't be used with irange.
    """
    def __init__(self, dim, layer_name, irange=None, istdev=None):
        """
        Initializes a projection layer.
        """
        super(ProjectionLayer, self).__init__()
        self.dim = dim
        self.layer_name = layer_name
        if irange is None and istdev is None:
            raise ValueError("ProjectionLayer needs either irange or"
                             "istdev in order to intitalize the projections.")
        elif irange is not None and istdev is not None:
            raise ValueError("ProjectionLayer was passed both irange and "
                             "istdev but needs only one")
        else:
            self._irange = irange
            self._istdev = istdev

    @wraps(Layer.get_monitoring_channels)
    def get_monitoring_channels(self):

        W, = self.transformer.get_params()

        assert W.ndim == 2

        sq_W = T.sqr(W)

        row_norms = T.sqrt(sq_W.sum(axis=1))
        col_norms = T.sqrt(sq_W.sum(axis=0))

        return OrderedDict([('row_norms_min',  row_norms.min()),
                            ('row_norms_mean', row_norms.mean()),
                            ('row_norms_max',  row_norms.max()),
                            ('col_norms_min',  col_norms.min()),
                            ('col_norms_mean', col_norms.mean()),
                            ('col_norms_max',  col_norms.max()), ])

    @wraps(Layer.set_input_space)
    def set_input_space(self, space):
        if isinstance(space, IndexSpace):
            self.input_dim = space.dim
            self.input_space = space
        else:
            raise ValueError("ProjectionLayer needs an IndexSpace as input")
        self.output_space = VectorSpace(self.dim * self.input_dim)
        rng = self.mlp.rng
        if self._irange is not None:
            W = rng.uniform(-self._irange,
                            self._irange,
                            (space.max_labels, self.dim))
        else:
            W = rng.randn(space.max_labels, self.dim) * self._istdev

        W = sharedX(W)
        W.name = self.layer_name + '_W'

        self.transformer = MatrixMul(W)

        W, = self.transformer.get_params()
        assert W.name is not None

    @wraps(Layer.fprop)
    def fprop(self, state_below):
        z = self.transformer.project(state_below)
        return z

    @wraps(Layer.get_params)
    def get_params(self):
        W, = self.transformer.get_params()
        assert W.name is not None
        params = [W]
        return params

########NEW FILE########
__FILENAME__ = tuple_var
"""
The skeleton of a Theano Variable and Type for representing a tuple.
Not really ready to use yet-- see theano-dev e-mail "TupleType"
"""

__author__ = "Ian Goodfellow"

from theano.gof.graph import Variable
from theano.gof.type import Type
from theano.tensor.basic import hashtype

class TupleType(Type):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    component_types : WRITEME
    """

    def __init__(self, component_types):
        if not isinstance(component_types, tuple):
            raise TypeError("Expected component_types to be a tuple, got "
                    + str(type(component_types)))
        assert all(isinstance(component, Type) for component in component_types)
        self.__dict__.update(locals())
        del self.self

    def __eq__(self, other):
        return type(self) == type(other) and len(self.component_types) == \
                len(other.component_types) and all(component_type == \
                other_component_type for component_type, other_component_type in \
                zip(self.component_types, other.component_types))

    def __hash__(self):
        return hashtype(self) ^ hash(self.component_types)


class TupleVariable(Variable):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    component_variables : WRITEME
    name : WRITEME
    """

    def __init__(self, component_variables, name = None):

        raise NotImplementedError("This should always be created via a MakeTuple Op (not implemented yet) so that the ancestors are correct.")
        raise NotImplementedError("This is not safe to use yet, since T.grad won't work right with it.")

        assert isinstance(component_variables, tuple)
        self.__dict__.update(locals())
        del self.self

        assert isinstance(component_variables, tuple)
        self.type = TupleType(tuple(var.type for var in component_variables))

    def __hash__(self):
        return hashtype(self) ^ hash(self.component_variables)

    def __eq__(self, other):
        return type(self) == type(other) and len(self.component_variables) == \
                len(other.component_variables) and all(component_variable == \
                other_component_variable for component_variable, other_component_variable in \
                zip(self.component_variables, other.component_variables))

    def __getitem__(self, index):
        """
        Deterministic, compile-time tuple indexing.
        """
        if isinstance(index, slice):
            if any(elem is not None or not isinstance(elem, int) for elem in \
                    [index.start, index.step, index.stop]):
                raise TypeError("slice elements must be int or None--symbolic indexing not supported yet")
            return tuple_variable(self.component_variables[index])

        if not isinstance(index, int):
            raise TypeError("index must be int or slice--symbolic indexing not supported yet")
        return self.component_variables[index]

    def __len__(self):
        return len(self.component_variables)

def tuple_variable(t):
    """
    Make a TupleVariable from a tuple t of TheanoVariables
    """

    raise NotImplementedError()

########NEW FILE########
__FILENAME__ = time_relu
'''
This is the benchmark of 4 different implementations
of rectified linear activation in Theano.
Two types of computations are tested w.r.t.
each implementation: fprop and grad.

Results: in seconds, float32 (details in the code)

Implementation, CPU (fprop, bprop), GPU (fprop, bprop), (final score)
a) ScalarRectifier:       (2.32, 2.40)    (1.36, 2.67)    (8.75)
b) T.max(.0, x):          (5.19, 3.65)    (1.38, 2.38)    (12.60)
c) x*(x>0.):              (2.85, 2.84)    (1.31, 2.91)    (9.91)
d) T.switch(x<0., 0., x): (2.32, 1.41)    (1.41, 2.84)    (8.39)

Conlusion:
In terms of efficiency, d) > a) > c) > b)
'''
__authors__ = "Li Yao and Frederic Bastien"
import theano
import theano.tensor as T

import numpy
import time

floatX = 'float32'
def relu(x):
    """
    relu implementation with T.maximum

    Parameters
    ----------
    x: tensor variable
    """
    return T.maximum(0.0, x)

def relu_(x):
    """
    Alternative relu implementation

    Parameters
    ----------
    x: tensor variable
    """
    return x * (x > 0)

def relu__(x):
    """
    Alternative relu implementation. The most efficient one.

    Parameters
    ----------
    x: tensor variable
    """
    return T.switch(x < 0., 0., x)


def test_scalar_rectifier():
    """
    verify different implementations of relu
    """
    x = T.fmatrix('inputs')
    y1 = relu(x)
    y3 = relu_(x)
    y4 = relu__(x)

    f1 = theano.function(inputs=[x], outputs=y1, name='benchmark_1_forward')
    f3 = theano.function(inputs=[x], outputs=y3, name='benchmark_3_forward')
    f4 = theano.function(inputs=[x], outputs=y4, name='benchmark_4_forward')

    g1 = theano.function(inputs=[x], outputs=T.grad(y1.sum(),x),
            name='benchmark_1_grad')
    g3 = theano.function(inputs=[x], outputs=T.grad(y3.sum(),x),
            name='benchmark_3_grad')
    g4 = theano.function(inputs=[x], outputs=T.grad(y4.sum(),x),
            name='benchmark_4_grad')

    for i in range(10):
        value = numpy.random.uniform(-1,1,size=(100,500)).astype(floatX)

        numpy.testing.assert_array_equal(f1(value), f3(value),
                                         err_msg='arrays not equal' )

        numpy.testing.assert_array_equal(f1(value), f4(value),
                                         err_msg='arrays not equal' )


        numpy.testing.assert_array_equal(g1(value), g3(value),
                                         err_msg='grad:arrays not equal' )

        numpy.testing.assert_array_equal(g1(value), g4(value),
                                         err_msg='grad:arrays not equal' )


def benchmark_relu():
    """
    Benchmark the speed of different relu implementations.
    Both fprop and grad are tested.
    """
    x = T.ftensor4('inputs')
    ops = [
        relu_(x).sum(), # old
        relu(x).sum(), # alter, short for alternative
        relu__(x).sum(), # alter 2
        T.grad(relu_(x).sum(),x), # grad_old
        T.grad(relu(x).sum(),x), # grad_alter
        T.grad(relu__(x).sum(),x), # grad_alter2
    ]

    names = ['fprop_old', 'fprop_alter', 'fprop_alter2',
             'grad_old', 'grad_alter', 'grad_alter2']


    value = numpy.random.uniform(size=(512,32,32,100)).astype(floatX)
    times = []
    for op, name in zip(ops, names):
        f = theano.function(inputs=[x], outputs=op, name=name)
        n_loops = 10

        t0 = time.time()
        for i in range(n_loops):
            f(value)
        t1 = time.time()
        benchmark = t1-t0
        times.append(benchmark)
        print name
        theano.printing.debugprint(f, print_type=True)
    print names
    print times


if __name__ == '__main__':
    benchmark_relu()
    #test_scalar_rectifier()

########NEW FILE########
__FILENAME__ = browse_small_norb
#!/usr/bin/env python

import sys, argparse, pickle
import numpy
from matplotlib import pyplot
from pylearn2.datasets import norb


def main():
    def parse_args():
        parser = argparse.ArgumentParser(
            description="Browser for SmallNORB dataset.")

        parser.add_argument('--which_set',
                            default='train',
                            help="'train', 'test', or the path to a .pkl file")

        parser.add_argument('--zca',
                            default=None,
                            help=("if --which_set points to a .pkl "
                                  "file storing a ZCA-preprocessed "
                                  "NORB dataset, you can optionally "
                                  "enter the preprocessor's .pkl "
                                  "file path here to undo the "
                                  "ZCA'ing for visualization "
                                  "purposes."))

        return parser.parse_args()

    def get_data(args):
        if args.which_set in ('train', 'test'):
            dataset = norb.SmallNORB(args.which_set, True)
        else:
            with open(args.which_set) as norb_file:
                dataset = pickle.load(norb_file)
                if len(dataset.y.shape) < 2 or dataset.y.shape[1] == 1:
                    print("This viewer does not support NORB datasets that "
                          "only have classification labels.")
                    sys.exit(1)

            if args.zca is not None:
                with open(args.zca) as zca_file:
                    zca = pickle.load(zca_file)
                    dataset.X = zca.inverse(dataset.X)

        num_examples = dataset.X.shape[0]

        topo_shape = ((num_examples, ) +
                      tuple(dataset.view_converter.shape))
        assert topo_shape[-1] == 1
        topo_shape = topo_shape[:-1]
        values = dataset.X.reshape(topo_shape)
        labels = numpy.array(dataset.y, 'int')
        return values, labels, dataset.which_set

    args = parse_args()
    values, labels, which_set = get_data(args)

    # For programming convenience, internally remap the instance labels to be
    # 0-4, and the azimuth labels to be 0-17. The user will still only see the
    # original, unmodified label values.

    instance_index = norb.SmallNORB.label_type_to_index['instance']

    def remap_instances(which_set, labels):
        if which_set == 'train':
            new_to_old_instance = [4, 6, 7, 8, 9]
        elif which_set == 'test':
            new_to_old_instance = [0, 1, 2, 3, 5]

        num_instances = len(new_to_old_instance)
        old_to_new_instance = numpy.ndarray(10, 'int')
        old_to_new_instance.fill(-1)
        old_to_new_instance[new_to_old_instance] = numpy.arange(num_instances)

        instance_slice = numpy.index_exp[:, instance_index]
        old_instances = labels[instance_slice]

        new_instances = old_to_new_instance[old_instances]
        labels[instance_slice] = new_instances

        azimuth_index = norb.SmallNORB.label_type_to_index['azimuth']
        azimuth_slice = numpy.index_exp[:, azimuth_index]
        labels[azimuth_slice] = labels[azimuth_slice] / 2

        return new_to_old_instance

    new_to_old_instance = remap_instances(which_set, labels)

    def get_new_azimuth_degrees(scalar_label):
        return 20 * scalar_label

    # Maps a label vector to the corresponding index in <values>
    num_labels_by_type = numpy.array(norb.SmallNORB.num_labels_by_type, 'int')
    num_labels_by_type[instance_index] = len(new_to_old_instance)

    label_to_index = numpy.ndarray(num_labels_by_type, 'int')
    label_to_index.fill(-1)

    for i, label in enumerate(labels):
        label_to_index[tuple(label)] = i

    assert not numpy.any(label_to_index == -1)  # all elements have been set

    figure, axes = pyplot.subplots(1, 2, squeeze=True)

    figure.canvas.set_window_title('Small NORB dataset (%sing set)' %
                                   which_set)

    # shift subplots down to make more room for the text
    figure.subplots_adjust(bottom=0.05)

    num_label_types = len(norb.SmallNORB.num_labels_by_type)
    current_labels = numpy.zeros(num_label_types, 'int')
    current_label_type = [0, ]

    label_text = figure.suptitle("title text",
                                 x=0.1,
                                 horizontalalignment="left")

    def redraw(redraw_text, redraw_images):
        if redraw_text:
            cl = current_labels

            lines = [
                'category: %s' % norb.SmallNORB.get_category(cl[0]),
                'instance: %d' % new_to_old_instance[cl[1]],
                'elevation: %d' % norb.SmallNORB.get_elevation_degrees(cl[2]),
                'azimuth: %d' % get_new_azimuth_degrees(cl[3]),
                'lighting: %d' % cl[4]]

            lt = current_label_type[0]
            lines[lt] = '==> ' + lines[lt]
            text = ('Up/down arrows choose label, left/right arrows change it'
                    '\n\n' +
                    '\n'.join(lines))
            label_text.set_text(text)

        if redraw_images:
            index = label_to_index[tuple(current_labels)]

            image_pair = values[index, :, :, :]
            for i in range(2):
                axes[i].imshow(image_pair[i, :, :], cmap='gray')

        figure.canvas.draw()

    def on_key_press(event):

        def add_mod(arg, step, size):
            return (arg + size + step) % size

        def incr_label_type(step):
            current_label_type[0] = add_mod(current_label_type[0],
                                            step,
                                            num_label_types)

        def incr_label(step):
            lt = current_label_type[0]
            num_labels = num_labels_by_type[lt]
            current_labels[lt] = add_mod(current_labels[lt], step, num_labels)

        if event.key == 'up':
            incr_label_type(-1)
            redraw(True, False)
        elif event.key == 'down':
            incr_label_type(1)
            redraw(True, False)
        elif event.key == 'left':
            incr_label(-1)
            redraw(True, True)
        elif event.key == 'right':
            incr_label(1)
            redraw(True, True)
        elif event.key == 'q':
            sys.exit(0)

    figure.canvas.mpl_connect('key_press_event', on_key_press)
    redraw(True, True)

    pyplot.show()


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = download_mnist
import os
import urllib
import gzip
assert 'PYLEARN2_DATA_PATH' in os.environ, "PYLEARN2_DATA_PATH not defined"
mnist_path = os.path.join(os.environ['PYLEARN2_DATA_PATH'], "mnist")

if not os.path.isdir(mnist_path):
    print "creating path: " + mnist_path
    os.makedirs(mnist_path)

in_dir = os.listdir(mnist_path)
mnist_files = ["t10k-images-idx3-ubyte", "t10k-labels-idx1-ubyte",
               "train-images-idx3-ubyte", "train-labels-idx1-ubyte"]
mnist_url = "http://yann.lecun.com/exdb/mnist/"

if not all([f in in_dir for f in mnist_files]) or in_dir == []:
    print "Downloading MNIST data..."
    gz_in = [os.path.join(mnist_path, f + ".gz") for f in mnist_files]
    gz_out = [os.path.join(mnist_path, f)for f in mnist_files]
    mnist_url = ["".join([mnist_url, f, ".gz"]) for f in mnist_files]

    for g_in, g_out, m_url in zip(gz_in, gz_out, mnist_url):
        print "Downloading " + m_url + "...",
        urllib.urlretrieve(m_url, filename=g_in)
        print " Done"

        with gzip.GzipFile(g_in) as f_gz:
            data = f_gz.read()

        with open(g_out, 'wb') as f_out:
            f_out.write(data)

    print "Done downloading MNIST"
else:
    print "MNIST files already in PYLEARN2_DATA_PATH"

########NEW FILE########
__FILENAME__ = make_cifar100_gcn_whitened
"""
This script makes a dataset of 32x32 contrast normalized, approximately
whitened CIFAR-100 images.

"""

from pylearn2.utils import serial
from pylearn2.datasets import preprocessing
from pylearn2.utils import string_utils
from pylearn2.datasets.cifar100 import CIFAR100

data_dir = string_utils.preprocess('${PYLEARN2_DATA_PATH}/cifar100')

print 'Loading CIFAR-100 train dataset...'
train = CIFAR100(which_set = 'train', gcn = 55.)

print "Preparing output directory..."
output_dir = data_dir + '/pylearn2_gcn_whitened'
serial.mkdir( output_dir )
README = open(output_dir + '/README','w')

README.write("""
The .pkl files in this directory may be opened in python using
cPickle, pickle, or pylearn2.serial.load.

train.pkl, and test.pkl each contain
a pylearn2 Dataset object defining a labeled
dataset of a 32x32 contrast normalized, approximately whitened version of the CIFAR-100
dataset. train.pkl contains labeled train examples. test.pkl
contains labeled test examples.

preprocessor.pkl contains a pylearn2 ZCA object that was used
to approximately whiten the images. You may want to use this
object later to preprocess other images.

They were created with the pylearn2 script make_cifar100_gcn_whitened.py.

All other files in this directory, including this README, were
created by the same script and are necessary for the other files
to function correctly.
""")

README.close()

print "Learning the preprocessor and preprocessing the unsupervised train data..."
preprocessor = preprocessing.ZCA()
train.apply_preprocessor(preprocessor = preprocessor, can_fit = True)

print 'Saving the training data'
train.use_design_loc(output_dir+'/train.npy')
serial.save(output_dir + '/train.pkl', train)

print "Loading the test data"
test = CIFAR100(which_set = 'test', gcn = 55.)

print "Preprocessing the test data"
test.apply_preprocessor(preprocessor = preprocessor, can_fit = False)

print "Saving the test data"
test.use_design_loc(output_dir+'/test.npy')
serial.save(output_dir+'/test.pkl', test)

serial.save(output_dir + '/preprocessor.pkl',preprocessor)


########NEW FILE########
__FILENAME__ = make_cifar100_patches
"""
This script makes a dataset of two million approximately whitened patches, extracted at random uniformly
from the CIFAR-100 train dataset.

This script is intended to reproduce the preprocessing used by Adam Coates et. al. in their work from
the first half of 2011 on the CIFAR-10 and STL-10 datasets.
"""

from pylearn2.utils import serial
from pylearn2.datasets import preprocessing
from pylearn2.datasets.cifar100 import CIFAR100
from pylearn2.utils import string_utils

data_dir = string_utils.preprocess('${PYLEARN2_DATA_PATH}')

print 'Loading CIFAR-100 train dataset...'
data = CIFAR100(which_set = 'train')

print "Preparing output directory..."
patch_dir = data_dir + '/cifar100/cifar100_patches'
serial.mkdir( patch_dir )
README = open(patch_dir + '/README','w')

README.write("""
The .pkl files in this directory may be opened in python using
cPickle, pickle, or pylearn2.serial.load.

data.pkl contains a pylearn2 Dataset object defining an unlabeled
dataset of 2 million 6x6 approximately whitened, contrast-normalized
patches drawn uniformly at random from the CIFAR-100 train set.

preprocessor.pkl contains a pylearn2 Pipeline object that was used
to extract the patches and approximately whiten / contrast normalize
them. This object is necessary when extracting features for
supervised learning or test set classification, because the
extracted features must be computed using inputs that have been
whitened with the ZCA matrix learned and stored by this Pipeline.

They were created with the pylearn2 script make_cifar100_patches.py.

All other files in this directory, including this README, were
created by the same script and are necessary for the other files
to function correctly.
""")

README.close()

print "Preprocessing the data..."
pipeline = preprocessing.Pipeline()
pipeline.items.append(preprocessing.ExtractPatches(patch_shape=(6,6),num_patches=2*1000*1000))
pipeline.items.append(preprocessing.GlobalContrastNormalization(sqrt_bias=10., use_std=True))
pipeline.items.append(preprocessing.ZCA())
data.apply_preprocessor(preprocessor = pipeline, can_fit = True)

data.use_design_loc(patch_dir + '/data.npy')

serial.save(patch_dir + '/data.pkl',data)

serial.save(patch_dir + '/preprocessor.pkl',pipeline)

########NEW FILE########
__FILENAME__ = make_cifar100_patches_8x8
"""
This script makes a dataset of two million approximately whitened patches, extracted at random uniformly
from the CIFAR-100 train dataset.

This script is intended to reproduce the preprocessing used by Adam Coates et. al. in their work from
the first half of 2011 on the CIFAR-10 and STL-10 datasets.
"""

from pylearn2.utils import serial
from pylearn2.datasets import preprocessing
from pylearn2.datasets.cifar100 import CIFAR100
from pylearn2.utils import string

data_dir = string.preprocess('${PYLEARN2_DATA_PATH}')

print 'Loading CIFAR-100 train dataset...'
data = CIFAR100(which_set = 'train')

print "Preparing output directory..."
patch_dir = data_dir + '/cifar100/cifar100_patches_8x8'
serial.mkdir( patch_dir )
README = open(patch_dir + '/README','w')

README.write("""
The .pkl files in this directory may be opened in python using
cPickle, pickle, or pylearn2.serial.load.

data.pkl contains a pylearn2 Dataset object defining an unlabeled
dataset of 2 million 8x8 approximately whitened, contrast-normalized
patches drawn uniformly at random from the CIFAR-100 train set.

preprocessor.pkl contains a pylearn2 Pipeline object that was used
to extract the patches and approximately whiten / contrast normalize
them. This object is necessary when extracting features for
supervised learning or test set classification, because the
extracted features must be computed using inputs that have been
whitened with the ZCA matrix learned and stored by this Pipeline.

They were created with the pylearn2 script make_cifar100_patches.py.

All other files in this directory, including this README, were
created by the same script and are necessary for the other files
to function correctly.
""")

README.close()

print "Preprocessing the data..."
pipeline = preprocessing.Pipeline()
pipeline.items.append(preprocessing.ExtractPatches(patch_shape=(8,8),num_patches=2*1000*1000))
pipeline.items.append(preprocessing.GlobalContrastNormalization(sqrt_bias=10., use_std=True))
pipeline.items.append(preprocessing.ZCA())
data.apply_preprocessor(preprocessor = pipeline, can_fit = True)

data.use_design_loc(patch_dir + '/data.npy')

serial.save(patch_dir + '/data.pkl',data)

serial.save(patch_dir + '/preprocessor.pkl',pipeline)

########NEW FILE########
__FILENAME__ = make_cifar100_whitened
"""
This script makes a dataset of 32x32 approximately whitened CIFAR-10 images.

"""

from pylearn2.utils import serial
from pylearn2.datasets import preprocessing
from pylearn2.utils import string
from pylearn2.datasets.cifar100 import CIFAR100

data_dir = string.preprocess('${PYLEARN2_DATA_PATH}/cifar100')

print 'Loading CIFAR-100 train dataset...'
train = CIFAR100(which_set = 'train')

print "Preparing output directory..."
output_dir = data_dir + '/whitened'
serial.mkdir( output_dir )
README = open(output_dir + '/README','w')

README.write("""
The .pkl files in this directory may be opened in python using
cPickle, pickle, or pylearn2.serial.load.

train.pkl, and test.pkl each contain
a pylearn2 Dataset object defining a labeled
dataset of an approximately whitened version of the CIFAR-100
dataset. train.pkl contains labeled train examples. test.pkl
contains labeled test examples.

preprocessor.pkl contains a pylearn2 ZCA object that was used
to approximately whiten the images. You may want to use this
object later to preprocess other images.

They were created with the pylearn2 script make_cifar10_whitened.py.

All other files in this directory, including this README, were
created by the same script and are necessary for the other files
to function correctly.
""")

README.close()

print "Learning the preprocessor and preprocessing the unsupervised train data..."
preprocessor = preprocessing.ZCA()
train.apply_preprocessor(preprocessor = preprocessor, can_fit = True)

print 'Saving the unsupervised data'
train.use_design_loc(output_dir+'/train.npy')
serial.save(output_dir + '/train.pkl', train)

print "Loading the test data"
test = CIFAR100(which_set = 'test')

print "Preprocessing the test data"
test.apply_preprocessor(preprocessor = preprocessor, can_fit = False)

print "Saving the test data"
test.use_design_loc(output_dir+'/test.npy')
serial.save(output_dir+'/test.pkl', test)

serial.save(output_dir + '/preprocessor.pkl',preprocessor)


########NEW FILE########
__FILENAME__ = make_cifar10_gcn_whitened
"""
This script makes a dataset of 32x32 contrast normalized, approximately
whitened CIFAR-10 images.

"""

from pylearn2.utils import serial
from pylearn2.datasets import preprocessing
from pylearn2.utils import string_utils
from pylearn2.datasets.cifar10 import CIFAR10

data_dir = string_utils.preprocess('${PYLEARN2_DATA_PATH}/cifar10')

print 'Loading CIFAR-10 train dataset...'
train = CIFAR10(which_set = 'train', gcn = 55.)

print "Preparing output directory..."
output_dir = data_dir + '/pylearn2_gcn_whitened'
serial.mkdir( output_dir )
README = open(output_dir + '/README','w')

README.write("""
The .pkl files in this directory may be opened in python using cPickle,
pickle, or pylearn2.serial.load.

train.pkl, and test.pkl each contain a pylearn2 Dataset object defining a
labeled dataset of a 32x32 contrast normalized, approximately whitened version
of the CIFAR-10 dataset. train.pkl contains labeled train examples. test.pkl
contains labeled test examples.

preprocessor.pkl contains a pylearn2 ZCA object that was used to approximately
whiten the images. You may want to use this object later to preprocess other
images.

They were created with the pylearn2 script make_cifar10_gcn_whitened.py.

All other files in this directory, including this README, were created by the
same script and are necessary for the other files to function correctly.
""")

README.close()

print "Learning the preprocessor and preprocessing the unsupervised train data..."
preprocessor = preprocessing.ZCA()
train.apply_preprocessor(preprocessor = preprocessor, can_fit = True)

print 'Saving the unsupervised data'
train.use_design_loc(output_dir+'/train.npy')
serial.save(output_dir + '/train.pkl', train)

print "Loading the test data"
test = CIFAR10(which_set = 'test', gcn = 55.)

print "Preprocessing the test data"
test.apply_preprocessor(preprocessor = preprocessor, can_fit = False)

print "Saving the test data"
test.use_design_loc(output_dir+'/test.npy')
serial.save(output_dir+'/test.pkl', test)

serial.save(output_dir + '/preprocessor.pkl',preprocessor)


########NEW FILE########
__FILENAME__ = make_cifar10_whitened
"""
This script makes a dataset of 32x32 approximately whitened CIFAR-10 images.

"""

from pylearn2.utils import serial
from pylearn2.datasets import preprocessing
from pylearn2.utils import string_utils
import numpy as np
from pylearn2.datasets.cifar10 import CIFAR10

data_dir = string_utils.preprocess('${PYLEARN2_DATA_PATH}/cifar10')

print 'Loading CIFAR-10 train dataset...'
train = CIFAR10(which_set = 'train')

print "Preparing output directory..."
output_dir = data_dir + '/pylearn2_whitened'
serial.mkdir( output_dir )
README = open(output_dir + '/README','w')

README.write("""
The .pkl files in this directory may be opened in python using
cPickle, pickle, or pylearn2.serial.load.

train.pkl, and test.pkl each contain
a pylearn2 Dataset object defining a labeled
dataset of a 32x32 approximately whitened version of the STL-10
dataset. train.pkl contains labeled train examples. test.pkl
contains labeled test examples.

preprocessor.pkl contains a pylearn2 ZCA object that was used
to approximately whiten the images. You may want to use this
object later to preprocess other images.

They were created with the pylearn2 script make_cifar10_whitened.py.

All other files in this directory, including this README, were
created by the same script and are necessary for the other files
to function correctly.
""")

README.close()

print "Learning the preprocessor and preprocessing the unsupervised train data..."
preprocessor = preprocessing.ZCA()
train.apply_preprocessor(preprocessor = preprocessor, can_fit = True)

print 'Saving the unsupervised data'
train.use_design_loc(output_dir+'/train.npy')
serial.save(output_dir + '/train.pkl', train)

print "Loading the test data"
test = CIFAR10(which_set = 'test')

print "Preprocessing the test data"
test.apply_preprocessor(preprocessor = preprocessor, can_fit = False)

print "Saving the test data"
test.use_design_loc(output_dir+'/test.npy')
serial.save(output_dir+'/test.pkl', test)

serial.save(output_dir + '/preprocessor.pkl',preprocessor)


########NEW FILE########
__FILENAME__ = make_downsampled_stl10
"""
Makes a version of the STL-10 dataset that has been downsampled by a factor of
3 along both axes.

This is to mimic the first step of preprocessing used in
'An Analysis of Single-Layer Networks in Unsupervised Feature Learning'
by Adam Coates, Honglak Lee, and Andrew Y. Ng

This script also translates the data to lie in [-127.5, 127.5] instead of
[0,255]. This makes it play nicer with some of pylearn's visualization tools.
"""

from pylearn2.datasets.stl10 import STL10
from pylearn2.datasets.preprocessing import Downsample
from pylearn2.utils import string_utils as string
from pylearn2.utils import serial
import numpy as np

print 'Preparing output directory...'

data_dir = string.preprocess('${PYLEARN2_DATA_PATH}')
downsampled_dir = data_dir + '/stl10_32x32'
serial.mkdir( downsampled_dir )
README = open(downsampled_dir + '/README','w')

README.write("""
The .pkl files in this directory may be opened in python using
cPickle, pickle, or pylearn2.serial.load. They contain pylearn2
Dataset objects defining the STL-10 dataset, but downsampled to
size 32x32 and translated to lie in [-127.5, 127.5 ].

They were created with the pylearn2 script make_downsampled_stl10.py

All other files in this directory, including this README, were
created by the same script and are necessary for the other files
to function correctly.
""")

README.close()

preprocessor = Downsample(sampling_factor = [3, 3] )


#Unlabeled dataset is huge, so do it in chunks
#(After downsampling it should be small enough to work with)
final_unlabeled = np.zeros((100*1000,32*32*3),dtype='float32')

for i in xrange(10):
    print 'Loading unlabeled chunk '+str(i+1)+'/10...'
    unlabeled = STL10(which_set = 'unlabeled', center = True,
            example_range = (i * 10000, (i+1) * 10000))

    print 'Preprocessing unlabeled chunk...'
    print 'before ',(unlabeled.X.min(),unlabeled.X.max())
    unlabeled.apply_preprocessor(preprocessor)
    print 'after ',(unlabeled.X.min(), unlabeled.X.max())

    final_unlabeled[i*10000:(i+1)*10000,:] = unlabeled.X

unlabeled.set_design_matrix(final_unlabeled)
print 'Saving unlabeleding set...'
unlabeled.enable_compression()
unlabeled.use_design_loc(downsampled_dir + '/unlabeled.npy')
serial.save(downsampled_dir+'/unlabeled.pkl',unlabeled)

del unlabeled
import gc
gc.collect()

print 'Loading testing set...'
test = STL10(which_set = 'test', center = True)

print 'Preprocessing testing set...'
print 'before ',(test.X.min(),test.X.max())
test.apply_preprocessor(preprocessor)
print 'after ',(test.X.min(), test.X.max())

print 'Saving testing set...'
test.enable_compression()
test.use_design_loc(downsampled_dir + '/test.npy')
serial.save(downsampled_dir+'/test.pkl',test)
del test

print 'Loading training set...'
train = STL10(which_set = 'train', center = True)

print 'Preprocessing training set...'
print 'before ',(train.X.min(),train.X.max())
train.apply_preprocessor(preprocessor)
print 'after ',(train.X.min(), train.X.max())

print 'Saving training set...'
train.enable_compression()
train.use_design_loc(downsampled_dir + '/train.npy')
serial.save(downsampled_dir+'/train.pkl',train)

del train


########NEW FILE########
__FILENAME__ = make_mnistplus
"""
Script to generate the MNIST+ dataset. The purpose of this dataset is to make a
more challenging MNIST-like dataset, with multiple factors of variation. These
factors can serve to evaluate a model's performance at learning invariant
features, or its ability to disentangle factors of variation in a multi-task
classification setting. The dataset is stored under $PYLEARN2_DATA_PATH.

The dataset variants are created as follows. For each MNIST image, we:
1. Perform a random rotation of the image (optional)
2. Rescale the image from 28x28 to 48x48, yielding variable `image`.
3.1 Extract a random patch `textured_patch` from a fixed or random image of the
Brodatz texture dataset.
3.2 Generate mask of MNIST digit outline, by thresholding MNIST digit at 0.1
3.3 Fuse MNIST digit and textured patch as follows:
    textured_patch[mask] <= image[mask]; image <= textured_patch;
4. Randomly select position of light source (optional)
5. Perform embossing operation, given fixed lighting position obtained in 4.
"""
import numpy
import pickle
import pylab as pl

from copy import copy
from optparse import OptionParser

from pylearn2.datasets import mnist
from pylearn2.utils import string_utils

import warnings
try:
    from PIL import Image
except ImportError:
    warnings.warn("Couldn't import Image from PIL, so far make_mnistplus "
            "is only supported with PIL")


OUTPUT_SIZE = 48
DOWN_SAMPLE = 1


def to_array(img):
    """
    Convert PIL.Image to numpy.ndarray.
    :param img: numpy.ndarray
    """
    return numpy.array(img.getdata()) / 255.


def to_img(arr, os):
    """
    Convert numpy.ndarray to PIL.Image
    :param arr: numpy.ndarray
    :param os: integer, size of output image.
    """
    return Image.fromarray(arr.reshape(os, os) * 255.)


def emboss(img, azi=45., ele=18., dep=2):
    """
    Perform embossing of image `img`.
    :param img: numpy.ndarray, matrix representing image to emboss.
    :param azi: azimuth (in degrees)
    :param ele: elevation (in degrees)
    :param dep: depth, (0-100)
    """
    # defining azimuth, elevation, and depth
    ele = (ele * 2 * numpy.pi) / 360.
    azi = (azi * 2 * numpy.pi) / 360.

    a = numpy.asarray(img).astype('float')
    # find the gradient
    grad = numpy.gradient(a)
    # (it is two arrays: grad_x and grad_y)
    grad_x, grad_y = grad
    # getting the unit incident ray
    gd = numpy.cos(ele) # length of projection of ray on ground plane
    dx = gd * numpy.cos(azi)
    dy = gd * numpy.sin(azi)
    dz = numpy.sin(ele)
    # adjusting the gradient by the "depth" factor
    # (I think this is how GIMP defines it)
    grad_x = grad_x * dep / 100.
    grad_y = grad_y * dep / 100.
    # finding the unit normal vectors for the image
    leng = numpy.sqrt(grad_x**2 + grad_y**2 + 1.)
    uni_x = grad_x/leng
    uni_y = grad_y/leng
    uni_z = 1./leng
    # take the dot product
    a2 = 255 * (dx*uni_x + dy*uni_y + dz*uni_z)
    # avoid overflow
    a2 = a2.clip(0, 255)
    # you must convert back to uint8 /before/ converting to an image
    return Image.fromarray(a2.astype('uint8'))


def extract_patch(textid, os, downsample):
    """
    Extract a patch of texture #textid of Brodatz dataset.
    :param textid: id of texture image to load.
    :param os: size of MNIST+ output images.
    :param downsample: integer, downsampling factor.
    """
    temp = '${PYLEARN2_DATA_PATH}/textures/brodatz/D%i.gif' % textid
    fname = string_utils.preprocess(temp)

    img_i = Image.open(fname)
    img_i = img_i.resize((img_i.size[0]/downsample,
                          img_i.size[1]/downsample), Image.BILINEAR)

    x = numpy.random.randint(0, img_i.size[0] - os)
    y = numpy.random.randint(0, img_i.size[1] - os)
    patch = img_i.crop((x, y, x+os, y+os))

    return patch, (x, y)


def gendata(enable, os, downsample, textid=None, seed=2313, verbose=False):
    """
    Generate the MNIST+ dataset.
    :param enable: dictionary of flags with keys ['texture', 'azimuth',
    'rotation', 'elevation'] to enable/disable a given factor of variation.
    :param textid: if enable['texture'], id number of the Brodatz texture to
    load. If textid is None, we load a random texture for each MNIST image.
    :param os: output size (width and height) of MNIST+ images.
    :param downsample: factor by which to downsample texture.
    :param seed: integer for seeding RNG.
    :param verbose: bool
    """
    rng = numpy.random.RandomState(seed)

    data  = mnist.MNIST('train')
    test  = mnist.MNIST('test')
    data.X = numpy.vstack((data.X, test.X))
    data.y = numpy.hstack((data.y, test.y))
    del test

    output = {}
    output['data']  = numpy.zeros((len(data.X), os*os))
    output['label'] = numpy.zeros(len(data.y))
    if enable['azimuth']:
        output['azimuth'] = numpy.zeros(len(data.y))
    if enable['elevation']:
        output['elevation'] = numpy.zeros(len(data.y))
    if enable['rotation']:
        output['rotation'] = numpy.zeros(len(data.y))
    if enable['texture']:
        output['texture_id']  = numpy.zeros(len(data.y))
        output['texture_pos'] = numpy.zeros((len(data.y), 2))

    for i in xrange(len(data.X)):

        # get MNIST image
        frgd_img = to_img(data.X[i], 28)
        frgd_img = frgd_img.convert('L')

        if enable['rotation']:
            rot = rng.randint(0, 360)
            output['rotation'][i] = rot
            frgd_img = frgd_img.rotate(rot, Image.BILINEAR)

        frgd_img = frgd_img.resize((os, os), Image.BILINEAR)

        if enable['texture']:

            if textid is None:
                # extract patch from texture database. Note that texture #14
                # does not exist.
                textid = 14
                while textid == 14:
                    textid = rng.randint(1, 113)

            patch_img, (px, py) = extract_patch(textid, os, downsample)
            patch_arr = to_array(patch_img)

            # store output details
            output['texture_id'][i] = textid
            output['texture_pos'][i] = (px, py)

            # generate binary mask for digit outline
            frgd_arr = to_array(frgd_img)
            mask_arr = frgd_arr > 0.1

            # copy contents of masked-MNIST image into background texture
            blend_arr = copy(patch_arr)
            blend_arr[mask_arr] = frgd_arr[mask_arr]

            # this now because the image to emboss
            frgd_img = to_img(blend_arr, os)

        azi = 45
        if enable['azimuth']:
            azi = rng.randint(0, 360)
            output['azimuth'][i] = azi
        ele = 18.
        if enable['elevation']:
            ele = rng.randint(0, 60)
            output['elevation'][i] = ele

        mboss_img = emboss(frgd_img, azi=azi, ele=ele)
        mboss_arr = to_array(mboss_img)

        output['data'][i] = mboss_arr
        output['label'][i] = data.y[i]

        if verbose:
            pl.imshow(mboss_arr.reshape(os, os))
            pl.gray()
            pl.show()

    fname = 'mnistplus'
    if enable['azimuth']:
        fname += "_azi"
    if enable['rotation']:
        fname += "_rot"
    if enable['texture']:
        fname += "_tex"
    fp = open(fname+'.pkl','w')
    pickle.dump(output, fp, protocol=pickle.HIGHEST_PROTOCOL)
    fp.close()

if __name__ == '__main__':
    parser = OptionParser()
    parser.add_option('-v', action='store_true', dest='verbose')
    parser.add_option('--azimuth', action='store_true', dest='azimuth',
            help='Enable random azimuth for light-source used in embossing.')
    parser.add_option('--elevation', action='store_true', dest='elevation',
            help='Enable random elevation for light-source used in embossing.')
    parser.add_option('--rotation', action='store_true', dest='rotation',
            help='Randomly rotate MNIST digit prior to embossing.')
    parser.add_option('--texture', action='store_true', dest='texture',
            help='Perform joint embossing of fused {MNIST + Texture} image.')
    parser.add_option('--textid', action='store', type='int', dest='textid',
            help='If specified, use a single texture ID for all MNIST images.',
            default=None)
    parser.add_option('--output_size', action='store', type='int', dest='os',
            help='Integer specifying size of (square) output images.',
            default=OUTPUT_SIZE)
    parser.add_option('--downsample', action='store', type='int',
            dest='downsample', default=DOWN_SAMPLE,
            help='Downsampling factor for Brodatz textures.')
    (opts, args) = parser.parse_args()

    enable = {'texture':   opts.texture,
              'azimuth':   opts.azimuth,
              'rotation':  opts.rotation,
              'elevation': opts.elevation}

    gendata(enable=enable, os=opts.os, downsample=opts.downsample,
            verbose=opts.verbose, textid=opts.textid)

########NEW FILE########
__FILENAME__ = make_stl10_patches
"""
This script makes a dataset of two million approximately whitened patches, extracted at random uniformly
from a downsampled version of the STL-10 unlabeled and train dataset.

It assumes that you have already run make_downsampled_stl10.py, which downsamples the STL-10 images to
1/3 of their original resolution.

This script is intended to reproduce the preprocessing used by Adam Coates et. al. in their work from
the first half of 2011.
"""

from pylearn2.utils import serial
from pylearn2.datasets import preprocessing
from pylearn2.utils import string_utils as string
import numpy as np

data_dir = string.preprocess('${PYLEARN2_DATA_PATH}/stl10')

print 'Loading STL10-10 unlabeled and train datasets...'
downsampled_dir = data_dir + '/stl10_32x32'

data = serial.load(downsampled_dir + '/unlabeled.pkl')
supplement = serial.load(downsampled_dir + '/train.pkl')

print 'Concatenating datasets...'
data.set_design_matrix(np.concatenate((data.X,supplement.X),axis=0))
del supplement


print "Preparing output directory..."
patch_dir = data_dir + '/stl10_patches'
serial.mkdir( patch_dir )
README = open(patch_dir + '/README','w')

README.write("""
The .pkl files in this directory may be opened in python using
cPickle, pickle, or pylearn2.serial.load.

data.pkl contains a pylearn2 Dataset object defining an unlabeled
dataset of 2 million 6x6 approximately whitened, contrast-normalized
patches drawn uniformly at random from a downsampled (to 32x32)
version of the STL-10 train and unlabeled datasets.

preprocessor.pkl contains a pylearn2 Pipeline object that was used
to extract the patches and approximately whiten / contrast normalize
them. This object is necessary when extracting features for
supervised learning or test set classification, because the
extracted features must be computed using inputs that have been
whitened with the ZCA matrix learned and stored by this Pipeline.

They were created with the pylearn2 script make_stl10_patches.py.

All other files in this directory, including this README, were
created by the same script and are necessary for the other files
to function correctly.
""")

README.close()

print "Preprocessing the data..."
pipeline = preprocessing.Pipeline()
pipeline.items.append(preprocessing.ExtractPatches(patch_shape=(6,6),num_patches=2*1000*1000))
pipeline.items.append(preprocessing.GlobalContrastNormalization(use_std=True, sqrt_bias=10.))
pipeline.items.append(preprocessing.ZCA())
data.apply_preprocessor(preprocessor = pipeline, can_fit = True)

data.use_design_loc(patch_dir + '/data.npy')

serial.save(patch_dir + '/data.pkl',data)

serial.save(patch_dir + '/preprocessor.pkl',pipeline)

########NEW FILE########
__FILENAME__ = make_stl10_patches_8x8
"""
This script makes a dataset of two million approximately whitened patches, extracted at random uniformly
from a downsampled version of the STL-10 unlabeled and train dataset.

It assumes that you have already run make_downsampled_stl10.py, which downsamples the STL-10 images to
1/3 of their original resolution.

This script is intended to reproduce the preprocessing used by Adam Coates et. al. in their work from
the first half of 2011.
"""

from pylearn2.utils import serial
from pylearn2.datasets import preprocessing
from pylearn2.utils import string_utils as string
import numpy as np

data_dir = string.preprocess('${PYLEARN2_DATA_PATH}/stl10')

print 'Loading STL10-10 unlabeled and train datasets...'
downsampled_dir = data_dir + '/stl10_32x32'

data = serial.load(downsampled_dir + '/unlabeled.pkl')
supplement = serial.load(downsampled_dir + '/train.pkl')

print 'Concatenating datasets...'
data.set_design_matrix(np.concatenate((data.X,supplement.X),axis=0))
del supplement


print "Preparing output directory..."
patch_dir = data_dir + '/stl10_patches_8x8'
serial.mkdir( patch_dir )
README = open(patch_dir + '/README','w')

README.write("""
The .pkl files in this directory may be opened in python using
cPickle, pickle, or pylearn2.serial.load.

data.pkl contains a pylearn2 Dataset object defining an unlabeled
dataset of 2 million 6x6 approximately whitened, contrast-normalized
patches drawn uniformly at random from a downsampled (to 32x32)
version of the STL-10 train and unlabeled datasets.

preprocessor.pkl contains a pylearn2 Pipeline object that was used
to extract the patches and approximately whiten / contrast normalize
them. This object is necessary when extracting features for
supervised learning or test set classification, because the
extracted features must be computed using inputs that have been
whitened with the ZCA matrix learned and stored by this Pipeline.

They were created with the pylearn2 script make_stl10_patches.py.

All other files in this directory, including this README, were
created by the same script and are necessary for the other files
to function correctly.
""")

README.close()

print "Preprocessing the data..."
pipeline = preprocessing.Pipeline()
pipeline.items.append(preprocessing.ExtractPatches(patch_shape=(8,8),num_patches=2*1000*1000))
pipeline.items.append(preprocessing.GlobalContrastNormalization(sqrt_bias=10., use_std=True))
pipeline.items.append(preprocessing.ZCA())
data.apply_preprocessor(preprocessor = pipeline, can_fit = True)

data.use_design_loc(patch_dir + '/data.npy')

serial.save(patch_dir + '/data.pkl',data)

serial.save(patch_dir + '/preprocessor.pkl',pipeline)

########NEW FILE########
__FILENAME__ = make_stl10_whitened
"""
This script makes a dataset of 32x32 approximately whitened STL-10 images.

It assumes that you have already run make_downsampled_stl10.py, which downsamples the STL-10 images to
1/3 of their original resolution.

"""

from pylearn2.utils import serial
from pylearn2.datasets import preprocessing
from pylearn2.utils import string_utils as string
import numpy as np

data_dir = string.preprocess('${PYLEARN2_DATA_PATH}/stl10')

print 'Loading STL-10 unlabeled and train datasets...'
downsampled_dir = data_dir + '/stl10_32x32'

data = serial.load(downsampled_dir + '/unlabeled.pkl')
supplement = serial.load(downsampled_dir + '/train.pkl')

print 'Concatenating datasets...'
data.set_design_matrix(np.concatenate((data.X,supplement.X),axis=0))


print "Preparing output directory..."
output_dir = data_dir + '/stl10_32x32_whitened'
serial.mkdir( output_dir )
README = open(output_dir + '/README','w')

README.write("""
The .pkl files in this directory may be opened in python using
cPickle, pickle, or pylearn2.serial.load.

unsupervised.pkl, unlabeled.pkl, train.pkl, and test.pkl each contain
a pylearn2 Dataset object defining an unlabeled
dataset of a 32x32 approximately whitened version of the STL-10
dataset. unlabeled.pkl contains unlabeled train examples. train.pkl
contains labeled train examples. unsupervised.pkl contains the union
of these (without any labels). test.pkl contains the labeled test
examples.

preprocessor.pkl contains a pylearn2 ZCA object that was used
to approximately whiten the images. You may want to use this
object later to preprocess other images.

They were created with the pylearn2 script make_stl10_whitened.py.

All other files in this directory, including this README, were
created by the same script and are necessary for the other files
to function correctly.
""")

README.close()

print "Learning the preprocessor and preprocessing the unsupervised train data..."
preprocessor = preprocessing.ZCA()
data.apply_preprocessor(preprocessor = preprocessor, can_fit = True)

print 'Saving the unsupervised data'
data.use_design_loc(output_dir+'/unsupervised.npy')
serial.save(output_dir + '/unsupervised.pkl', data)

X = data.X
unlabeled = X[0:100*1000,:]
labeled = X[100*1000:,:]
del X

print "Saving the unlabeled data"
data.X = unlabeled
data.use_design_loc(output_dir + '/unlabeled.npy')
serial.save(output_dir + '/unlabeled.pkl',data)
del data
del unlabeled

print "Saving the labeled train data"
supplement.X = labeled
supplement.use_design_loc(output_dir+'/train.npy')
serial.save(output_dir+'/train.pkl', supplement)
del supplement
del labeled

print "Loading the test data"
test = serial.load(downsampled_dir + '/test.pkl')

print "Preprocessing the test data"
test.apply_preprocessor(preprocessor = preprocessor, can_fit = False)

print "Saving the test data"
test.use_design_loc(output_dir+'/test.npy')
serial.save(output_dir+'/test.pkl', test)

serial.save(output_dir + '/preprocessor.pkl',preprocessor)

########NEW FILE########
__FILENAME__ = step_through_norb_foveated
__author__ = "Ian Goodfellow"
"""
A script for sequentially stepping through FoveatedNORB, viewing each image
and its label.
"""

import numpy as np

from pylearn2.datasets.norb_small import FoveatedNORB
from pylearn2.gui.patch_viewer import PatchViewer
from pylearn2.utils import get_choice

print 'Use test set?'
choices = {'y': 'test', 'n': 'train'}
which_set = choices[get_choice(choices)]

dataset = FoveatedNORB(which_set=which_set, center=True)

topo = dataset.get_topological_view()

b, r, c, ch = topo.shape

assert ch == 2

pv = PatchViewer((1, 2), (r, c), is_color=False)

i = 0
while True:
    patch = topo[i, :, :, :]
    patch = patch / np.abs(patch).max()

    pv.add_patch(patch[:,:,1], rescale=False)
    pv.add_patch(patch[:,:,0], rescale=False)

    pv.show()

    print dataset.y[i]

    choices = {'g': 'goto image', 'q': 'quit'}

    if i + 1 < b:
        choices['n'] = 'next image'

    choice = get_choice(choices)

    if choice == 'q':
        quit()

    if choice == 'n':
        i += 1

    if choice == 'g':
        i = int(raw_input('index: '))

########NEW FILE########
__FILENAME__ = step_through_small_norb
#! /usr/bin/env python

"""
A script for sequentially stepping through SmallNORB, viewing each image and
its label.

Intended as a demonstration of how to iterate through NORB images,
and as a way of testing SmallNORB's StereoViewConverter.

If you just want an image viewer, consider
pylearn2/scripts/show_binocular_grayscale_images.py,
which is not specific to SmallNORB.
"""

__author__ = "Matthew Koichi Grimes"
__copyright__ = "Copyright 2010-2014, Universite de Montreal"
__credits__ = __author__
__license__ = "3-clause BSD"
__maintainer__ = __author__
__email__ = "mkg alum mit edu (@..)"


import argparse, pickle, sys
from matplotlib import pyplot
from pylearn2.datasets.norb import SmallNORB
from pylearn2.utils import safe_zip


def main():

    def parse_args():
        parser = argparse.ArgumentParser(
            description="Step-through visualizer for SmallNORB dataset")

        parser.add_argument("--which_set",
                            default='train',
                            required=True,
                            help=("'train', 'test', or the path to a "
                                  "SmallNORB .pkl file"))
        return parser.parse_args()

    def load_norb(args):
        if args.which_set in ('test', 'train'):
            return SmallNORB(args.which_set, True)
        else:
            norb_file = open(args.which_set)
            return pickle.load(norb_file)

    args = parse_args()
    norb = load_norb(args)
    topo_space = norb.view_converter.topo_space  # does not include label space
    vec_space = norb.get_data_specs()[0].components[0]

    figure, axes = pyplot.subplots(1, 2, squeeze=True)
    figure.suptitle("Press space to step through, or 'q' to quit.")

    def draw_and_increment(iterator):
        """
        Draws the image pair currently pointed at by the iterator,
        then increments the iterator.
        """

        def draw(batch_pair):
            for axis, image_batch in safe_zip(axes, batch_pair):
                assert image_batch.shape[0] == 1
                grayscale_image = image_batch[0, :, :, 0]
                axis.imshow(grayscale_image, cmap='gray')

            figure.canvas.draw()

        def get_values_and_increment(iterator):
            try:
                vec_stereo_pair, labels = norb_iter.next()
            except StopIteration:
                return (None, None)

            topo_stereo_pair = vec_space.np_format_as(vec_stereo_pair,
                                                      topo_space)
            return topo_stereo_pair, labels

        batch_pair, labels = get_values_and_increment(norb_iter)
        draw(batch_pair)

    norb_iter = norb.iterator(mode='sequential',
                              batch_size=1,
                              data_specs=norb.get_data_specs())

    def on_key_press(event):
        if event.key == ' ':
            draw_and_increment(norb_iter)
        if event.key == 'q':
            sys.exit(0)

    figure.canvas.mpl_connect('key_press_event', on_key_press)
    draw_and_increment(norb_iter)
    pyplot.show()


if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = dbm_metrics
#!/usr/bin/env python
__authors__ = "Vincent Dumoulin"
__copyright__ = "Copyright 2013, Universite de Montreal"
__credits__ = ["Guillaume Desjargins", "Vincent Dumoulin"]
__license__ = "3-clause BSD"
__maintainer__ = "Vincent Dumoulin"

"""
This script computes both an estimate of the partition function of the provided
DBM model and an estimate of the log-likelihood on the given training and test
sets.

This is guaranteed to work only for DBMs with a BinaryVector visible layer and
BinaryVectorMaxPool hidden layers with pool sizes of 1.

It uses annealed importance sampling (AIS) to estimate Z, the partition
function.

TODO: add more details, cite paper


usage: dbm_metrics.py [-h] {ais} model_path

positional arguments:
    {ais}       the desired metric
    model_path  path to the pickled DBM model

optional arguments:
    -h, --help  show the help message and exit
"""

import argparse
import warnings
import numpy
import logging

import theano
import theano.tensor as T
from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams
from theano.sandbox.scan import scan
from theano.compat.python2x import OrderedDict

import pylearn2
from pylearn2.datasets.mnist import MNIST
from pylearn2.utils import serial
from pylearn2 import utils

floatX = theano.config.floatX
logging.basicConfig(level=logging.INFO)
rng = numpy.random.RandomState(9873242)
theano_rng = RandomStreams(rng.randint(2**30))


def _sample_even_odd(W_list, b_list, samples, beta, odd=True):
    """
    Sample from the even (or odd) layers given a list of previous states.

    Parameters
    ----------
    W_list : array-like object of theano shared variables
        Weight matrices of the DBM. Its first element is ignored, since in the
        Pylearn2 framework a visible layer does not have a weight matrix.
    b_list : array-like object of theano shared variables
        Biases of the DBM
    samples : array-like object of theano shared variables
        Samples corresponding to the previous states
    beta : theano.tensor.scalar
        Inverse temperature parameter
    odd : boolean
        Whether to sample from the odd or the even layers (defaults to sampling
        from odd layers)
    """
    for i in xrange(odd, len(samples), 2):
        samples[i] = sample_hi_given(samples, i, W_list, b_list, beta)


def _activation_even_odd(W_list, b_list, samples, beta, odd=True):
    """
    Compute the activation of the even (or odd) layers given a list of
    previous states.

    Parameters
    ----------
    W_list : array-like object of theano shared variables
        Weight matrices of the DBM. Its first element is ignored, since in the
        Pylearn2 framework a visible layer does not have a weight matrix.
    b_list : array-like object of theano shared variables
        Biases of the DBM
    samples : array-like object of theano shared variables
        Samples corresponding to the previous states
    beta : theano.tensor.scalar
        Inverse temperature parameter
    odd : boolean
        Whether to compute activation for the odd or the even layers (defaults
        to computing for odd layers)
    """
    for i in xrange(odd, len(samples), 2):
        samples[i] = hi_given(samples, i, W_list, b_list, beta,
                              apply_sigmoid=False)


def neg_sampling(W_list, b_list, nsamples, beta=1.0, pa_bias=None,
                 marginalize_odd=True, theano_rng=None):
    """
    Generate a sample from the intermediate distribution defined at inverse
    temperature 'beta', starting from state 'nsamples'. See file docstring for
    equation of p_k(h1).

    Parameters
    ----------
    W_list : array-like object of theano shared variables
        Weight matrices of the DBM. Its first element is ignored, since in the
        Pylearn2 framework a visible layer does not have a weight matrix.
    b_list : array-like object of theano shared variables
        Biases of the DBM
    nsamples : array-like object of theano shared variables
        Negative samples corresponding to the previous states
    beta : theano.tensor.scalar
        Inverse temperature parameter
    marginalize_odd : boolean
        Whether to marginalize odd layers
    theano_rng : theano RandomStreams
        Random number generator

    Returns
    -------
    new_nsamples : array-like object of symbolic matrices
        new_nsamples[i] contains new samples for i-th layer.
    """
    # There's as much layers in the DBM as there are bias vectors
    depth = len(b_list)

    new_nsamples = [nsamples[i] for i in xrange(depth)]

    # Contribution from model B, at temperature beta_k
    _sample_even_odd(W_list, b_list, new_nsamples, beta, odd=marginalize_odd)
    _activation_even_odd(W_list, b_list, new_nsamples, beta,
                         odd=not marginalize_odd)

    # Contribution from model A, at temperature (1 - beta_k)
    new_nsamples[not marginalize_odd] += pa_bias * (1. - beta)

    # Loop over all layers (not being marginalized)
    for i in xrange(not marginalize_odd, depth, 2):
        new_nsamples[i] = T.nnet.sigmoid(new_nsamples[i])
        new_nsamples[i] = theano_rng.binomial(
            size=nsamples[i].get_value().shape, n=1, p=new_nsamples[i],
            dtype=floatX
        )

    return new_nsamples


def free_energy_at_beta(W_list, b_list, samples, beta, pa_bias=None,
                        marginalize_odd=True):
    """
    Compute the free-energy of the sample 'h1_sample', for model p_k(h1).

    Parameters
    ----------
    W_list : array-like object of theano shared variables
        Weight matrices of the DBM. Its first element is ignored, since in the
        Pylearn2 framework a visible layer does not have a weight matrix.
    b_list : array-like object of theano shared variables
        Biases of the DBM
    samples : array-like object of theano shared variable
        Samples from which we extract the samples of layer h1
    beta : theano.tensor.scalar
        Inverse temperature beta_k of model p_k(h1) at which to measure the
        free-energy.
    pa_bias : array-like object of theano shared variables
        Biases for the A model
    marginalize_odd : boolean
        Whether to marginalize odd layers

    Returns
    -------
    fe : symbolic variable
        Free-energy of sample 'h1_sample', at inverse temperature beta
    """
    # There's as much layers in the DBM as there are bias vectors
    depth = len(b_list)

    fe = 0.

    # Contribution of biases
    keep_idx = numpy.arange(not marginalize_odd, depth, 2)
    for i in keep_idx:
        fe -= T.dot(samples[i], b_list[i]) * beta

    # Contribution of biases
    marg_idx = numpy.arange(marginalize_odd, depth, 2)
    for i in marg_idx:
        from_im1 = T.dot(samples[i-1], W_list[i]) if i >= 1 else 0.
        from_ip1 = T.dot(samples[i+1], W_list[i+1].T) if i < depth-1 else 0
        net_input = (from_im1 + from_ip1 + b_list[i]) * beta
        fe -= T.sum(T.nnet.softplus(net_input), axis=1)

    fe -= T.dot(samples[not marginalize_odd], pa_bias) * (1. - beta)

    return fe


def compute_log_ais_weights(batch_size, free_energy_fn, sample_fn, betas):
    """
    Compute log of the AIS weights

    Parameters
    ----------
    batch_size : scalar
        Size of a batch of samples
    free_energy_fn : theano.function
        Function which, given temperature beta_k, computes the free energy
        of the samples stored in model.samples. This function should return
        a symbolic vector.
    sample_fn : theano.function
        Function which, given temperature beta_k, generates samples h1 ~
        p_k(h1).
    betas : array-like object of scalars
        Inverse temperature parameters for which to compute the log_ais weights

    Returns
    -------
    log_ais_w : theano.tensor.vector
        Vector containing log ais-weights
    """
    # Initialize log-ais weights
    log_ais_w = numpy.zeros(batch_size, dtype=floatX)

    # Iterate from inverse  temperature beta_k=0 to beta_k=1...
    for i in range(len(betas) - 1):
        bp, bp1 = betas[i], betas[i+1]
        log_ais_w += free_energy_fn(bp) - free_energy_fn(bp1)
        sample_fn(bp1)
        if i % 1e3 == 0:
            logging.info('Temperature %f ' % bp1)

    return log_ais_w


def estimate_from_weights(log_ais_w):
    """
    Safely compute the log-average of the ais-weights

    Parameters
    ----------
    log_ais_w : theano.tensor.vector
        Symbolic vector containing log_ais_w^{(m)}.

    Returns
    -------
    dlogz : theano.tensor.scalar
        log(Z_B) - log(Z_A)
    var_dlogz : theano.tensor.scalar
        Variance of our estimator
    """
    # Utility function for safely computing log-mean of the ais weights
    ais_w = T.vector()
    max_ais_w = T.max(ais_w)
    dlogz = T.log(T.mean(T.exp(ais_w - max_ais_w))) + max_ais_w
    log_mean = theano.function([ais_w], dlogz, allow_input_downcast=False)

    # Estimate the log-mean of the AIS weights
    dlogz = log_mean(log_ais_w)

    # Estimate log-variance of the AIS weights
    # VAR(log(X)) \approx VAR(X) / E(X)^2 = E(X^2)/E(X)^2 - 1
    m = numpy.max(log_ais_w)
    var_dlogz = (log_ais_w.shape[0] *
                 numpy.sum(numpy.exp(2 * (log_ais_w - m))) /
                 numpy.sum(numpy.exp(log_ais_w - m)) ** 2 - 1.)

    return dlogz, var_dlogz


def compute_log_za(b_list, pa_bias, marginalize_odd=True):
    """
    Compute the exact partition function of model p_A(h1)

    Parameters
    ----------
    b_list : array-like object of theano shared variables
        Biases of the DBM
    pa_bias : array-like object of theano shared variables
        Biases for the A model
    marginalize_odd : boolean
        Whether to marginalize odd layers

    Returns
    -------
    log_za : scalar
        Partition function of model A
    """
    log_za = 0.

    for i, b in enumerate(b_list):
        if i == (not marginalize_odd):
            log_za += numpy.sum(numpy.log(1 + numpy.exp(pa_bias)))
        else:
            log_za += numpy.log(2) * b.get_value().shape[0]

    return log_za


def compute_likelihood_given_logz(nsamples, psamples, batch_size, energy_fn,
                                  inference_fn, log_z, test_x):
    """
    Compute test set likelihood as below, where q is the variational
    approximation to the posterior p(h1,h2|v).

        ln p(v) \approx \sum_h q(h) E(v,h1,h2) + H(q) - ln Z

    See section 3.2 of DBM paper for details.

    Parameters
    ----------
    nsamples : array-like object of theano shared variables
        Negative samples
    psamples : array-like object of theano shared variables
        Positive samples
    batch_size : scalar
        Size of a batch of samples
    energy_fn : theano.function
        Function which computes the (temperature 1) energy of the samples. This
        function should return a symbolic vector.
    inference_fn : theano.function
        Inference function for DBM. Function takes a T.matrix as input (data)
        and returns a list of length 'length(b_list)', where the i-th element
        is an ndarray containing approximate samples of layer i.
    log_z : scalar
        Estimate partition function of 'model'.
    test_x : numpy.ndarray
        Test set data, in dense design matrix format.

    Returns
    -------
    likelihood : scalar
        Negative log-likelihood of test data under the model
    """
    i = 0.
    likelihood = 0

    for i in xrange(0, len(test_x), batch_size):

        # Recast data as floatX and apply preprocessing if required
        x = numpy.array(test_x[i:numpy.minimum(test_x.shape[0], i + batch_size), :], dtype=floatX)
        batch_size0 = len(x)
        if len(x) < batch_size:
            # concatenate x to have some dummy entries
            x = numpy.concatenate((x, numpy.zeros((batch_size-len(x),x.shape[1]), dtype=floatX)), axis=0)

        # Perform inference
        inference_fn(x)

        # Entropy of h(q) adds contribution to variational lower-bound
        hq = 0
        for psample in psamples[1:]:
            temp = \
                - psample.get_value() * numpy.log(1e-5 + psample.get_value()) \
                - (1.-psample.get_value()) \
                * numpy.log(1. - psample.get_value() + 1e-5)
            hq += numpy.sum(temp, axis=1)

        # Copy into negative phase buffers to measure energy
        nsamples[0].set_value(x)
        for ii, psample in enumerate(psamples):
            if ii > 0:
                nsamples[ii].set_value(psample.get_value())

        # Compute sum of likelihood for current buffer
        x_likelihood = numpy.sum((-energy_fn(1.0) + hq - log_z)[:batch_size0])

        # Perform moving average of negative likelihood
        # Divide by len(x) and not bufsize, since last buffer might be smaller
        likelihood = (i * likelihood + x_likelihood) / (i + batch_size0)

    return likelihood


def hi_given(samples, i, W_list, b_list, beta=1.0, apply_sigmoid=True):
    """
    Compute the state of hidden layer i given all other layers

    Parameters
    ----------
    samples : array-like object of theano shared variables
        For the positive phase, samples[0] points to the input, while
        samples[i] contains the current state of the i-th layer. In the
        negative phase, samples[i] contains the persistent chain associated
        with the i-th layer.
    i : integer
        Compute activation of layer i of our DBM
    W_list : array-like object of theano shared variables
        Weight matrices of the DBM. Its first element is ignored, since in the
        Pylearn2 framework a visible layer does not have a weight matrix.
    b_list : array-like object of theano shared variables
        Biases of the DBM
    beta : scalar
        Inverse temperature parameter used when performing AIS
    apply_sigmoid : boolean
        When False, hi_given will not apply the sigmoid. Useful for AIS
        estimate.

    Returns
    -------
    hi_mean : symbolic variable
        Activation of the i-th layer
    """
    # There's as much layers in the DBM as there are bias vectors
    depth = len(samples)

    hi_mean = 0.
    if i < depth-1:
        # Top-down input
        wip1 = W_list[i+1]
        hi_mean += T.dot(samples[i+1], wip1.T) * beta

    if i > 0:
        # Bottom-up input
        wi = W_list[i]
        hi_mean += T.dot(samples[i-1], wi) * beta

    hi_mean += b_list[i] * beta

    if apply_sigmoid:
        return T.nnet.sigmoid(hi_mean)
    else:
        return hi_mean


def sample_hi_given(samples, i, W_list, b_list, beta=1.0):
    """
    Given current state of our DBM ('samples'), sample the values taken by
    the i-th layer.

    Parameters
    ----------
    samples : array-like object of theano shared variables
        For the positive phase, samples[0] points to the input, while
        samples[i] contains the current state of the i-th layer. In the
        negative phase, samples[i] contains the persistent chain associated
        with the i-th layer.
    i : integer
        Compute activation of layer i of our DBM
    W_list : array-like object of theano shared variables
        Weight matrices of the DBM. Its first element is ignored, since in the
        Pylearn2 framework a visible layer does not have a weight matrix.
    b_list : array-like object of theano shared variables
        Biases of the DBM
    beta : scalar
        Inverse temperature parameter used when performing AIS

    Returns
    -------
    hi_sample : symbolic variable
        State of the i-th layer
    """
    hi_mean = hi_given(samples, i, W_list, b_list, beta)

    hi_sample = theano_rng.binomial(
        size=samples[i].get_value().shape,
        n=1, p=hi_mean,
        dtype=floatX
    )

    return hi_sample


def _e_step(psamples, W_list, b_list, n_steps=100, eps=1e-5):
    """
    Performs 'n_steps' of mean-field inference (used to compute positive phase
    statistics)

    Parameters
    ----------
    psamples : array-like object of theano shared variables
        State of each layer of the DBM (during the inference process).
        psamples[0] points to the input
    n_steps :  integer
        Number of iterations of mean-field to perform
    """
    depth = len(psamples)

    new_psamples = [T.unbroadcast(T.shape_padleft(psample))
                    for psample in psamples]

    # now alternate mean-field inference for even/odd layers
    def mf_iteration(*psamples):
        new_psamples = [p for p in psamples]
        for i in xrange(1, depth, 2):
            new_psamples[i] = hi_given(psamples, i, W_list, b_list)
        for i in xrange(2, depth, 2):
            new_psamples[i] = hi_given(psamples, i, W_list, b_list)

        score = 0.
        for i in xrange(1, depth):
            score = T.maximum(T.mean(abs(new_psamples[i] - psamples[i])),
                              score)

        return new_psamples, theano.scan_module.until(score < eps)

    new_psamples, updates = scan(
        mf_iteration,
        states=new_psamples,
        n_steps=n_steps
    )

    return [x[0] for x in new_psamples]


def estimate_likelihood(W_list, b_list, trainset, testset, free_energy_fn=None,
                        batch_size=100, large_ais=False, log_z=None,
                        pos_mf_steps=50, pos_sample_steps=0):
    """
    Compute estimate of log-partition function and likelihood of trainset and
    testset

    Parameters
    ----------
    W_list : array-like object of theano shared variables
    b_list : array-like object of theano shared variables
        Biases of the DBM
    trainset : pylearn2.datasets.dataset.Dataset
        Training set
    testset : pylearn2.datasets.dataset.Dataset
        Test set
    free_energy_fn : theano.function
        Function which, given temperature beta_k, computes the free energy
        of the samples stored in model.samples. This function should return
        a symbolic vector.
    batch_size : integer
        Size of a batch of examples
    large_ais : boolean
        If True, will use 3e5 chains, instead of 3e4
    log_z : log-partition function (if precomputed)
    pos_mf_steps: the number of fixed-point iterations for approximate inference
    pos_sample_steps: same thing as pos_mf_steps
        when both pos_mf_steps > 0 and pos_sample_steps > 0,
        pos_mf_steps has a priority

    Returns
    -------
    nll : scalar
        Negative log-likelihood of data.X under `model`.
    logz : scalar
        Estimate of log-partition function of `model`.
    """

    warnings.warn("This is garanteed to work only for DBMs with a " +
                  "BinaryVector visible layer and BinaryVectorMaxPool " +
                  "hidden layers with pool sizes of 1.")

    # Add a dummy placeholder for visible layer's weights in W_list
    W_list = [None] + W_list

    # Depth of the DBM
    depth = len(b_list)

    # Initialize samples
    psamples = []
    nsamples = []
    for i, b in enumerate(b_list):
        psamples += [utils.sharedX(rng.rand(batch_size,
                                            b.get_value().shape[0]),
                                   name='psamples%i' % i)]
        nsamples += [utils.sharedX(rng.rand(batch_size,
                                            b.get_value().shape[0]),
                                   name='nsamples%i' % i)]
    psamples[0] = T.matrix('psamples0')

    ##########################
    ## BUILD THEANO FUNCTIONS
    ##########################
    beta = T.scalar()

    # For an even number of layers, we marginalize the odd layers
    # (and vice-versa)
    marginalize_odd = (depth % 2) == 0

    # Build function to retrieve energy.
    E = -T.dot(nsamples[0], b_list[0]) * beta
    for i in xrange(1, depth):
        E -= T.sum(T.dot(nsamples[i-1], W_list[i] * beta) * nsamples[i],
                   axis=1)
        E -= T.dot(nsamples[i], b_list[i] * beta)
    energy_fn = theano.function([beta], E)

    # Build inference function.
    assert (pos_mf_steps or pos_sample_steps)
    pos_steps = pos_mf_steps if pos_mf_steps else pos_sample_steps
    new_psamples = _e_step(psamples, W_list, b_list, n_steps=pos_steps)
    ups = OrderedDict()
    for psample, new_psample in zip(psamples[1:], new_psamples[1:]):
        ups[psample] = new_psample
    temp = numpy.asarray(trainset.X, dtype=floatX)
    mean_train = numpy.mean(temp, axis=0)
    inference_fn = theano.function(inputs=[psamples[0]], outputs=[],
                                   updates=ups)

    # Configure baserate bias for (h0 if `marginalize_odd` else h1)
    inference_fn(numpy.tile(mean_train, (batch_size, 1)))
    numpy_psamples = [mean_train[None, :]] + \
                     [psample.get_value() for psample in psamples[1:]]
    mean_pos = numpy.minimum(numpy_psamples[not marginalize_odd], 1-1e-5)
    mean_pos = numpy.maximum(mean_pos, 1e-5)
    pa_bias = -numpy.log(1./mean_pos[0] - 1.)

    # Build Theano function to sample from interpolating distributions.
    updates = OrderedDict()
    new_nsamples = neg_sampling(W_list, b_list, nsamples, beta=beta,
                                pa_bias=pa_bias,
                                marginalize_odd=marginalize_odd,
                                theano_rng=theano_rng)
    for (nsample, new_nsample) in zip(nsamples, new_nsamples):
        updates[nsample] = new_nsample
    sample_fn = theano.function([beta], [], updates=updates,
                                name='sample_func')

    # Build function to compute free-energy of p_k(h1).
    fe_bp_h1 = free_energy_at_beta(W_list, b_list, nsamples, beta,
                                   pa_bias, marginalize_odd=marginalize_odd)
    free_energy_fn = theano.function([beta], fe_bp_h1)

    ###########
    ## RUN AIS
    ###########

    # Generate exact sample for the base model.
    for i, nsample_i in enumerate(nsamples):
        bias = pa_bias if i == 1 else b_list[i].get_value()
        hi_mean_vec = 1. / (1. + numpy.exp(-bias))
        hi_mean = numpy.tile(hi_mean_vec, (batch_size, 1))
        r = rng.random_sample(hi_mean.shape)
        hi_sample = numpy.array(hi_mean > r, dtype=floatX)
        nsample_i.set_value(hi_sample)

    # Default configuration for interpolating distributions
    if large_ais:
        betas = numpy.cast[floatX](
            numpy.hstack((numpy.linspace(0, 0.5, 1e5+1)[:-1],
                         numpy.linspace(0.5, 0.9, 1e5+1)[:-1],
                         numpy.linspace(0.9, 1.0, 1e5))))
    else:
        betas = numpy.cast[floatX](
            numpy.hstack((numpy.linspace(0, 0.5, 1e4+1)[:-1],
                         numpy.linspace(0.5, 0.9, 1e4+1)[:-1],
                         numpy.linspace(0.9, 1.0, 1e4))))

    if log_z is None:
        log_ais_w = compute_log_ais_weights(batch_size, free_energy_fn,
                                            sample_fn, betas)
        dlogz, var_dlogz = estimate_from_weights(log_ais_w)
        log_za = compute_log_za(b_list, pa_bias, marginalize_odd)
        log_z = log_za + dlogz
        logging.info('log_z = %f' % log_z)
        logging.info('log_za = %f' % log_za)
        logging.info('dlogz = %f' % dlogz)
        logging.info('var_dlogz = %f' % var_dlogz)

    train_ll = compute_likelihood_given_logz(nsamples, psamples, batch_size,
                                             energy_fn, inference_fn, log_z,
                                             trainset.X)
    logging.info('Training likelihood = %f' % train_ll)
    test_ll = compute_likelihood_given_logz(nsamples, psamples, batch_size,
                                            energy_fn, inference_fn, log_z,
                                            testset.X)
    logging.info('Test likelihood = %f' % test_ll)

    return (train_ll, test_ll, log_z)


if __name__ == '__main__':
    # Possible metrics
    metrics = {'ais': estimate_likelihood}
    datasets = {'mnist': MNIST}

    # Argument parsing
    parser = argparse.ArgumentParser()
    parser.add_argument("metric", help="the desired metric",
                        choices=metrics.keys())
    parser.add_argument("dataset", help="the dataset used for computing the " +
                        "metric", choices=datasets.keys())
    parser.add_argument("model_path", help="path to the pickled DBM model")
    args = parser.parse_args()

    metric = metrics[args.metric]
    dataset = datasets[args.dataset]

    model = serial.load(args.model_path)
    layers = [model.visible_layer] + model.hidden_layers
    W_list = [theano.shared(hidden_layer.get_weights())
              for hidden_layer in model.hidden_layers]
    b_list = [theano.shared(layer.get_biases()) for layer in layers]

    trainset = dataset(which_set='train')
    testset = dataset(which_set='test')

    metric(W_list, b_list, trainset, testset, pos_mf_steps=5)

########NEW FILE########
__FILENAME__ = show_negative_chains
#!/usr/bin/env python

__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"


import sys
from pylearn2.utils import serial
from pylearn2.datasets import control
from pylearn2.config import yaml_parse
import numpy as np
from pylearn2.gui.patch_viewer import PatchViewer

ignore, model_path = sys.argv
model = serial.load(model_path)

control.push_load_data(False)

dataset = yaml_parse.load(model.dataset_yaml_src)

try:
    layer_to_chains = model.layer_to_chains
except AttributeError:
    print "This model doesn't have negative chains."
    quit(-1)

vis_chains = layer_to_chains[model.visible_layer]
vis_chains = vis_chains.get_value()
if vis_chains.ndim == 2:
    vis_chains = dataset.get_topological_view(vis_chains)
vis_chains = dataset.adjust_for_viewer(vis_chains)

m = vis_chains.shape[0]
r = int(np.sqrt(m))
c = m // r
while r * c < m:
    c += 1

pv = PatchViewer((r,c), vis_chains.shape[1:3],
        is_color = vis_chains.shape[-1] == 3)

for i in xrange(m):
    pv.add_patch(vis_chains[i,:], rescale = False)

pv.show()

########NEW FILE########
__FILENAME__ = show_reconstructions
#!/usr/bin/env python
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
"""

Usage: python show_reconstructions <path_to_a_saved_DBM.pkl>
Displays a batch of data from the DBM's training set.
Then shows how the DBM reconstructs it if you run mean field
to estimate the hidden units, then do one mean field downward
pass from hidden_layers[0] to the visible layer.
"""
from pylearn2.utils import serial
import sys
from pylearn2.config import yaml_parse
from pylearn2.gui.patch_viewer import PatchViewer
from theano import function

rows = 5
cols = 10
m = rows * cols

_, model_path = sys.argv

print 'Loading model...'
model = serial.load(model_path)
model.set_batch_size(m)


dataset_yaml_src = model.dataset_yaml_src

print 'Loading data...'
dataset = yaml_parse.load(dataset_yaml_src)

x = raw_input('use test set? (y/n) ')

if x == 'y':
    dataset = dataset.get_test_set()
else:
    assert x == 'n'

vis_batch = dataset.get_batch_topo(m)

_, patch_rows, patch_cols, channels = vis_batch.shape

assert _ == m

mapback = hasattr(dataset, 'mapback_for_viewer')

actual_cols = 2 * cols * (1 + mapback) * (1 + (channels == 2))
pv = PatchViewer((rows, actual_cols), (patch_rows, patch_cols), is_color=(channels == 3))


batch = model.visible_layer.space.make_theano_batch()
topo = batch.ndim > 2
reconstruction = model.reconstruct(batch)
recons_func = function([batch], reconstruction)

def show():
    ipt = vis_batch.copy()
    if not topo:
        ipt = dataset.get_design_matrix(ipt)
    recons_batch = recons_func(ipt.astype(batch.dtype))
    if not topo:
        recons_batch = dataset.get_topological_view(recons_batch)
    if mapback:
        design_vis_batch = vis_batch
        if design_vis_batch.ndim != 2:
            design_vis_batch = dataset.get_design_matrix(design_vis_batch.copy())
        mapped_batch_design = dataset.mapback(design_vis_batch.copy())
        mapped_batch = dataset.get_topological_view(
                mapped_batch_design.copy())
        design_r_batch = recons_batch.copy()
        if design_r_batch.ndim != 2:
            design_r_batch = dataset.get_design_matrix(design_r_batch.copy())
        mapped_r_design = dataset.mapback(design_r_batch.copy())
        mapped_r_batch = dataset.get_topological_view(mapped_r_design.copy())
    for row in xrange(rows):
        row_start = cols * row
        for j in xrange(cols):
            vis_patch = vis_batch[row_start+j,:,:,:].copy()
            adjusted_vis_patch = dataset.adjust_for_viewer(vis_patch)
            if vis_patch.shape[-1] == 2:
                pv.add_patch(adjusted_vis_patch[:,:,1], rescale=False)
                pv.add_patch(adjusted_vis_patch[:,:,0], rescale=False)
            else:
                pv.add_patch(adjusted_vis_patch, rescale = False)
            r = vis_patch
            #print 'vis: '
            #for ch in xrange(3):
            #    chv = r[:,:,ch]
            #    print '\t',ch,(chv.min(),chv.mean(),chv.max())
            if mapback:
                pv.add_patch(dataset.adjust_for_viewer(
                    mapped_batch[row_start+j,:,:,:].copy()), rescale = False)
            if recons_batch.shape[-1] == 2:
                pv.add_patch(dataset.adjust_to_be_viewed_with(
                recons_batch[row_start+j,:,:,1].copy(),
                vis_patch), rescale = False)
                pv.add_patch(dataset.adjust_to_be_viewed_with(
                recons_batch[row_start+j,:,:,0].copy(),
                vis_patch), rescale = False)
            else:
                pv.add_patch(dataset.adjust_to_be_viewed_with(
                recons_batch[row_start+j,:,:,:].copy(),
                vis_patch), rescale = False)
            r = recons_batch[row_start+j,:,:,:]
            #print 'recons: '
            #for ch in xrange(3):
            #    chv = r[:,:,ch]
            #    print '\t',ch,(chv.min(),chv.mean(),chv.max())
            if mapback:
                pv.add_patch(dataset.adjust_to_be_viewed_with(
                    mapped_r_batch[row_start+j,:,:,:].copy(),
                    mapped_batch[row_start+j,:,:,:].copy()),rescale = False)
    pv.show()


if hasattr(model.visible_layer, 'beta'):
    beta = model.visible_layer.beta.get_value()
    #model.visible_layer.beta.set_value(beta * 100.)
    print 'beta: ',(beta.min(), beta.mean(), beta.max())

while True:
    show()
    print 'Displaying reconstructions. (q to quit, ENTER = show more)'
    while True:
        x = raw_input()
        if x == 'q':
            quit()
        if x == '':
            x = 1
            break
        else:
            print 'Invalid input, try again'

    vis_batch = dataset.get_batch_topo(m)



########NEW FILE########
__FILENAME__ = show_samples
#!/usr/bin/env python
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
"""

Usage: python show_samples <path_to_a_saved_DBM.pkl>
Displays a batch of data from the DBM's training set.
Then interactively allows the user to run Gibbs steps
starting from that seed data to see how the DBM's MCMC
sampling changes the data.

"""

from pylearn2.utils import serial
import sys
from pylearn2.config import yaml_parse
from pylearn2.gui.patch_viewer import PatchViewer
import time
from theano import function
from theano.sandbox.rng_mrg import MRG_RandomStreams
import numpy as np
from pylearn2.expr.basic import is_binary

rows = 10
cols = 10
m = rows * cols

_, model_path = sys.argv

print 'Loading model...'
model = serial.load(model_path)
model.set_batch_size(m)


dataset_yaml_src = model.dataset_yaml_src

print 'Loading data (used for setting up visualization and seeding gibbs chain) ...'
dataset = yaml_parse.load(dataset_yaml_src)


vis_batch = dataset.get_batch_topo(m)

_, patch_rows, patch_cols, channels = vis_batch.shape

assert _ == m

mapback = hasattr(dataset, 'mapback_for_viewer')

pv = PatchViewer((rows,cols*(1+mapback)), (patch_rows,patch_cols), is_color = (channels==3))

def show():
    display_batch = dataset.adjust_for_viewer(vis_batch)
    if display_batch.ndim == 2:
        display_batch = dataset.get_topological_view(display_batch)
    if mapback:
        design_vis_batch = vis_batch
        if design_vis_batch.ndim != 2:
            design_vis_batch = dataset.get_design_matrix(design_vis_batch)
        mapped_batch_design = dataset.mapback_for_viewer(design_vis_batch)
        mapped_batch = dataset.get_topological_view(mapped_batch_design)
    for i in xrange(rows):
        row_start = cols * i
        for j in xrange(cols):
            pv.add_patch(display_batch[row_start+j,:,:,:], rescale = False)
            if mapback:
                pv.add_patch(mapped_batch[row_start+j,:,:,:], rescale = False)
    pv.show()


if hasattr(model.visible_layer, 'beta'):
    beta = model.visible_layer.beta.get_value()
#model.visible_layer.beta.set_value(beta * 100.)
    print 'beta: ',(beta.min(), beta.mean(), beta.max())

print 'showing seed data...'
show()

print 'How many Gibbs steps should I run with the seed data clamped? (negative = ignore seed data) '
x = int(input())


# Make shared variables representing the sampling state of the model
layer_to_state = model.make_layer_to_state(m)
# Seed the sampling with the data batch
vis_sample = layer_to_state[model.visible_layer]

def validate_all_samples():
    # Run some checks on the samples, this should help catch any bugs
    layers = [ model.visible_layer ] + model.hidden_layers

    def check_batch_size(l):
        if isinstance(l, (list, tuple)):
            map(check_batch_size, l)
        else:
            assert l.get_value().shape[0] == m


    for layer in layers:
        state = layer_to_state[layer]
        space = layer.get_total_state_space()
        space.validate(state)
        if 'DenseMaxPool' in str(type(layer)):
            p, h = state
            p = p.get_value()
            h = h.get_value()
            assert np.all(p == h)
            assert is_binary(p)
        if 'BinaryVisLayer' in str(type(layer)):
            v = state.get_value()
            assert is_binary(v)
        if 'Softmax' in str(type(layer)):
            y = state.get_value()
            assert is_binary(y)
            s = y.sum(axis=1)
            assert np.all(s == 1 )
        if 'Ising' in str(type(layer)):
            s = state.get_value()
            assert is_binary((s + 1.) / 2.)



validate_all_samples()

if x >= 0:
    if vis_sample.ndim == 4:
        vis_sample.set_value(vis_batch)
    else:
        vis_sample.set_value(dataset.get_design_matrix(vis_batch))

validate_all_samples()

theano_rng = MRG_RandomStreams(2012+9+18)

if x > 0:
    sampling_updates = model.get_sampling_updates(layer_to_state, theano_rng,
            layer_to_clamp = { model.visible_layer : True }, num_steps = x)

    t1 = time.time()
    sample_func = function([], updates=sampling_updates)
    t2 = time.time()
    print 'Clamped sampling function compilation took',t2-t1
    sample_func()


# Now compile the full sampling update
sampling_updates = model.get_sampling_updates(layer_to_state, theano_rng)
assert layer_to_state[model.visible_layer] in sampling_updates

t1 = time.time()
sample_func = function([], updates=sampling_updates)
t2 = time.time()

print 'Sampling function compilation took',t2-t1

while True:
    print 'Displaying samples. How many steps to take next? (q to quit, ENTER=1)'
    while True:
        x = raw_input()
        if x == 'q':
            quit()
        if x == '':
            x = 1
            break
        else:
            try:
                x = int(x)
                break
            except:
                print 'Invalid input, try again'

    for i in xrange(x):
        print i
        sample_func()

    validate_all_samples()

    vis_batch = vis_sample.get_value()
    show()

    if 'Softmax' in str(type(model.hidden_layers[-1])):
        state = layer_to_state[model.hidden_layers[-1]]
        value = state.get_value()
        y = np.argmax(value, axis=1)
        assert y.ndim == 1
        for i in xrange(0, y.shape[0], cols):
            print y[i:i+cols]




########NEW FILE########
__FILENAME__ = top_filters
#!/usr/bin/env python
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
"""

Usage: ./top_filters <path_to_a_saved_DBM.pkl> <optional: output path prefix>

Displays the matrix product of the layer 1 and layer 2 weights.
Also displays a grid visualization the connections in more detail.
Row i of the grid corresponds to the second layer hidden unit with the ith largest
filter norm. Grid cell (i,j) shows the filter for the first layer unit with the
jth largest weight going into the second layer unit for this row.
The cells is surrounded by a colored box. Its brightness indicates the relative
strength of the connection between the first layer unit and second layer unit,
and its color indicates the sign of that connection (yellow = positive / excitatory,
magenta = negative / inhibitory).

Optionally saves these images as png files prefixed with the given output path name
instead of displaying them. This can be useful when working over ssh.

"""

import sys
from pylearn2.utils import serial
import numpy as np
from pylearn2.gui.patch_viewer import PatchViewer
from pylearn2.gui.patch_viewer import make_viewer
from pylearn2.config import yaml_parse

if len(sys.argv) == 2:
    _, model_path = sys.argv
    out_prefix = None
else:
    _, model_path, out_prefix =sys.argv

model = serial.load(model_path)

layer_1, layer_2 = model.hidden_layers[0:2]

W1 = layer_1.get_weights()
W2 = layer_2.get_weights()
print W1.shape
print W2.shape

prod = np.dot(W1,W2)
pv = make_viewer(prod.T)
if out_prefix is None:
    pv.show()
else:
    pv.save(out_prefix+"_prod.png")


print 'Sorting so largest-norm layer 2 weights are plotted at the top'
norms = np.square(W2).sum(axis=0)
idxs = [elem[1] for elem in sorted( zip( -norms, range(norms.shape[0]) ) ) ]

new = W2.copy()

for i in xrange(len(idxs)):
    new[:,i] = W2[:,idxs[i]]
W2 = new


dataset_yaml_src = model.dataset_yaml_src
dataset = yaml_parse.load(dataset_yaml_src)

import numpy as np

imgs = dataset.get_weights_view(W1.T)

N1 = W1.shape[1]
N = W2.shape[1]

N = min(N,100)

thresh = .9
max_count = 0
total_counts = 0.
for i in xrange(N):
    w = W2[:,i]

    wa = np.abs(w)

    total = wa.sum()

    s = np.asarray(sorted(wa))

    count = 1

    while s[-count:].sum() < thresh * total:
        count += 1

    if count > max_count:
        max_count = count

    total_counts += count
ave = total_counts / float(N)

print 'average needed filters',ave

count = max_count

print 'It takes',count,'of',N1,'elements to account for ',(thresh*100.),'\% of the weight in at least one filter'

lim = 10
if count > lim:
    count = lim
    print 'Only displaying ',count,' elements though.'

if count > N1:
    count = N1

pv = PatchViewer( (N, count), imgs.shape[1:3], is_color = imgs.shape[3] == 3)

for i in xrange(N):
    w = W2[:, i]

    wneg = w[w < 0.]
    wpos = w[w > 0.]

    w /= np.abs(w).max()

    wa = np.abs(w)


    to_sort = zip(wa,range(N1), w )

    s = sorted(to_sort)

    for j in xrange(count):

        idx = s[N1-j-1][1]
        mag = s[N1-j-1][2]

        if mag > 0:
            act = (mag, 0)
        else:
            act = (0, -mag)

        pv.add_patch( imgs[idx,...], rescale = True, activation = act)

if out_prefix is None:
    pv.show()
else:
    pv.save(out_prefix+".png")

########NEW FILE########
__FILENAME__ = diff_monitor
#!/usr/bin/env python
"""
usage:

diff_monitor.py model_1.pkl model_2.pkl

Prints any difference in which set of channels were monitored,
then prints any difference in the length of the records, then
prints the first record entry at which each channel differs and
by how much.

Does not report timing differences, since these will essentially
never match.

"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"


import sys
from pylearn2.utils import serial
import numpy as np

# equal -> compare val record entries with np.all(x == y)
# allclose -> compare val record entries with np.allclose(x, y)
cmp_mode = 'equal'

if __name__ == "__main__":
    # Load the records
    _, model_0_path, model_1_path = sys.argv

    model_0, model_1 = [serial.load(path) for path in [model_0_path, model_1_path]]

    monitor_0, monitor_1 = [model.monitor for model in [model_0, model_1]]

    channels_0, channels_1 = [monitor.channels for monitor in [monitor_0, monitor_1]]

    # Print the difference in which channels were monitored
    intersect = []
    for channel in channels_0:
        if channel not in channels_1:
            print channel+' is in model 0 but not model 1'
        else:
            intersect.append(channel)
    for channel in channels_1:
        if channel not in channels_0:
            print channel+' is in model 1 but not model 0'

    # Print the difference in length between the records
    for channel in intersect:
        channel_0, channel_1 = [d[channel] for d in [channels_0, channels_1]]
        len_0, len_1 = [len(ch.batch_record) for ch in [channel_0, channel_1]]
        if len_0 != len_1:
            print 'Length of',channel,'differs:',len_0,'vs',len_1
        channel_0.length = min(len_0, len_1)


    # Print numerical differences between the channels
    record = 0
    clean = intersect
    while len(clean) > 0:
        bad_channel = []
        for channel in clean:
            channel_0 = channels_0[channel]
            channel_1 = channels_1[channel]

            # Quit scanning channels that we've read all of
            if record == channel_0.length:
                bad_channel.append(channel)
                continue

            ch0 = channel_0.batch_record[record]
            ch1 = channel_1.batch_record[record]
            eq = ch0 == ch1
            both_nan = np.isnan(ch0) and np.isnan(ch1)
            if not (eq or both_nan):
                print channel+'.batch_record differs at record entry',record
                print '\t',channel_0.batch_record[record], 'vs', channel_1.batch_record[record]
                bad_channel.append(channel)
                continue

            if not (channel_0.example_record[record] ==
                    channel_1.example_record[record]):
                print channel+'.example_record differs at record entry',record
                print '\t',channel_0.example_record[record], 'vs', channel_1.example_record[record]
                bad_channel.append(channel)
                continue

            if not (channel_0.epoch_record[record] ==
                    channel_1.epoch_record[record]):
                print channel+'.epoch_record differs at record entry',record
                print '\t',channel_0.epoch_record[record], 'vs', channel_1.epoch_record[record]
                bad_channel.append(channel)
                continue

            ch0 = channel_0.val_record[record]
            ch1 = channel_1.val_record[record]
            both_nan = np.isnan(ch0) and np.isnan(ch1)
            if both_nan:
                match = True
            else:
                if cmp_mode == 'equal':
                    match = ch0 == ch1
                elif cmp_mode == 'allclose':
                    match = np.allclose(channel_0.val_record[record],
                        channel_1.val_record[record])
                else:
                    assert False # unrecognized cmp_mode

            if not match:
                print channel+'.val_record differs at record entry',record
                print '\t',channel_0.val_record[record], 'vs', channel_1.val_record[record]
                bad_channel.append(channel)
                continue

        for channel in bad_channel:
            del clean[clean.index(channel)]
        record += 1


########NEW FILE########
__FILENAME__ = find_gpu_fields
#!/usr/bin/env python
"""
.. todo::

    WRITEME
"""
#argument: path to a pkl file
#loads the pkl file and figures out which fields are CudaNDArrays

import sys

if __name__ == "__main__":
    path = sys.argv[1]

    from pylearn2.utils import serial
    import inspect

    obj = serial.load(path)

    from theano.sandbox.cuda import CudaNdarray

    visited = set([])

    def find(cur_obj, cur_name):
        global visited

        if isinstance(cur_obj, CudaNdarray):
            print cur_name
        print cur_name
        for field, new_obj in inspect.getmembers(cur_obj):

            if new_obj in visited:
                continue

            visited = visited.union([new_obj])

            print visited

            find(new_obj,cur_name+'.'+field)

    find(obj,'')

########NEW FILE########
__FILENAME__ = get_version
#!/usr/bin/env python
"""
Script to obtain version of Python modules and basic information on the
experiment setup (e.g. cpu, os).
e.g. numpy:1.6.1 | pylearn:a6e634b83d | pylearn2:57a156beb0
     CPU: x86_64
     OS: Linux-2.6.35.14-106.fc14.x86_64-x86_64-with-fedora-14-Laughlin
"""
__authors__ = "Olivier Dellaleau and Raul Chandias Ferrari"
__copyright__ = "Copyright 2013, Universite de Montreal"
__credits__ = ["Olivier Dellaleau", "Raul Chandias Ferrari"]
__license__ = "3-clause BSD"
__maintainer__ = "Raul Chandias Ferrari"
__email__ = "chandiar@iro"


import argparse
import sys

from pylearn2.utils.track_version import LibVersion


def main():
    """
    Executable entry point.

    Returns
    -------
    rval : int
        0 on success, and a non-zero error code on failure.
    """
    args = parse_args()

    # Obtain versions of the various Python packages.
    libv = LibVersion()
    libv.print_versions()
    libv.print_exp_env_info(args.print_theano)

    return 0


def parse_args():
    """
    Parse command-line arguments.

    Returns
    -------
    WRITEME : WRITEME
        Parsed arguments
    """
    # The global program parser.
    parser = argparse.ArgumentParser(
            description='Obtain versions of relevant Python modules.')
    parser.add_argument('-p', '--print_theano_config', action='store_true',
                        dest='print_theano', help='''If set to true, theano config
                        will be displayed.''')
    args = parser.parse_args()
    return args


if __name__ == '__main__':
    sys.exit(main())

########NEW FILE########
__FILENAME__ = gpu_pkl_to_cpu_pkl
#!/usr/bin/env python
"""
Converts a pickle file containing CudaNdarraySharedVariables into
a pickle file containing only TensorSharedVariables.

Usage:

gpu_pkl_to_cpu_pkl.py <gpu.pkl> <cpu.pkl>

Loads gpu.pkl, replaces cuda shared variables with numpy ones,
and saves to cpu.pkl.

If you create a model while using GPU and later want to unpickle it
on a machine without a GPU, you must convert it this way.

This is theano's fault, not pylearn2's. I would like to fix theano,
but don't understand the innards of theano well enough, and none of
the theano developers has been willing to help me at all with this
issue. If it annoys you that you have to do this, please help me
persuade the theano developers that this issue is worth more of their
attention.

Note: This script is also useful if you want to create a model on GPU,
save it, and then run other theano functionality on CPU later, even
if your machine has a GPU. It could be useful to modify this script
to do the reverse conversion, so you can create a model on CPU, save
it, and then run theano functions on GPU later.

Further note: this script is very hacky and imprecise. It is likely
to do things like blow away subclasses of list and dict and turn them
into plain lists and dicts. It is also liable to overlook all sorts of
theano shared variables if you have an exotic data structure stored in
the pickle. You probably want to test that the cpu pickle file can be
loaded on a machine without GPU to be sure that the script actually
found them all.
"""
__author__ = "Ian Goodfellow"

import sys
import types

if __name__ == '__main__':
    _, in_path, out_path = sys.argv
    from pylearn2.utils import serial
    from theano import shared
    model = serial.load(in_path)

# map ids of objects we've fixed before to the fixed version, so we don't clone objects when fixing
# can't use object itself as key because not all objects are hashable
    already_fixed = {}

# ids of objects being fixed right now (we don't support cycles)
    currently_fixing = []

    blacklist = ["im_class", "func_closure", "co_argcount", "co_cellvars", "func_code",
            "append", "capitalize", "im_self", "func_defaults", "func_name"]
    blacklisted_keys = ["bytearray", "IndexError", "isinstance", "copyright", "main"]

    postponed_fixes = []

    class Placeholder(object):
        def __init__(self, id_to_sub):
            self.id_to_sub = id_to_sub

    class FieldFixer(object):

        def __init__(self, obj, field, fixed_field):
            self.obj = obj
            self.field = field
            self.fixed_field = fixed_field

        def apply(self):
            obj = self.obj
            field = self.field
            fixed_field = already_fixed[self.fixed_field.id_to_sub]
            setattr(obj, field, fixed_field)

    def fix(obj, stacklevel=0):
        prefix = ''.join(['.']*stacklevel)
        oid = id(obj)
        canary_oid = oid
        print prefix + 'fixing '+str(oid)
        if oid in already_fixed:
            return already_fixed[oid]
        if oid in currently_fixing:
            print 'returning placeholder for '+str(oid)
            return Placeholder(oid)
        currently_fixing.append(oid)
        if hasattr(obj, 'set_value'):
            # Base case: we found a shared variable, must convert it
            rval = shared(obj.get_value())
            # Sabotage its getstate so if something tries to pickle it, we'll find out
            obj.__getstate__ = None
        elif obj is None:
            rval = None
        elif isinstance(obj, list):
            print prefix + 'fixing a list'
            rval = []
            for i, elem in enumerate(obj):
                print prefix + '.fixing elem %d' % i
                fixed_elem = fix(elem, stacklevel + 2)
                if isinstance(fixed_elem, Placeholder):
                    raise NotImplementedError()
                rval.append(fixed_elem)
        elif isinstance(obj, dict):
            print prefix + 'fixing a dict'
            rval = obj
            """
            rval = {}
            for key in obj:
                if key in blacklisted_keys or (isinstance(key, str) and key.endswith('Error')):
                    print prefix + '.%s is blacklisted' % str(key)
                    rval[key] = obj[key]
                    continue
                print prefix + '.fixing key ' + str(key) + ' of type '+str(type(key))
                fixed_key = fix(key, stacklevel + 2)
                if isinstance(fixed_key, Placeholder):
                    raise NotImplementedError()
                print prefix + '.fixing value for key '+str(key)
                fixed_value = fix(obj[key], stacklevel + 2)
                if isinstance(fixed_value, Placeholder):
                    raise NotImplementedError()
                rval[fixed_key] = fixed_value
            """
        elif isinstance(obj, tuple):
            print prefix + 'fixing a tuple'
            rval = []
            for i, elem in enumerate(obj):
                print prefix + '.fixing elem %d' % i
                fixed_elem = fix(elem, stacklevel + 2)
                if isinstance(fixed_elem, Placeholder):
                    raise NotImplementedError()
                rval.append(fixed_elem)
            rval = tuple(rval)
        elif isinstance(obj, (int, float, str)):
            rval = obj
        else:
            print prefix + 'fixing a generic object'
            field_names = dir(obj)
            for field in field_names:
                if isinstance(getattr(obj, field), types.MethodType):
                    print prefix + '.%s is an instancemethod' % field
                    continue
                if field in blacklist or (field.startswith('__')):
                    print prefix + '.%s is blacklisted' % field
                    continue
                print prefix + '.fixing field %s' % field
                updated_field = fix(getattr(obj, field), stacklevel + 2)
                print prefix + '.applying fix to field %s' % field
                if isinstance(updated_field, Placeholder):
                    postponed_fixes.append(FieldFixer(obj, field, updated_field))
                else:
                    try:
                        setattr(obj, field, updated_field)
                    except Exception, e:
                        print "Couldn't do that because of exception: "+str(e)
            rval = obj
        already_fixed[oid] = rval
        print prefix+'stored fix for '+str(oid)
        assert canary_oid == oid
        del currently_fixing[currently_fixing.index(oid)]
        return rval

    model = fix(model)

    assert len(currently_fixing) == 0

    for fixer in postponed_fixes:
        fixer.apply()

    serial.save(out_path, model)


########NEW FILE########
__FILENAME__ = gsn_example
"""
.. todo::

    WRITEME
"""
import cPickle as pickle
import itertools

import numpy as np
import theano.tensor as T

from pylearn2.expr.activations import rescaled_softmax
from pylearn2.costs.autoencoder import MeanBinaryCrossEntropy
from pylearn2.costs.gsn import GSNCost
from pylearn2.corruption import (BinomialSampler, GaussianCorruptor,
                                 MultinomialSampler, SaltPepperCorruptor,
                                 SmoothOneHotCorruptor)
from pylearn2.datasets.mnist import MNIST
from pylearn2.distributions.parzen import ParzenWindows
from pylearn2.models.gsn import GSN, JointGSN
from pylearn2.termination_criteria import EpochCounter
from pylearn2.train import Train
from pylearn2.training_algorithms.sgd import SGD, MonitorBasedLRAdjuster
from pylearn2.utils import image, safe_zip

# define some common parameters
HIDDEN_SIZE = 1000
SALT_PEPPER_NOISE = 0.4
GAUSSIAN_NOISE = 0.5

WALKBACK = 0

LEARNING_RATE = 0.25
MOMENTUM = 0.75

MAX_EPOCHS = 100
BATCHES_PER_EPOCH = None # covers full training set
BATCH_SIZE = 100

ds = MNIST(which_set='train', one_hot=True)

def test_train_ae():
    """
    .. todo::

        WRITEME
    """
    GC = GaussianCorruptor

    gsn = GSN.new(
        layer_sizes=[ds.X.shape[1], 1000],
        activation_funcs=["sigmoid", "tanh"],
        pre_corruptors=[None, GC(1.0)],
        post_corruptors=[SaltPepperCorruptor(0.5), GC(1.0)],
        layer_samplers=[BinomialSampler(), None],
        tied=False
    )

    # average MBCE over example rather than sum it
    _mbce = MeanBinaryCrossEntropy()
    reconstruction_cost = lambda a, b: _mbce.cost(a, b) / ds.X.shape[1]

    c = GSNCost([(0, 1.0, reconstruction_cost)], walkback=WALKBACK)

    alg = SGD(
        LEARNING_RATE,
        init_momentum=MOMENTUM,
        cost=c,
        termination_criterion=EpochCounter(MAX_EPOCHS),
        batches_per_iter=BATCHES_PER_EPOCH,
        batch_size=BATCH_SIZE,
        monitoring_dataset=ds,
        monitoring_batches=10
   )

    trainer = Train(ds, gsn, algorithm=alg, save_path="gsn_ae_example.pkl",
                    save_freq=5)
    trainer.main_loop()
    print "done training"

def test_sample_ae():
    """
    Visualize some samples from the trained unsupervised GSN.
    """
    with open("gsn_ae_example.pkl") as f:
        gsn = pickle.load(f)

    # random point to start at
    mb_data = MNIST(which_set='test').X[105:106, :]

    history = gsn.get_samples([(0, mb_data)], walkback=1000,
                              symbolic=False, include_first=True)

    history = list(itertools.chain(*history))
    history = np.vstack(history)

    tiled = image.tile_raster_images(history,
                                     img_shape=[28,28],
                                     tile_shape=[50,50],
                                     tile_spacing=(2,2))
    image.save("gsn_ae_example.png", tiled)

    # code to get log likelihood from kernel density estimator
    # this crashed on GPU (out of memory), but works on CPU
    pw = ParzenWindows(MNIST(which_set='test').X, .20)
    print pw.get_ll(history)

def test_train_supervised():
    """
    Train a supervised GSN.
    """
    # initialize the GSN
    gsn = GSN.new(
        layer_sizes=[ds.X.shape[1], 1000, ds.y.shape[1]],
        activation_funcs=["sigmoid", "tanh", rescaled_softmax],
        pre_corruptors=[GaussianCorruptor(0.5)] * 3,
        post_corruptors=[SaltPepperCorruptor(.3), None, SmoothOneHotCorruptor(.5)],
        layer_samplers=[BinomialSampler(), None, MultinomialSampler()],
        tied=False
    )

    # average over costs rather than summing
    _rcost = MeanBinaryCrossEntropy()
    reconstruction_cost = lambda a, b: _rcost.cost(a, b) / ds.X.shape[1]

    _ccost = MeanBinaryCrossEntropy()
    classification_cost = lambda a, b: _ccost.cost(a, b) / ds.y.shape[1]

    # combine costs into GSNCost object
    c = GSNCost(
        [
            # reconstruction on layer 0 with weight 1.0
            (0, 1.0, reconstruction_cost),

            # classification on layer 2 with weight 2.0
            (2, 2.0, classification_cost)
        ],
        walkback=WALKBACK,
        mode="supervised"
    )

    alg = SGD(
        LEARNING_RATE,
        init_momentum=MOMENTUM,
        cost=c,
        termination_criterion=EpochCounter(MAX_EPOCHS),
        batches_per_iter=BATCHES_PER_EPOCH,
        batch_size=BATCH_SIZE,
        monitoring_dataset=ds,
        monitoring_batches=10,
    )

    trainer = Train(ds, gsn, algorithm=alg,
                    save_path="gsn_sup_example.pkl", save_freq=10,
                    extensions=[MonitorBasedLRAdjuster()])
    trainer.main_loop()
    print "done training"

def test_classify():
    """
    See how well a (supervised) GSN performs at classification.
    """
    with open("gsn_sup_example.pkl") as f:
        gsn = pickle.load(f)

    gsn = JointGSN.convert(gsn)

    # turn off corruption
    gsn._corrupt_switch = False

    ds = MNIST(which_set='test', one_hot=True)
    mb_data = ds.X
    y = ds.y

    for i in xrange(1, 10):
        y_hat = gsn.classify(mb_data, trials=i)
        errors = np.abs(y_hat - y).sum() / 2.0

        # error indices
        #np.sum(np.abs(y_hat - y), axis=1) != 0

        print i, errors, errors / mb_data.shape[0]

def test_sample_supervised(idxs=None, noisy=True):
    """
    Visualize samples and labels produced by GSN.
    """
    with open("gsn_sup_example.pkl") as f:
        gsn = pickle.load(f)

    gsn._corrupt_switch = noisy

    ds = MNIST(which_set='test', one_hot=True)

    if idxs is None:
        data = ds.X[100:150]
    else:
        data = ds.X[idxs]

    # change the walkback parameter to make the data fill up rows in image
    samples = gsn.get_samples([(0, data)],
                              indices=[0, 2],
                              walkback=21, symbolic=False,
                              include_first=True)
    stacked = vis_samples(samples)
    tiled = image.tile_raster_images(stacked,
                                     img_shape=[28,28],
                                     tile_shape=[50,50],
                                     tile_spacing=(2,2))
    image.save("gsn_sup_example.png", tiled)

def vis_samples(samples):
    """
    .. todo::

        WRITEME
    """
    from PIL import ImageDraw, ImageFont
    img = image.pil_from_ndarray(np.zeros((28, 28)))

    chains = []
    num_rows = samples[0][0].shape[0]

    for row in xrange(num_rows):
        images = []
        labels = []

        for step in samples:
            assert len(step) == 2

            images.append(step[0][row])

            vec = step[1][row]
            sorted_idxs = np.argsort(vec)
            label1 = sorted_idxs[-1]
            label2 = sorted_idxs[-2]

            if vec[label1] == 0:
                ratio = 1
            else:
                ratio = vec[label2] / vec[label1]

            c = img.copy()
            draw = ImageDraw.Draw(c)
            draw.text((8, 11), str(label1), 255)
            draw.text((14, 11), str(label2), int(255 * ratio))
            nd = image.ndarray_from_pil(c)[:, :, 0]
            nd = nd.reshape((1, 784))
            labels.append(nd)

        data = safe_zip(images, labels)
        data = list(itertools.chain(*data))

        # white block to indicate end of chain
        data.extend([np.ones((1, 784))] * 2)

        chains.append(np.vstack(data))

    return np.vstack(chains)


# some utility methods for viewing MNIST characters without any GUI
def print_char(A):
    """
    .. todo::

        WRITEME
    """
    print a_to_s(A.round().reshape((28, 28)))

def a_to_s(A):
    """Prints binary array"""
    strs = []
    for row in A:
        x = [None] * len(row)
        for i, num in enumerate(row):
            if num != 0:
                x[i] = "@"
            else:
                x[i] = " "
        strs.append("".join(x))
    return "\n".join(strs)


if __name__ == '__main__':
    test_train_supervised()
    test_classify()
    test_sample_supervised()

########NEW FILE########
__FILENAME__ = black_box_dataset
"""
A Pylearn2 Dataset class for accessing the data for the
facial expression recognition Kaggle contest for the ICML
2013 workshop on representation learning.
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2013, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"

import csv
import numpy as np
import os

from pylearn2.datasets.dense_design_matrix import DenseDesignMatrix
from pylearn2.utils import serial
from pylearn2.utils.string_utils import preprocess

class BlackBoxDataset(DenseDesignMatrix):
    """
    A Pylearn2 Dataset class for accessing the data for the
    facial expression recognition Kaggle contest for the ICML
    2013 workshop on representation learning.
    """

    def __init__(self, which_set,
            base_path = '${PYLEARN2_DATA_PATH}/icml_2013_black_box',
            start = None,
            stop = None,
            preprocessor = None,
            fit_preprocessor = False,
            fit_test_preprocessor = False):
        """
        which_set: A string specifying which portion of the dataset
            to load. Valid values are 'train' or 'public_test'
        base_path: The directory containing the .csv files from kaggle.com.
                This directory should be writable; if the .csv files haven't
                already been converted to npy, this class will convert them
                to save memory the next time they are loaded.
        fit_preprocessor: True if the preprocessor is allowed to fit the
                   data.
        fit_test_preprocessor: If we construct a test set based on this
                    dataset, should it be allowed to fit the test set?
        """

        self.test_args = locals()
        self.test_args['which_set'] = 'public_test'
        self.test_args['fit_preprocessor'] = fit_test_preprocessor
        del self.test_args['start']
        del self.test_args['stop']
        del self.test_args['self']

        files = {'train': 'train.csv', 'public_test' : 'test.csv'}
        sizes = {'train': 1000, 'public_test' : 10000, 'extra': 135735 }

        if which_set == 'extra':
            path = base_path + '/' + 'extra_unsupervised_data.npy'
            X = serial.load(path).T
            y = None
        else:
            try:
                filename = files[which_set]
            except KeyError:
                raise ValueError("Unrecognized dataset name: " + which_set)

            path = base_path + '/' + filename

            path = preprocess(path)

            expect_labels = which_set == 'train'

            X, y = self._load_data(path, expect_labels)
        size = sizes[which_set]
        if X.shape[0] != size:
            raise ValueError("Expected "+str(size)+" examples, got "+str(X.shape[0]))

        if start is not None:
            assert which_set != 'test'
            assert isinstance(start, int)
            assert isinstance(stop, int)
            assert start >= 0
            assert start < stop
            if not (stop <= X.shape[0]):
                raise ValueError("stop must be less than the # of examples but " +
                        "stop is " + str(stop) + " and there are " + str(X.shape[0]) +
                        " examples.")
            X = X[start:stop, :]
            if y is not None:
                y = y[start:stop, :]


        super(BlackBoxDataset, self).__init__(X=X, y=y)

        if preprocessor:
            preprocessor.apply(self, can_fit=fit_preprocessor)

    def adjust_for_viewer(self, X):
        return (X - 127.5) / 127.5

    def get_test_set(self):
        return BlackBoxDataset(**self.test_args)

    def _load_data(self, path, expect_labels):

        assert path.endswith('.csv')

        # If a previous call to this method has already converted
        # the data to numpy format, load the numpy directly
        X_path = path[:-4] + '.X.npy'
        Y_path = path[:-4] + '.Y.npy'
        if os.path.exists(X_path):
            X = np.load(X_path)
            if expect_labels:
                y = np.load(Y_path)
            else:
                y = None
            return X, y

        # Convert the .csv file to numpy
        csv_file = open(path, 'r')

        reader = csv.reader(csv_file)

        # Discard header
        row = reader.next()

        y_list = []
        X_list = []

        for row in reader:
            if expect_labels:
                y_str = row[0]
                row = row[1:]
                y = int(float(y_str))
                y_list.append(y)
            X_row = map(lambda x: float(x), row)
            X_list.append(X_row)

        X = np.asarray(X_list).astype('float32')
        if expect_labels:
            y = np.asarray(y_list)

            one_hot = np.zeros((y.shape[0],9),dtype='float32')
            for i in xrange(y.shape[0]):
                one_hot[i,y[i] - 1] = 1.
            y = one_hot
        else:
            y = None

        np.save(X_path, X)
        if y is not None:
            np.save(Y_path, y)

        return X, y

########NEW FILE########
__FILENAME__ = learn_zca
from pylearn2.datasets.preprocessing import ZCA
from pylearn2.utils import serial

from black_box_dataset import BlackBoxDataset

extra = BlackBoxDataset('extra')

zca = ZCA(filter_bias=.1)

zca.fit(extra.X)

serial.save('zca.pkl', zca)

########NEW FILE########
__FILENAME__ = make_submission
import sys

def usage():
    print """usage: python make_submission.py model.pkl submission.csv
Where model.pkl contains a trained pylearn2.models.mlp.MLP object.
The script will make submission.csv, which you may then upload to the
kaggle site."""


if len(sys.argv) != 3:
    usage()
    print "(You used the wrong # of arguments)"
    quit(-1)

_, model_path, out_path = sys.argv

import os
if os.path.exists(out_path):
    usage()
    print out_path+" already exists, and I don't want to overwrite anything just to be safe."
    quit(-1)

from pylearn2.utils import serial
try:
    model = serial.load(model_path)
except Exception, e:
    usage()
    print model_path + "doesn't seem to be a valid model path, I got this error when trying to load it: "
    print e

from pylearn2.config import yaml_parse

dataset = yaml_parse.load(model.dataset_yaml_src)
dataset = dataset.get_test_set()

# use smallish batches to avoid running out of memory
batch_size = 100
model.set_batch_size(batch_size)
# dataset must be multiple of batch size of some batches will have
# different sizes. theano convolution requires a hard-coded batch size
m = dataset.X.shape[0]
extra = batch_size - m % batch_size
assert (m + extra) % batch_size == 0
import numpy as np
if extra > 0:
    dataset.X = np.concatenate((dataset.X, np.zeros((extra, dataset.X.shape[1]),
    dtype=dataset.X.dtype)), axis=0)
assert dataset.X.shape[0] % batch_size == 0


X = model.get_input_space().make_batch_theano()
Y = model.fprop(X)

from theano import tensor as T

y = T.argmax(Y, axis=1)

from theano import function

f = function([X], y)


y = []

for i in xrange(dataset.X.shape[0] / batch_size):
    x_arg = dataset.X[i*batch_size:(i+1)*batch_size,:]
    if X.ndim > 2:
        x_arg = dataset.get_topological_view(x_arg)
    y.append(f(x_arg.astype(X.dtype)))

y = np.concatenate(y)
assert y.ndim == 1
assert y.shape[0] == dataset.X.shape[0]
# discard any zero-padding that was used to give the batches uniform size
y = y[:m]

out = open(out_path, 'w')
for i in xrange(y.shape[0]):
    out.write('%d.0\n' % (y[i] + 1))
out.close()



########NEW FILE########
__FILENAME__ = emotions_dataset
"""
A Pylearn2 Dataset class for accessing the data for the
facial expression recognition Kaggle contest for the ICML
2013 workshop on representation learning.
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2013, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"

import csv
import numpy as np
import os

from pylearn2.datasets.dense_design_matrix import DefaultViewConverter
from pylearn2.datasets.dense_design_matrix import DenseDesignMatrix
from pylearn2.utils.string_utils import preprocess

class EmotionsDataset(DenseDesignMatrix):
    """
    A Pylearn2 Dataset class for accessing the data for the
    facial expression recognition Kaggle contest for the ICML
    2013 workshop on representation learning.
    """

    def __init__(self, which_set,
            base_path = '${PYLEARN2_DATA_PATH}/icml_2013_emotions',
            start = None,
            stop = None,
            preprocessor = None,
            fit_preprocessor = False,
            axes = ('b', 0, 1, 'c'),
            fit_test_preprocessor = False):
        """
        which_set: A string specifying which portion of the dataset
            to load. Valid values are 'train' or 'public_test'
        base_path: The directory containing the .csv files from kaggle.com.
                This directory should be writable; if the .csv files haven't
                already been converted to npy, this class will convert them
                to save memory the next time they are loaded.
        fit_preprocessor: True if the preprocessor is allowed to fit the
                   data.
        fit_test_preprocessor: If we construct a test set based on this
                    dataset, should it be allowed to fit the test set?
        """

        self.test_args = locals()
        self.test_args['which_set'] = 'public_test'
        self.test_args['fit_preprocessor'] = fit_test_preprocessor
        del self.test_args['start']
        del self.test_args['stop']
        del self.test_args['self']

        files = {'train': 'train.csv', 'public_test' : 'test.csv'}

        try:
            filename = files[which_set]
        except KeyError:
            raise ValueError("Unrecognized dataset name: " + which_set)

        path = base_path + '/' + filename

        path = preprocess(path)

        X, y = self._load_data(path, which_set == 'train')


        if start is not None:
            assert which_set != 'test'
            assert isinstance(start, int)
            assert isinstance(stop, int)
            assert start >= 0
            assert start < stop
            assert stop <= X.shape[0]
            X = X[start:stop, :]
            if y is not None:
                y = y[start:stop, :]

        view_converter = DefaultViewConverter(shape=[48,48,1], axes=axes)

        super(EmotionsDataset, self).__init__(X=X, y=y, view_converter=view_converter)

        if preprocessor:
            preprocessor.apply(self, can_fit=fit_preprocessor)

    def adjust_for_viewer(self, X):
        return (X - 127.5) / 127.5

    def get_test_set(self):
        return EmotionsDataset(**self.test_args)

    def _load_data(self, path, expect_labels):

        assert path.endswith('.csv')

        # If a previous call to this method has already converted
        # the data to numpy format, load the numpy directly
        X_path = path[:-4] + '.X.npy'
        Y_path = path[:-4] + '.Y.npy'
        if os.path.exists(X_path):
            X = np.load(X_path)
            if expect_labels:
                y = np.load(Y_path)
            else:
                y = None
            return X, y

        # Convert the .csv file to numpy
        csv_file = open(path, 'r')

        reader = csv.reader(csv_file)

        # Discard header
        row = reader.next()

        y_list = []
        X_list = []

        for row in reader:
            if expect_labels:
                y_str, X_row_str = row
                y = int(y_str)
                y_list.append(y)
            else:
                X_row_str ,= row
            X_row_strs = X_row_str.split(' ')
            X_row = map(lambda x: float(x), X_row_strs)
            X_list.append(X_row)

        X = np.asarray(X_list).astype('float32')
        if expect_labels:
            y = np.asarray(y_list)

            one_hot = np.zeros((y.shape[0],7),dtype='float32')
            for i in xrange(y.shape[0]):
                one_hot[i,y[i]] = 1.
            y = one_hot
        else:
            y = None

        np.save(X_path, X)
        if y is not None:
            np.save(Y_path, y)

        return X, y

########NEW FILE########
__FILENAME__ = make_submission
import sys

def usage():
    print """usage: python make_submission.py model.pkl submission.csv
Where model.pkl contains a trained pylearn2.models.mlp.MLP object.
The script will make submission.csv, which you may then upload to the
kaggle site."""


if len(sys.argv) != 3:
    usage()
    print "(You used the wrong # of arguments)"
    quit(-1)

_, model_path, out_path = sys.argv

import os
if os.path.exists(out_path):
    usage()
    print out_path+" already exists, and I don't want to overwrite anything just to be safe."
    quit(-1)

from pylearn2.utils import serial
try:
    model = serial.load(model_path)
except Exception, e:
    usage()
    print model_path + "doesn't seem to be a valid model path, I got this error when trying to load it: "
    print e

from pylearn2.config import yaml_parse

dataset = yaml_parse.load(model.dataset_yaml_src)
dataset = dataset.get_test_set()

# use smallish batches to avoid running out of memory
batch_size = 100
model.set_batch_size(batch_size)
# dataset must be multiple of batch size of some batches will have
# different sizes. theano convolution requires a hard-coded batch size
m = dataset.X.shape[0]
extra = batch_size - m % batch_size
assert (m + extra) % batch_size == 0
import numpy as np
if extra > 0:
    dataset.X = np.concatenate((dataset.X, np.zeros((extra, dataset.X.shape[1]),
    dtype=dataset.X.dtype)), axis=0)
assert dataset.X.shape[0] % batch_size == 0


X = model.get_input_space().make_batch_theano()
Y = model.fprop(X)

from theano import tensor as T

y = T.argmax(Y, axis=1)

from theano import function

f = function([X], y)


y = []

for i in xrange(dataset.X.shape[0] / batch_size):
    x_arg = dataset.X[i*batch_size:(i+1)*batch_size,:]
    if X.ndim > 2:
        x_arg = dataset.get_topological_view(x_arg)
    y.append(f(x_arg.astype(X.dtype)))

y = np.concatenate(y)
assert y.ndim == 1
assert y.shape[0] == dataset.X.shape[0]
# discard any zero-padding that was used to give the batches uniform size
y = y[:m]

out = open(out_path, 'w')
for i in xrange(y.shape[0]):
    out.write('%d\n' % y[i])
out.close()



########NEW FILE########
__FILENAME__ = extract_kmeans_features
import numpy as np
import os
import sys

from pylearn2.utils import serial
from pylearn2.utils.string_utils import preprocess

def usage():
    print """
Run
python extract_kmeans_features.py public_test
to extract features for the ICML 2013 multimodal learning contest's public test images.
or
python extract_kmeans_features.py private_test
to extract features for the ICML 2013 multimodal learning contest's private test images
(which will be released 72 hours before the contest ends)
"""

if len(sys.argv) != 2:
    usage()
    print '(You used the wrong number of arguments)'
    quit(-1)

_, arg = sys.argv

if arg == 'public_test':
    base = preprocess('${PYLEARN2_DATA_PATH}/icml_2013_multimodal/public_test_lcn')
    expected_num_images = 500
elif arg == 'private_test':
    base = preprocess('${PYLEARN2_DATA_PATH}/icml_2013_multimodal/private_test_lcn')
    expected_num_images = 500
else:
    usage()
    print 'Unrecognized argument value:',arg
    print 'Recognized values are: public_test, private_test'

outdir = base[:-3] + 'layer_1_features'
serial.mkdir(outdir)

paths = os.listdir(base)
if len(paths) != expected_num_images:
    raise AssertionError("Something is wrong with your " + base \
            + "directory. It should contain " + str(expected_num_images) + \
            " lcn-preprocessed image files, but contains " + str(len(paths)))

means = np.load('lcn_means.npy')

norms = 1e-7 + np.sqrt(np.square(means).sum(axis=3).sum(axis=2).sum(axis=1))
means = np.transpose(np.transpose(means, (1, 2, 3, 0)) / norms, (3, 0, 1, 2))

from pylearn2.utils import sharedX
kernels = sharedX(np.transpose(means, (0, 3, 1, 2)))

import theano.tensor as T

X = T.TensorType(broadcastable=(False, False, False), dtype=kernels.dtype)()

bc01 = X.dimshuffle('x', 2, 0, 1)

Z = T.nnet.conv2d(input=bc01, filters=kernels, subsample = (means.shape[1] / 2, means.shape[2] /2),
        filter_shape = kernels.get_value().shape)

F = T.signal.downsample.max_pool_2d(Z, (2, 2))

F = T.clip(F - .5, 0., 10.)

from theano import function

f = function([X], F)

for i, path in enumerate(paths):
    print i
    try:
        X = np.load(base + '/' + path)

        assert X.ndim == 3

        F = f(X)

        np.save(outdir + '/' + path, F)
    except Exception, e:
        raise


########NEW FILE########
__FILENAME__ = extract_layer_2_kmeans_features
import numpy as np
import os
import sys

from pylearn2.utils import serial
from pylearn2.utils import sharedX
from pylearn2.utils.string_utils import preprocess

def usage():
    print """
Run
python extract_layer_2_kmeans_features.py public_test
to extract features for the ICML 2013 multimodal learning contest's public test images.
or
python extract_layer_2_kmeans_features.py private_test
to extract features for the ICML 2013 multimodal learning contest's private test images
(which will be released 72 hours before the contest ends)
"""

if len(sys.argv) != 2:
    usage()
    print '(You used the wrong number of arguments)'
    quit(-1)

_, arg = sys.argv

if arg == 'public_test':
    base = preprocess('${PYLEARN2_DATA_PATH}/icml_2013_multimodal/public_test_layer_1_features')
    expected_num_images = 500
elif arg == 'private_test':
    base = preprocess('${PYLEARN2_DATA_PATH}/icml_2013_multimodal/private_test_layer_1_features')
    expected_num_images = 500
else:
    usage()
    print 'Unrecognized argument value:',arg
    print 'Recognized values are: public_test, private_test'

outdir = base[:-len('layer_1_features')] + 'layer_2_features'
serial.mkdir(outdir)

paths = os.listdir(base)
if len(paths) != expected_num_images:
    raise AssertionError("Something is wrong with your " + base \
            + "directory. It should contain " + str(expected_num_images) + \
            " numpy files containing layer 1 features, but contains " + \
            str(len(paths)))

means = np.load('l1_means.npy')

norms = 1e-7 + np.sqrt(np.square(means).sum(axis=3).sum(axis=2).sum(axis=1))
means = np.transpose(np.transpose(means, (1, 2, 3, 0)) / norms, (3, 0, 1, 2))

kernels = sharedX(np.transpose(means, (0, 3, 1, 2)))


import theano.tensor as T

X = T.TensorType(broadcastable=(False, False, False, False), dtype=kernels.dtype)()

bc01 = X

Z = T.nnet.conv2d(input=bc01, filters=kernels, subsample = (means.shape[1] / 2, means.shape[2] /2),
        filter_shape = kernels.get_value().shape)

F = T.concatenate((-Z.min(axis=(2,3)), Z.max(axis=(2,3))), axis=1)

F = T.clip(F - .5, 0., 10.)

from theano import function

f = function([X], F)

for i, path in enumerate(paths):
    if i % 100 == 0:
        print i
    try:

        X = np.load(base + '/' + path)

        assert X.shape[0] == 1

        if X.shape[2] < 3:
            pad = np.zeros((1, X.shape[1], 3, X.shape[3]))
            pad[:,:,0:X.shape[2],:] = X.copy()
            X = pad

        if X.shape[3] < 3:
            pad = np.zeros((1, X.shape[1], X.shape[2], 3))
            pad[:,:,:,0:X.shape[3]] = X.copy()
            X = pad


        F = f(X)

        np.save(outdir + '/' + path, F)
    except Exception, e:
        raise


########NEW FILE########
__FILENAME__ = lcn

import numpy as np
import os
import sys

from pylearn2.utils import image
from pylearn2.utils import serial
from pylearn2.utils.string_utils import preprocess

def usage():
    print """
Run
python lcn.py public_test
to preprocess the ICML 2013 multimodal learning contest's public test images.
or
python lcn.py private_test
to preprocess the ICML 2013 multimodal learning contest's private test images
(which will be released 72 hours before the contest ends)
"""

if len(sys.argv) != 2:
    usage()
    print '(You used the wrong number of arguments)'
    quit(-1)

_, arg = sys.argv

if arg == 'public_test':
    base = preprocess('${PYLEARN2_DATA_PATH}/icml_2013_multimodal/public_test_images')
    outdir = base[:-6] + 'lcn'
    expected_num_images = 500
elif arg == 'private_test':
    base = preprocess('${PYLEARN2_DATA_PATH}/icml_2013_multimodal/private_test_images')
    outdir = base[:-6] + 'lcn'
    expected_num_images = 500
else:
    usage()
    print 'Unrecognized argument value:',arg
    print 'Recognized values are: public_test, private_test'

serial.mkdir(outdir)

paths = os.listdir(base)
if len(paths) != expected_num_images:
    raise AssertionError("Something is wrong with your " + base \
            + "directory. It should contain " + str(expected_num_images) + \
            " image files, but contains " + str(len(paths)))

kernel_shape = 7

from theano import tensor as T
from pylearn2.utils import sharedX
from pylearn2.datasets.preprocessing import gaussian_filter
from theano.tensor.nnet import conv2d

X = T.TensorType(dtype='float32', broadcastable=(True, False, False, True))()
from theano import config
if config.compute_test_value == 'raise':
    X.tag.test_value = np.zeros((1,32,32,1),dtype=X.dtype)
orig_X = X
filter_shape = (1, 1, kernel_shape, kernel_shape)
filters = sharedX(gaussian_filter(kernel_shape).reshape(filter_shape))

X = X.dimshuffle(0, 3, 1, 2)

convout = conv2d(X, filters=filters, border_mode='full')

# For each pixel, remove mean of 9x9 neighborhood
mid = int(np.floor(kernel_shape/ 2.))
centered_X = X - convout[:,:,mid:-mid,mid:-mid]

# Scale down norm of 9x9 patch if norm is bigger than 1
sum_sqr_XX = conv2d(T.sqr(X), filters=filters, border_mode='full')

denom = T.sqrt(sum_sqr_XX[:,:,mid:-mid,mid:-mid])
per_img_mean = denom.mean(axis = [2,3])
divisor = T.largest(per_img_mean.dimshuffle(0,1, 'x', 'x'), denom)

new_X = centered_X / T.maximum(1., divisor)

new_X = new_X.dimshuffle(0, 2, 3, 1)

from theano import function
f = function([orig_X], new_X)

j = 0
for path in paths:
    if j % 100 == 0:
        print j
    try:
        raw_path = path
        path = base + '/' + path
        img = image.load(path)

        #image.show(img)
        if len(img.shape) == 3 and img.shape[2] == 4:
            img = img[:, :, 0:3]
        img = img.reshape(*([1]+list(img.shape))).astype('float32')
        channels = [f(img[:,:,:,i:i+1]) for i in xrange(img.shape[3])]
        if len(channels) != 3:
            assert len(channels) == 1
            channels = [channels[0] ] * 3
        img = np.concatenate(channels, axis=3)
        img = img[0,:,:,:]

        assert not np.any(np.isnan(img))
        assert not np.any(np.isinf(img))

        path = outdir + '/' + raw_path
        path = path[0:-3]
        assert path.endswith('.')
        path = path + 'npy'
        np.save(path, img)
    except Exception, e:
        raise
        print e
    j += 1


########NEW FILE########
__FILENAME__ = make_submission
import numpy as np
import sys

from theano import function
from theano import tensor as T

from pylearn2.utils import serial
from pylearn2.utils.string_utils import preprocess

def usage():
    """
Run
python make_submission.py <model> <test set>
where <test set> is public_test or private_test
(private_test will be released 72 hours before the end of the contest)
"""

if len(sys.argv) != 3:
    usage()
    print "(You used the wrong number of arguments)"
    quit(-1)

_, model_path, test_set = sys.argv

model = serial.load(model_path)

# Load BOVW features
features_dir = preprocess('${PYLEARN2_DATA_PATH}/icml_2013_multimodal/'+test_set+'_layer_2_features')
vectors = []
for i in xrange(500):
    vectors.append(serial.load(features_dir + '/' + str(i) + '.npy'))
features = np.concatenate(vectors, axis=0)
del vectors


# Load BOW targets
f = open('wordlist.txt')
wordlist = f.readlines()
f.close()
options_dir = preprocess('${PYLEARN2_DATA_PATH}/icml_2013_multimodal/'+test_set+'_options')
def load_options(option):
    rval = np.zeros((500, 4000), dtype='float32')
    for i in xrange(500):
        f = open(options_dir + '/' + str(i) + '.option_' + str(option) + '.desc')
        l = f.readlines()
        f.close()
        for w in l:
            if w in wordlist:
                rval[i, wordlist.index(w)] = 1
    return rval
option_0, option_1 = [load_options(0), load_options(1)]

X = T.matrix()
Y0 = T.matrix()
Y1 = T.matrix()

Y_hat = model.fprop(X)

cost_0 = model.layers[-1].kl(Y=Y0, Y_hat=Y_hat)
cost_1 = model.layers[-1].kl(Y=Y1, Y_hat=Y_hat)

f = function([X, Y0, Y1], cost_1 < cost_0)

prediction = f(features, option_0, option_1)

f = open('submission.csv', 'w')

for i in xrange(500):
    f.write(str(prediction[i])+'\n')
f.close()

########NEW FILE########
__FILENAME__ = make_wordlist
import os
from pylearn2.utils.string_utils import preprocess
base = '${PYLEARN2_DATA_PATH}/esp_game/ESPGame100k/labels/'
base = preprocess(base)
paths = sorted(os.listdir(base))
assert len(paths)==100000

words = {}

for i, path in enumerate(paths):

    if i % 1000 == 0:
        print i
    path = base+path
    f = open(path,'r')
    lines = f.readlines()
    for line in lines:
        word = line[:-1]
        if word not in words:
            words[word] = 1
        else:
            words[word] += 1

ranked_words = sorted(words.keys(), key=lambda x: -words[x])

ranked_words = [word_ + '\n' for word_ in ranked_words[0:4000]]


f = open('wordlist.txt','w')
f.writelines(ranked_words)
f.close()

########NEW FILE########
__FILENAME__ = experiment
# Local imports
import pylearn2.config.yaml_parse

import jobman
from jobman.tools import expand, flatten


class ydict(dict):
    '''
    YAML-friendly subclass of dictionary.

    The special key "__builder__" is interpreted as the name of an object
    constructor.

    For instance, building a ydict from the following dictionary:

        {
            '__builder__': 'pylearn2.training_algorithms.sgd.EpochCounter',
            'max_epochs': 2
        }

    Will be displayed like:

        !obj:pylearn2.training_algorithms.sgd.EpochCounter {'max_epochs': 2}
    '''
    def __str__(self):
        args_dict = dict(self)
        builder = args_dict.pop('__builder__', '')
        ret_list = []
        if builder:
            ret_list.append('!obj:%s {' % builder)
        else:
            ret_list.append('{')

        for key, val in args_dict.iteritems():
            # This will call str() on keys and values, not repr(), so unicode
            # objects will have the form 'blah', not "u'blah'".
            ret_list.append('%s: %s,' % (key, val))

        ret_list.append('}')
        return '\n'.join(ret_list)


def train_experiment(state, channel):
    """
    Train a model specified in state, and extract required results.

    This function builds a YAML string from ``state.yaml_template``, taking
    the values of hyper-parameters from ``state.hyper_parameters``, creates
    the corresponding object and trains it (like train.py), then run the
    function in ``state.extract_results`` on it, and store the returned values
    into ``state.results``.

    To know how to use this function, you can check the example in tester.py
    (in the same directory).
    """
    yaml_template = state.yaml_template

    # Convert nested DD into nested ydict.
    hyper_parameters = expand(flatten(state.hyper_parameters), dict_type=ydict)

    # This will be the complete yaml string that should be executed
    final_yaml_str = yaml_template % hyper_parameters

    # Instantiate an object from YAML string
    train_obj = pylearn2.config.yaml_parse.load(final_yaml_str)

    try:
        iter(train_obj)
        iterable = True
    except TypeError:
        iterable = False
    if iterable:
        raise NotImplementedError(
                ('Current implementation does not support running multiple '
                 'models in one yaml string.  Please change the yaml template '
                 'and parameters to contain only one single model.'))
    else:
        # print "Executing the model."
        train_obj.main_loop()
        # This line will call a function defined by the user and pass train_obj
        # to it.
        state.results = jobman.tools.resolve(state.extract_results)(train_obj)
        return channel.COMPLETE

########NEW FILE########
__FILENAME__ = tester
"""
This an example script inserting a pylearn2 yaml code into a jobman database.

The code below defines a yaml template string in state.yaml_template,
and the values of its hyper-parameters in state.hyper_parameters, and
run the code that is located in state.extract_results on this model
using jobman.

Actually, we add the job here and it can be launched later as usual
(please check how to start jobs using jobman from the jobman tutorial
website)

"""

from jobman.tools import DD, flatten
from jobman import api0, sql

from pylearn2.scripts.jobman import experiment


def result_extractor(train_obj):
    """
    This is a user specific function, that is used by jobman to extract results

    The returned dictionary will be saved in state.results
    """
    import numpy

    channels = train_obj.model.monitor.channels
    train_cost = channels['sgd_cost(ExhaustiveSGD[X])']
    best_epoch = numpy.argmin(train_cost.val_record)
    best_rec_error = train_cost.val_record[best_epoch]
    batch_num = train_cost.batch_record[best_epoch]
    return dict(
            best_epoch=best_epoch,
            train_rec_error=best_rec_error,
            batch_num=batch_num)


if __name__ == '__main__':
    db = api0.open_db('sqlite:///test.db?table=test_jobman_pylearn2')

    state = DD()

    state.yaml_template = '''
        !obj:pylearn2.train.Train {
        "dataset": !obj:pylearn2.datasets.npy_npz.NpyDataset &dataset {
            "file" : "%(file)s"
        },
        "model": !obj:pylearn2.autoencoder.ContractiveAutoencoder {
            "nvis" : %(nvis)d,
            "nhid" : %(nhid)d,
            "irange" : 0.05,
            "act_enc": "sigmoid", #for some reason only sigmoid function works
            "act_dec": "sigmoid",
        },
        "algorithm": !obj:pylearn2.training_algorithms.sgd.SGD {
            "learning_rate" : %(learning_rate)f,
            "batch_size" : %(batch_size)d,
            "monitoring_batches" : 5,
            "monitoring_dataset" : *dataset,
            "cost" : !obj:pylearn2.costs.cost.SumOfCosts { 
                "costs": [
                    [1.0, !obj:pylearn2.costs.autoencoder.MeanBinaryCrossEntropy {} ],
                    [%(coefficient)f, !obj:pylearn2.costs.cost.MethodCost { method: 'contraction_penalty' } ]
                ]
            },
            "termination_criterion" : %(term_crit)s,
        }
    }
    '''

    state.hyper_parameters = {
            "file": "${PYLEARN2_DATA_PATH}/UTLC/pca/sylvester_train_x_pca32.npy",
            "nvis": 32,
            "nhid": 6,
            "learning_rate": 0.1,
            "batch_size": 10,
            "coefficient": 0.5,
            "term_crit": {
                "__builder__": "pylearn2.training_algorithms.sgd.EpochCounter",
                "max_epochs": 2
                }
            }

    state.extract_results = "pylearn2.scripts.jobman.tester.result_extractor"

    sql.insert_job(
            experiment.train_experiment,
            flatten(state),
            db,
            force_dup=True)

########NEW FILE########
__FILENAME__ = make_dataset
#replicate the preprocessing described in Kai Yu's paper Improving LCC with Local Tangents
from pylearn2.utils import serial
from pylearn2.datasets import cifar10
from pylearn2.datasets import preprocessing

train = cifar10.CIFAR10(which_set="train",center=True)

pipeline = preprocessing.Pipeline()
pipeline.items.append(preprocessing.GlobalContrastNormalization(subtract_mean=False,sqrt_bias=0.0, use_std=True))
pipeline.items.append(preprocessing.PCA(num_components=512))

test = cifar10.CIFAR10(which_set="test")

train.apply_preprocessor(preprocessor = pipeline, can_fit = True)
test.apply_preprocessor(preprocessor = pipeline, can_fit = False)

serial.save('cifar10_preprocessed_train.pkl',train)
serial.save('cifar10_preprocessed_test.pkl',test)



########NEW FILE########
__FILENAME__ = make_weights_image
#!/usr/bin/env python
"""
.. todo::

    WRITEME
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"
import sys
from pylearn2.gui import get_weights_report
import warnings

warnings.warn("make_weights_image.py is deprecated. Use show_weights.py with"
        " the --out flag. make_weights_image.py may be removed on or after "
        "2014-08-28.")

if __name__ == "__main__":
    print 'loading model'
    path = sys.argv[1]
    print 'loading done'

    rescale = True

    if len(sys.argv) > 2:
        rescale = eval(sys.argv[2])

    pv = get_weights_report.get_weights_report(path, rescale)

    pv.save(sys.argv[1]+'.png')

########NEW FILE########
__FILENAME__ = predict_csv
#!/usr/bin/env python
# coding: utf-8

"""
prediction code for classification, without using batches
(it's simpler that way)
if you run out of memory it could be resolved by implementing a batch version
the model should be an MLP
see http://fastml.com/how-to-get-predictions-from-pylearn2/
author: Zygmunt Zajc
"""

import sys
import os
import numpy as np
import cPickle as pickle

from pylearn2.utils import serial
from theano import tensor as T
from theano import function

if __name__ == "__main__":
    try:
        model_path = sys.argv[1]
        test_path = sys.argv[2]
        out_path = sys.argv[3]
    except IndexError:
        print "Usage: predict.py <model file> <test file> <output file>"
        print "       predict.py saved_model.pkl test_x.csv predictions.csv\n"
        quit(-1)

    print "loading model..."

    try:
        model = serial.load(model_path)
    except Exception, e:
        print "error loading {}:".format(model_path)
        print e
        quit(-1)

    print "setting up symbolic expressions..."

    X = model.get_input_space().make_theano_batch()
    Y = model.fprop(X)

# drop this when performing regression
    Y = T.argmax(Y, axis=1)

    f = function([X], Y)

    print "loading data and predicting..."

# x is a numpy array
# x = pickle.load(open(test_path, 'rb'))
    x = np.loadtxt(test_path, delimiter=',')	# no labels in the file

    y = f(x)

    print "writing predictions..."

    np.savetxt(out_path, y, fmt='%d')



########NEW FILE########
__FILENAME__ = num_parameters
#!/usr/bin/env python
"""
Usage: python num_parameters.py <model_file>.pkl

Prints the number of parameters in a saved model (total number of scalar
elements in all the arrays parameterizing the model).
"""
__author__ = "Ian Goodfellow"

import sys

from pylearn2.utils import serial

def num_parameters(model):
    """
    .. todo::

        WRITEME
    """
    params = model.get_params()
    return sum(map(lambda x: x.get_value().size, params))

if __name__ == '__main__':
    _, model_path = sys.argv
    model = serial.load(model_path)
    print num_parameters(model)

########NEW FILE########
__FILENAME__ = test_dropout
"""
Unit tests for dropout paper
"""

import os
from pylearn2.scripts.tests.yaml_testing import limited_epoch_train
from pylearn2.testing.skip import skip_if_no_data

yaml_file_path = os.path.abspath(os.path.join(os.path.dirname(__file__),
                                              '..'))
save_path = os.path.dirname(os.path.realpath(__file__))


def test_mnist_valid():
    """
    Tests mnist_valid.yaml by running it for only one epoch
    """

    skip_if_no_data()
    limited_epoch_train(os.path.join(yaml_file_path, 'mnist_valid.yaml'))
    try:
        os.remove(os.path.join(save_path, 'mnist_valid.pkl'))
        os.remove(os.path.join(save_path, 'mnist_valid_best.pkl'))
    except:
        pass


def test_mnist():
    """
    Tests mnist.yaml by running it for only one epoch
    """

    skip_if_no_data()
    limited_epoch_train(os.path.join(yaml_file_path, 'mnist.yaml'))
    try:
        os.remove(os.path.join(save_path, 'mnist.pkl'))
        os.remove(os.path.join(save_path, 'mnist_best.pkl'))
    except:
        pass

########NEW FILE########
__FILENAME__ = assemble
import numpy as np
import os

#check that the right files are present
names = os.listdir('.')

if 'features.npy' in names:
    print "Not doing anything, features.npy already exists."
    quit(0)

chunk_names = [ 'features_A.npy',
                'features_B.npy',
                'features_C.npy',
                'features_D.npy',
                'features_E.npy' ]

for name in chunk_names:
    assert name in names

for name in chunk_names:
    if name.startswith('features') and name.endswith('.npy'):
        if name not in chunk_names:
            print "I'm not sure what to do with "+name
            print "The existence of this file makes me think extract_features.yaml has changed"
            print "I don't want to do something incorrect so I'm going to give up."
            quit(-1)



#Do the conversion
print 'loading '+chunk_names[0]
first_chunk = np.load(chunk_names[0])

final_shape = list(first_chunk.shape)

final_shape[0] = 50000

print 'making output'
X = np.zeros(final_shape,dtype='float32')

idx = first_chunk.shape[0]

X[0:idx,:] = first_chunk

for i in xrange(1, len(chunk_names)):

    arg = chunk_names[i]
    print 'loading '+arg

    chunk = np.load(arg)

    chunk_span = chunk.shape[0]

    X[idx:idx+chunk_span,...] = chunk

    idx += chunk_span

print "Saving features.npy..."
np.save('features.npy',X)

print "Deleting the chunks..."
for chunk_name in chunk_names:
    os.remove(chunk_name)


########NEW FILE########
__FILENAME__ = evaluate
from optparse import OptionParser
import warnings
try:
    from sklearn.metrics import classification_report
except ImportError:
    classification_report = None
    warnings.warn("couldn't find sklearn.metrics.classification_report")
try:
    from sklearn.metrics import confusion_matrix
except ImportError:
    confusion_matrix = None
    warnings.warn("couldn't find sklearn.metrics.metrics.confusion_matrix")
from galatea.s3c.feature_loading import get_features
from pylearn2.utils import serial
from pylearn2.datasets.cifar10 import CIFAR10
from pylearn2.datasets.cifar100 import CIFAR100
import numpy as np

def test(model, X, y):
    print "Evaluating svm"
    y_pred = model.predict(X)
    #try:
    if True:
        acc = (y == y_pred).mean()
        print "Accuracy ",acc
    """except:
        print "something went wrong"
        print 'y:'
        print y
        print 'y_pred:'
        print y_pred
        print 'extra info'
        print type(y)
        print type(y_pred)
        print y.dtype
        print y_pred.dtype
        print y.shape
        print y_pred.shape
        raise
"""
#


def get_test_labels(cifar10, cifar100, stl10):
    assert cifar10 + cifar100 +  stl10 == 1

    if stl10:
        print 'loading entire stl-10 test set just to get the labels'
        stl10 = serial.load("${PYLEARN2_DATA_PATH}/stl10/stl10_32x32/test.pkl")
        return stl10.y
    if cifar10:
        print 'loading entire cifar10 test set just to get the labels'
        cifar10 = CIFAR10(which_set = 'test')
        return np.asarray(cifar10.y)
    if cifar100:
        print 'loading entire cifar100 test set just to get the fine labels'
        cifar100 = CIFAR100(which_set = 'test')
        return np.asarray(cifar100.y_fine)
    assert False


def main(model_path,
        test_path,
        dataset,
        **kwargs):

    model =  serial.load(model_path)

    cifar100 = dataset == 'cifar100'
    cifar10 = dataset == 'cifar10'
    stl10 = dataset == 'stl10'
    assert cifar10 + cifar100 + stl10 == 1

    y = get_test_labels(cifar10, cifar100, stl10)
    X = get_features(test_path, False, False)
    if stl10:
        num_examples = 8000
    if cifar10 or cifar100:
        num_examples = 10000
    if not X.shape[0] == num_examples:
        raise AssertionError('Expected %d examples but got %d' % (num_examples, X.shape[0]))
    assert y.shape[0] == num_examples

    test(model,X,y)


if __name__ == '__main__':
    """
    Useful for quick tests.
    Usage: python train_bilinear.py
    """

    parser = OptionParser()
    parser.add_option("-m", "--model",
                action="store", type="string", dest="model_path")
    parser.add_option("-t", "--test",
                action="store", type="string", dest="test")
    parser.add_option("-o", action="store", dest="output", default = None, help="path to write the report to")
    parser.add_option('--dataset', type='string', dest = 'dataset', action='store', default = None)

    #(options, args) = parser.parse_args()

    #assert options.output

    main(model_path='final_model.pkl',
         test_path='test_features.npy',
         dataset = 'cifar100',
    )

########NEW FILE########
__FILENAME__ = extract_features
"""

This file implements feature extraction using the k-means + triangle
code with random pooling from the Jia and Huang paper.

Usage: python feature_extractor.py config.yaml

config.yaml should deserialize to an extract_features.FeatureExtractor object.
See the FeatureExtractor.__init__ method for documentation of options.

"""


import os
from pylearn2.config import yaml_parse
import warnings
import time
import copy
import numpy as np
from theano import config
from theano import tensor as T
from theano import function
from pylearn2.datasets.preprocessing import ExtractPatches, ExtractGridPatches, ReassembleGridPatches
from pylearn2.utils import serial
from pylearn2.utils.rng import make_np_rng
from pylearn2.datasets.dense_design_matrix import DenseDesignMatrix, DefaultViewConverter
from pylearn2.datasets.cifar10 import CIFAR10
from pylearn2.datasets.cifar100 import CIFAR100
from pylearn2.datasets.tl_challenge import TL_Challenge
from pylearn2.expr.coding import triangle_code
from pylearn2.space import VectorSpace


import sys
config.floatX = 'float32'

#This is hard-coded to the value from the Jia and Huang paper
#I confirmed via e-mail with the authors that they indeed create
#4 superpixels along each axis, not superpixels of size 4, and
#that not all superpixels are the same size (since
#the dimensions of the detector feature map are not divisible by 4)
num_superpixels = 4

class halver:
    """
    This object splits the processing for an example in half.
    If you're using a lot of features, this is necessary to be able
    to fit the computation on the GPU

    Other than that, you can just pretend it's a theano function
    """

    def __init__(self,f,nhid):
        """ f: the theano function whose work needs to be split
            nhid: we assume f takes a matrix of shape (m,nvis) and
                  returns a matrix of shape (m,nhid)
        """
        self.f = f
        self.nhid = nhid

    def __call__(self,X):
        m = X.shape[0]
        #mid = m/2
        m1 = m/3
        m2 = 2*m/3

        rval = np.zeros((m,self.nhid),dtype='float32')
        rval[0:m1,:] = self.f(X[0:m1,:])
        rval[m1:m2,:] = self.f(X[m1:m2,:])
        rval[m2:,:] = self.f(X[m2:,:])

        return rval


class FeaturesDataset:
    def __init__(self, dataset_maker, num_examples, pipeline_path):
        """
            dataset_maker: A callable that returns a Dataset
            num_examples: the number of examples we expect the dataset to have
                          (just for error checking purposes)
        """
        self.dataset_maker = dataset_maker
        self.num_examples = num_examples
        self.pipeline_path = pipeline_path


stl10 = {}
stl10_no_shelling = {}
stl10_size = { 'train' : 5000, 'test' : 8000 }

cifar10 = {}
cifar10_size = { 'train' : 50000, 'test' : 10000 }

cifar100 = {}
cifar100_size = { 'train' : 50000, 'test' : 10000 }

tl_challenge = {}
tl_challenge_size = { 'train' : 120, 'test' : 0 }

class dataset_loader:
    def __init__(self, path):
        self.path = path

    def __call__(self):
        return serial.load(self.path)

class dataset_constructor:
    def __init__(self, cls, which_set):
        self.cls = cls
        self.which_set = which_set

    def __call__(self):
        return self.cls(self.which_set)


for which_set in ['train', 'test']:
    stl10[which_set] = {}
    stl10_no_shelling[which_set] = {}
    cifar10[which_set] = {}
    cifar100[which_set] = {}
    tl_challenge[which_set] = {}

    #this is for patch size, not datset size
    for size in [6]:
        stl10[which_set][size] = FeaturesDataset( dataset_maker = dataset_loader( '${PYLEARN2_DATA_PATH}/stl10/stl10_32x32/'+which_set+'.pkl'),
                                            num_examples = stl10_size[which_set],
                                            pipeline_path = '${PYLEARN2_DATA_PATH}/stl10/stl10_patches/preprocessor.pkl')

        stl10_no_shelling[which_set][size] = FeaturesDataset( dataset_maker = dataset_loader( '${PYLEARN2_DATA_PATH}/stl10/stl10_32x32/'+which_set+'.pkl'),
                                            num_examples = stl10_size[which_set],
                                            pipeline_path = '${GOODFELI_TMP}/stl10/stl10_patches_no_shelling/preprocessor.pkl')

        cifar10[which_set][size] = FeaturesDataset( dataset_maker = dataset_constructor( CIFAR10, which_set),
                                                num_examples = cifar10_size[which_set],
                                                pipeline_path = '${GOODFELI_TMP}/cifar10_preprocessed_pipeline_2M_6x6.pkl')

        cifar100[which_set][size] = FeaturesDataset( dataset_maker = dataset_constructor( CIFAR100, which_set),
                                                num_examples = cifar100_size[which_set],
                                                pipeline_path = '${PYLEARN2_DATA_PATH}/cifar100/cifar100_patches/preprocessor.pkl')

        tl_challenge[which_set][size] = FeaturesDataset( dataset_maker = dataset_constructor( TL_Challenge, which_set),
                                                num_examples = tl_challenge_size[which_set],
                                                pipeline_path = '${GOODFELI_TMP}/tl_challenge_patches_2M_6x6_prepro.pkl')


    stl10[which_set][8] = FeaturesDataset( dataset_maker = dataset_loader( '${PYLEARN2_DATA_PATH}/stl10/stl10_32x32/'+which_set+'.pkl'),
                                            num_examples = stl10_size[which_set],
                                            pipeline_path = '${PYLEARN2_DATA_PATH}/stl10/stl10_patches_8x8/preprocessor.pkl')



class FeatureExtractor:
    def __init__(self, batch_size, kmeans_path,
           save_path,  dataset_family, which_set,
           num_output_features,
           chunk_size = None, restrict = None, pool_mode = 'mean'):
        """
            batch_size:  the number of images to process simultaneously
                         this does not affect the final result, it is just for performance
                         larger values allow more parallel processing but require more memory
            kmeans_path: a path to a .pkl file containing a pylearn2.kmeans.KMeans instance
            save_path:   the base path to save to, should end in .npy
            dataset_family: extract_features.stl10, extract_features.cifar10, etc.
            which_set:     'train' or 'test'
            num_output_features: the number of randomly selected pooled features to extract per image
            chunk_size:   will build a design matrix of this many processed examples before serializing
                         them. if you use a chunk size of 10,000 on a dataset with 50,000 examples, and
                         a save_path of foo.npy, this will result in you obtaining foo_A.npy through
                         foo_E.npy.
                         Use a small chunk_size to avoid running out of memory.
            restrict:    a tuple of of (start,end) indices
                         restrict feature extraction to only these examples
                         the restrict option is used internally to implement the chunk_size option, so
                         you may not specify both for the same FeatureExtractor
            pool_mode:   'max' or 'mean'
        """

        if chunk_size is not None and restrict is not None:
            raise NotImplementedError("Currently restrict is used internally to "
                    "implement chunk_size, so a client may not specify both")

        self.batch_size = batch_size
        self.model_path = kmeans_path
        self.restrict = restrict
        self.pool_mode = pool_mode


        assert save_path is not None
        assert save_path.endswith('npy')

        assert pool_mode in [ 'mean', 'max' ]

        self.save_path = save_path
        self.which_set = which_set
        self.dataset_family = dataset_family
        self.chunk_size = chunk_size
        self.num_output_features = num_output_features


    def __call__(self):

        print 'loading model'
        model_path = self.model_path
        self.model = serial.load(model_path)
        self.model.set_dtype('float32')
        input_space = self.model.get_input_space()
        #This code all assumes the model works on vectors
        assert isinstance(input_space, VectorSpace)
        nvis = input_space.dim
        self.size = int(np.sqrt(nvis/3))

        rng = make_np_rng(None, [1,2,3], which_method="randint")

        #Generate the random pooling structure
        num_filters = self.model.mu.get_value().shape[0]

        idxs = rng.randint(0,num_filters,(self.num_output_features,))
        top = idxs.copy()
        bottom = idxs.copy()
        left = idxs.copy()
        right = idxs.copy()
        for i in xrange(self.num_output_features):
            top[i] = rng.randint(num_superpixels)
            bottom[i] = rng.randint(top[i],num_superpixels)
            left[i] = rng.randint(num_superpixels)
            right[i] = rng.randint(left[i],num_superpixels)

        #output[i] will pool over detector feature map i
        self.idxs = idxs
        #output [i] will pool over a rectangle with upper left coordinates (top[i],left[i])
        #and lower right coordinates (bottom[i],right[i])
        self.top = top
        self.bottom = bottom
        self.left = left
        self.right = right



        #Run the experiment
        if self.chunk_size is not None:
            dataset_family = self.dataset_family
            which_set = self.which_set
            dataset_descriptor = self.dataset_family[which_set][size]

            num_examples = dataset_descriptor.num_examples
            assert num_examples % self.chunk_size == 0

            self.chunk_id = 0
            for i in xrange(0,num_examples, self.chunk_size):
                self.restrict = (i, i + self.chunk_size)

                self._execute()

                self.chunk_id += 1
        else:
            self._execute()

    def _execute(self):

        global num_superpixels
        num_output_features = self.num_output_features
        idxs = self.idxs
        top = self.top
        bottom = self.bottom
        left = self.left
        right = self.right

        save_path = self.save_path
        batch_size = self.batch_size
        dataset_family = self.dataset_family
        which_set = self.which_set
        model = self.model
        size = self.size

        nan = 0


        dataset_descriptor = dataset_family[which_set][size]

        dataset = dataset_descriptor.dataset_maker()
        expected_num_examples = dataset_descriptor.num_examples

        full_X = dataset.get_design_matrix()
        num_examples = full_X.shape[0]
        assert num_examples == expected_num_examples

        if self.restrict is not None:
            assert self.restrict[1]  <= full_X.shape[0]

            print 'restricting to examples ',self.restrict[0],' through ',self.restrict[1],' exclusive'
            full_X = full_X[self.restrict[0]:self.restrict[1],:]

            assert self.restrict[1] > self.restrict[0]

        #update for after restriction
        num_examples = full_X.shape[0]

        assert num_examples > 0

        dataset.X = None
        dataset.design_loc = None
        dataset.compress = False

        patchifier = ExtractGridPatches( patch_shape = (size,size), patch_stride = (1,1) )

        pipeline = serial.load(dataset_descriptor.pipeline_path)

        assert isinstance(pipeline.items[0], ExtractPatches)
        pipeline.items[0] = patchifier


        print 'defining features'
        V = T.matrix('V')

        mu = model.mu

        feat = triangle_code(V, mu)


        assert feat.dtype == 'float32'
        print 'compiling theano function'
        f = function([V],feat)

        nhid = model.mu.get_value().shape[0]

        if config.device.startswith('gpu') and nhid >= 4000:
            f = halver(f, model.nhid)

        topo_feat_var = T.TensorType(broadcastable = (False,False,False,False), dtype='float32')()
        if self.pool_mode == 'mean':
            region_features = function([topo_feat_var],
                topo_feat_var.mean(axis=(1,2)) )
        elif self.pool_mode == 'max':
            region_features = function([topo_feat_var],
                    topo_feat_var.max(axis=(1,2)) )
        else:
            assert False

        def average_pool( stride ):
            def point( p ):
                return p * ns / stride

            rval = np.zeros( (topo_feat.shape[0], stride, stride, topo_feat.shape[3] ) , dtype = 'float32')

            for i in xrange(stride):
                for j in xrange(stride):
                    rval[:,i,j,:] = region_features( topo_feat[:,point(i):point(i+1), point(j):point(j+1),:] )

            return rval

        output =  np.zeros((num_examples,num_output_features),dtype='float32')


        fd = DenseDesignMatrix(X = np.zeros((1,1),dtype='float32'), view_converter = DefaultViewConverter([1, 1, nhid] ) )

        ns = 32 - size + 1
        depatchifier = ReassembleGridPatches( orig_shape  = (ns, ns), patch_shape=(1,1) )

        if len(range(0,num_examples-batch_size+1,batch_size)) <= 0:
            print num_examples
            print batch_size

        for i in xrange(0,num_examples-batch_size+1,batch_size):
            print i
            t1 = time.time()

            d = copy.copy(dataset)
            d.set_design_matrix(full_X[i:i+batch_size,:])

            t2 = time.time()

            #print '\tapplying preprocessor'
            d.apply_preprocessor(pipeline, can_fit = False)
            X2 = d.get_design_matrix()

            t3 = time.time()

            #print '\trunning theano function'
            feat = f(X2)

            t4 = time.time()

            assert feat.dtype == 'float32'

            feat_dataset = copy.copy(fd)

            if np.any(np.isnan(feat)):
                nan += np.isnan(feat).sum()
                feat[np.isnan(feat)] = 0

            feat_dataset.set_design_matrix(feat)

            #print '\treassembling features'
            feat_dataset.apply_preprocessor(depatchifier)

            #print '\tmaking topological view'
            topo_feat = feat_dataset.get_topological_view()
            assert topo_feat.shape[0] == batch_size

            t5 = time.time()

            #average pooling
            superpixels = average_pool(num_superpixels)

            assert batch_size == 1

            if self.pool_mode == 'mean':
                for j in xrange(num_output_features):
                    output[i:i+batch_size, j] = superpixels[:,top[j]:bottom[j]+1,
                            left[j]:right[j]+1, idxs[j]].mean()
            elif self.pool_mode == 'max':
                for j in xrange(num_output_features):
                    output[i:i+batch_size, j] = superpixels[:,top[j]:bottom[j]+1,
                            left[j]:right[j]+1, idxs[j]].max()
            else:
                assert False

            assert output[i:i+batch_size,:].max() < 1e20

            t6 = time.time()

            print (t6-t1, t2-t1, t3-t2, t4-t3, t5-t4, t6-t5)

        if self.chunk_size is not None:
            assert save_path.endswith('.npy')
            save_path_pieces = save_path.split('.npy')
            assert len(save_path_pieces) == 2
            assert save_path_pieces[1] == ''
            save_path = save_path_pieces[0] + '_' + chr(ord('A')+self.chunk_id)+'.npy'
        np.save(save_path,output)


        if nan > 0:
            warnings.warn(str(nan)+' features were nan')

if __name__ == '__main__':
    assert len(sys.argv) == 2
    yaml_path = sys.argv[1]

    assert yaml_path.endswith('.yaml')
    val = yaml_path[0:-len('.yaml')]
    os.environ['FEATURE_EXTRACTOR_YAML_PATH'] = val
    os.putenv('FEATURE_EXTRACTOR_YAML_PATH',val)
    val = val.split('/')[-1]
    os.environ['FEATURE_EXTRACTOR_YAML_NAME'] = val
    os.putenv('FEATURE_EXTRACTOR_YAML_NAME', val)


    extractor = yaml_parse.load_path(yaml_path)

    extractor()

########NEW FILE########
__FILENAME__ = fit_final_model
import numpy as np
from optparse import OptionParser
from pylearn2.models.independent_multiclass_logistic import IndependentMulticlassLogistic
from galatea.s3c.feature_loading import get_features
from pylearn2.utils import serial
from pylearn2.datasets.cifar10 import CIFAR10
from pylearn2.datasets.cifar100 import CIFAR100
import gc
gc.collect()

def train(fold_train_X, fold_train_y, C):

    model = IndependentMulticlassLogistic(C).fit(fold_train_X, fold_train_y)
    gc.collect()

    return model



def get_labels_and_fold_indices(cifar10, cifar100, stl10):
    assert stl10 or cifar10 or cifar100
    assert stl10+cifar10+cifar100 == 1

    if stl10:
        print 'loading entire stl-10 train set just to get the labels and folds'
        stl10 = serial.load("${PYLEARN2_DATA_PATH}/stl10/stl10_32x32/train.pkl")
        train_y = stl10.y

        fold_indices = stl10.fold_indices
    elif cifar10 or cifar100:
        if cifar10:
            print 'loading entire cifar10 train set just to get the labels'
            cifar = CIFAR10(which_set = 'train')
        else:
            assert cifar100
            print 'loading entire cifar100 train set just to get the labels'
            cifar = CIFAR100(which_set = 'train')
            cifar.y = cifar.y_fine
        train_y = cifar.y
        assert train_y is not None

        fold_indices = np.zeros((5,40000),dtype='uint16')
        idx_list = np.cast['uint16'](np.arange(1,50001)) #mimic matlab format of stl10
        for i in xrange(5):
            mask = idx_list < i * 10000 + 1
            mask += idx_list >= (i+1) * 10000 + 1
            fold_indices[i,:] = idx_list[mask]
        assert fold_indices.min() == 1
        assert fold_indices.max() == 50000


    return train_y, fold_indices


def main(train_path,
        out_path,
        dataset,
        standardize,
        C,
        **kwargs):

    stl10 = dataset == 'stl10'
    cifar10 = dataset == 'cifar10'
    cifar100 = dataset == 'cifar100'
    assert stl10 + cifar10 + cifar100 == 1

    print 'getting labels and oflds'
    train_y, fold_indices = get_labels_and_fold_indices(cifar10, cifar100, stl10)
    gc.collect()
    assert train_y is not None

    print 'loading training features'
    train_X = get_features(train_path, split = False, standardize = standardize)

    assert str(train_X.dtype) == 'float32'
    if stl10:
        assert train_X.shape[0] == 5000
    if cifar10 or cifar100:
        assert train_X.shape[0] == 50000
        assert train_y.shape == (50000,)

    print 'training model'
    model =  train(train_X, train_y, C)

    print 'saving model'
    serial.save(out_path, model)

if __name__ == '__main__':

    parser = OptionParser()
    parser.add_option("-d", "--train",
                action="store", type="string", dest="train")
    parser.add_option("-o", "--out",
                action="store", type="string", dest="out")
    parser.add_option('-C', type='float', dest='C', action='store', default = None)
    parser.add_option('--dataset', type='string', dest = 'dataset', action='store', default = None)
    parser.add_option('--standardize',action="store_true", dest="standardize", default=False)
    parser.add_option('--fold', action='store', type='int', dest='fold', default = None)

    #(options, args) = parser.parse_args()

    #assert options.dataset is not None
    #assert options.C is not None
    #assert options.out is not None
    #assert options.fold is not None


    #log = open(options.out+'.log.txt','w')
    #log.write('log file started succesfully\n')
    #log.flush()

    print 'parsed the args'
    main(train_path='features.npy',
         out_path = 'final_model.pkl',
         C = .01,
         dataset = 'cifar100',
         standardize = False,
         #fold = options.fold,
         #log = log
    )

    #log.close()

########NEW FILE########
__FILENAME__ = npy2mat
import sys
in_path, out_path = sys.argv[1:]
from scipy import io
import numpy as np
print 'loading'
X = np.load(in_path)
if len(X.shape) > 2:
    print 'reshaping'
    X = X.reshape(X.shape[0],X.shape[1] * X.shape[2] * X.shape[3])
assert len(X.shape) == 2
print 'saving'
io.savemat(out_path,{'X':X})

if X.shape[1] > 14400:
    print 'reloading to make sure it worked'
    print '(matlab format can fail for large arrays)'
    X = io.loadmat(out_path)
    print 'success'

########NEW FILE########
__FILENAME__ = compute_test_err
from pylearn2.utils import py_integer_types
from pylearn2.utils import serial
from pylearn2.config import yaml_parse
import sys

_, model_path = sys.argv

model = serial.load(model_path)

src = model.dataset_yaml_src
batch_size = 100
model.set_batch_size(batch_size)

assert src.find('train') != -1
test = yaml_parse.load(src)
test = test.get_test_set()
assert test.X.shape[0] == 10000

test.X = test.X.astype('float32')

import theano.tensor as T

Xb = model.get_input_space().make_batch_theano()
Xb.name = 'Xb'
yb = model.get_output_space().make_batch_theano()
yb.name = 'yb'

ymf = model.fprop(Xb)
ymf.name = 'ymf'

from theano import function

yl = T.argmax(yb,axis=1)

mf1acc = 1.-T.neq(yl , T.argmax(ymf,axis=1)).mean()

batch_acc = function([Xb,yb],[mf1acc])

# The averaging math assumes batches are all same size
assert test.X.shape[0] % batch_size == 0


def accs():
    mf1_accs = []
    assert isinstance(test.X.shape[0], (int, long))
    assert isinstance(batch_size, py_integer_types)
    iterator = test.iterator(mode = 'even_sequential',
                            batch_size = batch_size,
                            data_specs = model.cost_from_X_data_specs())
    for item in iterator:
        x_arg, y_arg = item
        if Xb.ndim > 2:
            x_arg = test.get_topological_view(x_arg)
        mf1_accs.append(batch_acc(x_arg, y_arg)[0])
    return sum(mf1_accs) / float(len(mf1_accs))


result = accs()

print 1. - result


########NEW FILE########
__FILENAME__ = svhn_preprocessing
import os
import logging
import shutil
from theano import config
from pylearn2.datasets import preprocessing
from pylearn2.datasets.svhn import SVHN
from pylearn2.utils.string_utils import preprocess

orig_path = preprocess('${PYLEARN2_DATA_PATH}/SVHN/format2')
try:
    local_path = preprocess('${SVHN_LOCAL_PATH}')
except ValueError:
    raise ValueError("You need to define SVHN_LOCAL_PATH environment "
                        "variable.")

train_name ='h5/splitted_train_32x32.h5'
valid_name = 'h5/valid_32x32.h5'
test_name = 'h5/test_32x32.h5'

# copy data if don't exist
if not os.path.isdir(os.path.join(local_path, 'h5')):
    os.makedirs(os.path.join(local_path, 'h5'))

for d_set in [train_name, valid_name, test_name]:
    if not os.path.isfile(os.path.join(local_path, d_set)):
        logging.info("Copying data from {0} to {1}".format(os.path.join(local_path, d_set), local_path))
        shutil.copyfile(os.path.join(orig_path, d_set),
                    os.path.join(local_path, d_set))

def check_dtype(data):
    if str(data.X.dtype) != config.floatX:
        logging.warning("The dataset is saved as {}, changing theano's floatX "\
                "to the same dtype".format(data.X.dtype))
        config.floatX = str(data.X.dtype)

# Load train data
train = SVHN('splitted_train', path=local_path)
check_dtype(train)

# prepare preprocessing
pipeline = preprocessing.Pipeline()
# without batch_size there is a high chance that you might encounter memory error
# or pytables crashes
pipeline.items.append(preprocessing.GlobalContrastNormalization(batch_size=5000))
pipeline.items.append(preprocessing.LeCunLCN((32,32)))

# apply the preprocessings to train
train.apply_preprocessor(pipeline, can_fit=True)
del train

# load and preprocess valid
valid = SVHN('valid', path=local_path)
check_dtype(valid)
valid.apply_preprocessor(pipeline, can_fit=False)

# load and preprocess test
test = SVHN('test', path=local_path)
check_dtype(test)
test.apply_preprocessor(pipeline, can_fit=False)

########NEW FILE########
__FILENAME__ = test_mnist
"""
This file tests some of the YAML files in the maxout paper
"""
import os

import pylearn2
from pylearn2.datasets import control
from pylearn2.datasets.mnist import MNIST
from pylearn2.termination_criteria import EpochCounter
from pylearn2.testing.skip import skip_if_no_gpu
from pylearn2.utils.serial import load_train_file


def test_mnist():
    """
    Test the mnist.yaml file from the maxout
    paper on random input
    """
    skip_if_no_gpu()
    train = load_train_file(os.path.join(pylearn2.__path__[0],
                                         "scripts/papers/maxout/mnist.yaml"))

    # Load fake MNIST data
    init_value = control.load_data
    control.load_data = [False]
    train.dataset = MNIST(which_set='train', one_hot=1,
                          axes=['c', 0, 1, 'b'], start=0, stop=100)
    train.algorithm._set_monitoring_dataset(train.dataset)
    control.load_data = init_value

    # Train shortly and prevent saving
    train.algorithm.termination_criterion = EpochCounter(max_epochs=1)
    train.extensions.pop(0)
    train.save_freq = 0
    train.main_loop()


def test_mnist_pi():
    """
    Test the mnist_pi.yaml file from the maxout
    paper on random input
    """
    train = load_train_file(
        os.path.join(pylearn2.__path__[0],
                     "scripts/papers/maxout/mnist_pi.yaml")
    )

    # Load fake MNIST data
    init_value = control.load_data
    control.load_data = [False]
    train.dataset = MNIST(which_set='train', start=0, stop=100)
    train.algorithm._set_monitoring_dataset(train.dataset)
    control.load_data = init_value

    # Train shortly and prevent saving
    train.algorithm.termination_criterion = EpochCounter(max_epochs=1)
    train.extensions.pop(0)
    train.save_freq = 0
    train.main_loop()

########NEW FILE########
__FILENAME__ = pkl_inspector
#!/usr/bin/env python
"""
Determines the contribution of different subcomponents of a file to its file size, serialization time,
and deserialization time.
"""
import sys
from pylearn2.utils import serial
import cPickle
import pickle
import time
from theano.printing import min_informative_str

__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

def usage():
    """
    .. todo::

        WRITEME
    """
    print """
Usage:
first argument is a cPickle file to load
if no more arguments are supplied, will analyze each field of the root-level object stored in the file
subsequent arguments let you index into fields / dictionary entries of the object
For example,
pkl_inspector.py foo.pkl .my_field [my_key] 3
will load an object obj from foo.pkl and analyze obj.my_field["my_key"][3]
"""

if __name__ == "__main__":
    if len(sys.argv) == 1:
        usage()
        sys.exit(-1)

    hp = pickle.HIGHEST_PROTOCOL

    filepath = sys.argv[1]

    orig_obj = serial.load(filepath)

    cycle_check = {}

    obj_name = 'root_obj'
    cycle_check[id(orig_obj)] = obj_name

    for field in sys.argv[2:]:
        if field.startswith('['):
            assert field.endswith(']')
            obj_name += '[' + field[1:-1] + ']'
            orig_obj = orig_obj[field[1:-1]]
        elif field.startswith('.'):
            obj_name += '.' + field
            orig_obj = getattr(orig_obj,field[1:])
        else:
            obj_name + '[' + field + ']'
            orig_obj = orig_obj[eval(field)]
        if id(orig_obj) in cycle_check:
            print "You're going in circles, "+obj_name+" is the same as "+cycle_check[id(orig_obj)]
            quit()
        cycle_check[id(orig_obj)] = obj_name


    print 'type of object: '+str(type(orig_obj))
    print 'object: '+str(orig_obj)
    print 'object, longer description:\n'+min_informative_str(orig_obj, indent_level = 1)

    t1 = time.time()
    s = cPickle.dumps(orig_obj, hp)
    t2 = time.time()
    prev_ts = t2 - t1

    prev_bytes = len(s)
    print 'orig_obj bytes: \t\t\t\t'+str(prev_bytes)
    t1 = time.time()
    x = cPickle.loads(s)
    t2 = time.time()
    prev_t = t2 - t1
    print 'orig load time: '+str(prev_t)
    print 'orig save time: '+str(prev_ts)

    idx = 0

    print 'field\t\t\tdelta bytes\t\t\tdelta load time\t\t\tdelta save time'

    if not (isinstance(orig_obj, dict) or isinstance(orig_obj, list) ):
        while len(dir(orig_obj)) > idx:
            stop = False

            while True:
                fields = dir(orig_obj)
                if idx >= len(fields):
                    stop = True
                    break
                field = fields[idx]

                if field in ['names_to_del','__dict__']:
                    print 'not deleting '+field
                    idx += 1
                    continue

                success = True
                try:
                    delattr(orig_obj,field)

                except:
                    #TODO: add a config flag to allow printing the following messages
                    #print "got error trying to delete "+field
                    idx += 1
                    success = False
                if success and field in dir(orig_obj):
                    print field + ' reappears after being deleted'
                    idx += 1
                if success:
                    break

            if stop:
                break

            #print hasattr(orig_obj, 'names_to_del')
            t1 = time.time()
            s = cPickle.dumps(orig_obj, hp)
            t2 = time.time()
            new_ts = t2 - t1
            diff_ts = prev_ts - new_ts
            prev_ts = new_ts
            new_bytes = len(s)
            diff_bytes = prev_bytes - new_bytes
            prev_bytes = new_bytes
            t1 = time.time()
            x = cPickle.loads(s)
            t2 = time.time()
            new_t = t2 - t1
            diff_t = prev_t - new_t
            prev_t = new_t
            print field+': \t\t\t\t'+str(diff_bytes)+'\t\t\t'+str(diff_t)+'\t\t\t'+str(diff_ts)

    if isinstance(orig_obj, dict):
        print 'orig_obj is a dictionary'

        keys = [ key for key in orig_obj.keys() ]

        for key in keys:
            orig_obj[key] = None

            s = cPickle.dumps(orig_obj, hp)
            new_bytes = len(s)
            t1 = time.time()
            x = cPickle.loads(s)
            t2 = time.time()
            new_t = t2 - t1
            diff_t = prev_t - new_t
            prev_t = new_t
            diff_bytes = prev_bytes - new_bytes
            prev_bytes = new_bytes
            print 'val for '+str(key)+': \t\t\t\t'+str(diff_bytes)+'\t\t\t'+str(diff_t)

        for key in keys:
            del orig_obj[key]

            s = cPickle.dumps(orig_obj, hp)
            new_bytes = len(s)
            t1 = time.time()
            x = cPickle.loads(s)
            t2 = time.time()
            new_t = t2 - t1
            diff_t = prev_t - new_t
            prev_t = new_t
            diff_bytes = prev_bytes - new_bytes
            prev_bytes = new_bytes
            print str(key)+': \t\t\t\t'+str(diff_bytes)+'\t\t\t'+str(diff_t)


    if isinstance(orig_obj, list):
        print 'orig_obj is a list'

        i = 0
        while len(orig_obj) > 0:
            stringrep = str(orig_obj[0])
            if len(stringrep) > 15:
                stringrep = stringrep[0:12] + "..."
            del orig_obj[0]

            t1 = time.time()
            s = cPickle.dumps(orig_obj, hp)
            t2 = time.time()
            new_ts = t2 - t1
            diff_ts = prev_ts - new_ts
            prev_ts = new_ts

            new_bytes = len(s)
            diff_bytes = prev_bytes - new_bytes
            prev_bytes = new_bytes

            t1 = time.time()
            x = cPickle.loads(s)
            t2 = time.time()
            new_t = t2 - t1
            diff_t = prev_t - new_t
            prev_t = new_t
            print stringrep+': \t\t\t\t'+str(diff_bytes)+'\t\t\t'+str(diff_t)+'\t\t\t'+str(diff_ts)

            i+= 1


########NEW FILE########
__FILENAME__ = plot_monitor
#!/usr/bin/env python
"""
usage:

plot_monitor.py model_1.pkl model_2.pkl ... model_n.pkl

Loads any number of .pkl files produced by train.py. Extracts
all of their monitoring channels and prompts the user to select
a subset of them to be plotted.

"""
__authors__ = "Ian Goodfellow, Harm Aarts"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

from pylearn2.utils import serial
import numpy as np
import sys
from theano.printing import _TagGenerator
from pylearn2.utils.string_utils import number_aware_alphabetical_key
import argparse

channels = {}

def unique_substring(s, other, min_size=1):
    """
    .. todo::

        WRITEME
    """
    size = min(len(s), min_size)
    while size <= len(s):
        for pos in xrange(0,len(s)-size+1):
            rval = s[pos:pos+size]
            fail = False
            for o in other:
                if o.find(rval) != -1:
                    fail = True
                    break
            if not fail:
                return rval
        size += 1
    # no unique substring
    return s

def unique_substrings(l, min_size=1):
    """
    .. todo::

        WRITEME
    """
    return [unique_substring(s, [x for x in l if x is not s], min_size)
            for s in l]

def main():
    """
    .. todo::

        WRITEME
    """
    parser = argparse.ArgumentParser()
    parser.add_argument("--out")
    parser.add_argument("model_paths", nargs='+')
    options = parser.parse_args()
    model_paths = options.model_paths

    if options.out is not None:
      import matplotlib
      matplotlib.use('Agg')
    import matplotlib.pyplot as plt

    print 'generating names...'
    model_names = [model_path.replace('.pkl', '!') for model_path in
            model_paths]
    model_names = unique_substrings(model_names, min_size=10)
    model_names = [model_name.replace('!','') for model_name in
            model_names]
    print '...done'

    for i, arg in enumerate(model_paths):
        try:
            model = serial.load(arg)
        except:
            if arg.endswith('.yaml'):
                print >> sys.stderr, arg + " is a yaml config file," + \
                "you need to load a trained model."
                quit(-1)
            raise
        this_model_channels = model.monitor.channels

        if len(sys.argv) > 2:
            postfix = ":" + model_names[i]
        else:
            postfix = ""

        for channel in this_model_channels:
            channels[channel+postfix] = this_model_channels[channel]


    while True:
        # Make a list of short codes for each channel so user can specify them
        # easily
        tag_generator = _TagGenerator()
        codebook = {}
        sorted_codes = []
        for channel_name in sorted(channels,
                key = number_aware_alphabetical_key):
            code = tag_generator.get_tag()
            codebook[code] = channel_name
            codebook['<'+channel_name+'>'] = channel_name
            sorted_codes.append(code)

        x_axis = 'example'
        print 'set x_axis to example'

        if len(channels.values()) == 0:
            print "there are no channels to plot"
            break

        # If there is more than one channel in the monitor ask which ones to
        # plot
        prompt = len(channels.values()) > 1

        if prompt:

            # Display the codebook
            for code in sorted_codes:
                print code + '. ' + codebook[code]

            print

            print "Put e, b, s or h in the list somewhere to plot " + \
                    "epochs, batches, seconds, or hours, respectively."
            response = raw_input('Enter a list of channels to plot ' + \
                    '(example: A, C,F-G, h, <test_err>) or q to quit' + \
                    ' or o for options: ')

            if response == 'o':
                print '1: smooth all channels'
                print 'any other response: do nothing, go back to plotting'
                response = raw_input('Enter your choice: ')
                if response == '1':
                    for channel in channels.values():
                        k = 5
                        new_val_record = []
                        for i in xrange(len(channel.val_record)):
                            new_val = 0.
                            count = 0.
                            for j in xrange(max(0, i-k), i+1):
                                new_val += channel.val_record[j]
                                count += 1.
                            new_val_record.append(new_val / count)
                        channel.val_record = new_val_record
                continue

            if response == 'q':
                break

            #Remove spaces
            response = response.replace(' ','')

            #Split into list
            codes = response.split(',')

            final_codes = set([])

            for code in codes:
                if code == 'e':
                    x_axis = 'epoch'
                    continue
                elif code == 'b':
                    x_axis = 'batche'
                elif code == 's':
                    x_axis = 'second'
                elif code == 'h':
                    x_axis = 'hour'
                elif code.startswith('<'):
                    assert code.endswith('>')
                    final_codes.add(code)
                elif code.find('-') != -1:
                    #The current list element is a range of codes

                    rng = code.split('-')

                    if len(rng) != 2:
                        print "Input not understood: "+code
                        quit(-1)

                    found = False
                    for i in xrange(len(sorted_codes)):
                        if sorted_codes[i] == rng[0]:
                            found = True
                            break

                    if not found:
                        print "Invalid code: "+rng[0]
                        quit(-1)

                    found = False
                    for j in xrange(i,len(sorted_codes)):
                        if sorted_codes[j] == rng[1]:
                            found = True
                            break

                    if not found:
                        print "Invalid code: "+rng[1]
                        quit(-1)

                    final_codes = final_codes.union(set(sorted_codes[i:j+1]))
                else:
                    #The current list element is just a single code
                    final_codes = final_codes.union(set([code]))
            # end for code in codes
        else:
            final_codes ,= set(codebook.keys())

        colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']
        styles = list(colors)
        styles += [color+'--' for color in colors]
        styles += [color+':' for color in colors]

        fig = plt.figure()
        ax = plt.subplot(1,1,1)

        # plot the requested channels
        for idx, code in enumerate(sorted(final_codes)):

            channel_name= codebook[code]
            channel = channels[channel_name]

            y = np.asarray(channel.val_record)

            if np.any(np.isnan(y)):
                print channel_name + ' contains NaNs'

            if np.any(np.isinf(y)):
                print channel_name + 'contains infinite values'

            if x_axis == 'example':
                x = np.asarray(channel.example_record)
            elif x_axis == 'batche':
                x = np.asarray(channel.batch_record)
            elif x_axis == 'epoch':
                try:
                    x = np.asarray(channel.epoch_record)
                except AttributeError:
                    # older saved monitors won't have epoch_record
                    x = np.arange(len(channel.batch_record))
            elif x_axis == 'second':
                x = np.asarray(channel.time_record)
            elif x_axis == 'hour':
                x = np.asarray(channel.time_record) / 3600.
            else:
                assert False


            ax.plot( x,
                      y,
                      styles[idx % len(styles)],
                      marker = '.', # add point margers to lines
                      label = channel_name)

        plt.xlabel('# '+x_axis+'s')
        ax.ticklabel_format( scilimits = (-3,3), axis = 'both')

        handles, labels = ax.get_legend_handles_labels()
        lgd = ax.legend(handles, labels, loc='upper center',
                bbox_to_anchor=(0.5,-0.1))
        # 0.046 is the size of 1 legend box
        fig.subplots_adjust(bottom=0.11 + 0.046 * len(final_codes))

        if options.out is None:
          plt.show()
        else:
          plt.savefig(options.out)

        if not prompt:
            break

if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = print_channel_doc
#!/usr/bin/env python
"""
.. todo::

    WRITEME
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

import sys
from pylearn2.utils import serial

if __name__ == "__main__":
    for model_path in sys.argv[1:]:
        if len(sys.argv) > 2:
            print model_path
        model = serial.load(model_path)
        monitor = model.monitor
        channels = monitor.channels
        for key in sorted(channels.keys()):
            print key
            value = channels[key]
            if not hasattr(value, 'doc'):
                print "\tOld pkl file, written before doc system."
            else:
                doc = value.doc
                if doc is None:
                    print "No doc available."
                else:
                    print doc
            print

########NEW FILE########
__FILENAME__ = print_model
#!/usr/bin/env python
"""
Usage: print_model.py <pickle file containing a model>
Prints out a saved model.
"""
__author__ = "Ian Goodfellow"

import sys

from pylearn2.utils import serial

if __name__ == "__main__":
    _, model_path = sys.argv

    model = serial.load(model_path)

    print model

########NEW FILE########
__FILENAME__ = print_monitor
#!/usr/bin/env python
"""
.. todo::

    WRITEME
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"
import sys
from pylearn2.utils import serial
for model_path in sys.argv[1:]:
    if len(sys.argv) > 2:
        print model_path
    model = serial.load(model_path)
    monitor = model.monitor
    channels = monitor.channels
    if not hasattr(monitor, '_epochs_seen'):
        print 'old file, not all fields parsed correctly'
    else:
        print 'epochs seen: ',monitor._epochs_seen
    print 'time trained: ',max(channels[key].time_record[-1] for key in
            channels)
    for key in sorted(channels.keys()):
        print key, ':', channels[key].val_record[-1]

########NEW FILE########
__FILENAME__ = print_monitor_average
#!/usr/bin/env python

"""
Print average channel values for a collection of models, such as that
serialized by TrainCV. Based on print_monitor.py

usage: print_monitor_average.py model.pkl
"""
__author__ = "Steven Kearnes"

import sys
from pylearn2.utils import serial
import numpy as np


def main():
    """Print average channel final values for a collection of models."""
    epochs = []
    time = []
    values = {}
    for filename in sys.argv[1:]:
        models = serial.load(filename)
        for model in list(models):
            monitor = model.monitor
            channels = monitor.channels
            epochs.append(monitor._epochs_seen)
            time.append(max(channels[key].time_record[-1] for key in channels))
            for key in sorted(channels.keys()):
                if key not in values:
                    values[key] = []
                values[key].append(channels[key].val_record[-1])
    if len(epochs) > 1:
        print 'epochs seen: {} +/- {}'.format(np.mean(epochs), np.std(epochs))
        print 'training time: {} +/- {}'.format(np.mean(time), np.std(time))
        for key in sorted(values.keys()):
            print '{}: {} +/- {}'.format(key, np.mean(values[key]),
                                         np.std(values[key]))
    else:
        print 'epochs seen: {}'.format(epochs[0])
        print 'training time: {}'.format(time[0])
        for key in sorted(values.keys()):
            print '{}: {}'.format(key, values[key][0])

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = show_binocular_greyscale_examples
#!/usr/bin/env python
"""
.. todo::

    WRITEME
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"
import numpy as N
from pylearn2.gui import patch_viewer
from pylearn2.config import yaml_parse
from optparse import OptionParser


def main(options, positional_args):
    """
    .. todo::

        WRITEME
    """
    assert len(positional_args) == 1

    path ,= positional_args

    out = options.out
    rescale = options.rescale

    if rescale == 'none':
        global_rescale = False
        patch_rescale = False
    elif rescale == 'global':
        global_rescale = True
        patch_rescale = False
    elif rescale == 'individual':
        global_rescale = False
        patch_rescale = True
    else:
        assert False

    if path.endswith('.pkl'):
        from pylearn2.utils import serial
        obj = serial.load(path)
    elif path.endswith('.yaml'):
        print 'Building dataset from yaml...'
        obj =yaml_parse.load_path(path)
        print '...done'
    else:
        obj = yaml_parse.load(path)

    rows = options.rows
    cols = options.cols

    if hasattr(obj,'get_batch_topo'):
        #obj is a Dataset
        dataset = obj

        examples = dataset.get_batch_topo(rows*cols)
    else:
        #obj is a Model
        model = obj
        from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams
        theano_rng = RandomStreams(42)
        design_examples_var = model.random_design_matrix(batch_size = rows * cols, theano_rng = theano_rng)
        from theano import function
        print 'compiling sampling function'
        f = function([],design_examples_var)
        print 'sampling'
        design_examples = f()
        print 'loading dataset'
        dataset = yaml_parse.load(model.dataset_yaml_src)
        examples = dataset.get_topological_view(design_examples)

    norms = N.asarray( [
            N.sqrt(N.sum(N.square(examples[i,:])))
                        for i in xrange(examples.shape[0])
                        ])
    print 'norms of examples: '
    print '\tmin: ',norms.min()
    print '\tmean: ',norms.mean()
    print '\tmax: ',norms.max()

    print 'range of elements of examples',(examples.min(),examples.max())
    print 'dtype: ', examples.dtype

    examples = dataset.adjust_for_viewer(examples)

    if global_rescale:
        examples /= N.abs(examples).max()

    if len(examples.shape) != 4:
        print 'sorry, view_examples.py only supports image examples for now.'
        print 'this dataset has '+str(len(examples.shape)-2)+' topological dimensions'
        quit(-1)

    is_color = False
    assert examples.shape[3] == 2

    print examples.shape[1:3]

    pv = patch_viewer.PatchViewer( (rows, cols * 2), examples.shape[1:3], is_color = is_color)

    for i in xrange(rows*cols):
        # Load patches in backwards order for easier cross-eyed viewing
        # (Ian can't do the magic eye thing where you focus your eyes past the screen, must
        # focus eyes in front of screen)
        pv.add_patch(examples[i,:,:,1], activation = 0.0, rescale = patch_rescale)
        pv.add_patch(examples[i,:,:,0], activation = 0.0, rescale = patch_rescale)

    if out is None:
        pv.show()
    else:
        pv.save(out)

if __name__ == "__main__":
    parser = OptionParser()

    parser.add_option('--rows', dest='rows', default=20, action='store', type='int')
    parser.add_option('--cols', dest='cols', default=20, action='store', type='int')
    parser.add_option('--rescale', dest='rescale', default='global', action='store', type='string',
            help="how to rescale the patches for display: rescale|global|individual")
    parser.add_option('--out', dest='out', default=None, action='store',type='string', help='if not specified, displays an image. otherwise saves an image to the specified path')

    (options, positional_args) = parser.parse_args()
    main(options, positional_args)

########NEW FILE########
__FILENAME__ = show_examples
#!/usr/bin/env python
"""
.. todo::

    WRITEME
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"
import numpy as N
from pylearn2.gui import patch_viewer
from pylearn2.config import yaml_parse
from optparse import OptionParser


def main(options, positional_args):
    """
    .. todo::

        WRITEME
    """
    assert len(positional_args) == 1

    path ,= positional_args

    out = options.out
    rescale = options.rescale

    if rescale == 'none':
        global_rescale = False
        patch_rescale = False
    elif rescale == 'global':
        global_rescale = True
        patch_rescale = False
    elif rescale == 'individual':
        global_rescale = False
        patch_rescale = True
    else:
        assert False

    if path.endswith('.pkl'):
        from pylearn2.utils import serial
        obj = serial.load(path)
    elif path.endswith('.yaml'):
        print 'Building dataset from yaml...'
        obj =yaml_parse.load_path(path)
        print '...done'
    else:
        obj = yaml_parse.load(path)

    rows = options.rows
    cols = options.cols

    if hasattr(obj,'get_batch_topo'):
        # obj is a Dataset
        dataset = obj

        examples = dataset.get_batch_topo(rows*cols)
    else:
        # obj is a Model
        model = obj
        from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams
        theano_rng = RandomStreams(42)
        design_examples_var = model.random_design_matrix(batch_size =
                rows * cols, theano_rng = theano_rng)
        from theano import function
        print 'compiling sampling function'
        f = function([],design_examples_var)
        print 'sampling'
        design_examples = f()
        print 'loading dataset'
        dataset = yaml_parse.load(model.dataset_yaml_src)
        examples = dataset.get_topological_view(design_examples)

    norms = N.asarray( [
            N.sqrt(N.sum(N.square(examples[i,:])))
                        for i in xrange(examples.shape[0])
                        ])
    print 'norms of examples: '
    print '\tmin: ',norms.min()
    print '\tmean: ',norms.mean()
    print '\tmax: ',norms.max()

    print 'range of elements of examples', \
            (examples.min(),examples.max())
    print 'dtype: ', examples.dtype

    examples = dataset.adjust_for_viewer(examples)

    if global_rescale:
        examples /= N.abs(examples).max()

    if len(examples.shape) != 4:
        print 'sorry, view_examples.py only supports image examples' + \
                'for now.'
        print 'this dataset has ' + \
                str(len(examples.shape)-2)+' topological dimensions'
        quit(-1)

    if examples.shape[3] == 1:
        is_color = False
    elif examples.shape[3] == 3:
        is_color = True
    else:
        print 'got unknown image format with ' + str(examples.shape[3]) + \
                ' channels'
        print 'supported formats are 1 channel greyscale or three channel RGB'
        quit(-1)

    print examples.shape[1:3]

    pv = patch_viewer.PatchViewer((rows, cols), examples.shape[1:3],
            is_color = is_color)

    for i in xrange(rows*cols):
        pv.add_patch(examples[i,:,:,:], activation=0.0, rescale=patch_rescale)

    if out is None:
        pv.show()
    else:
        pv.save(out)


if __name__ == "__main__":
    parser = OptionParser()

    parser.add_option('--rows', dest='rows', default=20, action='store',
            type='int')
    parser.add_option('--cols', dest='cols', default=20, action='store',
            type='int')
    parser.add_option('--rescale', dest='rescale', default='global',
            action='store', type='string',
            help="how to rescale the patches for display: "
            "rescale|global|individual")
    parser.add_option('--out', dest='out', default=None, action='store',
            type='string', help='if not specified, displays an image. '
            'otherwise saves an image to the specified path')

    (options, positional_args) = parser.parse_args()
    main(options, positional_args)

########NEW FILE########
__FILENAME__ = show_weights
#!/usr/bin/env python
"""
.. todo::

    WRITEME
"""
#usage: show_weights.py model.pkl
from pylearn2.gui import get_weights_report
import argparse

def main():
    """
    .. todo::

        WRITEME
    """
    parser = argparse.ArgumentParser()

    parser.add_argument("--rescale", default="individual")
    parser.add_argument("--out", default=None)
    parser.add_argument("--border", action="store_true", default=False)
    parser.add_argument("path")

    options = parser.parse_args()

    pv = get_weights_report.get_weights_report(model_path=options.path,
                                               rescale=options.rescale,
                                               border=options.border)

    if options.out is None:
        pv.show()
    else:
        pv.save(options.out)

if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = summarize_model
#!/usr/bin/env python
"""
.. todo::

    WRITEME
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

import numpy as np
import sys

if __name__ == "__main__":
    path = sys.argv[1]
    from pylearn2.utils import serial
    model = serial.load(path)
    for param in model.get_params():
        name = param.name
        if name is None:
            name = '<anon>'
        v = param.get_value()
        print name+': '+str((v.min(),v.mean(),v.max()))+' '+str(v.shape)
        if np.sign(v.min()) != np.sign(v.max()):
            v = np.abs(v)
            print 'abs('+name+'): '+str((v.min(),v.mean(),v.max()))
        if v.ndim == 2:
            row_norms = np.sqrt(np.square(v).sum(axis=1))
            print name +" row norms: ",(row_norms.min(), row_norms.mean(), row_norms.max())
            col_norms = np.sqrt(np.square(v).sum(axis=0))
            print name +" col norms: ",(col_norms.min(), col_norms.mean(), col_norms.max())

    if hasattr(model,'monitor'):
        print 'trained on',model.monitor.get_examples_seen(),' examples'
        print 'which corresponds to',model.monitor.get_batches_seen(),'batches'
        try:
            print model.monitor.get_epochs_seen(),'epochs'
        except:
            pass
        if hasattr(model.monitor, 'training_succeeded'):
            if model.monitor.training_succeeded:
                print 'Training succeeded'
            else:
                print 'Training was not yet completed at the time of this save.'
        else:
            print 'This pickle file is damaged, or was made before the Monitor tracked whether training completed.'

########NEW FILE########
__FILENAME__ = test_autoencoder
"""
Tests for the pylearn2 autoencoder module.
"""
import numpy as np
import os
from pylearn2.scripts.tests.yaml_testing import limited_epoch_train
import pylearn2

def test_hcae_yaml():
    limited_epoch_train(os.path.join(pylearn2.__path__[0],"scripts/autoencoder_example/hcae.yaml"))

def test_dae_yaml():
    limited_epoch_train(os.path.join(pylearn2.__path__[0],"scripts/autoencoder_example/dae.yaml"))

########NEW FILE########
__FILENAME__ = yaml_testing
"""
Methods for testing YAML files
"""
from pylearn2.utils.serial import load_train_file
from pylearn2.termination_criteria import EpochCounter


def limited_epoch_train(file_path, max_epochs=1):
    """
    This method trains a given YAML file for a single epoch

    Parameters
    ----------
    file_path : str
        The path to the YAML file to be trained
    max_epochs : int
        The number of epochs to train this YAML file for.
        Defaults to 1.
    """
    train = load_train_file(file_path)
    train.algorithm.termination_criterion = EpochCounter(max_epochs=max_epochs)
    train.main_loop()

########NEW FILE########
__FILENAME__ = train
#!/usr/bin/env python
"""
Script implementing the logic for training pylearn2 models.

This is a "driver" that we recommend using for all but the most unusual
training experiments.

Basic usage:

.. code-block:: none

    train.py yaml_file.yaml

The YAML file should contain a pylearn2 YAML description of a
`pylearn2.train.Train` object (or optionally, a list of Train objects to
run sequentially).

See `doc/yaml_tutorial` for a description of how to write the YAML syntax.

The following environment variables will be locally defined and available
for use within the YAML file:

- `PYLEARN2_TRAIN_BASE_NAME`: the name of the file within the directory
  (`foo/bar.yaml` -> `bar.yaml`)
- `PYLEARN2_TRAIN_DIR`: the directory containing the YAML file
  (`foo/bar.yaml` -> `foo`)
- `PYLEARN2_TRAIN_FILE_FULL_STEM`: the filepath with the file extension
  stripped off.
  `foo/bar.yaml` -> `foo/bar`)
- `PYLEARN2_TRAIN_FILE_STEM`: the stem of `PYLEARN2_TRAIN_BASE_NAME`
  (`foo/bar.yaml` -> `bar`)
- `PYLEARN2_TRAIN_PHASE` : set to `phase0`, `phase1`, etc. during iteration
  through a list of Train objects. Not defined for a single train object.

These environment variables are especially useful for setting the save
path. For example, to make sure that `foo/bar.yaml` saves to `foo/bar.pkl`,
use

.. code-block:: none

    save_path: "${PYLEARN2_TRAIN_FILE_FULL_STEM}.pkl"

This way, if you copy `foo/bar.yaml` to `foo/bar2.yaml`, the output of
`foo/bar2.yaml` won't overwrite `foo/bar.pkl`, but will automatically save
to foo/bar2.pkl.

For example configuration files that are consumable by this script, see

- `pylearn2/scripts/tutorials/grbm_smd`
- `pylearn2/scripts/tutorials/dbm_demo`
- `pylearn2/scripts/papers/maxout`

Use `train.py -h` to see an auto-generated description of advanced options.
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow", "David Warde-Farley"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"
# Standard library imports
import argparse
import gc
import logging
import os

# Third-party imports
import numpy as np

# Local imports
from pylearn2.utils import serial
from pylearn2.utils.logger import (
    CustomStreamHandler, CustomFormatter, restore_defaults
)


class FeatureDump(object):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    encoder : WRITEME
    dataset : WRITEME
    path : WRITEME
    batch_size : WRITEME
    topo : WRITEME
    """
    def __init__(self, encoder, dataset, path, batch_size=None, topo=False):
        self.encoder = encoder
        self.dataset = dataset
        self.path = path
        self.batch_size = batch_size
        self.topo = topo

    def main_loop(self, **kwargs):
        """
        .. todo::

            WRITEME
        """
        if self.batch_size is None:
            if self.topo:
                data = self.dataset.get_topological_view()
            else:
                data = self.dataset.get_design_matrix()
            output = self.encoder.perform(data)
        else:
            myiterator = self.dataset.iterator(mode='sequential',
                                               batch_size=self.batch_size,
                                               topo=self.topo)
            chunks = []
            for data in myiterator:
                chunks.append(self.encoder.perform(data))
            output = np.concatenate(chunks)
        np.save(self.path, output)


def make_argument_parser():
    """
    Creates an ArgumentParser to read the options for this script from
    sys.argv
    """
    parser = argparse.ArgumentParser(
        description="Launch an experiment from a YAML configuration file.",
        epilog='\n'.join(__doc__.strip().split('\n')[1:]).strip(),
        formatter_class=argparse.RawTextHelpFormatter
    )
    parser.add_argument('--level-name', '-L',
                        action='store_true',
                        help='Display the log level (e.g. DEBUG, INFO) '
                             'for each logged message')
    parser.add_argument('--timestamp', '-T',
                        action='store_true',
                        help='Display human-readable timestamps for '
                             'each logged message')
    parser.add_argument('--time-budget', '-t', type=int,
                        help='Time budget in seconds. Stop training at '
                             'the end of an epoch if more than this '
                             'number of seconds has elapsed.')
    parser.add_argument('--verbose-logging', '-V',
                        action='store_true',
                        help='Display timestamp, log level and source '
                             'logger for every logged message '
                             '(implies -T).')
    parser.add_argument('--debug', '-D',
                        action='store_true',
                        help='Display any DEBUG-level log messages, '
                             'suppressed by default.')
    parser.add_argument('config', action='store',
                        choices=None,
                        help='A YAML configuration file specifying the '
                             'training procedure')
    return parser


if __name__ == "__main__":
    """See module-level docstring for a description of the script."""
    parser = make_argument_parser()
    args = parser.parse_args()
    train_obj = serial.load_train_file(args.config)
    try:
        iter(train_obj)
        iterable = True
    except TypeError as e:
        iterable = False

    # Undo our custom logging setup.
    restore_defaults()
    # Set up the root logger with a custom handler that logs stdout for INFO
    # and DEBUG and stderr for WARNING, ERROR, CRITICAL.
    root_logger = logging.getLogger()
    if args.verbose_logging:
        formatter = logging.Formatter(fmt="%(asctime)s %(name)s %(levelname)s "
                                          "%(message)s")
        handler = CustomStreamHandler(formatter=formatter)
    else:
        if args.timestamp:
            prefix = '%(asctime)s '
        else:
            prefix = ''
        formatter = CustomFormatter(prefix=prefix, only_from='pylearn2')
        handler = CustomStreamHandler(formatter=formatter)
    root_logger.addHandler(handler)
    # Set the root logger level.
    if args.debug:
        root_logger.setLevel(logging.DEBUG)
    else:
        root_logger.setLevel(logging.INFO)

    if iterable:
        for number, subobj in enumerate(iter(train_obj)):
            # Publish a variable indicating the training phase.
            phase_variable = 'PYLEARN2_TRAIN_PHASE'
            phase_value = 'phase%d' % (number + 1)
            os.environ[phase_variable] = phase_value

            # Execute this training phase.
            subobj.main_loop(time_budget=args.time_budget)

            # Clean up, in case there's a lot of memory used that's
            # necessary for the next phase.
            del subobj
            gc.collect()
    else:
        train_obj.main_loop(time_budget=args.time_budget)

########NEW FILE########
__FILENAME__ = test_convnet
"""
Test that a smaller version of convolutional_network.ipynb works.

The differences (needed for speed) are:
    * output_channels: 4 instead of 64
    * train.stop: 500 instead of 50000
    * valid.stop: 50100 instead of 60000
    * test.start: 0 instead of non-specified
    * test.stop: 100 instead of non-specified
    * termination_criterion.max_epochs: 1 instead of 500

This should make the test run in about one minute.
"""

import os
from pylearn2.testing import skip
from pylearn2.testing import no_debug_mode
from pylearn2.config import yaml_parse


def test_convolutional_network():

    skip.skip_if_no_data()
    yaml_file_path = os.path.abspath(os.path.join(os.path.dirname(__file__),
                                                  '..'))
    save_path = os.path.dirname(os.path.realpath(__file__))

    yaml = open("{0}/conv.yaml".format(yaml_file_path), 'r').read()
    hyper_params = {'train_stop': 50,
                    'valid_stop': 50050,
                    'test_stop': 50,
                    'batch_size': 50,
                    'output_channels_h2': 4,
                    'output_channels_h3': 4,
                    'max_epochs': 1,
                    'save_path': save_path}
    yaml = yaml % (hyper_params)
    train = yaml_parse.load(yaml)
    train.main_loop()

    try:
        os.remove("{}/convolutional_network_best.pkl".format(save_path))
    except:
        pass


########NEW FILE########
__FILENAME__ = train_dbm
"""
This module trains dbm_demo/rbm.yaml
"""

import os

from pylearn2.testing import skip
from pylearn2.testing import no_debug_mode
from pylearn2.config import yaml_parse


@no_debug_mode
def train_yaml(yaml_file):

    train = yaml_parse.load(yaml_file)
    train.main_loop()


def train(yaml_file_path, save_path):

    yaml = open("{0}/rbm.yaml".format(yaml_file_path), 'r').read()
    hyper_params = {'detector_layer_dim': 500,
                    'monitoring_batches': 10,
                    'train_stop': 50000,
                    'max_epochs': 300,
                    'save_path': save_path}

    yaml = yaml % (hyper_params)
    train_yaml(yaml)


def train_dbm():

    skip.skip_if_no_data()

    yaml_file_path = os.path.abspath(os.path.join(os.path.dirname(__file__),
                                                  '../dbm_demo'))
    save_path = os.path.dirname(os.path.realpath(__file__))

    train(yaml_file_path, save_path)

if __name__ == '__main__':
    train_dbm()

########NEW FILE########
__FILENAME__ = run_deep_trainer
#!/usr/bin/env python
__author__ = "Li Yao"
"""
See readme.txt

A small example of how to glue shining features of pylearn2 together
to train models layer by layer.
"""

MAX_EPOCHS_UNSUPERVISED = 1
MAX_EPOCHS_SUPERVISED = 2

from pylearn2.config import yaml_parse
from pylearn2.corruption import BinomialCorruptor
from pylearn2.corruption import GaussianCorruptor
from pylearn2.costs.mlp import Default
from pylearn2.models.autoencoder import Autoencoder, DenoisingAutoencoder
from pylearn2.models.rbm import GaussianBinaryRBM
from pylearn2.models.softmax_regression import SoftmaxRegression
from pylearn2.training_algorithms.sgd import SGD
from pylearn2.costs.autoencoder import MeanSquaredReconstructionError
from pylearn2.termination_criteria import EpochCounter
from pylearn2.datasets.dense_design_matrix import DenseDesignMatrix
from pylearn2.energy_functions.rbm_energy import GRBM_Type_1
from pylearn2.blocks import StackedBlocks
from pylearn2.datasets.transformer_dataset import TransformerDataset
from pylearn2.costs.ebm_estimation import SMD
from pylearn2.training_algorithms.sgd import MonitorBasedLRAdjuster
from pylearn2.train import Train
from optparse import OptionParser

import numpy


class ToyDataset(DenseDesignMatrix):
    def __init__(self):

        # simulated random dataset
        rng = numpy.random.RandomState(seed=42)
        data = rng.normal(size=(1000, 10))
        self.y = numpy.ones((1000, 2))
        positive = numpy.random.binomial(1, 0.5, [1000])
        self.y[:,0]=positive
        self.y[:,1]=1-positive
        super(ToyDataset, self).__init__(X=data, y=self.y)

def get_dataset_toy():
    """
    The toy dataset is only meant to used for testing pipelines.
    Do not try to visualize weights on it. It is not picture and
    has no color channel info to support visualization
    """
    trainset = ToyDataset()
    testset = ToyDataset()

    return trainset, testset

def get_dataset_cifar10():

    print 'loading CIFAR-10 dataset...'

    # We create the dataset by parsing YAML strings describing the dataset.
    # The yaml parser will automatically tag trainset and testset with a
    # yaml_src field containing the YAML string that was used to specify them.
    # This is useful because later the training algorithm can store this YAML
    # string in the saved model to efficiently describe exactly what data it
    # was trained on.
    template = \
"""!obj:pylearn2.datasets.cifar10.CIFAR10 {
which_set: %s,
center: 1,
rescale: 1,
one_hot: 1
}"""
    trainset = yaml_parse.load(template % "train")
    testset = yaml_parse.load(template % "test")

    print '...done loading CIFAR-10.'

    return trainset, testset

def get_dataset_mnist():

    print 'loading MNIST dataset...'

    # We create the dataset by parsing YAML strings describing the dataset.
    # The yaml parser will automatically tag trainset and testset with a
    # yaml_src field containing the YAML string that was used to specify them.
    # This is useful because later the training algorithm can store this YAML
    # string in the saved model to efficiently describe exactly what data it
    # was trained on.
    template = \
"""!obj:pylearn2.datasets.mnist.MNIST {
which_set: %s,
one_hot: 1
}"""
    trainset = yaml_parse.load(template % "train")
    testset = yaml_parse.load(template % "test")

    print '...done loading MNIST.'

    return trainset, testset

def get_autoencoder(structure):
    n_input, n_output = structure
    config = {
        'nhid': n_output,
        'nvis': n_input,
        'tied_weights': True,
        'act_enc': 'sigmoid',
        'act_dec': 'sigmoid',
        'irange': 0.001,
    }
    return Autoencoder(**config)

def get_denoising_autoencoder(structure):
    n_input, n_output = structure
    curruptor = BinomialCorruptor(corruption_level=0.5)
    config = {
        'corruptor': curruptor,
        'nhid': n_output,
        'nvis': n_input,
        'tied_weights': True,
        'act_enc': 'sigmoid',
        'act_dec': 'sigmoid',
        'irange': 0.001,
    }
    return DenoisingAutoencoder(**config)

def get_grbm(structure):
    n_input, n_output = structure
    config = {
        'nvis': n_input,
        'nhid': n_output,
        "irange" : 0.05,
        "energy_function_class" : GRBM_Type_1,
        "learn_sigma" : True,
        "init_sigma" : .4,
        "init_bias_hid" : -2.,
        "mean_vis" : False,
        "sigma_lr_scale" : 1e-3
        }

    return GaussianBinaryRBM(**config)

def get_logistic_regressor(structure):
    n_input, n_output = structure

    layer = SoftmaxRegression(n_classes=n_output, irange=0.02, nvis=n_input)

    return layer

def get_layer_trainer_logistic(layer, trainset):
    # configs on sgd

    config = {'learning_rate': 0.1,
              'cost' : Default(),
              'batch_size': 10,
              'monitoring_batches': 10,
              'monitoring_dataset': trainset,
              'termination_criterion': EpochCounter(max_epochs=MAX_EPOCHS_SUPERVISED),
              'update_callbacks': None
              }

    train_algo = SGD(**config)
    model = layer
    return Train(model = model,
            dataset = trainset,
            algorithm = train_algo,
            extensions = None)

def get_layer_trainer_sgd_autoencoder(layer, trainset):
    # configs on sgd
    train_algo = SGD(
            learning_rate = 0.1,
              cost =  MeanSquaredReconstructionError(),
              batch_size =  10,
              monitoring_batches = 10,
              monitoring_dataset =  trainset,
              termination_criterion = EpochCounter(max_epochs=MAX_EPOCHS_UNSUPERVISED),
              update_callbacks =  None
              )

    model = layer
    extensions = None
    return Train(model = model,
            algorithm = train_algo,
            extensions = extensions,
            dataset = trainset)

def get_layer_trainer_sgd_rbm(layer, trainset):
    train_algo = SGD(
        learning_rate = 1e-1,
        batch_size =  5,
        #"batches_per_iter" : 2000,
        monitoring_batches =  20,
        monitoring_dataset =  trainset,
        cost = SMD(corruptor=GaussianCorruptor(stdev=0.4)),
        termination_criterion =  EpochCounter(max_epochs=MAX_EPOCHS_UNSUPERVISED),
        )
    model = layer
    extensions = [MonitorBasedLRAdjuster()]
    return Train(model = model, algorithm = train_algo,
                 save_path='grbm.pkl',save_freq=1,
                 extensions = extensions, dataset = trainset)

def main(args=None):
    """
    args is the list of arguments that will be passed to the option parser.
    The default (None) means use sys.argv[1:].
    """
    parser = OptionParser()
    parser.add_option("-d", "--data", dest="dataset", default="toy",
                      help="specify the dataset, either cifar10, mnist or toy")
    (options,args) = parser.parse_args(args=args)

    if options.dataset == 'toy':
        trainset, testset = get_dataset_toy()
        n_output = 2
    elif options.dataset == 'cifar10':
        trainset, testset, = get_dataset_cifar10()
        n_output = 10

    elif options.dataset == 'mnist':
        trainset, testset, = get_dataset_mnist()
        n_output = 10

    else:
        NotImplementedError()

    design_matrix = trainset.get_design_matrix()
    n_input = design_matrix.shape[1]

    # build layers
    layers = []
    structure = [[n_input, 10], [10, 50], [50, 100], [100, n_output]]
    # layer 0: gaussianRBM
    layers.append(get_grbm(structure[0]))
    # layer 1: denoising AE
    layers.append(get_denoising_autoencoder(structure[1]))
    # layer 2: AE
    layers.append(get_autoencoder(structure[2]))
    # layer 3: logistic regression used in supervised training
    layers.append(get_logistic_regressor(structure[3]))


    # construct training sets for different layers
    trainset = [ trainset ,
                TransformerDataset( raw = trainset, transformer = layers[0] ),
                TransformerDataset( raw = trainset, transformer = StackedBlocks( layers[0:2] )),
                TransformerDataset( raw = trainset, transformer = StackedBlocks( layers[0:3] ))  ]

    # construct layer trainers
    layer_trainers = []
    layer_trainers.append(get_layer_trainer_sgd_rbm(layers[0], trainset[0]))
    layer_trainers.append(get_layer_trainer_sgd_autoencoder(layers[1], trainset[1]))
    layer_trainers.append(get_layer_trainer_sgd_autoencoder(layers[2], trainset[2]))
    layer_trainers.append(get_layer_trainer_logistic(layers[3], trainset[3]))

    # unsupervised pretraining
    for i, layer_trainer in enumerate(layer_trainers[0:3]):
        print '-----------------------------------'
        print ' Unsupervised training layer %d, %s'%(i, layers[i].__class__)
        print '-----------------------------------'
        layer_trainer.main_loop()


    print '\n'
    print '------------------------------------------------------'
    print ' Unsupervised training done! Start supervised training...'
    print '------------------------------------------------------'
    print '\n'

    # supervised training
    layer_trainers[-1].main_loop()


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = test_deep_trainer
"""
A simple unit test of 'run_deep_trainer.py'
"""
import run_deep_trainer

def test_deep_trainer():
    # pass args=[] so we can pass options to nosetests on the command line
    run_deep_trainer.main(args=[])

########NEW FILE########
__FILENAME__ = make_dataset
# pylearn2 tutorial example: make_dataset.py by Ian Goodfellow
# See README before reading this file
#
#
# This script creates a preprocessed version of a dataset using pylearn2.
# It's not necessary to save preprocessed versions of your dataset to
# disk but this is an instructive example, because later we can show
# how to load your custom dataset in a yaml file.
#
# This is also a common use case because often you will want to preprocess
# your data once and then train several models on the preprocessed data.

import os.path
import pylearn2

# We'll need the serial module to save the dataset
from pylearn2.utils import serial

# Our raw dataset will be the CIFAR10 image dataset
from pylearn2.datasets import cifar10

# We'll need the preprocessing module to preprocess the dataset
from pylearn2.datasets import preprocessing

if __name__ == "__main__":
    # Our raw training set is 32x32 color images
    # TODO: the one_hot=True is only necessary because one_hot=False is
    # broken, remove it after one_hot=False is fixed.
    train = cifar10.CIFAR10(which_set="train", one_hot=True)

    # We'd like to do several operations on them, so we'll set up a pipeline to
    # do so.
    pipeline = preprocessing.Pipeline()

    # First we want to pull out small patches of the images, since it's easier
    # to train an RBM on these
    pipeline.items.append(
        preprocessing.ExtractPatches(patch_shape=(8, 8), num_patches=150000)
    )

    # Next we contrast normalize the patches. The default arguments use the
    # same "regularization" parameters as those used in Adam Coates, Honglak
    # Lee, and Andrew Ng's paper "An Analysis of Single-Layer Networks in
    # Unsupervised Feature Learning"
    pipeline.items.append(preprocessing.GlobalContrastNormalization(sqrt_bias=10., use_std=True))

    # Finally we whiten the data using ZCA. Again, the default parameters to
    # ZCA are set to the same values as those used in the previously mentioned
    # paper.
    pipeline.items.append(preprocessing.ZCA())

    # Here we apply the preprocessing pipeline to the dataset. The can_fit
    # argument indicates that data-driven preprocessing steps (such as the ZCA
    # step in this example) are allowed to fit themselves to this dataset.
    # Later we might want to run the same pipeline on the test set with the
    # can_fit flag set to False, in order to make sure that the same whitening
    # matrix was used on both datasets.
    train.apply_preprocessor(preprocessor=pipeline, can_fit=True)

    # Finally we save the dataset to the filesystem. We instruct the dataset to
    # store its design matrix as a numpy file because this uses less memory
    # when re-loading (Pickle files, in general, use double their actual size
    # in the process of being re-loaded into a running process).
    # The dataset object itself is stored as a pickle file.
    path = pylearn2.__path__[0]
    train_example_path = os.path.join(path, 'scripts', 'tutorials', 'grbm_smd')
    train.use_design_loc(os.path.join(train_example_path, 'cifar10_preprocessed_train_design.npy'))

    train_pkl_path = os.path.join(train_example_path, 'cifar10_preprocessed_train.pkl')
    serial.save(train_pkl_path, train)

########NEW FILE########
__FILENAME__ = test_grbm_smd
import pylearn2
from pylearn2.utils.serial import load_train_file
import os
from pylearn2.testing import no_debug_mode
from theano import config

@no_debug_mode
def test_train_example():
    """ tests that the grbm_smd example script runs correctly """

    assert config.mode != "DEBUG_MODE"
    path = pylearn2.__path__[0]
    train_example_path = os.path.join(path, 'scripts', 'tutorials', 'grbm_smd')
    cwd = os.getcwd()
    try:
        os.chdir(train_example_path)
        train_yaml_path = os.path.join(train_example_path, 'cifar_grbm_smd.yaml')
        train_object = load_train_file(train_yaml_path)

        #make the termination criterion really lax so the test won't run for long
        train_object.algorithm.termination_criterion.prop_decrease = 0.5
        train_object.algorithm.termination_criterion.N = 1

        train_object.main_loop()
    finally:
        os.chdir(cwd)

if __name__ == '__main__':
    test_train_example()

########NEW FILE########
__FILENAME__ = utils
import numpy
from jobman import tools
from jobman.tools import DD


def log_uniform(low, high):
    """
    Generates a number that's uniformly distributed in the log-space between
    `low` and `high`

    Parameters
    ----------
    low : float
        Lower bound of the randomly generated number
    high : float
        Upper bound of the randomly generated number

    Returns
    -------
    rval : float
        Random number uniformly distributed in the log-space specified by `low`
        and `high`
    """
    log_low = numpy.log(low)
    log_high = numpy.log(high)

    log_rval = numpy.random.uniform(log_low, log_high)
    rval = float(numpy.exp(log_rval))

    return rval


def results_extractor(train_obj):
    channels = train_obj.model.monitor.channels
    train_y_misclass = channels['y_misclass'].val_record[-1]
    train_y_nll = channels['y_nll'].val_record[-1]

    return DD(train_y_misclass=train_y_misclass,
              train_y_nll=train_y_nll)


def parse_results(cwd):
    optimal_dd = None
    optimal_measure = numpy.inf

    for tup in tools.find_conf_files(cwd):
        dd = tup[1]
        if 'results.train_y_misclass' in dd:
            if dd['results.train_y_misclass'] < optimal_measure:
                optimal_measure = dd['results.train_y_misclass']
                optimal_dd = dd

    print "Optimal " + "results.train_y_misclass" + ": " + str(optimal_measure)
    for key, value in optimal_dd.items():
        if 'hyper_parameters' in key:
            print key + ": " + str(value)

########NEW FILE########
__FILENAME__ = test_mlp
"""
Test for multilayer_perceptron.ipynb
"""

import os

import pylearn2
from pylearn2.termination_criteria import EpochCounter
from pylearn2.testing.skip import skip_if_no_data
from pylearn2.config import yaml_parse


YAML_FILE_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__),
                                              '..'))
SAVE_PATH = os.path.dirname(os.path.realpath(__file__))


def cleaunup(file_name):
    try:
        os.remove(os.path.join(SAVE_PATH, file_name))
    except:
        pass


def test_part_2():
    skip_if_no_data()
    with open(os.path.join(YAML_FILE_PATH,
              'mlp_tutorial_part_2.yaml'), 'r') as f:
        train = f.read()
    f.close()
    hyper_params = {'train_stop': 50,
                    'valid_stop': 50050,
                    'dim_h0': 5,
                    'max_epochs': 1,
                    'save_path': SAVE_PATH}
    train = train % (hyper_params)
    train = yaml_parse.load(train)
    train.main_loop()
    cleaunup("mlp_best.pkl")


def test_part_3():
    skip_if_no_data()
    with open(os.path.join(YAML_FILE_PATH,
              'mlp_tutorial_part_3.yaml'), 'r') as f:
        train_2 = f.read()
    f.close()
    hyper_params = {'train_stop': 50,
                    'valid_stop': 50050,
                    'dim_h0': 5,
                    'dim_h1': 10,
                    'sparse_init_h1': 2,
                    'max_epochs': 1,
                    'save_path': SAVE_PATH}
    train_2 = train_2 % (hyper_params)
    train_2 = yaml_parse.load(train_2)
    train_2.main_loop()
    cleaunup("mlp_2_best.pkl")


def test_part_4():
    skip_if_no_data()
    with open(os.path.join(YAML_FILE_PATH,
              'mlp_tutorial_part_4.yaml'), 'r') as f:
        train_3 = f.read()
    f.close()
    hyper_params = {'train_stop': 50,
                    'valid_stop': 50050,
                    'dim_h0': 5,
                    'dim_h1': 10,
                    'sparse_init_h1': 2,
                    'max_epochs': 1,
                    'save_path': SAVE_PATH}
    train_3 = train_3 % (hyper_params)
    train_3 = yaml_parse.load(train_3)
    train_3.main_loop()
    cleaunup("mlp_3_best.pkl")

########NEW FILE########
__FILENAME__ = test_softmaxreg
"""
Test for softmax_regression.ipynb
"""

import os

from pylearn2.testing.skip import skip_if_no_data
from pylearn2.config import yaml_parse


def test():
    skip_if_no_data()

    dirname = os.path.join(os.path.abspath(os.path.dirname(__file__)), '..')

    with open(os.path.join(dirname, 'sr_dataset.yaml'), 'r') as f:
        dataset = f.read()

    hyper_params = {'train_stop': 50}
    dataset = dataset % (hyper_params)

    with open(os.path.join(dirname, 'sr_model.yaml'), 'r') as f:
        model = f.read()

    with open(os.path.join(dirname, 'sr_algorithm.yaml'), 'r') as f:
        algorithm = f.read()

    hyper_params = {'batch_size': 10,
                    'valid_stop': 50050}
    algorithm = algorithm % (hyper_params)

    with open(os.path.join(dirname, 'sr_train.yaml'), 'r') as f:
        train = f.read()

    save_path = os.path.dirname(os.path.realpath(__file__))
    train = train % locals()

    train = yaml_parse.load(train)
    train.main_loop()

    try:
        os.remove("{}/softmax_regression.pkl".format(save_path))
        os.remove("{}/softmax_regression_best.pkl".format(save_path))
    except:
        pass

if __name__ == '__main__':
    test()

########NEW FILE########
__FILENAME__ = test_dae
"""
This module tests stacked_autoencoders.ipynb
"""

import os

from pylearn2.testing import skip
from pylearn2.testing import no_debug_mode
from pylearn2.config import yaml_parse


@no_debug_mode
def train_yaml(yaml_file):

    train = yaml_parse.load(yaml_file)
    train.main_loop()


def train_layer1(yaml_file_path, save_path):

    yaml = open("{0}/dae_l1.yaml".format(yaml_file_path), 'r').read()
    hyper_params = {'train_stop': 50,
                    'batch_size': 50,
                    'monitoring_batches': 1,
                    'nhid': 10,
                    'max_epochs': 1,
                    'save_path': save_path}
    yaml = yaml % (hyper_params)
    train_yaml(yaml)


def train_layer2(yaml_file_path, save_path):

    yaml = open("{0}/dae_l2.yaml".format(yaml_file_path), 'r').read()
    hyper_params = {'train_stop': 50,
                    'batch_size': 50,
                    'monitoring_batches': 1,
                    'nvis': 10,
                    'nhid': 10,
                    'max_epochs': 1,
                    'save_path': save_path}
    yaml = yaml % (hyper_params)
    train_yaml(yaml)


def train_mlp(yaml_file_path, save_path):

    yaml = open("{0}/dae_mlp.yaml".format(yaml_file_path), 'r').read()
    hyper_params = {'train_stop': 50,
                    'valid_stop': 50050,
                    'batch_size': 50,
                    'max_epochs': 1,
                    'save_path': save_path}
    yaml = yaml % (hyper_params)
    train_yaml(yaml)


def test_sda():

    skip.skip_if_no_data()

    yaml_file_path = os.path.abspath(os.path.join(os.path.dirname(__file__),
                                                  '..'))
    save_path = os.path.dirname(os.path.realpath(__file__))

    train_layer1(yaml_file_path, save_path)
    train_layer2(yaml_file_path, save_path)
    train_mlp(yaml_file_path, save_path)

    try:
        os.remove("{}/dae_l1.pkl".format(save_path))
        os.remove("{}/dae_l2.pkl".format(save_path))
    except:
        pass

if __name__ == '__main__':
    test_sda()

########NEW FILE########
__FILENAME__ = test_dbm
"""
This module tests dbm_demo/rbm.yaml
"""

import os

from pylearn2.testing import skip
from pylearn2.testing import no_debug_mode
from pylearn2.config import yaml_parse


@no_debug_mode
def train_yaml(yaml_file):

    train = yaml_parse.load(yaml_file)
    train.main_loop()


def train(yaml_file_path, save_path):

    yaml = open("{0}/rbm.yaml".format(yaml_file_path), 'r').read()
    hyper_params = {'detector_layer_dim': 5,
                    'monitoring_batches': 2,
                    'train_stop': 500,
                    'max_epochs': 7,
                    'save_path': save_path}

    yaml = yaml % (hyper_params)
    train_yaml(yaml)


def test_dbm():

    skip.skip_if_no_data()

    yaml_file_path = os.path.abspath(os.path.join(os.path.dirname(__file__),
                                                  '../dbm_demo'))
    save_path = os.path.dirname(os.path.realpath(__file__))

    train(yaml_file_path, save_path)

    try:
        os.remove("{}/dbm.pkl".format(save_path))
    except:
        pass

if __name__ == '__main__':
    test_dbm()

########NEW FILE########
__FILENAME__ = test_mlp_nested
"""
Test for multilayer_perceptron.ipynb
"""

import os

import pylearn2
from pylearn2.termination_criteria import EpochCounter
from pylearn2.testing.skip import skip_if_no_data
from pylearn2.config import yaml_parse


def test_nested():
    skip_if_no_data()
    with open(os.path.join(pylearn2.__path__[0], 'scripts', 'tutorials',
              'mlp_nested.yaml'), 'r') as f:
        train_3 = f.read()
    f.close()
    hyper_params = {'train_stop': 50,
                    'valid_stop': 50050,
                    'dim_h0': 5,
                    'dim_h1': 20,
                    'dim_h2': 30,
                    'dim_h3': 40,
                    'sparse_init_h1': 2,
                    'max_epochs': 1}
    train_3 = train_3 % (hyper_params)
    print train_3
    train_3 = yaml_parse.load(train_3)
    train_3.main_loop()

if __name__ == '__main__':
    test_nested()

########NEW FILE########
__FILENAME__ = test_space
"""
Tests for space utilities.
"""
import itertools
import warnings

import numpy as np
import theano
from theano import tensor

# Can't use nose.tools.assert_raises, only introduced in python 2.7. Use
# numpy.testing.assert_raises instead
from pylearn2.space import (SimplyTypedSpace,
                            VectorSpace,
                            Conv2DSpace,
                            CompositeSpace,
                            VectorSequenceSpace,
                            IndexSequenceSpace,
                            IndexSpace,
                            NullSpace,
                            is_symbolic_batch)
from pylearn2.utils import function, safe_zip


def test_np_format_as_vector2vector():
    vector_space_initial = VectorSpace(dim=8*8*3, sparse=False)
    vector_space_final = VectorSpace(dim=8*8*3, sparse=False)
    data = np.arange(5*8*8*3).reshape(5, 8*8*3)
    rval = vector_space_initial.np_format_as(data, vector_space_final)
    assert np.all(rval == data)


def test_np_format_as_index2index():
    index_space_initial = IndexSpace(max_labels=10, dim=1)

    index_space_final = IndexSpace(max_labels=10, dim=1)
    data = np.array([[0], [2], [1], [3], [5], [8], [1]])
    rval = index_space_initial.np_format_as(data, index_space_final)
    assert index_space_initial == index_space_final
    assert np.all(rval == data)

    index_space_downcast = IndexSpace(max_labels=10, dim=1, dtype='int32')
    rval = index_space_initial.np_format_as(data, index_space_downcast)
    assert index_space_initial != index_space_downcast
    assert np.all(rval == data)
    assert rval.dtype == 'int32' and data.dtype == 'int64'


def test_np_format_as_conv2d2conv2d():
    conv2d_space_initial = Conv2DSpace(shape=(8, 8), num_channels=3,
                                       axes=('b', 'c', 0, 1))
    conv2d_space_final = Conv2DSpace(shape=(8, 8), num_channels=3,
                                     axes=('b', 'c', 0, 1))
    data = np.arange(5*8*8*3).reshape(5, 3, 8, 8)
    rval = conv2d_space_initial.np_format_as(data, conv2d_space_final)
    assert np.all(rval == data)

    conv2d_space1 = Conv2DSpace(shape=(8, 8), num_channels=3,
                                axes=('c', 'b', 1, 0))
    conv2d_space0 = Conv2DSpace(shape=(8, 8), num_channels=3,
                                axes=('b', 'c', 0, 1))
    data = np.arange(5*8*8*3).reshape(5, 3, 8, 8)
    rval = conv2d_space0.np_format_as(data, conv2d_space1)
    nval = data.transpose(1, 0, 3, 2)
    assert np.all(rval == nval)


def test_np_format_as_vector2conv2d():
    vector_space = VectorSpace(dim=8*8*3, sparse=False)
    conv2d_space = Conv2DSpace(shape=(8, 8), num_channels=3,
                               axes=('b', 'c', 0, 1))
    data = np.arange(5*8*8*3).reshape(5, 8*8*3)
    rval = vector_space.np_format_as(data, conv2d_space)

    # Get data in a Conv2DSpace with default axes
    new_axes = conv2d_space.default_axes
    axis_to_shape = {'b': 5, 'c': 3, 0: 8, 1: 8}
    new_shape = tuple([axis_to_shape[ax] for ax in new_axes])
    nval = data.reshape(new_shape)
    # Then transpose
    nval = nval.transpose(*[new_axes.index(ax)
                            for ax in conv2d_space.axes])
    assert np.all(rval == nval)


def test_np_format_as_conv2d2vector():
    vector_space = VectorSpace(dim=8*8*3, sparse=False)
    conv2d_space = Conv2DSpace(shape=(8, 8), num_channels=3,
                               axes=('b', 'c', 0, 1))
    data = np.arange(5*8*8*3).reshape(5, 3, 8, 8)
    rval = conv2d_space.np_format_as(data, vector_space)
    nval = data.transpose(*[conv2d_space.axes.index(ax)
                            for ax in conv2d_space.default_axes])
    nval = nval.reshape(5, 3 * 8 * 8)
    assert np.all(rval == nval)

    vector_space = VectorSpace(dim=8*8*3, sparse=False)
    conv2d_space = Conv2DSpace(shape=(8, 8), num_channels=3,
                               axes=('c', 'b', 0, 1))
    data = np.arange(5*8*8*3).reshape(3, 5, 8, 8)
    rval = conv2d_space.np_format_as(data, vector_space)
    nval = data.transpose(*[conv2d_space.axes.index(ax)
                            for ax in conv2d_space.default_axes])
    nval = nval.reshape(5, 3 * 8 * 8)
    assert np.all(rval == nval)


def test_np_format_as_conv2d_vector_conv2d():
    conv2d_space1 = Conv2DSpace(shape=(8, 8), num_channels=3,
                                axes=('c', 'b', 1, 0))
    vector_space = VectorSpace(dim=8*8*3, sparse=False)
    conv2d_space0 = Conv2DSpace(shape=(8, 8), num_channels=3,
                                axes=('b', 'c', 0, 1))
    data = np.arange(5*8*8*3).reshape(5, 3, 8, 8)

    vecval = conv2d_space0.np_format_as(data, vector_space)
    rval1 = vector_space.np_format_as(vecval, conv2d_space1)
    rval2 = conv2d_space0.np_format_as(data, conv2d_space1)
    assert np.allclose(rval1, rval2)

    nval = data.transpose(1, 0, 3, 2)
    assert np.allclose(nval, rval1)


def test_np_format_as_vectorsequence2vectorsequence():
    vector_sequence_space1 = VectorSequenceSpace(dim=3, dtype='float32')
    vector_sequence_space2 = VectorSequenceSpace(dim=3, dtype='float64')

    data = np.random.uniform(low=0.0, high=1.0, size=(10, 3))
    rval = vector_sequence_space1.np_format_as(data, vector_sequence_space2)

    assert np.all(rval == data)


def test_np_format_as_indexsequence2indexsequence():
    index_sequence_space1 = IndexSequenceSpace(max_labels=6, dim=1,
                                               dtype='int16')
    index_sequence_space2 = IndexSequenceSpace(max_labels=6, dim=1,
                                               dtype='int32')

    data = np.random.randint(low=0, high=5, size=(10, 1))
    rval = index_sequence_space1.np_format_as(data, index_sequence_space2)

    assert np.all(rval == data)


def test_np_format_as_indexsequence2vectorsequence():
    index_sequence_space = IndexSequenceSpace(max_labels=6, dim=1)
    vector_sequence_space = VectorSequenceSpace(dim=6)

    data = np.array([[0], [1], [4], [3]])
    rval = index_sequence_space.np_format_as(data, vector_sequence_space)
    true_val = np.array([[1, 0, 0, 0, 0, 0],
                         [0, 1, 0, 0, 0, 0],
                         [0, 0, 0, 0, 1, 0],
                         [0, 0, 0, 1, 0, 0]])

    assert np.all(rval == true_val)


def test_np_format_as_sequence2other():
    vector_sequence_space = VectorSequenceSpace(dim=3)
    vector_space = VectorSpace(dim=3)

    data = np.random.uniform(low=0.0, high=1.0, size=(10, 3))
    np.testing.assert_raises(ValueError, vector_sequence_space.np_format_as,
                             data, vector_space)

    index_sequence_space = IndexSequenceSpace(max_labels=6, dim=1)
    index_space = IndexSpace(max_labels=6, dim=1)

    data = np.random.randint(low=0, high=5, size=(10, 1))
    np.testing.assert_raises(ValueError, index_sequence_space.np_format_as,
                             data, index_space)


def test_np_format_as_composite_composite():
    """
    Test using CompositeSpace.np_format_as() to convert between
    composite spaces that have the same tree structure, but different
    leaf spaces.
    """

    def make_composite_space(image_space):
        """
        Returns a compsite space with a particular tree structure.
        """
        return CompositeSpace((CompositeSpace((image_space,)*2),
                               VectorSpace(dim=1)))

    shape = np.array([8, 11])
    channels = 3
    datum_size = channels * shape.prod()

    composite_topo = make_composite_space(Conv2DSpace(shape=shape,
                                                      num_channels=channels))
    composite_flat = make_composite_space(VectorSpace(dim=datum_size))

    def make_vector_data(batch_size, space):
        """
        Returns a batch of synthetic data appropriate to the provided space.
        Supports VectorSpaces, and CompositeSpaces of VectorSpaces.  synthetic
        data.

        """
        if isinstance(space, CompositeSpace):
            return tuple(make_vector_data(batch_size, subspace)
                         for subspace in space.components)
        else:
            assert isinstance(space, VectorSpace)
            result = np.random.rand(batch_size, space.dim)
            if space.dtype is not None:
                return np.asarray(result, dtype=space.dtype)
            else:
                return result

    batch_size = 5
    flat_data = make_vector_data(batch_size, composite_flat)
    composite_flat.np_validate(flat_data)

    topo_data = composite_flat.np_format_as(flat_data, composite_topo)
    composite_topo.np_validate(topo_data)
    new_flat_data = composite_topo.np_format_as(topo_data,
                                                composite_flat)

    def get_shape(batch):
        """
        Returns the (nested) shape(s) of a (nested) batch.
        """
        if isinstance(batch, np.ndarray):
            return batch.shape
        else:
            return tuple(get_shape(b) for b in batch)

    def batch_equals(batch_0, batch_1):
        """
        Returns true if all corresponding elements of two batches are
        equal.  Supports composite data (i.e. nested tuples of data).
        """
        assert type(batch_0) == type(batch_1)
        if isinstance(batch_0, tuple):
            if len(batch_0) != len(batch_1):
                return False

            return np.all(tuple(batch_equals(b0, b1)
                                for b0, b1 in zip(batch_0, batch_1)))
        else:
            assert isinstance(batch_0, np.ndarray)
            return np.all(batch_0 == batch_1)

    assert batch_equals(new_flat_data, flat_data)


def test_vector_to_conv_c01b_invertible():

    """
    Tests that the format_as methods between Conv2DSpace
    and VectorSpace are invertible for the ('c', 0, 1, 'b')
    axis format.
    """

    rng = np.random.RandomState([2013, 5, 1])

    batch_size = 3
    rows = 4
    cols = 5
    channels = 2

    conv = Conv2DSpace([rows, cols],
                       channels=channels,
                       axes=('c', 0, 1, 'b'))
    vec = VectorSpace(conv.get_total_dimension())

    X = conv.make_batch_theano()
    Y = conv.format_as(X, vec)
    Z = vec.format_as(Y, conv)

    A = vec.make_batch_theano()
    B = vec.format_as(A, conv)
    C = conv.format_as(B, vec)

    f = function([X, A], [Z, C])

    X = rng.randn(*(conv.get_origin_batch(batch_size).shape)).astype(X.dtype)
    A = rng.randn(*(vec.get_origin_batch(batch_size).shape)).astype(A.dtype)

    Z, C = f(X, A)

    np.testing.assert_allclose(Z, X)
    np.testing.assert_allclose(C, A)


def test_broadcastable():
    v = VectorSpace(5).make_theano_batch(batch_size=1)
    np.testing.assert_(v.broadcastable[0])
    c = Conv2DSpace((5, 5), channels=3,
                    axes=['c', 0, 1, 'b']).make_theano_batch(batch_size=1)
    np.testing.assert_(c.broadcastable[-1])
    d = Conv2DSpace((5, 5), channels=3,
                    axes=['b', 0, 1, 'c']).make_theano_batch(batch_size=1)
    np.testing.assert_(d.broadcastable[0])


def test_compare_index():
    dims = [5, 5, 5, 6]
    max_labels = [10, 10, 9, 10]
    index_spaces = [IndexSpace(dim=dim, max_labels=max_label)
                    for dim, max_label in zip(dims, max_labels)]
    assert index_spaces[0] == index_spaces[1]
    assert not any(index_spaces[i] == index_spaces[j]
                   for i, j in itertools.combinations([1, 2, 3], 2))
    vector_space = VectorSpace(dim=5)
    conv2d_space = Conv2DSpace(shape=(8, 8), num_channels=3,
                               axes=('b', 'c', 0, 1))
    composite_space = CompositeSpace((index_spaces[0],))
    assert not any(index_space == vector_space for index_space in index_spaces)
    assert not any(index_space == composite_space
                   for index_space in index_spaces)
    assert not any(index_space == conv2d_space for index_space in index_spaces)


def test_np_format_as_index2vector():
    # Test 5 random batches for shape, number of non-zeros
    for _ in xrange(5):
        max_labels = np.random.randint(2, 10)
        batch_size = np.random.randint(1, 10)
        labels = np.random.randint(1, 10)
        batch = np.random.random_integers(max_labels - 1,
                                          size=(batch_size, labels))
        index_space = IndexSpace(dim=labels, max_labels=max_labels)
        vector_space_merge = VectorSpace(dim=max_labels)
        vector_space_concatenate = VectorSpace(dim=max_labels * labels)
        merged = index_space.np_format_as(batch, vector_space_merge)
        concatenated = index_space.np_format_as(batch,
                                                vector_space_concatenate)
        assert merged.shape == (batch_size, max_labels)
        assert concatenated.shape == (batch_size, max_labels * labels)
        assert np.count_nonzero(merged) <= batch.size
        assert np.count_nonzero(concatenated) == batch.size
        assert np.all(np.unique(concatenated) == np.array([0, 1]))
    # Make sure Theano variables give the same result
    batch = tensor.lmatrix('batch')
    single = tensor.lvector('single')
    batch_size = np.random.randint(1, 10)
    np_batch = np.random.random_integers(max_labels - 1,
                                         size=(batch_size, labels))
    np_single = np.random.random_integers(max_labels - 1,
                                          size=(labels))
    f_batch_merge = theano.function(
        [batch], index_space._format_as_impl(False, batch, vector_space_merge)
    )
    f_batch_concatenate = theano.function(
        [batch], index_space._format_as_impl(False, batch,
                                             vector_space_concatenate)
    )
    f_single_merge = theano.function(
        [single], index_space._format_as_impl(False, single,
                                              vector_space_merge)
    )
    f_single_concatenate = theano.function(
        [single], index_space._format_as_impl(False, single,
                                              vector_space_concatenate)
    )
    np.testing.assert_allclose(
        f_batch_merge(np_batch),
        index_space._format_as_impl(True, np_batch, vector_space_merge)
    )
    np.testing.assert_allclose(
        f_batch_concatenate(np_batch),
        index_space._format_as_impl(True, np_batch, vector_space_concatenate)
    )
    np.testing.assert_allclose(
        f_single_merge(np_single),
        index_space._format_as_impl(True, np_single, vector_space_merge)
    )
    np.testing.assert_allclose(
        f_single_concatenate(np_single),
        index_space._format_as_impl(True, np_single, vector_space_concatenate)
    )


def test_dtypes():

    batch_size = 2
    dtype_is_none_msg = ("self.dtype is None, so you must provide a "
                         "non-None dtype argument to this method.")

    all_scalar_dtypes = tuple(t.dtype
                              for t in theano.scalar.all_types)

    def underspecifies_dtypes(from_space, to_dtype):
        """
        Returns True iff the from_space and to_dtype are both None. If
        from_space is a CompositeSpace, this recurses into its tree of
        subspaces.
        """
        if isinstance(from_space, CompositeSpace):
            if not isinstance(to_dtype, tuple):
                return any(underspecifies_dtypes(s, to_dtype)
                           for s in from_space.components)
            else:
                return any(underspecifies_dtypes(s, d)
                           for s, d
                           in safe_zip(from_space.components, to_dtype))
        else:
            assert not isinstance(to_dtype, tuple), ("Tree structure "
                                                     "mismatch between "
                                                     "from_space and "
                                                     "to_dtype.")
            return from_space.dtype is None and to_dtype is None

    def get_expected_batch_dtype(from_space, to_dtype):
        """
        Returns the expected dtype of a batch returned from
        from_space.f(batch, to_dtype), where f is one of the three batch
        creation methods (get_origin_batch, make_theano_batch, and
        make_shared_batch)
        """
        if to_dtype == 'floatX':
            to_dtype = theano.config.floatX

        if isinstance(from_space, CompositeSpace):
            if not isinstance(to_dtype, tuple):
                to_dtype = (to_dtype, ) * len(from_space.components)

            return tuple(get_expected_batch_dtype(subspace, subtype)
                         for subspace, subtype
                         in safe_zip(from_space.components, to_dtype))
        else:
            assert not (from_space.dtype is None and to_dtype is None)
            return from_space.dtype if to_dtype is None else to_dtype

    def get_batch_dtype(batch):
        """
        Returns the dtype of a batch, as a string, or nested tuple of strings.
        For simple batches such as ndarray, this returns str(batch.dtype).
        For the None batches "used" by NullSpace, this returns a special string
        "NullSpace dtype".
        For composite batches, this returns (nested) tuples of dtypes.
        """

        if isinstance(batch, tuple):
            return tuple(get_batch_dtype(b) for b in batch)
        elif batch is None:
            return "NullSpace dtype"
        else:
            return batch.dtype

    def test_get_origin_batch(from_space, to_type):

        # Expect failure if neither we nor the from_space specifies a dtype
        if underspecifies_dtypes(from_space, to_type):
            try:
                from_space.get_origin_batch(batch_size, dtype=to_type)
            except TypeError, ex:
                assert dtype_is_none_msg in str(ex)
            except Exception, unexpected_ex:
                print ("Expected an exception of type TypeError with message "
                       "%s, got a %s instead with message %s." %
                       (dtype_is_none_msg,
                        type(unexpected_ex),
                        str(unexpected_ex)))
                raise unexpected_ex
            finally:
                return

        batch = from_space.get_origin_batch(batch_size, dtype=to_type)
        assert get_batch_dtype(batch) == get_expected_batch_dtype(from_space,
                                                                  to_type)

    def test_make_shared_batch(from_space, to_type):

        if underspecifies_dtypes(from_space, to_type):
            try:
                from_space.make_shared_batch(batch_size, dtype=to_type)
            except TypeError, ex:
                assert dtype_is_none_msg in str(ex)
            except Exception, unexpected_ex:
                print ("Expected an exception of type TypeError with message "
                       "%s, got a %s instead with message %s." %
                       (dtype_is_none_msg,
                        type(unexpected_ex),
                        str(unexpected_ex)))
                raise unexpected_ex
            finally:
                return

        batch = from_space.make_shared_batch(batch_size=batch_size,
                                             name='batch',
                                             dtype=to_type)

        assert (get_batch_dtype(batch) ==
                get_expected_batch_dtype(from_space, to_type)), \
               ("\nget_batch_dtype(batch): %s\n"
                "get_expected_batch_dtype(from_space, to_type): %s" %
                (get_batch_dtype(batch),
                 get_expected_batch_dtype(from_space, to_type)))

    def test_make_theano_batch(from_space, to_type):
        kwargs = {'name': 'batch',
                  'dtype': to_type}

        # Sparse VectorSpaces throw an exception if batch_size is specified.
        if not (isinstance(from_space, VectorSpace) and from_space.sparse):
            kwargs['batch_size'] = batch_size

        if underspecifies_dtypes(from_space, to_type):
            try:
                from_space.make_theano_batch(**kwargs)
            except TypeError, ex:
                assert dtype_is_none_msg in str(ex)
            except Exception, unexpected_ex:
                print ("Expected an exception of type TypeError with message "
                       "%s, got a %s instead with message %s." %
                       (dtype_is_none_msg,
                        type(unexpected_ex),
                        str(unexpected_ex)))
                raise unexpected_ex
            finally:
                return

        batch = from_space.make_theano_batch(**kwargs)
        assert get_batch_dtype(batch) == get_expected_batch_dtype(from_space,
                                                                  to_type)

    def test_format(from_space, to_space, using_numeric_batch):
        """
        Unit test for a call to from_space.np_format_as(batch, to_space)
        """

        # Type-checks the arguments
        for space, name in zip((from_space, to_space),
                               ("from_space", "to_space")):
            if not isinstance(space,
                              (VectorSpace, Conv2DSpace, CompositeSpace)):
                raise TypeError("This test only supports spaces of type "
                                "VectorSpace, Conv2DSpace, and "
                                "CompositeSpace, not %s's type %s" %
                                (name, type(space)))

        def get_batch(space, using_numeric_batch):
            """
            Uses space.get_origin_batch() to return a numeric batch,
            or space.get_theano_batch() to return a symbolic
            Uses a fallback dtype if the space itself doesn't have one.
            """

            def specifies_all_dtypes(space):
                """
                Returns True iff space has a completely specified dtype.
                """
                if isinstance(space, CompositeSpace):
                    return all(specifies_all_dtypes(subspace)
                               for subspace in space.components)
                else:
                    return space.dtype is not None

            def replace_none_dtypes(dtype, fallback_dtype):
                """
                Returns dtype, with any Nones replaced by fallback_dtype.
                """

                if isinstance(dtype, tuple):
                    return tuple(replace_none_dtypes(d, fallback_dtype)
                                 for d in dtype)
                else:
                    return fallback_dtype if dtype is None else dtype

            kwargs = {"batch_size": batch_size}

            # Use this when space doesn't specify a dtype
            fallback_dtype = theano.config.floatX

            if not specifies_all_dtypes(space):
                kwargs["dtype"] = replace_none_dtypes(space.dtype,
                                                      fallback_dtype)

            if using_numeric_batch:
                return space.get_origin_batch(**kwargs)
            else:
                # Sparse VectorSpaces throw an exception if batch_size is
                # specified
                if isinstance(space, VectorSpace) and space.sparse:
                    del kwargs["batch_size"]

                kwargs["name"] = "space-generated batch"
                return space.make_theano_batch(**kwargs)

        def get_expected_warning(from_space, from_batch, to_space):

            # composite -> composite
            if isinstance(from_space, CompositeSpace) and \
               isinstance(to_space, CompositeSpace):
                for fs, fb, ts in safe_zip(from_space.components,
                                           from_batch,
                                           to_space.components):
                    warning, message = get_expected_warning(fs, fb, ts)
                    if warning is not None:
                        return warning, message

                return None, None

            # composite -> simple
            if isinstance(from_space, CompositeSpace):
                for fs, fb in safe_zip(from_space.components, from_batch):
                    warning, message = get_expected_warning(fs, fb, to_space)
                    if warning is not None:
                        return warning, message

                return None, None

            # simple -> composite
            if isinstance(to_space, CompositeSpace):
                if isinstance(from_space, VectorSpace) and \
                   isinstance(from_batch, theano.sparse.SparseVariable):
                    assert from_space.sparse
                    return (UserWarning,
                            'Formatting from a sparse VectorSpace to a '
                            'CompositeSpace is currently (2 Jan 2014) a '
                            'non-differentiable action. This is because it '
                            'calls slicing operations on a sparse batch '
                            '(e.g. "my_matrix[r:R, c:C]", which Theano does '
                            'not yet have a gradient operator for. If '
                            'autodifferentiation is reporting an error, '
                            'this may be why.')

                for ts in to_space.components:
                    warning, message = get_expected_warning(from_space,
                                                            from_batch,
                                                            ts)
                    if warning is not None:
                        return warning, message

                return None, None

            # simple -> simple
            return None, None

        def get_expected_error(from_space, from_batch, to_space):
            """
            Returns the type of error to be expected when calling
            from_space.np_format_as(batch, to_space). Returns None if no error
            should be expected.
            """

            def contains_different_dtypes(space):
                """
                Returns true if space contains different dtypes. None is
                considered distinct from all actual dtypes.
                """

                assert isinstance(space, CompositeSpace)

                def get_shared_dtype_if_any(space):
                    """
                    Returns space's dtype. If space is composite, returns the
                    dtype used by all of its subcomponents. Returns False if
                    the subcomponents use different dtypes.
                    """
                    if isinstance(space, CompositeSpace):
                        dtypes = tuple(get_shared_dtype_if_any(c)
                                       for c in space.components)
                        assert(len(dtypes) > 0)
                        if any(d != dtypes[0] for d in dtypes[1:]):
                            return False

                        return dtypes[0]  # could be False, but that's fine
                    else:
                        return space.dtype

                return get_shared_dtype_if_any(space) is False

            assert (isinstance(from_space, CompositeSpace) ==
                    isinstance(from_batch, tuple))

            # composite -> composite
            if isinstance(from_space, CompositeSpace) and \
               isinstance(to_space, CompositeSpace):
                for fs, fb, ts in safe_zip(from_space.components,
                                           from_batch,
                                           to_space.components):
                    error, message = get_expected_error(fs, fb, ts)
                    if error is not None:
                        return error, message

                return None, None

            # composite -> simple
            if isinstance(from_space, CompositeSpace):
                if isinstance(to_space, Conv2DSpace):
                    return (NotImplementedError,
                            "CompositeSpace does not know how to format as "
                            "Conv2DSpace")

                for fs, fb in safe_zip(from_space.components, from_batch):
                    error, message = get_expected_error(fs, fb, to_space)
                    if error is not None:
                        return error, message

                if isinstance(to_space, VectorSpace) and \
                   contains_different_dtypes(from_space) and \
                   to_space.dtype is None:
                    return (TypeError,
                            "Tried to format components with differing dtypes "
                            "into a VectorSpace with no dtype of its own. "
                            "dtypes: ")

                return None, None

            # simple -> composite
            if isinstance(to_space, CompositeSpace):

                if isinstance(from_space, VectorSpace) and \
                   isinstance(from_batch, theano.sparse.SparseVariable):
                    assert from_space.sparse
                    return (UserWarning,
                            'Formatting from a sparse VectorSpace to a '
                            'CompositeSpace is currently (2 Jan 2014) a '
                            'non-differentiable action. This is because it '
                            'calls slicing operations on a sparse batch '
                            '(e.g. "my_matrix[r:R, c:C]", which Theano does '
                            'not yet have a gradient operator for. If '
                            'autodifferentiation is reporting an error, '
                            'this may be why.')

                if isinstance(from_space, Conv2DSpace):
                    return (NotImplementedError,
                            "Conv2DSpace does not know how to format as "
                            "CompositeSpace")

                for ts in to_space.components:
                    error, message = get_expected_error(from_space,
                                                        from_batch,
                                                        ts)
                    if error is not None:
                        return error, message

                return None, None

            #
            # simple -> simple
            #

            def is_sparse(space):
                return isinstance(space, VectorSpace) and space.sparse

            def is_complex(arg):
                """
                Returns whether a space or a batch has a complex dtype.
                """
                return (arg.dtype is not None and
                        str(arg.dtype).startswith('complex'))

            if isinstance(from_batch, tuple):
                return (TypeError,
                        "This space only supports simple dtypes, but received "
                        "a composite batch.")

            if is_complex(from_batch) and not is_complex(from_space):
                return (TypeError,
                        "This space has a non-complex dtype (%s), and "
                        "thus cannot support complex batches of type %s." %
                        (from_space.dtype, from_batch.dtype))

            if from_space.dtype is not None and \
               from_space.dtype != from_batch.dtype:
                return (TypeError,
                        "This space is for dtype %s, but recieved a "
                        "batch of dtype %s." %
                        (from_space.dtype, from_batch.dtype))

            if is_sparse(from_space) and isinstance(to_space, Conv2DSpace):
                return (TypeError,
                        "Formatting a SparseVariable to a Conv2DSpace "
                        "is not supported, since neither scipy nor "
                        "Theano has sparse tensors with more than 2 "
                        "dimensions. We need 4 dimensions to "
                        "represent a Conv2DSpace batch")

            if is_complex(from_space) and not is_complex(to_space):
                if is_symbolic_batch(from_batch):
                    return (TypeError,
                            "Casting from complex to real is ambiguous")
                else:
                    return (np.ComplexWarning,
                            "Casting complex values to real discards the "
                            "imaginary part")

            return None, None

        def get_expected_formatted_dtype(from_batch, to_space):
            """
            Returns the expected dtype of the batch returned from a call to
            from_batch.format_as(batch, to_space). If the returned batch is a
            nested tuple, the expected dtype will also a nested tuple.
            """

            def get_single_dtype(batch):
                """
                Returns the dtype shared by all leaf nodes of the nested batch.
                If the nested batch contains differing dtypes, this throws an
                AssertionError. None counts as a different dtype than non-None.
                """
                if isinstance(batch, tuple):
                    assert len(batch) > 0
                    child_dtypes = tuple(get_single_dtype(b) for b in batch)
                    if any(c != child_dtypes[0] for c in child_dtypes[1:]):
                        return False

                    return child_dtypes[0]  # may be False, but that's correct.
                else:
                    return batch.dtype

            # composite -> composite
            if isinstance(from_batch, tuple) and \
               isinstance(to_space, CompositeSpace):
                return tuple(get_expected_formatted_dtype(b, s)
                             for b, s in safe_zip(from_batch,
                                                  to_space.components))
            # composite -> simple
            elif isinstance(from_batch, tuple):
                if to_space.dtype is not None:
                    return to_space.dtype
                else:
                    result = get_batch_dtype(from_batch)
                    if result is False:
                        raise TypeError("From_batch doesn't have a single "
                                        "dtype: %s" %
                                        str(get_batch_dtype(from_batch)))
                    return result

            # simple -> composite
            elif isinstance(to_space, CompositeSpace):
                return tuple(get_expected_formatted_dtype(from_batch, s)
                             for s in to_space.components)
            # simple -> simple with no dtype
            elif to_space.dtype is None:
                assert from_batch.dtype is not None
                return str(from_batch.dtype)
            # simple -> simple with a dtype
            else:
                return to_space.dtype

        from_batch = get_batch(from_space, using_numeric_batch)
        expected_error, expected_error_msg = get_expected_error(from_space,
                                                                from_batch,
                                                                to_space)

        # For some reason, the "with assert_raises(expected_error) as context:"
        # idiom isn't catching all the expceted_errors. Use this instead:
        if expected_error is not None:
            try:
                # temporarily upgrades warnings to exceptions within this block
                with warnings.catch_warnings():
                    warnings.simplefilter("error")
                    from_space._format_as(using_numeric_batch,
                                          from_batch,
                                          to_space)
            except expected_error, ex:
                assert str(ex).find(expected_error_msg) >= 0
            except Exception, unknown_ex:
                print "Expected exception of type %s, got %s." % \
                      (expected_error.__name__, type(unknown_ex))
                raise unknown_ex
            finally:
                return

        to_batch = from_space._format_as(using_numeric_batch,
                                         from_batch,
                                         to_space)
        expected_dtypes = get_expected_formatted_dtype(from_batch, to_space)
        actual_dtypes = get_batch_dtype(to_batch)

        assert expected_dtypes == actual_dtypes, \
            ("\nexpected_dtypes: %s,\n"
             "actual_dtypes: %s \n"
             "from_space: %s\n"
             "from_batch's dtype: %s\n"
             "from_batch is theano?: %s\n"
             "to_space: %s" % (expected_dtypes,
                               actual_dtypes,
                               from_space,
                               get_batch_dtype(from_batch),
                               is_symbolic_batch(from_batch),
                               to_space))

    #
    #
    # End of test_format() function.

    def test_dtype_getter(space):
        """
        Tests the getter method of space's dtype property.
        """

        def assert_composite_dtype_eq(space, dtype):
            """
            Asserts that dtype is a nested tuple with exactly the same tree
            structure as space, and that the dtypes of space's components and
            their corresponding elements in <dtype> are equal.
            """
            assert (isinstance(space, CompositeSpace) ==
                    isinstance(dtype, tuple))

            if isinstance(space, CompositeSpace):
                for s, d in safe_zip(space.components, dtype):
                    assert_composite_dtype_eq(s, d)
            else:
                assert space.dtype == dtype

        if isinstance(space, SimplyTypedSpace):
            assert space.dtype == space._dtype
        elif isinstance(space, NullSpace):
            assert space.dtype == "NullSpace's dtype"
        elif isinstance(space, CompositeSpace):
            assert_composite_dtype_eq(space, space.dtype)

    def test_dtype_setter(space, dtype):
        """
        Tests the setter method of space's dtype property.
        """
        def get_expected_error(space, dtype):
            """
            If calling space.dtype = dtype is expected to throw an exception,
            this returns (exception_class, exception_message).

            If no exception is to be expected, this returns (None, None).
            """
            if isinstance(space, CompositeSpace):
                if isinstance(dtype, tuple):
                    if len(space.components) != len(dtype):
                        return ValueError, "Argument 0 has length "

                    for s, d in safe_zip(space.components, dtype):
                        error, message = get_expected_error(s, d)
                        if error is not None:
                            return error, message
                else:
                    for s in space.components:
                        error, message = get_expected_error(s, dtype)
                        if error is not None:
                            return error, message

                return None, None

            if isinstance(space, SimplyTypedSpace):
                if not any((dtype is None,
                            dtype == 'floatX',
                            dtype in all_scalar_dtypes)):
                    return (TypeError,
                            'Unrecognized value "%s" (type %s) for dtype arg' %
                            (dtype, type(dtype)))

                return None, None

            if isinstance(space, NullSpace):
                nullspace_dtype = NullSpace().dtype
                if dtype != nullspace_dtype:
                    return (TypeError,
                            'NullSpace can only take the bogus dtype "%s"' %
                            nullspace_dtype)

                return None, None

            raise NotImplementedError("%s not yet supported by this test" %
                                      type(space))

        def assert_dtype_equiv(space, dtype):
            """
            Asserts that space.dtype and dtype are equivalent.
            """

            if isinstance(space, CompositeSpace):
                if isinstance(dtype, tuple):
                    for s, d in safe_zip(space.components, dtype):
                        assert_dtype_equiv(s, d)
                else:
                    for s in space.components:
                        assert_dtype_equiv(s, dtype)
            else:
                assert not isinstance(dtype, tuple)
                if dtype == 'floatX':
                    dtype = theano.config.floatX

                assert space.dtype == dtype, ("%s not equal to %s" %
                                              (space.dtype, dtype))

        expected_error, expected_message = get_expected_error(space, dtype)
        if expected_error is not None:
            try:
                space.dtype = dtype
            except expected_error, ex:
                assert expected_message in str(ex)
            except Exception:
                print "Expected exception of type %s, got %s instead." % \
                      (expected_error.__name__, type(ex))
                raise ex
            return
        else:
            space.dtype = dtype
            assert_dtype_equiv(space, dtype)

    #
    #
    # End of test_dtype_setter() function

    shape = np.array([2, 3, 4], dtype='int')
    assert len(shape) == 3  # This test depends on this being true

    dtypes = ('floatX', None) + all_scalar_dtypes

    #
    # spaces with the same number of elements
    #

    vector_spaces = tuple(VectorSpace(dim=shape.prod(), dtype=dt, sparse=s)
                          for dt in dtypes for s in (True, False))
    conv2d_spaces = tuple(Conv2DSpace(shape=shape[:2],
                                      dtype=dt,
                                      num_channels=shape[2])
                          for dt in dtypes)

    # no need to make CompositeSpaces with components spanning all possible
    # dtypes. Just try 2 dtype combos. No need to try different sparsities
    # either. That will be tested by the non-composite space conversions.
    n_dtypes = 2
    old_nchannels = shape[2]
    shape[2] = old_nchannels / 2
    assert shape[2] * 2 == old_nchannels, \
        ("test code is broken: # of channels should start as an even "
         "number, not %d." % old_nchannels)

    def make_composite_space(dtype0, dtype1, use_conv2d):
        if use_conv2d:
            second_space = Conv2DSpace(shape=shape[:2],
                                       dtype=dtype1,
                                       num_channels=shape[2])
        else:
            second_space = VectorSpace(dim=np.prod(shape),
                                       dtype=dtype1)

        return CompositeSpace((VectorSpace(dim=shape.prod(), dtype=dtype0),
                               second_space))

    composite_spaces = tuple(make_composite_space(dtype0, dtype1, use_conv2d)
                             for dtype0, dtype1 in zip(dtypes[:n_dtypes],
                                                       dtypes[-n_dtypes:])
                             for use_conv2d in [True, False])
    del n_dtypes

    # A few composite dtypes to try throwing at CompositeSpace's batch-making
    # methods.
    composite_dtypes = ((None, 'int8'),
                        ('complex128', theano.config.floatX))

    # Tests CompositeSpace's batch-making methods and dtype setter
    # with composite dtypes
    for from_space in composite_spaces:
        for to_dtype in composite_dtypes:
            test_get_origin_batch(from_space, to_dtype)
            test_make_shared_batch(from_space, to_dtype)
            test_make_theano_batch(from_space, to_dtype)
            test_dtype_setter(from_space, to_dtype)

    all_spaces = vector_spaces + conv2d_spaces + composite_spaces
    for from_space in all_spaces:
        test_dtype_getter(from_space)

        # Tests batch-making and dtype setting methods with non-composite
        # dtypes.
        for to_dtype in dtypes:
            test_get_origin_batch(from_space, to_dtype)
            test_make_shared_batch(from_space, to_dtype)
            test_make_theano_batch(from_space, to_dtype)
            test_dtype_setter(from_space, to_dtype)

        # Tests _format_as
        for to_space in all_spaces:
            for is_numeric in (True, False):
                test_format(from_space, to_space, is_numeric)

########NEW FILE########
__FILENAME__ = test_init
"""
Tests for pylearn2.termination_criteria.__init__ functions and classes.
"""


from pylearn2.termination_criteria import EpochCounter

from pylearn2.datasets.dense_design_matrix import DenseDesignMatrix
from pylearn2.models.mlp import MLP, Softmax
from pylearn2.monitor import push_monitor
from pylearn2.train import Train
from pylearn2.training_algorithms.sgd import SGD

import numpy as np


def test_epoch_counter():
    """
    Test epoch counter with max_epochs={True,False}
    """

    N = 5

    def produce_train_obj(new_epochs, model=None):
        if model is None:
            model = MLP(layers=[Softmax(layer_name='y',
                                        n_classes=2,
                                        irange=0.)],
                        nvis=3)
        else:
            model = push_monitor(model, 'old_monitor',
                                 transfer_experience=True)

        dataset = DenseDesignMatrix(X=np.random.normal(size=(6, 3)),
                                    y=np.random.normal(size=(6, 2)))

        epoch_counter = EpochCounter(max_epochs=N,
                                     new_epochs=new_epochs)

        algorithm = SGD(batch_size=2, learning_rate=0.1,
                        termination_criterion=epoch_counter)

        return Train(dataset=dataset,
                     model=model,
                     algorithm=algorithm)

    def test_epochs(epochs_seen, n):
        assert epochs_seen == n, \
            "%d epochs seen and should be %d" % (epochs_seen, n)

    # Tests for N new epochs
    train_obj = produce_train_obj(new_epochs=True)
    train_obj.main_loop()
    test_epochs(train_obj.model.monitor.get_epochs_seen(), N)
    train_obj = produce_train_obj(new_epochs=True, model=train_obj.model)
    train_obj.main_loop()
    test_epochs(train_obj.model.monitor.get_epochs_seen(), 2*N)

    # Tests for N max epochs
    train_obj = produce_train_obj(new_epochs=False)
    train_obj.main_loop()
    test_epochs(train_obj.model.monitor.get_epochs_seen(), N)
    # Try training while already reached max_epochs, should stop after 1 epoch
    # on first continue_learning() call
    train_obj = produce_train_obj(new_epochs=False, model=train_obj.model)
    train_obj.main_loop()
    test_epochs(train_obj.model.monitor.get_epochs_seen(), N+1)

########NEW FILE########
__FILENAME__ = cost
""" Simple costs to be used for unit tests. """
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

from pylearn2.costs.cost import Cost
from pylearn2.space import NullSpace
from pylearn2.utils import CallbackOp
from pylearn2.utils import safe_zip
from pylearn2.utils.data_specs import DataSpecsMapping


class CallbackCost(Cost):
    """
    A Cost that runs callbacks on the data.  Returns the sum of the data
    multiplied by the sum of all model parameters as the cost.  The callback is
    run via the CallbackOp so the cost must be used to compute one of the
    outputs of your theano graph if you want the callback to get called.  The
    is cost is designed so that the SGD algorithm will result in in the
    CallbackOp getting evaluated.

    Parameters
    ----------
    data_callback : optional, callbacks to run on data.
        It is either a Python callable, or a tuple (possibly nested),
        in the same format as data_specs.
    data_specs : (space, source) pair specifying the format
        and label associated to the data.
    """
    def __init__(self, data_callbacks, data_specs):
        self.data_callbacks = data_callbacks
        self.data_specs = data_specs
        self._mapping = DataSpecsMapping(data_specs)

    def get_data_specs(self, model):
        return self.data_specs

    def expr(self, model, data):
        self.get_data_specs(model)[0].validate(data)
        callbacks = self.data_callbacks

        cb_tuple = self._mapping.flatten(callbacks, return_tuple=True)
        data_tuple = self._mapping.flatten(data, return_tuple=True)

        costs = []
        for (callback, data_var) in safe_zip(cb_tuple, data_tuple):
            orig_var = data_var
            data_var = CallbackOp(callback)(data_var)
            assert len(data_var.owner.inputs) == 1
            assert orig_var is data_var.owner.inputs[0]

            costs.append(data_var.sum())

        # sum() will call theano.add on the symbolic variables
        cost = sum(costs)
        model_terms = sum([param.sum() for param in model.get_params()])
        cost = cost * model_terms
        return cost


class SumOfParams(Cost):
    """
    A cost that is just the sum of all parameters, so the gradient
    on every parameter is 1.
    """

    def expr(self, model, data):
        self.get_data_specs(model)[0].validate(data)
        return sum(param.sum() for param in model.get_params())

    def get_data_specs(self, model):
        # This cost does not need any data
        return (NullSpace(), '')


class SumOfOneHalfParamsSquared(Cost):
    """
    A cost that is just 0.5 * the sum of all parameters squared, so the gradient
    on every parameter is the parameter itself.
    """

    def expr(self, model, data):
        self.get_data_specs(model)[0].validate(data)
        return 0.5 * sum((param**2).sum() for param in model.get_params())

    def get_data_specs(self, model):
        # This cost does not need any data
        return (NullSpace(), '')

########NEW FILE########
__FILENAME__ = datasets
""" Simple datasets to be used for unit tests. """
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

import numpy as np
from pylearn2.datasets.dense_design_matrix import DenseDesignMatrix

class ArangeDataset(DenseDesignMatrix):
    """
    A dataset where example i is just the number i. Makes it easy to track
    which sets of examples are visited.

    Parameters
    ----------
    num_examples : WRITEME
    """
    def __init__(self, num_examples):
        X = np.zeros((num_examples,1))
        X[:,0] = np.arange(num_examples)
        super(ArangeDataset, self).__init__(X)

def random_dense_design_matrix(rng, num_examples, dim, num_classes):
    X = rng.randn(num_examples, dim)

    if num_classes:
        Y = rng.randint(0, num_classes, (num_examples,1))
    else:
        Y = None

    return DenseDesignMatrix(X=X, y=Y)

def random_one_hot_dense_design_matrix(rng, num_examples, dim, num_classes):
    X = rng.randn(num_examples, dim)


    idx = rng.randint(0, num_classes, (num_examples,))
    Y = np.zeros((num_examples,num_classes))
    for i in xrange(num_examples):
        Y[i,idx[i]] = 1

    return DenseDesignMatrix(X=X, y=Y)

def random_one_hot_topological_dense_design_matrix(rng, num_examples, shape, channels, axes, num_classes):

    dims = {
            'b': num_examples,
            'c': channels
            }

    for i, dim in enumerate(shape):
        dims[i] = dim

    shape = [dims[axis] for axis in axes]

    X = rng.randn(*shape)

    idx = rng.randint(0, num_classes, (num_examples,))
    Y = np.zeros((num_examples,num_classes))
    for i in xrange(num_examples):
        Y[i,idx[i]] = 1

    return DenseDesignMatrix(topo_view=X, axes=axes, y=Y)

########NEW FILE########
__FILENAME__ = prereqs
""" Objects to be used as Monitor prereqs during testing. """


class ReadVerifyPrereq(object):
    """
    Part of tests/test_monitor.py. Just put here so it be serialized.

    Parameters
    ----------
    counter_idx : WRITEME
    counter : WRITEME
    """
    def __init__(self, counter_idx, counter):
        self.counter_idx = counter_idx
        self.counter = counter

    def __call__(self, *data):
        # We set up each dataset with a different batch size
        # check here that we're getting the right one
        X, = data
        assert X.shape[0] == self.counter_idx + 1
        # Each dataset has different content, make sure we
        # get the right one
        assert X[0,0] == self.counter_idx
        prereq_counter = self.counter
        prereq_counter.set_value(
            prereq_counter.get_value()+1)

########NEW FILE########
__FILENAME__ = skip
"""
Helper functions for determining which tests to skip.
"""

__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"
from nose.plugins.skip import SkipTest
import os
from theano.sandbox import cuda

scipy_works = True
try:
    import scipy
except ImportError:
    # pyflakes gets mad if you set scipy to None here
    scipy_works = False

sklearn_works = True
try:
    import sklearn
except ImportError:
    sklearn_works = False

h5py_works = True
try:
    import h5py
except ImportError:
    h5py_works = False

def skip_if_no_data():
    if 'PYLEARN2_DATA_PATH' not in os.environ:
        raise SkipTest()

def skip_if_no_scipy():
    if not scipy_works:
        raise SkipTest()

def skip_if_no_sklearn():
    if not sklearn_works:
        raise SkipTest()

def skip_if_no_gpu():
    if cuda.cuda_available == False:
        raise SkipTest('Optional package cuda disabled.')

def skip_if_no_h5py():
    if not h5py_works:
        raise SkipTest()

########NEW FILE########
__FILENAME__ = test
from pylearn2.testing import no_debug_mode

from theano import config

@no_debug_mode
def assert_not_debug_mode():
    assert config.mode != "DEBUG_MODE"

def test_no_debug_mode():
    orig_mode = config.mode
    config.mode = "DEBUG_MODE"
    try:
        # make sure the decorator gets rid of DEBUG_MODE
        assert_not_debug_mode()
    finally:
        # make sure the decorator restores DEBUG_MODE when it's done
        assert config.mode == "DEBUG_MODE"
        config.mode = orig_mode


########NEW FILE########
__FILENAME__ = test_ais
import numpy
import time
import warnings
try:
    from scipy import io
except ImportError:
    warnings.warn("couldn't import scipy")

import theano
from theano import config
import theano.tensor as T

from pylearn2 import rbm_tools
from pylearn2.models import rbm


def load_rbm_params(fname):
    mnistvh = io.loadmat(fname)
    rbm_params = [numpy.asarray(mnistvh['vishid'], dtype=config.floatX),
                  numpy.asarray(mnistvh['visbiases'][0], dtype=config.floatX),
                  numpy.asarray(mnistvh['hidbiases'][0], dtype=config.floatX)]
    return rbm_params


def compute_logz(rbm_params):
    (nvis, nhid) = rbm_params[0].shape

    model = rbm.RBM(nvis, nhid)
    model.transformer.get_params()[0].set_value(rbm_params[0])
    model.bias_vis.set_value(rbm_params[1])
    model.bias_hid.set_value(rbm_params[2])

    hid = T.matrix('hid')
    hid_fe = model.free_energy_given_h(hid)
    free_energy_fn = theano.function([hid], hid_fe)

    return rbm_tools.compute_log_z(model, free_energy_fn)


def ais_nodata(fname, do_exact=True):

    rbm_params = load_rbm_params(fname)

    # ais estimate using tempered models as intermediate distributions
    t1 = time.time()
    (logz, log_var_dz), aisobj = \
        rbm_tools.rbm_ais(rbm_params, n_runs=100, seed=123)
    print 'AIS logZ         : %f' % logz
    print '    log_variance : %f' % log_var_dz
    print 'Elapsed time: ', time.time() - t1

    if do_exact:
        exact_logz = compute_logz(rbm_params)
        print 'Exact logZ = %f' % exact_logz
        # accept less than 1% error
        assert abs(exact_logz - logz) < 0.01*exact_logz


def ais_data(fname, do_exact=True):

    rbm_params = load_rbm_params(fname)

    # load data to set visible biases to ML solution
    from pylearn2.datasets.mnist import MNIST
    dataset = MNIST(which_set='train', one_hot=True)
    data = numpy.asarray(dataset.X, dtype=config.floatX)

    # run ais using B=0 model with ML visible biases
    t1 = time.time()
    (logz, log_var_dz), aisobj = \
        rbm_tools.rbm_ais(rbm_params, n_runs=100, seed=123, data=data)
    print 'AIS logZ         : %f' % logz
    print '    log_variance : %f' % log_var_dz
    print 'Elapsed time: ', time.time() - t1

    if do_exact:
        exact_logz = compute_logz(rbm_params)
        print 'Exact logZ = %f' % exact_logz
        numpy.testing.assert_almost_equal(exact_logz, logz, decimal=0)


def test_ais():

    ais_data('mnistvh.mat', do_exact=True)

    # Estimate can be off when using the wrong base-rate model.
    ais_nodata('mnistvh.mat', do_exact=True)

########NEW FILE########
__FILENAME__ = test_dbm_metrics
"""
Test dbm_metrics script
"""
import numpy
import theano
from theano import tensor as T
from pylearn2.models.dbm.dbm import DBM
from pylearn2.models.dbm.layer import BinaryVector, BinaryVectorMaxPool
from pylearn2.scripts.dbm import dbm_metrics
from pylearn2 import rbm_tools
from pylearn2.datasets.mnist import MNIST
from nose.plugins.skip import SkipTest
from pylearn2.datasets.exc import NoDataPathError
from pylearn2.testing import no_debug_mode


@no_debug_mode
def test_ais():
    """
    Test ais computation by comparing the output of estimate_likelihood to
    Russ's code's output for the same parameters.
    """
    try:
        # TODO: the one_hot=True is only necessary because one_hot=False is
        # broken, remove it after one_hot=False is fixed.
        trainset = MNIST(which_set='train', one_hot=True)
        testset = MNIST(which_set='test', one_hot=True)
    except NoDataPathError:
        raise SkipTest("PYLEARN2_DATA_PATH environment variable not defined")

    nvis = 784
    nhid = 20
    # Random initialization of RBM parameters
    numpy.random.seed(98734)
    w_hid = 10 * numpy.cast[theano.config.floatX](numpy.random.randn(nvis,
                                                                     nhid))
    b_vis = 10 * numpy.cast[theano.config.floatX](numpy.random.randn(nvis))
    b_hid = 10 * numpy.cast[theano.config.floatX](numpy.random.randn(nhid))

    # Initialization of RBM
    visible_layer = BinaryVector(nvis)
    hidden_layer = BinaryVectorMaxPool(detector_layer_dim=nhid, pool_size=1,
                                       layer_name='h', irange=0.1)
    rbm = DBM(100, visible_layer, [hidden_layer], 1)
    rbm.visible_layer.set_biases(b_vis)
    rbm.hidden_layers[0].set_weights(w_hid)
    rbm.hidden_layers[0].set_biases(b_hid)
    rbm.nvis = nvis
    rbm.nhid = nhid

    # Compute real logz and associated train_ll and test_ll using rbm_tools
    v_sample = T.matrix('v_sample')
    h_sample = T.matrix('h_sample')
    W = theano.shared(rbm.hidden_layers[0].get_weights())
    hbias = theano.shared(rbm.hidden_layers[0].get_biases())
    vbias = theano.shared(rbm.visible_layer.get_biases())

    wx_b = T.dot(v_sample, W) + hbias
    vbias_term = T.dot(v_sample, vbias)
    hidden_term = T.sum(T.log(1 + T.exp(wx_b)), axis=1)
    free_energy_v = -hidden_term - vbias_term
    free_energy_v_fn = theano.function(inputs=[v_sample],
                                       outputs=free_energy_v)

    wh_c = T.dot(h_sample, W.T) + vbias
    hbias_term = T.dot(h_sample, hbias)
    visible_term = T.sum(T.log(1 + T.exp(wh_c)), axis=1)
    free_energy_h = -visible_term - hbias_term
    free_energy_h_fn = theano.function(inputs=[h_sample],
                                       outputs=free_energy_h)

    real_logz = rbm_tools.compute_log_z(rbm, free_energy_h_fn)

    real_ais_train_ll = -rbm_tools.compute_nll(rbm,
                                               trainset.get_design_matrix(),
                                               real_logz, free_energy_v_fn)
    real_ais_test_ll = -rbm_tools.compute_nll(rbm, testset.get_design_matrix(),
                                              real_logz, free_energy_v_fn)

    # Compute train_ll, test_ll and logz using dbm_metrics
    train_ll, test_ll, logz = dbm_metrics.estimate_likelihood([W],
                                                              [vbias, hbias],
                                                              trainset,
                                                              testset,
                                                              pos_mf_steps=100)
    assert (real_logz - logz) < 2.0
    assert (real_ais_train_ll - train_ll) < 2.0
    assert (real_ais_test_ll - test_ll) < 2.0

if __name__ == '__main__':
    test_ais()

########NEW FILE########
__FILENAME__ = test_monitor
import numpy as np
import warnings

from theano.compat import exc_message
from theano import shared
from theano import tensor as T

from pylearn2.datasets.dense_design_matrix import DenseDesignMatrix
from pylearn2.models.model import Model
from pylearn2.models.s3c import S3C, E_Step, Grad_M_Step
from pylearn2.monitor import _err_ambig_data
from pylearn2.monitor import _err_no_data
from pylearn2.monitor import Monitor
from pylearn2.monitor import push_monitor
from pylearn2.space import VectorSpace
from pylearn2.testing.datasets import ArangeDataset
from pylearn2.training_algorithms.default import DefaultTrainingAlgorithm
from pylearn2.utils.iteration import _iteration_schemes, has_uniform_batch_size
from pylearn2.utils import py_integer_types
from pylearn2.utils.serial import from_string
from pylearn2.utils.serial import to_string
from pylearn2.utils import sharedX
from pylearn2.testing.prereqs import ReadVerifyPrereq


class DummyModel(Model):
    def  __init__(self, num_features):
        self.input_space = VectorSpace(num_features)


class DummyDataset(DenseDesignMatrix):
    def __init__(self, num_examples, num_features):
        rng = np.random.RandomState([4, 12, 17])
        super(DummyDataset, self).__init__(
            X=rng.uniform(1., 2., (num_examples, num_features))
        )

    def __getstate__(self):
        raise AssertionError("These unit tests only test Monitor "
                "functionality. If the Monitor tries to serialize a "
                "Dataset, that is an error.")


def test_channel_scaling_sequential():
    def channel_scaling_checker(num_examples, mode, num_batches, batch_size):
        num_features = 2
        monitor = Monitor(DummyModel(num_features))
        dataset = DummyDataset(num_examples, num_features)
        monitor.add_dataset(dataset=dataset, mode=mode,
                                num_batches=num_batches, batch_size=batch_size)
        vis_batch = T.matrix()
        mean = vis_batch.mean()
        data_specs = (monitor.model.get_input_space(),
                      monitor.model.get_input_source())
        monitor.add_channel(name='mean', ipt=vis_batch, val=mean, dataset=dataset,
                            data_specs=data_specs)
        monitor()
        assert 'mean' in monitor.channels
        mean = monitor.channels['mean']
        assert len(mean.val_record) == 1
        actual = mean.val_record[0]
        X = dataset.get_design_matrix()
        if batch_size is not None and num_batches is not None:
            total = min(num_examples, num_batches * batch_size)
        else:
            total = num_examples
        expected = X[:total].mean()
        if not np.allclose(expected, actual):
            raise AssertionError("Expected monitor to contain %f but it has "
                                 "%f" % (expected, actual))

    # Specifying num_batches; even split
    yield channel_scaling_checker, 10, 'sequential', 5, None
    # Specifying num_batches; even split
    yield channel_scaling_checker, 10, 'sequential', 2, None
    # Specifying batch_size; even split
    yield channel_scaling_checker, 10, 'sequential', None, 5
    # Specifying batch_size; even split
    yield channel_scaling_checker, 10, 'sequential', None, 2
    # Specifying num_batches; uneven split
    yield channel_scaling_checker, 10, 'sequential', 4, None
    # Specifying num_batches; uneven split
    yield channel_scaling_checker, 10, 'sequential', 3, None
    # Specifying batch_size; uneven split
    yield channel_scaling_checker, 10, 'sequential', None, 3
    # Specifying batch_size; uneven split
    yield channel_scaling_checker, 10, 'sequential', None, 4
    # Specifying both, even split
    yield channel_scaling_checker, 10, 'sequential', 2, 5
    # Specifying both, even split
    yield channel_scaling_checker, 10, 'sequential', 5, 2
    # Specifying both, uneven split, dangling batch
    yield channel_scaling_checker, 10, 'sequential', 3, 4
    # Specifying both, uneven split, non-exhaustive
    yield channel_scaling_checker, 10, 'sequential', 3, 3

def test_counting():
    BATCH_SIZE = 2
    BATCHES = 3
    NUM_FEATURES = 4
    num_examples = BATCHES * BATCH_SIZE
    dataset = DummyDataset( num_examples = num_examples,
            num_features = NUM_FEATURES)
    algorithm = DefaultTrainingAlgorithm( batch_size = BATCH_SIZE,
            batches_per_iter = BATCHES)
    model = S3C( nvis = NUM_FEATURES, nhid = 1,
            irange = .01, init_bias_hid = 0., init_B = 1.,
            min_B = 1., max_B = 1., init_alpha = 1.,
            min_alpha = 1., max_alpha = 1., init_mu = 0.,
            m_step = Grad_M_Step( learning_rate = 0.),
            e_step = E_Step( h_new_coeff_schedule = [ 1. ]))
    algorithm.setup(model = model, dataset = dataset)
    algorithm.train(dataset = dataset)
    if not ( model.monitor.get_batches_seen() == BATCHES):
        raise AssertionError('Should have seen '+str(BATCHES) + \
                ' batches but saw '+str(model.monitor.get_batches_seen()))

    assert model.monitor.get_examples_seen() == num_examples
    assert isinstance(model.monitor.get_examples_seen(), py_integer_types)
    assert isinstance(model.monitor.get_batches_seen(), py_integer_types)

def test_reject_empty():

    # Test that Monitor raises an error if asked to iterate over 0 batches

    BATCH_SIZE = 2
    num_examples = BATCH_SIZE
    NUM_FEATURES = 3

    model = DummyModel(NUM_FEATURES)
    monitor = Monitor.get_monitor(model)

    monitoring_dataset = DummyDataset(num_examples = num_examples,
            num_features = NUM_FEATURES)

    monitor.add_dataset(monitoring_dataset, 'sequential', batch_size=BATCH_SIZE,
            num_batches = 0)

    name = 'z'

    monitor.add_channel(name = name,
            ipt = model.input_space.make_theano_batch(),
            val = 0.,
            data_specs=(model.get_input_space(), model.get_input_source()))

    try:
        monitor()
    except ValueError:
        return
    assert False

def test_prereqs():

    # Test that prereqs get run before the monitoring channels are computed

    BATCH_SIZE = 2
    num_examples = BATCH_SIZE
    NUM_FEATURES = 3

    model = DummyModel(NUM_FEATURES)
    monitor = Monitor.get_monitor(model)

    monitoring_dataset = DummyDataset(num_examples = num_examples,
            num_features = NUM_FEATURES)

    monitor.add_dataset(monitoring_dataset, 'sequential', batch_size=BATCH_SIZE)

    prereq_counter = sharedX(0.)
    def prereq(*data):
        prereq_counter.set_value(
                prereq_counter.get_value()+1.)

    name = 'num_prereq_calls'

    monitor.add_channel(name = name,
            ipt = model.input_space.make_theano_batch(),
            val = prereq_counter,
            prereqs = [ prereq ],
            data_specs=(model.get_input_space(), model.get_input_source()))

    channel = monitor.channels[name]

    assert len(channel.val_record) == 0
    monitor()
    assert channel.val_record == [1]
    monitor()
    assert channel.val_record == [1,2]

def test_revisit():

    # Test that each call to monitor revisits exactly the same data

    BATCH_SIZE = 3
    MAX_BATCH_SIZE = 12
    BATCH_SIZE_STRIDE = 3
    NUM_BATCHES = 10
    num_examples = NUM_BATCHES * BATCH_SIZE

    monitoring_dataset = ArangeDataset(num_examples)

    for mon_batch_size in xrange(BATCH_SIZE, MAX_BATCH_SIZE + 1,
            BATCH_SIZE_STRIDE):
        for num_mon_batches in [ 1, 3, num_examples / mon_batch_size, None ]:
            for mode in sorted(_iteration_schemes):

                if num_mon_batches is None and mode in ['random_uniform', 'random_slice']:
                    continue

                if has_uniform_batch_size(mode) and \
                   num_mon_batches is not None and \
                   num_mon_batches * mon_batch_size > num_examples:

                    num_mon_batches = int(num_examples / float(mon_batch_size))

                model = DummyModel(1)
                monitor = Monitor.get_monitor(model)

                try:
                    monitor.add_dataset(monitoring_dataset, mode,
                        batch_size=mon_batch_size, num_batches=num_mon_batches)
                except TypeError:
                    monitor.add_dataset(monitoring_dataset, mode,
                        batch_size=mon_batch_size, num_batches=num_mon_batches,
                        seed = 0)

                if has_uniform_batch_size(mode) and num_mon_batches is None:
                    num_mon_batches = int(num_examples / float(mon_batch_size))
                elif num_mon_batches is None:
                    num_mon_batches = int(np.ceil(float(num_examples) /
                                          float(mon_batch_size)))

                batches = [ None ] * num_mon_batches
                visited = [ False ] * num_mon_batches

                batch_idx = shared(0)

                class RecorderAndValidator(object):

                    def __init__(self):
                        self.validate = False

                    def __call__(self, *data):
                        """ Initially, records the batches the monitor shows it.
                        When set to validate mode, makes sure the batches shown
                        on the second monitor call match those from the first."""
                        X, = data

                        idx = batch_idx.get_value()
                        batch_idx.set_value(idx + 1)

                        # Note: if the monitor starts supporting variable batch sizes,
                        # take this out. Maybe move it to a new test that the iterator's
                        # uneven property is set accurately
                        warnings.warn("TODO: add unit test that iterators uneven property is set correctly.")
                        # assert X.shape[0] == mon_batch_size

                        if self.validate:
                            previous_batch = batches[idx]
                            assert not visited[idx]
                            visited[idx] = True
                            if not np.allclose(previous_batch, X):
                                print 'Visited different data in batch',idx
                                print previous_batch
                                print X
                                print 'Iteration mode', mode
                                assert False
                        else:
                            batches[idx] = X
                        # end if
                    # end __call__
                #end class

                prereq = RecorderAndValidator()

                monitor.add_channel(name = 'dummy',
                    ipt = model.input_space.make_theano_batch(),
                    val = 0.,
                    prereqs = [ prereq ],
                    data_specs=(model.get_input_space(),
                                model.get_input_source()))

                try:
                    monitor()
                except RuntimeError:
                    print 'monitor raised RuntimeError for iteration mode', mode
                    raise


                assert None not in batches

                batch_idx.set_value(0)
                prereq.validate = True

                monitor()

                assert all(visited)

def test_prereqs_batch():

    # Test that prereqs get run before each monitoring batch

    BATCH_SIZE = 2
    num_examples = 2 * BATCH_SIZE
    NUM_FEATURES = 3

    model = DummyModel(NUM_FEATURES)
    monitor = Monitor.get_monitor(model)

    monitoring_dataset = DummyDataset(num_examples = num_examples,
            num_features = NUM_FEATURES)

    monitor.add_dataset(monitoring_dataset, 'sequential', batch_size=BATCH_SIZE)

    sign = sharedX(1.)
    def prereq(*data):
        sign.set_value(
                -sign.get_value())

    name = 'batches_should_cancel_to_0'

    monitor.add_channel(name = name,
            ipt = model.input_space.make_theano_batch(),
            val = sign,
            prereqs = [ prereq ],
            data_specs=(model.get_input_space(), model.get_input_source()))

    channel = monitor.channels[name]

    assert len(channel.val_record) == 0
    monitor()
    assert channel.val_record == [0]
    monitor()
    assert channel.val_record == [0,0]


def test_dont_serialize_dataset():

    # Test that serializing the monitor does not serialize the dataset

    BATCH_SIZE = 2
    num_examples = 2 * BATCH_SIZE
    NUM_FEATURES = 3

    model = DummyModel(NUM_FEATURES)
    monitor = Monitor.get_monitor(model)

    monitoring_dataset = DummyDataset(num_examples = num_examples,
            num_features = NUM_FEATURES)
    monitoring_dataset.yaml_src = ""

    monitor.add_dataset(monitoring_dataset, 'sequential', batch_size=BATCH_SIZE)

    monitor()

    to_string(monitor)

def test_serialize_twice():

    # Test that a monitor can be serialized twice
    # with the same result

    model = DummyModel(1)
    monitor = Monitor.get_monitor(model)

    x = to_string(monitor)
    y = to_string(monitor)

    assert x == y

def test_save_load_save():

    """
    Test that a monitor can be saved, then loaded, and then the loaded
    copy can be saved again.
    This only tests that the serialization and deserialization processes
    don't raise an exception. It doesn't test for correctness at all.
    """

    model = DummyModel(1)
    monitor = Monitor.get_monitor(model)

    num_examples = 2
    num_features = 3
    num_batches = 1
    batch_size = 2

    dataset = DummyDataset(num_examples, num_features)
    monitor.add_dataset(dataset=dataset,
                            num_batches=num_batches, batch_size=batch_size)
    vis_batch = T.matrix()
    mean = vis_batch.mean()
    data_specs = (monitor.model.get_input_space(),
                  monitor.model.get_input_source())
    monitor.add_channel(name='mean', ipt=vis_batch, val=mean, dataset=dataset,
                        data_specs=data_specs)

    saved = to_string(monitor)
    monitor = from_string(saved)
    saved_again = to_string(monitor)

def test_valid_after_serialize():

    # Test that serializing the monitor does not ruin it

    BATCH_SIZE = 2
    num_examples = 2 * BATCH_SIZE
    NUM_FEATURES = 3

    model = DummyModel(NUM_FEATURES)
    monitor = Monitor.get_monitor(model)

    monitoring_dataset = DummyDataset(num_examples = num_examples,
            num_features = NUM_FEATURES)
    monitoring_dataset.yaml_src = ""

    monitor.add_dataset(monitoring_dataset, 'sequential', batch_size=BATCH_SIZE)

    to_string(monitor)

    monitor.redo_theano()

def test_deserialize():

    # Test that a monitor can be deserialized

    model = DummyModel(1)
    monitor = Monitor.get_monitor(model)

    x = to_string(monitor)
    monitor = from_string(x)
    y = to_string(monitor)

def test_prereqs_multidataset():

    # Test that prereqs are run on the right datasets

    NUM_DATASETS = 4
    NUM_FEATURES = 3

    model = DummyModel(NUM_FEATURES)
    monitor = Monitor.get_monitor(model)

    prereq_counters = []
    datasets = []
    for i in xrange(NUM_DATASETS):

        batch_size = i + 1
        num_examples = batch_size
        dataset = DummyDataset(num_examples = num_examples,
                num_features = NUM_FEATURES)
        dataset.X[:] = i
        datasets.append(dataset)

        monitor.add_dataset(dataset, 'sequential', batch_size=batch_size)

        prereq_counters.append(sharedX(0.))



    channels = []
    for i in xrange(NUM_DATASETS):
        monitor.add_channel(name = str(i),
                ipt = model.input_space.make_theano_batch(),
                val = prereq_counters[i],
                dataset = datasets[i],
                prereqs = [ ReadVerifyPrereq(i, prereq_counters[i]) ],
                data_specs=(model.get_input_space(), model.get_input_source()))

        channels.append(monitor.channels[str(i)])

    for channel in channels:
        assert len(channel.val_record) == 0
    monitor()
    for channel in channels:
        assert channel.val_record == [1]
    monitor()
    for channel in channels:
        assert channel.val_record == [1,2]

    # check that handling all these datasets did not
    # result in them getting serialized
    to_string(monitor)


def test_reject_bad_add_dataset():

    model = DummyModel(1)
    monitor = Monitor.get_monitor(model)
    dataset = DummyDataset(1,1)

    try:
        monitor.add_dataset([dataset],mode=['sequential', 'shuffled'])
    except ValueError:
        return

    raise AssertionError("Monitor.add_dataset accepted bad arguments to "
            "add_dataset.")

def test_no_data():

    # test that the right error is raised if you
    # add a channel to a monitor that has no datasets

    BATCH_SIZE = 2
    num_examples = BATCH_SIZE
    NUM_FEATURES = 3

    model = DummyModel(NUM_FEATURES)
    monitor = Monitor.get_monitor(model)

    name = 'num_prereq_calls'

    try:
        monitor.add_channel(name = name,
            ipt = model.input_space.make_theano_batch(),
            data_specs = (model.input_space, 'features'),
            val = 0.)
    except ValueError, e:
        assert exc_message(e) == _err_no_data
        return
    assert False

def test_ambig_data():

    # test that the right error is raised if you
    # add a channel to a monitor that has multiple datasets
    # and don't specify the dataset

    BATCH_SIZE = 2
    num_examples = BATCH_SIZE
    NUM_FEATURES = 3

    model = DummyModel(NUM_FEATURES)
    monitor = Monitor.get_monitor(model)

    first = DummyDataset(num_examples = num_examples,
            num_features = NUM_FEATURES)
    second = DummyDataset(num_examples = num_examples,
            num_features = NUM_FEATURES)

    monitor.add_dataset(first, 'sequential', batch_size=BATCH_SIZE)
    monitor.add_dataset(second, 'sequential', batch_size=BATCH_SIZE)


    name = 'num_prereq_calls'

    try:
        monitor.add_channel(name = name,
            ipt = model.input_space.make_theano_batch(),
            val = 0.,
            data_specs=(model.get_input_space(), model.get_input_source()))
    except ValueError, e:
        assert exc_message(e) == _err_ambig_data
        return
    assert False

def test_transfer_experience():

    # Makes sure the transfer_experience flag of push_monitor works

    model = DummyModel(num_features = 3)
    monitor = Monitor.get_monitor(model)
    monitor.report_batch(2)
    monitor.report_batch(3)
    monitor.report_epoch()
    model = push_monitor(model, "old_monitor", transfer_experience=True)
    assert model.old_monitor is monitor
    monitor = model.monitor
    assert monitor.get_epochs_seen() == 1
    assert monitor.get_batches_seen() == 2
    assert monitor.get_epochs_seen() == 1




if __name__ == '__main__':
    test_revisit()

########NEW FILE########
__FILENAME__ = test_theano
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"
from theano import tensor as T

def test_grad():
    """Tests that the theano grad method returns a list if it is passed a list
        and a single variable if it is passed a single variable.
       pylearn2 depends on theano behaving this way but theano developers have
       repeatedly changed it """

    X = T.matrix()
    y = X.sum()

    G = T.grad(y, [X])

    assert isinstance(G,list)

    G = T.grad(y, X)

    assert not isinstance(G,list)

########NEW FILE########
__FILENAME__ = test_train
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"
import numpy as np
from pylearn2.monitor import Monitor
from pylearn2.train import Train
from pylearn2.datasets.dense_design_matrix import DenseDesignMatrix
from pylearn2.models.model import Model
from pylearn2.space import VectorSpace
from pylearn2.training_algorithms.training_algorithm import TrainingAlgorithm

class DummyModel(Model):

    _params = []

    def  __init__(self, num_features):
        super(DummyModel, self).__init__()
        self.input_space = VectorSpace(num_features)

class DummyAlgorithm(TrainingAlgorithm):
    pass

def test_serialization_guard():

    # tests that Train refuses to serialize the dataset

    dim = 2
    m = 11

    rng = np.random.RandomState([28,9,2012])
    X = rng.randn(m, dim)
    dataset = DenseDesignMatrix(X=X)

    model = DummyModel(dim)
    # make the dataset part of the model, so it will get
    # serialized
    model.dataset = dataset

    Monitor.get_monitor(model)

    algorithm = DummyAlgorithm()

    train = Train(dataset, model, algorithm, save_path='_tmp_unit_test.pkl',
                 save_freq=1, extensions=None)

    try:
        train.main_loop()
    except RuntimeError:
        return
    assert False # train did not complain, this is a bug

########NEW FILE########
__FILENAME__ = train
"""Module containing the Train class and support functionality."""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"
from datetime import datetime
import os
import sys
import logging
import warnings
from pylearn2.utils import serial
from pylearn2.utils.string_utils import preprocess
from pylearn2.monitor import Monitor
from pylearn2.space import NullSpace
from pylearn2.utils.timing import log_timing, total_seconds
from pylearn2.utils import sharedX


log = logging.getLogger(__name__)


class Train(object):
    """
    A class representing the main loop of the training script.  Trains the
    specified model using the specified algorithm on the specified dataset.
    After each call to the training algorithm, the model is saved to
    `save_path`. May be enhanced with `TrainExtension` plugins.

    Parameters
    ----------
    dataset : `pylearn2.datasets.dataset.Dataset`
    model : `pylearn2.models.model.Model`
    algorithm : \
        `pylearn2.training_algorithms.training_algorithm.TrainingAlgorithm`, \
        optional
    save_path : str, optional
        Path to save (with pickle / joblib) the model.
    save_freq : int, optional
        Frequency of saves, in epochs. A frequency of zero disables
        automatic saving altogether. A frequency of 1 saves every
        epoch. A frequency of 2 saves every other epoch, etc.
        (default=0, i.e. never save). Note: when automatic saving is
        enabled (eg save_freq > 0), the model is always saved after
        learning, even when the final epoch is not a multiple of
        `save_freq`.
    extensions : iterable, optional
        A collection of `TrainExtension` objects whose callbacks are
        triggered at various points in learning.
    allow_overwrite : bool, optional
        If `True`, will save the model to save_path even if there is
        already something there. Otherwise, will raise an error if the
        `save_path` is already occupied.
    """

    def __init__(self, dataset, model, algorithm=None, save_path=None,
                 save_freq=0, extensions=None, allow_overwrite=True):
        self.allow_overwrite = allow_overwrite
        self.first_save = True
        self.dataset = dataset
        self.model = model
        self.algorithm = algorithm
        if save_path is not None:
            if save_freq == 0:
                warnings.warn('save_path specified but save_freq is 0 '
                              '(never save). Is this intentional?')
            self.save_path = preprocess(save_path)
        else:
            if save_freq > 0:
                phase_variable = 'PYLEARN2_TRAIN_PHASE'
                if phase_variable in os.environ:
                    phase = 'phase%d' % os.environ[phase_variable]
                    tokens = [os.environ['PYLEARN2_TRAIN_FILE_FULL_STEM'],
                              phase, 'pkl']
                else:
                    tokens = os.environ['PYLEARN2_TRAIN_FILE_FULL_STEM'], 'pkl'
                self.save_path = '.'.join(tokens)
        self.save_freq = save_freq

        if hasattr(self.dataset, 'yaml_src'):
            self.model.dataset_yaml_src = self.dataset.yaml_src
        else:
            warnings.warn("dataset has no yaml src, model won't know what " +
                          "data it was trained on")

        self.extensions = extensions if extensions is not None else []
        self.training_seconds = sharedX(value=0,
                                        name='training_seconds_this_epoch')
        self.total_seconds = sharedX(value=0, name='total_seconds_last_epoch')

    def setup_extensions(self):
        """ Calls setup on all extensions."""
        for ext in self.extensions:
            ext.setup(self.model, self.dataset, self.algorithm)

    def exceeded_time_budget(self, t0, time_budget):
        """
        .. todo::

            WRITEME
        """
        dt = total_seconds(datetime.now() - t0)
        if time_budget is not None and dt >= time_budget:
            log.warning("Time budget exceeded (%.3f/%d seconds).",
                        dt, time_budget)
            self.model.monitor.time_budget_exceeded = True
            return True
        else:
            return False


    def setup(self):
        """
        Sets up the main loop. This is also called at the start of the
        main loop, so you need only call it if you're using a driver
        script that replaces the main loop with something else.
        """
        self.model.monitor = Monitor.get_monitor(self.model)
        self.model.monitor.time_budget_exceeded = False
        if self.algorithm is not None:
            self.algorithm.setup(model=self.model, dataset=self.dataset)
        self.setup_extensions()

        # Model.censor_updates is used by the training algorithm to
        # enforce constraints after each step of learning. Here we
        # make sure the constraints are enforced from the start.
        self.model.enforce_constraints()

    def main_loop(self, time_budget=None):
        """
        Repeatedly runs an epoch of the training algorithm, runs any
        epoch-level callbacks, and saves the model.

        Parameters
        ----------
        time_budget : int, optional
            The maximum number of seconds before interrupting
            training. Default is `None`, no time limit.
        """
        t0 = datetime.now()
        self.setup()
        if self.algorithm is None:
            self.run_callbacks_and_monitoring()
            while True:
                if self.exceeded_time_budget(t0, time_budget):
                    break

                rval = self.model.train_all(dataset=self.dataset)
                if rval is not None:
                    raise ValueError("Model.train_all should not return " +
                                     "anything. Use Model.continue_learning " +
                                     "to control whether learning continues.")
                self.model.monitor.report_epoch()
                extension_continue = self.run_callbacks_and_monitoring()
                freq = self.save_freq
                if freq > 0 and self.model.monitor.get_epochs_seen() % freq == 0:
                    self.save()
                continue_learning = (self.model.continue_learning() and
                                     extension_continue)
                assert continue_learning in [True, False, 0, 1]
                if not continue_learning:
                    break
        else:
            if not hasattr(self.model, 'monitor'):
                # TODO: is this really necessary? I just put this error here
                # to prevent an AttributeError later, but I think we could
                # rewrite to avoid the AttributeError
                raise RuntimeError("The algorithm is responsible for setting"
                                   " up the Monitor, but failed to.")
            if len(self.model.monitor._datasets) > 0:
                # This monitoring channel keeps track of a shared variable,
                # which does not need inputs nor data.
                self.training_seconds.__doc__ = """\
The number of seconds that were spent in actual training during the most
recent epoch. This excludes seconds that were spent running callbacks for
the extensions, computing monitoring channels, etc."""
                self.model.monitor.add_channel(
                    name="training_seconds_this_epoch",
                    ipt=None,
                    val=self.training_seconds,
                    data_specs=(NullSpace(), ''),
                    dataset=self.model.monitor._datasets[0])
                self.total_seconds.__doc__ = """\
The number of seconds that were spent on the entirety of processing for the
previous epoch. This includes not only training but also the computation of
the monitoring channels, running TrainExtension callbacks, etc. This value
is reported for the *previous* epoch because the amount of time spent on
monitoring for this epoch is not known until the monitoring channels have
already been reported."""
                self.model.monitor.add_channel(
                    name="total_seconds_last_epoch",
                    ipt=None,
                    val=self.total_seconds,
                    data_specs=(NullSpace(), ''),
                    dataset=self.model.monitor._datasets[0])
            self.run_callbacks_and_monitoring()
            while True:
                if self.exceeded_time_budget(t0, time_budget):
                    break

                with log_timing(log, None, level=logging.DEBUG,
                                callbacks=[self.total_seconds.set_value]):
                    with log_timing(
                            log, None, final_msg='Time this epoch:',
                            callbacks=[self.training_seconds.set_value]):
                        rval = self.algorithm.train(dataset=self.dataset)
                    if rval is not None:
                        raise ValueError("TrainingAlgorithm.train should not "
                                         "return anything. Use "
                                         "TrainingAlgorithm.continue_learning "
                                         "to control whether learning "
                                         "continues.")
                    self.model.monitor.report_epoch()
                    extension_continue = self.run_callbacks_and_monitoring()
                    if self.save_freq > 0 and \
                       self.model.monitor.get_epochs_seen() % self.save_freq == 0:
                        self.save()
                continue_learning = (
                    self.algorithm.continue_learning(self.model) and
                    extension_continue
                )
                assert continue_learning in [True, False, 0, 1]
                if not continue_learning:
                    break

        self.model.monitor.training_succeeded = True

        if self.save_freq > 0:
            self.save()

    def run_callbacks_and_monitoring(self):
        """
        Runs the monitor, then calls Extension.on_monitor for all extensions.

        Returns
        -------
        continue_learning : bool
            If `False`, signals that at least one train
            extension wants to stop learning.
        """
        self.model.monitor()
        continue_learning = True
        for extension in self.extensions:
            try:
                extension.on_monitor(self.model, self.dataset, self.algorithm)
            except TypeError:
                logging.warning('Failure during callback ' + str(extension))
                raise
            # We catch an exception here instead of relying on return
            # values for backward compatibility. Lots of extensions
            # exist that don't return anything, currently.
            except StopIteration:
                log.info("Extension requested training halt.")
                continue_learning = False
        return continue_learning

    def save(self):
        """Saves the model."""
        #TODO-- save state of training algorithm so training can be
        # resumed after a crash
        for extension in self.extensions:
            extension.on_save(self.model, self.dataset, self.algorithm)
        if self.save_path is not None:
            with log_timing(log, 'Saving to ' + self.save_path):
                if self.first_save and (not self.allow_overwrite) \
                   and os.path.exists(self.save_path):
                    # Every job overwrites its own output on the second save
                    # and every save thereafter. The "allow_overwrite" flag
                    # only pertains to overwriting the output of previous jobs.
                    raise IOError("Trying to overwrite file when not allowed.")
                try:
                    # Make sure that saving does not serialize the dataset
                    self.dataset._serialization_guard = SerializationGuard()
                    serial.save(self.save_path, self.model,
                                on_overwrite='backup')
                finally:
                    self.dataset._serialization_guard = None
            self.first_save = False


class SerializationGuard(object):
    """
    This class exists to make objects that cannot be serialized. It is used to
    make sure you don't accidentally put pointers to objects that should not
    be serialized, such as the dataset, into objects that Train automatically
    serializes, such as the Model.
    """

    def __getstate__(self):
        """
        This method is called when someone attempts to serialize the object.
        This method raises an exception to prevent the serialization from
        occurring.
        """
        raise IOError("You tried to serialize something that should not"
                      " be serialized.")


if __name__ == "__main__":
    log.error("You probably meant to run scripts/train.py")
    sys.exit(1)

########NEW FILE########
__FILENAME__ = bgd
"""
Module for performing batch gradient methods.
Technically, SGD and BGD both work with any batch size, but SGD has no line
search functionality and is thus best suited to small batches, while BGD
supports line searches and thuse works best with large batches.
"""
__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow"]
__license__ = "3-clause BSD"
__maintainer__ = "LISA Lab"
__email__ = "pylearn-dev@googlegroups"

import logging
import numpy as np
from theano import config
from theano.compat.python2x import OrderedDict

from pylearn2.monitor import Monitor
from pylearn2.optimization.batch_gradient_descent import BatchGradientDescent
from pylearn2.utils.iteration import is_stochastic
from pylearn2.training_algorithms.training_algorithm import TrainingAlgorithm
from pylearn2.utils import safe_zip
from pylearn2.train_extensions import TrainExtension
from pylearn2.termination_criteria import TerminationCriterion
from pylearn2.utils import sharedX
from pylearn2.space import CompositeSpace, NullSpace
from pylearn2.utils.data_specs import DataSpecsMapping
from pylearn2.utils.rng import make_np_rng


logger = logging.getLogger(__name__)


class BGD(TrainingAlgorithm):
    """
    Batch Gradient Descent training algorithm class

    Parameters
    ----------
    cost : pylearn2.costs.Cost, optional
        A pylearn2 Cost, or None, in which case model.get_default_cost()
        will be used
    batch_size : int, optional
        Like the SGD TrainingAlgorithm, this TrainingAlgorithm still
        iterates over minibatches of data. The difference is that this
        class uses partial line searches to choose the step size along
        each gradient direction, and can do repeated updates on the same
        batch. The assumption is that you use big enough minibatches with
        this algorithm that a large step size will generalize reasonably
        well to other minibatches. To implement true Batch Gradient
        Descent, set the batch_size to the total number of examples
        available. If batch_size is None, it will revert to the model's
        force_batch_size attribute.
    batches_per_iter : int, optional
        WRITEME
    updates_per_batch : int, optional
        Passed through to the optimization.BatchGradientDescent's
        `max_iters parameter`
    monitoring_batch_size : int
        Size of monitoring batches.
    monitoring_batches : WRITEME
    monitoring_dataset : Dataset or dict, optional
        A Dataset or a dictionary mapping string dataset names to Datasets
    termination_criterion : WRITEME
    set_batch_size : bool, optional
        If True, BGD will attempt to override the model's
        `force_batch_size` attribute by calling set_batch_size on it.
    reset_alpha : bool, optional
        Passed through to the optimization.BatchGradientDescent's
        `reset_alpha` parameter
    conjugate : bool, optional
        Passed through to the optimization.BatchGradientDescent's
        `conjugate` parameter
    min_init_alpha : float, optional
        WRITEME
    reset_conjugate : bool, optional
        Passed through to the optimization.BatchGradientDescent's
        `reset_conjugate` parameter
    line_search_mode : WRITEME
    verbose_optimization : bool, optional
        WRITEME
    scale_step : float, optional
        WRITEME
    theano_function_mode : WRITEME
    init_alpha : WRITEME
    seed : WRITEME
    """
    def __init__(self, cost=None, batch_size=None, batches_per_iter=None,
                 updates_per_batch=10, monitoring_batch_size=None,
                 monitoring_batches=None, monitoring_dataset=None,
                 termination_criterion=None, set_batch_size=False,
                 reset_alpha=True, conjugate=False, min_init_alpha=.001,
                 reset_conjugate=True, line_search_mode=None,
                 verbose_optimization=False, scale_step=1.,
                 theano_function_mode=None, init_alpha=None, seed=None):

        self.__dict__.update(locals())
        del self.self

        if monitoring_dataset is None:
            assert monitoring_batches is None
            assert monitoring_batch_size is None

        self._set_monitoring_dataset(monitoring_dataset)

        self.bSetup = False
        self.termination_criterion = termination_criterion
        self.rng = make_np_rng(seed, [2012, 10, 16],
                which_method=["randn","randint"])

    def setup(self, model, dataset):
        """
        Allows the training algorithm to do some preliminary configuration
        *before* we actually start training the model. The dataset is provided
        in case other derived training algorithms need to modify model based on
        the dataset.

        Parameters
        ----------
        model : object
            A Python object representing the model to train. Loosely
            implementing the interface of models.model.Model.
        dataset : pylearn2.datasets.dataset.Dataset
            Dataset object used to draw training data
        """
        self.model = model

        if self.cost is None:
            self.cost = model.get_default_cost()

        if self.batch_size is None:
            self.batch_size = model.force_batch_size
        else:
            batch_size = self.batch_size
            if self.set_batch_size:
                model.set_batch_size(batch_size)
            elif hasattr(model, 'force_batch_size'):
                if not (model.force_batch_size <= 0 or batch_size ==
                        model.force_batch_size):
                    raise ValueError("batch_size is %d but " +
                                     "model.force_batch_size is %d" %
                                     (batch_size, model.force_batch_size))

        self.monitor = Monitor.get_monitor(model)
        self.monitor.set_theano_function_mode(self.theano_function_mode)

        data_specs = self.cost.get_data_specs(model)
        mapping = DataSpecsMapping(data_specs)
        space_tuple = mapping.flatten(data_specs[0], return_tuple=True)
        source_tuple = mapping.flatten(data_specs[1], return_tuple=True)

        # Build a flat tuple of Theano Variables, one for each space,
        # named according to the sources.
        theano_args = []
        for space, source in safe_zip(space_tuple, source_tuple):
            name = 'BGD_[%s]' % source
            arg = space.make_theano_batch(name=name)
            theano_args.append(arg)
        theano_args = tuple(theano_args)

        # Methods of `self.cost` need args to be passed in a format compatible
        # with their data_specs
        nested_args = mapping.nest(theano_args)
        fixed_var_descr = self.cost.get_fixed_var_descr(model, nested_args)
        self.on_load_batch = fixed_var_descr.on_load_batch

        cost_value = self.cost.expr(model, nested_args,
                                    ** fixed_var_descr.fixed_vars)
        grads, grad_updates = self.cost.get_gradients(
                model, nested_args, ** fixed_var_descr.fixed_vars)

        assert isinstance(grads, OrderedDict)
        assert isinstance(grad_updates, OrderedDict)

        if cost_value is None:
            raise ValueError("BGD is incompatible with " + str(self.cost) +
                             " because it is intractable, but BGD uses the " +
                             "cost function value to do line searches.")

        # obj_prereqs has to be a list of function f called with f(*data),
        # where data is a data tuple coming from the iterator.
        # this function enables capturing "mapping" and "f", while
        # enabling the "*data" syntax
        def capture(f, mapping=mapping):
            new_f = lambda *args: f(mapping.flatten(args, return_tuple=True))
            return new_f

        obj_prereqs = [capture(f) for f in fixed_var_descr.on_load_batch]

        if self.monitoring_dataset is not None:
            if (self.monitoring_batch_size is None and
                    self.monitoring_batches is None):
                self.monitoring_batch_size = self.batch_size
                self.monitoring_batches = self.batches_per_iter
            self.monitor.setup(
                    dataset=self.monitoring_dataset,
                    cost=self.cost,
                    batch_size=self.monitoring_batch_size,
                    num_batches=self.monitoring_batches,
                    obj_prereqs=obj_prereqs,
                    cost_monitoring_args=fixed_var_descr.fixed_vars)

        params = model.get_params()


        self.optimizer = BatchGradientDescent(
                            objective = cost_value,
                            gradients = grads,
                            gradient_updates = grad_updates,
                            params = params,
                            param_constrainers = [ model.modify_updates ],
                            lr_scalers = model.get_lr_scalers(),
                            inputs = theano_args,
                            verbose = self.verbose_optimization,
                            max_iter = self.updates_per_batch,
                            reset_alpha = self.reset_alpha,
                            conjugate = self.conjugate,
                            reset_conjugate = self.reset_conjugate,
                            min_init_alpha = self.min_init_alpha,
                            line_search_mode = self.line_search_mode,
                            theano_function_mode=self.theano_function_mode,
                            init_alpha=self.init_alpha)

        # These monitoring channels keep track of shared variables,
        # which do not need inputs nor data.
        if self.monitoring_dataset is not None:
            self.monitor.add_channel(
                    name='ave_step_size',
                    ipt=None,
                    val=self.optimizer.ave_step_size,
                    data_specs=(NullSpace(), ''),
                    dataset=self.monitoring_dataset.values()[0])
            self.monitor.add_channel(
                    name='ave_grad_size',
                    ipt=None,
                    val=self.optimizer.ave_grad_size,
                    data_specs=(NullSpace(), ''),
                    dataset=self.monitoring_dataset.values()[0])
            self.monitor.add_channel(
                    name='ave_grad_mult',
                    ipt=None,
                    val=self.optimizer.ave_grad_mult,
                    data_specs=(NullSpace(), ''),
                    dataset=self.monitoring_dataset.values()[0])

        self.first = True
        self.bSetup = True

    def train(self, dataset):
        """
        .. todo::

            WRITEME
        """
        assert self.bSetup
        model = self.model

        rng = self.rng
        train_iteration_mode = 'shuffled_sequential'
        if not is_stochastic(train_iteration_mode):
            rng = None

        data_specs = self.cost.get_data_specs(self.model)
        # The iterator should be built from flat data specs, so it returns
        # flat, non-redundent tuples of data.
        mapping = DataSpecsMapping(data_specs)
        space_tuple = mapping.flatten(data_specs[0], return_tuple=True)
        source_tuple = mapping.flatten(data_specs[1], return_tuple=True)
        if len(space_tuple) == 0:
            # No data will be returned by the iterator, and it is impossible
            # to know the size of the actual batch.
            # It is not decided yet what the right thing to do should be.
            raise NotImplementedError("Unable to train with BGD, because "
                    "the cost does not actually use data from the data set. "
                    "data_specs: %s" % str(data_specs))
        flat_data_specs = (CompositeSpace(space_tuple), source_tuple)

        iterator = dataset.iterator(mode=train_iteration_mode,
                batch_size=self.batch_size,
                num_batches=self.batches_per_iter,
                data_specs=flat_data_specs, return_tuple=True,
                rng = rng)

        mode = self.theano_function_mode
        for data in iterator:
            if ('targets' in source_tuple and mode is not None
                    and hasattr(mode, 'record')):
                Y = data[source_tuple.index('targets')]
                stry = str(Y).replace('\n',' ')
                mode.record.handle_line('data Y '+stry+'\n')

            for on_load_batch in self.on_load_batch:
                on_load_batch(mapping.nest(data))

            self.before_step(model)
            self.optimizer.minimize(*data)
            self.after_step(model)
            actual_batch_size = flat_data_specs[0].np_batch_size(data)
            model.monitor.report_batch(actual_batch_size)

    def continue_learning(self, model):
        """
        .. todo::

            WRITEME
        """
        if self.termination_criterion is None:
            return True
        else:
            rval = self.termination_criterion.continue_learning(self.model)
            assert rval in [True, False, 0, 1]
            return rval

    def before_step(self, model):
        """
        .. todo::

            WRITEME
        """
        if self.scale_step != 1.:
            self.params = list(model.get_params())
            self.value = [ param.get_value() for param in self.params ]

    def after_step(self, model):
        """
        .. todo::

            WRITEME
        """
        if self.scale_step != 1:
            for param, value in safe_zip(self.params, self.value):
                value = (1.-self.scale_step) * value + self.scale_step \
                        * param.get_value()
                param.set_value(value)

class StepShrinker(TrainExtension, TerminationCriterion):
    """
    .. todo::

        WRITEME
    """

    def __init__(self, channel, scale, giveup_after, scale_up=1.,
            max_scale=1.):
        self.__dict__.update(locals())
        del self.self
        self.continue_learning = True
        self.first = True
        self.prev = np.inf

    def on_monitor(self, model, dataset, algorithm):
        """
        .. todo::

            WRITEME
        """
        monitor = model.monitor

        if self.first:
            self.first = False
            self.monitor_channel = sharedX(algorithm.scale_step)
            # TODO: make monitor accept channels not associated with any
            # dataset,
            # so this hack won't be necessary
            hack = monitor.channels.values()[0]
            monitor.add_channel('scale_step', hack.graph_input,
                    self.monitor_channel, dataset=hack.dataset,
                    data_specs=hack.data_specs)
        channel = monitor.channels[self.channel]
        v = channel.val_record
        if len(v) == 1:
            return
        latest = v[-1]
        logger.info("Latest {0}: {1}".format(self.channel, latest))
        # Only compare to the previous step, not the best step so far
        # Another extension can be in charge of saving the best parameters ever
        # seen.We want to keep learning as long as we're making progress. We
        # don't want to give up on a step size just because it failed to undo
        # the damage of the bigger one that preceded it in a single epoch
        logger.info("Previous is {0}".format(self.prev))
        cur = algorithm.scale_step
        if latest >= self.prev:
            logger.info("Looks like using {0} "
                        "isn't working out so great for us.".format(cur))
            cur *= self.scale
            if cur < self.giveup_after:
                logger.info("Guess we just have to give up.")
                self.continue_learning = False
                cur = self.giveup_after
            logger.info("Let's see how {0} does.".format(cur))
        elif latest <= self.prev and self.scale_up != 1.:
            logger.info("Looks like we're making progress "
                        "on the validation set, let's try speeding up")
            cur *= self.scale_up
            if cur > self.max_scale:
                cur = self.max_scale
            logger.info("New scale is {0}".format(cur))
        algorithm.scale_step = cur
        self.monitor_channel.set_value(np.cast[config.floatX](cur))
        self.prev = latest


    def __call__(self, model):
        """
        .. todo::

            WRITEME
        """
        return self.continue_learning

class ScaleStep(TrainExtension):
    """
    .. todo::

        WRITEME
    """

    def __init__(self, scale, min_value):
        self.scale = scale
        self.min_value = min_value
        self.first = True

    def on_monitor(self, model, dataset, algorithm):
        """
        .. todo::

            WRITEME
        """
        if self.first:
            monitor = model.monitor
            self.first = False
            self.monitor_channel = sharedX(algorithm.scale_step)
            # TODO: make monitor accept channels not associated with any
            # dataset,
            # so this hack won't be necessary
            hack = monitor.channels.values()[0]
            monitor.add_channel('scale_step', hack.graph_input,
                                self.monitor_channel, dataset=hack.dataset)
        cur = algorithm.scale_step
        cur *= self.scale
        cur = max(cur, self.min_value)
        algorithm.scale_step = cur
        self.monitor_channel.set_value(np.cast[config.floatX](cur))

class BacktrackingStepShrinker(TrainExtension, TerminationCriterion):
    """
    .. todo::

        WRITEME
    """

    def __init__(self, channel, scale, giveup_after, scale_up=1.,
            max_scale=1.):
        self.__dict__.update(locals())
        del self.self
        self.continue_learning = True
        self.first = True
        self.prev = np.inf

    def on_monitor(self, model, dataset, algorithm):
        """
        .. todo::

            WRITEME
        """
        monitor = model.monitor

        if self.first:
            self.first = False
            self.monitor_channel = sharedX(algorithm.scale_step)
            # TODO: make monitor accept channels not associated with any
            # dataset,
            # so this hack won't be necessary
            hack = monitor.channels.values()[0]
            monitor.add_channel('scale_step', hack.graph_input,
                                self.monitor_channel, dataset=hack.dataset)
        channel = monitor.channels[self.channel]
        v = channel.val_record
        if len(v) == 1:
            return
        latest = v[-1]
        logger.info("Latest {0}: {1}".format(self.channel, latest))
        # Only compare to the previous step, not the best step so far
        # Another extension can be in charge of saving the best parameters ever
        # seen.We want to keep learning as long as we're making progress. We
        # don't want to give up on a step size just because it failed to undo
        # the damage of the bigger one that preceded it in a single epoch
        logger.info("Previous is {0}".format(self.prev))
        cur = algorithm.scale_step
        if latest >= self.prev:
            logger.info("Looks like using {0} "
                        "isn't working out so great for us.".format(cur))
            cur *= self.scale
            if cur < self.giveup_after:
                logger.info("Guess we just have to give up.")
                self.continue_learning = False
                cur = self.giveup_after
            logger.info("Let's see how {0} does.".format(cur))
            logger.info("Reloading saved params from last call")
            for p, v in safe_zip(model.get_params(), self.stored_values):
                p.set_value(v)
            latest = self.prev
        elif latest <= self.prev and self.scale_up != 1.:
            logger.info("Looks like we're making progress "
                        "on the validation set, let's try speeding up")
            cur *= self.scale_up
            if cur > self.max_scale:
                cur = self.max_scale
            logger.info("New scale is {0}".format(cur))
        algorithm.scale_step = cur
        self.monitor_channel.set_value(np.cast[config.floatX](cur))
        self.prev = latest
        self.stored_values = [param.get_value() for param in
                model.get_params()]


    def __call__(self, model):
        """
        .. todo::

            WRITEME
        """
        return self.continue_learning

########NEW FILE########
__FILENAME__ = default
"""
A generic training algorithm that implements no real training code of its
own but just calls the model.train_batch method on minibatches of data.
"""
import functools

from pylearn2.monitor import Monitor
from pylearn2.training_algorithms.training_algorithm import TrainingAlgorithm
from pylearn2.utils import safe_zip
from pylearn2.utils.data_specs import DataSpecsMapping

class DefaultTrainingAlgorithm(TrainingAlgorithm):
    """
    A generic training algorithm that implements no real training code of its
    own but just calls the model.train_batch method on minibatches of data.

    Parameters
    ----------
    batch_size : int, optional
        If batch_size is None, reverts to the `force_batch_size` field of
        the model
    batches_per_iter : int, optional
        WRITEME
    monitoring_batch_size : int, optional
        Size of monitoring batches.
    monitoring_batches : int, optional
        WRITEME
    monitoring_dataset : Dataset or dict, optional
        A Dataset or a dictionary mapping string dataset names to Datasets
    termination_criterion : WRITEME
        If specified, can cause the algorithm to terminate before
        `model.learn_batch` says to
    set_batch_size : bool, optional
        If True, if `model` has a batch size but is not forced to use that
        one, the training algorithm will set the model to use `batch_size`
        instead.
    """

    def __init__(self, batch_size=None, batches_per_iter=1000,
                 monitoring_batch_size=None, monitoring_batches=-1,
                 monitoring_dataset=None, termination_criterion=None,
                 set_batch_size=False):
        self.__dict__.update(locals())
        del self.self
        if monitoring_dataset is None:
            assert monitoring_batches == -1
            assert monitoring_batch_size is None

        self._set_monitoring_dataset(monitoring_dataset)
        self.monitoring_batches = monitoring_batches
        self.bSetup = False
        self.termination_criterion = termination_criterion

    def setup(self, model, dataset):
        """
        Allows the training algorithm to do some preliminary configuration
        *before* we actually start training the model. The dataset is provided
        in case other derived training algorithms need to modify model based on
        the dataset.

        Parameters
        ----------
        model : object
            Python object representing the model to train loosely
            implementing the interface of models.model.Model.

        dataset : pylearn2.datasets.dataset.Dataset
            Dataset object used to draw training data
        """
        self._synchronize_batch_size(model)

        self.model = model

        self.monitor = Monitor.get_monitor(model)

        if self.monitoring_dataset is not None:
            # Get the data specifications needed by the model
            space, source = model.get_monitoring_data_specs()

            # Create Theano variables for each of the individual components
            # of that data. Usually, it will be X for inputs and Y for targets.
            # First, we need to find these components, and put them in a tuple
            mapping = DataSpecsMapping((space, source))
            space_tuple = mapping.flatten(space, return_tuple=True)
            source_tuple = mapping.flatten(source, return_tuple=True)
            # Then, build a flat tuple of these Theano variables
            ipt = tuple(sp.make_theano_batch(name='monitor_%s' % src)
                        for (sp, src) in safe_zip(space_tuple, source_tuple))
            # Finally, organize them back into a structure expected by the
            # monitoring channels of the model
            nested_ipt = mapping.nest(ipt)

            channels = model.get_monitoring_channels(nested_ipt)
            if not isinstance(channels, dict):
                raise TypeError("model.get_monitoring_channels must return a "
                                "dictionary, but it returned " + str(channels))

            for dataset_name in self.monitoring_dataset:
                if dataset_name == '':
                    prefix = ''
                else:
                    prefix = dataset_name + '_'
                monitoring_dataset = self.monitoring_dataset[dataset_name]

                if (self.monitoring_batch_size is None and
                        self.monitoring_batches == -1):
                    self.monitoring_batch_size = self.batch_size
                    self.monitoring_batches = self.batches_per_iter
                self.monitor.add_dataset(dataset=monitoring_dataset,
                                         mode="sequential",
                                         batch_size=self.monitoring_batch_size,
                                         num_batches=self.monitoring_batches)

                for name in channels:
                    J = channels[name]
                    if isinstance(J, tuple):
                        assert len(J) == 2
                        J, prereqs = J
                    else:
                        prereqs = None

                    self.monitor.add_channel(name=prefix + name,
                                             ipt=nested_ipt,
                                             val=J,
                                             dataset=monitoring_dataset,
                                             prereqs=prereqs,
                                             data_specs=(space, source))

        self.first = True
        self.bSetup = True

    @functools.wraps(TrainingAlgorithm.train)
    def train(self, dataset):
        assert self.bSetup
        model = self.model
        batch_size = self.batch_size

        for i in xrange(self.batches_per_iter):
            # model.train_batch and self.train both return False when training
            # should terminate.
            learn_more = model.train_batch(dataset, batch_size)
            model.monitor.report_batch(batch_size)
            if not learn_more:
                break

        # Make sure we didn't exit training loop because Model.learn
        # hasn't been updated to new interface yet.
        if learn_more not in [True, False]:
            msg = ('The learn method of model %s did not return a boolean ' +
                   'value. Please update your model accordingly.')
            raise ValueError(msg % str(model))
        self.learn_more = learn_more

    def continue_learning(self, model):
        """
        .. todo::

            WRITEME
        """
        if self.learn_more:
            if self.termination_criterion is not None:
                return self.termination_criterion.continue_learning(model)
            return True
        return False

########NEW FILE########
__FILENAME__ = learning_rule
"""
A module containing different learning rules for use with the SGD training
algorithm.
"""
import numpy as np

from theano import config
from theano import tensor as T

from theano.compat.python2x import OrderedDict
from pylearn2.space import NullSpace
from pylearn2.train_extensions import TrainExtension
from pylearn2.utils import sharedX


class LearningRule():
    """
    A pylearn2 learning rule is an object which computes new parameter values
    given (1) a learning rate (2) current parameter values and (3) the current
    estimated gradient.
    """

    def add_channels_to_monitor(self, monitor, monitoring_dataset):
        """
        Method called by the training algorithm, which allows LearningRules to
        add monitoring channels.

        Parameters
        ----------
        monitor : pylearn2.monitor.Monitor
            Monitor object, to which the rule should register additional
            monitoring channels.
        monitoring_dataset : pylearn2.datasets.dataset.Dataset or dict
            Dataset instance or dictionary whose values are Dataset objects.
        """
        raise NotImplementedError()

    def get_updates(self, learning_rate, grads, lr_scalers=None):
        """
        Provides the symbolic (theano) description of the updates needed to
        perform this learning rule.

        Parameters
        ----------
        learning_rate : float
            Learning rate coefficient.
        grads : dict
            A dictionary mapping from the model's parameters to their
            gradients.
        lr_scalers : dict
            A dictionary mapping from the model's parameters to a learning
            rate multiplier.

        Returns
        -------
        updates : OrderdDict
            A dictionary mapping from the old model parameters, to their new
            values after a single iteration of the learning rule.

        Notes
        -----
        e.g. for standard SGD, one would return `sgd_rule_updates` defined
        below. Note that such a `LearningRule` object is not implemented, as
        these updates are implemented by default when the `learning_rule`
        parameter of sgd.SGD.__init__ is None.

        .. code-block::  python

            sgd_rule_updates = OrderedDict()
            for (param, grad) in grads.iteritems():
                sgd_rule_updates[k] = (param - learning_rate *
                                       lr_scalers.get(param, 1.) * grad)
        """
        raise NotImplementedError(str(type(self)) + " does not implement "
                "get_updates.")


class Momentum(LearningRule):
    """
    Implements momentum as described in Section 9 of
    "A Practical Guide to Training Restricted Boltzmann Machines",
    Geoffrey Hinton.

    Parameters are updated by the formula:
    inc := momentum * inc - learning_rate * d cost / d param
    param := param + inc

    Parameters
    ----------
    init_momentum : float
        Initial value for the momentum coefficient. It remains fixed during
        training unless used with a `training_algorithms.sgd.MomentumAdjustor`
        extension.
    """

    def __init__(self, init_momentum):
        assert init_momentum >= 0.
        assert init_momentum < 1.
        self.momentum = sharedX(init_momentum, 'momentum')

    def add_channels_to_monitor(self, monitor, monitoring_dataset):
        """Activates monitoring of the momentum."""
        monitor.add_channel(
            name='momentum',
            ipt=None,
            val=self.momentum,
            data_specs=(NullSpace(), ''),
            dataset=monitoring_dataset)

    def get_updates(self, learning_rate, grads, lr_scalers=None):
        """
        Provides the updates for learning with gradient descent + momentum.
        """

        updates = OrderedDict()

        for (param, grad) in grads.iteritems():
            inc = sharedX(param.get_value() * 0.)
            assert param.dtype == inc.dtype
            assert grad.dtype == param.dtype
            if param.name is not None:
                inc.name = 'inc_'+param.name
            updated_inc = self.momentum * inc -\
                learning_rate * lr_scalers.get(param, 1.) * grad
            assert updated_inc.dtype == inc.dtype
            updates[inc] = updated_inc
            updates[param] = param + updated_inc

        return updates


class MomentumAdjustor(TrainExtension):
    """
    A TrainExtension that implements a linear momentum schedule.

    Parameters
    ----------
    final_momentum : float
        The momentum coefficient to use at the end of learning.
    start : int
        The epoch on which to start growing the momentum coefficient.
    saturate : int
        The epoch on which the moment should reach its final value.
    """
    def __init__(self, final_momentum, start, saturate):
        if saturate < start:
            raise TypeError("Momentum can't saturate at its maximum value " +
                            "before it starts increasing.")

        self.__dict__.update(locals())
        del self.self
        self._initialized = False
        self._count = 0

    def on_monitor(self, model, dataset, algorithm):
        """Updates the momentum according to the linear schedule."""
        if hasattr(algorithm, 'learning_rule'):
            momentum = algorithm.learning_rule.momentum
        else:
            # TODO: remove once training_algorithm.sgd.SGD(init_momentum)
            # is officially deprecated.
            momentum = algorithm.momentum

        if not self._initialized:
            self._init_momentum = momentum.get_value()
            self._initialized = True
        self._count += 1
        momentum.set_value(np.cast[config.floatX](self.current_momentum()))

    def current_momentum(self):
        """Returns the momentum currently desired by the schedule."""
        w = self.saturate - self.start

        if w == 0:
            # saturate=start, so just jump straight to final momentum
            if self._count >= self.start:
                return self.final_momentum
            return self._init_momentum

        alpha = float(self._count - self.start) / float(w)
        if alpha < 0.:
            alpha = 0.
        if alpha > 1.:
            alpha = 1.
        return self._init_momentum * (1.-alpha)+alpha*self.final_momentum


class AdaDelta(LearningRule):
    """
    Implements the AdaDelta learning rule as described in:
    "AdaDelta: An Adaptive Learning Rate Method", Matthew D. Zeiler.

    Parameters
    ----------
    decay : float, optional
        Decay rate :math:`\\rho` in Algorithm 1 of the aforementioned
        paper.
    """

    def __init__(self, decay=0.95):
        assert decay >= 0.
        assert decay < 1.
        self.decay = decay

    def add_channels_to_monitor(self, monitor, monitoring_dataset):
        """
        .. todo::

            WRITEME
        """
        # TODO: add channels worth monitoring
        return

    def get_updates(self, learning_rate, grads, lr_scalers=None):
        """
        .. todo::

            WRITEME
        """
        updates = OrderedDict()
        for param in grads.keys():

            # mean_squared_grad := E[g^2]_{t-1}
            mean_square_grad = sharedX(param.get_value() * 0.)
            # mean_square_dx := E[(\Delta x)^2]_{t-1}
            mean_square_dx = sharedX(param.get_value() * 0.)

            if param.name is not None:
                mean_square_grad.name = 'mean_square_grad_' + param.name
                mean_square_dx.name = 'mean_square_dx_' + param.name

            # Accumulate gradient
            new_mean_squared_grad = \
                    self.decay * mean_square_grad +\
                    (1 - self.decay) * T.sqr(grads[param])

            # Compute update
            epsilon = lr_scalers.get(param, 1.) * learning_rate
            rms_dx_tm1 = T.sqrt(mean_square_dx + epsilon)
            rms_grad_t = T.sqrt(new_mean_squared_grad + epsilon)
            delta_x_t = - rms_dx_tm1 / rms_grad_t * grads[param]

            # Accumulate updates
            new_mean_square_dx = \
                    self.decay * mean_square_dx + \
                    (1 - self.decay) * T.sqr(delta_x_t)

            # Apply update
            updates[mean_square_grad] = new_mean_squared_grad
            updates[mean_square_dx] = new_mean_square_dx
            updates[param] = param + delta_x_t

        return updates


########NEW FILE########
__FILENAME__ = sgd
"""
Stochastic Gradient Descent and related functionality such as
learning rate adaptation, momentum, and Polyak averaging.
"""
from __future__ import division

__authors__ = "Ian Goodfellow"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["Ian Goodfellow, David Warde-Farley"]
__license__ = "3-clause BSD"
__maintainer__ = "David Warde-Farley"
__email__ = "pylearn-dev@googlegroups"

import logging
import warnings
import numpy as np

from theano import config
from theano import function
from theano.compat.python2x import OrderedDict
from theano.gof.op import get_debug_values

from pylearn2.monitor import Monitor
from pylearn2.space import CompositeSpace, NullSpace
from pylearn2.train_extensions import TrainExtension
from pylearn2.training_algorithms.training_algorithm import TrainingAlgorithm
from pylearn2.training_algorithms.learning_rule import Momentum
from pylearn2.training_algorithms.learning_rule import MomentumAdjustor \
        as LRMomentumAdjustor
from pylearn2.utils.iteration import is_stochastic, has_uniform_batch_size
from pylearn2.utils import py_integer_types, py_float_types
from pylearn2.utils import safe_zip
from pylearn2.utils import serial
from pylearn2.utils import sharedX
from pylearn2.utils.data_specs import DataSpecsMapping
from pylearn2.utils.timing import log_timing
from pylearn2.utils.rng import make_np_rng


log = logging.getLogger(__name__)


class SGD(TrainingAlgorithm):
    """
    SGD = (Minibatch) Stochastic Gradient Descent.
    A TrainingAlgorithm that does stochastic gradient descent on minibatches
    of training examples.

    For theoretical background on this algorithm, see Yoshua Bengio's machine
    learning course notes on the subject:

    http://www.iro.umontreal.ca/~pift6266/H10/notes/gradient.html

    Parameters
    ----------
    learning_rate : float
        The learning rate to use. Train object callbacks can change the
        learning rate after each epoch. SGD update_callbacks can change
        it after each minibatch.
    cost : pylearn2.costs.cost.Cost, optional
        Cost object specifying the objective function to be minimized.
        Optionally, may be None. In this case, SGD will call the model's
        get_default_cost method to obtain the objective function.
    batch_size : int, optional
        The size of the batch to be used.
        If not specified, the model will be asked for the batch size, so
        you must have specified the batch size there.
        (Some models are rigidly defined to only work with one batch size)
    monitoring_batch_size : int, optional
        The size of the monitoring batches.
    monitoring_batches : int, optional
        At the start of each epoch, we run "monitoring", to evaluate
        quantities such as the validation set error.
        monitoring_batches, if specified, determines the number of batches
        to draw from the iterator for each monitoring dataset.
        Unnecessary if not using monitoring or if `monitor_iteration_mode`
        is 'sequential' and `batch_size` is specified (number of
        batches will be calculated based on full dataset size).
        TODO: make it possible to specify different monitoring_batches
        for each monitoring dataset. The Monitor itself already supports
        this.
    monitoring_dataset : Dataset or dictionary, optional
        If not specified, no monitoring is used.
        If specified to be a Dataset, monitor on that Dataset.
        If specified to be dictionary, the keys should be string names
        of datasets, and the values should be Datasets. All monitoring
        channels will be computed for all monitoring Datasets and will
        have the dataset name and an underscore prepended to them.
    monitor_iteration_mode : str, optional
        The iteration mode used to iterate over the examples in all
        monitoring datasets. If not specified, defaults to 'sequential'.
        TODO: make it possible to specify different modes for different
        datasets.
    termination_criterion : instance of \
        pylearn2.termination_criteria.TerminationCriterion, optional

        Used to determine when the algorithm should stop running.
        If not specified, runs forever--or more realistically, until
        external factors halt the python process (Kansas 1977).
    update_callbacks : list, optional
        If specified, each member of the list should be a callable that
        accepts an SGD instance as its only argument.
        All callbacks will be called with this SGD instance after each
        SGD step.
    learning_rule : training_algorithms.learning_rule.LearningRule, optional
        A learning rule computes the new parameter values given old
        parameters and first-order gradients. If learning_rule is None,
        sgd.SGD will update parameters according to the standard SGD
        learning rule:

        .. code-block:: none

            param := param - learning_rate * d cost / d param

        This argument allows more sophisticated learning rules, such
        as SGD with momentum.
    init_momentum : float, **DEPRECATED** option
        Use learning_rule instead.
        If None, does not use momentum otherwise, use momentum and
        initialize the momentum coefficient to init_momentum. Callbacks
        can change this over time just like the learning rate. If the
        gradient is the same on every step, then the update taken by the
        SGD algorithm is scaled by a factor of 1/(1-momentum). See
        section 9 of Geoffrey Hinton's "A Practical Guide to Training
        Restricted Boltzmann Machines" for details.
    set_batch_size : bool, optional
        Defaults to False.
        If True, and batch_size conflicts with model.force_batch_size,
        will call model.set_batch_size(batch_size) in an attempt to
        change model.force_batch_size
    train_iteration_mode : str, optional
        Defaults to 'shuffled_sequential'.
        The iteration mode to use for iterating through training examples.
    batches_per_iter : int, optional
        The number of batches to draw from the iterator over training
        examples.
        If iteration mode is 'sequential' or 'shuffled_sequential', this
        is unnecessary; when unspecified we will iterate over all examples.
    theano_function_mode : a valid argument to theano.function's \
        'mode' parameter, optional

        The theano mode to compile the updates function with. Note that
        pylearn2 includes some wraplinker modes that are not bundled with
        theano. See pylearn2.devtools. These extra modes let you do
        things like check for NaNs at every step, or record md5 digests
        of all computations performed by the update function to help
        isolate problems with nondeterminism.
    monitoring_costs : list, optional
        a list of Cost instances. The Monitor will also include all
        channels defined by these Costs, even though we don't train
        using them.
    seed : valid argument to np.random.RandomState, optional
        The seed used for the random number generate to be passed to the
        training dataset iterator (if any)
    """
    def __init__(self, learning_rate, cost=None, batch_size=None,
                 monitoring_batch_size=None, monitoring_batches=None,
                 monitoring_dataset=None, monitor_iteration_mode='sequential',
                 termination_criterion=None, update_callbacks=None,
                 learning_rule = None, init_momentum = None,
                 set_batch_size = False,
                 train_iteration_mode = None, batches_per_iter=None,
                 theano_function_mode = None, monitoring_costs=None,
                 seed=[2012, 10, 5]):

        if isinstance(cost, (list, tuple, set)):
            raise TypeError("SGD no longer supports using collections of " +
                            "Costs to represent a sum of Costs. Use " +
                            "pylearn2.costs.cost.SumOfCosts instead.")

        if init_momentum:
            warnings.warn("init_momentum interface is deprecated and will "
            "become officially unsuported as of May 9, 2014. Please use the "
            "`learning_rule` parameter instead, providing an object of type "
            "`pylearn2.training_algorithms.learning_rule.Momentum` instead")
            # Convert to new interface under the hood.
            self.learning_rule = Momentum(init_momentum)
        else:
            self.learning_rule = learning_rule

        self.learning_rate = sharedX(learning_rate, 'learning_rate')
        self.cost = cost
        self.batch_size = batch_size
        self.set_batch_size = set_batch_size
        self.batches_per_iter = batches_per_iter
        self._set_monitoring_dataset(monitoring_dataset)
        self.monitoring_batch_size = monitoring_batch_size
        self.monitoring_batches = monitoring_batches
        self.monitor_iteration_mode = monitor_iteration_mode
        if monitoring_dataset is None:
            if monitoring_batch_size is not None:
                raise ValueError("Specified a monitoring batch size " +
                                 "but not a monitoring dataset.")
            if monitoring_batches is not None:
                raise ValueError("Specified an amount of monitoring batches " +
                                 "but not a monitoring dataset.")
        self.termination_criterion = termination_criterion
        self._register_update_callbacks(update_callbacks)
        if train_iteration_mode is None:
            train_iteration_mode = 'shuffled_sequential'
        self.train_iteration_mode = train_iteration_mode
        self.first = True
        self.rng = make_np_rng(seed, which_method=["randn","randint"])
        self.theano_function_mode = theano_function_mode
        self.monitoring_costs = monitoring_costs

    def setup(self, model, dataset):
        """
        Compiles the theano functions needed for the train method.

        Parameters
        ----------
        model : a Model instance
        dataset : Dataset
        """
        if self.cost is None:
            self.cost = model.get_default_cost()

        inf_params = [param for param in model.get_params()
                      if np.any(np.isinf(param.get_value()))]
        if len(inf_params) > 0:
            raise ValueError("These params are Inf: "+str(inf_params))
        if any([np.any(np.isnan(param.get_value()))
                for param in model.get_params()]):
            nan_params = [param for param in model.get_params()
                          if np.any(np.isnan(param.get_value()))]
            raise ValueError("These params are NaN: "+str(nan_params))
        self.model = model

        self._synchronize_batch_size(model)
        model._test_batch_size = self.batch_size
        self.monitor = Monitor.get_monitor(model)
        self.monitor._sanity_check()

        # test if force batch size and batch size
        if getattr(model, "force_batch_size", False) and \
           any(dataset.get_design_matrix().shape[0] % self.batch_size != 0 for
               dataset in self.monitoring_dataset.values()) and \
           not has_uniform_batch_size(self.monitor_iteration_mode):

            raise ValueError("Dataset size is not a multiple of batch size."
                             "You should set monitor_iteration_mode to "
                             "even_sequential, even_shuffled_sequential or "
                             "even_batchwise_shuffled_sequential")

        data_specs = self.cost.get_data_specs(self.model)
        mapping = DataSpecsMapping(data_specs)
        space_tuple = mapping.flatten(data_specs[0], return_tuple=True)
        source_tuple = mapping.flatten(data_specs[1], return_tuple=True)

        # Build a flat tuple of Theano Variables, one for each space.
        # We want that so that if the same space/source is specified
        # more than once in data_specs, only one Theano Variable
        # is generated for it, and the corresponding value is passed
        # only once to the compiled Theano function.
        theano_args = []
        for space, source in safe_zip(space_tuple, source_tuple):
            name = '%s[%s]' % (self.__class__.__name__, source)
            arg = space.make_theano_batch(name=name,
                                          batch_size=self.batch_size)
            theano_args.append(arg)
        theano_args = tuple(theano_args)

        # Methods of `self.cost` need args to be passed in a format compatible
        # with data_specs
        nested_args = mapping.nest(theano_args)
        fixed_var_descr = self.cost.get_fixed_var_descr(model, nested_args)
        self.on_load_batch = fixed_var_descr.on_load_batch

        cost_value = self.cost.expr(model, nested_args,
                                    ** fixed_var_descr.fixed_vars)

        if cost_value is not None and cost_value.name is None:
            # Concatenate the name of all tensors in theano_args !?
            cost_value.name = 'objective'

        # Set up monitor to model the objective value, learning rate,
        # momentum (if applicable), and extra channels defined by
        # the cost
        learning_rate = self.learning_rate
        if self.monitoring_dataset is not None:
            if (self.monitoring_batch_size is None and
                    self.monitoring_batches is None):
                self.monitoring_batch_size = self.batch_size
                self.monitoring_batches = self.batches_per_iter
            self.monitor.setup(dataset=self.monitoring_dataset,
                               cost=self.cost,
                               batch_size=self.monitoring_batch_size,
                               num_batches=self.monitoring_batches,
                               extra_costs=self.monitoring_costs,
                               mode=self.monitor_iteration_mode)
            dataset_name = self.monitoring_dataset.keys()[0]
            monitoring_dataset = self.monitoring_dataset[dataset_name]
            #TODO: have Monitor support non-data-dependent channels
            self.monitor.add_channel(name='learning_rate',
                                     ipt=None,
                                     val=learning_rate,
                                     data_specs=(NullSpace(), ''),
                                     dataset=monitoring_dataset)

            if self.learning_rule:
                self.learning_rule.add_channels_to_monitor(
                        self.monitor,
                        monitoring_dataset)

        params = list(model.get_params())
        assert len(params) > 0
        for i, param in enumerate(params):
            if param.name is None:
                param.name = 'sgd_params[%d]' % i

        grads, updates = self.cost.get_gradients(model, nested_args,
                                                 ** fixed_var_descr.fixed_vars)
        if not isinstance(grads, OrderedDict):
            raise TypeError(str(type(self.cost)) + ".get_gradients returned " +
                            "something with" + str(type(grads)) + "as its " +
                            "first member. Expected OrderedDict.")

        for param in grads:
            assert param in params
        for param in params:
            assert param in grads

        for param in grads:
            if grads[param].name is None and cost_value is not None:
                grads[param].name = ('grad(%(costname)s, %(paramname)s)' %
                                     {'costname': cost_value.name,
                                      'paramname': param.name})
            assert grads[param].dtype == param.dtype

        lr_scalers = model.get_lr_scalers()

        for key in lr_scalers:
            if key not in params:
                raise ValueError("Tried to scale the learning rate on " +\
                        str(key)+" which is not an optimization parameter.")

        log.info('Parameter and initial learning rate summary:')
        for param in params:
            param_name = param.name
            if param_name is None:
                param_name = 'anon_param'
            lr = learning_rate.get_value() * lr_scalers.get(param,1.)
            log.info('\t' + param_name + ': ' + str(lr))

        if self.learning_rule:
            updates.update(self.learning_rule.get_updates(
                learning_rate, grads, lr_scalers))
        else:
            # Use standard SGD updates with fixed learning rate.
            updates.update( dict(safe_zip(params, [param - learning_rate * \
                lr_scalers.get(param, 1.) * grads[param]
                                    for param in params])))

        for param in params:
            if updates[param].name is None:
                updates[param].name = 'sgd_update(' + param.name + ')'
        model.modify_updates(updates)
        for param in params:
            update = updates[param]
            if update.name is None:
                update.name = 'censor(sgd_update(' + param.name + '))'
            for update_val in get_debug_values(update):
                if np.any(np.isinf(update_val)):
                    raise ValueError("debug value of %s contains infs" %
                            update.name)
                if np.any(np.isnan(update_val)):
                    raise ValueError("debug value of %s contains nans" %
                            update.name)


        with log_timing(log, 'Compiling sgd_update'):
            self.sgd_update = function(theano_args,
                                       updates=updates,
                                       name='sgd_update',
                                       on_unused_input='ignore',
                                       mode=self.theano_function_mode)
        self.params = params

    def train(self, dataset):
        """
        Runs one epoch of SGD training on the specified dataset.

        Parameters
        ----------
        dataset : Dataset
        """
        if not hasattr(self, 'sgd_update'):
            raise Exception("train called without first calling setup")

        # Make sure none of the parameters have bad values
        for param in self.params:
            value = param.get_value(borrow=True)
            if np.any(np.isnan(value)) or np.any(np.isinf(value)):
                raise Exception("NaN in " + param.name)

        self.first = False
        rng = self.rng
        if not is_stochastic(self.train_iteration_mode):
            rng = None

        data_specs = self.cost.get_data_specs(self.model)

        # The iterator should be built from flat data specs, so it returns
        # flat, non-redundent tuples of data.
        mapping = DataSpecsMapping(data_specs)
        space_tuple = mapping.flatten(data_specs[0], return_tuple=True)
        source_tuple = mapping.flatten(data_specs[1], return_tuple=True)
        if len(space_tuple) == 0:
            # No data will be returned by the iterator, and it is impossible
            # to know the size of the actual batch.
            # It is not decided yet what the right thing to do should be.
            raise NotImplementedError("Unable to train with SGD, because "
                    "the cost does not actually use data from the data set. "
                    "data_specs: %s" % str(data_specs))
        flat_data_specs = (CompositeSpace(space_tuple), source_tuple)

        iterator = dataset.iterator(mode=self.train_iteration_mode,
                batch_size=self.batch_size,
                data_specs=flat_data_specs, return_tuple=True,
                rng = rng, num_batches = self.batches_per_iter)

        on_load_batch = self.on_load_batch
        for batch in iterator:
            for callback in on_load_batch:
                callback(*batch)
            self.sgd_update(*batch)
            # iterator might return a smaller batch if dataset size
            # isn't divisible by batch_size
            # Note: if data_specs[0] is a NullSpace, there is no way to know
            # how many examples would actually have been in the batch,
            # since it was empty, so actual_batch_size would be reported as 0.
            actual_batch_size = flat_data_specs[0].np_batch_size(batch)
            self.monitor.report_batch(actual_batch_size)
            for callback in self.update_callbacks:
                callback(self)

        # Make sure none of the parameters have bad values
        for param in self.params:
            value = param.get_value(borrow=True)
            if np.any(np.isnan(value)) or np.any(np.isinf(value)):
                raise Exception("NaN in " + param.name)

    def continue_learning(self, model):
        """
        Returns True if the algorithm should continue running, or False
        if it has reached convergence / started overfitting and should
        stop.

        Parameters
        ----------
        model : a Model instance
        """
        if self.termination_criterion is None:
            return True
        else:
            return self.termination_criterion.continue_learning(self.model)

class MonitorBasedLRAdjuster(TrainExtension):
    """
    A TrainExtension that uses the on_monitor callback to adjust
    the learning rate on each epoch. It pulls out a channel
    from the model's monitor and adjusts the learning rate
    based on what happened to the monitoring channel on the last
    epoch. If the channel is greater than high_trigger times
    its previous value, the learning rate will be scaled by
    shrink_amt (which should be < 1 for this scheme to make
    sense). The idea is that in this case the learning algorithm
    is overshooting the bottom of the objective function.

    If the objective is less than high_trigger but
    greater than low_trigger times its previous value, the
    learning rate will be scaled by grow_amt (which should be > 1
    for this scheme to make sense). The idea is that the learning
    algorithm is making progress but at too slow of a rate.

    Parameters
    ----------
    high_trigger : float, optional
        See class-level docstring
    low_trigger : float, optional
        See class-level docstring
    grow_amt : float, optional
        See class-level docstring
    min_lr : float, optional
        All updates to the learning rate are clipped to be at least
        this value.
    max_lr : float, optional
        All updates to the learning rate are clipped to be at most
        this value.
    dataset_name : str, optional
        If specified, use dataset_name + "_objective" as the channel
        to guide the learning rate adaptation.
    channel_name : str, optional
        If specified, use channel_name as the channel to guide the
        learning rate adaptation. Conflicts with dataset_name.
        If neither dataset_name nor channel_name is specified, uses
        "objective"
    """

    def __init__(self, high_trigger=1., shrink_amt=.99,
                 low_trigger=.99, grow_amt=1.01,
                 min_lr = 1e-7, max_lr = 1.,
                 dataset_name=None, channel_name=None):
        self.high_trigger = high_trigger
        self.shrink_amt = shrink_amt
        self.low_trigger = low_trigger
        self.grow_amt = grow_amt
        self.min_lr = min_lr
        self.max_lr = max_lr
        self.dataset_name = None
        if channel_name is not None:
            self.channel_name = channel_name
        else:
            if dataset_name is not None:
                self.channel_name = dataset_name + '_objective'
                self.dataset_name = dataset_name
            else:
                self.channel_name = None

    def on_monitor(self, model, dataset, algorithm):
        """
        Adjusts the learning rate based on the contents of model.monitor

        Parameters
        ----------
        model : a Model instance
        dataset : Dataset
        algorithm : WRITEME
        """
        model = algorithm.model
        lr = algorithm.learning_rate
        current_learning_rate = lr.get_value()
        assert hasattr(model, 'monitor'), ("no monitor associated with "
                + str(model))
        monitor = model.monitor
        monitor_channel_specified = True

        if self.channel_name is None:
            monitor_channel_specified = False
            channels = [elem for elem in monitor.channels
                    if elem.endswith("objective")]
            if len(channels) < 1:
                raise ValueError("There are no monitoring channels that end "
                        "with \"objective\". Please specify either "
                        "channel_name or dataset_name.")
            elif len(channels) > 1:
                datasets = algorithm.monitoring_dataset.keys()
                raise ValueError("There are multiple monitoring channels that"
                        "end with \"_objective\". The list of available "
                        "datasets are: " +
                                str(datasets) + " . Please specify either "
                                "channel_name or dataset_name in the "
                                "MonitorBasedLRAdjuster constructor to "
                                'disambiguate.')
            else:
                self.channel_name = channels[0]
                warnings.warn('The channel that has been chosen for '
                        'monitoring is: ' +
                              str(self.channel_name) + '.')

        try:
            v = monitor.channels[self.channel_name].val_record
        except KeyError:
            err_input = ''
            if monitor_channel_specified:
                if self.dataset_name:
                    err_input = 'The dataset_name \'' + str(
                            self.dataset_name) + '\' is not valid.'
                else:
                    err_input = 'The channel_name \'' + str(
                            self.channel_name) + '\' is not valid.'
            err_message = 'There is no monitoring channel named \'' + \
                    str(self.channel_name) + '\'. You probably need to ' + \
                    'specify a valid monitoring channel by using either ' + \
                    'dataset_name or channel_name in the ' + \
                    'MonitorBasedLRAdjuster constructor. ' + err_input
            raise ValueError(err_message)

        if len(v) < 1:
            if monitor.dataset is None:
                assert len(v) == 0
                raise ValueError("You're trying to use a monitor-based "
                        "learning rate adjustor but the monitor has no "
                        "entries because you didn't specify a "
                        "monitoring dataset.")

            raise ValueError("For some reason there are no monitor entries"
                                 "yet the MonitorBasedLRAdjuster has been "
                                 "called. This should never happen. The Train"
                                 " object should call the monitor once on "
                                 "initialization, then call the callbacks. "
                                 "It seems you are either calling the "
                                 "callback manually rather than as part of a "
                                 "training algorithm, or there is a problem "
                                "with the Train object.")
        if len(v) == 1:
            #only the initial monitoring has happened
            #no learning has happened, so we can't adjust the learning rate yet
            #just do nothing
            return

        rval = current_learning_rate

        log.info("monitoring channel is {0}".format(self.channel_name))

        if v[-1] > self.high_trigger * v[-2]:
            rval *= self.shrink_amt
            log.info("shrinking learning rate to %f" % rval)
        elif v[-1] > self.low_trigger * v[-2]:
            rval *= self.grow_amt
            log.info("growing learning rate to %f" % rval)

        rval = max(self.min_lr, rval)
        rval = min(self.max_lr, rval)

        lr.set_value(np.cast[lr.dtype](rval))


class PatienceBasedTermCrit(object):
    """
    A monitor-based termination criterion using a geometrically increasing
    amount of patience. If the selected channel has decreased by a certain
    proportion when comparing to the lowest value seen yet, the patience is
    set to a factor of the number of examples seen, which by default
    (patience_increase=2.) ensures the model has seen as many examples as the
    number of examples that lead to the lowest value before concluding a local
    optima has been reached.

    Note: Technically, the patience corresponds to a number of epochs to be
    independent of the size of the dataset, so be aware of that when choosing
    initial_patience.

    Parameters
    ----------
    prop_decrease : float
        The factor X in the (1 - X) * best_value threshold
    initial_patience : int
        Minimal number of epochs the model has to run before it can stop
    patience_increase : float, optional
        The factor X in the patience = X * n_iter update.
    channel_name : string, optional
        Name of the channel to examine. If None and the monitor
        has only one channel, this channel will be used; otherwise, an
        error will be raised.
    """
    def __init__(self, prop_decrease, initial_patience,
                 patience_increase=2., channel_name=None):
        self._channel_name = channel_name
        self.prop_decrease = prop_decrease
        self.patience = initial_patience
        self.best_value = np.inf
        self.patience_increase = patience_increase

    def __call__(self, model):
        """
        Returns True or False depending on whether the optimization should
        stop or not. The optimization should stop if it has run for a number
        of epochs superior to the patience without any improvement.

        Parameters
        ----------
        model : Model
            The model used in the experiment and from which the monitor used
            in the termination criterion will be extracted.

        Returns
        -------
        bool
            True or False, indicating if the optimization should stop or not.
        """
        monitor = model.monitor
        # In the case the monitor has only one channel, the channel_name can
        # be omitted and the criterion will examine the only channel
        # available. However, if the monitor has multiple channels, leaving
        # the channel_name unspecified will raise an error.
        if self._channel_name is None:
            if len(monitor.channels) != 1:
                raise ValueError("Only single-channel monitors are supported "
                                 "for channel_name == None")
            v = monitor.channels.values()[0].val_record
        else:
            v = monitor.channels[self._channel_name].val_record
        # If the channel value decrease is higher than the threshold, we
        # update the best value to this value and we update the patience.
        if v[-1] < self.best_value * (1. - self.prop_decrease):
            # Using the max between actual patience and updated patience
            # ensures that the model will run for at least the initial
            # patience and that it would behave correctly if the user
            # chooses a dumb value (i.e. less than 1)
            self.patience = max(self.patience, len(v) * self.patience_increase)
            self.best_value = v[-1]

        return len(v) < self.patience


class AnnealedLearningRate(object):
    """
    This is a callback for the SGD algorithm rather than the Train object.
    This anneals the learning rate to decrease as 1/t where t is the number
    of gradient descent updates done so far. Use OneOverEpoch as Train object
    callback if you would prefer 1/t where t is epochs.

    Parameters
    ----------
    anneal_start : int
        The epoch on which to begin annealing
    """
    def __init__(self, anneal_start):
        self._initialized = False
        self._count = 0
        self._anneal_start = anneal_start

    def __call__(self, algorithm):
        """
        Updates the learning rate according to the annealing schedule.

        Parameters
        ----------
        algorithm : WRITEME
        """
        if not self._initialized:
            self._base = algorithm.learning_rate.get_value()
        self._count += 1
        algorithm.learning_rate.set_value(self.current_learning_rate())

    def current_learning_rate(self):
        """
        Returns the current desired learning rate according to the
        annealing schedule.
        """
        return self._base * min(1, self._anneal_start / self._count)

class ExponentialDecay(object):
    """
    This is a callback for the `SGD` algorithm rather than the `Train` object.
    This anneals the learning rate by dividing by decay_factor after each
    gradient descent step. It will not shrink the learning rate beyond
    `min_lr`.

    Parameters
    ----------
    decay_factor : float
        The learning rate at step t is given by
        `init_learning_rate / (decay_factor ** t)`
    min_lr : float
        The learning rate will be clipped to be at least this value
    """

    def __init__(self, decay_factor, min_lr):
        if isinstance(decay_factor, str):
            decay_factor = float(decay_factor)
        if isinstance(min_lr, str):
            min_lr = float(min_lr)
        assert isinstance(decay_factor, float)
        assert isinstance(min_lr, float)
        self.__dict__.update(locals())
        del self.self
        self._count = 0
        self._min_reached = False

    def __call__(self, algorithm):
        """
        Updates the learning rate according to the exponential decay schedule.

        Parameters
        ----------
        algorithm : SGD
            The SGD instance whose `learning_rate` field should be modified.
        """
        if self._count == 0:
            self._base_lr = algorithm.learning_rate.get_value()
        self._count += 1

        if not self._min_reached:
            # If we keep on executing the exponentiation on each mini-batch,
            # we will eventually get an OverflowError. So make sure we
            # only do the computation until min_lr is reached.
            new_lr = self._base_lr / (self.decay_factor ** self._count)
            if new_lr <= self.min_lr:
                self._min_reached = True
                new_lr = self.min_lr
        else:
            new_lr = self.min_lr

        new_lr = np.cast[config.floatX](new_lr)
        algorithm.learning_rate.set_value(new_lr)

class LinearDecay(object):
    """
    This is a callback for the SGD algorithm rather than the Train object.
    This anneals the learning rate to decay_factor times of the initial value
    during time start till saturate.

    Parameters
    ----------
    start : int
        The step at which to start decreasing the learning rate
    saturate : int
        The step at which to stop decreating the learning rate
    decay_factor : float
        `final learning rate = decay_factor * initial learning rate`
    """

    def __init__(self, start, saturate, decay_factor):
        if isinstance(decay_factor, str):
            decay_factor = float(decay_factor)
        if isinstance(start, str):
            start = float(start)
        if isinstance(saturate, str):
            saturate = float(saturate)
        assert isinstance(decay_factor, float)
        assert isinstance(start, (py_integer_types, py_float_types))
        assert isinstance(saturate, (py_integer_types, py_float_types))
        assert saturate > start
        assert start > 0
        self.__dict__.update(locals())
        del self.self
        self._count = 0

    def __call__(self, algorithm):
        """
        Adjusts the learning rate according to the linear decay schedule

        Parameters
        ----------
        algorithm : WRITEME
        """
        if self._count == 0:
            self._base_lr = algorithm.learning_rate.get_value()
            self._step = ((self._base_lr - self._base_lr * self.decay_factor) /
                          (self.saturate - self.start + 1))
        self._count += 1
        if self._count >= self.start:
            if self._count < self.saturate:
                new_lr = self._base_lr - self._step * (self._count
                        - self.start + 1)
            else:
                new_lr = self._base_lr * self.decay_factor
        else:
            new_lr = self._base_lr
        assert new_lr > 0
        new_lr = np.cast[config.floatX](new_lr)
        algorithm.learning_rate.set_value(new_lr)


def MomentumAdjustor(final_momentum, start, saturate):
    """
    Deprecated class used with the deprecated init_momentum argument.
    Use learning_rule.MomentumAdjustor instead.

    Parameters
    ----------
    final_momentum : WRITEME
    start : WRITEME
    saturate : WRITEME
    """
    warnings.warn("sgd.MomentumAdjustor interface is deprecated and will "
    "become officially unsupported as of May 9, 2014. Please use "
    "`learning_rule.MomentumAdjustor` instead.")
    return LRMomentumAdjustor(final_momentum, start, saturate)


class OneOverEpoch(TrainExtension):
    """
    Scales the learning rate like one over # epochs

    Parameters
    ----------
    start : int
        The epoch on which to start shrinking the learning rate
    half_life : int, optional
        How many epochs after start it will take for the learning rate to lose
        half its value for the first time (to lose the next half of its value
        will take twice as long)
    min_lr : float, optional
        The minimum value the learning rate can take on
    """
    def __init__(self, start, half_life = None, min_lr = 1e-6):
        self.__dict__.update(locals())
        del self.self
        self._initialized = False
        self._count = 0
        assert start >= 0
        if half_life is None:
            self.half_life = start + 1
        else:
            assert half_life > 0

    def on_monitor(self, model, dataset, algorithm):
        """
        Adjusts the learning rate according to the decay schedule.

        Parameters
        ----------
        model : a Model instance
        dataset : Dataset
        algorithm : WRITEME
        """

        if not self._initialized:
            self._init_lr = algorithm.learning_rate.get_value()
            if self._init_lr < self.min_lr:
                raise ValueError("The initial learning rate is smaller than " +
                                 "the minimum allowed learning rate.")
            self._initialized = True
        self._count += 1
        algorithm.learning_rate.set_value(np.cast[config.floatX](
            self.current_lr()))

    def current_lr(self):
        """
        Returns the learning rate currently desired by the decay schedule.
        """
        if self._count < self.start:
            scale = 1
        else:
            scale = float(self.half_life) / float(self._count - self.start
                    + self.half_life)
        lr = self._init_lr * scale
        clipped = max(self.min_lr, lr)
        return clipped

class LinearDecayOverEpoch(TrainExtension):
    """
    Scales the learning rate linearly on each epochs

    Parameters
    ----------
    start : int
        The epoch on which to start shrinking the learning rate
    saturate : int
        The epoch to saturate the shrinkage
    decay_factor : float
        The final value would be initial learning rate times decay_factor
    """

    def __init__(self, start, saturate, decay_factor):
        self.__dict__.update(locals())
        del self.self
        self._initialized = False
        self._count = 0
        assert isinstance(decay_factor, float)
        assert isinstance(start, (py_integer_types, py_float_types))
        assert isinstance(saturate, (py_integer_types, py_float_types))
        assert saturate > start
        assert start >= 0
        assert saturate >= start

    def on_monitor(self, model, dataset, algorithm):
        """
        Updates the learning rate based on the linear decay schedule.

        Parameters
        ----------
        model : a Model instance
        dataset : Dataset
        algorithm : WRITEME
        """
        if not self._initialized:
            self._init_lr = algorithm.learning_rate.get_value()
            self._step = ((self._init_lr - self._init_lr * self.decay_factor) /
                          (self.saturate - self.start + 1))
            self._initialized = True
        self._count += 1
        algorithm.learning_rate.set_value(np.cast[config.floatX](
            self.current_lr()))

    def current_lr(self):
        """
        Returns the learning rate currently desired by the decay schedule.
        """
        if self._count >= self.start:
            if self._count < self.saturate:
                new_lr = self._init_lr - self._step * (self._count
                        - self.start + 1)
            else:
                new_lr = self._init_lr * self.decay_factor
        else:
            new_lr = self._init_lr
        assert new_lr > 0
        return new_lr

class _PolyakWorker(object):
    """
    Only to be used by the PolyakAveraging TrainingCallback below.
    Do not use directly.
    A callback for the SGD class.

    Parameters
    ----------
    model : a Model
        The model whose parameters we want to train with Polyak averaging
    """

    def __init__(self, model):
        avg_updates = OrderedDict()
        t = sharedX(1.)
        self.param_to_mean = OrderedDict()
        for param in model.get_params():
            mean = sharedX(param.get_value())
            assert type(mean) == type(param)
            self.param_to_mean[param] = mean
            avg_updates[mean] = mean - (mean - param) / t
            avg_updates[t] = t + 1.
        self.avg = function([], updates = avg_updates)

    def __call__(self, algorithm):
        """
        To be called after each SGD step.
        Updates the Polyak averaged-parameters for this model

        Parameters
        ----------
        algorithm : WRITEME
        """
        self.avg()

class PolyakAveraging(TrainExtension):
    """
    See "A Tutorial on Stochastic Approximation Algorithms
        for Training Restricted Boltzmann Machines and
        Deep Belief Nets" by Kevin Swersky et al

    This functionality is still a work in progress. Currently,
    your model needs to implement "add_polyak_channels" to
    use it.

    The problem is that Polyak averaging shouldn't modify
    the model parameters. It should keep a second copy
    that it averages in the background. This second copy
    doesn't get to come back in and affect the learning process
    though.

    (IG tried having the second copy get pushed back into
    the model once per epoch, but this turned out to be
    harmful, at least in limited tests)

    So we need a cleaner interface for monitoring the
    averaged copy of the parameters, and we need to make
    sure the saved model at the end uses the averaged
    parameters, not the parameters used for computing
    the gradients during training.

    TODO: make use of the new on_save callback instead
        of duplicating Train's save_freq flag

    Parameters
    ----------
    start : int
        The epoch after which to start averaging (0 = start averaging
        immediately)
    save_path : str, optional
        WRITEME
    save_freq : int, optional
        WRITEME

    Notes
    -----
    This is usually used with a fixed, rather than annealed learning
    rate. It may be used in conjunction with momentum.
    """

    def __init__(self, start, save_path=None, save_freq=1):
        self.__dict__.update(locals())
        del self.self
        self._count = 0
        assert isinstance(start, py_integer_types)
        assert start >= 0

    def on_monitor(self, model, dataset, algorithm):
        """
        Make sure Polyak-averaged model gets monitored.
        Save the model if necessary.

        Parameters
        ----------
        model : a Model instance
        dataset : Dataset
        algorithm : WRITEME
        """
        if self._count == self.start:
            self._worker = _PolyakWorker(model)
            algorithm.update_callbacks.append(self._worker)
            #HACK
            try:
                model.add_polyak_channels(self._worker.param_to_mean,
                                          algorithm.monitoring_dataset)
            except AttributeError:
                pass
        elif self.save_path is not None and self._count > self.start and \
                self._count % self.save_freq == 0:
            saved_params = OrderedDict()
            for param in model.get_params():
                saved_params[param] = param.get_value()
                param.set_value(self._worker.param_to_mean[param].get_value())
            serial.save(self.save_path, model)
            for param in model.get_params():
                param.set_value(saved_params[param])
        self._count += 1

########NEW FILE########
__FILENAME__ = test_bgd
from pylearn2.train import Train
from pylearn2.datasets.dense_design_matrix import DenseDesignMatrix
from pylearn2.models.model import Model
from pylearn2.space import CompositeSpace, VectorSpace
from pylearn2.utils import sharedX
from pylearn2.training_algorithms.bgd import BGD
from pylearn2.termination_criteria import EpochCounter
from pylearn2.costs.cost import Cost
import theano.tensor as T
import numpy as np
import cStringIO
from pylearn2.devtools.record import Record
from pylearn2.devtools.record import RecordMode
from theano.tests import disturb_mem
from pylearn2.utils import safe_union
from pylearn2.utils import safe_izip
from pylearn2.utils.data_specs import DataSpecsMapping
from theano import shared
from pylearn2.utils import function
from pylearn2.costs.cost import FixedVarDescr
from pylearn2.costs.cost import SumOfCosts

class SoftmaxModel(Model):
    """A dummy model used for testing.
       Important properties:
           has a parameter (P) for SGD to act on
           has a get_output_space method, so it can tell the
           algorithm what kind of space the targets for supervised
           learning live in
           has a get_input_space method, so it can tell the
           algorithm what kind of space the features live in
    """

    def __init__(self, dim):
        self.dim = dim
        rng = np.random.RandomState([2012,9,25])
        self.P = sharedX( rng.uniform(-1.,1.,(dim,)))
        self.force_batch_size = None

    def get_params(self):
        return [ self.P ]

    def get_input_space(self):
        return VectorSpace(self.dim)

    def get_output_space(self):
        return VectorSpace(self.dim)

    def __call__(self, X):
        # Make the test fail if algorithm does not
        # respect get_input_space
        assert X.ndim == 2
        # Multiplying by P ensures the shape as well
        # as ndim is correct
        return T.nnet.softmax(X*self.P)



def test_bgd_unsup():

    # tests that we can run the bgd algorithm
    # on an supervised cost.
    # does not test for correctness at all, just
    # that the algorithm runs without dying

    dim = 3
    m = 10

    rng = np.random.RandomState([25,9,2012])

    X = rng.randn(m, dim)

    dataset = DenseDesignMatrix(X=X)

    m = 15
    X = rng.randn(m, dim)


    # including a monitoring datasets lets us test that
    # the monitor works with supervised data
    monitoring_dataset = DenseDesignMatrix(X=X)

    model = SoftmaxModel(dim)

    learning_rate = 1e-3
    batch_size = 5

    class DummyCost(Cost):

        def expr(self, model, data):
            self.get_data_specs(model)[0].validate(data)
            X = data
            return T.square(model(X) - X).mean()

        def get_data_specs(self, model):
            return (model.get_input_space(), model.get_input_source())

    cost = DummyCost()

    # We need to include this so the test actually stops running at some point
    termination_criterion = EpochCounter(5)

    algorithm = BGD(cost, batch_size=5,
                monitoring_batches=2, monitoring_dataset= monitoring_dataset,
                termination_criterion = termination_criterion)

    train = Train(dataset, model, algorithm, save_path=None,
                 save_freq=0, extensions=None)

    train.main_loop()

def test_determinism():

    """
    Tests that apply nodes are all passed inputs
    with the same md5sums, apply nodes are run in same order, etc.
    Uses disturb_mem to try to cause dictionaries to iterate in different
    orders, etc.
    """

    def run_bgd(mode):
        # Must be seeded the same both times run_bgd is called
        disturb_mem.disturb_mem()
        rng = np.random.RandomState([2012, 11, 27, 8])

        batch_size = 5
        train_batches = 3
        valid_batches = 4
        num_features = 2

        # Synthesize dataset with a linear decision boundary
        w = rng.randn(num_features)

        def make_dataset(num_batches):
            disturb_mem.disturb_mem()
            m = num_batches*batch_size
            X = rng.randn(m, num_features)
            y = np.zeros((m,1))
            y[:,0] = np.dot(X, w) > 0.

            rval =  DenseDesignMatrix(X=X, y=y)

            rval.yaml_src = "" # suppress no yaml_src warning

            X = rval.get_batch_design(batch_size)
            assert X.shape == (batch_size, num_features)

            return rval

        train = make_dataset(train_batches)
        valid = make_dataset(valid_batches)

        num_chunks = 10
        chunk_width = 2
        class ManyParamsModel(Model):
            """
            Make a model with lots of parameters, so that there are many
            opportunities for their updates to get accidentally re-ordered
            non-deterministically. This makes non-determinism bugs manifest
            more frequently.
            """

            def __init__(self):
                self.W1 = [sharedX(rng.randn(num_features, chunk_width)) for i
                    in xrange(num_chunks)]
                disturb_mem.disturb_mem()
                self.W2 = [sharedX(rng.randn(chunk_width)) for i in
                        xrange(num_chunks)]
                self._params = safe_union(self.W1, self.W2)
                self.input_space = VectorSpace(num_features)
                self.output_space = VectorSpace(1)

        disturb_mem.disturb_mem()
        model = ManyParamsModel()
        disturb_mem.disturb_mem()


        class LotsOfSummingCost(Cost):
            """
            Make a cost whose gradient on the parameters involves summing many
            terms together,
            so that T.grad is more likely to sum things in a random order.
            """

            supervised = True

            def expr(self, model, data, **kwargs):
                self.get_data_specs(model)[0].validate(data)
                X, Y = data
                disturb_mem.disturb_mem()
                def mlp_pred(non_linearity):
                    Z = [T.dot(X, W) for W in model.W1]
                    H = map(non_linearity, Z)
                    Z = [T.dot(h, W) for h, W in safe_izip(H, model.W2)]
                    pred = sum(Z)
                    return pred

                nonlinearity_predictions = map(mlp_pred, [T.nnet.sigmoid,
                    T.nnet.softplus, T.sqr, T.sin])
                pred = sum(nonlinearity_predictions)
                disturb_mem.disturb_mem()

                return abs(pred-Y[:,0]).sum()

            def get_data_specs(self, model):
                data = CompositeSpace((model.get_input_space(),
                                       model.get_output_space()))
                source = (model.get_input_source(), model.get_target_source())
                return (data, source)

        cost = LotsOfSummingCost()

        disturb_mem.disturb_mem()

        algorithm = BGD(cost=cost,
                batch_size=batch_size,
                updates_per_batch=5,
                scale_step=.5,
                conjugate=1,
                reset_conjugate=0,
                monitoring_dataset={'train': train, 'valid':valid},
                termination_criterion=EpochCounter(max_epochs=5))

        disturb_mem.disturb_mem()

        train_object = Train(
                dataset=train,
                model=model,
                algorithm=algorithm,
                save_freq=0)

        disturb_mem.disturb_mem()

        train_object.main_loop()



    output = cStringIO.StringIO()
    record = Record(file_object=output, replay=False)
    record_mode = RecordMode(record)

    run_bgd(record_mode)

    output = cStringIO.StringIO(output.getvalue())
    playback = Record(file_object=output, replay=True)
    playback_mode = RecordMode(playback)

    run_bgd(playback_mode)


def test_fixed_vars():

    """
    A very basic test of the the fixed vars interface.
    Checks that the costs' expr and get_gradients methods
    are called with the right parameters and that the updates
    functions are called the right number of times.
    """

    """
    Notes: this test is fairly messy. PL made some change to how
    FixedVarDescr worked. FixedVarDescr got an added data_specs
    field. But BGD itself was never changed to obey this data_specs.
    Somehow these tests passed regardless. It looks like PL just built
    a lot of machinery into the test itself to make the individual
    callbacks reformat data internally. This mechanism required the
    data_specs field to be present. Weirdly, the theano functions
    never actually used any of the data, so their data_specs should
    have just been NullSpace anyway. IG deleted a lot of this useless
    code from these tests but there is still a lot of weird stuff here
    that he has not attempted to clean up.
    """

    rng = np.random.RandomState([2012, 11, 27, 9])

    batch_size = 5
    updates_per_batch = 4
    train_batches = 3
    num_features = 2

    # Synthesize dataset with a linear decision boundary
    w = rng.randn(num_features)

    def make_dataset(num_batches):
        m = num_batches*batch_size
        X = rng.randn(m, num_features)
        y = rng.randn(m, num_features)

        rval =  DenseDesignMatrix(X=X, y=y)

        rval.yaml_src = "" # suppress no yaml_src warning

        return rval

    train = make_dataset(train_batches)

    model = SoftmaxModel(num_features)

    unsup_counter = shared(0)
    grad_counter = shared(0)

    called = [False, False, False, False]

    class UnsupervisedCostWithFixedVars(Cost):

        def expr(self, model, data, unsup_aux_var=None, **kwargs):
            self.get_data_specs(model)[0].validate(data)
            X = data
            assert unsup_aux_var is unsup_counter
            called[0] = True
            return (model.P * X).sum()

        def get_gradients(self, model, data, unsup_aux_var=None, **kwargs):
            self.get_data_specs(model)[0].validate(data)
            assert unsup_aux_var is unsup_counter
            called[1] = True
            gradients, updates = Cost.get_gradients(self, model, data,
                    unsup_aux_var=unsup_aux_var)
            updates[grad_counter] = grad_counter + 1
            return gradients, updates

        def get_fixed_var_descr(self, model, data, **kwargs):
            data_specs = self.get_data_specs(model)
            data_specs[0].validate(data)
            rval = FixedVarDescr()
            rval.fixed_vars = {'unsup_aux_var': unsup_counter}

            # The input to function should be a flat, non-redundent tuple
            mapping = DataSpecsMapping(data_specs)
            data_tuple = mapping.flatten(data, return_tuple=True)
            theano_func = function([],
                    updates=[(unsup_counter, unsup_counter + 1)])
            def on_load(batch, mapping=mapping, theano_func=theano_func):
                return theano_func()
            rval.on_load_batch = [on_load]

            return rval

        def get_data_specs(self, model):
            return (model.get_input_space(), model.get_input_source())

    sup_counter = shared(0)

    class SupervisedCostWithFixedVars(Cost):

        supervised = True

        def expr(self, model, data, sup_aux_var=None, **kwargs):
            self.get_data_specs(model)[0].validate(data)
            X, Y = data
            assert sup_aux_var is sup_counter
            called[2] = True
            return (model.P * X * Y).sum()

        def get_gradients(self, model, data, sup_aux_var=None, **kwargs):
            self.get_data_specs(model)[0].validate(data)
            assert sup_aux_var is sup_counter
            called[3] = True
            return super(SupervisedCostWithFixedVars, self).get_gradients(
                    model=model, data=data, sup_aux_var=sup_aux_var)

        def get_fixed_var_descr(self, model, data):
            data_specs = self.get_data_specs(model)
            data_specs[0].validate(data)
            rval = FixedVarDescr()
            rval.fixed_vars = {'sup_aux_var': sup_counter}

            theano_func = function([], updates=[(sup_counter,
                sup_counter + 1)])
            def on_load(data):
                theano_func()
            rval.on_load_batch = [on_load]
            return rval

        def get_data_specs(self, model):
            space = CompositeSpace((model.get_input_space(),
                                   model.get_output_space()))
            source = (model.get_input_source(), model.get_target_source())
            return (space, source)

    cost = SumOfCosts(costs=[UnsupervisedCostWithFixedVars(),
                             SupervisedCostWithFixedVars()])

    algorithm = BGD(cost=cost, batch_size=batch_size,
            conjugate=1, line_search_mode='exhaustive',
            updates_per_batch=updates_per_batch)

    algorithm.setup(model=model, dataset=train)

    # Make sure all the right methods were used to compute the updates
    assert all(called)

    algorithm.train(dataset=train)

    # Make sure the load_batch callbacks were called the right amount of times
    assert unsup_counter.get_value() == train_batches
    assert sup_counter.get_value() == train_batches

    # Make sure the gradient updates were run the right amount of times
    assert grad_counter.get_value() == train_batches * updates_per_batch

########NEW FILE########
__FILENAME__ = test_default
import numpy as np

from pylearn2.datasets.dense_design_matrix import DenseDesignMatrix
from pylearn2.models.rbm import RBM
from pylearn2.models.s3c import S3C, E_Step, Grad_M_Step
from pylearn2.training_algorithms.default import DefaultTrainingAlgorithm
from pylearn2.training_algorithms.training_algorithm import NoBatchSizeError


def test_multiple_monitoring_datasets():
    # tests that DefaultTrainingAlgorithm can take multiple
    # monitoring datasets.

    BATCH_SIZE = 2
    BATCHES = 3
    NUM_FEATURES = 4
    dim = 3
    m = 10

    rng = np.random.RandomState([2014, 02, 25])
    X = rng.randn(m, dim)
    Y = rng.randn(m, dim)

    train = DenseDesignMatrix(X=X)
    test = DenseDesignMatrix(X=Y)

    algorithm = DefaultTrainingAlgorithm(
        batch_size=BATCH_SIZE,
        batches_per_iter=BATCHES,
        monitoring_dataset={'train': train, 'test': test})

    model = S3C(nvis=NUM_FEATURES, nhid=1,
                irange=.01, init_bias_hid=0., init_B=1.,
                min_B=1., max_B=1., init_alpha=1.,
                min_alpha=1., max_alpha=1., init_mu=0.,
                m_step=Grad_M_Step(learning_rate=0.),
                e_step=E_Step(h_new_coeff_schedule=[1.]))

    algorithm.setup(model=model, dataset=train)
    algorithm.train(dataset=train)

def test_unspecified_batch_size():

    # Test that failing to specify the batch size results in a
    # NoBatchSizeError

    m = 1
    dim = 2
    rng = np.random.RandomState([2014, 03, 17])
    X = rng.randn(m, dim)
    train = DenseDesignMatrix(X=X)

    rbm = RBM(nvis=dim, nhid=3)
    trainer = DefaultTrainingAlgorithm()
    try:
        trainer.setup(rbm, train)
    except NoBatchSizeError:
        return
    raise AssertionError("Missed the lack of a batch size")


if __name__ == '__main__':
    test_multiple_monitoring_datasets()

########NEW FILE########
__FILENAME__ = test_learning_rule
import numpy as np
import warnings

import theano.tensor as T
from theano.tests import disturb_mem

from pylearn2.costs.cost import SumOfCosts
from pylearn2.testing.cost import SumOfOneHalfParamsSquared
from pylearn2.models.model import Model
from pylearn2.space import VectorSpace
from pylearn2.testing.cost import SumOfParams
from pylearn2.testing.datasets import ArangeDataset
from pylearn2.training_algorithms.sgd import SGD
from pylearn2.training_algorithms.learning_rule import Momentum
from pylearn2.training_algorithms.learning_rule import AdaDelta
from pylearn2.utils import sharedX

from test_sgd import DummyCost, DummyModel


def test_momentum():
    """
    Make sure that learning_rule.Momentum obtains the same parameter values as
    with a hand-crafted sgd w/ momentum implementation, given a dummy model and
    learning rate scaler for each parameter.
    """
    # We include a cost other than SumOfParams so that data is actually
    # queried from the training set, and the expected number of updates
    # are applied.
    cost = SumOfCosts([SumOfParams(), (0., DummyCost())])

    scales = [ .01, .02, .05, 1., 5. ]
    shapes = [(1,), (9,), (8, 7), (6, 5, 4), (3, 2, 2, 2)]

    model = DummyModel(shapes, lr_scalers=scales)
    dataset = ArangeDataset(1)
    learning_rate = .001
    momentum = 0.5

    sgd = SGD(cost=cost,
              learning_rate=learning_rate,
              learning_rule = Momentum(momentum),
              batch_size=1)

    sgd.setup(model=model, dataset=dataset)

    manual = [param.get_value() for param in model.get_params()]
    inc = [ - learning_rate * scale for param, scale in
            zip(manual, scales)]
    manual = [param + i for param, i in zip(manual, inc)]

    sgd.train(dataset=dataset)

    assert all(np.allclose(manual_param, sgd_param.get_value()) for manual_param,
            sgd_param in zip(manual, model.get_params()))

    manual = [param - learning_rate * scale + i * momentum for param, scale, i in
            zip(manual, scales, inc)]

    sgd.train(dataset=dataset)

    assert all(np.allclose(manual_param, sgd_param.get_value()) for manual_param,
            sgd_param in zip(manual, model.get_params()))


def test_adadelta():
    """
    Make sure that learning_rule.AdaDelta obtains the same parameter values as
    with a hand-crafted AdaDelta implementation, given a dummy model and
    learning rate scaler for each parameter.
    
    Reference:
    "AdaDelta: An Adaptive Learning Rate Method", Matthew D. Zeiler.
    """

    # We include a cost other than SumOfParams so that data is actually
    # queried from the training set, and the expected number of updates
    # are applied.
    cost = SumOfCosts([SumOfOneHalfParamsSquared(), (0., DummyCost())])

    scales = [ .01, .02, .05, 1., 5. ]
    shapes = [(1,), (9,), (8, 7), (6, 5, 4), (3, 2, 2, 2)]

    model = DummyModel(shapes, lr_scalers=scales)
    dataset = ArangeDataset(1)
    learning_rate = .001
    decay = 0.95

    sgd = SGD(cost=cost,
              learning_rate=learning_rate,
              learning_rule = AdaDelta(decay),
              batch_size=1)

    sgd.setup(model=model, dataset=dataset)

    state = {}
    for param in model.get_params():
        param_shape = param.get_value().shape
        state[param] = {}
        state[param]['g2'] = np.zeros(param_shape)
        state[param]['dx2'] = np.zeros(param_shape)

    def adadelta_manual(model, state):
        inc = []
        rval = []
        for scale, param in zip(scales, model.get_params()):
            pstate = state[param]
            param_val =  param.get_value()
            # begin adadelta
            pstate['g2'] = decay * pstate['g2'] + (1. - decay) * param_val**2
            rms_g_t = np.sqrt(pstate['g2'] + scale * learning_rate)
            rms_dx_tm1 = np.sqrt(pstate['dx2'] + scale * learning_rate)
            dx_t = - rms_dx_tm1 / rms_g_t * param_val
            pstate['dx2'] = decay * pstate['dx2'] + (1. - decay) * dx_t**2
            rval += [param_val + dx_t]
        return rval

    manual = adadelta_manual(model, state)
    sgd.train(dataset=dataset)
    assert all(np.allclose(manual_param, sgd_param.get_value()) for manual_param,
            sgd_param in zip(manual, model.get_params()))

    manual = adadelta_manual(model, state)
    sgd.train(dataset=dataset)
    assert all(np.allclose(manual_param, sgd_param.get_value()) for manual_param,
            sgd_param in zip(manual, model.get_params()))

########NEW FILE########
__FILENAME__ = test_monitoring_batch_size
"""
Tests for monitoring_batch_size.
"""
from pylearn2.config import yaml_parse


def test_monitoring_batch_size():
    """Test monitoring_batch_size."""
    trainer = yaml_parse.load(test_yaml)
    trainer.main_loop()

test_yaml = """
!obj:pylearn2.train.Train {
    dataset:
      &train !obj:pylearn2.testing.datasets.random_one_hot_dense_design_matrix
      {
          rng: !obj:numpy.random.RandomState {},
          num_examples: 1000,
          dim: 15,
          num_classes: 2,
      },
    model: !obj:pylearn2.models.mlp.MLP {
        nvis: 15,
        layers: [
            !obj:pylearn2.models.mlp.Sigmoid {
                layer_name: 'h0',
                dim: 15,
                sparse_init: 15,
            },
            !obj:pylearn2.models.mlp.Softmax {
                layer_name: 'y',
                n_classes: 2,
                irange: 0.005,
            }
        ],
    },
    algorithm: !obj:pylearn2.training_algorithms.bgd.BGD {
        monitoring_dataset: {
            'train': *train,
        },
        monitoring_batch_size: 500,
        batch_size: 100,
        termination_criterion: !obj:pylearn2.termination_criteria.And {
            criteria: [
                !obj:pylearn2.termination_criteria.EpochCounter {
                    max_epochs: 1,
                },
            ],
        },
    },
}
"""

########NEW FILE########
__FILENAME__ = test_sgd
import cStringIO
import numpy as np

import theano.tensor as T
from theano.tests import disturb_mem
import warnings

from pylearn2.costs.cost import Cost, SumOfCosts, DefaultDataSpecsMixin
from pylearn2.devtools.record import Record, RecordMode
from pylearn2.datasets.dense_design_matrix import DenseDesignMatrix
from pylearn2.models.model import Model
from pylearn2.monitor import Monitor
from pylearn2.space import CompositeSpace, Conv2DSpace, VectorSpace
from pylearn2.termination_criteria import EpochCounter
from pylearn2.testing.cost import CallbackCost, SumOfParams
from pylearn2.testing.datasets import ArangeDataset
from pylearn2.train import Train
from pylearn2.training_algorithms.sgd import (ExponentialDecay,
                                              MomentumAdjustor,
                                              PolyakAveraging,
                                              LinearDecay,
                                              LinearDecayOverEpoch,
                                              MonitorBasedLRAdjuster,
                                              SGD)
from pylearn2.utils.iteration import _iteration_schemes
from pylearn2.utils import safe_izip, safe_union, sharedX


class SupervisedDummyCost(DefaultDataSpecsMixin, Cost):
    supervised = True

    def expr(self, model, data):
        space, sources = self.get_data_specs(model)
        space.validate(data)
        (X, Y) = data
        return T.square(model(X) - Y).mean()


class DummyCost(DefaultDataSpecsMixin, Cost):
    def expr(self, model, data):
        space, sources = self.get_data_specs(model)
        space.validate(data)
        X = data
        return T.square(model(X) - X).mean()


class DummyModel(Model):

    def __init__(self, shapes, lr_scalers=None):
        self._params = [sharedX(np.random.random(shape)) for shape in shapes]
        self.input_space = VectorSpace(1)
        self.lr_scalers = lr_scalers

    def __call__(self, X):
        # Implemented only so that DummyCost would work
        return X

    def get_lr_scalers(self):
        if self.lr_scalers:
            return dict(zip(self._params, self.lr_scalers))
        else:
            return dict()


class SoftmaxModel(Model):
    """A dummy model used for testing.
       Important properties:
           has a parameter (P) for SGD to act on
           has a get_output_space method, so it can tell the
           algorithm what kind of space the targets for supervised
           learning live in
           has a get_input_space method, so it can tell the
           algorithm what kind of space the features live in
    """

    def __init__(self, dim):
        self.dim = dim
        rng = np.random.RandomState([2012, 9, 25])
        self.P = sharedX(rng.uniform(-1., 1., (dim, )))

    def get_params(self):
        return [self.P]

    def get_input_space(self):
        return VectorSpace(self.dim)

    def get_output_space(self):
        return VectorSpace(self.dim)

    def __call__(self, X):
        # Make the test fail if algorithm does not
        # respect get_input_space
        assert X.ndim == 2
        # Multiplying by P ensures the shape as well
        # as ndim is correct
        return T.nnet.softmax(X*self.P)


class TopoSoftmaxModel(Model):
    """A dummy model used for testing.
       Like SoftmaxModel but its features have 2 topological
       dimensions. This tests that the training algorithm
       will provide topological data correctly.
    """

    def __init__(self, rows, cols, channels):
        dim = rows * cols * channels
        self.input_space = Conv2DSpace((rows, cols), channels)
        self.dim = dim
        rng = np.random.RandomState([2012, 9, 25])
        self.P = sharedX(rng.uniform(-1., 1., (dim, )))

    def get_params(self):
        return [self.P]

    def get_output_space(self):
        return VectorSpace(self.dim)

    def __call__(self, X):
        # Make the test fail if algorithm does not
        # respect get_input_space
        assert X.ndim == 4
        # Multiplying by P ensures the shape as well
        # as ndim is correct
        return T.nnet.softmax(X.reshape((X.shape[0], self.dim)) * self.P)


def test_sgd_unspec_num_mon_batch():

    # tests that if you don't specify a number of
    # monitoring batches, SGD configures the monitor
    # to run on all the data

    m = 25

    visited = [False] * m
    rng = np.random.RandomState([25, 9, 2012])
    X = np.zeros((m, 1))
    X[:, 0] = np.arange(m)
    dataset = DenseDesignMatrix(X=X)

    model = SoftmaxModel(1)

    learning_rate = 1e-3
    batch_size = 5

    cost = DummyCost()

    algorithm = SGD(learning_rate,
                    cost,
                    batch_size=batch_size,
                    monitoring_batches=None,
                    monitoring_dataset=dataset,
                    termination_criterion=None,
                    update_callbacks=None,
                    init_momentum=None,
                    set_batch_size=False)

    algorithm.setup(dataset=dataset, model=model)

    monitor = Monitor.get_monitor(model)

    X = T.matrix()

    def tracker(*data):
        X, = data
        assert X.shape[1] == 1
        for i in xrange(X.shape[0]):
            visited[int(X[i, 0])] = True

    monitor.add_channel(name='tracker',
                        ipt=X,
                        val=0.,
                        prereqs=[tracker],
                        data_specs=(model.get_input_space(),
                                    model.get_input_source()))

    monitor()

    if False in visited:
        print visited
        assert False


def test_sgd_sup():

    # tests that we can run the sgd algorithm
    # on a supervised cost.
    # does not test for correctness at all, just
    # that the algorithm runs without dying

    dim = 3
    m = 10

    rng = np.random.RandomState([25, 9, 2012])

    X = rng.randn(m, dim)

    idx = rng.randint(0, dim, (m, ))
    Y = np.zeros((m, dim))
    for i in xrange(m):
        Y[i, idx[i]] = 1

    dataset = DenseDesignMatrix(X=X, y=Y)

    m = 15
    X = rng.randn(m, dim)

    idx = rng.randint(0, dim, (m,))
    Y = np.zeros((m, dim))
    for i in xrange(m):
        Y[i, idx[i]] = 1

    # Including a monitoring dataset lets us test that
    # the monitor works with supervised data
    monitoring_dataset = DenseDesignMatrix(X=X, y=Y)

    model = SoftmaxModel(dim)

    learning_rate = 1e-3
    batch_size = 5

    cost = SupervisedDummyCost()

    # We need to include this so the test actually stops running at some point
    termination_criterion = EpochCounter(5)

    algorithm = SGD(learning_rate, cost,
                    batch_size=batch_size,
                    monitoring_batches=3,
                    monitoring_dataset=monitoring_dataset,
                    termination_criterion=termination_criterion,
                    update_callbacks=None,
                    init_momentum=None,
                    set_batch_size=False)

    train = Train(dataset,
                  model,
                  algorithm,
                  save_path=None,
                  save_freq=0,
                  extensions=None)

    train.main_loop()


def test_sgd_unsup():

    # tests that we can run the sgd algorithm
    # on an supervised cost.
    # does not test for correctness at all, just
    # that the algorithm runs without dying

    dim = 3
    m = 10

    rng = np.random.RandomState([25, 9, 2012])

    X = rng.randn(m, dim)

    dataset = DenseDesignMatrix(X=X)

    m = 15
    X = rng.randn(m, dim)

    # Including a monitoring dataset lets us test that
    # the monitor works with unsupervised data
    monitoring_dataset = DenseDesignMatrix(X=X)

    model = SoftmaxModel(dim)

    learning_rate = 1e-3
    batch_size = 5

    cost = DummyCost()

    # We need to include this so the test actually stops running at some point
    termination_criterion = EpochCounter(5)

    algorithm = SGD(learning_rate,
                    cost,
                    batch_size=batch_size,
                    monitoring_batches=3,
                    monitoring_dataset=monitoring_dataset,
                    termination_criterion=termination_criterion,
                    update_callbacks=None,
                    init_momentum=None,
                    set_batch_size=False)

    train = Train(dataset,
                  model,
                  algorithm,
                  save_path=None,
                  save_freq=0,
                  extensions=None)

    train.main_loop()


def get_topological_dataset(rng, rows, cols, channels, m):
    X = rng.randn(m, rows, cols, channels)

    dim = rows * cols * channels

    idx = rng.randint(0, dim, (m,))
    Y = np.zeros((m, dim))
    for i in xrange(m):
        Y[i, idx[i]] = 1

    return DenseDesignMatrix(topo_view=X, y=Y)


def test_linear_decay():

    # tests that the class LinearDecay in sgd.py
    # gets the learning rate properly over the training batches
    # it runs a small softmax and at the end checks the learning values.
    # the learning rates are expected to start changing at batch 'start'
    # by an amount of 'step' specified below.
    # the decrease of the learning rate should continue linearly until
    # we reach batch 'saturate' at which the learning rate equals
    # 'learning_rate * decay_factor'

    class LearningRateTracker(object):
        def __init__(self):
            self.lr_rates = []

        def __call__(self, algorithm):
            self.lr_rates.append(algorithm.learning_rate.get_value())

    dim = 3
    dataset_size = 10

    rng = np.random.RandomState([25, 9, 2012])

    X = rng.randn(dataset_size, dim)

    dataset = DenseDesignMatrix(X=X)

    m = 15
    X = rng.randn(m, dim)

    # including a monitoring datasets lets us test that
    # the monitor works with supervised data
    monitoring_dataset = DenseDesignMatrix(X=X)

    model = SoftmaxModel(dim)

    learning_rate = 1e-1
    batch_size = 5

    # We need to include this so the test actually stops running at some point
    epoch_num = 15
    termination_criterion = EpochCounter(epoch_num)

    cost = DummyCost()

    start = 5
    saturate = 10
    decay_factor = 0.1
    linear_decay = LinearDecay(start=start, saturate=saturate,
                               decay_factor=decay_factor)

    # including this extension for saving learning rate value after each batch
    lr_tracker = LearningRateTracker()
    algorithm = SGD(learning_rate,
                    cost,
                    batch_size=batch_size,
                    monitoring_batches=3,
                    monitoring_dataset=monitoring_dataset,
                    termination_criterion=termination_criterion,
                    update_callbacks=[linear_decay, lr_tracker],
                    init_momentum=None,
                    set_batch_size=False)

    train = Train(dataset,
                  model,
                  algorithm,
                  save_path=None,
                  save_freq=0,
                  extensions=None)

    train.main_loop()

    step = (learning_rate - learning_rate*decay_factor)/(saturate - start + 1)

    num_batches = np.ceil(dataset_size / float(batch_size)).astype(int)
    for i in xrange(epoch_num * num_batches):
        actual = lr_tracker.lr_rates[i]
        batches_seen = i + 1
        if batches_seen < start:
            expected = learning_rate
        elif batches_seen >= saturate:
            expected = learning_rate*decay_factor
        elif (start <= batches_seen) and (batches_seen < saturate):
            expected = (decay_factor * learning_rate +
                        (saturate - batches_seen) * step)
        if not np.allclose(actual, expected):
            raise AssertionError("After %d batches, expected learning rate to "
                                 "be %f, but it is %f." %
                                 (batches_seen, expected, actual))


def test_linear_decay_over_epoch():

    # tests that the class LinearDecayOverEpoch in sgd.py
    # gets the learning rate properly over the training epochs
    # it runs a small softmax and at the end checks the learning values.
    # the learning rates are expected to start changing at epoch 'start' by an
    # amount of 'step' specified below.
    # the decrease of the learning rate should continue linearly until we
    # reach epoch 'saturate' at which the learning rate equals
    # 'learning_rate * decay_factor'

    dim = 3
    m = 10

    rng = np.random.RandomState([25, 9, 2012])

    X = rng.randn(m, dim)

    dataset = DenseDesignMatrix(X=X)

    m = 15
    X = rng.randn(m, dim)

    # including a monitoring datasets lets us test that
    # the monitor works with supervised data
    monitoring_dataset = DenseDesignMatrix(X=X)

    model = SoftmaxModel(dim)

    learning_rate = 1e-1
    batch_size = 5

    # We need to include this so the test actually stops running at some point
    epoch_num = 15
    termination_criterion = EpochCounter(epoch_num)

    cost = DummyCost()

    algorithm = SGD(learning_rate, cost, batch_size=batch_size,
                    monitoring_batches=3,
                    monitoring_dataset=monitoring_dataset,
                    termination_criterion=termination_criterion,
                    update_callbacks=None,
                    init_momentum=None,
                    set_batch_size=False)

    start = 5
    saturate = 10
    decay_factor = 0.1
    linear_decay = LinearDecayOverEpoch(start=start,
                                        saturate=saturate,
                                        decay_factor=decay_factor)

    train = Train(dataset,
                  model,
                  algorithm,
                  save_path=None,
                  save_freq=0,
                  extensions=[linear_decay])

    train.main_loop()

    lr = model.monitor.channels['learning_rate']
    step = (learning_rate - learning_rate*decay_factor)/(saturate - start + 1)

    for i in xrange(epoch_num + 1):
        actual = lr.val_record[i]
        if i < start:
            expected = learning_rate
        elif i >= saturate:
            expected = learning_rate*decay_factor
        elif (start <= i) and (i < saturate):
            expected = decay_factor * learning_rate + (saturate - i) * step
        if not np.allclose(actual, expected):
            raise AssertionError("After %d epochs, expected learning rate to "
                                 "be %f, but it is %f." %
                                 (i, expected, actual))


def test_monitor_based_lr():
    # tests that the class MonitorBasedLRAdjuster in sgd.py
    # gets the learning rate properly over the training epochs
    # it runs a small softmax and at the end checks the learning values. It
    # runs 2 loops. Each loop evaluates one of the if clauses when checking
    # the observation channels. Otherwise, longer training epochs are needed
    # to observe both if and elif cases.

    high_trigger = 1.0
    shrink_amt = 0.99
    low_trigger = 0.99
    grow_amt = 1.01
    min_lr = 1e-7
    max_lr = 1.

    dim = 3
    m = 10

    rng = np.random.RandomState([25, 9, 2012])

    X = rng.randn(m, dim)

    dataset = DenseDesignMatrix(X=X)

    m = 15
    X = rng.randn(m, dim)
    learning_rate = 1e-2
    batch_size = 5

    # We need to include this so the test actually stops running at some point
    epoch_num = 5

    # including a monitoring datasets lets us test that
    # the monitor works with supervised data
    monitoring_dataset = DenseDesignMatrix(X=X)

    cost = DummyCost()

    for i in xrange(2):

        if i == 1:
            high_trigger = 0.99

        model = SoftmaxModel(dim)

        termination_criterion = EpochCounter(epoch_num)

        algorithm = SGD(learning_rate,
                        cost,
                        batch_size=batch_size,
                        monitoring_batches=3,
                        monitoring_dataset=monitoring_dataset,
                        termination_criterion=termination_criterion,
                        update_callbacks=None,
                        init_momentum=None,
                        set_batch_size=False)

        monitor_lr = MonitorBasedLRAdjuster(high_trigger=high_trigger,
                                            shrink_amt=shrink_amt,
                                            low_trigger=low_trigger,
                                            grow_amt=grow_amt,
                                            min_lr=min_lr,
                                            max_lr=max_lr)

        train = Train(dataset,
                      model,
                      algorithm,
                      save_path=None,
                      save_freq=0,
                      extensions=[monitor_lr])

        train.main_loop()

        v = model.monitor.channels['objective'].val_record
        lr = model.monitor.channels['learning_rate'].val_record
        lr_monitor = learning_rate

        for i in xrange(2, epoch_num + 1):
            if v[i-1] > high_trigger * v[i-2]:
                lr_monitor *= shrink_amt
            elif v[i-1] > low_trigger * v[i-2]:
                lr_monitor *= grow_amt
            lr_monitor = max(min_lr, lr_monitor)
            lr_monitor = min(max_lr, lr_monitor)
            assert np.allclose(lr_monitor, lr[i])


def test_bad_monitoring_input_in_monitor_based_lr():
    # tests that the class MonitorBasedLRAdjuster in sgd.py avoids wrong
    # settings of channel_name or dataset_name in the constructor.

    dim = 3
    m = 10

    rng = np.random.RandomState([06, 02, 2014])

    X = rng.randn(m, dim)

    learning_rate = 1e-2
    batch_size = 5

    # We need to include this so the test actually stops running at some point
    epoch_num = 2

    dataset = DenseDesignMatrix(X=X)

    # including a monitoring datasets lets us test that
    # the monitor works with supervised data
    monitoring_dataset = DenseDesignMatrix(X=X)

    cost = DummyCost()

    model = SoftmaxModel(dim)

    termination_criterion = EpochCounter(epoch_num)

    algorithm = SGD(learning_rate,
                    cost,
                    batch_size=batch_size,
                    monitoring_batches=2,
                    monitoring_dataset=monitoring_dataset,
                    termination_criterion=termination_criterion,
                    update_callbacks=None,
                    init_momentum=None,
                    set_batch_size=False)

    #testing for bad dataset_name input
    dummy = 'void'

    monitor_lr = MonitorBasedLRAdjuster(dataset_name=dummy)

    train = Train(dataset,
                  model,
                  algorithm,
                  save_path=None,
                  save_freq=0,
                  extensions=[monitor_lr])
    try:
        train.main_loop()
    except ValueError as e:
        err_input = 'The dataset_name \'' + dummy + '\' is not valid.'
        channel_name = dummy + '_objective'
        err_message = ('There is no monitoring channel named \'' +
                       channel_name +
                       '\'. You probably need to specify a valid monitoring '
                       'channel by using either dataset_name or channel_name '
                       'in the MonitorBasedLRAdjuster constructor. ' +
                       err_input)
        assert err_message == str(e)
    except:
        raise AssertionError("MonitorBasedLRAdjuster takes dataset_name that "
                             "is invalid ")

    #testing for bad channel_name input
    monitor_lr2 = MonitorBasedLRAdjuster(channel_name=dummy)

    model2 = SoftmaxModel(dim)
    train2 = Train(dataset,
                   model2,
                   algorithm,
                   save_path=None,
                   save_freq=0,
                   extensions=[monitor_lr2])

    try:
        train2.main_loop()
    except ValueError as e:
        err_input = 'The channel_name \'' + dummy + '\' is not valid.'
        err_message = ('There is no monitoring channel named \'' + dummy +
                       '\'. You probably need to specify a valid monitoring '
                       'channel by using either dataset_name or channel_name '
                       'in the MonitorBasedLRAdjuster constructor. ' +
                       err_input)
        assert err_message == str(e)
    except:
        raise AssertionError("MonitorBasedLRAdjuster takes channel_name that "
                             "is invalid ")

    return


def testing_multiple_datasets_in_monitor_based_lr():
    # tests that the class MonitorBasedLRAdjuster in sgd.py does not take
    # multiple datasets in which multiple channels ending in '_objective'
    # exist.
    # This case happens when the user has not specified either channel_name or
    # dataset_name in the constructor

    dim = 3
    m = 10

    rng = np.random.RandomState([06, 02, 2014])

    X = rng.randn(m, dim)
    Y = rng.randn(m, dim)

    learning_rate = 1e-2
    batch_size = 5

    # We need to include this so the test actually stops running at some point
    epoch_num = 1

    # including a monitoring datasets lets us test that
    # the monitor works with supervised data
    monitoring_train = DenseDesignMatrix(X=X)
    monitoring_test = DenseDesignMatrix(X=Y)

    cost = DummyCost()

    model = SoftmaxModel(dim)

    dataset = DenseDesignMatrix(X=X)

    termination_criterion = EpochCounter(epoch_num)

    algorithm = SGD(learning_rate,
                    cost,
                    batch_size=batch_size,
                    monitoring_batches=2,
                    monitoring_dataset={'train': monitoring_train,
                                        'test': monitoring_test},
                    termination_criterion=termination_criterion,
                    update_callbacks=None,
                    init_momentum=None,
                    set_batch_size=False)

    monitor_lr = MonitorBasedLRAdjuster()

    train = Train(dataset,
                  model,
                  algorithm,
                  save_path=None,
                  save_freq=0,
                  extensions=[monitor_lr])

    try:
        train.main_loop()
    except ValueError:
        return

    raise AssertionError("MonitorBasedLRAdjuster takes multiple dataset names "
                         "in which more than one \"objective\" channel exist "
                         "and the user has not specified either channel_name "
                         "or database_name in the constructor to "
                         "disambiguate.")


def testing_multiple_datasets_with_specified_dataset_in_monitor_based_lr():
    # tests that the class MonitorBasedLRAdjuster in sgd.py can properly use
    # the spcified dataset_name in the constructor when multiple datasets
    # exist.

    dim = 3
    m = 10

    rng = np.random.RandomState([06, 02, 2014])

    X = rng.randn(m, dim)
    Y = rng.randn(m, dim)

    learning_rate = 1e-2
    batch_size = 5

    # We need to include this so the test actually stops running at some point
    epoch_num = 1

    # including a monitoring datasets lets us test that
    # the monitor works with supervised data
    monitoring_train = DenseDesignMatrix(X=X)
    monitoring_test = DenseDesignMatrix(X=Y)

    cost = DummyCost()

    model = SoftmaxModel(dim)

    dataset = DenseDesignMatrix(X=X)

    termination_criterion = EpochCounter(epoch_num)

    monitoring_dataset = {'train': monitoring_train, 'test': monitoring_test}

    algorithm = SGD(learning_rate,
                    cost,
                    batch_size=batch_size,
                    monitoring_batches=2,
                    monitoring_dataset=monitoring_dataset,
                    termination_criterion=termination_criterion,
                    update_callbacks=None,
                    init_momentum=None,
                    set_batch_size=False)

    dataset_name = monitoring_dataset.keys()[0]
    monitor_lr = MonitorBasedLRAdjuster(dataset_name=dataset_name)

    train = Train(dataset,
                  model,
                  algorithm,
                  save_path=None,
                  save_freq=0,
                  extensions=[monitor_lr])

    train.main_loop()


def test_sgd_topo():
    # tests that we can run the sgd algorithm
    # on data with topology
    # does not test for correctness at all, just
    # that the algorithm runs without dying

    rows = 3
    cols = 4
    channels = 2
    dim = rows * cols * channels
    m = 10

    rng = np.random.RandomState([25, 9, 2012])

    dataset = get_topological_dataset(rng, rows, cols, channels, m)

    # including a monitoring datasets lets us test that
    # the monitor works with supervised data
    m = 15
    monitoring_dataset = get_topological_dataset(rng, rows, cols, channels, m)

    model = TopoSoftmaxModel(rows, cols, channels)

    learning_rate = 1e-3
    batch_size = 5

    cost = SupervisedDummyCost()

    # We need to include this so the test actually stops running at some point
    termination_criterion = EpochCounter(5)

    algorithm = SGD(learning_rate,
                    cost,
                    batch_size=batch_size,
                    monitoring_batches=3,
                    monitoring_dataset=monitoring_dataset,
                    termination_criterion=termination_criterion,
                    update_callbacks=None,
                    init_momentum=None,
                    set_batch_size=False)

    train = Train(dataset,
                  model,
                  algorithm,
                  save_path=None,
                  save_freq=0,
                  extensions=None)

    train.main_loop()


def test_sgd_no_mon():

    # tests that we can run the sgd algorithm
    # wihout a monitoring dataset
    # does not test for correctness at all, just
    # that the algorithm runs without dying

    dim = 3
    m = 10

    rng = np.random.RandomState([25, 9, 2012])

    X = rng.randn(m, dim)

    idx = rng.randint(0, dim, (m,))
    Y = np.zeros((m, dim))
    for i in xrange(m):
        Y[i, idx[i]] = 1

    dataset = DenseDesignMatrix(X=X, y=Y)

    m = 15
    X = rng.randn(m, dim)

    idx = rng.randint(0, dim, (m,))
    Y = np.zeros((m, dim))
    for i in xrange(m):
        Y[i, idx[i]] = 1

    model = SoftmaxModel(dim)

    learning_rate = 1e-3
    batch_size = 5

    cost = SupervisedDummyCost()

    # We need to include this so the test actually stops running at some point
    termination_criterion = EpochCounter(5)

    algorithm = SGD(learning_rate,
                    cost,
                    batch_size=batch_size,
                    monitoring_dataset=None,
                    termination_criterion=termination_criterion,
                    update_callbacks=None,
                    init_momentum=None,
                    set_batch_size=False)

    train = Train(dataset,
                  model,
                  algorithm,
                  save_path=None,
                  save_freq=0,
                  extensions=None)

    train.main_loop()


def test_reject_mon_batch_without_mon():

    # tests that setting up the sgd algorithm
    # without a monitoring dataset
    # but with monitoring_batches specified is an error

    dim = 3
    m = 10

    rng = np.random.RandomState([25, 9, 2012])

    X = rng.randn(m, dim)

    idx = rng.randint(0, dim, (m,))
    Y = np.zeros((m, dim))
    for i in xrange(m):
        Y[i, idx[i]] = 1

    dataset = DenseDesignMatrix(X=X, y=Y)

    m = 15
    X = rng.randn(m, dim)

    idx = rng.randint(0, dim, (m, ))
    Y = np.zeros((m, dim))
    for i in xrange(m):
        Y[i, idx[i]] = 1

    model = SoftmaxModel(dim)

    learning_rate = 1e-3
    batch_size = 5

    cost = SupervisedDummyCost()

    try:
        algorithm = SGD(learning_rate,
                        cost,
                        batch_size=batch_size,
                        monitoring_batches=3,
                        monitoring_dataset=None,
                        update_callbacks=None,
                        init_momentum=None,
                        set_batch_size=False)
    except ValueError:
        return

    assert False


def test_sgd_sequential():

    # tests that requesting train_iteration_mode = 'sequential'
    # works

    dim = 1
    batch_size = 3
    m = 5 * batch_size

    dataset = ArangeDataset(m)

    model = SoftmaxModel(dim)

    learning_rate = 1e-3
    batch_size = 5

    visited = [False] * m

    def visit(X):
        assert X.shape[1] == 1
        assert np.all(X[1:] == X[0:-1]+1)
        start = int(X[0, 0])
        if start > 0:
            assert visited[start - 1]
        for i in xrange(batch_size):
            assert not visited[start+i]
            visited[start+i] = 1

    data_specs = (model.get_input_space(), model.get_input_source())
    cost = CallbackCost(visit, data_specs)

    # We need to include this so the test actually stops running at some point
    termination_criterion = EpochCounter(5)

    algorithm = SGD(learning_rate,
                    cost,
                    batch_size=batch_size,
                    train_iteration_mode='sequential',
                    monitoring_dataset=None,
                    termination_criterion=termination_criterion,
                    update_callbacks=None,
                    init_momentum=None,
                    set_batch_size=False)

    algorithm.setup(dataset=dataset, model=model)

    algorithm.train(dataset)

    assert all(visited)


def test_determinism():

    # Verifies that running SGD twice results in the same examples getting
    # visited in the same order

    for mode in _iteration_schemes:
        dim = 1
        batch_size = 3
        num_batches = 5
        m = num_batches * batch_size

        dataset = ArangeDataset(m)

        model = SoftmaxModel(dim)

        learning_rate = 1e-3
        batch_size = 5

        visited = [[-1] * m]

        def visit(X):
            mx = max(visited[0])
            counter = mx + 1
            for i in X[:, 0]:
                i = int(i)
                assert visited[0][i] == -1
                visited[0][i] = counter
                counter += 1

        data_specs = (model.get_input_space(), model.get_input_source())
        cost = CallbackCost(visit, data_specs)

        # We need to include this so the test actually stops running at some
        # point
        termination_criterion = EpochCounter(5)

        def run_algorithm():
            unsupported_modes = ['random_slice', 'random_uniform']
            algorithm = SGD(learning_rate,
                            cost,
                            batch_size=batch_size,
                            train_iteration_mode=mode,
                            monitoring_dataset=None,
                            termination_criterion=termination_criterion,
                            update_callbacks=None,
                            init_momentum=None,
                            set_batch_size=False)

            algorithm.setup(dataset=dataset, model=model)

            raised = False
            try:
                algorithm.train(dataset)
            except ValueError:
                print mode
                assert mode in unsupported_modes
                raised = True
            if mode in unsupported_modes:
                assert raised
                return True
            return False

        if run_algorithm():
            continue

        visited.insert(0, [-1] * m)

        del model.monitor

        run_algorithm()

        for v in visited:
            assert len(v) == m
            for elem in range(m):
                assert elem in v

        assert len(visited) == 2

        print visited[0]
        print visited[1]
        assert np.all(np.asarray(visited[0]) == np.asarray(visited[1]))


def test_determinism_2():

    """
    A more aggressive determinism test. Tests that apply nodes are all passed
    inputs with the same md5sums, apply nodes are run in same order, etc.  Uses
    disturb_mem to try to cause dictionaries to iterate in different orders,
    etc.
    """

    def run_sgd(mode):
        # Must be seeded the same both times run_sgd is called
        disturb_mem.disturb_mem()
        rng = np.random.RandomState([2012, 11, 27])

        batch_size = 5
        train_batches = 3
        valid_batches = 4
        num_features = 2

        # Synthesize dataset with a linear decision boundary
        w = rng.randn(num_features)

        def make_dataset(num_batches):
            disturb_mem.disturb_mem()
            m = num_batches*batch_size
            X = rng.randn(m, num_features)
            y = np.zeros((m, 1))
            y[:, 0] = np.dot(X, w) > 0.

            rval = DenseDesignMatrix(X=X, y=y)

            rval.yaml_src = ""  # suppress no yaml_src warning

            X = rval.get_batch_design(batch_size)
            assert X.shape == (batch_size, num_features)

            return rval

        train = make_dataset(train_batches)
        valid = make_dataset(valid_batches)

        num_chunks = 10
        chunk_width = 2

        class ManyParamsModel(Model):
            """
            Make a model with lots of parameters, so that there are many
            opportunities for their updates to get accidentally re-ordered
            non-deterministically. This makes non-determinism bugs manifest
            more frequently.
            """

            def __init__(self):
                self.W1 = [sharedX(rng.randn(num_features, chunk_width)) for i
                           in xrange(num_chunks)]
                disturb_mem.disturb_mem()
                self.W2 = [sharedX(rng.randn(chunk_width))
                           for i in xrange(num_chunks)]
                self._params = safe_union(self.W1, self.W2)
                self.input_space = VectorSpace(num_features)
                self.output_space = VectorSpace(1)

        disturb_mem.disturb_mem()
        model = ManyParamsModel()
        disturb_mem.disturb_mem()

        class LotsOfSummingCost(Cost):
            """
            Make a cost whose gradient on the parameters involves summing many
            terms together, so that T.grad is more likely to sum things in a
            random order.
            """

            supervised = True

            def expr(self, model, data, **kwargs):
                self.get_data_specs(model)[0].validate(data)
                X, Y = data
                disturb_mem.disturb_mem()

                def mlp_pred(non_linearity):
                    Z = [T.dot(X, W) for W in model.W1]
                    H = map(non_linearity, Z)
                    Z = [T.dot(h, W) for h, W in safe_izip(H, model.W2)]
                    pred = sum(Z)
                    return pred

                nonlinearity_predictions = map(mlp_pred,
                                               [T.nnet.sigmoid,
                                                T.nnet.softplus,
                                                T.sqr,
                                                T.sin])
                pred = sum(nonlinearity_predictions)
                disturb_mem.disturb_mem()

                return abs(pred-Y[:, 0]).sum()

            def get_data_specs(self, model):
                data = CompositeSpace((model.get_input_space(),
                                       model.get_output_space()))
                source = (model.get_input_source(), model.get_target_source())
                return (data, source)

        cost = LotsOfSummingCost()

        disturb_mem.disturb_mem()

        algorithm = SGD(cost=cost,
                        batch_size=batch_size,
                        init_momentum=.5,
                        learning_rate=1e-3,
                        monitoring_dataset={'train': train, 'valid': valid},
                        update_callbacks=[ExponentialDecay(decay_factor=2.,
                                                           min_lr=.0001)],
                        termination_criterion=EpochCounter(max_epochs=5))

        disturb_mem.disturb_mem()

        train_object = Train(dataset=train,
                             model=model,
                             algorithm=algorithm,
                             extensions=[PolyakAveraging(start=0),
                                         MomentumAdjustor(final_momentum=.9,
                                                          start=1,
                                                          saturate=5), ],
                             save_freq=0)

        disturb_mem.disturb_mem()

        train_object.main_loop()

    output = cStringIO.StringIO()
    record = Record(file_object=output, replay=False)
    record_mode = RecordMode(record)

    run_sgd(record_mode)

    output = cStringIO.StringIO(output.getvalue())
    playback = Record(file_object=output, replay=True)
    playback_mode = RecordMode(playback)

    run_sgd(playback_mode)


def test_lr_scalers():
    """
    Tests that SGD respects Model.get_lr_scalers
    """
    # We include a cost other than SumOfParams so that data is actually
    # queried from the training set, and the expected number of updates
    # are applied.
    cost = SumOfCosts([SumOfParams(), (0., DummyCost())])

    scales = [.01, .02, .05, 1., 5.]
    shapes = [(1,), (9,), (8, 7), (6, 5, 4), (3, 2, 2, 2)]

    learning_rate = .001

    class ModelWithScalers(Model):
        def __init__(self):
            self._params = [sharedX(np.zeros(shape)) for shape in shapes]
            self.input_space = VectorSpace(1)

        def __call__(self, X):
            # Implemented only so that DummyCost would work
            return X

        def get_lr_scalers(self):
            return dict(zip(self._params, scales))

    model = ModelWithScalers()

    dataset = ArangeDataset(1)

    sgd = SGD(cost=cost,
              learning_rate=learning_rate,
              init_momentum=0.,
              batch_size=1)

    sgd.setup(model=model, dataset=dataset)

    manual = [param.get_value() for param in model.get_params()]
    manual = [param - learning_rate * scale for param, scale in
              zip(manual, scales)]

    sgd.train(dataset=dataset)

    assert all(np.allclose(manual_param, sgd_param.get_value())
               for manual_param, sgd_param
               in zip(manual, model.get_params()))

    manual = [param - learning_rate * scale
              for param, scale
              in zip(manual, scales)]

    sgd.train(dataset=dataset)

    assert all(np.allclose(manual_param, sgd_param.get_value())
               for manual_param, sgd_param
               in zip(manual, model.get_params()))


def test_lr_scalers_momentum():
    """
    Tests that SGD respects Model.get_lr_scalers when using
    momentum.
    """
    # We include a cost other than SumOfParams so that data is actually
    # queried from the training set, and the expected number of updates
    # are applied.
    cost = SumOfCosts([SumOfParams(), (0., DummyCost())])

    scales = [.01, .02, .05, 1., 5.]
    shapes = [(1,), (9,), (8, 7), (6, 5, 4), (3, 2, 2, 2)]

    model = DummyModel(shapes, lr_scalers=scales)
    dataset = ArangeDataset(1)
    learning_rate = .001
    momentum = 0.5

    sgd = SGD(cost=cost,
              learning_rate=learning_rate,
              init_momentum=momentum,
              batch_size=1)

    sgd.setup(model=model, dataset=dataset)

    manual = [param.get_value() for param in model.get_params()]
    inc = [-learning_rate * scale for param, scale in zip(manual, scales)]
    manual = [param + i for param, i in zip(manual, inc)]

    sgd.train(dataset=dataset)

    assert all(np.allclose(manual_param, sgd_param.get_value())
               for manual_param, sgd_param
               in zip(manual, model.get_params()))

    manual = [param - learning_rate * scale + i * momentum
              for param, scale, i in
              zip(manual, scales, inc)]

    sgd.train(dataset=dataset)

    assert all(np.allclose(manual_param, sgd_param.get_value())
               for manual_param, sgd_param
               in zip(manual, model.get_params()))


def test_batch_size_specialization():

    # Tests that using a batch size of 1 for training and a batch size
    # other than 1 for monitoring does not result in a crash.
    # This catches a bug reported in the pylearn-dev@googlegroups.com
    # e-mail "[pylearn-dev] monitor assertion error: channel_X.type != X.type"
    # The training data was specialized to a row matrix (theano tensor with
    # first dim broadcastable) and the monitor ended up with expressions
    # mixing the specialized and non-specialized version of the expression.

    m = 2
    rng = np.random.RandomState([25, 9, 2012])
    X = np.zeros((m, 1))
    dataset = DenseDesignMatrix(X=X)

    model = SoftmaxModel(1)

    learning_rate = 1e-3

    cost = DummyCost()

    algorithm = SGD(learning_rate, cost,
                    batch_size=1,
                    monitoring_batches=1,
                    monitoring_dataset=dataset,
                    termination_criterion=EpochCounter(max_epochs=1),
                    update_callbacks=None,
                    set_batch_size=False)

    train = Train(dataset,
                  model,
                  algorithm,
                  save_path=None,
                  save_freq=0,
                  extensions=None)

    train.main_loop()


if __name__ == '__main__':
    test_monitor_based_lr()

########NEW FILE########
__FILENAME__ = training_algorithm
"""Module defining the interface for training algorithms."""
from pylearn2.datasets.dataset import Dataset

class TrainingAlgorithm(object):
    """
    An abstract superclass that defines the interface of training
    algorithms.
    """

    def _register_update_callbacks(self, update_callbacks):
        """
        .. todo::

            WRITEME
        """
        if update_callbacks is None:
            update_callbacks = []
        # If it's iterable, we're fine. If not, it's a single callback,
        # so wrap it in a list.
        try:
            iter(update_callbacks)
            self.update_callbacks = update_callbacks
        except TypeError:
            self.update_callbacks = [update_callbacks]

    def setup(self, model, dataset):
        """
        Initialize the given training algorithm.

        Parameters
        ----------
        model : object
            Object that implements the Model interface defined in
            `pylearn2.models`.
        dataset : object
            Object that implements the Dataset interface defined in
            `pylearn2.datasets`.

        Notes
        -----
        Called by the training script prior to any calls involving data.
        This is a good place to compile theano functions for doing learning.
        """
        self.model = model

    def train(self, dataset):
        """
        Performs some amount of training, generally one "epoch" of online
        learning

        Parameters
        ----------
        dataset : object
            Object implementing the dataset interface defined in
            `pylearn2.datasets.dataset.Dataset`.

        Returns
        -------
        None
        """
        raise NotImplementedError()

    def _set_monitoring_dataset(self, monitoring_dataset):
        """
        .. todo::

            WRITEME

        Parameters
        ----------
        monitoring_dataset : None or Dataset or dict
            None for no monitoring, or Dataset, to monitor on one dataset,
            or dict mapping string names to Datasets
        """
        if isinstance(monitoring_dataset, Dataset):
            self.monitoring_dataset = { '': monitoring_dataset }
        else:
            if monitoring_dataset is not None:
                assert isinstance(monitoring_dataset, dict)
                for key in monitoring_dataset:
                    assert isinstance(key, str)
                    value = monitoring_dataset[key]
                    if not isinstance(value, Dataset):
                        raise TypeError("Monitoring dataset with name " + key +
                                        " is not a dataset, it is a " +
                                        str(type(value)))
            self.monitoring_dataset = monitoring_dataset

    def continue_learning(self, model):
        """
        Return True to continue learning. Called after the Monitor
        has been run on the latest parameters so the monitor may be used
        to determine convergence.

        Parameters
        ----------
        model : WRITEME
        """
        raise NotImplementedError(str(type(self))+" does not implement " +
                                  "continue_learning.")

    def _synchronize_batch_size(self, model):
        """
        Adapts `self.batch_size` to be consistent with `model`

        Parameters
        ----------
        model : Model
            The model to synchronize the batch size with
        """
        batch_size = self.batch_size
        if hasattr(model, "force_batch_size"):
            if model.force_batch_size > 0:
                if batch_size is not None:
                    if batch_size != model.force_batch_size:
                        if self.set_batch_size:
                            model.set_batch_size(batch_size)
                        else:
                            raise ValueError("batch_size argument to " +
                                             str(type(self)) +
                                             "conflicts with model's " +
                                             "force_batch_size attribute")
                else:
                    self.batch_size = model.force_batch_size
        if self.batch_size is None:
            raise NoBatchSizeError()

class NoBatchSizeError(ValueError):
    """
    An exception raised when the user does not specify a batch size anywhere.
    """
    def __init__(self):
        super(NoBatchSizeError, self).__init__("Neither the "
                "TrainingAlgorithm nor the model were given a specification "
                "of the batch size.")


########NEW FILE########
__FILENAME__ = best_params
"""
.. todo::

    WRITEME
"""
import logging
import os.path
import socket
import numpy
np = numpy
from pylearn2.train_extensions import TrainExtension
import theano
import theano.tensor as T
from pylearn2.utils import serial

log = logging.getLogger(__name__)


class KeepBestParams(TrainExtension):
    """
    A callback which keeps track of a model's best parameters based on its
    performance for a given cost on a given dataset.

    Parameters
    ----------
    model : pylearn2.models.model.Model
        the model whose best parameters we want to keep track of
    cost : tensor_like
        cost function used to evaluate the model's performance
    monitoring_dataset : pylearn2.datasets.dataset.Dataset
        dataset on which to compute the cost
    batch_size : int
        size of the batches used to compute the cost
    """

    def __init__(self, model, cost, monitoring_dataset, batch_size):
        self.model = model
        self.cost = cost
        self.dataset = monitoring_dataset
        self.batch_size = batch_size
        self.minibatch = T.matrix('minibatch')
        self.target = T.matrix('target')
        if cost.supervised:
            self.supervised = True
            self.cost_function = theano.function(inputs=[self.minibatch,
                                                          self.target],
                                                  outputs=cost(model,
                                                               self.minibatch,
                                                               self.target))
        else:
            self.supervised = False
            self.cost_function = theano.function(inputs=[self.minibatch],
                                                 outputs=cost(model,
                                                              self.minibatch))
        self.best_cost = numpy.inf
        self.best_params = model.get_param_values()

    def on_monitor(self, model, dataset, algorithm):
        """
        Looks whether the model performs better than earlier. If it's the
        case, records the model's parameters.

        Parameters
        ----------
        model : pylearn2.models.model.Model
            Not used
        dataset : pylearn2.datasets.dataset.Dataset
            Not used
        algorithm : TrainingAlgorithm
            Not used
        """
        if self.supervised:
            it = self.dataset.iterator('sequential',
                                       batch_size=self.batch_size,
                                       targets=True)
            new_cost = numpy.mean([self.cost_function(minibatch, target)
                                   for minibatch, target in it])
        else:
            it = self.dataset.iterator('sequential',
                                       batch_size=self.batch_size,
                                       targets=False)
            new_cost = numpy.mean([self.cost_function(minibatch)
                                   for minibatch in it])
        if new_cost < self.best_cost:
            self.best_cost = new_cost
            self.best_params = self.model.get_param_values()

    def get_best_params(self):
        """Returns the best parameters up to now for the model."""
        return self.best_params


class MonitorBasedSaveBest(TrainExtension):
    """
    A callback that saves a copy of the model every time it achieves
    a new minimal value of a monitoring channel.

    Parameters
    ----------
    channel_name : str
        The name of the channel we want to minimize
    save_path : str
        Path to save the best model to
    higher_is_better : bool, optional
        WRITEME
    tag_key : str, optional
        A unique key to use for storing diagnostic information in
        `model.tag`. If `None`, use the class name (default).
    """

    def __init__(self, channel_name, save_path,higher_is_better=False,
                 tag_key=None):
        self.__dict__.update(locals())
        del self.self
        if higher_is_better:
            self.coeff = -1.
        else:
            self.coeff = 1.
        self.best_cost = np.inf

        # If no tag key is provided, use the class name by default.
        if tag_key is None:
            tag_key = self.__class__.__name__
        self._tag_key = tag_key

    def setup(self, model, dataset, algorithm):
        """
        Sets some model tag entries.

        Parameters
        ----------
        model : pylearn2.models.model.Model
        dataset : pylearn2.datasets.dataset.Dataset
            Not used
        algorithm : TrainingAlgorithm
            Not used
        """
        if self._tag_key in model.tag:
            log.warning('Model tag key "%s" already found. This may indicate '
                        'multiple instances of %s trying to use the same tag '
                        'entry.',
                        self._tag_key, self.__class__.__name__)
            log.warning('If this is the case, specify tag key manually in '
                        '%s constructor.', self.__class__.__name__)
        # This only needs to be written once.
        model.tag[self._tag_key]['channel_name'] = self.channel_name
        # Useful information for locating the saved model.
        model.tag[self._tag_key]['save_path'] = os.path.abspath(self.save_path)
        model.tag[self._tag_key]['hostname'] = socket.gethostname()
        self._update_tag(model)

    def on_monitor(self, model, dataset, algorithm):
        """
        Looks whether the model performs better than earlier. If it's the
        case, saves the model.

        Parameters
        ----------
        model : pylearn2.models.model.Model
            model.monitor must contain a channel with name given by
            self.channel_name
        dataset : pylearn2.datasets.dataset.Dataset
            Not used
        algorithm : TrainingAlgorithm
            Not used
        """

        monitor = model.monitor
        channels = monitor.channels
        channel = channels[self.channel_name]
        val_record = channel.val_record
        new_cost = self.coeff * val_record[-1]


        if new_cost < self.best_cost:
            self.best_cost = new_cost
            # Update the tag of the model object before saving it.
            self._update_tag(model)
            serial.save(self.save_path, model, on_overwrite = 'backup')

    def _update_tag(self, model):
        """
        Update `model.tag` with information about the current best.

        Parameters
        ----------
        model : pylearn2.models.model.Model
            The model to update.
        """
        # More stuff to be added later. For now, we care about the best cost.
        model.tag[self._tag_key]['best_cost'] = self.best_cost

########NEW FILE########
__FILENAME__ = test_monitor_based_save_best
"""Tests for the MonitorBasedSaveBest class."""

__author__ = "David Warde-Farley"
__copyright__ = "Copyright 2014, Universite de Montreal"
__credits__ = ["David Warde-Farley"]
__license__ = "3-clause BSD"
__email__ = "wardefar@iro"
__maintainer__ = "David Warde-Farley"

import os
import tempfile
from pylearn2.models.model import Model
from pylearn2.train_extensions.best_params import MonitorBasedSaveBest


class MockModel(Model):
    """An empty model."""
    def __init__(self):
        pass


class MockChannel(object):
    """A mock object for MonitorChannel."""
    def __init__(self):
        self.val_record = []


class MockMonitor(object):
    """A mock object for Monitor."""
    def __init__(self):
        self.channels = {}


def test_tagging():
    """Test the tagging functionality of this extension."""
    try:
        # TODO: serial.save should be able to take an open file-like object so
        # we can direct its output to a StringIO or something and not need to
        # screw around like this in tests that don't actually need to touch
        # the filesystem. /dev/null would work but the test would fail on
        # Windows.
        fd, fn = tempfile.mkstemp(suffix='.pkl')
        os.close(fd)

        # Test that the default key gets created.
        def_model = MockModel()
        def_model.monitor = MockMonitor()
        def_ext = MonitorBasedSaveBest(channel_name='foobar', save_path=fn)
        def_ext.setup(def_model, None, None)
        assert 'MonitorBasedSaveBest' in def_model.tag

        # Test with a custom key.
        model = MockModel()
        model.monitor = MockMonitor()
        model.monitor.channels['foobar'] = MockChannel()
        ext = MonitorBasedSaveBest(channel_name='foobar', tag_key='test123',
                                   save_path=fn)
        # Best cost is initially infinity.
        ext.setup(model, None, None)
        assert model.tag['test123']['best_cost'] == float("inf")
        # Best cost after one iteration.
        model.monitor.channels['foobar'].val_record.append(5.0)
        ext.on_monitor(model, None, None)
        assert model.tag['test123']['best_cost'] == 5.0
        # Best cost after a second, worse iteration.
        model.monitor.channels['foobar'].val_record.append(7.0)
        ext.on_monitor(model, None, None)
        assert model.tag['test123']['best_cost'] == 5.0
        # Best cost after a third iteration better than 2 but worse than 1.
        model.monitor.channels['foobar'].val_record.append(6.0)
        ext.on_monitor(model, None, None)
        assert model.tag['test123']['best_cost'] == 5.0
        # Best cost after a fourth, better iteration.
        model.monitor.channels['foobar'].val_record.append(3.0)
        ext.on_monitor(model, None, None)
        assert model.tag['test123']['best_cost'] == 3.0
    finally:
        os.remove(fn)

########NEW FILE########
__FILENAME__ = test_window_flip
import hashlib
import itertools
import numpy
from pylearn2.train_extensions.window_flip import WindowAndFlip
from pylearn2.train_extensions.window_flip import WindowAndFlipC01B

from pylearn2.datasets.dense_design_matrix import (
    DenseDesignMatrix,
    DefaultViewConverter
)
from pylearn2.utils.testing import assert_equal, assert_contains, assert_


class DummyDataset(DenseDesignMatrix):
    def __init__(self, axes=('c', 0, 1, 'b')):
        assert_contains([('c', 0, 1, 'b'), ('b', 0, 1, 'c')], axes)
        axes = list(axes)
        vc = DefaultViewConverter((5, 5, 2), axes=axes)
        rng = numpy.random.RandomState([2013, 3, 12])
        X = rng.normal(size=(4, 50)).astype('float32')
        super(DummyDataset, self).__init__(X=X, view_converter=vc, axes=axes)


def _hash_array(arr):
    h = hashlib.sha1(arr.copy())
    return h.hexdigest()


def test_window_flip_coverage():
    # Old interface WindowAndFlipC01B
    yield check_window_flip_coverage_C01B, True, True
    yield check_window_flip_coverage_C01B, False, True
    # New interface WindowAndFlip
    yield check_window_flip_coverage_C01B, True
    yield check_window_flip_coverage_C01B, False
    yield check_window_flip_coverage_B01C, True
    yield check_window_flip_coverage_B01C, False


def check_window_flip_coverage_C01B(flip, use_old_c01b=False):
    ddata = DummyDataset(axes=('c', 0, 1, 'b'))
    topo = ddata.get_topological_view()
    ref_win = [set() for _ in xrange(4)]
    for b in xrange(topo.shape[-1]):
        for i in xrange(3):
            for j in xrange(3):
                window = topo[:, i:i + 3, j:j + 3, b]
                assert_equal((3, 3), window.shape[1:])
                ref_win[b].add(_hash_array(window))
                if flip:
                    ref_win[b].add(_hash_array(window[:, :, ::-1]))
    actual_win = [set() for _ in xrange(4)]

    if use_old_c01b:
        wf_cls = WindowAndFlipC01B
    else:
        wf_cls = WindowAndFlip

    wf = wf_cls(window_shape=(3, 3), randomize=[ddata], flip=flip)
    wf.setup(None, ddata, None)
    curr_topo = ddata.get_topological_view()
    assert_equal((2, 3, 3, 4), curr_topo.shape)
    for b in xrange(topo.shape[-1]):
        hashed = _hash_array(curr_topo[..., b])
        assert_contains(ref_win[b], hashed)
        actual_win[b].add(hashed)
    while not all(len(a) == len(b) for a, b in zip(ref_win, actual_win)):
        prev_topo = curr_topo.copy()
        wf.on_monitor(None, ddata, None)
        curr_topo = ddata.get_topological_view()
        assert_(not (prev_topo == curr_topo).all())
        for b in xrange(topo.shape[-1]):
            hashed = _hash_array(curr_topo[..., b])
            assert_contains(ref_win[b], hashed)
            actual_win[b].add(hashed)


def check_window_flip_coverage_B01C(flip):
    ddata = DummyDataset(axes=('b', 0, 1, 'c'))
    topo = ddata.get_topological_view()
    ref_win = [set() for _ in xrange(4)]
    for b in xrange(topo.shape[0]):
        for i in xrange(3):
            for j in xrange(3):
                window = topo[b, i:i + 3, j:j + 3, :]
                assert_equal((3, 3), window.shape[:-1])
                ref_win[b].add(_hash_array(window))
                if flip:
                    ref_win[b].add(_hash_array(window[:, ::-1, :]))
    actual_win = [set() for _ in xrange(4)]
    wf = WindowAndFlip(window_shape=(3, 3), randomize=[ddata], flip=flip)
    wf.setup(None, ddata, None)
    curr_topo = ddata.get_topological_view()
    assert_equal((4, 3, 3, 2), curr_topo.shape)
    for b in xrange(topo.shape[0]):
        hashed = _hash_array(curr_topo[b, ...])
        assert_contains(ref_win[b], hashed)
        actual_win[b].add(hashed)
    while not all(len(a) == len(b) for a, b in zip(ref_win, actual_win)):
        prev_topo = curr_topo.copy()
        wf.on_monitor(None, ddata, None)
        curr_topo = ddata.get_topological_view()
        assert_(not (prev_topo == curr_topo).all())
        for b in xrange(topo.shape[0]):
            hashed = _hash_array(curr_topo[b, ...])
            assert_contains(ref_win[b], hashed)
            actual_win[b].add(hashed)


def test_padding():
    # Old interface WindowAndFlipC01B
    yield check_padding, ('c', 0, 1, 'b'), True
    # New interface WindowAndFlip
    yield check_padding, ('c', 0, 1, 'b')
    yield check_padding, ('b', 0, 1, 'c')


def check_padding(axes, use_old_c01b=False):

    padding = 3
    ddata = DummyDataset()
    topo = ddata.get_topological_view()

    if use_old_c01b:
        wf_cls = WindowAndFlipC01B
    else:
        wf_cls = WindowAndFlip

    wf = wf_cls(window_shape=(5, 5), randomize=[ddata],
                           pad_randomized=padding)
    wf.setup(None, None, None)
    new_topo = ddata.get_topological_view()
    assert_equal(topo.shape, new_topo.shape)
    saw_padding = dict([((direction, amount), False) for direction, amount
                        in itertools.product(['l', 'b', 'r', 't'],
                                             xrange(padding))])
    iters = 0
    while not all(saw_padding.values()) and iters < 50:
        for image in new_topo.swapaxes(0, 3):
            for i in xrange(padding):
                if (image[:i] == 0).all():
                    saw_padding['t', i] = True
                if (image[-i:] == 0).all():
                    saw_padding['b', i] = True
                if (image[:, -i:] == 0).all():
                    saw_padding['r', i] = True
                if (image[:, :i] == 0).all():
                    saw_padding['l', i] = True
        wf.on_monitor(None, None, None)
        new_topo = ddata.get_topological_view()
        iters += 1


def test_WindowAndFlipC01B_axes_guard():
    ddata = DummyDataset(axes=('b', 0, 1, 'c'))
    raised_error = False
    try:
        wf = WindowAndFlipC01B(window_shape=(3, 3), randomize=[ddata])
    except ValueError:
        raised_error = True
    assert_equal(raised_error, True)

########NEW FILE########
__FILENAME__ = window_flip
"""
TrainExtensions for doing random spatial windowing and flipping of an
image dataset on every epoch.
"""
import warnings
import numpy
from . import TrainExtension
from pylearn2.datasets.preprocessing import CentralWindow
from pylearn2.utils.rng import make_np_rng

try:
    from ..utils._window_flip import random_window_and_flip_c01b
    from ..utils._window_flip import random_window_and_flip_b01c
except ImportError:
    raise ImportError("Import of Cython module failed. Please make sure you "
                      "have run 'python setup.py develop' in the pylearn2 "
                      "directory")

__authors__ = "David Warde-Farley"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["David Warde-Farley"]
__license__ = "3-clause BSD"
__maintainer__ = "David Warde-Farley"
__email__ = "wardefar@iro"


def _zero_pad(array, amount, axes=(1, 2)):
    """
    .. todo::

        WRITEME
    """
    if amount == 0:
        return array
    new_shape = []
    slices = []
    for i, s in enumerate(array.shape):
        if i in axes:
            new_shape.append(s + 2 * amount)
            slices.append(slice(amount, -amount))
        else:
            new_shape.append(s)
            slices.append(slice(None))
    new_shape = tuple(new_shape)
    slices = tuple(slices)
    new_array = numpy.zeros(new_shape, dtype=array.dtype)
    new_array[slices] = array
    return new_array


class WindowAndFlip(TrainExtension):
    """
    An extension that allows an image dataset to be flipped and
    windowed after each epoch of training.

    Parameters
    ----------
    window_shape : WRITEME
    randomize : list, optional
        If specified, a list of Datasets to randomly window and
        flip at each epoch.
    randomize_once : list, optional
        If specified, a list of Datasets to randomly window and
        flip once at the start of training.
    center : list, optional
        If specified, a list of Datasets to centrally window
        once at the start of training.
    rng : numpy.random.RandomState object or seed, optional
        A random number generator or seed used to create one.
        Seeded deterministically by default.
    pad_randomized : int, optional
        Amount of padding to add to each side of the images
        in `randomize` and `randomize_once`. Useful if you
        want to do zero-padded windowing with `window_shape`
        the actual size of the dataset, and validate/test on
        full-size images instead of central patches. Default
        is 0.
    flip : bool, optional
        Reflect images on the horizontal axis with probability
        0.5. `True` by default.
    """
    def __init__(self, window_shape, randomize=None, randomize_once=None,
            center=None, rng=(2013, 02, 20), pad_randomized=0, flip=True):
        self._window_shape = tuple(window_shape)
        self._original = None

        self._randomize = randomize if randomize else []
        self._randomize_once = randomize_once if randomize_once else []
        self._center = center if center else []
        self._pad_randomized = pad_randomized
        self._flip = flip

        if randomize is None and randomize_once is None and center is None:
            warnings.warn(self.__class__.__name__ + " instantiated without "
                          "any dataset arguments, and therefore does nothing",
                          stacklevel=2)

        self._rng = make_np_rng(rng, which_method="random_integers")

    def setup(self, model, dataset, algorithm):
        """
        .. todo::

            WRITEME

        Notes
        -----
        `dataset` argument is ignored
        """
        dataset = None

        # Central windowing of auxiliary datasets (e.g. validation sets)
        preprocessor = CentralWindow(self._window_shape)
        for data in self._center:
            preprocessor.apply(data)

        # Do the initial random windowing
        randomize_now = self._randomize + self._randomize_once
        self._original = dict((data,
            _zero_pad(data.get_topological_view().astype('float32'),
                self._pad_randomized)) for data in randomize_now)
        self.randomize_datasets(randomize_now)

    def randomize_datasets(self, datasets):
        """
        Applies random translations and flips to the selected datasets.

        Parameters
        ----------
        datasets : WRITEME
        """
        for dataset in datasets:
            if tuple(dataset.view_converter.axes) == ('c', 0, 1, 'b'):
                wf_func = random_window_and_flip_c01b
            elif tuple(dataset.view_converter.axes) == ('b', 0, 1, 'c'):
                wf_func = random_window_and_flip_b01c
            else:
                raise ValueError("Axes of dataset is not supported: %s" %
                                 (str(dataset.view_converter.axes)))
            arr = wf_func(self._original[dataset],
                          self._window_shape,
                          rng=self._rng, flip=self._flip)
            dataset.set_topological_view(arr, axes=dataset.view_converter.axes)

    def on_monitor(self, model, dataset, algorithm):
        """
        .. todo::

            WRITEME

        Notes
        -----
        All arguments are ignored.
        """
        model = None
        dataset = None
        algorithm = None

        self.randomize_datasets(self._randomize)


class WindowAndFlipC01B(WindowAndFlip):
    """
    A specialized version of WindowAndFlip accepting datasets with axes C01B.
    It exists due to backward compatibility.

    Parameters
    ----------
    window_shape : WRITEME
    randomize : list, optional
        If specified, a list of Datasets to randomly window and
        flip at each epoch.
    randomize_once : list, optional
        If specified, a list of Datasets to randomly window and
        flip once at the start of training.
    center : list, optional
        If specified, a list of Datasets to centrally window
        once at the start of training.
    rng : numpy.random.RandomState object or seed, optional
        A random number generator or seed used to create one.
        Seeded deterministically by default.
    pad_randomized : int, optional
        Amount of padding to add to each side of the images
        in `randomize` and `randomize_once`. Useful if you
        want to do zero-padded windowing with `window_shape`
        the actual size of the dataset, and validate/test on
        full-size images instead of central patches. Default
        is 0.
    flip : bool, optional
        Reflect images on the horizontal axis with probability
        0.5. `True` by default.
    """

    def __init__(self, window_shape, randomize=None, randomize_once=None,
            center=None, rng=(2013, 02, 20), pad_randomized=0, flip=True):

        _randomize = randomize if randomize else []
        _randomize_once = randomize_once if randomize_once else []

        for data in _randomize + _randomize_once:
            if tuple(data.view_converter.axes) != ('c', 0, 1, 'b'):
                raise ValueError("Expected axes: ('c', 0, 1, 'b') "
                                 "Actual axes: %s" %
                                 str(tuple(data.view_converter.axes)))

        warnings.warn("WindowAndFlipC01B is deprecated, use WindowAndFlip. " +
                      "WindowAndFlipC01B will be removed on or " +
                      "after August 25, 2014.", stacklevel=2)

        super(WindowAndFlipC01B, self).__init__(window_shape,
                                                randomize=randomize,
                                                randomize_once=randomize_once,
                                                center=center,
                                                rng=rng,
                                                pad_randomized=pad_randomized,
                                                flip=flip)

########NEW FILE########
__FILENAME__ = bit_strings
"""Utilities for manipulating binary strings/masks."""
__author__ = "David Warde-Farley"
__copyright__ = "Copyright 2012, Universite de Montreal"
__credits__ = ["David Warde-Farley"]
__license__ = "3-clause BSD"
__email__ = "wardefar@iro"
__maintainer__ = "David Warde-Farley"

import numpy as np


def all_bit_strings(bits, dtype='uint8'):
    """
    Create a matrix of all binary strings of a given width as the rows.

    Parameters
    ----------
    bits : int
        The number of bits to count through.

    dtype : str or dtype object
        The dtype of the returned array.

    Returns
    -------
    bit_strings : ndarray, shape (2 ** bits, bits)
        The numbers from 0 to 2 ** bits - 1 as binary numbers, most
        significant bit first.

    Notes
    -----
    Obviously the memory requirements of this are exponential in the first
    argument, so use with caution.
    """
    return np.array([map(int, np.binary_repr(i, width=bits))
                     for i in xrange(0, 2 ** bits)], dtype=dtype)

########NEW FILE########
__FILENAME__ = call_check
"""
Utility functions for checking passed arguments against call signature
of a function or class constructor.
"""
import functools
import inspect
import types
from pylearn2.utils.string_utils import match

def check_call_arguments(to_call, kwargs):
    """
    Check the call signature against a dictionary of proposed arguments,
    raising an informative exception in the case of mismatch.

    Parameters
    ----------
    to_call : class or callable
        Function or class to examine (in the case of classes, the constructor
        call signature is analyzed).
    kwargs : dict
        Dictionary mapping parameter names (including positional arguments)
        to proposed values.
    """
    if 'self' in kwargs.keys():
        raise TypeError("Your dictionary includes an entry for 'self', "
                        "which is just asking for trouble")

    orig_to_call = getattr(to_call, '__name__', str(to_call))
    if not isinstance(to_call, types.FunctionType):
        if hasattr(to_call, '__init__'):
            to_call = to_call.__init__
        elif hasattr(to_call, '__call__'):
            to_call = to_call.__call__

    args, varargs, keywords, defaults = inspect.getargspec(to_call)

    if any(not isinstance(arg, str) for arg in args):
        raise TypeError('%s uses argument unpacking, which is deprecated and '
                        'unsupported by this pylearn2' % orig_to_call)

    if varargs is not None:
        raise TypeError('%s has a variable length argument list, but '
                        'this is not supported by config resolution' %
                        orig_to_call)

    if keywords is None:
        bad_keywords = [arg_name for arg_name in kwargs.keys()
                        if arg_name not in args]

        if len(bad_keywords) > 0:
            bad = ', '.join(bad_keywords)
            args = [ arg for arg in args if arg != 'self' ]
            if len(args) == 0:
                matched_str = '(It does not support any keywords, actually)'
            else:
                matched = [ match(keyword, args) for keyword in bad_keywords ]
                matched_str = 'Did you mean %s?' % (', '.join(matched))
            raise TypeError('%s does not support the following '
                            'keywords: %s. %s' %
                            (orig_to_call, bad, matched_str))

    if defaults is None:
        num_defaults = 0
    else:
        num_defaults = len(defaults)

    required = args[:len(args) - num_defaults]
    missing = [arg for arg in required if arg not in kwargs]

    if len(missing) > 0:
        #iff the im_self (or __self__) field is present, this is a
        # bound method, which has 'self' listed as an argument, but
        # which should not be supplied by kwargs
        is_bound = hasattr(to_call, 'im_self') or hasattr(to_call, '__self__')
        if len(missing) > 1 or missing[0] != 'self' or not is_bound:
            if 'self' in missing:
                missing.remove('self')
            missing = ', '.join([str(m) for m in missing])
            raise TypeError('%s did not get these expected '
                            'arguments: %s' % (orig_to_call, missing))

def checked_call(to_call, kwargs):
    """
    Attempt calling a function or instantiating a class with a given set of
    arguments, raising a more helpful exception in the case of argument
    mismatch.

    Parameters
    ----------
    to_call : class or callable
        Function or class to examine (in the case of classes, the constructor
        call signature is analyzed).
    kwargs : dict
        Dictionary mapping parameter names (including positional arguments)
        to proposed values.
    """
    try:
        return to_call(**kwargs)
    except TypeError:
        check_call_arguments(to_call, kwargs)
        raise

def sensible_argument_errors(func):
    """
    .. todo::

        WRITEME
    """
    @functools.wraps(func)
    def wrapped_func(*args, **kwargs):
        """
        .. todo::

            WRITEME
        """
        try:
            func(*args, **kwargs)
        except TypeError:
            argnames, varargs, keywords, defaults = inspect.getargspec(func)
            posargs = dict(zip(argnames, args))
            bad_keywords = []
            for keyword in kwargs:
                if keyword not in argnames:
                    bad_keywords.append(keyword)

            if len(bad_keywords) > 0:
                bad = ', '.join(bad_keywords)
                raise TypeError('%s() does not support the following '
                                'keywords: %s' % (str(func.func_name), bad))
            allargsgot = set(list(kwargs.keys()) + list(posargs.keys()))
            numrequired = len(argnames) - len(defaults)
            diff = list(set(argnames[:numrequired]) - allargsgot)
            if len(diff) > 0:
                raise TypeError('%s() did not get required args: %s' %
                                (str(func.func_name), ', '.join(diff)))
            raise
    return wrapped_func

########NEW FILE########
__FILENAME__ = common_strings
environment_variable_essay = """
Platform-specific instructions for setting environment variables:

Linux
=====
On most linux setups, you can define your environment variable by adding this
line to your ~/.bashrc file:

export PYLEARN2_VIEWER_COMMAND="eog --new-instance"

*** YOU MUST INCLUDE THE WORD "export". DO NOT JUST ASSIGN TO THE ENVIRONMENT VARIABLE ***
If you do not include the word "export", the environment variable will be set
in your bash shell, but will not be visible to processes that you launch from
it, like the python interpreter.

Don't forget that changes from your .bashrc file won't apply until you run

source ~/.bashrc

or open a new terminal window. If you're seeing this from an ipython notebook
you'll need to restart the ipython notebook, or maybe modify os.environ from
an ipython cell.

Mac OS X
========

Environment variables on Mac OS X work the same as in Linux, except you should
modify and run the "source" command on ~/.profile rather than ~/.bashrc.
"""

########NEW FILE########
__FILENAME__ = compile
"""Utilities related to the compilation of Theano functions."""
import functools

__author__ = "David Warde-Farley"
__copyright__ = "Copyright 2012, David Warde-Farley / Universite de Montreal"
__license__ = "3-clause BSD"
__maintainer__ = "David Warde-Farley"
__email__ = "wardefar@iro"
__all__ = ["compiled_theano_function", "HasCompiledFunctions"]


def compiled_theano_function(fn):
    """
    Method decorator that enables lazy on-demand compilation of Theano
    functions.

    Parameters
    ----------
    fn : bound method
        Method that takes exactly one parameter (i.e. `self`). This method
        should return a compiled Theano function when called.

    Notes
    -----
    This will return an object property (i.e. the `property()` construct)
    that returns a Theano function, and store it in a dictionary attribute
    called `_compiled_functions` on the object. If the function has
    already been accessed it is not compiled again.

    Objects wishing to take advantage of this decorator should inherit
    from `HasCompiledFunctions` in this module to have the object's
    `_compiled_functions` attribute removed upon pickling.

    Examples
    --------
    >>> from pylearn2.utils.compile import compiled_theano
    >>> import theano
    >>> class Foo(object):
    ...     @compiled_theano_function
    ...     def bar(self):
    ...         x = theano.tensor.vector()
    ...         y = theano.tensor.vector()
    ...         print "Compiling..."
    ...         return theano.function([x, y], theano.tensor.dot(x, y))
    ...
    >>> from numpy.random import randn, seed
    >>> o = Foo()
    >>> seed(0)
    >>> xx, yy = randn(50), randn(50)
    >>> o.bar(xx, yy)  # first call, method body will be run
    Compiling...
    array(-3.0349256483108418)
    >>> o.bar(xx, yy)  # function already compiled, no print.
    array(-3.0349256483108418)
    >>> o.bar(randn(5), randn(5))  # different args, still no print
    array(5.294487561729036)
    """
    @functools.wraps(fn)
    def wrapped(self):
        try:
            func = self._compiled_functions[fn.func_name]
        except (AttributeError, KeyError):
            if not hasattr(self, '_compiled_functions'):
                self._compiled_functions = {}
            self._compiled_functions[fn.func_name] = func = fn(self)
        return func
    return property(wrapped)


class HasCompiledFunctions(object):
    """
    Base class/mixin that will automatically strip a `_compiled_functions`
    attribute when pickling.
    """
    def __getstate__(self):
        """
        .. todo::

            WRITEME
        """
        state = self.__dict__.copy()
        if '_compiled_functions' in state:
            del state['_compiled_functions']
        return state

########NEW FILE########
__FILENAME__ = datasets
"""
Several utilities to evaluate an ALC on the dataset, to iterate over
minibatches from a dataset, or to merge three data with given proportions
"""
# Standard library imports
import logging
import os
import functools
from itertools import repeat
import warnings

# Third-party imports
import numpy
import scipy
import theano
try:
    from matplotlib import pyplot
    from mpl_toolkits.mplot3d import Axes3D
except ImportError:
    warnings.warn("Could not import some dependencies.")

# Local imports
from pylearn2.utils import sharedX
from pylearn2.utils.rng import make_np_rng


logger = logging.getLogger(__name__)


##################################################
# 3D Visualization
##################################################

def do_3d_scatter(x, y, z, figno=None, title=None):
    """
    Generate a 3D scatterplot figure and optionally give it a title.

    Parameters
    ----------
    x : WRITEME
    y : WRITEME
    z : WRITEME
    figno : WRITEME
    title : WRITEME
    """
    fig = pyplot.figure(figno)
    ax = Axes3D(fig)
    ax.scatter(x, y, z)
    ax.set_xlabel("X")
    ax.set_ylabel("Y")
    ax.set_zlabel("Z")
    pyplot.suptitle(title)

def save_plot(repr, path, name="figure.pdf", title="features"):
    """
    .. todo::

        WRITEME
    """
    # TODO : Maybe run a PCA if shape[1] > 3
    assert repr.get_value(borrow=True).shape[1] == 3

    # Take the first 3 columns
    x, y, z = repr.get_value(borrow=True).T
    do_3d_scatter(x, y, z)

    # Save the produces figure
    filename = os.path.join(path, name)
    pyplot.savefig(filename, format="pdf")
    logger.info('... figure saved: {0}'.format(filename))

##################################################
# Features or examples filtering
##################################################

def filter_labels(train, label, classes=None):
    """
    Filter examples of train for which we have labels

    Parameters
    ----------
    train : WRITEME
    label : WRITEME
    classes : WRITEME

    Returns
    -------
    WRITEME
    """
    if isinstance(train, theano.tensor.sharedvar.SharedVariable):
        train = train.get_value(borrow=True)
    if isinstance(label, theano.tensor.sharedvar.SharedVariable):
        label = label.get_value(borrow=True)

    if not (isinstance(train, numpy.ndarray) or scipy.sparse.issparse(train)):
        raise TypeError('train must be a numpy array, a scipy sparse matrix,'
                        ' or a theano shared array')

    # Examples for which any label is set
    if classes is not None:
        label = label[:, classes]

    # Special case for sparse matrices
    if scipy.sparse.issparse(train):
        idx = label.sum(axis=1).nonzero()[0]
        return (train[idx], label[idx])

    # Compress train and label arrays according to condition
    condition = label.any(axis=1)
    return tuple(var.compress(condition, axis=0) for var in (train, label))

def nonzero_features(data, combine=None):
    """
    Get features for which there are nonzero entries in the data.

    Parameters
    ----------
    data : list of matrices
        List of data matrices, either in sparse format or not.
        They must have the same number of features (column number).
    combine : function, optional
        A function to combine elementwise which features to keep.
        Default keeps the intersection of each non-zero columns.

    Returns
    -------
    indices : ndarray object
        Indices of the nonzero features.

    Notes
    -----
    I would return a mask (bool array) here, but scipy.sparse doesn't appear to
    fully support advanced indexing.
    """

    if combine is None:
        combine = functools.partial(reduce, numpy.logical_and)

    # Assumes all values are >0, which is the case for all sparse datasets.
    masks = numpy.asarray([subset.sum(axis=0) for subset in data]).squeeze()
    nz_feats = combine(masks).nonzero()[0]

    return nz_feats


# TODO: Is this a duplicate?
def filter_nonzero(data, combine=None):
    """
    Filter non-zero features of data according to a certain combining function

    Parameters
    ----------
    data : list of matrices
        List of data matrices, either in sparse format or not.
        They must have the same number of features (column number).
    combine : function
        A function to combine elementwise which features to keep.
        Default keeps the intersection of each non-zero columns.

    Returns
    -------
    indices : ndarray object
        Indices of the nonzero features.
    """

    nz_feats = nonzero_features(data, combine)

    return [set[:, nz_feats] for set in data]

##################################################
# Iterator object for minibatches of datasets
##################################################

class BatchIterator(object):
    """
    Builds an iterator object that can be used to go through the minibatches
    of a dataset, with respect to the given proportions in conf

    Parameters
    ----------
    dataset : WRITEME
    set_proba : WRITEME
    batch_size : WRITEME
    seed : WRITEME
    """

    def __init__(self, dataset, set_proba, batch_size, seed=300):
        # Local shortcuts for array operations
        flo = numpy.floor
        sub = numpy.subtract
        mul = numpy.multiply
        div = numpy.divide
        mod = numpy.mod

        # Record external parameters
        self.batch_size = batch_size
        if (isinstance(dataset[0], theano.Variable)):
            self.dataset = [set.get_value(borrow=True) for set in dataset]
        else:
            self.dataset = dataset

        # Compute maximum number of samples for one loop
        set_sizes = [set.shape[0] for set in self.dataset]
        set_batch = [float(self.batch_size) for i in xrange(3)]
        set_range = div(mul(set_proba, set_sizes), set_batch)
        set_range = map(int, numpy.ceil(set_range))

        # Upper bounds for each minibatch indexes
        set_limit = numpy.ceil(numpy.divide(set_sizes, set_batch))
        self.limit = map(int, set_limit)

        # Number of rows in the resulting union
        set_tsign = sub(set_limit, flo(div(set_sizes, set_batch)))
        set_tsize = mul(set_tsign, flo(div(set_range, set_limit)))

        l_trun = mul(flo(div(set_range, set_limit)), mod(set_sizes, set_batch))
        l_full = mul(sub(set_range, set_tsize), set_batch)

        self.length = sum(l_full) + sum(l_trun)

        # Random number generation using a permutation
        index_tab = []
        for i in xrange(3):
            index_tab.extend(repeat(i, set_range[i]))

        # Use a deterministic seed
        self.seed = seed
        rng = make_np_rng(seed, which_method="permutation")
        self.permut = rng.permutation(index_tab)

    def __iter__(self):
        """Generator function to iterate through all minibatches"""
        counter = [0, 0, 0]
        for chosen in self.permut:
            # Retrieve minibatch from chosen set
            index = counter[chosen]
            minibatch = self.dataset[chosen][
                index * self.batch_size:(index + 1) * self.batch_size
            ]
            # Increment the related counter
            counter[chosen] = (counter[chosen] + 1) % self.limit[chosen]
            # Return the computed minibatch
            yield minibatch

    def __len__(self):
        """Return length of the weighted union"""
        return self.length

    def by_index(self):
        """Same generator as __iter__, but yield only the chosen indexes"""
        counter = [0, 0, 0]
        for chosen in self.permut:
            index = counter[chosen]
            counter[chosen] = (counter[chosen] + 1) % self.limit[chosen]
            yield chosen, index

##################################################
# Miscellaneous
##################################################


def blend(dataset, set_proba, **kwargs):
    """
    Randomized blending of datasets in data according to parameters in conf

    .. note:: pylearn2.utils.datasets.blend is deprecated and will be
              removed on or after 13 August 2014.

    Parameters
    ----------
    set_proba : WRITEME
    kwargs : WRITEME

    Returns
    -------
    WRITEME
    """
    warnings.warn("pylearn2.utils.datasets.blend is deprecated"
                  "and will be removed on or after 13 August 2014.",
                  stacklevel=2)
    iterator = BatchIterator(dataset, set_proba, 1, **kwargs)
    nrow = len(iterator)
    if (isinstance(dataset[0], theano.Variable)):
        ncol = dataset[0].get_value().shape[1]
    else:
        ncol = dataset[0].shape[1]
    if (scipy.sparse.issparse(dataset[0])):
        # Special case: the dataset is sparse
        blocks = [[batch] for batch in iterator]
        return scipy.sparse.bmat(blocks, 'csr')

    else:
        # Normal case: the dataset is dense
        row = 0
        array = numpy.empty((nrow, ncol), dataset[0].dtype)
        for batch in iterator:
            array[row] = batch
            row += 1

        return sharedX(array, borrow=True)

def minibatch_map(fn, batch_size, input_data, output_data=None,
                  output_width=None):
    """
    Apply a function on input_data, one minibatch at a time.

    Storage for the output can be provided. If it is the case,
    it should have appropriate size.

    If output_data is not provided, then output_width should be specified.

    Parameters
    ----------
    fn : WRITEME
    batch_size : WRITEME
    input_data : WRITEME
    output_data : WRITEME
    output_width : WRITEME

    Returns
    -------
    WRITEME
    """

    if output_width is None:
        if output_data is None:
            raise ValueError('output_data or output_width should be provided')

        output_width = output_data.shape[1]

    output_length = input_data.shape[0]
    if output_data is None:
        output_data = numpy.empty((output_length, output_width))
    else:
        assert output_data.shape[0] == input_data.shape[0], ('output_data '
                'should have the same length as input_data',
                output_data.shape[0], input_data.shape[0])

    for i in xrange(0, output_length, batch_size):
        output_data[i:i+batch_size] = fn(input_data[i:i+batch_size])

    return output_data

########NEW FILE########
__FILENAME__ = data_specs
"""
Utilities for working with data format specifications.

See :ref:`data_specs` for a high level overview of the relevant concepts.
"""
from pylearn2.space import CompositeSpace, NullSpace, Space
from pylearn2.utils import safe_zip


class DataSpecsMapping(object):
    """
    Converts between nested tuples and non-redundant flattened ones.

    The mapping is built from data specifications, provided as a
    (space, sources) pair, where space can be a composite space (possibly
    of other composite spaces), and sources is a tuple of string identifiers
    or other sources. Both space and sources must have the same structure.

    Parameters
    ----------
    data_specs : WRITEME
        WRITEME

    Attributes
    ----------
    specs_to_index : dict
        Maps one elementary (not composite) data_specs pair to its
        index in the flattened space.  Not sure if this one should
        be a member, or passed as a parameter to _fill_mapping. It
        might be us
    """
    #might be useful to get the index of one data_specs later
    #but if it is not, then we should remove it.
    def __init__(self, data_specs):
        self.specs_to_index = {}

        # Size of the flattened space
        self.n_unique_specs = 0

        # Builds the mapping
        space, source = data_specs
        self.spec_mapping = self._fill_mapping(space, source)

    def _fill_mapping(self, space, source):
        """
        Builds a nested tuple of integers representing the mapping

        Parameters
        ----------
        space : WRITEME
        source : WRITEME

        Returns
        -------
        WRITEME
        """
        if isinstance(space, NullSpace):
            # This Space does not contain any data, and should not
            # be mapped to anything
            assert source == ''
            return None

        elif not isinstance(space, CompositeSpace):
            # Space is a simple Space, source should be a simple source
            if isinstance(source, tuple):
                source, = source

            # If (space, source) has not already been seen, insert it.
            # We need both the space and the source to match.
            if (space, source) in self.specs_to_index:
                spec_index = self.specs_to_index[(space, source)]
            else:
                spec_index = self.n_unique_specs
                self.specs_to_index[(space, source)] = spec_index
                self.n_unique_specs += 1

            return spec_index

        else:
            # Recursively fill the mapping, and return it
            spec_mapping = tuple(
                    self._fill_mapping(sub_space, sub_source)
                    for sub_space, sub_source in safe_zip(
                        space.components, source))

            return spec_mapping

    def _fill_flat(self, nested, mapping, rval):
        """
        Auxiliary recursive function used by self.flatten

        Parameters
        ----------
        nested : WRITEME
        mapping : WRITEME
        rval : WRITEME

        Returns
        -------
        WRITEME
        """
        if isinstance(nested, CompositeSpace):
            nested = tuple(nested.components)

        if mapping is None:
            # The corresponding Space was a NullSpace, which does
            # not correspond to actual data, so nested should evaluate
            # to False, and should not be included in the flattened version
            if not isinstance(nested, NullSpace):
                assert not nested, ("The following element is mapped to "
                    "NullSpace, so it should evaluate to False (for instance, "
                    "None, an empty string or an empty tuple), but is %s"
                    % nested)
            return

        if isinstance(mapping, int):
            # "nested" should actually be a single element
            idx = mapping
            if isinstance(nested, tuple):
                if len(nested) != 1:
                    raise ValueError("When mapping is an int, we expect "
                            "nested to be a single element. But mapping is "
                            + str(mapping) + " and nested is a tuple of "
                            "length " + str(len(nested)))
                nested, = nested

            if rval[idx] is None:
                rval[idx] = nested
            else:
                assert rval[idx] == nested, ("This mapping was built "
                        "with the same element occurring more than once "
                        "in the nested representation, but current nested "
                        "sequence has different values (%s and %s) at "
                        "these positions." % (rval[idx], nested))
        else:
            for sub_nested, sub_mapping in safe_zip(nested, mapping):
                self._fill_flat(sub_nested, sub_mapping, rval)

    def flatten(self, nested, return_tuple=False):
        """
        Iterate jointly through nested and spec_mapping, returns a flat tuple.

        The integer in spec_mapping corresponding to each element in nested
        represents the index of that element in the returned sequence.
        If the original data_specs had duplicate elements at different places,
        then "nested" also have to have equal elements at these positions.
        "nested" can be a nested tuple, or composite space. If it is a
        composite space, a flattened composite space will be returned.

        If `return_tuple` is True, a tuple is always returned (tuple of
        non-composite Spaces if nested is a Space, empty tuple if all
        Spaces are NullSpaces, length-1 tuple if there is only one
        non-composite Space, etc.).

        Parameters
        ----------
        nested : WRITEME
        return_tuple : WRITEME

        Returns
        -------
        WRITEME
        """
        # Initialize the flatten returned value with Nones
        rval = [None] * self.n_unique_specs

        # Fill rval with the auxiliary function
        self._fill_flat(nested, self.spec_mapping, rval)

        assert None not in rval, ("This mapping is invalid, as it did not "
                "contain all numbers from 0 to %i (or None was in nested), "
                "nested: %s" % (self.n_unique_specs - 1, nested))

        if return_tuple:
            return tuple(rval)

        # else, return something close to the type of nested
        if len(rval) == 1:
            return rval[0]
        if isinstance(nested, tuple):
            return tuple(rval)
        elif isinstance(nested, Space):
            return CompositeSpace(rval)

    def _make_nested_tuple(self, flat, mapping):
        """
        Auxiliary recursive function used by self.nest

        Parameters
        ----------
        flat : WRITEME
        mapping : WRITEME

        Returns
        -------
        WRITEME
        """
        if mapping is None:
            # The corresponding space was a NullSpace,
            # and there is no corresponding value in flat,
            # we use None as a placeholder
            return None
        if isinstance(mapping, int):
            # We are at a leaf of the tree
            idx = mapping
            if isinstance(flat, tuple):
                assert 0 <= idx < len(flat)
                return flat[idx]
            else:
                assert idx == 0
                return flat
        else:
            return tuple(
                    self._make_nested_tuple(flat, sub_mapping)
                    for sub_mapping in mapping)

    def _make_nested_space(self, flat, mapping):
        """
        Auxiliary recursive function used by self.nest

        Parameters
        ----------
        flat : WRITEME
        mapping : WRITEME

        Returns
        -------
        WRITEME
        """
        if isinstance(mapping, int):
            # We are at a leaf of the tree
            idx = mapping
            if isinstance(flat, CompositeSpace):
                assert 0 <= idx < len(flat.components)
                return flat.components[idx]
            else:
                assert idx == 0
                return flat
        else:
            return CompositeSpace([
                    self._make_nested_space(flat, sub_mapping)
                    for sub_mapping in mapping])

    def nest(self, flat):
        """
        Iterate through spec_mapping, building a nested tuple from "flat".

        The length of "flat" should be equal to self.n_unique_specs.

        Parameters
        ----------
        flat : Space or tuple
            WRITEME

        Returns
        -------
        WRITEME
        """
        if isinstance(flat, Space):
            if isinstance(flat, CompositeSpace):
                assert len(flat.components) == self.n_unique_specs
            else:
                assert self.n_unique_specs == 1
            return self._make_nested_space(flat, self.spec_mapping)
        else:
            if isinstance(flat, tuple):
                assert len(flat) == self.n_unique_specs
            else:
                # flat is not iterable, this is valid only if spec_mapping
                # contains only 0's, that is, when self.n_unique_specs == 1
                assert self.n_unique_specs == 1
            return self._make_nested_tuple(flat, self.spec_mapping)


def is_flat_space(space):
    """
    Returns True for elementary Spaces and non-nested CompositeSpaces

    Parameters
    ----------
    space : WRITEME

    Returns
    -------
    WRITEME
    """
    if isinstance(space, CompositeSpace):
        for sub_space in space.components:
            if isinstance(sub_space, CompositeSpace):
                return False
    elif not isinstance(space, Space):
        raise TypeError("space is not a Space: %s (%s)"
                % (space, type(space)))
    return True


def is_flat_source(source):
    """
    Returns True for a string or a non-nested tuple of strings

    Parameters
    ----------
    source : WRITEME

    Returns
    -------
    WRITEME
    """
    if isinstance(source, tuple):
        for sub_source in source:
            if isinstance(sub_source, tuple):
                return False
    elif not isinstance(source, str):
        raise TypeError("source should be a string or a non-nested tuple "
                "of strings: %s" % source)
    return True


def is_flat_specs(data_specs):
    """
    .. todo::

        WRITEME
    """
    return is_flat_space(data_specs[0]) and is_flat_source(data_specs[1])

########NEW FILE########
__FILENAME__ = environ
"""
    Utilities for working with environment variables.

    DEPRECATED -- This file can be removed.
"""
import os
import warnings

def putenv(key, value):
    """
    Sets environment variables and ensures that the
    changes are visible for both the current process
    and for its children.

    .. note::

        pylearn.utils.environ.putenv is deprecated.\n
        Use os.environ['SOME_VAR'] = 'VALUE'; it is exactly equivalent.\n
        Avoid os.putenv(..), it will set the environment for childs only.
        This entire module can be removed on or after 2014-08-01.
    """
    warnings.warn("pylearn.utils.environ.putenv is deprecated.\n" +
        "Use os.environ['SOME_VAR'] = 'VALUE'; it is exactly equivalent.\n" +
        "Avoid os.putenv(..), it will set the environment for childs only."
        "This entire module can be removed on or after 2014-08-01.")

    # Make changes visible in this process and to subprocesses
    os.environ[key] = value


########NEW FILE########
__FILENAME__ = exc
"""Exceptions used by basic support utilities."""
__author__ = "Ian Goodfellow"

from pylearn2.utils.common_strings import environment_variable_essay


class EnvironmentVariableError(Exception):
    """
    An exception raised when a required environment variable is not defined
    """

    def __init__(self, *args):
        super(EnvironmentVariableError, self).__init__(*args)


# This exception is here as string_utils need it and setting it in
# datasets.exc would create a circular import.
class NoDataPathError(EnvironmentVariableError):
    """
    Exception raised when PYLEARN2_DATA_PATH is required but has not been
    defined.
    """
    def __init__(self):
        """
        .. todo::

            WRITEME
        """
        super(NoDataPathError, self).__init__(data_path_essay +
                                              environment_variable_essay)

data_path_essay = """\
You need to define your PYLEARN2_DATA_PATH environment variable. If you are
using a computer at LISA, this should be set to /data/lisa/data.
"""

########NEW FILE########
__FILENAME__ = general
"""
.. todo::

    WRITEME
"""


def is_iterable(obj):
    """
    Robustly test whether an object is iterable.

    Parameters
    ----------
    obj : object
        The object to be checked.

    Returns
    -------
    is_iterable : bool
        `True` if the object is iterable, `False` otherwise.

    Notes
    -----
    This test iterability by calling `iter()` and catching a `TypeError`.
    Various other ways might occur to you, but they all have flaws:

    * `hasattr(obj, '__len__')` will fail for objects that can be iterated
      on despite not knowing their length a priori.
    * `hasattr(obj, '__iter__')` will fail on objects like Theano tensors
      that implement it solely to raise a `TypeError` (because Theano
      tensors implement `__getitem__` semantics, Python 2.x will try
      to iterate on them via this legacy method if `__iter__` is not
      defined).
    * `hasattr` has a tendency to swallow other exception-like objects
      (`KeyboardInterrupt`, etc.) anyway, and should be avoided for this
      reason in Python 2.x, but `getattr()` with a sentinel value suffers
      from the exact same pitfalls above.
    """
    try:
        iter(obj)
    except TypeError:
        return False
    return True

########NEW FILE########
__FILENAME__ = image
"""
.. todo::

    WRITEME
"""
import logging
import numpy as np
plt = None
axes = None
import warnings
try:
    import matplotlib.pyplot as plt
    import matplotlib.axes
except (RuntimeError, ImportError), matplotlib_exception:
    warnings.warn("Unable to import matplotlib. Some features unavailable. "
            "Original exception: " + str(matplotlib_exception))
import os

try:
    from PIL import Image
except ImportError:
    Image = None

from pylearn2.utils import string_utils as string
from tempfile import mkstemp
from multiprocessing import Process

import subprocess

logger = logging.getLogger(__name__)


def ensure_Image():
    """Makes sure Image has been imported from PIL"""
    global Image
    if Image is None:
        raise RuntimeError("You are trying to use PIL-dependent functionality"
                           " but don't have PIL installed.")


def imview(*args, **kwargs):
    """
    A more sensible matplotlib-based image viewer command,
    a wrapper around `matplotlib.pyplot.imshow`.

    Parameters are identical to `matplotlib.pyplot.imshow`
    but this behaves somewhat differently:

    * By default, it creates a new figure (unless a
      `figure` keyword argument is supplied.
    * It modifies the axes of that figure to use the
      full frame, without ticks or tick labels.
    * It turns on `nearest` interpolation by default
      (i.e., it does not antialias pixel data). This
      can be overridden with the `interpolation`
      argument as in `imshow`.

    All other arguments and keyword arguments are passed
    on to `imshow`.`
    """
    if 'figure' not in kwargs:
        f = plt.figure()
    else:
        f = kwargs['figure']
    new_ax = matplotlib.axes.Axes(f,
                                  [0, 0, 1, 1],
                                  xticks=[],
                                  yticks=[],
                                  frame_on=False)
    f.delaxes(f.gca())
    f.add_axes(new_ax)
    if len(args) < 5 and 'interpolation' not in kwargs:
        kwargs['interpolation'] = 'nearest'
    plt.imshow(*args, **kwargs)


def imview_async(*args, **kwargs):
    """
    A version of `imview` that forks a separate process and
    immediately shows the image.

    Supports the `window_title` keyword argument to cope with
    the title always being 'Figure 1'.

    Returns the `multiprocessing.Process` handle.
    """
    if 'figure' in kwargs:
        raise ValueError("passing a figure argument not supported")

    def fork_image_viewer():
        f = plt.figure()
        kwargs['figure'] = f
        imview(*args, **kwargs)
        if 'window_title' in kwargs:
            f.set_window_title(kwargs['window_title'])
        plt.show()

    p = Process(None, fork_image_viewer)
    p.start()
    return p


def show(image):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    image : PIL Image object or ndarray
        If ndarray, integer formats are assumed to use 0-255
        and float formats are assumed to use 0-1
    """
    if hasattr(image, '__array__'):
        #do some shape checking because PIL just raises a tuple indexing error
        #that doesn't make it very clear what the problem is
        if len(image.shape) < 2 or len(image.shape) > 3:
            raise ValueError('image must have either 2 or 3 dimensions but its'
                             ' shape is ' + str(image.shape))

        if image.dtype == 'int8':
            image = np.cast['uint8'](image)
        elif str(image.dtype).startswith('float'):
            #don't use *=, we don't want to modify the input array
            image = image * 255.
            image = np.cast['uint8'](image)

        #PIL is too stupid to handle single-channel arrays
        if len(image.shape) == 3 and image.shape[2] == 1:
            image = image[:,:,0]

        try:
            ensure_Image()
            image = Image.fromarray(image)
        except TypeError:
            raise TypeError("PIL issued TypeError on ndarray of shape " +
                            str(image.shape) + " and dtype " +
                            str(image.dtype))

    # Create a temporary file with the suffix '.png'.
    fd, name = mkstemp(suffix='.png')
    os.close(fd)

    # Note:
    #   Although we can use tempfile.NamedTemporaryFile() to create
    #   a temporary file, the function should be used with care.
    #
    #   In Python earlier than 2.7, a temporary file created by the
    #   function will be deleted just after the file is closed.
    #   We can re-use the name of the temporary file, but there is an
    #   instant where a file with the name does not exist in the file
    #   system before we re-use the name. This may cause a race
    #   condition.
    #
    #   In Python 2.7 or later, tempfile.NamedTemporaryFile() has
    #   the 'delete' argument which can control whether a temporary
    #   file will be automatically deleted or not. With the argument,
    #   the above race condition can be avoided.
    #

    image.save(name)
    viewer_command = string.preprocess('${PYLEARN2_VIEWER_COMMAND}')
    if os.name == 'nt':
        subprocess.Popen(viewer_command + ' ' + name +' && del ' + name,
                         shell=True)
    else:
        subprocess.Popen(viewer_command + ' ' + name +' ; rm ' + name,
                         shell=True)

def pil_from_ndarray(ndarray):
    """
    .. todo::

        WRITEME
    """
    try:
        if ndarray.dtype == 'float32' or ndarray.dtype == 'float64':
            assert ndarray.min() >= 0.0
            assert ndarray.max() <= 1.0

            ndarray = np.cast['uint8'](ndarray * 255)

            if len(ndarray.shape) == 3 and ndarray.shape[2] == 1:
                ndarray = ndarray[:, :, 0]

        ensure_Image()
        rval = Image.fromarray(ndarray)
        return rval
    except Exception, e:
        logger.exception('original exception: ')
        logger.exception(e)
        logger.exception('ndarray.dtype: {0}'.format(ndarray.dtype))
        logger.exception('ndarray.shape: {0}'.format(ndarray.shape))
        raise

    assert False


def ndarray_from_pil(pil, dtype='uint8'):
    """
    .. todo::

        WRITEME
    """
    rval = np.asarray(pil)

    if dtype != rval.dtype:
        rval = np.cast[dtype](rval)

    if str(dtype).startswith('float'):
        rval /= 255.

    if len(rval.shape) == 2:
        rval = rval.reshape(rval.shape[0], rval.shape[1], 1)

    return rval


def rescale(image, shape):
    """
    Scales image to be no larger than shape. PIL might give you
    unexpected results beyond that.

    Parameters
    ----------
    image : WRITEME
    shape : WRITEME

    Returns
    -------
    WRITEME
    """

    assert len(image.shape) == 3  # rows, cols, channels
    assert len(shape) == 2  # rows, cols

    i = pil_from_ndarray(image)

    ensure_Image()
    i.thumbnail([shape[1], shape[0]], Image.ANTIALIAS)

    rval = ndarray_from_pil(i, dtype=image.dtype)

    return rval
resize = rescale


def fit_inside(image, shape):
    """
    Scales image down to fit inside shape preserves proportions of image

    Parameters
    ----------
    image : WRITEME
    shape : WRITEME

    Returns
    -------
    WRITEME
    """

    assert len(image.shape) == 3  # rows, cols, channels
    assert len(shape) == 2  # rows, cols

    if image.shape[0] <= shape[0] and image.shape[1] <= shape[1]:
        return image.copy()

    row_ratio = float(image.shape[0]) / float(shape[0])
    col_ratio = float(image.shape[1]) / float(shape[1])

    if row_ratio > col_ratio:
        target_shape = [shape[0], min(image.shape[1] / row_ratio, shape[1])]
    else:
        target_shape = [min(image.shape[0] / col_ratio, shape[0]), shape[1]]

    assert target_shape[0] <= shape[0]
    assert target_shape[1] <= shape[1]
    assert target_shape[0] == shape[0] or target_shape[1] == shape[1]
    rval = rescale(image, target_shape)
    return rval


def letterbox(image, shape):
    """
    Pads image with black letterboxing to bring image.shape up to shape

    Parameters
    ----------
    image : WRITEME
    shape : WRITEME

    Returns
    -------
    WRITEME
    """

    assert len(image.shape) == 3  # rows, cols, channels
    assert len(shape) == 2  # rows, cols

    assert image.shape[0] <= shape[0]
    assert image.shape[1] <= shape[1]

    if image.shape[0] == shape[0] and image.shape[1] == shape[1]:
        return image.copy()

    rval = np.zeros((shape[0], shape[1], image.shape[2]), dtype=image.dtype)

    rstart = (shape[0] - image.shape[0]) / 2
    cstart = (shape[1] - image.shape[1]) / 2

    rend = rstart + image.shape[0]
    cend = cstart + image.shape[1]
    rval[rstart:rend, cstart:cend] = image

    return rval


def make_letterboxed_thumbnail(image, shape):
    """
    Scales image down to shape. Preserves proportions of image, introduces
    black letterboxing if necessary.

    Parameters
    ----------
    image : WRITEME
    shape : WRITEME

    Returns
    -------
    WRITEME
    """

    assert len(image.shape) == 3
    assert len(shape) == 2

    shrunk = fit_inside(image, shape)
    letterboxed = letterbox(shrunk, shape)

    return letterboxed


def load(filepath, rescale_image=True, dtype='float64'):
    """
    .. todo::

        WRITEME
    """
    assert type(filepath) == str

    if rescale_image == False and dtype == 'uint8':
        ensure_Image()
        rval = np.asarray(Image.open(filepath))
        # print 'image.load: ' + str((rval.min(), rval.max()))
        assert rval.dtype == 'uint8'
        return rval

    s = 1.0
    if rescale_image:
        s = 255.
    try:
        ensure_Image()
        rval = Image.open(filepath)
    except:
        raise Exception("Could not open "+filepath)

    numpy_rval = np.array(rval)

    if numpy_rval.ndim not in [2,3]:
        logger.error(dir(rval))
        logger.error(rval)
        logger.error(rval.size)
        rval.show()
        raise AssertionError("Tried to load an image, got an array with " +
                str(numpy_rval.ndim)+" dimensions. Expected 2 or 3."
                "This may indicate a mildly corrupted image file. Try "
                "converting it to a different image format with a different "
                "editor like gimp or imagemagic. Sometimes these programs are "
                "more robust to minor corruption than PIL and will emit a "
                "correctly formatted image in the new format."
                )
    rval = numpy_rval

    rval = np.cast[dtype](rval) / s

    if rval.ndim == 2:
        rval = rval.reshape(rval.shape[0], rval.shape[1], 1)

    if rval.ndim != 3:
        raise AssertionError("Something went wrong opening " +
                             filepath + '. Resulting shape is ' +
                             str(rval.shape) +
                             " (it's meant to have 3 dimensions by now)")

    return rval


def save(filepath, ndarray):
    """
    .. todo::

        WRITEME
    """
    pil_from_ndarray(ndarray).save(filepath)


def scale_to_unit_interval(ndar, eps=1e-8):
    """
    Scales all values in the ndarray ndar to be between 0 and 1

    Parameters
    ----------
    ndar : WRITEME
    eps : WRITEME

    Returns
    -------
    WRITEME
    """
    ndar = ndar.copy()
    ndar -= ndar.min()
    ndar *= 1.0 / (ndar.max() + eps)
    return ndar


def tile_raster_images(X, img_shape, tile_shape, tile_spacing=(0, 0),
                       scale_rows_to_unit_interval=True,
                       output_pixel_vals=True):
    """
    Transform an array with one flattened image per row, into an array in
    which images are reshaped and layed out like tiles on a floor.

    This function is useful for visualizing datasets whose rows are images,
    and also columns of matrices for transforming those rows
    (such as the first layer of a neural net).

    Parameters
    ----------
    x : numpy.ndarray
        2-d ndarray or 4 tuple of 2-d ndarrays or None for channels,
        in which every row is a flattened image.

    shape : 2-tuple of ints
        The first component is the height of each image,
        the second component is the width.

    tile_shape : 2-tuple of ints
        The number of images to tile in (row, columns) form.

    scale_rows_to_unit_interval : bool
        Whether or not the values need to be before being plotted to [0, 1].

    output_pixel_vals : bool
        Whether or not the output should be pixel values (int8) or floats.

    Returns
    -------
    y : 2d-ndarray
        The return value has the same dtype as X, and is suitable for
        viewing as an image with PIL.Image.fromarray.
    """

    assert len(img_shape) == 2
    assert len(tile_shape) == 2
    assert len(tile_spacing) == 2

    # The expression below can be re-written in a more C style as
    # follows :
    #
    # out_shape    = [0,0]
    # out_shape[0] = (img_shape[0]+tile_spacing[0])*tile_shape[0] -
    #                tile_spacing[0]
    # out_shape[1] = (img_shape[1]+tile_spacing[1])*tile_shape[1] -
    #                tile_spacing[1]
    out_shape = [(ishp + tsp) * tshp - tsp for ishp, tshp, tsp
                 in zip(img_shape, tile_shape, tile_spacing)]

    if isinstance(X, tuple):
        assert len(X) == 4
        # Create an output np ndarray to store the image
        if output_pixel_vals:
            out_array = np.zeros((out_shape[0], out_shape[1], 4),
                                 dtype='uint8')
        else:
            out_array = np.zeros((out_shape[0], out_shape[1], 4),
                                 dtype=X.dtype)

        #colors default to 0, alpha defaults to 1 (opaque)
        if output_pixel_vals:
            channel_defaults = [0, 0, 0, 255]
        else:
            channel_defaults = [0., 0., 0., 1.]

        for i in xrange(4):
            if X[i] is None:
                # if channel is None, fill it with zeros of the correct
                # dtype
                dt = out_array.dtype
                if output_pixel_vals:
                    dt = 'uint8'
                out_array[:, :, i] = np.zeros(out_shape, dtype=dt) + \
                                     channel_defaults[i]
            else:
                # use a recurrent call to compute the channel and store it
                # in the output
                out_array[:, :, i] = tile_raster_images(
                    X[i], img_shape, tile_shape, tile_spacing,
                    scale_rows_to_unit_interval, output_pixel_vals)
        return out_array

    else:
        # if we are dealing with only one channel
        H, W = img_shape
        Hs, Ws = tile_spacing

        # generate a matrix to store the output
        dt = X.dtype
        if output_pixel_vals:
            dt = 'uint8'
        out_array = np.zeros(out_shape, dtype=dt)

        for tile_row in xrange(tile_shape[0]):
            for tile_col in xrange(tile_shape[1]):
                if tile_row * tile_shape[1] + tile_col < X.shape[0]:
                    this_x = X[tile_row * tile_shape[1] + tile_col]
                    if scale_rows_to_unit_interval:
                        # if we should scale values to be between 0 and 1
                        # do this by calling the `scale_to_unit_interval`
                        # function
                        this_img = scale_to_unit_interval(
                            this_x.reshape(img_shape))
                    else:
                        this_img = this_x.reshape(img_shape)
                    # add the slice to the corresponding position in the
                    # output array
                    c = 1
                    if output_pixel_vals:
                        c = 255
                    out_array[
                        tile_row * (H + Hs): tile_row * (H + Hs) + H,
                        tile_col * (W + Ws): tile_col * (W + Ws) + W
                        ] = this_img * c
        return out_array


if __name__ == '__main__':
    black = np.zeros((50, 50, 3), dtype='uint8')

    red = black.copy()
    red[:, :, 0] = 255

    green = black.copy()
    green[:, :, 1] = 255

    show(black)
    show(green)
    show(red)

########NEW FILE########
__FILENAME__ = insert_along_axis
"""
Shove the contents of one array into a larger array with a fill value
everywhere else.
"""

__author__ = "David Warde-Farley"
__copyright__ = "Copyright (c) 2012, Universite de Montreal"
__credits__ = [__author__]
__license__ = "BSD"
__maintainer__ = "David Warde-Farley"
__email__ = "wardefar@iro"

import numpy as np
import theano
import theano.tensor as tensor
from theano.gradient import grad_not_implemented


def index_along_axis(index, ndim, axis):
    """
    Create a slice tuple for indexing into a NumPy array along a
    (single) given axis.

    Parameters
    ----------
    index : array_like or slice
        The value you wish to index with along `axis`.
    ndim : int
        The number of dimensions in the array into which you are indexing
        (i.e. the value returned in the `.ndim` attribute).
    axis : int
        The axis along which you wish to index.

    Returns
    -------
    indices : tuple
        A slice tuple that can be used to index an array, selecting all
        elements along every axis except `axis`, for which `index` is used
        instead.

    Examples
    --------
    >>> import numpy as np
    >>> a = np.arange(27).reshape((3, 3, 3))
    >>> index = index_along_axis([0, 2], 3, 2)
    >>> np.all(a[index] == a[:, :, [0, 2]])
    True
    >>> index = index_along_axis([0, 2], 3, 1)
    >>> np.all(a[index] == a[:, [0, 2]])
    True
    >>> index = index_along_axis([0, 2], 3, 0)
    >>> np.all(a[index] == a[[0, 2]])
    True
    """
    indices = [slice(None)] * ndim
    indices[axis] = index
    return tuple(indices)


class InsertAlongAxis(theano.Op):
    """
    Inserts values from one array into an output array with one axis
    having a longer length, inserting values from the original array
    at specified positions along the given axis. The remaining
    entries are filled with `fill`.

    This is useful in the case that certain features take on a
    constant values (and thus should not be fed into/predicted by a
    neural net) but are nonetheless necessary for some sort of
    post-processing and need to be re-added later in the pipeline.

    Parameters
    ----------
    ndim : WRITEME
    axis : WRITEME
    fill : WRITEME
    """

    def __init__(self, ndim, axis, fill=0):
        assert axis < ndim, "axis >= ndim not allowed (doesn't make sense)"
        self.ndim = ndim
        self.axis = axis
        self.fill = fill

    def __eq__(self, other):
        """
        .. todo::

            WRITEME
        """
        return (type(self) == type(other) and self.ndim == other.ndim and
                self.axis == other.axis and self.fill == other.fill)

    def __hash__(self):
        """
        .. todo::

            WRITEME
        """
        return (hash(type(self)) ^ hash(self.ndim) ^ hash(self.axis) ^
                hash(self.fill))

    def make_node(self, x, new_length, insert_at):
        """
        .. todo::

            WRITEME
        """
        x_ = tensor.as_tensor_variable(x)
        new_length_ = tensor.as_tensor_variable(new_length)
        insert_at_ = tensor.as_tensor_variable(insert_at)
        assert x_.ndim == self.ndim, (
            "%s instance expected x.ndim = %d, got %d" %
            (self.__class__.__name__, self.ndim, x.ndim)
        )
        assert new_length_.ndim == 0, "new_length must be a scalar"
        assert insert_at_.ndim == 1, "insert_at must be vector"
        assert (new_length_.dtype.startswith('int') or
                new_length.dtype.startswith('uint')), (
                    "new_length must be integer type"
                )
        assert (insert_at_.dtype.startswith('int') or
                insert_at_.dtype.startswith('uint')), (
                    "insert_at must be integer type"
                )
        return theano.Apply(self,
          inputs=[x_, new_length_, insert_at_],
          outputs=[x_.type()])

    def perform(self, node, inputs, output_storage):
        """
        .. todo::

            WRITEME
        """
        x, new_length, nonconstants = inputs
        nonconstant_set = set(nonconstants)
        constant = sorted(set(xrange(new_length)) - nonconstant_set)
        assert x.shape[self.axis] == len(nonconstant_set), (
            "x.shape[%d] != len(set(nonconstants))" % self.axis
        )
        assert new_length >= x.shape[self.axis], (
            "number of items along axis in new array is less than old array"
        )
        new_shape = (x.shape[:self.axis] +
                     (int(new_length),) +
                     x.shape[(self.axis + 1):])
        z = output_storage[0][0] = np.empty(new_shape, dtype=x.dtype)
        z[index_along_axis(nonconstants, self.ndim, self.axis)] = x
        z[index_along_axis(constant, self.ndim, self.axis)] = self.fill

    def grad(self, inputs, gradients):
        """
        .. todo::

            WRITEME
        """
        x, new_length, nonconstants = inputs
        d_out = gradients[0]
        swap = range(self.ndim)
        swap.remove(self.axis)
        swap.insert(0, self.axis)
        return [d_out.dimshuffle(swap)[nonconstants].dimshuffle(swap),
                grad_not_implemented(self, 1, new_length),
                grad_not_implemented(self, 2, nonconstants)]

    def __str__(self):
        """
        .. todo::

            WRITEME
        """
        return "%s{ndim=%d,axis=%d,fill=%s}" % (self.__class__.__name__,
                                                      self.ndim,
                                                      self.axis,
                                                      str(self.fill))


insert_rows = InsertAlongAxis(2, 0)
insert_columns = InsertAlongAxis(2, 1)

########NEW FILE########
__FILENAME__ = iteration
"""
Iterators providing indices for different kinds of iteration over
datasets.

Presets:

- sequential: iterates through fixed slices of the dataset in sequence
- shuffled_sequential: iterates through a shuffled version of the dataset
  in sequence
- random_slice: on each call to next, returns a slice of the dataset,
  chosen uniformly at random over contiguous slices.
  Samples with replacement, but still reports that
  container is empty after num_examples / batch_size calls
- random_uniform: on each call to next, returns a random subset of the
  dataset. Samples with replacement, but still reports that
  container is empty after num_examples / batch_size calls
"""
from __future__ import division
import functools
import inspect
import numpy as np

from pylearn2.space import CompositeSpace
from pylearn2.utils import safe_izip, wraps
from pylearn2.utils.data_specs import is_flat_specs
from pylearn2.utils.rng import make_np_rng

# Make sure that the docstring uses restructured text list format.
# If you change the module-level docstring, please re-run
# pylearn2/doc/scripts/docgen.py and make sure sphinx doesn't issue any
# warnings for this file.
# This particular docstring was being frequently broken prior to the
# addition of this test.
# TODO: have nosetests run docgen.py in warning=error mode, remove
# tests for specific conditions
assert """Presets:

- sequential: iterates through fixed slices of the dataset in sequence
- s""" in __doc__


class SubsetIterator(object):
    """
    An iterator that returns slices or lists of indices into a dataset
    of a given fixed size.

    Parameters
    ----------
    dataset_size : int
        The number of examples, total, in the dataset.
    batch_size : int, optional
        The (typical/maximum) number of examples per batch. Less
        may be returned in the very last batch if batch size
        does not evenly divide `dataset_size`.
    num_batches : int, optional
        The number of batches to return. Needn't be specified
        if `batch_size` is specified. If both `batch_size` and
        `num_batches` are specified then it must be true that
        `batch_size * num_batches <= dataset_size`.
    rng : `np.random.RandomState` or seed, optional
        A `np.random.RandomState` object or the seed to be
        used to create one. A deterministic default seed is
        used otherwise.
    """
    # This breaks the doc generation, so until we figure out why, not in the
    # docstring.
    #
    # Attributes
    # ----------
    # batch_size : int
    # num_batches : int
    # num_examples : int
    # uneven : bool
    # fancy : bool
    #     `True` if this iterator produces lists of indices,
    #     `False` if it produces slices.
    # stochastic : bool
    #     `True` if this iterator makes use of the random number
    #     generator, and will therefore produce different sequences
    #     depending on the RNG state. `False` otherwise.

    def __init__(self, dataset_size, batch_size=None,
                 num_batches=None, rng=None):
        raise NotImplementedError()

    def next(self):
        """
        Retrieves description of the next batch of examples.

        Returns
        -------
        next_batch : `slice` or list of int
            An object describing the indices in the dataset of
            a batch of data. Either a `slice` object or a list
            of integers specifying individual indices of
            examples.

        Raises
        ------
        StopIteration
            When there are no more batches to return.
        """
        raise NotImplementedError()

    def __iter__(self):
        return self

    # Does this return subsets that need fancy indexing? (i.e. lists
    # of indices)
    fancy = False

    # Does this class make use of random number generators?
    stochastic = False

    # Does it ensure that every batch has the same size?
    uniform_batch_size = False

    @property
    def batch_size(self):
        """
        The (maximum) number of examples in each batch.

        Returns
        -------
        batch_size : int
            The (maximum) number of examples in each batch. This is
            either as specified via the constructor, or inferred from
            the dataset size and the number of batches requested.
        """
        return self._batch_size

    @property
    def num_batches(self):
        """
        The total number of batches that the iterator will ever return.

        Returns
        -------
        num_batches : int
            The total number of batches the iterator will ever return.
            This is either as specified via the constructor, or
            inferred from the dataset size and the batch size.
        """
        return self._num_batches

    @property
    def num_examples(self):
        """
        The total number of examples over which the iterator operates.

        Returns
        -------
        num_examples : int
            The total number of examples over which the iterator operates.
            May be less than the dataset size.
        """
        return self.batch_size * self.num_batches

    @property
    def uneven(self):
        """
        Whether every batch will be the same size.

        Returns
        -------
        uneven : bool
            `True` if returned batches may be of differing sizes,
            `False` otherwise.
        """
        raise NotImplementedError()


class ForcedEvenIterator(SubsetIterator):
    """
    A class which wraps other iterators to ensure equal batch size.
    This class needs to be completed using type() metaclass, see
    Examples section to see how to use it.

    Parameters
    ----------
    dataset_size : int
        Total number of examples in the dataset
    batch_size : int or None
        The size of the batches.
        If set to None and num_batches is defined, batch_size will be
        calculated based on dataset_size.
    num_batches : int or None
        The number of batch in the dataset.
        If set to None and batch_size is defined, num_batches will be
        calculated based on dataset_size.
    *args : Variable length argument list for _base_iterator_cls
    **kwargs : Arbitrary keyword arguments for _base_iterator_cls

    Notes
    -----
        This class can not be initialized because it needs to be completed
        using type() metaclass. See Examples section for more details.

        Batches of size unequal to batch_size will be discarded. Those
        examples will never be visited.

    Examples
    --------
    >>> dct = ForcedEvenIterator.__dict__.copy()
    >>> dct["_base_iterator_cls"] = SequentialSubsetIterator
    >>> dct["fancy"] = SequentialSubsetIterator.fancy
    >>> dct["stochastic"] = SequentialSubsetIterator.stochastic
    >>>
    >>> NewForcedEvenClass = type("ForcedEvenDummyIterator",
    ...     ForcedEvenIterator.__bases__, dct)
    >>>
    >>> even_iterator = NewForcedEvenClass(dataset_size=100,
    ...     batch_size=30, num_batches=None)

    For a shortcut use function as_even()

    >>> NewForcedEvenClass = as_even(SequentialSubsetIterator)
    >>> even_iterator = NewForcedEvenClass(dataset_size=100,
    ...     batch_size=30, num_batches=None)
    """

    def __init__(self, dataset_size, batch_size, num_batches, *args, **kwargs):

        if self.fancy is None or self.stochastic is None or \
           self._base_iterator_cls is None:
            raise ValueError("You must pre-define fancy, stochastic and "
                             "_base_iterator_cls arguments by creating a new "
                             "class using the metaclass type()."
                             "See function as_even() for an example.")

        if batch_size is None:
            if num_batches is not None:
                batch_size = int(dataset_size / num_batches)
            else:
                raise ValueError("need one of batch_size, num_batches "
                                 "for sequential batch iteration")
        elif batch_size is not None:
            if num_batches is not None:
                max_num_batches = int(dataset_size / batch_size)
                if num_batches > max_num_batches:
                    raise ValueError("dataset of %d examples can only provide "
                                     "%d batches of equal size with batch_size"
                                     " %d, but %d batches were requested" %
                                     (dataset_size, max_num_batches,
                                      batch_size, num_batches))
            else:
                num_batches = int(dataset_size / batch_size)

        self._base_iterator = self._base_iterator_cls(dataset_size, batch_size,
                                                      num_batches, *args,
                                                      **kwargs)

    # Does it ensure that every batch has the same size?
    uniform_batch_size = True

    # Does this return subsets that need fancy indexing? (i.e. lists
    # of indices)
    # Needs to be set before initialization. See Examples section in class docs
    fancy = None

    # Does this class make use of random number generators?
    # Needs to be set before initialization. See Examples section in class docs
    stochastic = None

    # base iterator that ForcedEvenIterator class wraps
    # Needs to be set before initialization. See Examples section in class docs
    _base_iterator_cls = None

    @property
    def _dataset_size(self):
        return self._base_iterator._dataset_size

    @property
    def _batch_size(self):
        return self._base_iterator._batch_size

    @property
    def _num_batches(self):
        return self._base_iterator._num_batches

    @property
    def num_examples(self):
        """
        Number of examples that will be visited
        by the iterator. (May be lower than dataset_size)
        """

        product = self.batch_size * self.num_batches

        if product > self._dataset_size:
            return self.batch_size * (self.num_batches - 1)
        else:
            return product

    def next(self):
        """
        Returns next batch of _base_iterator

        Raises
        ------
        StopException
            When _base_iterator reachs the end of the dataset

        Notes
        -----
            Uneven batches may be discarded and StopException
            will be raised without having iterated throught
            every examples.
        """

        length = -1

        # check if the batch has wrong length, throw it away
        while length != self.batch_size:
            batch = self._base_iterator.next()

            if isinstance(batch, slice):
                length = batch.stop-batch.start
            else:
                length = len(batch)

        return batch


def as_even(iterator_cls):
    """
    Returns a class wrapping iterator_cls that forces equal batch size.

    Parameters
    ----------
    iterator_cls : class
        An iterator class that inherits from SubsetIterator

    Returns
    -------
    class
        An iterator class ForcedEven{put the name of iterator_cls here}, based
        on ForcedEvenIterator, that wraps iterator_cls.
    """

    assert issubclass(iterator_cls, SubsetIterator)

    dct = ForcedEvenIterator.__dict__.copy()
    dct["_base_iterator_cls"] = iterator_cls
    dct["fancy"] = iterator_cls.fancy
    dct["stochastic"] = iterator_cls.stochastic

    NewForcedEvenClass = type("ForcedEven%s" % iterator_cls.__name__,
                              ForcedEvenIterator.__bases__, dct)

    return NewForcedEvenClass


class SequentialSubsetIterator(SubsetIterator):
    """
    Returns mini-batches proceeding sequentially through the dataset.

    Notes
    -----
    Returns slice objects to represent ranges of indices (`fancy = False`).

    See :py:class:`SubsetIterator` for detailed constructor parameter
    and attribute documentation.
    """

    def __init__(self, dataset_size, batch_size, num_batches, rng=None):
        if rng is not None:
            raise ValueError("non-None rng argument not supported for "
                             "sequential batch iteration")
        assert num_batches is None or num_batches >= 0
        self._dataset_size = dataset_size
        if batch_size is None:
            if num_batches is not None:
                batch_size = int(np.ceil(self._dataset_size / num_batches))
            else:
                raise ValueError("need one of batch_size, num_batches "
                                 "for sequential batch iteration")
        elif batch_size is not None:
            if num_batches is not None:
                max_num_batches = np.ceil(self._dataset_size / batch_size)
                if num_batches > max_num_batches:
                    raise ValueError("dataset of %d examples can only provide "
                                     "%d batches with batch_size %d, but %d "
                                     "batches were requested" %
                                     (self._dataset_size, max_num_batches,
                                      batch_size, num_batches))
            else:
                num_batches = np.ceil(self._dataset_size / batch_size)
        self._batch_size = batch_size
        self._num_batches = num_batches
        self._next_batch_no = 0
        self._idx = 0
        self._batch = 0

    @wraps(SubsetIterator.next, assigned=(), updated=())
    def next(self):
        if self._batch >= self.num_batches or self._idx >= self._dataset_size:
            raise StopIteration()

        # this fix the problem where dataset_size % batch_size != 0
        elif (self._idx + self._batch_size) > self._dataset_size:
            self._last = slice(self._idx, self._dataset_size)
            self._idx = self._dataset_size
            return self._last

        else:
            self._last = slice(self._idx, self._idx + self._batch_size)
            self._idx += self._batch_size
            self._batch += 1
            return self._last

    fancy = False
    stochastic = False
    uniform_batch_size = False

    @property
    @wraps(SubsetIterator.num_examples, assigned=(), updated=())
    def num_examples(self):
        product = self.batch_size * self.num_batches
        return min(product, self._dataset_size)

    @property
    @wraps(SubsetIterator.uneven, assigned=(), updated=())
    def uneven(self):
        return self.batch_size * self.num_batches > self._dataset_size


class ShuffledSequentialSubsetIterator(SequentialSubsetIterator):
    """
    Randomly shuffles the example indices and then proceeds sequentially
    through the permutation.

    Notes
    -----
    Returns lists of indices (`fancy = True`).

    See :py:class:`SubsetIterator` for detailed constructor parameter
    and attribute documentation.
    """
    stochastic = True
    fancy = True
    uniform_batch_size = False

    def __init__(self, dataset_size, batch_size, num_batches, rng=None):
        super(ShuffledSequentialSubsetIterator, self).__init__(
            dataset_size,
            batch_size,
            num_batches,
            None
        )
        self._rng = make_np_rng(rng, which_method=["random_integers",
                                                   "shuffle"])
        self._shuffled = np.arange(self._dataset_size)
        self._rng.shuffle(self._shuffled)

    @wraps(SubsetIterator.next)
    def next(self):
        if self._batch >= self.num_batches or self._idx >= self._dataset_size:
            raise StopIteration()

        # this fix the problem where dataset_size % batch_size != 0
        elif (self._idx + self._batch_size) > self._dataset_size:
            rval = self._shuffled[self._idx: self._dataset_size]
            self._idx = self._dataset_size
            return rval
        else:
            rval = self._shuffled[self._idx: self._idx + self._batch_size]
            self._idx += self._batch_size
            self._batch += 1
            return rval


class RandomUniformSubsetIterator(SubsetIterator):
    """
    Selects minibatches of examples by drawing indices uniformly
    at random, with replacement.

    Notes
    -----
    Returns lists of indices (`fancy = True`).

    See :py:class:`SubsetIterator` for detailed constructor parameter
    and attribute documentation.
    """

    def __init__(self, dataset_size, batch_size, num_batches, rng=None):
        self._rng = make_np_rng(rng, which_method=["random_integers",
                                                   "shuffle"])
        if batch_size is None:
            raise ValueError("batch_size cannot be None for random uniform "
                             "iteration")
        elif num_batches is None:
            raise ValueError("num_batches cannot be None for random uniform "
                             "iteration")
        self._dataset_size = dataset_size
        self._batch_size = batch_size
        self._num_batches = num_batches
        self._next_batch_no = 0

    @wraps(SubsetIterator.next)
    def next(self):
        if self._next_batch_no >= self._num_batches:
            raise StopIteration()
        else:
            self._last = self._rng.random_integers(low=0,
                                                   high=self._dataset_size - 1,
                                                   size=(self._batch_size,))
            self._next_batch_no += 1
            return self._last

    fancy = True
    stochastic = True
    uniform_batch_size = True


class RandomSliceSubsetIterator(RandomUniformSubsetIterator):
    """
    Returns minibatches that are randomly selected contiguous slices in
    index space.

    Notes
    -----
    Returns slice objects to represent ranges of indices (`fancy = False`).

    See :py:class:`SubsetIterator` for detailed constructor parameter
    and attribute documentation.
    """

    def __init__(self, dataset_size, batch_size, num_batches, rng=None):
        if batch_size is None:
            raise ValueError("batch_size cannot be None for random slice "
                             "iteration")
        elif num_batches is None:
            raise ValueError("num_batches cannot be None for random slice "
                             "iteration")
        super(RandomSliceSubsetIterator, self).__init__(dataset_size,
                                                        batch_size,
                                                        num_batches, rng)
        self._last_start = self._dataset_size - self._batch_size
        if self._last_start < 0:
            raise ValueError("batch_size > dataset_size not supported for "
                             "random slice iteration")

    @wraps(SubsetIterator.next)
    def next(self):
        if self._next_batch_no >= self._num_batches:
            raise StopIteration()
        else:
            start = self._rng.random_integers(low=0, high=self._last_start)
            self._last = slice(start, start + self._batch_size)
            self._next_batch_no += 1
            return self._last

    fancy = False
    stochastic = True
    uniform_batch_size = True


class BatchwiseShuffledSequentialIterator(SequentialSubsetIterator):
    """
    Returns minibatches randomly, but sequential inside each minibatch.

    Notes
    -----
    Returns slice objects to represent ranges of indices (`fancy = False`).

    See :py:class:`SubsetIterator` for detailed constructor parameter
    and attribute documentation.
    """

    def __init__(self, dataset_size, batch_size, num_batches=None, rng=None):
        self._rng = make_np_rng(rng, which_method=["random_integers",
                                                   "shuffle"])
        assert num_batches is None or num_batches >= 0
        self._dataset_size = dataset_size
        if batch_size is None:
            if num_batches is not None:
                batch_size = int(np.ceil(self._dataset_size / num_batches))
            else:
                raise ValueError("need one of batch_size, num_batches "
                                 "for sequential batch iteration")
        elif batch_size is not None:
            if num_batches is not None:
                max_num_batches = np.ceil(self._dataset_size / batch_size)
                if num_batches > max_num_batches:
                    raise ValueError("dataset of %d examples can only provide "
                                     "%d batches with batch_size %d, but %d "
                                     "batches were requested" %
                                     (self._dataset_size, max_num_batches,
                                      batch_size, num_batches))
            else:
                num_batches = np.ceil(self._dataset_size / batch_size)

        self._batch_size = batch_size
        self._num_batches = int(num_batches)
        self._next_batch_no = 0
        self._idx = 0
        self._batch_order = range(self._num_batches)
        self._rng.shuffle(self._batch_order)

    @wraps(SubsetIterator.next)
    def next(self):
        if self._next_batch_no >= self._num_batches:
            raise StopIteration()
        else:
            start = self._batch_order[self._next_batch_no] * self._batch_size
            if start + self._batch_size > self._dataset_size:
                self._last = slice(start, self._dataset_size)
            else:
                self._last = slice(start, start + self._batch_size)
            self._next_batch_no += 1
            return self._last

    fancy = False
    stochastic = True
    uniform_batch_size = False


_iteration_schemes = {
    'sequential': SequentialSubsetIterator,
    'shuffled_sequential': ShuffledSequentialSubsetIterator,
    'random_slice': RandomSliceSubsetIterator,
    'random_uniform': RandomUniformSubsetIterator,
    'batchwise_shuffled_sequential': BatchwiseShuffledSequentialIterator,
    'even_sequential': as_even(SequentialSubsetIterator),
    'even_shuffled_sequential': as_even(ShuffledSequentialSubsetIterator),
    'even_batchwise_shuffled_sequential':
    as_even(BatchwiseShuffledSequentialIterator),
}


def has_uniform_batch_size(mode):
    """
    Returns True if the iteration scheme has uniform batch size,
    False if not

    Parameters
    ----------
    mode: string
        A string defining an iteration scheme in _iteration_schemes

    Returns
    -------
    boolean
        True if the iteration scheme has uniform batch size,
        False otherwise
    """
    return resolve_iterator_class(mode).uniform_batch_size


def is_stochastic(mode):
    """

    """
    return resolve_iterator_class(mode).stochastic


def resolve_iterator_class(mode):
    """
    Map textual representations of default iteration modes to classes.

    Parameters
    ----------
    mode : str or class object
        If a string, identifier string for the built-in iteration modes.
        See the module documentation of :py:mod:`pylearn2.utils.iteration`
        for a list of available modes. If a class, it is expected to
        be a class that respects the constructor and attribute interface
        defined in :py:class:`SubsetIterator`.

    Returns
    -------
    subset_iter_class : class
        A class instance (i.e., an instance of type `type`) that
        interface defined in :py:class:`SubsetIterator`.
    """
    if isinstance(mode, basestring) and mode not in _iteration_schemes:
        raise ValueError("unknown iteration mode string: %s" % mode)
    elif mode in _iteration_schemes:
        subset_iter_class = _iteration_schemes[mode]
    else:
        subset_iter_class = mode
    return subset_iter_class


class FiniteDatasetIterator(object):
    """
    A wrapper around subset iterators that actually retrieves
    data.

    Parameters
    ----------
    dataset : `Dataset` object
        The dataset over which to iterate.
    data_specs : tuple
        A `(space, source)` tuple. See :ref:`data_specs` for a full
        description. Must not contain nested composite spaces.
    subset_iterator : object
        An iterator object that returns slice objects or lists of
        examples, conforming to the interface specified by
        :py:class:`SubsetIterator`.
    return_tuple : bool, optional
        Always return a tuple, even if there is exactly one source
        of data being returned. Defaults to `False`.
    convert : list of callables
        A list of callables, in the same order as the sources
        in `data_specs`, that will be called on the individual
        source batches prior to any further processing.

    Notes
    -----
    See the documentation for :py:class:`SubsetIterator` for
    attribute documentation.
    """

    def __init__(self, dataset, subset_iterator, data_specs=None,
                 return_tuple=False, convert=None):
        self._data_specs = data_specs
        self._dataset = dataset
        self._subset_iterator = subset_iterator
        self._return_tuple = return_tuple

        # Keep only the needed sources in self._raw_data.
        # Remember what source they correspond to in self._source
        assert is_flat_specs(data_specs)

        dataset_space, dataset_source = self._dataset.get_data_specs()
        assert is_flat_specs((dataset_space, dataset_source))

        # the dataset's data spec is either a single (space, source) pair,
        # or a pair of (non-nested CompositeSpace, non-nested tuple).
        # We could build a mapping and call flatten(..., return_tuple=True)
        # but simply putting spaces, sources and data in tuples is simpler.
        if not isinstance(dataset_source, tuple):
            dataset_source = (dataset_source,)

        if not isinstance(dataset_space, CompositeSpace):
            dataset_sub_spaces = (dataset_space,)
        else:
            dataset_sub_spaces = dataset_space.components
        assert len(dataset_source) == len(dataset_sub_spaces)

        all_data = self._dataset.get_data()
        if not isinstance(all_data, tuple):
            all_data = (all_data,)

        space, source = data_specs
        if not isinstance(source, tuple):
            source = (source,)
        if not isinstance(space, CompositeSpace):
            sub_spaces = (space,)
        else:
            sub_spaces = space.components
        assert len(source) == len(sub_spaces)

        self._raw_data = tuple(all_data[dataset_source.index(s)]
                               for s in source)
        self._source = source

        if convert is None:
            self._convert = [None for s in source]
        else:
            assert len(convert) == len(source)
            self._convert = convert

        for i, (so, sp, dt) in enumerate(safe_izip(source,
                                                   sub_spaces,
                                                   self._raw_data)):
            idx = dataset_source.index(so)
            dspace = dataset_sub_spaces[idx]

            init_fn = self._convert[i]
            fn = init_fn

            # If there is an init_fn, it is supposed to take
            # care of the formatting, and it should be an error
            # if it does not. If there was no init_fn, then
            # the iterator will try to format using the generic
            # space-formatting functions.
            if init_fn is None:
                # "dspace" and "sp" have to be passed as parameters
                # to lambda, in order to capture their current value,
                # otherwise they would change in the next iteration
                # of the loop.
                if fn is None:
                    fn = (lambda batch, dspace=dspace, sp=sp:
                          dspace.np_format_as(batch, sp))
                else:
                    fn = (lambda batch, dspace=dspace, sp=sp, fn_=fn:
                          dspace.np_format_as(fn_(batch), sp))

            self._convert[i] = fn

    def __iter__(self):
        return self

    @wraps(SubsetIterator.next)
    def next(self):
        """
        Retrieves the next batch of examples.

        Returns
        -------
        next_batch : object
            An object representing a mini-batch of data, conforming
            to the space specified in the `data_specs` constructor
            argument to this iterator. Will be a tuple if more
            than one data source was specified or if the constructor
            parameter `return_tuple` was `True`.

        Raises
        ------
        StopIteration
            When there are no more batches to return.
        """
        next_index = self._subset_iterator.next()
        # TODO: handle fancy-index copies by allocating a buffer and
        # using np.take()

        rval = tuple(
            fn(data[next_index]) if fn else data[next_index]
            for data, fn in safe_izip(self._raw_data, self._convert))
        if not self._return_tuple and len(rval) == 1:
            rval, = rval
        return rval

    @property
    @wraps(SubsetIterator.batch_size, assigned=(), updated=())
    def batch_size(self):
        return self._subset_iterator.batch_size

    @property
    @wraps(SubsetIterator.num_batches, assigned=(), updated=())
    def num_batches(self):
        return self._subset_iterator.num_batches

    @property
    @wraps(SubsetIterator.num_examples, assigned=(), updated=())
    def num_examples(self):
        return self._subset_iterator.num_examples

    @property
    @wraps(SubsetIterator.uneven, assigned=(), updated=())
    def uneven(self):
        return self._subset_iterator.uneven

    @property
    @wraps(SubsetIterator.stochastic, assigned=(), updated=())
    def stochastic(self):
        return self._subset_iterator.stochastic

########NEW FILE########
__FILENAME__ = key_aware
"""A key-aware analog to defaultdict."""


class KeyAwareDefaultDict(dict):
    """
    Like a standard library defaultdict, but pass the key
    to the default factory.

    Parameters
    ----------
    default_factory : WRITEME
    """
    def __init__(self, default_factory=None):
        self.default_factory = default_factory

    def __getitem__(self, key):
        """
        .. todo::

            WRITEME
        """
        if key not in self and self.default_factory is not None:
            self[key] = val = self.default_factory(key)
            return val
        else:
            raise KeyError(str(key))

########NEW FILE########
__FILENAME__ = logger
"""Local facilities to configure the logger to our needs."""

__author__ = "David Warde-Farley"
__copyright__ = "Copyright 2012, Universite de Montreal"
__credits__ = ["David Warde-Farley"]
__license__ = "3-clause BSD"
__email__ = "wardefar@iro"
__maintainer__ = "David Warde-Farley"

# Portions cribbed from the standard library logging module,
# Copyright 2001-2010 by Vinay Sajip. All Rights Reserved.
#
# Permission to use, copy, modify, and distribute this software and its
# documentation for any purpose and without fee is hereby granted,
# provided that the above copyright notice appear in all copies and that
# both that copyright notice and this permission notice appear in
# supporting documentation, and that the name of Vinay Sajip
# not be used in advertising or publicity pertaining to distribution
# of the software without specific, written prior permission.
# VINAY SAJIP DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
# ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL
# VINAY SAJIP BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR
# ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER
# IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT
# OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.

import logging
import sys
from logging import Handler, Formatter


class CustomFormatter(Formatter):
    """
    Conditionally displays log level names and source loggers, only if
    the log level is WARNING or greater.

    Parameters
    ----------
    prefix : WRITEME
    only_from : WRITEME
    """
    def __init__(self, prefix='', only_from=None):
        Formatter.__init__(self)
        self._info_fmt = prefix + "%(message)s"
        self._fmt = prefix + "%(levelname)s (%(name)s): %(message)s"
        self._only_from = only_from

    def format(self, record):
        """
        Format the specified record as text.

        Parameters
        ----------
        record : object
            A LogRecord object with the appropriate attributes.

        Returns
        -------
        s : str
            A string containing the formatted log message.

        Notes
        -----
        The record's attribute dictionary is used as the operand to a
        string formatting operation which yields the returned string.
        Before formatting the dictionary, a couple of preparatory
        steps are carried out. The message attribute of the record is
        computed using LogRecord.getMessage(). If the formatting
        string uses the time (as determined by a call to usesTime(),
        formatTime() is called to format the event time. If there is
        exception information, it is formatted using formatException()
        and appended to the message.
        """
        record.message = record.getMessage()
        # Python 2.6 don't have usesTime() fct.
        # So we skip that information for them.
        if hasattr(self, 'usesTime') and self.usesTime():
            record.asctime = self.formatTime(record, self.datefmt)

        emit_special = (self._only_from is None or
                        record.name.startswith(self._only_from))
        if record.levelno == logging.INFO and emit_special:
            s = self._info_fmt % record.__dict__
        else:
            s = self._fmt % record.__dict__
        if record.exc_info:
            # Cache the traceback text to avoid converting it multiple times
            # (it's constant anyway)
            if not record.exc_text:
                record.exc_text = self.formatException(record.exc_info)
        if record.exc_text:
            if s[-1:] != "\n":
                s = s + "\n"
            try:
                s = s + record.exc_text
            except UnicodeError:
                # Sometimes filenames have non-ASCII chars, which can lead
                # to errors when s is Unicode and record.exc_text is str
                # See issue 8924
                s = s + record.exc_text.decode(sys.getfilesystemencoding())
        return s


class CustomStreamHandler(Handler):
    """
    A handler class which writes logging records, appropriately
    formatted, to one of two streams. DEBUG and INFO messages
    get written to the provided `stdout`, all other messages to
    `stderr`.

    If stream is not specified, sys.stderr is used.

    Parameters
    ----------
    stdout : file-like object, optional
        Stream to which DEBUG and INFO messages should be written.
        If `None`, `sys.stdout` will be used.
    stderr : file-like object, optional
        Stream to which WARNING, ERROR, CRITICAL messages will be
        written. If `None`, `sys.stderr` will be used.
    formatter : `logging.Formatter` object, optional
        Assigned to `self.formatter`, used to format outgoing log messages.

    Notes
    -----
    N.B. it is **not** recommended to pass `sys.stdout` or `sys.stderr` as
    constructor arguments explicitly, as certain things (like nosetests) can
    reassign these during code execution! Instead, simply pass `None`.
    """
    def __init__(self, stdout=None, stderr=None, formatter=None):
        Handler.__init__(self)
        self._stdout = stdout
        self._stderr = stderr
        self.formatter = formatter

    @property
    def stdout(self):
        """
        .. todo::

            WRITEME
        """
        return sys.stdout if self._stdout is None else self._stdout

    @property
    def stderr(self):
        """
        .. todo::

            WRITEME
        """
        return sys.stderr if self._stderr is None else self._stderr

    def flush(self):
        """Flushes the stream."""
        for stream in (self.stdout, self.stderr):
            stream.flush()

    def emit(self, record):
        """
        Emit a record.

        If a formatter is specified, it is used to format the record.
        The record is then written to the stream with a trailing newline.  If
        exception information is present, it is formatted using
        traceback.print_exception and appended to the stream.  If the stream
        has an 'encoding' attribute, it is used to determine how to do the
        output to the stream.

        Parameters
        ----------
        record : WRITEME
        """
        try:
            msg = self.format(record)
            if record.levelno > logging.INFO:
                stream = self.stderr
            else:
                stream = self.stdout
            fs = "%s\n"
            #if no unicode support...
            #Python 2.6 don't have logging._unicode, so use the no unicode path
            # as stream.encoding also don't exist.
            if not getattr(logging, '_unicode', True):
                stream.write(fs % msg)
            else:
                try:
                    if (isinstance(msg, unicode) and
                        getattr(stream, 'encoding', None)):
                        ufs = fs.decode(stream.encoding)
                        try:
                            stream.write(ufs % msg)
                        except UnicodeEncodeError:
                            # Printing to terminals sometimes fails. For
                            # example, with an encoding of 'cp1251', the above
                            # write will work if written to a stream opened or
                            # wrapped by the codecs module, but fail when
                            # writing to a terminal even when the codepage is
                            # set to cp1251.  An extra encoding step seems to
                            # be needed.
                            stream.write((ufs % msg).encode(stream.encoding))
                    else:
                        stream.write(fs % msg)
                except UnicodeError:
                    stream.write(fs % msg.encode("UTF-8"))
            self.flush()
        except (KeyboardInterrupt, SystemExit):
            raise
        except:
            self.handleError(record)


def configure_custom(debug=False, stdout=None, stderr=None):
    """
    Configure the logging module to output logging messages to the
    console via `stdout` and `stderr`.

    Parameters
    ----------
    debug : bool
        If `True`, display DEBUG messages on `stdout` along with
        INFO-level messages.
    stdout : file-like object, optional
        Stream to which DEBUG and INFO messages should be written.
        If `None`, `sys.stdout` will be used.
    stderr : file-like object, optional
        Stream to which WARNING, ERROR, CRITICAL messages will be
        written. If `None`, `sys.stderr` will be used.

    Notes
    -----
    This uses `CustomStreamHandler` defined in this module to
    set up a console logger. By default, messages are formatted
    as "LEVEL: message", where "LEVEL:" is omitted if the
    level is INFO.

    WARNING, ERROR and CRITICAL level messages are logged to
    `stderr` (or the provided substitute)

    N.B. it is **not** recommended to pass `sys.stdout` or
    `sys.stderr` as constructor arguments explicitly, as certain
    things (like nosetests) can reassign these during code
    execution! Instead, simply pass `None`.
    """
    top_level_logger = logging.getLogger(__name__.split('.')[0])

    # Do not propagate messages to the root logger.
    top_level_logger.propagate = False

    # Set the log level of our logger, either to DEBUG or INFO.
    top_level_logger.setLevel(logging.DEBUG if debug else logging.INFO)

    # Get rid of any extant logging handlers that are installed.
    # This means we can call configure_custom() more than once
    # and have it be idempotent.
    while top_level_logger.handlers:
        top_level_logger.handlers.pop()

    # Install our custom-configured handler and formatter.
    fmt = CustomFormatter()
    handler = CustomStreamHandler(stdout=stdout, stderr=stderr, formatter=fmt)
    top_level_logger.addHandler(handler)


def restore_defaults():
    """
    Use this if you are embedding our library in a larger application
    and wish to handle logging yourself at the level of the root
    logger.

    Undoes the  effects of `configure_custom()`. By default, this
    shuts us up on the console except for WARNING, ERROR, and
    CRITICAL. See the documentation for the `logging` standard library
    module for details.
    """
    top_level_logger = logging.getLogger(__name__.split('.')[0])

    # Propagate log messages upwards.
    top_level_logger.propagate = True

    # Restore the log level to its default value, i.e. logging.NOTSET.
    top_level_logger.setLevel(logging.NOTSET)

    # Delete any handlers that might be installed on our logger.
    while top_level_logger.handlers:
        top_level_logger.handlers.pop()


def newline(logger, nb_blank_lines=1):
    """
    A simple method to write a real new line to logging.
    Only works with the INFO level at the moment.

    Parameters
    ----------
    logger : Logger object
        The logger where the blank line will be added.
    nb_blank_lines : int, optional
        Number of blank lines in a row.
    """
    formatter = logging.Formatter(fmt='')
    handler = CustomStreamHandler(formatter=formatter)

    logger.addHandler(handler)

    for i in xrange(nb_blank_lines):
        logger.info('')

    logger.removeHandler(handler)

########NEW FILE########
__FILENAME__ = mem
"""
.. todo::

    WRITEME
"""
import subprocess
import os


def get_memory_usage():
    """
    Return int containing memory used by this process. Don't trust this
    too much, I'm not totally sure what ps rss measures.
    """

    pid = os.getpid()
    process = subprocess.Popen("ps -o rss %s | awk '{sum+=$1} END {print sum}'" % pid,
                                    shell=True,
                                    stdout=subprocess.PIPE,
                                    )
    stdout_list = process.communicate()[0].split('\n')
    return int(stdout_list[0])


class TypicalMemoryError(MemoryError):
    """
    Memory error that could have been caused by typical errors such
    as using 32bit python while having more than 2-4GB or RAM computer.

    This is to help users understand what could have possibly caused the
    memory error. A more detailed explanation needs to be given for
    each case.

    Parameters
    ----------
    value: str
        String explaining what the program was trying to do and
        why the memory error possibly happened.
    """
    def __init__(self, value):
        super(TypicalMemoryError, self).__init__()

        # could add more typical errors to this string
        self.value = value + ("\n + Make sure you use a 64bit python version. "
                              "32bit python version can only access 2GB of "
                              "memory on windows and 4GB of memory on "
                              "linux/OSX.")

    def __str__(self):
        return repr(self.value)

########NEW FILE########
__FILENAME__ = mnist_ubyte
"""
Low-level utilities for reading in raw MNIST files.
"""

__author__ = "David Warde-Farley"
__copyright__ = "Copyright 2012, Universite de Montreal"
__credits__ = ["David Warde-Farley"]
__license__ = "3-clause BSD"
__email__ = "wardefar@iro"
__maintainer__ = "David Warde-Farley"


import struct
import numpy

MNIST_IMAGE_MAGIC = 2051
MNIST_LABEL_MAGIC = 2049


class open_if_filename(object):
    """
    .. todo::

        WRITEME

    Parameters
    ----------
    f : WRITEME
    mode : WRITEME
    buffering : WRITEME
    """
    def __init__(self, f, mode='r', buffering=-1):
        self._f = f
        self._mode = mode
        self._buffering = buffering
        self._handle = None

    def __enter__(self):
        """
        .. todo::

            WRITEME
        """
        if isinstance(self._f, basestring):
            self._handle = open(self._f, self._mode, self._buffering)
        else:
            self._handle = self._f
        return self._handle

    def __exit__(self, exc_type, exc_value, traceback):
        """
        .. todo::

            WRITEME
        """
        if self._handle is not self._f:
            self._handle.close()


def read_mnist_images(fn, dtype=None):
    """
    Read MNIST images from the original ubyte file format.

    Parameters
    ----------
    fn : str or object
        Filename/path from which to read labels, or an open file
        object for the same (will not be closed for you).

    dtype : str or object, optional
        A NumPy dtype or string that can be converted to one.
        If unspecified, images will be returned in their original
        unsigned byte format.

    Returns
    -------
    images : ndarray, shape (n_images, n_rows, n_cols)
        An image array, with individual examples indexed along the
        first axis and the image dimensions along the second and
        third axis.

    Notes
    -----
    If the dtype provided was boolean, the resulting array will
    be boolean with `True` if the corresponding pixel had a value
    greater than or equal to 128, `False` otherwise.

    If the dtype provided was a float or complex dtype, the values
    will be mapped to the unit interval [0, 1], with pixel values
    that were 255 in the original unsigned byte representation
    equal to 1.0.
    """
    with open_if_filename(fn, 'rb') as f:
        magic, number, rows, cols = struct.unpack('>iiii', f.read(16))
        if magic != MNIST_IMAGE_MAGIC:
            raise ValueError('wrong magic number reading MNIST image file: ' +
                             fn)
        array = numpy.fromfile(f, dtype='uint8').reshape((number, rows, cols))
    if dtype:
        dtype = numpy.dtype(dtype)
        # If the user wants booleans, threshold at half the range.
        if dtype.kind is 'b':
            array = array >= 128
        else:
            # Otherwise, just convert.
            array = array.astype(dtype)
        # I don't know why you'd ever turn MNIST into complex,
        # but just in case, check for float *or* complex dtypes.
        # Either way, map to the unit interval.
        if dtype.kind in ('f', 'c'):
            array /= 255.
    return array


def read_mnist_labels(fn):
    """
    Read MNIST labels from the original ubyte file format.

    Parameters
    ----------
    fn : str or object
        Filename/path from which to read labels, or an open file
        object for the same (will not be closed for you).

    Returns
    -------
    labels : ndarray, shape (nlabels,)
        A one-dimensional unsigned byte array containing the
        labels as integers.
    """
    with open_if_filename(fn, 'rb') as f:
        magic, number = struct.unpack('>ii', f.read(8))
        if magic != MNIST_LABEL_MAGIC:
            raise ValueError('wrong magic number reading MNIST label file: ' +
                             fn)
        array = numpy.fromfile(f, dtype='uint8')
    return array

########NEW FILE########
__FILENAME__ = one_hot
"""Low-level NumPy functions for building one-hot and k-hot matrices."""


__author__ = "David Warde-Farley"
__copyright__ = "Copyright 2013, Universite de Montreal"
__credits__ = ["David Warde-Farley"]
__license__ = "3-clause BSD"
__email__ = "wardefar@iro"
__maintainer__ = "David Warde-Farley"
__all__ = ['one_hot', 'k_hot', "compressed_one_hot"]

import numpy as np
import warnings


def _validate_labels(labels, ndim):
    """
    .. todo::

        WRITEME
    """
    labels = np.asarray(labels)
    if labels.dtype.kind not in ('u', 'i'):
        raise ValueError("labels must have int or uint dtype")
    if ndim == 1 and labels.ndim != 1:
        if labels.ndim == 2 and labels.shape[1] == 1:
            labels = labels.squeeze()
        else:
            raise ValueError("labels must be 1-dimensional")
    elif ndim == 2 and labels.ndim != 2:
        raise ValueError("labels must be 2-dimensional, no ragged "
                         "lists-of-lists")
    return labels


def _validate_max_label(labels, max_label):
    """
    .. todo::

        WRITEME
    """
    max_actual_label = labels.max()
    if max_label is None:
        max_label = max_actual_label
    elif max_actual_label > max_label:
        raise ValueError("max_label = %d provided, but labels "
                         "contains %d" % (max_label, max_actual_label))
    return max_label


def _validate_dtype(labels, dtype, out):
    """
    .. todo::

        WRITEME
    """
    if dtype is not None and out is not None:
        raise ValueError("supplied both output array and dtype; "
                         "only supply one or the other")
    elif dtype is None:
        dtype = labels.dtype
    else:
        dtype = np.dtype(dtype)
    return dtype


def _validate_out(nlabels, max_label, dtype, out):
    """
    .. todo::

        WRITEME
    """
    if out is None:
        out = np.zeros((nlabels, max_label + 1), dtype=dtype)
    else:
        if nlabels != out.shape[0]:
            raise ValueError("supplied output array has wrong "
                             "first dimension")
        if max_label >= out.shape[1]:
            raise ValueError("not enough columns in supplied output array "
                             "for %d distinct labels (out.shape[1] == %d)"
                             % (max_label + 1, out.shape[1]))
    return out


def _one_hot_fill(labels, out):
    """
    .. todo::

        WRITEME
    """
    out.flat[np.arange(0, out.size, out.shape[1]) + labels] = 1


def one_hot(labels, max_label=None, dtype=None, out=None):
    """
    Construct a one-hot matrix from a vector of integer labels.
    Each row will have a single 1 with all other elements 0.

    .. note::
        `pylearn2.utils.one_hot is deprecated`. Use
        `pylearn2.format.target_format.OneHotFormatter`
        instead. `pylearn2.utils.one_hot` will be removed
        on or after 13 August 2014".

    Parameters
    ----------
    labels : array_like, 1-dimensional (or 2-dimensional (nlabels, 1))
        The integer labels to use to construct the one hot matrix.

    max_label : int, optional
        The maximum valid label. Must be greater than or equal to
        `numpy.amax(labels)`.

    dtype : str or dtype object, optional
        The dtype you wish the returned array to have. Defaults
        to `labels.dtype` if not provided.

    out : ndarray, optional
        An array to use in lieu of allocating one. Must be the
        right shape, i.e. same first dimension as `labels` and
        second dimension greater than or equal to `labels.max() + 1`.

    Returns
    -------
    out : ndarray, (nlabels, max_label + 1)
        The resulting one-hot matrix.
    """
    warnings.warn("pylearn2.utils.one_hot is deprecated. Use "
                  "pylearn2.format.target_format.OneHotFormatter "
                  "instead. pylearn2.utils.one_hot will be removed "
                  "on or after 13 August 2014", stacklevel=2)
    labels = _validate_labels(labels, 1)
    max_label = _validate_max_label(labels, max_label)
    dtype = _validate_dtype(labels, dtype, out)
    out = _validate_out(labels.shape[0], max_label, dtype, out)
    out[...] = 0.
    _one_hot_fill(labels, out)
    return out


def k_hot(labels, max_label=None, dtype=None, out=None):
    """
    Create a matrix of k-hot rows, where k (or less) elements
    are 1 and the rest are 0.

    .. note::
        `pylearn2.utils.one_hot is deprecated`. Use
        `pylearn2.format.target_format.OneHotFormatter`
        instead. `pylearn2.utils.one_hot` will be removed
        on or after 13 August 2014".

    Parameters
    ----------
    labels : array_like, 2-dimensional (nlabels, k)
        The integer labels to use to construct the k-hot matrix.

    max_label : int, optional
        The maximum valid label. Must be greater than or equal to
        `numpy.amax(labels)`.

    dtype : str or dtype object, optional
        The dtype you wish the returned array to have. Defaults
        to `labels.dtype` if not provided.

    out : ndarray, optional
        An array to use in lieu of allocating one. Must be the
        right shape, i.e. same first dimension as `labels` and
        second dimension greater than or equal to `labels.max() + 1`.

    Returns
    -------
    out : ndarray, (nlabels, max_label + 1)
        The resulting k-hot matrix. If a given integer appeared
        in the same row more than once then there may be less
        than k elements active in the corresponding row of `out`.
    """
    warnings.warn("pylearn2.utils.one_hot is deprecated. Use "
                  "pylearn2.format.target_format.OneHotFormatter "
                  "instead. pylearn2.utils.one_hot will be removed "
                  "on or after 13 August 2014", stacklevel=2)
    labels = _validate_labels(labels, 2)
    max_label = _validate_max_label(labels, max_label)
    dtype = _validate_dtype(labels, dtype, out)
    out = _validate_out(labels.shape[0], max_label, dtype, out)
    # If the out array was passed in, zero it once.
    if out is not None:
        out[...] = 0
    for column in labels.T:
        _one_hot_fill(column, out)
    return out


def compressed_one_hot(labels, dtype=None, out=None, simplify_binary=True):
    """
    Construct a one-hot matrix from a vector of integer labels, but
    only including columns corresponding to integer labels that
    actually appear.

    .. note::
        `pylearn2.utils.one_hot is deprecated`. Use
        `pylearn2.format.target_format.OneHotFormatter`
        instead. `pylearn2.utils.one_hot` will be removed
        on or after 13 August 2014".

    Parameters
    ----------
    labels : array_like, 1-dimensional (or 2-dimensional (nlabels, 1))
        The integer labels to use to construct the one hot matrix.

    dtype : str or dtype object, optional
        The dtype you wish the returned array to have. Defaults
        to `labels.dtype` if not provided.

    out : ndarray, optional
        An array to use in lieu of allocating one. Must be the
        right shape, i.e. same first dimension as `labels` and
        second dimension greater than or equal to the number of
        unique values in `labels`.

    simplify_binary : bool, optional
        If `True`, if there are only two distinct labels, return
        an `(nlabels, 1)` matrix with 0 lesser the lesser integer
        label and 1 denoting the greater, instead of a redundant
        `(nlabels, 2)` matrix.

    Returns
    -------
    out : ndarray, (nlabels, max_label + 1) or (nlabels, 1)
        The resulting one-hot matrix.

    uniq : ndarray, 1-dimensional
        The array of unique values in `labels` in the order
        in which the corresponding columns appear in `out`.
    """
    warnings.warn("pylearn2.utils.one_hot is deprecated. Use "
                  "pylearn2.format.target_format.OneHotFormatter "
                  "instead. pylearn2.utils.one_hot will be removed "
                  "on or after 13 August 2014", stacklevel=2)
    labels = _validate_labels(labels, ndim=1)
    labels_ = labels.copy()
    uniq = np.unique(labels_)
    for i, e in enumerate(uniq):
        labels_[labels_ == e] = i
    if simplify_binary and len(uniq) == 2:
        return labels_.reshape((labels_.shape[0], 1)), uniq
    else:
        return one_hot(labels_, dtype=dtype, out=out), uniq

########NEW FILE########
__FILENAME__ = pooling
"""
Support code for pooling operations (in pooled ICA type models, for now).
"""
import numpy as np
import theano
import warnings
try:
    import scipy.sparse
except ImportError:
    warnings.warn("Could not import scipy")
from itertools import izip


def pooling_matrix(groups, per_group, strides=None, dtype=None, sparse=None):
    """
    Construct a pooling matrix, optionally with overlapping pools
    arranged in a 1 or 2D topology.

    Parameters
    ----------
    groups : int or tuple
        The grid dimensions of a 1- or 2-dimensional pooling grid.
    per_group : int or tuple
        The grid dimensions of a single 1- or 2-dimensional feature
        pool. Must be same length as `groups`.
    strides : int or tuple, optional
        The stride of the pools along each dimension. A value of `None`
        is equivalent to setting equal to `per_group`, i.e. no overlap
    dtype : dtype object or str, optional
        The dtype of the resulting pooling matrix.
    sparse : str, optional
        If `None`, the function will return a dense matrix (a rank-2
        `numpy.ndarray`). Specifying 'csc' or 'csr' in this argument will
        cause the function to return a `scipy.sparse.csc_matrix` or a
        `scipy.sparse.csr_matrix`, instead.

    Returns
    -------
    pools : ndarray or sparse matrix
        Either a dense 2-dimensional NumPy array or one of
        `scipy.sparse.csc_matrix` or `scipy.sparse.csr_matrix`, depending
        on the value of the `sparse` argument. In any case, the shape is
        `(n_pools, n_filters)` and the value of `pools[i, j]` is 1 if
        feature `j` is in pool `i`, and 0 otherwise.
    """
    # Error-check arguments and fill in row_stride and col_stride
    # if either argument is absent.
    def _validate_shape(shape, param_name):
        try:
            shape = tuple(shape)
            [int(val) for val in shape]
        except (ValueError, TypeError):
            try:
                shape = (int(shape),)
            except TypeError:
                raise TypeError("%s must be int or int tuple" % param_name)
        return shape

    groups = _validate_shape(groups, 'groups')
    per_group = _validate_shape(per_group, 'per_group')
    if strides is not None:
        strides = _validate_shape(strides, 'strides')
    else:
        strides = per_group
    if len(groups) != len(per_group):
        raise ValueError('groups and per_group must have the same length')
    elif len(per_group) != len(strides):
        raise ValueError('per_group and strides must have the same length')
    if len(groups) > 2 or len(per_group) > 2:
        raise ValueError('only <= 2-dimensional pooling grids are supported')
    if not all(stride <= dim for stride, dim in izip(strides, per_group)):
        raise ValueError('strides must each be <= per_group dimensions')
    try:
        group_rows, group_cols = groups
        rows_per_group, cols_per_group = per_group
        row_stride, col_stride = strides
    except ValueError:
        group_rows, group_cols = groups[0], 1
        rows_per_group, cols_per_group = per_group[0], 1
        row_stride, col_stride = strides[0], 1
    if sparse is not None and sparse not in ('csc', 'csr'):
        raise ValueError("sparse must be one of (None, 'csr', 'csc')")
    # The total number of filters along either dimension is the
    # the number of groups times the stride, plus whatever dangles
    # off the last filter (the added term is zero if there's no
    # overlapping pools).
    filter_rows = group_rows * row_stride + (rows_per_group - row_stride)
    filter_cols = group_cols * col_stride + (cols_per_group - col_stride)
    if dtype is None:
        dtype = theano.config.floatX
    # If the return type is dense we can treat it as a 4-tensor and
    # then reshape. If not we'll need some index math, but it happens
    shape = (group_rows, group_cols, filter_rows, filter_cols)
    matrix_shape = group_rows * group_cols, filter_rows * filter_cols
    if sparse is not None:
        # Use a dictionary-of-keys matrix at construction time,
        # since they are efficient for arbitrary assignment.
        # TODO: I think CSC/CSR are fast to construct if you know the total
        # number of elements, which should be easy to calculate.
        pools = scipy.sparse.dok_matrix(matrix_shape, dtype=dtype)
    else:
        pools = np.zeros(shape, dtype=dtype)
    for g_row in xrange(group_rows):
        for g_col in xrange(group_cols):
            # The start and end points of the contiguous block of 1's.
            row_start = row_stride * g_row
            row_end = row_start + rows_per_group
            col_start = col_stride * g_col
            col_end = col_start + cols_per_group
            if sparse is not None:
                for f_row in xrange(row_start, row_end):
                    matrix_cols = slice(f_row * shape[3] + col_start,
                                        f_row * shape[3] + col_end)
                    # The group to which this belongs.
                    matrix_row = g_row * shape[1] + g_col
                    pools[matrix_row, matrix_cols] = 1.
            else:
                # If the matrix is a dense 4-tensor then we can get
                # away with doing an entire pool in one assignment.
                pools[g_row, g_col, row_start:row_end, col_start:col_end] = 1
    if sparse is not None:
        # Call either .tocsr() or .tocsc()
        pools = getattr(pools, 'to' + sparse)()
    else:
        pools = pools.reshape(matrix_shape)
    return pools

########NEW FILE########
__FILENAME__ = python26
"""
.. todo::

    WRITEME
"""
import functools
import sys


if sys.version_info[:2] < (2, 7):
    def cmp_to_key(mycmp):
        """Convert a cmp= function into a key= function"""
        class K(object):
            __slots__ = ['obj']
            def __init__(self, obj, *args):
                self.obj = obj
            def __lt__(self, other):
                return mycmp(self.obj, other.obj) < 0
            def __gt__(self, other):
                return mycmp(self.obj, other.obj) > 0
            def __eq__(self, other):
                return mycmp(self.obj, other.obj) == 0
            def __le__(self, other):
                return mycmp(self.obj, other.obj) <= 0
            def __ge__(self, other):
                return mycmp(self.obj, other.obj) >= 0
            def __ne__(self, other):
                return mycmp(self.obj, other.obj) != 0
            def __hash__(self):
                raise TypeError('hash not implemented')
        return K
else:
    from functools import cmp_to_key

########NEW FILE########
__FILENAME__ = rng
"""
Different type of random seed generator, created to prevent redundancy
across the code.

Reference: https://github.com/lisa-lab/pylearn2/issues/165
"""

__author__ = "Abhishek Aggarwal, Xavier Bouthillier"
__copyright__ = "Copyright 2012, Universite de Montreal"
__credits__ = ["Abhishek Aggarwal", "Xavier Bouthillier"]
__license__ = "3-clause BSD"
__email__ = "bouthilx@iro"

import numpy

from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams
# more distributions but slower
# from theano.tensor.shared_randomstreams import RandomStreams


def make_rng(rng_or_seed=None, default_seed=None,
             which_method=None, constructor=None):
    """
    Returns a RNG object.

    Parameters
    ----------
    rng_or_seed : RNG object or integer or list of integers
        A random number generator or a valid seed.
    default_seed : integer or list of integers
        Seed used if rng_or_seed is None
    which_method : string or list of strings
        One or more methods that must be defined by the RNG object.
        If one or more specified methods are not defined by it, a
        new one will be constructed with the given constructor.
    constructor : function or class
        Must return a RNG object.
        constructor is called with rng_or_seed, default_seed or 42
        as argument.

    Notes
    -----
    The RNG object is generated using the first of these cases that produces a
    valid result and doesn't use an argument with the value of None:

    1) rng_or_seed itself
    2) constructor(rng_or_seed)
    3) constructor(default_seed)
    4) constructor(42)
    """

    if isinstance(which_method, basestring):
        which_method = [which_method]

    if rng_or_seed is not None and \
       all(hasattr(rng_or_seed, attr) for attr in which_method):
        rng = rng_or_seed
    elif rng_or_seed is not None:
        rng = constructor(rng_or_seed)
    elif default_seed is not None:
        rng = constructor(default_seed)
    else:
        rng = constructor(42)

    return rng


def make_np_rng(rng_or_seed=None, default_seed=None, which_method=None):
    """
    Returns a numpy RandomState.

    Parameters
    ----------
    rng_or_seed : RNG object or integer or list of integers
        A random number generator or a valid seed.
    default_seed : integer or list of integers
        Seed used if rng_or_seed is None
    which_method : string or list of strings
        One or more methods that must be defined by the RNG object.
        If one or more specified methods are not defined by it, a
        new one will be constructed from RandomState.

    Notes
    -----
    The returned RandomState is generated using the first of these cases that
    produces a valid result and doesn't use an argument with the value of None:

    1) rng_or_seed itself
    2) RandomState(rng_or_seed)
    3) RandomState(default_seed)
    4) RandomState(42)
    """
    return make_rng(rng_or_seed, default_seed, which_method,
                    numpy.random.RandomState)


def make_theano_rng(rng_or_seed=None, default_seed=None, which_method=None):
    """
    Returns a theano RandomStreams.

    Parameters
    ----------
    rng_or_seed : RNG object or integer or list of integers
        A random number generator or a valid seed.
    default_seed : integer or list of integers
        Seed used if rng_or_seed is None
    which_method : string or list of strings
        One or more methods that must be defined by the RNG object.
        If one or more specified methods are not defined by it, a
        new one will be constructed from RandomState.

    Notes
    -----
    The returned RandomStreams is generated using the first of these cases that
    produces a valid result and doesn't use an argument with the value of None:

    1) rng_or_seed itself
    2) RandomState(rng_or_seed)
    3) RandomState(default_seed)
    4) RandomState(42)
    """

    return make_rng(rng_or_seed, default_seed, which_method, RandomStreams)

########NEW FILE########
__FILENAME__ = serial
"""
.. todo::

    WRITEME
"""
import cPickle
import pickle
import logging
import numpy as np
import os
import time
import warnings
import sys
from pylearn2.utils.string_utils import preprocess
from pylearn2.utils.mem import TypicalMemoryError
from cPickle import BadPickleGet
io = None
hdf_reader = None
import struct
from pylearn2.utils.string_utils import match
import shutil

logger = logging.getLogger(__name__)


def raise_cannot_open(path):
    """
    .. todo::

        WRITEME
    """
    pieces = path.split('/')
    for i in xrange(1,len(pieces)+1):
        so_far = '/'.join(pieces[0:i])
        if not os.path.exists(so_far):
            if i == 1:
                if so_far == '':
                    continue
                raise IOError('Cannot open '+path+' ('+so_far+' does not exist)')
            parent = '/'.join(pieces[0:i-1])
            bad = pieces[i-1]

            if not os.path.isdir(parent):
                raise IOError("Cannot open "+path+" because "+parent+" is not a directory.")

            candidates = os.listdir(parent)

            if len(candidates) == 0:
                raise IOError("Cannot open "+path+" because "+parent+" is empty.")

            if len(candidates) > 100:
                # Don't attempt to guess the right name if the directory is huge
                raise IOError("Cannot open "+path+" but can open "+parent+".")

            if os.path.islink(path):
                raise IOError(path + " appears to be a symlink to a non-existent file")
            raise IOError("Cannot open "+path+" but can open "+parent+". Did you mean "+match(bad,candidates)+" instead of "+bad+"?")
        # end if
    # end for
    assert False

def load(filepath, recurse_depth=0, retry=True):
    """
    .. todo::

        WRITEME

    .. todo::

        Refactor to hide recurse_depth from end users

    Parameters
    ----------
    filepath : str
        A path to a file to load. Should be a pickle, Matlab, or NumPy
        file; or a .txt or .amat file that numpy.loadtxt can load.
    recurse_depth : int, optional
        End users should not use this argument. It is used by the function
        itself to implement the `retry` option recursively.
    retry : bool, optional
        If True, will make a handful of attempts to load the file before
        giving up. This can be useful if you are for example calling
        show_weights.py on a file that is actively being written to by a
        training script--sometimes the load attempt might fail if the
        training script writes at the same time show_weights tries to
        read, but if you try again after a few seconds you should be able
        to open the file.

    Returns
    -------
    loaded_object : object
        The object that was stored in the file.
    """
    try:
        import joblib
        joblib_available = True
    except ImportError:
        joblib_available = False
    if recurse_depth == 0:
        filepath = preprocess(filepath)

    if filepath.endswith('.npy') or filepath.endswith('.npz'):
        return np.load(filepath)

    if filepath.endswith('.amat') or filepath.endswith('txt'):
        try:
            return np.loadtxt(filepath)
        except Exception:
            logger.exception("{0} cannot be loaded by serial.load (trying to"
                             " use np.loadtxt)".format(filepath))
            raise

    if filepath.endswith('.mat'):
        global io
        if io is None:
            import scipy.io
            io = scipy.io
        try:
            return io.loadmat(filepath)
        except NotImplementedError, nei:
            if str(nei).find('HDF reader') != -1:
                global hdf_reader
                if hdf_reader is None:
                    import h5py
                    hdf_reader = h5py
                return hdf_reader.File(filepath)
            else:
                raise
        #this code should never be reached
        assert False

    def exponential_backoff():
        if recurse_depth > 9:
            logger.info('Max number of tries exceeded while trying to open '
                        '{0}'.format(filepath))
            logger.info('attempting to open via reading string')
            f = open(filepath, 'rb')
            lines = f.readlines()
            f.close()
            content = ''.join(lines)
            return cPickle.loads(content)
        else:
            nsec = 0.5 * (2.0 ** float(recurse_depth))
            logger.info("Waiting {0} seconds and trying again".format(nsec))
            time.sleep(nsec)
            return load(filepath, recurse_depth + 1, retry)

    try:
        if not joblib_available:
            with open(filepath, 'rb') as f:
                obj = cPickle.load(f)
        else:
            try:
                obj = joblib.load(filepath)
            except Exception, e:
                if os.path.exists(filepath) and not os.path.isdir(filepath):
                    raise
                raise_cannot_open(filepath)
    except MemoryError, e:
        # We want to explicitly catch this exception because for MemoryError
        # __str__ returns the empty string, so some of our default printouts
        # below don't make a lot of sense.
        # Also, a lot of users assume any exception is a bug in the library,
        # so we can cut down on mail to pylearn-users by adding a message
        # that makes it clear this exception is caused by their machine not
        # meeting requirements.
        if os.path.splitext(filepath)[1] == ".pkl":
            raise TypicalMemoryError("You do not have enough memory to open "
                                     "%s \n + Try using numpy.{save,load} (file "
                                     "with extension '.npy') to save your file. "
                                     "It uses less memory"
                                     " when reading and writing files than "
                                     "pickled files." % filepath)
        else:
            raise TypicalMemoryError("You do not have enough memory to open %s"
                                     % filepath)

    except BadPickleGet, e:
        logger.exception('Failed to open {0} due to BadPickleGet '
                         'with exception string {1}'.format(filepath, e))

        if not retry:
            raise
        obj =  exponential_backoff()
    except EOFError, e:

        logger.exception('Failed to open {0} due to EOFError '
                         'with exception string {1}'.format(filepath, e))

        if not retry:
            raise
        obj =  exponential_backoff()
    except ValueError, e:
        logger.exception('Failed to open {0} due to ValueError '
                         'with string {1}'.format(filepath, e))

        if not retry:
            raise
        obj =  exponential_backoff()
    except Exception, e:
        #assert False
        exc_str = str(e)
        if len(exc_str) > 0:
            import pdb
            tb = pdb.traceback.format_exc()
            raise Exception("Couldn't open '" + str(filepath) +
                            "' due to: " + str(type(e)) + ', ' + str(e) +
                            ". Orig traceback:\n" + tb)
        else:
            logger.exception("Couldn't open '{0}' and exception has no string."
                             "Opening it again outside the try/catch "
                             "so you can see whatever error it prints "
                             "on its own.".format(filepath))
            f = open(filepath, 'rb')
            obj = cPickle.load(f)
            f.close()

    #if the object has no yaml_src, we give it one that just says it
    #came from this file. could cause trouble if you save obj again
    #to a different location
    if not hasattr(obj,'yaml_src'):
        try:
            obj.yaml_src = '!pkl: "'+os.path.abspath(filepath)+'"'
        except:
            pass

    return obj

def save(filepath, obj, on_overwrite = 'ignore'):
    """
    Serialize `object` to a file denoted by `filepath`.

    Parameters
    ----------
    filepath : str
        A filename. If the suffix is `.joblib` and joblib can be
        imported, `joblib.dump` is used in place of the regular
        pickling mechanisms; this results in much faster saves by
        saving arrays as separate .npy files on disk. If the file
        suffix is `.npy` than `numpy.save` is attempted on `obj`.
        Otherwise, (c)pickle is used.

    obj : object
        A Python object to be serialized.

    on_overwrite : str, optional
        A string specifying what to do if the file already exists.
        Possible values include:

        - "ignore" : Just overwrite the existing file.
        - "backup" : Make a backup copy of the file (<filepath>.bak).
          Save the new copy. Then delete the backup copy. This allows
          recovery of the old version of the file if saving the new one
          fails.
    """
    filepath = preprocess(filepath)

    if os.path.exists(filepath):
        if on_overwrite == 'backup':
            backup = filepath + '.bak'
            shutil.move(filepath, backup)
            save(filepath, obj)
            try:
                os.remove(backup)
            except Exception, e:
                warnings.warn("Got an error while traing to remove "+backup+":"+str(e))
            return
        else:
            assert on_overwrite == 'ignore'


    try:
        _save(filepath, obj)
    except RuntimeError, e:
        """ Sometimes for large theano graphs, pickle/cPickle exceed the
            maximum recursion depth. This seems to me like a fundamental
            design flaw in pickle/cPickle. The workaround I employ here
            is the one recommended to someone who had a similar problem
            on stackexchange:

            http://stackoverflow.com/questions/2134706/hitting-maximum-recursion-depth-using-pythons-pickle-cpickle

            Obviously this does not scale and could cause a crash
            but I don't see another solution short of writing our
            own implementation of pickle.
        """
        if str(e).find('recursion') != -1:
            logger.warning('pylearn2.utils.save encountered the following '
                           'error: ' + str(e) +
                           '\nAttempting to resolve this error by calling ' +
                           'sys.setrecusionlimit and retrying')
            old_limit = sys.getrecursionlimit()
            try:
                sys.setrecursionlimit(50000)
                _save(filepath, obj)
            finally:
                sys.setrecursionlimit(old_limit)

def get_pickle_protocol():
    """
    Allow configuration of the pickle protocol on a per-machine basis.

    This way, if you use multiple platforms with different versions of
    pickle, you can configure each of them to use the highest protocol
    supported by all of the machines that you want to be able to
    communicate.
    """
    try:
        protocol_str = os.environ['PYLEARN2_PICKLE_PROTOCOL']
    except KeyError:
        # If not defined, we default to 0 because this is the default
        # protocol used by cPickle.dump (and because it results in
        # maximum portability)
        protocol_str = '0'
    if protocol_str == 'pickle.HIGHEST_PROTOCOL':
        return pickle.HIGHEST_PROTOCOL
    return int(protocol_str)

def _save(filepath, obj):
    """
    .. todo::

        WRITEME
    """
    try:
        import joblib
        joblib_available = True
    except ImportError:
        joblib_available = False
    if filepath.endswith('.npy'):
        np.save(filepath, obj)
        return
    # This is dumb
    # assert filepath.endswith('.pkl')
    save_dir = os.path.dirname(filepath)
    # Handle current working directory case.
    if save_dir == '':
        save_dir = '.'
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    if os.path.exists(save_dir) and not os.path.isdir(save_dir):
        raise IOError("save path %s exists, not a directory" % save_dir)
    elif not os.access(save_dir, os.W_OK):
        raise IOError("permission error creating %s" % filepath)
    try:
        if joblib_available and filepath.endswith('.joblib'):
            joblib.dump(obj, filepath)
        else:
            if filepath.endswith('.joblib'):
                warnings.warn('Warning: .joblib suffix specified but joblib '
                              'unavailable. Using ordinary pickle.')
            with open(filepath, 'wb') as filehandle:
                cPickle.dump(obj, filehandle, get_pickle_protocol())
    except Exception, e:
        logger.exception("cPickle has failed to write an object to "
                         "{0}".format(filepath))
        if str(e).find('maximum recursion depth exceeded') != -1:
            raise
        try:
            logger.info('retrying with pickle')
            with open(filepath, "wb") as f:
                pickle.dump(obj, f)
        except Exception, e2:
            if str(e) == '' and str(e2) == '':
                logger.exception('neither cPickle nor pickle could write to '
                                 '{0}'.format(filepath))
                logger.exception(
                    'moreover, neither of them raised an exception that '
                    'can be converted to a string'
                )
                logger.exception(
                    'now re-attempting to write with cPickle outside the '
                    'try/catch loop so you can see if it prints anything '
                    'when it dies'
                )
                with open(filepath, 'wb') as f:
                    cPickle.dump(obj, f, get_pickle_protocol())
                logger.info('Somehow or other, the file write worked once '
                            'we quit using the try/catch.')
            else:
                if str(e2) == 'env':
                    raise

                import pdb
                tb = pdb.traceback.format_exc()
                raise IOError(str(obj) +
                              ' could not be written to '+
                              str(filepath) +
                              ' by cPickle due to ' + str(e) +
                              ' nor by pickle due to ' + str(e2) +
                              '. \nTraceback '+ tb)
        logger.warning('{0} was written by pickle instead of cPickle, due to '
                       '{1} (perhaps your object'
                       ' is really big?)'.format(filepath, e))


def clone_via_serialize(obj):
    """
    .. todo::

        WRITEME
    """
    s = cPickle.dumps(obj, get_pickle_protocol())
    return cPickle.loads(s)

def to_string(obj):
    """
    .. todo::

        WRITEME
    """
    return cPickle.dumps(obj, get_pickle_protocol())

def from_string(s):
    """
    .. todo::

        WRITEME
    """
    return cPickle.loads(s)

def mkdir(filepath):
    """
    Make a directory.

    Should succeed even if it needs to make more than one
    directory and nest subdirectories to do so. Raises an error if the
    directory can't be made. Does not raise an error if the directory
    already exists.

    Parameters
    ----------
    filepath : WRITEME
    """
    try:
        os.makedirs(filepath)
    except:
        if not os.path.isdir(filepath):
            raise

def read_int( fin, n = 1):
    """
    .. todo::

        WRITEME
    """
    if n == 1:
        s = fin.read(4)
        if len(s) != 4:
            raise ValueError('fin did not contain 4 bytes')
        return struct.unpack('i', s)[0]
    else:
        rval = []
        for i in xrange(n):
            rval.append(read_int(fin))
        return rval

#dictionary to convert lush binary matrix magic numbers
#to dtypes
lush_magic = {
            507333717 : 'uint8',
            507333716 : 'int32',
            507333713 : 'float32',
            507333715 : 'float64'
        }

def read_bin_lush_matrix(filepath):
    """
    .. todo::

        WRITEME
    """
    f = open(filepath,'rb')
    try:
        magic = read_int(f)
    except ValueError:
        raise ValueError("Couldn't read magic number")
    ndim = read_int(f)

    if ndim == 0:
        shape = ()
    else:
        shape = read_int(f, max(3, ndim))

    total_elems = 1
    for dim in shape:
        total_elems *= dim

    try:
        dtype = lush_magic[magic]
    except KeyError:
        raise ValueError('Unrecognized lush magic number '+str(magic))

    rval = np.fromfile(file = f, dtype = dtype, count = total_elems)

    excess = f.read(-1)

    if excess != '':
        raise ValueError(str(len(excess))+' extra bytes found at end of file.'
                ' This indicates  mismatch between header and content')

    rval = rval.reshape(*shape)

    f.close()

    return rval

def load_train_file(config_file_path, environ=None):
    """
    Loads and parses a yaml file for a Train object.
    Publishes the relevant training environment variables

    Parameters
    ----------
    config_file_path : str
        Path to a config file containing a YAML string describing a
        pylearn2.train.Train object
    environ : dict, optional
        A dictionary used for ${FOO} substitutions in addition to
        environment variables when parsing the YAML file. If a key appears
        both in `os.environ` and this dictionary, the value in this
        dictionary is used.


    Returns
    -------
    Object described by the YAML string stored in the config file
    """
    from pylearn2.config import yaml_parse

    suffix_to_strip = '.yaml'

    # Publish environment variables related to file name
    if config_file_path.endswith(suffix_to_strip):
        config_file_full_stem = config_file_path[0:-len(suffix_to_strip)]
    else:
        config_file_full_stem = config_file_path

    os.environ["PYLEARN2_TRAIN_FILE_FULL_STEM"] = config_file_full_stem

    directory = config_file_path.split('/')[:-1]
    directory = '/'.join(directory)
    if directory != '':
        directory += '/'
    os.environ["PYLEARN2_TRAIN_DIR"] = directory
    os.environ["PYLEARN2_TRAIN_BASE_NAME"] = config_file_path.split('/')[-1]
    os.environ["PYLEARN2_TRAIN_FILE_STEM"] = config_file_full_stem.split('/')[-1]

    return yaml_parse.load_path(config_file_path, environ=environ)

########NEW FILE########
__FILENAME__ = shell
"""
Utilities for running shell scripts and interacting with the terminal
"""
import subprocess as sp
import sys


def run_shell_command(cmd):
    """
    Runs cmd as a shell command. Waits for it to finish executing,
    then returns all output printed to standard error and standard out,
    and the return code.

    Parameters
    ----------
    cmd : str
        The shell command to run

    Returns
    -------
    output : str
        The string output of the process
    rc : WRITEME
        The numeric return code of the process
    """
    child = sp.Popen(cmd, shell=True, stdout=sp.PIPE, stderr=sp.STDOUT)
    output = child.communicate()[0]
    rc = child.returncode
    return output, rc


def print_progression(percent, width=50, delimiters=['[', ']'], symbol='#'):
    """
    Prints a progress bar to the command line

    Parameters
    ----------
    percent : float
        Completion value between 0 and 100
    width : int, optional
        Number of symbols corresponding to a 100 percent completion
    delimiters : list of str, optional
        Character delimiters for the progression bar
    symbol : str, optional
        Symbol representing one unit of progression
    """
    n_symbols = int(percent/100.0*width)
    progress_bar = delimiters[0] + n_symbols * symbol \
                                 + (width - n_symbols) * ' ' \
                                 + delimiters[1] + " "
    sys.stdout.write("\r" + progress_bar + str(percent) + "%")
    sys.stdout.flush()

########NEW FILE########
__FILENAME__ = string_utils
""" Utilities for modifying strings"""

import os
import re

from pylearn2.utils.exc import EnvironmentVariableError, NoDataPathError
from pylearn2.utils.python26 import cmp_to_key
from pylearn2.utils.common_strings import environment_variable_essay


def preprocess(string, environ=None):
    """
    Preprocesses a string, by replacing `${VARNAME}` with
    `os.environ['VARNAME']` and ~ with the path to the user's
    home directory

    Parameters
    ----------
    string : str
        String object to preprocess
    environ : dict, optional
        If supplied, preferentially accept values from
        this dictionary as well as `os.environ`. That is,
        if a key appears in both, this dictionary takes
        precedence.

    Returns
    -------
    rval : str
        The preprocessed string
    """
    if environ is None:
        environ = {}

    split = string.split('${')

    rval = [split[0]]

    for candidate in split[1:]:
        subsplit = candidate.split('}')

        if len(subsplit) < 2:
            raise ValueError('Open ${ not followed by } before '
                             'end of string or next ${ in "' + string + '"')

        varname = subsplit[0]
        try:
            val = (environ[varname] if varname in environ
                   else os.environ[varname])
        except KeyError:
            if varname == 'PYLEARN2_DATA_PATH':
                raise NoDataPathError()
            if varname == 'PYLEARN2_VIEWER_COMMAND':
                raise EnvironmentVariableError(viewer_command_error_essay +
                                               environment_variable_essay)

            raise ValueError('Unrecognized environment variable "' +
                             varname + '". Did you mean ' +
                             match(varname, os.environ.keys()) + '?')

        rval.append(val)

        rval.append('}'.join(subsplit[1:]))

    rval = ''.join(rval)

    string = os.path.expanduser(string)

    return rval


def find_number(s):
    """
    Returns None if there are no numbers in the string. Otherwise,
    returns the range of characters occupied by the first number in
    the string.

    Parameters
    ----------
    s : str
        WRITEME

    Returns
    -------
    WRITEME
    """

    r = re.search('-?\d+[.e]?\d*', s)
    if r is not None:
        return r.span(0)
    return None


def tokenize_by_number(s):
    """
    Splits a string into a list of tokens. Each is either a string
    containing no numbers or a float.

    Parameters
    ----------
    s : str
        WRITEME

    Returns
    -------
    WRITEME
    """

    r = find_number(s)

    if r is None:
        return [s]
    else:
        tokens = []
        if r[0] > 0:
            tokens.append(s[0:r[0]])
        tokens.append(float(s[r[0]:r[1]]))
        if r[1] < len(s):
            tokens.extend(tokenize_by_number(s[r[1]:]))
        return tokens
    assert False  # line should be unreached


def number_aware_alphabetical_cmp(str1, str2):
    """
    cmp function for sorting a list of strings by alphabetical
    order, but with numbers sorted numerically, i.e. `foo1,
    foo2, foo10, foo11` instead of `foo1, foo10, foo11, foo2`.

    Parameters
    ----------
    str1 : str
        WRITEME
    str2 : str
        WRITEME

    Returns
    -------
    WRITEME
    """

    def flatten_tokens(tokens):
        l = []
        for token in tokens:
            if isinstance(token, str):
                for char in token:
                    l.append(char)
            else:
                assert isinstance(token, float)
                l.append(token)
        return l

    seq1 = flatten_tokens(tokenize_by_number(str1))
    seq2 = flatten_tokens(tokenize_by_number(str2))

    l = min(len(seq1), len(seq2))

    i = 0

    while i < l:
        if seq1[i] < seq2[i]:
            return -1
        elif seq1[i] > seq2[i]:
            return 1
        i += 1

    if len(seq1) < len(seq2):
        return -1
    elif len(seq1) > len(seq2):
        return 1

    return 0

#key for sorting strings alphabetically with numbers
number_aware_alphabetical_key = cmp_to_key(number_aware_alphabetical_cmp)


def match(wrong, candidates):
    """
    Returns a guess of which candidate is the right one
    based on the wrong word.

    Parameters
    ----------
    wrong : str
        A mispelling
    candidates : list of str
        A set of correct words

    Returns
    -------
    WRITEME

    Notes
    -----
    This should be used with a small number of candidates and a high
    potential edit distance (i.e. use it to correct a wrong filename in
    a directory, wrong class name in a module, etc.) Don't use it to
    correct small typos of freeform natural language words.
    """

    assert len(candidates) > 0

    # Current implementation tries all candidates and outputs the one
    # with the min score
    # Could try to do something smarter

    def score(w1, w2):
        # Current implementation returns negative dot product of
        # the two words mapped into a feature space by mapping phi
        # w -> [ phi(w1), .1 phi(first letter of w), .1 phi(last letter of w) ]
        # Could try to do something smarter

        w1 = w1.lower()
        w2 = w2.lower()

        def phi(w):
            # Current feature mapping is to the vector of counts of
            # all letters and two-letter sequences
            # Could try to do something smarter
            rval = {}

            for i in xrange(len(w)):
                l = w[i]
                rval[l] = rval.get(l, 0.) + 1.
                if i < len(w) - 1:
                    b = w[i:i + 2]
                    rval[b] = rval.get(b, 0.) + 1.

            return rval

        def mul(d1, d2):
            rval = 0

            for key in set(d1).union(d2):
                rval += d1.get(key, 0) * d2.get(key, 0)

            return rval

        tot_score = mul(phi(w1), phi(w2)) / float(len(w1) * len(w2)) + \
            0.1 * mul(phi(w1[0:1]), phi(w2[0:1])) + \
            0.1 * mul(phi(w1[-1:]), phi(w2[-1:]))

        return tot_score

    scored_candidates = [(-score(wrong, candidate), candidate)
                         for candidate in candidates]

    scored_candidates.sort()

    return scored_candidates[0][1]


def censor_non_alphanum(s):
    """
    Returns s with all non-alphanumeric characters replaced with *
    """

    def censor(ch):
        if (ch >= 'A' and ch <= 'z') or (ch >= '0' and ch <= '9'):
            return ch
        return '*'

    return ''.join(censor(ch) for ch in s)


viewer_command_error_essay = """
PYLEARN2_VIEWER_COMMAND not defined. PLEASE READ THE FOLLOWING MESSAGE
CAREFULLY TO SET UP THIS ENVIRONMENT VARIABLE:
pylearn2 uses an external program to display images. Because different
systems have different image programs available, pylearn2 requires the
 user to specify what image viewer program to use.

You need to choose an image viewer program that pylearn2 should use.
Then tell pylearn2 to use that image viewer program by defining your
PYLEARN2_VIEWER_COMMAND environment variable.

You need to choose PYLEARN_VIEWER_COMMAND such that running

${PYLEARN2_VIEWER_COMMAND} image.png

in a command prompt on your machine will do the following:
    -open an image viewer in a new process.
    -not return until you have closed the image.

Platform-specific recommendations follow.

Linux
=====

Acceptable commands include:
    gwenview
    eog --new-instance

This is assuming that you have gwenview or a version of eog that supports
--new-instance installed on your machine. If you don't, install one of those,
or figure out a command that has the above properties that is available from
your setup.

Mac OS X
========

Acceptable commands include:
    open -Wn

"""

########NEW FILE########
__FILENAME__ = testing
"""
These are helper methods that provide assertions with informative error
messages, and also avoid the builtin 'assert' statement in their
implementation (which makes them more appropriate for testing, as they
run even with assertions disabled, and even in "python -O" mode).
"""

__author__ = "David Warde-Farley"
__copyright__ = "Copyright 2012, Universite de Montreal"
__credits__ = ["David Warde-Farley"]
__license__ = "3-clause BSD"
__email__ = "wardefar@iro"
__maintainer__ = "David Warde-Farley"

from numpy.testing import assert_


def assert_equal(expected, actual):
    """
    Equality assertion with a more informative error message.

    Parameters
    ----------
    expected : WRITEME
    actual : WRITEME
    """
    if expected != actual:
        raise AssertionError("values not equal, expected: %r, actual: %r" %
                             (expected, actual))


def assert_same_object(expected, actual):
    """
    Asserting object identity.

    Parameters
    ----------
    expected : WRITEME
    actual : WRITEME
    """
    if expected is not actual:
        raise AssertionError("values not identical, expected %r, actual %r" %
                             (expected, actual))


def assert_contains(haystack, needle):
    """
    Check if `needle` is in `haystack`.

    Parameters
    ----------
    haystack : WRITEME
    needle : WRITEME
    """
    if needle not in haystack:
        raise AssertionError("item %r not found in collection %r" %
                             (needle, haystack))

########NEW FILE########
__FILENAME__ = test_bit_strings
from pylearn2.utils.bit_strings import all_bit_strings
import numpy as np

def test_bit_strings():
    np.testing.assert_equal((all_bit_strings(3) *
                             (2 ** np.arange(2, -1, -1))).sum(axis=1),
                            np.arange(2 ** 3))

########NEW FILE########
__FILENAME__ = test_compile
"""Tests for compilation utilities."""
import theano
import pickle

from pylearn2.utils.compile import (
    compiled_theano_function, HasCompiledFunctions
)


class Dummy(HasCompiledFunctions):
    const = 3.14159

    @compiled_theano_function
    def func(self):
        val = theano.tensor.as_tensor_variable(self.const)
        return theano.function([], val)


def test_simple_compilation():
    x = Dummy()
    f = x.func
    g = x.func
    assert f is g
    assert abs(x.func() - Dummy.const) < 1e-6


def test_pickling():
    a = Dummy()
    assert abs(a.func() - Dummy.const) < 1e-6
    serialized = pickle.dumps(a)
    b = pickle.loads(serialized)
    assert not hasattr(b, '_compiled_functions')
    assert abs(b.func() - Dummy.const) < 1e-6
    assert not (a.func is b.func)

########NEW FILE########
__FILENAME__ = test_data_specs
"""Tests for compilation utilities."""
import theano.tensor as TT
from pylearn2.utils.data_specs import DataSpecsMapping
from pylearn2.space import VectorSpace, \
        CompositeSpace


def assert_equal(a, b):
    if isinstance(a, tuple) and isinstance(b, tuple):
        msg = 'Length of %s, %d not equal with length of %s, %d' % (
            str(a), len(a), str(b), len(b))
        assert len(a) == len(b), msg
        for elemA, elemB in zip(a, b):
            assert_equal(elemA, elemB)
    else:
        msg = '%s not equal to %s' % (str(a), str(b))
        assert a == b, msg


def test_flatten_specs():
    for space, source, flat_space, flat_source in [
            #(None, None),
            (VectorSpace(dim=5), 'features', VectorSpace(dim=5), 'features'),
            (CompositeSpace([VectorSpace(dim=5), VectorSpace(dim=2)]),
                ('features', 'features'),
                CompositeSpace([VectorSpace(dim=5), VectorSpace(dim=2)]),
                ('features', 'features')),
            (CompositeSpace([VectorSpace(dim=5), VectorSpace(dim=5)]),
                ('features', 'targets'),
                CompositeSpace([VectorSpace(dim=5), VectorSpace(dim=5)]),
                ('features', 'targets')),
            (CompositeSpace([VectorSpace(dim=5), VectorSpace(dim=5)]),
                ('features', 'features'),
                VectorSpace(dim=5),
                'features'),
            (CompositeSpace([VectorSpace(dim=5),
                             CompositeSpace([VectorSpace(dim=9),
                                             VectorSpace(dim=12)])]),
                ('features', ('features', 'targets')),
                CompositeSpace([VectorSpace(dim=5),
                                VectorSpace(dim=9),
                                VectorSpace(dim=12)]),
                ('features', 'features', 'targets')),
            (CompositeSpace([VectorSpace(dim=5),
                             VectorSpace(dim=9),
                             VectorSpace(dim=12)]),
                ('features', 'features', 'targets'),
                CompositeSpace([VectorSpace(dim=5),
                                VectorSpace(dim=9),
                                VectorSpace(dim=12)]),
                ('features', 'features', 'targets'))
            ]:

        mapping = DataSpecsMapping((space, source))
        rval = (mapping.flatten(space), mapping.flatten(source))
        assert_equal((flat_space, flat_source), rval)


def test_nest_specs():
    x1 = TT.matrix('x1')
    x2 = TT.matrix('x2')
    x3 = TT.matrix('x3')
    x4 = TT.matrix('x4')

    for nested_space, nested_source, nested_data in [
            (VectorSpace(dim=10), 'target', x2),
            (CompositeSpace([VectorSpace(dim=3), VectorSpace(dim=9)]),
                ('features', 'features'),
                (x1, x4)),
            (CompositeSpace([VectorSpace(dim=3),
                             CompositeSpace([VectorSpace(dim=10),
                                             VectorSpace(dim=7)])]),
                ('features', ('target', 'features')),
                (x1, (x2, x3))),
            ]:

        mapping = DataSpecsMapping((nested_space, nested_source))
        flat_space = mapping.flatten(nested_space)
        flat_source = mapping.flatten(nested_source)
        flat_data = mapping.flatten(nested_data)

        renested_space = mapping.nest(flat_space)
        renested_source = mapping.nest(flat_source)
        renested_data = mapping.nest(flat_data)

        assert_equal(renested_space, nested_space)
        assert_equal(renested_source, nested_source)
        assert_equal(renested_data, nested_data)

########NEW FILE########
__FILENAME__ = test_insert_along_axis
"""Tests for the InsertAlongAxis op."""
import numpy as np
import theano
from theano import config
from theano import tensor

from pylearn2.utils.insert_along_axis import (
    insert_columns, insert_rows, InsertAlongAxis
)


def test_insert_along_axis():
    x = tensor.matrix()
    y = insert_columns(x, 10, range(0, 10, 2))
    f = theano.function([x], y)
    x_ = np.random.normal(size=(7, 5)).astype(config.floatX)
    f_val = f(x_)
    assert f_val.shape == (7, 10)
    assert np.all(f_val[:, range(0, 10, 2)] == x_)
    assert f_val.dtype == x_.dtype

    y = insert_rows(x, 10, range(0, 10, 2))
    f = theano.function([x], y)
    x_ = np.random.normal(size=(5, 6)).astype(config.floatX)
    f_val = f(x_)
    assert f_val.shape == (10, 6)
    assert np.all(f_val[range(0, 10, 2)] == x_)
    assert f_val.dtype == x_.dtype

    x = tensor.tensor3()
    y = InsertAlongAxis(3, 1)(x, 10, range(0, 10, 2))
    f = theano.function([x], y)
    x_ = np.random.normal(size=(2, 5, 2)).astype(config.floatX)
    f_val = f(x_)
    assert f_val.shape == (2, 10, 2)
    assert np.all(f_val[:, range(0, 10, 2), :] == x_)
    assert f_val.dtype == x_.dtype

    x = tensor.tensor3()
    y = InsertAlongAxis(3, 1, fill=2)(x, 10, range(0, 10, 2))
    f = theano.function([x], y)
    x_ = np.random.normal(size=(2, 5, 2)).astype(config.floatX)
    f_val = f(x_)
    assert f_val.shape == (2, 10, 2)
    assert np.all(f_val[:, range(0, 10, 2), :] == x_)
    assert np.all(f_val[:, range(1, 10, 2), :] == 2)
    assert f_val.dtype == x_.dtype


def test_insert_along_axis_gradient():
    x = tensor.matrix()
    y = insert_columns(x, 10, range(0, 10, 2))
    f = theano.function([x], tensor.grad(y.sum(), x))
    f_val = f(np.random.normal(size=(7, 5)).astype(config.floatX))
    assert np.all(f_val == 1)
    assert f_val.shape == (7, 5)

    y = insert_rows(x, 10, range(0, 10, 2))
    f = theano.function([x], tensor.grad(y.sum(), x))
    f_val = f(np.random.normal(size=(5, 6)).astype(config.floatX))
    assert np.all(f_val == 1)
    assert f_val.shape == (5, 6)

    x = tensor.tensor3()
    y = InsertAlongAxis(3, 1)(x, 10, range(0, 10, 2))
    f = theano.function([x], tensor.grad(y.sum(), x))
    f_val = f(np.random.normal(size=(2, 5, 2)).astype(config.floatX))
    assert np.all(f_val == 1)
    assert f_val.shape == (2, 5, 2)

########NEW FILE########
__FILENAME__ = test_iteration
"""Tests for iterators."""
import numpy as np
from pylearn2.utils.iteration import (
    SubsetIterator,
    SequentialSubsetIterator,
    ShuffledSequentialSubsetIterator,
    RandomSliceSubsetIterator,
    RandomUniformSubsetIterator,
    BatchwiseShuffledSequentialIterator,
    as_even
)


def test_misc_exceptions():
    raised = False
    try:
        SubsetIterator.__new__(SubsetIterator).next()
    except NotImplementedError:
        raised = True
    assert raised
    raised = False
    try:
        SubsetIterator(1, 2, 3)
    except NotImplementedError:
        raised = True
    assert raised
    raised = False
    try:
        SequentialSubsetIterator(10, 3, 3, rng=0)
    except ValueError:
        raised = True
    assert raised


def test_correct_sequential_slices():
    iterator = SequentialSubsetIterator(10, 3, 4)
    sl = iterator.next()
    assert sl.start == 0
    assert sl.stop == 3
    assert sl.step is None
    sl = iterator.next()
    assert sl.start == 3
    assert sl.stop == 6
    assert sl.step is None
    sl = iterator.next()
    assert sl.start == 6
    assert sl.stop == 9
    assert sl.step is None
    sl = iterator.next()
    assert sl.start == 9
    assert sl.stop == 10
    assert sl.step is None

def test_correct_shuffled_sequential_slices():

    dataset_size = 13
    batch_size = 3

    iterator = ShuffledSequentialSubsetIterator(
            dataset_size = dataset_size, batch_size = batch_size,
            num_batches = None, rng = 2)
    visited = [ False ] * dataset_size

    num_batches = 0

    for idxs in iterator:
        for idx in idxs:
            assert not visited[idx]
            visited[idx] = True
        num_batches += 1

    print visited
    assert all(visited)

    assert num_batches == np.ceil(float(dataset_size)/float(batch_size))


def test_sequential_num_batches_and_batch_size():
    try:
        # This should be fine, we have enough examples for 4 batches
        # (with one under-sized batch).
        iterator = SequentialSubsetIterator(10, 3, 4)
        for i in range(4):
            iterator.next()
    except Exception as e:
        assert False
    raised = False
    try:
        iterator.next()
    except StopIteration:
        raised = True
    assert raised
    try:
        # This should be fine, we have enough examples for 4 batches
        # (with one to spare).
        iterator = SequentialSubsetIterator(10, 3, 3)
        for i in range(3):
            iterator.next()
    except Exception:
        assert False
    raised = False
    try:
        iterator.next()
    except StopIteration:
        raised = True
    assert raised
    try:
        # This should fail, since you can't make 5 batches of 3 from 10.
        iterator = SequentialSubsetIterator(10, 3, 5)
    except ValueError:
        return
    assert False


def test_random_slice():
    iterator = RandomSliceSubsetIterator(50, num_batches=10, batch_size=5)
    num = 0
    for iter_slice in iterator:
        assert iter_slice.start >= 0
        assert iter_slice.step is None or iter_slice.step == 1
        assert iter_slice.stop <= 50
        assert iter_slice.stop - iter_slice.start == 5
        num +=  1
    assert num  == 10


def test_random_uniform():
    iterator = RandomUniformSubsetIterator(50, num_batches=10, batch_size=5)
    num = 0
    for iter_slice in iterator:
        assert len(iter_slice ) == 5
        arr = np.array(iter_slice)
        assert np.all(arr < 50)
        assert np.all(arr >= 0)
        num += 1
    assert num == 10

def test_batchwise_shuffled_sequential():

    iterator = BatchwiseShuffledSequentialIterator(30, batch_size = 7)
    for iter_slice in iterator:
        assert iter_slice.start >= 0
        assert iter_slice.step is None or iter_slice.step == 1


def test_uneven_batches():
    dataset_size = 50
    batch_size = 20

    def test_ignore_uneven_iterator(Iterator):
        iterator = as_even(Iterator)(dataset_size, batch_size, None)

        num = 0
        for iter_slice in iterator:
            if isinstance(iter_slice, slice):
                length = iter_slice.stop-iter_slice.start
            else:
                length = len(iter_slice)
            assert length == batch_size
            num += 1
        assert num == 2

    def test_include_uneven_iterator(Iterator):
        iterator = Iterator(dataset_size, batch_size, None)

        num = 0
        for iter_slice in iterator:
            if isinstance(iter_slice, slice):
                length = iter_slice.stop-iter_slice.start
            else:
                length = len(iter_slice)
            assert length in [batch_size, dataset_size % batch_size]
            num += 1
        assert num == 3

    test_ignore_uneven_iterator(SequentialSubsetIterator)
    test_ignore_uneven_iterator(ShuffledSequentialSubsetIterator)
    test_ignore_uneven_iterator(BatchwiseShuffledSequentialIterator)

    test_include_uneven_iterator(SequentialSubsetIterator)
    test_include_uneven_iterator(ShuffledSequentialSubsetIterator)
    test_include_uneven_iterator(BatchwiseShuffledSequentialIterator)

########NEW FILE########
__FILENAME__ = test_key_aware
from pylearn2.utils.key_aware import KeyAwareDefaultDict


def test_key_aware_default_dict():
    a = KeyAwareDefaultDict(str)
    assert a[5] == '5'
    assert a[4] == '4'
    assert a[(3, 2)] == '(3, 2)'
    try:
        b = KeyAwareDefaultDict()
        b[5]
    except KeyError:
        return
    assert False

########NEW FILE########
__FILENAME__ = test_logger_utils
"""Tests for logger utils methods."""

import logging
from pylearn2.utils.logger import newline

logger = logging.getLogger(__name__)


def test_newline():
    """
    Test the state of a the logger passed to the newline function.
    The state has to be the same.
    """
    # Save current properties
    handlers = logger.handlers
    level = logger.getEffectiveLevel()

    newline(logger)

    # Ensure that the logger didn't change
    assert handlers == logger.handlers
    assert level == logger.getEffectiveLevel()

########NEW FILE########
__FILENAME__ = test_mem
"""
Tests for pylearn2.utils.mem functions and classes.
"""


from pylearn2.utils.mem import TypicalMemoryError


def test_typical_memory_error():
    """
    A dummy test that instantiates a TypicalMemoryError
    to see if there is no bugs.
    """
    try:
        raise TypicalMemoryError("test")
    except TypicalMemoryError as e:
        pass

########NEW FILE########
__FILENAME__ = test_mnist_ubyte
import struct
import tempfile
import numpy
from pylearn2.utils.mnist_ubyte import read_mnist_images, read_mnist_labels
from pylearn2.utils.mnist_ubyte import MNIST_LABEL_MAGIC, MNIST_IMAGE_MAGIC

def test_read_labels():
    with tempfile.TemporaryFile() as f:
        data = struct.pack('>iiBBBB', MNIST_LABEL_MAGIC, 4, 9, 4, 3, 1)
        f.write(data)
        f.seek(0)
        arr = read_mnist_labels(f)
        assert arr.shape == (4,)
        assert arr.dtype == numpy.dtype('uint8')
        assert arr[0] == 9
        assert arr[1] == 4
        assert arr[2] == 3
        assert arr[3] == 1

def test_read_images():
    header = struct.pack('>iiii', MNIST_IMAGE_MAGIC, 4, 3, 2)
    data =  ('\x00\x00\x00\x04\x00\x00\x00\x00\x00\x00'
             '\t\x00\x00\x00\x00\x00\x00\xff.\x00\x00\x00\x00\x00')
    with tempfile.TemporaryFile() as f:
        buf = header + data
        f.write(buf)
        f.seek(0)
        arr = read_mnist_images(f)
        assert arr.dtype == numpy.dtype('uint8')
        assert arr[0, 1, 1] == 4
        assert arr[1, 2, 0] == 9
        assert arr[2, 2, 1] == 255
        assert arr[3, 0, 0] == 46
        assert (arr == 0).sum() == 20
        f.seek(0)
        arr = read_mnist_images(f, dtype='float32')
        assert arr.dtype == numpy.dtype('float32')
        assert arr[0, 1, 1] == numpy.float32(4 / 255.)
        assert arr[1, 2, 0] == numpy.float32(9 / 255.)
        assert arr[2, 2, 1] == 1.0
        assert arr[3, 0, 0] == numpy.float32(46 / 255.)
        assert (arr == 0).sum() == 20
        f.seek(0)
        arr = read_mnist_images(f, dtype='bool')
        assert arr.dtype == numpy.dtype('bool')
        assert arr[2, 2, 1] == True
        assert (arr == 0).sum() == 23

########NEW FILE########
__FILENAME__ = test_one_hot
import numpy as np
from numpy.testing import assert_equal, assert_, assert_raises

from pylearn2.utils.one_hot import one_hot, k_hot, compressed_one_hot


def test_one_hot_basic():
    assert_equal(one_hot([1, 2]), [[0, 1, 0], [0, 0, 1]])
    assert_equal(one_hot([[1], [2], [1]], max_label=3),
                 [[0, 1, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0]])


def test_one_hot_dtypes():
    int_dt = ['int8', 'int16', 'int32', 'int64']
    int_dt += ['u' + dt for dt in int_dt]
    float_dt = ['float64', 'float32', 'complex64', 'complex128']
    all_dt = int_dt + float_dt
    assert_(all(one_hot([5], dtype=dt).dtype == np.dtype(dt) for dt in all_dt))


def test_one_hot_out():
    out = np.empty((2, 3), dtype='uint8')
    assert_equal(one_hot([1, 2], out=out),
                 [[0, 1, 0], [0, 0, 1]])
    assert_equal(out, [[0, 1, 0], [0, 0, 1]])


def test_k_hot_basic():
    assert_equal(k_hot([[1], [2], [1]], max_label=3),
                 [[0, 1, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0]])
    assert_equal(k_hot([[1, 2], [2, 0], [1, 0]], max_label=3),
                 [[0, 1, 1, 0], [1, 0, 1, 0], [1, 1, 0, 0]])


def test_k_hot_dtypes():
    int_dt = ['int8', 'int16', 'int32', 'int64']
    int_dt += ['u' + dt for dt in int_dt]
    float_dt = ['float64', 'float32', 'complex64', 'complex128']
    all_dt = int_dt + float_dt
    assert_(all(k_hot([[5, 3]], dtype=dt).dtype == np.dtype(dt)
                for dt in all_dt))


def test_k_hot_out():
    out = np.empty((2, 3), dtype='uint8')
    assert_equal(k_hot([[1, 0], [2, 1]], out=out),
                 [[1, 1, 0], [0, 1, 1]])
    assert_equal(out, [[1, 1, 0], [0, 1, 1]])


def test_out_compressed_one_hot():
    out, uniq = compressed_one_hot([2, 5, 3])
    assert_equal(out, [[1, 0, 0], [0, 0, 1], [0, 1, 0]])
    assert_equal(uniq, [2, 3, 5])

    out, uniq = compressed_one_hot([2, 5])
    assert_equal(out, [[0], [1]])
    assert_equal(uniq, [2, 5])

    out, uniq = compressed_one_hot([2, 5], simplify_binary=False)
    assert_equal(out, [[1, 0], [0, 1]])
    assert_equal(uniq, [2, 5])


def test_exceptions():
    assert_raises(ValueError, one_hot, [5, 3], 4)
    assert_raises(ValueError, one_hot, [5., 3.])
    assert_raises(ValueError, one_hot, [[5, 3], [2, 4]])
    assert_raises(ValueError, k_hot, [[5, 3], [3, 4, 5]])
    assert_raises(ValueError, k_hot, [5, 3, 3, 4, 5])
    assert_raises(ValueError, one_hot, [5, 3, 3, 4, 5], None, None,
                  np.empty((3, 3)))
    assert_raises(ValueError, one_hot, [5, 3, 3, 4, 5], None, None,
                  np.empty((5, 3)))
    assert_raises(ValueError, one_hot, [5, 3, 3, 4, 5], None, 'int8',
                  np.empty((5, 3)))

########NEW FILE########
__FILENAME__ = test_pooling
"""Test pooling-related code in pooling.py"""

import numpy as np
from pylearn2.utils.pooling import pooling_matrix
from pylearn2.testing.skip import skip_if_no_scipy


def test_pooling_no_topology():
    mat = pooling_matrix(4, 5)
    assert mat.shape == (4, 20)
    expected = np.array([[1] * 5 + [0] * 15,
                         [0] * 5 + [1] * 5 + [0] * 10,
                         [0] * 10 + [1] * 5 + [0] * 5,
                         [0] * 15 + [1] * 5])
    assert np.all(mat == expected)
    skip_if_no_scipy()
    spmat = pooling_matrix(4, 5, sparse='csr')
    assert np.all(spmat.todense() == expected)


def test_pooling_1d_topology():
    mat = pooling_matrix(3, 4, 2)
    assert mat.shape == (3, 8)
    expected = np.array([[1, 1, 1, 1, 0, 0, 0, 0],
                         [0, 0, 1, 1, 1, 1, 0, 0],
                         [0, 0, 0, 0, 1, 1, 1, 1]])
    assert np.all(mat == expected)
    skip_if_no_scipy()
    spmat = pooling_matrix(3, 4, 2, sparse='csr')
    assert np.all(spmat.todense() == expected)


def test_pooling_1d_topology_tuples():
    mat = pooling_matrix((3,), (4,), (2,))
    assert mat.shape == (3, 8)
    expected = np.array([[1, 1, 1, 1, 0, 0, 0, 0],
                         [0, 0, 1, 1, 1, 1, 0, 0],
                         [0, 0, 0, 0, 1, 1, 1, 1]])
    assert np.all(mat == expected)
    skip_if_no_scipy()
    spmat = pooling_matrix(3, 4, 2, sparse='csr')
    assert np.all(spmat.todense() == expected)


def test_pooling_2d_topology():
    mat = pooling_matrix((3, 3), (2, 2), (1, 1))
    assert mat.shape == (9, 16)
    expected = np.zeros((9, 16))
    maps = expected.reshape((3, 3, 4, 4))
    maps[0, 0, 0:2, 0:2] = 1.
    maps[1, 0, 1:3, 0:2] = 1.
    maps[2, 0, 2:4, 0:2] = 1.
    maps[0, 1, 0:2, 1:3] = 1.
    maps[1, 1, 1:3, 1:3] = 1.
    maps[2, 1, 2:4, 1:3] = 1.
    maps[0, 2, 0:2, 2:4] = 1.
    maps[1, 2, 1:3, 2:4] = 1.
    maps[2, 2, 2:4, 2:4] = 1.
    assert np.all(mat == expected)
    skip_if_no_scipy()
    spmat = pooling_matrix((3, 3), (2, 2), (1, 1), sparse='csr')
    assert np.all(spmat.todense() == expected)


def test_pooling_2d_topology_stride2():
    mat = pooling_matrix((3, 3), (3, 3), (2, 2))
    expected = np.zeros((9, 49))
    maps = expected.reshape((3, 3, 7, 7))
    maps[0, 0, 0:3, 0:3] = 1.
    maps[0, 1, 0:3, 2:5] = 1.
    maps[0, 2, 0:3, 4:7] = 1.
    maps[1, 0, 2:5, 0:3] = 1.
    maps[1, 1, 2:5, 2:5] = 1.
    maps[1, 2, 2:5, 4:7] = 1.
    maps[2, 0, 4:7, 0:3] = 1.
    maps[2, 1, 4:7, 2:5] = 1.
    maps[2, 2, 4:7, 4:7] = 1.
    assert np.all(mat == expected)
    skip_if_no_scipy()
    spmat = pooling_matrix((3, 3), (3, 3), (2, 2), sparse='csr')
    assert np.all(spmat.todense() == expected)


def test_pooling_2d_non_overlapping():
    mat = pooling_matrix((3, 3), (3, 3), (3, 3))
    assert mat.shape == (9, 81)
    expected = np.zeros((9, 81))
    maps = expected.reshape((3, 3, 9, 9))
    maps[0, 0, 0:3, 0:3] = 1.
    maps[0, 1, 0:3, 3:6] = 1.
    maps[0, 2, 0:3, 6:9] = 1.
    maps[1, 0, 3:6, 0:3] = 1.
    maps[1, 1, 3:6, 3:6] = 1.
    maps[1, 2, 3:6, 6:9] = 1.
    maps[2, 0, 6:9, 0:3] = 1.
    maps[2, 1, 6:9, 3:6] = 1.
    maps[2, 2, 6:9, 6:9] = 1.
    assert np.all(mat == expected)
    skip_if_no_scipy()
    spmat = pooling_matrix((3, 3), (3, 3), (3, 3), sparse='csr')
    assert np.all(spmat.todense() == expected)


def test_exceptions():
    def check_raised(exc_type, func, *args, **kwargs):
        try:
            func(*args, **kwargs)
        except exc_type:
            return
        assert False

    yield (check_raised, TypeError, pooling_matrix, 'hello', 2)
    yield (check_raised, TypeError, pooling_matrix, 2, 'hello')
    yield (check_raised, TypeError, pooling_matrix, 3, 3, 'hello')
    yield (check_raised, ValueError, pooling_matrix, 2, (3, 4))
    yield (check_raised, ValueError, pooling_matrix, (4, 5), 2)
    yield (check_raised, ValueError, pooling_matrix, (4, 5), (5, 6), (3,))
    yield (check_raised, ValueError, pooling_matrix, (4, 5, 6), (3, 2))
    yield (check_raised, ValueError, pooling_matrix, (3, 3), (2, 2), (5, 5))
    yield (check_raised, ValueError, pooling_matrix,
           (3, 3, 3), (2, 2, 2), (2, 2, 1))
    yield (check_raised, ValueError, pooling_matrix, 5, 2, 1, 'float32', 'abc')

########NEW FILE########
__FILENAME__ = test_rng
from pylearn2.utils.rng import make_np_rng, make_theano_rng
import numpy

import theano
from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams


def test_np_rng():
    """
        Tests that the four possible ways of creating
        a numpy RNG give the same results with the same seed
    """

    rngs = [make_np_rng(rng_or_seed=42, which_method='uniform'),
            make_np_rng(rng_or_seed=numpy.random.RandomState(42),
                        which_method='uniform'),
            make_np_rng(default_seed=42),
            make_np_rng()]

    random_numbers = rngs[0].uniform(size=(100,))
    equals = numpy.ones((100,))
    for rng in rngs[1:]:
        equal = random_numbers == rng.uniform(size=(100,))
        equals *= equal

    assert equals.all()


def test_theano_rng():
    """
        Tests that the four possible ways of creating
        a theano RNG give the same results with the same seed
    """

    rngs = [make_theano_rng(rng_or_seed=42, which_method='uniform'),
            make_theano_rng(rng_or_seed=RandomStreams(42),
                            which_method='uniform'),
            make_theano_rng(default_seed=42),
            make_theano_rng()]

    functions = [theano.function([], rng.uniform(size=(100,)))
                 for rng in rngs]

    random_numbers = functions[0]()
    equals = numpy.ones((100,))
    for function in functions[1:]:
        equal = random_numbers == function()
        equals *= equal

    assert equals.all()

########NEW FILE########
__FILENAME__ = test_serial
"""
Tests for the pylearn2.utils.serial module. Currently only tests
read_bin_lush_matrix and load_train_file methods.
"""
import pylearn2
from pylearn2.utils.serial import read_bin_lush_matrix, load_train_file
import numpy as np

pylearn2_path = pylearn2.__path__[0]
example_bin_lush_path = pylearn2_path + '/utils/tests/example_bin_lush/'
yaml_path = pylearn2_path + '/utils/tests/'


def test_read_bin_lush_matrix_ubyte_scalar():
    """
    Read data from a lush file with uint8 data (scalar).

    Note: When you write a scalar from Koray's matlab code it always makes
    everything 3D. Writing it straight from lush you might be able to get
    a true scalar
    """
    path = example_bin_lush_path + 'ubyte_scalar.lushbin'
    result = read_bin_lush_matrix(path)

    assert str(result.dtype) == 'uint8'
    assert len(result.shape) == 3
    assert result.shape[0] == 1
    assert result.shape[1] == 1
    assert result.shape[1] == 1
    assert result[0, 0] == 12


def test_read_bin_lush_matrix_ubyte_3tensor():
    """
    Read data from a lush file with uint8 data (3D-tensor)
    """
    path = example_bin_lush_path + 'ubyte_3tensor.lushbin'
    result = read_bin_lush_matrix(path)

    assert str(result.dtype) == 'uint8'

    assert len(result.shape) == 3
    if result.shape != (2, 3, 4):
        raise AssertionError(
            "ubyte_3tensor.lushbin stores a 3-tensor "
            "of shape (2,3,4), but read_bin_lush_matrix thinks it has "
            "shape " + str(result.shape)
        )

    for i in xrange(1, 3):
        for j in xrange(1, 4):
            for k in xrange(1, 5):
                assert result[i-1, j-1, k-1] == i + 3 * j + 12 * k


def test_read_bin_lush_matrix_int_3tensor():
    """
    Read data from a lush file with int32 data (3D-tensor)
    """
    path = example_bin_lush_path + 'int_3tensor.lushbin'
    result = read_bin_lush_matrix(path)

    assert str(result.dtype) == 'int32'

    assert len(result.shape) == 3
    if result.shape != (3, 2, 4):
        raise AssertionError(
            "ubyte_3tensor.lushbin stores a 3-tensor "
            "of shape (3,2,4), but read_bin_lush_matrix thinks it has "
            "shape " + str(result.shape)
        )

    for i in xrange(1, result.shape[0]+1):
        for j in xrange(1, result.shape[1]+1):
            for k in xrange(1, result.shape[2]+1):
                assert (result[i - 1, j - 1, k - 1] ==
                        (i + 10000 ** j) * ((-2) ** k))


def test_read_bin_lush_matrix_float_3tensor():
    """
    Read data from a lush file with float32 data (3D-tensor)
    """
    path = example_bin_lush_path + 'float_3tensor.lushbin'
    result = read_bin_lush_matrix(path)

    assert str(result.dtype) == 'float32'

    assert len(result.shape) == 3
    if result.shape != (4, 3, 2):
        raise AssertionError(
            "ubyte_3tensor.lushbin stores a 3-tensor "
            "of shape (4,3,2), but read_bin_lush_matrix thinks it has "
            "shape " + str(result.shape)
        )

    for i in xrange(1, result.shape[0] + 1):
        for j in xrange(1, result.shape[1] + 1):
            for k in xrange(1, result.shape[2] + 1):
                assert np.allclose(result[i - 1, j - 1, k - 1],
                                   i + 1.5 * j + 1.7 * k)


def test_read_bin_lush_matrix_double_3tensor():
    """
    Read data from a lush file with float64 data (3D-tensor)
    """
    path = example_bin_lush_path + 'double_3tensor.lushbin'
    result = read_bin_lush_matrix(path)

    assert str(result.dtype) == 'float64'

    assert len(result.shape) == 3
    if result.shape != (4, 2, 3):
        raise AssertionError(
            "ubyte_3tensor.lushbin stores a 3-tensor "
            "of shape (4,2,3), but read_bin_lush_matrix thinks it has "
            "shape " + str(result.shape)
        )

    for i in xrange(1, result.shape[0]+1):
        for j in xrange(1, result.shape[1]+1):
            for k in xrange(1, result.shape[2]+1):
                assert np.allclose(result[i - 1, j - 1, k - 1],
                                   i + 1.5 * j + (-1.7) ** k)


def test_load_train_file():
    """
    Loads a YAML file with and without environment variables.
    """
    environ = {
        'PYLEARN2_DATA_PATH': '/just/a/test/path/'
    }
    load_train_file(yaml_path + 'test_model.yaml')
    load_train_file(yaml_path + 'test_model.yaml', environ=environ)

########NEW FILE########
__FILENAME__ = test_string_utils
from numpy.testing import assert_
import os
import uuid
from pylearn2.utils.string_utils import find_number
from pylearn2.utils.string_utils import preprocess
from pylearn2.utils.string_utils import tokenize_by_number
from pylearn2.utils.string_utils import number_aware_alphabetical_key


def test_preprocess():
    try:
        keys = ["PYLEARN2_" + str(uuid.uuid1())[:8] for _ in xrange(3)]
        strs = ["${%s}" % k for k in keys]
        os.environ[keys[0]] = keys[1]
        # Test with os.environ only.
        assert preprocess(strs[0]) == keys[1]
        # Test with provided dict only.
        assert preprocess(strs[1], environ={keys[1]: keys[2]}) == keys[2]
        # Provided overrides os.environ.
        assert preprocess(strs[0], environ={keys[0]: keys[2]}) == keys[2]
        raised = False
        try:
            preprocess(strs[2], environ={keys[1]: keys[0]})
        except ValueError:
            raised = True
        assert raised

    finally:
        for key in keys:
            if key in os.environ:
                del os.environ[key]


def test_find_number_0():
    r = find_number('sss')
    assert r is None

def test_find_number_1():
    s = 'jashlhl123sfs'
    r = find_number(s)
    assert s[r[0]:r[1]] == '123'

def test_find_number_2():
    s = 'aghwirougiuhfajlsopka"-987?'
    r = find_number(s)
    assert s[r[0]:r[1]] == '-987'


def test_find_number_3():
    s = 'jq% misdirect/ 82ghn 931'
    r = find_number(s)
    assert s[r[0]:r[1]] == '82'

def test_find_number_4():
    s = 'the quick brown fox 54.6 jumped'
    r = find_number(s)
    assert s[r[0]:r[1]] == '54.6'


def test_find_number_5():
    s = 'over the laz-91.2y dog'
    r = find_number(s)
    assert s[r[0]:r[1]] == '-91.2'

def test_find_number_6():
    s = 'query1e5 not found'
    r = find_number(s)
    assert s[r[0]:r[1]] == '1e5'

def test_find_number_7():
    s = 'sdglk421.e6'
    r = find_number(s)
    assert s[r[0]:r[1]] == '421.'

def test_find_number_8():
    s = 'ryleh -14e7$$!$'
    r = find_number(s)
    assert s[r[0]:r[1]] == '-14e7'

def token_lists_nearly_match(l,r):
    if len(l) != len(r):
        print "lengths don't match"
        print len(l)
        print len(r)
        return False

    for l_elem, r_elem in zip(l,r):
        if l_elem != r_elem:
            print l_elem," doesn't match ",r_elem
            return False

    return True

def test_tokenize_0():
    s = ' 123 klsdgh 56.7?98.2---\%-1e3'
    true_tokens = [' ',123,' klsdgh ',56.7,'?',98.2,'---\%',-1e3]
    tokens = tokenize_by_number(s)
    assert token_lists_nearly_match(tokens, true_tokens)

def test_number_aware_alphabetical_key():

    l = [ 'mystr_1', 'mystr_10', 'mystr_2', 'mystr_1_a', 'mystr' ]

    l.sort(key = number_aware_alphabetical_key )

    print l

    assert l == [ 'mystr', 'mystr_1', 'mystr_1_a', 'mystr_2', 'mystr_10' ]

########NEW FILE########
__FILENAME__ = test_utlc
# Right now this file just tests that the utlc module can be imported
from pylearn2.utils import utlc

########NEW FILE########
__FILENAME__ = test_video
"""Tests for pylearn2.utils.video"""
import numpy
from pylearn2.utils.video import FrameLookup, spatiotemporal_cubes

__author__ = "David Warde-Farley"
__copyright__ = "Copyright 2011, David Warde-Farley / Universite de Montreal"
__license__ = "BSD"
__maintainer__ = "David Warde-Farley"
__email__ = "wardefar@iro"


# TODO: write a test for get_video_dims, raising SkipTest
# if pyffmpeg can't be imported


def test_frame_lookup():
    input_data = [('foo', 15), ('bar', 19), ('baz', 26)]
    lookup = FrameLookup(input_data)
    assert len(lookup) == (15 + 19 + 26)
    assert lookup[15] == ('bar', 19, 0)
    assert lookup[14] == ('foo', 15, 14)
    assert lookup[15 + 19 + 4] == ('baz', 26, 4)


def test_spatiotemporal_cubes():
    def check_patch_coverage(files):
        rng = numpy.random.RandomState(1)
        inputs = [(fname, array.shape) for fname, array in files.iteritems()]
        shape = (5, 7, 7)
        for fname, index in spatiotemporal_cubes(inputs, shape, 50000, rng):
            cube = files[fname][index]
            if len(files[fname].shape) == 3:
                assert cube.shape == shape
            else:
                assert cube.shape[:3] == shape[:3]
            cube[...] = True
        for fname, array in files.iteritems():
            assert array.all()

    files = {
        'file1': numpy.zeros((10, 30, 21), dtype=bool),
        'file2': numpy.zeros((15, 25, 28), dtype=bool),
        'file3': numpy.zeros((7, 18, 22), dtype=bool),
    }
    check_patch_coverage(files)

    # Check that stuff still works with an extra color channel dimension.
    files = {
        'file1': numpy.zeros((10, 30, 21, 3), dtype=bool),
        'file2': numpy.zeros((15, 25, 28, 3), dtype=bool),
        'file3': numpy.zeros((7, 18, 22, 3), dtype=bool),
    }
    check_patch_coverage(files)

########NEW FILE########
__FILENAME__ = theano_graph
"""Utility functions that manipulate Theano graphs."""

import theano.tensor as tensor

def is_pure_elemwise(graph, inputs):
    """
    Checks whether a graph is purely elementwise and containing only
    inputs from a given list.

    Parameters
    ----------
    graph : TensorVariable object
        Graph to perform checks against.
    inputs : list
        List of acceptable inputs to the graph.

    Returns
    -------
    elemwise_or_not : bool
        Returns `True` if

        a) everything in the graph is an Elemwise or a DimShuffle
           (DimShuffles are only acceptable to broadcast up constants)
           and
        b) all nodes without an owner appear in `inputs` or are
           constants.

        Returns `False` otherwise.
    """
    allowed_ops = tensor.basic.DimShuffle, tensor.basic.Elemwise
    owner = graph.owner
    op = graph.owner.op if graph.owner is not None else None
    # Ownerless stuff is fine if it's in inputs.
    if owner is None and graph in inputs:
        return True
    # Constants are okay.
    elif owner is None and isinstance(graph, tensor.basic.TensorConstant):
        return True
    # But if it's not a constant and has no owner, it's not.
    elif owner is None and graph not in inputs:
        return False
    # Anything but Elemwise and DimShuffle should be rejected.
    elif op is not None and not isinstance(op, allowed_ops):
        return False
    else:
        if isinstance(graph.owner.op, tensor.basic.DimShuffle):
            shuffled = graph.owner.inputs[0]
            if not isinstance(shuffled, tensor.basic.TensorConstant):
                return False
        for inp in graph.owner.inputs:
            if not is_pure_elemwise(inp, inputs):
                return False
        return True

########NEW FILE########
__FILENAME__ = timing
"""Utilities related to timing various segments of code."""

__authors__ = "David Warde-Farley"
__copyright__ = "Copyright 2010-2012, Universite de Montreal"
__credits__ = ["David Warde-Farley"]
__license__ = "3-clause BSD"
__maintainer__ = "David Warde-Farley"
__email__ = "wardefar@iro"

from contextlib import contextmanager
import logging
import datetime


def total_seconds(delta):
    """
    Extract the total number of seconds from a timedelta object
    in a way that is compatible with Python <= 2.6.

    Parameters
    ----------
    delta : object
        A `datetime.timedelta` object.

    Returns
    -------
    total : float
        The time quantity represented by `delta` in seconds,
        with a fractional portion.
    """
    if hasattr(delta, 'total_seconds'):
        return delta.total_seconds()
    else:
        return (delta.microseconds +
                (delta.seconds + delta.days * 24 * 3600) * 10 ** 6
                ) / float(10 ** 6)


@contextmanager
def log_timing(logger, task, level=logging.INFO, final_msg=None,
               callbacks=None):
    """
    Context manager that logs the start/end of an operation,
    and timing information, to a given logger.

    Parameters
    ----------
    logger : object
        A Python standard library logger object, or an object
        that supports the `logger.log(level, message, ...)`
        API it defines.
    task : str
        A string indicating the operation being performed.
        A '...' will be appended to the initial logged message.
        If `None`, no initial message will be printed.
    level : int, optional
        The log level to use. Default `logging.INFO`.
    final_msg : str, optional
        Display this before the reported time instead of
        '<task> done. Time elapsed:'. A space will be
        added between this message and the reported
        time.
    callbacks: list, optional
        A list of callbacks taking as argument an
        integer representing the total number of seconds.
    """
    start = datetime.datetime.now()
    if task is not None:
        logger.log(level, str(task) + '...')
    yield
    end = datetime.datetime.now()
    delta = end - start
    total = total_seconds(delta)
    if total < 60:
        delta_str = '%f seconds' % total
    else:
        delta_str = str(delta)
    if final_msg is None:
        logger.log(level, str(task) + ' done. Time elapsed: %s' % delta_str)
    else:
        logger.log(level, ' '.join((final_msg, delta_str)))
    if callbacks is not None:
        for callback in callbacks:
            callback(total)

########NEW FILE########
__FILENAME__ = track_version
#!/usr/bin/env python
"""
Script to obtain version of Python modules and basic information on the
experiment setup (e.g. cpu, os), e.g.

* numpy: 1.6.1 | pylearn: a6e634b83d | pylearn2: 57a156beb0
* CPU: x86_64
* OS: Linux-2.6.35.14-106.fc14.x86_64-x86_64-with-fedora-14-Laughlin

You can also define the modules to be tracked with the environment
variable `PYLEARN2_TRACK_MODULES`.  Use ":" to separate module names
between them, e.g. `PYLEARN2_TRACK_MODULES = module1:module2:module3`

By default, the following modules are tracked: pylearn2, theano, numpy, scipy
"""
__authors__ = "Olivier Dellaleau and Raul Chandias Ferrari"
__copyright__ = "Copyright 2013, Universite de Montreal"
__credits__ = ["Olivier Dellaleau", "Raul Chandias Ferrari"]
__license__ = "3-clause BSD"
__maintainer__ = "Raul Chandias Ferrari"
__email__ = "chandiar@iro"


import copy
import logging
import os
import platform
import socket
import subprocess
import sys

logger = logging.getLogger(__name__)


class MetaLibVersion(type):
    """
    Constructor that will be called everytime another's class
    constructor is called (if the "__metaclass__ = MetaLibVersion"
    line is present in the other class definition).

    Parameters
    ----------
    cls : WRITEME
    name : WRITEME
    bases : WRITEME
    dict : WRITEME
    """

    def __init__(cls, name, bases, dict):
        type.__init__(cls, name, bases, dict)
        cls.libv = LibVersion()


class LibVersion(object):
    """
    Initialize a LibVersion object that will store the version of python
    packages in a dictionary (versions).  The python packages that are
    supported are: pylearn, pylearn2, theano, jobman, numpy and scipy.

    The key for the versions dict is the name of the package and the
    associated value is the version number.
    """

    def __init__(self):
        self.versions = {}
        self.str_versions = ''
        self.exp_env_info = {}
        self._get_lib_versions()
        self._get_exp_env_info()

    def _get_exp_env_info(self):
        """
        Get information about the experimental environment such as the
        cpu, os and the hostname of the machine on which the experiment
        is running.
        """
        self.exp_env_info['host'] = socket.gethostname()
        self.exp_env_info['cpu'] = platform.processor()
        self.exp_env_info['os'] = platform.platform()
        if 'theano' in sys.modules:
            self.exp_env_info['theano_config'] = sys.modules['theano'].config
        else:
            self.exp_env_info['theano_config'] = None

    def _get_lib_versions(self):
        """Get version of Python packages."""
        repos = os.getenv('PYLEARN2_TRACK_MODULES', '')
        default_repos = 'pylearn2:theano:numpy:scipy'
        repos = default_repos + ":" + repos
        repos = set(repos.split(':'))
        for repo in repos:
            try:
                if repo == '':
                    continue
                __import__(repo)
                if hasattr(sys.modules[repo], '__version__'):
                    v = sys.modules[repo].__version__
                    if v != 'unknown':
                        self.versions[repo] = v
                        continue
                self.versions[repo] = self._get_git_version(self._get_module_parent_path(sys.modules[repo]))
            except ImportError:
                self.versions[repo] = None

        known = copy.copy(self.versions)
        # Put together all modules with unknown versions.
        unknown = []
        for k, v in known.items():
            if v is None:
                unknown.append(k)
                del known[k]

        # Print versions.
        self.str_versions = ' | '.join(['%s:%s' % (k, v)
                               for k, v in sorted(known.iteritems())] +
                               ['%s:?' % ','.join(sorted(unknown))])

    def __str__(self):
        """
        Return version of the Python packages as a string.
        e.g. numpy:1.6.1 | pylearn:a6e634b83d | pylearn2:57a156beb0
        """
        return self.str_versions

    def _get_git_version(self, root):
        """
        Return the git revision of a repository with the letter 'M'
        appended to the revision if the repo was modified.

        e.g. 10d3046e85 M

        Parameters
        ----------
        root : str
            Root folder of the repository

        Returns
        -------
        rval : str or None
            A string with the revision hash, or None if it could not be
            retrieved (e.g. if it is not actually a git repository)
        """
        if not os.path.isdir(os.path.join(root, '.git')):
            return None
        cwd_backup = os.getcwd()
        try:
            os.chdir(root)
            sub_p = subprocess.Popen(['git', 'rev-parse', 'HEAD'],
                                    stdout=subprocess.PIPE,
                                    stderr=subprocess.PIPE)
            version = sub_p.communicate()[0][0:10].strip()
            sub_p = subprocess.Popen(['git', 'diff', '--name-only'],
                                    stdout=subprocess.PIPE,
                                    stderr=subprocess.PIPE)
            modified = sub_p.communicate()[0]
            if len(modified):
                version += ' M'
            return version
        finally:
            os.chdir(cwd_backup)

    def _get_hg_version(self, root):
        """Same as `get_git_version` but for a Mercurial repository."""
        if not os.path.isdir(os.path.join(root, '.hg')):
            return None
        cwd_backup = os.getcwd()
        try:
            os.chdir(root)
            sub_p = subprocess.Popen(['hg', 'parents'],
                                    stdout=subprocess.PIPE,
                                    stderr=subprocess.PIPE)
            sub_p_output = sub_p.communicate()[0]
        finally:
            os.chdir(cwd_backup)
        first_line = sub_p_output.split('\n')[0]
        # The first line looks like:
        #   changeset:   1517:a6e634b83d88
        return first_line.split(':')[2][0:10]

    def _get_module_path(self, module):
        """Return path to a given module."""
        return os.path.realpath(module.__path__[0])

    def _get_module_parent_path(self, module):
        """Return path to the parent directory of a given module."""
        return os.path.dirname(self._get_module_path(module))

    def print_versions(self):
        """
        Print version of the Python packages as a string.
        e.g. numpy:1.6.1 | pylearn:a6e634b83d | pylearn2:57a156beb0
        """
        logger.info(self.__str__())

    def print_exp_env_info(self, print_theano_config=False):
        """
        Return basic information about the experiment setup such as
        the hostname of the machine the experiment was run on, the
        operating system installed on the machine.

        Parameters
        ----------
        print_theano_config : bool, optional
            If True, information about the theano configuration will be
            displayed.
        """
        logger.info('HOST: {0}'.format(self.exp_env_info['host']))
        logger.info('CPU: {0}'.format(self.exp_env_info['cpu']))
        logger.info('OS: {0}'.format(self.exp_env_info['os']))
        if print_theano_config:
            logger.info(self.exp_env_info['theano_config'])

########NEW FILE########
__FILENAME__ = utlc
"""Several utilities for experimenting upon utlc datasets"""
# Standard library imports
import logging
import os
import inspect
import zipfile
from tempfile import TemporaryFile

# Third-party imports
import numpy
import theano
from pylearn2.datasets.utlc import load_ndarray_dataset, load_sparse_dataset
from pylearn2.utils import subdict, sharedX


logger = logging.getLogger(__name__)


##################################################
# Shortcuts and auxiliary functions
##################################################

def getboth(dict1, dict2, key, default=None):
    """
    Try to retrieve key from dict1 if exists, otherwise try with dict2.
    If the key is not found in any of them, raise an exception.

    Parameters
    ----------
    dict1 : dict
        WRITEME
    dict2 : dict
        WRITEME
    key : WRITEME
    default : WRITEME

    Returns
    -------
    WRITEME
    """
    try:
        return dict1[key]
    except KeyError:
        if default is None:
            return dict2[key]
        else:
            return dict2.get(key, default)

##################################################
# Datasets loading and contest facilities
##################################################

def load_data(conf):
    """
    Loads a specified dataset according to the parameters in the dictionary

    Parameters
    ----------
    conf : WRITEME

    Returns
    -------
    WRITEME
    """
    logger.info('... loading dataset')

    # Special case for sparse format
    if conf.get('sparse', False):
        expected = inspect.getargspec(load_sparse_dataset)[0][1:]
        data = load_sparse_dataset(conf['dataset'], **subdict(conf, expected))
        valid, test = data[1:3]

        # Sparse TERRY data on LISA servers contains an extra null first row in
        # valid and test subsets.
        if conf['dataset'] == 'terry':
            valid = valid[1:]
            test = test[1:]
            assert valid.shape[0] == test.shape[0] == 4096, \
                'Sparse TERRY data loaded has wrong number of examples'

        if len(data) == 3:
            return [data[0], valid, test]
        else:
            return [data[0], valid, test, data[3]]

    # Load as the usual ndarray
    expected = inspect.getargspec(load_ndarray_dataset)[0][1:]
    data = load_ndarray_dataset(conf['dataset'], **subdict(conf, expected))

    # Special case for on-the-fly normalization
    if conf.get('normalize_on_the_fly', False):
        return data

    # Allocate shared variables
    def shared_dataset(data_x):
        """Function that loads the dataset into shared variables"""
        if conf.get('normalize', True):
            return sharedX(data_x, borrow=True)
        else:
            return theano.shared(theano._asarray(data_x), borrow=True)

    return map(shared_dataset, data)


def save_submission(conf, valid_repr, test_repr):
    """
    Create a submission file given a configuration dictionary and a
    representation for valid and test.

    Parameters
    ----------
    conf : WRITEME
    valid_repr : WRITEME
    test_repr : WRITEME
    """
    logger.info('... creating zipfile')

    # Ensure the given directory is correct
    submit_dir = conf['savedir']
    if not os.path.exists(submit_dir):
        os.makedirs(submit_dir)
    elif not os.path.isdir(submit_dir):
        raise IOError('savedir %s is not a directory' % submit_dir)

    basename = os.path.join(submit_dir, conf['dataset'] + '_' + conf['expname'])

    # If there are too much features, outputs kernel matrices
    if (valid_repr.shape[1] > valid_repr.shape[0]):
        valid_repr = numpy.dot(valid_repr, valid_repr.T)
        test_repr = numpy.dot(test_repr, test_repr.T)

    # Quantitize data
    valid_repr = numpy.floor((valid_repr / valid_repr.max())*999)
    test_repr = numpy.floor((test_repr / test_repr.max())*999)

    # Store the representations in two temporary files
    valid_file = TemporaryFile()
    test_file = TemporaryFile()

    numpy.savetxt(valid_file, valid_repr, fmt="%.3f")
    numpy.savetxt(test_file, test_repr, fmt="%.3f")

    # Reread those files and put them together in a .zip
    valid_file.seek(0)
    test_file.seek(0)

    submission = zipfile.ZipFile(basename + ".zip", "w",
                                 compression=zipfile.ZIP_DEFLATED)
    submission.writestr(basename + '_valid.prepro', valid_file.read())
    submission.writestr(basename + '_final.prepro', test_file.read())

    submission.close()
    valid_file.close()
    test_file.close()

def create_submission(conf, transform_valid, transform_test=None, features=None):
    """
    Create a submission file given a configuration dictionary and a
    computation function.

    Note that it always reload the datasets to ensure valid & test
    are not permuted.

    Parameters
    ----------
    conf : WRITEME
    transform_valid : WRITEME
    transform_test : WRITEME
    features : WRITEME
    """
    if transform_test is None:
        transform_test = transform_valid

    # Load the dataset, without permuting valid and test
    kwargs = subdict(conf, ['dataset', 'normalize', 'normalize_on_the_fly', 'sparse'])
    kwargs.update(randomize_valid=False, randomize_test=False)
    valid_set, test_set = load_data(kwargs)[1:3]

    # Sparse datasets are not stored as Theano shared vars.
    if not conf.get('sparse', False):
        valid_set = valid_set.get_value(borrow=True)
        test_set = test_set.get_value(borrow=True)

    # Prefilter features, if needed.
    if features is not None:
        valid_set = valid_set[:, features]
        test_set = test_set[:, features]

    # Valid and test representations
    valid_repr = transform_valid(valid_set)
    test_repr = transform_test(test_set)

    # Convert into text info
    save_submission(conf, valid_repr, test_repr)

##################################################
# Proxies for representation evaluations
##################################################

def compute_alc(valid_repr, test_repr):
    """
    Returns the ALC of the valid set VS test set
    Note: This proxy won't work in the case of transductive learning
    (This is an assumption) but it seems to be a good proxy in the
    normal case (i.e only train on training set)

    Parameters
    ----------
    valid_repr : WRITEME
    test_repr : WRITEME

    Returns
    -------
    WRITEME
    """

    # Concatenate the sets, and give different one hot labels for valid and test
    n_valid = valid_repr.shape[0]
    n_test = test_repr.shape[0]

    _labvalid = numpy.hstack((numpy.ones((n_valid, 1)),
                              numpy.zeros((n_valid, 1))))
    _labtest = numpy.hstack((numpy.zeros((n_test, 1)),
                             numpy.ones((n_test, 1))))

    dataset = numpy.vstack((valid_repr, test_repr))
    label = numpy.vstack((_labvalid, _labtest))

    logger.info('... computing the ALC')
    raise NotImplementedError("This got broken by embed no longer being "
            "where it used to be (if it even still exists, I haven't "
            "looked for it)")
    # return embed.score(dataset, label)


def lookup_alc(data, transform):
    """
    .. todo::

        WRITEME
    """
    valid_repr = transform(data[1].get_value(borrow=True))
    test_repr = transform(data[2].get_value(borrow=True))

    return compute_alc(valid_repr, test_repr)

########NEW FILE########
__FILENAME__ = video
"""
Utilities for working with videos, pulling out patches, etc.
"""
import numpy

from pylearn2.utils.rng import make_np_rng

__author__ = "David Warde-Farley"
__copyright__ = "Copyright 2011, David Warde-Farley / Universite de Montreal"
__license__ = "BSD"
__maintainer__ = "David Warde-Farley"
__email__ = "wardefar@iro"
__all__ = ["get_video_dims", "spatiotemporal_cubes"]


def get_video_dims(fname):
    """
    Pull out the frame length, spatial height and spatial width of
    a video file using ffmpeg.

    Parameters
    ----------
    fname : str
        Path to video file to be inspected.

    Returns
    -------
    shape : tuple
        The spatiotemporal dimensions of the video
        (length, height, width).
    """
    try:
        import pyffmpeg
    except ImportError:
        raise ImportError("This function requires pyffmpeg "
                          "<http://code.google.com/p/pyffmpeg/>")
    mp = pyffmpeg.FFMpegReader()
    try:
        mp.open(fname)
        tracks = mp.get_tracks()
        for track in tracks:
            if isinstance(track, pyffmpeg.VideoTrack):
                break
        else:
            raise ValueError('no video track found')
        return (track.duration(),) + track.get_orig_size()
    finally:
        mp.close()


class FrameLookup(object):
    """
    Class encapsulating the logic of turning a frame index into a
    collection of files into the frame index of a specific video file.

    Item-indexing on this object will yield a (filename, nframes, frame_no)
    tuple, where nframes is the number of frames in the given file
    (mainly for checking that we're far enough from the end so that we
    can sample a big enough chunk).

    Parameters
    ----------
    names_ang_lengths : WRITEME
    """
    def __init__(self, names_and_lengths):
        self.files, self.lengths = zip(*names_and_lengths)
        self.terminals = numpy.cumsum([s[1] for s in names_and_lengths])

    def __getitem__(self, i):
        idx = (i < self.terminals).nonzero()[0][0]
        frame_no = i
        if idx > 0:
            frame_no -= self.terminals[idx - 1]
        return self.files[idx], self.lengths[idx], frame_no

    def __len__(self):
        return self.terminals[-1]

    def __iter__(self):
        raise TypeError('iteration not supported')


def spatiotemporal_cubes(file_tuples, shape, n_patches=numpy.inf, rng=None):
    """
    Generator function that yields a stream of (filename, slicetuple)
    representing a spatiotemporal patch of that file.

    Parameters
    ----------
    file_tuples : list of tuples
        Each element should be a 2-tuple consisting of a filename
        (or arbitrary identifier) and a (length, height, width)
        shape tuple of the dimensions (number of frames in the video,
        height and width of each frame).

    shape : tuple
        A shape tuple consisting of the desired (length, height, width)
        of each spatiotemporal patch.

    n_patches : int, optional
        The number of patches to generate. By default, generates patches
        infinitely.

    rng : RandomState object or seed, optional
        The random number generator (or seed) to use. Defaults to None,
        meaning it will be seeded from /dev/urandom or the clock.

    Returns
    -------
    generator : generator object
        A generator that yields a stream of (filename, slicetuple) tuples.
        The slice tuple is such that it indexes into a 3D array containing
        the entire clip with frames indexed along the first axis, rows
        along the second and columns along the third.
    """
    frame_lookup = FrameLookup([(a, b[0]) for a, b in file_tuples])
    file_lookup = dict(file_tuples)
    patch_length, patch_height, patch_width = shape
    done = 0
    rng = make_np_rng(rng, which_method="random_integers")
    while done < n_patches:
        frame = numpy.random.random_integers(0, len(frame_lookup) - 1)
        filename, file_length, frame_no = frame_lookup[frame]
        # Check that there is a contiguous block of frames starting at
        # frame_no that is at least as long as our desired cube length.
        if file_length - frame_no < patch_length:
            continue
        _, video_height, video_width = file_lookup[filename][:3]
        # The last row and column in which a patch could "start" to still
        # fall within frame.
        last_row = video_height - patch_height
        last_col = video_width - patch_width
        row = numpy.random.random_integers(0, last_row)
        col = numpy.random.random_integers(0, last_col)
        patch_slice = (slice(frame_no, frame_no + patch_length),
                       slice(row, row + patch_height),
                       slice(col, col + patch_width))
        done += 1
        yield filename, patch_slice

########NEW FILE########

## Licensed to Cloudera, Inc. under one
## or more contributor license agreements.  See the NOTICE file
## distributed with this work for additional information
## regarding copyright ownership.  Cloudera, Inc. licenses this file
## to you under the Apache License, Version 2.0 (the
## "License"); you may not use this file except in compliance
## with the License.  You may obtain a copy of the License at
##
##     http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.
<%!
  from filebrowser.views import location_to_url
  from desktop.views import commonheader, commonfooter
  from django.utils.translation import ugettext as _
%>

<%namespace name="components" file="components.mako" />

${ commonheader(_('Table Partitions: %(tableName)s') % dict(tableName=table.name), app_name, user) | n,unicode }
${ components.menubar() }

<div class="container-fluid">
  <div class="row-fluid">
    <div class="span3">
      <div class="sidebar-nav card-small">
        <ul class="nav nav-list">
          <li class="nav-header">${_('Actions')}</li>
          <li><a href="${ url('metastore:describe_table', database=database, table=table.name) }"><i class="fa fa-reply"></i> ${_('Show Table')}</a></li>
        </ul>
      </div>
    </div>
    <div class="span9">
      <div class="card card-small">
        <h1 class="card-heading simple">${ components.breadcrumbs(breadcrumbs) }</h1>
          <div class="card-body">
            <p>
          % if partitions:
          <table class="table table-striped table-condensed datatables">
          <tr>
          % for field in table.partition_keys:
              <th>${field.name}</th>
          % endfor
            <th>${_('Path')}</th>
          </tr>
          % for partition_id, partition in enumerate(partitions):
            <tr>
            % for idx, key in enumerate(partition.values):
                <td><a href="${ url('metastore:read_partition', database=database, table=table.name, partition_id=partition_id) }" data-row-selector="true">${key}</a></td>
            % endfor
            <% location = location_to_url(partition.sd.location) %>
            % if url:
                <td data-row-selector-exclude="true">
                  <a href="${location}">${partition.sd.location}</a>
                </td>
            % else:
                <td>
                ${partition.sd.location}
                </td>
            % endif
            </tr>
          % endfor
          </table>
          % else:
              <div class="alert">${_('The table %s has no partitions.' % table.name)}</div>
          % endif
              </p>
            </div>
        </div>
    </div>
  </div>
</div>

<link rel="stylesheet" href="/metastore/static/css/metastore.css" type="text/css">

<script type="text/javascript" charset="utf-8">
  $(document).ready(function () {
    $("a[data-row-selector='true']").jHueRowSelector();
  });
</script>

${ commonfooter(messages) | n,unicode }

## Licensed to Cloudera, Inc. under one
## or more contributor license agreements.  See the NOTICE file
## distributed with this work for additional information
## regarding copyright ownership.  Cloudera, Inc. licenses this file
## to you under the Apache License, Version 2.0 (the
## "License"); you may not use this file except in compliance
## with the License.  You may obtain a copy of the License at
##
##     http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.
<%!
from django.utils.html import escape

from desktop.lib.i18n import smart_unicode
from desktop.views import commonheader, commonfooter
from django.utils.translation import ugettext as _
%>

<%namespace name="components" file="components.mako" />

<%
  if table.is_view:
    view_or_table_noun = _("View")
  else:
    view_or_table_noun = _("Table")
%>

${ commonheader(_("%s : %s") % (view_or_table_noun, table.name), app_name, user) | n,unicode }
${ components.menubar() }

<%def name="column_table(cols)">
  <table class="table table-striped table-condensed datatables">
    <thead>
      <tr>
        <th>&nbsp;</th>
        <th>${_('Name')}</th>
        <th>${_('Type')}</th>
        <th>${_('Comment')}</th>
      </tr>
    </thead>
    <tbody>
      % for column in cols:
        <tr>
          <td>${ loop.index }</td>
          <td title="${ _("Scroll to the column") }">
            <a href="javascript:void(0)" data-row-selector="true" class="column-selector">${ column.name }</a>
          </td>
          <td>${ column.type }</td>
          <td>${ column.comment != 'None' and column.comment or "" }</td>
        </tr>
      % endfor
    </tbody>
  </table>
</%def>

<div class="container-fluid">
  <div class="row-fluid">
    <div class="span3">
      <div class="sidebar-nav card-small">
        <ul class="nav nav-list">
          <li class="nav-header">${_('Actions')}</li>
          % if has_write_access:
          <li><a href="#" id="import-data-btn"><i class="fa fa-arrow-circle-o-down"></i> ${_('Import Data')}</a></li>
          % endif
          <li><a href="${ url('metastore:read_table', database=database, table=table.name) }"><i class="fa fa-list"></i> ${_('Browse Data')}</a></li>
          % if has_write_access:
          <li><a href="#dropTable" data-toggle="modal"><i class="fa fa-trash-o"></i> ${_('Drop')} ${view_or_table_noun}</a></li>
          % endif
          <li><a href="${ table.hdfs_link }" rel="${ table.path_location }"><i class="fa fa-share-square-o"></i> ${_('View File Location')}</a></li>
          % if table.partition_keys:
          <li><a href="${ url('metastore:describe_partitions', database=database, table=table.name) }"><i class="fa fa-sitemap"></i> ${_('Show Partitions')} (${ len(partitions) })</a></li>
          % endif
        </ul>
      </div>
    </div>
    <div class="span9">
      <div class="card card-small">
        <h1 class="card-heading simple">${ components.breadcrumbs(breadcrumbs) }</h1>
        <div class="card-body">
          <p>
            % if table.comment:
            <div class="alert alert-info">${ _('Comment:') } ${ table.comment }</div>
            % endif

            <ul class="nav nav-tabs">
              <li class="active"><a href="#columns" data-toggle="tab">${_('Columns')}</a></li>
              % if table.partition_keys:
              <li><a href="#partitionColumns" data-toggle="tab">${_('Partition Columns')}</a></li>
              % endif
              % if sample is not None:
              <li><a href="#sample" data-toggle="tab">${_('Sample')}</a></li>
              % endif
              <li><a href="#properties" data-toggle="tab">${ _('Properties') }</a></li>
            </ul>

            <div class="tab-content">
              <div class="active tab-pane" id="columns">
                ${column_table(table.cols)}
              </div>

              % if table.partition_keys:
              <div class="tab-pane" id="partitionColumns">
                ${column_table(table.partition_keys)}
              </div>
              % endif

              % if sample is not None:
              <div class="tab-pane" id="sample">
              % if error_message:
                <div class="alert alert-error">
                  <h3>${_('Error!')}</h3>
                  <pre>${error_message | h}</pre>
                </div>
              % else:
                <table id="sampleTable" class="table table-striped table-condensed sampleTable">
                  <thead>
                    <tr>
                    % for col in table.cols:
                      <th>${col.name}</th>
                    % endfor
                    </tr>
                  </thead>
                  <tbody>
                  % for i, row in enumerate(sample):
                    <tr>
                    % for item in row:
                      <td>
                        % if item is None:
                          NULL
                        % else:
                          ${ escape(smart_unicode(item, errors='ignore')).replace(' ', '&nbsp;') | n,unicode }
                        % endif
                      </td>
                    % endfor
                    </tr>
                  % endfor
                  </tbody>
                </table>
              % endif
              </div>
              % endif

              <div class="tab-pane" id="properties">
                <table class="table table-striped table-condensed">
                  <thead>
                    <tr>
                      <th>${ _('Name') }</th>
                      <th>${ _('Value') }</th>
                    </tr>
                  </thead>
                  <tbody>
                    % for name, value in table.properties:
                      <tr>
                        <td>${ name }</td>
                        <td>${ value }</td>
                      </tr>
                     % endfor
                  </tbody>
                </table>
              </div>
            </div>
          </p>
        </div>
      </div>
    </div>
  </div>
</div>

<div id="dropTable" class="modal hide fade">
  <form id="dropTableForm" method="POST" action="${ url('metastore:drop_table', database=database) }">
    <div class="modal-header">
      <a href="#" class="close" data-dismiss="modal">&times;</a>

      <h3>${_('Drop Table')}</h3>
    </div>
    <div class="modal-body">
      <div id="dropTableMessage">
      </div>
    </div>
    <div class="modal-footer">
      <input type="button" class="btn" data-dismiss="modal" value="${_('Cancel')}"/>
      <input type="submit" class="btn btn-danger" value="${_('Yes, drop this table')}"/>
    </div>
    <div class="hide">
      <select name="table_selection">
        <option value="${ table.name }" selected>${ table.name }</option>
      </select>
    </div>
  </form>
</div>

<div id="import-data-modal" class="modal hide fade"></div>
</div>

<style type="text/css">
  .sampleTable td, .sampleTable th {
    white-space: nowrap;
  }
</style>

<link rel="stylesheet" href="/metastore/static/css/metastore.css" type="text/css">

<script type="text/javascript" charset="utf-8">
  $(document).ready(function () {
    $(".datatables").dataTable({
      "bPaginate": false,
      "bLengthChange": false,
      "bInfo": false,
      "bFilter": false,
      "oLanguage": {
        "sEmptyTable": "${_('No data available')}",
        "sZeroRecords": "${_('No matching records')}",
      },
      "aoColumns": [
        { "sWidth" : "10px" },
        null,
        null,
        { "bSortable": false }
      ],
    });

    $(".column-selector").on("click", function () {
      var _t = $("#sample");
      var _text = $.trim($(this).text().split("(")[0]);
      var _col = _t.find("th").filter(function() {
        return $.trim($(this).text()) == _text;
      });
      _t.find(".columnSelected").removeClass("columnSelected");
      _t.find("tr td:nth-child(" + (_col.index() + 1) + ")").addClass("columnSelected");
      $("a[href='#sample']").click();
    });

    % if has_write_access:
        $.getJSON("${ url('metastore:drop_table', database=database) }", function (data) {
          $("#dropTableMessage").text(data.title);
        });
    % endif

    $('a[data-toggle="tab"]').on('shown', function () {
      $(".sampleTable").not('.initialized').addClass('initialized').dataTable({
        "bPaginate": false,
        "bLengthChange": false,
        "bInfo": false,
        "bFilter": false,
        "fnInitComplete": function () {
          $(".sampleTable").parent().jHueTableScroller();
          $(".sampleTable").jHueTableExtender({
            hintElement: "#jumpToColumnAlert",
            fixedHeader: true
          });
        },
        "oLanguage": {
          "sEmptyTable": "${_('No data available')}",
          "sZeroRecords": "${_('No matching records')}",
        }
      });
    });

    $("#import-data-btn").click(function () {
      $.get("${ url('metastore:load_table', database=database, table=table.name) }", function (response) {
          $("#import-data-modal").html(response['data']);
          $("#import-data-modal").modal("show");
        }
      );
    });
  });
</script>

${ commonfooter(messages) | n,unicode }

Edgewall Documentation Utilities
================================

This repository contains distutils commands for generating offline documentation
from reStructuredText files and Python source code. These tools are shared among
a couple of different Edgewall projects to ensure common style and
functionality.

About Babel
===========

Babel is a Python library that provides an integrated collection of
utilities that assist with internationalizing and localizing Python
applications (in particular web-based applications.)

Details can be found in the HTML files in the `doc` folder.

For more information please visit the Babel web site:

  <http://babel.edgewall.org/>

Tools for using Babel with Django
=================================

This package contains various utilities for integration of Babel into the
Django web framework:

 * A message extraction plugin for Django templates.
 * A middleware class that adds the Babel `Locale` object to requests.
 * A set of template tags for date and number formatting.

For more information please visit the wiki page for this package:

  <http://babel.edgewall.org/wiki/BabelDjango>

I added support to run ctypes from a CVS sandbox without the need to
install the package. This even works for different Python versions
on the same machine, or different machines sharing the same directory.

This behaviour is implemented by the _ctypes.py file in this
directory.  When imported, it replaces itself in sys.modules by the
correct _ctypes.pyd / _ctypes.so file from the platform specific build
directory.

Just do a 'setup.py build' and insert the top-level directory into
sys.path (with setting the PYTHONPATH env variable, with a .pth file,
or however), and it should work.

Overview

    ctypes is a ffi (Foreign Function Interface) package for Python.

    It allows to call functions exposed from dlls/shared libraries and
    has extensive facilities to create, access and manipulate simpole
    and complicated C data types transparently from Python - in other
    words: wrap libraries in pure Python.

    ctypes runs on Windows, Windows CE, MacOS X, Linux, Solaris,
    FreeBSD, OpenBSD.  It may also run on other systems, provided that
    libffi supports this platform.

    ctypes includes libffi, which is copyright Red Hat, Inc.  Complete
    license see below.


Requirements

    ctypes requires Python 2.3 or higher, since it makes intensive use
    of the new type system.

    In Python 2.5, the ctypes package is already included.


Installation

    Windows

        On Windows, it is the easiest to download the executable
        installer for your Python version and run it.

    Installation from source

        To install ctypes from source, unpack the distribution, enter
        the ctypes-x.y.z source directory, and enter

            python setup.py build

	This will build the Python extension modules.  A C compiler is
	required.

	To run the supplied tests, enter

	    python setup.py test

	To install ctypes, enter

            python setup.py install --help

        to see the avaibable options, and finally

	    python setup.py install [options]


    Windows CE

        Required software to build the _ctypes.pyd extension module
        for Windows CE (all these are free downloads):

	- Embedded Visual C 4.0 with service pack 2 (3?, 4?)

	- Pocket PC 2003 SDK

	- The Python 2.3 or 2.4 windows CE development files: include
	files, header files.

	Open the wince\_ctypes.vcw project file with embedded visual C
	4.0.  Select "POCKET PC 2003", "Win32 (WCE ARMV4) Release",
	and "POCKET PC 2003 Device" in the comboboxes.

	From the menu, select Tools->Options->Directories, and add
	these directories:

	    Include files: add c:\Python-2.4.3-wince-dev\INCLUDE
	    Library files: add c:\Python-2.4.3-wince-dev\LIB

	Right click '_ctypes files' in the FileView, select
	'Settings', then the 'Debug' tab.

	Enter '\Program Files\Python24' in the 'Download directory'
	box.  Do the same for the '_ctypes_test' project.

	Now you can connect your pocket PC, build the projects in
	visual C, and both _ctypes.pyd and _ctypes_test.pyd should be
	downloaded automatically to your device.


ctypes license

  Copyright (c) 2000, 2001, 2002, 2003, 2004, 2005, 2006 Thomas Heller

  Permission is hereby granted, free of charge, to any person
  obtaining a copy of this software and associated documentation files
  (the "Software"), to deal in the Software without restriction,
  including without limitation the rights to use, copy, modify, merge,
  publish, distribute, sublicense, and/or sell copies of the Software,
  and to permit persons to whom the Software is furnished to do so,
  subject to the following conditions:

  The above copyright notice and this permission notice shall be
  included in all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
  EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
  MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
  NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
  BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
  ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
  CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
  SOFTWARE.

libffi license

  libffi - Copyright (c) 1996-2003  Red Hat, Inc.

  Permission is hereby granted, free of charge, to any person
  obtaining a copy of this software and associated documentation files
  (the ``Software''), to deal in the Software without restriction,
  including without limitation the rights to use, copy, modify, merge,
  publish, distribute, sublicense, and/or sell copies of the Software,
  and to permit persons to whom the Software is furnished to do so,
  subject to the following conditions:

  The above copyright notice and this permission notice shall be
  included in all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED ``AS IS'', WITHOUT WARRANTY OF ANY KIND,
  EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
  MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
  NONINFRINGEMENT.  IN NO EVENT SHALL CYGNUS SOLUTIONS BE LIABLE FOR
  ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF
  CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
  WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

This directory contains the libffi package, which is not part of GCC but
shipped with GCC as convenience.

Status
======

libffi-2.00 has not been released yet! This is a development snapshot!

libffi-1.20 was released on October 5, 1998. Check the libffi web
page for updates: <URL:http://sources.redhat.com/libffi/>.


What is libffi?
===============

Compilers for high level languages generate code that follow certain
conventions. These conventions are necessary, in part, for separate
compilation to work. One such convention is the "calling
convention". The "calling convention" is essentially a set of
assumptions made by the compiler about where function arguments will
be found on entry to a function. A "calling convention" also specifies
where the return value for a function is found.

Some programs may not know at the time of compilation what arguments
are to be passed to a function. For instance, an interpreter may be
told at run-time about the number and types of arguments used to call
a given function. Libffi can be used in such programs to provide a
bridge from the interpreter program to compiled code.

The libffi library provides a portable, high level programming
interface to various calling conventions. This allows a programmer to
call any function specified by a call interface description at run
time.  

Ffi stands for Foreign Function Interface. A foreign function
interface is the popular name for the interface that allows code
written in one language to call code written in another language. The
libffi library really only provides the lowest, machine dependent
layer of a fully featured foreign function interface. A layer must
exist above libffi that handles type conversions for values passed
between the two languages.


Supported Platforms and Prerequisites
=====================================

Libffi has been ported to:

	SunOS 4.1.3 & Solaris 2.x (SPARC-V8, SPARC-V9)

	Irix 5.3 & 6.2 (System V/o32 & n32)

	Intel x86 - Linux (System V ABI)

	Alpha - Linux and OSF/1

	m68k - Linux (System V ABI)

	PowerPC - Linux (System V ABI, Darwin, AIX)

	ARM - Linux (System V ABI)

Libffi has been tested with the egcs 1.0.2 gcc compiler. Chances are
that other versions will work.  Libffi has also been built and tested
with the SGI compiler tools.

On PowerPC, the tests failed (see the note below).

You must use GNU make to build libffi. SGI's make will not work.
Sun's probably won't either.
	
If you port libffi to another platform, please let me know! I assume
that some will be easy (x86 NetBSD), and others will be more difficult
(HP).


Installing libffi
=================

[Note: before actually performing any of these installation steps,
 you may wish to read the "Platform Specific Notes" below.]

First you must configure the distribution for your particular
system. Go to the directory you wish to build libffi in and run the
"configure" program found in the root directory of the libffi source
distribution.

You may want to tell configure where to install the libffi library and
header files. To do that, use the --prefix configure switch.  Libffi
will install under /usr/local by default. 

If you want to enable extra run-time debugging checks use the the
--enable-debug configure switch. This is useful when your program dies
mysteriously while using libffi. 

Another useful configure switch is --enable-purify-safety. Using this
will add some extra code which will suppress certain warnings when you
are using Purify with libffi. Only use this switch when using 
Purify, as it will slow down the library.

Configure has many other options. Use "configure --help" to see them all.

Once configure has finished, type "make". Note that you must be using
GNU make. SGI's make will not work.  Sun's probably won't either.
You can ftp GNU make from prep.ai.mit.edu:/pub/gnu.

To ensure that libffi is working as advertised, type "make test".

To install the library and header files, type "make install".


Using libffi
============

	The Basics
	----------

Libffi assumes that you have a pointer to the function you wish to
call and that you know the number and types of arguments to pass it,
as well as the return type of the function.

The first thing you must do is create an ffi_cif object that matches
the signature of the function you wish to call. The cif in ffi_cif
stands for Call InterFace. To prepare a call interface object, use the
following function:

ffi_status ffi_prep_cif(ffi_cif *cif, ffi_abi abi,
			unsigned int nargs, 
			ffi_type *rtype, ffi_type **atypes);

	CIF is a pointer to the call interface object you wish
		to initialize.

	ABI is an enum that specifies the calling convention 
		to use for the call. FFI_DEFAULT_ABI defaults
		to the system's native calling convention. Other
		ABI's may be used with care. They are system
		specific.

	NARGS is the number of arguments this function accepts.	
		libffi does not yet support vararg functions.

	RTYPE is a pointer to an ffi_type structure that represents
		the return type of the function. Ffi_type objects
		describe the types of values. libffi provides
		ffi_type objects for many of the native C types:
		signed int, unsigned int, signed char, unsigned char,
		etc. There is also a pointer ffi_type object and
		a void ffi_type. Use &ffi_type_void for functions that 
		don't return values.

	ATYPES is a vector of ffi_type pointers. ARGS must be NARGS long.
		If NARGS is 0, this is ignored.


ffi_prep_cif will return a status code that you are responsible 
for checking. It will be one of the following:

	FFI_OK - All is good.

	FFI_BAD_TYPEDEF - One of the ffi_type objects that ffi_prep_cif
		came across is bad.


Before making the call, the VALUES vector should be initialized 
with pointers to the appropriate argument values.

To call the the function using the initialized ffi_cif, use the
ffi_call function:

void ffi_call(ffi_cif *cif, void *fn, void *rvalue, void **avalues);

	CIF is a pointer to the ffi_cif initialized specifically
		for this function.

	FN is a pointer to the function you want to call.

	RVALUE is a pointer to a chunk of memory that is to hold the
		result of the function call. Currently, it must be
		at least one word in size (except for the n32 version
		under Irix 6.x, which must be a pointer to an 8 byte 
		aligned value (a long long). It must also be at least 
		word aligned (depending on the return type, and the
		system's alignment requirements). If RTYPE is 
		&ffi_type_void, this is ignored. If RVALUE is NULL, 
		the return value is discarded.

	AVALUES is a vector of void* that point to the memory locations
		holding the argument values for a call.
		If NARGS is 0, this is ignored.


If you are expecting a return value from FN it will have been stored
at RVALUE.



	An Example
	----------

Here is a trivial example that calls puts() a few times.

    #include <stdio.h>
    #include <ffi.h>
    
    int main()
    {
      ffi_cif cif;
      ffi_type *args[1];
      void *values[1];
      char *s;
      int rc;
      
      /* Initialize the argument info vectors */    
      args[0] = &ffi_type_uint;
      values[0] = &s;
      
      /* Initialize the cif */
      if (ffi_prep_cif(&cif, FFI_DEFAULT_ABI, 1, 
    		       &ffi_type_uint, args) == FFI_OK)
        {
          s = "Hello World!";
          ffi_call(&cif, puts, &rc, values);
          /* rc now holds the result of the call to puts */
          
          /* values holds a pointer to the function's arg, so to 
	     call puts() again all we need to do is change the 
             value of s */
          s = "This is cool!";
          ffi_call(&cif, puts, &rc, values);
        }
      
      return 0;
    }



	Aggregate Types
	---------------

Although libffi has no special support for unions or bit-fields, it is
perfectly happy passing structures back and forth. You must first
describe the structure to libffi by creating a new ffi_type object
for it. Here is the definition of ffi_type:

    typedef struct _ffi_type
    {
      unsigned size;
      short alignment;
      short type;
      struct _ffi_type **elements;
    } ffi_type;
    
All structures must have type set to FFI_TYPE_STRUCT.  You may set
size and alignment to 0. These will be calculated and reset to the
appropriate values by ffi_prep_cif().

elements is a NULL terminated array of pointers to ffi_type objects
that describe the type of the structure elements. These may, in turn,
be structure elements.

The following example initializes a ffi_type object representing the
tm struct from Linux's time.h:

				    struct tm {
					int tm_sec;
					int tm_min;
					int tm_hour;
					int tm_mday;
					int tm_mon;
					int tm_year;
					int tm_wday;
					int tm_yday;
					int tm_isdst;
					/* Those are for future use. */
					long int __tm_gmtoff__;
					__const char *__tm_zone__;
				    };

    {
      ffi_type tm_type;
      ffi_type *tm_type_elements[12];
      int i;

      tm_type.size = tm_type.alignment = 0;
      tm_type.elements = &tm_type_elements;
    
      for (i = 0; i < 9; i++)
          tm_type_elements[i] = &ffi_type_sint;

      tm_type_elements[9] = &ffi_type_slong;
      tm_type_elements[10] = &ffi_type_pointer;
      tm_type_elements[11] = NULL;

      /* tm_type can now be used to represent tm argument types and
	 return types for ffi_prep_cif() */
    }



Platform Specific Notes
=======================

	Intel x86
	---------

There are no known problems with the x86 port.

	Sun SPARC - SunOS 4.1.3 & Solaris 2.x
	-------------------------------------

You must use GNU Make to build libffi on Sun platforms.

	MIPS - Irix 5.3 & 6.x
	---------------------

Irix 6.2 and better supports three different calling conventions: o32,
n32 and n64. Currently, libffi only supports both o32 and n32 under
Irix 6.x, but only o32 under Irix 5.3. Libffi will automatically be
configured for whichever calling convention it was built for.

By default, the configure script will try to build libffi with the GNU
development tools. To build libffi with the SGI development tools, set
the environment variable CC to either "cc -32" or "cc -n32" before
running configure under Irix 6.x (depending on whether you want an o32
or n32 library), or just "cc" for Irix 5.3.

With the n32 calling convention, when returning structures smaller
than 16 bytes, be sure to provide an RVALUE that is 8 byte aligned.
Here's one way of forcing this:

	double struct_storage[2];
	my_small_struct *s = (my_small_struct *) struct_storage;  
	/* Use s for RVALUE */

If you don't do this you are liable to get spurious bus errors. 

"long long" values are not supported yet.

You must use GNU Make to build libffi on SGI platforms.

	ARM - System V ABI
	------------------

The ARM port was performed on a NetWinder running ARM Linux ELF
(2.0.31) and gcc 2.8.1.



	PowerPC System V ABI
	--------------------

There are two `System V ABI's which libffi implements for PowerPC.
They differ only in how small structures are returned from functions.

In the FFI_SYSV version, structures that are 8 bytes or smaller are
returned in registers.  This is what GCC does when it is configured
for solaris, and is what the System V ABI I have (dated September
1995) says.

In the FFI_GCC_SYSV version, all structures are returned the same way:
by passing a pointer as the first argument to the function.  This is
what GCC does when it is configured for linux or a generic sysv
target.

EGCS 1.0.1 (and probably other versions of EGCS/GCC) also has a
inconsistency with the SysV ABI: When a procedure is called with many
floating-point arguments, some of them get put on the stack.  They are
all supposed to be stored in double-precision format, even if they are
only single-precision, but EGCS stores single-precision arguments as
single-precision anyway.  This causes one test to fail (the `many
arguments' test).


What's With The Crazy Comments?
===============================

You might notice a number of cryptic comments in the code, delimited
by /*@ and @*/. These are annotations read by the program LCLint, a
tool for statically checking C programs. You can read all about it at
<http://larch-www.lcs.mit.edu:8001/larch/lclint/index.html>.


History
=======

1.20 Oct-5-98
	Raffaele Sena produces ARM port.

1.19 Oct-5-98
	Fixed x86 long double and long long return support.
	m68k bug fixes from Andreas Schwab.
	Patch for DU assembler compatibility for the Alpha from Richard
	Henderson.

1.18 Apr-17-98
	Bug fixes and MIPS configuration changes.

1.17 Feb-24-98
	Bug fixes and m68k port from Andreas Schwab. PowerPC port from
	Geoffrey Keating. Various bug x86, Sparc and MIPS bug fixes.

1.16 Feb-11-98
	Richard Henderson produces Alpha port.

1.15 Dec-4-97
	Fixed an n32 ABI bug. New libtool, auto* support.

1.14 May-13-97
	libtool is now used to generate shared and static libraries.
	Fixed a minor portability problem reported by Russ McManus
	<mcmanr@eq.gs.com>.

1.13 Dec-2-96
	Added --enable-purify-safety to keep Purify from complaining
	about certain low level code.
	Sparc fix for calling functions with < 6 args.
	Linux x86 a.out fix.

1.12 Nov-22-96
	Added missing ffi_type_void, needed for supporting void return 
	types. Fixed test case for non MIPS machines. Cygnus Support 
	is now Cygnus Solutions. 

1.11 Oct-30-96
	Added notes about GNU make.

1.10 Oct-29-96
	Added configuration fix for non GNU compilers.

1.09 Oct-29-96
	Added --enable-debug configure switch. Clean-ups based on LCLint 
	feedback. ffi_mips.h is always installed. Many configuration 
	fixes. Fixed ffitest.c for sparc builds.

1.08 Oct-15-96
	Fixed n32 problem. Many clean-ups.

1.07 Oct-14-96
	Gordon Irlam rewrites v8.S again. Bug fixes.

1.06 Oct-14-96
	Gordon Irlam improved the sparc port. 

1.05 Oct-14-96
	Interface changes based on feedback.

1.04 Oct-11-96
	Sparc port complete (modulo struct passing bug).

1.03 Oct-10-96
	Passing struct args, and returning struct values works for
	all architectures/calling conventions. Expanded tests.

1.02 Oct-9-96
	Added SGI n32 support. Fixed bugs in both o32 and Linux support.
	Added "make test".

1.01 Oct-8-96
	Fixed float passing bug in mips version. Restructured some
	of the code. Builds cleanly with SGI tools.

1.00 Oct-7-96
	First release. No public announcement.


Authors & Credits
=================

libffi was written by Anthony Green <green@cygnus.com>.

Portions of libffi were derived from Gianni Mariani's free gencall
library for Silicon Graphics machines.

The closure mechanism was designed and implemented by Kresten Krab
Thorup.

The Sparc port was derived from code contributed by the fine folks at
Visible Decisions Inc <http://www.vdi.com>. Further enhancements were
made by Gordon Irlam at Cygnus Solutions <http://www.cygnus.com>.

The Alpha port was written by Richard Henderson at Cygnus Solutions.

Andreas Schwab ported libffi to m68k Linux and provided a number of
bug fixes.

Geoffrey Keating ported libffi to the PowerPC.

Raffaele Sena ported libffi to the ARM.

Jesper Skov and Andrew Haley both did more than their fair share of
stepping through the code and tracking down bugs.

Thanks also to Tom Tromey for bug fixes and configuration help.

Thanks to Jim Blandy, who provided some useful feedback on the libffi
interface.

If you have a problem, or have found a bug, please send a note to
green@cygnus.com.

The documentation in this tree is in plain text files and can be viewed using
any text file viewer.

Technically speaking, it uses ReST (reStructuredText) [1], and the Sphinx
documentation system [2].  This allows it to be built into other forms for
easier viewing and browsing.

To create an HTML version of the docs:

* Install Sphinx (using ``sudo pip install Sphinx`` or some other method)

* In this docs/ directory, type ``make html`` (or ``make.bat html`` on
  Windows) at a shell prompt.

The documentation in _build/html/index.html can then be viewed in a web browser.

[1] http://docutils.sourceforge.net/rst.html
[2] http://sphinx.pocoo.org/

This directory contains extra stuff that can improve your Django experience.

Django is a high-level Python Web framework that encourages rapid development
and clean, pragmatic design.

All documentation is in the "docs" directory and online at
http://docs.djangoproject.com/en/dev/. If you're just getting started, here's
how we recommend you read the docs:

    * First, read docs/intro/install.txt for instructions on installing Django.

    * Next, work through the tutorials in order (docs/intro/tutorial01.txt,
      docs/intro/tutorial02.txt, etc.).

    * If you want to set up an actual deployment server, read
      docs/howto/deployment/index.txt for instructions.

    * You'll probably want to read through the topical guides (in docs/topics)
      next; from there you can jump to the HOWTOs (in docs/howto) for specific
      problems, and check out the reference (docs/ref) for gory details.

    * See docs/README for instructions on building an HTML version of the docs.

Docs are updated rigorously. If you find any problems in the docs, or think they
should be clarified in any way, please take 30 seconds to fill out a ticket
here:

http://code.djangoproject.com/newticket

To get more help:

    * Join the #django channel on irc.freenode.net. Lots of helpful people
      hang out there. Read the archives at http://django-irc-logs.com/.

    * Join the django-users mailing list, or read the archives, at
      http://groups.google.com/group/django-users.

To contribute to Django:

    * Check out http://www.djangoproject.com/community/ for information
      about getting involved.

To run Django's test suite:

    * Follow the instructions in the "Unit tests" section of
      docs/internals/contributing/writing-code/unit-tests.txt, published online at
      https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/unit-tests/#running-the-unit-tests

Description in your templates: {{ obj }}
This is a Django authentication backend that authenticates against an LDAP
service. Configuration can be as simple as a single distinguished name
template, but there are many rich configuration options for working with users,
groups, and permissions.

This version is tested on Python 2.6 to 2.7, Django >= 1.3, and python-ldap
2.4.13.

Full documentation can be found at http://pythonhosted.org/django-auth-ldap/;
following is an example configuration, just to whet your appetite::

    import ldap
    from django_auth_ldap.config import LDAPSearch, GroupOfNamesType


    # Baseline configuration.
    AUTH_LDAP_SERVER_URI = "ldap://ldap.example.com"

    AUTH_LDAP_BIND_DN = "cn=django-agent,dc=example,dc=com"
    AUTH_LDAP_BIND_PASSWORD = "phlebotinum"
    AUTH_LDAP_USER_SEARCH = LDAPSearch("ou=users,dc=example,dc=com",
        ldap.SCOPE_SUBTREE, "(uid=%(user)s)")
    # or perhaps:
    # AUTH_LDAP_USER_DN_TEMPLATE = "uid=%(user)s,ou=users,dc=example,dc=com"

    # Set up the basic group parameters.
    AUTH_LDAP_GROUP_SEARCH = LDAPSearch("ou=django,ou=groups,dc=example,dc=com",
        ldap.SCOPE_SUBTREE, "(objectClass=groupOfNames)"
    )
    AUTH_LDAP_GROUP_TYPE = GroupOfNamesType()

    # Simple group restrictions
    AUTH_LDAP_REQUIRE_GROUP = "cn=enabled,ou=django,ou=groups,dc=example,dc=com"
    AUTH_LDAP_DENY_GROUP = "cn=disabled,ou=django,ou=groups,dc=example,dc=com"

    # Populate the Django user from the LDAP directory.
    AUTH_LDAP_USER_ATTR_MAP = {
        "first_name": "givenName",
        "last_name": "sn",
        "email": "mail"
    }

    AUTH_LDAP_PROFILE_ATTR_MAP = {
        "employee_number": "employeeNumber"
    }

    AUTH_LDAP_USER_FLAGS_BY_GROUP = {
        "is_active": "cn=active,ou=django,ou=groups,dc=example,dc=com",
        "is_staff": "cn=staff,ou=django,ou=groups,dc=example,dc=com",
        "is_superuser": "cn=superuser,ou=django,ou=groups,dc=example,dc=com"
    }

    AUTH_LDAP_PROFILE_FLAGS_BY_GROUP = {
        "is_awesome": "cn=awesome,ou=django,ou=groups,dc=example,dc=com",
    }

    # Use LDAP group membership to calculate group permissions.
    AUTH_LDAP_FIND_GROUP_PERMS = True

    # Cache group memberships for an hour to minimize LDAP traffic
    AUTH_LDAP_CACHE_GROUPS = True
    AUTH_LDAP_GROUP_CACHE_TIMEOUT = 3600


    # Keep ModelBackend around for per-user permissions and maybe a local
    # superuser.
    AUTHENTICATION_BACKENDS = (
        'django_auth_ldap.backend.LDAPBackend',
        'django.contrib.auth.backends.ModelBackend',
    )

===========
django-nose
===========

.. image:: https://travis-ci.org/jbalogh/django-nose.png
  :target: https://travis-ci.org/jbalogh/django-nose

Features
--------

* All the goodness of `nose`_ in your Django tests, like...

  * Testing just your apps by default, not all the standard ones that happen to
    be in ``INSTALLED_APPS``
  * Running the tests in one or more specific modules (or apps, or classes, or
    folders, or just running a specific test)
  * Obviating the need to import all your tests into ``tests/__init__.py``.
    This not only saves busy-work but also eliminates the possibility of
    accidentally shadowing test classes.
  * Taking advantage of all the useful `nose plugins`_
* Fixture bundling, an optional feature which speeds up your fixture-based
  tests by a factor of 4
* Reuse of previously created test DBs, cutting 10 seconds off startup time
* Hygienic TransactionTestCases, which can save you a DB flush per test
* Support for various databases. Tested with MySQL, PostgreSQL, and SQLite.
  Others should work as well.

.. _nose: http://somethingaboutorange.com/mrl/projects/nose/
.. _nose plugins: http://nose-plugins.jottit.com/


Installation
------------

You can get django-nose from PyPI with... ::

    pip install django-nose

The development version can be installed with... ::

    pip install -e git://github.com/jbalogh/django-nose.git#egg=django-nose

Since django-nose extends Django's built-in test command, you should add it to
your ``INSTALLED_APPS`` in ``settings.py``::

    INSTALLED_APPS = (
        ...
        'django_nose',
        ...
    )

Then set ``TEST_RUNNER`` in ``settings.py``::

    TEST_RUNNER = 'django_nose.NoseTestSuiteRunner'


Use
---

The day-to-day use of django-nose is mostly transparent; just run ``./manage.py
test`` as usual.

See ``./manage.py help test`` for all the options nose provides, and look to
the `nose docs`_ for more help with nose.

.. _nose docs: http://somethingaboutorange.com/mrl/projects/nose/


Enabling Database Reuse
-----------------------

You can save several seconds at the beginning and end of your test suite by
reusing the test database from the last run. To do this, set the environment
variable ``REUSE_DB`` to 1::

    REUSE_DB=1 ./manage.py test

The one new wrinkle is that, whenever your DB schema changes, you should leave
the flag off the next time you run tests. This will cue the test runner to
reinitialize the test database.

Also, REUSE_DB is not compatible with TransactionTestCases that leave junk in
the DB, so be sure to make your TransactionTestCases hygienic (see below) if
you want to use it.


Enabling Fast Fixtures
----------------------

django-nose includes a fixture bundler which drastically speeds up your tests
by eliminating redundant setup of Django test fixtures. To use it...

1. Subclass ``django_nose.FastFixtureTestCase`` instead of
   ``django.test.TestCase``. (I like to import it ``as TestCase`` in my
   project's ``tests/__init__.py`` and then import it from there into my actual
   tests. Then it's easy to sub the base class in and out.) This alone will
   cause fixtures to load once per class rather than once per test.
2. Activate fixture bundling by passing the ``--with-fixture-bundling`` option
   to ``./manage.py test``. This loads each unique set of fixtures only once,
   even across class, module, and app boundaries.

How Fixture Bundling Works
~~~~~~~~~~~~~~~~~~~~~~~~~~

The fixture bundler reorders your test classes so that ones with identical sets
of fixtures run adjacently. It then advises the first of each series to load
the fixtures once for all of them (and the remaining ones not to bother). It
also advises the last to tear them down. Depending on the size and repetition
of your fixtures, you can expect a 25% to 50% speed increase.

Incidentally, the author prefers to avoid Django fixtures, as they encourage
irrelevant coupling between tests and make tests harder to comprehend and
modify. For future tests, it is better to use the "model maker" pattern,
creating DB objects programmatically. This way, tests avoid setup they don't
need, and there is a clearer tie between a test and the exact state it
requires. The fixture bundler is intended to make existing tests, which have
already committed to fixtures, more tolerable.

Troubleshooting
~~~~~~~~~~~~~~~

If using ``--with-fixture-bundling`` causes test failures, it likely indicates
an order dependency between some of your tests. Here are the most frequent
sources of state leakage we have encountered:

* Locale activation, which is maintained in a threadlocal variable. Be sure to
  reset your locale selection between tests.
* memcached contents. Be sure to flush between tests. Many test superclasses do
  this automatically.

It's also possible that you have ``post_save`` signal handlers which create
additional database rows while loading the fixtures. ``FastFixtureTestCase``
isn't yet smart enough to notice this and clean up after it, so you'll have to
go back to plain old ``TestCase`` for now.

Exempting A Class From Bundling
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In some unusual cases, it is desirable to exempt a test class from fixture
bundling, forcing it to set up and tear down its fixtures at the class
boundaries. For example, we might have a ``TestCase`` subclass which sets up
some state outside the DB in ``setUpClass`` and tears it down in
``tearDownClass``, and it might not be possible to adapt those routines to heed
the advice of the fixture bundler. In such a case, simply set the
``exempt_from_fixture_bundling`` attribute of the test class to ``True``.


Speedy Hygienic TransactionTestCases
------------------------------------

Unlike the stock Django test runner, django-nose lets you write custom
TransactionTestCase subclasses which expect to start with an unmarred DB,
saving an entire DB flush per test.

Background
~~~~~~~~~~

The default Django TransactionTestCase class `can leave the DB in an unclean
state`_ when it's done. To compensate, TransactionTestCase does a
time-consuming flush of the DB *before* each test to ensure it begins with a
clean slate. Django's stock test runner then runs TransactionTestCases last so
they don't wreck the environment for better-behaved tests. django-nose
replicates this behavior.

Escaping the Grime
~~~~~~~~~~~~~~~~~~

Some people, however, have made subclasses of TransactionTestCase that clean up
after themselves (and can do so efficiently, since they know what they've
changed). Like TestCase, these may assume they start with a clean DB. However,
any TransactionTestCases that run before them and leave a mess could cause them
to fail spuriously.

django-nose offers to fix this. If you include a special attribute on your
well-behaved TransactionTestCase... ::

    class MyNiceTestCase(TransactionTestCase):
        cleans_up_after_itself = True

...django-nose will run it before any of those nasty, trash-spewing test cases.
You can thus enjoy a big speed boost any time you make a TransactionTestCase
clean up after itself: skipping a whole DB flush before every test. With a
large schema, this can save minutes of IO.

django-nose's own FastFixtureTestCase uses this feature, even though it
ultimately acts more like a TestCase than a TransactionTestCase.

.. _can leave the DB in an unclean state: https://docs.djangoproject.com/en/1.4/topics/testing/#django.test.TransactionTestCase


Test-Only Models
----------------

If you have a model that is used only by tests (for example, to test an
abstract model base class), you can put it in any file that's imported in the
course of loading tests. For example, if the tests that need it are in
``test_models.py``, you can put the model in there, too. django-nose will make
sure its DB table gets created.


Assertions
----------

``django-nose.tools`` provides pep8 versions of Django's TestCase asserts
and some of its own as functions. ::

   assert_redirects(response, expected_url, status_code=302, target_status_code=200, host=None, msg_prefix='')

   assert_contains(response, text, count=None, status_code=200, msg_prefix='')
   assert_not_contains(response, text, count=None, status_code=200, msg_prefix='')

   assert_form_error(response, form, field, errors, msg_prefix='')

   assert_template_used(response, template_name, msg_prefix='')
   assert_template_not_used(response, template_name, msg_prefix='')

   assert_queryset_equal(qs, values, transform=repr)

   assert_num_queries(num, func=None, *args, **kwargs)

   assert_code(response, status_code, msg_prefix='')

   assert_ok(response, msg_prefix='')

   assert_mail_count(count, msg=None)


Using With South
----------------

`South`_ installs its own test command that turns off migrations during
testing. Make sure that django-nose comes *after* ``south`` in
``INSTALLED_APPS`` so that django_nose's test command is used.

.. _South: http://south.aeracode.org/


Always Passing The Same Options
-------------------------------

To always set the same command line options you can use a `nose.cfg or
setup.cfg`_ (as usual) or you can specify them in settings.py like this::

    NOSE_ARGS = ['--failed', '--stop']

.. _nose.cfg or setup.cfg: http://somethingaboutorange.com/mrl/projects/nose/0.11.2/usage.html#configuration


Custom Plugins
--------------

If you need to `make custom plugins`_, you can define each plugin class
somewhere within your app and load them from settings.py like this::

    NOSE_PLUGINS = [
        'yourapp.tests.plugins.SystematicDysfunctioner',
        # ...
    ]

Just like middleware or anything else, each string must be a dot-separated,
importable path to an actual class. Each plugin class will be instantiated and
added to the Nose test runner.

.. _make custom plugins: http://somethingaboutorange.com/mrl/projects/nose/0.11.2/plugins.html#writing-plugins


Older Versions of Django
------------------------
Upgrading from Django <= 1.3 to Django 1.4
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In versions of Django < 1.4 the project folder was in fact a python package as
well (note the __init__.py in your project root). In Django 1.4, there is no
such file and thus the project is not a python module.

**When you upgrade your Django project to the Django 1.4 layout, you need to
remove the __init__.py file in the root of your project (and move any python
files that reside there other than the manage.py) otherwise you will get a
`ImportError: No module named urls` exception.**

This happens because Nose will intelligently try to populate your sys.path, and
in this particular case includes your parent directory if your project has a
__init__.py file (see: https://github.com/nose-devs/nose/blob/release_1.1.2/nose/importer.py#L134).

This means that even though you have set up your directory structure properly and
set your `ROOT_URLCONF='my_project.urls'` to match the new structure, when running
django-nose's test runner it will try to find your urls.py file in `'my_project.my_project.urls'`.




Upgrading from Django < 1.2
~~~~~~~~~~~~~~~~~~~~~~~~~~~

Django 1.2 switches to a `class-based test runner`_. To use django-nose
with Django 1.2, change your ``TEST_RUNNER`` from ``django_nose.run_tests`` to
``django_nose.NoseTestSuiteRunner``.

``django_nose.run_tests`` will continue to work in Django 1.2 but will raise a
warning. In Django 1.3, it will stop working.

If you were using ``django_nose.run_gis_tests``, you should also switch to
``django_nose.NoseTestSuiteRunner`` and use one of the `spatial backends`_ in
your ``DATABASES`` settings.

.. _class-based test runner: http://docs.djangoproject.com/en/dev/releases/1.2/#function-based-test-runners
.. _spatial backends: http://docs.djangoproject.com/en/dev/ref/contrib/gis/db-api/#id1

Django 1.1
~~~~~~~~~~

If you want to use django-nose with Django 1.1, use
https://github.com/jbalogh/django-nose/tree/django-1.1 or
http://pypi.python.org/pypi/django-nose/0.0.3.

Django 1.0
~~~~~~~~~~

django-nose does not support Django 1.0.


Recent Version History
----------------------

1.1 (2012-05-19)
  * Django TransactionTestCases don't clean up after themselves; they leave
    junk in the DB and clean it up only on ``_pre_setup``. Thus, Django makes
    sure these tests run last. Now django-nose does, too. This means one fewer
    source of failures on existing projects. (Erik Rose)
  * Add support for hygienic TransactionTestCases. (Erik Rose)
  * Support models that are used only for tests. Just put them in any file
    imported in the course of loading tests. No more crazy hacks necessary.
    (Erik Rose)
  * Make the fixture bundler more conservative, fixing some conceivable
    situations in which fixtures would not appear as intended if a
    TransactionTestCase found its way into the middle of a bundle. (Erik Rose)
  * Fix an error that would surface when using SQLAlchemy with connection
    pooling. (Roger Hu)
  * Gracefully ignore the new ``--liveserver`` option introduced in Django 1.4;
    don't let it through to nose. (Adam DePue)

1.0 (2012-03-12)
  * New fixture-bundling plugin for avoiding needless fixture setup (Erik Rose)
  * Moved FastFixtureTestCase in from test-utils, so now all the
    fixture-bundling stuff is in one library. (Erik Rose)
  * Added the REUSE_DB setting for faster startup and shutdown. (Erik Rose)
  * Fixed a crash when printing options with certain verbosities. (Daniel Abel)
  * Broke hard dependency on MySQL. Support PostgreSQL. (Roger Hu)
  * Support SQLite, both memory- and disk-based. (Roger Hu and Erik Rose)
  * Nail down versions of the package requirements. (Daniel Mizyrycki)

0.1.3 (2010-04-15)
  * Even better coverage support (rozza)
  * README fixes (carljm and ionelmc)
  * optparse OptionGroups are handled better (outofculture)
  * nose plugins are loaded before listing options

See more in changelog.txt.

= Django OpenID Authentication Support =

This package provides integration between Django's authentication
system and OpenID authentication.  It also includes support for using
a fixed OpenID server endpoint, which can be useful when implementing
single signon systems.


== Basic Installation ==

 1. Install the Jan Rain Python OpenID library.  It can be found at:

        http://openidenabled.com/python-openid/

    It can also be found in most Linux distributions packaged as
    "python-openid".  You will need version 2.2.0 or later.

 2. Add 'django_openid_auth' to INSTALLED_APPS for your application.
    At a minimum, you'll need the following in there:

        INSTALLED_APPS = (
            'django.contrib.auth',
            'django.contrib.contenttypes',
            'django.contrib.sessions',
            'django_openid_auth',
        )

 3. Add 'django_auth_openid.auth.OpenIDBackend' to
    AUTHENTICATION_BACKENDS.  This should be in addition to the
    default ModelBackend:

        AUTHENTICATION_BACKENDS = (
            'django_openid_auth.auth.OpenIDBackend',
            'django.contrib.auth.backends.ModelBackend',
        )

 4. To create users automatically when a new OpenID is used, add the
    following to the settings:

        OPENID_CREATE_USERS = True

 5. To have user details updated from OpenID Simple Registration or
    Attribute Exchange extension data each time they log in, add the
    following:

        OPENID_UPDATE_DETAILS_FROM_SREG = True

 6. Hook up the login URLs to your application's urlconf with
    something like:

        urlpatterns = patterns('',
            ...
            (r'^openid/', include('django_openid_auth.urls')),
            ...
        )

 7. Configure the LOGIN_URL and LOGIN_REDIRECT_URL appropriately for
    your site:

        LOGIN_URL = '/openid/login/'
        LOGIN_REDIRECT_URL = '/'

    This will allow pages that use the standard @login_required
    decorator to use the OpenID login page.

 8. Rerun "python manage.py syncdb" to add the UserOpenID table to
    your database.


== Configuring Single Sign-On ==

If you only want to accept identities from a single OpenID server and
that server implemnts OpenID 2.0 identifier select mode, add the
following setting to your app:

    OPENID_SSO_SERVER_URL = 'server-endpoint-url'

With this setting enabled, the user will not be prompted to enter
their identity URL, and instead an OpenID authentication request will
be started with the given server URL.

As an example, to use Launchpad accounts for SSO, you'd use:

     OPENID_SSO_SERVER_URL = 'https://login.launchpad.net/'


== Launchpad Teams Support ==

This library supports the Launchpad Teams OpenID extension.  Using
this feature, it is possible to map Launchpad team memberships to
Django group memberships.  It can be configured with:

    OPENID_SSO_SERVER_URL = 'https://login.launchpad.net/'
    OPENID_LAUNCHPAD_TEAMS_MAPPING = {
        'launchpad-team-1': 'django-group-1',
        'launchpad-team-2': 'django-group-2',
        }

When a user logs in, they will be added or removed from the relevant
teams listed in the mapping.

If you have already django-groups and want to map these groups automatically, you can use the OPENID_LAUNCHPAD_TEAMS_MAPPING_AUTO variable in your settings.py file.

	OPENID_LAUNCHPAD_TEAMS_MAPPING_AUTO = True

If you use OPENID_LAUNCHPAD_TEAMS_MAPPING_AUTO, the variable OPENID_LAUNCHPAD_TEAMS_MAPPING will be ignored.
If you want to exclude some groups from the auto mapping, use OPENID_LAUNCHPAD_TEAMS_MAPPING_AUTO_BLACKLIST. This variable has only an effect if OPENID_LAUNCHPAD_TEAMS_MAPPING_AUTO is True.

	OPENID_LAUNCHPAD_TEAMS_MAPPING_AUTO_BLACKLIST = ['django-group1', 'django-group2']
	
== External redirect domains ==

By default, redirecting back to an external URL after auth is forbidden. To permit redirection to external URLs on a separate domain, define ALLOWED_EXTERNAL_OPENID_REDIRECT_DOMAINS in your settings.py file as a list of permitted domains:

	ALLOWED_EXTERNAL_OPENID_REDIRECT_DOMAINS = ['example.com', 'example.org']

and redirects to external URLs on those domains will additionally be permitted.

== Use as /admin (django.admin.contrib) login ==

If you require openid authentication into the admin application, add the following setting:

        OPENID_USE_AS_ADMIN_LOGIN = True

It is worth noting that a user needs to be be marked as a "staff user" to be able to access the admin interface.  A new openid user will not normally be a "staff user".  
The easiest way to resolve this is to use traditional authentication (OPENID_USE_AS_ADMIN_LOGIN = False) to sign in as your first user with a password and authorise your 
openid user to be staff.

== Change Django usernames if the nickname changes on the provider ==

If you want your Django username to change when a user updates the nickname on their provider, add the following setting:

        OPENID_FOLLOW_RENAMES = True

If the new nickname is available as a Django username, the user is renamed.
Otherwise the user will be renamed to nickname+i for an incrememnting value of i until no conflict occurs.
If the user has already been renamed to nickname+1 due to a conflict, and the nickname is still not available, the user will keep their existing username.

== Require a valid nickname ==

If you must have a valid, unique nickname in order to create a user accont, add the following setting:

        OPENID_STRICT_USERNAMES = True
        
This will cause an OpenID login attempt to fail if the provider does not return a 'nickname' (username) for the user, or if the nickname conflicts with an existing user with a different openid identiy url.
Without this setting, logins without a nickname will be given the username 'openiduser', and upon conflicts with existing username, an incrementing number will be appended to the username until it is unique.

== Require Physical Multi-Factor Authentication ==

If your users should use a physical multi-factor authentication method, such as RSA tokens or YubiKey, add the following setting:

        OPENID_PHYSICAL_MULTIFACTOR_REQUIRED = True
        
If the user's OpenID provider supports the PAPE extension and provides the Physical Multifactor authentication policy, this will
cause the OpenID login to fail if the user does not provide valid physical authentication to the provider.

== Override Login Failure Handling ==

You can optionally provide your own handler for login failures by adding the following setting:

        OPENID_RENDER_FAILURE = failure_handler_function

Where failure_handler_function is a function reference that will take the following parameters:

        def failure_handler_function(request, message, status=None, template_name=None, exception=None)

This function must return a Django.http.HttpResponse instance.

== Use the user's email for suggested usernames ==

You can optionally strip out non-alphanumeric characters from the user's email
to generate a preferred username, if the server doesn't provide nick
information, by setting the following setting:

        OPENID_USE_EMAIL_FOR_USERNAME = True

Otherwise, and by default, if the server omits nick information and a user is
created it'll receive a username 'openiduser' + a number.
Consider also the OPENID_STRICT_USERNAMES setting (see ``Require a valid nickname``)

$Id: README 2326 2005-03-17 07:45:21Z fredrik $

=======================
The elementtree library
=======================

This kit contains the ElementTree library, a light-weight toolkit for
XML processing in Python.

For more information on this library, see:

    docs/index.html
    http://effbot.org/zone/element.htm

The modules are designed to work with Python 2.1 and newer.  The core
ElementTree module and the SimpleXMLTreeBuilder class also works under
1.5.2 and 2.0.

Enjoy /F

fredrik@pythonware.com
http://www.pythonware.com

--------------------------------------------------------------------
The ElementTree Library is

Copyright (c) 1999-2005 by Secret Labs AB
Copyright (c) 1999-2005 by Fredrik Lundh

By obtaining, using, and/or copying this software and/or its
associated documentation, you agree that you have read, understood,
and will comply with the following terms and conditions:

Permission to use, copy, modify, and distribute this software and its
associated documentation for any purpose and without fee is hereby
granted, provided that the above copyright notice appears in all
copies, and that both that copyright notice and this permission notice
appear in supporting documentation, and that the name of Secret Labs
AB or the author not be used in advertising or publicity pertaining to
distribution of the software without specific, written prior
permission.

SECRET LABS AB AND THE AUTHOR DISCLAIMS ALL WARRANTIES WITH REGARD TO
THIS SOFTWARE, INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND
FITNESS.  IN NO EVENT SHALL SECRET LABS AB OR THE AUTHOR BE LIABLE FOR
ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT
OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
--------------------------------------------------------------------

release info
------------

This is release 1.2.6 of the ElementTree library.

For a list of changes in this release, see the CHANGES document.

The latest version of this library can be downloaded from:

    http://effbot.org/downloads

Comments, bug reports, and patches are welcome.  Send them to
fredrik@pythonware.com.

Note that this is free software, with limited support.  If you need
commercial support on this module, contact fredrik@pythonware.com.

--------------------------------------------------------------------
contents
--------------------------------------------------------------------

README                        This file

CHANGES                       List of changes in this release.

elementtree/

  ElementTree.py              Element tree implementation.  For a minimal
                              install, this file is all you need.

  ElementPath.py              Element path support module.  Adds limited
                              XPath support to find/findtext/findall.

  ElementInclude.py           Element include support module.  Adds limited
                              XInclude support.

  HTMLTreeBuilder.py          Element tree builder for HTML.  This only
                              works for mostly well-formed HTML; if you
                              need something that can parse arbitrary
                              HTML at least as good as your browser, use
                              TidyHTMLTreeBuilder or TidyTools (see below).

  XMLTreeBuilder.py           Element tree builder for XML (compatibility;
                              new code should use the tree builder in the
                              ElementTree module).

  TidyHTMLTreeBuilder.py      Element tree builder for HTML, based on the
                              tidylib library.  This tree builder requires
                              the _elementtidy extension module (available
                              from http://effbot.org/downloads).

  SimpleXMLTreeBuilder.py     Old element tree builder for XML, based on
                              xmllib, for Python versions where "expat" is
                              not available.   Due to bugs in xmllib, the
                              namespace support is not reliable (run the
                              module as a script to find out exactly how
                              unreliable it is on your Python version...)

  SgmlopXMLTreeBuilder.py     Simple element tree builder based on the
                              SGMLOP parser.  Note: The current version
                              does not support namespaces.

  SimpleXMLWriter.py          Simple XML writer

  TidyTools.py                Build element trees from HTML, using the
                              external 'tidy' utility.

setup.py                      Build/installation script

docs/index.html		      API reference pages.
docs/*

demo*.py                      Sample scripts
samples/*                     Sample data

selftest.py                   Selftest (requires Python 2.1 or later)
tidytest.py                   Selftest for TidyHTMLTreeBuilder components.

benchmark.py                  Benchmark script (usage: benchmark.py file)

This is Guppy-PE version 0.1.10

A Python Programming Environment.

CONTENTS

This distribution provides a root package, guppy, with the following
subpackages:

doc	Documentation data files: *.html, *.jpg, and help support.

etc	Support modules. Contains especially the Glue protocol module.

gsl	The Guppy Specification Language implementation. This can
	be used to create documents and tests from a common source.

heapy	The heap analysis toolset. It can be used to find information
	about the objects in the heap and display the information
	in various ways.

sets	Bitsets and 'nodesets' implemented in C.

The following files are not in packages but on the toplevel:

gsl-mode-0.1.el		Emacs mode for GSL
specs/*.gsl		Specifications
specs/genguppydoc.py	Regenerates the doc/*.html files from specs/*.gsl

REQUIREMENTS

You should have Python 2.3, 2.4, 2.5 or 2.6

To build the system so you can use Heapy, you should have what is
needed to build extension modules from source code. The extension
modules are used not only in Heapy but also in GSL in some
situations. (The test generator uses bitsets to speed up an
algorithm.)

To use the graphical browser, Tkinter is needed.
To use the remote monitor, threading must be available.

INSTALLATION

Extract the files from the .tar.gz file, in Linux for example by:

tar xzf guppy-x.y.z.tar.gz

where x.y.z is the current version number. This will create the
directory guppy-x.y.z .  You should then cd into this directory.  You
can then compile the extension modules by:

python setup.py build

And to install you can do, for example, as follows to install to the
default location (when you are super-user):

python setup.py install

NOTE that you typically must LEAVE the guppy-x.y.z directory to use
Heapy, since the current directory is usually in the Python search
path, and then Python will find the guppy directory there FIRST and
will NOT find the extension modules. This may be a somewhat common
problem -- so I am noting it here for lack of a suitable fix at the
moment.

I also note that the documentation files and emacs-mode are not
automatically installed.

HEAPY USAGE EXAMPLE

The following example shows

1. How to create the session context: h=hpy()
2. How to show the reachable objects in the heap: h.heap()
3. How to create and show a set of objects: h.iso(1,[],{})
4. How to show the shortest paths from the root to x: h.iso(x).sp

>>> from guppy import hpy; h=hpy()
>>> h.heap()
Partition of a set of 48477 objects. Total size = 3265516 bytes.
 Index  Count   %     Size   % Cumulative  % Kind (class / dict of class)
     0  25773  53  1612820  49   1612820  49 str
     1  11699  24   483960  15   2096780  64 tuple
     2    174   0   241584   7   2338364  72 dict of module
     3   3478   7   222592   7   2560956  78 types.CodeType
     4   3296   7   184576   6   2745532  84 function
     5    401   1   175112   5   2920644  89 dict of class
     6    108   0    81888   3   3002532  92 dict (no owner)
     7    114   0    79632   2   3082164  94 dict of type
     8    117   0    51336   2   3133500  96 type
     9    667   1    24012   1   3157512  97 __builtin__.wrapper_descriptor
<76 more rows. Type e.g. '_.more' to view.>
>>> h.iso(1,[],{})
Partition of a set of 3 objects. Total size = 176 bytes.
 Index  Count   %     Size   % Cumulative  % Kind (class / dict of class)
     0      1  33      136  77       136  77 dict (no owner)
     1      1  33       28  16       164  93 list
     2      1  33       12   7       176 100 int
>>> x=[]
>>> h.iso(x).sp
 0: h.Root.i0_modules['__main__'].__dict__['x']
>>> 

TEST

To test if the heapy build and installation was ok, you can do:

>>> from guppy import hpy
>>> hpy().test()
Testing sets
Test #0
Test #1
Test #2
...

There will be several more tests. Some tests may take a while.

Caveat 1: Some tests may be somewhat picky, and may have to be relaxed
to pass in different installations.

Caveat 2: All tests don't currently work always in Python 2.7

LICENSE

Copyright (C) 2005 -- 2008 Sverker Nilsson, S. Nilsson Computer System AB

The right is granted to copy, use, modify and redistribute this code
according to the rules in what is commonly referred to as an MIT
license.

*** USE AT YOUR OWN RISK AND BE AWARE THAT THIS IS AN EARLY RELEASE ***

CONTACT INFORMATION

Sverker Nilsson <sn@sncs.se> (Homepage: http://sncs.se)
Guppy-PE homepage: http://guppy-pe.sourceforge.net

httplib2 for Python 3

This directory contains a port of httplib2 to Python 3. As you may
know, Python 3 is not backward-compatible with Python 2. The biggest
change in Python 3 (that affects httplib2) is the distinction between
bytes and strings.

To successfully use http2lib for Python 3, you absolutely must
understand the following sentence:

** THE RESPONSE HEADERS ARE STRINGS, BUT THE CONTENT BODY IS BYTES **


Example:

>>> import httplib2, pprint
>>> h = httplib2.Http(".cache")
>>> (resp_headers, content) = h.request("http://example.org/", "GET")
>>> pprint.pprint(resp_headers)
{'accept-ranges': 'bytes',
 'connection': 'close',
 'content-length': '438',
 'content-location': 'http://example.org/',
 'content-type': 'text/html; charset=UTF-8',
 'date': 'Fri, 29 May 2009 03:57:29 GMT',
 'etag': '"b80f4-1b6-80bfd280"',
 'last-modified': 'Tue, 15 Nov 2005 13:24:10 GMT',
 'server': 'Apache/2.2.3 (CentOS)',
 'status': '200'}
>>> type(content)
<class 'bytes'>
>>> content[:49]
b'<HTML>\r\n<HEAD>\r\n  <TITLE>Example Web Page</TITLE>'


Further reading:

  * http://diveintopython3.org/strings.html
  * http://docs.python.org/3.0/whatsnew/3.0.html#text-vs-data-instead-of-unicode-vs-8-bit
  * http://docs.python.org/3.0/howto/unicode.html


--------------------------------------------------------------------
Httplib2 Software License

Copyright (c) 2006 by Joe Gregorio
Copyright (c) 2009 by Mark Pilgrim

Permission is hereby granted, free of charge, to any person 
obtaining a copy of this software and associated documentation 
files (the "Software"), to deal in the Software without restriction, 
including without limitation the rights to use, copy, modify, merge, 
publish, distribute, sublicense, and/or sell copies of the Software, 
and to permit persons to whom the Software is furnished to do so, 
subject to the following conditions:

The above copyright notice and this permission notice shall be 
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, 
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES 
OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND 
NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS 
BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN 
ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN 
CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE 
SOFTWARE.


Httplib2

--------------------------------------------------------------------
Introduction

A comprehensive HTTP client library, httplib2.py supports many
features left out of other HTTP libraries.

HTTP and HTTPS
    HTTPS support is only available if the socket module was
    compiled with SSL support.
Keep-Alive
    Supports HTTP 1.1 Keep-Alive, keeping the socket open and
    performing multiple requests over the same connection if
    possible.
Authentication
    The following three types of HTTP Authentication are
    supported. These can be used over both HTTP and HTTPS.

        * Digest
        * Basic
        * WSSE

Caching
    The module can optionally operate with a private cache that
    understands the Cache-Control: header and uses both the ETag
    and Last-Modified cache validators.
All Methods
    The module can handle any HTTP request method, not just GET
    and POST.
Redirects
    Automatically follows 3XX redirects on GETs.
Compression
    Handles both 'deflate' and 'gzip' types of compression.
Lost update support
    Automatically adds back ETags into PUT requests to resources
    we have already cached. This implements Section 3.2 of
    Detecting the Lost Update Problem Using Unreserved Checkout.
Unit Tested
    A large and growing set of unit tests.


For more information on this module, see:

    http://bitworking.org/projects/httplib2/


--------------------------------------------------------------------
Installation

The httplib2 module is shipped as a distutils package.  To install
the library, unpack the distribution archive, and issue the following
command:

    $ python setup.py install


--------------------------------------------------------------------
Usage
A simple retrieval:

  import httplib2
  h = httplib2.Http(".cache")
  (resp_headers, content) = h.request("http://example.org/", "GET")

The 'content' is the content retrieved from the URL. The content
is already decompressed or unzipped if necessary.

To PUT some content to a server that uses SSL and Basic authentication:

  import httplib2
  h = httplib2.Http(".cache")
  h.add_credentials('name', 'password')
  (resp, content) = h.request("https://example.org/chapter/2",
                            "PUT", body="This is text",
                            headers={'content-type':'text/plain'} )

Use the Cache-Control: header to control how the caching operates.

  import httplib2
  h = httplib2.Http(".cache")
  (resp, content) = h.request("http://bitworking.org/", "GET")
  ...
  (resp, content) = h.request("http://bitworking.org/", "GET",
                            headers={'cache-control':'no-cache'})

The first request will be cached and since this is a request
to bitworking.org it will be set to be cached for two hours,
because that is how I have my server configured. Any subsequent
GET to that URI will return the value from the on-disk cache
and no request will be made to the server. You can use the
Cache-Control: header to change the caches behavior and in
this example the second request adds the Cache-Control:
header with a value of 'no-cache' which tells the library
that the cached copy must not be used when handling this request.


--------------------------------------------------------------------
Httplib2 Software License

Copyright (c) 2006 by Joe Gregorio

Permission is hereby granted, free of charge, to any person
obtaining a copy of this software and associated documentation
files (the "Software"), to deal in the Software without restriction,
including without limitation the rights to use, copy, modify, merge,
publish, distribute, sublicense, and/or sell copies of the Software,
and to permit persons to whom the Software is furnished to do so,
subject to the following conditions:

The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES
OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.


=========================================================
PyKerberos Package

Copyright (c) 2006-2008 Apple Inc. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

=========================================================

This Python package is a high-level wrapper for Kerberos (GSSAPI) operations.
The goal is to avoid having to build a module that wraps the entire Kerberos.framework,
and instead offer a limited set of functions that do what is needed for client/server
Kerberos authentication based on <http://www.ietf.org/rfc/rfc4559.txt>.

Much of the C-code here is adapted from Apache's mod_auth_kerb-5.0rc7.

========
CONTENTS
========

    src/               : directory in which C source code resides.
    setup.py           : Python distutils extension build script.
    config/            : directory of useful Kerberos config files.
      edu.mit.Kerberos : example Kerberos .ini file.
    README.txt         : this file!
    kerberos.py        : Python api documentation/stub implementation.

=====
BUILD
=====

In this directory, run:

    python setup.py build

=======
TESTING
=======

You must have a valid Kerberos setup on the test machine and you should ensure that you have valid
Kerberos tickets for any client authentication being done (run 'klist' on the command line).
Additionally, for the server: it must have been configured as a valid Kerberos service with the Kerbersos server
for its realm - this usually requires running kadmin on the server machine to add the principal and generate a keytab
entry for it (run 'sudo klist -k' to see the currently available keytab entries).

Make sure that PYTHONPATH includes the appropriate build/lib.xxxx directory.
Then run test.py with suitable command line arguments:

    python test.py -u userid -p password -s service
    
    -u : user id for basic authenticate
    -p : password for basic authenticate
    -s : service principal for GSSAPI authentication (defaults to 'http@host.example.com')

===========
Python APIs
===========

See kerberos.py.

The lockfile module exports a FileLock class which provides a simple API for
locking files.  Unlike the Windows msvcrt.locking function, the fcntl.lockf
and flock functions, and the deprecated posixfile module, the API is
identical across both Unix (including Linux and Mac) and Windows platforms.
The lock mechanism relies on the atomic nature of the link (on Unix) and
mkdir (on Windows) system calls.  An implementation based on SQLite is also
provided, more as a demonstration of the possibilities it provides than as
production-quality code.

To install:

    python setup.py install

See doc/main.txt and doc/intro.txt

For installation information, see INSTALL.txt

=========================
Mako Templates for Python
=========================

Mako is a template library written in Python. It provides a familiar, non-XML 
syntax which compiles into Python modules for maximum performance. Mako's 
syntax and API borrows from the best ideas of many others, including Django
templates, Cheetah, Myghty, and Genshi. Conceptually, Mako is an embedded 
Python (i.e. Python Server Page) language, which refines the familiar ideas
of componentized layout and inheritance to produce one of the most 
straightforward and flexible models available, while also maintaining close 
ties to Python calling and scoping semantics.

Nutshell
========

::

    <%inherit file="base.html"/>
    <%
        rows = [[v for v in range(0,10)] for row in range(0,10)]
    %>
    <table>
        % for row in rows:
            ${makerow(row)}
        % endfor
    </table>

    <%def name="makerow(row)">
        <tr>
        % for name in row:
            <td>${name}</td>\
        % endfor
        </tr>
    </%def>

Philosophy
===========

Python is a great scripting language. Don't reinvent the wheel...your templates can handle it !

Documentation
==============

See documentation for Mako at http://www.makotemplates.org/docs/

License
========

Mako is licensed under an MIT-style license (see LICENSE).
Other incorporated projects may be licensed under different licenses.
All licenses allow for non-commercial and commercial use.

[Python-Markdown][]
===================

This is a Python implementation of John Gruber's [Markdown][]. 
It is almost completely compliant with the reference implementation,
though there are a few known issues. See [Features][] for information 
on what exactly is supported and what is not. Additional features are 
supported by the [Available Extensions][].

[Python-Markdown]: http://freewisdom.org/projects/python-markdown
[Markdown]: http://daringfireball.net/projects/markdown/
[Features]: http://www.freewisdom.org/projects/python-markdown/Features
[Available Extensions]: http://www.freewisdom.org/projects/python-markdown/Available_Extensions


Documentation
-------------

Installation and usage documentation is available in the `docs/` directory
of the distribution and on the project website at 
<http://freewisdom.org/projects/python-markdown>.

Support
-------

You may ask for help and discuss various other issues on the [mailing list][] and report bugs on the [bug tracker][].

[mailing list]: http://lists.sourceforge.net/lists/listinfo/python-markdown-discuss
[bug tracker]: http://www.freewisdom.org/projects/python-markdown/Tickets 


<h1><a href="http://freewisdom.org/projects/python-markdown">Python-Markdown</a></h1>
<p>This is a Python implementation of John Gruber's <a href="http://daringfireball.net/projects/markdown/">Markdown</a>. 
It is almost completely compliant with the reference implementation,
though there are a few known issues. See <a href="http://www.freewisdom.org/projects/python-markdown/Features">Features</a> for information 
on what exactly is supported and what is not. Additional features are 
supported by the <a href="http://www.freewisdom.org/projects/python-markdown/Available_Extensions">Available Extensions</a>.</p>
<h2>Documentation</h2>
<p>Installation and usage documentation is available in the <code>docs/</code> directory
of the distribution and on the project website at 
<a href="http://freewisdom.org/projects/python-markdown">http://freewisdom.org/projects/python-markdown</a>.</p>
<h2>Support</h2>
<p>You may ask for help and discuss various other issues on the <a href="http://lists.sourceforge.net/lists/listinfo/python-markdown-discuss">mailing list</a> and report bugs on the <a href="http://www.freewisdom.org/projects/python-markdown/Tickets">bug tracker</a>.</p>
MarkdownTest_1.0_2007-05-09 updated for the new version of tidy.

MarkupSafe
==========

Implements a unicode subclass that supports HTML strings:

>>> from markupsafe import Markup, escape
>>> escape("<script>alert(document.cookie);</script>")
Markup(u'&lt;script&gt;alert(document.cookie);&lt;/script&gt;')
>>> tmpl = Markup("<em>%s</em>")
>>> tmpl % "Peter > Lustig"
Markup(u'<em>Peter &gt; Lustig</em>')

If you want to make an object unicode that is not yet unicode
but don't want to lose the taint information, you can use the
`soft_unicode` function:

>>> from markupsafe import soft_unicode
>>> soft_unicode(42)
u'42'
>>> soft_unicode(Markup('foo'))
Markup(u'foo')

Objects can customize their HTML markup equivalent by overriding
the `__html__` function:

>>> class Foo(object):
...  def __html__(self):
...   return '<strong>Nice</strong>'
...
>>> escape(Foo())
Markup(u'<strong>Nice</strong>')
>>> Markup(Foo())
Markup(u'<strong>Nice</strong>')

====================
MySQLdb Installation
====================

.. contents::
..

Prerequisites
-------------

+ Python 2.3.4 or higher

  * http://www.python.org/

  * Versions lower than 2.3 WON'T WORK.

  * 2.4 is the primary test environment.
  
  * Red Hat Linux:

    - Make sure you have the Python development headers and libraries
      (python-devel).

+ setuptools

  * http://pypi.python.org/pypi/setuptools

+ MySQL 3.23.32 or higher

  * http://www.mysql.com/downloads/

  * Versions lower than 3.22 definitely WON'T WORK.

  * Versions lower than 3.22.19 might not work.

  * MySQL-3.22 might work but isn't supported anymore. It's very old.

  * MySQL-3.23 ought to work, but it's pretty elderly.

  * MySQL-4.0 is supported, but not tested and slightly discouraged.

  * MySQL-4.1 is supported. The prepared statements API is not
    supported, and won't be until MySQLdb-1.3 or 2.0, if ever.

  * MySQL-5.0 is supported and tested, including stored procedures.
      
  * MySQL-5.1 is supported (currently a release candidate) but untested.
    It should work.

  * MySQL-6.0 is sorta-kinda-supported (currently alpha) but untested.
    It should work.

  * Drizzle <https://launchpad.net/drizzle> is a fork of MySQL. So far
    the C API looks really similar except everything is renamed.
    Drizzle support probably won't happen in 1.2. There may be have to
    be an entirely different module, but still using DB-API.

  * MaxDB, formerly known as SAP DB (and maybe Adabas D?), is a
    completely different animal. Use the sapdb.sql module that comes
    with MaxDB.

  * Red Hat Linux packages:

    - mysql-devel to compile

    - mysql and/or mysql-devel to run

  * MySQL.com RPM packages:

    - MySQL-devel to compile

    - MySQL-shared if you want to use their shared
      library. Otherwise you'll get a statically-linked module,
      which may or may not be what you want.

    - MySQL-shared to run if you compiled with MySQL-shared installed

  * Transactions (particularly InnoDB tables) are supported for
    MySQL-3.23 and up. You may need a special package from your vendor
    with this support turned on.

+  zlib

   * Required for MySQL-3.23 and newer.

   * Red Hat Linux

     - zlib-devel to compile

     - zlib to run

+ openssl

  * May be needed for MySQL-4.0 or newer, depending on compilation
    options. If you need it, you probably already have it.
    
    - you may need openssl-devel on some platforms

+ C compiler

  * Most free software-based systems already have this, usually gcc.

  * Most commercial UNIX platforms also come with a C compiler, or
    you can also use gcc.

  * If you have some Windows flavor, you usually have to pay extra
    for this, or you can use Cygwin_.

.. _Cygwin: http://www.cygwin.com/


Building and installing
-----------------------

The setup.py script uses mysql_config to find all compiler and linker
options, and should work as is on any POSIX-like platform, so long as
mysql_config is in your path.

Depending on which version of MySQL you have, you may have the option
of using three different client libraries. To select the client library,
edit the [options] section of site.cfg:

    embedded
        use embedded server library (libmysqld) if True; otherwise use
	one of the client libraries (default).

    threadsafe
        thread-safe client library (libmysqlclient_r) if True (default);
	otherwise use non-thread-safe (libmysqlclient). You should
	always use the thread-safe library if you have the option;
	otherwise you *may* have problems.

    static
        if True, try to link against a static library; otherwise link
	against dynamic libraries (default). You may need static linking
	to use the embedded server.
	

Finally, putting it together::

  $ tar xfz MySQL-python-1.2.1.tar.gz
  $ cd MySQL-python-1.2.1
  $ # edit site.cfg if necessary
  $ python setup.py build
  $ sudo python setup.py install # or su first


Windows
.......

I don't do Windows. However if someone provides me with a package for
Windows, I'll make it available. Don't ask me for help with Windows
because I can't help you.

Generally, though, running setup.py is similar to above::

  C:\...> python setup.py install
  C:\...> python setup.py bdist_wininst

The latter example should build a Windows installer package, if you
have the correct tools. In any event, you *must* have a C compiler.
Additionally, you have to set an environment variable (mysqlroot)
which is the path to your MySQL installation. In theory, it would be
possible to get this information out of the registry, but like I said,
I don't do Windows, but I'll accept a patch that does this.

On Windows, you will definitely have to edit site.cfg since there is
no mysql_config in the MySQL package.


Zope
....

If you are using a binary package of Zope, you need run setup.py with
the python executable that came with Zope. Otherwise, you'll install
into the wrong Python tree and Zope (ZMySQLDA) will not be able to
find _mysql.


Binary Packages
---------------

I don't plan to make binary packages any more. However, if someone
contributes one, I will make it available. Several OS vendors have
their own packages available.


RPMs
....
    
If you prefer to install RPMs, you can use the bdist_rpm command with
setup.py. This only builds the RPM; it does not install it. You may
want to use the --python=XXX option, where XXX is the name of the
Python executable, i.e. python, python2, python2.4; the default is
python. Using this will incorporate the Python executable name into
the package name for the RPM so you have install the package multiple
times if you need to support more than one version of Python. You can
also set this in setup.cfg.


Red Hat Linux
.............

MySQL-python is pre-packaged in Red Hat Linux 7.x and newer. This
includes Fedora Core and Red Hat Enterprise Linux. You can also
build your own RPM packages as described above.


Debian GNU/Linux
................

Packaged as `python-mysqldb`_::

	# apt-get install python-mysqldb

Or use Synaptic.

.. _`python-mysqldb`: http://packages.debian.org/python-mysqldb


Ubuntu
......

Same as with Debian.


Gentoo Linux
............

Packaged as `mysql-python`_. ::

      # emerge sync
      # emerge mysql-python
      # emerge zmysqlda # if you use Zope

.. _`mysql-python`: http://packages.gentoo.org/search/?sstring=mysql-python


BSD
...

MySQL-python is a ported package in FreeBSD, NetBSD, and OpenBSD,
although the name may vary to match OS conventions.


License
-------

GPL or the original license based on Python 1.5.2's license.


:Author: Andy Dustman <andy@dustman.net>
:Revision: $Id: README 557 2008-10-17 01:07:01Z adustman $

# parquet-python

parquet-python is a pure-python implementation (currently with only read-support) of the [parquet format](https://github.com/Parquet/parquet-format). It comes with a script for reading parquet files and outputting the data to stdout as JSON or TSV (without the overhead of JVM startup). Performance has not yet been optimized, but it's useful for debugging and quick viewing of data in files.

Not all parts of the parquet-format have been implemented yet or tested e.g. nested data and the deprecated bit-packing encoding -- see Todos below for a full list. With that said, parquet-python is capable of reading all the data files from the [parquet-compatability](https://github.com/Parquet/parquet-compatibility) project.


# requirements

parquet-python has been tested on python 2.7. It depends on `thrift` (0.9) and `python-snappy` (for snappy compressed files).


# getting started

parquet-python is not yet uploaded to PyPi as the code has a lot of bugs. To get started, clone the project, change into the parquet-python directory, and run `python -m parquet`.

You may need to install the `thrift` and `python-snappy` projects with `easy_install` or `pip`. To install parquet-python system-wide, run `python setup.py install`.


# Todos

* Support the deprecated bitpacking
* Support for bitwidths > 8
* Fix handling of repetition-levels and definition-levels
* Tests for nested schemas, null data
* Support reading of data from HDFS via snakebite and/or webhdfs.
* Implement writing
* performance evaluation and optimization (i.e. how does it compare to the c++, java implementations)


# Contributing

Is done via Pull Requests. Please include tests with your changes and follow [pep8](http://www.python.org/dev/peps/pep-0008/).

This archive contains the standard Python documentation in GNU info
format.  Five manuals are included:

    python-ref.info*	Python Reference Manual
    python-mac.info*	Python Macintosh Modules
    python-lib.info*	Python Library Reference
    python-ext.info*	Extending and Embedding the Python Interpreter
    python-api.info*	Python/C API Reference
    python-tut.info*	Python Tutorial

The file python.dir is a fragment of a "dir" file that can be used to
incorporate these documents into an existing GNU info installation:
insert the contents of this file into the "dir" or "localdir" file at
an appropriate point and copy the python-*.info* files to the same
directory.

Thanks go to Milan Zamazal <pdm@freesoft.cz> for providing this
conversion to the info format.

Questions and comments on these documents should be directed to
python-docs@python.org.

These scripts and Makefile fragment are used to convert the Python
documentation in LaTeX format to XML.

This material is preliminary and incomplete.  Python 2.0 is required.

To convert all documents to XML:

	cd Doc/
	make -f tools/sgmlconv/Makefile

To convert one document to XML:

	cd Doc/<document-dir>
	make -f ../tools/sgmlconv/make.rules TOOLSDIR=../tools

Please send comments and bug reports to python-docs@python.org.


What do the tools do?
---------------------

latex2esis.py
    Reads in a conversion specification written in XML
    (conversion.xml), reads a LaTeX document fragment, and interprets
    the markup according to the specification.  The output is a stream
    of ESIS events like those created by the nsgmls SGML parser, but
    is *not* guaranteed to represent a single tree!  This is done to
    allow conversion per entity rather than per document.  Since many
    of the LaTeX files for the Python documentation contain two
    sections on closely related modules, it is important to allow both
    of the resulting <section> elements to exist in the same output
    stream.  Additionally, since comments are not supported in ESIS,
    comments are converted to <COMMENT> elements, which might exist at
    the same level as the top-level content elements.

    The output of latex2esis.py gets saved as <filename>.esis1.

docfixer.py
    This is the really painful part of the conversion.  Well, it's the 
    second really painful part, but more of the pain is specific to
    the structure of the Python documentation and desired output
    rather than to the parsing of LaTeX markup.

    This script loads the ESIS data created by latex2esis.py into a
    DOM document *fragment* (remember, the latex2esis.py output may
    not be well-formed).  Once loaded, it walks over the tree many
    times looking for a variety of possible specific
    micro-conversions.  Most of the code is not in any way "general".
    After processing the fragment, a new ESIS data stream is written
    out.  Like the input, it may not represent a well-formed
    document, but does represent a parsed entity.

    The output of docfixer.py is what gets saved in <filename>.esis.

esis2sgml.py
    Reads an ESIS stream and convert to SGML or XML.  This also
    converts <COMMENT> elements to real comments.  This works quickly
    because there's not much to actually do.

I've finally gotten around to writing some examples :-)

They aren't many, but at least it's something. If you write any, feel free to
send them to me and I will add themn.


certgen.py - Certificate generation module
==========================================

Example module with three functions:
  createKeyPair     - Create a public/private key pair
  createCertRequest - Create a certificate request
  createCertificate - Create a certificate given a cert request
In fact, I created the certificates and keys in the 'simple' directory with
the script mk_simple_certs.py


simple - Simple client/server example
=====================================

Start the server with
    python server.py PORT
and start clients with
    python client.py HOST PORT

The server is a simple echo server, anything a client sends, it sends back.


proxy.py - Example of an SSL-enabled proxy
==========================================

The proxy example demonstrate how to use set_connect_state to start
talking SSL over an already connected socket.

Usage: python proxy.py server[:port] proxy[:port]

Contributed by Mihai Ibanescu


SecureXMLRPCServer.py - SSL-enabled version of SimpleXMLRPCServer
=================================================================

This acts exactly like SimpleXMLRPCServer from the standard python library,
but uses secure connections. The technique and classes should work for any
SocketServer style server. However, the code has not been extensively tested.

Contributed by Michal Wallace


To use this example, first generate keys and certificates for both the
client and the server.  You can do this with the script in the directory
above this one, mk_simple_certs.py.

This directory contains client and server examples for the "Server Name
Indication" (SNI) feature.

Run server.py with no arguments.  It will accept one client connection and
then exit.  It has two certificates it can use, one for "example.invalid"
and another for "another.invalid".  If a client indicates one of these names
to it, it will use the corresponding certificate for that connection (if a
client doesn't indicate a name or indicates another name, it won't try to
use any certificate).

Run client.py with one argument, the server name to indicate.  For example:

  $ python client.py example.invalid
  Connecting... connected ('127.0.0.1', 8443)
  Server subject is <X509Name object '/OU=Security/O=pyOpenSSL/CN=example.invalid/ST=New York/C=US/emailAddress=invalid@example.invalid/L=New York'>
  $

Depending on what hostname is supplied, the server will select a different
certificate to use and the client output will be different.


pyOpenSSL - A Python wrapper around the OpenSSL library
------------------------------------------------------------------------------

See the file INSTALL for installation instructions.

I appreciate bug reports and patches.  Please visit
<http://bugs.launchpad.net/pyopenssl>.

## This file is in the moin format. The latest version is found
## at https://moin.conectiva.com.br/DateUtil

== Contents ==
[[TableOfContents]]

== Description ==
The '''dateutil''' module provides powerful extensions to
the standard '''datetime''' module, available in Python 2.3+.

== Features ==

  * Computing of relative deltas (next month, next year,
  next monday, last week of month, etc);

  * Computing of relative deltas between two given
  date and/or datetime objects;

  * Computing of dates based on very flexible recurrence rules,
  using a superset of the
  [ftp://ftp.rfc-editor.org/in-notes/rfc2445.txt iCalendar]
  specification. Parsing of RFC strings is supported as well.

  * Generic parsing of dates in almost any string format;

  * Timezone (tzinfo) implementations for tzfile(5) format
  files (/etc/localtime, /usr/share/zoneinfo, etc), TZ
  environment string (in all known formats), iCalendar
  format files, given ranges (with help from relative deltas),
  local machine timezone, fixed offset timezone, UTC timezone,
  and Windows registry-based time zones.

  * Internal up-to-date world timezone information based on
  Olson's database.

  * Computing of Easter Sunday dates for any given year,
  using Western, Orthodox or Julian algorithms;

  * More than 400 test cases.

== Quick example ==
Here's a snapshot, just to give an idea about the power of the
package. For more examples, look at the documentation below.

Suppose you want to know how much time is left, in
years/months/days/etc, before the next easter happening on a
year with a Friday 13th in August, and you want to get today's
date out of the "date" unix system command. Here is the code:
{{{
from dateutil.relativedelta import *
from dateutil.easter import *
from dateutil.rrule import *
from dateutil.parser import *
from datetime import *
import commands
import os
now = parse(commands.getoutput("date"))
today = now.date()
year = rrule(YEARLY,bymonth=8,bymonthday=13,byweekday=FR)[0].year
rdelta = relativedelta(easter(year), today)
print "Today is:", today
print "Year with next Aug 13th on a Friday is:", year
print "How far is the Easter of that year:", rdelta
print "And the Easter of that year is:", today+rdelta
}}}

And here's the output:
{{{
Today is: 2003-10-11
Year with next Aug 13th on a Friday is: 2004
How far is the Easter of that year: relativedelta(months=+6)
And the Easter of that year is: 2004-04-11
}}}

{i} Being exactly 6 months ahead was '''really''' a coincidence :)

== Download ==
The following files are available.
  * attachment:python-dateutil-1.0.tar.bz2
  * attachment:python-dateutil-1.0-1.noarch.rpm

== Author ==
The dateutil module was written by GustavoNiemeyer <gustavo@niemeyer.net>.

== Documentation ==
The following modules are available.

=== relativedelta ===
This module offers the '''relativedelta''' type, which is based 
on the specification of the excelent work done by M.-A. Lemburg in his
[http://www.egenix.com/files/python/mxDateTime.html mxDateTime]
extension. However, notice that this type '''does not''' implement the
same algorithm as his work. Do not expect it to behave like
{{{mxDateTime}}}'s counterpart.

==== relativedelta type ====

There's two different ways to build a relativedelta instance. The
first one is passing it two {{{date}}}/{{{datetime}}} instances:
{{{
relativedelta(datetime1, datetime2)
}}}

This will build the relative difference between {{{datetime1}}} and
{{{datetime2}}}, so that the following constraint is always true:
{{{
datetime2+relativedelta(datetime1, datetime2) == datetime1
}}}

Notice that instead of {{{datetime}}} instances, you may use
{{{date}}} instances, or a mix of both.

And the other way is to use any of the following keyword arguments:

  year, month, day, hour, minute, second, microsecond::
  Absolute information.

  years, months, weeks, days, hours, minutes, seconds, microseconds::
  Relative information, may be negative.

  weekday::
  One of the weekday instances ({{{MO}}}, {{{TU}}}, etc). These
  instances may receive a parameter {{{n}}}, specifying the {{{n}}}th
  weekday, which could be positive or negative (like {{{MO(+2)}}} or
  {{{MO(-3)}}}. Not specifying it is the same as specifying {{{+1}}}.
  You can also use an integer, where {{{0=MO}}}. Notice that,
  for example, if the calculated date is already Monday, using
  {{{MO}}} or {{{MO(+1)}}} (which is the same thing in this context),
  won't change the day.

  leapdays::
  Will add given days to the date found, but only if the computed
  year is a leap year and the computed date is post 28 of february.

  yearday, nlyearday::
  Set the yearday or the non-leap year day (jump leap days).
  These are converted to {{{day}}}/{{{month}}}/{{{leapdays}}}
  information.

==== Behavior of operations ====
If you're curious about exactly how the relative delta will act
on operations, here is a description of its behavior.

  1. Calculate the absolute year, using the {{{year}}} argument, or the
  original datetime year, if the argument is not present.
  1. Add the relative {{{years}}} argument to the absolute year.
  1. Do steps 1 and 2 for {{{month}}}/{{{months}}}.
  1. Calculate the absolute day, using the {{{day}}} argument, or the
  original datetime day, if the argument is not present. Then, subtract
  from the day until it fits in the year and month found after their
  operations.
  1. Add the relative {{{days}}} argument to the absolute day. Notice
  that the {{{weeks}}} argument is multiplied by 7 and added to {{{days}}}.
  1. If {{{leapdays}}} is present, the computed year is a leap year, and
  the computed month is after february, remove one day from the found date.
  1. Do steps 1 and 2 for {{{hour}}}/{{{hours}}}, {{{minute}}}/{{{minutes}}},
  {{{second}}}/{{{seconds}}}, {{{microsecond}}}/{{{microseconds}}}.
  1. If the {{{weekday}}} argument is present, calculate the {{{n}}}th
  occurrence of the given weekday.

==== Examples ====

Let's begin our trip.
{{{
>>> from datetime import *; from dateutil.relativedelta import *
>>> import calendar
}}}

Store some values.
{{{
>>> NOW = datetime.now()
>>> TODAY = date.today()
>>> NOW
datetime.datetime(2003, 9, 17, 20, 54, 47, 282310)
>>> TODAY
datetime.date(2003, 9, 17)
}}}

Next month.
{{{
>>> NOW+relativedelta(months=+1)
datetime.datetime(2003, 10, 17, 20, 54, 47, 282310)
}}}

Next month, plus one week.
{{{
>>> NOW+relativedelta(months=+1, weeks=+1)
datetime.datetime(2003, 10, 24, 20, 54, 47, 282310)
}}}

Next month, plus one week, at 10am.
{{{
>>> TODAY+relativedelta(months=+1, weeks=+1, hour=10)
datetime.datetime(2003, 10, 24, 10, 0)
}}}

Let's try the other way around. Notice that the
hour setting we get in the relativedelta is relative,
since it's a difference, and the weeks parameter
has gone.
{{{
>>> relativedelta(datetime(2003, 10, 24, 10, 0), TODAY)
relativedelta(months=+1, days=+7, hours=+10)
}}}

One month before one year.
{{{
>>> NOW+relativedelta(years=+1, months=-1)
datetime.datetime(2004, 8, 17, 20, 54, 47, 282310)
}}}

How does it handle months with different numbers of days?
Notice that adding one month will never cross the month
boundary.
{{{
>>> date(2003,1,27)+relativedelta(months=+1)
datetime.date(2003, 2, 27)
>>> date(2003,1,31)+relativedelta(months=+1)
datetime.date(2003, 2, 28)
>>> date(2003,1,31)+relativedelta(months=+2)
datetime.date(2003, 3, 31)
}}}

The logic for years is the same, even on leap years.
{{{
>>> date(2000,2,28)+relativedelta(years=+1)
datetime.date(2001, 2, 28)
>>> date(2000,2,29)+relativedelta(years=+1)
datetime.date(2001, 2, 28)

>>> date(1999,2,28)+relativedelta(years=+1)
datetime.date(2000, 2, 28)
>>> date(1999,3,1)+relativedelta(years=+1)
datetime.date(2000, 3, 1)

>>> date(2001,2,28)+relativedelta(years=-1)
datetime.date(2000, 2, 28)
>>> date(2001,3,1)+relativedelta(years=-1)
datetime.date(2000, 3, 1)
}}}

Next friday.
{{{
>>> TODAY+relativedelta(weekday=FR)
datetime.date(2003, 9, 19)

>>> TODAY+relativedelta(weekday=calendar.FRIDAY)
datetime.date(2003, 9, 19)
}}}

Last friday in this month.
{{{
>>> TODAY+relativedelta(day=31, weekday=FR(-1))
datetime.date(2003, 9, 26)
}}}

Next wednesday (it's today!).
{{{
>>> TODAY+relativedelta(weekday=WE(+1))
datetime.date(2003, 9, 17)
}}}

Next wednesday, but not today.
{{{
>>> TODAY+relativedelta(days=+1, weekday=WE(+1))
datetime.date(2003, 9, 24)
}}}

Following
[http://www.cl.cam.ac.uk/~mgk25/iso-time.html ISO year week number notation]
find the first day of the 15th week of 1997.
{{{
>>> datetime(1997,1,1)+relativedelta(day=4, weekday=MO(-1), weeks=+14)
datetime.datetime(1997, 4, 7, 0, 0)
}}}

How long ago has the millennium changed?
{{{
>>> relativedelta(NOW, date(2001,1,1))
relativedelta(years=+2, months=+8, days=+16,
	      hours=+20, minutes=+54, seconds=+47, microseconds=+282310)
}}}

How old is John?
{{{
>>> johnbirthday = datetime(1978, 4, 5, 12, 0)
>>> relativedelta(NOW, johnbirthday)
relativedelta(years=+25, months=+5, days=+12,
	      hours=+8, minutes=+54, seconds=+47, microseconds=+282310)
}}}

It works with dates too.
{{{
>>> relativedelta(TODAY, johnbirthday)
relativedelta(years=+25, months=+5, days=+11, hours=+12)
}}}

Obtain today's date using the yearday:
{{{
>>> date(2003, 1, 1)+relativedelta(yearday=260)
datetime.date(2003, 9, 17)
}}}

We can use today's date, since yearday should be absolute
in the given year:
{{{
>>> TODAY+relativedelta(yearday=260)
datetime.date(2003, 9, 17)
}}}

Last year it should be in the same day:
{{{
>>> date(2002, 1, 1)+relativedelta(yearday=260)
datetime.date(2002, 9, 17)
}}}

But not in a leap year:
{{{
>>> date(2000, 1, 1)+relativedelta(yearday=260)
datetime.date(2000, 9, 16)
}}}

We can use the non-leap year day to ignore this:
{{{
>>> date(2000, 1, 1)+relativedelta(nlyearday=260)
datetime.date(2000, 9, 17)
}}}

=== rrule ===
The rrule module offers a small, complete, and very fast, implementation
of the recurrence rules documented in the 
[ftp://ftp.rfc-editor.org/in-notes/rfc2445.txt iCalendar RFC], including
support for caching of results.

==== rrule type ====
That's the base of the rrule operation. It accepts all the keywords
defined in the RFC as its constructor parameters (except {{{byday}}},
which was renamed to {{{byweekday}}}) and more. The constructor
prototype is:
{{{
rrule(freq)
}}}

Where {{{freq}}} must be one of {{{YEARLY}}}, {{{MONTHLY}}},
{{{WEEKLY}}}, {{{DAILY}}}, {{{HOURLY}}}, {{{MINUTELY}}},
or {{{SECONDLY}}}.

Additionally, it supports the following keyword arguments:

    cache::
    If given, it must be a boolean value specifying to enable
    or disable caching of results. If you will use the same
    {{{rrule}}} instance multiple times, enabling caching will
    improve the performance considerably.

    dtstart::
    The recurrence start. Besides being the base for the
    recurrence, missing parameters in the final recurrence
    instances will also be extracted from this date. If not
    given, {{{datetime.now()}}} will be used instead.

    interval::
    The interval between each {{{freq}}} iteration. For example,
    when using {{{YEARLY}}}, an interval of {{{2}}} means
    once every two years, but with {{{HOURLY}}}, it means
    once every two hours. The default interval is {{{1}}}.

    wkst::
    The week start day. Must be one of the {{{MO}}}, {{{TU}}},
    {{{WE}}} constants, or an integer, specifying the first day
    of the week. This will affect recurrences based on weekly
    periods. The default week start is got from
    {{{calendar.firstweekday()}}}, and may be modified by
    {{{calendar.setfirstweekday()}}}.

    count::
    How many occurrences will be generated.

    until::
    If given, this must be a {{{datetime}}} instance, that will
    specify the limit of the recurrence. If a recurrence instance
    happens to be the same as the {{{datetime}}} instance given
    in the {{{until}}} keyword, this will be the last occurrence.

    bysetpos::
    If given, it must be either an integer, or a sequence of
    integers, positive or negative. Each given integer will
    specify an occurrence number, corresponding to the nth
    occurrence of the rule inside the frequency period. For
    example, a {{{bysetpos}}} of {{{-1}}} if combined with a
    {{{MONTHLY}}} frequency, and a {{{byweekday}}} of
    {{{(MO, TU, WE, TH, FR)}}}, will result in the last work
    day of every month.

    bymonth::
    If given, it must be either an integer, or a sequence of
    integers, meaning the months to apply the recurrence to.

    bymonthday::
    If given, it must be either an integer, or a sequence of
    integers, meaning the month days to apply the recurrence to.

    byyearday::
    If given, it must be either an integer, or a sequence of
    integers, meaning the year days to apply the recurrence to.

    byweekno::
    If given, it must be either an integer, or a sequence of
    integers, meaning the week numbers to apply the recurrence
    to. Week numbers have the meaning described in ISO8601,
    that is, the first week of the year is that containing at
    least four days of the new year.
    
    byweekday::
    If given, it must be either an integer ({{{0 == MO}}}), a
    sequence of integers, one of the weekday constants
    ({{{MO}}}, {{{TU}}}, etc), or a sequence of these constants.
    When given, these variables will define the weekdays where
    the recurrence will be applied. It's also possible to use
    an argument {{{n}}} for the weekday instances, which will
    mean the {{{n}}}''th'' occurrence of this weekday in the
    period. For example, with {{{MONTHLY}}}, or with
    {{{YEARLY}}} and {{{BYMONTH}}}, using {{{FR(+1)}}}
    in {{{byweekday}}} will specify the first friday of the
    month where the recurrence happens. Notice that in the RFC
    documentation, this is specified as {{{BYDAY}}}, but was
    renamed to avoid the ambiguity of that keyword.

    byhour::
    If given, it must be either an integer, or a sequence of
    integers, meaning the hours to apply the recurrence to.

    byminute::
    If given, it must be either an integer, or a sequence of
    integers, meaning the minutes to apply the recurrence to.

    bysecond::
    If given, it must be either an integer, or a sequence of
    integers, meaning the seconds to apply the recurrence to.

    byeaster::
    If given, it must be either an integer, or a sequence of
    integers, positive or negative. Each integer will define
    an offset from the Easter Sunday. Passing the offset
    {{{0}}} to {{{byeaster}}} will yield the Easter Sunday
    itself. This is an extension to the RFC specification.

==== rrule methods ====
The following methods are available in {{{rrule}}} instances:

    rrule.before(dt, inc=False)::
    Returns the last recurrence before the given {{{datetime}}}
    instance. The {{{inc}}} keyword defines what happens if
    {{{dt}}} '''is''' an occurrence. With {{{inc == True}}},
    if {{{dt}}} itself is an occurrence, it will be returned.

    rrule.after(dt, inc=False)::
    Returns the first recurrence after the given {{{datetime}}}
    instance. The {{{inc}}} keyword defines what happens if
    {{{dt}}} '''is''' an occurrence. With {{{inc == True}}},
    if {{{dt}}} itself is an occurrence, it will be returned.

    rrule.between(after, before, inc=False)::
    Returns all the occurrences of the rrule between {{{after}}}
    and {{{before}}}. The {{{inc}}} keyword defines what happens
    if {{{after}}} and/or {{{before}}} are themselves occurrences.
    With {{{inc == True}}}, they will be included in the list,
    if they are found in the recurrence set.

    rrule.count()::
    Returns the number of recurrences in this set. It will have
    go trough the whole recurrence, if this hasn't been done
    before.

Besides these methods, {{{rrule}}} instances also support
the {{{__getitem__()}}} and {{{__contains__()}}} special methods,
meaning that these are valid expressions:
{{{
rr = rrule(...)
if datetime(...) in rr:
    ...
print rr[0]
print rr[-1]
print rr[1:2]
print rr[::-2]
}}}

The getitem/slicing mechanism is smart enough to avoid getting the whole
recurrence set, if possible.

==== Notes ====

  * The rrule type has no {{{byday}}} keyword. The equivalent keyword
  has been replaced by the {{{byweekday}}} keyword, to remove the
  ambiguity present in the original keyword.

  * Unlike documented in the RFC, the starting datetime ({{{dtstart}}})
  is not the first recurrence instance, unless it does fit in the
  specified rules. In a python module context, this behavior makes more
  sense than otherwise. Notice that you can easily get the original
  behavior by using a rruleset and adding the {{{dtstart}}} as an
  {{{rdate}}} recurrence.

  * Unlike documented in the RFC, every keyword is valid on every
  frequency (the RFC documents that {{{byweekno}}} is only valid
  on yearly frequencies, for example).

  * In addition to the documented keywords, a {{{byeaster}}} keyword
  was introduced, making it easy to compute recurrent events relative
  to the Easter Sunday.

==== rrule examples ====
These examples were converted from the RFC.

Prepare the environment.
{{{
>>> from dateutil.rrule import *
>>> from dateutil.parser import *
>>> from datetime import *

>>> import pprint
>>> import sys
>>> sys.displayhook = pprint.pprint
}}}

Daily, for 10 occurrences.
{{{
>>> list(rrule(DAILY, count=10,
	       dtstart=parse("19970902T090000")))
[datetime.datetime(1997, 9, 2, 9, 0),
 datetime.datetime(1997, 9, 3, 9, 0),
 datetime.datetime(1997, 9, 4, 9, 0),
 datetime.datetime(1997, 9, 5, 9, 0),
 datetime.datetime(1997, 9, 6, 9, 0),
 datetime.datetime(1997, 9, 7, 9, 0),
 datetime.datetime(1997, 9, 8, 9, 0),
 datetime.datetime(1997, 9, 9, 9, 0),
 datetime.datetime(1997, 9, 10, 9, 0),
 datetime.datetime(1997, 9, 11, 9, 0)]
}}}

Daily until December 24, 1997
{{{
>>> list(rrule(DAILY,
	       dtstart=parse("19970902T090000"),
	       until=parse("19971224T000000")))
[datetime.datetime(1997, 9, 2, 9, 0),
 datetime.datetime(1997, 9, 3, 9, 0),
 datetime.datetime(1997, 9, 4, 9, 0),
 (...)
 datetime.datetime(1997, 12, 21, 9, 0),
 datetime.datetime(1997, 12, 22, 9, 0),
 datetime.datetime(1997, 12, 23, 9, 0)]
}}}

Every other day, 5 occurrences.
{{{
>>> list(rrule(DAILY, interval=2, count=5,
	       dtstart=parse("19970902T090000")))
[datetime.datetime(1997, 9, 2, 9, 0),
 datetime.datetime(1997, 9, 4, 9, 0),
 datetime.datetime(1997, 9, 6, 9, 0),
 datetime.datetime(1997, 9, 8, 9, 0),
 datetime.datetime(1997, 9, 10, 9, 0)]
}}}

Every 10 days, 5 occurrences.
{{{
>>> list(rrule(DAILY, interval=10, count=5,
	       dtstart=parse("19970902T090000")))
[datetime.datetime(1997, 9, 2, 9, 0),
 datetime.datetime(1997, 9, 12, 9, 0),
 datetime.datetime(1997, 9, 22, 9, 0),
 datetime.datetime(1997, 10, 2, 9, 0),
 datetime.datetime(1997, 10, 12, 9, 0)]
}}}

Everyday in January, for 3 years.
{{{
>>> list(rrule(YEARLY, bymonth=1, byweekday=range(7),
	       dtstart=parse("19980101T090000"),
	       until=parse("20000131T090000")))
[datetime.datetime(1998, 1, 1, 9, 0),
 datetime.datetime(1998, 1, 2, 9, 0),
 (...)
 datetime.datetime(1998, 1, 30, 9, 0),
 datetime.datetime(1998, 1, 31, 9, 0),
 datetime.datetime(1999, 1, 1, 9, 0),
 datetime.datetime(1999, 1, 2, 9, 0),
 (...)
 datetime.datetime(1999, 1, 30, 9, 0),
 datetime.datetime(1999, 1, 31, 9, 0),
 datetime.datetime(2000, 1, 1, 9, 0),
 datetime.datetime(2000, 1, 2, 9, 0),
 (...)
 datetime.datetime(2000, 1, 29, 9, 0),
 datetime.datetime(2000, 1, 31, 9, 0)]
}}} 

Same thing, in another way.
{{{
>>> list(rrule(DAILY, bymonth=1,
               dtstart=parse("19980101T090000"),
	       until=parse("20000131T090000")))
(...)
}}}

Weekly for 10 occurrences.
{{{
>>> list(rrule(WEEKLY, count=10,
	       dtstart=parse("19970902T090000")))
[datetime.datetime(1997, 9, 2, 9, 0),
 datetime.datetime(1997, 9, 9, 9, 0),
 datetime.datetime(1997, 9, 16, 9, 0),
 datetime.datetime(1997, 9, 23, 9, 0),
 datetime.datetime(1997, 9, 30, 9, 0),
 datetime.datetime(1997, 10, 7, 9, 0),
 datetime.datetime(1997, 10, 14, 9, 0),
 datetime.datetime(1997, 10, 21, 9, 0),
 datetime.datetime(1997, 10, 28, 9, 0),
 datetime.datetime(1997, 11, 4, 9, 0)]
}}}

Every other week, 6 occurrences.
{{{
>>> list(rrule(WEEKLY, interval=2, count=6,
	       dtstart=parse("19970902T090000")))
[datetime.datetime(1997, 9, 2, 9, 0),
 datetime.datetime(1997, 9, 16, 9, 0),
 datetime.datetime(1997, 9, 30, 9, 0),
 datetime.datetime(1997, 10, 14, 9, 0),
 datetime.datetime(1997, 10, 28, 9, 0),
 datetime.datetime(1997, 11, 11, 9, 0)]
}}}

Weekly on Tuesday and Thursday for 5 weeks.
{{{
>>> list(rrule(WEEKLY, count=10, wkst=SU, byweekday=(TU,TH),
	       dtstart=parse("19970902T090000")))
[datetime.datetime(1997, 9, 2, 9, 0),
 datetime.datetime(1997, 9, 4, 9, 0),
 datetime.datetime(1997, 9, 9, 9, 0),
 datetime.datetime(1997, 9, 11, 9, 0),
 datetime.datetime(1997, 9, 16, 9, 0),
 datetime.datetime(1997, 9, 18, 9, 0),
 datetime.datetime(1997, 9, 23, 9, 0),
 datetime.datetime(1997, 9, 25, 9, 0),
 datetime.datetime(1997, 9, 30, 9, 0),
 datetime.datetime(1997, 10, 2, 9, 0)]
}}}

Every other week on Tuesday and Thursday, for 8 occurrences.
{{{
>>> list(rrule(WEEKLY, interval=2, count=8,
	       wkst=SU, byweekday=(TU,TH),
	       dtstart=parse("19970902T090000")))
[datetime.datetime(1997, 9, 2, 9, 0),
 datetime.datetime(1997, 9, 4, 9, 0),
 datetime.datetime(1997, 9, 16, 9, 0),
 datetime.datetime(1997, 9, 18, 9, 0),
 datetime.datetime(1997, 9, 30, 9, 0),
 datetime.datetime(1997, 10, 2, 9, 0),
 datetime.datetime(1997, 10, 14, 9, 0),
 datetime.datetime(1997, 10, 16, 9, 0)]
}}}

Monthly on the 1st Friday for ten occurrences.
{{{
>>> list(rrule(MONTHLY, count=10, byweekday=FR(1),
	       dtstart=parse("19970905T090000")))
[datetime.datetime(1997, 9, 5, 9, 0),
 datetime.datetime(1997, 10, 3, 9, 0),
 datetime.datetime(1997, 11, 7, 9, 0),
 datetime.datetime(1997, 12, 5, 9, 0),
 datetime.datetime(1998, 1, 2, 9, 0),
 datetime.datetime(1998, 2, 6, 9, 0),
 datetime.datetime(1998, 3, 6, 9, 0),
 datetime.datetime(1998, 4, 3, 9, 0),
 datetime.datetime(1998, 5, 1, 9, 0),
 datetime.datetime(1998, 6, 5, 9, 0)]
}}}

Every other month on the 1st and last Sunday of the month for 10 occurrences.
{{{
>>> list(rrule(MONTHLY, interval=2, count=10,
	       byweekday=(SU(1), SU(-1)),
	       dtstart=parse("19970907T090000")))
[datetime.datetime(1997, 9, 7, 9, 0),
 datetime.datetime(1997, 9, 28, 9, 0),
 datetime.datetime(1997, 11, 2, 9, 0),
 datetime.datetime(1997, 11, 30, 9, 0),
 datetime.datetime(1998, 1, 4, 9, 0),
 datetime.datetime(1998, 1, 25, 9, 0),
 datetime.datetime(1998, 3, 1, 9, 0),
 datetime.datetime(1998, 3, 29, 9, 0),
 datetime.datetime(1998, 5, 3, 9, 0),
 datetime.datetime(1998, 5, 31, 9, 0)]
}}}

Monthly on the second to last Monday of the month for 6 months.
{{{
>>> list(rrule(MONTHLY, count=6, byweekday=MO(-2),
	       dtstart=parse("19970922T090000")))
[datetime.datetime(1997, 9, 22, 9, 0),
 datetime.datetime(1997, 10, 20, 9, 0),
 datetime.datetime(1997, 11, 17, 9, 0),
 datetime.datetime(1997, 12, 22, 9, 0),
 datetime.datetime(1998, 1, 19, 9, 0),
 datetime.datetime(1998, 2, 16, 9, 0)]
}}}

Monthly on the third to the last day of the month, for 6 months.
{{{
>>> list(rrule(MONTHLY, count=6, bymonthday=-3,
	       dtstart=parse("19970928T090000")))
[datetime.datetime(1997, 9, 28, 9, 0),
 datetime.datetime(1997, 10, 29, 9, 0),
 datetime.datetime(1997, 11, 28, 9, 0),
 datetime.datetime(1997, 12, 29, 9, 0),
 datetime.datetime(1998, 1, 29, 9, 0),
 datetime.datetime(1998, 2, 26, 9, 0)]
}}}

Monthly on the 2nd and 15th of the month for 5 occurrences.
{{{
>>> list(rrule(MONTHLY, count=5, bymonthday=(2,15),
	       dtstart=parse("19970902T090000")))
[datetime.datetime(1997, 9, 2, 9, 0),
 datetime.datetime(1997, 9, 15, 9, 0),
 datetime.datetime(1997, 10, 2, 9, 0),
 datetime.datetime(1997, 10, 15, 9, 0),
 datetime.datetime(1997, 11, 2, 9, 0)]
}}}

Monthly on the first and last day of the month for 3 occurrences.
{{{
>>> list(rrule(MONTHLY, count=5, bymonthday=(-1,1,),
               dtstart=parse("1997090
2T090000")))
[datetime.datetime(1997, 9, 30, 9, 0),
 datetime.datetime(1997, 10, 1, 9, 0),
 datetime.datetime(1997, 10, 31, 9, 0),
 datetime.datetime(1997, 11, 1, 9, 0),
 datetime.datetime(1997, 11, 30, 9, 0)]
}}}

Every 18 months on the 10th thru 15th of the month for 10 occurrences.
{{{
>>> list(rrule(MONTHLY, interval=18, count=10,
	       bymonthday=range(10,16),
	       dtstart=parse("19970910T090000")))
[datetime.datetime(1997, 9, 10, 9, 0),
 datetime.datetime(1997, 9, 11, 9, 0),
 datetime.datetime(1997, 9, 12, 9, 0),
 datetime.datetime(1997, 9, 13, 9, 0),
 datetime.datetime(1997, 9, 14, 9, 0),
 datetime.datetime(1997, 9, 15, 9, 0),
 datetime.datetime(1999, 3, 10, 9, 0),
 datetime.datetime(1999, 3, 11, 9, 0),
 datetime.datetime(1999, 3, 12, 9, 0),
 datetime.datetime(1999, 3, 13, 9, 0)]
}}}

Every Tuesday, every other month, 6 occurences.
{{{
>>> list(rrule(MONTHLY, interval=2, count=6, byweekday=TU,
	       dtstart=parse("19970902T090000")))
[datetime.datetime(1997, 9, 2, 9, 0),
 datetime.datetime(1997, 9, 9, 9, 0),
 datetime.datetime(1997, 9, 16, 9, 0),
 datetime.datetime(1997, 9, 23, 9, 0),
 datetime.datetime(1997, 9, 30, 9, 0),
 datetime.datetime(1997, 11, 4, 9, 0)]
}}}

Yearly in June and July for 10 occurrences.
{{{
>>> list(rrule(YEARLY, count=4, bymonth=(6,7),
	       dtstart=parse("19970610T0900
00")))
[datetime.datetime(1997, 6, 10, 9, 0),
 datetime.datetime(1997, 7, 10, 9, 0),
 datetime.datetime(1998, 6, 10, 9, 0),
 datetime.datetime(1998, 7, 10, 9, 0)]
}}}

Every 3rd year on the 1st, 100th and 200th day for 4 occurrences.
{{{
>>> list(rrule(YEARLY, count=4, interval=3, byyearday=(1,100,200),
	       dtstart=parse("19970101T090000")))
[datetime.datetime(1997, 1, 1, 9, 0),
 datetime.datetime(1997, 4, 10, 9, 0),
 datetime.datetime(1997, 7, 19, 9, 0),
 datetime.datetime(2000, 1, 1, 9, 0)]
}}}

Every 20th Monday of the year, 3 occurrences.
{{{
>>> list(rrule(YEARLY, count=3, byweekday=MO(20),
	       dtstart=parse("19970519T090000")))
[datetime.datetime(1997, 5, 19, 9, 0),
 datetime.datetime(1998, 5, 18, 9, 0),
 datetime.datetime(1999, 5, 17, 9, 0)]
}}}

Monday of week number 20 (where the default start of the week is Monday),
3 occurrences.
{{{
>>> list(rrule(YEARLY, count=3, byweekno=20, byweekday=MO,
	       dtstart=parse("19970512T090000")))
[datetime.datetime(1997, 5, 12, 9, 0),
 datetime.datetime(1998, 5, 11, 9, 0),
 datetime.datetime(1999, 5, 17, 9, 0)]
}}}

The week number 1 may be in the last year.
{{{
>>> list(rrule(WEEKLY, count=3, byweekno=1, byweekday=MO,
	       dtstart=parse("19970902T090000")))
[datetime.datetime(1997, 12, 29, 9, 0),
 datetime.datetime(1999, 1, 4, 9, 0),
 datetime.datetime(2000, 1, 3, 9, 0)]
}}}

And the week numbers greater than 51 may be in the next year.
{{{
>>> list(rrule(WEEKLY, count=3, byweekno=52, byweekday=SU,
	       dtstart=parse("19970902T090000")))
[datetime.datetime(1997, 12, 28, 9, 0),
 datetime.datetime(1998, 12, 27, 9, 0),
 datetime.datetime(2000, 1, 2, 9, 0)]
}}}

Only some years have week number 53:
{{{
>>> list(rrule(WEEKLY, count=3, byweekno=53, byweekday=MO,
	       dtstart=parse("19970902T090000")))
[datetime.datetime(1998, 12, 28, 9, 0),
 datetime.datetime(2004, 12, 27, 9, 0),
 datetime.datetime(2009, 12, 28, 9, 0)]
}}}

Every Friday the 13th, 4 occurrences.
{{{
>>> list(rrule(YEARLY, count=4, byweekday=FR, bymonthday=13,
	       dtstart=parse("19970902T090000")))
[datetime.datetime(1998, 2, 13, 9, 0),
 datetime.datetime(1998, 3, 13, 9, 0),
 datetime.datetime(1998, 11, 13, 9, 0),
 datetime.datetime(1999, 8, 13, 9, 0)]
}}}

Every four years, the first Tuesday after a Monday in November,
3 occurrences (U.S. Presidential Election day):
{{{
>>> list(rrule(YEARLY, interval=4, count=3, bymonth=11,
	       byweekday=TU, bymonthday=(2,3,4,5,6,7,8),
	       dtstart=parse("19961105T090000")))
[datetime.datetime(1996, 11, 5, 9, 0),
 datetime.datetime(2000, 11, 7, 9, 0),
 datetime.datetime(2004, 11, 2, 9, 0)]
}}}

The 3rd instance into the month of one of Tuesday, Wednesday or
Thursday, for the next 3 months:
{{{
>>> list(rrule(MONTHLY, count=3, byweekday=(TU,WE,TH),
	       bysetpos=3, dtstart=parse("19970904T090000")))
[datetime.datetime(1997, 9, 4, 9, 0),
 datetime.datetime(1997, 10, 7, 9, 0),
 datetime.datetime(1997, 11, 6, 9, 0)]
}}}

The 2nd to last weekday of the month, 3 occurrences.
{{{
>>> list(rrule(MONTHLY, count=3, byweekday=(MO,TU,WE,TH,FR),
	       bysetpos=-2, dtstart=parse("19970929T090000")))
[datetime.datetime(1997, 9, 29, 9, 0),
 datetime.datetime(1997, 10, 30, 9, 0),
 datetime.datetime(1997, 11, 27, 9, 0)]
}}}

Every 3 hours from 9:00 AM to 5:00 PM on a specific day.
{{{
>>> list(rrule(HOURLY, interval=3,
	       dtstart=parse("19970902T090000"),
	       until=parse("19970902T170000")))
[datetime.datetime(1997, 9, 2, 9, 0),
 datetime.datetime(1997, 9, 2, 12, 0),
 datetime.datetime(1997, 9, 2, 15, 0)]
}}}

Every 15 minutes for 6 occurrences.
{{{
>>> list(rrule(MINUTELY, interval=15, count=6,
	       dtstart=parse("19970902T090000")))
[datetime.datetime(1997, 9, 2, 9, 0),
 datetime.datetime(1997, 9, 2, 9, 15),
 datetime.datetime(1997, 9, 2, 9, 30),
 datetime.datetime(1997, 9, 2, 9, 45),
 datetime.datetime(1997, 9, 2, 10, 0),
 datetime.datetime(1997, 9, 2, 10, 15)]
}}}

Every hour and a half for 4 occurrences.
{{{
>>> list(rrule(MINUTELY, interval=90, count=4,
	       dtstart=parse("19970902T090000")))
[datetime.datetime(1997, 9, 2, 9, 0),
 datetime.datetime(1997, 9, 2, 10, 30),
 datetime.datetime(1997, 9, 2, 12, 0),
 datetime.datetime(1997, 9, 2, 13, 30)]
}}}

Every 20 minutes from 9:00 AM to 4:40 PM for two days.
{{{
>>> list(rrule(MINUTELY, interval=20, count=48,
	       byhour=range(9,17), byminute=(0,20,40),
	       dtstart=parse("19970902T090000")))
[datetime.datetime(1997, 9, 2, 9, 0),
 datetime.datetime(1997, 9, 2, 9, 20),
 (...)
 datetime.datetime(1997, 9, 2, 16, 20),
 datetime.datetime(1997, 9, 2, 16, 40),
 datetime.datetime(1997, 9, 3, 9, 0),
 datetime.datetime(1997, 9, 3, 9, 20),
 (...)
 datetime.datetime(1997, 9, 3, 16, 20),
 datetime.datetime(1997, 9, 3, 16, 40)]
}}}

An example where the days generated makes a difference because of {{{wkst}}}.
{{{
>>> list(rrule(WEEKLY, interval=2, count=4,
	       byweekday=(TU,SU), wkst=MO,
	       dtstart=parse("19970805T090000")))
[datetime.datetime(1997, 8, 5, 9, 0),
 datetime.datetime(1997, 8, 10, 9, 0),
 datetime.datetime(1997, 8, 19, 9, 0),
 datetime.datetime(1997, 8, 24, 9, 0)]

>>> list(rrule(WEEKLY, interval=2, count=4,
	       byweekday=(TU,SU), wkst=SU,
	       dtstart=parse("19970805T090000")))
[datetime.datetime(1997, 8, 5, 9, 0),
 datetime.datetime(1997, 8, 17, 9, 0),
 datetime.datetime(1997, 8, 19, 9, 0),
 datetime.datetime(1997, 8, 31, 9, 0)]
}}}

==== rruleset type ====
The {{{rruleset}}} type allows more complex recurrence setups, mixing
multiple rules, dates, exclusion rules, and exclusion dates.
The type constructor takes the following keyword arguments:

    cache::
    If True, caching of results will be enabled, improving performance
    of multiple queries considerably.

==== rruleset methods ====
The following methods are available:

    rruleset.rrule(rrule)::
    Include the given {{{rrule}}} instance in the recurrence set
    generation.

    rruleset.rdate(dt)::
    Include the given {{{datetime}}} instance in the recurrence
    set generation.
    
    rruleset.exrule(rrule)::
    Include the given {{{rrule}}} instance in the recurrence set
    exclusion list. Dates which are part of the given recurrence
    rules will not be generated, even if some inclusive {{{rrule}}}
    or {{{rdate}}} matches them.

    rruleset.exdate(dt)::
    Include the given {{{datetime}}} instance in the recurrence set
    exclusion list. Dates included that way will not be generated,
    even if some inclusive {{{rrule}}} or {{{rdate}}} matches them.

    rruleset.before(dt, inc=False)::
    Returns the last recurrence before the given {{{datetime}}}
    instance. The {{{inc}}} keyword defines what happens if
    {{{dt}}} '''is''' an occurrence. With {{{inc == True}}},
    if {{{dt}}} itself is an occurrence, it will be returned.

    rruleset.after(dt, inc=False)::
    Returns the first recurrence after the given {{{datetime}}}
    instance. The {{{inc}}} keyword defines what happens if
    {{{dt}}} '''is''' an occurrence. With {{{inc == True}}},
    if {{{dt}}} itself is an occurrence, it will be returned.

    rruleset.between(after, before, inc=False)::
    Returns all the occurrences of the rrule between {{{after}}}
    and {{{before}}}. The {{{inc}}} keyword defines what happens
    if {{{after}}} and/or {{{before}}} are themselves occurrences.
    With {{{inc == True}}}, they will be included in the list,
    if they are found in the recurrence set.

    rruleset.count()::
    Returns the number of recurrences in this set. It will have
    go trough the whole recurrence, if this hasn't been done
    before.

Besides these methods, {{{rruleset}}} instances also support
the {{{__getitem__()}}} and {{{__contains__()}}} special methods,
meaning that these are valid expressions:
{{{
set = rruleset(...)
if datetime(...) in set:
    ...
print set[0]
print set[-1]
print set[1:2]
print set[::-2]
}}}

The getitem/slicing mechanism is smart enough to avoid getting the whole
recurrence set, if possible.

==== rruleset examples ====
Daily, for 7 days, jumping Saturday and Sunday occurrences.
{{{
>>> set = rruleset()
>>> set.rrule(rrule(DAILY, count=7,
		    dtstart=parse("19970902T090000")))
>>> set.exrule(rrule(YEARLY, byweekday=(SA,SU),
		     dtstart=parse("19970902T090000")))
>>> list(set)
[datetime.datetime(1997, 9, 2, 9, 0),
 datetime.datetime(1997, 9, 3, 9, 0),
 datetime.datetime(1997, 9, 4, 9, 0),
 datetime.datetime(1997, 9, 5, 9, 0),
 datetime.datetime(1997, 9, 8, 9, 0)]
}}}

Weekly, for 4 weeks, plus one time on day 7, and not on day 16.
{{{
>>> set = rruleset()
>>> set.rrule(rrule(WEEKLY, count=4,
		    dtstart=parse("19970902T090000")))
>>> set.rdate(datetime.datetime(1997, 9, 7, 9, 0))
>>> set.exdate(datetime.datetime(1997, 9, 16, 9, 0))
>>> list(set)
[datetime.datetime(1997, 9, 2, 9, 0),
 datetime.datetime(1997, 9, 7, 9, 0),
 datetime.datetime(1997, 9, 9, 9, 0),
 datetime.datetime(1997, 9, 23, 9, 0)]
}}}

==== rrulestr() function ====
The {{{rrulestr()}}} function is a parser for ''RFC-like'' syntaxes.
The function prototype is:
{{{
rrulestr(str)
}}}

The string passed as parameter may be a multiple line string, a
single line string, or just the {{{RRULE}}} property value.

Additionally, it accepts the following keyword arguments:

    cache::
    If {{{True}}}, the {{{rruleset}}} or {{{rrule}}} created instance
    will cache its results. Default is not to cache.

    dtstart::
    If given, it must be a {{{datetime}}} instance that will be used
    when no {{{DTSTART}}} property is found in the parsed string. If
    it is not given, and the property is not found, {{{datetime.now()}}}
    will be used instead.

    unfold::
    If set to {{{True}}}, lines will be unfolded following the RFC
    specification. It defaults to {{{False}}}, meaning that spaces
    before every line will be stripped.

    forceset::
    If set to {{{True}}} a {{{rruleset}}} instance will be returned,
    even if only a single rule is found. The default is to return an
    {{{rrule}}} if possible, and an {{{rruleset}}} if necessary.

    compatible::
    If set to {{{True}}}, the parser will operate in RFC-compatible
    mode. Right now it means that {{{unfold}}} will be turned on,
    and if a {{{DTSTART}}} is found, it will be considered the first     
    recurrence instance, as documented in the RFC.

    ignoretz::
    If set to {{{True}}}, the date parser will ignore timezone
    information available in the {{{DTSTART}}} property, or the
    {{{UNTIL}}} attribute.

    tzinfos::
    If set, it will be passed to the datetime string parser to
    resolve unknown timezone settings. For more information about
    what could be used here, check the parser documentation.

==== rrulestr() examples ====

Every 10 days, 5 occurrences.
{{{
>>> list(rrulestr("""
... DTSTART:19970902T090000
... RRULE:FREQ=DAILY;INTERVAL=10;COUNT=5
... """))
[datetime.datetime(1997, 9, 2, 9, 0),
 datetime.datetime(1997, 9, 12, 9, 0),
 datetime.datetime(1997, 9, 22, 9, 0),
 datetime.datetime(1997, 10, 2, 9, 0),
 datetime.datetime(1997, 10, 12, 9, 0)]
}}}

Same thing, but passing only the {{{RRULE}}} value.
{{{
>>> list(rrulestr("FREQ=DAILY;INTERVAL=10;COUNT=5",
		  dtstart=parse("19970902T090000")))
[datetime.datetime(1997, 9, 2, 9, 0),
 datetime.datetime(1997, 9, 12, 9, 0),
 datetime.datetime(1997, 9, 22, 9, 0),
 datetime.datetime(1997, 10, 2, 9, 0),
 datetime.datetime(1997, 10, 12, 9, 0)]
}}}

Notice that when using a single rule, it returns an
{{{rrule}}} instance, unless {{{forceset}}} was used.
{{{
>>> rrulestr("FREQ=DAILY;INTERVAL=10;COUNT=5")
<dateutil.rrule.rrule instance at 0x30269f08>

>>> rrulestr("""
... DTSTART:19970902T090000
... RRULE:FREQ=DAILY;INTERVAL=10;COUNT=5
... """)
<dateutil.rrule.rrule instance at 0x302699e0>

>>> rrulestr("FREQ=DAILY;INTERVAL=10;COUNT=5", forceset=True)
<dateutil.rrule.rruleset instance at 0x30269f08>
}}}

But when an {{{rruleset}}} is needed, it is automatically used.
{{{
>>> rrulestr("""
... DTSTART:19970902T090000
... RRULE:FREQ=DAILY;INTERVAL=10;COUNT=5
... RRULE:FREQ=DAILY;INTERVAL=5;COUNT=3
... """)
<dateutil.rrule.rruleset instance at 0x302699e0>
}}}

=== parser ===
This module offers a generic date/time string parser which is
able to parse most known formats to represent a date and/or
time.

==== parse() function ====
That's probably the only function you'll need from this module.
It offers you an interface to access the parser functionality and
extract a {{{datetime}}} type out of a string.

The prototype of this function is:
{{{
parse(timestr)
}}}

Additionally, the following keyword arguments are available:

    default::
    If given, this must be a {{{datetime}}} instance. Any fields
    missing in the parsed date will be copied from this instance.
    The default value is the current date, at 00:00:00am.

    ignoretz::
    If this is true, even if a timezone is found in the string,
    the parser will not use it.

    tzinfos::
    Using this keyword argument you may provide custom timezones
    to the parser. If given, it must be either a dictionary with
    the timezone abbreviation as key, or a function accepting a
    timezone abbreviation and offset as argument. The dictionary
    values and the function return must be a timezone offset
    in seconds, a tzinfo subclass, or a string defining the
    timezone (in the TZ environment variable format).

    dayfirst::
    This option allow one to change the precedence in which
    days are parsed in date strings. The default is given in the
    parserinfo instance (the default parserinfo has it set to
    False). If {{{dayfirst}}} is False, the {{{MM-DD-YYYY}}}
    format will have precedence over {{{DD-MM-YYYY}}} in an
    ambiguous date.

    yearfirst::
    This option allow one to change the precedence in which
    years are parsed in date strings. The default is given in
    the parserinfo instance (the default parserinfo has it set
    to False). If {{{yearfirst}}} is false, the {{{MM-DD-YY}}}
    format will have precedence over {{{YY-MM-DD}}} in an
    ambiguous date.

    fuzzy::
    If {{{fuzzy}}} is set to True, unknown tokens in the string
    will be ignored.

    parserinfo::
    This parameter allows one to change how the string is parsed,
    by using a different parserinfo class instance. Using it you
    may, for example, intenationalize the parser strings, or make
    it ignore additional words.

==== Format precedence ====
Whenever an ambiguous date is found, the {{{dayfirst}}} and
{{{yearfirst}}} parameters will control how the information
is processed. Here is the precedence in each case:

If {{{dayfirst}}} is {{{False}}} and {{{yearfirst}}} is {{{False}}},
(default, if no parameter is given):

    * {{{MM-DD-YY}}}
    * {{{DD-MM-YY}}}
    * {{{YY-MM-DD}}}

If {{{dayfirst}}} is {{{True}}} and {{{yearfirst}}} is {{{False}}}:

    * {{{DD-MM-YY}}}
    * {{{MM-DD-YY}}}
    * {{{YY-MM-DD}}}

If {{{dayfirst}}} is {{{False}}} and {{{yearfirst}}} is {{{True}}}:

    * {{{YY-MM-DD}}}
    * {{{MM-DD-YY}}}
    * {{{DD-MM-YY}}}

If {{{dayfirst}}} is {{{True}}} and {{{yearfirst}}} is {{{True}}}:

    * {{{YY-MM-DD}}}
    * {{{DD-MM-YY}}}
    * {{{MM-DD-YY}}}

==== Converting two digit years ====
When a two digit year is found, it is processed considering
the current year, so that the computed year is never more
than 49 years after the current year, nor 50 years before the
current year. In other words, if we are in year 2003, and the
year 30 is found, it will be considered as 2030, but if the
year 60 is found, it will be considered 1960.

==== Examples ====
The following code will prepare the environment:
{{{
>>> from dateutil.parser import *
>>> from dateutil.tz import *
>>> from datetime import *
>>> TZOFFSETS = {"BRST": -10800}
>>> BRSTTZ = tzoffset(-10800, "BRST")
>>> DEFAULT = datetime(2003, 9, 25)
}}}

Some simple examples based on the {{{date}}} command, using the
{{{TZOFFSET}}} dictionary to provide the BRST timezone offset.
{{{
>>> parse("Thu Sep 25 10:36:28 BRST 2003", tzinfos=TZOFFSETS)
datetime.datetime(2003, 9, 25, 10, 36, 28,
		  tzinfo=tzoffset('BRST', -10800))

>>> parse("2003 10:36:28 BRST 25 Sep Thu", tzinfos=TZOFFSETS)
datetime.datetime(2003, 9, 25, 10, 36, 28,
		  tzinfo=tzoffset('BRST', -10800))
}}}

Notice that since BRST is my local timezone, parsing it without
further timezone settings will yield a {{{tzlocal}}} timezone.
{{{
>>> parse("Thu Sep 25 10:36:28 BRST 2003")
datetime.datetime(2003, 9, 25, 10, 36, 28, tzinfo=tzlocal())
}}}

We can also ask to ignore the timezone explicitly:
{{{
>>> parse("Thu Sep 25 10:36:28 BRST 2003", ignoretz=True)
datetime.datetime(2003, 9, 25, 10, 36, 28)
}}}

That's the same as processing a string without timezone:
{{{
>>> parse("Thu Sep 25 10:36:28 2003")
datetime.datetime(2003, 9, 25, 10, 36, 28)
}}}

Without the year, but passing our {{{DEFAULT}}} datetime to return
the same year, no mattering what year we currently are in:
{{{
>>> parse("Thu Sep 25 10:36:28", default=DEFAULT)
datetime.datetime(2003, 9, 25, 10, 36, 28)
}}}

Strip it further:
{{{
>>> parse("Thu Sep 10:36:28", default=DEFAULT)
datetime.datetime(2003, 9, 25, 10, 36, 28)

>>> parse("Thu 10:36:28", default=DEFAULT)
datetime.datetime(2003, 9, 25, 10, 36, 28)

>>> parse("Thu 10:36", default=DEFAULT)
datetime.datetime(2003, 9, 25, 10, 36)

>>> parse("10:36", default=DEFAULT)
datetime.datetime(2003, 9, 25, 10, 36)
>>> 
}}}

Strip in a different way:
{{{
>>> parse("Thu Sep 25 2003")
datetime.datetime(2003, 9, 25, 0, 0)

>>> parse("Sep 25 2003")
datetime.datetime(2003, 9, 25, 0, 0)

>>> parse("Sep 2003", default=DEFAULT)
datetime.datetime(2003, 9, 25, 0, 0)

>>> parse("Sep", default=DEFAULT)
datetime.datetime(2003, 9, 25, 0, 0)

>>> parse("2003", default=DEFAULT)
datetime.datetime(2003, 9, 25, 0, 0)
}}}

Another format, based on {{{date -R}}} (RFC822):
{{{
>>> parse("Thu, 25 Sep 2003 10:49:41 -0300")
datetime.datetime(2003, 9, 25, 10, 49, 41,
		  tzinfo=tzoffset(None, -10800))
}}}

ISO format:
{{{
>>> parse("2003-09-25T10:49:41.5-03:00")
datetime.datetime(2003, 9, 25, 10, 49, 41, 500000,
		  tzinfo=tzoffset(None, -10800))
}}}

Some variations:
{{{
>>> parse("2003-09-25T10:49:41")
datetime.datetime(2003, 9, 25, 10, 49, 41)

>>> parse("2003-09-25T10:49")
datetime.datetime(2003, 9, 25, 10, 49)

>>> parse("2003-09-25T10")
datetime.datetime(2003, 9, 25, 10, 0)

>>> parse("2003-09-25")
datetime.datetime(2003, 9, 25, 0, 0)
}}}

ISO format, without separators:
{{{
>>> parse("20030925T104941.5-0300")
datetime.datetime(2003, 9, 25, 10, 49, 41, 500000,
		  tzinfo=tzinfo=tzoffset(None, -10800))

>>> parse("20030925T104941-0300")
datetime.datetime(2003, 9, 25, 10, 49, 41,
		  tzinfo=tzoffset(None, -10800))

>>> parse("20030925T104941")
datetime.datetime(2003, 9, 25, 10, 49, 41)

>>> parse("20030925T1049")
datetime.datetime(2003, 9, 25, 10, 49)

>>> parse("20030925T10")
datetime.datetime(2003, 9, 25, 10, 0)

>>> parse("20030925")
datetime.datetime(2003, 9, 25, 0, 0)
}}}

Everything together.
{{{
>>> parse("199709020900")
datetime.datetime(1997, 9, 2, 9, 0)
>>> parse("19970902090059")
datetime.datetime(1997, 9, 2, 9, 0, 59)
}}}

Different date orderings:
{{{
>>> parse("2003-09-25")
datetime.datetime(2003, 9, 25, 0, 0)

>>> parse("2003-Sep-25")
datetime.datetime(2003, 9, 25, 0, 0)

>>> parse("25-Sep-2003")
datetime.datetime(2003, 9, 25, 0, 0)

>>> parse("Sep-25-2003")
datetime.datetime(2003, 9, 25, 0, 0)

>>> parse("09-25-2003")
datetime.datetime(2003, 9, 25, 0, 0)

>>> parse("25-09-2003")
datetime.datetime(2003, 9, 25, 0, 0)
}}}

Check some ambiguous dates:
{{{
>>> parse("10-09-2003")
datetime.datetime(2003, 10, 9, 0, 0)

>>> parse("10-09-2003", dayfirst=True)
datetime.datetime(2003, 9, 10, 0, 0)

>>> parse("10-09-03")
datetime.datetime(2003, 10, 9, 0, 0)

>>> parse("10-09-03", yearfirst=True)
datetime.datetime(2010, 9, 3, 0, 0)
}}}

Other date separators are allowed:
{{{
>>> parse("2003.Sep.25")
datetime.datetime(2003, 9, 25, 0, 0)

>>> parse("2003/09/25")
datetime.datetime(2003, 9, 25, 0, 0)
}}}

Even with spaces:
{{{
>>> parse("2003 Sep 25")
datetime.datetime(2003, 9, 25, 0, 0)

>>> parse("2003 09 25")
datetime.datetime(2003, 9, 25, 0, 0)
}}}

Hours with letters work:
{{{
>>> parse("10h36m28.5s", default=DEFAULT)
datetime.datetime(2003, 9, 25, 10, 36, 28, 500000)

>>> parse("01s02h03m", default=DEFAULT)
datetime.datetime(2003, 9, 25, 2, 3, 1)

>>> parse("01h02m03", default=DEFAULT)
datetime.datetime(2003, 9, 3, 1, 2)

>>> parse("01h02", default=DEFAULT)
datetime.datetime(2003, 9, 2, 1, 0)

>>> parse("01h02s", default=DEFAULT)
datetime.datetime(2003, 9, 25, 1, 0, 2)
}}}

With AM/PM:
{{{
>>> parse("10h am", default=DEFAULT)
datetime.datetime(2003, 9, 25, 10, 0)

>>> parse("10pm", default=DEFAULT)
datetime.datetime(2003, 9, 25, 22, 0)

>>> parse("12:00am", default=DEFAULT)
datetime.datetime(2003, 9, 25, 0, 0)

>>> parse("12pm", default=DEFAULT)
datetime.datetime(2003, 9, 25, 12, 0)
}}}

Some special treating for ''pertain'' relations:
{{{
>>> parse("Sep 03", default=DEFAULT)
datetime.datetime(2003, 9, 3, 0, 0)

>>> parse("Sep of 03", default=DEFAULT)
datetime.datetime(2003, 9, 25, 0, 0)
}}}

Fuzzy parsing:
{{{
>>> s = "Today is 25 of September of 2003, exactly " \
...     "at 10:49:41 with timezone -03:00."
>>> parse(s, fuzzy=True)
datetime.datetime(2003, 9, 25, 10, 49, 41,
		  tzinfo=tzoffset(None, -10800))
}}}

Other random formats:
{{{
>>> parse("Wed, July 10, '96")
datetime.datetime(1996, 7, 10, 0, 0)

>>> parse("1996.07.10 AD at 15:08:56 PDT", ignoretz=True)
datetime.datetime(1996, 7, 10, 15, 8, 56)

>>> parse("Tuesday, April 12, 1952 AD 3:30:42pm PST", ignoretz=True)
datetime.datetime(1952, 4, 12, 15, 30, 42)

>>> parse("November 5, 1994, 8:15:30 am EST", ignoretz=True)
datetime.datetime(1994, 11, 5, 8, 15, 30)

>>> parse("3rd of May 2001")
datetime.datetime(2001, 5, 3, 0, 0)

>>> parse("5:50 A.M. on June 13, 1990")
datetime.datetime(1990, 6, 13, 5, 50)
}}}

=== easter ===
This module offers a generic easter computing method for
any given year, using Western, Orthodox or Julian algorithms.

==== easter() function ====
This method was ported from the work done by
[http://users.chariot.net.au/~gmarts/eastalg.htm GM Arts],
on top of the algorithm by
[http://www.tondering.dk/claus/calendar.html Claus Tondering],
which was based in part on the algorithm of Ouding (1940),
as quoted in "Explanatory Supplement to the Astronomical
Almanac", P.  Kenneth Seidelmann, editor.

This algorithm implements three different easter
calculation methods:

    1. Original calculation in Julian calendar, valid in
    dates after 326 AD
    1. Original method, with date converted to Gregorian
    calendar, valid in years 1583 to 4099
    1. Revised method, in Gregorian calendar, valid in
    years 1583 to 4099 as well

These methods are represented by the constants:
{{{
EASTER_JULIAN   = 1
EASTER_ORTHODOX = 2
EASTER_WESTERN  = 3
}}}

The default method is method 3.

=== tz ===
This module offers timezone implementations subclassing
the abstract {{{datetime.tzinfo}}} type. There are
classes to handle [http://www.twinsun.com/tz/tz-link.htm tzfile]
format files (usually are in /etc/localtime,
/usr/share/zoneinfo, etc), TZ environment string (in all
known formats), given ranges (with help from relative
deltas), local machine timezone, fixed offset timezone,
and UTC timezone.

==== tzutc type ====
This type implements a basic UTC timezone. The constructor of this
type accepts no parameters.

==== tzutc examples ====
{{{
>>> from datetime import *
>>> from dateutil.tz import *

>>> datetime.now()
datetime.datetime(2003, 9, 27, 9, 40, 1, 521290)

>>> datetime.now(tzutc())
datetime.datetime(2003, 9, 27, 12, 40, 12, 156379, tzinfo=tzutc())

>>> datetime.now(tzutc()).tzname()
'UTC'
}}}

==== tzoffset type ====
This type implements a fixed offset timezone, with no
support to daylight saving times. Here is the prototype of the
type constructor:
{{{
tzoffset(name, offset)
}}}

The {{{name}}} parameter may be optionally set to {{{None}}}, and
{{{offset}}} must be given in seconds.

==== tzoffset examples ====
{{{
>>> from datetime import *
>>> from dateutil.tz import *

>>> datetime.now(tzoffset("BRST", -10800))
datetime.datetime(2003, 9, 27, 9, 52, 43, 624904,
		  tzinfo=tzinfo=tzoffset('BRST', -10800))

>>> datetime.now(tzoffset("BRST", -10800)).tzname()
'BRST'

>>> datetime.now(tzoffset("BRST", -10800)).astimezone(tzutc())
datetime.datetime(2003, 9, 27, 12, 53, 11, 446419,
		  tzinfo=tzutc())
}}}

==== tzlocal type ====
This type implements timezone settings as known by the
operating system. The constructor of this type accepts no
parameters.

==== tzlocal examples ====
{{{
>>> from datetime import *
>>> from dateutil.tz import *

>>> datetime.now(tzlocal())
datetime.datetime(2003, 9, 27, 10, 1, 43, 673605,
		  tzinfo=tzlocal())

>>> datetime.now(tzlocal()).tzname()
'BRST'

>>> datetime.now(tzlocal()).astimezone(tzoffset(None, 0))
datetime.datetime(2003, 9, 27, 13, 3, 0, 11493,
		  tzinfo=tzoffset(None, 0))
}}}

==== tzstr type ====
This type implements timezone settings extracted from a
string in known TZ environment variable formats. Here is the prototype
of the constructor:
{{{
tzstr(str)
}}}

==== tzstr examples ====
Here are examples of the recognized formats:

  * {{{EST5EDT}}}
  * {{{EST5EDT,4,0,6,7200,10,0,26,7200,3600}}}
  * {{{EST5EDT,4,1,0,7200,10,-1,0,7200,3600}}}
  * {{{EST5EDT4,M4.1.0/02:00:00,M10-5-0/02:00}}}
  * {{{EST5EDT4,95/02:00:00,298/02:00}}}
  * {{{EST5EDT4,J96/02:00:00,J299/02:00}}}

Notice that if daylight information is not present, but a
daylight abbreviation was provided, {{{tzstr}}} will follow the
convention of using the first sunday of April to start daylight
saving, and the last sunday of October to end it. If start or
end time is not present, 2AM will be used, and if the daylight
offset is not present, the standard offset plus one hour will
be used. This convention is the same as used in the GNU libc.

This also means that some of the above examples are exactly
equivalent, and all of these examples are equivalent
in the year of 2003.

Here is the example mentioned in the
[http://www.python.org/doc/current/lib/module-time.html time module documentation].
{{{
>>> os.environ['TZ'] = 'EST+05EDT,M4.1.0,M10.5.0'
>>> time.tzset()
>>> time.strftime('%X %x %Z')
'02:07:36 05/08/03 EDT'
>>> os.environ['TZ'] = 'AEST-10AEDT-11,M10.5.0,M3.5.0'
>>> time.tzset()
>>> time.strftime('%X %x %Z')
'16:08:12 05/08/03 AEST'
}}}

And here is an example showing the same information using {{{tzstr}}},
without touching system settings.
{{{
>>> tz1 = tzstr('EST+05EDT,M4.1.0,M10.5.0')
>>> tz2 = tzstr('AEST-10AEDT-11,M10.5.0,M3.5.0')
>>> dt = datetime(2003, 5, 8, 2, 7, 36, tzinfo=tz1)
>>> dt.strftime('%X %x %Z')
'02:07:36 05/08/03 EDT'
>>> dt.astimezone(tz2).strftime('%X %x %Z')
'16:07:36 05/08/03 AEST'
}}}

Are these really equivalent?
{{{
>>> tzstr('EST5EDT') == tzstr('EST5EDT,4,1,0,7200,10,-1,0,7200,3600')
True
}}}

Check the daylight limit.
{{{
>>> datetime(2003, 4, 6, 1, 59, tzinfo=tz).tzname()
'EST'
>>> datetime(2003, 4, 6, 2, 00, tzinfo=tz).tzname()
'EDT'
>>> datetime(2003, 10, 26, 0, 59, tzinfo=tz).tzname()
'EDT'
>>> datetime(2003, 10, 26, 1, 00, tzinfo=tz).tzname()
'EST'
}}}  

==== tzrange type ====
This type offers the same functionality as the {{{tzstr}}} type, but
instead of timezone strings, information is passed using
{{{relativedelta}}}s which are applied to a datetime set to the first
day of the year. Here is the prototype of this type's constructor:
{{{
tzrange(stdabbr, stdoffset=None, dstabbr=None, dstoffset=None,
	start=None, end=None):
}}}

Offsets must be given in seconds. Information not provided will be
set to the defaults, as explained in the {{{tzstr}}} section above.

==== tzrange examples ====
{{{
>>> tzstr('EST5EDT') == tzrange("EST", -18000, "EDT")
True

>>> from dateutil.relativedelta import *
>>> range1 = tzrange("EST", -18000, "EDT")
>>> range2 = tzrange("EST", -18000, "EDT", -14400,
...                  relativedelta(hours=+2, month=4, day=1,
				   weekday=SU(+1)),
...                  relativedelta(hours=+1, month=10, day=31,
				   weekday=SU(-1)))
>>> tzstr('EST5EDT') == range1 == range2
True
}}}

Notice a minor detail in the last example: while the DST should end
at 2AM, the delta will catch 1AM. That's because the daylight saving
time should end at 2AM standard time (the difference between STD and
DST is 1h in the given example) instead of the DST time. That's how
the {{{tzinfo}}} subtypes should deal with the extra hour that happens
when going back to the standard time. Check
[http://www.python.org/doc/current/lib/datetime-tzinfo.html tzinfo documentation]
for more information.

==== tzfile type ====
This type allows one to use tzfile(5) format timezone files to extract
current and historical zone information. Here is the type constructor
prototype:
{{{
tzfile(fileobj)
}}}

Where {{{fileobj}}} is either a filename or a file-like object with
a {{{read()}}} method.

==== tzfile examples ====
{{{
>>> tz = tzfile("/etc/localtime")
>>> datetime.now(tz)
datetime.datetime(2003, 9, 27, 12, 3, 48, 392138,
		  tzinfo=tzfile('/etc/localtime'))

>>> datetime.now(tz).astimezone(tzutc())
datetime.datetime(2003, 9, 27, 15, 3, 53, 70863,
		  tzinfo=tzutc())

>>> datetime.now(tz).tzname()
'BRST'
>>> datetime(2003, 1, 1, tzinfo=tz).tzname()
'BRDT'
}}}

Check the daylight limit.
{{{
>>> tz = tzfile('/usr/share/zoneinfo/EST5EDT')
>>> datetime(2003, 4, 6, 1, 59, tzinfo=tz).tzname()
'EST'
>>> datetime(2003, 4, 6, 2, 00, tzinfo=tz).tzname()
'EDT'
>>> datetime(2003, 10, 26, 0, 59, tzinfo=tz).tzname()
'EDT'
>>> datetime(2003, 10, 26, 1, 00, tzinfo=tz).tzname()
'EST'
}}}  

==== tzical type ====
This type is able to parse
[ftp://ftp.rfc-editor.org/in-notes/rfc2445.txt iCalendar]
style {{{VTIMEZONE}}} sessions into a Python timezone object.
The constuctor prototype is:
{{{
tzical(fileobj)
}}}

Where {{{fileobj}}} is either a filename or a file-like object with
a {{{read()}}} method.

==== tzical methods ====

    tzical.get(tzid=None)::
    Since a single iCalendar file may contain more than one timezone,
    you must ask for the timezone you want with this method. If there's
    more than one timezone in the parsed file, you'll need to pass the
    {{{tzid}}} parameter. Otherwise, leaving it empty will yield the only
    available timezone.

==== tzical examples ====
Here is a sample file extracted from the RFC. This file defines
the {{{EST5EDT}}} timezone, and will be used in the following example.
{{{
BEGIN:VTIMEZONE
TZID:US-Eastern
LAST-MODIFIED:19870101T000000Z
TZURL:http://zones.stds_r_us.net/tz/US-Eastern
BEGIN:STANDARD
DTSTART:19671029T020000
RRULE:FREQ=YEARLY;BYDAY=-1SU;BYMONTH=10
TZOFFSETFROM:-0400
TZOFFSETTO:-0500
TZNAME:EST
END:STANDARD
BEGIN:DAYLIGHT
DTSTART:19870405T020000
RRULE:FREQ=YEARLY;BYDAY=1SU;BYMONTH=4
TZOFFSETFROM:-0500
TZOFFSETTO:-0400
TZNAME:EDT
END:DAYLIGHT
END:VTIMEZONE
}}}

And here is an example exploring a {{{tzical}}} type:
{{{
>>> from dateutil.tz import *; from datetime import *

>>> tz = tzical('EST5EDT.ics')
>>> tz.keys()
['US-Eastern']

>>> est = tz.get('US-Eastern')
>>> est
<tzicalvtz 'US-Eastern'>

>>> datetime.now(est)
datetime.datetime(2003, 10, 6, 19, 44, 18, 667987,
		  tzinfo=<tzicalvtz 'US-Eastern'>)

>>> est == tz.get()
True
}}}

Let's check the daylight ranges, as usual:
{{{
>>> datetime(2003, 4, 6, 1, 59, tzinfo=est).tzname()
'EST'
>>> datetime(2003, 4, 6, 2, 00, tzinfo=est).tzname()
'EDT'

>>> datetime(2003, 10, 26, 0, 59, tzinfo=est).tzname()
'EDT'
>>> datetime(2003, 10, 26, 1, 00, tzinfo=est).tzname()
'EST'
}}}

==== tzwin type ====
This type offers access to internal registry-based Windows timezones.
The constuctor prototype is:
{{{
tzwin(name)
}}}

Where {{{name}}} is the timezone name. There's a static {{{tzwin.list()}}}
method to check the available names,

==== tzwin methods ====

    tzwin.display()::
    This method returns the timezone extended name.

    tzwin.list()::
    This static method lists all available timezone names.

==== tzwin examples ====
{{{
>>> tz = tzwin("E. South America Standard Time")
}}}

==== tzwinlocal type ====
This type offers access to internal registry-based Windows timezones.
The constructor accepts no parameters, so the prototype is:
{{{
tzwinlocal()
}}}

==== tzwinlocal methods ====

    tzwinlocal.display()::
    This method returns the timezone extended name, and returns
    {{{None}}} if one is not available.

==== tzwinlocal examples ====
{{{
>>> tz = tzwinlocal()
}}}

==== gettz() function ====
This function is a helper that will try its best to get the right
timezone for your environment, or for the given string. The prototype
is as follows:
{{{
gettz(name=None)
}}}

If given, the parameter may be a filename, a path relative to the base
of the timezone information path (the base could be
{{{/usr/share/zoneinfo}}}, for example), a string timezone
specification, or a timezone abbreviation. If {{{name}}} is not given,
and the {{{TZ}}} environment variable is set, it's used instead. If the
parameter is not given, and {{{TZ}}} is not set, the default tzfile
paths will be tried. Then, if no timezone information is found,
an internal compiled database of timezones is used. When running
on Windows, the internal registry-based Windows timezones are also
considered.

Example:
{{{
>>> from dateutil.tz import *
>>> gettz()
tzfile('/etc/localtime')

>>> gettz("America/Sao Paulo")
tzfile('/usr/share/zoneinfo/America/Sao_Paulo')

>>> gettz("EST5EDT")
tzfile('/usr/share/zoneinfo/EST5EDT')

>>> gettz("EST5")
tzstr('EST5')

>>> gettz('BRST')
tzlocal()

>>> os.environ["TZ"] = "America/Sao Paulo"
>>> gettz()
tzfile('/usr/share/zoneinfo/America/Sao_Paulo')

>>> os.environ["TZ"] = "BRST"
>>> gettz()
tzlocal()

>>> gettz("Unavailable")
>>> 
}}}

=== zoneinfo ===
This module provides direct access to the internal compiled
database of timezones. The timezone data and the compiling tools
are obtained from the following project:

  http://www.twinsun.com/tz/tz-link.htm

==== gettz() function ====
This function will try to retrieve the given timezone information
from the internal compiled database, and will cache its results.

Example:
{{{
>>> from dateutil import zoneinfo
>>> zoneinfo.gettz("Brazil/East")
tzfile('Brazil/East')
}}}

## vim:ft=moin

The sample scripts herein require module pyasn1.

See http://pyasn1.sourceforge.net/

---------------------------------------
python-ldap: LDAP client API for Python
---------------------------------------

What is python-ldap?

python-ldap provides an object-oriented API to access LDAP
directory servers from Python programs. Mainly it wraps the
OpenLDAP client libs for that purpose.

Additionally the package contains modules for other LDAP-related
stuff (e.g. processing LDIF, LDAPURLs, LDAPv3 sub-schema, etc.).

Not included: Direct BER support

See INSTALL for version compability

See TODO for planned features. Contributors welcome.

For module documentation, see:

	http://www.python-ldap.org/

Quick usage example:
    import ldap
    l = ldap.initialize("ldap://my_ldap_server.my_domain")
    l.simple_bind_s("","")
    l.search_s("o=My Organisation, c=AU", ldap.SCOPE_SUBTREE, "objectclass=*")

See directory Demo/ of source distribution package for more
example code.

Author(s) contact:
   http://www.python-ldap.org/

   If you are looking for help, please try the mailing list archives
   first, then send a question to the mailing list.
   Be warned that questions will be ignored if they can be
   trivially answered by referring to the documentation.

   If you are interested in helping, please contact the mailing list.
   If you want new features or upgrades, please check the mailing list
   archives and then enquire about any progress.

Acknowledgements:

   Thanks to Konstantin Chuguev <Konstantin.Chuguev at dante.org.uk>
   and Steffen Ries <steffen.ries at sympatico.ca> for working
   on support for OpenLDAP 2.0.x features.

   Thanks to Michael Stroeder <michael at stroeder.com> for the 
   modules ldif, ldapurl, ldap/schema/*.py, ldap/*.py.

   Thanks to Hans Aschauer <Hans.Aschauer at Physik.uni-muenchen.de>
   for the C wrapper schema and SASL support.

   Thanks to Mauro Cicognini <mcicogni at siosistemi.it> for the 
   WIN32/MSVC6 bits, and the pre-built WIN32 ldap.pyd.
   
   Thanks to Waldemar Osuch <waldemar.osuch at gmail.com> for contributing
   the new-style docs based on reStructuredText.

   Thanks to Torsten Kurbad <torsten at tk-webart.de> for the
   easy_install support.

   Thanks to James Andrewartha <jamesa at daa.com.au> for
   significant contribution to Doc/*.tex.
   
   These very kind people have supplied patches or suggested changes:

       Federico Di Gregorio <fog at mixadlive.com>
       John Benninghoff <johnb at netscape.com>
       Donn Cave <donn at u.washington.edu>
       Jason Gunthorpe <jgg at debian.org>
       gurney_j <gurney_j at 4j.lane.edu>
       Eric S. Johansson <esj at harvee.billerica.ma.us>
       David Margrave <davidma at premier1.net>
       Uche Ogbuji <uche.ogbuji at fourthought.com>
       Neale Pickett <neale at lanl.gov>
       Blake Weston <weston at platinum1.cambridge.scr.slb.com>
       Wido Depping <wido.depping at gmail.com>
       Deepak Giridharagopal <deepak at arlut.utexas.edu>
       Ingo Steuwer <steuwer at univention.de>
       Andreas Hasenack <ahasenack at terra.com.br>       
       Matej Vela <vela at debian.org>

   Thanks to all the guys on the python-ldap-dev mailing list for
   their contributions and input into this package.

   Thanks! We may have missed someone: please mail us if we have omitted
   your name.

$Id: README,v 1.19 2009/07/26 11:07:29 stroeder Exp $

# Overview

This code was originally forked from [Leah Culver and Andy Smith's oauth.py code](http://github.com/leah/python-oauth/). Some of the tests come from a [fork by Vic Fryzel](http://github.com/shellsage/python-oauth), while a revamped Request class and more tests were merged in from [Mark Paschal's fork](http://github.com/markpasc/python-oauth). A number of notable differences exist between this code and its forefathers:

* 100% unit test coverage.
* The <code>DataStore</code> object has been completely ripped out. While creating unit tests for the library I found several substantial bugs with the implementation and confirmed with Andy Smith that it was never fully baked.
* Classes are no longer prefixed with <code>OAuth</code>.
* The <code>Request</code> class now extends from <code>dict</code>.
* The library is likely no longer compatible with Python 2.3.
* The <code>Client</code> class works and extends from <code>httplib2</code>. It's a thin wrapper that handles automatically signing any normal HTTP request you might wish to make.

# Signing a Request

    import oauth2 as oauth
    import time
    
    # Set the API endpoint 
    url = "http://example.com/photos"
    
    # Set the base oauth_* parameters along with any other parameters required
    # for the API call.
    params = {
        'oauth_version': "1.0",
        'oauth_nonce': oauth.generate_nonce(),
        'oauth_timestamp': int(time.time())
        'user': 'joestump',
        'photoid': 555555555555
    }
    
    # Set up instances of our Token and Consumer. The Consumer.key and 
    # Consumer.secret are given to you by the API provider. The Token.key and
    # Token.secret is given to you after a three-legged authentication.
    token = oauth.Token(key="tok-test-key", secret="tok-test-secret")
    consumer = oauth.Consumer(key="con-test-key", secret="con-test-secret")
    
    # Set our token/key parameters
    params['oauth_token'] = token.key
    params['oauth_consumer_key'] = consumer.key
    
    # Create our request. Change method, etc. accordingly.
    req = oauth.Request(method="GET", url=url, parameters=params)
    
    # Sign the request.
    signature_method = oauth.SignatureMethod_HMAC_SHA1()
    req.sign_request(signature_method, consumer, token)

# Using the Client

The <code>oauth2.Client</code> is based on <code>httplib2</code> and works just as you'd expect it to. The only difference is the first two arguments to the constructor are an instance of <code>oauth2.Consumer</code> and <code>oauth2.Token</code> (<code>oauth2.Token</code> is only needed for three-legged requests).

    import oauth2 as oauth
    
    # Create your consumer with the proper key/secret.
    consumer = oauth.Consumer(key="your-twitter-consumer-key", 
        secret="your-twitter-consumer-secret")
    
    # Request token URL for Twitter.
    request_token_url = "http://twitter.com/oauth/request_token"
    
    # Create our client.
    client = oauth.Client(consumer)
    
    # The OAuth Client request works just like httplib2 for the most part.
    resp, content = client.request(request_token_url, "GET")
    print resp
    print content

# Twitter Three-legged OAuth Example

Below is an example of how one would go through a three-legged OAuth flow to
gain access to protected resources on Twitter. This is a simple CLI script, but
can be easily translated to a web application.

    import urlparse
    import oauth2 as oauth
    
    consumer_key = 'my_key_from_twitter'
    consumer_secret = 'my_secret_from_twitter'
    
    request_token_url = 'http://twitter.com/oauth/request_token'
    access_token_url = 'http://twitter.com/oauth/access_token'
    authorize_url = 'http://twitter.com/oauth/authorize'
    
    consumer = oauth.Consumer(consumer_key, consumer_secret)
    client = oauth.Client(consumer)
    
    # Step 1: Get a request token. This is a temporary token that is used for 
    # having the user authorize an access token and to sign the request to obtain 
    # said access token.
    
    resp, content = client.request(request_token_url, "GET")
    if resp['status'] != '200':
        raise Exception("Invalid response %s." % resp['status'])
    
    request_token = dict(urlparse.parse_qsl(content))
    
    print "Request Token:"
    print "    - oauth_token        = %s" % request_token['oauth_token']
    print "    - oauth_token_secret = %s" % request_token['oauth_token_secret']
    print 
    
    # Step 2: Redirect to the provider. Since this is a CLI script we do not 
    # redirect. In a web application you would redirect the user to the URL
    # below.
    
    print "Go to the following link in your browser:"
    print "%s?oauth_token=%s" % (authorize_url, request_token['oauth_token'])
    print 
    
    # After the user has granted access to you, the consumer, the provider will
    # redirect you to whatever URL you have told them to redirect to. You can 
    # usually define this in the oauth_callback argument as well.
    accepted = 'n'
    while accepted.lower() == 'n':
        accepted = raw_input('Have you authorized me? (y/n) ')
    oauth_verifier = raw_input('What is the PIN? ')
    
    # Step 3: Once the consumer has redirected the user back to the oauth_callback
    # URL you can request the access token the user has approved. You use the 
    # request token to sign this request. After this is done you throw away the
    # request token and use the access token returned. You should store this 
    # access token somewhere safe, like a database, for future use.
    token = oauth.Token(request_token['oauth_token'],
        request_token['oauth_token_secret'])
    token.set_verifier(oauth_verifier)
    client = oauth.Client(consumer, token)
    
    resp, content = client.request(access_token_url, "POST")
    access_token = dict(urlparse.parse_qsl(content))
    
    print "Access Token:"
    print "    - oauth_token        = %s" % access_token['oauth_token']
    print "    - oauth_token_secret = %s" % access_token['oauth_token_secret']
    print
    print "You may now access protected resources using the access tokens above." 
    print

# Logging into Django w/ Twitter

Twitter also has the ability to authenticate a user [via an OAuth flow](http://apiwiki.twitter.com/Sign-in-with-Twitter). This
flow is exactly like the three-legged OAuth flow, except you send them to a 
slightly different URL to authorize them. 

In this example we'll look at how you can implement this login flow using 
Django and python-oauth2. 

## Set up a Profile model

You'll need a place to store all of your Twitter OAuth credentials after the
user has logged in. In your app's `models.py` file you should add something
that resembles the following model.

    class Profile(models.Model):
        user = models.ForeignKey(User)
        oauth_token = models.CharField(max_length=200)
        oauth_secret = models.CharField(max_length=200)

## Set up your Django views

### `urls.py`

Your `urls.py` should look something like the following. Basically, you need to
have a login URL, a callback URL that Twitter will redirect your users back to,
and a logout URL.

In this example `^login/` and `twitter_login` will send the user to Twitter to
be logged in, `^login/authenticated/` and `twitter_authenticated` will confirm
the login, create the account if necessary, and log the user into the 
application, and `^logout`/ logs the user out in the `twitter_logout` view.


    from django.conf.urls.defaults import *
    from django.contrib import admin
    from mytwitterapp.views import twitter_login, twitter_logout, \
        twitter_authenticated

    admin.autodiscover()

    urlpatterns = patterns('',
        url(r'^admin/doc/', include('django.contrib.admindocs.urls')),
        url(r'^admin/', include(admin.site.urls)),
        url(r'^login/?$', twitter_login),
        url(r'^logout/?$', twitter_logout),
        url(r'^login/authenticated/?$', twitter_authenticated),
    )

### `views.py`

*NOTE:* The following code was coded for Python 2.4 so some of the libraries 
and code here might need to be updated if you are using Python 2.6+. 

    # Python
    import oauth2 as oauth
    import cgi

    # Django
    from django.shortcuts import render_to_response
    from django.http import HttpResponseRedirect
    from django.conf import settings
    from django.contrib.auth import authenticate, login, logout
    from django.contrib.auth.models import User
    from django.contrib.auth.decorators import login_required

    # Project
    from mytwitterapp.models import Profile

    # It's probably a good idea to put your consumer's OAuth token and
    # OAuth secret into your project's settings. 
    consumer = oauth.Consumer(settings.TWITTER_TOKEN, settings.TWITTER_SECRET)
    client = oauth.Client(consumer)

    request_token_url = 'http://twitter.com/oauth/request_token'
    access_token_url = 'http://twitter.com/oauth/access_token'

    # This is the slightly different URL used to authenticate/authorize.
    authenticate_url = 'http://twitter.com/oauth/authenticate'

    def twitter_login(request):
        # Step 1. Get a request token from Twitter.
        resp, content = client.request(request_token_url, "GET")
        if resp['status'] != '200':
            raise Exception("Invalid response from Twitter.")

        # Step 2. Store the request token in a session for later use.
        request.session['request_token'] = dict(cgi.parse_qsl(content))

        # Step 3. Redirect the user to the authentication URL.
        url = "%s?oauth_token=%s" % (authenticate_url,
            request.session['request_token']['oauth_token'])

        return HttpResponseRedirect(url)

    
    @login_required
    def twitter_logout(request):
        # Log a user out using Django's logout function and redirect them
        # back to the homepage.
        logout(request)
        return HttpResponseRedirect('/')

    def twitter_authenticated(request):
        # Step 1. Use the request token in the session to build a new client.
        token = oauth.Token(request.session['request_token']['oauth_token'],
            request.session['request_token']['oauth_token_secret'])
        client = oauth.Client(consumer, token)
    
        # Step 2. Request the authorized access token from Twitter.
        resp, content = client.request(access_token_url, "GET")
        if resp['status'] != '200':
            print content
            raise Exception("Invalid response from Twitter.")
    
        """
        This is what you'll get back from Twitter. Note that it includes the
        user's user_id and screen_name.
        {
            'oauth_token_secret': 'IcJXPiJh8be3BjDWW50uCY31chyhsMHEhqJVsphC3M',
            'user_id': '120889797', 
            'oauth_token': '120889797-H5zNnM3qE0iFoTTpNEHIz3noL9FKzXiOxwtnyVOD',
            'screen_name': 'heyismysiteup'
        }
        """
        access_token = dict(cgi.parse_qsl(content))
    
        # Step 3. Lookup the user or create them if they don't exist.
        try:
            user = User.objects.get(username=access_token['screen_name'])
        except User.DoesNotExist:
            # When creating the user I just use their screen_name@twitter.com
            # for their email and the oauth_token_secret for their password.
            # These two things will likely never be used. Alternatively, you 
            # can prompt them for their email here. Either way, the password 
            # should never be used.
            user = User.objects.create_user(access_token['screen_name'],
                '%s@twitter.com' % access_token['screen_name'],
                access_token['oauth_token_secret'])
    
            # Save our permanent token and secret for later.
            profile = Profile()
            profile.user = user
            profile.oauth_token = access_token['oauth_token']
            profile.oauth_secret = access_token['oauth_token_secret']
            profile.save()
    
        # Authenticate the user and log them in using Django's pre-built 
        # functions for these things.
        user = authenticate(username=access_token['screen_name'],
            password=access_token['oauth_token_secret'])
        login(request, user)
    
        return HttpResponseRedirect('/')
    

### `settings.py`

* You'll likely want to set `LOGIN_URL` to `/login/` so that users are properly redirected to your Twitter login handler when you use `@login_required` in other parts of your Django app.
* You can also set `AUTH_PROFILE_MODULE = 'mytwitterapp.Profile'` so that you can easily access the Twitter OAuth token/secret for that user using the `User.get_profile()` method in Django.

# XOAUTH for IMAP and SMTP

Gmail supports OAuth over IMAP and SMTP via a standard they call XOAUTH. This allows you to authenticate against Gmail's IMAP and SMTP servers using an OAuth token and secret. It also has the added benefit of allowing you to use vanilla SMTP and IMAP libraries. The `python-oauth2` package provides both IMAP and SMTP libraries that implement XOAUTH and wrap `imaplib.IMAP4_SSL` and `smtplib.SMTP`. This allows you to connect to Gmail with OAuth credentials using standard Python libraries. 

## IMAP

    import oauth2 as oauth
    import oauth2.clients.imap as imaplib

    # Set up your Consumer and Token as per usual. Just like any other
    # three-legged OAuth request.
    consumer = oauth.Consumer('your_consumer_key', 'your_consumer_secret')
    token = oauth.Token('your_users_3_legged_token', 
        'your_users_3_legged_token_secret')

    # Setup the URL according to Google's XOAUTH implementation. Be sure
    # to replace the email here with the appropriate email address that
    # you wish to access.
    url = "https://mail.google.com/mail/b/your_users_email@gmail.com/imap/"

    conn = imaplib.IMAP4_SSL('imap.googlemail.com')
    conn.debug = 4 

    # This is the only thing in the API for impaplib.IMAP4_SSL that has 
    # changed. You now authenticate with the URL, consumer, and token.
    conn.authenticate(url, consumer, token)

    # Once authenticated everything from the impalib.IMAP4_SSL class will 
    # work as per usual without any modification to your code.
    conn.select('INBOX')
    print conn.list()


## SMTP

    import oauth2 as oauth
    import oauth2.clients.smtp as smtplib

    # Set up your Consumer and Token as per usual. Just like any other
    # three-legged OAuth request.
    consumer = oauth.Consumer('your_consumer_key', 'your_consumer_secret')
    token = oauth.Token('your_users_3_legged_token', 
        'your_users_3_legged_token_secret')

    # Setup the URL according to Google's XOAUTH implementation. Be sure
    # to replace the email here with the appropriate email address that
    # you wish to access.
    url = "https://mail.google.com/mail/b/your_users_email@gmail.com/smtp/"

    conn = smtplib.SMTP('smtp.googlemail.com', 587)
    conn.set_debuglevel(True)
    conn.ehlo('test')
    conn.starttls()

    # Again the only thing modified from smtplib.SMTP is the authenticate
    # method, which works identically to the imaplib.IMAP4_SSL method.
    conn.authenticate(url, consumer, token)




DJANGO EXAMPLE PACKAGE
======================

This package implements an example consumer and server for the Django
Python web framework.  You can get Django (and learn more about it) at

  http://www.djangoproject.com/

SETUP
=====

 1. Install the OpenID library, version 2.0.0 or later.

 2. Install Django 0.95.1.

    If you find that the examples run on even newer versions of
    Django, please let us know!

 3. Modify djopenid/settings.py appropriately; you may wish to change
    the database type or path, although the default settings should be
    sufficient for most systems.

 4. In examples/djopenid/ run:

    python manage.py syncdb

 5. To run the example consumer or server, run

    python manage.py runserver PORT

    where PORT is the port number on which to listen.

    Note that if you want to try both the consumer and server at the
    same time, run the command twice with two different values for
    PORT.

 6. Point your web browser at the server at

    http://localhost:PORT/

    to begin.

ABOUT THE CODE
==============

The example server and consumer code provided in this package are
intended to be instructional in the use of this OpenID library.  While
it is not recommended to use the example code in production, the code
should be sufficient to explain the general use of the library.

If you aren't familiar with the Django web framework, you can quickly
start looking at the important code by looking in the 'views' modules:

  djopenid.consumer.views
  djopenid.server.views

Each view is a python callable that responds to an HTTP request.
Regardless of whether you use a framework, your application should
look similar to these example applications.

CONTACT
=======

Please send bug reports, patches, and other feedback to

  http://openid.net/developers/dev-mailing-lists/

Python OpenID library example code
==================================

The examples directory contains working code illustrating the use of
the library for performing OpenID authentication, both as a consumer
and a server. There are two kinds of examples, one that can run
without any external dependencies, and one that uses the Django Web
framework. The examples do not illustrate how to use all of the
features of the library, but they should be a good starting point to
see how to use this library with your code.

Both the Django libraries and the BaseHTTPServer examples require that
the OpenID library is installed or that it has been added to Python's
search path (PYTHONPATH environment variable or sys.path).

The Django example is probably a good place to start reading the
code. There is little that is Django-specific about the OpenID logic
in the example, and it should be easy to port to any framework. To run
the django examples, see the README file in the djopenid subdirectory.

The other examples use Python's built-in BaseHTTPServer and have a
good deal of ad-hoc dispatching and rendering code mixed in

Using the BaseHTTPServer examples
=================================

This directory contains a working server and consumer that use this
OpenID library. They are both written using python's standard
BaseHTTPServer.


To run the example system:

1. Make sure you've installed the library, as explained in the
   installation instructions.

2. Start the consumer server:

        python consumer.py --port 8001


3. In another terminal, start the identity server:

        python server.py --port 8000

   (Hit Ctrl-C in either server's window to stop that server.)


4. Open your web broswer, and go to the consumer server:

        http://localhost:8001/

   Note that all pages the consumer server shows will have "Python OpenID
   Consumer Example" across the top.


5. Enter an identity url managed by the sample identity server:

        http://localhost:8000/id/bob


6. The browser will be redirected to the sample server, which will be
   requesting that you log in to proceed.  Enter the username for the
   identity URL into the login box:

        bob

   Note that all pages the identity server shows will have "Python
   OpenID Server Example" across the top.


7. After you log in as bob, the server example will ask you if you
   want to allow http://localhost:8001/ to know your identity.  Say
   yes.


8. You should end up back on the consumer site, at a page indicating
   you've logged in successfully.


That's a basic OpenID login procedure.  You can continue through it,
playing with variations to see how they work.  The python code is
intended to be a straightforward example of how to use the python
OpenID library to function as either an identity server or consumer.

Getting help
============

Please send bug reports, patches, and other feedback to

  http://openid.net/developers/dev-mailing-lists/

delegated-20060809.xrds    - results from proxy.xri.net, determined by 
                             Drummond and Kevin to be incorrect.
delegated-20060809-r1.xrds - Drummond's 1st correction
delegated-20060809-r2.xrds - Drummond's 2nd correction

spoofs: keturn's (=!E4)'s attempts to log in with Drummond's i-number (=!D2)
spoof1.xrds
spoof2.xrds
spoof3.xrds - attempt to steal @!C0!D2 by having "at least one" CanonicalID
    match the $res service ProviderID.

ref.xrds - resolving @ootao*test.ref, which refers to a neustar XRI.

This is the Python OpenID library.

REQUIREMENTS
============

 - Python 2.3, 2.4, or 2.5.

 - ElementTree.  This is included in the Python 2.5 standard library,
   but users of earlier versions of Python may need to install it
   seperately.

 - pycrypto, if on Python 2.3 and without /dev/urandom, or on Python
   2.3 or 2.4 and you want SHA256.


INSTALLATION
============

To install the base library, just run the following command:

python setup.py install

To run setup.py you need the distutils module from the Python standard
library; some distributions package this seperately in a "python-dev"
package.


GETTING STARTED
===============

The examples directory includes an example server and consumer
implementation.  See the README file in that directory for more
information on running the examples.

Library documentation is available in html form in the doc directory.


LOGGING
=======

This library offers a logging hook that will record unexpected
conditions that occur in library code. If a condition is recoverable,
the library will recover and issue a log message. If it is not
recoverable, the library will raise an exception. See the
documentation for the openid.oidutil module for more on the logging
hook.


DOCUMENTATION
=============

The documentation in this library is in Epydoc format, which is
detailed at:

  http://epydoc.sourceforge.net/


CONTACT
=======

Send bug reports, suggestions, comments, and questions to
http://openid.net/developers/dev-mailing-lists/.

If you have a bugfix or feature you'd like to contribute, don't
hesitate to send it to us.  For more detailed information on how to
contribute, see

  http://openidenabled.com/contribute/

For documentation, see docs/html/index.html in this distribution, or
http://countergram.com/open-source/pytidylib/

Small example of use:

from tidylib import tidy_document
document, errors = tidy_document('''<p>f&otilde;o <img src="bar.jpg">''',
    options={'numeric-entities':1})
print document
print errors

pytz - World Timezone Definitions for Python
============================================

:Author: Stuart Bishop <stuart@stuartbishop.net>

Introduction
~~~~~~~~~~~~

pytz brings the Olson tz database into Python. This library allows
accurate and cross platform timezone calculations using Python 2.4
or higher. It also solves the issue of ambiguous times at the end
of daylight savings, which you can read more about in the Python
Library Reference (``datetime.tzinfo``).

Almost all of the Olson timezones are supported.

.. note::

    This library differs from the documented Python API for
    tzinfo implementations; if you want to create local wallclock
    times you need to use the ``localize()`` method documented in this
    document. In addition, if you perform date arithmetic on local
    times that cross DST boundaries, the result may be in an incorrect
    timezone (ie. subtract 1 minute from 2002-10-27 1:00 EST and you get
    2002-10-27 0:59 EST instead of the correct 2002-10-27 1:59 EDT). A
    ``normalize()`` method is provided to correct this. Unfortunately these
    issues cannot be resolved without modifying the Python datetime
    implementation (see PEP-431).


Installation
~~~~~~~~~~~~

This package can either be installed from a .egg file using setuptools,
or from the tarball using the standard Python distutils.

If you are installing from a tarball, run the following command as an
administrative user::

    python setup.py install

If you are installing using setuptools, you don't even need to download
anything as the latest version will be downloaded for you
from the Python package index::

    easy_install --upgrade pytz

If you already have the .egg file, you can use that too::

    easy_install pytz-2008g-py2.6.egg


Example & Usage
~~~~~~~~~~~~~~~

Localized times and date arithmetic
-----------------------------------

>>> from datetime import datetime, timedelta
>>> from pytz import timezone
>>> import pytz
>>> utc = pytz.utc
>>> utc.zone
'UTC'
>>> eastern = timezone('US/Eastern')
>>> eastern.zone
'US/Eastern'
>>> amsterdam = timezone('Europe/Amsterdam')
>>> fmt = '%Y-%m-%d %H:%M:%S %Z%z'

This library only supports two ways of building a localized time. The
first is to use the ``localize()`` method provided by the pytz library.
This is used to localize a naive datetime (datetime with no timezone
information):

>>> loc_dt = eastern.localize(datetime(2002, 10, 27, 6, 0, 0))
>>> print(loc_dt.strftime(fmt))
2002-10-27 06:00:00 EST-0500

The second way of building a localized time is by converting an existing
localized time using the standard ``astimezone()`` method:

>>> ams_dt = loc_dt.astimezone(amsterdam)
>>> ams_dt.strftime(fmt)
'2002-10-27 12:00:00 CET+0100'

Unfortunately using the tzinfo argument of the standard datetime
constructors ''does not work'' with pytz for many timezones.

>>> datetime(2002, 10, 27, 12, 0, 0, tzinfo=amsterdam).strftime(fmt)
'2002-10-27 12:00:00 AMT+0020'

It is safe for timezones without daylight savings trasitions though, such
as UTC:

>>> datetime(2002, 10, 27, 12, 0, 0, tzinfo=pytz.utc).strftime(fmt)
'2002-10-27 12:00:00 UTC+0000'

The preferred way of dealing with times is to always work in UTC,
converting to localtime only when generating output to be read
by humans.

>>> utc_dt = datetime(2002, 10, 27, 6, 0, 0, tzinfo=utc)
>>> loc_dt = utc_dt.astimezone(eastern)
>>> loc_dt.strftime(fmt)
'2002-10-27 01:00:00 EST-0500'

This library also allows you to do date arithmetic using local
times, although it is more complicated than working in UTC as you
need to use the ``normalize()`` method to handle daylight savings time
and other timezone transitions. In this example, ``loc_dt`` is set
to the instant when daylight savings time ends in the US/Eastern
timezone.

>>> before = loc_dt - timedelta(minutes=10)
>>> before.strftime(fmt)
'2002-10-27 00:50:00 EST-0500'
>>> eastern.normalize(before).strftime(fmt)
'2002-10-27 01:50:00 EDT-0400'
>>> after = eastern.normalize(before + timedelta(minutes=20))
>>> after.strftime(fmt)
'2002-10-27 01:10:00 EST-0500'

Creating local times is also tricky, and the reason why working with
local times is not recommended. Unfortunately, you cannot just pass
a ``tzinfo`` argument when constructing a datetime (see the next
section for more details)

>>> dt = datetime(2002, 10, 27, 1, 30, 0)
>>> dt1 = eastern.localize(dt, is_dst=True)
>>> dt1.strftime(fmt)
'2002-10-27 01:30:00 EDT-0400'
>>> dt2 = eastern.localize(dt, is_dst=False)
>>> dt2.strftime(fmt)
'2002-10-27 01:30:00 EST-0500'

Converting between timezones also needs special attention. We also need
to use the ``normalize()`` method to ensure the conversion is correct.

>>> utc_dt = utc.localize(datetime.utcfromtimestamp(1143408899))
>>> utc_dt.strftime(fmt)
'2006-03-26 21:34:59 UTC+0000'
>>> au_tz = timezone('Australia/Sydney')
>>> au_dt = au_tz.normalize(utc_dt.astimezone(au_tz))
>>> au_dt.strftime(fmt)
'2006-03-27 08:34:59 EST+1100'
>>> utc_dt2 = utc.normalize(au_dt.astimezone(utc))
>>> utc_dt2.strftime(fmt)
'2006-03-26 21:34:59 UTC+0000'

You can take shortcuts when dealing with the UTC side of timezone
conversions. ``normalize()`` and ``localize()`` are not really
necessary when there are no daylight savings time transitions to
deal with.

>>> utc_dt = datetime.utcfromtimestamp(1143408899).replace(tzinfo=utc)
>>> utc_dt.strftime(fmt)
'2006-03-26 21:34:59 UTC+0000'
>>> au_tz = timezone('Australia/Sydney')
>>> au_dt = au_tz.normalize(utc_dt.astimezone(au_tz))
>>> au_dt.strftime(fmt)
'2006-03-27 08:34:59 EST+1100'
>>> utc_dt2 = au_dt.astimezone(utc)
>>> utc_dt2.strftime(fmt)
'2006-03-26 21:34:59 UTC+0000'


``tzinfo`` API
--------------

The ``tzinfo`` instances returned by the ``timezone()`` function have
been extended to cope with ambiguous times by adding an ``is_dst``
parameter to the ``utcoffset()``, ``dst()`` && ``tzname()`` methods.

>>> tz = timezone('America/St_Johns')

>>> normal = datetime(2009, 9, 1)
>>> ambiguous = datetime(2009, 10, 31, 23, 30)

The ``is_dst`` parameter is ignored for most timestamps. It is only used
during DST transition ambiguous periods to resulve that ambiguity.

>>> tz.utcoffset(normal, is_dst=True)
datetime.timedelta(-1, 77400)
>>> tz.dst(normal, is_dst=True)
datetime.timedelta(0, 3600)
>>> tz.tzname(normal, is_dst=True)
'NDT'

>>> tz.utcoffset(ambiguous, is_dst=True)
datetime.timedelta(-1, 77400)
>>> tz.dst(ambiguous, is_dst=True)
datetime.timedelta(0, 3600)
>>> tz.tzname(ambiguous, is_dst=True)
'NDT'

>>> tz.utcoffset(normal, is_dst=False)
datetime.timedelta(-1, 77400)
>>> tz.dst(normal, is_dst=False)
datetime.timedelta(0, 3600)
>>> tz.tzname(normal, is_dst=False)
'NDT'

>>> tz.utcoffset(ambiguous, is_dst=False)
datetime.timedelta(-1, 73800)
>>> tz.dst(ambiguous, is_dst=False)
datetime.timedelta(0)
>>> tz.tzname(ambiguous, is_dst=False)
'NST'

If ``is_dst`` is not specified, ambiguous timestamps will raise
an ``pytz.exceptions.AmbiguousTimeError`` exception.

>>> tz.utcoffset(normal)
datetime.timedelta(-1, 77400)
>>> tz.dst(normal)
datetime.timedelta(0, 3600)
>>> tz.tzname(normal)
'NDT'

>>> import pytz.exceptions
>>> try:
...     tz.utcoffset(ambiguous)
... except pytz.exceptions.AmbiguousTimeError:
...     print('pytz.exceptions.AmbiguousTimeError: %s' % ambiguous)
pytz.exceptions.AmbiguousTimeError: 2009-10-31 23:30:00
>>> try:
...     tz.dst(ambiguous)
... except pytz.exceptions.AmbiguousTimeError:
...     print('pytz.exceptions.AmbiguousTimeError: %s' % ambiguous)
pytz.exceptions.AmbiguousTimeError: 2009-10-31 23:30:00
>>> try:
...     tz.tzname(ambiguous)
... except pytz.exceptions.AmbiguousTimeError:
...     print('pytz.exceptions.AmbiguousTimeError: %s' % ambiguous)
pytz.exceptions.AmbiguousTimeError: 2009-10-31 23:30:00


Problems with Localtime
~~~~~~~~~~~~~~~~~~~~~~~

The major problem we have to deal with is that certain datetimes
may occur twice in a year. For example, in the US/Eastern timezone
on the last Sunday morning in October, the following sequence
happens:

    - 01:00 EDT occurs
    - 1 hour later, instead of 2:00am the clock is turned back 1 hour
      and 01:00 happens again (this time 01:00 EST)

In fact, every instant between 01:00 and 02:00 occurs twice. This means
that if you try and create a time in the 'US/Eastern' timezone using
the standard datetime syntax, there is no way to specify if you meant
before of after the end-of-daylight-savings-time transition.

>>> loc_dt = datetime(2002, 10, 27, 1, 30, 00, tzinfo=eastern)
>>> loc_dt.strftime(fmt)
'2002-10-27 01:30:00 EST-0500'

As you can see, the system has chosen one for you and there is a 50%
chance of it being out by one hour. For some applications, this does
not matter. However, if you are trying to schedule meetings with people
in different timezones or analyze log files it is not acceptable. 

The best and simplest solution is to stick with using UTC.  The pytz
package encourages using UTC for internal timezone representation by
including a special UTC implementation based on the standard Python
reference implementation in the Python documentation.

The UTC timezone unpickles to be the same instance, and pickles to a
smaller size than other pytz tzinfo instances.  The UTC implementation
can be obtained as pytz.utc, pytz.UTC, or pytz.timezone('UTC').

>>> import pickle, pytz
>>> dt = datetime(2005, 3, 1, 14, 13, 21, tzinfo=utc)
>>> naive = dt.replace(tzinfo=None)
>>> p = pickle.dumps(dt, 1)
>>> naive_p = pickle.dumps(naive, 1)
>>> len(p) - len(naive_p)
17
>>> new = pickle.loads(p)
>>> new == dt
True
>>> new is dt
False
>>> new.tzinfo is dt.tzinfo
True
>>> pytz.utc is pytz.UTC is pytz.timezone('UTC')
True

Note that this instance is not the same instance (or implementation) as
other timezones with the same meaning (GMT, Greenwich, Universal, etc.).

>>> utc is pytz.timezone('GMT')
False

If you insist on working with local times, this library provides a
facility for constructing them unambiguously:

>>> loc_dt = datetime(2002, 10, 27, 1, 30, 00)
>>> est_dt = eastern.localize(loc_dt, is_dst=True)
>>> edt_dt = eastern.localize(loc_dt, is_dst=False)
>>> print(est_dt.strftime(fmt) + ' / ' + edt_dt.strftime(fmt))
2002-10-27 01:30:00 EDT-0400 / 2002-10-27 01:30:00 EST-0500

If you pass None as the is_dst flag to localize(), pytz will refuse to
guess and raise exceptions if you try to build ambiguous or non-existent
times.

For example, 1:30am on 27th Oct 2002 happened twice in the US/Eastern
timezone when the clocks where put back at the end of Daylight Savings
Time:

>>> dt = datetime(2002, 10, 27, 1, 30, 00)
>>> try:
...     eastern.localize(dt, is_dst=None)
... except pytz.exceptions.AmbiguousTimeError:
...     print('pytz.exceptions.AmbiguousTimeError: %s' % dt)
pytz.exceptions.AmbiguousTimeError: 2002-10-27 01:30:00

Similarly, 2:30am on 7th April 2002 never happened at all in the
US/Eastern timezone, as the clocks where put forward at 2:00am skipping
the entire hour:

>>> dt = datetime(2002, 4, 7, 2, 30, 00)
>>> try:
...     eastern.localize(dt, is_dst=None)
... except pytz.exceptions.NonExistentTimeError:
...     print('pytz.exceptions.NonExistentTimeError: %s' % dt)
pytz.exceptions.NonExistentTimeError: 2002-04-07 02:30:00

Both of these exceptions share a common base class to make error handling
easier:

>>> isinstance(pytz.AmbiguousTimeError(), pytz.InvalidTimeError)
True
>>> isinstance(pytz.NonExistentTimeError(), pytz.InvalidTimeError)
True

Although ``localize()`` handles many cases, it is still not possible
to handle all. In cases where countries change their timezone definitions,
cases like the end-of-daylight-savings-time occur with no way of resolving
the ambiguity. For example, in 1915 Warsaw switched from Warsaw time to
Central European time. So at the stroke of midnight on August 5th 1915
the clocks were wound back 24 minutes creating an ambiguous time period
that cannot be specified without referring to the timezone abbreviation
or the actual UTC offset. In this case midnight happened twice, neither
time during a daylight savings time period:

>>> warsaw = pytz.timezone('Europe/Warsaw')
>>> loc_dt1 = warsaw.localize(datetime(1915, 8, 4, 23, 59, 59), is_dst=False)
>>> loc_dt1.strftime(fmt)
'1915-08-04 23:59:59 WMT+0124'
>>> loc_dt2 = warsaw.localize(datetime(1915, 8, 5, 00, 00, 00), is_dst=False)
>>> loc_dt2.strftime(fmt)
'1915-08-05 00:00:00 CET+0100'
>>> str(loc_dt2 - loc_dt1)
'0:24:01'

The only way of creating a time during the missing 24 minutes is
converting from another timezone - because neither of the timezones
involved where in daylight savings mode the API simply provides no way
to express it:

>>> utc_dt = datetime(1915, 8, 4, 22, 36, tzinfo=pytz.utc)
>>> utc_dt.astimezone(warsaw).strftime(fmt)
'1915-08-04 23:36:00 CET+0100'

The standard Python way of handling all these ambiguities is not to
handle them, such as demonstrated in this example using the US/Eastern
timezone definition from the Python documentation (Note that this
implementation only works for dates between 1987 and 2006 - it is
included for tests only!):

>>> from pytz.reference import Eastern # pytz.reference only for tests
>>> dt = datetime(2002, 10, 27, 0, 30, tzinfo=Eastern)
>>> str(dt)
'2002-10-27 00:30:00-04:00'
>>> str(dt + timedelta(hours=1))
'2002-10-27 01:30:00-05:00'
>>> str(dt + timedelta(hours=2))
'2002-10-27 02:30:00-05:00'
>>> str(dt + timedelta(hours=3))
'2002-10-27 03:30:00-05:00'

Notice the first two results? At first glance you might think they are
correct, but taking the UTC offset into account you find that they are
actually two hours appart instead of the 1 hour we asked for.

>>> from pytz.reference import UTC # pytz.reference only for tests
>>> str(dt.astimezone(UTC))
'2002-10-27 04:30:00+00:00'
>>> str((dt + timedelta(hours=1)).astimezone(UTC))
'2002-10-27 06:30:00+00:00'


Country Information
~~~~~~~~~~~~~~~~~~~

A mechanism is provided to access the timezones commonly in use
for a particular country, looked up using the ISO 3166 country code.
It returns a list of strings that can be used to retrieve the relevant
tzinfo instance using ``pytz.timezone()``:

>>> print(' '.join(pytz.country_timezones['nz']))
Pacific/Auckland Pacific/Chatham

The Olson database comes with a ISO 3166 country code to English country
name mapping that pytz exposes as a dictionary:

>>> print(pytz.country_names['nz'])
New Zealand


What is UTC
~~~~~~~~~~~

'UTC' is Universal Time, also known as Greenwich Mean Time or GMT
in the United Kingdom. All other timezones are given as offsets from
UTC. No daylight savings time occurs in UTC, making it a useful timezone
to perform date arithmetic without worrying about the confusion and
ambiguities caused by daylight savings time transitions, your country
changing its timezone, or mobile computers that move roam through
multiple timezones.


Helpers
~~~~~~~

There are two lists of timezones provided.

``all_timezones`` is the exhaustive list of the timezone names that can
be used.

>>> from pytz import all_timezones
>>> len(all_timezones) >= 500
True
>>> 'Etc/Greenwich' in all_timezones
True

``common_timezones`` is a list of useful, current timezones. It doesn't
contain deprecated zones or historical zones, except for a few I've
deemed in common usage, such as US/Eastern (open a bug report if you
think other timezones are deserving of being included here). It is also
a sequence of strings.

>>> from pytz import common_timezones
>>> len(common_timezones) < len(all_timezones)
True
>>> 'Etc/Greenwich' in common_timezones
False
>>> 'Australia/Melbourne' in common_timezones
True
>>> 'US/Eastern' in common_timezones
True
>>> 'Canada/Eastern' in common_timezones
True
>>> 'US/Pacific-New' in all_timezones
True
>>> 'US/Pacific-New' in common_timezones
False

Both ``common_timezones`` and ``all_timezones`` are alphabetically
sorted:

>>> common_timezones_dupe = common_timezones[:]
>>> common_timezones_dupe.sort()
>>> common_timezones == common_timezones_dupe
True
>>> all_timezones_dupe = all_timezones[:]
>>> all_timezones_dupe.sort()
>>> all_timezones == all_timezones_dupe
True

``all_timezones`` and ``common_timezones`` are also available as sets.

>>> from pytz import all_timezones_set, common_timezones_set
>>> 'US/Eastern' in all_timezones_set
True
>>> 'US/Eastern' in common_timezones_set
True
>>> 'Australia/Victoria' in common_timezones_set
False

You can also retrieve lists of timezones used by particular countries
using the ``country_timezones()`` function. It requires an ISO-3166
two letter country code.

>>> from pytz import country_timezones
>>> print(' '.join(country_timezones('ch')))
Europe/Zurich
>>> print(' '.join(country_timezones('CH')))
Europe/Zurich


License
~~~~~~~

MIT license.

This code is also available as part of Zope 3 under the Zope Public
License,  Version 2.1 (ZPL).

I'm happy to relicense this code if necessary for inclusion in other
open source projects.


Latest Versions
~~~~~~~~~~~~~~~

This package will be updated after releases of the Olson timezone
database.  The latest version can be downloaded from the `Python Package
Index <http://pypi.python.org/pypi/pytz/>`_.  The code that is used
to generate this distribution is hosted on launchpad.net and available
using the `Bazaar version control system <http://bazaar-vcs.org>`_
using::

    bzr branch lp:pytz

Announcements of new releases are made on
`Launchpad <https://launchpad.net/pytz>`_, and the
`Atom feed <http://feeds.launchpad.net/pytz/announcements.atom>`_
hosted there.


Bugs, Feature Requests & Patches
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Bugs can be reported using `Launchpad <https://bugs.launchpad.net/pytz>`_.


Issues & Limitations
~~~~~~~~~~~~~~~~~~~~

- Offsets from UTC are rounded to the nearest whole minute, so timezones
  such as Europe/Amsterdam pre 1937 will be up to 30 seconds out. This
  is a limitation of the Python datetime library.

- If you think a timezone definition is incorrect, I probably can't fix
  it. pytz is a direct translation of the Olson timezone database, and
  changes to the timezone definitions need to be made to this source.
  If you find errors they should be reported to the time zone mailing
  list, linked from http://www.iana.org/time-zones.


Further Reading
~~~~~~~~~~~~~~~

More info than you want to know about timezones:
http://www.twinsun.com/tz/tz-link.htm


Contact
~~~~~~~

Stuart Bishop <stuart@stuartbishop.net>



pytz - World Timezone Definitions for Python
============================================

:Author: Stuart Bishop <stuart@stuartbishop.net>

Introduction
~~~~~~~~~~~~

pytz brings the Olson tz database into Python. This library allows
accurate and cross platform timezone calculations using Python 2.4
or higher. It also solves the issue of ambiguous times at the end
of daylight saving time, which you can read more about in the Python
Library Reference (``datetime.tzinfo``).

Almost all of the Olson timezones are supported.

.. note::

    This library differs from the documented Python API for
    tzinfo implementations; if you want to create local wallclock
    times you need to use the ``localize()`` method documented in this
    document. In addition, if you perform date arithmetic on local
    times that cross DST boundaries, the result may be in an incorrect
    timezone (ie. subtract 1 minute from 2002-10-27 1:00 EST and you get
    2002-10-27 0:59 EST instead of the correct 2002-10-27 1:59 EDT). A
    ``normalize()`` method is provided to correct this. Unfortunately these
    issues cannot be resolved without modifying the Python datetime
    implementation (see PEP-431).


Installation
~~~~~~~~~~~~

This package can either be installed from a .egg file using setuptools,
or from the tarball using the standard Python distutils.

If you are installing from a tarball, run the following command as an
administrative user::

    python setup.py install

If you are installing using setuptools, you don't even need to download
anything as the latest version will be downloaded for you
from the Python package index::

    easy_install --upgrade pytz

If you already have the .egg file, you can use that too::

    easy_install pytz-2008g-py2.6.egg


Example & Usage
~~~~~~~~~~~~~~~

Localized times and date arithmetic
-----------------------------------

>>> from datetime import datetime, timedelta
>>> from pytz import timezone
>>> import pytz
>>> utc = pytz.utc
>>> utc.zone
'UTC'
>>> eastern = timezone('US/Eastern')
>>> eastern.zone
'US/Eastern'
>>> amsterdam = timezone('Europe/Amsterdam')
>>> fmt = '%Y-%m-%d %H:%M:%S %Z%z'

This library only supports two ways of building a localized time. The
first is to use the ``localize()`` method provided by the pytz library.
This is used to localize a naive datetime (datetime with no timezone
information):

>>> loc_dt = eastern.localize(datetime(2002, 10, 27, 6, 0, 0))
>>> print(loc_dt.strftime(fmt))
2002-10-27 06:00:00 EST-0500

The second way of building a localized time is by converting an existing
localized time using the standard ``astimezone()`` method:

>>> ams_dt = loc_dt.astimezone(amsterdam)
>>> ams_dt.strftime(fmt)
'2002-10-27 12:00:00 CET+0100'

Unfortunately using the tzinfo argument of the standard datetime
constructors ''does not work'' with pytz for many timezones.

>>> datetime(2002, 10, 27, 12, 0, 0, tzinfo=amsterdam).strftime(fmt)
'2002-10-27 12:00:00 AMT+0020'

It is safe for timezones without daylight saving transitions though, such
as UTC:

>>> datetime(2002, 10, 27, 12, 0, 0, tzinfo=pytz.utc).strftime(fmt)
'2002-10-27 12:00:00 UTC+0000'

The preferred way of dealing with times is to always work in UTC,
converting to localtime only when generating output to be read
by humans.

>>> utc_dt = datetime(2002, 10, 27, 6, 0, 0, tzinfo=utc)
>>> loc_dt = utc_dt.astimezone(eastern)
>>> loc_dt.strftime(fmt)
'2002-10-27 01:00:00 EST-0500'

This library also allows you to do date arithmetic using local
times, although it is more complicated than working in UTC as you
need to use the ``normalize()`` method to handle daylight saving time
and other timezone transitions. In this example, ``loc_dt`` is set
to the instant when daylight saving time ends in the US/Eastern
timezone.

>>> before = loc_dt - timedelta(minutes=10)
>>> before.strftime(fmt)
'2002-10-27 00:50:00 EST-0500'
>>> eastern.normalize(before).strftime(fmt)
'2002-10-27 01:50:00 EDT-0400'
>>> after = eastern.normalize(before + timedelta(minutes=20))
>>> after.strftime(fmt)
'2002-10-27 01:10:00 EST-0500'

Creating local times is also tricky, and the reason why working with
local times is not recommended. Unfortunately, you cannot just pass
a ``tzinfo`` argument when constructing a datetime (see the next
section for more details)

>>> dt = datetime(2002, 10, 27, 1, 30, 0)
>>> dt1 = eastern.localize(dt, is_dst=True)
>>> dt1.strftime(fmt)
'2002-10-27 01:30:00 EDT-0400'
>>> dt2 = eastern.localize(dt, is_dst=False)
>>> dt2.strftime(fmt)
'2002-10-27 01:30:00 EST-0500'

Converting between timezones also needs special attention. We also need
to use the ``normalize()`` method to ensure the conversion is correct.

>>> utc_dt = utc.localize(datetime.utcfromtimestamp(1143408899))
>>> utc_dt.strftime(fmt)
'2006-03-26 21:34:59 UTC+0000'
>>> au_tz = timezone('Australia/Sydney')
>>> au_dt = au_tz.normalize(utc_dt.astimezone(au_tz))
>>> au_dt.strftime(fmt)
'2006-03-27 08:34:59 EST+1100'
>>> utc_dt2 = utc.normalize(au_dt.astimezone(utc))
>>> utc_dt2.strftime(fmt)
'2006-03-26 21:34:59 UTC+0000'

You can take shortcuts when dealing with the UTC side of timezone
conversions. ``normalize()`` and ``localize()`` are not really
necessary when there are no daylight saving time transitions to
deal with.

>>> utc_dt = datetime.utcfromtimestamp(1143408899).replace(tzinfo=utc)
>>> utc_dt.strftime(fmt)
'2006-03-26 21:34:59 UTC+0000'
>>> au_tz = timezone('Australia/Sydney')
>>> au_dt = au_tz.normalize(utc_dt.astimezone(au_tz))
>>> au_dt.strftime(fmt)
'2006-03-27 08:34:59 EST+1100'
>>> utc_dt2 = au_dt.astimezone(utc)
>>> utc_dt2.strftime(fmt)
'2006-03-26 21:34:59 UTC+0000'


``tzinfo`` API
--------------

The ``tzinfo`` instances returned by the ``timezone()`` function have
been extended to cope with ambiguous times by adding an ``is_dst``
parameter to the ``utcoffset()``, ``dst()`` && ``tzname()`` methods.

>>> tz = timezone('America/St_Johns')

>>> normal = datetime(2009, 9, 1)
>>> ambiguous = datetime(2009, 10, 31, 23, 30)

The ``is_dst`` parameter is ignored for most timestamps. It is only used
during DST transition ambiguous periods to resulve that ambiguity.

>>> tz.utcoffset(normal, is_dst=True)
datetime.timedelta(-1, 77400)
>>> tz.dst(normal, is_dst=True)
datetime.timedelta(0, 3600)
>>> tz.tzname(normal, is_dst=True)
'NDT'

>>> tz.utcoffset(ambiguous, is_dst=True)
datetime.timedelta(-1, 77400)
>>> tz.dst(ambiguous, is_dst=True)
datetime.timedelta(0, 3600)
>>> tz.tzname(ambiguous, is_dst=True)
'NDT'

>>> tz.utcoffset(normal, is_dst=False)
datetime.timedelta(-1, 77400)
>>> tz.dst(normal, is_dst=False)
datetime.timedelta(0, 3600)
>>> tz.tzname(normal, is_dst=False)
'NDT'

>>> tz.utcoffset(ambiguous, is_dst=False)
datetime.timedelta(-1, 73800)
>>> tz.dst(ambiguous, is_dst=False)
datetime.timedelta(0)
>>> tz.tzname(ambiguous, is_dst=False)
'NST'

If ``is_dst`` is not specified, ambiguous timestamps will raise
an ``pytz.exceptions.AmbiguousTimeError`` exception.

>>> tz.utcoffset(normal)
datetime.timedelta(-1, 77400)
>>> tz.dst(normal)
datetime.timedelta(0, 3600)
>>> tz.tzname(normal)
'NDT'

>>> import pytz.exceptions
>>> try:
...     tz.utcoffset(ambiguous)
... except pytz.exceptions.AmbiguousTimeError:
...     print('pytz.exceptions.AmbiguousTimeError: %s' % ambiguous)
pytz.exceptions.AmbiguousTimeError: 2009-10-31 23:30:00
>>> try:
...     tz.dst(ambiguous)
... except pytz.exceptions.AmbiguousTimeError:
...     print('pytz.exceptions.AmbiguousTimeError: %s' % ambiguous)
pytz.exceptions.AmbiguousTimeError: 2009-10-31 23:30:00
>>> try:
...     tz.tzname(ambiguous)
... except pytz.exceptions.AmbiguousTimeError:
...     print('pytz.exceptions.AmbiguousTimeError: %s' % ambiguous)
pytz.exceptions.AmbiguousTimeError: 2009-10-31 23:30:00


Problems with Localtime
~~~~~~~~~~~~~~~~~~~~~~~

The major problem we have to deal with is that certain datetimes
may occur twice in a year. For example, in the US/Eastern timezone
on the last Sunday morning in October, the following sequence
happens:

    - 01:00 EDT occurs
    - 1 hour later, instead of 2:00am the clock is turned back 1 hour
      and 01:00 happens again (this time 01:00 EST)

In fact, every instant between 01:00 and 02:00 occurs twice. This means
that if you try and create a time in the 'US/Eastern' timezone using
the standard datetime syntax, there is no way to specify if you meant
before of after the end-of-daylight-saving-time transition.

>>> loc_dt = datetime(2002, 10, 27, 1, 30, 00, tzinfo=eastern)
>>> loc_dt.strftime(fmt)
'2002-10-27 01:30:00 EST-0500'

As you can see, the system has chosen one for you and there is a 50%
chance of it being out by one hour. For some applications, this does
not matter. However, if you are trying to schedule meetings with people
in different timezones or analyze log files it is not acceptable. 

The best and simplest solution is to stick with using UTC.  The pytz
package encourages using UTC for internal timezone representation by
including a special UTC implementation based on the standard Python
reference implementation in the Python documentation.

The UTC timezone unpickles to be the same instance, and pickles to a
smaller size than other pytz tzinfo instances.  The UTC implementation
can be obtained as pytz.utc, pytz.UTC, or pytz.timezone('UTC').

>>> import pickle, pytz
>>> dt = datetime(2005, 3, 1, 14, 13, 21, tzinfo=utc)
>>> naive = dt.replace(tzinfo=None)
>>> p = pickle.dumps(dt, 1)
>>> naive_p = pickle.dumps(naive, 1)
>>> len(p) - len(naive_p)
17
>>> new = pickle.loads(p)
>>> new == dt
True
>>> new is dt
False
>>> new.tzinfo is dt.tzinfo
True
>>> pytz.utc is pytz.UTC is pytz.timezone('UTC')
True

Note that some other timezones are commonly thought of as the same (GMT,
Greenwich, Universal, etc.). The definition of UTC is distinct from these
other timezones, and they are not equivalent. For this reason, they will
not compare the same in Python.

>>> utc == pytz.timezone('GMT')
False

See the section `What is UTC`_, below.

If you insist on working with local times, this library provides a
facility for constructing them unambiguously:

>>> loc_dt = datetime(2002, 10, 27, 1, 30, 00)
>>> est_dt = eastern.localize(loc_dt, is_dst=True)
>>> edt_dt = eastern.localize(loc_dt, is_dst=False)
>>> print(est_dt.strftime(fmt) + ' / ' + edt_dt.strftime(fmt))
2002-10-27 01:30:00 EDT-0400 / 2002-10-27 01:30:00 EST-0500

If you pass None as the is_dst flag to localize(), pytz will refuse to
guess and raise exceptions if you try to build ambiguous or non-existent
times.

For example, 1:30am on 27th Oct 2002 happened twice in the US/Eastern
timezone when the clocks where put back at the end of Daylight Saving
Time:

>>> dt = datetime(2002, 10, 27, 1, 30, 00)
>>> try:
...     eastern.localize(dt, is_dst=None)
... except pytz.exceptions.AmbiguousTimeError:
...     print('pytz.exceptions.AmbiguousTimeError: %s' % dt)
pytz.exceptions.AmbiguousTimeError: 2002-10-27 01:30:00

Similarly, 2:30am on 7th April 2002 never happened at all in the
US/Eastern timezone, as the clocks where put forward at 2:00am skipping
the entire hour:

>>> dt = datetime(2002, 4, 7, 2, 30, 00)
>>> try:
...     eastern.localize(dt, is_dst=None)
... except pytz.exceptions.NonExistentTimeError:
...     print('pytz.exceptions.NonExistentTimeError: %s' % dt)
pytz.exceptions.NonExistentTimeError: 2002-04-07 02:30:00

Both of these exceptions share a common base class to make error handling
easier:

>>> isinstance(pytz.AmbiguousTimeError(), pytz.InvalidTimeError)
True
>>> isinstance(pytz.NonExistentTimeError(), pytz.InvalidTimeError)
True

Although ``localize()`` handles many cases, it is still not possible
to handle all. In cases where countries change their timezone definitions,
cases like the end-of-daylight-saving-time occur with no way of resolving
the ambiguity. For example, in 1915 Warsaw switched from Warsaw time to
Central European time. So at the stroke of midnight on August 5th 1915
the clocks were wound back 24 minutes creating an ambiguous time period
that cannot be specified without referring to the timezone abbreviation
or the actual UTC offset. In this case midnight happened twice, neither
time during a daylight saving time period:

>>> warsaw = pytz.timezone('Europe/Warsaw')
>>> loc_dt1 = warsaw.localize(datetime(1915, 8, 4, 23, 59, 59), is_dst=False)
>>> loc_dt1.strftime(fmt)
'1915-08-04 23:59:59 WMT+0124'
>>> loc_dt2 = warsaw.localize(datetime(1915, 8, 5, 00, 00, 00), is_dst=False)
>>> loc_dt2.strftime(fmt)
'1915-08-05 00:00:00 CET+0100'
>>> str(loc_dt2 - loc_dt1)
'0:24:01'

The only way of creating a time during the missing 24 minutes is
converting from another timezone - because neither of the timezones
involved where in daylight saving mode the API simply provides no way
to express it:

>>> utc_dt = datetime(1915, 8, 4, 22, 36, tzinfo=pytz.utc)
>>> utc_dt.astimezone(warsaw).strftime(fmt)
'1915-08-04 23:36:00 CET+0100'

The standard Python way of handling all these ambiguities is not to
handle them, such as demonstrated in this example using the US/Eastern
timezone definition from the Python documentation (Note that this
implementation only works for dates between 1987 and 2006 - it is
included for tests only!):

>>> from pytz.reference import Eastern # pytz.reference only for tests
>>> dt = datetime(2002, 10, 27, 0, 30, tzinfo=Eastern)
>>> str(dt)
'2002-10-27 00:30:00-04:00'
>>> str(dt + timedelta(hours=1))
'2002-10-27 01:30:00-05:00'
>>> str(dt + timedelta(hours=2))
'2002-10-27 02:30:00-05:00'
>>> str(dt + timedelta(hours=3))
'2002-10-27 03:30:00-05:00'

Notice the first two results? At first glance you might think they are
correct, but taking the UTC offset into account you find that they are
actually two hours appart instead of the 1 hour we asked for.

>>> from pytz.reference import UTC # pytz.reference only for tests
>>> str(dt.astimezone(UTC))
'2002-10-27 04:30:00+00:00'
>>> str((dt + timedelta(hours=1)).astimezone(UTC))
'2002-10-27 06:30:00+00:00'


Country Information
~~~~~~~~~~~~~~~~~~~

A mechanism is provided to access the timezones commonly in use
for a particular country, looked up using the ISO 3166 country code.
It returns a list of strings that can be used to retrieve the relevant
tzinfo instance using ``pytz.timezone()``:

>>> print(' '.join(pytz.country_timezones['nz']))
Pacific/Auckland Pacific/Chatham

The Olson database comes with a ISO 3166 country code to English country
name mapping that pytz exposes as a dictionary:

>>> print(pytz.country_names['nz'])
New Zealand


What is UTC
~~~~~~~~~~~

'UTC' is `Coordinated Universal Time`_. It is a successor to, but distinct
from, Greenwich Mean Time (GMT) and the various definitions of Universal
Time. UTC is now the worldwide standard for regulating clocks and time
measurement.

All other timezones are defined relative to UTC, and include offsets like
UTC+0800 - hours to add or subtract from UTC to derive the local time. No
daylight saving time occurs in UTC, making it a useful timezone to perform
date arithmetic without worrying about the confusion and ambiguities caused
by daylight saving time transitions, your country changing its timezone, or
mobile computers that roam through multiple timezones.

..  _Coordinated Universal Time: https://en.wikipedia.org/wiki/Coordinated_Universal_Time


Helpers
~~~~~~~

There are two lists of timezones provided.

``all_timezones`` is the exhaustive list of the timezone names that can
be used.

>>> from pytz import all_timezones
>>> len(all_timezones) >= 500
True
>>> 'Etc/Greenwich' in all_timezones
True

``common_timezones`` is a list of useful, current timezones. It doesn't
contain deprecated zones or historical zones, except for a few I've
deemed in common usage, such as US/Eastern (open a bug report if you
think other timezones are deserving of being included here). It is also
a sequence of strings.

>>> from pytz import common_timezones
>>> len(common_timezones) < len(all_timezones)
True
>>> 'Etc/Greenwich' in common_timezones
False
>>> 'Australia/Melbourne' in common_timezones
True
>>> 'US/Eastern' in common_timezones
True
>>> 'Canada/Eastern' in common_timezones
True
>>> 'US/Pacific-New' in all_timezones
True
>>> 'US/Pacific-New' in common_timezones
False

Both ``common_timezones`` and ``all_timezones`` are alphabetically
sorted:

>>> common_timezones_dupe = common_timezones[:]
>>> common_timezones_dupe.sort()
>>> common_timezones == common_timezones_dupe
True
>>> all_timezones_dupe = all_timezones[:]
>>> all_timezones_dupe.sort()
>>> all_timezones == all_timezones_dupe
True

``all_timezones`` and ``common_timezones`` are also available as sets.

>>> from pytz import all_timezones_set, common_timezones_set
>>> 'US/Eastern' in all_timezones_set
True
>>> 'US/Eastern' in common_timezones_set
True
>>> 'Australia/Victoria' in common_timezones_set
False

You can also retrieve lists of timezones used by particular countries
using the ``country_timezones()`` function. It requires an ISO-3166
two letter country code.

>>> from pytz import country_timezones
>>> print(' '.join(country_timezones('ch')))
Europe/Zurich
>>> print(' '.join(country_timezones('CH')))
Europe/Zurich


License
~~~~~~~

MIT license.

This code is also available as part of Zope 3 under the Zope Public
License,  Version 2.1 (ZPL).

I'm happy to relicense this code if necessary for inclusion in other
open source projects.


Latest Versions
~~~~~~~~~~~~~~~

This package will be updated after releases of the Olson timezone
database.  The latest version can be downloaded from the `Python Package
Index <http://pypi.python.org/pypi/pytz/>`_.  The code that is used
to generate this distribution is hosted on launchpad.net and available
using the `Bazaar version control system <http://bazaar-vcs.org>`_
using::

    bzr branch lp:pytz

Announcements of new releases are made on
`Launchpad <https://launchpad.net/pytz>`_, and the
`Atom feed <http://feeds.launchpad.net/pytz/announcements.atom>`_
hosted there.


Bugs, Feature Requests & Patches
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Bugs can be reported using `Launchpad <https://bugs.launchpad.net/pytz>`_.


Issues & Limitations
~~~~~~~~~~~~~~~~~~~~

- Offsets from UTC are rounded to the nearest whole minute, so timezones
  such as Europe/Amsterdam pre 1937 will be up to 30 seconds out. This
  is a limitation of the Python datetime library.

- If you think a timezone definition is incorrect, I probably can't fix
  it. pytz is a direct translation of the Olson timezone database, and
  changes to the timezone definitions need to be made to this source.
  If you find errors they should be reported to the time zone mailing
  list, linked from http://www.iana.org/time-zones.


Further Reading
~~~~~~~~~~~~~~~

More info than you want to know about timezones:
http://www.twinsun.com/tz/tz-link.htm


Contact
~~~~~~~

Stuart Bishop <stuart@stuartbishop.net>



PyYAML - The next generation YAML parser and emitter for Python.

To install, type 'python setup.py install'.

By default, the setup.py script checks whether LibYAML is installed
and if so, builds and installs LibYAML bindings.  To skip the check
and force installation of LibYAML bindings, use the option '--with-libyaml':
'python setup.py --with-libyaml install'.  To disable the check and
skip building and installing LibYAML bindings, use '--without-libyaml':
'python setup.py --without-libyaml install'.

When LibYAML bindings are installed, you may use fast LibYAML-based
parser and emitter as follows:

    >>> yaml.load(stream, Loader=yaml.CLoader)
    >>> yaml.dump(data, Dumper=yaml.CDumper)

PyYAML includes a comprehensive test suite.  To run the tests,
type 'python setup.py test'.

For more information, check the PyYAML homepage:
'http://pyyaml.org/wiki/PyYAML'.

For PyYAML tutorial and reference, see:
'http://pyyaml.org/wiki/PyYAMLDocumentation'.

Post your questions and opinions to the YAML-Core mailing list:
'http://lists.sourceforge.net/lists/listinfo/yaml-core'.

Submit bug reports and feature requests to the PyYAML bug tracker:
'http://pyyaml.org/newticket?component=pyyaml'.

PyYAML is written by Kirill Simonov <xi@resolvent.net>.  It is released
under the MIT license. See the file LICENSE for more details.


krTheme Sphinx Style
====================

This repository contains sphinx styles Kenneth Reitz uses in most of
his projects. It is a derivative of Mitsuhiko's themes for Flask and Flask related
projects.  To use this style in your Sphinx documentation, follow
this guide:

1. put this folder as _themes into your docs folder.  Alternatively
   you can also use git submodules to check out the contents there.

2. add this to your conf.py: ::

    sys.path.append(os.path.abspath('_themes'))
    html_theme_path = ['_themes']
    html_theme = 'flask'

The following themes exist:

**kr**
    the standard flask documentation theme for large projects

**kr_small**
    small one-page theme.  Intended to be used by very small addon libraries.


Requests: HTTP for Humans
=========================

.. image:: https://badge.fury.io/py/requests.png
    :target: http://badge.fury.io/py/requests

.. image:: https://travis-ci.org/kennethreitz/requests.png?branch=master
        :target: https://travis-ci.org/kennethreitz/requests

.. image:: https://pypip.in/d/requests/badge.png
        :target: https://crate.io/packages/requests/

Requests is an Apache2 Licensed HTTP library, written in Python, for human
beings.

Most existing Python modules for sending HTTP requests are extremely
verbose and cumbersome. Python's builtin urllib2 module provides most of
the HTTP capabilities you should need, but the api is thoroughly broken.
It requires an enormous amount of work (even method overrides) to
perform the simplest of tasks.

Things shouldn't be this way. Not in Python.

.. code-block:: pycon

    >>> r = requests.get('https://api.github.com', auth=('user', 'pass'))
    >>> r.status_code
    204
    >>> r.headers['content-type']
    'application/json'
    >>> r.text
    ...

See `the same code, without Requests <https://gist.github.com/973705>`_.

Requests allow you to send HTTP/1.1 requests. You can add headers, form data,
multipart files, and parameters with simple Python dictionaries, and access the
response data in the same way. It's powered by httplib and `urllib3
<https://github.com/shazow/urllib3>`_, but it does all the hard work and crazy
hacks for you.


Features
--------

- International Domains and URLs
- Keep-Alive & Connection Pooling
- Sessions with Cookie Persistence
- Browser-style SSL Verification
- Basic/Digest Authentication
- Elegant Key/Value Cookies
- Automatic Decompression
- Unicode Response Bodies
- Multipart File Uploads
- Connection Timeouts
- Thread-safety
- HTTP(S) proxy support


Installation
------------

To install Requests, simply:

.. code-block:: bash

    $ pip install requests

Or, if you absolutely must:

.. code-block:: bash

    $ easy_install requests

But, you really shouldn't do that.


Documentation
-------------

Documentation is available at http://docs.python-requests.org/.


Contribute
----------

#. Check for open issues or open a fresh issue to start a discussion around a feature idea or a bug. There is a `Contributor Friendly`_ tag for issues that should be ideal for people who are not very familiar with the codebase yet.
#. If you feel uncomfortable or uncertain about an issue or your changes, feel free to email @sigmavirus24 and he will happily help you via email, Skype, remote pairing or whatever you are comfortable with.
#. Fork `the repository`_ on GitHub to start making your changes to the **master** branch (or branch off of it).
#. Write a test which shows that the bug was fixed or that the feature works as expected.
#. Send a pull request and bug the maintainer until it gets merged and published. :) Make sure to add yourself to AUTHORS_.

.. _`the repository`: http://github.com/kennethreitz/requests
.. _AUTHORS: https://github.com/kennethreitz/requests/blob/master/AUTHORS.rst
.. _Contributor Friendly: https://github.com/kennethreitz/requests/issues?direction=desc&labels=Contributor+Friendly&page=1&sort=updated&state=open

requests Kerberos/GSSAPI authentication library
===============================================

Requests is an HTTP library, written in Python, for human beings. This library
adds optional Kerberos/GSSAPI authentication support and supports mutual
authentication. Basic GET usage:


.. code-block:: pycon

    >>> import requests
    >>> from requests_kerberos import HTTPKerberosAuth
    >>> r = requests.get("http://example.org", auth=HTTPKerberosAuth())
    ...

The entire ``requests.api`` should be supported.

Authentication Failures
-----------------------

Client authentication failures will be communicated to the caller by returning
the 401 response.

Mutual Authentication
---------------------

By default, ``HTTPKerberosAuth`` will require mutual authentication from the
server, and if a server emits a non-error response which is cannot be
authenticated, a ``requests_kerberos.errors.MutualAuthenticationError`` will be
raised. IF a server emits an error which cannot be authenticated, it will be
returned to the user but with it's contents and headers stripped.

OPTIONAL
^^^^^^^^

If you'd prefer to not require mutual authentication, you can set your
preference when constructing your ``HTTPKerberosAuth`` object:

.. code-block:: pycon

    >>> import requests
    >>> from requests_kerberos import HTTPKerberosAuth, OPTIONAL
    >>> kerberos_auth = HTTPKerberosAuth(mutual_authentication=OPTIONAL)
    >>> r = requests.get("http://example.org", auth=kerberos_auth)
    ...

This will cause ``requests_kerberos`` to attempt mutual authentication if the
server advertises that it supports it, and cause a failure if authentication
fails, but not if the server does not support it at all.

DISABLED
^^^^^^^^

While we don't recommend it, if you'd prefer to never attempt mutual
authentication, you can do that as well:

.. code-block:: pycon

    >>> import requests
    >>> from requests_kerberos import HTTPKerberosAuth, DISABLED
    >>> kerberos_auth = HTTPKerberosAuth(mutual_authentication=DISABLED)
    >>> r = requests.get("http://example.org", auth=kerberos_auth)
    ...

Logging
-------

This library makes extensive use of python's logging facilities. 

Log messages are logged to the ``requests_kerberos`` and
``requests_kerberos.kerberos_`` named loggers.

If you are having difficulty we suggest you configure logging. Issues with the
underlying kerberos libraries will be made apparent. Additionally, copious debug
information is made available which may assist in troubleshooting if you
increase your log level all the way up to debug.

This is South, a Django application to provide schema and data migrations.

Documentation on South is currently available on our project site;
you can find it at http://south.aeracode.org/docs/

South is compatable with Django 1.2 and higher, and Python 2.6 and higher.

krTheme Sphinx Style
====================

This repository contains sphinx styles Kenneth Reitz uses in most of 
his projects. It is a drivative of Mitsuhiko's themes for Flask and Flask related
projects.  To use this style in your Sphinx documentation, follow
this guide:

1. put this folder as _themes into your docs folder.  Alternatively
   you can also use git submodules to check out the contents there.

2. add this to your conf.py: ::

	sys.path.append(os.path.abspath('_themes'))
	html_theme_path = ['_themes']
	html_theme = 'flask'

The following themes exist:

**kr**
	the standard flask documentation theme for large projects

**kr_small**
	small one-page theme.  Intended to be used by very small addon libraries.


Tablib: format-agnostic tabular dataset library
===============================================

::

	_____         ______  ___________ ______
	__  /_______ ____  /_ ___  /___(_)___  /_
	_  __/_  __ `/__  __ \__  / __  / __  __ \
	/ /_  / /_/ / _  /_/ /_  /  _  /  _  /_/ /
	\__/  \__,_/  /_.___/ /_/   /_/   /_.___/



Tablib is a format-agnostic tabular dataset library, written in Python.

Output formats supported:

- Excel (Sets + Books)
- JSON (Sets + Books)
- YAML (Sets + Books)
- HTML (Sets)
- TSV (Sets)
- CSV (Sets)

Note that tablib *purposefully* excludes XML support. It always will. (Note: This is a joke. Pull requests are welcome.)

Overview
--------

`tablib.Dataset()`
	A Dataset is a table of tabular data. It may or may not have a header row. They can be build and manipulated as raw Python datatypes (Lists of tuples|dictionaries). Datasets can be imported from JSON, YAML, and CSV; they can be exported to XLSX, XLS, ODS, JSON, YAML, CSV, TSV, and HTML.

`tablib.Databook()`
	A Databook is a set of Datasets. The most common form of a Databook is an Excel file with multiple spreadsheets. Databooks can be imported from JSON and YAML; they can be exported to XLSX, XLS, ODS, JSON, and YAML.

Usage
-----


Populate fresh data files: ::

    headers = ('first_name', 'last_name')

    data = [
        ('John', 'Adams'),
        ('George', 'Washington')
    ]

    data = tablib.Dataset(*data, headers=headers)


Intelligently add new rows: ::

    >>> data.append(('Henry', 'Ford'))

Intelligently add new columns: ::

    >>> data.append_col((90, 67, 83), header='age')

Slice rows:  ::

    >>> print data[:2]
    [('John', 'Adams', 90), ('George', 'Washington', 67)]


Slice columns by header: ::

    >>> print data['first_name']
    ['John', 'George', 'Henry']

Easily delete rows: ::

    >>> del data[1]

Exports
-------

Drumroll please...........

JSON!
+++++
::

	>>> print data.json
	[
	  {
	    "last_name": "Adams",
	    "age": 90,
	    "first_name": "John"
	  },
	  {
	    "last_name": "Ford",
	    "age": 83,
	    "first_name": "Henry"
	  }
	]


YAML!
+++++
::

	>>> print data.yaml
	- {age: 90, first_name: John, last_name: Adams}
	- {age: 83, first_name: Henry, last_name: Ford}

CSV...
++++++
::

	>>> print data.csv
	first_name,last_name,age
	John,Adams,90
	Henry,Ford,83

EXCEL!
++++++
::

	>>> with open('people.xls', 'wb') as f:
	...     f.write(data.xls)

It's that easy.


Installation
------------

To install tablib, simply: ::

	$ pip install tablib

Or, if you absolutely must: ::

	$ easy_install tablib

Contribute
----------

If you'd like to contribute, simply fork `the repository`_, commit your
changes to the **develop** branch (or branch off of it), and send a pull
request. Make sure you add yourself to AUTHORS_.




.. _`the repository`: http://github.com/kennethreitz/tablib
.. _AUTHORS: http://github.com/kennethreitz/tablib/blob/master/AUTHORS

Obtaining tracebacks on other threads in Python
===============================================
by Fazal Majid (www.majid.info), 2004-06-10

David Beazley added advanced debugging functions to the Python interpreter,
and they have been folded into the 2.2 release. Guido van Rossum added in
Python 2.3 the thread ID to the interpreter state structure, and this allows
us to produce a dictionary mapping thread IDs to frames.

I used these hooks to build a debugging module that is useful when you
are looking for deadlocks in a multithreaded application. I've built
and tested this only on Solaris 8/x86, but the code should be pretty
portable.

Of course, I disclaim any liability if this code should crash your system,
erase your homework, eat your dog (who also ate your homework) or otherwise
have any undesirable effect.

Building and installing
=======================

Download threadframe-0.2.tar.gz. You can use the Makefile or the setup.py
script. There is a small test program test.py that illustrates how to use this
module to dump stack frames of all the Python interpreter threads. A sample
run is available for your perusal.

Thrift Python Software Library

License
=======

Licensed to the Apache Software Foundation (ASF) under one
or more contributor license agreements. See the NOTICE file
distributed with this work for additional information
regarding copyright ownership. The ASF licenses this file
to you under the Apache License, Version 2.0 (the
"License"); you may not use this file except in compliance
with the License. You may obtain a copy of the License at

  http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an
"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, either express or implied. See the License for the
specific language governing permissions and limitations
under the License.

Using Thrift with Python
========================

Thrift is provided as a set of Python packages. The top level package is
thrift, and there are subpackages for the protocol, transport, and server
code. Each package contains modules using standard Thrift naming conventions
(i.e. TProtocol, TTransport) and implementations in corresponding modules
(i.e. TSocket).  There is also a subpackage reflection, which contains
the generated code for the reflection structures.

The Python libraries can be installed manually using the provided setup.py
file, or automatically using the install hook provided via autoconf/automake.
To use the latter, become superuser and do make install.

Help for your app, written in [MarkDown](http://daringfireball.net/projects/markdown/syntax) syntax.

Help for your app, written in [MarkDown](http://daringfireball.net/projects/markdown/syntax) syntax.

jqCron
======

Cron jQuery plugin

Originaly created in july 2012.

Simple documentation available at [this link](http://arnapou.net/2012-07-jquery-cron/)

Features
========
* multi select
* i18n
* custom binding with dom element (can be two ways sync for instance with input)
* lots of options (reset button, default value, ...)
* php class to test matching dates

Examples/screenshots
====================
![screen1](/screenshots/example1.png)
![screen2](/screenshots/example2.png)
![screen3](/screenshots/example3.png)
![screen4](/screenshots/example4.png)
![screen5](/screenshots/cron_php.png)

                        Thrift API for HDFS
                        ==================

Introduction:
============

The Hadoop Distributed File System is written in Java. An application
that wants to store/fetch data to/from HDFS can use the Java API
This means that applications that are not written in Java cannot
access HDFS in an elegant manner.

Thrift is a software framework for scalable cross-language services 
development. It combines a powerful software stack with a code generation 
engine to build services that work efficiently and seamlessly 
between C++, Java, Python, PHP, and Ruby.

This project exposes HDFS APIs using the Thrift software stack. This
allows applciations written in a myriad of languages to access
HDFS elegantly.


The Application Programming Interface (API)
===========================================
The HDFS API that is exposed through Thrift can be found in if/hadoopfs.thrift.

Compilation
===========
The compilation process creates a server org.apache.hadoop.thriftfs.HadooopThriftServer
that implements the Thrift interface defined in if/hadoopfs.thrift.

The thrift compiler is used to generate API stubs in python, php, ruby,
cocoa, etc. The generated code is checked into the directories gen-*.
The generated java API is checked into lib/hadoopthriftapi.jar.

There is a sample python script hdfs.py in the scripts directory. This python 
script, when invoked, creates a HadoopThriftServer in the background, and then
communicates with HDFS using the API. This script is for demonstration purposes
only.


If you modified any of the thrift files you need to regenerate
the generated code by running regenerate-thrift.sh, in this
directory.  Finally, checkin any files that were generated during the previous
steps.

If you need to install the Thrift compiler, go to http://thrift.apache.org/ and
pick the version 0.7.0.

Introduction
------------
Solr Search Velocity Templates

A quick demo of using Solr using http://wiki.apache.org/solr/VelocityResponseWriter

You typically access these templates via:
	http://localhost:8983/solr/collection1/browse

It's called "browse" because you can click around with your mouse
without needing to type any search terms.  And of course it
also works as a standard search app as well.

Known Limitations
-----------------
* The /browse and the VelocityResponseWriter component
  serve content directly from Solr, which usually requires
  Solr's HTTP API to be exposed.  Advanced users could
  potentially access other parts of Solr directly.
* There are some hard coded fields in these templates.
  Since these templates live under conf, they should be
  considered part of the overall configuration, and
  must be coordinated with schema.xml and solrconfig.xml

Velocity Info
-------------
Java-based template language.

It's nice in this context because change to the templates
are immediately visible in browser on the next visit.

Links:
	http://velocity.apache.org
	http://wiki.apache.org/velocity/
	http://velocity.apache.org/engine/releases/velocity-1.7/user-guide.html


File List
---------

System and Misc:
  VM_global_library.vm    - Macros used other templates,
                            exact filename is important for Velocity to see it
  error.vm                - shows errors, if any
  debug.vm                - includes toggle links for "explain" and "all fields"
                            activated by debug link in footer.vm
  README.txt              - this file

Overall Page Composition:
  browse.vm               - Main entry point into templates
  layout.vm               - overall HTML page layout
  head.vm                 - elements in the <head> section of the HTML document
  header.vm               - top section of page visible to users
  footer.vm               - bottom section of page visible to users,
                            includes debug and help links
  main.css                - CSS style for overall pages
                            see also jquery.autocomplete.css

Query Form and Options:
  query_form.vm           - renders query form
  query_group.vm          - group by fields
                            e.g.: Manufacturer or Poplularity
  query_spatial.vm        - select box for location based Geospacial search

Spelling Suggestions:
  did_you_mean.vm         - hyperlinked spelling suggestions in results
  suggest.vm              - dynamic spelling suggestions
                            as you type in the search form
  jquery.autocomplete.js  - supporting files for dynamic suggestions
  jquery.autocomplete.css - Most CSS is defined in main.css


Search Results, General:
  (see also browse.vm)
  tabs.vm                 - provides navigation to advanced search options
  pagination_top.vm       - paging and staticis at top of results
  pagination_bottom.vm    - paging and staticis at bottom of results
  results_list.vm
  hit.vm                  - called for each matching doc,
                            decides which template to use
  hit_grouped.vm          - display results grouped by field values
  product_doc.vm          - display a Product
  join_doc.vm             - display a joined document
  richtext_doc.vm         - display a complex/misc. document
  hit_plain.vm            - basic display of all fields,
                            edit results_list.vm to enable this


Search Results, Facets & Clusters:
  facets.vm               - calls the 4 facet and 1 cluster template
  facet_fields.vm         - display facets based on field values
                            e.g.: fields specified by &facet.field=
  facet_queries.vm        - display facets based on specific facet queries
                            e.g.: facets specified by &facet.query=
  facet_ranges.vm         - display facets based on ranges
                            e.g.: ranges specified by &facet.range=
  facet_pivot.vm          - display pivot based facets
                            e.g.: facets specified by &facet.pivot=
  cluster.vm              - if clustering is available
                            then call cluster_results.vm
  cluster_results.vm      - actual rendering of clusters

=========================================
          Hue - Hadoop UI
=========================================

Instructions to install the tarball release of Hue are available
in the manual from http://gethue.com.

If you're impatient, these are the key steps.  Please check the full manual
for more details.


Install
-------
## Install in any directory, e.g. /home/hue, /usr/share, /home/my_user
PREFIX=/usr/share make install

## If you don't have the permissions, you will need to 'sudo' the command and
## make sure that the 'hue' user has write access to the 'logs' directory and 'desktop/desktop.db'.

## Run!
${PREFIX}/hue/build/env/bin/supervisor


Configure Hadoop
----------------

## Install JobTracker plug-in
cd /usr/lib/hadoop-0.20-mapreduce/lib
ln -s ${PREFIX}/hue/desktop/libs/hadoop/java-lib/hue*jar

## Configure Hadoop
Edit hdfs-site.xml:

<property>
  <name>dfs.webhdfs.enable</name>
  <value>true</value>
</property>

Edit mapred-site.xml:

<property>
  <name>mapred.jobtracker.plugins</name>
  <value>org.apache.hadoop.thriftfs.ThriftJobTrackerPlugin</value>
  <description>Comma-separated list of jobtracker plug-ins to be activated.
  </description>
</property>


Problems?
---------

Search or ask questions on the forum and http://groups.google.com/a/cloudera.org/group/hue-user

Some packages might be required during the 'make install': https://github.com/cloudera/hue#development-prerequisites)

Download a pre-built packaged version (apt-get/yum install...) of Hue on http://gethue.com

Replaced the plain DocBook XSL admonition icons with Jimmac's DocBook
icons (http://jimmac.musichall.cz/ikony.php3). I dropped transparency
from the Jimmac icons to get round MS IE and FOP PNG incompatibilies.

Stuart Rackham


<link rel="stylesheet" href="docbook.css" type="text/css" media="screen" title="no title" charset="utf-8"></link>

Hue SDK Documentation
=====================

[TOC]

Introduction and Overview
=========================

Hue leverages the browser to provide users with an environment for exploring
and analyzing data.

Build on top of the Hue SDK to enable your application to interact efficiently with
Hadoop and the other Hue services.

By building on top of Hue SDK, you get, out of the box:

+ Configuration Management
+ Hadoop interoperability
+ Supervision of subprocesses
+ A collaborative UI
+ Basic user administration and authorization

This document will orient you with the general structure of Hue
and will walk you through adding a new application using the SDK.

NOTE: Hue began its life as "Cloudera Desktop," so you may find
references to "Desktop" in a few places.

From 30,000 feet
----------------

![From up on high](from30kfeet.png)

Hue, as a "container" web application, sits in between your Hadoop installation
and the browser.  It hosts all the Hue Apps, including the built-in ones, and
ones that you may write yourself.

The Hue Server
--------------

![Web Back-end](webbackend.png)

Hue is a web application built on the Django python web framework.
Django, running on the WSGI container/web server (typically CherryPy), manages
the url dispatch, executes application logic code, and puts together the views
from their templates.  Django uses a database (typically sqlite)
to manage session data, and Hue applications can use it as well
for their "models".  (For example, the JobDesigner application stores
job designs in the database.)

In addition to the web server, some Hue applications run
daemon processes "on the side".  For example, Beeswax runs a daemon
("beeswax_server") that keeps track of query states.  Running
a separate process for applications is the preferred
way to manage long-running tasks that you may wish
to co-exist with web page rendering.  The web "views"
typically communicate with these side daemons
by using Thrift (e.g., for Beeswax query execution) or by exchanging state
through the database.

Interacting with Hadoop
-----------------------

![Interacting with Hadoop](interactingwithhadoop.png)

Hue provides some APIs for interacting with Hadoop.
Most noticeably, there are python file-object-like APIs for
interacting with HDFS.  These APIs work by making REST API or Thrift calls
the Hadoop daemons. The Hadoop administrator must enable these interfaces from
Hadoop.

On the Front-End
----------------

Hue provides a front-end framework based on
[Bootstrap](http://twitter.github.com/bootstrap/) and
[jQuery](http://jquery.com/).

If you are used to the Hue 1.x front-end, this is a major difference. All
application pages are full screen requests from the browser. The HTML generated
by your application's template is directly rendered. You do not need to
worry about interference from another application. And you have more freedom to
customize the front-end behavior of your application.

An Architectural View
---------------------

![Architecture](architecture.png)

A Hue application may span three tiers: (1) the UI
and user interaction in the client's browser, (2) the
core application logic in the Hue web
server, and (3) external services with which applications
may interact.

The absolute minimum that you must implement (besides
boilerplate), is a
"Django [view](https://docs.djangoproject.com/en/1.2/topics/http/views/)"
function that processes the request and the associated template
to render the response into HTML.

Many apps will evolve to have a bit of custom JavaScript and
CSS styles.  Apps that need to talk to an external service
will pull in the code necessary to talk to that service.

Pre-requisites
==============

Software
--------

Developing for the Hue SDK has similar requirements to running
Hue itself.  We require python (2.4 to 2.7), Django (1.2 included
with our distribution), Hadoop (Cloudera's Distribution including Apache Hadoop,
at least version 4), Java (Sun Java 1.6), and Firefox (at least 3.0).

Recommended Reading / Important Technologies
--------------------------------------------

The following are core technologies used inside of Hue.

* Python.  <a href="http://diveintopython.net/">Dive Into Python</a> is one of
  several excellent books on python.
* Django.  Start with [The Django Tutorial](http://docs.djangoproject.com/en/1.2/intro/tutorial01/).
* [Thrift](http://incubator.apache.org/thrift/) is used for communication
  between daemons.
* [Mako](http://www.makotemplates.org/) is the preferred templating language.

Fast-Guide to Creating a New Hue Application
============================================

Now that we have a high-level overview of what's going on,
let's go ahead and create a new installation.

Download, Unpack, Build Distro
------------------------------

The Hue SDK is available from [Github](http://github.com/cloudera/hue). Releases
can be found on the [download page](https://github.com/cloudera/hue/downloads).
Releases are missing a few dependencies that could not be included because of
licencing issues (e.g. the werkzeug module). So if you prefer to have an
environment ready from scratch, it is preferable to checkout a particular
release tag instead.

    $ cd hue
    ## Build
    $ make apps
    ## Run
    $ build/env/bin/hue runserver_plus
    $ build/env/bin/hue beeswax_server
    ## Alternative run
    $ build/env/bin/hue supervisor
    ## Visit http://localhost:8000/ with your web browser.

<div class="note">
  Why <code>runserver_plus</code>?   <code>runserver_plus</code>
  enables the <a href="http://werkzeug.pocoo.org/">Werkzeug</a> debugger,
  which is very handy.
</div>

Run "create_desktop_app" to Set up a New Source Tree
--------------------------------------------

    $ ./build/env/bin/hue create_desktop_app calculator
    $ find calculator -type f
    calculator/setup.py                                 # distutils setup file
    calculator/src/calculator/__init__.py               # main src module
    calculator/src/calculator/forms.py
    calculator/src/calculator/models.py
    calculator/src/calculator/settings.py               # app metadata setting
    calculator/src/calculator/urls.py                   # url mapping
    calculator/src/calculator/views.py                  # app business logic
    calculator/src/calculator/templates/index.mako
    calculator/src/calculator/templates/shared_components.mako

    # Static resources
    calculator/src/calculator/static/art/calculator.png # logo
    calculator/src/calculator/static/css/calculator.css
    calculator/src/calculator/static/js/calculator.js


Install SDK Application
-----------------------

As you'll discover if you look at calculator's <tt>setup.py</tt>,
Hue uses a distutils <tt>entrypoint</tt> to
register applications.  By installing the calculator
package into Hue's python virtual environment,
you'll install a new app.  The "app_reg.py" tool manages
the applications that are installed. Note that in the following example, the value after the
"--install" option is the path to the root directory of the application you want to install. In this
example, it is a relative path to "/Users/philip/src/hue/calculator".

        $ ./build/env/bin/python tools/app_reg/app_reg.py --install calculator
        === Installing app at calculator
        Updating registry with calculator (version 0.1)
        --- Making egg-info for calculator

        $ ./build/env/bin/python tools/app_reg/app_reg.py --list 2>&1 | grep calculator
        calculator           0.1     /Users/philip/src/hue/calculator

<div class="note">
  If you'd like to customize the build process, you can modify (or even complete
  rewrite) your own `Makefile`, as long as it supports the set of required
  targets. Please see `Makefile.sdk` for the required targets and their
  semantics.
</div>

Congrats, you've added a new app!

<div class="note">
  What was that all about?
  <a href="http://pypi.python.org/pypi/virtualenv">virtualenv</a>
  is a way to isolate python environments in your system, and isolate
  incompatible versions of dependencies.  Hue uses the system python, and
  that's about all.  It installs its own versions of dependencies.

  <a href="http://peak.telecommunity.com/DevCenter/PkgResources#entry-points">Entry Points</a>
  are a way for packages to optionally hook up with other packages.
</div>

You can now browse the new application.

    # If you haven't killed the old process, do so now.
    $ build/env/bin/hue runserver_plus

And then visit <a href="http://localhost:8000">http://localhost:8000/</a> to check it out!
You should see the app (with a boring "SDK" icon) in the dock, and clicking it
will bring up a boring screen:

<img src="new_app_in_dock.png">


Customizing Views and Templates
-------------------------------

Now that your app has been installed, you'll want to customize it.
As you may have guessed, we're going to build a small calculator
application.  Edit `calculator/src/calculator/templates/index.mako`
to include a simple form:

    <%!from desktop.views import commonheader, commonfooter %>
    <%namespace name="shared" file="shared_components.mako" />

    ${commonheader("Calculator", "calculator", user, "100px") | n,unicode}

    ## Main body

    <div class="container-fluid">
      % if op:
      <span>${a} ${op} ${b} = ${result}</span>
      % endif
      <form action=${url("calculator.views.index")} method=POST>
        <input name="a">
        <input type="radio" name="op" value="add">+</input>
        <input type="radio" name="op" value="subtract">-</input>
        <input type="radio" name="op" value="multiply">*</input>
        <input type="radio" name="op" value="divide">/</input>
        <input name="b">
        <input type="submit" value="Calculate">
      </form>
    </div>
    ${commonfooter(messages) | n,unicode}

The template language here is <a href="http://www.makotemplates.org/docs/">Mako</a>,
which is flexible and powerful.  If you use the "`.html`" extension, Hue
will render your page using
<a href="http://docs.djangoproject.com/en/1.2/topics/templates/#topics-templates">Django templates</a>
instead.

Note that we used the `url()` function to generate the URL to the calculator
view.  This trick protects you a bit from changing URLs.

Let's edit `calculator/src/calculator/views.py` to process that form:

    #!/usr/bin/env python

    from desktop.lib.django_util import render
    import operator

    OPS=dict(add=operator.add, subtract=operator.sub, multiply=operator.mul, divide=operator.truediv)
    OP_STRING=dict(add="+", subtract="-", multiply="*", divide="/")

    def index(request):
      if "op" not in request.REQUEST:
        return render('index.mako', request, dict())
      a = float(request.REQUEST["a"])
      b = float(request.REQUEST["b"])
      op = request.REQUEST["op"]
      result = OPS[op](a, b)
      return render('index.mako', request,
        dict(a=a, b=b, op=OP_STRING[op], result=result))

For more complicated forms, you may want to use Django Forms and
avoid explicitly using `request.REQUEST`, but this is shorter.

You can now go and try the calculator.  If you set everything up right, you
should see something like:

<img src="calculator_working.png">

Debugging Django
----------------

<img src="calculator_error.png">

If you enter a number only in the first text box and hit "Calculate", you'll
hit an error. If you're using `runserver_plus`, you'll get a handy debugging
page. You can click on any stack frame to get a debugging console:

<img src="calculator_werkzeug.png">

Great! Now that we've added a single application, we're going to
delve further into the back-end.


Integrate external Web applications in any language
===================================================
Use the [create_proxy_app command](http://gethue.tumblr.com/post/66367939672/integrate-external-web-applications-in-any-language)


A Look at Three Existing Apps
=============================

![Arch](arch_examples.png)

Help
----

The Help application is as minimal as they get.  Take a look at it!
The core logic is in the "views.py" file.  The central function
there takes `(app, path)` (which are mapped from the request URL
by the regular expression in `urls.py`).  The view function
finds the data file that needs to be rendered, renders it through
the markdown module, if necessary, and then displays it through
a simple template.

You'll note that the "Help Index" is presented in a "split view".
No JavaScript was written to make this happen!  Instead, the template
applied certain CSS classes to the relevant `div`'s, and JFrame
did the rest.

Proxy
-----

### Setup

You need to have Hue running:

    $ ./build/env/bin/hue runserver

Then if you want to access localhost/50030/jobtracker.jsp you just do:

    http://127.0.0.1:8000/proxy/localhost/50030/jobtracker.jsp

and the page will be displayed within Hue.

You can configure it in ``desktop/conf/pseudo-distributed.ini``

    [proxy]
    whitelist="(localhost|127\.0\.0\.1)50030|50070|50060|50075)",
    #Comma-separated list of regular expressions, which match 'host:port' of requested proxy target.

    blacklist=""
    #Comma-separated list of regular expressions, which match any prefix of 'host:port/path' of requested proxy target.
    # This does not support matching GET parameters.

### Usage

You can create a new app (or modify a current one for testing).

Then in order to display the proxied page in your app, you could add in the template of a view of the new app a
snippet of Javacript similar to this for loading the JobTracker page:

    <script>
        $.get('/proxy/localhost/50030/jobtracker.jsp', function(data) { $('#proxy-body').html(data); alert('Load was performed.'); });
    </script>

or alternatively get the page in the view (better solution) with the Hue
[REST API](https://github.com/cloudera/hue/tree/master/desktop/core/src/desktop/lib/rest). Example of use of
this API can be found in the [HDFS lib](https://github.com/cloudera/hue/blob/master/desktop/libs/hadoop/src/hadoop/fs/webhdfs.py).

If you need to browse through the proxied page, using an iframe might be a better solution.


Beeswax
-------

Beeswax is on the opposite end of the complexity scale from Help.
In addition to many views (in `views.py`), Beeswax uses
Django Forms for server-side form validation (the forms are in `forms.py`),
several features of the Mako templating engine (especially includes and
functions), a separate server (implemented in Java), and significant
JavaScript for user interaction.

Backend Development
===================

This section goes into greater detail on useful features within
the Hue environment.

User Management
---------------

Except for static content, `request.user` is always populated.  It is a
standard Django `models.User` object.  If you were to set a breakpoint at the
`index()` function in our calculator app, you will find:

    >>> request.user
    <User: test>

<div class="note">
  "Under the covers:" Django uses a notion called
  <a href="https://docs.djangoproject.com/en/1.2/topics/http/middleware/">middleware</a>
  that's called in between the request coming in and the view being executed.
  That's how <code>request.user</code> gets populated.  There's also a
  middleware for Hue that makes sure that no pages are displayed unless the
  user is authenticated.
</div>

Configuration
-------------

### Configuration File

Hue uses a typed configuration system that reads configuration files (in an
ini-style format).  By default, Hue loads all `*.ini` files in the `build/desktop/conf`
directory.  The configuration files have the following format:

    # This is a comment
    [ app_name ]          # Same as your app's name
    app_property = "Pink Floyd"

    [[ section_a ]]         # The double brackets start a section under [ app_name ]
    a_weight = 80         # that is useful for grouping
    a_height = 180

    [[ filesystems ]]       # Sections are also useful for making a list
    [[[ cluster_1 ]]]       # All list members are sub-sections of the same type
    namenode_host = localhost
    # User may define more:
    # [[[ cluster_2 ]]]
    # namenode_host = 10.0.0.1


### Configuration Variables

Your application's `conf.py` is special. It provides access to the configuration file (and even
default configurations not specified in the file). Using the above example, your `conf.py` should
define the following:

* A `desktop.lib.conf.Config` object for `app_property`, such as:
<pre>
  MY_PROPERTY = Config(key='app_property', default='Beatles', help='blah')
</pre>
  You can access its value by `MY_PROPERTY.get()`.

* A `desktop.lib.conf.ConfigSection` object for `section_a`, such as:
<pre>
  SECTION_A = ConfigSection(key='section_a',
        help='blah',
        members=dict(
          AWEIGHT=Config(key='a_weight', type=int, default=0),
          AHEIGHT=Config(key='a_height', type=int, default=0)))
</pre>
  You can access the values by `SECTION_A.AWEIGHT.get()`.

* A `desktop.lib.conf.UnspecifiedConfigSection` object for `filesystems`, such as:
<pre>
  FS = UnspecifiedConfigSection(
      key='filesystems',
      each=ConfigSection(members=dict(
          nn_host=Config(key='namenode_host', required=True))
</pre>
  An `UnspecifiedConfigSection` is useful when the children of the section are not known.
  When Hue loads your application's configuration, it binds all sub-sections. You can
  access the values by:
<pre>
  cluster1_val = FS['cluster_1'].nn_host.get()
  all_clusters = FS.keys()
  for cluster in all_clusters:
      val = FS[cluster].nn_host.get()
</pre>

Your Hue application can automatically detect configuration problems and alert
the admin. To take advantage of this feature, create a `config_validator`
function in your `conf.py`:

<pre>
  def config_validator(user):
    """
    config_validator(user) -> [(config_variable, error_msg)] or None
    Called by core check_config() view.
    """
    res = [ ]
    if not REQUIRED_PROPERTY.get():
      res.append((REQUIRED_PROPERTY, "This variable must be set"))
    if MY_INT_PROPERTY.get() < 0:
      res.append((MY_INT_PROPERTY, "This must be a non-negative number"))
    return res
</pre>


<div class="note">
  You should specify the <code>help="..."</code> argument to all configuration
  related objects in your <code>conf.py</code>. The examples omit some for the
  sake of space. But you and your application's users can view all the
  configuration variables by doing:
  <pre>
    $ build/env/bin/hue config_help
  </pre>
</div>


Running "Helper Processes"
--------------------------

Some Hue applications need to run separate daemon processes on the side.
For example, `BeeswaxServer` is responsible for managing Hive query states.
The Hue "views" communicate with it through Thrift and shared states in the
Django database.

Suppose your application needs a helper `my_daemon.py`. You need to register it by:

* In `setup.py`, add to `entry_points`:
<pre>
    entry_points = {
      'desktop.sdk.application': 'my_app = my_app',
      'desktop.supervisor.specs': [ 'my_daemon = my_app:SUPERVISOR_SPEC' ] }
</pre>

* In `src/my_app/__init__.py`, tell Hue what to run by adding:
<pre>
    SUPERVISOR_SPEC = dict(django_command="my_daemon")
</pre>

* Then in `src/my_app/management/commands`, create `__init__.py` and `my_daemon.py`. Your
  daemon program has only one requirement: it must define a class called `Command` that
  extends `django.core.management.base.BaseCommand`. Please see `kt_renewer.py` for an example.

The next time Hue restarts, your `my_daemon` will start automatically.
If your daemon program dies (exits with a non-zero exit code), Hue will
restart it.

"Under the covers:" Threading.  Hue, by default, runs CherryPy web server.
If Hue is configured (and it may be, in the future)
to use mod_wsgi under Apache httpd, then there would be multiple python
processes serving the backend.  This means that your Django application
code should avoid depending on shared process state.  Instead, place
the stored state in a database or run a separate server.

<!-- "Wheel reinvention" Supervisor is following the Erlang model. -->

Walk-through of a Django View
-----------------------------

![Django Flow](django_request.png)

Django is an MVC framework, except that the controller is called a
"[view](https://docs.djangoproject.com/en/1.2/topics/http/views/)" and
the "view" is called a "template".  For an application developer, the essential
flow to understand is how the "urls.py" file provides a mapping between URLs (expressed as a
regular expression, optionally with captured parameters) and view functions.
These view functions typically use their arguments (for example, the captured parameters) and
their request object (which has, for example, the POST and GET parameters) to
prepare dynamic content to be rendered using a template.

Templates: Django and Mako
--------------------------

In Hue, the typical pattern for rendering data through a template
is:

    from desktop.lib.django_util import render

    def view_function(request):
      return render('view_function.mako', request, dict(greeting="hello"))

The `render()` function chooses a template engine (either Django or Mako) based on the
extension of the template file (".html" or ".mako").  Mako templates are more powerful,
in that they allow you to run arbitrary code blocks quite easily, and are more strict (some
would say finicky); Django templates are simpler, but are less expressive.

Django Models
-------------

[Django Models](http://docs.djangoproject.com/en/1.2/topics/db/models/#topics-db-models)
are Django's Object-Relational Mapping framework.  If your application
needs to store data (history, for example), models are a good way to do it.

From an abstraction perspective, it's common to imagine external services
as "models".  For example, the Job Browser treats the Hadoop JobTracker
as a "model", even though there's no database involved.

Accessing Hadoop
----------------

It is common for applications to need to access the underlying HDFS.
The `request.fs` object is a "file system" object that exposes
operations that manipulate HDFS.  It is pre-configured to access
HDFS as the user that's currently logged in.  Operations available
on `request.fs` are similar to the file operations typically
available in python.  See `webhdfs.py` for details; the list
of functions available is as follows:
`chmod`,
`chown`,
`exists`,
`isdir`,
`isfile`,
`listdir` (and `listdir_stats`),
`mkdir`,
`open` (which exposes a file-like object with `read()`, `write()`, `seek()`, and `tell()` methods),
`remove`,
`rmdir`,
`rmtree`, and
`stats`.


Making Your Views Thread-safe
-----------------------------

Hue works in any WSGI-compliant container web server.
The current recommended deployment server is the built-in CherryPy server.
The CherryPy server, which is multi-threaded, is invoked by `runcpserver`
and is configured to start when Hue's `supervisor` script is used.
Meanwhile, `runserver` and `runserver_plus` start a single-threaded
testing server.

Because multiple threads may be accessing your views
concurrently, your views should not use shared state.
An exception is that it is acceptable to initialize
some state when the module is first imported.
If you must use shared state, use Python's `threading.Lock`.

Note that any module initialization may happen multiple times.
Some WSGI containers (namely, Apache), will start multiple
Unix processes, each with multiple threads.  So, while
you have to use locks to protect state within the process,
there still may be multiple copies of this state.

For persistent global state, it is common to place the state
in the database.  If the state needs to be managed with application code,
a common pattern to push state into a "helper process".  For example, in the Job Designer,
a helper process keeps track of the processes that have been launched.  The Django views
themselves are stateless, but they talk to this stateful helper process for
updates.  A similar approach is taken with updating metrics for
the Beeswax application.

Authentication Backends
-----------------------

Hue exposes a configuration flag ("auth") to configure
a custom authentication backend.  See
See http://docs.djangoproject.com/en/dev/topics/auth/#writing-an-authentication-backend
for writing such a backend.

In addition to that, backends may support a `manages_passwords_externally()` method, returning
True or False, to tell the user manager application whether or not changing
passwords within Hue is possible.

Authorization
-------------

Applications may define permission sets for different actions. Administrators
can assign permissions to user groups in the UserAdmin application. To define
custom permission sets, modify your app's `settings.py` to create a list of
`(identifier, description)` tuples:

    PERMISSION_ACTIONS = [
      ("delete", "Delete really important data"),
      ("email", "Send email to the entire company"),
      ("identifier", "Description of the permission")
    ]

Then you can use this decorator on your view functions to enforce permission:

    @desktop.decorators.hue_permission_required("delete", "my_app_name")
    def delete_financial_report(request):
      ...

Using and Installing Thrift
---------------------------
Right now, we check in the generated thrift code.
To generate the code, you'll need the thrift binary version 0.9.0.
Please download from http://thrift.apache.org/.

The modules using ``Thrift`` have some helper scripts like ``regenerate_thrift.sh``
for regenerating the code from the interfaces.

Profiling Hue Apps
------------------
Hue has a profiling system built in, which can be used to analyze server-side
performance of applications.  To enable profiling::

    $ build/env/bin/hue runprofileserver

Then, access the page that you want to profile.  This will create files like
/tmp/useradmin.users.000072ms.2011-02-21T13:03:39.745851.prof.  The format for
the file names is /tmp/<app_module>.<page_url>.<time_taken>.<timestamp>.prof.

Hue uses the hotshot profiling library for instrumentation.  The documentation
for this library is located at: http://docs.python.org/library/hotshot.html.

You can use kcachegrind to view the profiled data graphically::

    $ hotshot2calltree /tmp/xyz.prof > /tmp/xyz.trace
    $ kcachegrind /tmp/xyz.trace

More generally, you can programmatically inspect a trace::

    #!/usr/bin/python
    import hotshot.stats
    import sys

    stats = hotshot.stats.load(sys.argv[1])
    stats.sort_stats('cumulative', 'calls')
    stats.print_stats(100)

This script takes in a .prof file, and orders function calls by the cumulative
time spent in that function, followed by the number of times the function was
called, and then prints out the top 100 time-wasters.  For information on the
other stats available, take a look at this website:
http://docs.python.org/library/profile.html#pstats.Stats


<!--
## Django Models

## Caution: upgrade path
-->

Front-end Development
=====================

Developing applications for Hue requires a minimal amount of CSS
(and potentially JavaScript) to use existing functionality. As covered above,
creating an application for the Hue is a matter of creating a standard HTML
application.

In a nutshell, front-end development in Hue is using
[Bootstrap](http://twitter.github.com/bootstrap/) and
[jQuery](http://jquery.com/) to layout your app and script the custom
interactions.


CSS Styles
----------

Hue uses [Bootstrap](http://twitter.github.com/bootstrap/) version 2.0 CSS
styles and layouts. They are highly reusable and flexible. Your app doesn't
have to use these styles, but if you do, it'll save you some time and make your
app look at home in Hue.

On top of the standard Bootstrap styles, Hue defines a small set of custom
styles in *desktop/core/static/css/jhue.css*.

Defining Styles for Your Application
------------------------------------

When you create your application it will provision a CSS file for you in the
*static/css* directory. For organization purposes, your styles should go here
(and any images you have should go in *static/art*). Your app's name will be a
class that is assigned to the root of your app in the DOM. So if you created an
app called "calculator" then every window you create for your app will have the
class "calculator".  Every style you define should be prefixed with this to
prevent you from accidentally colliding with the framework style. Examples:

    /* the right way: */
    .calculator p {
      /* all my paragraphs should have a margin of 8px */
      margin: 8px;
      /* and a background from my art directory */
      background: url(../art/paragraph.gif);
    }
    /* the wrong way: */
    p {
      /* woops; we're styling all the paragraphs on the page, affecting
         the common header! */
      margin: 8px;
      background: url(../art/paragraph.gif);
    }

Icons
-----

You should create an icon for your application that is a transparent png sized
24px by 24px. Your `settings.py` file should point to your icon via the `ICON`
variable. The `create_desktop_app` command creates a default icon for you.

<div class="note">
  If you do not define an application icon, your application will not show up
  in the navigation bar.
</div>

Hue ships with Twitter Bootstrap and Font Awesome 3 (http://fortawesome.github.io/Font-Awesome/)
so you have plenty of scalable icons to choose from. You can style your elements to use them
like this (in your mako template):

    <!-- show a trash icon in a link -->
    <a href="#something"><i class="icon-trash"></i> Trash</a>

Adding Interactive Elements to Your UI
--------------------------------------

Hue by default loads these JavaScript components:

* jQuery 1.8.1
* jQuery.dataTables 1.8.2
* Bootstrap 2.1.1

These are used by some Hue applications, but not loaded by default:

* Knockout 2.1.1 (`desktop/core/static/ext/js/knockout-min.js`)
* DataTables pagination using the Bootstrap style (`desktop/core/static/ext/js/datatables-paging-0.1.js`)
* jQuery UI autocomplete 1.8.18 (`desktop/core/static/ext/js/jquery/plugins/jquery-ui-autocomplete-1.8.18.min.js`)

These standard components have their own online documentation, which we will
not repeat here. They let you write interactive behaviors with little or no
JavaScript.

## Key Differences from Hue 1.x

Here are the key differences between the Hue 1.x front-end SDK and the later
versions. In Hue 2.0 and beyond:

* Since each page view only loads one application, you can declare HTML
  elements by ID, you can declare and load your JavaScripts anywhere, and
  are in full control of all the UI interactions.
* You can use standard HTML links to other applications.
* You do not need to register your application with the front-end, or declare
  any dependencies using YAML.
* The navigation bar is not pluggable in Hue 2.0.
* The old "accordion" behavior can be replaced by
  [Bootstrap collapse](http://twitter.github.com/bootstrap/javascript.html#collapse).
* The old "art buttons" pattern can be replaced by [Bootstrap
  buttons](http://twitter.github.com/bootstrap/base-css.html#buttons), and
  [button
  groups](http://twitter.github.com/bootstrap/components.html#buttonGroups).
* The old "art inputs" pattern can be replaced by [Bootstrap
  form inputs](http://twitter.github.com/bootstrap/base-css.html#forms).
* The old "autocomplete" behavior can be replaced by [jQuery
  autocomplete](http://jqueryui.com/demos/autocomplete/) or [Bootstrap
  typeahead](http://twitter.github.com/bootstrap/javascript.html#typeahead).
* The old "collapser" behavior can be replaced by
  [Bootstrap collapse](http://twitter.github.com/bootstrap/javascript.html#collapse).
* The old "context menu" behavior can be replaced by [Bootstrap button
  dropdowns](http://twitter.github.com/bootstrap/components.html#buttonDropdowns).
* The old "fittext" behavior is no longer supported.
* The old "flash message" behavior is no longer supported.
* The old "html table" behavior can be replaced by
  [DataTables](http://datatables.net/).
* The old "overtext" behavior can be replaced by [Bootstrap form
  placeholder](http://twitter.github.com/bootstrap/base-css.html#forms).
* The old "popup" behavior can be replaced by
  [Bootstrap modals](http://twitter.github.com/bootstrap/javascript.html#modals).
* The old "side-by-side select" pattern is no longer supported.
* The old "splitview" layout is no longer supported.
* The old "tabs" layout can be replaced by [Bootstrap
  tabs](http://twitter.github.com/bootstrap/javascript.html#tabs).
* The old "tool tips" behavior can be replaced by [Bootstrap
  tooltips](http://twitter.github.com/bootstrap/javascript.html#tooltips).


Including Other JavaScript Frameworks
-------------------------------------

It is possible to include other JavaScript frameworks to do your development.
Simply include them to your application's pages.  MooTools, Dojo, YUI, etc are
all fine. Including them represents an additional burden for your users to
download, and they also make it harder for us to support you, but it is your
call.

<!-- ## Adding dynamic data to the nav bar -->

<!-- ## Knockout, jQuery -->

<!-- ## Lost: Keyboard shortcuts -->


Internationalization
====================
How to update all the messages and compile them::

    $ make locales

How to update and compile the messages of one app::

    $ cd apps/beeswax
    $ make compile-locale

How to create a new locale for an app::

    $ cd $APP_ROOT/src/$APP_NAME/locale
    $ $HUE_ROOT/build/env/bin/pybabel init -D django -i en_US.pot -d . -l fr


Debugging Tips and Tricks
=========================

* Set `DESKTOP_DEBUG=1` as an environment variable if you want logs to go to stderr
  as well as to the respective log files.
* Use runserver_plus.  If you want to set a CLI breakpoint, just insert
  `__import__("ipdb").set_trace()`
  into your code.  If you want to inspect variables, you can simply insert
  `raise None`, and visit the URL of the view you're interested in, activating
  the Werkzeug debugger.
* Django tends to restart its server whenever it notices a file changes.  For
  certain things (like configuration changes), this is not sufficient.  Restart
  the server whole-heartedly.
* If you find yourself writing a lot of JavaScript, you'll want to disable the
  JavaScript caching that the server does. At startup Hue reads all your
  dependencies and JS files into memory to make things faster. You can disable
  this by executing the runserver_plus command with an environment variable
  set. Hue will be a little slower, but your JS will always represent what's on
  the disk. Here's what that looks like:

    `$ DESKTOP_DEPENDER_DEBUG=1 build/env/bin/hue runserver_plus`

* We highly recommend developing with the [Firebug](http://getfirebug.com)
  debugging plugin for Firefox. With it enabled, you can use a utility called
  [dbug](http://www.clientcide.com/docs/Core/dbug) which wraps Firebug
  commands. This allows you to leave debug statements in your code and display
  them on demand. In particular, typing in `dbug.cookie()` in Firebug will set
  a cookie in your browser that will turn these statements on until you type
  that command again to toggle them off. You'll see some of our own debugging
  statements and you can add your own. In the future, entering this state may
  also provide access to additional debugging features.
* When the dbug state is enabled in the browser, right clicking on elements is
  re-enabled which makes element inspection a little easier in Firebug.

<!--

## runserver_plus

## testing with windmill

## Testing with django

# Packaging your app for installation elsewhere

# Advanced Issues

## Modifying the Hadoop plug-ins


Build system:
- How to add external dependencies
- Plugging into the status_bar.
- Password_protecting and not password protecting.

-->


<link rel="stylesheet" href="docbook.css" type="text/css" media="screen" title="no title" charset="utf-8"></link>

About Hue
=========

The **About Hue** application displays the version of Hue you are
running. If you are a superuser, it lets you perform Hue setup tasks,
and lets you view configuration and logs.

Starting About Hue
------------------

To start the About Hue application, click
![image](images/quick_start.png) in the navigation bar at the top of the
Hue browser page. It opens to the Quick Start Wizard screen.

Quick Start Wizard
------------------

The Quick Start wizard allows you to perform the following Hue setup
operations by clicking the tab of each step or sequentially by clicking
Next in each screen:

1.  **Check Configuration** validates your Hue configuration. It will
    note any potential misconfiguration and provide hints as to how to
    fix them. You can edit the configuration file described in the next
    section or use Cloudera Manager, if installed, to manage your
    changes.
2.  **Examples** contains links to install examples into the Beeswax,
    Cloudera Impala, Metastore Manager, Job Designer, Oozie
    Editor/Dashboard, and Pig Editor applications.
3.  **Users** contains a link to the User Admin application to create or
    import users and a checkbox to enable and disable collection of
    usage information.
4.  **Go!** - displays the Hue home screen, which contains links to the
    different categories of applications supported by Hue: Query,
    Hadoop, and Workflow.

Configuration
-------------

Displays a list of the installed Hue applications and their
configuration. The location of the folder containing the Hue
configuration files is shown at the top of the page. Hue configuration
settings are in the hue.ini configuration file.

Click the tabs under **Configuration Sections and Variables** to see the
settings configured for each application. For information on configuring
these settings, see Hue Configuration in the Hue installation manual.

Server Logs
-----------

Displays the Hue Server log and allows you to download the log to your
local system in a zip file.


<link rel="stylesheet" href="docbook.css" type="text/css" media="screen" title="no title" charset="utf-8"></link>

User Admin
==========

The User Admin application lets a superuser add, delete, and manage Hue
users and groups, and configure group permissions. Superusers can add
users and groups individually, or import them from an LDAP directory.
Group permissions define the Hue applications visible to group members
when they log into Hue and the application features available to them.

Starting User Admin
-------------------

Click the **User Admin** icon (![image](images/icon_useradmin_24.png))
in the navigation bar at the top of the Hue browser page. The Hue Users
page opens.

Users
-----

The User Admin application provides two levels of user privileges:
superusers and users.

-   Superusers  The first user who logs into Hue after its initial
    installation becomes the first superuser. Superusers have
    permissions to perform administrative functions:
    -   Add and delete users
    -   Add and delete groups
    -   Assign permissions to groups
    -   Change a user into a superuser
    -   Import users and groups from an LDAP server

-   Users  can change their name, e-mail address, and password and log
    in to Hue and run Hue applications, subject to the permissions
    provided by the Hue groups to which they belong.

### Adding a User

1.  In the **User Admin** page, click **Add User**.
2.  In the **Credentials** screen, add required information about the
    user. Once you provide the required information you can click the
    wizard step tabs to set other information.
    
 <table>
<tr><td>Username</td><td>  A user name that contains only letters, numbers, and underscores;
    blank spaces are not allowed and the name cannot begin with a
    number. The user name is used to log into Hue and in file
    permissions and job submissions. This is a required field.
</td></tr>
<tr><td>Password and Password confirmation</td><td>    A password for the user. This is a required field.</td></tr>
<tr><td>Create home directory</td><td>   Indicate whether to create a directory named /user/username in HDFS.
    For non-superusers, the user and group of the directory are
    username. For superusers, the user and group are username and
    supergroup.</td></tr></table>

 

3.  Click **Add User** to save the information you specified and close
    the **Add User** wizard or click **Next**.
4.  In the **Names and Groups** screen, add optional information.

<table>
<tr><td>First name and Last name</td><td> The user's first and last name.
</td></tr>
<tr><td>E-mail address</td><td>The user's e-mail address. The e-mail address is used by the Job
    Designer and Beeswax applications to send users an e-mail message
    after certain actions have occurred. The Job Designer sends an
    e-mail message after a job has completed. Beeswax sends a message
    after a query has completed. If an e-mail address is not specified,
    the application will not attempt to email the user.</td></tr>
<tr><td>Groups</td><td> The groups to which the user belongs. By default, a user is assigned
    to the **default** group, which allows access to all applications.
    See [Permissions](#permissions).</td></tr></table>
    

5.  Click **Add User** to save the information you specified and close
    the **Add User** wizard or click **Next**.
6.  In the **Advanced** screen, add status information.

<table>
<tr><td>Active</td><td> Indicate that the user is enabled and allowed to log in. Default: checked.</td></tr>
<tr><td>Superuser status</td><td> Assign superuser privileges to the user.</td></tr></table>

7.  Click **Add User** to save the information you specified and close
    the **Add User** wizard.

### Deleting a User

1.  Check the checkbox next to the user name and click **Delete**.
2.  Click **Yes** to confirm.

### Editing a User

1.  Click the user you want to edit in the **Hue Users** list.
2.  Make the changes to the user and then click **Update user**.

### Importing Users from an LDAP Directory

Hue must be configured to use an external LDAP directory (OpenLDAP or
Active Directory). See Hue Installation in [CDH4
Installation](http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/latest/CDH4-Installation-Guide/CDH4-Installation-Guide.html).

![image](images/note.jpg) **Note**:

Importing users from an LDAP directory does not import any password
information. You must add passwords manually in order for a user to log
in.

To add a user from an external LDAP directory:

1.  Click **Add/sync LDAP user**.
2.  Specify the user properties:

<table>
<tr><td>Username</td><td>The user name.</td></tr>
<tr><td>Distinguished name</td><td>Indicate that Hue should use a full distinguished name for the user.
    This imports the user's first and last name, username, and email,
    but does not store the user password.</td></tr>
    <tr><td>Create home directory</td><td> Indicate that Hue should create a home directory for the user in
    HDFS.</td></tr></table>


3.  Click **Add/sync user**.

    If the user already exists in the User Admin, the user information
    in User Admin is synced with what is currently in the LDAP
    directory.

### Syncing Users and Groups with an LDAP Directory

You can sync the Hue user database with the current state of the LDAP
directory using the **Sync LDAP users/groups** function. This updates
the user and group information for the already imported users and
groups. It does not import any new users or groups.

1.  Click **Sync LDAP users/groups**.
2.  The **Create Home Directories** checkbox creates home directories in
    HDFS for existing imported members that don't have home directories.
3.  In the **Sync LDAP users and groups** dialog, click **Sync** to
    perform the sync.

Groups
------

Superusers can add and delete groups, configure group permissions, and
assign users to group memberships.

### Adding a Group

You can add groups, and delete the groups you've added. You can also
import groups from an LDAP directory.

1.  In the **User Admin** window, click **Groups** and then click **Add
    Group**.
2.  Specify the group properties:

<table>
<tr><td>Name</td><td> The name of the group. Group names can only be letters, numbers, and
    underscores; blank spaces are not allowed.</td></tr>
<tr><td>Members</td><td>The users in the group. Check user names or check Select all.</td></tr>
    <tr><td>Permissions</td><td>The applications the users in the group can access. Check
    application names or check Select all.</td></tr></table>

3.  Click **Add group**.

### Adding Users to a Group

1.  In the **User Admin** window, click **Groups**.
2.  Click the group.
3.  To add users to the group, check the names in the list provided or
    check **Select All**.
4.  Click **Update group**.

### Deleting a Group

1.  Click **Groups**.
2.  Check the checkbox next to the group and click **Delete**.
3.  Click **Yes** to confirm.

### Importing Groups from an LDAP Directory

1.  From the **Groups** tab, click **Add/sync LDAP group**.
2.  Specify the group properties:

<table>
<tr><td>Name</td><td> The name of the group.</td></tr>
<tr><td>Distinguished name</td><td> Indicate that Hue should use a full distinguished name for the
    group.</td></tr>
    <tr><td>Import new members</td><td>  Indicate that Hue should import the members of the group.</td></tr>
        <tr><td>Import new members from all subgroups</td><td>
    Indicate that Hue should import the members of the subgroups.</td></tr>
            <tr><td>Create home directories</td><td> Indicate that Hue should create home directories in HDFS for the
    imported members.</td></tr></table>

3.  Click **Add/sync group**.

<a id="permissions"></a>
Permissions
-----------

Permissions for Hue applications are granted to groups, with users
gaining permissions based on their group membership. Group permissions
define the Hue applications visible to group members when they log into
Hue and the application features available to them.

1.  Click **Permissions**.
2.  Click the application for which you want to assign permissions.
3.  Check the checkboxes next to the groups you want to have permission
    for the application. Check **Select all** to select all groups.
4.  Click **Update permission**. The new groups will appear in the
    Groups column in the **Hue Permissions** list.


<link rel="stylesheet" href="docbook.css" type="text/css" media="screen" title="no title" charset="utf-8"></link>

Beeswax
=======

The Beeswax application enables you to perform queries on Apache Hive, a
data warehousing system designed to work with Hadoop. For information
about Hive, see [Hive
Documentation](http://archive.cloudera.com/cdh4/cdh/4/hive/). You can
create Hive databases, tables and partitions, load data, create, run,
and manage queries, and download the results in a Microsoft Office Excel
worksheet file or a comma-separated values file.

Beeswax and Hive Installation and Configuration
-----------------------------------------------

Beeswax is installed and configured as part of Hue. For information
about installing and configuring Hue, see the Hue Installation
manual.

Beeswax assumes an existing Hive installation. The Hue installation
instructions include the configuration necessary for Beeswax to access
Hive. You can view the current Hive configuration from the **Settings**
tab in the Beeswax application.

By default, a Beeswax user can see the saved queries for all users -
both his/her own queries and those of other Beeswax users. To restrict
viewing saved queries to the query owner and Hue administrators, set the
share\_saved\_queries property under the [beeswax] section in the Hue
configuration file to false.

Starting Beeswax
----------------

Click the **Beeswax** icon (![image](images/icon_beeswax_24.png)) in the
navigation bar at the top of the Hue browser page.

Managing Databases, Tables, and Partitions
------------------------------------------

You can create databases, tables, partitions, and load data by executing
[Hive data manipulation
statements](http://archive.cloudera.com/cdh4/cdh/4/hive/language_manual/data-manipulation-statements.html)
in the Beeswax application.

You can also use the [Metastore
Manager](../metastore_manager.html)
application to manage the databases, tables, and partitions and load
data.

Installing Example Queries and Tables
-------------------------------------

![image](images/note.jpg) **Note**: You must be a superuser to perform
this task.

1.  Click ![image](images/quick_start.png). The Quick Start Wizard
    opens.
2.  Click **Step 2: Examples**.
3.  Click **Beeswax (Hive UI)**.

Query Editor
------------

The Query Editor view lets you create, save, and submit queries in the
[Hive Query Language
(HQL)](http://wiki.apache.org/hadoop/Hive/LanguageManual), which is
similar to Structured Query Language (SQL). When you submit a query, the
Beeswax Server uses Hive to run the queries. You can either wait for the
query to complete, or return later to find the queries in the History
view. You can also request to receive an email message after the query
is completed.

In the box to the left of the Query field, you can select a database,
override the default Hive and Hadoop settings, specify file resources
and user-defined functions, enable users to enter parameters at
run-time, and request email notification when the job is complete. See
[Advanced Query Settings](#advancedQuerySettings) for details on using these
settings.

### Creating Queries

1.  In the Query Editor window, type a query or multiple queries
    separated by a semicolon ";". To be presented with a drop-down of
    autocomplete options, type CTRL+spacebar when entering a query.
2.  To save your query and advanced settings to use again later, click
    **Save As**, enter a name and description, and then click **OK**. To
    save changes to an existing query, click **Save.**
3.  If you want to view the execution plan for the query, click
    **Explain**. For more information, see
    [http://wiki.apache.org/hadoop/Hive/LanguageManual/Explain](http://wiki.apache.org/hadoop/Hive/LanguageManual/Explain).

### Loading Queries into the Query Editor

1.  Do one of the following:
    -   Click the My Queries tab.
        1.  Click the Recent Saved Queries or Recent Run Queries tab to
            display the respective queries.

    -   Click the Saved Queries tab.

2.  Click a query name. The query is loaded into the Query Editor.

### Running Queries

![image](images/note.jpg) **Note**: To run a query, you must be logged
in to Hue as a user that also has a Unix user account on the remote
server.

1.  To execute a portion of the query, highlight one or more query
    statements.
2.  Click **Execute**. The Query Results window appears with the results
    of your query.
    -   To view a log of the query execution, click **Log** at the top
        of the results display. You can use the information in this tab
        to debug your query.
    -   To view the query that generated these results, click **Query**
        at the top of the results display.
    -   To view the columns of the query, click **Columns**.
    -   To return to the query in the Query Editor, click **Unsaved
        Query**.

3.  If there are multiple statements in the query, click Next in the
    Multi-statement query pane to execute the remaining statements.

![image](images/note.jpg) **Note**: Under **MR JOBS**, you can view any
MapReduce jobs that the query generated.

### Downloading and Saving Query Results

![image](images/important.jpg) **Important**:

-   You can only save results to a file when the results were generated
    by a MapReduce job.
-   This is the preferred way to save when the result is large (for
    example \> 1M rows).

1.  Do any of the following to download or save the query results:
    -   Click **Download as CSV** to download the results in a
        comma-separated values file suitable for use in other
        applications.
    -   Click **Download as XLS** to download the results in a Microsoft
        Office Excel worksheet file.
    -   Click **Save** to save the results in a table or HDFS file.
        -   To save the results in a new table, select **In a new
            table**, enter a table name, and then click **Save**.
        -   To save the results in an HDFS file, select **In an HDFS
            directory**, enter a path and then click **Save**. You can
            then download the file with [File Browser](../filebrowser.html).

<a id="advancedQuerySettings"></a>
### Advanced Query Settings

The pane to the left of the Query Editor lets you specify the following
options:


<table>
<tr><td>DATABASE</td><td>The database containing the table definitions.</td></tr>
<tr><td>SETTINGS</td><td>Override the Hive and Hadoop default settings. To configure a new
setting:

<ol>
<li> Click Add.
<li> For Key, enter a Hive or Hadoop configuration variable name.
<li> For Value, enter the value you want to use for the variable.

For example, to override the directory where structured Hive query logs
are created, you would enter hive.querylog.location for Key, and a
path for Value.
</ol>

To view the default settings, click the Settings tab at the top of
the page. For information about Hive configuration variables, see:
[http://wiki.apache.org/hadoop/Hive/AdminManual/Configuration](http://wiki.apache.org/hadoop/Hive/AdminManual/Configuration).
For information about Hadoop configuration variables, see:
[http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml](http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml).</td></tr>
<tr><td>FILE RESOURCES</td><td>Make files locally accessible at query execution time available on the
Hadoop cluster. Hive uses the Hadoop Distributed Cache to distribute the
added files to all machines in the cluster at query execution time.

<ol>
<li>  Click Add to configure a new setting.
<li>   From the Type drop-down menu, choose one of the following:
<ul>
   <li>jar - Adds the specified resources to the Java classpath.
   <li>archive - Unarchives the specified resources when
        distributing them.
    <li>file - Adds the specified resources to the distributed
        cache. Typically, this might be a transform script (or similar)
        to be executed.

<li>   For Path, enter the path to the file or click
    ![image](images/browse.png) to browse and select the file.
</ol>

![image](images/note.jpg) Note: It is not necessary to specify files
used in a transform script if the files are available in the same path
on all machines in the Hadoop cluster.</td></tr>
<tr><td>USER-DEFINED FUNCTIONS</td><td>Specify user-defined functions. Click Add to configure a new
setting. Specify the function name in the Name field, and specify
the class name for Classname.

You *must* specify a JAR file for the user-defined functions in FILE RESOURCES.

To include a user-defined function in a query, add a $ (dollar sign)
before the function name in the query. For example, if MyTable is a
user-defined function name in the query, you would type: SELECT $MyTable
</td></tr>
<tr><td>PARAMETERIZATION</td><td>Indicate that a dialog box should display to enter parameter values when
a query containing the string $parametername is executed. Enabled by
default.</td></tr>
<tr><td>EMAIL NOTIFICATION</td><td>Indicate that an email message should be sent after a query completes.
The email is sent to the email address specified in the logged-in user's
profile.</td></tr>
</table>


### Viewing Query History

You can view the history of queries that you have run previously.
Results for these queries are available for one week or until Hue is
restarted.

1.  Click **History**. A list of your saved and unsaved queries displays
    in the Query History window.
2.  To display the queries for all users, click **Show everyone's
    queries**. To display your queries only, click **Show my queries**.
3.  To display the automatically generated actions performed on a user's
    behalf, click **Show auto actions**. To display user queries again,
    click **Show user queries**.

### Viewing, Editing, Copying, and Deleting Saved Queries

You can view a list of saved queries of all users by clicking **My
Queries** and then selecting either Recent Saved Queries or Recent Run
Queries tab to display the respective queries or clicking **Saved
Queries**. You can copy any query, but you can edit, delete, and view
the history of only your own queries.


**Edit**

1.  Click **Saved Queries**. The Queries window displays.
2.  Check the checkbox next to the query and click **Edit**. The query
    displays in the Query Editor window.
3.  Change the query and then click **Save.** You can also click **Save
    As**, enter a new name, and click **OK** to save a copy of the
    query.

**Copy**

1.  Click **Saved Queries**. The Queries window displays.
2.  Check the checkbox next to the query and click **Copy**. The query
    displays in the Query Editor window.
3.  Change the query as necessary and then click **Save.** You can also
    click **Save As**, enter a new name, and click **OK** to save a copy
    of the query.

**Copy in Query History**

1.  Click **History**. The Query History window displays.
2.  To display the queries for all users, click **Show everyone's
    queries**. The queries for all users display in the History window.
3.  Click the query you want to copy. A copy of the query displays in
    the Query Editor window.
4.  Change the query, if necessary, and then click **Save As**, enter a
    new name, and click **OK** to save the query.

**Delete**

1.  Click **Saved Queries**. The Queries window displays.
2.  Check the checkbox next to the query and click **Delete**.
3.  Click **Yes** to confirm the deletion.


<link rel="stylesheet" href="docbook.css" type="text/css" media="screen" title="no title" charset="utf-8"></link>

File Browser
============

The File Browser application lets you browse and manipulate files and
directories in the Hadoop Distributed File System (HDFS) while using
Hue. With File Browser, you can:

-   Create files and directories, upload and download files, upload zip
    archives, and rename, move, and delete files and directories. You
    can also change a file's or directory's owner, group, and
    permissions. See [Files and Directories](#filesAndDirectories).
-   Search for files, directories, owners, and groups. See [Searching
    for Files and Directories](#searching).
-   View and edit files as text or binary. See [Viewing and Editing
    Files](#viewAndEdit).

File Browser Installation and Configuration
-------------------------------------------

File Browser is one of the applications installed as part of Hue. For
information about installing and configuring Hue, see the Hue Installation
manual.

Starting File Browser
---------------------

Click the **File Browser** icon
(![image](images/icon_filebrowser_24.png)) in the navigation bar at the
top of the Hue browser page.


<a id="fileAndDirectories"></a>
Files and Directories
---------------------

You can use File Browser to view the input and output files of your
MapReduce jobs. Typically, you can save your output files in /tmp or in
your home directory if your system administrator set one up for you. You
must have the proper permissions to manipulate other user's files.

### Creating Directories

1.  In the File Browser window, select **New > Directory**.
2.  In the **Create Directory** dialog box, enter a directory name and
    then click **Submit**.

### Changing Directories

-   Click the directory name or parent directory dots in the **File
    Browser** window.
-   Click the ![image](images/edit.png) icon, type a directory name, and
    press **Enter**.

To change to your home directory, click **Home** in the path field at
the top of the **File Browser** window.

![image](images/note.jpg) **Note**:

The **Home** button is disabled if you do not have a home directory. Ask
a Hue administrator to create a home directory for you.

### Creating Files

1.  In the File Browser window, select **New > File**.
2.  In the **Create File** dialog box, enter a file name and then click
    **Submit**.


<a id="uploadingFiles"></a>
### Uploading Files

You can upload text and binary files to the HDFS.

1.  In the **File Browser** window, browse to the directory where you
    want to upload the file.
2.  Select **Upload \> Files**.
3.  In the box that opens, click **Upload a File** to browse to and
    select the file(s) you want to upload, and then click **Open**.

### Copying a File

1.  In the **File Browser** window, check the checkbox next to the file
    you want to copy.
2.  Click the ![image](images/copy.png) Copy button.

### Downloading Files

You can download text and binary files to the HDFS.

1.  In the **File Browser** window, check the checkbox next to the file
    you want to download.
2.  Click the **Download** button.

### Uploading Zip Archives

You can upload zip archives to the HDFS. The archive is uploaded and
extracted to a directory named archivename.

1.  In the **File Browser** window, browse to the directory where you
    want to upload the archive.
2.  Select **Upload > Zip file**.
3.  In the box that opens, click **Upload a zip file** to browse to and
    select the archive you want to upload, and then click **Open**.

### Trash Folder

File Browser supports the HDFS trash folder (*home directory*/.Trash) to
contain files and directories before they are permanently deleted. Files
in the folder have the full path of the deleted files (in order to be
able to restore them if needed) and checkpoints. The length of time a
file or directory stays in the trash depends on HDFS properties.

**Open**

1.  In the **File Browser** window, click ![image](images/fbtrash.png).

**Move Files and Directories To**

1.  In the **File Browser** window, check the checkbox next to one or
    more files and directories.
2.  Select **Delete > Move to trash**.

**Empty**

1.  In the **File Browser** window, click ![image](images/fbtrash.png).
2.  Click **Empty**.

### Renaming, Moving, Deleting, and Restoring Files and Directories


**Rename**

1.  In the **File Browser** window, check the checkbox next to the file
    or directory you want to rename.
2.  Click the **Rename** button.
3.  Enter the new name and then click **Submit**.

**Move**

1.  In the **File Browser** window, check the checkbox next to the file
    or directory you want to move.
2.  Click the **Move** button.
3.  In the **Move** dialog box, browse to or type the new directory, and
    then click **Submit**.

**Delete**

1.  In the **File Browser** window, check the checkbox next to the file
    or directory you want to delete. If you select a directory, all of
    the files and subdirectories contained within that directory are
    also deleted.
2.  Choose one of the following:
    -   **Delete > Move to trash**
    -   **Delete > Delete forever**

3.  Click **Yes** to confirm. When you move a file to trash it is stored
    in the .Trash folder in your home directory.

**Restore**

1.  In the **File Browser** window, open the .Trash folder.
2.  Navigate to the folder containing the file you want to restore.
3.  Check the checkbox next to the file.
4.  Click **Restore**.

### Changing a File's or Directory's Owner, Group, or Permissions

![image](images/note.jpg) **Note**:

Only the Hadoop superuser can change a file's or directory's owner,
group, or permissions. The user who starts Hadoop is the Hadoop
superuser. The Hadoop superuser account is not necessarily the same as a
Hue superuser account. If you create a Hue user (in User Admin) with the
same user name and password as the Hadoop superuser, then that Hue user
can change a file's or directory's owner, group, or permissions.

**Owner or Group**

1.  In the **File Browser** window, check the checkbox next to the
    select the file or directory whose owner or group you want to
    change.
2.  Choose **Change Owner/Group** from the Options menu.
3.  In the **Change Owner/Group** dialog box:
    -   Choose the new user from the **User** drop-down menu.
    -   Choose the new group from the **Group** drop-down menu.
    -   Check the **Recursive** checkbox to propagate the change.

4.  Click **Submit** to make the changes.

**Permissions**

1.  In the **File Browser** window, check the checkbox next to the file
    or directory whose permissions you want to change.
2.  Click the **Change Permissions** button.
3.  In the **Change Permissions** dialog box, select the permissions you
    want to assign and then click **Submit**.
    
    
<a id="searching"></a>
Searching for Files and Directories
-----------------------------------

To search for files or directories by name using the query search box,
enter the name of the file or directory in the query search box. File
Browser lists the files or directories matching the search criteria.

<a id="viewAndEdit"></a>
Viewing and Editing Files
-------------------------

You can view and edit files as text or binary.


**View**

1.  In the **File Browser** window, click the file you want to view.
    File Browser displays the first 4,096 bytes of the file in the
    **File Viewer** window.
    -   If the file is larger than 4,096 bytes, use the Block navigation
        buttons (First Block, Previous Block, Next Block, Last Block) to
        scroll through the file block by block. The **Viewing Bytes**
        fields show the range of bytes you are currently viewing.
    -   To switch the view from text to binary, click **View as Binary**
        to view a hex dump.
    -   To switch the view from binary to text, click **View as Text**.

**Edit**

1.  If you are viewing a text file, click **Edit File**. File Browser
    displays the contents of the file in the **File Editor** window.
2.  Edit the file and then click **Save** or **Save As** to save the
    file.

**View Location in HDFS**

Click **View File Location**. File Browser displays the file's location
in the **File Browser** window.


<link rel="stylesheet" href="docbook.css" type="text/css" media="screen" title="no title" charset="utf-8"></link>

HBase Browser
=============

We'll take a look at the new [HBase Browser App](http://gethue.tumblr.com/post/59071544309/the-web-ui-for-hbase-hbase-browser)
added in Hue 2.5 and improved significantly since.

Prerequisites before using the app:

\1. Have HBase and Thrift Service 1 initiated (Thrift can be configured)

\2. Configure your list of HBase Clusters in
[hue.ini](https://github.com/cloudera/hue/blob/master/desktop/conf.dist/hue.ini#L467)
to point to your Thrift IP/Port


SmartView
---------

The smartview is the view that you land on when you first enter a table.
On the left hand side are the row keys and hovering over a row reveals a
list of controls on the right. Click a row to select it, and once
selected you can perform batch operations, sort columns, or do any
amount of standard database operations. To explore a row, simple scroll
to the right. By scrolling, the row should continue to lazily-load cells
until the end.

### Adding Data

To initially populate the table, you can insert a new row or bulk upload
CSV/TSV/etc. type data into your table.


On the right hand side of a row is a '+' sign that lets you insert
columns into your
row

### Mutating Data

To edit a cell, simply click to edit inline.

If you need more control or data about your cell, click Full Editor to
edit.

In the full editor, you can view cell history or upload binary data to
the cell. Binary data of certain MIME Types are detected, meaning you
can view and edit images, PDFs, JSON, XML, and other types directly in
your browser!

Hovering over a cell also reveals some more controls (such as the delete
button or the timestamp). Click the title to select a few and do batch
operations:

If you need some sample data to get started and explore, check out this
howto create [HBase table
tutorial](http://gethue.tumblr.com/post/58181985680/hadoop-tutorial-how-to-create-example-tables-in-hbase).


### Smart Searchbar

The "Smart Searchbar" is a sophisticated tool that helps you zero-in on
your data. The smart search supports a number of operations. The most
basic ones include finding and scanning row keys. Here I am selecting
two row keys with:


    domain.100, domain.200


Submitting this query gives me the two rows I was looking for. If I want
to fetch rows after one of these, I have to do a scan. This is as easy
as writing a '+' followed by the number of rows you want to fetch.


    domain.100, domain.200 +5


Fetches domain.100 and domain.200 followed by the next 5 rows. If you're
ever confused about your results, you can look down below and the query
bar and also click in to edit your query.

The Smart Search also supports column filtering. On any row, I can
specify the specific columns or families I want to retrieve. With:


    domain.100[column_family:]


I can select a bare family, or mix columns from different families like
so:


    domain.100[family1:, family2:, family3:column_a]


Doing this will restrict my results from one row key to the columns I
specified. If you want to restrict column families only, the same effect
can be achieved with the filters on the right. Just click to toggle a
filter.


Finally, let's try some more complex column filters. I can query for
bare columns:


    domain.100[column_a]

This will multiply my query over all column families. I can also do
prefixes and scans:


 domain.100[family: prefix* +3]


This will fetch me all columns that start with prefix\* limited to 3
results. Finally, I can filter on range:


    domain.100[family: column1 to column100]


This will fetch me all columns in 'family:' that are lexicographically
\>= column1 but <= column100. The first column ('column1') must be a
valid column, but the second can just be any string for comparison.

The Smart Search also supports prefix filtering on rows. To select a
prefixed row, simply type the row key followed by a star \*. The prefix
should be highlighted like any other searchbar keyword. A prefix scan is
performed exactly like a regular scan, but with a prefixed row.


    domain.10* +10


Finally, as a new feature, you can also take full advantage of the
[HBase filtering](denied:about:blank)language, by typing your filter
string between curly braces. HBase Browser autocompletes your filters
for you so you don't have to look them up every time. You can apply
filters to rows or scans.


    domain.1000 {ColumnPrefixFilter('100-') AND ColumnCountGetFilter(3)}


This doc only covers a few basic features of the Smart Search. You can
take advantage of the full querying language by referring to the help
menu when using the app. These include column prefix, bare columns,
column range, etc. Remember that if you ever need help with the
searchbar, you can use the help menu that pops up while typing, which
will suggest next steps to complete your query.




<link rel="stylesheet" href="docbook.css" type="text/css" media="screen" title="no title" charset="utf-8"></link>

Cloudera Impala Query UI
========================

The Cloudera Impala Query UI application enables you to perform queries
on Apache Hadoop data stored in HDFS or HBase using Cloudera Impala. For
information about Cloudera Impala, see [Installing and Using Cloudera
Impala](/content/support/en/documentation/cloudera-impala/cloudera-impala-documentation-v1-latest.html).
You can create, run, and manage queries, and download the results in a
Microsoft Office Excel worksheet file or a comma-separated values file.

Cloudera Impala Query UI Installation and Configuration
-------------------------------------------------------

The Cloudera Impala Query UI application is one of the applications
installed as part of Hue. For information about installing and
configuring Hue, see the Hue Installation
manual..

The Cloudera Impala Query UI assumes an existing Cloudera Impala
installation. The Hue installation instructions include the
configuration necessary for Impala. You can view the current
configuration from the **Settings** tab.

Starting Cloudera Impala Query UI
---------------------------------

Click the **Cloudera Impala Query UI** icon
(![image](images/icon_impala_24.png)) in the navigation bar at the top
of the Hue browser page.

Managing Databases, Tables, and Partitions
------------------------------------------

You can create databases, tables, partitions, and load data by executing
[Hive data manipulation
statements](http://archive.cloudera.com/cdh4/cdh/4/hive/language_manual/data-manipulation-statements.html)
in the Beeswax application.

You can also use the [Metastore
Manager]()
application to manage the databases, tables, and partitions and load
data.

When you change the metastore using one of these applications, you must
click the Refresh button under METASTORE CATALOG in the pane to the left
of the Query Editor to make the metastore update visible to the Cloudera
Impala server.

Installing Example Queries and Tables
-------------------------------------

![image](images/note.jpg) **Note**: You must be a superuser to perform
this task.

1.  Click ![image](images/quick_start.png). The Quick Start Wizard
    opens.
2.  Click **Step 2: Examples**.
3.  Click **Cloudera Impala Query UI**.

Query Editor
------------

The Query Editor view lets you create queries in the Cloudera Impala
Query Language, which is based on the Hive Standard Query Language
(HiveQL) and described in the Cloudera Impala Language Reference topic
in [Installing and Using Cloudera
Impala](http://www.cloudera.com/content/cloudera-content/cloudera-docs/Impala/latest/Installing-and-Using-Impala/Installing-and-Using-Impala.html).

You can name and save your queries to use later.

When you submit a query, you can either wait for the query to complete,
or return later to find the queries in the **History** view.

In the box to the left of the Query field, you can select a database,
override the default Cloudera Impala settings, enable users to enter
parameters at run-time. See [Advanced Query Settings](#advancedQuerySettings) for
details on using these settings.

### Creating Queries

1.  In the Query Editor window, type a query or multiple queries
    separated by a semicolon ";". To be presented with a drop-down of
    autocomplete options, type CTRL+spacebar when entering a query.
2.  To save your query and advanced settings to use again later, click
    **Save As**, enter a name and description, and then click **OK**. To
    save changes to an existing query, click **Save.**

### Loading Queries into the Query Editor

1.  Do one of the following:
    -   Click the My Queries tab.
        1.  Click the Recent Saved Queries or Recent Run Queries tab to
            display the respective queries.

    -   Click the Saved Queries tab.

2.  Click a query name. The query is loaded into the Query Editor.

### Running Queries

![image](images/note.jpg) **Note**: To run a query, you must be logged
in to Hue as a user that also has a Unix user account on the remote
server.

1.  To execute a portion of the query, highlight one or more query
    statements.
2.  Click **Execute**. The Query Results window appears with the results
    of your query.
    -   To view a log of the query execution, click **Log** at the top
        of the results display. You can use the information in this tab
        to debug your query.
    -   To view the query that generated these results, click **Query**
        at the top of the results display.
    -   To view the columns of the query, click **Columns**.
    -   To return to the query in the Query Editor, click **Unsaved
        Query**.

3.  If there are multiple statements in the query, click Next in the
    Multi-statement query pane to execute the remaining statements.

<a id="advancedQuerySettings"></a>
### Advanced Query Settings

The pane to the left of the Query Editor lets you specify the following
options:

<table>
<tr><td>DATABASE</td><td>The database containing the table definitions.</td></tr>
<tr><td>SETTINGS</td><td>Override the Cloudera Impala  default settings. To configure a new
setting:

<ol>
<li> Click Add.
<li> For Key, enter a Hive or Hadoop configuration variable name.
<li> For Value, enter the value you want to use for the variable.

For example, to override the directory where structured Hive query logs
are created, you would enter hive.querylog.location for Key, and a
path for Value.
</ol>
To view the default settings, click the Settings tab at the top of
the page. 
</td></tr>  
<tr><td>PARAMETERIZATION</td><td>Indicate that a dialog box should display to enter parameter values when
a query containing the string $parametername is executed. Enabled by
default.</td></tr>
<tr><td>METASTORE CATALOG</td><td>Refresh metadata. It is best to refresh metadata after making changes to
databases such as adding or dropping a table.</td></tr>
</table>


### Viewing Query History

You can view the history of queries that you have run previously.
Results for these queries are available for one week or until Hue is
restarted.

1.  Click **History**. A list of your saved and unsaved queries displays
    in the Query History window.
2.  To display the queries for all users, click **Show everyone's
    queries**. To display your queries only, click **Show my queries**.
3.  To display the automatically generated actions performed on a user's
    behalf, click **Show auto actions**. To display user queries again,
    click **Show user queries**.

### Viewing, Editing, Copying, and Deleting Saved Queries

You can view a list of saved queries of all users by clicking **My
Queries** and then selecting either Recent Saved Queries or Recent Run
Queries tab to display the respective queries or clicking **Saved
Queries**. You can copy any query, but you can edit, delete, and view
the history of only your own queries.


**Edit**

1.  Click **Saved Queries**. The Queries window displays.
2.  Check the checkbox next to the query and click **Edit**. The query
    displays in the Query Editor window.
3.  Change the query and then click **Save.** You can also click **Save
    As**, enter a new name, and click **OK** to save a copy of the
    query.

**Copy**

1.  Click **Saved Queries**. The Queries window displays.
2.  Check the checkbox next to the query and click **Copy**. The query
    displays in the Query Editor window.
3.  Change the query as necessary and then click **Save.** You can also
    click **Save As**, enter a new name, and click **OK** to save a copy
    of the query.

**Copy in Query History**

1.  Click **History**. The Query History window displays.
2.  To display the queries for all users, click **Show everyone's
    queries**. The queries for all users display in the History window.
3.  Click the query you want to copy. A copy of the query displays in
    the Query Editor window.
4.  Change the query, if necessary, and then click **Save As**, enter a
    new name, and click **OK** to save the query.

**Delete**

1.  Click **Saved Queries**. The Queries window displays.
2.  Check the checkbox next to the query and click **Delete**.
3.  Click **Yes** to confirm the deletion.


<link rel="stylesheet" href="docbook.css" type="text/css" media="screen" title="no title" charset="utf-8"></link>

Hue User Guide
==============

-   [Introducing Hue](introducing.html)
-   [About Hue](about.html)
-   [Beeswax Hive Editor](beeswax.html)
-   [Impala Query UI](impala.html)
-   [Pig Editor](pig.html)
-   [File Browser](filebrowser.html)
-   [Metastore Manager](metastore_manager.html)
-   [Sqoop UI](sqoop.html)
-   [Job Browser](jobbrowser.html)
-   [Job Designer](jobdesigner.html)
-   [Oozie Editor and Dashboard](oozie.html)
-   [Solr Search](search.html)
-   [ZooKeeper Browser](zookeeper.html)
-   [HBase Browser](hbase.html)
-   [User Admin](admin.html)



<link rel="stylesheet" href="docbook.css" type="text/css" media="screen" title="no title" charset="utf-8"></link>

Introducing Hue
===============

Hue is a set of web applications that enable you to interact with a Hadoop
cluster. Hue applications let you browse HDFS and jobs, manage a Hive metastore,
run Hive, Cloudera Impala queries and Pig scripts, browse HBase,
export data with Sqoop, submit MapReduce programs, build custom search engines
with Solr, and schedule repetitive workflows with Oozie.

Hue Architecture
----------------

Hue applications run in a Web browser and require no client
installation.

The following figure illustrates how Hue works. Hue Server is a
"container" web application that sits in between CDH and the browser. It
hosts all the Hue web applications and communicates with CDH components.

![image](images/huearch.jpg)

Starting Applications
---------------------

To open a Hue application, click the appropriate icon in the navigation
bar at the top of the Hue web browser window. To open a second application concurrently (or a second instance of the
same application), right-click the icon and select **Open link in new
tab**.

Displaying Help for the Hue Applications
----------------------------------------

To display the help text for a Hue application, click the **Help** (
![image](images/icon_help_24.png) ) tab in the Hue navigation bar, then
click the appropriate link in the Help navigation bar at the top of the
Help window.

Logging In and Out
------------------

To log out of Hue, click **Sign Out** from the pull-down list under the
logged-in user name (at the right of the Hue navigation bar).

Notice of Misconfiguration
--------------------------

If Hue detects a misconfiguration, an indicator
![image](images/misconfiguration.png) appears in the navigation bar at
the top of the page. Clicking this indicator takes you to the [Check
Configuration](../about.html) screen
which will indicate the potential misconfiguration(s) with hints about
fixing them.

Changing Your Password
----------------------

You can go directly to your own information by selecting username \>
Profile at the right of the Hue navigation bar.

If authentication is managed by Hue (that is, authentication is not
managed via some external mechanism), and you are a superuser, you can
use the [User Admin](../useradmin.html) application to
change password and personal details.

Seeking Help, Reporting Bugs, and Providing Feedback
----------------------------------------------------

The Hue team strongly values your feedback. The best way to contact us
is to send email to
[hue-user@cloudera.org](mailto:hue-user@cloudera.org).

If you're experiencing transient errors (typically an error message
saying a service is down), contact your system administrator first.


<link rel="stylesheet" href="docbook.css" type="text/css" media="screen" title="no title" charset="utf-8"></link>

Job Browser
===========

The Job Browser application lets you to examine the Hadoop MapReduce
jobs running on your Hadoop cluster. Job Browser presents the job and
tasks in layers. The top layer is a list of jobs, and you can link to a
list of that job's tasks. You can then view a task's attempts and the
properties of each attempt, such as state, start and end time, and
output size. To troubleshoot failed jobs, you can also view the logs of
each attempt.

Job Browser Installation and Configuration
------------------------------------------

Job Browser is one of the applications installed as part of Hue. For
information about installing and configuring Hue, see the Hue Installation
manual.

Job Browser can display both MRv1 and MRv2 jobs, but must be configured
to display one type at a time. 

Starting Job Browser
--------------------

Click the **Job Browser** icon (![image](images/icon_jobbrowser_24.png))
in the navigation bar at the top of the Hue web page.

If there are no jobs that have been run, the **Welcome to the Job
Browser** page opens, with links to the Job Designer and Beeswax.

If there are jobs running, then the Job Browser list appears.

Filtering the Job Browser List
------------------------------

-   To filter the jobs by their state (such as **Running** or
    **Completed**), choose a state from the **Job status** drop-down
    menu.
-   To filter by a user who ran the jobs, enter the user's name in the
    **User Name** query box.
-   To filter by job name, enter the name in the **Text** query box.
-   To clear the filters, choose **All States** from the **Job status**
    drop-down menu and delete any text in the **User Name** and **Text**
    query boxes.
-   To display retired jobs, check the **Show retired jobs** checkbox.
    Retired jobs show somewhat limited information  for example,
    information on maps and reduces and job duration is not available.
    Jobs are designated as Retired by the JobTracker based on the value
    of mapred.jobtracker.retirejob.interval. The retired jobs no longer
    display after the JobTracker is restarted.

Viewing Job Information and Logs
--------------------------------

![image](images/note.jpg) **Note**: At any level you can view the log
for an object by clicking the ![image](images/log.png) icon in the Logs
column.

**To view job information for an individual job:**

1.  In the **Job Browser** window, click **View** at the right of the
    job you want to view. This shows the **Job** page for the job, with
    the recent tasks associated with the job are displayed in the
    **Tasks** tab.
2.  Click the **Metadata** tab to view the metadata for this job.
3.  Click the **Counters** tab to view the counter metrics for the job.

**To view details about the tasks associated with the job:**

1.  In the **Job** window, click the **View All Tasks** link at the
    right just above the **Recent Tasks** list. This lists all the tasks
    associated with the job.
2.  Click **Attempts** to the right of a task to view the attempts for
    that task.

**To view information about an individual task:**

1.  In the **Job** window, click the **View** link to the right of the
    task. The attempts associated with the task are displayed.
2.  Click the **Metadata** tab to view metadata for this task. The
    metadata associated with the task is displayed.
3.  To view the Hadoop counters for a task, click the **Counters** tab.
    The counters associated with the task are displayed.
4.  To return to the **Job** window for this job, click the job number
    in the status panel at the left of the window.

**To view details about a task attempt:**

1.  In the **Job Task** window, click the **View** link to the right of
    the task attempt. The metadata associated with the attempt is
    displayed under the **Metadata** tab.
2.  To view the Hadoop counters for the task attempt, click the
    **Counters** tab. The counters associated with the attempt are
    displayed.
3.  To view the logs associated with the task attempt, click the
    **Logs** tab. The logs associated with the task attempt are
    displayed.
4.  To return to the list of tasks for the current job, click the task
    number in the status panel at the left of the window.

Viewing Job Output
------------------

1.  In the **Job Browser** window, click the link in the ID column.
2.  To view the output of the job, click the link under **OUTPUT** in
    the panel at the left of the window. This takes you to the job
    output directory in the **File Browser**.


<link rel="stylesheet" href="docbook.css" type="text/css" media="screen" title="no title" charset="utf-8"></link>

Job Designer
============

The Job Designer application enables you to create and submit jobs to
the Hadoop cluster. You can include variables with your jobs to enable
you and other users to enter values for the variables when they run your
job. The Job Designer supports the actions supported by
[Oozie](http://archive.cloudera.com/cdh4/cdh/4/oozie/):
[MapReduce](/content/cloudera-content/cloudera-docs/HadoopTutorial/CDH4/index.html),
Streaming, Java, Pig, Hive, Sqoop, Shell, Ssh, DistCp, Fs, and Email.

Job Designer Installation and Configuration
-------------------------------------------

Job Designer is one of the applications installed as part of Hue. For
information about installing and configuring Hue, see the Hue Installation
manual..

In order to run DistCp, Streaming, Pig, Sqoop, and Hive jobs, Oozie must
be configured to use the Oozie ShareLib. See the Oozie Installation manual.

Starting Job Designer
---------------------

Click the **Job Designer** icon (![image](images/icon_jobsub_24.png)) in
the navigation bar at the top of the Hue web page. The **Job Designs**
page opens in the browser.

Installing the Example Job Designs
----------------------------------

![image](images/note.jpg) **Note**: You must be a superuser to perform
this task.

1.  Click ![image](images/quick_start.png). The Quick Start Wizard
    opens.
2.  Click **Step 2: Examples**.
3.  Click **Job Designer**.

Job Designs
-----------

A job design specifies several meta-level properties of a job, including
the job design name, description, the executable scripts or classes, and
any parameters for those scripts or classes.

### Filtering Job Designs

You can filter the job designs that appear in the list by owner, name,
type, and description.

**To filter the Job Designs list:**

1.  In the **Job Designs** window, click **Designs**.
2.  Enter text in the Filter text box at the top of the **Job Designs**
    window. When you type in the Filter field, the designs are
    dynamically filtered to display only those rows containing text that
    matches the specified substring.

### Creating a Job Design

1.  In the **Job Designs** window, click New Action \> Action, where
    Action is MapReduce, Streaming, Java, Pig, Hive, Sqoop, Shell, Ssh,
    DistCp, Fs, or Email.
2.  In the **Job Design (Action type)** window, specify the common and
    job type specific information.
3.  Click **Save** to save the job settings.

### Deleting and Restoring Job Designs

You can move job designs to the trash and later restore or permanently
delete them.

#### Deleting Job Designs

1.  In a Manager screen, check the checkbox next to one or more job
    designs.
2.  Choose one of the following:
    -   Delete \> Move to trash
    -   Delete \> Delete forever

#### Restoring Job Designs

1.  In a Manager screen, click ![image](images/trash.png) **Trash**.
2.  Check the checkbox next to one or more job designs.
3.  Click Restore.

### Job Design Settings

#### Job Design Common Settings

Most job design types support all the settings listed in the following
table. For job type specific settings, see:
[MapReduce](#mapreduce),
[Streaming](#streaming),
[Java](#java),
[Pig](#pig),
[Hive](#hive),
[Sqoop](#sqoop),
[Shell](#shell),
[Ssh](#ssh),
[DistCp](#distcp),
[Fs](#fs), and
[Email](#email).

All job design settings except Name and Description support the use of
variables of the form $variable\_name. When you run the job, a dialog
box will appear to enable you to specify the values of the variables.

<table>
<tr><td>Name</td><td>Identifies the job and its collection of properties and parameters.</td></tr>
<tr><td>Description</td><td>A description of the job. The description is displayed in the dialog box
that appears if you specify variables for the job.</td></tr>
<tr><td>Advanced</td><td>Advanced settings:<ul><li>Is shared- Indicate whether to share the action with all users.<li>Oozie parameters - parameters to pass to Oozie</td></tr>
<tr><td>Prepare</td><td>Specifies paths to create or delete before starting the workflow job.</td></tr>
<tr><td>Params</td>Parameters to pass to a script or command. The parameters are expressed
using the [JSP 2.0 Specification (JSP.2.3) Expression
Language](http://jcp.org/aboutJava/communityprocess/final/jsr152/),
allowing variables, functions, and complex expressions as parameters.<td></td></tr>
<tr><td>Job Properties</td><td>Job properties. To set a property value, click <b>Add Property</b>.<ol><li>Property name -  a configuration property name. This field provides autocompletion, so you can type the first few characters of a property name and then select the one you want from the drop-down
    list.<li>Valuethe property value.</td></tr>
<tr><td>Files</td><td>Files to pass to the job. Equivalent to the Hadoop -files option.</td></tr>
<tr><td>Archives</td><td>Files to pass to the job. Archives to pass to the job. Equivalent to the Hadoop -archives option.</td></tr></table>

<a id="mapreduce"></a>
#### MapReduce Job Design

A MapReduce job design consists of MapReduce functions written in Java.
You can create a MapReduce job design from existing mapper and reducer
classes without having to write a main Java class. You must specify the
mapper and reducer classes as well as other MapReduce properties in the
Job Properties setting.

<table>
<tr><td>Jar path</td><td>The fully-qualified path to a JAR file containing the classes that
implement the Mapper and Reducer functions.</td></tr>
</table>

<a id="streaming"></a>
#### Streaming Job Design

Hadoop streaming jobs enable you to create MapReduce functions in any
non-Java language that reads standard Unix input and writes standard
Unix output. For more information about Hadoop streaming jobs, see
[Hadoop
Streaming](http://archive.cloudera.com/cdh/3/hadoop-0.20.2+320/streaming.html).

<table>
<tr><td>Mapper</td><td>The path to the mapper script or class. If the mapper file is not on the
machines on the cluster, use the Files option to pass it as a part
of job submission. Equivalent to the Hadoop -mapper option.</td></tr>
<tr><td>Reducer</td><td>The path to the reducer script or class. If the reducer file is not on
the machines on the cluster, use the Files option to pass it as a
part of job submission. Equivalent to the Hadoop -reducer option.</td></tr>
</table>

<a id="java"></a>
#### Java Job Design

A Java job design consists of a main class written in Java.

<table>
<tr><td>Jar path</td><td>The fully-qualified path to a JAR file containing the main class.</td></tr>
<tr><td>Main class</td><td>The main class to invoke the program.</td></tr>
<tr><td>Args</td><td>The arguments to pass to the main class.</td></tr>
<tr><td>Java opts</td><td>The options to pass to the JVM.</td></tr>
</table>

<a id="pig"></a>
#### Pig Job Design


A Pig job design consists of a Pig script.

<table>
<tr><td>Script name</td><td>Script name or path to the Pig script.</td></tr>
</table>

<a id="hive"></a>
#### Hive Job Design

A Hive job design consists of a Hive script.

<table>
<tr><td>Script name</td><td>Script name or path to the Hive script.</td></tr>
</table>


<a id="sqoop"></a>
#### Sqoop Job Design

A Sqoop job design consists of a Sqoop command.

<table>
<tr><td>Command</td><td>The Sqoop command.</td></tr>
</table>

<a id="shell"></a>
#### Shell Job Design

A Shell job design consists of a shell command.

<table>
<tr><td>Command</td><td>The shell command.</td></tr>
<tr><td></td>Capture output<td>Indicate whether to capture the output of the command.</td></tr>
</table>

<a id="ssh"></a>
#### Ssh Job Design

A Ssh job design consists of an ssh command.

<table>
<tr><td>User</td><td>The name of the user to run the command as.</td></tr>
<tr><td>Host</td><td>The name of the host to run the command on.</td></tr>
<tr><td>Command</td><td>The ssh command.</td></tr>
<tr><td></td>Capture output<td>Indicate whether to capture the output of the command.</td></tr>
</table>

<a id="distcp"></a>
#### DistCp Job Design

A DistCp job design consists of a DistCp command.

<a id="fs"></a>
#### Fs Job Design

A Fs job design consists of a command that operates on HDFS.

<table>
<tr><td>Delete path</td><td>The path to delete. If it is a directory, it deletes recursively all its
content and then deletes the directory.</td></tr>
<tr><td></td>Create directory<td>The path of a directory to create.</td></tr>
<tr><td>Move file</td><td>The source and destination paths to the file to be moved.</td></tr>
<tr><td>Change permissions</td><td>The path whose permissions are to be changed, the permissions, and an
indicator of whether to change permission recursively.</td></tr></table>

<a id="email"></a>
#### Email Job Design

A Email job design consists of an email message.

<table>
<tr><td>To addresses</td><td>The recipient of the email message.</td></tr>
<tr><td>CC addresses (optional)</td><td>The cc recipients of the email message.</td></tr>
<tr><td>Subject</td><td>The subject of the email message.</td></tr>
<tr><td>Body</td><td>The body of the email message.</td></tr>
</table>


### Submitting a Job Design

![image](images/note.jpg) **Note**:

A job's input files must be uploaded to the cluster before you can
submit the job.

**To submit a job design:**

1.  In the **Job Designs** window, click **Designs** in the upper left
    corner. Your jobs and other users' jobs are displayed in the **Job
    Designs** window.
2.  Check the checkbox next to the job you want to submit.
3.  Click the **Submit** button.
    1.  If the job contains variables, enter the information requested
        in the dialog box that appears. For example, the sample grep
        MapReduce design displays a dialog where you specify the output
        directory.
    2.  Click **Submit** to submit the job.

After the job is complete, the Job Designer displays the results of the
job. For information about displaying job results, see [Displaying the
Results of Submitting a Job](#submitJob).

### Copying, Editing, and Deleting a Job Design

If you want to edit and use a job but you don't own it, you can make a
copy of it and then edit and use the copied job.


**Copy**

1.  In the **Job Designs** window, click **Designs**. The jobs are
    displayed in the **Job Designs** window.
2.  Check the checkbox next to the job you want to copy.
3.  Click the **Copy** button.
4.  In the **Job Design Editor** window, change the settings and then
    click **Save** to save the job settings.

**Edit**

1.  In the **Job Designs** window, click **Designs**. The jobs are
    displayed in the **Job Designs** window.
2.  Check the checkbox next to the job you want to edit.
3.  Click the **Edit** button.
4.  In the **Job Design** window, change the settings and then click
    **Save** to save the job settings.

Delete

1.  In the **Job Designs** window, click **Designs**. The jobs are
    displayed in the **Job Designs** window.
2.  Check the checkbox next to the job you want to delete.
3.  Click the **Delete** button.
4.  Click **OK** to confirm the deletion.

<a id="submitJob"></a>
Displaying Results of Submitting a Job
--------------------------------------

**To display the Job Submission History:**

In the **Job Designs** window, click the **History** tab. The jobs are
displayed in the **Job Submissions History** listed by Oozie job ID.

**To display Job Details:**

In the **Job Submission History** window, click an Oozie Job ID. The
results of the job display:

-   Actions - a list of actions in the job.
-   Click ![image](images/gear.png) to display the action configuration.
    In the action configuration for a MapReduce action, click the value
    of the mapred.output.dir property to display the job output.
-   In the root-node row, click the Id in the External Id column to view
    the job in the Job Browser.
-   Details - the job details. Click ![image](images/gear.png) to
    display the Oozie application configuration.
-   Definition - the Oozie application definition.
-   Log - the output log.


<link rel="stylesheet" href="docbook.css" type="text/css" media="screen" title="no title" charset="utf-8"></link>

Metastore Manager
=================

The Metastore Manager application enables you to manage the databases,
tables, and partitions of the
[Hive](http://archive.cloudera.com/cdh4/cdh/4/hive/) metastore shared by
the ([Beeswax](../beeswax.html) and [Cloudera Impala Query
UI](../impala.html)) applications. You can use Metastore
Manager to perform the following operations:

-   Databases
    -   [Select a database](#selectDatabase)
    -   [Create a database](#createDatabase)
    -   [Drop databases](#dropDatabase)

-   Tables
    -   [Create tables](#createTables)
    -   [Browse tables](#browseTables)
    -   [Import data into a table](#importDataIntoTables)
    -   [Drop tables](#dropTables)
    -   [View the location of a table](#viewTableLocation)

Metastore Manager Installation and Configuration
------------------------------------------------

Metastore Manager is one of the applications installed as part of Hue.
For information about installing and configuring Hue, see the Hue Installation
manual.

Starting Metastore Manager
--------------------------

Click the **Metastore Manager** icon
(![image](images/icon_table_browser_24.png)) in the navigation bar at
the top of the Hue browser page.

### Installing Sample Tables

![image](images/note.jpg) **Note**: You must be a superuser to perform
this task.

1.  Click ![image](images/quick_start.png). The Quick Start Wizard
    opens.
2.  Click **Step 2: Examples**.
3.  Click **Beeswax (Hive UI)** or **Cloudera Impala Query UI**.

### Importing Data

If you want to import your own data instead of installing the sample
tables, follow the procedure in [Creating Tables](#createTables).

<a id="selectDatabase"></a>
Selecting a Database
--------------------

1.  In the pane on the left, select the database from the DATABASE
    drop-down list.

<a id="createDatabase"></a>
Creating a Database
-------------------

1.  Click ![image](images/databases.png).
2.  Click **Create a new database**.
    1.  Specify a database name and optional description. Database names
        are not case-sensitive. Click **Next**.
    2.  Do one of the following:
        -   Keep the default location in the Hive warehouse folder.
        -   Specify an external location within HDFS:
            1.  Uncheck the **Location** checkbox.
            2.  In the External location field, type a path to a folder
                on HDFS or click ![image](images/browse.png) to browse
                to a folder and click **Select this folder**.

    3.  Click the **Create Database** button.
    
<a id="selectDatabase"></a>
Dropping Databases
------------------

1.  Click ![image](images/databases.png).
2.  In the list of databases, check the checkbox next to one or more
    databases.
3.  Click the ![image](images/trash.png) Drop button.
4.  Confirm whether you want to delete the databases.

<a id="createTables"></a>
Creating Tables
---------------

Although you can create tables by executing the appropriate Hive HQL DDL
query commands, it is easier to create a table using the Metastore
Manager table creation wizard.

There are two ways to create a table: from a file or manually. If you
create a table from a file, the format of the data in the file will
determine some of the properties of the table, such as the record and
file formats. The data from the file you specify is imported
automatically upon table creation. When you create a file manually, you
specify all the properties of the table, and then execute the resulting
query to actually create the table. You then import data into the table
as an additional step.

**From a File**

1.  In the ACTIONS pane in the Metastore Manager window, click **Create
    a new table from a file**. The table creation wizard starts.
2.  Follow the instructions in the wizard to create the table. The basic
    steps are:
    -   Choose your input file. The input file you specify must exist.
        Note that you can choose to have Beeswax create the table
        definition only based on the import file you select, without
        actually importing data from that file.
    -   Specify the column delimiter.
    -   Define your columns, providing a name and selecting the type.

3.  Click **Create Table** to create the table. The new table's metadata
    displays on the right side of the **Table Metadata** window. At this
    point, you can view the metadata or a sample of the data in the
    table. From the ACTIONS pane you can import new data into the table,
    browse the table, drop it, or go to the File Browser to see the
    location of the data.

**Manually**

1.  In the ACTIONS pane in the Metastore Manager window, click **Create
    a new table manually**. The table creation wizard starts.
2.  Follow the instructions in the wizard to create the table. The basic
    steps are:
    -   Name the table.
    -   Choose the record format.
    -   Configure record serialization by specifying delimiters for
        columns, collections, and map keys.
    -   Choose the file format.
    -   Specify the location for your table's data.
    -   Specify the columns, providing a name and selecting the type for
        each column.
    -   Specify partition columns, providing a name and selecting the
        type for each column.

3.  Click **Create table**. The Table Metadata window displays.

<a id="browseTables"></a>
Browsing Tables
---------------

**To browse table data:**

In the Table List window, check the checkbox next to a table name and
click **Browse Data**. The table's data displays in the Query Results
window.

**To browse table metadata:**

Do one of the following:

-   In the Table List window, click a table name.
-   Check the checkbox next to a table name and click **View**.

-   The table's metadata displays in the **Columns** tab. You can view
    the table data by selecting the **Sample** tab.
-   If the table is partitioned, you can view the partition columns by
    clicking the **Partition Columns** tab and display the partitions by
    clicking **Show Partitions(n)**, where n is the number of partitions
    in the ACTIONS pane on the left.

<a id="importDataIntoTables"></a>
Importing Data into a Table
---------------------------

When importing data, you can choose to append or overwrite the table's
data with data from a file.

1.  In the Table List window, click the table name. The Table Metadata
    window displays.
2.  In the ACTIONS pane, click **Import Data**.
3.  For **Path**, enter the path to the file that contains the data you
    want to import.
4.  Check **Overwrite existing data** to replace the data in the
    selected table with the imported data. Leave unchecked to append to
    the table.
5.  Click **Submit**.

<a id="dropTables"></a>
Dropping Tables
---------------

1.  In the Table List window, click the table name. The Table Metadata
    window displays.
2.  In the ACTIONS pane, click **Drop Table**.
3.  Click **Yes** to confirm the deletion.

<a id="viewTableLocation"></a>
Viewing a Table's Location
--------------------------

1.  In the Table List window, click the table name. The Table Metadata
    window displays.
2.  Click **View File Location**. The file location of the selected
    table displays in its directory in the File Browser window.


<link rel="stylesheet" href="docbook.css" type="text/css" media="screen" title="no title" charset="utf-8"></link>

Oozie Editor and Dashboard
==========================

The Oozie Editor/Dashboard application allows you to define Oozie
workflow, coordinator, and bundle applications, run workflow,
coordinator, and bundle jobs, and view the status of jobs. For
information about Oozie, see [Oozie
Documentation](http://archive.cloudera.com/cdh4/cdh/4/oozie/).

A workflow application is a collection of actions arranged in a directed
acyclic graph (DAG). It includes two types of nodes:

-   Control flow - start, end, fork, join, decision, and kill
-   Action - [MapReduce](../jobdesigner.html#mapreduce),
[Streaming](../jobdesigner.html#streaming),
[Java](../jobdesigner.html#java),
[Pig](../jobdesigner.html#pig),
[Hive](../jobdesigner.html#hive),
[Sqoop](../jobdesigner.html#sqoop),
[Shell](../jobdesigner.html#shell),
[Ssh](../jobdesigner.html#ssh),
[DistCp](../jobdesigner.html#distcp),
[Fs](../jobdesigner.html#fs), and
[Email](../jobdesigner.html#email).
    In order to run DistCp, Streaming, Pig, Sqoop, and Hive jobs, Oozie
    must be configured to use the Oozie ShareLib. See the Oozie Installation
   manual.

A coordinator application allows you to define and execute recurrent and
interdependent workflow jobs. The coordinator application defines the
conditions under which the execution of workflows can occur.

A bundle application allows you to batch a set of coordinator
applications.

Oozie Editor/Dashboard Installation and Configuration
-----------------------------------------------------

Oozie Editor/Dashboard is one of the applications installed as part of
Hue. For information about installing and configuring Hue, see the Hue Installation
manual.

Starting Oozie Editor/Dashboard
-------------------------------

Click the **Oozie Editor/Dashboard** icon
(![image](images/icon_oozie_24.png)) in the navigation bar at the top of
the Hue browser page. **Oozie Editor/Dashboard** opens with the
following screens:

-   [Dashboard](#dashboard) - shows the running and completed workflow,
    coordinator, and bundle jobs and information about Oozie
    instrumentation and configuration. The screen is selected and opened
    to the Workflows page.
-   [Workflow Manager](#workflowManager) - shows available workflows and
    allows you to create and import workflows.
-   [Coordinator Manager](#coordinatorManager) - shows available coordinators and
    allows you to create coordinators.
-   [Bundle Manager](#bundleManager) - shows available bundles and
    allows you to create bundles.

Installing Oozie Editor/Dashboard Examples
------------------------------------------

![image](images/note.jpg) **Note**: You must be a superuser to perform
this task.

1.  Click ![image](images/quick_start.png). The Quick Start Wizard
    opens.
2.  Click **Step 2: Examples**.
3.  Click **Oozie Editor/Dashboard**.

Filtering Lists in Oozie Editor/Dashboard
-----------------------------------------

Many screens contain lists. When you type in the Filter field on
screens, the lists are dynamically filtered to display only those rows
containing text that matches the specified substring.

Permissions in Oozie Editor/Dashboard
-------------------------------------

In the Dashboard workflows, coordinators, and bundles can only be
viewed, submitted, and modified by their owner or a superuser.

Editor permissions for performing actions on workflows, coordinators,
and bundles are summarized in the following table:

<table>
<th><td>Action</td><td>Superuser or Owner</td><td>All</td></th>
<tr><td>View</td><td>Y.</td><td>Only if "Is shared" is set</td></tr>
<tr><td>Submit</td><td>Y.</td><td>Only if "Is shared" is set</td></tr>
<tr><td>Modify</td><td>Y.</td><td>N</td></tr>
</table>


Deleting and Restoring Workflows, Coordinators, and Bundles
-----------------------------------------------------------

You can move workflows, coordinators, and bundles to the trash and later
restore or permanently delete them.

### Deleting Workflows, Coordinators, and Bundles

1.  In a Manager screen, check the checkbox next to one or more
    workflows, coordinators or bundles.
2.  Choose one of the following:
    -   Delete \> Move to trash
    -   Delete \> Delete forever

### Restoring Workflows, Coordinators, and Bundles

1.  In a Manager screen, click ![image](images/trash.png) **Trash**.
2.  Check the checkbox next to one or more workflows, coordinators or
    bundles.
3.  Click Restore.


<a id="dashboard"></a>
Dashboard
---------

The Dashboard shows a summary of the running and completed workflow,
coordinator, and bundle jobs.

You can view jobs for a period up to the last 30 days.

You can filter the list by date (1, 7, 15, or 30 days) or status
(Succeeded, Running, or Killed). The date and status buttons are
toggles.


### Workflows

Click the **Workflows** tab to view the running and completed workflow
jobs for the filters you have specified.

Click a workflow row in the Running or Completed table to view detailed
information about that workflow job.

In the left pane contains a link to the workflow and the variable values
specified.![image](images/workflow.jpg)

For the selected job, the following information is available in the
right area.

-   **Graph** tab shows the workflow DAG.
-   **Actions** tab shows you details about the actions that make up the
    workflow.
    -   Click the **Id** link to see additional details about the
        action.
    -   Click the **External Id** link to view the job in the Job
        Browser.

-   **Details** tab shows job statistics including start and end times.
-   **Configuration** tab shows selected job configuration settings.
-   **Logs** tab shows log output generated by the workflow job.
-   **Definition** tab shows the Oozie workflow definition, as it
    appears in the workflow.xml file (also linked under the application
    path properties in the **Details** tab and the **Configuration**
    tab).

For each action in the workflow you can:

-   Click the ![image](images/eye.png) icon to view the action screen,
    which contains:
    -   **Details** tab shows job statistics including start and end
        times.
    -   **Configuration** tab shows the action configuration settings.
    -   **Child Jobs** tab lists jobs generated by the action.

-   Click the ![image](images/log.png) icon to view the log in the Job
    Browser.


### Coordinators

Click the **Coordinators** tab to view the running and completed
coordinator jobs for the filters you have specified.

For the selected job, the following information is available.

-   The **Calendar** tab shows the timestamp of the job. Click the
    timestamp to open the workflow DAG.
-   The **Actions** tab shows you details about the actions that make up
    the coordinator.
    -   Click the **Id** link to see additional details about the
        action.
    -   Click the **External Id** link to view the job in the Job
        Browser.

-   The **Configuration** tab shows selected job configuration settings.
-   The **Logs** tab shows log output generated by the coordinator.
-   The **Definition** tab shows the Oozie coordinator definition, as it
    appears in the coordinator.xml file (also linked under the
    oozie.coord.application.path property in the **Configuration** tab).

### Bundles

Click the **Bundles** tab to view the running and completed bundle jobs
for the filters you have specified.

### Oozie

The Oozie tab provides subtabs that give you access to Oozie
instrumentation and configuration settings.

#### Instrumentation

For information on the instrumentation metrics supported by Oozie, see
[Oozie
Monitoring](http://oozie.apache.org/docs/3.3.0/AG_Monitoring.html).

#### Configuration

For information on the configuration properties supported by Oozie, see
[Oozie
Configuration](http://oozie.apache.org/docs/3.3.0/AG_Install.html#Oozie_Configuration).

<a id="workflowManager"></a>
Workflow Manager
----------------

In Workflow Manager you create Oozie workflows and submit them for
execution.

Click the **Workflows** tab to open the Workflow Manager.

Each row shows a workflow: its name, description, timestamp of its last
modification. It also shows:

-   **Steps** - the number of steps in the workflow execution path. This
    is the number of execution steps between the start and end of the
    workflow. This will not necessarily be the same as the number of
    actions in the workflow, if there are control flow nodes in the
    control path.
-   **Status** - who can run the workflow. **shared** means users other
    than the owner can access the workflow. **personal** means only the
    owner can modify or submit the workflow. The default is personal.
-   **Owner** - the user that created the workflow.

In Workflow Editor you edit workflows that include MapReduce, Streaming,
Java, Pig, Hive, Sqoop, Shell, Ssh, DistCp, Fs, Email, Sub-workflow, and
Generic actions. You can configure these actions in the Workflow Editor,
or you can import job designs from Job Designer to be used as actions in
your workflow. For information about defining workflows, see the
[Workflow
Specification](http://archive.cloudera.com/cdh4/cdh/4/oozie/WorkflowFunctionalSpec.html).

### Opening a Workflow

To open a workflow, in Workflow Manager, click the workflow. Proceed
with [Editing a Workflow](#editingWorkflow).

### Creating a Workflow

1.  Click the **Create** button at the top right.
2.  In the Name field, type a name.
3.  Check the Is shared checkbox to allow all users to access the
    workflow.
4.  Click **advanced** to specify the deployment directory or a job.xml
    file.
5.  Click **Save**. The Workflow Editor opens. Proceed with [Editing a
    Workflow](#editingWorkflow).

### Importing a Workflow

1.  Click the **Import** button at the top right.
2.  In the Name field, type a name.
3.  In the **Local workflow.xml file** field, click **Choose File** and
    select a workflow file.
4.  Click **advanced** to specify whether the workflow is shared, the
    deployment directory, or a job.xml file.
5.  Click **Save**. The Workflow Editor opens. Proceed with [Editing a
    Workflow](#editingWorkflow).

### Submitting a Workflow

To submit a workflow for execution, do one of the following:

-   In the Workflow Manager, click the radio button next to the
    workflow, and click the **Submit** button.
-   In the Workflow Editor, click the **Submit** button.

The workflow job is submitted and the Dashboard displays the workflow
job.

To view the output of the job, click ![image](images/log.png) **View the
logs**.

#### Suspending a Running Job

In the pane on the left, click the **Suspend** button.

1.  Verify that you want to suspend the job.

#### Resuming a Suspended Job

In the pane on the left, click the **Resume** button.

1.  Verify that you want to resume the job.

#### Rerunning a Workflow

In the pane on the left, click the **Rerun** button.

1.  Check the checkboxes next to the actions to rerun.
2.  Specify required variables.
3.  Click **Submit**.

### Scheduling a Workflow

To schedule a workflow for recurring execution, do one of the following:

-   In the Workflow Manager, check the checkbox next to the workflow and
    click the **Schedule** button.
-   In the Workflow Editor, click the **Schedule** button.

A coordinator is created and opened in the Coordinator Editor. Proceed
with [Editing a Coordinator](#editingCoordinator).

<a id="editingWorkflow"></a>
### Editing a Workflow

In the Workflow Editor you can easily perform operations on Oozie action
and control nodes.

#### Action Nodes

The Workflow Editor supports dragging and dropping action nodes. As you
move the action over other actions and forks, highlights indicate active
areas. If there are actions in the workflow, the active areas are the
actions themselves and the areas above and below the actions. If you
drop an action on an existing action, a fork and join is added to the
workflow.

-   Add actions to the workflow by clicking an action
    ![image](images/action.png) button and drop the action on the
    workflow. The Edit Node screen displays.
    1.  Set the action properties and click **Done**. Each action in a
        workflow must have a unique name.

-   Copy an action by clicking the ![image](images/copy.png) **Copy**
    button.

1.  The action is opened in the Edit Node screen.
2.  Edit the action properties and click **Done**. The action is added
    to the end of the workflow.

-   Delete an action by clicking the ![image](images/trash.png) button.
-   Edit an action by clicking the ![image](images/edit.png) button.
-   Change the position of an action by left-clicking and dragging an
    action to a new location.

#### Control Nodes

-   Create a fork and join by dropping an action on top of another
    action.
-   Remove a fork and join by dragging a forked action and dropping it
    above the fork.
-   Convert a fork to a decision by clicking the
    ![image](images/convert.png) button.
-   To edit a decision:
    1.  Click the ![image](images/edit.png) button.
    2.  Fill in the predicates that determine which action to perform
        and select the default action from the drop-down list.
    3.  Click **Done**.

### Uploading Workflow Files

In the Workflow Editor, click the **Upload** button.

The workspace of the workflow is opened in the File Browser application.
Follow the procedure in [Uploading
Files](../filebrowser.html#uploadingFiles) to upload the files. You must
put JAR files in a lib directory in the workspace.

### Editing Workflow Properties

1.  In the Workflow Editor, click the link under the Name or Description
    fields in the left pane.
2.  To share the workflow with all users, check the **Is shared**
    checkbox.
3.  To set advanced execution options, click **advanced** and edit the
    deployment directory, add parameters and job properties, or specify
    a job.xml file.
4.  Click **Save**.

### Displaying the History of a Workflow

1.  Click the **Dashboard** tab.
2.  Click the **Workflows** tab.
3.  Click a workflow.
4.  Click the **Actions** tab.

<a id="coordinatorManager"></a>
Coordinator Manager
-------------------

In Coordinator Manager you create Oozie coordinator applications and
submit them for execution.

Click the **Coordinators** tab to open the Coordinator Manager.

Each row shows a coordinator: its name, description, timestamp of its
last modification. It also shows:

-   **Workflow** - the workflow that will be run by the coordinator.
-   **Frequency** - how often the workflow referenced by the coordinator
    will be run.
-   **Status** - who can run the coordinator. **shared** means users
    other than the owner can access the workflow. **personal** means
    only the owner can modify or submit the workflow. The default is
    personal.
-   **Owner** - the user that created the coordinator.

In Coordinator Editor, you edit coordinators and the datasets required
by the coordinators. For information about defining coordinators and
datasets, see the [Coordinator
Specification](http://archive.cloudera.com/cdh4/cdh/4/oozie/CoordinatorFunctionalSpec.html).

### Opening a Coordinator

To open a coordinator, in Coordinator Manager, click the coordinator.
Proceed with [Editing a Coordinator](#editingCoordinator).

### Creating a Coordinator

To create a coordinator, in Coordinator Manager:

1.  Click the **Create** button at the top right. The Coordinator wizard
    opens. Proceed with [Editing a Coordinator](#editingCoordinator).

### Submitting a Coordinator

To submit a coordinator for execution, check the checkbox next to the
coordinator and click the **Submit** button.

<a id="editingCoordinator"></a>
### Editing a Coordinator

In the Coordinator Editor you specify coordinator properties and the
datasets on which the workflow scheduled by the coordinator will operate
by stepping through screens in a wizard. You can also advance to
particular steps and revisit steps by clicking the Step "tabs" above the
screens. The following instructions walk you through the wizard.

1.  Type a name, select the workflow, check the **Is shared checkbox**
    to share the job, and click **Next**. If the Coordinator Editor was
    opened after scheduling a workflow, the workflow will be set.
2.  Select how many times the coordinator will run for each specified
    unit, the start and end times of the coordinator, the timezone of
    the start and end times, and click **Next**. Times must be expressed
    as UTC times. For example, to run at 10 pm PST, specify a start time
    of 6 am UTC of the following day (+8 hours) and set the Timezone
    field to America/Los\_Angeles.
3.  Click **Add** to select an input dataset and click **Next**. If no
    datasets exist, follow the procedure in [Creating a
    Dataset](#creatingDataset).
4.  Click **Add** to select an output dataset. Click **Save
    coordinator** or click **Next** to specify advanced settings.
5.  To share the coordinator with all users, check the **Is****shared**
    checkbox.
6.  Fill in parameters to pass to Oozie, properties that determine how
    long a coordinator will wait before timing out, how many
    coordinators can run and wait concurrently, and the coordinator
    execution policy.
7.  Click **Save coordinator**.

<a id="creatingDataset"></a>
### Creating a Dataset

1.  In the Coordinator Editor, do one of the following:
    -   Click **here** in the Inputs or Outputs pane at the top of the
        editor.
    -   In the pane at the left, click the **Create new** link. Proceed
        with [Editing a Dataset](#editingDataset).

### Displaying Datasets

1.  In the Coordinator Editor, click **Show existing** in pane at the
    left.
2.  To edit a dataset, click the dataset name in the Existing datasets
    table. Proceed with [Editing a Dataset](#editingDataset).

<a id="editingDataset"></a>
### Editing a Dataset

1.  Type a name for the dataset.
2.  In the Start and Frequency fields, specify when and how often the
    dataset will be available.
3.  In the URI field, specify a URI template for the location of the
    dataset. To construct URIs and URI paths containing dates and
    timestamps, you can specify the variables
    ${YEAR},${MONTH},${DAY},${HOUR},${MINUTE}. For example:
    hdfs://foo:9000/usr/app/stats/${YEAR}/${MONTH}/data.
4.  In the Instance field, click a button to choose a default, single,
    or range of data instances. For example, if frequency==DAY, a window
    of the last rolling 5 days (not including today) would be expressed
    as start: -5 and end: -1. Check the advanced checkbox to display a
    field where you can specify a coordinator [EL
    function](http://archive.cloudera.com/cdh4/cdh/4/oozie/CoordinatorFunctionalSpec.html).
5.  Specify the timezone of the start date.
6.  In the Done flag field, specify the flag that identifies when input
    datasets are no longer ready.

### Displaying the History of a Coordinator

1.  Click the **Dashboard** tab.
2.  Click the **Coordinators** tab.
3.  Click a coordinator.
4.  Click the **Actions** tab.

<a id="bundleManager"></a>
Bundle Manager
--------------

In Bundle Manager you create Oozie bundle applications and submit them
for execution.

Click the **Bundle** tab to open the Bundle Manager.

Each row shows a bundle: its name, description, timestamp of its last
modification. It also shows:

-   **Coordinators** - the coordinators that will be run by the bundle.
-   **Kick off** - the UTC time when the coordinators referenced by the
    bundle will be started.
-   **Status** - who can run the bundle. **shared** means users other
    than the owner can access the workflow. **personal** means only the
    owner can modify or submit the workflow. The default is personal.
-   **Owner** - the user that created the bundle.

For information about defining bundles, see the [Bundle
Specification](http://archive.cloudera.com/cdh4/cdh/4/oozie/BundleFunctionalSpec.html).

### Opening a Bundle

To open a bundle, in Bundle Manager, click the bundle. Proceed with
[Editing a Bundle](#editingBundle).

### Creating a Bundle

1.  Click the **Create** button at the top right.
2.  In the Name field, type a name.
3.  In the Kick off time field, choose a kick off time.
4.  Check the Is shared checkbox to allow all users to access the
    workflow.
5.  Click **Save**. The Bundle Editor opens. Proceed with [Editing a
    Bundle](#editingBundle).

### Submitting a Bundle

To submit a bundle for execution, check the checkbox next to the bundle
and click the **Submit** button.

<a id="editingBundle"></a>
### Editing a Bundle

In the Bundle Editor, you specify properties by stepping through screens
in a wizard. You can also advance to particular steps and revisit steps
by clicking the Step "tabs" above the screens. The following
instructions walk you through the wizard.

1.  Click **Add** to select a coordinator that the bundle will kick off.
2.  Choose the kick off time. The time must be expressed as a UTC time.
    For example, to run at 10 pm PST, specify a start time of 6 am UTC
    of the following day (+8 hours).
3.  To share the bundle with all users, check the **Is shared**
    checkbox.
4.  Click **Next** to specify advanced settings or click **Save
    bundle**.
5.  Fill in parameters to pass to Oozie.
6.  Click **Save bundle**.

Displaying the History of a Bundle
----------------------------------

1.  Click the **Dashboard** tab.
2.  Click the **Bundles** tab.
3.  Click a bundle.
4.  Click the **Actions** tab.


<link rel="stylesheet" href="docbook.css" type="text/css" media="screen" title="no title" charset="utf-8"></link>

Pig Editor
==========

The Pig Editor application allows you to define Pig scripts, run
scripts, and view the status of jobs. For information about Pig, see
[Pig Documentation](http://archive.cloudera.com/cdh4/cdh/4/pig/).

Pig Editor Installation and Configuration
-----------------------------------------

Pig Editor is one of the applications installed as part of Hue. For
information about installing and configuring Hue, see the Hue Installation manual.

Pig Editor assumes an existing Pig installation. The Hue installation
instructions include the configuration necessary for Pig Editor to
access Pig.

Starting Pig Editor
-------------------

Click the **Pig Editor** icon (![image](images/icon_pig_24.png)) in the
navigation bar at the top of the Hue browser page. The Pig Editor opens
with three tabs:

-   Editor - editor where you can create, edit, run, save, copy, and
    delete scripts and edit script properties.
-   Scripts - script manager where you can create, open, run, copy, and
    delete scripts.
-   Dashboard - dashboard where you can view running and completed
    scripts and view the log of a job.

Pig Scripts
-----------

### Installing the Example Scripts

![image](images/note.jpg) **Note**: You must be a superuser to perform
this task.

1.  Click ![image](images/quick_start.png). The Quick Start Wizard
    opens.
2.  Click **Step 2: Examples**.
3.  Click **Pig Editor**.

### Creating a Script

1.  In either the Editor or Scripts screen, click New script. Edit the
    script as desired.
2.  Click Edit Properties. In the Script name field, type a name for the
    script.
3.  Click Save.

### Opening a Script

1.  Click the Scripts tab.
2.  In the list of scripts, click a script.

### Running a Script

1.  Do one of the following:
    -   In the Editor screen, click the Run button.
    -   In the Scripts screen, check the checkbox next to a script and
        click the Run button.

### Viewing the Result of Running a Script

1.  Click the Dashboard tab.
2.  Click a job.


<link rel="stylesheet" href="docbook.css" type="text/css" media="screen" title="no title" charset="utf-8"></link>

# Solr Search


The Solr Search application, which is based on  [Apache Solr](http://lucene.apache.org/solr/), allows you to perform keyword searches across Hadoop data. A wizard lets you style the result snippets, specify facets to group the results, sort the results, and highlight result fields.

## Solr Search Installation and Configuration

Solr Search is one of the applications installed as part of Hue. For information about installing and configuring Hue, see the Hue Installation
manual.

## Starting  Solr Search

Click the ** Solr Search** icon
(![image](images/icon_search_24.png)) in the navigation bar at the top of
the Hue browser page. **Solr Search** opens to the  [Collection Manager](#collectionManager). If there are no collections, the [Import Collections and Cores](#importCollection) dialog displays.

<a id="collectionManager"></a>
## Collection Manager

In Collection Manager you import, copy, and delete [collections](http://wiki.apache.org/solr/SolrCloud#A_little_about_SolrCores_and_Collections).

### Displaying the Collection Manager
When you start Solr Search, the Collection Manager displays. You navigate to the Collection Manager by clicking **Collection manager** in the Search page or the Template Editor.

###  Filtering Collections

When you type in the Filter field, the list of collections is dynamically filtered to display only those rows
containing text that matches the specified substring.

<a id="importCollection"></a>
### Importing Collections 

1. If there are existing collections, click the ![image](images/import.png) **Import** button at the top right. The Import Collections and Cores dialog displays.
1. Check the checkboxes next to the collections to import.
1. Click **Import Selected**. The collection is added to the Collection Manager.

### Editing Collection Properties
1. In the Collection Manager, click a collection.
1. In the **COLLECTION** area on the left, click **Properties**.
1. Edit a property and click **Save**.


### Searching a Collection

1. In the Collection Manager, click **Search page** or click **Search it** in the Collection area on the left. The Search page displays.
1. Select a collection from the **Search in** drop-down list.
1. Type a search string in the **Search...** text box.
1. Press **Enter** or click the ![image](images/eyeglass.png)  icon.

-  If you have defined [facets](#facets), click a facet to display only those results in the group defined by the facet.
-  If you have defined [sorting fields](#sorting), select from the **Sort by** drop-down list to sort the results.
-  Click ![image](images/clear.png) to clear the search string.

## Styling Search Results

Do one of the following:

- In the Collection Manager, click a collection.
- In the Search page, select a collection from the **Search in** drop-down list and click ** Customize this collection**.  The Template Editor displays.

### Template Editor

The Template Editor provides four features:

- [Snippet editor](#snippetEditor) - Specify the layout of the search result snippet, which fields appear in the snippet, and style the results.
- [Facet editor](#facetEditor) - Define buckets in which to group results.
- [Sort editor](#sortEditor) - Specify on which fields and order the results are sorted. 
- [Highlighting editor](#highlightingEditor) - Enable highlighting of search fields. 

<a id="snippetEditor"></a>
#### Snippet Editor

1. In the Snippet Editor, click a tab to choose the method for editing the search snippet fields and styling:
1. - **Visual editor** - Click ![image](images/layoutChooser.png) to choose an overall layout for the snippet.
1. - - Select the fields and functions from the drop-down lists on the right and click ![image](images/add.png). 
1. - - Select fields, right-click, and select **Cut** and **Paste** to place the fields on the canvas. 
1. - - Select fields and apply styling using the buttons on top.
1. - **Source** - 
1. - - Select the data fields and functions from the drop-down lists on the right.
1. - - Specify layout and styling using HTML tags.
1. - **Preview** - Preview the snippet.
1. - **Advanced** - Specify styles for CSS classes specified in the Source tab.
1. Click **Save**.

<a id="facetEditor"></a>
#### Facet Editor

By default, faceting  search result fields is disabled. Click **Enabled** to enable faceting.

1. In the Template Editor, click **2. Facets**. You can move between the facet tabs by clicking each **Step** tab, or by clicking **Back** and **Next**.
1. In the General tab, specify 
1. - **Limit** - the maximum number of values for each facet.
1. - **Mincount** - the minimum number of search results that fall into a group for the facet to display on the Search page.
1. In the Field, Range, and Date Facet tabs,  specify the facet properties and click ![image](images/add.png) **Add**.
1. In the Facets Order tab, drag and drop the facet to specify the order in they appear in the Search page.
1. Click **Save**. When you display the Search page, the facets display on the left.

<a id="sortEditor"></a>
#### Sorting Editor 

By default, sorting on search result fields is disabled. Click **Enabled** to enable sorting.

1. In the Template Editor, click **3. Sorting**. 
1. In the Field drop-down, select a field. Optionally specify a label for the field. 
1. The default order is ascending. Click the arrows to change the order.
1. Click  ![image](images/add.png) **Add**.
1. Click **Save**. When you display search results, the results are sorted by the fields in the order that  they appear left to right. 

<a id="highlightingEditor"></a>
#### Highlighting Editor 

By default,highlighting search result fields is disabled. Click **Enabled** to enable highlighting.

1. In the Template Editor, click **3. Highlighting**. 
1. Select the fields to be highlighted.
1. Click **Save**. When you display search results, the selected fields are displayed with the style of the **em** class defined in the Advanced tab of the [Snippet editor](#snippetEditor). 







<link rel="stylesheet" href="docbook.css" type="text/css" media="screen" title="no title" charset="utf-8"></link>

Hue Shell
=========

The Hue Shell application provides access to the Pig, HBase, and Sqoop 2
command-line shells. The Shell application is designed to have the same
look and feel as a Unix terminal. In addition to the shells configured
by default, it is possible to include almost any process that exposes a
command-line interface as an option in this Hue application.

Hue Shell Installation and Configuration
----------------------------------------

Hue Shell is one of the applications installed as part of Hue. For
information about installing and configuring Hue, see Hue Installation
in
[http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/latest/CDH4-Installation-Guide/CDH4-Installation-Guide.html](http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/latest/CDH4-Installation-Guide/CDH4-Installation-Guide.html).

### Unix User Accounts

To properly isolate subprocesses so as to guarantee security, each Hue
user who is using the Shell subprocess must have a Unix user account.
The link between Hue users and Unix user accounts is the username, and
so every Hue user who wants to use the Shell application must have a
Unix user account with the same name on the system that runs the Hue
Server. See Unix User Accounts in
[http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/latest/CDH4-Installation-Guide/CDH4-Installation-Guide.html](http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/latest/CDH4-Installation-Guide/CDH4-Installation-Guide.html)
for instructions.

Starting Hue Shell
------------------

1.  Click the **Shell** icon (![image](images/icon_shell_24.png)) in the
    navigation bar at the top of the Hue web page. (To start a second
    instance of the Shell application, right-click the link and select
    **Open link in new tab**.) The **Shell** window opens in the Hue web
    page.
2.  Click any of the tabs at the top of the Shell window to open a
    subprocess shell of that type.
    ![image](images/note.jpg) **Note**: If a button is disabled, the
    program is not on the system path. Ask your Hue administrator to fix
    this problem.
3.  After opening a subprocess, click anywhere in the body of the Shell
    application window to focus the command line. (Tab-completion is not
    supported.)
4.  To end a process, type exit or quit depending on the type of
    subprocess you have opened.
    ![image](images/note.jpg) **Note**: If you close your browser, the
    underlying shell process remains running for the amount of time
    specified by your Hue administrator. After this time the process is
    killed.

Viewing Documentation for the Shells
------------------------------------

For information about using each of the default shells, see the
documentation on the following sites:

-   [Pig](http://archive.cloudera.com/cdh4/cdh/4/pig/)
-   [HBase](http://archive.cloudera.com/cdh4/cdh/4/hbase/)
-   [Sqoop 2](http://archive.cloudera.com/cdh4/cdh/4/sqoop2/)


<link rel="stylesheet" href="docbook.css" type="text/css" media="screen" title="no title" charset="utf-8"></link>

Sqoop UI
========

The Sqoop UI enables transfering data from a relational database
to Hadoop and vice versa. The UI lives uses Apache Sqoop to do this.
See the [Sqoop Documentation](http://sqoop.apache.org/docs/1.99.2/index.html) for more details on Sqoop.

Installation and Configuration
------------------------------

The Sqoop UI is one of the applications installed as part of
Hue. For information about installing and configuring Hue, see the Hue Installation
manual.

Starting
--------

Click the **Sqoop** icon
(![image](images/icon_sqoop_24.png)) in the navigation bar at the top of
the Hue browser page.

Sqoop Jobs
----------
Sqoop UI is oriented around jobs in Apache Sqoop.

### Creating a New Job

1. Click the **New job** button at the top right.
2. In the Name field, enter a name.
3. Choose the type of job: import or export.
   The proceeding form fields will change depending on which type is chosen.
4. Select a connection, or create one if it does not exist.
5. Fill in the rest of the fields for the job.
   For importing, the "Table name", "Storage type", "Output format", and "Output directory" are necessary at a minimum.
   For exporting, the "Table name" and "Input directory" are necessary at a minimum.
6. Click **save** to finish.

### Editing a Job

1. In the list of jobs, click on the name of the job.
2. Edit the desired form fields in the job.

### Copying a Job

1. In the list of jobs, click on the name of the job.
2. On the left hand side of the job editor, there should be a panel containing actions.
   Click **Copy**.

### Removing a Job

1. In the list of jobs, click on the name of the job.
2. On the left hand side of the job editor, there should be a panel containing actions.
   Click **Delete**.

### Running a Job

There's a status on each of the items in the job list indicating
the last time a job was ran. The progress of the job should dynamically
update. There's a progress bar at the bottom of each item on the job list
as well.

1. In the list of jobs, click on the name of the job.
2. On the left hand side of the job editor, there should be a panel containing actions.
   Click **Run**.

### Creating a New Connection

1. Click the **New job** button at the top right.
2. At the connection field, click the link titled **Add a new connection**.
3. Fill in the displayed fields.
4. Click **save** to finish.

### Editing a Connection

1. Click the **New job** button at the top right.
2. At the connection field, select the connection by name that should be edited.
3. Click **Edit**.
4. Edit the any of the fields.
5. Click **save** to finish.

### Removing a Connection

1. Click the **New job** button at the top right.
2. At the connection field, select the connection by name that should be deleted.
3. Click **Delete**.

NOTE: If this does not work, it's like because a job is using that connection.
      Make sure not jobs are using the connection that will be deleted.

### Filtering Sqoop Jobs

The text field in the top, left corner of the Sqoop Jobs page enables fast filtering
of sqoop jobs by name.

<link rel="stylesheet" href="docbook.css" type="text/css" media="screen" title="no title" charset="utf-8"></link>

ZooKeeper Browser
=================

The main two features are:

- Listing of the ZooKeeper cluster stats and clients
- Browsing and edition of the ZNode hierarchy


ZooKeeper Browser requires the [ZooKeeper
REST](https://github.com/apache/zookeeper/tree/trunk/src/contrib/rest)
service to be running. Here is how to setup this one:

First get and build ZooKeeper:

<pre>
git clone https://github.com/apache/zookeeper
cd zookeeper
ant
Buildfile: /home/hue/Development/zookeeper/build.xml

init:
       [mkdir] Created dir: /home/hue/Development/zookeeper/build/classes
       [mkdir] Created dir: /home/hue/Development/zookeeper/build/lib
       [mkdir] Created dir: /home/hue/Development/zookeeper/build/package/lib
       [mkdir] Created dir: /home/hue/Development/zookeeper/build/test/lib

   ...
</pre>

And start the REST service:

<pre>
cd src/contrib/rest
nohup ant run&
</pre>

If ZooKeeper and the REST service are not on the same machine as Hue, go
update the [Hue
settings](https://github.com/cloudera/hue/blob/master/desktop/conf.dist/hue.ini#L581)
and specify the correct hostnames and ports:

<pre>
    [zookeeper]

      [[clusters]]

        [[[default]]]
          # Zookeeper ensemble. Comma separated list of Host/Port.
          # e.g. localhost:2181,localhost:2182,localhost:2183
          ## host_ports=localhost:2181

          # The URL of the REST contrib service
          ## rest_url=http://localhost:9998
</pre>


This directory contains thirdparty code outside Hue.

Please update this table for any third party dependencies.

---
Checked-in third party dependencies

|Re-distributed|Project|Version|License|Source URL|
|--------------|-------|-------|-------|----------|
|Y|A Midsummer Nights Dream by Shakespeare|?|Public Domain from Gutenberg|http://www.gutenberg.org/dirs/etext98/2ws1710.txt|
|Y|Avro|1.5.0|ASL2|http://avro.apache.org/|
|Y|CherryPy|3.1.2|BSD|http://www.cherrypy.org/|
|Y|ConfigObj|4.6.0|BSD|http://www.voidspace.org.uk/python/configobj.html|
|Y|ctypes|1.0.2|MIT|http://pypi.python.org/pypi/ctypes|
|Y|Django|1.2.3|BSD|http://www.djangoproject.com/download/1.2.3/tarball/|
|Y|django-auth-ldap|1.0.7|BSD|http://bitbucket.org/psagers/django-auth-ldap/|
|Y|Django_cpserver|19739bea057d824d9cd6c10c4caec622e8e1c0b9|BSD|http://github.com/lincolnloop/django-cpserver/|
|Y|Django Extensions|0.5|New BSD|http://pypi.python.org/pypi/django-extensions/0.5|
|Y|Django_nose (part of basie)|?|MIT|http://code.basieproject.org/trunk/apps/django_nose/nose_runner.py|
|Y|elementtree|1.2.6-20050316|Python(MIT)|http://effbot.org/downloads#elementtree|
|Y|enum|0.4.4|Python|http://pypi.python.org/pypi/enum/|
|Y|kerberos|1.1.1|ASL2|http://pypi.python.org/pypi/kerberos|
|Y|lockfile|0.8|MIT|http://smontanaro.dyndns.org/python/lockfile-0.8.tar.gz|
|Y|lxml|2.2.2|BSD|http://codespeak.net/lxml/|
|Y|Mako|0.3.4|MIT|http://makotemplates.org/|
|Y|Markdown|2.0.3|BSD|http://pypi.python.org/packages/source/M/Markdown/Markdown-2.0.3.tar.gz|
|Y|MarkupSafe|0.9.3|BSD|http://pypi.python.org/pypi/MarkupSafe|
|Y|MySQL for Python|1.2.3c1|GPL or the original license based on Python 1.5.2|http://sourceforge.net/projects/mysql-python/|
|Y|parquet-python||ASL2|https://github.com/jcrobak/parquet-python|
|Y|Pygments|1.3.1|BSD|http://pypi.python.org/pypi/Pygments|
|Y|PyOpenSSL|0.13|ASL2|https://launchpad.net/pyopenssl|
|Y|pysqlite|2.55|zlib/libpng|http://oss.itsystementwicklung.de/download/pysqlite/2.5/2.5.5/|
|Y|python-daemon|1.5.1|Python|http://pypi.python.org/pypi/python-daemon/|
|Y|python-ldap|2.3.13|Python|http://pypi.python.org/pypi/python-ldap/|
|Y|Python-pam|0.1.3|MIT|http://atlee.ca/software/pam/dist/0.1.3|
|Y|Python-paste|1.7.2|MIT|http://pythonpaste.org|
|Y|pytidylib|0.2.1|MIT|http://cloud.github.com/downloads/countergram/pytidylib/pytidylib-0.2.1.tar.gz|
|Y|PyYAML|3.09|MIT|http://pyyaml.org/wiki/PyYAML|
|Y|requests|2.0.0|ASL2|https://github.com/kennethreitz/requests/|
|Y|Shakespeares Sonnets|?|Public Domain from Gutenberg|http://www.gutenberg.org/dirs/etext97/wssnt10.txt|
|Y|sasl|0.1.1|Apache|http://pypi.python.org/pypi/sasl/0.1.1|
|Y|simplejson|2.0.9|MIT|http://pypi.python.org/packages/source/s/simplejson/simplejson-2.0.9.tar.gz|
|Y|South|0.7|Apache|http://south.aeracode.org/|
|Y|ssl|1.15|Python|http://pypi.python.org/pypi/ssl/1.15|
|Y|tablib|0.9.11|MIT|http://docs.python-tablib.org/en/latest/|
|Y|threadframe|0.2|Python|http://www.majid.info/mylos/stories/2004/06/10/threadframe.html|
|Y|Thrift|?|Apache|http://incubator.apache.org/thrift/download/|
|Y|urllib2_kerberos|0.1.6|ASL2|http://pypi.python.org/pypi/urllib2_kerberos|
|Y|Werkzeug|0.5.1|BSD|http://pypi.python.org/packages/source/W/Werkzeug/Werkzeug-0.5.1.zip|

---
Downloaded third party dependencies during build

|Re-distributed|Project|Version|License|Source URL|
|--------------|-------|-------|-------|----------|
|Y|ipython|0.10|BSD|http://ipython.scipy.org/dist|
|Y|MooTools-Core|?|MIT|http://mootools.net/download|
|Y|Nose|0.11.1|LGPL|http://somethingaboutorange.com/mrl/projects/nose/0.11.1/|
|Y|nosetty|0.4|LGPL|http://code.google.com/p/nosetty/|
|N|pylint|0.19.0|GPL|http://www.logilab.org/project/pylint|
|Y|Werkzeug|0.5.1|BSD|http://werkzeug.pocoo.org/|
|Y|windmill|1.3|Apache2|http://www.getwindmill.com/|

.. image:: docs/images/hue_logo.png

Welcome to the repository for Hue
=================================

Hue is an open source Web interface for analyzing data with Apache Hadoop: `gethue.com
<http://gethue.com>`_ 

.. image:: docs/images/hue-screen.png

It features:

      * File Browser for accessing HDFS
      * Hive Editor for developing and running Hive queries
      * Search App for querying, exploring, visualizing data and dashboards with Solr
      * Impala App for executing interactive SQL queries
      * Spark Editor and Dashboard
      * Pig Editor for submitting Pig scripts
      * Oozie Editor and Dashboard for submitting and monitoring workflows, coordinators and bundles
      * HBase Browser for visualizing, querying and modifying HBase tables
      * Metastore Browser for accessing Hive metadata and HCatalog
      * Job Browser for accessing MapReduce jobs (MR1/MR2-YARN)
      * Job Designer for creating MapReduce/Streaming/Java jobs
      * A Sqoop 2 Editor and Dashboard
      * A ZooKeeper Browser and Editor
      * A DB Query Editor for MySql, PostGres, Sqlite and Oracle

On top of that, a SDK is available for creating new apps integrated with Hadoop.

More user and developer documentation is available at http://gethue.com.


Getting Started
===============
To build and get the development server running::

    $ git clone http://github.com/cloudera/hue.git
    $ cd hue
    $ make apps
    $ build/env/bin/hue runserver

Now Hue should be running on http://localhost:8000 !

The configuration in development mode is ``desktop/conf/pseudo-distributed.ini``.


Note: to start the production server (but lose the automatic reloading after source modification)::

   $ build/env/bin/supervisor

To run the tests::

   Install the mini cluster (only once):
   $ ./tools/jenkins/jenkins.sh slow

   Run all the tests:
   $ build/env/bin/hue test all

   Or just some parts of the tests, e.g.:
   $ build/env/bin/hue test specific impala
   $ build/env/bin/hue test specific impala.tests:TestMockedImpala
   $ build/env/bin/hue test specific impala.tests:TestMockedImpala.test_basic_flow


Development Prerequisites
===========================
You'll need these library development packages and tools installed on
your system:

    Ubuntu:
      * ant
      * gcc
      * g++
      * libkrb5-dev
      * libmysqlclient-dev
      * libssl-dev
      * libsasl2-dev
      * libsasl2-modules-gssapi-mit
      * libsqlite3-dev
      * libtidy-0.99-0 (for unit tests only)
      * libxml2-dev
      * libxslt-dev
      * mvn (from ``maven2`` package or tarball)
      * openldap-dev / libldap2-dev
      * python-dev
      * python-simplejson
      * python-setuptools

    CentOS:
      * ant
      * asciidoc
      * cyrus-sasl-devel
      * cyrus-sasl-gssapi
      * gcc
      * gcc-c++
      * krb5-devel
      * libtidy (for unit tests only)
      * libxml2-devel
      * libxslt-devel
      * mvn (from ``maven2`` package or tarball)
      * mysql
      * mysql-devel
      * openldap-devel
      * python-devel
      * python-simplejson
      * sqlite-devel

    MacOS (mac port):
      * liblxml
      * libxml2
      * libxslt
      * mysql5-devel
      * simplejson (easy_install)
      * sqlite3


File Layout
===========
The Hue "framework" is in ``desktop``. ``/core/`` contains the Web components and
``desktop/libs/`` the API for talking to Hadoop.
The installable apps live in ``apps/``.  Please place third-party dependencies in the app's ext-py/
directory.

The typical directory structure for inside an application includes:

  src/
    for Python/Django code
      models.py
      urls.py
      views.py
      forms.py
      settings.py

  conf/
    for configuration (``.ini``) files to be installed

  static/
    for static HTML/js resources and help doc

  templates/
    for data to be put through a template engine

  locales/
    for localizations in multiple languages

For the URLs within your application, you should make your own ``urls.py``
which will be automatically rooted at ``/yourappname/`` in the global
namespace.  See ``apps/about/src/about/urls.py`` for an example.


Main Stack
==========
Hue would not be possible without:

   * Python 2.6 - 2.7
   * Django 1.4 (https://docs.djangoproject.com/en/1.4/)
   * Knockout.js (http://knockoutjs.com/)
   * jQuery (http://jquery.com/)
   * Bootstrap (http://getbootstrap.com/)


Community
=========
   * User group: http://groups.google.com/a/cloudera.org/group/hue-user
   * Jira: https://issues.cloudera.org/browse/HUE
   * Reviews: https://review.cloudera.org/dashboard/?view=to-group&group=hue (repo 'hue-rw')


License
=======
Apache License, Version 2.0
http://www.apache.org/licenses/LICENSE-2.0


# Based on [Bootplus v1.0.3](https://github.io/aozora/bootplus)

Bootplus is a front-end framework for faster and easier web development inspired by the lates Google+ look & feel, created and maintained by [Aozora](http://twitter.com/aozoralabs).

Bootplus is based on [Twitter Bootstrap](http://twitter.github.io/bootstrap)
To get started, check out [http://aozora.github.com/bootplus](http://aozora.github.io/bootplus)!

## Compiling CSS

Bootplus includes a [makefile](Makefile) with convenient methods for working with the framework. Before getting started, be sure to install [the necessary local dependencies](package.json):

```
$ npm install
```

When completed, you'll be able to run the various make commands provided:

#### build - `make`
Runs the recess compiler to rebuild the `/less` files and compiles the docs. Requires recess.

#### watch - `make watch`
This is a convenience method for watching just Less files and automatically building them whenever you save. Requires the Watchr gem.

Should you encounter problems with installing dependencies or running the makefile commands, be sure to first uninstall any previous versions (global and local) you may have installed, and then rerun `npm install`.


## Authors

**Marcello Palmitessa**

+ [http://twitter.com/aozoralabs](http://twitter.com/aozoralabs)
+ [https://github.com/aozora](https://github.com/aozora)


## Copyright and license

Bootplus is dual licensed, GPL-2 and Apache-2; see the LICENSE file.


__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# Holland documentation build configuration file, created by
# sphinx-quickstart on Sat Jan 29 23:27:28 2011.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#sys.path.insert(0, os.path.abspath('.'))

# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc', 'sphinx.ext.doctest', 'sphinx.ext.intersphinx', 'sphinx.ext.todo', 'sphinx.ext.coverage', 'sphinx.ext.ifconfig', 
        # 'sphinx.ext.viewcode'
        ]

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'Holland'
copyright = u'2011-2013, Holland Core Team'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = '1.0'
# The full version, including alpha/beta/rc tags.
version = '1.0'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = []

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'holland'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
html_theme_path = ['_themes']

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
html_logo = 'holland_logo_64x64.png'

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'Hollanddoc'


# -- Options for LaTeX output --------------------------------------------------

# The paper size ('letter' or 'a4').
#latex_paper_size = 'letter'

# The font size ('10pt', '11pt' or '12pt').
#latex_font_size = '10pt'

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'Holland.tex', u'Holland Documentation',
   u'Holland Core Team', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Additional stuff for the LaTeX preamble.
#latex_preamble = ''

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'holland', u'Holland Documentation',
     [u'Holland Core Team'], 1)
]


# Example configuration for intersphinx: refer to the Python standard library.
intersphinx_mapping = {'http://docs.python.org/': None}

########NEW FILE########
__FILENAME__ = backup
import os, sys
from subprocess import Popen, PIPE
import time
import errno
import fcntl
import logging
from holland.core.command import Command, option, run
from holland.core.backup import BackupRunner, BackupError
from holland.core.exceptions import BackupError
from holland.core.config import hollandcfg, ConfigError
from holland.core.spool import spool
from holland.core.util.fmt import format_interval, format_bytes
from holland.core.util.path import disk_free, disk_capacity, getmount
from holland.core.util.lock import Lock, LockError
from holland.core.util.pycompat import Template

LOG = logging.getLogger(__name__)

class Backup(Command):
    """${cmd_usage}

    Backup the specified backupsets or all
    active backupsets specified in holland.conf

    ${cmd_option_list}

    """

    name = 'backup'

    aliases = [
        'bk'
    ]

    options = [
        option('--abort-immediately', action='store_true',
                help="Abort on the first backupset that fails."),
        option('--dry-run', '-n', action='store_true',
                help="Print backup commands without executing them."),
        option('--no-lock', '-f', action='store_true', default=False,
                help="Run even if another copy of Holland is running.")
    ]

    description = 'Run backups for active backupsets'

    def run(self, cmd, opts, *backupsets):
        if not backupsets:
            backupsets = hollandcfg.lookup('holland.backupsets')

        # strip empty items from backupsets list
        backupsets = [name for name in backupsets if name]

        if not backupsets:
            LOG.info("Nothing to backup")
            return 1

        runner = BackupRunner(spool)

        # dry-run implies no-lock
        if opts.dry_run:
            opts.no_lock = True

        # don't purge if doing a dry-run, or when simultaneous backups may be running
        if not opts.no_lock:
            purge_mgr = PurgeManager()

            runner.register_cb('before-backup', purge_mgr)
            runner.register_cb('after-backup', purge_mgr)
            runner.register_cb('failed-backup', purge_backup)

        runner.register_cb('after-backup', report_low_space)

        runner.register_cb('before-backup', call_hooks)
        runner.register_cb('after-backup', call_hooks)
        runner.register_cb('failed-backup', call_hooks)

        error = 1
        LOG.info("--- Starting %s run ---", opts.dry_run and 'dry' or 'backup')
        for name in backupsets:
            try:
                config = hollandcfg.backupset(name)
                # ensure we have at least an empty holland:backup section
                config.setdefault('holland:backup', {})
            except (SyntaxError, IOError), exc:
                LOG.error("Could not load backupset '%s': %s", name, exc)
                break

            if not opts.no_lock:
                lock = Lock(config.filename)
                try:
                    lock.acquire()
                    LOG.debug("Set advisory lock on %s", lock.path)
                except LockError:
                    LOG.debug("Unable to acquire advisory lock on %s",
                              lock.path)
                    LOG.error("Another holland backup process is already "
                              "running backupset '%s'. Aborting.", name)
                    break

            try:
                try:
                    runner.backup(name, config, opts.dry_run)
                except BackupError, exc:
                    LOG.error("Backup failed: %s", exc.args[0])
                    break
                except ConfigError, exc:
                    break
            finally:
                if not opts.no_lock:
                    if lock.is_locked():
                        lock.release()
                    LOG.info("Released lock %s", lock.path)
        else:
            error = 0
        LOG.info("--- Ending %s run ---", opts.dry_run and 'dry' or 'backup')
        return error

def purge_backup(event, entry):
    if entry.config['holland:backup']['auto-purge-failures']:
        entry.purge()
        LOG.info("Purged failed backup: %s", entry.name)
    else:
        LOG.info("auto-purge-failures not enabled. Failed backup not purged.")

def call_hooks(event, entry):
    hook = event + "-command"

    if entry.config['holland:backup'][hook] is not None:
        cmd = entry.config['holland:backup'][hook]
        try:
            cmd = Template(cmd).safe_substitute(
                        hook=hook,
                        backupset=entry.backupset,
                        backupdir=entry.path
            )
            LOG.info(" [%s]> %s", hook, cmd)
            process = Popen(cmd,
                            shell=True,
                            stdin=open("/dev/null", "r"),
                            stdout=PIPE,
                            stderr=PIPE,
                            close_fds=True)
            output, errors = process.communicate()
        except OSError, exc:
            raise BackupError("%s", exc)

        for line in errors.splitlines():
            LOG.error(" ! %s", line)
        for line in output.splitlines():
            LOG.info(" + %s", line)
        if process.returncode != 0:
            raise BackupError("%s command failed" % hook)
    return 0

class PurgeManager(object):
    def __call__(self, event, entry):
        purge_policy = entry.config['holland:backup']['purge-policy']

        if event == 'before-backup' and purge_policy != 'before-backup':
            return
        if event == 'after-backup' and purge_policy != 'after-backup':
            return

        backupset = spool.find_backupset(entry.backupset)
        if not backupset:
            LOG.info("Nothing to purge")
            return

        retention_count = entry.config['holland:backup']['backups-to-keep']
        retention_count = int(retention_count)
        if event == 'after-backup' and retention_count == 0:
            # Always maintain latest backup
            LOG.warning("!! backups-to-keep set to 0, but "
                        "purge-policy = after-backup. This would immediately "
                        "purge all backups which is probably not intended. "
                        "Setting backups-to-keep to 1")
            retention_count = 1
        if event == 'before-backup':
            retention_count += 1
        self.purge_backupset(backupset, retention_count)
        backupset.update_symlinks()

    def purge_backupset(self, backupset, retention_count):
        purge_count = 0
        for backup in backupset.purge(retention_count):
            purge_count += 1
            LOG.info("Purged %s", backup.name)

        if purge_count == 0:
            LOG.info("No backups purged")
        else:
            LOG.info("%d backups purged", purge_count)

def report_low_space(event, entry):
    total_space = disk_capacity(entry.path)
    free_space = disk_free(entry.path)
    if free_space < 0.10*total_space:
        LOG.warning("Extremely low free space on %s's filesystem (%s).",
                    entry.path,
                    getmount(entry.path))
        LOG.warning("%s of %s [%.2f%%] remaining",
                    format_bytes(free_space),
                    format_bytes(total_space),
                    (float(free_space) / total_space)*100)

########NEW FILE########
__FILENAME__ = help
import os
import sys
# for python2.3 support
if sys.version_info[:2] < (2, 4):
    from sets import Set as set
from holland.core.command.command import Command, option
from holland.core.plugin import get_commands
from holland.core.cmdshell import parser

class Help(Command):
    """${cmd_usage}

    ${cmd_option_list}

    """
    name = 'help'
    aliases = [
        'h'
    ]
    options = [
        option('-v', '--verbose', action='store_true',
                help="Display more information")
    ]
    description = 'Show help for a command'
    def run(self, cmd, opts, command=None):

        commands = get_commands()
        if not command:
            print >>sys.stderr, "No command specified"
            parser.print_help()

            if not commands:
                print >>sys.stderr, "No available commands"
            else:
                print "Available Commands:"
                commands = list(set(commands.values()))
                commands.sort(lambda x,y: cmp(x.name, y.name))
                for cls in commands:
                    if cls.aliases:
                        cmdname = "%-13s (%s)" % (cls.name, ','.join(cls.aliases))
                    else:
                        cmdname = cls.name
                    print "   %-19s  %s" % (cmdname, cls.description)

            return 1

        if not command in commands:
            print >>sys.stderr, "No such command: %r" % command
            return os.EX_TEMPFAIL

        cmdinst = commands[command]()
        print cmdinst.help()

########NEW FILE########
__FILENAME__ = list_backups
import os
import textwrap
from holland.core.command import Command, option
from holland.core.spool import spool
from holland.core.config import hollandcfg
from holland.core.plugin import load_backup_plugin

class ListBackups(Command):
    """${cmd_usage}

    ${cmd_option_list}

    Lists available backups
    """
    name = 'list-backups'
    aliases = [
        'lb'
    ]
    description = 'List available backups'
    options = [
        option('-v', '--verbose', action='store_true',
                help="Verbose output")
    ]

    def print_table(self, table):
        header = table[0]
        rest = table[1:]
        fmt = "%-28s %-9s %-16s %s"
        print fmt % tuple(header)
        print "-"*80
        for row in rest:
            print fmt % tuple(row)

    def run(self, cmd, opts):
        backup_list = [x for x in spool.list_backups()]
        if not backup_list:
            print "No backups"
            return 0

        backupsets_seen = []
        for backup in backup_list:
            if backup.backupset not in backupsets_seen:
                backupsets_seen.append(backup.backupset)
                print "Backupset[%s]:" % (backup.backupset)
            # Read the backup.conf
            backup.load_config()
            plugin_name = backup.config.get('holland:backup', {})['plugin']
            if not plugin_name:
                print "Skipping broken backup: %s" % backup.name
                continue
            print "\t%s" % backup.name 
            if opts.verbose:
                print "\t", backup.info()
                plugin = load_backup_plugin(plugin_name)
                plugin = plugin(backup.backupset, backup.config, backup.path)
                if hasattr(plugin, 'info'):
                    plugin_info = plugin.info()
                    import re
                    rec = re.compile(r'^', re.M)
                    print rec.sub('\t\t', plugin_info)

        return 0

########NEW FILE########
__FILENAME__ = list_plugins
import os
import textwrap
from holland.core.command import Command
from holland.core.plugin import iter_plugins

class ListPlugins(Command):
    """${cmd_usage}

    ${cmd_option_list}

    Lists installed plugins
    """
    name = 'list-plugins'
    aliases = [
        'lp'
    ]
    description = 'List installed plugins'


    def print_table(self, table):
        header = table[0]
        rest = table[1:]
        fmt = "%-12s %-15s %-9s %-16s %s"
        print fmt % tuple(header)
        print "-"*80
        for row in rest:
            print fmt % tuple(row)

    def run(self, cmd, opts):
        table_header = ["Plugin-Type", "Plugin-Name", "Version", "Author", "Summary"]
        table = []

        for plugin, metainfo in iter_plugins('holland.backup'):
            row = ['backup', plugin]
            for key in ['version','author','summary']:
                row.append(metainfo.get(key, '-'))
            table.append(row)

        for plugin, metainfo in iter_plugins('holland.commands'):
            row = ['command', plugin]
            for key in ['version','author','summary']:
                row.append(metainfo.get(key, '-'))
            table.append(row)
        table.sort()
        table.insert(0, table_header)
        if len(table) == 1:
            print "No Plugins Found"
        else:
            self.print_table(table)

        return 0

########NEW FILE########
__FILENAME__ = mk_config
"""
Command support for generating backupset configs
"""

import os
import sys
import tempfile
import logging
import subprocess
from StringIO import StringIO

from holland.core.command import Command, option
from holland.core.plugin import load_first_entrypoint, PluginLoadError
from holland.core.config.configobj import ConfigObj, flatten_errors, ParseError
from holland.core.config import hollandcfg
from holland.core.config.validate import Validator
from holland.core.config.checks import validator

LOGGER = logging.getLogger(__name__)


def which(cmd, search_path=None):
    """Find the canonical path for a command"""
    if cmd == '':
        return None

    if not search_path:
        search_path = os.getenv('PATH', '').split(':')

    logging.debug("Using search_path: %r", search_path)
    for name in search_path:
        cmd_path = os.path.join(name, cmd)
        if os.access(cmd_path, os.X_OK):
            return cmd_path
    else:
        return None

def _find_editor():
    candidates = [
        os.getenv('VISUAL',''),
        os.getenv('EDITOR',''),
        '/usr/bin/editor',
        'vim',
        'vi',
        'ed'
    ]
    for command in candidates:
        real_cmd = which(command)
        logging.debug("%r => %r", command, real_cmd)
        if real_cmd:
            return real_cmd
    else:
        return None

def _report_errors(cfg, errors):
    for entry in flatten_errors(cfg, errors):
        (section,), key, error = entry
        param = '.'.join((section, key))
        if key is not None:
            pass
        else:
            param = ' '.join((section, '[missing section]'))
        if error == False:
            error = 'Missing value or section'
        print >>sys.stderr, param, ' = ', error

def confirm(prompt=None, resp=False):
    """prompts for yes or no response from the user. Returns True for yes and
    False for no.

    'resp' should be set to the default value assumed by the caller when
    user simply types ENTER.

    >>> confirm(prompt='Create Directory?', resp=True)
    Create Directory? [y]|n:
    True
    >>> confirm(prompt='Create Directory?', resp=False)
    Create Directory? [n]|y:
    False
    >>> confirm(prompt='Create Directory?', resp=False)
    Create Directory? [n]|y: y
    True

    """

    if prompt is None:
        prompt = 'Confirm'

    if resp:
        prompt = '%s ([%s] or %s): ' % (prompt, 'y', 'n')
    else:
        prompt = '%s ([%s] or %s): ' % (prompt, 'n', 'y')

    while True:
        ans = raw_input(prompt)
        if not ans:
            return resp
        if ans not in ['y', 'Y', 'n', 'N']:
            print >>sys.stderr, 'please enter y or n.'
            continue
        if ans == 'y' or ans == 'Y':
            return True
        if ans == 'n' or ans == 'N':
            return False

class MkConfig(Command):
    """${cmd_usage}

    Generate a config file for a backup
    plugin.

    ${cmd_option_list}

    """

    name = 'mk-config'

    aliases = [
        'mc'
    ]

    options = [
        option('--name',
                help='Name of the backupset'),
        option('--edit', action='store_true',
                help='Edit the generated config'),
        option('--provider', action='store_true',
                help='Generate a provider config'),
        option('--file', '-f',
                help='Save the final config to the specified file'),
        option('--minimal', '-m', action='store_true', default=False,
               help="Do not include comment from a backup "
                    "plugin's configspec"),
    ]

    description = 'Generate a config file for a backup plugin'


    # After initial validation:
    #   run through and flag required parameters with a 'REQUIRED' comment
    #   run through and comment out default=None parameters
    def _cleanup_config(self, config, skip_comments=False):
        errors = config.validate(validator, preserve_errors=True,copy=True)
        # First flag any required parameters
        for entry in flatten_errors(config, errors):
            section_list, key, error = entry
            section_name, = section_list
            if error is False:
                config[section_name][key] = ''
                config[section_name].comments[key].append('REQUIRED')
            elif error:
                print >>sys.stderr, "Bad configspec generated error", error

        pending_comments = []
        for section in list(config):
            if pending_comments:
                if skip_comments:
                    comments = []
                else:
                    comments = config.comments.get(section, [])
                comments = pending_comments + comments
                config.comments[section] = comments
                del pending_comments[:]
            for idx, (key, value) in enumerate(config[section].items()):
                if value is None:
                    if not skip_comments:
                        pending_comments.extend(config[section].comments.get(key, []))
                    pending_comments.append('%s = "" # no default' % key)
                    del config[section][key]
                else:
                    if skip_comments:
                        del config[section].comments[key][:]
                    if pending_comments:
                        if skip_comments:
                            comments = []
                        else:
                            comments = config[section].comments.get(key, [])
                        comments = pending_comments + comments
                        config[section].comments[key] = comments
                        del pending_comments[:]
                    if value is True or value is False:
                        config[section][key] = ['no','yes'][value]

        if pending_comments:
            if skip_comments:
                config.final_comment = pending_comments
            else:
                config.final_comment = pending_comments + config.final_comment

        # drop initial whitespace
        config.initial_comment = []
        # insert a blank between [holland:backup] and first section
        try:
            config.comments[config.sections[1]].insert(0, '')
        except IndexError:
            pass

    def run(self, cmd, opts, plugin_type):
        if opts.name and opts.provider:
            print >>sys.stderr, "Can't specify a name for a global provider config"
            return 1

        try:
            plugin_cls = load_first_entrypoint('holland.backup', plugin_type)
        except PluginLoadError, exc:
            logging.info("Failed to load backup plugin %r: %s",
                         plugin_type, exc)
            return 1

        try:
            cfgspec = sys.modules[plugin_cls.__module__].CONFIGSPEC
        except:
            print >>sys.stderr, "Could not load config-spec from plugin %r" % plugin_type
            return 1

        base_config = """
        [holland:backup]
        plugin                  = ""
        backups-to-keep         = 1
        auto-purge-failures     = yes
        purge-policy            = after-backup
        estimated-size-factor   = 1.0
        """.lstrip().splitlines()
        cfg = ConfigObj(base_config, configspec=cfgspec, list_values=True,stringify=True)
        cfg['holland:backup']['plugin'] = plugin_type
        self._cleanup_config(cfg, skip_comments=opts.minimal)

        if opts.edit:
            done = False
            editor = _find_editor()
            if not editor:
                print >>sys.stderr, "Could not find a valid editor"
                return 1

            tmpfileobj = tempfile.NamedTemporaryFile()
            cfg.filename = tmpfileobj.name
            cfg.write()
            while not done:
                status = subprocess.call([editor, cfg.filename])
                if status != 0:
                    if not confirm("Editor exited with non-zero status[%d]. "
                                   "Would you like to retry?" % status):
                        print >>sys.stderr, "Aborting"
                        return 1
                    else:
                        continue
                try:
                    cfg.reload()
                except ParseError, exc:
                    print >>sys.stderr, "%s : %s" % \
                    (exc.msg, exc.line)
                else:
                    errors = cfg.validate(validator,preserve_errors=True)
                    if errors is True:
                        done = True
                        continue
                    else:
                        _report_errors(cfg, errors)

                if not confirm('There were configuration errors. Continue?'):
                    print >>sys.stderr, "Aborting"
                    return 1
            tmpfileobj.close()

        if not opts.name and not opts.file:
            cfg.write(sys.stdout)

        if opts.file:
            print >>sys.stderr, "Saving config to %r" % opts.file
            cfg.write(open(opts.file, 'w'))
        elif opts.name:
            base_dir = os.path.dirname(hollandcfg.filename)
            path = os.path.join(base_dir, 'backupsets', opts.name + '.conf')
            print >>sys.stderr, "Saving config to %r" % path
            cfg.write(open(path, 'w'))
        return 0

########NEW FILE########
__FILENAME__ = purge
import os
import sys
import logging
import itertools
from holland.core.command import Command, option
from holland.core.config import hollandcfg, ConfigError
from holland.core.spool import spool, CONFIGSPEC
from holland.core.util.fmt import format_bytes

LOG = logging.getLogger(__name__)

class Purge(Command):
    """${cmd_usage}

    Purge the requested job runs 

    ${cmd_option_list}
        
    """

    name = 'purge'

    aliases = [
        'pg'
    ]

    options = [
        option('--dry-run', '-n', action='store_true', dest='force',
                default=False,
                help="Print what would be purged without actually purging"),
        option('--all', '-a', action='store_true', default=False,
                help="When purging a backupset purge everything rather than "
                     "using the retention count from the active configuration"),
        option('--force', '-f', action='store_true', default=False,
               help="Execute the purge (disable dry-run). Alias for --execute"),
        option('--execute', action='store_true', dest='force',
               help="Execute the purge (disable dry-run)")
    ]

    description = 'Purge the requested job runs'
    
    def run(self, cmd, opts, *backups):
        error = 0

        if not backups:
            LOG.info("No backupsets specified - using backupsets from %s",
                     hollandcfg.filename)
            backups = hollandcfg.lookup('holland.backupsets')

        if not backups:
            LOG.warn("Nothing to purge")
            return 0

        if not opts.force:
            LOG.warn("Running in dry-run mode.  Use --execute to do a real purge.")

        for name in backups:
            if '/' not in name:
                backupset = spool.find_backupset(name)
                if not backupset:
                    LOG.error("Failed to find backupset '%s'", name)
                    error = 1
                    continue
                purge_backupset(backupset, opts.force, opts.all)
            else:
                backup = spool.find_backup(name)
                if not backup:
                    LOG.error("Failed to find single backup '%s'", name)
                    error = 1
                    continue
                purge_backup(backup, opts.force)
                if opts.force:
                    spool.find_backupset(backup.backupset).update_symlinks()
        return error

def purge_backupset(backupset, force=False, all_backups=False):
    """Purge a whole backupset either entirely or per the configured
    retention count

    :param backupset: Backupset object to purge
    :param force: Force the purge - this is not a dry-run
    :param all_backupsets: purge all backups regardless of configured
                           retention count
    """
    if all_backups:
        retention_count = 0
    else:
        try:
            config = hollandcfg.backupset(backupset.name)
            config.validate_config(CONFIGSPEC, suppress_warnings=True)
        except (IOError, ConfigError), exc:
            LOG.error("Failed to load backupset '%s': %s", backupset.name, exc)
            LOG.error("Aborting, because I could not tell how many backups to "
                      "preserve.")
            LOG.error("You can still purge the backupset by using the --all "
                      "option or specifying specific backups to purge")
            return 1
        retention_count = config['holland:backup']['backups-to-keep']

    LOG.info("Evaluating purge for backupset %s", backupset.name)
    LOG.info("Retaining up to %d backup%s", 
             retention_count, 's'[0:bool(retention_count)])
    backups = []
    bytes = 0
    backup_list = backupset.list_backups(reverse=True)
    for backup in itertools.islice(backup_list, retention_count, None):
        backups.append(backup)
        config = backup.config['holland:backup']
        bytes += int(config['on-disk-size'])

    LOG.info("    %d total backups", len(backup_list))
    for backup in backup_list:
        LOG.info("        * %s", backup.path)
    LOG.info("    %d backups to keep", len(backup_list) - len(backups))
    for backup in backup_list[0:-len(backups)]:
        LOG.info("        + %s", backup.path)
    LOG.info("    %d backups to purge", len(backups))
    for backup in backups:
        LOG.info("        - %s", backup.path)
    LOG.info("    %s total to purge", format_bytes(bytes))

    if force:
        count = 0
        for backup in backupset.purge(retention_count):
            count += 1
            LOG.info("Purged %s", backup.name)   
        if count == 0:
            LOG.info("No backups purged.")
        else:
            LOG.info("Purged %d backup%s", count, 's'[0:bool(count)])
    else:
        LOG.info("Skipping purge in dry-run mode.")
    backupset.update_symlinks()

def purge_backup(backup, force=False):
    """Purge a single backup

    :param backup: Backup object to purge
    :param force: Force the purge - this is not a dry-run
    """
    if not force:
        config = backup.config['holland:backup']
        LOG.info("Would purge single backup '%s' %s",
                 backup.name,
                 format_bytes(int(config['on-disk-size'])))
    else:
        backup.purge()
        LOG.info("Purged %s", backup.name)

########NEW FILE########
__FILENAME__ = restore
import logging
from holland.core.command import Command, option
from holland.core.plugin import load_first_entrypoint
from holland.core.spool import spool

LOGGER = logging.getLogger(__name__)

class Restore(Command):
    """${cmd_usage}

    Restore data from an existing Holland backup

    The actual restore is delegated to the backup plugin that
    created the backup. 

    Example:
    holland ${cmd_name} some-backup --help

    # Example restore for a mysqldump based backup
    holland ${cmd_name} some-backup --table mysql.proc 

    ${cmd_option_list}
        
    """

    name = 'restore'

    aliases = [
        're'
    ]

    options = [
        option('--dry-run', '-n', action='store_true',
                help="Print what restore actually would do without actually running the restore")
    ]

    description = 'Restore data from an existing Holland Backup'
    def __init__(self):
        Command.__init__(self)
        self.optparser.disable_interspersed_args()

    def run(self, cmd, opts, backup_name, *restore_options):
        backup = spool.find_backup(backup_name)
        if not backup:
            logging.error("No backup found named %s", backup_name)
            return 1
        config = backup.config
        plugin_name = config.get('holland:backup', {}).get('plugin')
        plugin = load_first_entrypoint('holland.restore', plugin_name)(backup)
        plugin.dispatch([plugin_name]  + list(restore_options))
        return 1

########NEW FILE########
__FILENAME__ = config
# Copyright 2001-2005 by Vinay Sajip. All Rights Reserved.
#
# Permission to use, copy, modify, and distribute this software and its
# documentation for any purpose and without fee is hereby granted,
# provided that the above copyright notice appear in all copies and that
# both that copyright notice and this permission notice appear in
# supporting documentation, and that the name of Vinay Sajip
# not be used in advertising or publicity pertaining to distribution
# of the software without specific, written prior permission.
# VINAY SAJIP DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
# ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL
# VINAY SAJIP BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR
# ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER
# IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT
# OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.

"""
Configuration functions for the logging package for Python. The core package
is based on PEP 282 and comments thereto in comp.lang.python, and influenced
by Apache's log4j system.

Should work under Python versions >= 1.5.2, except that source line
information is not available unless 'sys._getframe()' is.

Copyright (C) 2001-2004 Vinay Sajip. All Rights Reserved.

To use, simply 'import logging' and log away!
"""

import sys, logging, logging.handlers, string, socket, struct, os

try:
    import thread
    import threading
except ImportError:
    thread = None

from SocketServer import ThreadingTCPServer, StreamRequestHandler


DEFAULT_LOGGING_CONFIG_PORT = 9030

if sys.platform == "win32":
    RESET_ERROR = 10054   #WSAECONNRESET
else:
    RESET_ERROR = 104     #ECONNRESET

#
#   The following code implements a socket listener for on-the-fly
#   reconfiguration of logging.
#
#   _listener holds the server object doing the listening
_listener = None

def fileConfig(fname, defaults=None):
    """
    Read the logging configuration from a ConfigParser-format file.

    This can be called several times from an application, allowing an end user
    the ability to select from various pre-canned configurations (if the
    developer provides a mechanism to present the choices and load the chosen
    configuration).
    In versions of ConfigParser which have the readfp method [typically
    shipped in 2.x versions of Python], you can pass in a file-like object
    rather than a filename, in which case the file-like object will be read
    using readfp.
    """
    import ConfigParser

    cp = ConfigParser.ConfigParser(defaults)
    if hasattr(cp, 'readfp') and hasattr(fname, 'readline'):
        cp.readfp(fname)
    else:
        cp.read(fname)
    #first, do the formatters...
    flist = cp.get("formatters", "keys")
    if len(flist):
        flist = string.split(flist, ",")
        formatters = {}
        for form in flist:
            sectname = "formatter_%s" % form
            opts = cp.options(sectname)
            if "format" in opts:
                fs = cp.get(sectname, "format", 1)
            else:
                fs = None
            if "datefmt" in opts:
                dfs = cp.get(sectname, "datefmt", 1)
            else:
                dfs = None
            f = logging.Formatter(fs, dfs)
            formatters[form] = f
    #next, do the handlers...
    #critical section...
    logging._acquireLock()
    try:
        try:
            #first, lose the existing handlers...
            logging._handlers.clear()
            #now set up the new ones...
            hlist = cp.get("handlers", "keys")
            if len(hlist):
                hlist = string.split(hlist, ",")
                handlers = {}
                fixups = [] #for inter-handler references
                for hand in hlist:
                    try:
                        sectname = "handler_%s" % hand
                        klass = cp.get(sectname, "class")
                        opts = cp.options(sectname)
                        if "formatter" in opts:
                            fmt = cp.get(sectname, "formatter")
                        else:
                            fmt = ""
                        klass = eval(klass, vars(logging))
                        args = cp.get(sectname, "args")
                        args = eval(args, vars(logging))
                        h = apply(klass, args)
                        if "level" in opts:
                            level = cp.get(sectname, "level")
                            h.setLevel(logging._levelNames[level])
                        if len(fmt):
                            h.setFormatter(formatters[fmt])
                        #temporary hack for FileHandler and MemoryHandler.
                        if klass == logging.handlers.MemoryHandler:
                            if "target" in opts:
                                target = cp.get(sectname,"target")
                            else:
                                target = ""
                            if len(target): #the target handler may not be loaded yet, so keep for later...
                                fixups.append((h, target))
                        handlers[hand] = h
                    except:     #if an error occurs when instantiating a handler, too bad
                        pass    #this could happen e.g. because of lack of privileges
                #now all handlers are loaded, fixup inter-handler references...
                for fixup in fixups:
                    h = fixup[0]
                    t = fixup[1]
                    h.setTarget(handlers[t])
            #at last, the loggers...first the root...
            llist = cp.get("loggers", "keys")
            llist = string.split(llist, ",")
            llist.remove("root")
            sectname = "logger_root"
            root = logging.root
            log = root
            opts = cp.options(sectname)
            if "level" in opts:
                level = cp.get(sectname, "level")
                log.setLevel(logging._levelNames[level])
            for h in root.handlers[:]:
                root.removeHandler(h)
            hlist = cp.get(sectname, "handlers")
            if len(hlist):
                hlist = string.split(hlist, ",")
                for hand in hlist:
                    log.addHandler(handlers[hand])
            #and now the others...
            #we don't want to lose the existing loggers,
            #since other threads may have pointers to them.
            #existing is set to contain all existing loggers,
            #and as we go through the new configuration we
            #remove any which are configured. At the end,
            #what's left in existing is the set of loggers
            #which were in the previous configuration but
            #which are not in the new configuration.
            existing = root.manager.loggerDict.keys()
            #now set up the new ones...
            for log in llist:
                sectname = "logger_%s" % log
                qn = cp.get(sectname, "qualname")
                opts = cp.options(sectname)
                if "propagate" in opts:
                    propagate = cp.getint(sectname, "propagate")
                else:
                    propagate = 1
                logger = logging.getLogger(qn)
                if qn in existing:
                    existing.remove(qn)
                if "level" in opts:
                    level = cp.get(sectname, "level")
                    logger.setLevel(logging._levelNames[level])
                for h in logger.handlers[:]:
                    logger.removeHandler(h)
                logger.propagate = propagate
                logger.disabled = 0
                hlist = cp.get(sectname, "handlers")
                if len(hlist):
                    hlist = string.split(hlist, ",")
                    for hand in hlist:
                        logger.addHandler(handlers[hand])
            #Disable any old loggers. There's no point deleting
            #them as other threads may continue to hold references
            #and by disabling them, you stop them doing any logging.
            for log in existing:
                root.manager.loggerDict[log].disabled = 1
        except:
            import traceback
            ei = sys.exc_info()
            traceback.print_exception(ei[0], ei[1], ei[2], None, sys.stderr)
            del ei
    finally:
        logging._releaseLock()

def listen(port=DEFAULT_LOGGING_CONFIG_PORT):
    """
    Start up a socket server on the specified port, and listen for new
    configurations.

    These will be sent as a file suitable for processing by fileConfig().
    Returns a Thread object on which you can call start() to start the server,
    and which you can join() when appropriate. To stop the server, call
    stopListening().
    """
    if not thread:
        raise NotImplementedError, "listen() needs threading to work"

    class ConfigStreamHandler(StreamRequestHandler):
        """
        Handler for a logging configuration request.

        It expects a completely new logging configuration and uses fileConfig
        to install it.
        """
        def handle(self):
            """
            Handle a request.

            Each request is expected to be a 4-byte length,
            followed by the config file. Uses fileConfig() to do the
            grunt work.
            """
            import tempfile
            try:
                conn = self.connection
                chunk = conn.recv(4)
                if len(chunk) == 4:
                    slen = struct.unpack(">L", chunk)[0]
                    chunk = self.connection.recv(slen)
                    while len(chunk) < slen:
                        chunk = chunk + conn.recv(slen - len(chunk))
                    #Apply new configuration. We'd like to be able to
                    #create a StringIO and pass that in, but unfortunately
                    #1.5.2 ConfigParser does not support reading file
                    #objects, only actual files. So we create a temporary
                    #file and remove it later.
                    file = tempfile.mktemp(".ini")
                    f = open(file, "w")
                    f.write(chunk)
                    f.close()
                    fileConfig(file)
                    os.remove(file)
            except socket.error, e:
                if type(e.args) != types.TupleType:
                    raise
                else:
                    errcode = e.args[0]
                    if errcode != RESET_ERROR:
                        raise

    class ConfigSocketReceiver(ThreadingTCPServer):
        """
        A simple TCP socket-based logging config receiver.
        """

        allow_reuse_address = 1

        def __init__(self, host='localhost', port=DEFAULT_LOGGING_CONFIG_PORT,
                     handler=None):
            ThreadingTCPServer.__init__(self, (host, port), handler)
            logging._acquireLock()
            self.abort = 0
            logging._releaseLock()
            self.timeout = 1

        def serve_until_stopped(self):
            import select
            abort = 0
            while not abort:
                rd, wr, ex = select.select([self.socket.fileno()],
                                           [], [],
                                           self.timeout)
                if rd:
                    self.handle_request()
                logging._acquireLock()
                abort = self.abort
                logging._releaseLock()

    def serve(rcvr, hdlr, port):
        server = rcvr(port=port, handler=hdlr)
        global _listener
        logging._acquireLock()
        _listener = server
        logging._releaseLock()
        server.serve_until_stopped()

    return threading.Thread(target=serve,
                            args=(ConfigSocketReceiver,
                                  ConfigStreamHandler, port))

def stopListening():
    """
    Stop the listening server which was created with a call to listen().
    """
    global _listener
    if _listener:
        logging._acquireLock()
        _listener.abort = 1
        _listener = None
        logging._releaseLock()

########NEW FILE########
__FILENAME__ = handlers
# Copyright 2001-2005 by Vinay Sajip. All Rights Reserved.
#
# Permission to use, copy, modify, and distribute this software and its
# documentation for any purpose and without fee is hereby granted,
# provided that the above copyright notice appear in all copies and that
# both that copyright notice and this permission notice appear in
# supporting documentation, and that the name of Vinay Sajip
# not be used in advertising or publicity pertaining to distribution
# of the software without specific, written prior permission.
# VINAY SAJIP DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
# ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL
# VINAY SAJIP BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR
# ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER
# IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT
# OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.

"""
Additional handlers for the logging package for Python. The core package is
based on PEP 282 and comments thereto in comp.lang.python, and influenced by
Apache's log4j system.

Should work under Python versions >= 1.5.2, except that source line
information is not available unless 'sys._getframe()' is.

Copyright (C) 2001-2004 Vinay Sajip. All Rights Reserved.

To use, simply 'import logging' and log away!
"""

import sys, socket, types, os, string, cPickle, struct, time, glob

try:
    import logging
except ImportError:
    import holland.backports.logging as logging
    
try:
    import codecs
except ImportError:
    codecs = None

#
# Some constants...
#

DEFAULT_TCP_LOGGING_PORT    = 9020
DEFAULT_UDP_LOGGING_PORT    = 9021
DEFAULT_HTTP_LOGGING_PORT   = 9022
DEFAULT_SOAP_LOGGING_PORT   = 9023
SYSLOG_UDP_PORT             = 514

class BaseRotatingHandler(logging.FileHandler):
    """
    Base class for handlers that rotate log files at a certain point.
    Not meant to be instantiated directly.  Instead, use RotatingFileHandler
    or TimedRotatingFileHandler.
    """
    def __init__(self, filename, mode, encoding=None):
        """
        Use the specified filename for streamed logging
        """
        if codecs is None:
            encoding = None
        logging.FileHandler.__init__(self, filename, mode, encoding)
        self.mode = mode
        self.encoding = encoding

    def emit(self, record):
        """
        Emit a record.

        Output the record to the file, catering for rollover as described
        in doRollover().
        """
        try:
            if self.shouldRollover(record):
                self.doRollover()
            logging.FileHandler.emit(self, record)
        except:
            self.handleError(record)

class RotatingFileHandler(BaseRotatingHandler):
    """
    Handler for logging to a set of files, which switches from one file
    to the next when the current file reaches a certain size.
    """
    def __init__(self, filename, mode='a', maxBytes=0, backupCount=0, encoding=None):
        """
        Open the specified file and use it as the stream for logging.

        By default, the file grows indefinitely. You can specify particular
        values of maxBytes and backupCount to allow the file to rollover at
        a predetermined size.

        Rollover occurs whenever the current log file is nearly maxBytes in
        length. If backupCount is >= 1, the system will successively create
        new files with the same pathname as the base file, but with extensions
        ".1", ".2" etc. appended to it. For example, with a backupCount of 5
        and a base file name of "app.log", you would get "app.log",
        "app.log.1", "app.log.2", ... through to "app.log.5". The file being
        written to is always "app.log" - when it gets filled up, it is closed
        and renamed to "app.log.1", and if files "app.log.1", "app.log.2" etc.
        exist, then they are renamed to "app.log.2", "app.log.3" etc.
        respectively.

        If maxBytes is zero, rollover never occurs.
        """
        if maxBytes > 0:
            mode = 'a' # doesn't make sense otherwise!
        BaseRotatingHandler.__init__(self, filename, mode, encoding)
        self.maxBytes = maxBytes
        self.backupCount = backupCount

    def doRollover(self):
        """
        Do a rollover, as described in __init__().
        """

        self.stream.close()
        if self.backupCount > 0:
            for i in range(self.backupCount - 1, 0, -1):
                sfn = "%s.%d" % (self.baseFilename, i)
                dfn = "%s.%d" % (self.baseFilename, i + 1)
                if os.path.exists(sfn):
                    #print "%s -> %s" % (sfn, dfn)
                    if os.path.exists(dfn):
                        os.remove(dfn)
                    os.rename(sfn, dfn)
            dfn = self.baseFilename + ".1"
            if os.path.exists(dfn):
                os.remove(dfn)
            os.rename(self.baseFilename, dfn)
            #print "%s -> %s" % (self.baseFilename, dfn)
        if self.encoding:
            self.stream = codecs.open(self.baseFilename, 'w', self.encoding)
        else:
            self.stream = open(self.baseFilename, 'w')

    def shouldRollover(self, record):
        """
        Determine if rollover should occur.

        Basically, see if the supplied record would cause the file to exceed
        the size limit we have.
        """
        if self.maxBytes > 0:                   # are we rolling over?
            msg = "%s\n" % self.format(record)
            self.stream.seek(0, 2)  #due to non-posix-compliant Windows feature
            if self.stream.tell() + len(msg) >= self.maxBytes:
                return 1
        return 0

class TimedRotatingFileHandler(BaseRotatingHandler):
    """
    Handler for logging to a file, rotating the log file at certain timed
    intervals.

    If backupCount is > 0, when rollover is done, no more than backupCount
    files are kept - the oldest ones are deleted.
    """
    def __init__(self, filename, when='h', interval=1, backupCount=0, encoding=None):
        BaseRotatingHandler.__init__(self, filename, 'a', encoding)
        self.when = string.upper(when)
        self.backupCount = backupCount
        # Calculate the real rollover interval, which is just the number of
        # seconds between rollovers.  Also set the filename suffix used when
        # a rollover occurs.  Current 'when' events supported:
        # S - Seconds
        # M - Minutes
        # H - Hours
        # D - Days
        # midnight - roll over at midnight
        # W{0-6} - roll over on a certain day; 0 - Monday
        #
        # Case of the 'when' specifier is not important; lower or upper case
        # will work.
        currentTime = int(time.time())
        if self.when == 'S':
            self.interval = 1 # one second
            self.suffix = "%Y-%m-%d_%H-%M-%S"
        elif self.when == 'M':
            self.interval = 60 # one minute
            self.suffix = "%Y-%m-%d_%H-%M"
        elif self.when == 'H':
            self.interval = 60 * 60 # one hour
            self.suffix = "%Y-%m-%d_%H"
        elif self.when == 'D' or self.when == 'MIDNIGHT':
            self.interval = 60 * 60 * 24 # one day
            self.suffix = "%Y-%m-%d"
        elif self.when.startswith('W'):
            self.interval = 60 * 60 * 24 * 7 # one week
            if len(self.when) != 2:
                raise ValueError("You must specify a day for weekly rollover from 0 to 6 (0 is Monday): %s" % self.when)
            if self.when[1] < '0' or self.when[1] > '6':
                raise ValueError("Invalid day specified for weekly rollover: %s" % self.when)
            self.dayOfWeek = int(self.when[1])
            self.suffix = "%Y-%m-%d"
        else:
            raise ValueError("Invalid rollover interval specified: %s" % self.when)

        self.interval = self.interval * interval # multiply by units requested
        self.rolloverAt = currentTime + self.interval

        # If we are rolling over at midnight or weekly, then the interval is already known.
        # What we need to figure out is WHEN the next interval is.  In other words,
        # if you are rolling over at midnight, then your base interval is 1 day,
        # but you want to start that one day clock at midnight, not now.  So, we
        # have to fudge the rolloverAt value in order to trigger the first rollover
        # at the right time.  After that, the regular interval will take care of
        # the rest.  Note that this code doesn't care about leap seconds. :)
        if self.when == 'MIDNIGHT' or self.when.startswith('W'):
            # This could be done with less code, but I wanted it to be clear
            t = time.localtime(currentTime)
            currentHour = t[3]
            currentMinute = t[4]
            currentSecond = t[5]
            # r is the number of seconds left between now and midnight
            r = (24 - currentHour) * 60 * 60 # number of hours in seconds
            r = r + (59 - currentMinute) * 60 # plus the number of minutes (in secs)
            r = r + (59 - currentSecond) # plus the number of seconds
            self.rolloverAt = currentTime + r
            # If we are rolling over on a certain day, add in the number of days until
            # the next rollover, but offset by 1 since we just calculated the time
            # until the next day starts.  There are three cases:
            # Case 1) The day to rollover is today; in this case, do nothing
            # Case 2) The day to rollover is further in the interval (i.e., today is
            #         day 2 (Wednesday) and rollover is on day 6 (Sunday).  Days to
            #         next rollover is simply 6 - 2 - 1, or 3.
            # Case 3) The day to rollover is behind us in the interval (i.e., today
            #         is day 5 (Saturday) and rollover is on day 3 (Thursday).
            #         Days to rollover is 6 - 5 + 3, or 4.  In this case, it's the
            #         number of days left in the current week (1) plus the number
            #         of days in the next week until the rollover day (3).
            if when.startswith('W'):
                day = t[6] # 0 is Monday
                if day > self.dayOfWeek:
                    daysToWait = (day - self.dayOfWeek) - 1
                    self.rolloverAt = self.rolloverAt + (daysToWait * (60 * 60 * 24))
                if day < self.dayOfWeek:
                    daysToWait = (6 - self.dayOfWeek) + day
                    self.rolloverAt = self.rolloverAt + (daysToWait * (60 * 60 * 24))

        #print "Will rollover at %d, %d seconds from now" % (self.rolloverAt, self.rolloverAt - currentTime)

    def shouldRollover(self, record):
        """
        Determine if rollover should occur

        record is not used, as we are just comparing times, but it is needed so
        the method siguratures are the same
        """
        t = int(time.time())
        if t >= self.rolloverAt:
            return 1
        #print "No need to rollover: %d, %d" % (t, self.rolloverAt)
        return 0

    def doRollover(self):
        """
        do a rollover; in this case, a date/time stamp is appended to the filename
        when the rollover happens.  However, you want the file to be named for the
        start of the interval, not the current time.  If there is a backup count,
        then we have to get a list of matching filenames, sort them and remove
        the one with the oldest suffix.
        """
        self.stream.close()
        # get the time that this sequence started at and make it a TimeTuple
        t = self.rolloverAt - self.interval
        timeTuple = time.localtime(t)
        dfn = self.baseFilename + "." + time.strftime(self.suffix, timeTuple)
        if os.path.exists(dfn):
            os.remove(dfn)
        os.rename(self.baseFilename, dfn)
        if self.backupCount > 0:
            # find the oldest log file and delete it
            s = glob.glob(self.baseFilename + ".20*")
            if len(s) > self.backupCount:
                s.sort()
                os.remove(s[0])
        #print "%s -> %s" % (self.baseFilename, dfn)
        if self.encoding:
            self.stream = codecs.open(self.baseFilename, 'w', self.encoding)
        else:
            self.stream = open(self.baseFilename, 'w')
        self.rolloverAt = int(time.time()) + self.interval

class SocketHandler(logging.Handler):
    """
    A handler class which writes logging records, in pickle format, to
    a streaming socket. The socket is kept open across logging calls.
    If the peer resets it, an attempt is made to reconnect on the next call.
    The pickle which is sent is that of the LogRecord's attribute dictionary
    (__dict__), so that the receiver does not need to have the logging module
    installed in order to process the logging event.

    To unpickle the record at the receiving end into a LogRecord, use the
    makeLogRecord function.
    """

    def __init__(self, host, port):
        """
        Initializes the handler with a specific host address and port.

        The attribute 'closeOnError' is set to 1 - which means that if
        a socket error occurs, the socket is silently closed and then
        reopened on the next logging call.
        """
        logging.Handler.__init__(self)
        self.host = host
        self.port = port
        self.sock = None
        self.closeOnError = 0
        self.retryTime = None
        #
        # Exponential backoff parameters.
        #
        self.retryStart = 1.0
        self.retryMax = 30.0
        self.retryFactor = 2.0

    def makeSocket(self):
        """
        A factory method which allows subclasses to define the precise
        type of socket they want.
        """
        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        s.connect((self.host, self.port))
        return s

    def createSocket(self):
        """
        Try to create a socket, using an exponential backoff with
        a max retry time. Thanks to Robert Olson for the original patch
        (SF #815911) which has been slightly refactored.
        """
        now = time.time()
        # Either retryTime is None, in which case this
        # is the first time back after a disconnect, or
        # we've waited long enough.
        if self.retryTime is None:
            attempt = 1
        else:
            attempt = (now >= self.retryTime)
        if attempt:
            try:
                self.sock = self.makeSocket()
                self.retryTime = None # next time, no delay before trying
            except:
                #Creation failed, so set the retry time and return.
                if self.retryTime is None:
                    self.retryPeriod = self.retryStart
                else:
                    self.retryPeriod = self.retryPeriod * self.retryFactor
                    if self.retryPeriod > self.retryMax:
                        self.retryPeriod = self.retryMax
                self.retryTime = now + self.retryPeriod

    def send(self, s):
        """
        Send a pickled string to the socket.

        This function allows for partial sends which can happen when the
        network is busy.
        """
        if self.sock is None:
            self.createSocket()
        #self.sock can be None either because we haven't reached the retry
        #time yet, or because we have reached the retry time and retried,
        #but are still unable to connect.
        if self.sock:
            try:
                if hasattr(self.sock, "sendall"):
                    self.sock.sendall(s)
                else:
                    sentsofar = 0
                    left = len(s)
                    while left > 0:
                        sent = self.sock.send(s[sentsofar:])
                        sentsofar = sentsofar + sent
                        left = left - sent
            except socket.error:
                self.sock.close()
                self.sock = None  # so we can call createSocket next time

    def makePickle(self, record):
        """
        Pickles the record in binary format with a length prefix, and
        returns it ready for transmission across the socket.
        """
        ei = record.exc_info
        if ei:
            dummy = self.format(record) # just to get traceback text into record.exc_text
            record.exc_info = None  # to avoid Unpickleable error
        s = cPickle.dumps(record.__dict__, 1)
        if ei:
            record.exc_info = ei  # for next handler
        slen = struct.pack(">L", len(s))
        return slen + s

    def handleError(self, record):
        """
        Handle an error during logging.

        An error has occurred during logging. Most likely cause -
        connection lost. Close the socket so that we can retry on the
        next event.
        """
        if self.closeOnError and self.sock:
            self.sock.close()
            self.sock = None        #try to reconnect next time
        else:
            logging.Handler.handleError(self, record)

    def emit(self, record):
        """
        Emit a record.

        Pickles the record and writes it to the socket in binary format.
        If there is an error with the socket, silently drop the packet.
        If there was a problem with the socket, re-establishes the
        socket.
        """
        try:
            s = self.makePickle(record)
            self.send(s)
        except:
            self.handleError(record)

    def close(self):
        """
        Closes the socket.
        """
        if self.sock:
            self.sock.close()
            self.sock = None
        logging.Handler.close(self)

class DatagramHandler(SocketHandler):
    """
    A handler class which writes logging records, in pickle format, to
    a datagram socket.  The pickle which is sent is that of the LogRecord's
    attribute dictionary (__dict__), so that the receiver does not need to
    have the logging module installed in order to process the logging event.

    To unpickle the record at the receiving end into a LogRecord, use the
    makeLogRecord function.

    """
    def __init__(self, host, port):
        """
        Initializes the handler with a specific host address and port.
        """
        SocketHandler.__init__(self, host, port)
        self.closeOnError = 0

    def makeSocket(self):
        """
        The factory method of SocketHandler is here overridden to create
        a UDP socket (SOCK_DGRAM).
        """
        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        return s

    def send(self, s):
        """
        Send a pickled string to a socket.

        This function no longer allows for partial sends which can happen
        when the network is busy - UDP does not guarantee delivery and
        can deliver packets out of sequence.
        """
        if self.sock is None:
            self.createSocket()
        self.sock.sendto(s, (self.host, self.port))

class SysLogHandler(logging.Handler):
    """
    A handler class which sends formatted logging records to a syslog
    server. Based on Sam Rushing's syslog module:
    http://www.nightmare.com/squirl/python-ext/misc/syslog.py
    Contributed by Nicolas Untz (after which minor refactoring changes
    have been made).
    """

    # from <linux/sys/syslog.h>:
    # ======================================================================
    # priorities/facilities are encoded into a single 32-bit quantity, where
    # the bottom 3 bits are the priority (0-7) and the top 28 bits are the
    # facility (0-big number). Both the priorities and the facilities map
    # roughly one-to-one to strings in the syslogd(8) source code.  This
    # mapping is included in this file.
    #
    # priorities (these are ordered)

    LOG_EMERG     = 0       #  system is unusable
    LOG_ALERT     = 1       #  action must be taken immediately
    LOG_CRIT      = 2       #  critical conditions
    LOG_ERR       = 3       #  error conditions
    LOG_WARNING   = 4       #  warning conditions
    LOG_NOTICE    = 5       #  normal but significant condition
    LOG_INFO      = 6       #  informational
    LOG_DEBUG     = 7       #  debug-level messages

    #  facility codes
    LOG_KERN      = 0       #  kernel messages
    LOG_USER      = 1       #  random user-level messages
    LOG_MAIL      = 2       #  mail system
    LOG_DAEMON    = 3       #  system daemons
    LOG_AUTH      = 4       #  security/authorization messages
    LOG_SYSLOG    = 5       #  messages generated internally by syslogd
    LOG_LPR       = 6       #  line printer subsystem
    LOG_NEWS      = 7       #  network news subsystem
    LOG_UUCP      = 8       #  UUCP subsystem
    LOG_CRON      = 9       #  clock daemon
    LOG_AUTHPRIV  = 10  #  security/authorization messages (private)

    #  other codes through 15 reserved for system use
    LOG_LOCAL0    = 16      #  reserved for local use
    LOG_LOCAL1    = 17      #  reserved for local use
    LOG_LOCAL2    = 18      #  reserved for local use
    LOG_LOCAL3    = 19      #  reserved for local use
    LOG_LOCAL4    = 20      #  reserved for local use
    LOG_LOCAL5    = 21      #  reserved for local use
    LOG_LOCAL6    = 22      #  reserved for local use
    LOG_LOCAL7    = 23      #  reserved for local use

    priority_names = {
        "alert":    LOG_ALERT,
        "crit":     LOG_CRIT,
        "critical": LOG_CRIT,
        "debug":    LOG_DEBUG,
        "emerg":    LOG_EMERG,
        "err":      LOG_ERR,
        "error":    LOG_ERR,        #  DEPRECATED
        "info":     LOG_INFO,
        "notice":   LOG_NOTICE,
        "panic":    LOG_EMERG,      #  DEPRECATED
        "warn":     LOG_WARNING,    #  DEPRECATED
        "warning":  LOG_WARNING,
        }

    facility_names = {
        "auth":     LOG_AUTH,
        "authpriv": LOG_AUTHPRIV,
        "cron":     LOG_CRON,
        "daemon":   LOG_DAEMON,
        "kern":     LOG_KERN,
        "lpr":      LOG_LPR,
        "mail":     LOG_MAIL,
        "news":     LOG_NEWS,
        "security": LOG_AUTH,       #  DEPRECATED
        "syslog":   LOG_SYSLOG,
        "user":     LOG_USER,
        "uucp":     LOG_UUCP,
        "local0":   LOG_LOCAL0,
        "local1":   LOG_LOCAL1,
        "local2":   LOG_LOCAL2,
        "local3":   LOG_LOCAL3,
        "local4":   LOG_LOCAL4,
        "local5":   LOG_LOCAL5,
        "local6":   LOG_LOCAL6,
        "local7":   LOG_LOCAL7,
        }

    def __init__(self, address=('localhost', SYSLOG_UDP_PORT), facility=LOG_USER):
        """
        Initialize a handler.

        If address is specified as a string, UNIX socket is used.
        If facility is not specified, LOG_USER is used.
        """
        logging.Handler.__init__(self)

        self.address = address
        self.facility = facility
        if type(address) == types.StringType:
            self._connect_unixsocket(address)
            self.unixsocket = 1
        else:
            self.socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            self.unixsocket = 0

        self.formatter = None

    def _connect_unixsocket(self, address):
        self.socket = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM)
        # syslog may require either DGRAM or STREAM sockets
        try:
            self.socket.connect(address)
        except socket.error:
            self.socket.close()
            self.socket = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
        self.socket.connect(address)

    # curious: when talking to the unix-domain '/dev/log' socket, a
    #   zero-terminator seems to be required.  this string is placed
    #   into a class variable so that it can be overridden if
    #   necessary.
    log_format_string = '<%d>%s\000'

    def encodePriority (self, facility, priority):
        """
        Encode the facility and priority. You can pass in strings or
        integers - if strings are passed, the facility_names and
        priority_names mapping dictionaries are used to convert them to
        integers.
        """
        if type(facility) == types.StringType:
            facility = self.facility_names[facility]
        if type(priority) == types.StringType:
            priority = self.priority_names[priority]
        return (facility << 3) | priority

    def close (self):
        """
        Closes the socket.
        """
        if self.unixsocket:
            self.socket.close()
        logging.Handler.close(self)

    def emit(self, record):
        """
        Emit a record.

        The record is formatted, and then sent to the syslog server. If
        exception information is present, it is NOT sent to the server.
        """
        msg = self.format(record)
        """
        We need to convert record level to lowercase, maybe this will
        change in the future.
        """
        msg = self.log_format_string % (
            self.encodePriority(self.facility,
                                string.lower(record.levelname)),
            msg)
        try:
            if self.unixsocket:
                try:
                    self.socket.send(msg)
                except socket.error:
                    self._connect_unixsocket(self.address)
                    self.socket.send(msg)
            else:
                self.socket.sendto(msg, self.address)
        except:
            self.handleError(record)

class SMTPHandler(logging.Handler):
    """
    A handler class which sends an SMTP email for each logging event.
    """
    def __init__(self, mailhost, fromaddr, toaddrs, subject):
        """
        Initialize the handler.

        Initialize the instance with the from and to addresses and subject
        line of the email. To specify a non-standard SMTP port, use the
        (host, port) tuple format for the mailhost argument.
        """
        logging.Handler.__init__(self)
        if type(mailhost) == types.TupleType:
            host, port = mailhost
            self.mailhost = host
            self.mailport = port
        else:
            self.mailhost = mailhost
            self.mailport = None
        self.fromaddr = fromaddr
        if type(toaddrs) == types.StringType:
            toaddrs = [toaddrs]
        self.toaddrs = toaddrs
        self.subject = subject

    def getSubject(self, record):
        """
        Determine the subject for the email.

        If you want to specify a subject line which is record-dependent,
        override this method.
        """
        return self.subject

    weekdayname = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']

    monthname = [None,
                 'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',
                 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']

    def date_time(self):
        """
        Return the current date and time formatted for a MIME header.
        Needed for Python 1.5.2 (no email package available)
        """
        year, month, day, hh, mm, ss, wd, y, z = time.gmtime(time.time())
        s = "%s, %02d %3s %4d %02d:%02d:%02d GMT" % (
                self.weekdayname[wd],
                day, self.monthname[month], year,
                hh, mm, ss)
        return s

    def emit(self, record):
        """
        Emit a record.

        Format the record and send it to the specified addressees.
        """
        try:
            import smtplib
            try:
                from email.Utils import formatdate
            except:
                formatdate = self.date_time
            port = self.mailport
            if not port:
                port = smtplib.SMTP_PORT
            smtp = smtplib.SMTP(self.mailhost, port)
            msg = self.format(record)
            msg = "From: %s\r\nTo: %s\r\nSubject: %s\r\nDate: %s\r\n\r\n%s" % (
                            self.fromaddr,
                            string.join(self.toaddrs, ","),
                            self.getSubject(record),
                            formatdate(), msg)
            smtp.sendmail(self.fromaddr, self.toaddrs, msg)
            smtp.quit()
        except:
            self.handleError(record)

class NTEventLogHandler(logging.Handler):
    """
    A handler class which sends events to the NT Event Log. Adds a
    registry entry for the specified application name. If no dllname is
    provided, win32service.pyd (which contains some basic message
    placeholders) is used. Note that use of these placeholders will make
    your event logs big, as the entire message source is held in the log.
    If you want slimmer logs, you have to pass in the name of your own DLL
    which contains the message definitions you want to use in the event log.
    """
    def __init__(self, appname, dllname=None, logtype="Application"):
        logging.Handler.__init__(self)
        try:
            import win32evtlogutil, win32evtlog
            self.appname = appname
            self._welu = win32evtlogutil
            if not dllname:
                dllname = os.path.split(self._welu.__file__)
                dllname = os.path.split(dllname[0])
                dllname = os.path.join(dllname[0], r'win32service.pyd')
            self.dllname = dllname
            self.logtype = logtype
            self._welu.AddSourceToRegistry(appname, dllname, logtype)
            self.deftype = win32evtlog.EVENTLOG_ERROR_TYPE
            self.typemap = {
                logging.DEBUG   : win32evtlog.EVENTLOG_INFORMATION_TYPE,
                logging.INFO    : win32evtlog.EVENTLOG_INFORMATION_TYPE,
                logging.WARNING : win32evtlog.EVENTLOG_WARNING_TYPE,
                logging.ERROR   : win32evtlog.EVENTLOG_ERROR_TYPE,
                logging.CRITICAL: win32evtlog.EVENTLOG_ERROR_TYPE,
         }
        except ImportError:
            print "The Python Win32 extensions for NT (service, event "\
                        "logging) appear not to be available."
            self._welu = None

    def getMessageID(self, record):
        """
        Return the message ID for the event record. If you are using your
        own messages, you could do this by having the msg passed to the
        logger being an ID rather than a formatting string. Then, in here,
        you could use a dictionary lookup to get the message ID. This
        version returns 1, which is the base message ID in win32service.pyd.
        """
        return 1

    def getEventCategory(self, record):
        """
        Return the event category for the record.

        Override this if you want to specify your own categories. This version
        returns 0.
        """
        return 0

    def getEventType(self, record):
        """
        Return the event type for the record.

        Override this if you want to specify your own types. This version does
        a mapping using the handler's typemap attribute, which is set up in
        __init__() to a dictionary which contains mappings for DEBUG, INFO,
        WARNING, ERROR and CRITICAL. If you are using your own levels you will
        either need to override this method or place a suitable dictionary in
        the handler's typemap attribute.
        """
        return self.typemap.get(record.levelno, self.deftype)

    def emit(self, record):
        """
        Emit a record.

        Determine the message ID, event category and event type. Then
        log the message in the NT event log.
        """
        if self._welu:
            try:
                id = self.getMessageID(record)
                cat = self.getEventCategory(record)
                type = self.getEventType(record)
                msg = self.format(record)
                self._welu.ReportEvent(self.appname, id, cat, type, [msg])
            except:
                self.handleError(record)

    def close(self):
        """
        Clean up this handler.

        You can remove the application name from the registry as a
        source of event log entries. However, if you do this, you will
        not be able to see the events as you intended in the Event Log
        Viewer - it needs to be able to access the registry to get the
        DLL name.
        """
        #self._welu.RemoveSourceFromRegistry(self.appname, self.logtype)
        logging.Handler.close(self)

class HTTPHandler(logging.Handler):
    """
    A class which sends records to a Web server, using either GET or
    POST semantics.
    """
    def __init__(self, host, url, method="GET"):
        """
        Initialize the instance with the host, the request URL, and the method
        ("GET" or "POST")
        """
        logging.Handler.__init__(self)
        method = string.upper(method)
        if method not in ["GET", "POST"]:
            raise ValueError, "method must be GET or POST"
        self.host = host
        self.url = url
        self.method = method

    def mapLogRecord(self, record):
        """
        Default implementation of mapping the log record into a dict
        that is sent as the CGI data. Overwrite in your class.
        Contributed by Franz  Glasner.
        """
        return record.__dict__

    def emit(self, record):
        """
        Emit a record.

        Send the record to the Web server as an URL-encoded dictionary
        """
        try:
            import httplib, urllib
            h = httplib.HTTP(self.host)
            url = self.url
            data = urllib.urlencode(self.mapLogRecord(record))
            if self.method == "GET":
                if (string.find(url, '?') >= 0):
                    sep = '&'
                else:
                    sep = '?'
                url = url + "%c%s" % (sep, data)
            h.putrequest(self.method, url)
            if self.method == "POST":
                h.putheader("Content-length", str(len(data)))
            h.endheaders()
            if self.method == "POST":
                h.send(data)
            h.getreply()    #can't do anything with the result
        except:
            self.handleError(record)

class BufferingHandler(logging.Handler):
    """
  A handler class which buffers logging records in memory. Whenever each
  record is added to the buffer, a check is made to see if the buffer should
  be flushed. If it should, then flush() is expected to do what's needed.
    """
    def __init__(self, capacity):
        """
        Initialize the handler with the buffer size.
        """
        logging.Handler.__init__(self)
        self.capacity = capacity
        self.buffer = []

    def shouldFlush(self, record):
        """
        Should the handler flush its buffer?

        Returns true if the buffer is up to capacity. This method can be
        overridden to implement custom flushing strategies.
        """
        return (len(self.buffer) >= self.capacity)

    def emit(self, record):
        """
        Emit a record.

        Append the record. If shouldFlush() tells us to, call flush() to process
        the buffer.
        """
        self.buffer.append(record)
        if self.shouldFlush(record):
            self.flush()

    def flush(self):
        """
        Override to implement custom flushing behaviour.

        This version just zaps the buffer to empty.
        """
        self.buffer = []

    def close(self):
        """
        Close the handler.

        This version just flushes and chains to the parent class' close().
        """
        self.flush()
        logging.Handler.close(self)

class MemoryHandler(BufferingHandler):
    """
    A handler class which buffers logging records in memory, periodically
    flushing them to a target handler. Flushing occurs whenever the buffer
    is full, or when an event of a certain severity or greater is seen.
    """
    def __init__(self, capacity, flushLevel=logging.ERROR, target=None):
        """
        Initialize the handler with the buffer size, the level at which
        flushing should occur and an optional target.

        Note that without a target being set either here or via setTarget(),
        a MemoryHandler is no use to anyone!
        """
        BufferingHandler.__init__(self, capacity)
        self.flushLevel = flushLevel
        self.target = target

    def shouldFlush(self, record):
        """
        Check for buffer full or a record at the flushLevel or higher.
        """
        return (len(self.buffer) >= self.capacity) or \
                (record.levelno >= self.flushLevel)

    def setTarget(self, target):
        """
        Set the target handler for this handler.
        """
        self.target = target

    def flush(self):
        """
        For a MemoryHandler, flushing means just sending the buffered
        records to the target, if there is one. Override if you want
        different behaviour.
        """
        if self.target:
            for record in self.buffer:
                self.target.handle(record)
            self.buffer = []

    def close(self):
        """
        Flush, set the target to None and lose the buffer.
        """
        self.flush()
        self.target = None
        BufferingHandler.close(self)

########NEW FILE########
__FILENAME__ = optparse
"""optparse - a powerful, extensible, and easy-to-use option parser.

By Greg Ward <gward@python.net>

Originally distributed as Optik; see http://optik.sourceforge.net/ .

If you have problems with this module, please do not file bugs,
patches, or feature requests with Python; instead, use Optik's
SourceForge project page:
  http://sourceforge.net/projects/optik

For support, use the optik-users@lists.sourceforge.net mailing list
(http://lists.sourceforge.net/lists/listinfo/optik-users).
"""

# Python developers: please do not make changes to this file, since
# it is automatically generated from the Optik source code.

__version__ = "1.5.3"

__all__ = ['Option',
           'SUPPRESS_HELP',
           'SUPPRESS_USAGE',
           'Values',
           'OptionContainer',
           'OptionGroup',
           'OptionParser',
           'HelpFormatter',
           'IndentedHelpFormatter',
           'TitledHelpFormatter',
           'OptParseError',
           'OptionError',
           'OptionConflictError',
           'OptionValueError',
           'BadOptionError']

__copyright__ = """
Copyright (c) 2001-2006 Gregory P. Ward.  All rights reserved.
Copyright (c) 2002-2006 Python Software Foundation.  All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:

  * Redistributions of source code must retain the above copyright
    notice, this list of conditions and the following disclaimer.

  * Redistributions in binary form must reproduce the above copyright
    notice, this list of conditions and the following disclaimer in the
    documentation and/or other materials provided with the distribution.

  * Neither the name of the author nor the names of its
    contributors may be used to endorse or promote products derived from
    this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS
IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A
PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR
CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
"""

import sys, os
import types
import textwrap

def _repr(self):
    return "<%s at 0x%x: %s>" % (self.__class__.__name__, id(self), self)


# This file was generated from:
#   Id: option_parser.py 527 2006-07-23 15:21:30Z greg
#   Id: option.py 522 2006-06-11 16:22:03Z gward
#   Id: help.py 527 2006-07-23 15:21:30Z greg
#   Id: errors.py 509 2006-04-20 00:58:24Z gward

try:
    from gettext import gettext
except ImportError:
    def gettext(message):
        return message
_ = gettext


class OptParseError (Exception):
    def __init__(self, msg):
        self.msg = msg

    def __str__(self):
        return self.msg


class OptionError (OptParseError):
    """
    Raised if an Option instance is created with invalid or
    inconsistent arguments.
    """

    def __init__(self, msg, option):
        self.msg = msg
        self.option_id = str(option)

    def __str__(self):
        if self.option_id:
            return "option %s: %s" % (self.option_id, self.msg)
        else:
            return self.msg

class OptionConflictError (OptionError):
    """
    Raised if conflicting options are added to an OptionParser.
    """

class OptionValueError (OptParseError):
    """
    Raised if an invalid option value is encountered on the command
    line.
    """

class BadOptionError (OptParseError):
    """
    Raised if an invalid option is seen on the command line.
    """
    def __init__(self, opt_str):
        self.opt_str = opt_str

    def __str__(self):
        return _("no such option: %s") % self.opt_str

class AmbiguousOptionError (BadOptionError):
    """
    Raised if an ambiguous option is seen on the command line.
    """
    def __init__(self, opt_str, possibilities):
        BadOptionError.__init__(self, opt_str)
        self.possibilities = possibilities

    def __str__(self):
        return (_("ambiguous option: %s (%s?)")
                % (self.opt_str, ", ".join(self.possibilities)))


class HelpFormatter:

    """
    Abstract base class for formatting option help.  OptionParser
    instances should use one of the HelpFormatter subclasses for
    formatting help; by default IndentedHelpFormatter is used.

    Instance attributes:
      parser : OptionParser
        the controlling OptionParser instance
      indent_increment : int
        the number of columns to indent per nesting level
      max_help_position : int
        the maximum starting column for option help text
      help_position : int
        the calculated starting column for option help text;
        initially the same as the maximum
      width : int
        total number of columns for output (pass None to constructor for
        this value to be taken from the $COLUMNS environment variable)
      level : int
        current indentation level
      current_indent : int
        current indentation level (in columns)
      help_width : int
        number of columns available for option help text (calculated)
      default_tag : str
        text to replace with each option's default value, "%default"
        by default.  Set to false value to disable default value expansion.
      option_strings : { Option : str }
        maps Option instances to the snippet of help text explaining
        the syntax of that option, e.g. "-h, --help" or
        "-fFILE, --file=FILE"
      _short_opt_fmt : str
        format string controlling how short options with values are
        printed in help text.  Must be either "%s%s" ("-fFILE") or
        "%s %s" ("-f FILE"), because those are the two syntaxes that
        Optik supports.
      _long_opt_fmt : str
        similar but for long options; must be either "%s %s" ("--file FILE")
        or "%s=%s" ("--file=FILE").
    """

    NO_DEFAULT_VALUE = "none"

    def __init__(self,
                 indent_increment,
                 max_help_position,
                 width,
                 short_first):
        self.parser = None
        self.indent_increment = indent_increment
        self.help_position = self.max_help_position = max_help_position
        if width is None:
            try:
                width = int(os.environ['COLUMNS'])
            except (KeyError, ValueError):
                width = 80
            width -= 2
        self.width = width
        self.current_indent = 0
        self.level = 0
        self.help_width = None          # computed later
        self.short_first = short_first
        self.default_tag = "%default"
        self.option_strings = {}
        self._short_opt_fmt = "%s %s"
        self._long_opt_fmt = "%s=%s"

    def set_parser(self, parser):
        self.parser = parser

    def set_short_opt_delimiter(self, delim):
        if delim not in ("", " "):
            raise ValueError(
                "invalid metavar delimiter for short options: %r" % delim)
        self._short_opt_fmt = "%s" + delim + "%s"

    def set_long_opt_delimiter(self, delim):
        if delim not in ("=", " "):
            raise ValueError(
                "invalid metavar delimiter for long options: %r" % delim)
        self._long_opt_fmt = "%s" + delim + "%s"

    def indent(self):
        self.current_indent += self.indent_increment
        self.level += 1

    def dedent(self):
        self.current_indent -= self.indent_increment
        assert self.current_indent >= 0, "Indent decreased below 0."
        self.level -= 1

    def format_usage(self, usage):
        raise NotImplementedError, "subclasses must implement"

    def format_heading(self, heading):
        raise NotImplementedError, "subclasses must implement"

    def _format_text(self, text):
        """
        Format a paragraph of free-form text for inclusion in the
        help output at the current indentation level.
        """
        text_width = self.width - self.current_indent
        indent = " "*self.current_indent
        return textwrap.fill(text,
                             text_width,
                             initial_indent=indent,
                             subsequent_indent=indent)

    def format_description(self, description):
        if description:
            return self._format_text(description) + "\n"
        else:
            return ""

    def format_epilog(self, epilog):
        if epilog:
            return "\n" + self._format_text(epilog) + "\n"
        else:
            return ""


    def expand_default(self, option):
        if self.parser is None or not self.default_tag:
            return option.help

        default_value = self.parser.defaults.get(option.dest)
        if default_value is NO_DEFAULT or default_value is None:
            default_value = self.NO_DEFAULT_VALUE

        return option.help.replace(self.default_tag, str(default_value))

    def format_option(self, option):
        # The help for each option consists of two parts:
        #   * the opt strings and metavars
        #     eg. ("-x", or "-fFILENAME, --file=FILENAME")
        #   * the user-supplied help string
        #     eg. ("turn on expert mode", "read data from FILENAME")
        #
        # If possible, we write both of these on the same line:
        #   -x      turn on expert mode
        #
        # But if the opt string list is too long, we put the help
        # string on a second line, indented to the same column it would
        # start in if it fit on the first line.
        #   -fFILENAME, --file=FILENAME
        #           read data from FILENAME
        result = []
        opts = self.option_strings[option]
        opt_width = self.help_position - self.current_indent - 2
        if len(opts) > opt_width:
            opts = "%*s%s\n" % (self.current_indent, "", opts)
            indent_first = self.help_position
        else:                       # start help on same line as opts
            opts = "%*s%-*s  " % (self.current_indent, "", opt_width, opts)
            indent_first = 0
        result.append(opts)
        if option.help:
            help_text = self.expand_default(option)
            help_lines = textwrap.wrap(help_text, self.help_width)
            result.append("%*s%s\n" % (indent_first, "", help_lines[0]))
            result.extend(["%*s%s\n" % (self.help_position, "", line)
                           for line in help_lines[1:]])
        elif opts[-1] != "\n":
            result.append("\n")
        return "".join(result)

    def store_option_strings(self, parser):
        self.indent()
        max_len = 0
        for opt in parser.option_list:
            strings = self.format_option_strings(opt)
            self.option_strings[opt] = strings
            max_len = max(max_len, len(strings) + self.current_indent)
        self.indent()
        for group in parser.option_groups:
            for opt in group.option_list:
                strings = self.format_option_strings(opt)
                self.option_strings[opt] = strings
                max_len = max(max_len, len(strings) + self.current_indent)
        self.dedent()
        self.dedent()
        self.help_position = min(max_len + 2, self.max_help_position)
        self.help_width = self.width - self.help_position

    def format_option_strings(self, option):
        """Return a comma-separated list of option strings & metavariables."""
        if option.takes_value():
            metavar = option.metavar or option.dest.upper()
            short_opts = [self._short_opt_fmt % (sopt, metavar)
                          for sopt in option._short_opts]
            long_opts = [self._long_opt_fmt % (lopt, metavar)
                         for lopt in option._long_opts]
        else:
            short_opts = option._short_opts
            long_opts = option._long_opts

        if self.short_first:
            opts = short_opts + long_opts
        else:
            opts = long_opts + short_opts

        return ", ".join(opts)

class IndentedHelpFormatter (HelpFormatter):
    """Format help with indented section bodies.
    """

    def __init__(self,
                 indent_increment=2,
                 max_help_position=24,
                 width=None,
                 short_first=1):
        HelpFormatter.__init__(
            self, indent_increment, max_help_position, width, short_first)

    def format_usage(self, usage):
        return _("Usage: %s\n") % usage

    def format_heading(self, heading):
        return "%*s%s:\n" % (self.current_indent, "", heading)


class TitledHelpFormatter (HelpFormatter):
    """Format help with underlined section headers.
    """

    def __init__(self,
                 indent_increment=0,
                 max_help_position=24,
                 width=None,
                 short_first=0):
        HelpFormatter.__init__ (
            self, indent_increment, max_help_position, width, short_first)

    def format_usage(self, usage):
        return "%s  %s\n" % (self.format_heading(_("Usage")), usage)

    def format_heading(self, heading):
        return "%s\n%s\n" % (heading, "=-"[self.level] * len(heading))


def _parse_num(val, type):
    if val[:2].lower() == "0x":         # hexadecimal
        radix = 16
    elif val[:2].lower() == "0b":       # binary
        radix = 2
        val = val[2:] or "0"            # have to remove "0b" prefix
    elif val[:1] == "0":                # octal
        radix = 8
    else:                               # decimal
        radix = 10

    return type(val, radix)

def _parse_int(val):
    return _parse_num(val, int)

def _parse_long(val):
    return _parse_num(val, long)

_builtin_cvt = { "int" : (_parse_int, _("integer")),
                 "long" : (_parse_long, _("long integer")),
                 "float" : (float, _("floating-point")),
                 "complex" : (complex, _("complex")) }

def check_builtin(option, opt, value):
    (cvt, what) = _builtin_cvt[option.type]
    try:
        return cvt(value)
    except ValueError:
        raise OptionValueError(
            _("option %s: invalid %s value: %r") % (opt, what, value))

def check_choice(option, opt, value):
    if value in option.choices:
        return value
    else:
        choices = ", ".join(map(repr, option.choices))
        raise OptionValueError(
            _("option %s: invalid choice: %r (choose from %s)")
            % (opt, value, choices))

# Not supplying a default is different from a default of None,
# so we need an explicit "not supplied" value.
NO_DEFAULT = ("NO", "DEFAULT")


class Option:
    """
    Instance attributes:
      _short_opts : [string]
      _long_opts : [string]

      action : string
      type : string
      dest : string
      default : any
      nargs : int
      const : any
      choices : [string]
      callback : function
      callback_args : (any*)
      callback_kwargs : { string : any }
      help : string
      metavar : string
    """

    # The list of instance attributes that may be set through
    # keyword args to the constructor.
    ATTRS = ['action',
             'type',
             'dest',
             'default',
             'nargs',
             'const',
             'choices',
             'callback',
             'callback_args',
             'callback_kwargs',
             'help',
             'metavar']

    # The set of actions allowed by option parsers.  Explicitly listed
    # here so the constructor can validate its arguments.
    ACTIONS = ("store",
               "store_const",
               "store_true",
               "store_false",
               "append",
               "append_const",
               "count",
               "callback",
               "help",
               "version")

    # The set of actions that involve storing a value somewhere;
    # also listed just for constructor argument validation.  (If
    # the action is one of these, there must be a destination.)
    STORE_ACTIONS = ("store",
                     "store_const",
                     "store_true",
                     "store_false",
                     "append",
                     "append_const",
                     "count")

    # The set of actions for which it makes sense to supply a value
    # type, ie. which may consume an argument from the command line.
    TYPED_ACTIONS = ("store",
                     "append",
                     "callback")

    # The set of actions which *require* a value type, ie. that
    # always consume an argument from the command line.
    ALWAYS_TYPED_ACTIONS = ("store",
                            "append")

    # The set of actions which take a 'const' attribute.
    CONST_ACTIONS = ("store_const",
                     "append_const")

    # The set of known types for option parsers.  Again, listed here for
    # constructor argument validation.
    TYPES = ("string", "int", "long", "float", "complex", "choice")

    # Dictionary of argument checking functions, which convert and
    # validate option arguments according to the option type.
    #
    # Signature of checking functions is:
    #   check(option : Option, opt : string, value : string) -> any
    # where
    #   option is the Option instance calling the checker
    #   opt is the actual option seen on the command-line
    #     (eg. "-a", "--file")
    #   value is the option argument seen on the command-line
    #
    # The return value should be in the appropriate Python type
    # for option.type -- eg. an integer if option.type == "int".
    #
    # If no checker is defined for a type, arguments will be
    # unchecked and remain strings.
    TYPE_CHECKER = { "int"    : check_builtin,
                     "long"   : check_builtin,
                     "float"  : check_builtin,
                     "complex": check_builtin,
                     "choice" : check_choice,
                   }


    # CHECK_METHODS is a list of unbound method objects; they are called
    # by the constructor, in order, after all attributes are
    # initialized.  The list is created and filled in later, after all
    # the methods are actually defined.  (I just put it here because I
    # like to define and document all class attributes in the same
    # place.)  Subclasses that add another _check_*() method should
    # define their own CHECK_METHODS list that adds their check method
    # to those from this class.
    CHECK_METHODS = None


    # -- Constructor/initialization methods ----------------------------

    def __init__(self, *opts, **attrs):
        # Set _short_opts, _long_opts attrs from 'opts' tuple.
        # Have to be set now, in case no option strings are supplied.
        self._short_opts = []
        self._long_opts = []
        opts = self._check_opt_strings(opts)
        self._set_opt_strings(opts)

        # Set all other attrs (action, type, etc.) from 'attrs' dict
        self._set_attrs(attrs)

        # Check all the attributes we just set.  There are lots of
        # complicated interdependencies, but luckily they can be farmed
        # out to the _check_*() methods listed in CHECK_METHODS -- which
        # could be handy for subclasses!  The one thing these all share
        # is that they raise OptionError if they discover a problem.
        for checker in self.CHECK_METHODS:
            checker(self)

    def _check_opt_strings(self, opts):
        # Filter out None because early versions of Optik had exactly
        # one short option and one long option, either of which
        # could be None.
        opts = filter(None, opts)
        if not opts:
            raise TypeError("at least one option string must be supplied")
        return opts

    def _set_opt_strings(self, opts):
        for opt in opts:
            if len(opt) < 2:
                raise OptionError(
                    "invalid option string %r: "
                    "must be at least two characters long" % opt, self)
            elif len(opt) == 2:
                if not (opt[0] == "-" and opt[1] != "-"):
                    raise OptionError(
                        "invalid short option string %r: "
                        "must be of the form -x, (x any non-dash char)" % opt,
                        self)
                self._short_opts.append(opt)
            else:
                if not (opt[0:2] == "--" and opt[2] != "-"):
                    raise OptionError(
                        "invalid long option string %r: "
                        "must start with --, followed by non-dash" % opt,
                        self)
                self._long_opts.append(opt)

    def _set_attrs(self, attrs):
        for attr in self.ATTRS:
            if attrs.has_key(attr):
                setattr(self, attr, attrs[attr])
                del attrs[attr]
            else:
                if attr == 'default':
                    setattr(self, attr, NO_DEFAULT)
                else:
                    setattr(self, attr, None)
        if attrs:
            attrs = attrs.keys()
            attrs.sort()
            raise OptionError(
                "invalid keyword arguments: %s" % ", ".join(attrs),
                self)


    # -- Constructor validation methods --------------------------------

    def _check_action(self):
        if self.action is None:
            self.action = "store"
        elif self.action not in self.ACTIONS:
            raise OptionError("invalid action: %r" % self.action, self)

    def _check_type(self):
        if self.type is None:
            if self.action in self.ALWAYS_TYPED_ACTIONS:
                if self.choices is not None:
                    # The "choices" attribute implies "choice" type.
                    self.type = "choice"
                else:
                    # No type given?  "string" is the most sensible default.
                    self.type = "string"
        else:
            # Allow type objects or builtin type conversion functions
            # (int, str, etc.) as an alternative to their names.  (The
            # complicated check of __builtin__ is only necessary for
            # Python 2.1 and earlier, and is short-circuited by the
            # first check on modern Pythons.)
            import __builtin__
            if ( type(self.type) is types.TypeType or
                 (hasattr(self.type, "__name__") and
                  getattr(__builtin__, self.type.__name__, None) is self.type) ):
                self.type = self.type.__name__

            if self.type == "str":
                self.type = "string"

            if self.type not in self.TYPES:
                raise OptionError("invalid option type: %r" % self.type, self)
            if self.action not in self.TYPED_ACTIONS:
                raise OptionError(
                    "must not supply a type for action %r" % self.action, self)

    def _check_choice(self):
        if self.type == "choice":
            if self.choices is None:
                raise OptionError(
                    "must supply a list of choices for type 'choice'", self)
            elif type(self.choices) not in (types.TupleType, types.ListType):
                raise OptionError(
                    "choices must be a list of strings ('%s' supplied)"
                    % str(type(self.choices)).split("'")[1], self)
        elif self.choices is not None:
            raise OptionError(
                "must not supply choices for type %r" % self.type, self)

    def _check_dest(self):
        # No destination given, and we need one for this action.  The
        # self.type check is for callbacks that take a value.
        takes_value = (self.action in self.STORE_ACTIONS or
                       self.type is not None)
        if self.dest is None and takes_value:

            # Glean a destination from the first long option string,
            # or from the first short option string if no long options.
            if self._long_opts:
                # eg. "--foo-bar" -> "foo_bar"
                self.dest = self._long_opts[0][2:].replace('-', '_')
            else:
                self.dest = self._short_opts[0][1]

    def _check_const(self):
        if self.action not in self.CONST_ACTIONS and self.const is not None:
            raise OptionError(
                "'const' must not be supplied for action %r" % self.action,
                self)

    def _check_nargs(self):
        if self.action in self.TYPED_ACTIONS:
            if self.nargs is None:
                self.nargs = 1
        elif self.nargs is not None:
            raise OptionError(
                "'nargs' must not be supplied for action %r" % self.action,
                self)

    def _check_callback(self):
        if self.action == "callback":
            if not callable(self.callback):
                raise OptionError(
                    "callback not callable: %r" % self.callback, self)
            if (self.callback_args is not None and
                type(self.callback_args) is not types.TupleType):
                raise OptionError(
                    "callback_args, if supplied, must be a tuple: not %r"
                    % self.callback_args, self)
            if (self.callback_kwargs is not None and
                type(self.callback_kwargs) is not types.DictType):
                raise OptionError(
                    "callback_kwargs, if supplied, must be a dict: not %r"
                    % self.callback_kwargs, self)
        else:
            if self.callback is not None:
                raise OptionError(
                    "callback supplied (%r) for non-callback option"
                    % self.callback, self)
            if self.callback_args is not None:
                raise OptionError(
                    "callback_args supplied for non-callback option", self)
            if self.callback_kwargs is not None:
                raise OptionError(
                    "callback_kwargs supplied for non-callback option", self)


    CHECK_METHODS = [_check_action,
                     _check_type,
                     _check_choice,
                     _check_dest,
                     _check_const,
                     _check_nargs,
                     _check_callback]


    # -- Miscellaneous methods -----------------------------------------

    def __str__(self):
        return "/".join(self._short_opts + self._long_opts)

    __repr__ = _repr

    def takes_value(self):
        return self.type is not None

    def get_opt_string(self):
        if self._long_opts:
            return self._long_opts[0]
        else:
            return self._short_opts[0]


    # -- Processing methods --------------------------------------------

    def check_value(self, opt, value):
        checker = self.TYPE_CHECKER.get(self.type)
        if checker is None:
            return value
        else:
            return checker(self, opt, value)

    def convert_value(self, opt, value):
        if value is not None:
            if self.nargs == 1:
                return self.check_value(opt, value)
            else:
                return tuple([self.check_value(opt, v) for v in value])

    def process(self, opt, value, values, parser):

        # First, convert the value(s) to the right type.  Howl if any
        # value(s) are bogus.
        value = self.convert_value(opt, value)

        # And then take whatever action is expected of us.
        # This is a separate method to make life easier for
        # subclasses to add new actions.
        return self.take_action(
            self.action, self.dest, opt, value, values, parser)

    def take_action(self, action, dest, opt, value, values, parser):
        if action == "store":
            setattr(values, dest, value)
        elif action == "store_const":
            setattr(values, dest, self.const)
        elif action == "store_true":
            setattr(values, dest, True)
        elif action == "store_false":
            setattr(values, dest, False)
        elif action == "append":
            values.ensure_value(dest, []).append(value)
        elif action == "append_const":
            values.ensure_value(dest, []).append(self.const)
        elif action == "count":
            setattr(values, dest, values.ensure_value(dest, 0) + 1)
        elif action == "callback":
            args = self.callback_args or ()
            kwargs = self.callback_kwargs or {}
            self.callback(self, opt, value, parser, *args, **kwargs)
        elif action == "help":
            parser.print_help()
            parser.exit()
        elif action == "version":
            parser.print_version()
            parser.exit()
        else:
            raise RuntimeError, "unknown action %r" % self.action

        return 1

# class Option


SUPPRESS_HELP = "SUPPRESS"+"HELP"
SUPPRESS_USAGE = "SUPPRESS"+"USAGE"

# For compatibility with Python 2.2
try:
    True, False
except NameError:
    (True, False) = (1, 0)

try:
    basestring
except NameError:
    def isbasestring(x):
        return isinstance(x, (types.StringType, types.UnicodeType))
else:
    def isbasestring(x):
        return isinstance(x, basestring)


class Values:

    def __init__(self, defaults=None):
        if defaults:
            for (attr, val) in defaults.items():
                setattr(self, attr, val)

    def __str__(self):
        return str(self.__dict__)

    __repr__ = _repr

    def __cmp__(self, other):
        if isinstance(other, Values):
            return cmp(self.__dict__, other.__dict__)
        elif isinstance(other, types.DictType):
            return cmp(self.__dict__, other)
        else:
            return -1

    def _update_careful(self, dict):
        """
        Update the option values from an arbitrary dictionary, but only
        use keys from dict that already have a corresponding attribute
        in self.  Any keys in dict without a corresponding attribute
        are silently ignored.
        """
        for attr in dir(self):
            if dict.has_key(attr):
                dval = dict[attr]
                if dval is not None:
                    setattr(self, attr, dval)

    def _update_loose(self, dict):
        """
        Update the option values from an arbitrary dictionary,
        using all keys from the dictionary regardless of whether
        they have a corresponding attribute in self or not.
        """
        self.__dict__.update(dict)

    def _update(self, dict, mode):
        if mode == "careful":
            self._update_careful(dict)
        elif mode == "loose":
            self._update_loose(dict)
        else:
            raise ValueError, "invalid update mode: %r" % mode

    def read_module(self, modname, mode="careful"):
        __import__(modname)
        mod = sys.modules[modname]
        self._update(vars(mod), mode)

    def read_file(self, filename, mode="careful"):
        vars = {}
        execfile(filename, vars)
        self._update(vars, mode)

    def ensure_value(self, attr, value):
        if not hasattr(self, attr) or getattr(self, attr) is None:
            setattr(self, attr, value)
        return getattr(self, attr)


class OptionContainer:

    """
    Abstract base class.

    Class attributes:
      standard_option_list : [Option]
        list of standard options that will be accepted by all instances
        of this parser class (intended to be overridden by subclasses).

    Instance attributes:
      option_list : [Option]
        the list of Option objects contained by this OptionContainer
      _short_opt : { string : Option }
        dictionary mapping short option strings, eg. "-f" or "-X",
        to the Option instances that implement them.  If an Option
        has multiple short option strings, it will appears in this
        dictionary multiple times. [1]
      _long_opt : { string : Option }
        dictionary mapping long option strings, eg. "--file" or
        "--exclude", to the Option instances that implement them.
        Again, a given Option can occur multiple times in this
        dictionary. [1]
      defaults : { string : any }
        dictionary mapping option destination names to default
        values for each destination [1]

    [1] These mappings are common to (shared by) all components of the
        controlling OptionParser, where they are initially created.

    """

    def __init__(self, option_class, conflict_handler, description):
        # Initialize the option list and related data structures.
        # This method must be provided by subclasses, and it must
        # initialize at least the following instance attributes:
        # option_list, _short_opt, _long_opt, defaults.
        self._create_option_list()

        self.option_class = option_class
        self.set_conflict_handler(conflict_handler)
        self.set_description(description)

    def _create_option_mappings(self):
        # For use by OptionParser constructor -- create the master
        # option mappings used by this OptionParser and all
        # OptionGroups that it owns.
        self._short_opt = {}            # single letter -> Option instance
        self._long_opt = {}             # long option -> Option instance
        self.defaults = {}              # maps option dest -> default value


    def _share_option_mappings(self, parser):
        # For use by OptionGroup constructor -- use shared option
        # mappings from the OptionParser that owns this OptionGroup.
        self._short_opt = parser._short_opt
        self._long_opt = parser._long_opt
        self.defaults = parser.defaults

    def set_conflict_handler(self, handler):
        if handler not in ("error", "resolve"):
            raise ValueError, "invalid conflict_resolution value %r" % handler
        self.conflict_handler = handler

    def set_description(self, description):
        self.description = description

    def get_description(self):
        return self.description


    def destroy(self):
        """see OptionParser.destroy()."""
        del self._short_opt
        del self._long_opt
        del self.defaults


    # -- Option-adding methods -----------------------------------------

    def _check_conflict(self, option):
        conflict_opts = []
        for opt in option._short_opts:
            if self._short_opt.has_key(opt):
                conflict_opts.append((opt, self._short_opt[opt]))
        for opt in option._long_opts:
            if self._long_opt.has_key(opt):
                conflict_opts.append((opt, self._long_opt[opt]))

        if conflict_opts:
            handler = self.conflict_handler
            if handler == "error":
                raise OptionConflictError(
                    "conflicting option string(s): %s"
                    % ", ".join([co[0] for co in conflict_opts]),
                    option)
            elif handler == "resolve":
                for (opt, c_option) in conflict_opts:
                    if opt.startswith("--"):
                        c_option._long_opts.remove(opt)
                        del self._long_opt[opt]
                    else:
                        c_option._short_opts.remove(opt)
                        del self._short_opt[opt]
                    if not (c_option._short_opts or c_option._long_opts):
                        c_option.container.option_list.remove(c_option)

    def add_option(self, *args, **kwargs):
        """add_option(Option)
           add_option(opt_str, ..., kwarg=val, ...)
        """
        if type(args[0]) is types.StringType:
            option = self.option_class(*args, **kwargs)
        elif len(args) == 1 and not kwargs:
            option = args[0]
            if not isinstance(option, Option):
                raise TypeError, "not an Option instance: %r" % option
        else:
            raise TypeError, "invalid arguments"

        self._check_conflict(option)

        self.option_list.append(option)
        option.container = self
        for opt in option._short_opts:
            self._short_opt[opt] = option
        for opt in option._long_opts:
            self._long_opt[opt] = option

        if option.dest is not None:     # option has a dest, we need a default
            if option.default is not NO_DEFAULT:
                self.defaults[option.dest] = option.default
            elif not self.defaults.has_key(option.dest):
                self.defaults[option.dest] = None

        return option

    def add_options(self, option_list):
        for option in option_list:
            self.add_option(option)

    # -- Option query/removal methods ----------------------------------

    def get_option(self, opt_str):
        return (self._short_opt.get(opt_str) or
                self._long_opt.get(opt_str))

    def has_option(self, opt_str):
        return (self._short_opt.has_key(opt_str) or
                self._long_opt.has_key(opt_str))

    def remove_option(self, opt_str):
        option = self._short_opt.get(opt_str)
        if option is None:
            option = self._long_opt.get(opt_str)
        if option is None:
            raise ValueError("no such option %r" % opt_str)

        for opt in option._short_opts:
            del self._short_opt[opt]
        for opt in option._long_opts:
            del self._long_opt[opt]
        option.container.option_list.remove(option)


    # -- Help-formatting methods ---------------------------------------

    def format_option_help(self, formatter):
        if not self.option_list:
            return ""
        result = []
        for option in self.option_list:
            if not option.help is SUPPRESS_HELP:
                result.append(formatter.format_option(option))
        return "".join(result)

    def format_description(self, formatter):
        return formatter.format_description(self.get_description())

    def format_help(self, formatter):
        result = []
        if self.description:
            result.append(self.format_description(formatter))
        if self.option_list:
            result.append(self.format_option_help(formatter))
        return "\n".join(result)


class OptionGroup (OptionContainer):

    def __init__(self, parser, title, description=None):
        self.parser = parser
        OptionContainer.__init__(
            self, parser.option_class, parser.conflict_handler, description)
        self.title = title

    def _create_option_list(self):
        self.option_list = []
        self._share_option_mappings(self.parser)

    def set_title(self, title):
        self.title = title

    def destroy(self):
        """see OptionParser.destroy()."""
        OptionContainer.destroy(self)
        del self.option_list

    # -- Help-formatting methods ---------------------------------------

    def format_help(self, formatter):
        result = formatter.format_heading(self.title)
        formatter.indent()
        result += OptionContainer.format_help(self, formatter)
        formatter.dedent()
        return result


class OptionParser (OptionContainer):

    """
    Class attributes:
      standard_option_list : [Option]
        list of standard options that will be accepted by all instances
        of this parser class (intended to be overridden by subclasses).

    Instance attributes:
      usage : string
        a usage string for your program.  Before it is displayed
        to the user, "%prog" will be expanded to the name of
        your program (self.prog or os.path.basename(sys.argv[0])).
      prog : string
        the name of the current program (to override
        os.path.basename(sys.argv[0])).
      epilog : string
        paragraph of help text to print after option help

      option_groups : [OptionGroup]
        list of option groups in this parser (option groups are
        irrelevant for parsing the command-line, but very useful
        for generating help)

      allow_interspersed_args : bool = true
        if true, positional arguments may be interspersed with options.
        Assuming -a and -b each take a single argument, the command-line
          -ablah foo bar -bboo baz
        will be interpreted the same as
          -ablah -bboo -- foo bar baz
        If this flag were false, that command line would be interpreted as
          -ablah -- foo bar -bboo baz
        -- ie. we stop processing options as soon as we see the first
        non-option argument.  (This is the tradition followed by
        Python's getopt module, Perl's Getopt::Std, and other argument-
        parsing libraries, but it is generally annoying to users.)

      process_default_values : bool = true
        if true, option default values are processed similarly to option
        values from the command line: that is, they are passed to the
        type-checking function for the option's type (as long as the
        default value is a string).  (This really only matters if you
        have defined custom types; see SF bug #955889.)  Set it to false
        to restore the behaviour of Optik 1.4.1 and earlier.

      rargs : [string]
        the argument list currently being parsed.  Only set when
        parse_args() is active, and continually trimmed down as
        we consume arguments.  Mainly there for the benefit of
        callback options.
      largs : [string]
        the list of leftover arguments that we have skipped while
        parsing options.  If allow_interspersed_args is false, this
        list is always empty.
      values : Values
        the set of option values currently being accumulated.  Only
        set when parse_args() is active.  Also mainly for callbacks.

    Because of the 'rargs', 'largs', and 'values' attributes,
    OptionParser is not thread-safe.  If, for some perverse reason, you
    need to parse command-line arguments simultaneously in different
    threads, use different OptionParser instances.

    """

    standard_option_list = []

    def __init__(self,
                 usage=None,
                 option_list=None,
                 option_class=Option,
                 version=None,
                 conflict_handler="error",
                 description=None,
                 formatter=None,
                 add_help_option=True,
                 prog=None,
                 epilog=None):
        OptionContainer.__init__(
            self, option_class, conflict_handler, description)
        self.set_usage(usage)
        self.prog = prog
        self.version = version
        self.allow_interspersed_args = True
        self.process_default_values = True
        if formatter is None:
            formatter = IndentedHelpFormatter()
        self.formatter = formatter
        self.formatter.set_parser(self)
        self.epilog = epilog

        # Populate the option list; initial sources are the
        # standard_option_list class attribute, the 'option_list'
        # argument, and (if applicable) the _add_version_option() and
        # _add_help_option() methods.
        self._populate_option_list(option_list,
                                   add_help=add_help_option)

        self._init_parsing_state()


    def destroy(self):
        """
        Declare that you are done with this OptionParser.  This cleans up
        reference cycles so the OptionParser (and all objects referenced by
        it) can be garbage-collected promptly.  After calling destroy(), the
        OptionParser is unusable.
        """
        OptionContainer.destroy(self)
        for group in self.option_groups:
            group.destroy()
        del self.option_list
        del self.option_groups
        del self.formatter


    # -- Private methods -----------------------------------------------
    # (used by our or OptionContainer's constructor)

    def _create_option_list(self):
        self.option_list = []
        self.option_groups = []
        self._create_option_mappings()

    def _add_help_option(self):
        self.add_option("-h", "--help",
                        action="help",
                        help=_("show this help message and exit"))

    def _add_version_option(self):
        self.add_option("--version",
                        action="version",
                        help=_("show program's version number and exit"))

    def _populate_option_list(self, option_list, add_help=True):
        if self.standard_option_list:
            self.add_options(self.standard_option_list)
        if option_list:
            self.add_options(option_list)
        if self.version:
            self._add_version_option()
        if add_help:
            self._add_help_option()

    def _init_parsing_state(self):
        # These are set in parse_args() for the convenience of callbacks.
        self.rargs = None
        self.largs = None
        self.values = None


    # -- Simple modifier methods ---------------------------------------

    def set_usage(self, usage):
        if usage is None:
            self.usage = _("%prog [options]")
        elif usage is SUPPRESS_USAGE:
            self.usage = None
        # For backwards compatibility with Optik 1.3 and earlier.
        elif usage.lower().startswith("usage: "):
            self.usage = usage[7:]
        else:
            self.usage = usage

    def enable_interspersed_args(self):
        self.allow_interspersed_args = True

    def disable_interspersed_args(self):
        self.allow_interspersed_args = False

    def set_process_default_values(self, process):
        self.process_default_values = process

    def set_default(self, dest, value):
        self.defaults[dest] = value

    def set_defaults(self, **kwargs):
        self.defaults.update(kwargs)

    def _get_all_options(self):
        options = self.option_list[:]
        for group in self.option_groups:
            options.extend(group.option_list)
        return options

    def get_default_values(self):
        if not self.process_default_values:
            # Old, pre-Optik 1.5 behaviour.
            return Values(self.defaults)

        defaults = self.defaults.copy()
        for option in self._get_all_options():
            default = defaults.get(option.dest)
            if isbasestring(default):
                opt_str = option.get_opt_string()
                defaults[option.dest] = option.check_value(opt_str, default)

        return Values(defaults)


    # -- OptionGroup methods -------------------------------------------

    def add_option_group(self, *args, **kwargs):
        # XXX lots of overlap with OptionContainer.add_option()
        if type(args[0]) is types.StringType:
            group = OptionGroup(self, *args, **kwargs)
        elif len(args) == 1 and not kwargs:
            group = args[0]
            if not isinstance(group, OptionGroup):
                raise TypeError, "not an OptionGroup instance: %r" % group
            if group.parser is not self:
                raise ValueError, "invalid OptionGroup (wrong parser)"
        else:
            raise TypeError, "invalid arguments"

        self.option_groups.append(group)
        return group

    def get_option_group(self, opt_str):
        option = (self._short_opt.get(opt_str) or
                  self._long_opt.get(opt_str))
        if option and option.container is not self:
            return option.container
        return None


    # -- Option-parsing methods ----------------------------------------

    def _get_args(self, args):
        if args is None:
            return sys.argv[1:]
        else:
            return args[:]              # don't modify caller's list

    def parse_args(self, args=None, values=None):
        """
        parse_args(args : [string] = sys.argv[1:],
                   values : Values = None)
        -> (values : Values, args : [string])

        Parse the command-line options found in 'args' (default:
        sys.argv[1:]).  Any errors result in a call to 'error()', which
        by default prints the usage message to stderr and calls
        sys.exit() with an error message.  On success returns a pair
        (values, args) where 'values' is an Values instance (with all
        your option values) and 'args' is the list of arguments left
        over after parsing options.
        """
        rargs = self._get_args(args)
        if values is None:
            values = self.get_default_values()

        # Store the halves of the argument list as attributes for the
        # convenience of callbacks:
        #   rargs
        #     the rest of the command-line (the "r" stands for
        #     "remaining" or "right-hand")
        #   largs
        #     the leftover arguments -- ie. what's left after removing
        #     options and their arguments (the "l" stands for "leftover"
        #     or "left-hand")
        self.rargs = rargs
        self.largs = largs = []
        self.values = values

        try:
            stop = self._process_args(largs, rargs, values)
        except (BadOptionError, OptionValueError), err:
            self.error(str(err))

        args = largs + rargs
        return self.check_values(values, args)

    def check_values(self, values, args):
        """
        check_values(values : Values, args : [string])
        -> (values : Values, args : [string])

        Check that the supplied option values and leftover arguments are
        valid.  Returns the option values and leftover arguments
        (possibly adjusted, possibly completely new -- whatever you
        like).  Default implementation just returns the passed-in
        values; subclasses may override as desired.
        """
        return (values, args)

    def _process_args(self, largs, rargs, values):
        """_process_args(largs : [string],
                         rargs : [string],
                         values : Values)

        Process command-line arguments and populate 'values', consuming
        options and arguments from 'rargs'.  If 'allow_interspersed_args' is
        false, stop at the first non-option argument.  If true, accumulate any
        interspersed non-option arguments in 'largs'.
        """
        while rargs:
            arg = rargs[0]
            # We handle bare "--" explicitly, and bare "-" is handled by the
            # standard arg handler since the short arg case ensures that the
            # len of the opt string is greater than 1.
            if arg == "--":
                del rargs[0]
                return
            elif arg[0:2] == "--":
                # process a single long option (possibly with value(s))
                self._process_long_opt(rargs, values)
            elif arg[:1] == "-" and len(arg) > 1:
                # process a cluster of short options (possibly with
                # value(s) for the last one only)
                self._process_short_opts(rargs, values)
            elif self.allow_interspersed_args:
                largs.append(arg)
                del rargs[0]
            else:
                return                  # stop now, leave this arg in rargs

        # Say this is the original argument list:
        # [arg0, arg1, ..., arg(i-1), arg(i), arg(i+1), ..., arg(N-1)]
        #                            ^
        # (we are about to process arg(i)).
        #
        # Then rargs is [arg(i), ..., arg(N-1)] and largs is a *subset* of
        # [arg0, ..., arg(i-1)] (any options and their arguments will have
        # been removed from largs).
        #
        # The while loop will usually consume 1 or more arguments per pass.
        # If it consumes 1 (eg. arg is an option that takes no arguments),
        # then after _process_arg() is done the situation is:
        #
        #   largs = subset of [arg0, ..., arg(i)]
        #   rargs = [arg(i+1), ..., arg(N-1)]
        #
        # If allow_interspersed_args is false, largs will always be
        # *empty* -- still a subset of [arg0, ..., arg(i-1)], but
        # not a very interesting subset!

    def _match_long_opt(self, opt):
        """_match_long_opt(opt : string) -> string

        Determine which long option string 'opt' matches, ie. which one
        it is an unambiguous abbrevation for.  Raises BadOptionError if
        'opt' doesn't unambiguously match any long option string.
        """
        return _match_abbrev(opt, self._long_opt)

    def _process_long_opt(self, rargs, values):
        arg = rargs.pop(0)

        # Value explicitly attached to arg?  Pretend it's the next
        # argument.
        if "=" in arg:
            (opt, next_arg) = arg.split("=", 1)
            rargs.insert(0, next_arg)
            had_explicit_value = True
        else:
            opt = arg
            had_explicit_value = False

        opt = self._match_long_opt(opt)
        option = self._long_opt[opt]
        if option.takes_value():
            nargs = option.nargs
            if len(rargs) < nargs:
                if nargs == 1:
                    self.error(_("%s option requires an argument") % opt)
                else:
                    self.error(_("%s option requires %d arguments")
                               % (opt, nargs))
            elif nargs == 1:
                value = rargs.pop(0)
            else:
                value = tuple(rargs[0:nargs])
                del rargs[0:nargs]

        elif had_explicit_value:
            self.error(_("%s option does not take a value") % opt)

        else:
            value = None

        option.process(opt, value, values, self)

    def _process_short_opts(self, rargs, values):
        arg = rargs.pop(0)
        stop = False
        i = 1
        for ch in arg[1:]:
            opt = "-" + ch
            option = self._short_opt.get(opt)
            i += 1                      # we have consumed a character

            if not option:
                raise BadOptionError(opt)
            if option.takes_value():
                # Any characters left in arg?  Pretend they're the
                # next arg, and stop consuming characters of arg.
                if i < len(arg):
                    rargs.insert(0, arg[i:])
                    stop = True

                nargs = option.nargs
                if len(rargs) < nargs:
                    if nargs == 1:
                        self.error(_("%s option requires an argument") % opt)
                    else:
                        self.error(_("%s option requires %d arguments")
                                   % (opt, nargs))
                elif nargs == 1:
                    value = rargs.pop(0)
                else:
                    value = tuple(rargs[0:nargs])
                    del rargs[0:nargs]

            else:                       # option doesn't take a value
                value = None

            option.process(opt, value, values, self)

            if stop:
                break


    # -- Feedback methods ----------------------------------------------

    def get_prog_name(self):
        if self.prog is None:
            return os.path.basename(sys.argv[0])
        else:
            return self.prog

    def expand_prog_name(self, s):
        return s.replace("%prog", self.get_prog_name())

    def get_description(self):
        return self.expand_prog_name(self.description)

    def exit(self, status=0, msg=None):
        if msg:
            sys.stderr.write(msg)
        sys.exit(status)

    def error(self, msg):
        """error(msg : string)

        Print a usage message incorporating 'msg' to stderr and exit.
        If you override this in a subclass, it should not return -- it
        should either exit or raise an exception.
        """
        self.print_usage(sys.stderr)
        self.exit(2, "%s: error: %s\n" % (self.get_prog_name(), msg))

    def get_usage(self):
        if self.usage:
            return self.formatter.format_usage(
                self.expand_prog_name(self.usage))
        else:
            return ""

    def print_usage(self, file=None):
        """print_usage(file : file = stdout)

        Print the usage message for the current program (self.usage) to
        'file' (default stdout).  Any occurence of the string "%prog" in
        self.usage is replaced with the name of the current program
        (basename of sys.argv[0]).  Does nothing if self.usage is empty
        or not defined.
        """
        if self.usage:
            print >>file, self.get_usage()

    def get_version(self):
        if self.version:
            return self.expand_prog_name(self.version)
        else:
            return ""

    def print_version(self, file=None):
        """print_version(file : file = stdout)

        Print the version message for this program (self.version) to
        'file' (default stdout).  As with print_usage(), any occurence
        of "%prog" in self.version is replaced by the current program's
        name.  Does nothing if self.version is empty or undefined.
        """
        if self.version:
            print >>file, self.get_version()

    def format_option_help(self, formatter=None):
        if formatter is None:
            formatter = self.formatter
        formatter.store_option_strings(self)
        result = []
        result.append(formatter.format_heading(_("Options")))
        formatter.indent()
        if self.option_list:
            result.append(OptionContainer.format_option_help(self, formatter))
            result.append("\n")
        for group in self.option_groups:
            result.append(group.format_help(formatter))
            result.append("\n")
        formatter.dedent()
        # Drop the last "\n", or the header if no options or option groups:
        return "".join(result[:-1])

    def format_epilog(self, formatter):
        return formatter.format_epilog(self.epilog)

    def format_help(self, formatter=None):
        if formatter is None:
            formatter = self.formatter
        result = []
        if self.usage:
            result.append(self.get_usage() + "\n")
        if self.description:
            result.append(self.format_description(formatter) + "\n")
        result.append(self.format_option_help(formatter))
        result.append(self.format_epilog(formatter))
        return "".join(result)

    # used by test suite
    def _get_encoding(self, file):
        encoding = getattr(file, "encoding", None)
        if not encoding:
            encoding = sys.getdefaultencoding()
        return encoding

    def print_help(self, file=None):
        """print_help(file : file = stdout)

        Print an extended help message, listing all options and any
        help text provided with them, to 'file' (default stdout).
        """
        if file is None:
            file = sys.stdout
        encoding = self._get_encoding(file)
        file.write(self.format_help().encode(encoding, "replace"))

# class OptionParser


def _match_abbrev(s, wordmap):
    """_match_abbrev(s : string, wordmap : {string : Option}) -> string

    Return the string key in 'wordmap' for which 's' is an unambiguous
    abbreviation.  If 's' is found to be ambiguous or doesn't match any of
    'words', raise BadOptionError.
    """
    # Is there an exact match?
    if wordmap.has_key(s):
        return s
    else:
        # Isolate all words with s as a prefix.
        possibilities = [word for word in wordmap.keys()
                         if word.startswith(s)]
        # No exact match, so there had better be just one possibility.
        if len(possibilities) == 1:
            return possibilities[0]
        elif not possibilities:
            raise BadOptionError(s)
        else:
            # More than one possible completion: ambiguous prefix.
            possibilities.sort()
            raise AmbiguousOptionError(s, possibilities)


# Some day, there might be many Option classes.  As of Optik 1.3, the
# preferred way to instantiate Options is indirectly, via make_option(),
# which will become a factory function when there are many Option
# classes.
make_option = Option

########NEW FILE########
__FILENAME__ = subprocess
# subprocess - Subprocesses with accessible I/O streams
#
# For more information about this module, see PEP 324.
#
# This module should remain compatible with Python 2.2, see PEP 291.
#
# Copyright (c) 2003-2005 by Peter Astrand <astrand@lysator.liu.se>
#
# Licensed to PSF under a Contributor Agreement.
# See http://www.python.org/2.4/license for licensing details.

r"""subprocess - Subprocesses with accessible I/O streams

This module allows you to spawn processes, connect to their
input/output/error pipes, and obtain their return codes.  This module
intends to replace several other, older modules and functions, like:

os.system
os.spawn*
os.popen*
popen2.*
commands.*

Information about how the subprocess module can be used to replace these
modules and functions can be found below.



Using the subprocess module
===========================
This module defines one class called Popen:

class Popen(args, bufsize=0, executable=None,
            stdin=None, stdout=None, stderr=None,
            preexec_fn=None, close_fds=False, shell=False,
            cwd=None, env=None, universal_newlines=False,
            startupinfo=None, creationflags=0):


Arguments are:

args should be a string, or a sequence of program arguments.  The
program to execute is normally the first item in the args sequence or
string, but can be explicitly set by using the executable argument.

On UNIX, with shell=False (default): In this case, the Popen class
uses os.execvp() to execute the child program.  args should normally
be a sequence.  A string will be treated as a sequence with the string
as the only item (the program to execute).

On UNIX, with shell=True: If args is a string, it specifies the
command string to execute through the shell.  If args is a sequence,
the first item specifies the command string, and any additional items
will be treated as additional shell arguments.

On Windows: the Popen class uses CreateProcess() to execute the child
program, which operates on strings.  If args is a sequence, it will be
converted to a string using the list2cmdline method.  Please note that
not all MS Windows applications interpret the command line the same
way: The list2cmdline is designed for applications using the same
rules as the MS C runtime.

bufsize, if given, has the same meaning as the corresponding argument
to the built-in open() function: 0 means unbuffered, 1 means line
buffered, any other positive value means use a buffer of
(approximately) that size.  A negative bufsize means to use the system
default, which usually means fully buffered.  The default value for
bufsize is 0 (unbuffered).

stdin, stdout and stderr specify the executed programs' standard
input, standard output and standard error file handles, respectively.
Valid values are PIPE, an existing file descriptor (a positive
integer), an existing file object, and None.  PIPE indicates that a
new pipe to the child should be created.  With None, no redirection
will occur; the child's file handles will be inherited from the
parent.  Additionally, stderr can be STDOUT, which indicates that the
stderr data from the applications should be captured into the same
file handle as for stdout.

If preexec_fn is set to a callable object, this object will be called
in the child process just before the child is executed.

If close_fds is true, all file descriptors except 0, 1 and 2 will be
closed before the child process is executed.

if shell is true, the specified command will be executed through the
shell.

If cwd is not None, the current directory will be changed to cwd
before the child is executed.

If env is not None, it defines the environment variables for the new
process.

If universal_newlines is true, the file objects stdout and stderr are
opened as a text files, but lines may be terminated by any of '\n',
the Unix end-of-line convention, '\r', the Macintosh convention or
'\r\n', the Windows convention.  All of these external representations
are seen as '\n' by the Python program.  Note: This feature is only
available if Python is built with universal newline support (the
default).  Also, the newlines attribute of the file objects stdout,
stdin and stderr are not updated by the communicate() method.

The startupinfo and creationflags, if given, will be passed to the
underlying CreateProcess() function.  They can specify things such as
appearance of the main window and priority for the new process.
(Windows only)


This module also defines two shortcut functions:

call(*popenargs, **kwargs):
    Run command with arguments.  Wait for command to complete, then
    return the returncode attribute.

    The arguments are the same as for the Popen constructor.  Example:

    retcode = call(["ls", "-l"])

check_call(*popenargs, **kwargs):
    Run command with arguments.  Wait for command to complete.  If the
    exit code was zero then return, otherwise raise
    CalledProcessError.  The CalledProcessError object will have the
    return code in the returncode attribute.

    The arguments are the same as for the Popen constructor.  Example:

    check_call(["ls", "-l"])

Exceptions
----------
Exceptions raised in the child process, before the new program has
started to execute, will be re-raised in the parent.  Additionally,
the exception object will have one extra attribute called
'child_traceback', which is a string containing traceback information
from the childs point of view.

The most common exception raised is OSError.  This occurs, for
example, when trying to execute a non-existent file.  Applications
should prepare for OSErrors.

A ValueError will be raised if Popen is called with invalid arguments.

check_call() will raise CalledProcessError, if the called process
returns a non-zero return code.


Security
--------
Unlike some other popen functions, this implementation will never call
/bin/sh implicitly.  This means that all characters, including shell
metacharacters, can safely be passed to child processes.


Popen objects
=============
Instances of the Popen class have the following methods:

poll()
    Check if child process has terminated.  Returns returncode
    attribute.

wait()
    Wait for child process to terminate.  Returns returncode attribute.

communicate(input=None)
    Interact with process: Send data to stdin.  Read data from stdout
    and stderr, until end-of-file is reached.  Wait for process to
    terminate.  The optional input argument should be a string to be
    sent to the child process, or None, if no data should be sent to
    the child.

    communicate() returns a tuple (stdout, stderr).

    Note: The data read is buffered in memory, so do not use this
    method if the data size is large or unlimited.

The following attributes are also available:

stdin
    If the stdin argument is PIPE, this attribute is a file object
    that provides input to the child process.  Otherwise, it is None.

stdout
    If the stdout argument is PIPE, this attribute is a file object
    that provides output from the child process.  Otherwise, it is
    None.

stderr
    If the stderr argument is PIPE, this attribute is file object that
    provides error output from the child process.  Otherwise, it is
    None.

pid
    The process ID of the child process.

returncode
    The child return code.  A None value indicates that the process
    hasn't terminated yet.  A negative value -N indicates that the
    child was terminated by signal N (UNIX only).


Replacing older functions with the subprocess module
====================================================
In this section, "a ==> b" means that b can be used as a replacement
for a.

Note: All functions in this section fail (more or less) silently if
the executed program cannot be found; this module raises an OSError
exception.

In the following examples, we assume that the subprocess module is
imported with "from subprocess import *".


Replacing /bin/sh shell backquote
---------------------------------
output=`mycmd myarg`
==>
output = Popen(["mycmd", "myarg"], stdout=PIPE).communicate()[0]


Replacing shell pipe line
-------------------------
output=`dmesg | grep hda`
==>
p1 = Popen(["dmesg"], stdout=PIPE)
p2 = Popen(["grep", "hda"], stdin=p1.stdout, stdout=PIPE)
output = p2.communicate()[0]


Replacing os.system()
---------------------
sts = os.system("mycmd" + " myarg")
==>
p = Popen("mycmd" + " myarg", shell=True)
pid, sts = os.waitpid(p.pid, 0)

Note:

* Calling the program through the shell is usually not required.

* It's easier to look at the returncode attribute than the
  exitstatus.

A more real-world example would look like this:

try:
    retcode = call("mycmd" + " myarg", shell=True)
    if retcode < 0:
        print >>sys.stderr, "Child was terminated by signal", -retcode
    else:
        print >>sys.stderr, "Child returned", retcode
except OSError, e:
    print >>sys.stderr, "Execution failed:", e


Replacing os.spawn*
-------------------
P_NOWAIT example:

pid = os.spawnlp(os.P_NOWAIT, "/bin/mycmd", "mycmd", "myarg")
==>
pid = Popen(["/bin/mycmd", "myarg"]).pid


P_WAIT example:

retcode = os.spawnlp(os.P_WAIT, "/bin/mycmd", "mycmd", "myarg")
==>
retcode = call(["/bin/mycmd", "myarg"])


Vector example:

os.spawnvp(os.P_NOWAIT, path, args)
==>
Popen([path] + args[1:])


Environment example:

os.spawnlpe(os.P_NOWAIT, "/bin/mycmd", "mycmd", "myarg", env)
==>
Popen(["/bin/mycmd", "myarg"], env={"PATH": "/usr/bin"})


Replacing os.popen*
-------------------
pipe = os.popen(cmd, mode='r', bufsize)
==>
pipe = Popen(cmd, shell=True, bufsize=bufsize, stdout=PIPE).stdout

pipe = os.popen(cmd, mode='w', bufsize)
==>
pipe = Popen(cmd, shell=True, bufsize=bufsize, stdin=PIPE).stdin


(child_stdin, child_stdout) = os.popen2(cmd, mode, bufsize)
==>
p = Popen(cmd, shell=True, bufsize=bufsize,
          stdin=PIPE, stdout=PIPE, close_fds=True)
(child_stdin, child_stdout) = (p.stdin, p.stdout)


(child_stdin,
 child_stdout,
 child_stderr) = os.popen3(cmd, mode, bufsize)
==>
p = Popen(cmd, shell=True, bufsize=bufsize,
          stdin=PIPE, stdout=PIPE, stderr=PIPE, close_fds=True)
(child_stdin,
 child_stdout,
 child_stderr) = (p.stdin, p.stdout, p.stderr)


(child_stdin, child_stdout_and_stderr) = os.popen4(cmd, mode, bufsize)
==>
p = Popen(cmd, shell=True, bufsize=bufsize,
          stdin=PIPE, stdout=PIPE, stderr=STDOUT, close_fds=True)
(child_stdin, child_stdout_and_stderr) = (p.stdin, p.stdout)


Replacing popen2.*
------------------
Note: If the cmd argument to popen2 functions is a string, the command
is executed through /bin/sh.  If it is a list, the command is directly
executed.

(child_stdout, child_stdin) = popen2.popen2("somestring", bufsize, mode)
==>
p = Popen(["somestring"], shell=True, bufsize=bufsize
          stdin=PIPE, stdout=PIPE, close_fds=True)
(child_stdout, child_stdin) = (p.stdout, p.stdin)


(child_stdout, child_stdin) = popen2.popen2(["mycmd", "myarg"], bufsize, mode)
==>
p = Popen(["mycmd", "myarg"], bufsize=bufsize,
          stdin=PIPE, stdout=PIPE, close_fds=True)
(child_stdout, child_stdin) = (p.stdout, p.stdin)

The popen2.Popen3 and popen2.Popen4 basically works as subprocess.Popen,
except that:

* subprocess.Popen raises an exception if the execution fails
* the capturestderr argument is replaced with the stderr argument.
* stdin=PIPE and stdout=PIPE must be specified.
* popen2 closes all filedescriptors by default, but you have to specify
  close_fds=True with subprocess.Popen.


"""

import sys
mswindows = (sys.platform == "win32")

import os
import types
import traceback
import gc

# Exception classes used by this module.
class CalledProcessError(Exception):
    """This exception is raised when a process run by check_call() returns
    a non-zero exit status.  The exit status will be stored in the
    returncode attribute."""
    def __init__(self, returncode, cmd):
        self.returncode = returncode
        self.cmd = cmd
    def __str__(self):
        return "Command '%s' returned non-zero exit status %d" % (self.cmd, self.returncode)


if mswindows:
    import threading
    import msvcrt
    if 0: # <-- change this to use pywin32 instead of the _subprocess driver
        import pywintypes
        from win32api import GetStdHandle, STD_INPUT_HANDLE, \
                             STD_OUTPUT_HANDLE, STD_ERROR_HANDLE
        from win32api import GetCurrentProcess, DuplicateHandle, \
                             GetModuleFileName, GetVersion
        from win32con import DUPLICATE_SAME_ACCESS, SW_HIDE
        from win32pipe import CreatePipe
        from win32process import CreateProcess, STARTUPINFO, \
                                 GetExitCodeProcess, STARTF_USESTDHANDLES, \
                                 STARTF_USESHOWWINDOW, CREATE_NEW_CONSOLE
        from win32event import WaitForSingleObject, INFINITE, WAIT_OBJECT_0
    else:
        from _subprocess import *
        class STARTUPINFO:
            dwFlags = 0
            hStdInput = None
            hStdOutput = None
            hStdError = None
            wShowWindow = 0
        class pywintypes:
            error = IOError
else:
    import select
    import errno
    import fcntl
    import pickle

__all__ = ["Popen", "PIPE", "STDOUT", "call", "check_call", "CalledProcessError"]

try:
    MAXFD = os.sysconf("SC_OPEN_MAX")
except:
    MAXFD = 256

# True/False does not exist on 2.2.0
try:
    False
except NameError:
    False = 0
    True = 1

_active = []

def _cleanup():
    for inst in _active[:]:
        if inst._internal_poll(_deadstate=sys.maxint) >= 0:
            try:
                _active.remove(inst)
            except ValueError:
                # This can happen if two threads create a new Popen instance.
                # It's harmless that it was already removed, so ignore.
                pass

PIPE = -1
STDOUT = -2


def call(*popenargs, **kwargs):
    """Run command with arguments.  Wait for command to complete, then
    return the returncode attribute.

    The arguments are the same as for the Popen constructor.  Example:

    retcode = call(["ls", "-l"])
    """
    return Popen(*popenargs, **kwargs).wait()


def check_call(*popenargs, **kwargs):
    """Run command with arguments.  Wait for command to complete.  If
    the exit code was zero then return, otherwise raise
    CalledProcessError.  The CalledProcessError object will have the
    return code in the returncode attribute.

    The arguments are the same as for the Popen constructor.  Example:

    check_call(["ls", "-l"])
    """
    retcode = call(*popenargs, **kwargs)
    cmd = kwargs.get("args")
    if cmd is None:
        cmd = popenargs[0]
    if retcode:
        raise CalledProcessError(retcode, cmd)
    return retcode


def list2cmdline(seq):
    """
    Translate a sequence of arguments into a command line
    string, using the same rules as the MS C runtime:

    1) Arguments are delimited by white space, which is either a
       space or a tab.

    2) A string surrounded by double quotation marks is
       interpreted as a single argument, regardless of white space
       contained within.  A quoted string can be embedded in an
       argument.

    3) A double quotation mark preceded by a backslash is
       interpreted as a literal double quotation mark.

    4) Backslashes are interpreted literally, unless they
       immediately precede a double quotation mark.

    5) If backslashes immediately precede a double quotation mark,
       every pair of backslashes is interpreted as a literal
       backslash.  If the number of backslashes is odd, the last
       backslash escapes the next double quotation mark as
       described in rule 3.
    """

    # See
    # http://msdn.microsoft.com/library/en-us/vccelng/htm/progs_12.asp
    result = []
    needquote = False
    for arg in seq:
        bs_buf = []

        # Add a space to separate this argument from the others
        if result:
            result.append(' ')

        needquote = (" " in arg) or ("\t" in arg) or arg == ""
        if needquote:
            result.append('"')

        for c in arg:
            if c == '\\':
                # Don't know if we need to double yet.
                bs_buf.append(c)
            elif c == '"':
                # Double backspaces.
                result.append('\\' * len(bs_buf)*2)
                bs_buf = []
                result.append('\\"')
            else:
                # Normal char
                if bs_buf:
                    result.extend(bs_buf)
                    bs_buf = []
                result.append(c)

        # Add remaining backspaces, if any.
        if bs_buf:
            result.extend(bs_buf)

        if needquote:
            result.extend(bs_buf)
            result.append('"')

    return ''.join(result)


class Popen(object):
    def __init__(self, args, bufsize=0, executable=None,
                 stdin=None, stdout=None, stderr=None,
                 preexec_fn=None, close_fds=False, shell=False,
                 cwd=None, env=None, universal_newlines=False,
                 startupinfo=None, creationflags=0):
        """Create new Popen instance."""
        _cleanup()

        self._child_created = False
        if not isinstance(bufsize, (int, long)):
            raise TypeError("bufsize must be an integer")

        if mswindows:
            if preexec_fn is not None:
                raise ValueError("preexec_fn is not supported on Windows "
                                 "platforms")
            if close_fds:
                raise ValueError("close_fds is not supported on Windows "
                                 "platforms")
        else:
            # POSIX
            if startupinfo is not None:
                raise ValueError("startupinfo is only supported on Windows "
                                 "platforms")
            if creationflags != 0:
                raise ValueError("creationflags is only supported on Windows "
                                 "platforms")

        self.stdin = None
        self.stdout = None
        self.stderr = None
        self.pid = None
        self.returncode = None
        self.universal_newlines = universal_newlines

        # Input and output objects. The general principle is like
        # this:
        #
        # Parent                   Child
        # ------                   -----
        # p2cwrite   ---stdin--->  p2cread
        # c2pread    <--stdout---  c2pwrite
        # errread    <--stderr---  errwrite
        #
        # On POSIX, the child objects are file descriptors.  On
        # Windows, these are Windows file handles.  The parent objects
        # are file descriptors on both platforms.  The parent objects
        # are None when not using PIPEs. The child objects are None
        # when not redirecting.

        (p2cread, p2cwrite,
         c2pread, c2pwrite,
         errread, errwrite) = self._get_handles(stdin, stdout, stderr)

        self._execute_child(args, executable, preexec_fn, close_fds,
                            cwd, env, universal_newlines,
                            startupinfo, creationflags, shell,
                            p2cread, p2cwrite,
                            c2pread, c2pwrite,
                            errread, errwrite)

        # On Windows, you cannot just redirect one or two handles: You
        # either have to redirect all three or none. If the subprocess
        # user has only redirected one or two handles, we are
        # automatically creating PIPEs for the rest. We should close
        # these after the process is started. See bug #1124861.
        if mswindows:
            if stdin is None and p2cwrite is not None:
                os.close(p2cwrite)
                p2cwrite = None
            if stdout is None and c2pread is not None:
                os.close(c2pread)
                c2pread = None
            if stderr is None and errread is not None:
                os.close(errread)
                errread = None

        if p2cwrite:
            self.stdin = os.fdopen(p2cwrite, 'wb', bufsize)
        if c2pread:
            if universal_newlines:
                self.stdout = os.fdopen(c2pread, 'rU', bufsize)
            else:
                self.stdout = os.fdopen(c2pread, 'rb', bufsize)
        if errread:
            if universal_newlines:
                self.stderr = os.fdopen(errread, 'rU', bufsize)
            else:
                self.stderr = os.fdopen(errread, 'rb', bufsize)


    def _translate_newlines(self, data):
        data = data.replace("\r\n", "\n")
        data = data.replace("\r", "\n")
        return data


    def __del__(self, sys=sys):
        if not self._child_created:
            # We didn't get to successfully create a child process.
            return
        # In case the child hasn't been waited on, check if it's done.
        self._internal_poll(_deadstate=sys.maxint)
        if self.returncode is None and _active is not None:
            # Child is still running, keep us alive until we can wait on it.
            _active.append(self)


    def communicate(self, input=None):
        """Interact with process: Send data to stdin.  Read data from
        stdout and stderr, until end-of-file is reached.  Wait for
        process to terminate.  The optional input argument should be a
        string to be sent to the child process, or None, if no data
        should be sent to the child.

        communicate() returns a tuple (stdout, stderr)."""

        # Optimization: If we are only using one pipe, or no pipe at
        # all, using select() or threads is unnecessary.
        if [self.stdin, self.stdout, self.stderr].count(None) >= 2:
            stdout = None
            stderr = None
            if self.stdin:
                if input:
                    self._fo_write_no_intr(self.stdin, input)
                self.stdin.close()
            elif self.stdout:
                stdout = self._fo_read_no_intr(self.stdout)
                self.stdout.close()
            elif self.stderr:
                stderr = self._fo_read_no_intr(self.stderr)
                self.stderr.close()
            self.wait()
            return (stdout, stderr)

        return self._communicate(input)


    def poll(self):
        return self._internal_poll()


    if mswindows:
        #
        # Windows methods
        #
        def _get_handles(self, stdin, stdout, stderr):
            """Construct and return tupel with IO objects:
            p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite
            """
            if stdin is None and stdout is None and stderr is None:
                return (None, None, None, None, None, None)

            p2cread, p2cwrite = None, None
            c2pread, c2pwrite = None, None
            errread, errwrite = None, None

            if stdin is None:
                p2cread = GetStdHandle(STD_INPUT_HANDLE)
            if p2cread is not None:
                pass
            elif stdin is None or stdin == PIPE:
                p2cread, p2cwrite = CreatePipe(None, 0)
                # Detach and turn into fd
                p2cwrite = p2cwrite.Detach()
                p2cwrite = msvcrt.open_osfhandle(p2cwrite, 0)
            elif isinstance(stdin, int):
                p2cread = msvcrt.get_osfhandle(stdin)
            else:
                # Assuming file-like object
                p2cread = msvcrt.get_osfhandle(stdin.fileno())
            p2cread = self._make_inheritable(p2cread)

            if stdout is None:
                c2pwrite = GetStdHandle(STD_OUTPUT_HANDLE)
            if c2pwrite is not None:
                pass
            elif stdout is None or stdout == PIPE:
                c2pread, c2pwrite = CreatePipe(None, 0)
                # Detach and turn into fd
                c2pread = c2pread.Detach()
                c2pread = msvcrt.open_osfhandle(c2pread, 0)
            elif isinstance(stdout, int):
                c2pwrite = msvcrt.get_osfhandle(stdout)
            else:
                # Assuming file-like object
                c2pwrite = msvcrt.get_osfhandle(stdout.fileno())
            c2pwrite = self._make_inheritable(c2pwrite)

            if stderr is None:
                errwrite = GetStdHandle(STD_ERROR_HANDLE)
            if errwrite is not None:
                pass
            elif stderr is None or stderr == PIPE:
                errread, errwrite = CreatePipe(None, 0)
                # Detach and turn into fd
                errread = errread.Detach()
                errread = msvcrt.open_osfhandle(errread, 0)
            elif stderr == STDOUT:
                errwrite = c2pwrite
            elif isinstance(stderr, int):
                errwrite = msvcrt.get_osfhandle(stderr)
            else:
                # Assuming file-like object
                errwrite = msvcrt.get_osfhandle(stderr.fileno())
            errwrite = self._make_inheritable(errwrite)

            return (p2cread, p2cwrite,
                    c2pread, c2pwrite,
                    errread, errwrite)


        def _make_inheritable(self, handle):
            """Return a duplicate of handle, which is inheritable"""
            return DuplicateHandle(GetCurrentProcess(), handle,
                                   GetCurrentProcess(), 0, 1,
                                   DUPLICATE_SAME_ACCESS)


        def _find_w9xpopen(self):
            """Find and return absolut path to w9xpopen.exe"""
            w9xpopen = os.path.join(os.path.dirname(GetModuleFileName(0)),
                                    "w9xpopen.exe")
            if not os.path.exists(w9xpopen):
                # Eeek - file-not-found - possibly an embedding
                # situation - see if we can locate it in sys.exec_prefix
                w9xpopen = os.path.join(os.path.dirname(sys.exec_prefix),
                                        "w9xpopen.exe")
                if not os.path.exists(w9xpopen):
                    raise RuntimeError("Cannot locate w9xpopen.exe, which is "
                                       "needed for Popen to work with your "
                                       "shell or platform.")
            return w9xpopen


        def _execute_child(self, args, executable, preexec_fn, close_fds,
                           cwd, env, universal_newlines,
                           startupinfo, creationflags, shell,
                           p2cread, p2cwrite,
                           c2pread, c2pwrite,
                           errread, errwrite):
            """Execute program (MS Windows version)"""

            if not isinstance(args, types.StringTypes):
                args = list2cmdline(args)

            # Process startup details
            if startupinfo is None:
                startupinfo = STARTUPINFO()
            if None not in (p2cread, c2pwrite, errwrite):
                startupinfo.dwFlags |= STARTF_USESTDHANDLES
                startupinfo.hStdInput = p2cread
                startupinfo.hStdOutput = c2pwrite
                startupinfo.hStdError = errwrite

            if shell:
                startupinfo.dwFlags |= STARTF_USESHOWWINDOW
                startupinfo.wShowWindow = SW_HIDE
                comspec = os.environ.get("COMSPEC", "cmd.exe")
                args = comspec + " /c " + args
                if (GetVersion() >= 0x80000000L or
                        os.path.basename(comspec).lower() == "command.com"):
                    # Win9x, or using command.com on NT. We need to
                    # use the w9xpopen intermediate program. For more
                    # information, see KB Q150956
                    # (http://web.archive.org/web/20011105084002/http://support.microsoft.com/support/kb/articles/Q150/9/56.asp)
                    w9xpopen = self._find_w9xpopen()
                    args = '"%s" %s' % (w9xpopen, args)
                    # Not passing CREATE_NEW_CONSOLE has been known to
                    # cause random failures on win9x.  Specifically a
                    # dialog: "Your program accessed mem currently in
                    # use at xxx" and a hopeful warning about the
                    # stability of your system.  Cost is Ctrl+C wont
                    # kill children.
                    creationflags |= CREATE_NEW_CONSOLE

            # Start the process
            try:
                hp, ht, pid, tid = CreateProcess(executable, args,
                                         # no special security
                                         None, None,
                                         # must inherit handles to pass std
                                         # handles
                                         1,
                                         creationflags,
                                         env,
                                         cwd,
                                         startupinfo)
            except pywintypes.error, e:
                # Translate pywintypes.error to WindowsError, which is
                # a subclass of OSError.  FIXME: We should really
                # translate errno using _sys_errlist (or simliar), but
                # how can this be done from Python?
                raise WindowsError(*e.args)

            # Retain the process handle, but close the thread handle
            self._child_created = True
            self._handle = hp
            self.pid = pid
            ht.Close()

            # Child is launched. Close the parent's copy of those pipe
            # handles that only the child should have open.  You need
            # to make sure that no handles to the write end of the
            # output pipe are maintained in this process or else the
            # pipe will not close when the child process exits and the
            # ReadFile will hang.
            if p2cread is not None:
                p2cread.Close()
            if c2pwrite is not None:
                c2pwrite.Close()
            if errwrite is not None:
                errwrite.Close()


        def _internal_poll(self, _deadstate=None):
            """Check if child process has terminated.  Returns returncode
            attribute."""
            if self.returncode is None:
                if WaitForSingleObject(self._handle, 0) == WAIT_OBJECT_0:
                    self.returncode = GetExitCodeProcess(self._handle)
            return self.returncode


        def wait(self):
            """Wait for child process to terminate.  Returns returncode
            attribute."""
            if self.returncode is None:
                obj = WaitForSingleObject(self._handle, INFINITE)
                self.returncode = GetExitCodeProcess(self._handle)
            return self.returncode


        def _readerthread(self, fh, buffer):
            buffer.append(fh.read())


        def _communicate(self, input):
            stdout = None # Return
            stderr = None # Return

            if self.stdout:
                stdout = []
                stdout_thread = threading.Thread(target=self._readerthread,
                                                 args=(self.stdout, stdout))
                stdout_thread.setDaemon(True)
                stdout_thread.start()
            if self.stderr:
                stderr = []
                stderr_thread = threading.Thread(target=self._readerthread,
                                                 args=(self.stderr, stderr))
                stderr_thread.setDaemon(True)
                stderr_thread.start()

            if self.stdin:
                if input is not None:
                    self.stdin.write(input)
                self.stdin.close()

            if self.stdout:
                stdout_thread.join()
            if self.stderr:
                stderr_thread.join()

            # All data exchanged.  Translate lists into strings.
            if stdout is not None:
                stdout = stdout[0]
            if stderr is not None:
                stderr = stderr[0]

            # Translate newlines, if requested.  We cannot let the file
            # object do the translation: It is based on stdio, which is
            # impossible to combine with select (unless forcing no
            # buffering).
            if self.universal_newlines and hasattr(file, 'newlines'):
                if stdout:
                    stdout = self._translate_newlines(stdout)
                if stderr:
                    stderr = self._translate_newlines(stderr)

            self.wait()
            return (stdout, stderr)

    else:
        #
        # POSIX methods
        #
        def _get_handles(self, stdin, stdout, stderr):
            """Construct and return tupel with IO objects:
            p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite
            """
            p2cread, p2cwrite = None, None
            c2pread, c2pwrite = None, None
            errread, errwrite = None, None

            if stdin is None:
                pass
            elif stdin == PIPE:
                p2cread, p2cwrite = os.pipe()
            elif isinstance(stdin, int):
                p2cread = stdin
            else:
                # Assuming file-like object
                p2cread = stdin.fileno()

            if stdout is None:
                pass
            elif stdout == PIPE:
                c2pread, c2pwrite = os.pipe()
            elif isinstance(stdout, int):
                c2pwrite = stdout
            else:
                # Assuming file-like object
                c2pwrite = stdout.fileno()

            if stderr is None:
                pass
            elif stderr == PIPE:
                errread, errwrite = os.pipe()
            elif stderr == STDOUT:
                errwrite = c2pwrite
            elif isinstance(stderr, int):
                errwrite = stderr
            else:
                # Assuming file-like object
                errwrite = stderr.fileno()

            return (p2cread, p2cwrite,
                    c2pread, c2pwrite,
                    errread, errwrite)


        def _set_cloexec_flag(self, fd):
            try:
                cloexec_flag = fcntl.FD_CLOEXEC
            except AttributeError:
                cloexec_flag = 1

            old = fcntl.fcntl(fd, fcntl.F_GETFD)
            fcntl.fcntl(fd, fcntl.F_SETFD, old | cloexec_flag)


        def _close_fds(self, but):
            for i in xrange(3, MAXFD):
                if i == but:
                    continue
                try:
                    os.close(i)
                except:
                    pass


        def _read_no_intr(self, fd, buffersize):
            """Like os.read, but retries on EINTR"""
            while True:
                try:
                    return os.read(fd, buffersize)
                except OSError, e:
                    if e.errno == errno.EINTR:
                        continue
                    else:
                        raise


        def _write_no_intr(self, fd, s):
            """Like os.write, but retries on EINTR"""
            while True:
                try:
                    return os.write(fd, s)
                except OSError, e:
                    if e.errno == errno.EINTR:
                        continue
                    else:
                        raise

        def _waitpid_no_intr(self, pid, options):
            """Like os.waitpid, but retries on EINTR"""
            while True:
                try:
                    return os.waitpid(pid, options)
                except OSError, e:
                    if e.errno == errno.EINTR:
                        continue
                    else:
                        raise

        def _fo_read_no_intr(self, obj):
            """Like obj.read(), but retries on EINTR"""
            while True:
                try:
                    return obj.read()
                except IOError, e:
                    if e.errno == errno.EINTR:
                        continue
                    else:
                        raise

        def _fo_write_no_intr(self, obj, data):
            """Like obj.write(), but retries on EINTR"""
            while True:
                try:
                    return obj.write(data)
                except IOError, e:
                    if e.errno == errno.EINTR:
                        continue
                    else:
                        raise

        def _execute_child(self, args, executable, preexec_fn, close_fds,
                           cwd, env, universal_newlines,
                           startupinfo, creationflags, shell,
                           p2cread, p2cwrite,
                           c2pread, c2pwrite,
                           errread, errwrite):
            """Execute program (POSIX version)"""

            if isinstance(args, types.StringTypes):
                args = [args]
            else:
                args = list(args)

            if shell:
                args = ["/bin/sh", "-c"] + args

            if executable is None:
                executable = args[0]

            # For transferring possible exec failure from child to parent
            # The first char specifies the exception type: 0 means
            # OSError, 1 means some other error.
            errpipe_read, errpipe_write = os.pipe()
            self._set_cloexec_flag(errpipe_write)

            gc_was_enabled = gc.isenabled()
            # Disable gc to avoid bug where gc -> file_dealloc ->
            # write to stderr -> hang.  http://bugs.python.org/issue1336
            gc.disable()
            try:
                self.pid = os.fork()
            except:
                if gc_was_enabled:
                    gc.enable()
                raise
            self._child_created = True
            if self.pid == 0:
                # Child
                try:
                    # Close parent's pipe ends
                    if p2cwrite:
                        os.close(p2cwrite)
                    if c2pread:
                        os.close(c2pread)
                    if errread:
                        os.close(errread)
                    os.close(errpipe_read)

                    # Dup fds for child
                    if p2cread:
                        os.dup2(p2cread, 0)
                    if c2pwrite:
                        os.dup2(c2pwrite, 1)
                    if errwrite:
                        os.dup2(errwrite, 2)

                    # Close pipe fds.  Make sure we don't close the same
                    # fd more than once, or standard fds.
                    if p2cread and p2cread not in (0,):
                        os.close(p2cread)
                    if c2pwrite and c2pwrite not in (p2cread, 1):
                        os.close(c2pwrite)
                    if errwrite and errwrite not in (p2cread, c2pwrite, 2):
                        os.close(errwrite)

                    # Close all other fds, if asked for
                    if close_fds:
                        self._close_fds(but=errpipe_write)

                    if cwd is not None:
                        os.chdir(cwd)

                    if preexec_fn:
                        apply(preexec_fn)

                    if env is None:
                        os.execvp(executable, args)
                    else:
                        os.execvpe(executable, args, env)

                except:
                    exc_type, exc_value, tb = sys.exc_info()
                    # Save the traceback and attach it to the exception object
                    exc_lines = traceback.format_exception(exc_type,
                                                           exc_value,
                                                           tb)
                    exc_value.child_traceback = ''.join(exc_lines)
                    self._write_no_intr(errpipe_write, pickle.dumps(exc_value))

                # This exitcode won't be reported to applications, so it
                # really doesn't matter what we return.
                os._exit(255)

            # Parent
            if gc_was_enabled:
                gc.enable()
            os.close(errpipe_write)
            if p2cread and p2cwrite:
                os.close(p2cread)
            if c2pwrite and c2pread:
                os.close(c2pwrite)
            if errwrite and errread:
                os.close(errwrite)

            # Wait for exec to fail or succeed; possibly raising exception
            data = self._read_no_intr(errpipe_read, 1048576) # Exceptions limited to 1 MB
            os.close(errpipe_read)
            if data != "":
                self._waitpid_no_intr(self.pid, 0)
                child_exception = pickle.loads(data)
                raise child_exception


        def _handle_exitstatus(self, sts):
            if os.WIFSIGNALED(sts):
                self.returncode = -os.WTERMSIG(sts)
            elif os.WIFEXITED(sts):
                self.returncode = os.WEXITSTATUS(sts)
            else:
                # Should never happen
                raise RuntimeError("Unknown child exit status!")


        def _internal_poll(self, _deadstate=None):
            """Check if child process has terminated.  Returns returncode
            attribute."""
            if self.returncode is None:
                try:
                    pid, sts = self._waitpid_no_intr(self.pid, os.WNOHANG)
                    if pid == self.pid:
                        self._handle_exitstatus(sts)
                except os.error:
                    if _deadstate is not None:
                        self.returncode = _deadstate
            return self.returncode


        def wait(self):
            """Wait for child process to terminate.  Returns returncode
            attribute."""
            if self.returncode is None:
                pid, sts = self._waitpid_no_intr(self.pid, 0)
                self._handle_exitstatus(sts)
            return self.returncode


        def _communicate(self, input):
            read_set = []
            write_set = []
            stdout = None # Return
            stderr = None # Return

            if self.stdin:
                # Flush stdio buffer.  This might block, if the user has
                # been writing to .stdin in an uncontrolled fashion.
                self.stdin.flush()
                if input:
                    write_set.append(self.stdin)
                else:
                    self.stdin.close()
            if self.stdout:
                read_set.append(self.stdout)
                stdout = []
            if self.stderr:
                read_set.append(self.stderr)
                stderr = []

            input_offset = 0
            while read_set or write_set:
                try:
                    rlist, wlist, xlist = select.select(read_set, write_set, [])
                except select.error, e:
                    if e.args[0] == errno.EINTR:
                        continue
                    raise

                if self.stdin in wlist:
                    # When select has indicated that the file is writable,
                    # we can write up to PIPE_BUF bytes without risk
                    # blocking.  POSIX defines PIPE_BUF >= 512
                    bytes_written = self._write_no_intr(self.stdin.fileno(), buffer(input, input_offset, 512))
                    input_offset += bytes_written
                    if input_offset >= len(input):
                        self.stdin.close()
                        write_set.remove(self.stdin)

                if self.stdout in rlist:
                    data = self._read_no_intr(self.stdout.fileno(), 1024)
                    if data == "":
                        self.stdout.close()
                        read_set.remove(self.stdout)
                    stdout.append(data)

                if self.stderr in rlist:
                    data = self._read_no_intr(self.stderr.fileno(), 1024)
                    if data == "":
                        self.stderr.close()
                        read_set.remove(self.stderr)
                    stderr.append(data)

            # All data exchanged.  Translate lists into strings.
            if stdout is not None:
                stdout = ''.join(stdout)
            if stderr is not None:
                stderr = ''.join(stderr)

            # Translate newlines, if requested.  We cannot let the file
            # object do the translation: It is based on stdio, which is
            # impossible to combine with select (unless forcing no
            # buffering).
            if self.universal_newlines and hasattr(file, 'newlines'):
                if stdout:
                    stdout = self._translate_newlines(stdout)
                if stderr:
                    stderr = self._translate_newlines(stderr)

            self.wait()
            return (stdout, stderr)


def _demo_posix():
    #
    # Example 1: Simple redirection: Get process list
    #
    plist = Popen(["ps"], stdout=PIPE).communicate()[0]
    print "Process list:"
    print plist

    #
    # Example 2: Change uid before executing child
    #
    if os.getuid() == 0:
        p = Popen(["id"], preexec_fn=lambda: os.setuid(100))
        p.wait()

    #
    # Example 3: Connecting several subprocesses
    #
    print "Looking for 'hda'..."
    p1 = Popen(["dmesg"], stdout=PIPE)
    p2 = Popen(["grep", "hda"], stdin=p1.stdout, stdout=PIPE)
    print repr(p2.communicate()[0])

    #
    # Example 4: Catch execution error
    #
    print
    print "Trying a weird file..."
    try:
        print Popen(["/this/path/does/not/exist"]).communicate()
    except OSError, e:
        if e.errno == errno.ENOENT:
            print "The file didn't exist.  I thought so..."
            print "Child traceback:"
            print e.child_traceback
        else:
            print "Error", e.errno
    else:
        print >>sys.stderr, "Gosh.  No error."


def _demo_windows():
    #
    # Example 1: Connecting several subprocesses
    #
    print "Looking for 'PROMPT' in set output..."
    p1 = Popen("set", stdout=PIPE, shell=True)
    p2 = Popen('find "PROMPT"', stdin=p1.stdout, stdout=PIPE)
    print repr(p2.communicate()[0])

    #
    # Example 2: Simple execution of program
    #
    print "Executing calc..."
    p = Popen("calc")
    p.wait()


if __name__ == "__main__":
    if mswindows:
        _demo_windows()
    else:
        _demo_posix()

########NEW FILE########
__FILENAME__ = zipfile
"""
Read and write ZIP files.
"""
import struct, os, time, sys, shutil
import binascii, cStringIO

try:
    import zlib # We may need its compression method
    crc32 = zlib.crc32
except ImportError:
    zlib = None
    crc32 = binascii.crc32

__all__ = ["BadZipfile", "error", "ZIP_STORED", "ZIP_DEFLATED", "is_zipfile",
           "ZipInfo", "ZipFile", "PyZipFile", "LargeZipFile" ]

class BadZipfile(Exception):
    pass


class LargeZipFile(Exception):
    """
    Raised when writing a zipfile, the zipfile requires ZIP64 extensions
    and those extensions are disabled.
    """

error = BadZipfile      # The exception raised by this module

ZIP64_LIMIT= (1L << 31L) - 1L
ZIP_FILECOUNT_LIMIT = 1 << 16
ZIP_MAX_COMMENT = (1 << 16) - 1

# constants for Zip file compression methods
ZIP_STORED = 0
ZIP_DEFLATED = 8
# Other ZIP compression methods not supported

# Below are some formats and associated data for reading/writing headers using
# the struct module.  The names and structures of headers/records are those used
# in the PKWARE description of the ZIP file format:
#     http://www.pkware.com/documents/casestudies/APPNOTE.TXT
# (URL valid as of January 2008)

# The "end of central directory" structure, magic number, size, and indices
# (section V.I in the format document)
structEndArchive = "<4s4H2LH"
stringEndArchive = "PK\005\006"
sizeEndCentDir = struct.calcsize(structEndArchive)

_ECD_SIGNATURE = 0
_ECD_DISK_NUMBER = 1
_ECD_DISK_START = 2
_ECD_ENTRIES_THIS_DISK = 3
_ECD_ENTRIES_TOTAL = 4
_ECD_SIZE = 5
_ECD_OFFSET = 6
_ECD_COMMENT_SIZE = 7
# These last two indices are not part of the structure as defined in the
# spec, but they are used internally by this module as a convenience
_ECD_COMMENT = 8
_ECD_LOCATION = 9

# The "central directory" structure, magic number, size, and indices
# of entries in the structure (section V.F in the format document)
structCentralDir = "<4s4B4HL2L5H2L"
stringCentralDir = "PK\001\002"
sizeCentralDir = struct.calcsize(structCentralDir)

# indexes of entries in the central directory structure
_CD_SIGNATURE = 0
_CD_CREATE_VERSION = 1
_CD_CREATE_SYSTEM = 2
_CD_EXTRACT_VERSION = 3
_CD_EXTRACT_SYSTEM = 4
_CD_FLAG_BITS = 5
_CD_COMPRESS_TYPE = 6
_CD_TIME = 7
_CD_DATE = 8
_CD_CRC = 9
_CD_COMPRESSED_SIZE = 10
_CD_UNCOMPRESSED_SIZE = 11
_CD_FILENAME_LENGTH = 12
_CD_EXTRA_FIELD_LENGTH = 13
_CD_COMMENT_LENGTH = 14
_CD_DISK_NUMBER_START = 15
_CD_INTERNAL_FILE_ATTRIBUTES = 16
_CD_EXTERNAL_FILE_ATTRIBUTES = 17
_CD_LOCAL_HEADER_OFFSET = 18

# The "local file header" structure, magic number, size, and indices
# (section V.A in the format document)
structFileHeader = "<4s2B4HL2L2H"
stringFileHeader = "PK\003\004"
sizeFileHeader = struct.calcsize(structFileHeader)

_FH_SIGNATURE = 0
_FH_EXTRACT_VERSION = 1
_FH_EXTRACT_SYSTEM = 2
_FH_GENERAL_PURPOSE_FLAG_BITS = 3
_FH_COMPRESSION_METHOD = 4
_FH_LAST_MOD_TIME = 5
_FH_LAST_MOD_DATE = 6
_FH_CRC = 7
_FH_COMPRESSED_SIZE = 8
_FH_UNCOMPRESSED_SIZE = 9
_FH_FILENAME_LENGTH = 10
_FH_EXTRA_FIELD_LENGTH = 11

# The "Zip64 end of central directory locator" structure, magic number, and size
structEndArchive64Locator = "<4sLQL"
stringEndArchive64Locator = "PK\x06\x07"
sizeEndCentDir64Locator = struct.calcsize(structEndArchive64Locator)

# The "Zip64 end of central directory" record, magic number, size, and indices
# (section V.G in the format document)
structEndArchive64 = "<4sQ2H2L4Q"
stringEndArchive64 = "PK\x06\x06"
sizeEndCentDir64 = struct.calcsize(structEndArchive64)

_CD64_SIGNATURE = 0
_CD64_DIRECTORY_RECSIZE = 1
_CD64_CREATE_VERSION = 2
_CD64_EXTRACT_VERSION = 3
_CD64_DISK_NUMBER = 4
_CD64_DISK_NUMBER_START = 5
_CD64_NUMBER_ENTRIES_THIS_DISK = 6
_CD64_NUMBER_ENTRIES_TOTAL = 7
_CD64_DIRECTORY_SIZE = 8
_CD64_OFFSET_START_CENTDIR = 9

def is_zipfile(filename):
    """Quickly see if file is a ZIP file by checking the magic number."""
    try:
        fpin = open(filename, "rb")
        endrec = _EndRecData(fpin)
        fpin.close()
        if endrec:
            return True                 # file has correct magic number
    except IOError:
        pass
    return False

def _EndRecData64(fpin, offset, endrec):
    """
    Read the ZIP64 end-of-archive records and use that to update endrec
    """
    fpin.seek(offset - sizeEndCentDir64Locator, 2)
    data = fpin.read(sizeEndCentDir64Locator)
    sig, diskno, reloff, disks = struct.unpack(structEndArchive64Locator, data)
    if sig != stringEndArchive64Locator:
        return endrec

    if diskno != 0 or disks != 1:
        raise BadZipfile("zipfiles that span multiple disks are not supported")

    # Assume no 'zip64 extensible data'
    fpin.seek(offset - sizeEndCentDir64Locator - sizeEndCentDir64, 2)
    data = fpin.read(sizeEndCentDir64)
    sig, sz, create_version, read_version, disk_num, disk_dir, \
            dircount, dircount2, dirsize, diroffset = \
            struct.unpack(structEndArchive64, data)
    if sig != stringEndArchive64:
        return endrec

    # Update the original endrec using data from the ZIP64 record
    endrec[_ECD_SIGNATURE] = sig
    endrec[_ECD_DISK_NUMBER] = disk_num
    endrec[_ECD_DISK_START] = disk_dir
    endrec[_ECD_ENTRIES_THIS_DISK] = dircount
    endrec[_ECD_ENTRIES_TOTAL] = dircount2
    endrec[_ECD_SIZE] = dirsize
    endrec[_ECD_OFFSET] = diroffset
    return endrec


def _EndRecData(fpin):
    """Return data from the "End of Central Directory" record, or None.

    The data is a list of the nine items in the ZIP "End of central dir"
    record followed by a tenth item, the file seek offset of this record."""

    # Determine file size
    fpin.seek(0, 2)
    filesize = fpin.tell()

    # Check to see if this is ZIP file with no archive comment (the
    # "end of central directory" structure should be the last item in the
    # file if this is the case).
    fpin.seek(-sizeEndCentDir, 2)
    data = fpin.read()
    if data[0:4] == stringEndArchive and data[-2:] == "\000\000":
        # the signature is correct and there's no comment, unpack structure
        endrec = struct.unpack(structEndArchive, data)
        endrec=list(endrec)

        # Append a blank comment and record start offset
        endrec.append("")
        endrec.append(filesize - sizeEndCentDir)
        if endrec[_ECD_OFFSET] == 0xffffffffL:
            # the value for the "offset of the start of the central directory"
            # indicates that there is a "Zip64 end of central directory"
            # structure present, so go look for it
            return _EndRecData64(fpin, -sizeEndCentDir, endrec)

        return endrec

    # Either this is not a ZIP file, or it is a ZIP file with an archive
    # comment.  Search the end of the file for the "end of central directory"
    # record signature. The comment is the last item in the ZIP file and may be
    # up to 64K long.  It is assumed that the "end of central directory" magic
    # number does not appear in the comment.
    maxCommentStart = max(filesize - (1 << 16) - sizeEndCentDir, 0)
    fpin.seek(maxCommentStart, 0)
    data = fpin.read()
    start = data.rfind(stringEndArchive)
    if start >= 0:
        # found the magic number; attempt to unpack and interpret
        recData = data[start:start+sizeEndCentDir]
        endrec = list(struct.unpack(structEndArchive, recData))
        comment = data[start+sizeEndCentDir:]
        # check that comment length is correct
        if endrec[_ECD_COMMENT_SIZE] == len(comment):
            # Append the archive comment and start offset
            endrec.append(comment)
            endrec.append(maxCommentStart + start)
            if endrec[_ECD_OFFSET] == 0xffffffffL:
                # There is apparently a "Zip64 end of central directory"
                # structure present, so go look for it
                return _EndRecData64(fpin, start - filesize, endrec)
            return endrec

    # Unable to find a valid end of central directory structure
    return


class ZipInfo (object):
    """Class with attributes describing each file in the ZIP archive."""

    __slots__ = (
            'orig_filename',
            'filename',
            'date_time',
            'compress_type',
            'comment',
            'extra',
            'create_system',
            'create_version',
            'extract_version',
            'reserved',
            'flag_bits',
            'volume',
            'internal_attr',
            'external_attr',
            'header_offset',
            'CRC',
            'compress_size',
            'file_size',
            '_raw_time',
        )

    def __init__(self, filename="NoName", date_time=(1980,1,1,0,0,0)):
        self.orig_filename = filename   # Original file name in archive

        # Terminate the file name at the first null byte.  Null bytes in file
        # names are used as tricks by viruses in archives.
        null_byte = filename.find(chr(0))
        if null_byte >= 0:
            filename = filename[0:null_byte]
        # This is used to ensure paths in generated ZIP files always use
        # forward slashes as the directory separator, as required by the
        # ZIP format specification.
        if os.sep != "/" and os.sep in filename:
            filename = filename.replace(os.sep, "/")

        self.filename = filename        # Normalized file name
        self.date_time = date_time      # year, month, day, hour, min, sec
        # Standard values:
        self.compress_type = ZIP_STORED # Type of compression for the file
        self.comment = ""               # Comment for each file
        self.extra = ""                 # ZIP extra data
        if sys.platform == 'win32':
            self.create_system = 0          # System which created ZIP archive
        else:
            # Assume everything else is unix-y
            self.create_system = 3          # System which created ZIP archive
        self.create_version = 20        # Version which created ZIP archive
        self.extract_version = 20       # Version needed to extract archive
        self.reserved = 0               # Must be zero
        self.flag_bits = 0              # ZIP flag bits
        self.volume = 0                 # Volume number of file header
        self.internal_attr = 0          # Internal attributes
        self.external_attr = 0          # External file attributes
        # Other attributes are set by class ZipFile:
        # header_offset         Byte offset to the file header
        # CRC                   CRC-32 of the uncompressed file
        # compress_size         Size of the compressed file
        # file_size             Size of the uncompressed file

    def FileHeader(self):
        """Return the per-file header as a string."""
        dt = self.date_time
        dosdate = (dt[0] - 1980) << 9 | dt[1] << 5 | dt[2]
        dostime = dt[3] << 11 | dt[4] << 5 | (dt[5] // 2)
        if self.flag_bits & 0x08:
            # Set these to zero because we write them after the file data
            CRC = compress_size = file_size = 0
        else:
            CRC = self.CRC
            compress_size = self.compress_size
            file_size = self.file_size

        extra = self.extra

        if file_size > ZIP64_LIMIT or compress_size > ZIP64_LIMIT:
            # File is larger than what fits into a 4 byte integer,
            # fall back to the ZIP64 extension
            fmt = '<HHQQ'
            extra = extra + struct.pack(fmt,
                    1, struct.calcsize(fmt)-4, file_size, compress_size)
            file_size = 0xffffffffL
            compress_size = 0xffffffffL
            self.extract_version = max(45, self.extract_version)
            self.create_version = max(45, self.extract_version)

        filename, flag_bits = self._encodeFilenameFlags()
        header = struct.pack(structFileHeader, stringFileHeader,
                 self.extract_version, self.reserved, flag_bits,
                 self.compress_type, dostime, dosdate, CRC,
                 compress_size, file_size,
                 len(filename), len(extra))
        return header + filename + extra

    def _encodeFilenameFlags(self):
        if isinstance(self.filename, unicode):
            try:
                return self.filename.encode('ascii'), self.flag_bits
            except UnicodeEncodeError:
                return self.filename.encode('utf-8'), self.flag_bits | 0x800
        else:
            return self.filename, self.flag_bits

    def _decodeFilename(self):
        if self.flag_bits & 0x800:
            return self.filename.decode('utf-8')
        else:
            return self.filename

    def _decodeExtra(self):
        # Try to decode the extra field.
        extra = self.extra
        unpack = struct.unpack
        while extra:
            tp, ln = unpack('<HH', extra[:4])
            if tp == 1:
                if ln >= 24:
                    counts = unpack('<QQQ', extra[4:28])
                elif ln == 16:
                    counts = unpack('<QQ', extra[4:20])
                elif ln == 8:
                    counts = unpack('<Q', extra[4:12])
                elif ln == 0:
                    counts = ()
                else:
                    raise RuntimeError, "Corrupt extra field %s"%(ln,)

                idx = 0

                # ZIP64 extension (large files and/or large archives)
                if self.file_size in (0xffffffffffffffffL, 0xffffffffL):
                    self.file_size = counts[idx]
                    idx += 1

                if self.compress_size == 0xFFFFFFFFL:
                    self.compress_size = counts[idx]
                    idx += 1

                if self.header_offset == 0xffffffffL:
                    old = self.header_offset
                    self.header_offset = counts[idx]
                    idx+=1

            extra = extra[ln+4:]


class _ZipDecrypter:
    """Class to handle decryption of files stored within a ZIP archive.

    ZIP supports a password-based form of encryption. Even though known
    plaintext attacks have been found against it, it is still useful
    to be able to get data out of such a file.

    Usage:
        zd = _ZipDecrypter(mypwd)
        plain_char = zd(cypher_char)
        plain_text = map(zd, cypher_text)
    """

    def _GenerateCRCTable():
        """Generate a CRC-32 table.

        ZIP encryption uses the CRC32 one-byte primitive for scrambling some
        internal keys. We noticed that a direct implementation is faster than
        relying on binascii.crc32().
        """
        poly = 0xedb88320L
        table = [0] * 256
        for i in range(256):
            crc = i
            for j in range(8):
                if crc & 1:
                    crc = ((crc >> 1) & 0x7FFFFFFF) ^ poly
                else:
                    crc = ((crc >> 1) & 0x7FFFFFFF)
            table[i] = crc
        return table
    crctable = _GenerateCRCTable()

    def _crc32(self, ch, crc):
        """Compute the CRC32 primitive on one byte."""
        return ((crc >> 8) & 0xffffff) ^ self.crctable[(crc ^ ord(ch)) & 0xff]

    def __init__(self, pwd):
        self.key0 = 305419896
        self.key1 = 591751049
        self.key2 = 878082192
        for p in pwd:
            self._UpdateKeys(p)

    def _UpdateKeys(self, c):
        self.key0 = self._crc32(c, self.key0)
        self.key1 = (self.key1 + (self.key0 & 255)) & 4294967295
        self.key1 = (self.key1 * 134775813 + 1) & 4294967295
        self.key2 = self._crc32(chr((self.key1 >> 24) & 255), self.key2)

    def __call__(self, c):
        """Decrypt a single character."""
        c = ord(c)
        k = self.key2 | 2
        c = c ^ (((k * (k^1)) >> 8) & 255)
        c = chr(c)
        self._UpdateKeys(c)
        return c

class ZipExtFile:
    """File-like object for reading an archive member.
       Is returned by ZipFile.open().
    """

    def __init__(self, fileobj, zipinfo, decrypt=None):
        self.fileobj = fileobj
        self.decrypter = decrypt
        self.bytes_read = 0L
        self.rawbuffer = ''
        self.readbuffer = ''
        self.linebuffer = ''
        self.eof = False
        self.univ_newlines = False
        self.nlSeps = ("\n", )
        self.lastdiscard = ''

        self.compress_type = zipinfo.compress_type
        self.compress_size = zipinfo.compress_size

        self.closed  = False
        self.mode    = "r"
        self.name = zipinfo.filename

        # read from compressed files in 64k blocks
        self.compreadsize = 64*1024
        if self.compress_type == ZIP_DEFLATED:
            self.dc = zlib.decompressobj(-15)

    def set_univ_newlines(self, univ_newlines):
        self.univ_newlines = univ_newlines

        # pick line separator char(s) based on universal newlines flag
        self.nlSeps = ("\n", )
        if self.univ_newlines:
            self.nlSeps = ("\r\n", "\r", "\n")

    def __iter__(self):
        return self

    def next(self):
        nextline = self.readline()
        if not nextline:
            raise StopIteration()

        return nextline

    def close(self):
        self.closed = True

    def _checkfornewline(self):
        nl, nllen = -1, -1
        if self.linebuffer:
            # ugly check for cases where half of an \r\n pair was
            # read on the last pass, and the \r was discarded.  In this
            # case we just throw away the \n at the start of the buffer.
            if (self.lastdiscard, self.linebuffer[0]) == ('\r','\n'):
                self.linebuffer = self.linebuffer[1:]

            for sep in self.nlSeps:
                nl = self.linebuffer.find(sep)
                if nl >= 0:
                    nllen = len(sep)
                    return nl, nllen

        return nl, nllen

    def readline(self, size = -1):
        """Read a line with approx. size. If size is negative,
           read a whole line.
        """
        if size < 0:
            size = sys.maxint
        elif size == 0:
            return ''

        # check for a newline already in buffer
        nl, nllen = self._checkfornewline()

        if nl >= 0:
            # the next line was already in the buffer
            nl = min(nl, size)
        else:
            # no line break in buffer - try to read more
            size -= len(self.linebuffer)
            while nl < 0 and size > 0:
                buf = self.read(min(size, 100))
                if not buf:
                    break
                self.linebuffer += buf
                size -= len(buf)

                # check for a newline in buffer
                nl, nllen = self._checkfornewline()

            # we either ran out of bytes in the file, or
            # met the specified size limit without finding a newline,
            # so return current buffer
            if nl < 0:
                s = self.linebuffer
                self.linebuffer = ''
                return s

        buf = self.linebuffer[:nl]
        self.lastdiscard = self.linebuffer[nl:nl + nllen]
        self.linebuffer = self.linebuffer[nl + nllen:]

        # line is always returned with \n as newline char (except possibly
        # for a final incomplete line in the file, which is handled above).
        return buf + "\n"

    def readlines(self, sizehint = -1):
        """Return a list with all (following) lines. The sizehint parameter
        is ignored in this implementation.
        """
        result = []
        while True:
            line = self.readline()
            if not line: break
            result.append(line)
        return result

    def read(self, size = None):
        # act like file() obj and return empty string if size is 0
        if size == 0:
            return ''

        # determine read size
        bytesToRead = self.compress_size - self.bytes_read

        # adjust read size for encrypted files since the first 12 bytes
        # are for the encryption/password information
        if self.decrypter is not None:
            bytesToRead -= 12

        if size is not None and size >= 0:
            if self.compress_type == ZIP_STORED:
                lr = len(self.readbuffer)
                bytesToRead = min(bytesToRead, size - lr)
            elif self.compress_type == ZIP_DEFLATED:
                if len(self.readbuffer) > size:
                    # the user has requested fewer bytes than we've already
                    # pulled through the decompressor; don't read any more
                    bytesToRead = 0
                else:
                    # user will use up the buffer, so read some more
                    lr = len(self.rawbuffer)
                    bytesToRead = min(bytesToRead, self.compreadsize - lr)

        # avoid reading past end of file contents
        if bytesToRead + self.bytes_read > self.compress_size:
            bytesToRead = self.compress_size - self.bytes_read

        # try to read from file (if necessary)
        if bytesToRead > 0:
            bytes = self.fileobj.read(bytesToRead)
            self.bytes_read += len(bytes)
            self.rawbuffer += bytes

            # handle contents of raw buffer
            if self.rawbuffer:
                newdata = self.rawbuffer
                self.rawbuffer = ''

                # decrypt new data if we were given an object to handle that
                if newdata and self.decrypter is not None:
                    newdata = ''.join(map(self.decrypter, newdata))

                # decompress newly read data if necessary
                if newdata and self.compress_type == ZIP_DEFLATED:
                    newdata = self.dc.decompress(newdata)
                    self.rawbuffer = self.dc.unconsumed_tail
                    if self.eof and len(self.rawbuffer) == 0:
                        # we're out of raw bytes (both from the file and
                        # the local buffer); flush just to make sure the
                        # decompressor is done
                        newdata += self.dc.flush()
                        # prevent decompressor from being used again
                        self.dc = None

                self.readbuffer += newdata


        # return what the user asked for
        if size is None or len(self.readbuffer) <= size:
            bytes = self.readbuffer
            self.readbuffer = ''
        else:
            bytes = self.readbuffer[:size]
            self.readbuffer = self.readbuffer[size:]

        return bytes


class ZipFile:
    """ Class with methods to open, read, write, close, list zip files.

    z = ZipFile(file, mode="r", compression=ZIP_STORED, allowZip64=False)

    file: Either the path to the file, or a file-like object.
          If it is a path, the file will be opened and closed by ZipFile.
    mode: The mode can be either read "r", write "w" or append "a".
    compression: ZIP_STORED (no compression) or ZIP_DEFLATED (requires zlib).
    allowZip64: if True ZipFile will create files with ZIP64 extensions when
                needed, otherwise it will raise an exception when this would
                be necessary.

    """

    fp = None                   # Set here since __del__ checks it

    def __init__(self, file, mode="r", compression=ZIP_STORED, allowZip64=False):
        """Open the ZIP file with mode read "r", write "w" or append "a"."""
        if mode not in ("r", "w", "a"):
            raise RuntimeError('ZipFile() requires mode "r", "w", or "a"')

        if compression == ZIP_STORED:
            pass
        elif compression == ZIP_DEFLATED:
            if not zlib:
                raise RuntimeError,\
                      "Compression requires the (missing) zlib module"
        else:
            raise RuntimeError, "That compression method is not supported"

        self._allowZip64 = allowZip64
        self._didModify = False
        self.debug = 0  # Level of printing: 0 through 3
        self.NameToInfo = {}    # Find file info given name
        self.filelist = []      # List of ZipInfo instances for archive
        self.compression = compression  # Method of compression
        self.mode = key = mode.replace('b', '')[0]
        self.pwd = None
        self.comment = ''

        # Check if we were passed a file-like object
        if isinstance(file, basestring):
            self._filePassed = 0
            self.filename = file
            modeDict = {'r' : 'rb', 'w': 'wb', 'a' : 'r+b'}
            try:
                self.fp = open(file, modeDict[mode])
            except IOError:
                if mode == 'a':
                    mode = key = 'w'
                    self.fp = open(file, modeDict[mode])
                else:
                    raise
        else:
            self._filePassed = 1
            self.fp = file
            self.filename = getattr(file, 'name', None)

        if key == 'r':
            self._GetContents()
        elif key == 'w':
            pass
        elif key == 'a':
            try:                        # See if file is a zip file
                self._RealGetContents()
                # seek to start of directory and overwrite
                self.fp.seek(self.start_dir, 0)
            except BadZipfile:          # file is not a zip file, just append
                self.fp.seek(0, 2)
        else:
            if not self._filePassed:
                self.fp.close()
                self.fp = None
            raise RuntimeError, 'Mode must be "r", "w" or "a"'

    def _GetContents(self):
        """Read the directory, making sure we close the file if the format
        is bad."""
        try:
            self._RealGetContents()
        except BadZipfile:
            if not self._filePassed:
                self.fp.close()
                self.fp = None
            raise

    def _RealGetContents(self):
        """Read in the table of contents for the ZIP file."""
        fp = self.fp
        endrec = _EndRecData(fp)
        if not endrec:
            raise BadZipfile, "File is not a zip file"
        if self.debug > 1:
            print endrec
        size_cd = endrec[_ECD_SIZE]             # bytes in central directory
        offset_cd = endrec[_ECD_OFFSET]         # offset of central directory
        self.comment = endrec[_ECD_COMMENT]     # archive comment

        # "concat" is zero, unless zip was concatenated to another file
        concat = endrec[_ECD_LOCATION] - size_cd - offset_cd
        if endrec[_ECD_SIGNATURE] == stringEndArchive64:
            # If Zip64 extension structures are present, account for them
            concat -= (sizeEndCentDir64 + sizeEndCentDir64Locator)

        if self.debug > 2:
            inferred = concat + offset_cd
            print "given, inferred, offset", offset_cd, inferred, concat
        # self.start_dir:  Position of start of central directory
        self.start_dir = offset_cd + concat
        fp.seek(self.start_dir, 0)
        data = fp.read(size_cd)
        fp = cStringIO.StringIO(data)
        total = 0
        while total < size_cd:
            centdir = fp.read(sizeCentralDir)
            if centdir[0:4] != stringCentralDir:
                raise BadZipfile, "Bad magic number for central directory"
            centdir = struct.unpack(structCentralDir, centdir)
            if self.debug > 2:
                print centdir
            filename = fp.read(centdir[_CD_FILENAME_LENGTH])
            # Create ZipInfo instance to store file information
            x = ZipInfo(filename)
            x.extra = fp.read(centdir[_CD_EXTRA_FIELD_LENGTH])
            x.comment = fp.read(centdir[_CD_COMMENT_LENGTH])
            x.header_offset = centdir[_CD_LOCAL_HEADER_OFFSET]
            (x.create_version, x.create_system, x.extract_version, x.reserved,
                x.flag_bits, x.compress_type, t, d,
                x.CRC, x.compress_size, x.file_size) = centdir[1:12]
            x.volume, x.internal_attr, x.external_attr = centdir[15:18]
            # Convert date/time code to (year, month, day, hour, min, sec)
            x._raw_time = t
            x.date_time = ( (d>>9)+1980, (d>>5)&0xF, d&0x1F,
                                     t>>11, (t>>5)&0x3F, (t&0x1F) * 2 )

            x._decodeExtra()
            x.header_offset = x.header_offset + concat
            x.filename = x._decodeFilename()
            self.filelist.append(x)
            self.NameToInfo[x.filename] = x

            # update total bytes read from central directory
            total = (total + sizeCentralDir + centdir[_CD_FILENAME_LENGTH]
                     + centdir[_CD_EXTRA_FIELD_LENGTH]
                     + centdir[_CD_COMMENT_LENGTH])

            if self.debug > 2:
                print "total", total


    def namelist(self):
        """Return a list of file names in the archive."""
        l = []
        for data in self.filelist:
            l.append(data.filename)
        return l

    def infolist(self):
        """Return a list of class ZipInfo instances for files in the
        archive."""
        return self.filelist

    def printdir(self):
        """Print a table of contents for the zip file."""
        print "%-46s %19s %12s" % ("File Name", "Modified    ", "Size")
        for zinfo in self.filelist:
            date = "%d-%02d-%02d %02d:%02d:%02d" % zinfo.date_time[:6]
            print "%-46s %s %12d" % (zinfo.filename, date, zinfo.file_size)

    def testzip(self):
        """Read all the files and check the CRC."""
        chunk_size = 2 ** 20
        for zinfo in self.filelist:
            try:
                # Read by chunks, to avoid an OverflowError or a
                # MemoryError with very large embedded files.
                f = self.open(zinfo.filename, "r")
                while f.read(chunk_size):     # Check CRC-32
                    pass
            except BadZipfile:
                return zinfo.filename

    def getinfo(self, name):
        """Return the instance of ZipInfo given 'name'."""
        info = self.NameToInfo.get(name)
        if info is None:
            raise KeyError(
                'There is no item named %r in the archive' % name)

        return info

    def setpassword(self, pwd):
        """Set default password for encrypted files."""
        self.pwd = pwd

    def read(self, name, pwd=None):
        """Return file bytes (as a string) for name."""
        return self.open(name, "r", pwd).read()

    def open(self, name, mode="r", pwd=None):
        """Return file-like object for 'name'."""
        if mode not in ("r", "U", "rU"):
            raise RuntimeError, 'open() requires mode "r", "U", or "rU"'
        if not self.fp:
            raise RuntimeError, \
                  "Attempt to read ZIP archive that was already closed"

        # Only open a new file for instances where we were not
        # given a file object in the constructor
        if self._filePassed:
            zef_file = self.fp
        else:
            zef_file = open(self.filename, 'rb')

        # Make sure we have an info object
        if isinstance(name, ZipInfo):
            # 'name' is already an info object
            zinfo = name
        else:
            # Get info object for name
            zinfo = self.getinfo(name)

        zef_file.seek(zinfo.header_offset, 0)

        # Skip the file header:
        fheader = zef_file.read(sizeFileHeader)
        if fheader[0:4] != stringFileHeader:
            raise BadZipfile, "Bad magic number for file header"

        fheader = struct.unpack(structFileHeader, fheader)
        fname = zef_file.read(fheader[_FH_FILENAME_LENGTH])
        if fheader[_FH_EXTRA_FIELD_LENGTH]:
            zef_file.read(fheader[_FH_EXTRA_FIELD_LENGTH])

        if fname != zinfo.orig_filename:
            raise BadZipfile, \
                      'File name in directory "%s" and header "%s" differ.' % (
                          zinfo.orig_filename, fname)

        # check for encrypted flag & handle password
        is_encrypted = zinfo.flag_bits & 0x1
        zd = None
        if is_encrypted:
            if not pwd:
                pwd = self.pwd
            if not pwd:
                raise RuntimeError, "File %s is encrypted, " \
                      "password required for extraction" % name

            zd = _ZipDecrypter(pwd)
            # The first 12 bytes in the cypher stream is an encryption header
            #  used to strengthen the algorithm. The first 11 bytes are
            #  completely random, while the 12th contains the MSB of the CRC,
            #  or the MSB of the file time depending on the header type
            #  and is used to check the correctness of the password.
            bytes = zef_file.read(12)
            h = map(zd, bytes[0:12])
            if zinfo.flag_bits & 0x8:
                # compare against the file type from extended local headers
                check_byte = (zinfo._raw_time >> 8) & 0xff
            else:
                # compare against the CRC otherwise
                check_byte = (zinfo.CRC >> 24) & 0xff
            if ord(h[11]) != check_byte:
                raise RuntimeError("Bad password for file", name)

        # build and return a ZipExtFile
        if zd is None:
            zef = ZipExtFile(zef_file, zinfo)
        else:
            zef = ZipExtFile(zef_file, zinfo, zd)

        # set universal newlines on ZipExtFile if necessary
        if "U" in mode:
            zef.set_univ_newlines(True)
        return zef

    def extract(self, member, path=None, pwd=None):
        """Extract a member from the archive to the current working directory,
           using its full name. Its file information is extracted as accurately
           as possible. `member' may be a filename or a ZipInfo object. You can
           specify a different directory using `path'.
        """
        if not isinstance(member, ZipInfo):
            member = self.getinfo(member)

        if path is None:
            path = os.getcwd()

        return self._extract_member(member, path, pwd)

    def extractall(self, path=None, members=None, pwd=None):
        """Extract all members from the archive to the current working
           directory. `path' specifies a different directory to extract to.
           `members' is optional and must be a subset of the list returned
           by namelist().
        """
        if members is None:
            members = self.namelist()

        for zipinfo in members:
            self.extract(zipinfo, path, pwd)

    def _extract_member(self, member, targetpath, pwd):
        """Extract the ZipInfo object 'member' to a physical
           file on the path targetpath.
        """
        # build the destination pathname, replacing
        # forward slashes to platform specific separators.
        if targetpath[-1:] == "/":
            targetpath = targetpath[:-1]

        # don't include leading "/" from file name if present
        if os.path.isabs(member.filename):
            targetpath = os.path.join(targetpath, member.filename[1:])
        else:
            targetpath = os.path.join(targetpath, member.filename)

        targetpath = os.path.normpath(targetpath)

        # Create all upper directories if necessary.
        upperdirs = os.path.dirname(targetpath)
        if upperdirs and not os.path.exists(upperdirs):
            os.makedirs(upperdirs)

        source = self.open(member, pwd=pwd)
        target = file(targetpath, "wb")
        shutil.copyfileobj(source, target)
        source.close()
        target.close()

        return targetpath

    def _writecheck(self, zinfo):
        """Check for errors before writing a file to the archive."""
        if zinfo.filename in self.NameToInfo:
            if self.debug:      # Warning for duplicate names
                print "Duplicate name:", zinfo.filename
        if self.mode not in ("w", "a"):
            raise RuntimeError, 'write() requires mode "w" or "a"'
        if not self.fp:
            raise RuntimeError, \
                  "Attempt to write ZIP archive that was already closed"
        if zinfo.compress_type == ZIP_DEFLATED and not zlib:
            raise RuntimeError, \
                  "Compression requires the (missing) zlib module"
        if zinfo.compress_type not in (ZIP_STORED, ZIP_DEFLATED):
            raise RuntimeError, \
                  "That compression method is not supported"
        if zinfo.file_size > ZIP64_LIMIT:
            if not self._allowZip64:
                raise LargeZipFile("Filesize would require ZIP64 extensions")
        if zinfo.header_offset > ZIP64_LIMIT:
            if not self._allowZip64:
                raise LargeZipFile("Zipfile size would require ZIP64 extensions")

    def write(self, filename, arcname=None, compress_type=None):
        """Put the bytes from filename into the archive under the name
        arcname."""
        if not self.fp:
            raise RuntimeError(
                  "Attempt to write to ZIP archive that was already closed")

        st = os.stat(filename)
        mtime = time.localtime(st.st_mtime)
        date_time = mtime[0:6]
        # Create ZipInfo instance to store file information
        if arcname is None:
            arcname = filename
        arcname = os.path.normpath(os.path.splitdrive(arcname)[1])
        while arcname[0] in (os.sep, os.altsep):
            arcname = arcname[1:]
        zinfo = ZipInfo(arcname, date_time)
        zinfo.external_attr = (st[0] & 0xFFFF) << 16L      # Unix attributes
        if compress_type is None:
            zinfo.compress_type = self.compression
        else:
            zinfo.compress_type = compress_type

        zinfo.file_size = st.st_size
        zinfo.flag_bits = 0x00
        zinfo.header_offset = self.fp.tell()    # Start of header bytes

        self._writecheck(zinfo)
        self._didModify = True
        fp = open(filename, "rb")
        # Must overwrite CRC and sizes with correct data later
        zinfo.CRC = CRC = 0
        zinfo.compress_size = compress_size = 0
        zinfo.file_size = file_size = 0
        self.fp.write(zinfo.FileHeader())
        if zinfo.compress_type == ZIP_DEFLATED:
            cmpr = zlib.compressobj(zlib.Z_BEST_SPEED,
                 zlib.DEFLATED, -15)
        else:
            cmpr = None
        while 1:
            buf = fp.read(1024 * 8)
            if not buf:
                break
            file_size = file_size + len(buf)
            CRC = crc32(buf, CRC) & 0xffffffffL
            if cmpr:
                buf = cmpr.compress(buf)
                compress_size = compress_size + len(buf)
            self.fp.write(buf)
        fp.close()
        if cmpr:
            buf = cmpr.flush()
            compress_size = compress_size + len(buf)
            self.fp.write(buf)
            zinfo.compress_size = compress_size
        else:
            zinfo.compress_size = file_size
        zinfo.CRC = CRC
        zinfo.file_size = file_size
        # Seek backwards and write CRC and file sizes
        position = self.fp.tell()       # Preserve current position in file
        self.fp.seek(zinfo.header_offset + 14, 0)
        self.fp.write(struct.pack("<LLL", zinfo.CRC, zinfo.compress_size,
              zinfo.file_size))
        self.fp.seek(position, 0)
        self.filelist.append(zinfo)
        self.NameToInfo[zinfo.filename] = zinfo

    def writestr(self, zinfo_or_arcname, bytes):
        """Write a file into the archive.  The contents is the string
        'bytes'.  'zinfo_or_arcname' is either a ZipInfo instance or
        the name of the file in the archive."""
        if not isinstance(zinfo_or_arcname, ZipInfo):
            zinfo = ZipInfo(filename=zinfo_or_arcname,
                            date_time=time.localtime(time.time())[:6])
            zinfo.compress_type = self.compression
            zinfo.external_attr = 0600 << 16
        else:
            zinfo = zinfo_or_arcname

        if not self.fp:
            raise RuntimeError(
                  "Attempt to write to ZIP archive that was already closed")

        zinfo.file_size = len(bytes)            # Uncompressed size
        zinfo.header_offset = self.fp.tell()    # Start of header bytes
        self._writecheck(zinfo)
        self._didModify = True
        zinfo.CRC = crc32(bytes) & 0xffffffff       # CRC-32 checksum
        if zinfo.compress_type == ZIP_DEFLATED:
            co = zlib.compressobj(zlib.Z_BEST_SPEED,
                 zlib.DEFLATED, -15)
            bytes = co.compress(bytes) + co.flush()
            zinfo.compress_size = len(bytes)    # Compressed size
        else:
            zinfo.compress_size = zinfo.file_size
        zinfo.header_offset = self.fp.tell()    # Start of header bytes
        self.fp.write(zinfo.FileHeader())
        self.fp.write(bytes)
        self.fp.flush()
        if zinfo.flag_bits & 0x08:
            # Write CRC and file sizes after the file data
            self.fp.write(struct.pack("<lLL", zinfo.CRC, zinfo.compress_size,
                  zinfo.file_size))
        self.filelist.append(zinfo)
        self.NameToInfo[zinfo.filename] = zinfo

    def __del__(self):
        """Call the "close()" method in case the user forgot."""
        self.close()

    def close(self):
        """Close the file, and for mode "w" and "a" write the ending
        records."""
        if self.fp is None:
            return

        if self.mode in ("w", "a") and self._didModify: # write ending records
            count = 0
            pos1 = self.fp.tell()
            for zinfo in self.filelist:         # write central directory
                count = count + 1
                dt = zinfo.date_time
                dosdate = (dt[0] - 1980) << 9 | dt[1] << 5 | dt[2]
                dostime = dt[3] << 11 | dt[4] << 5 | (dt[5] // 2)
                extra = []
                if zinfo.file_size > ZIP64_LIMIT \
                        or zinfo.compress_size > ZIP64_LIMIT:
                    extra.append(zinfo.file_size)
                    extra.append(zinfo.compress_size)
                    file_size = 0xffffffff
                    compress_size = 0xffffffff
                else:
                    file_size = zinfo.file_size
                    compress_size = zinfo.compress_size

                if zinfo.header_offset > ZIP64_LIMIT:
                    extra.append(zinfo.header_offset)
                    header_offset = 0xffffffffL
                else:
                    header_offset = zinfo.header_offset

                extra_data = zinfo.extra
                if extra:
                    # Append a ZIP64 field to the extra's
                    extra_data = struct.pack(
                            '<HH' + 'Q'*len(extra),
                            1, 8*len(extra), *extra) + extra_data

                    extract_version = max(45, zinfo.extract_version)
                    create_version = max(45, zinfo.create_version)
                else:
                    extract_version = zinfo.extract_version
                    create_version = zinfo.create_version

                try:
                    filename, flag_bits = zinfo._encodeFilenameFlags()
                    centdir = struct.pack(structCentralDir,
                     stringCentralDir, create_version,
                     zinfo.create_system, extract_version, zinfo.reserved,
                     flag_bits, zinfo.compress_type, dostime, dosdate,
                     zinfo.CRC, compress_size, file_size,
                     len(filename), len(extra_data), len(zinfo.comment),
                     0, zinfo.internal_attr, zinfo.external_attr,
                     header_offset)
                except DeprecationWarning:
                    print >>sys.stderr, (structCentralDir,
                     stringCentralDir, create_version,
                     zinfo.create_system, extract_version, zinfo.reserved,
                     zinfo.flag_bits, zinfo.compress_type, dostime, dosdate,
                     zinfo.CRC, compress_size, file_size,
                     len(zinfo.filename), len(extra_data), len(zinfo.comment),
                     0, zinfo.internal_attr, zinfo.external_attr,
                     header_offset)
                    raise
                self.fp.write(centdir)
                self.fp.write(filename)
                self.fp.write(extra_data)
                self.fp.write(zinfo.comment)

            pos2 = self.fp.tell()
            # Write end-of-zip-archive record
            centDirOffset = pos1
            if pos1 > ZIP64_LIMIT:
                # Need to write the ZIP64 end-of-archive records
                zip64endrec = struct.pack(
                        structEndArchive64, stringEndArchive64,
                        44, 45, 45, 0, 0, count, count, pos2 - pos1, pos1)
                self.fp.write(zip64endrec)

                zip64locrec = struct.pack(
                        structEndArchive64Locator,
                        stringEndArchive64Locator, 0, pos2, 1)
                self.fp.write(zip64locrec)
                centDirOffset = 0xFFFFFFFF

            # check for valid comment length
            if len(self.comment) >= ZIP_MAX_COMMENT:
                if self.debug > 0:
                    msg = 'Archive comment is too long; truncating to %d bytes' \
                          % ZIP_MAX_COMMENT
                self.comment = self.comment[:ZIP_MAX_COMMENT]

            endrec = struct.pack(structEndArchive, stringEndArchive,
                                 0, 0, count % ZIP_FILECOUNT_LIMIT,
                                 count % ZIP_FILECOUNT_LIMIT, pos2 - pos1,
                                 centDirOffset, len(self.comment))
            self.fp.write(endrec)
            self.fp.write(self.comment)
            self.fp.flush()

        if not self._filePassed:
            self.fp.close()
        self.fp = None


class PyZipFile(ZipFile):
    """Class to create ZIP archives with Python library files and packages."""

    def writepy(self, pathname, basename = ""):
        """Add all files from "pathname" to the ZIP archive.

        If pathname is a package directory, search the directory and
        all package subdirectories recursively for all *.py and enter
        the modules into the archive.  If pathname is a plain
        directory, listdir *.py and enter all modules.  Else, pathname
        must be a Python *.py file and the module will be put into the
        archive.  Added modules are always module.pyo or module.pyc.
        This method will compile the module.py into module.pyc if
        necessary.
        """
        dir, name = os.path.split(pathname)
        if os.path.isdir(pathname):
            initname = os.path.join(pathname, "__init__.py")
            if os.path.isfile(initname):
                # This is a package directory, add it
                if basename:
                    basename = "%s/%s" % (basename, name)
                else:
                    basename = name
                if self.debug:
                    print "Adding package in", pathname, "as", basename
                fname, arcname = self._get_codename(initname[0:-3], basename)
                if self.debug:
                    print "Adding", arcname
                self.write(fname, arcname)
                dirlist = os.listdir(pathname)
                dirlist.remove("__init__.py")
                # Add all *.py files and package subdirectories
                for filename in dirlist:
                    path = os.path.join(pathname, filename)
                    root, ext = os.path.splitext(filename)
                    if os.path.isdir(path):
                        if os.path.isfile(os.path.join(path, "__init__.py")):
                            # This is a package directory, add it
                            self.writepy(path, basename)  # Recursive call
                    elif ext == ".py":
                        fname, arcname = self._get_codename(path[0:-3],
                                         basename)
                        if self.debug:
                            print "Adding", arcname
                        self.write(fname, arcname)
            else:
                # This is NOT a package directory, add its files at top level
                if self.debug:
                    print "Adding files from directory", pathname
                for filename in os.listdir(pathname):
                    path = os.path.join(pathname, filename)
                    root, ext = os.path.splitext(filename)
                    if ext == ".py":
                        fname, arcname = self._get_codename(path[0:-3],
                                         basename)
                        if self.debug:
                            print "Adding", arcname
                        self.write(fname, arcname)
        else:
            if pathname[-3:] != ".py":
                raise RuntimeError, \
                      'Files added with writepy() must end with ".py"'
            fname, arcname = self._get_codename(pathname[0:-3], basename)
            if self.debug:
                print "Adding file", arcname
            self.write(fname, arcname)

    def _get_codename(self, pathname, basename):
        """Return (filename, archivename) for the path.

        Given a module name path, return the correct file path and
        archive name, compiling if necessary.  For example, given
        /python/lib/string, return (/python/lib/string.pyc, string).
        """
        file_py  = pathname + ".py"
        file_pyc = pathname + ".pyc"
        file_pyo = pathname + ".pyo"
        if os.path.isfile(file_pyo) and \
                            os.stat(file_pyo).st_mtime >= os.stat(file_py).st_mtime:
            fname = file_pyo    # Use .pyo file
        elif not os.path.isfile(file_pyc) or \
             os.stat(file_pyc).st_mtime < os.stat(file_py).st_mtime:
            import py_compile
            if self.debug:
                print "Compiling", file_py
            try:
                py_compile.compile(file_py, file_pyc, None, True)
            except py_compile.PyCompileError,err:
                print err.msg
            fname = file_pyc
        else:
            fname = file_pyc
        archivename = os.path.split(fname)[1]
        if basename:
            archivename = "%s/%s" % (basename, archivename)
        return (fname, archivename)


def main(args = None):
    import textwrap
    USAGE=textwrap.dedent("""\
        Usage:
            zipfile.py -l zipfile.zip        # Show listing of a zipfile
            zipfile.py -t zipfile.zip        # Test if a zipfile is valid
            zipfile.py -e zipfile.zip target # Extract zipfile into target dir
            zipfile.py -c zipfile.zip src ... # Create zipfile from sources
        """)
    if args is None:
        args = sys.argv[1:]

    if not args or args[0] not in ('-l', '-c', '-e', '-t'):
        print USAGE
        sys.exit(1)

    if args[0] == '-l':
        if len(args) != 2:
            print USAGE
            sys.exit(1)
        zf = ZipFile(args[1], 'r')
        zf.printdir()
        zf.close()

    elif args[0] == '-t':
        if len(args) != 2:
            print USAGE
            sys.exit(1)
        zf = ZipFile(args[1], 'r')
        zf.testzip()
        print "Done testing"

    elif args[0] == '-e':
        if len(args) != 3:
            print USAGE
            sys.exit(1)

        zf = ZipFile(args[1], 'r')
        out = args[2]
        for path in zf.namelist():
            if path.startswith('./'):
                tgt = os.path.join(out, path[2:])
            else:
                tgt = os.path.join(out, path)

            tgtdir = os.path.dirname(tgt)
            if not os.path.exists(tgtdir):
                os.makedirs(tgtdir)
            fp = open(tgt, 'wb')
            fp.write(zf.read(path))
            fp.close()
        zf.close()

    elif args[0] == '-c':
        if len(args) < 3:
            print USAGE
            sys.exit(1)

        def addToZip(zf, path, zippath):
            if os.path.isfile(path):
                zf.write(path, zippath, ZIP_DEFLATED)
            elif os.path.isdir(path):
                for nm in os.listdir(path):
                    addToZip(zf,
                            os.path.join(path, nm), os.path.join(zippath, nm))
            # else: ignore

        zf = ZipFile(args[1], 'w', allowZip64=True)
        for src in args[2:]:
            addToZip(zf, src, os.path.basename(src))

        zf.close()

if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = base
import os
import sys
import time
import errno
import logging
from holland.core.plugin import PluginLoadError, load_backup_plugin
from holland.core.util.path import directory_size, disk_free
from holland.core.util.fmt import format_bytes, format_interval

MAX_SPOOL_RETRIES = 5

LOG = logging.getLogger(__name__)

class BackupError(Exception):
    """Error during a backup"""

class BackupPlugin(object):
    def __init__(self, name, config, target_directory, dry_run=False):
        self.name = name
        self.config = config
        self.target_directory = target_directory
        self.dry_run = dry_run

    def estimate_backup_size(self):
        raise NotImplementedError()

    def backup(self):
        raise NotImplementedError()

    def info(self):
        raise NotImplementedError()

    def configspec(self):
        raise NotImplementedError()


def load_plugin(name, config, path, dry_run):
        try:
            plugin_cls = load_backup_plugin(config['holland:backup']['plugin'])
        except KeyError, exc:
            raise BackupError("No plugin defined for backupset '%s'.", name)
        except PluginLoadError, exc:
            raise BackupError(str(exc))


        try:
            return plugin_cls(name=name,
                              config=config,
                              target_directory=path,
                              dry_run=dry_run)
        except (KeyboardInterrupt, SystemExit):
            raise
        except Exception, exc:
            LOG.debug("Error while initializing %r : %s",
                      plugin_cls, exc, exc_info=True)
            raise BackupError("Error initializing %s plugin: %s" %
                              (config['holland:backup']['plugin'],
                               str(exc))
                             )


class BackupRunner(object):
    def __init__(self, spool):
        self.spool = spool
        self._registry = {}

    def register_cb(self, event, callback):
        self._registry.setdefault(event, []).append(callback)

    def apply_cb(self, event, *args, **kwargs):
        for callback in self._registry.get(event, []):
            try:
                callback(event, *args, **kwargs)
            except (KeyboardInterrupt, SystemExit):
                raise
            except:
                raise BackupError(str(sys.exc_info()[1]))

    def backup(self, name, config, dry_run=False):
        """Run a backup for the named backupset using the provided
        configuration

        :param name: name of the backupset
        :param config: dict-like object providing the backupset configuration

        :raises: BackupError if a backup fails
        """

        for i in xrange(MAX_SPOOL_RETRIES):
            try:
                spool_entry = self.spool.add_backup(name)
                break
            except OSError, exc:
                if exc.errno != errno.EEXIST:
                    raise BackupError("Failed to create spool: %s" % exc)
                sys.exc_clear()
                LOG.debug("Failed to create spool.  Retrying in %d seconds.", i+1)
                time.sleep(i+1)
        else:
            raise BackupError("Failed to create a new backup directory for %s" % name)

        spool_entry.config.merge(config)
        spool_entry.validate_config()

        if dry_run:
            # always purge the spool
            self.register_cb('post-backup',
                             lambda *args, **kwargs: spool_entry.purge())

        plugin = load_plugin(name,
                             spool_entry.config,
                             spool_entry.path,
                             dry_run)

        spool_entry.config['holland:backup']['start-time'] = time.time()
        spool_entry.flush()
        self.apply_cb('before-backup', spool_entry)

        try:
            estimated_size = self.check_available_space(plugin, spool_entry, dry_run)
            LOG.info("Starting backup[%s] via plugin %s",
                     spool_entry.name,
                     spool_entry.config['holland:backup']['plugin'])
            plugin.backup()
        except KeyboardInterrupt:
            LOG.warning("Backup aborted by interrupt")
            spool_entry.config['holland:backup']['failed'] = True
        except:
            spool_entry.config['holland:backup']['failed'] = True
        else:
            spool_entry.config['holland:backup']['failed'] = False

        spool_entry.config['holland:backup']['stop-time'] = time.time()
        if not dry_run and not spool_entry.config['holland:backup']['failed']:
            final_size = directory_size(spool_entry.path)
            LOG.info("Final on-disk backup size %s", format_bytes(final_size))
            if estimated_size > 0:
                LOG.info("%.2f%% of estimated size %s",
                     (float(final_size) / estimated_size)*100.0,
                     format_bytes(estimated_size))

            spool_entry.config['holland:backup']['on-disk-size'] = final_size
            spool_entry.flush()

        start_time = spool_entry.config['holland:backup']['start-time']
        stop_time = spool_entry.config['holland:backup']['stop-time']

        if spool_entry.config['holland:backup']['failed']:
            LOG.error("Backup failed after %s",
                      format_interval(stop_time - start_time))
        else:
            LOG.info("Backup completed in %s",
                     format_interval(stop_time - start_time))


        if dry_run:
            spool_entry.purge()

        if sys.exc_info() != (None, None, None):
            self.apply_cb('failed-backup', spool_entry)
            raise
        else:
            self.apply_cb('after-backup', spool_entry)

    def free_required_space(self, name, required_bytes, dry_run=False):
        """Attempt to free at least ``required_bytes`` of old backups from a backupset

        :param name: name of the backupset to free space from
        :param required_bytes: integer number of bytes required for the backupset path
        :param dry_run: if true, this will only generate log messages but won't actually free space
        :returns: bool; True if freed or False otherwise
        """
        LOG.info("Insufficient disk space for adjusted estimated backup size: %s",
                 format_bytes(required_bytes))
        LOG.info("purge-on-demand is enabled. Discovering old backups to purge.")
        available_bytes = disk_free(os.path.join(self.spool.path, name))
        to_purge = {}
        for backup in self.spool.list_backups(name):
            backup_size = directory_size(backup.path)
            LOG.info("Found backup '%s': %s",
                     backup.path, format_bytes(backup_size))
            available_bytes += backup_size
            to_purge[backup] = backup_size
            if available_bytes > required_bytes:
                break
        else:
            LOG.info("Purging would only recover an additional %s", 
                     format_bytes(sum(to_purge.values())))
            LOG.info("Only %s total would be available, but the current "
                     "backup requires %s",
                     format_bytes(available_bytes),
                     format_bytes(required_bytes))
            return False

        purge_bytes = sum(to_purge.values())
        LOG.info("Found %d backups to purge which will recover %s",
                 len(to_purge), format_bytes(purge_bytes))

        for backup in to_purge:
            if dry_run:
                LOG.info("Would purge: %s", backup.path)
            else:
                LOG.info("Purging: %s", backup.path)
                backup.purge()
        LOG.info("%s now has %s of available space",
                 os.path.join(self.spool.path, name),
                 format_bytes(disk_free(os.path.join(self.spool.path, name))))
        return True

    def check_available_space(self, plugin, spool_entry, dry_run=False):
        available_bytes = disk_free(spool_entry.path)

        estimated_bytes_required = plugin.estimate_backup_size()
        LOG.info("Estimated Backup Size: %s",
                 format_bytes(estimated_bytes_required))

        config = plugin.config['holland:backup']
        adjustment_factor = config['estimated-size-factor']
        adjusted_bytes_required = (estimated_bytes_required*adjustment_factor)

        if adjusted_bytes_required != estimated_bytes_required:
            LOG.info("Adjusting estimated size by %.2f to %s",
                     adjustment_factor,
                     format_bytes(adjusted_bytes_required))

        if available_bytes <= adjusted_bytes_required:
            if not (config['purge-on-demand'] and 
                    self.free_required_space(spool_entry.backupset,
                                         adjusted_bytes_required,
                                         dry_run)):
                msg = ("Insufficient Disk Space. %s required, "
                       "but only %s available on %s") % (
                       format_bytes(adjusted_bytes_required),
                       format_bytes(available_bytes),
                       self.spool.path)
                LOG.error(msg)
                if not dry_run:
                    raise BackupError(msg)
        return estimated_bytes_required

########NEW FILE########
__FILENAME__ = legacy
"""Holland Backup Module"""

import os
import sys
import time
import pprint
import logging
from holland.core.exceptions import BackupError
from holland.core.util.path import disk_free, directory_size
from holland.core.util.fmt import format_interval, format_bytes
from holland.core.plugin import load_backup_plugin, PluginLoadError
from holland.core.config import load_backupset_config
from holland.core.spool import spool

LOGGER = logging.getLogger(__name__)

def load_plugin(cfg):
    provider = cfg.lookup('holland:backup.plugin')

    LOGGER.info("Loading Backup Plugin '%s'", provider)
    if not provider:
        raise IOError("No provider defined")
    try:
        plugincls = load_first_entrypoint("holland.backup", provider)
    except PluginLoadError, e:
        raise LookupError("Failed to load plugin %r: %s" % (provider, e))

    if not plugincls:
        raise LookupError("Plugin %r not found" % ('holland.backup.' + provider))
    return plugincls

def _find_existing_parent(path):
    path = os.path.abspath(path)
    while not os.path.exists(path):
        path, _ = os.path.split(path)
        if _ == '':
            break

    return path 

def verify_space(required_space, target_directory):
    available_space = disk_free(_find_existing_parent(target_directory))
    if required_space >= available_space:
        LOGGER.error("Insufficient Disk Space.  Required: %s Available: %s", 
                    format_bytes(required_space), 
                    format_bytes(available_space))
        raise BackupError("%s required but only %s available on %s" % \
                            (format_bytes(required_space),
                            format_bytes(available_space),
                            target_directory))

def purge_old_backups(backupset, backups_to_keep=1, exclude=()):
    assert backups_to_keep > 0
    LOGGER.info("Purging old backups from backupset '%s'", backupset)
    backupset = spool.find_backupset(backupset)
    if not backupset:
        backups = []
    else:
        backups = [bk for bk in backupset.list_backups(reverse=True) 
                   if bk not in exclude]
    # Make sure we keep holland:backup.backups-to-keep
    LOGGER.info("Found %d backups.  Keeping %d", len(backups), backups_to_keep)
    purge_list = []
    for backup in backups:
        if backup.config.get('holland:backup',{}).get('stop-time', 0) == 0:
            LOGGER.debug("Purging broken backup")
            purge_list.insert(0, backup)
        elif backups_to_keep == 0:
            LOGGER.debug("Purging old backup")
            purge_list.insert(0, backup)
        else:
            LOGGER.debug("Retaining backup %s", backup.name)
            backups_to_keep -= 1

    if not purge_list:
        LOGGER.info("No backups to purge")
    else:
        for backup in purge_list:
            LOGGER.info("Purging %s", backup.name)
            backup.purge()

def backup(backupset_name, dry_run=False, skip_purge=False):


    # May raise a ConfigError if not backupset is found
    LOGGER.info("Loading config for backupset %s", backupset_name)
    try:
        backupset_cfg = load_backupset_config(backupset_name)
    except IOError, e:
        LOGGER.error("Failed to load backupset %s: %s", backupset_name, e)
        raise BackupError("Aborting due to previous errors.")
    except SyntaxError, e:
        LOGGER.error("Failed to load backupset config %r [%s]. %s", 
                        backupset_name, 
                        e.config.filename,
                        e
                    )
        LOGGER.error("Bad line appears to be '%s'", e.line)
        raise BackupError("Aborting due to previous errors.")

    # May raise a PluginError if the plugin could not be loaded
    LOGGER.info("Loading plugin %s", backupset_cfg.lookup('holland:backup.plugin'))
    try:
        plugincls = load_backup_plugin(backupset_cfg.lookup('holland:backup.plugin'))
    except PluginLoadError, e:
        LOGGER.error("Failed to load plugin %s: %s", 
                     backupset_cfg.lookup('holland:backup.plugin'), e)
        raise BackupError(e)
    
    # Possible IOError here if we cannot write to spool
    # Don't create the directory in dry-run mode
    backup_job = spool.add_backup(backupset_name)
    LOGGER.info("Prepared backup spool %s", backup_job.path)
    # Always merge in the backupset config to the backup-local config
    LOGGER.debug("Merging backupset config into local backup.conf config")
    backup_job.config.merge(backupset_cfg)
    backup_job.validate_config()

    # Plugin may fail to initialize due to programming error
    LOGGER.debug("Initializing backup plugin instance")
    try:
        plugin = plugincls(backupset_name, backup_job.config, backup_job.path, dry_run)
    except Exception, e:
        LOGGER.debug("Failed to instantiate backup plugin %s: %s",
                     backupset_cfg.lookup('holland:backup.plugin'),
                     e, exc_info=True)
        raise BackupError("Failed to initialize backup plugin %s: %s" % 
                          (backupset_cfg.lookup('holland:backup.plugin'), e))

    # Plugin may raise exception due to programming error, be careful
    estimated_size = plugin.estimate_backup_size()
    estimate_factor = backup_job.config['holland:backup']['estimated-size-factor']
    adjusted_estimate = estimate_factor*estimated_size

    LOGGER.info("Estimated Backup Size: %s",
                 format_bytes(estimated_size)
               )

    if adjusted_estimate != estimated_size:
        LOGGER.info("Using estimated-size-factor=%.2f and adjusting estimate to %s",
                     estimate_factor,
                     format_bytes(adjusted_estimate)
                   )
    # Save the estimated size in the backup.conf
    backup_job.config['holland:backup']['estimated-size'] = estimated_size
    try:
        verify_space(adjusted_estimate, backup_job.path)
    except BackupError, exc:
        if not dry_run:
            raise

    if not dry_run:
        LOGGER.info("Purging old backup jobs")
        purge_old_backups(backupset_name, 
                          backup_job.config.lookup('holland:backup.backups-to-keep'),
                          exclude=[backup_job])

    # Start backup
    backup_job.config['holland:backup']['start-time'] = time.time()
    # initialize spool directory
    if not dry_run:
        backup_job.prepare()

    exc = None
    try:
        LOGGER.info("Starting backup[%s] via plugin %s", 
                    backup_job.name,
                    backupset_cfg.lookup('holland:backup.plugin'))
        plugin.backup()
    except KeyboardInterrupt:
        exc = BackupError("Interrupted")
    except Exception, exc:
        if not isinstance(exc, BackupError):
            LOGGER.debug("Unexpected exception when running backups.", exc_info=True)
            exc = BackupError(exc)

    backup_job.config['holland:backup']['stop-time'] = time.time()
    backup_interval = (backup_job.config['holland:backup']['stop-time'] -
                       backup_job.config['holland:backup']['start-time'])
    if dry_run:
        LOGGER.info("Dry-run completed in %s",
                    format_interval(backup_interval))
    else:
        LOGGER.info("Backup completed in %s", 
                    format_interval(backup_interval))

    if not dry_run and exc is None:
        final_size = directory_size(backup_job.path)
        LOGGER.info("Final on-disk backup size: %s %.2f%% of estimated size %s", 
                    format_bytes(final_size), 
                    estimated_size and 100*(float(final_size)/estimated_size) or 0.0,
                    format_bytes(estimated_size))
        backup_job.config['holland:backup']['on-disk-size'] = final_size
        LOGGER.debug("Flushing backup job")
        backup_job.flush()

    if exc is not None:
        if backup_job.config['holland:backup']['auto-purge-failures'] is True:
            LOGGER.warning("Purging this backup (%s) due to failure", backup_job.name)
            backup_job.purge()
        raise

########NEW FILE########
__FILENAME__ = cmdshell
import os, sys
import optparse
import logging
from holland.core.plugin import iter_entry_points, get_distribution
from holland.core.util.bootstrap import bootstrap
from holland.core.command import run
from holland.core.config.checks import is_logging_level

HOLLAND_VERSION = get_distribution('holland').version
HOLLAND_BANNER = """
Holland Backup v%s
Copyright (c) 2008-2010 Rackspace US, Inc.
More info available at http://hollandbackup.org

[[[[[[[]]]]]]] [[[[[[[]]]]]]]
[[[[[[[]]]]]]]       [[[[[[[]]]]]]]
[[[[[[[]]]]]]] [[[[[[[]]]]]]]
[[[[[[[]]]]]]] [[[[[[[]]]]]]]

""" % HOLLAND_VERSION

LOGGER = logging.getLogger(__name__)

## global parser
parser = optparse.OptionParser(add_help_option=False,version=HOLLAND_BANNER)
parser.add_option('-h', '--help', action='store_true',
                  help="Show help")
parser.add_option('-v', '--verbose', action='store_const', const='info',
                    dest='log_level',
                    help="Log verbose output")
parser.add_option('-d', '--debug', action='store_const', const='debug',
                    dest='log_level',
                    help="Log debug output")
parser.add_option('-c', '--config-file', metavar="<file>",
                  help="Read configuration from the given file")
parser.add_option('-q', '--quiet',  action='store_true',
                  help="Don't log to console")
parser.add_option('-l', '--log-level', type='choice', metavar='<log-level>',
                  choices=['critical', 'error','warning','info', 'debug'],
                  help="Specify the log level. "
                       "One of: critical,error,warning,info,debug")
parser.set_defaults(log_level=None,
                    quiet=False,
                    config_file=os.getenv('HOLLAND_CONFIG',
                                          '/etc/holland/holland.conf')
                   )
parser.disable_interspersed_args()

# main entrypoint for holland's cmdshell 'hl'
def main():
    opts, args = parser.parse_args(sys.argv[1:])

    logging.raiseExceptions = bool(opts.log_level == 'debug')

    if opts.log_level:
        opts.log_level = is_logging_level(opts.log_level)

    if not args:
        args = ['help']

    if opts.help or args[0] == 'help':
        if args[0] == 'help':
            args = args[1:]
        return run(['help'] + args)

    # Bootstrap the environment
    bootstrap(opts)

    LOGGER.info("Holland %s started with pid %d", HOLLAND_VERSION, os.getpid())
    return run(args)

########NEW FILE########
__FILENAME__ = command
"""
Pluggable command support
"""

import os
import sys
import re
import optparse
import textwrap
import logging
from types import StringTypes
from inspect import getargspec, getdoc
import logging
from holland.core.util.template import Template

LOGGER = logging.getLogger(__name__)


def option(*args, **kwargs):
    return optparse.make_option(*args, **kwargs)

class StopOptionProcessing(Exception):
    pass


class _OptionParserEx(optparse.OptionParser):
    def __init__(self, **kwargs):
        optparse.OptionParser.__init__(self, **kwargs)
        #self.remove_option('--help')

    def error(self, msg):
        raise optparse.OptParseError(msg)

    def exit(self, status=0, msg=None):
        if not status:
            raise StopOptionProcessing(msg)
        else:
            # TODO: don't lose status info here
            raise optparse.OptParseError(msg)
 
option = optparse.make_option

class Command(object):
    """Base Command class for implementing pluggable
    commmands.

    User commands typically inherit this class and 
    implement an appropriate run(self, cmdname, opts, [args...])
    and this parent class will discover the acceptable arguments
    based on the run() method signature
    """

    name = None
    aliases = [
    ]

    options = [
    ]

    description = ''

    def __init__(self):
        help_fmt = optparse.IndentedHelpFormatter()
        self.optparser = _OptionParserEx(prog=self.name,
                                         add_help_option=False,
                                         formatter=help_fmt)
        self.optparser.add_option('--help', '-h', action='store_true',
                                  help='show this help message and exit')
        self.optparser.add_options(self.options)

    def format_cmd_options(self):
        """
        Format the options this command supports

        Default behavior is to delegate to format_option_help() of
        the associated OptionParser instance for this command
        """
        return self.optparser.format_option_help()

    def format_arg(self, arg):
        """
        Format an individual argument this command supports
        """
        return arg.replace('_', '-')

    def format_varargs(self, arg):
        """
        Format how variable arguments (\*args) are displayed
        """
        return '[%s...]' % self.format_arg(arg)

    def _check_argspec(self, args, varargs, varkw, defaults):
        for arg in args:
            if not isinstance(arg, StringTypes):
                raise AssertionError('Tuple arguments are not supported')
        if varkw:
            raise AssertionError('Keyword arguments are not supported')

    def format_cmd_args(self):
        """
        Format all the arguments accepted by this command

        Defers to self.format_arg and self.format_varargs
        """

        args, varargs, varkw, defaults = getargspec(self.run)
        self._check_argspec(args, varargs, varkw, defaults)
        args = args[3:]
        specs = []
        if defaults:
            firstdefault = len(args) - len(defaults)
        for i in range(len(args)):
            spec = self.format_arg(args[i])
            if defaults and i >= firstdefault:
                spec = '[' + spec + ']'
            specs.append(spec)
        if varargs is not None:
            specs.append(self.format_varargs(varargs))
        return ' '.join(specs)

    def usage(self):
        """
        Format this command's usage string
        """
        tpl = Template("Usage: ${cmd_name} ${options}${cmd_args}")
        return tpl.safe_substitute(cmd_name=self.name,
                                   options=self.options and "[options] " or "",
                                   cmd_args=self.format_cmd_args())

    def reformat_paragraphs(self, str):
        from textwrap import wrap
        paragraphs = []
        buffer = ''
        for line in str.splitlines():
            if not line and buffer:
                paragraphs.append("\n".join(wrap(buffer, 65)))
                buffer = ''
            else:
                buffer += line
        if buffer:
            paragraphs.append(buffer)

        return "\n\n".join(paragraphs)

    def help(self):
        """
        Format this command's help output

        Default is to use the class' docstring as a 
        template and interpolate the name, options and 
        arguments
        """
        usage_str = getdoc(self) or ''
        usage_str = self.reformat_paragraphs(usage_str)
        cmd_name = self.name
        cmd_opts = self.format_cmd_options()
        cmd_args = self.format_cmd_args()
        help_str = Template(usage_str).safe_substitute(cmd_name=cmd_name,
                                                   cmd_option_list=cmd_opts,
                                                   cmd_args=cmd_args,
                                                   cmd_usage=self.usage()
                                                   ).rstrip()
        return re.sub(r'\n\n+', r'\n\n', help_str)

    def parse_args(self, argv):
        """
        Parse the options for this command
        """
        self.optparser.prog = argv.pop(0)

        opts, args = self.optparser.parse_args(argv)
        return opts, args

    def dispatch(self, argv):
        """
        Dispatch arguments to this command

        Parses the arguments through this command's
        option parser and delegates to self.run(\*args)
        """
        run_args, run_varargs, run_varkw, run_defaults = getargspec(self.run)
        try:
            opts, args = self.parse_args(argv)
        except StopOptionProcessing, e:
            return 1
        except optparse.OptParseError, e:
            print >>sys.stderr, self.usage()
            print
            print >>sys.stderr, "%s: error: %s" % (self.name, e)
            return 1

        if opts.help:
            print self.help()
            return os.EX_USAGE

        cmd_name = self.optparser.prog

        if len(args) > len(run_args[3:]) and not run_varargs:
            print >>sys.stderr, "Error: %s only accepts %d arguments but %d were provided" % ( (self.name, len(run_args[3:]), len(args)))
            print self.help()
            return os.EX_USAGE

        num_req = len(run_defaults or []) or 0
        if len(args) < len(run_args[3:-num_req or None]):
            print >>sys.stderr, "Failed: %s requires %d arguments required, %d provided" % (cmd_name,len(run_args[3:-num_req or None]), len(args))
            print self.help()
            return os.EX_USAGE
        try:
            return self.run(self.optparser.prog, opts, *args)
        except KeyboardInterrupt:
            raise
        except Exception, e:
            LOGGER.error("Uncaught exception while running command '%s': %r", cmd_name, e, exc_info=True)
            return os.EX_SOFTWARE


    def run(self, cmd, opts, *args):
        """
        This should be overridden by subclasses
        """
        pass

    def __cmp__(self, other):
        """
        Sort this commmand alphabetically
        """
        # Useful for sorting a list of commands alphabetically
        return cmp(self.name, other.name)

########NEW FILE########
__FILENAME__ = checks
"""
Extra check methods to work with
a validate.py Validator instance
"""

import logging
import shlex
from types import StringTypes
import validate as validate
from validate import Validator

def is_coerced_list(value, min_val=None, max_val=None):
    """
    Checks if a value is a list, if not coerces
    it to a list
    """
    if isinstance(value, StringTypes):
        value = [value]
    return validate.is_list(value, min_val, max_val)

def is_octal(value, min_val=None, max_val=None):
    """
    Coerces a value to octal
    """
    if not isinstance(value, StringTypes):
        return validate.is_integer(value, min_val, max_val)
        
    try:
        value = int(value, 8)
    except ValueError:
        raise validate.VdtTypeError(value)
    return validate.is_integer(value, min_val, max_val)

def is_logging_level(value):
    """
    Coerces a string to an integer logging level which
    maps to a standard python logging level
    """
    std_levels = {
        'debug'     : logging.DEBUG,
        'info'      : logging.INFO,
        'warning'   : logging.WARNING,
        'error'     : logging.ERROR,
        'critical'  : logging.CRITICAL
    }
    
    try:
        level = value.lower().strip()
    except:
        raise validate.VdtTypeError(value)
    
    if not std_levels.get(level):
        raise validate.ValidateError(value)
    
    return std_levels.get(level)

def is_cmdline(value):
    try:
        return shlex.split(value)
    except:
        raise validate.VdtTypeError(value)

validator = Validator({
    'octal' : is_octal,
    'logging_level' : is_logging_level,
    'coerced_list' : is_coerced_list,
    'cmd_args' : is_cmdline
})

########NEW FILE########
__FILENAME__ = config
"""
Configuration API support
"""

import os
import logging
from configobj import ConfigObj, Section, flatten_errors, get_extra_values
from checks import validator

LOGGER = logging.getLogger(__name__)

CONFIG_SUFFIX = '.conf'

# Main Holland configspec
CONFIGSPEC = """
[holland]
tmpdir              = string(default=None)
plugin-dirs         = coerced_list(default=list('/usr/share/holland/plugins'))
backup-directory    = string(default=/var/spool/holland)
backupsets          = coerced_list(default=list())
umask               = octal(default='007')
path                = string(default=None)

[logging]
level               = logging_level(default='info')
filename            = string(default=None)
""".splitlines()

class ConfigError(Exception):
    pass

class BaseConfig(ConfigObj):

    """
    Provides basic configuration support.  This
    is a subclass of ConfigObj but adds a few
    extra convenient method.
    """

    def __init__(self, path, configspec=None, file_error=True):
        ConfigObj.__init__(self,
                            path,
                            file_error=file_error,
                            interpolation=False,
                            write_empty_values=True,
                            encoding='utf8',
                            configspec={})

    def _canonicalize(self, section, key):
        """Rewrite all keys so that underscores are normalized to dashes"""
        section.rename(key, str(key.replace('_', '-')))

    def reload(self):
        ConfigObj.reload(self)
        self.walk(self._canonicalize, call_on_sections=True)

    def validate_config(self, configspec, suppress_warnings=False):
        """
        Validate this config with the given configspec
        """
        self._handle_configspec(configspec)
        errors = self.validate(validator, preserve_errors=True)
        for entry in flatten_errors(self, errors):
            section_list, key, error = entry
            if not error:
                LOGGER.error("Missing parameter %s", '.'.join(section_list + [key]))
            else:
                LOGGER.error("Configuration error %s: %s", '.'.join(section_list + [key]), error)

        # warn about any unknown parameters before we potentially abort on
        # validation errors
        if not suppress_warnings:
            for sections, name in get_extra_values(self):
                LOGGER.warn("Unknown parameter '%s' in section '%s'",
                            name, ".".join(sections))

        if errors is not True:
            raise ConfigError("Configuration errors were encountered while validating %r" % self.filename)
        return errors

    def lookup(self, key, safe=True):
        """
        Lookup a configuration item based on the
        dot-separated path.
        """
        parts = key.split('.')
        # lookup key as a . separated hierarchy path
        section = self
        result = None
        count = 0
        for count, name in enumerate(parts):
            if not isinstance(section, Section):
                result = None
                break
            result = section.get(name)
            section = result
        if not result and not safe:
            raise KeyError('%r not found (%r)' % (key, parts[count]))
        return result

class BackupConfig(BaseConfig):
    """
    Load config for a backupset and merge with
    its provider config
    """
    def __init__(self, path):
        BaseConfig.__init__(self, None)
        basecfg = BaseConfig(path)
        basecfg.walk(self._canonicalize, call_on_sections=True)
        provider = basecfg.lookup('holland:backup.plugin')
        if provider:
            try:
                configbase = os.path.dirname(os.path.dirname(path))
                providerpath = os.path.join(configbase, 'providers', provider)
                providerpath += CONFIG_SUFFIX
                providercfg = BaseConfig(providerpath)
                providercfg.walk(self._canonicalize, call_on_sections=True)
                self.merge(providercfg)
            except IOError, ex:
                LOGGER.warning("Failed to load config for provider %r (%s)" %
                                    (provider, ex))
        self.merge(basecfg)
        self.filename = basecfg.filename


class GlobalConfig(BaseConfig):
    """
    Load Holland's global config.
    """
    def __init__(self, filename):
        if filename:
            self.filename = os.path.abspath(filename)
            self.configdir = os.path.dirname(self.filename)
        else:
            self.filename = None
            self.configdir = None
        BaseConfig.__init__(self, self.filename)

    def provider(self, name):
        """
        Load the provider config relative to this configs
        base directory
        """
        path = os.path.join(self.configdir, 'providers', name) + CONFIG_SUFFIX
        return BaseConfig(path)

    def backupset(self, name):
        """
        Load the backupset config relative to this configs
        base directory
        """
        if not self.configdir:
            raise IOError("Config has not been initialized")
        path = os.path.join(self.configdir, 'backupsets', name) + CONFIG_SUFFIX
        return BackupConfig(path)

    def hook_config(self, name):
        for section_name in self:
            if not isinstance(self[section_name], Section):
                continue
            if section_name.startswith('hook:'):
                hook_name = section_name[len('hook:'):]
                if hook_name == name:
                    return BaseConfig(self[section_name])

hollandcfg = GlobalConfig(None)

def load_backupset_config(name):
    return hollandcfg.backupset(name)

def setup_config(config_file):
    """
    Configure the default hollandcfg instance in this module
    """
    global hollandcfg

    if not config_file:
        LOGGER.debug("load_config called with not configuration file")
        hollandcfg.validate_config(CONFIGSPEC)
        print hollandcfg
        return

    config_file = os.path.abspath(config_file)
    LOGGER.debug("Loading %r", config_file)
    hollandcfg.clear()
    hollandcfg.filename = config_file
    hollandcfg.reload()
    hollandcfg.validate_config(CONFIGSPEC)
    hollandcfg.configdir = os.path.dirname(config_file)

########NEW FILE########
__FILENAME__ = configobj
# configobj.py
# A config file reader/writer that supports nested sections in config files.
# Copyright (C) 2005-2010 Michael Foord, Nicola Larosa
# E-mail: fuzzyman AT voidspace DOT org DOT uk
#         nico AT tekNico DOT net

# ConfigObj 4
# http://www.voidspace.org.uk/python/configobj.html

# Released subject to the BSD License
# Please see http://www.voidspace.org.uk/python/license.shtml

# Scripts maintained at http://www.voidspace.org.uk/python/index.shtml
# For information about bugfixes, updates and support, please join the
# ConfigObj mailing list:
# http://lists.sourceforge.net/lists/listinfo/configobj-develop
# Comments, suggestions and bug reports welcome.

from __future__ import generators

import os
import re
import sys

from codecs import BOM_UTF8, BOM_UTF16, BOM_UTF16_BE, BOM_UTF16_LE


# imported lazily to avoid startup performance hit if it isn't used
compiler = None

# A dictionary mapping BOM to
# the encoding to decode with, and what to set the
# encoding attribute to.
BOMS = {
    BOM_UTF8: ('utf_8', None),
    BOM_UTF16_BE: ('utf16_be', 'utf_16'),
    BOM_UTF16_LE: ('utf16_le', 'utf_16'),
    BOM_UTF16: ('utf_16', 'utf_16'),
    }
# All legal variants of the BOM codecs.
# TODO: the list of aliases is not meant to be exhaustive, is there a
#   better way ?
BOM_LIST = {
    'utf_16': 'utf_16',
    'u16': 'utf_16',
    'utf16': 'utf_16',
    'utf-16': 'utf_16',
    'utf16_be': 'utf16_be',
    'utf_16_be': 'utf16_be',
    'utf-16be': 'utf16_be',
    'utf16_le': 'utf16_le',
    'utf_16_le': 'utf16_le',
    'utf-16le': 'utf16_le',
    'utf_8': 'utf_8',
    'u8': 'utf_8',
    'utf': 'utf_8',
    'utf8': 'utf_8',
    'utf-8': 'utf_8',
    }

# Map of encodings to the BOM to write.
BOM_SET = {
    'utf_8': BOM_UTF8,
    'utf_16': BOM_UTF16,
    'utf16_be': BOM_UTF16_BE,
    'utf16_le': BOM_UTF16_LE,
    None: BOM_UTF8
    }


def match_utf8(encoding):
    return BOM_LIST.get(encoding.lower()) == 'utf_8'


# Quote strings used for writing values
squot = "'%s'"
dquot = '"%s"'
noquot = "%s"
wspace_plus = ' \r\n\v\t\'"'
tsquot = '"""%s"""'
tdquot = "'''%s'''"

# Sentinel for use in getattr calls to replace hasattr
MISSING = object()

__version__ = '4.7.1'

try:
    any
except NameError:
    def any(iterable):
        for entry in iterable:
            if entry:
                return True
        return False


__all__ = (
    '__version__',
    'DEFAULT_INDENT_TYPE',
    'DEFAULT_INTERPOLATION',
    'ConfigObjError',
    'NestingError',
    'ParseError',
    'DuplicateError',
    'ConfigspecError',
    'ConfigObj',
    'SimpleVal',
    'InterpolationError',
    'InterpolationLoopError',
    'MissingInterpolationOption',
    'RepeatSectionError',
    'ReloadError',
    'UnreprError',
    'UnknownType',
    'flatten_errors',
    'get_extra_values'
)

DEFAULT_INTERPOLATION = 'configparser'
DEFAULT_INDENT_TYPE = '    '
MAX_INTERPOL_DEPTH = 10

OPTION_DEFAULTS = {
    'interpolation': True,
    'raise_errors': False,
    'list_values': True,
    'create_empty': False,
    'file_error': False,
    'configspec': None,
    'stringify': True,
    # option may be set to one of ('', ' ', '\t')
    'indent_type': None,
    'encoding': None,
    'default_encoding': None,
    'unrepr': False,
    'write_empty_values': False,
}



def getObj(s):
    global compiler
    if compiler is None:
        import compiler
    s = "a=" + s
    p = compiler.parse(s)
    return p.getChildren()[1].getChildren()[0].getChildren()[1]


class UnknownType(Exception):
    pass


class Builder(object):
    
    def build(self, o):
        m = getattr(self, 'build_' + o.__class__.__name__, None)
        if m is None:
            raise UnknownType(o.__class__.__name__)
        return m(o)
    
    def build_List(self, o):
        return map(self.build, o.getChildren())
    
    def build_Const(self, o):
        return o.value
    
    def build_Dict(self, o):
        d = {}
        i = iter(map(self.build, o.getChildren()))
        for el in i:
            d[el] = i.next()
        return d
    
    def build_Tuple(self, o):
        return tuple(self.build_List(o))
    
    def build_Name(self, o):
        if o.name == 'None':
            return None
        if o.name == 'True':
            return True
        if o.name == 'False':
            return False
        
        # An undefined Name
        raise UnknownType('Undefined Name')
    
    def build_Add(self, o):
        real, imag = map(self.build_Const, o.getChildren())
        try:
            real = float(real)
        except TypeError:
            raise UnknownType('Add')
        if not isinstance(imag, complex) or imag.real != 0.0:
            raise UnknownType('Add')
        return real+imag
    
    def build_Getattr(self, o):
        parent = self.build(o.expr)
        return getattr(parent, o.attrname)
    
    def build_UnarySub(self, o):
        return -self.build_Const(o.getChildren()[0])
    
    def build_UnaryAdd(self, o):
        return self.build_Const(o.getChildren()[0])


_builder = Builder()


def unrepr(s):
    if not s:
        return s
    return _builder.build(getObj(s))



class ConfigObjError(SyntaxError):
    """
    This is the base class for all errors that ConfigObj raises.
    It is a subclass of SyntaxError.
    """
    def __init__(self, message='', line_number=None, line=''):
        self.line = line
        self.line_number = line_number
        SyntaxError.__init__(self, message)


class NestingError(ConfigObjError):
    """
    This error indicates a level of nesting that doesn't match.
    """


class ParseError(ConfigObjError):
    """
    This error indicates that a line is badly written.
    It is neither a valid ``key = value`` line,
    nor a valid section marker line.
    """


class ReloadError(IOError):
    """
    A 'reload' operation failed.
    This exception is a subclass of ``IOError``.
    """
    def __init__(self):
        IOError.__init__(self, 'reload failed, filename is not set.')


class DuplicateError(ConfigObjError):
    """
    The keyword or section specified already exists.
    """


class ConfigspecError(ConfigObjError):
    """
    An error occured whilst parsing a configspec.
    """


class InterpolationError(ConfigObjError):
    """Base class for the two interpolation errors."""


class InterpolationLoopError(InterpolationError):
    """Maximum interpolation depth exceeded in string interpolation."""

    def __init__(self, option):
        InterpolationError.__init__(
            self,
            'interpolation loop detected in value "%s".' % option)


class RepeatSectionError(ConfigObjError):
    """
    This error indicates additional sections in a section with a
    ``__many__`` (repeated) section.
    """


class MissingInterpolationOption(InterpolationError):
    """A value specified for interpolation was missing."""
    def __init__(self, option):
        msg = 'missing option "%s" in interpolation.' % option
        InterpolationError.__init__(self, msg)


class UnreprError(ConfigObjError):
    """An error parsing in unrepr mode."""



class InterpolationEngine(object):
    """
    A helper class to help perform string interpolation.

    This class is an abstract base class; its descendants perform
    the actual work.
    """

    # compiled regexp to use in self.interpolate()
    _KEYCRE = re.compile(r"%\(([^)]*)\)s")
    _cookie = '%'

    def __init__(self, section):
        # the Section instance that "owns" this engine
        self.section = section


    def interpolate(self, key, value):
        # short-cut
        if not self._cookie in value:
            return value
        
        def recursive_interpolate(key, value, section, backtrail):
            """The function that does the actual work.

            ``value``: the string we're trying to interpolate.
            ``section``: the section in which that string was found
            ``backtrail``: a dict to keep track of where we've been,
            to detect and prevent infinite recursion loops

            This is similar to a depth-first-search algorithm.
            """
            # Have we been here already?
            if (key, section.name) in backtrail:
                # Yes - infinite loop detected
                raise InterpolationLoopError(key)
            # Place a marker on our backtrail so we won't come back here again
            backtrail[(key, section.name)] = 1

            # Now start the actual work
            match = self._KEYCRE.search(value)
            while match:
                # The actual parsing of the match is implementation-dependent,
                # so delegate to our helper function
                k, v, s = self._parse_match(match)
                if k is None:
                    # That's the signal that no further interpolation is needed
                    replacement = v
                else:
                    # Further interpolation may be needed to obtain final value
                    replacement = recursive_interpolate(k, v, s, backtrail)
                # Replace the matched string with its final value
                start, end = match.span()
                value = ''.join((value[:start], replacement, value[end:]))
                new_search_start = start + len(replacement)
                # Pick up the next interpolation key, if any, for next time
                # through the while loop
                match = self._KEYCRE.search(value, new_search_start)

            # Now safe to come back here again; remove marker from backtrail
            del backtrail[(key, section.name)]

            return value

        # Back in interpolate(), all we have to do is kick off the recursive
        # function with appropriate starting values
        value = recursive_interpolate(key, value, self.section, {})
        return value


    def _fetch(self, key):
        """Helper function to fetch values from owning section.

        Returns a 2-tuple: the value, and the section where it was found.
        """
        # switch off interpolation before we try and fetch anything !
        save_interp = self.section.main.interpolation
        self.section.main.interpolation = False

        # Start at section that "owns" this InterpolationEngine
        current_section = self.section
        while True:
            # try the current section first
            val = current_section.get(key)
            if val is not None:
                break
            # try "DEFAULT" next
            val = current_section.get('DEFAULT', {}).get(key)
            if val is not None:
                break
            # move up to parent and try again
            # top-level's parent is itself
            if current_section.parent is current_section:
                # reached top level, time to give up
                break
            current_section = current_section.parent

        # restore interpolation to previous value before returning
        self.section.main.interpolation = save_interp
        if val is None:
            raise MissingInterpolationOption(key)
        return val, current_section


    def _parse_match(self, match):
        """Implementation-dependent helper function.

        Will be passed a match object corresponding to the interpolation
        key we just found (e.g., "%(foo)s" or "$foo"). Should look up that
        key in the appropriate config file section (using the ``_fetch()``
        helper function) and return a 3-tuple: (key, value, section)

        ``key`` is the name of the key we're looking for
        ``value`` is the value found for that key
        ``section`` is a reference to the section where it was found

        ``key`` and ``section`` should be None if no further
        interpolation should be performed on the resulting value
        (e.g., if we interpolated "$$" and returned "$").
        """
        raise NotImplementedError()
    


class ConfigParserInterpolation(InterpolationEngine):
    """Behaves like ConfigParser."""
    _cookie = '%'
    _KEYCRE = re.compile(r"%\(([^)]*)\)s")

    def _parse_match(self, match):
        key = match.group(1)
        value, section = self._fetch(key)
        return key, value, section



class TemplateInterpolation(InterpolationEngine):
    """Behaves like string.Template."""
    _cookie = '$'
    _delimiter = '$'
    _KEYCRE = re.compile(r"""
        \$(?:
          (?P<escaped>\$)              |   # Two $ signs
          (?P<named>[_a-z][_a-z0-9]*)  |   # $name format
          {(?P<braced>[^}]*)}              # ${name} format
        )
        """, re.IGNORECASE | re.VERBOSE)

    def _parse_match(self, match):
        # Valid name (in or out of braces): fetch value from section
        key = match.group('named') or match.group('braced')
        if key is not None:
            value, section = self._fetch(key)
            return key, value, section
        # Escaped delimiter (e.g., $$): return single delimiter
        if match.group('escaped') is not None:
            # Return None for key and section to indicate it's time to stop
            return None, self._delimiter, None
        # Anything else: ignore completely, just return it unchanged
        return None, match.group(), None


interpolation_engines = {
    'configparser': ConfigParserInterpolation,
    'template': TemplateInterpolation,
}


def __newobj__(cls, *args):
    # Hack for pickle
    return cls.__new__(cls, *args) 

class Section(dict):
    """
    A dictionary-like object that represents a section in a config file.
    
    It does string interpolation if the 'interpolation' attribute
    of the 'main' object is set to True.
    
    Interpolation is tried first from this object, then from the 'DEFAULT'
    section of this object, next from the parent and its 'DEFAULT' section,
    and so on until the main object is reached.
    
    A Section will behave like an ordered dictionary - following the
    order of the ``scalars`` and ``sections`` attributes.
    You can use this to change the order of members.
    
    Iteration follows the order: scalars, then sections.
    """

    
    def __setstate__(self, state):
        dict.update(self, state[0])
        self.__dict__.update(state[1])

    def __reduce__(self):
        state = (dict(self), self.__dict__)
        return (__newobj__, (self.__class__,), state)
    
    
    def __init__(self, parent, depth, main, indict=None, name=None):
        """
        * parent is the section above
        * depth is the depth level of this section
        * main is the main ConfigObj
        * indict is a dictionary to initialise the section with
        """
        if indict is None:
            indict = {}
        dict.__init__(self)
        # used for nesting level *and* interpolation
        self.parent = parent
        # used for the interpolation attribute
        self.main = main
        # level of nesting depth of this Section
        self.depth = depth
        # purely for information
        self.name = name
        #
        self._initialise()
        # we do this explicitly so that __setitem__ is used properly
        # (rather than just passing to ``dict.__init__``)
        for entry, value in indict.iteritems():
            self[entry] = value
            
            
    def _initialise(self):
        # the sequence of scalar values in this Section
        self.scalars = []
        # the sequence of sections in this Section
        self.sections = []
        # for comments :-)
        self.comments = {}
        self.inline_comments = {}
        # the configspec
        self.configspec = None
        # for defaults
        self.defaults = []
        self.default_values = {}
        self.extra_values = []
        self._created = False


    def _interpolate(self, key, value):
        try:
            # do we already have an interpolation engine?
            engine = self._interpolation_engine
        except AttributeError:
            # not yet: first time running _interpolate(), so pick the engine
            name = self.main.interpolation
            if name == True:  # note that "if name:" would be incorrect here
                # backwards-compatibility: interpolation=True means use default
                name = DEFAULT_INTERPOLATION
            name = name.lower()  # so that "Template", "template", etc. all work
            class_ = interpolation_engines.get(name, None)
            if class_ is None:
                # invalid value for self.main.interpolation
                self.main.interpolation = False
                return value
            else:
                # save reference to engine so we don't have to do this again
                engine = self._interpolation_engine = class_(self)
        # let the engine do the actual work
        return engine.interpolate(key, value)


    def __getitem__(self, key):
        """Fetch the item and do string interpolation."""
        val = dict.__getitem__(self, key)
        if self.main.interpolation: 
            if isinstance(val, basestring):
                return self._interpolate(key, val)
            if isinstance(val, list):
                def _check(entry):
                    if isinstance(entry, basestring):
                        return self._interpolate(key, entry)
                    return entry
                return [_check(entry) for entry in val]
        return val


    def __setitem__(self, key, value, unrepr=False):
        """
        Correctly set a value.
        
        Making dictionary values Section instances.
        (We have to special case 'Section' instances - which are also dicts)
        
        Keys must be strings.
        Values need only be strings (or lists of strings) if
        ``main.stringify`` is set.
        
        ``unrepr`` must be set when setting a value to a dictionary, without
        creating a new sub-section.
        """
        if not isinstance(key, basestring):
            raise ValueError('The key "%s" is not a string.' % key)
        
        # add the comment
        if key not in self.comments:
            self.comments[key] = []
            self.inline_comments[key] = ''
        # remove the entry from defaults
        if key in self.defaults:
            self.defaults.remove(key)
        #
        if isinstance(value, Section):
            if key not in self:
                self.sections.append(key)
            dict.__setitem__(self, key, value)
        elif isinstance(value, dict) and not unrepr:
            # First create the new depth level,
            # then create the section
            if key not in self:
                self.sections.append(key)
            new_depth = self.depth + 1
            dict.__setitem__(
                self,
                key,
                Section(
                    self,
                    new_depth,
                    self.main,
                    indict=value,
                    name=key))
        else:
            if key not in self:
                self.scalars.append(key)
            if not self.main.stringify:
                if isinstance(value, basestring):
                    pass
                elif isinstance(value, (list, tuple)):
                    for entry in value:
                        if not isinstance(entry, basestring):
                            raise TypeError('Value is not a string "%s".' % entry)
                else:
                    raise TypeError('Value is not a string "%s".' % value)
            dict.__setitem__(self, key, value)


    def __delitem__(self, key):
        """Remove items from the sequence when deleting."""
        dict. __delitem__(self, key)
        if key in self.scalars:
            self.scalars.remove(key)
        else:
            self.sections.remove(key)
        del self.comments[key]
        del self.inline_comments[key]


    def get(self, key, default=None):
        """A version of ``get`` that doesn't bypass string interpolation."""
        try:
            return self[key]
        except KeyError:
            return default


    def update(self, indict):
        """
        A version of update that uses our ``__setitem__``.
        """
        for entry in indict:
            self[entry] = indict[entry]


    def pop(self, key, *args):
        """
        'D.pop(k[,d]) -> v, remove specified key and return the corresponding value.
        If key is not found, d is returned if given, otherwise KeyError is raised'
        """
        val = dict.pop(self, key, *args)
        if key in self.scalars:
            del self.comments[key]
            del self.inline_comments[key]
            self.scalars.remove(key)
        elif key in self.sections:
            del self.comments[key]
            del self.inline_comments[key]
            self.sections.remove(key)
        if self.main.interpolation and isinstance(val, basestring):
            return self._interpolate(key, val)
        return val


    def popitem(self):
        """Pops the first (key,val)"""
        sequence = (self.scalars + self.sections)
        if not sequence:
            raise KeyError(": 'popitem(): dictionary is empty'")
        key = sequence[0]
        val =  self[key]
        del self[key]
        return key, val


    def clear(self):
        """
        A version of clear that also affects scalars/sections
        Also clears comments and configspec.
        
        Leaves other attributes alone :
            depth/main/parent are not affected
        """
        dict.clear(self)
        self.scalars = []
        self.sections = []
        self.comments = {}
        self.inline_comments = {}
        self.configspec = None
        self.defaults = []
        self.extra_values = []


    def setdefault(self, key, default=None):
        """A version of setdefault that sets sequence if appropriate."""
        try:
            return self[key]
        except KeyError:
            self[key] = default
            return self[key]


    def items(self):
        """D.items() -> list of D's (key, value) pairs, as 2-tuples"""
        return zip((self.scalars + self.sections), self.values())


    def keys(self):
        """D.keys() -> list of D's keys"""
        return (self.scalars + self.sections)


    def values(self):
        """D.values() -> list of D's values"""
        return [self[key] for key in (self.scalars + self.sections)]


    def iteritems(self):
        """D.iteritems() -> an iterator over the (key, value) items of D"""
        return iter(self.items())


    def iterkeys(self):
        """D.iterkeys() -> an iterator over the keys of D"""
        return iter((self.scalars + self.sections))

    __iter__ = iterkeys


    def itervalues(self):
        """D.itervalues() -> an iterator over the values of D"""
        return iter(self.values())


    def __repr__(self):
        """x.__repr__() <==> repr(x)"""
        return '{%s}' % ', '.join([('%s: %s' % (repr(key), repr(self[key])))
            for key in (self.scalars + self.sections)])

    __str__ = __repr__
    __str__.__doc__ = "x.__str__() <==> str(x)"


    # Extra methods - not in a normal dictionary

    def dict(self):
        """
        Return a deepcopy of self as a dictionary.
        
        All members that are ``Section`` instances are recursively turned to
        ordinary dictionaries - by calling their ``dict`` method.
        
        >>> n = a.dict()
        >>> n == a
        1
        >>> n is a
        0
        """
        newdict = {}
        for entry in self:
            this_entry = self[entry]
            if isinstance(this_entry, Section):
                this_entry = this_entry.dict()
            elif isinstance(this_entry, list):
                # create a copy rather than a reference
                this_entry = list(this_entry)
            elif isinstance(this_entry, tuple):
                # create a copy rather than a reference
                this_entry = tuple(this_entry)
            newdict[entry] = this_entry
        return newdict


    def merge(self, indict):
        """
        A recursive update - useful for merging config files.
        
        >>> a = '''[section1]
        ...     option1 = True
        ...     [[subsection]]
        ...     more_options = False
        ...     # end of file'''.splitlines()
        >>> b = '''# File is user.ini
        ...     [section1]
        ...     option1 = False
        ...     # end of file'''.splitlines()
        >>> c1 = ConfigObj(b)
        >>> c2 = ConfigObj(a)
        >>> c2.merge(c1)
        >>> c2
        ConfigObj({'section1': {'option1': 'False', 'subsection': {'more_options': 'False'}}})
        """
        for key, val in indict.items():
            if (key in self and isinstance(self[key], dict) and
                                isinstance(val, dict)):
                self[key].merge(val)
            else:   
                self[key] = val


    def rename(self, oldkey, newkey):
        """
        Change a keyname to another, without changing position in sequence.
        
        Implemented so that transformations can be made on keys,
        as well as on values. (used by encode and decode)
        
        Also renames comments.
        """
        if oldkey in self.scalars:
            the_list = self.scalars
        elif oldkey in self.sections:
            the_list = self.sections
        else:
            raise KeyError('Key "%s" not found.' % oldkey)
        pos = the_list.index(oldkey)
        #
        val = self[oldkey]
        dict.__delitem__(self, oldkey)
        dict.__setitem__(self, newkey, val)
        the_list.remove(oldkey)
        the_list.insert(pos, newkey)
        comm = self.comments[oldkey]
        inline_comment = self.inline_comments[oldkey]
        del self.comments[oldkey]
        del self.inline_comments[oldkey]
        self.comments[newkey] = comm
        self.inline_comments[newkey] = inline_comment


    def walk(self, function, raise_errors=True,
            call_on_sections=False, **keywargs):
        """
        Walk every member and call a function on the keyword and value.
        
        Return a dictionary of the return values
        
        If the function raises an exception, raise the errror
        unless ``raise_errors=False``, in which case set the return value to
        ``False``.
        
        Any unrecognised keyword arguments you pass to walk, will be pased on
        to the function you pass in.
        
        Note: if ``call_on_sections`` is ``True`` then - on encountering a
        subsection, *first* the function is called for the *whole* subsection,
        and then recurses into it's members. This means your function must be
        able to handle strings, dictionaries and lists. This allows you
        to change the key of subsections as well as for ordinary members. The
        return value when called on the whole subsection has to be discarded.
        
        See  the encode and decode methods for examples, including functions.
        
        .. admonition:: caution
        
            You can use ``walk`` to transform the names of members of a section
            but you mustn't add or delete members.
        
        >>> config = '''[XXXXsection]
        ... XXXXkey = XXXXvalue'''.splitlines()
        >>> cfg = ConfigObj(config)
        >>> cfg
        ConfigObj({'XXXXsection': {'XXXXkey': 'XXXXvalue'}})
        >>> def transform(section, key):
        ...     val = section[key]
        ...     newkey = key.replace('XXXX', 'CLIENT1')
        ...     section.rename(key, newkey)
        ...     if isinstance(val, (tuple, list, dict)):
        ...         pass
        ...     else:
        ...         val = val.replace('XXXX', 'CLIENT1')
        ...         section[newkey] = val
        >>> cfg.walk(transform, call_on_sections=True)
        {'CLIENT1section': {'CLIENT1key': None}}
        >>> cfg
        ConfigObj({'CLIENT1section': {'CLIENT1key': 'CLIENT1value'}})
        """
        out = {}
        # scalars first
        for i in range(len(self.scalars)):
            entry = self.scalars[i]
            try:
                val = function(self, entry, **keywargs)
                # bound again in case name has changed
                entry = self.scalars[i]
                out[entry] = val
            except Exception:
                if raise_errors:
                    raise
                else:
                    entry = self.scalars[i]
                    out[entry] = False
        # then sections
        for i in range(len(self.sections)):
            entry = self.sections[i]
            if call_on_sections:
                try:
                    function(self, entry, **keywargs)
                except Exception:
                    if raise_errors:
                        raise
                    else:
                        entry = self.sections[i]
                        out[entry] = False
                # bound again in case name has changed
                entry = self.sections[i]
            # previous result is discarded
            out[entry] = self[entry].walk(
                function,
                raise_errors=raise_errors,
                call_on_sections=call_on_sections,
                **keywargs)
        return out


    def as_bool(self, key):
        """
        Accepts a key as input. The corresponding value must be a string or
        the objects (``True`` or 1) or (``False`` or 0). We allow 0 and 1 to
        retain compatibility with Python 2.2.
        
        If the string is one of  ``True``, ``On``, ``Yes``, or ``1`` it returns 
        ``True``.
        
        If the string is one of  ``False``, ``Off``, ``No``, or ``0`` it returns 
        ``False``.
        
        ``as_bool`` is not case sensitive.
        
        Any other input will raise a ``ValueError``.
        
        >>> a = ConfigObj()
        >>> a['a'] = 'fish'
        >>> a.as_bool('a')
        Traceback (most recent call last):
        ValueError: Value "fish" is neither True nor False
        >>> a['b'] = 'True'
        >>> a.as_bool('b')
        1
        >>> a['b'] = 'off'
        >>> a.as_bool('b')
        0
        """
        val = self[key]
        if val == True:
            return True
        elif val == False:
            return False
        else:
            try:
                if not isinstance(val, basestring):
                    # TODO: Why do we raise a KeyError here?
                    raise KeyError()
                else:
                    return self.main._bools[val.lower()]
            except KeyError:
                raise ValueError('Value "%s" is neither True nor False' % val)


    def as_int(self, key):
        """
        A convenience method which coerces the specified value to an integer.
        
        If the value is an invalid literal for ``int``, a ``ValueError`` will
        be raised.
        
        >>> a = ConfigObj()
        >>> a['a'] = 'fish'
        >>> a.as_int('a')
        Traceback (most recent call last):
        ValueError: invalid literal for int() with base 10: 'fish'
        >>> a['b'] = '1'
        >>> a.as_int('b')
        1
        >>> a['b'] = '3.2'
        >>> a.as_int('b')
        Traceback (most recent call last):
        ValueError: invalid literal for int() with base 10: '3.2'
        """
        return int(self[key])


    def as_float(self, key):
        """
        A convenience method which coerces the specified value to a float.
        
        If the value is an invalid literal for ``float``, a ``ValueError`` will
        be raised.
        
        >>> a = ConfigObj()
        >>> a['a'] = 'fish'
        >>> a.as_float('a')
        Traceback (most recent call last):
        ValueError: invalid literal for float(): fish
        >>> a['b'] = '1'
        >>> a.as_float('b')
        1.0
        >>> a['b'] = '3.2'
        >>> a.as_float('b')
        3.2000000000000002
        """
        return float(self[key])
    
    
    def as_list(self, key):
        """
        A convenience method which fetches the specified value, guaranteeing
        that it is a list.
        
        >>> a = ConfigObj()
        >>> a['a'] = 1
        >>> a.as_list('a')
        [1]
        >>> a['a'] = (1,)
        >>> a.as_list('a')
        [1]
        >>> a['a'] = [1]
        >>> a.as_list('a')
        [1]
        """
        result = self[key]
        if isinstance(result, (tuple, list)):
            return list(result)
        return [result]
        

    def restore_default(self, key):
        """
        Restore (and return) default value for the specified key.
        
        This method will only work for a ConfigObj that was created
        with a configspec and has been validated.
        
        If there is no default value for this key, ``KeyError`` is raised.
        """
        default = self.default_values[key]
        dict.__setitem__(self, key, default)
        if key not in self.defaults:
            self.defaults.append(key)
        return default

    
    def restore_defaults(self):
        """
        Recursively restore default values to all members
        that have them.
        
        This method will only work for a ConfigObj that was created
        with a configspec and has been validated.
        
        It doesn't delete or modify entries without default values.
        """
        for key in self.default_values:
            self.restore_default(key)
            
        for section in self.sections:
            self[section].restore_defaults()


class ConfigObj(Section):
    """An object to read, create, and write config files."""

    _keyword = re.compile(r'''^ # line start
        (\s*)                   # indentation
        (                       # keyword
            (?:".*?")|          # double quotes
            (?:'.*?')|          # single quotes
            (?:[^'"=].*?)       # no quotes
        )
        \s*=\s*                 # divider
        (.*)                    # value (including list values and comments)
        $   # line end
        ''',
        re.VERBOSE)

    _sectionmarker = re.compile(r'''^
        (\s*)                     # 1: indentation
        ((?:\[\s*)+)              # 2: section marker open
        (                         # 3: section name open
            (?:"\s*\S.*?\s*")|    # at least one non-space with double quotes
            (?:'\s*\S.*?\s*')|    # at least one non-space with single quotes
            (?:[^'"\s].*?)        # at least one non-space unquoted
        )                         # section name close
        ((?:\s*\])+)              # 4: section marker close
        \s*(\#.*)?                # 5: optional comment
        $''',
        re.VERBOSE)

    # this regexp pulls list values out as a single string
    # or single values and comments
    # FIXME: this regex adds a '' to the end of comma terminated lists
    #   workaround in ``_handle_value``
    _valueexp = re.compile(r'''^
        (?:
            (?:
                (
                    (?:
                        (?:
                            (?:".*?")|              # double quotes
                            (?:'.*?')|              # single quotes
                            (?:[^'",\#][^,\#]*?)    # unquoted
                        )
                        \s*,\s*                     # comma
                    )*      # match all list items ending in a comma (if any)
                )
                (
                    (?:".*?")|                      # double quotes
                    (?:'.*?')|                      # single quotes
                    (?:[^'",\#\s][^,]*?)|           # unquoted
                    (?:(?<!,))                      # Empty value
                )?          # last item in a list - or string value
            )|
            (,)             # alternatively a single comma - empty list
        )
        \s*(\#.*)?          # optional comment
        $''',
        re.VERBOSE)

    # use findall to get the members of a list value
    _listvalueexp = re.compile(r'''
        (
            (?:".*?")|          # double quotes
            (?:'.*?')|          # single quotes
            (?:[^'",\#]?.*?)       # unquoted
        )
        \s*,\s*                 # comma
        ''',
        re.VERBOSE)

    # this regexp is used for the value
    # when lists are switched off
    _nolistvalue = re.compile(r'''^
        (
            (?:".*?")|          # double quotes
            (?:'.*?')|          # single quotes
            (?:[^'"\#].*?)|     # unquoted
            (?:)                # Empty value
        )
        \s*(\#.*)?              # optional comment
        $''',
        re.VERBOSE)

    # regexes for finding triple quoted values on one line
    _single_line_single = re.compile(r"^'''(.*?)'''\s*(#.*)?$")
    _single_line_double = re.compile(r'^"""(.*?)"""\s*(#.*)?$')
    _multi_line_single = re.compile(r"^(.*?)'''\s*(#.*)?$")
    _multi_line_double = re.compile(r'^(.*?)"""\s*(#.*)?$')

    _triple_quote = {
        "'''": (_single_line_single, _multi_line_single),
        '"""': (_single_line_double, _multi_line_double),
    }

    # Used by the ``istrue`` Section method
    _bools = {
        'yes': True, 'no': False,
        'on': True, 'off': False,
        '1': True, '0': False,
        'true': True, 'false': False,
        }


    def __init__(self, infile=None, options=None, configspec=None, encoding=None,
                 interpolation=True, raise_errors=False, list_values=True,
                 create_empty=False, file_error=False, stringify=True,
                 indent_type=None, default_encoding=None, unrepr=False,
                 write_empty_values=False, _inspec=False):
        """
        Parse a config file or create a config file object.
        
        ``ConfigObj(infile=None, configspec=None, encoding=None,
                    interpolation=True, raise_errors=False, list_values=True,
                    create_empty=False, file_error=False, stringify=True,
                    indent_type=None, default_encoding=None, unrepr=False,
                    write_empty_values=False, _inspec=False)``
        """
        self._inspec = _inspec
        # init the superclass
        Section.__init__(self, self, 0, self)
        
        infile = infile or []
        
        _options = {'configspec': configspec,
                    'encoding': encoding, 'interpolation': interpolation,
                    'raise_errors': raise_errors, 'list_values': list_values,
                    'create_empty': create_empty, 'file_error': file_error,
                    'stringify': stringify, 'indent_type': indent_type,
                    'default_encoding': default_encoding, 'unrepr': unrepr,
                    'write_empty_values': write_empty_values}

        if options is None:
            options = _options
        else:
            import warnings
            warnings.warn('Passing in an options dictionary to ConfigObj() is '
                          'deprecated. Use **options instead.',
                          DeprecationWarning, stacklevel=2)
            
            # TODO: check the values too.
            for entry in options:
                if entry not in OPTION_DEFAULTS:
                    raise TypeError('Unrecognised option "%s".' % entry)
            for entry, value in OPTION_DEFAULTS.items():
                if entry not in options:
                    options[entry] = value
                keyword_value = _options[entry]
                if value != keyword_value:
                    options[entry] = keyword_value
        
        # XXXX this ignores an explicit list_values = True in combination
        # with _inspec. The user should *never* do that anyway, but still...
        if _inspec:
            options['list_values'] = False
        
        self._initialise(options)
        configspec = options['configspec']
        self._original_configspec = configspec
        self._load(infile, configspec)
        
        
    def _load(self, infile, configspec):
        if isinstance(infile, basestring):
            self.filename = infile
            if os.path.isfile(infile):
                h = open(infile, 'rb')
                infile = h.read() or []
                h.close()
            elif self.file_error:
                # raise an error if the file doesn't exist
                raise IOError('Config file not found: "%s".' % self.filename)
            else:
                # file doesn't already exist
                if self.create_empty:
                    # this is a good test that the filename specified
                    # isn't impossible - like on a non-existent device
                    h = open(infile, 'w')
                    h.write('')
                    h.close()
                infile = []
                
        elif isinstance(infile, (list, tuple)):
            infile = list(infile)
            
        elif isinstance(infile, dict):
            # initialise self
            # the Section class handles creating subsections
            if isinstance(infile, ConfigObj):
                # get a copy of our ConfigObj
                def set_section(in_section, this_section):
                    for entry in in_section.scalars:
                        this_section[entry] = in_section[entry]
                    for section in in_section.sections:
                        this_section[section] = {}
                        set_section(in_section[section], this_section[section])
                set_section(infile, self)
                
            else:
                for entry in infile:
                    self[entry] = infile[entry]
            del self._errors
            
            if configspec is not None:
                self._handle_configspec(configspec)
            else:
                self.configspec = None
            return
        
        elif getattr(infile, 'read', MISSING) is not MISSING:
            # This supports file like objects
            infile = infile.read() or []
            # needs splitting into lines - but needs doing *after* decoding
            # in case it's not an 8 bit encoding
        else:
            raise TypeError('infile must be a filename, file like object, or list of lines.')
        
        if infile:
            # don't do it for the empty ConfigObj
            infile = self._handle_bom(infile)
            # infile is now *always* a list
            #
            # Set the newlines attribute (first line ending it finds)
            # and strip trailing '\n' or '\r' from lines
            for line in infile:
                if (not line) or (line[-1] not in ('\r', '\n', '\r\n')):
                    continue
                for end in ('\r\n', '\n', '\r'):
                    if line.endswith(end):
                        self.newlines = end
                        break
                break

            infile = [line.rstrip('\r\n') for line in infile]
            
        self._parse(infile)
        # if we had any errors, now is the time to raise them
        if self._errors:
            info = "at line %s." % self._errors[0].line_number
            if len(self._errors) > 1:
                msg = "Parsing failed with several errors.\nFirst error %s" % info
                error = ConfigObjError(msg)
            else:
                error = self._errors[0]
            # set the errors attribute; it's a list of tuples:
            # (error_type, message, line_number)
            error.errors = self._errors
            # set the config attribute
            error.config = self
            raise error
        # delete private attributes
        del self._errors
        
        if configspec is None:
            self.configspec = None
        else:
            self._handle_configspec(configspec)
    
    
    def _initialise(self, options=None):
        if options is None:
            options = OPTION_DEFAULTS
            
        # initialise a few variables
        self.filename = None
        self._errors = []
        self.raise_errors = options['raise_errors']
        self.interpolation = options['interpolation']
        self.list_values = options['list_values']
        self.create_empty = options['create_empty']
        self.file_error = options['file_error']
        self.stringify = options['stringify']
        self.indent_type = options['indent_type']
        self.encoding = options['encoding']
        self.default_encoding = options['default_encoding']
        self.BOM = False
        self.newlines = None
        self.write_empty_values = options['write_empty_values']
        self.unrepr = options['unrepr']
        
        self.initial_comment = []
        self.final_comment = []
        self.configspec = None
        
        if self._inspec:
            self.list_values = False
        
        # Clear section attributes as well
        Section._initialise(self)
        
        
    def __repr__(self):
        return ('ConfigObj({%s})' % 
                ', '.join([('%s: %s' % (repr(key), repr(self[key]))) 
                for key in (self.scalars + self.sections)]))
    
    
    def _handle_bom(self, infile):
        """
        Handle any BOM, and decode if necessary.
        
        If an encoding is specified, that *must* be used - but the BOM should
        still be removed (and the BOM attribute set).
        
        (If the encoding is wrongly specified, then a BOM for an alternative
        encoding won't be discovered or removed.)
        
        If an encoding is not specified, UTF8 or UTF16 BOM will be detected and
        removed. The BOM attribute will be set. UTF16 will be decoded to
        unicode.
        
        NOTE: This method must not be called with an empty ``infile``.
        
        Specifying the *wrong* encoding is likely to cause a
        ``UnicodeDecodeError``.
        
        ``infile`` must always be returned as a list of lines, but may be
        passed in as a single string.
        """
        if ((self.encoding is not None) and
            (self.encoding.lower() not in BOM_LIST)):
            # No need to check for a BOM
            # the encoding specified doesn't have one
            # just decode
            return self._decode(infile, self.encoding)
        
        if isinstance(infile, (list, tuple)):
            line = infile[0]
        else:
            line = infile
        if self.encoding is not None:
            # encoding explicitly supplied
            # And it could have an associated BOM
            # TODO: if encoding is just UTF16 - we ought to check for both
            # TODO: big endian and little endian versions.
            enc = BOM_LIST[self.encoding.lower()]
            if enc == 'utf_16':
                # For UTF16 we try big endian and little endian
                for BOM, (encoding, final_encoding) in BOMS.items():
                    if not final_encoding:
                        # skip UTF8
                        continue
                    if infile.startswith(BOM):
                        ### BOM discovered
                        ##self.BOM = True
                        # Don't need to remove BOM
                        return self._decode(infile, encoding)
                    
                # If we get this far, will *probably* raise a DecodeError
                # As it doesn't appear to start with a BOM
                return self._decode(infile, self.encoding)
            
            # Must be UTF8
            BOM = BOM_SET[enc]
            if not line.startswith(BOM):
                return self._decode(infile, self.encoding)
            
            newline = line[len(BOM):]
            
            # BOM removed
            if isinstance(infile, (list, tuple)):
                infile[0] = newline
            else:
                infile = newline
            self.BOM = True
            return self._decode(infile, self.encoding)
        
        # No encoding specified - so we need to check for UTF8/UTF16
        for BOM, (encoding, final_encoding) in BOMS.items():
            if not line.startswith(BOM):
                continue
            else:
                # BOM discovered
                self.encoding = final_encoding
                if not final_encoding:
                    self.BOM = True
                    # UTF8
                    # remove BOM
                    newline = line[len(BOM):]
                    if isinstance(infile, (list, tuple)):
                        infile[0] = newline
                    else:
                        infile = newline
                    # UTF8 - don't decode
                    if isinstance(infile, basestring):
                        return infile.splitlines(True)
                    else:
                        return infile
                # UTF16 - have to decode
                return self._decode(infile, encoding)
            
        # No BOM discovered and no encoding specified, just return
        if isinstance(infile, basestring):
            # infile read from a file will be a single string
            return infile.splitlines(True)
        return infile


    def _a_to_u(self, aString):
        """Decode ASCII strings to unicode if a self.encoding is specified."""
        if self.encoding:
            return aString.decode('ascii')
        else:
            return aString


    def _decode(self, infile, encoding):
        """
        Decode infile to unicode. Using the specified encoding.
        
        if is a string, it also needs converting to a list.
        """
        if isinstance(infile, basestring):
            # can't be unicode
            # NOTE: Could raise a ``UnicodeDecodeError``
            return infile.decode(encoding).splitlines(True)
        for i, line in enumerate(infile):
            if not isinstance(line, unicode):
                # NOTE: The isinstance test here handles mixed lists of unicode/string
                # NOTE: But the decode will break on any non-string values
                # NOTE: Or could raise a ``UnicodeDecodeError``
                infile[i] = line.decode(encoding)
        return infile


    def _decode_element(self, line):
        """Decode element to unicode if necessary."""
        if not self.encoding:
            return line
        if isinstance(line, str) and self.default_encoding:
            return line.decode(self.default_encoding)
        return line


    def _str(self, value):
        """
        Used by ``stringify`` within validate, to turn non-string values
        into strings.
        """
        if not isinstance(value, basestring):
            return str(value)
        else:
            return value


    def _parse(self, infile):
        """Actually parse the config file."""
        temp_list_values = self.list_values
        if self.unrepr:
            self.list_values = False
            
        comment_list = []
        done_start = False
        this_section = self
        maxline = len(infile) - 1
        cur_index = -1
        reset_comment = False
        
        while cur_index < maxline:
            if reset_comment:
                comment_list = []
            cur_index += 1
            line = infile[cur_index]
            sline = line.strip()
            # do we have anything on the line ?
            if not sline or sline.startswith('#'):
                reset_comment = False
                comment_list.append(line)
                continue
            
            if not done_start:
                # preserve initial comment
                self.initial_comment = comment_list
                comment_list = []
                done_start = True
                
            reset_comment = True
            # first we check if it's a section marker
            mat = self._sectionmarker.match(line)
            if mat is not None:
                # is a section line
                (indent, sect_open, sect_name, sect_close, comment) = mat.groups()
                if indent and (self.indent_type is None):
                    self.indent_type = indent
                cur_depth = sect_open.count('[')
                if cur_depth != sect_close.count(']'):
                    self._handle_error("Cannot compute the section depth at line %s.",
                                       NestingError, infile, cur_index)
                    continue
                
                if cur_depth < this_section.depth:
                    # the new section is dropping back to a previous level
                    try:
                        parent = self._match_depth(this_section,
                                                   cur_depth).parent
                    except SyntaxError:
                        self._handle_error("Cannot compute nesting level at line %s.",
                                           NestingError, infile, cur_index)
                        continue
                elif cur_depth == this_section.depth:
                    # the new section is a sibling of the current section
                    parent = this_section.parent
                elif cur_depth == this_section.depth + 1:
                    # the new section is a child the current section
                    parent = this_section
                else:
                    self._handle_error("Section too nested at line %s.",
                                       NestingError, infile, cur_index)
                    
                sect_name = self._unquote(sect_name)
                if sect_name in parent:
                    self._handle_error('Duplicate section name at line %s.',
                                       DuplicateError, infile, cur_index)
                    continue
                
                # create the new section
                this_section = Section(
                    parent,
                    cur_depth,
                    self,
                    name=sect_name)
                parent[sect_name] = this_section
                parent.inline_comments[sect_name] = comment
                parent.comments[sect_name] = comment_list
                continue
            #
            # it's not a section marker,
            # so it should be a valid ``key = value`` line
            mat = self._keyword.match(line)
            if mat is None:
                # it neither matched as a keyword
                # or a section marker
                self._handle_error(
                    'Invalid line at line "%s".',
                    ParseError, infile, cur_index)
            else:
                # is a keyword value
                # value will include any inline comment
                (indent, key, value) = mat.groups()
                if indent and (self.indent_type is None):
                    self.indent_type = indent
                # check for a multiline value
                if value[:3] in ['"""', "'''"]:
                    try:
                        value, comment, cur_index = self._multiline(
                            value, infile, cur_index, maxline)
                    except SyntaxError:
                        self._handle_error(
                            'Parse error in value at line %s.',
                            ParseError, infile, cur_index)
                        continue
                    else:
                        if self.unrepr:
                            comment = ''
                            try:
                                value = unrepr(value)
                            except Exception, e:
                                if type(e) == UnknownType:
                                    msg = 'Unknown name or type in value at line %s.'
                                else:
                                    msg = 'Parse error in value at line %s.'
                                self._handle_error(msg, UnreprError, infile,
                                    cur_index)
                                continue
                else:
                    if self.unrepr:
                        comment = ''
                        try:
                            value = unrepr(value)
                        except Exception, e:
                            if isinstance(e, UnknownType):
                                msg = 'Unknown name or type in value at line %s.'
                            else:
                                msg = 'Parse error in value at line %s.'
                            self._handle_error(msg, UnreprError, infile,
                                cur_index)
                            continue
                    else:
                        # extract comment and lists
                        try:
                            (value, comment) = self._handle_value(value)
                        except SyntaxError:
                            self._handle_error(
                                'Parse error in value at line %s.',
                                ParseError, infile, cur_index)
                            continue
                #
                key = self._unquote(key)
                if key in this_section:
                    self._handle_error(
                        'Duplicate keyword name at line %s.',
                        DuplicateError, infile, cur_index)
                    continue
                # add the key.
                # we set unrepr because if we have got this far we will never
                # be creating a new section
                this_section.__setitem__(key, value, unrepr=True)
                this_section.inline_comments[key] = comment
                this_section.comments[key] = comment_list
                continue
        #
        if self.indent_type is None:
            # no indentation used, set the type accordingly
            self.indent_type = ''

        # preserve the final comment
        if not self and not self.initial_comment:
            self.initial_comment = comment_list
        elif not reset_comment:
            self.final_comment = comment_list
        self.list_values = temp_list_values


    def _match_depth(self, sect, depth):
        """
        Given a section and a depth level, walk back through the sections
        parents to see if the depth level matches a previous section.
        
        Return a reference to the right section,
        or raise a SyntaxError.
        """
        while depth < sect.depth:
            if sect is sect.parent:
                # we've reached the top level already
                raise SyntaxError()
            sect = sect.parent
        if sect.depth == depth:
            return sect
        # shouldn't get here
        raise SyntaxError()


    def _handle_error(self, text, ErrorClass, infile, cur_index):
        """
        Handle an error according to the error settings.
        
        Either raise the error or store it.
        The error will have occured at ``cur_index``
        """
        line = infile[cur_index]
        cur_index += 1
        message = text % cur_index
        error = ErrorClass(message, cur_index, line)
        if self.raise_errors:
            # raise the error - parsing stops here
            raise error
        # store the error
        # reraise when parsing has finished
        self._errors.append(error)


    def _unquote(self, value):
        """Return an unquoted version of a value"""
        if not value:
            # should only happen during parsing of lists
            raise SyntaxError
        if (value[0] == value[-1]) and (value[0] in ('"', "'")):
            value = value[1:-1]
        return value


    def _quote(self, value, multiline=True):
        """
        Return a safely quoted version of a value.
        
        Raise a ConfigObjError if the value cannot be safely quoted.
        If multiline is ``True`` (default) then use triple quotes
        if necessary.
        
        * Don't quote values that don't need it.
        * Recursively quote members of a list and return a comma joined list.
        * Multiline is ``False`` for lists.
        * Obey list syntax for empty and single member lists.
        
        If ``list_values=False`` then the value is only quoted if it contains
        a ``\\n`` (is multiline) or '#'.
        
        If ``write_empty_values`` is set, and the value is an empty string, it
        won't be quoted.
        """
        if multiline and self.write_empty_values and value == '':
            # Only if multiline is set, so that it is used for values not
            # keys, and not values that are part of a list
            return ''
        
        if multiline and isinstance(value, (list, tuple)):
            if not value:
                return ','
            elif len(value) == 1:
                return self._quote(value[0], multiline=False) + ','
            return ', '.join([self._quote(val, multiline=False)
                for val in value])
        if not isinstance(value, basestring):
            if self.stringify:
                value = str(value)
            else:
                raise TypeError('Value "%s" is not a string.' % value)

        if not value:
            return '""'
        
        no_lists_no_quotes = not self.list_values and '\n' not in value and '#' not in value
        need_triple = multiline and ((("'" in value) and ('"' in value)) or ('\n' in value ))
        hash_triple_quote = multiline and not need_triple and ("'" in value) and ('"' in value) and ('#' in value)
        check_for_single = (no_lists_no_quotes or not need_triple) and not hash_triple_quote
        
        if check_for_single:
            if not self.list_values:
                # we don't quote if ``list_values=False``
                quot = noquot
            # for normal values either single or double quotes will do
            elif '\n' in value:
                # will only happen if multiline is off - e.g. '\n' in key
                raise ConfigObjError('Value "%s" cannot be safely quoted.' % value)
            elif ((value[0] not in wspace_plus) and
                    (value[-1] not in wspace_plus) and
                    (',' not in value)):
                quot = noquot
            else:
                quot = self._get_single_quote(value)
        else:
            # if value has '\n' or "'" *and* '"', it will need triple quotes
            quot = self._get_triple_quote(value)
        
        if quot == noquot and '#' in value and self.list_values:
            quot = self._get_single_quote(value)
                
        return quot % value
    
    
    def _get_single_quote(self, value):
        if ("'" in value) and ('"' in value):
            raise ConfigObjError('Value "%s" cannot be safely quoted.' % value)
        elif '"' in value:
            quot = squot
        else:
            quot = dquot
        return quot
    
    
    def _get_triple_quote(self, value):
        if (value.find('"""') != -1) and (value.find("'''") != -1):
            raise ConfigObjError('Value "%s" cannot be safely quoted.' % value)
        if value.find('"""') == -1:
            quot = tdquot
        else:
            quot = tsquot 
        return quot


    def _handle_value(self, value):
        """
        Given a value string, unquote, remove comment,
        handle lists. (including empty and single member lists)
        """
        if self._inspec:
            # Parsing a configspec so don't handle comments
            return (value, '')
        # do we look for lists in values ?
        if not self.list_values:
            mat = self._nolistvalue.match(value)
            if mat is None:
                raise SyntaxError()
            # NOTE: we don't unquote here
            return mat.groups()
        #
        mat = self._valueexp.match(value)
        if mat is None:
            # the value is badly constructed, probably badly quoted,
            # or an invalid list
            raise SyntaxError()
        (list_values, single, empty_list, comment) = mat.groups()
        if (list_values == '') and (single is None):
            # change this if you want to accept empty values
            raise SyntaxError()
        # NOTE: note there is no error handling from here if the regex
        # is wrong: then incorrect values will slip through
        if empty_list is not None:
            # the single comma - meaning an empty list
            return ([], comment)
        if single is not None:
            # handle empty values
            if list_values and not single:
                # FIXME: the '' is a workaround because our regex now matches
                #   '' at the end of a list if it has a trailing comma
                single = None
            else:
                single = single or '""'
                single = self._unquote(single)
        if list_values == '':
            # not a list value
            return (single, comment)
        the_list = self._listvalueexp.findall(list_values)
        the_list = [self._unquote(val) for val in the_list]
        if single is not None:
            the_list += [single]
        return (the_list, comment)


    def _multiline(self, value, infile, cur_index, maxline):
        """Extract the value, where we are in a multiline situation."""
        quot = value[:3]
        newvalue = value[3:]
        single_line = self._triple_quote[quot][0]
        multi_line = self._triple_quote[quot][1]
        mat = single_line.match(value)
        if mat is not None:
            retval = list(mat.groups())
            retval.append(cur_index)
            return retval
        elif newvalue.find(quot) != -1:
            # somehow the triple quote is missing
            raise SyntaxError()
        #
        while cur_index < maxline:
            cur_index += 1
            newvalue += '\n'
            line = infile[cur_index]
            if line.find(quot) == -1:
                newvalue += line
            else:
                # end of multiline, process it
                break
        else:
            # we've got to the end of the config, oops...
            raise SyntaxError()
        mat = multi_line.match(line)
        if mat is None:
            # a badly formed line
            raise SyntaxError()
        (value, comment) = mat.groups()
        return (newvalue + value, comment, cur_index)


    def _handle_configspec(self, configspec):
        """Parse the configspec."""
        # FIXME: Should we check that the configspec was created with the 
        #        correct settings ? (i.e. ``list_values=False``)
        if not isinstance(configspec, ConfigObj):
            try:
                configspec = ConfigObj(configspec,
                                       raise_errors=True,
                                       file_error=True,
                                       _inspec=True)
            except ConfigObjError, e:
                # FIXME: Should these errors have a reference
                #        to the already parsed ConfigObj ?
                raise ConfigspecError('Parsing configspec failed: %s' % e)
            except IOError, e:
                raise IOError('Reading configspec failed: %s' % e)
        
        self.configspec = configspec
            

        
    def _set_configspec(self, section, copy):
        """
        Called by validate. Handles setting the configspec on subsections
        including sections to be validated by __many__
        """
        configspec = section.configspec
        many = configspec.get('__many__')
        if isinstance(many, dict):
            for entry in section.sections:
                if entry not in configspec:
                    section[entry].configspec = many
                    
        for entry in configspec.sections:
            if entry == '__many__':
                continue
            if entry not in section:
                section[entry] = {}
                section[entry]._created = True
                if copy:
                    # copy comments
                    section.comments[entry] = configspec.comments.get(entry, [])
                    section.inline_comments[entry] = configspec.inline_comments.get(entry, '')
                
            # Could be a scalar when we expect a section
            if isinstance(section[entry], Section):
                section[entry].configspec = configspec[entry]
                        

    def _write_line(self, indent_string, entry, this_entry, comment):
        """Write an individual line, for the write method"""
        # NOTE: the calls to self._quote here handles non-StringType values.
        if not self.unrepr:
            val = self._decode_element(self._quote(this_entry))
        else:
            val = repr(this_entry)
        return '%s%s%s%s%s' % (indent_string,
                               self._decode_element(self._quote(entry, multiline=False)),
                               self._a_to_u(' = '),
                               val,
                               self._decode_element(comment))


    def _write_marker(self, indent_string, depth, entry, comment):
        """Write a section marker line"""
        return '%s%s%s%s%s' % (indent_string,
                               self._a_to_u('[' * depth),
                               self._quote(self._decode_element(entry), multiline=False),
                               self._a_to_u(']' * depth),
                               self._decode_element(comment))


    def _handle_comment(self, comment):
        """Deal with a comment."""
        if not comment:
            return ''
        start = self.indent_type
        if not comment.startswith('#'):
            start += self._a_to_u(' # ')
        return (start + comment)


    # Public methods

    def write(self, outfile=None, section=None):
        """
        Write the current ConfigObj as a file
        
        tekNico: FIXME: use StringIO instead of real files
        
        >>> filename = a.filename
        >>> a.filename = 'test.ini'
        >>> a.write()
        >>> a.filename = filename
        >>> a == ConfigObj('test.ini', raise_errors=True)
        1
        """
        if self.indent_type is None:
            # this can be true if initialised from a dictionary
            self.indent_type = DEFAULT_INDENT_TYPE
            
        out = []
        cs = self._a_to_u('#')
        csp = self._a_to_u('# ')
        if section is None:
            int_val = self.interpolation
            self.interpolation = False
            section = self
            for line in self.initial_comment:
                line = self._decode_element(line)
                stripped_line = line.strip()
                if stripped_line and not stripped_line.startswith(cs):
                    line = csp + line
                out.append(line)
                
        indent_string = self.indent_type * section.depth
        for entry in (section.scalars + section.sections):
            if entry in section.defaults:
                # don't write out default values
                continue
            for comment_line in section.comments[entry]:
                comment_line = self._decode_element(comment_line.lstrip())
                if comment_line and not comment_line.startswith(cs):
                    comment_line = csp + comment_line
                out.append(indent_string + comment_line)
            this_entry = section[entry]
            comment = self._handle_comment(section.inline_comments[entry])
            
            if isinstance(this_entry, dict):
                # a section
                out.append(self._write_marker(
                    indent_string,
                    this_entry.depth,
                    entry,
                    comment))
                out.extend(self.write(section=this_entry))
            else:
                out.append(self._write_line(
                    indent_string,
                    entry,
                    this_entry,
                    comment))
                
        if section is self:
            for line in self.final_comment:
                line = self._decode_element(line)
                stripped_line = line.strip()
                if stripped_line and not stripped_line.startswith(cs):
                    line = csp + line
                out.append(line)
            self.interpolation = int_val
            
        if section is not self:
            return out
        
        if (self.filename is None) and (outfile is None):
            # output a list of lines
            # might need to encode
            # NOTE: This will *screw* UTF16, each line will start with the BOM
            if self.encoding:
                out = [l.encode(self.encoding) for l in out]
            if (self.BOM and ((self.encoding is None) or
                (BOM_LIST.get(self.encoding.lower()) == 'utf_8'))):
                # Add the UTF8 BOM
                if not out:
                    out.append('')
                out[0] = BOM_UTF8 + out[0]
            return out
        
        # Turn the list to a string, joined with correct newlines
        newline = self.newlines or os.linesep
        output = self._a_to_u(newline).join(out)
        if self.encoding:
            output = output.encode(self.encoding)
        if self.BOM and ((self.encoding is None) or match_utf8(self.encoding)):
            # Add the UTF8 BOM
            output = BOM_UTF8 + output
            
        if not output.endswith(newline):
            output += newline
        if outfile is not None:
            outfile.write(output)
        else:
            h = open(self.filename, 'wb')
            h.write(output)
            h.close()


    def validate(self, validator, preserve_errors=False, copy=False,
                 section=None):
        """
        Test the ConfigObj against a configspec.
        
        It uses the ``validator`` object from *validate.py*.
        
        To run ``validate`` on the current ConfigObj, call: ::
        
            test = config.validate(validator)
        
        (Normally having previously passed in the configspec when the ConfigObj
        was created - you can dynamically assign a dictionary of checks to the
        ``configspec`` attribute of a section though).
        
        It returns ``True`` if everything passes, or a dictionary of
        pass/fails (True/False). If every member of a subsection passes, it
        will just have the value ``True``. (It also returns ``False`` if all
        members fail).
        
        In addition, it converts the values from strings to their native
        types if their checks pass (and ``stringify`` is set).
        
        If ``preserve_errors`` is ``True`` (``False`` is default) then instead
        of a marking a fail with a ``False``, it will preserve the actual
        exception object. This can contain info about the reason for failure.
        For example the ``VdtValueTooSmallError`` indicates that the value
        supplied was too small. If a value (or section) is missing it will
        still be marked as ``False``.
        
        You must have the validate module to use ``preserve_errors=True``.
        
        You can then use the ``flatten_errors`` function to turn your nested
        results dictionary into a flattened list of failures - useful for
        displaying meaningful error messages.
        """
        if section is None:
            if self.configspec is None:
                raise ValueError('No configspec supplied.')
            if preserve_errors:
                # We do this once to remove a top level dependency on the validate module
                # Which makes importing configobj faster
                from validate import VdtMissingValue
                self._vdtMissingValue = VdtMissingValue
                
            section = self

            if copy:
                section.initial_comment = section.configspec.initial_comment
                section.final_comment = section.configspec.final_comment
                section.encoding = section.configspec.encoding
                section.BOM = section.configspec.BOM
                section.newlines = section.configspec.newlines
                section.indent_type = section.configspec.indent_type
            
        #
        # section.default_values.clear() #??
        configspec = section.configspec
        self._set_configspec(section, copy)

        
        def validate_entry(entry, spec, val, missing, ret_true, ret_false):
            section.default_values.pop(entry, None)
                
            try:
                section.default_values[entry] = validator.get_default_value(configspec[entry])
            except (KeyError, AttributeError, validator.baseErrorClass):
                # No default, bad default or validator has no 'get_default_value'
                # (e.g. SimpleVal)
                pass
            
            try:
                check = validator.check(spec,
                                        val,
                                        missing=missing
                                        )
            except validator.baseErrorClass, e:
                if not preserve_errors or isinstance(e, self._vdtMissingValue):
                    out[entry] = False
                else:
                    # preserve the error
                    out[entry] = e
                    ret_false = False
                ret_true = False
            else:
                ret_false = False
                out[entry] = True
                if self.stringify or missing:
                    # if we are doing type conversion
                    # or the value is a supplied default
                    if not self.stringify:
                        if isinstance(check, (list, tuple)):
                            # preserve lists
                            check = [self._str(item) for item in check]
                        elif missing and check is None:
                            # convert the None from a default to a ''
                            check = ''
                        else:
                            check = self._str(check)
                    if (check != val) or missing:
                        section[entry] = check
                if not copy and missing and entry not in section.defaults:
                    section.defaults.append(entry)
            return ret_true, ret_false
        
        #
        out = {}
        ret_true = True
        ret_false = True
        
        unvalidated = [k for k in section.scalars if k not in configspec]
        incorrect_sections = [k for k in configspec.sections if k in section.scalars]        
        incorrect_scalars = [k for k in configspec.scalars if k in section.sections]
        
        for entry in configspec.scalars:
            if entry in ('__many__', '___many___'):
                # reserved names
                continue
            if (not entry in section.scalars) or (entry in section.defaults):
                # missing entries
                # or entries from defaults
                missing = True
                val = None
                if copy and entry not in section.scalars:
                    # copy comments
                    section.comments[entry] = (
                        configspec.comments.get(entry, []))
                    section.inline_comments[entry] = (
                        configspec.inline_comments.get(entry, ''))
                #
            else:
                missing = False
                val = section[entry]
            
            ret_true, ret_false = validate_entry(entry, configspec[entry], val, 
                                                 missing, ret_true, ret_false)
        
        many = None
        if '__many__' in configspec.scalars:
            many = configspec['__many__']
        elif '___many___' in configspec.scalars:
            many = configspec['___many___']
        
        if many is not None:
            for entry in unvalidated:
                val = section[entry]
                ret_true, ret_false = validate_entry(entry, many, val, False,
                                                     ret_true, ret_false)
            unvalidated = []

        for entry in incorrect_scalars:
            ret_true = False
            if not preserve_errors:
                out[entry] = False
            else:
                ret_false = False
                msg = 'Value %r was provided as a section' % entry
                out[entry] = validator.baseErrorClass(msg)
        for entry in incorrect_sections:
            ret_true = False
            if not preserve_errors:
                out[entry] = False
            else:
                ret_false = False
                msg = 'Section %r was provided as a single value' % entry
                out[entry] = validator.baseErrorClass(msg)
                
        # Missing sections will have been created as empty ones when the
        # configspec was read.
        for entry in section.sections:
            # FIXME: this means DEFAULT is not copied in copy mode
            if section is self and entry == 'DEFAULT':
                continue
            if section[entry].configspec is None:
                unvalidated.append(entry)
                continue
            if copy:
                section.comments[entry] = configspec.comments.get(entry, [])
                section.inline_comments[entry] = configspec.inline_comments.get(entry, '')
            check = self.validate(validator, preserve_errors=preserve_errors, copy=copy, section=section[entry])
            out[entry] = check
            if check == False:
                ret_true = False
            elif check == True:
                ret_false = False
            else:
                ret_true = False
        
        section.extra_values = unvalidated
        if preserve_errors and not section._created:
            # If the section wasn't created (i.e. it wasn't missing)
            # then we can't return False, we need to preserve errors
            ret_false = False
        #
        if ret_false and preserve_errors and out:
            # If we are preserving errors, but all
            # the failures are from missing sections / values
            # then we can return False. Otherwise there is a
            # real failure that we need to preserve.
            ret_false = not any(out.values())
        if ret_true:
            return True
        elif ret_false:
            return False
        return out


    def reset(self):
        """Clear ConfigObj instance and restore to 'freshly created' state."""
        self.clear()
        self._initialise()
        # FIXME: Should be done by '_initialise', but ConfigObj constructor (and reload)
        #        requires an empty dictionary
        self.configspec = None
        # Just to be sure ;-)
        self._original_configspec = None
        
        
    def reload(self):
        """
        Reload a ConfigObj from file.
        
        This method raises a ``ReloadError`` if the ConfigObj doesn't have
        a filename attribute pointing to a file.
        """
        if not isinstance(self.filename, basestring):
            raise ReloadError()

        filename = self.filename
        current_options = {}
        for entry in OPTION_DEFAULTS:
            if entry == 'configspec':
                continue
            current_options[entry] = getattr(self, entry)
            
        configspec = self._original_configspec
        current_options['configspec'] = configspec
            
        self.clear()
        self._initialise(current_options)
        self._load(filename, configspec)
        


class SimpleVal(object):
    """
    A simple validator.
    Can be used to check that all members expected are present.
    
    To use it, provide a configspec with all your members in (the value given
    will be ignored). Pass an instance of ``SimpleVal`` to the ``validate``
    method of your ``ConfigObj``. ``validate`` will return ``True`` if all
    members are present, or a dictionary with True/False meaning
    present/missing. (Whole missing sections will be replaced with ``False``)
    """
    
    def __init__(self):
        self.baseErrorClass = ConfigObjError
    
    def check(self, check, member, missing=False):
        """A dummy check method, always returns the value unchanged."""
        if missing:
            raise self.baseErrorClass()
        return member


def flatten_errors(cfg, res, levels=None, results=None):
    """
    An example function that will turn a nested dictionary of results
    (as returned by ``ConfigObj.validate``) into a flat list.
    
    ``cfg`` is the ConfigObj instance being checked, ``res`` is the results
    dictionary returned by ``validate``.
    
    (This is a recursive function, so you shouldn't use the ``levels`` or
    ``results`` arguments - they are used by the function.)
    
    Returns a list of keys that failed. Each member of the list is a tuple::
    
        ([list of sections...], key, result)
    
    If ``validate`` was called with ``preserve_errors=False`` (the default)
    then ``result`` will always be ``False``.

    *list of sections* is a flattened list of sections that the key was found
    in.
    
    If the section was missing (or a section was expected and a scalar provided
    - or vice-versa) then key will be ``None``.
    
    If the value (or section) was missing then ``result`` will be ``False``.
    
    If ``validate`` was called with ``preserve_errors=True`` and a value
    was present, but failed the check, then ``result`` will be the exception
    object returned. You can use this as a string that describes the failure.
    
    For example *The value "3" is of the wrong type*.
    """
    if levels is None:
        # first time called
        levels = []
        results = []
    if res == True:
        return results
    if res == False or isinstance(res, Exception):
        results.append((levels[:], None, res))
        if levels:
            levels.pop()
        return results
    for (key, val) in res.items():
        if val == True:
            continue
        if isinstance(cfg.get(key), dict):
            # Go down one level
            levels.append(key)
            flatten_errors(cfg[key], val, levels, results)
            continue
        results.append((levels[:], key, val))
    #
    # Go up one level
    if levels:
        levels.pop()
    #
    return results


def get_extra_values(conf, _prepend=()):
    """
    Find all the values and sections not in the configspec from a validated
    ConfigObj.
    
    ``get_extra_values`` returns a list of tuples where each tuple represents
    either an extra section, or an extra value.
    
    The tuples contain two values, a tuple representing the section the value 
    is in and the name of the extra values. For extra values in the top level
    section the first member will be an empty tuple. For values in the 'foo'
    section the first member will be ``('foo',)``. For members in the 'bar'
    subsection of the 'foo' section the first member will be ``('foo', 'bar')``.
    
    NOTE: If you call ``get_extra_values`` on a ConfigObj instance that hasn't
    been validated it will return an empty list.
    """
    out = []
    
    out.extend([(_prepend, name) for name in conf.extra_values])
    for name in conf.sections:
        if name not in conf.extra_values:
            out.extend(get_extra_values(conf[name], _prepend + (name,)))
    return out


"""*A programming language is a medium of expression.* - Paul Graham"""

########NEW FILE########
__FILENAME__ = validate
# validate.py
# A Validator object
# Copyright (C) 2005-2010 Michael Foord, Mark Andrews, Nicola Larosa
# E-mail: fuzzyman AT voidspace DOT org DOT uk
#         mark AT la-la DOT com
#         nico AT tekNico DOT net

# This software is licensed under the terms of the BSD license.
# http://www.voidspace.org.uk/python/license.shtml
# Basically you're free to copy, modify, distribute and relicense it,
# So long as you keep a copy of the license with it.

# Scripts maintained at http://www.voidspace.org.uk/python/index.shtml
# For information about bugfixes, updates and support, please join the
# ConfigObj mailing list:
# http://lists.sourceforge.net/lists/listinfo/configobj-develop
# Comments, suggestions and bug reports welcome.

"""
    The Validator object is used to check that supplied values 
    conform to a specification.
    
    The value can be supplied as a string - e.g. from a config file.
    In this case the check will also *convert* the value to
    the required type. This allows you to add validation
    as a transparent layer to access data stored as strings.
    The validation checks that the data is correct *and*
    converts it to the expected type.
    
    Some standard checks are provided for basic data types.
    Additional checks are easy to write. They can be
    provided when the ``Validator`` is instantiated or
    added afterwards.
    
    The standard functions work with the following basic data types :
    
    * integers
    * floats
    * booleans
    * strings
    * ip_addr
    
    plus lists of these datatypes
    
    Adding additional checks is done through coding simple functions.
    
    The full set of standard checks are : 
    
    * 'integer': matches integer values (including negative)
                 Takes optional 'min' and 'max' arguments : ::
    
                   integer()
                   integer(3, 9)  # any value from 3 to 9
                   integer(min=0) # any positive value
                   integer(max=9)
    
    * 'float': matches float values
               Has the same parameters as the integer check.
    
    * 'boolean': matches boolean values - ``True`` or ``False``
                 Acceptable string values for True are :
                   true, on, yes, 1
                 Acceptable string values for False are :
                   false, off, no, 0
    
                 Any other value raises an error.
    
    * 'ip_addr': matches an Internet Protocol address, v.4, represented
                 by a dotted-quad string, i.e. '1.2.3.4'.
    
    * 'string': matches any string.
                Takes optional keyword args 'min' and 'max'
                to specify min and max lengths of the string.
    
    * 'list': matches any list.
              Takes optional keyword args 'min', and 'max' to specify min and
              max sizes of the list. (Always returns a list.)
    
    * 'tuple': matches any tuple.
              Takes optional keyword args 'min', and 'max' to specify min and
              max sizes of the tuple. (Always returns a tuple.)
    
    * 'int_list': Matches a list of integers.
                  Takes the same arguments as list.
    
    * 'float_list': Matches a list of floats.
                    Takes the same arguments as list.
    
    * 'bool_list': Matches a list of boolean values.
                   Takes the same arguments as list.
    
    * 'ip_addr_list': Matches a list of IP addresses.
                     Takes the same arguments as list.
    
    * 'string_list': Matches a list of strings.
                     Takes the same arguments as list.
    
    * 'mixed_list': Matches a list with different types in 
                    specific positions. List size must match
                    the number of arguments.
    
                    Each position can be one of :
                    'integer', 'float', 'ip_addr', 'string', 'boolean'
    
                    So to specify a list with two strings followed
                    by two integers, you write the check as : ::
    
                      mixed_list('string', 'string', 'integer', 'integer')
    
    * 'pass': This check matches everything ! It never fails
              and the value is unchanged.
    
              It is also the default if no check is specified.
    
    * 'option': This check matches any from a list of options.
                You specify this check with : ::
    
                  option('option 1', 'option 2', 'option 3')
    
    You can supply a default value (returned if no value is supplied)
    using the default keyword argument.
    
    You specify a list argument for default using a list constructor syntax in
    the check : ::
    
        checkname(arg1, arg2, default=list('val 1', 'val 2', 'val 3'))
    
    A badly formatted set of arguments will raise a ``VdtParamError``.
"""

__version__ = '1.0.1'


__all__ = (
    '__version__',
    'dottedQuadToNum',
    'numToDottedQuad',
    'ValidateError',
    'VdtUnknownCheckError',
    'VdtParamError',
    'VdtTypeError',
    'VdtValueError',
    'VdtValueTooSmallError',
    'VdtValueTooBigError',
    'VdtValueTooShortError',
    'VdtValueTooLongError',
    'VdtMissingValue',
    'Validator',
    'is_integer',
    'is_float',
    'is_boolean',
    'is_list',
    'is_tuple',
    'is_ip_addr',
    'is_string',
    'is_int_list',
    'is_bool_list',
    'is_float_list',
    'is_string_list',
    'is_ip_addr_list',
    'is_mixed_list',
    'is_option',
    '__docformat__',
)


import re


_list_arg = re.compile(r'''
    (?:
        ([a-zA-Z_][a-zA-Z0-9_]*)\s*=\s*list\(
            (
                (?:
                    \s*
                    (?:
                        (?:".*?")|              # double quotes
                        (?:'.*?')|              # single quotes
                        (?:[^'",\s\)][^,\)]*?)  # unquoted
                    )
                    \s*,\s*
                )*
                (?:
                    (?:".*?")|              # double quotes
                    (?:'.*?')|              # single quotes
                    (?:[^'",\s\)][^,\)]*?)  # unquoted
                )?                          # last one
            )
        \)
    )
''', re.VERBOSE | re.DOTALL)    # two groups

_list_members = re.compile(r'''
    (
        (?:".*?")|              # double quotes
        (?:'.*?')|              # single quotes
        (?:[^'",\s=][^,=]*?)       # unquoted
    )
    (?:
    (?:\s*,\s*)|(?:\s*$)            # comma
    )
''', re.VERBOSE | re.DOTALL)    # one group

_paramstring = r'''
    (?:
        (
            (?:
                [a-zA-Z_][a-zA-Z0-9_]*\s*=\s*list\(
                    (?:
                        \s*
                        (?:
                            (?:".*?")|              # double quotes
                            (?:'.*?')|              # single quotes
                            (?:[^'",\s\)][^,\)]*?)       # unquoted
                        )
                        \s*,\s*
                    )*
                    (?:
                        (?:".*?")|              # double quotes
                        (?:'.*?')|              # single quotes
                        (?:[^'",\s\)][^,\)]*?)       # unquoted
                    )?                              # last one
                \)
            )|
            (?:
                (?:".*?")|              # double quotes
                (?:'.*?')|              # single quotes
                (?:[^'",\s=][^,=]*?)|       # unquoted
                (?:                         # keyword argument
                    [a-zA-Z_][a-zA-Z0-9_]*\s*=\s*
                    (?:
                        (?:".*?")|              # double quotes
                        (?:'.*?')|              # single quotes
                        (?:[^'",\s=][^,=]*?)       # unquoted
                    )
                )
            )
        )
        (?:
            (?:\s*,\s*)|(?:\s*$)            # comma
        )
    )
    '''

_matchstring = '^%s*' % _paramstring

# Python pre 2.2.1 doesn't have bool
try:
    bool
except NameError:
    def bool(val):
        """Simple boolean equivalent function. """
        if val:
            return 1
        else:
            return 0


def dottedQuadToNum(ip):
    """
    Convert decimal dotted quad string to long integer
    
    >>> int(dottedQuadToNum('1 '))
    1
    >>> int(dottedQuadToNum(' 1.2'))
    16777218
    >>> int(dottedQuadToNum(' 1.2.3 '))
    16908291
    >>> int(dottedQuadToNum('1.2.3.4'))
    16909060
    >>> dottedQuadToNum('1.2.3. 4')
    16909060
    >>> dottedQuadToNum('255.255.255.255')
    4294967295L
    >>> dottedQuadToNum('255.255.255.256')
    Traceback (most recent call last):
    ValueError: Not a good dotted-quad IP: 255.255.255.256
    """
    
    # import here to avoid it when ip_addr values are not used
    import socket, struct
    
    try:
        return struct.unpack('!L',
            socket.inet_aton(ip.strip()))[0]
    except socket.error:
        # bug in inet_aton, corrected in Python 2.3
        if ip.strip() == '255.255.255.255':
            return 0xFFFFFFFFL
        else:
            raise ValueError('Not a good dotted-quad IP: %s' % ip)
    return


def numToDottedQuad(num):
    """
    Convert long int to dotted quad string
    
    >>> numToDottedQuad(-1L)
    Traceback (most recent call last):
    ValueError: Not a good numeric IP: -1
    >>> numToDottedQuad(1L)
    '0.0.0.1'
    >>> numToDottedQuad(16777218L)
    '1.0.0.2'
    >>> numToDottedQuad(16908291L)
    '1.2.0.3'
    >>> numToDottedQuad(16909060L)
    '1.2.3.4'
    >>> numToDottedQuad(4294967295L)
    '255.255.255.255'
    >>> numToDottedQuad(4294967296L)
    Traceback (most recent call last):
    ValueError: Not a good numeric IP: 4294967296
    """
    
    # import here to avoid it when ip_addr values are not used
    import socket, struct
    
    # no need to intercept here, 4294967295L is fine
    if num > 4294967295L or num < 0:
        raise ValueError('Not a good numeric IP: %s' % num)
    try:
        return socket.inet_ntoa(
            struct.pack('!L', long(num)))
    except (socket.error, struct.error, OverflowError):
        raise ValueError('Not a good numeric IP: %s' % num)


class ValidateError(Exception):
    """
    This error indicates that the check failed.
    It can be the base class for more specific errors.
    
    Any check function that fails ought to raise this error.
    (or a subclass)
    
    >>> raise ValidateError
    Traceback (most recent call last):
    ValidateError
    """


class VdtMissingValue(ValidateError):
    """No value was supplied to a check that needed one."""


class VdtUnknownCheckError(ValidateError):
    """An unknown check function was requested"""

    def __init__(self, value):
        """
        >>> raise VdtUnknownCheckError('yoda')
        Traceback (most recent call last):
        VdtUnknownCheckError: the check "yoda" is unknown.
        """
        ValidateError.__init__(self, 'the check "%s" is unknown.' % (value,))


class VdtParamError(SyntaxError):
    """An incorrect parameter was passed"""

    def __init__(self, name, value):
        """
        >>> raise VdtParamError('yoda', 'jedi')
        Traceback (most recent call last):
        VdtParamError: passed an incorrect value "jedi" for parameter "yoda".
        """
        SyntaxError.__init__(self, 'passed an incorrect value "%s" for parameter "%s".' % (value, name))


class VdtTypeError(ValidateError):
    """The value supplied was of the wrong type"""

    def __init__(self, value):
        """
        >>> raise VdtTypeError('jedi')
        Traceback (most recent call last):
        VdtTypeError: the value "jedi" is of the wrong type.
        """
        ValidateError.__init__(self, 'the value "%s" is of the wrong type.' % (value,))


class VdtValueError(ValidateError):
    """The value supplied was of the correct type, but was not an allowed value."""
    
    def __init__(self, value):
        """
        >>> raise VdtValueError('jedi')
        Traceback (most recent call last):
        VdtValueError: the value "jedi" is unacceptable.
        """
        ValidateError.__init__(self, 'the value "%s" is unacceptable.' % (value,))


class VdtValueTooSmallError(VdtValueError):
    """The value supplied was of the correct type, but was too small."""

    def __init__(self, value):
        """
        >>> raise VdtValueTooSmallError('0')
        Traceback (most recent call last):
        VdtValueTooSmallError: the value "0" is too small.
        """
        ValidateError.__init__(self, 'the value "%s" is too small.' % (value,))


class VdtValueTooBigError(VdtValueError):
    """The value supplied was of the correct type, but was too big."""

    def __init__(self, value):
        """
        >>> raise VdtValueTooBigError('1')
        Traceback (most recent call last):
        VdtValueTooBigError: the value "1" is too big.
        """
        ValidateError.__init__(self, 'the value "%s" is too big.' % (value,))


class VdtValueTooShortError(VdtValueError):
    """The value supplied was of the correct type, but was too short."""

    def __init__(self, value):
        """
        >>> raise VdtValueTooShortError('jed')
        Traceback (most recent call last):
        VdtValueTooShortError: the value "jed" is too short.
        """
        ValidateError.__init__(
            self,
            'the value "%s" is too short.' % (value,))


class VdtValueTooLongError(VdtValueError):
    """The value supplied was of the correct type, but was too long."""

    def __init__(self, value):
        """
        >>> raise VdtValueTooLongError('jedie')
        Traceback (most recent call last):
        VdtValueTooLongError: the value "jedie" is too long.
        """
        ValidateError.__init__(self, 'the value "%s" is too long.' % (value,))


class Validator(object):
    """
    Validator is an object that allows you to register a set of 'checks'.
    These checks take input and test that it conforms to the check.
    
    This can also involve converting the value from a string into
    the correct datatype.
    
    The ``check`` method takes an input string which configures which
    check is to be used and applies that check to a supplied value.
    
    An example input string would be:
    'int_range(param1, param2)'
    
    You would then provide something like:
    
    >>> def int_range_check(value, min, max):
    ...     # turn min and max from strings to integers
    ...     min = int(min)
    ...     max = int(max)
    ...     # check that value is of the correct type.
    ...     # possible valid inputs are integers or strings
    ...     # that represent integers
    ...     if not isinstance(value, (int, long, basestring)):
    ...         raise VdtTypeError(value)
    ...     elif isinstance(value, basestring):
    ...         # if we are given a string
    ...         # attempt to convert to an integer
    ...         try:
    ...             value = int(value)
    ...         except ValueError:
    ...             raise VdtValueError(value)
    ...     # check the value is between our constraints
    ...     if not min <= value:
    ...          raise VdtValueTooSmallError(value)
    ...     if not value <= max:
    ...          raise VdtValueTooBigError(value)
    ...     return value
    
    >>> fdict = {'int_range': int_range_check}
    >>> vtr1 = Validator(fdict)
    >>> vtr1.check('int_range(20, 40)', '30')
    30
    >>> vtr1.check('int_range(20, 40)', '60')
    Traceback (most recent call last):
    VdtValueTooBigError: the value "60" is too big.
    
    New functions can be added with : ::
    
    >>> vtr2 = Validator()       
    >>> vtr2.functions['int_range'] = int_range_check
    
    Or by passing in a dictionary of functions when Validator 
    is instantiated.
    
    Your functions *can* use keyword arguments,
    but the first argument should always be 'value'.
    
    If the function doesn't take additional arguments,
    the parentheses are optional in the check.
    It can be written with either of : ::
    
        keyword = function_name
        keyword = function_name()
    
    The first program to utilise Validator() was Michael Foord's
    ConfigObj, an alternative to ConfigParser which supports lists and
    can validate a config file using a config schema.
    For more details on using Validator with ConfigObj see:
    http://www.voidspace.org.uk/python/configobj.html
    """

    # this regex does the initial parsing of the checks
    _func_re = re.compile(r'(.+?)\((.*)\)', re.DOTALL)

    # this regex takes apart keyword arguments
    _key_arg = re.compile(r'^([a-zA-Z_][a-zA-Z0-9_]*)\s*=\s*(.*)$',  re.DOTALL)


    # this regex finds keyword=list(....) type values
    _list_arg = _list_arg

    # this regex takes individual values out of lists - in one pass
    _list_members = _list_members

    # These regexes check a set of arguments for validity
    # and then pull the members out
    _paramfinder = re.compile(_paramstring, re.VERBOSE | re.DOTALL)
    _matchfinder = re.compile(_matchstring, re.VERBOSE | re.DOTALL)


    def __init__(self, functions=None):
        """
        >>> vtri = Validator()
        """
        self.functions = {
            '': self._pass,
            'integer': is_integer,
            'float': is_float,
            'boolean': is_boolean,
            'ip_addr': is_ip_addr,
            'string': is_string,
            'list': is_list,
            'tuple': is_tuple,
            'int_list': is_int_list,
            'float_list': is_float_list,
            'bool_list': is_bool_list,
            'ip_addr_list': is_ip_addr_list,
            'string_list': is_string_list,
            'mixed_list': is_mixed_list,
            'pass': self._pass,
            'option': is_option,
            'force_list': force_list,
        }
        if functions is not None:
            self.functions.update(functions)
        # tekNico: for use by ConfigObj
        self.baseErrorClass = ValidateError
        self._cache = {}


    def check(self, check, value, missing=False):
        """
        Usage: check(check, value)
        
        Arguments:
            check: string representing check to apply (including arguments)
            value: object to be checked
        Returns value, converted to correct type if necessary
        
        If the check fails, raises a ``ValidateError`` subclass.
        
        >>> vtor.check('yoda', '')
        Traceback (most recent call last):
        VdtUnknownCheckError: the check "yoda" is unknown.
        >>> vtor.check('yoda()', '')
        Traceback (most recent call last):
        VdtUnknownCheckError: the check "yoda" is unknown.
        
        >>> vtor.check('string(default="")', '', missing=True)
        ''
        """
        fun_name, fun_args, fun_kwargs, default = self._parse_with_caching(check)
            
        if missing:
            if default is None:
                # no information needed here - to be handled by caller
                raise VdtMissingValue()
            value = self._handle_none(default)
                
        if value is None:
            return None
        
        return self._check_value(value, fun_name, fun_args, fun_kwargs)


    def _handle_none(self, value):
        if value == 'None':
            value = None
        elif value in ("'None'", '"None"'):
            # Special case a quoted None
            value = self._unquote(value)
        return value


    def _parse_with_caching(self, check):
        if check in self._cache:
            fun_name, fun_args, fun_kwargs, default = self._cache[check]
            # We call list and dict below to work with *copies* of the data
            # rather than the original (which are mutable of course)
            fun_args = list(fun_args)
            fun_kwargs = dict(fun_kwargs)
        else:
            fun_name, fun_args, fun_kwargs, default = self._parse_check(check)
            fun_kwargs = dict([(str(key), value) for (key, value) in fun_kwargs.items()])
            self._cache[check] = fun_name, list(fun_args), dict(fun_kwargs), default
        return fun_name, fun_args, fun_kwargs, default
        
        
    def _check_value(self, value, fun_name, fun_args, fun_kwargs):
        try:
            fun = self.functions[fun_name]
        except KeyError:
            raise VdtUnknownCheckError(fun_name)
        else:
            return fun(value, *fun_args, **fun_kwargs)


    def _parse_check(self, check):
        fun_match = self._func_re.match(check)
        if fun_match:
            fun_name = fun_match.group(1)
            arg_string = fun_match.group(2)
            arg_match = self._matchfinder.match(arg_string)
            if arg_match is None:
                # Bad syntax
                raise VdtParamError('Bad syntax in check "%s".' % check)
            fun_args = []
            fun_kwargs = {}
            # pull out args of group 2
            for arg in self._paramfinder.findall(arg_string):
                # args may need whitespace removing (before removing quotes)
                arg = arg.strip()
                listmatch = self._list_arg.match(arg)
                if listmatch:
                    key, val = self._list_handle(listmatch)
                    fun_kwargs[key] = val
                    continue
                keymatch = self._key_arg.match(arg)
                if keymatch:
                    val = keymatch.group(2)
                    if not val in ("'None'", '"None"'):
                        # Special case a quoted None
                        val = self._unquote(val)
                    fun_kwargs[keymatch.group(1)] = val
                    continue
                
                fun_args.append(self._unquote(arg))
        else:
            # allows for function names without (args)
            return check, (), {}, None

        # Default must be deleted if the value is specified too,
        # otherwise the check function will get a spurious "default" keyword arg
        try:
            default = fun_kwargs.pop('default', None)
        except AttributeError:
            # Python 2.2 compatibility
            default = None
            try:
                default = fun_kwargs['default']
                del fun_kwargs['default']
            except KeyError:
                pass
            
        return fun_name, fun_args, fun_kwargs, default


    def _unquote(self, val):
        """Unquote a value if necessary."""
        if (len(val) >= 2) and (val[0] in ("'", '"')) and (val[0] == val[-1]):
            val = val[1:-1]
        return val


    def _list_handle(self, listmatch):
        """Take apart a ``keyword=list('val, 'val')`` type string."""
        out = []
        name = listmatch.group(1)
        args = listmatch.group(2)
        for arg in self._list_members.findall(args):
            out.append(self._unquote(arg))
        return name, out


    def _pass(self, value):
        """
        Dummy check that always passes
        
        >>> vtor.check('', 0)
        0
        >>> vtor.check('', '0')
        '0'
        """
        return value
    
    
    def get_default_value(self, check):
        """
        Given a check, return the default value for the check
        (converted to the right type).
        
        If the check doesn't specify a default value then a
        ``KeyError`` will be raised.
        """
        fun_name, fun_args, fun_kwargs, default = self._parse_with_caching(check)
        if default is None:
            raise KeyError('Check "%s" has no default value.' % check)
        value = self._handle_none(default)
        if value is None:
            return value
        return self._check_value(value, fun_name, fun_args, fun_kwargs)


def _is_num_param(names, values, to_float=False):
    """
    Return numbers from inputs or raise VdtParamError.
    
    Lets ``None`` pass through.
    Pass in keyword argument ``to_float=True`` to
    use float for the conversion rather than int.
    
    >>> _is_num_param(('', ''), (0, 1.0))
    [0, 1]
    >>> _is_num_param(('', ''), (0, 1.0), to_float=True)
    [0.0, 1.0]
    >>> _is_num_param(('a'), ('a'))
    Traceback (most recent call last):
    VdtParamError: passed an incorrect value "a" for parameter "a".
    """
    fun = to_float and float or int
    out_params = []
    for (name, val) in zip(names, values):
        if val is None:
            out_params.append(val)
        elif isinstance(val, (int, long, float, basestring)):
            try:
                out_params.append(fun(val))
            except ValueError, e:
                raise VdtParamError(name, val)
        else:
            raise VdtParamError(name, val)
    return out_params


# built in checks
# you can override these by setting the appropriate name
# in Validator.functions
# note: if the params are specified wrongly in your input string,
#       you will also raise errors.

def is_integer(value, min=None, max=None):
    """
    A check that tests that a given value is an integer (int, or long)
    and optionally, between bounds. A negative value is accepted, while
    a float will fail.
    
    If the value is a string, then the conversion is done - if possible.
    Otherwise a VdtError is raised.
    
    >>> vtor.check('integer', '-1')
    -1
    >>> vtor.check('integer', '0')
    0
    >>> vtor.check('integer', 9)
    9
    >>> vtor.check('integer', 'a')
    Traceback (most recent call last):
    VdtTypeError: the value "a" is of the wrong type.
    >>> vtor.check('integer', '2.2')
    Traceback (most recent call last):
    VdtTypeError: the value "2.2" is of the wrong type.
    >>> vtor.check('integer(10)', '20')
    20
    >>> vtor.check('integer(max=20)', '15')
    15
    >>> vtor.check('integer(10)', '9')
    Traceback (most recent call last):
    VdtValueTooSmallError: the value "9" is too small.
    >>> vtor.check('integer(10)', 9)
    Traceback (most recent call last):
    VdtValueTooSmallError: the value "9" is too small.
    >>> vtor.check('integer(max=20)', '35')
    Traceback (most recent call last):
    VdtValueTooBigError: the value "35" is too big.
    >>> vtor.check('integer(max=20)', 35)
    Traceback (most recent call last):
    VdtValueTooBigError: the value "35" is too big.
    >>> vtor.check('integer(0, 9)', False)
    0
    """
    (min_val, max_val) = _is_num_param(('min', 'max'), (min, max))
    if not isinstance(value, (int, long, basestring)):
        raise VdtTypeError(value)
    if isinstance(value, basestring):
        # if it's a string - does it represent an integer ?
        try:
            value = int(value)
        except ValueError:
            raise VdtTypeError(value)
    if (min_val is not None) and (value < min_val):
        raise VdtValueTooSmallError(value)
    if (max_val is not None) and (value > max_val):
        raise VdtValueTooBigError(value)
    return value


def is_float(value, min=None, max=None):
    """
    A check that tests that a given value is a float
    (an integer will be accepted), and optionally - that it is between bounds.
    
    If the value is a string, then the conversion is done - if possible.
    Otherwise a VdtError is raised.
    
    This can accept negative values.
    
    >>> vtor.check('float', '2')
    2.0
    
    From now on we multiply the value to avoid comparing decimals
    
    >>> vtor.check('float', '-6.8') * 10
    -68.0
    >>> vtor.check('float', '12.2') * 10
    122.0
    >>> vtor.check('float', 8.4) * 10
    84.0
    >>> vtor.check('float', 'a')
    Traceback (most recent call last):
    VdtTypeError: the value "a" is of the wrong type.
    >>> vtor.check('float(10.1)', '10.2') * 10
    102.0
    >>> vtor.check('float(max=20.2)', '15.1') * 10
    151.0
    >>> vtor.check('float(10.0)', '9.0')
    Traceback (most recent call last):
    VdtValueTooSmallError: the value "9.0" is too small.
    >>> vtor.check('float(max=20.0)', '35.0')
    Traceback (most recent call last):
    VdtValueTooBigError: the value "35.0" is too big.
    """
    (min_val, max_val) = _is_num_param(
        ('min', 'max'), (min, max), to_float=True)
    if not isinstance(value, (int, long, float, basestring)):
        raise VdtTypeError(value)
    if not isinstance(value, float):
        # if it's a string - does it represent a float ?
        try:
            value = float(value)
        except ValueError:
            raise VdtTypeError(value)
    if (min_val is not None) and (value < min_val):
        raise VdtValueTooSmallError(value)
    if (max_val is not None) and (value > max_val):
        raise VdtValueTooBigError(value)
    return value


bool_dict = {
    True: True, 'on': True, '1': True, 'true': True, 'yes': True, 
    False: False, 'off': False, '0': False, 'false': False, 'no': False,
}


def is_boolean(value):
    """
    Check if the value represents a boolean.
    
    >>> vtor.check('boolean', 0)
    0
    >>> vtor.check('boolean', False)
    0
    >>> vtor.check('boolean', '0')
    0
    >>> vtor.check('boolean', 'off')
    0
    >>> vtor.check('boolean', 'false')
    0
    >>> vtor.check('boolean', 'no')
    0
    >>> vtor.check('boolean', 'nO')
    0
    >>> vtor.check('boolean', 'NO')
    0
    >>> vtor.check('boolean', 1)
    1
    >>> vtor.check('boolean', True)
    1
    >>> vtor.check('boolean', '1')
    1
    >>> vtor.check('boolean', 'on')
    1
    >>> vtor.check('boolean', 'true')
    1
    >>> vtor.check('boolean', 'yes')
    1
    >>> vtor.check('boolean', 'Yes')
    1
    >>> vtor.check('boolean', 'YES')
    1
    >>> vtor.check('boolean', '')
    Traceback (most recent call last):
    VdtTypeError: the value "" is of the wrong type.
    >>> vtor.check('boolean', 'up')
    Traceback (most recent call last):
    VdtTypeError: the value "up" is of the wrong type.
    
    """
    if isinstance(value, basestring):
        try:
            return bool_dict[value.lower()]
        except KeyError:
            raise VdtTypeError(value)
    # we do an equality test rather than an identity test
    # this ensures Python 2.2 compatibilty
    # and allows 0 and 1 to represent True and False
    if value == False:
        return False
    elif value == True:
        return True
    else:
        raise VdtTypeError(value)


def is_ip_addr(value):
    """
    Check that the supplied value is an Internet Protocol address, v.4,
    represented by a dotted-quad string, i.e. '1.2.3.4'.
    
    >>> vtor.check('ip_addr', '1 ')
    '1'
    >>> vtor.check('ip_addr', ' 1.2')
    '1.2'
    >>> vtor.check('ip_addr', ' 1.2.3 ')
    '1.2.3'
    >>> vtor.check('ip_addr', '1.2.3.4')
    '1.2.3.4'
    >>> vtor.check('ip_addr', '0.0.0.0')
    '0.0.0.0'
    >>> vtor.check('ip_addr', '255.255.255.255')
    '255.255.255.255'
    >>> vtor.check('ip_addr', '255.255.255.256')
    Traceback (most recent call last):
    VdtValueError: the value "255.255.255.256" is unacceptable.
    >>> vtor.check('ip_addr', '1.2.3.4.5')
    Traceback (most recent call last):
    VdtValueError: the value "1.2.3.4.5" is unacceptable.
    >>> vtor.check('ip_addr', 0)
    Traceback (most recent call last):
    VdtTypeError: the value "0" is of the wrong type.
    """
    if not isinstance(value, basestring):
        raise VdtTypeError(value)
    value = value.strip()
    try:
        dottedQuadToNum(value)
    except ValueError:
        raise VdtValueError(value)
    return value


def is_list(value, min=None, max=None):
    """
    Check that the value is a list of values.
    
    You can optionally specify the minimum and maximum number of members.
    
    It does no check on list members.
    
    >>> vtor.check('list', ())
    []
    >>> vtor.check('list', [])
    []
    >>> vtor.check('list', (1, 2))
    [1, 2]
    >>> vtor.check('list', [1, 2])
    [1, 2]
    >>> vtor.check('list(3)', (1, 2))
    Traceback (most recent call last):
    VdtValueTooShortError: the value "(1, 2)" is too short.
    >>> vtor.check('list(max=5)', (1, 2, 3, 4, 5, 6))
    Traceback (most recent call last):
    VdtValueTooLongError: the value "(1, 2, 3, 4, 5, 6)" is too long.
    >>> vtor.check('list(min=3, max=5)', (1, 2, 3, 4))
    [1, 2, 3, 4]
    >>> vtor.check('list', 0)
    Traceback (most recent call last):
    VdtTypeError: the value "0" is of the wrong type.
    >>> vtor.check('list', '12')
    Traceback (most recent call last):
    VdtTypeError: the value "12" is of the wrong type.
    """
    (min_len, max_len) = _is_num_param(('min', 'max'), (min, max))
    if isinstance(value, basestring):
        raise VdtTypeError(value)
    try:
        num_members = len(value)
    except TypeError:
        raise VdtTypeError(value)
    if min_len is not None and num_members < min_len:
        raise VdtValueTooShortError(value)
    if max_len is not None and num_members > max_len:
        raise VdtValueTooLongError(value)
    return list(value)


def is_tuple(value, min=None, max=None):
    """
    Check that the value is a tuple of values.
    
    You can optionally specify the minimum and maximum number of members.
    
    It does no check on members.
    
    >>> vtor.check('tuple', ())
    ()
    >>> vtor.check('tuple', [])
    ()
    >>> vtor.check('tuple', (1, 2))
    (1, 2)
    >>> vtor.check('tuple', [1, 2])
    (1, 2)
    >>> vtor.check('tuple(3)', (1, 2))
    Traceback (most recent call last):
    VdtValueTooShortError: the value "(1, 2)" is too short.
    >>> vtor.check('tuple(max=5)', (1, 2, 3, 4, 5, 6))
    Traceback (most recent call last):
    VdtValueTooLongError: the value "(1, 2, 3, 4, 5, 6)" is too long.
    >>> vtor.check('tuple(min=3, max=5)', (1, 2, 3, 4))
    (1, 2, 3, 4)
    >>> vtor.check('tuple', 0)
    Traceback (most recent call last):
    VdtTypeError: the value "0" is of the wrong type.
    >>> vtor.check('tuple', '12')
    Traceback (most recent call last):
    VdtTypeError: the value "12" is of the wrong type.
    """
    return tuple(is_list(value, min, max))


def is_string(value, min=None, max=None):
    """
    Check that the supplied value is a string.
    
    You can optionally specify the minimum and maximum number of members.
    
    >>> vtor.check('string', '0')
    '0'
    >>> vtor.check('string', 0)
    Traceback (most recent call last):
    VdtTypeError: the value "0" is of the wrong type.
    >>> vtor.check('string(2)', '12')
    '12'
    >>> vtor.check('string(2)', '1')
    Traceback (most recent call last):
    VdtValueTooShortError: the value "1" is too short.
    >>> vtor.check('string(min=2, max=3)', '123')
    '123'
    >>> vtor.check('string(min=2, max=3)', '1234')
    Traceback (most recent call last):
    VdtValueTooLongError: the value "1234" is too long.
    """
    if not isinstance(value, basestring):
        raise VdtTypeError(value)
    (min_len, max_len) = _is_num_param(('min', 'max'), (min, max))
    try:
        num_members = len(value)
    except TypeError:
        raise VdtTypeError(value)
    if min_len is not None and num_members < min_len:
        raise VdtValueTooShortError(value)
    if max_len is not None and num_members > max_len:
        raise VdtValueTooLongError(value)
    return value


def is_int_list(value, min=None, max=None):
    """
    Check that the value is a list of integers.
    
    You can optionally specify the minimum and maximum number of members.
    
    Each list member is checked that it is an integer.
    
    >>> vtor.check('int_list', ())
    []
    >>> vtor.check('int_list', [])
    []
    >>> vtor.check('int_list', (1, 2))
    [1, 2]
    >>> vtor.check('int_list', [1, 2])
    [1, 2]
    >>> vtor.check('int_list', [1, 'a'])
    Traceback (most recent call last):
    VdtTypeError: the value "a" is of the wrong type.
    """
    return [is_integer(mem) for mem in is_list(value, min, max)]


def is_bool_list(value, min=None, max=None):
    """
    Check that the value is a list of booleans.
    
    You can optionally specify the minimum and maximum number of members.
    
    Each list member is checked that it is a boolean.
    
    >>> vtor.check('bool_list', ())
    []
    >>> vtor.check('bool_list', [])
    []
    >>> check_res = vtor.check('bool_list', (True, False))
    >>> check_res == [True, False]
    1
    >>> check_res = vtor.check('bool_list', [True, False])
    >>> check_res == [True, False]
    1
    >>> vtor.check('bool_list', [True, 'a'])
    Traceback (most recent call last):
    VdtTypeError: the value "a" is of the wrong type.
    """
    return [is_boolean(mem) for mem in is_list(value, min, max)]


def is_float_list(value, min=None, max=None):
    """
    Check that the value is a list of floats.
    
    You can optionally specify the minimum and maximum number of members.
    
    Each list member is checked that it is a float.
    
    >>> vtor.check('float_list', ())
    []
    >>> vtor.check('float_list', [])
    []
    >>> vtor.check('float_list', (1, 2.0))
    [1.0, 2.0]
    >>> vtor.check('float_list', [1, 2.0])
    [1.0, 2.0]
    >>> vtor.check('float_list', [1, 'a'])
    Traceback (most recent call last):
    VdtTypeError: the value "a" is of the wrong type.
    """
    return [is_float(mem) for mem in is_list(value, min, max)]


def is_string_list(value, min=None, max=None):
    """
    Check that the value is a list of strings.
    
    You can optionally specify the minimum and maximum number of members.
    
    Each list member is checked that it is a string.
    
    >>> vtor.check('string_list', ())
    []
    >>> vtor.check('string_list', [])
    []
    >>> vtor.check('string_list', ('a', 'b'))
    ['a', 'b']
    >>> vtor.check('string_list', ['a', 1])
    Traceback (most recent call last):
    VdtTypeError: the value "1" is of the wrong type.
    >>> vtor.check('string_list', 'hello')
    Traceback (most recent call last):
    VdtTypeError: the value "hello" is of the wrong type.
    """
    if isinstance(value, basestring):
        raise VdtTypeError(value)
    return [is_string(mem) for mem in is_list(value, min, max)]


def is_ip_addr_list(value, min=None, max=None):
    """
    Check that the value is a list of IP addresses.
    
    You can optionally specify the minimum and maximum number of members.
    
    Each list member is checked that it is an IP address.
    
    >>> vtor.check('ip_addr_list', ())
    []
    >>> vtor.check('ip_addr_list', [])
    []
    >>> vtor.check('ip_addr_list', ('1.2.3.4', '5.6.7.8'))
    ['1.2.3.4', '5.6.7.8']
    >>> vtor.check('ip_addr_list', ['a'])
    Traceback (most recent call last):
    VdtValueError: the value "a" is unacceptable.
    """
    return [is_ip_addr(mem) for mem in is_list(value, min, max)]


def force_list(value, min=None, max=None):
    """
    Check that a value is a list, coercing strings into
    a list with one member. Useful where users forget the
    trailing comma that turns a single value into a list.
    
    You can optionally specify the minimum and maximum number of members.
    A minumum of greater than one will fail if the user only supplies a
    string.
    
    >>> vtor.check('force_list', ())
    []
    >>> vtor.check('force_list', [])
    []
    >>> vtor.check('force_list', 'hello')
    ['hello']
    """
    if not isinstance(value, (list, tuple)):
        value = value and [value] or []
    return is_list(value, min, max)
    
    

fun_dict = {
    'integer': is_integer,
    'float': is_float,
    'ip_addr': is_ip_addr,
    'string': is_string,
    'boolean': is_boolean,
}


def is_mixed_list(value, *args):
    """
    Check that the value is a list.
    Allow specifying the type of each member.
    Work on lists of specific lengths.
    
    You specify each member as a positional argument specifying type
    
    Each type should be one of the following strings :
      'integer', 'float', 'ip_addr', 'string', 'boolean'
    
    So you can specify a list of two strings, followed by
    two integers as :
    
      mixed_list('string', 'string', 'integer', 'integer')
    
    The length of the list must match the number of positional
    arguments you supply.
    
    >>> mix_str = "mixed_list('integer', 'float', 'ip_addr', 'string', 'boolean')"
    >>> check_res = vtor.check(mix_str, (1, 2.0, '1.2.3.4', 'a', True))
    >>> check_res == [1, 2.0, '1.2.3.4', 'a', True]
    1
    >>> check_res = vtor.check(mix_str, ('1', '2.0', '1.2.3.4', 'a', 'True'))
    >>> check_res == [1, 2.0, '1.2.3.4', 'a', True]
    1
    >>> vtor.check(mix_str, ('b', 2.0, '1.2.3.4', 'a', True))
    Traceback (most recent call last):
    VdtTypeError: the value "b" is of the wrong type.
    >>> vtor.check(mix_str, (1, 2.0, '1.2.3.4', 'a'))
    Traceback (most recent call last):
    VdtValueTooShortError: the value "(1, 2.0, '1.2.3.4', 'a')" is too short.
    >>> vtor.check(mix_str, (1, 2.0, '1.2.3.4', 'a', 1, 'b'))
    Traceback (most recent call last):
    VdtValueTooLongError: the value "(1, 2.0, '1.2.3.4', 'a', 1, 'b')" is too long.
    >>> vtor.check(mix_str, 0)
    Traceback (most recent call last):
    VdtTypeError: the value "0" is of the wrong type.
    
    This test requires an elaborate setup, because of a change in error string
    output from the interpreter between Python 2.2 and 2.3 .
    
    >>> res_seq = (
    ...     'passed an incorrect value "',
    ...     'yoda',
    ...     '" for parameter "mixed_list".',
    ... )
    >>> res_str = "'".join(res_seq)
    >>> try:
    ...     vtor.check('mixed_list("yoda")', ('a'))
    ... except VdtParamError, err:
    ...     str(err) == res_str
    1
    """
    try:
        length = len(value)
    except TypeError:
        raise VdtTypeError(value)
    if length < len(args):
        raise VdtValueTooShortError(value)
    elif length > len(args):
        raise VdtValueTooLongError(value)
    try:
        return [fun_dict[arg](val) for arg, val in zip(args, value)]
    except KeyError, e:
        raise VdtParamError('mixed_list', e)


def is_option(value, *options):
    """
    This check matches the value to any of a set of options.
    
    >>> vtor.check('option("yoda", "jedi")', 'yoda')
    'yoda'
    >>> vtor.check('option("yoda", "jedi")', 'jed')
    Traceback (most recent call last):
    VdtValueError: the value "jed" is unacceptable.
    >>> vtor.check('option("yoda", "jedi")', 0)
    Traceback (most recent call last):
    VdtTypeError: the value "0" is of the wrong type.
    """
    if not isinstance(value, basestring):
        raise VdtTypeError(value)
    if not value in options:
        raise VdtValueError(value)
    return value


def _test(value, *args, **keywargs):
    """
    A function that exists for test purposes.
    
    >>> checks = [
    ...     '3, 6, min=1, max=3, test=list(a, b, c)',
    ...     '3',
    ...     '3, 6',
    ...     '3,',
    ...     'min=1, test="a b c"',
    ...     'min=5, test="a, b, c"',
    ...     'min=1, max=3, test="a, b, c"',
    ...     'min=-100, test=-99',
    ...     'min=1, max=3',
    ...     '3, 6, test="36"',
    ...     '3, 6, test="a, b, c"',
    ...     '3, max=3, test=list("a", "b", "c")',
    ...     '''3, max=3, test=list("'a'", 'b', "x=(c)")''',
    ...     "test='x=fish(3)'",
    ...    ]
    >>> v = Validator({'test': _test})
    >>> for entry in checks:
    ...     print v.check(('test(%s)' % entry), 3)
    (3, ('3', '6'), {'test': ['a', 'b', 'c'], 'max': '3', 'min': '1'})
    (3, ('3',), {})
    (3, ('3', '6'), {})
    (3, ('3',), {})
    (3, (), {'test': 'a b c', 'min': '1'})
    (3, (), {'test': 'a, b, c', 'min': '5'})
    (3, (), {'test': 'a, b, c', 'max': '3', 'min': '1'})
    (3, (), {'test': '-99', 'min': '-100'})
    (3, (), {'max': '3', 'min': '1'})
    (3, ('3', '6'), {'test': '36'})
    (3, ('3', '6'), {'test': 'a, b, c'})
    (3, ('3',), {'test': ['a', 'b', 'c'], 'max': '3'})
    (3, ('3',), {'test': ["'a'", 'b', 'x=(c)'], 'max': '3'})
    (3, (), {'test': 'x=fish(3)'})
    
    >>> v = Validator()
    >>> v.check('integer(default=6)', '3')
    3
    >>> v.check('integer(default=6)', None, True)
    6
    >>> v.get_default_value('integer(default=6)')
    6
    >>> v.get_default_value('float(default=6)')
    6.0
    >>> v.get_default_value('pass(default=None)')
    >>> v.get_default_value("string(default='None')")
    'None'
    >>> v.get_default_value('pass')
    Traceback (most recent call last):
    KeyError: 'Check "pass" has no default value.'
    >>> v.get_default_value('pass(default=list(1, 2, 3, 4))')
    ['1', '2', '3', '4']
    
    >>> v = Validator()
    >>> v.check("pass(default=None)", None, True)
    >>> v.check("pass(default='None')", None, True)
    'None'
    >>> v.check('pass(default="None")', None, True)
    'None'
    >>> v.check('pass(default=list(1, 2, 3, 4))', None, True)
    ['1', '2', '3', '4']
    
    Bug test for unicode arguments
    >>> v = Validator()
    >>> v.check(u'string(min=4)', u'test')
    u'test'
    
    >>> v = Validator()
    >>> v.get_default_value(u'string(min=4, default="1234")')
    u'1234'
    >>> v.check(u'string(min=4, default="1234")', u'test')
    u'test'
    
    >>> v = Validator()
    >>> default = v.get_default_value('string(default=None)')
    >>> default == None
    1
    """
    return (value, args, keywargs)


def _test2():
    """
    >>> 
    >>> v = Validator()
    >>> v.get_default_value('string(default="#ff00dd")')
    '#ff00dd'
    >>> v.get_default_value('integer(default=3) # comment')
    3
    """

def _test3():
    r"""
    >>> vtor.check('string(default="")', '', missing=True)
    ''
    >>> vtor.check('string(default="\n")', '', missing=True)
    '\n'
    >>> print vtor.check('string(default="\n")', '', missing=True),
    <BLANKLINE>
    >>> vtor.check('string()', '\n')
    '\n'
    >>> vtor.check('string(default="\n\n\n")', '', missing=True)
    '\n\n\n'
    >>> vtor.check('string()', 'random \n text goes here\n\n')
    'random \n text goes here\n\n'
    >>> vtor.check('string(default=" \nrandom text\ngoes \n here\n\n ")',
    ... '', missing=True)
    ' \nrandom text\ngoes \n here\n\n '
    >>> vtor.check("string(default='\n\n\n')", '', missing=True)
    '\n\n\n'
    >>> vtor.check("option('\n','a','b',default='\n')", '', missing=True)
    '\n'
    >>> vtor.check("string_list()", ['foo', '\n', 'bar'])
    ['foo', '\n', 'bar']
    >>> vtor.check("string_list(default=list('\n'))", '', missing=True)
    ['\n']
    """
    
    
if __name__ == '__main__':
    # run the code tests in doctest format
    import sys
    import doctest
    m = sys.modules.get('__main__')
    globs = m.__dict__.copy()
    globs.update({
        'vtor': Validator(),
    })
    doctest.testmod(m, globs=globs)

########NEW FILE########
__FILENAME__ = exceptions
"""
Standard public exceptions that are raised by
the various APIs in holland-core
"""
from holland.core.backup import BackupError

class ConfigError(Exception):
    """Configuration error"""
    pass

class InsufficientSpaceError(Exception):
    """Operation could not complete due to disk space"""
    pass

class ArgumentError(Exception):
    """Invalid argument"""

########NEW FILE########
__FILENAME__ = log
import os
import sys
import logging

__all__ = [
    'clear_root_handlers',
    'setup_console_logging',
    'setup_file_logging'
]

DEFAULT_DATE_FORMAT = '%a, %d %b %Y %H:%M:%S'
DEFAULT_LOG_FORMAT = '%(asctime)s [%(levelname)s] %(message)s'
DEFAULT_LOG_LEVEL = logging.INFO

class NullHandler(logging.Handler):
    def emit(self, record):
            pass

def clear_root_handlers():
    root = logging.getLogger()
    map(root.removeHandler, root.handlers)

def setup_console_logging(level=DEFAULT_LOG_LEVEL, 
                          format='%(message)s', 
                          datefmt=DEFAULT_DATE_FORMAT):
    root = logging.getLogger()
    root.setLevel(level)
    handler = logging.StreamHandler()
    formatter = logging.Formatter(format, datefmt)
    handler.setFormatter(formatter)
    logging.getLogger().addHandler(handler)

def setup_file_logging(filename, 
                       level=DEFAULT_LOG_LEVEL, 
                       format=DEFAULT_LOG_FORMAT, 
                       datefmt=DEFAULT_DATE_FORMAT):
    root = logging.getLogger()
    root.setLevel(level)
    handler = logging.FileHandler(filename, 'a', encoding='utf8')
    formatter = logging.Formatter(format)
    handler.setFormatter(formatter)
    logging.getLogger().addHandler(handler)

########NEW FILE########
__FILENAME__ = plugin
"""
Core plugin support
"""

import logging
from pkg_resources import working_set, Environment, iter_entry_points, \
                            get_distribution, find_distributions, \
                            DistributionNotFound, VersionConflict

LOGGER = logging.getLogger(__name__)

plugin_directories = []

class PluginLoadError(Exception):
    pass

def add_plugin_dir(plugin_dir):
    LOGGER.debug("Adding plugin directory: %r", plugin_dir)
    env = Environment([plugin_dir])
    dists, errors = working_set.find_plugins(env)
    for dist in dists:
        LOGGER.debug("Adding distribution: %r", dist)
        working_set.add(dist)

    if errors:
        for dist, error in errors.items():
            errmsg = None
            if isinstance(error, DistributionNotFound):
                req, = error.args
                errmsg = "%r not found" % req.project_name
            elif isinstance(error, VersionConflict):
                dist, req = error.args
                errmsg = "Version Conflict. Requested %s Found %s" % (req, dist)
            else:
                # FIXME: Are there other types of failures?
                errmsg = repr(error)
            LOGGER.error("Failed to load %s: %r", dist, errmsg)
    global plugin_directories
    plugin_directories.append(plugin_dir)   


def load_first_entrypoint(group, name=None):
    """
    load the first entrypoint in any distribution
    matching group and name
    """
    for ep in iter_entry_points(group, name):
        try:
            return ep.load()
        except DistributionNotFound, e:
            raise PluginLoadError("Could not find dependency '%s'" % e)
        except ImportError, e:
            raise PluginLoadError(e)
    raise PluginLoadError("'%s' not found" % '.'.join((group, name)))

def load_backup_plugin(name):
    return load_first_entrypoint('holland.backup', name)

def load_restore_plugin(name):
    return load_first_entry_point('holland.restore', name)

def get_commands():
    cmds = {}
    for ep in iter_entry_points('holland.commands'):
        try:
            cmdcls = ep.load()
        except Exception, e:
            LOGGER.warning("Skipping command plugin %s: %s", ep.name, e)
            continue
        cmds[cmdcls.name] = cmdcls
        for alias in cmdcls.aliases:
            cmds[alias] = cmdcls
    return cmds

def iter_plugins(group, name=None):
    """
    Iterate over all unique distributions defining
    entrypoints with the given group and name
    """
    for ep in working_set.iter_entry_points(group, name):
        yield ep.name, dist_metainfo_dict(ep.dist)

def dist_metainfo_dict(dist):
    """Convert an Egg's PKG-INFO into a dict"""
    from rfc822 import Message
    from cStringIO import StringIO
    distmetadata = dist.get_metadata('PKG-INFO')
    msg = Message(StringIO(distmetadata))
    return dict(msg.items())

def iter_plugininfo():
    """
    Iterate over the plugins loaded so far
    """
    from rfc822 import Message
    from cStringIO import StringIO
    global plugin_directories
    for plugin_dir in plugin_directories:
        for dist in find_distributions(plugin_dir):
            distmetadata = dist.get_metadata('PKG-INFO')
            msg = Message(StringIO(distmetadata))
            filtered_keys = ['metadata-version', 'home-page', 'platform']
            distinfo = filter(lambda x: x[0] not in filtered_keys, msg.items())
            yield dist, dict(distinfo)

########NEW FILE########
__FILENAME__ = spool
"""
Utilities to manage spool directory
"""

import os
import sys
import time
import errno
import logging
import itertools
import shutil
from holland.core.config import BaseConfig

LOGGER = logging.getLogger(__name__)

def timestamp_dir(when=None):
    """
    Returns a directory named after the specified time or
    the current time, if no specific time is given
    """
    if when is None:
        when = time.time()
    return time.strftime("%Y%m%d_%H%M%S", time.localtime(when))

class Spool(object):
    """
    A directory spool where backups are saved
    """
    def __init__(self, path=None):
        self.path = path or '/var/spool/holland'

    def find_backup(self, name):
        """
        Find a the specified backup, if it exists. If the backup does
        not exist, returns None

        The backup name must be in <backupset>/<timestamp> format
        """
        try:
            backupset_name, timestamp = name.split('/')
            backupset = self.find_backupset(backupset_name)
            if backupset:
                return backupset.find_backup(timestamp)
            else:
                return None
        except ValueError, e:
            LOGGER.warning("Invalid backup name: %s", name)
            return None

    def add_backup(self, backupset_name):
        """
        Add a new backup to the specified backupset_name, which will also be
        initialized if it does not exist.

        The backup will only exist in memory until its 'flush' method is called
        """
        backupset = self.find_backupset(backupset_name)
        if not backupset:
            backupset = self.add_backupset(backupset_name)
        return backupset.add_backup()

    def find_backupset(self, backupset_name):
        """
        Find an existing backupset.

        If the backupset does not exist None is returned
        """
        path = os.path.join(self.path, backupset_name)
        if not os.path.exists(path):
            return None
        return Backupset(backupset_name, path)

    def add_backupset(self, backupset_name):
        """
        Add a new backupset to this spool.

        If the backupset already exists an IOError is raised
        """
        path = os.path.join(self.path, backupset_name)
        if os.path.exists(path):
            raise IOError("Backupset %s already exists" % backupset_name)
        return Backupset(backupset_name, path)

    def list_backupsets(self, name=None, reverse=False):
        """
        Return a list of backupsets under this spool in lexicographical order.

        If reverse is True, the results will be returned in descending lex
        order, ascending otherwise
        """
        if not os.path.exists(self.path):
            return []

        backupsets= []
        dirs = []
        if name:
            if not os.path.exists(os.path.join(self.path, name)):
                return []
            dirs = [name]
        else:
            dirs = [backupset for backupset in os.listdir(self.path)
                    if os.path.isdir(os.path.join(self.path, backupset))]

        backupsets = [Backupset(dir, os.path.join(self.path, dir)) \
                      for dir in dirs]

        backupsets.sort()

        if reverse:
            backupsets.reverse()

        return backupsets

    def list_backups(self, backupset_name=None):
        for backupset in self.list_backupsets(backupset_name):
            for backup in backupset.list_backups():
                yield backup

    def __iter__(self):
        return iter(self.list_backupsets())


class Backupset(object):
    def __init__(self, name, path):
        self.name = name
        self.path = path

    def find_backup(self, name):
        backups = self.list_backups(name)
        if not backups:
            return None
        return backups[0]

    def add_backup(self):
        """
        Create a new instance for this job
        """
        backup_name = timestamp_dir()
        backup_path = os.path.join(self.path, backup_name)
        backup = Backup(backup_path, self.name, backup_name)
        backup.prepare()
        return backup

    def purge(self, retention_count=0):
        if retention_count < 0:
            raise ValueError("Invalid retention count %s" % retention_count)
        for backup in itertools.islice(self.list_backups(reverse=True), retention_count, None):
            backup.purge()
            yield backup

    def list_backups(self, name=None, reverse=False):
        """
        Return list of backups for this backupset in order of their
        creation date.
        """
        if not os.path.exists(self.path):
            return None

        name = (name or "").strip()

        backup_list = []
        if name:
            path = os.path.join(self.path, name)
            return [Backup(path, self.name, name) for x in range(1)
                        if os.path.exists(path)]

        dirs = [backup for backup in os.listdir(self.path)
                   if os.path.isdir(os.path.join(self.path, backup))
                    and backup not in ('oldest', 'newest')]

        backup_list = [Backup(os.path.join(self.path, dir),
                              self.name,
                              dir) for dir in dirs]

        backup_list.sort()
        if reverse:
            backup_list.reverse()

        return backup_list

    def update_symlinks(self):
        "Update symlinks for newest and oldest backup in the set"
        backups = self.list_backups()

        oldest_link = os.path.join(self.path, 'oldest')
        newest_link = os.path.join(self.path, 'newest')
        try:
            os.remove(oldest_link)
        except OSError, exc:
            if exc.errno != errno.ENOENT:
                raise
        try:
            os.remove(newest_link)
        except OSError, exc:
            if exc.errno != errno.ENOENT:
                raise
        if not backups:
            return
        oldest_path = backups[0].path
        newest_path = backups[-1].path
        os.symlink(oldest_path, oldest_link)
        os.symlink(newest_path, newest_link)

    def __iter__(self):
        return iter(self.list_backups())

    def __str__(self):
        return "%s [%s]" % (self.name, self.path)

    def __cmp__(self, other):
        return cmp(self.name, other.name)

CONFIGSPEC = """
[holland:backup]
plugin                  = string(default="")
start-time              = float(default=0)
stop-time               = float(default=0)
failed-backup           = boolean(default=no)
estimated-size          = float(default=0)
on-disk-size            = float(default=0)
estimated-size-factor   = float(default=1.0)
backups-to-keep         = integer(min=0, default=1)
auto-purge-failures     = boolean(default=yes)
purge-policy            = option(manual, before-backup, after-backup, default='after-backup')
purge-on-demand         = boolean(default=no)
before-backup-command   = string(default=None)
after-backup-command    = string(default=None)
failed-backup-command   = string(default=None)
""".splitlines()

class Backup(object):
    """
    Representation of a backup instance.
    """
    def __init__(self, path, backupset, name):
        self.path = path
        self.backupset = backupset
        self.name = '/'.join((backupset, name))
        # Initialize an empty config
        # This will not be loaded until load_config is called
        config_path = os.path.join(self.path, 'backup.conf')
        self.config = BaseConfig({}, file_error=False)
        self.config.filename = config_path
        if os.path.exists(config_path):
            self.load_config()
        else:
            self.validate_config()

    def validate_config(self):
        self.config.validate_config(CONFIGSPEC, suppress_warnings=True)

    def load_config(self):
        """
        (Re)Load the config for this backup.
        """
        self.config.reload()
        self.validate_config()

    def purge(self, data_only=False):
        """
        Purge this backup.
        """
        assert(os.path.realpath(self.path) != '/')
        # purge the entire backup directory
        try:
            shutil.rmtree(self.path)
        except OSError, exc:
            if exc.errno != errno.ENOENT:
                raise

    def exists(self):
        """
        Check if this backup exists on disk
        """
        return os.path.exists(self.path)

    def prepare(self):
        """
        Prepare this backup on disk.  Ensures the path to this backup is created,
        but does not flush any other backup metadata.
        """
        os.makedirs(self.path)
        LOGGER.info("Creating backup path %s", self.path)

    def flush(self):
        """
        Flush this backup to disk.  Ensure the path to this backup is created
        and write the backup.conf to the backup directory.
        """
        LOGGER.debug("Writing out config to %s", self.config.filename)
        self.config.write()

    def _formatted_config(self):
        from holland.core.util.fmt import format_bytes, format_datetime
        cfg = dict(self.config['holland:backup'])
        cfg['stop-time'] = format_datetime(cfg['stop-time'])
        cfg['start-time'] = format_datetime(cfg['start-time'])
        cfg['estimated-size'] = format_bytes(cfg['estimated-size'])
        cfg['on-disk-size'] = format_bytes(cfg['on-disk-size'])
        return cfg

    def info(self):
        from holland.core.util.template import Template
        from textwrap import dedent, wrap
        tmpl = Template("""
        backup-plugin   = ${plugin}
        backup-started  = ${start-time}
        backup-finished = ${stop-time}
        estimated size  = ${estimated-size}
        on-disk size    = ${on-disk-size}
        """)
        str = tmpl.safe_substitute(self._formatted_config())
        str = "\t" + dedent(str).lstrip()
        str = "\n\t\t".join(str.splitlines())
        return str

    def __str__(self):
        from textwrap import dedent
        from holland.core.util.fmt import format_bytes, format_datetime

        return dedent("""
        Backup: %s
        start-time:     %s
        stop-time:      %s
        estimated-size: %s
        on-disk-size:   %s
        """).strip() % (
            self.name,
            format_datetime(self.config.lookup('holland:backup.start-time')),
            format_datetime(self.config.lookup('holland:backup.stop-time')),
            format_bytes(self.config.lookup('holland:backup.estimated-size')),
            format_bytes(self.config.lookup('holland:backup.on-disk-size'))
        )

    def __cmp__(self, other):
        return cmp(self.config['holland:backup']['start-time'],
                   other.config['holland:backup']['start-time'])

    __repr__ = __str__

spool = Spool()

########NEW FILE########
__FILENAME__ = bootstrap
"""
Functions to support bootstrapping.

These functions should only be called when starting up a holland session.
They initialize things like logging and the config system.
"""
import os
import sys
import logging
import warnings
from holland.core.plugin import add_plugin_dir
from holland.core.config import hollandcfg, setup_config as _setup_config
from holland.core.log import setup_console_logging, setup_file_logging, clear_root_handlers
from holland.core.spool import spool

LOGGER = logging.getLogger(__name__)

def setup_config(opts):
    if not opts.quiet:
        debug = opts.log_level == 'debug'
        setup_console_logging(level=[logging.INFO,logging.DEBUG][debug])
    try:
        _setup_config(opts.config_file)
    except IOError, e:
        LOGGER.error("Failed to load holland config: %s", e)
        sys.exit(os.EX_CONFIG)

def log_warnings(message, category, filename, lineno, file=None, line=None):
    WARNLOG = logging.getLogger("Python")
    logging.debug("message=%s message=%r category=%r", message, message, category)
    warning_string = warnings.formatwarning(message,
                                            category,
                                            filename,
                                            lineno)
    WARNLOG.debug("%s", warning_string)

def setup_logging(opts):
    clear_root_handlers()
    if hasattr(opts, 'log_level'):
        log_level = opts.log_level or hollandcfg.lookup('logging.level')
    else:
        log_level = hollandcfg.lookup('logging.level')

    if (os.isatty(sys.stdin.fileno()) and not opts.quiet):
        setup_console_logging(level=log_level)

    if hollandcfg.lookup('logging.filename'):
        try:
            setup_file_logging(filename=str(hollandcfg.lookup('logging.filename')),
                               level=log_level)
        except IOError, exc:
            LOGGER.warn("Skipping file logging: %s", exc)

    # Monkey patch in routing warnings through logging
    old_showwarning = warnings.showwarning
    warnings.showwarning = log_warnings

def setup_umask():
    os.umask(hollandcfg.lookup('holland.umask'))

def setup_path():
    if hollandcfg.lookup('holland.path'):
        os.putenv('PATH', hollandcfg.lookup('holland.path'))
        os.environ['PATH'] = hollandcfg.lookup('holland.path')

def setup_plugins():
    map(add_plugin_dir, hollandcfg.lookup('holland.plugin-dirs'))

def bootstrap(opts):
    # Setup the configuration
    setup_config(opts)
    # use umask setting
    setup_umask()
    # Setup logging per config
    setup_logging(opts)
    # setup tmpdir
    if hollandcfg.lookup('holland.tmpdir'):
        os.environ['TMPDIR'] = str(hollandcfg.lookup('holland.tmpdir'))
    # configure our PATH
    setup_path()
    # Setup plugin directories
    setup_plugins()
    # Setup spool
    spool.path = hollandcfg.lookup('holland.backup-directory')

########NEW FILE########
__FILENAME__ = fmt
def format_interval(seconds):
    """ Format an integer number of seconds to a human readable string."""
    units = [
        (('week', 'weeks'), 604800),
        (('day', 'days'), 86400),
        (('hour', 'hours'), 3600),
        (('minute', 'minutes'), 60),
        #(('second', 'seconds'), 1)
    ]
    result = []
    for names, value in units:
        n, seconds = divmod(seconds, value)
        if n > 0:
            result.append('%d %s' % (n, names[n > 1]))
    if seconds:
        result.append("%.2f %s" % (seconds, ['second', 'seconds'][seconds != 1.0]))
    return ', '.join(result)

def format_datetime(epoch):
    from time import strftime, localtime
    return strftime("%a %b %d %Y %I:%M:%S%p", localtime(epoch))

def format_bytes(bytes, precision=2):
    """Format an integer number of bytes to a human readable string."""
    import math

    if bytes < 0:
        raise ArithmeticError("Only Positive Integers Allowed")

    if bytes != 0:
        exponent = math.floor(math.log(bytes, 1024))
    else:
        exponent = 0

    return "%.*f%s" % (
        precision,
        bytes / (1024 ** exponent),
        ['B','KB','MB','GB','TB','PB','EB','ZB','YB'][int(exponent)]
    )

def format_loglevel(str_level):
    """
    Coerces a string to an integer logging level which
    maps to a standard python logging level
    """
    import logging
    std_levels = {
        'debug'     : logging.DEBUG,
        'info'      : logging.INFO,
        'warning'   : logging.WARNING,
        'error'     : logging.ERROR,
        'critical'  : logging.CRITICAL
    }

    level = str_level.lower().strip()

    return std_levels.get(level)

########NEW FILE########
__FILENAME__ = lock
"""A simple fcntl/flock implementation"""

from os import getpid
from fcntl import flock, LOCK_EX, LOCK_UN, LOCK_NB

class LockError(Exception):
    """Raised when an error is encountered during a lock operation"""

    def __init__(self, message, exc=None):
        Exception.__init__(self, message, exc)
        self.message = message
        self.exc = exc

class Lock(object):
    """A simple flock based file lock implementation"""

    def __init__(self, path):
        self.path = path
        self.lock = None

    def acquire(self):
        """Acquire a lock on the path associated with this lock object"""

        if self.is_locked():
            return

        try:
            self.lock = open(self.path, 'r')
            flock(self.lock, LOCK_EX|LOCK_NB)
        except IOError, exc:
            self.lock = None
            raise LockError(str(exc), exc)
        else:
            return True

    def is_locked(self):
        return self.lock is not None

    def release(self):
        """Release a currently open lock"""
        if self.lock is None:
            raise LockError("No lock acquired to release")
        try:
            self.acquire()
            flock(self.lock, LOCK_UN)
            self.lock = None
        except IOError, exc:
            raise LockError(str(exc), exc)
        else:
            return True

########NEW FILE########
__FILENAME__ = path
# $Id$
"""
Utility functions
"""

# Functions added here should really be as portable as possible
# and generally useful.

import os
import sys
import stat
import time
import logging

LOG = logging.getLogger(__name__)

def ensure_dir(dir_path):
    """
    Ensure a directory path exists (by creating it if it doesn't).
    """

    if not os.path.exists(dir_path):
        try:
            os.makedirs(dir_path)
            LOG.debug("created directory %s" % dir_path)
            return True
        except OSError, e:
            # FIX ME: Need error codes/etc so this will exit(<code>) or raise
            # an appropriate holland exception
            LOG.error("os.makedirs(%s): %s" % (dir_path, e))
            raise
    return False

def protected_path(path):
    """
    Take a path, and if the file/dir exist pass back a protected path
    (suffixed).

    Returns:

        string = new file path

    Example:

        >>> mypath = '/tmp'
        >>> new_path = helpers.protected_path(mypath)
        >>> new_path
        '/tmp.0'
    """
    log = logging.getLogger(__name__)
    safety = 0
    safe_path = path
    while True:
        if os.path.exists(safe_path):
            safe_path = "%s.%s" % (path, safety)
        else:
            break
        safety = safety + 1
    return safe_path

def format_bytes(bytes, precision=2):
    """
    Format an integer number of bytes to a human
    readable string.

    If bytes is negative, this method raises ArithmeticError
    """
    import math

    if bytes < 0:
        raise ArithmeticError("Only Positive Integers Allowed")

    if bytes != 0:
        exponent = math.floor(math.log(bytes, 1024))
    else:
        exponent = 0

    return "%.*f%s" % (
        precision,
        bytes / (1024 ** exponent),
        ['B','KB','MB','GB','TB','PB','EB','ZB','YB'][int(exponent)]
    )


def normpath(path):
    from os.path import normpath, abspath
    return abspath(normpath(path))

def relpath(path, start=os.curdir):
    """Return a relative version of a path"""

    if not path:
        raise ValueError("no path specified")

    start_list = [x for x in os.path.abspath(start).split(os.sep) if x]
    path_list = [x for x in os.path.abspath(path).split(os.sep) if x]

    # Work out how much of the filepath is shared by start and path.
    i = len(os.path.commonprefix([start_list, path_list]))

    rel_list = [os.pardir] * (len(start_list)-i) + path_list[i:]
    if not rel_list:
        return os.curdir
    return os.path.join(*rel_list)

def getmount(path):
    """Return the mount point of a path

    :param path: path to find the mountpoint for

    :returns: str mounpoint path
    """

    path = os.path.realpath(path)

    while path != os.path.sep:
        if os.path.ismount(path):
            return path
        path = os.path.abspath(os.path.join(path, os.pardir))
    return path

def disk_capacity(target_path):
    """Find the total capacity of the filesystem that target_path is on

    :returns: integer number of bytes
    """
    path = getmount(target_path)
    info = os.statvfs(path)
    return info.f_frsize*info.f_blocks

def disk_free(target_path):
    """
    Find the amount of space free on a given path
    Path must exist.
    This method does not take into account quotas

    returns the size in bytes potentially available
    to a non privileged user
    """
    path = getmount(target_path)
    info = os.statvfs(path)
    return info.f_frsize*info.f_bavail

def directory_size(path):
    """
    Find the size of all files in a directory, recursively

    Returns the size in bytes on success
    """
    from os.path import join, getsize
    result = 0
    for root, dirs, files in os.walk(path):
        for name in files:
            try:
                sz = getsize(join(root,name))
                result = result + sz
            except OSError, exc:
                pass
    return result

########NEW FILE########
__FILENAME__ = pycompat
"""This module provides backports for older python releases"""

import re

class Scanner(object):
    def __init__(self, lexicon, flags=0):
        import sre_parse
        import sre_compile
        from sre_constants import BRANCH, SUBPATTERN
        self.lexicon = lexicon
        # combine phrases into a compound pattern
        p = []
        s = sre_parse.Pattern()
        s.flags = flags
        for phrase, action in lexicon:
            p.append(sre_parse.SubPattern(s, [
                (SUBPATTERN, (len(p)+1, sre_parse.parse(phrase, flags))),
                ]))
        s.groups = len(p)+1
        p = sre_parse.SubPattern(s, [(BRANCH, (None, p))])
        self.scanner = sre_compile.compile(p)
    def scan(self, string):
        result = []
        append = result.append
        match = self.scanner.scanner(string).match
        i = 0
        while 1:
            m = match()
            if not m:
                break
            j = m.end()
            if i == j:
                break
            action = self.lexicon[m.lastindex-1][1]
            if hasattr(action, '__call__'):
                self.match = m
                action = action(self, m.group())
            if action is not None:
                append(action)
            i = j
        return result, string[i:]


class _multimap:
    """Helper class for combining multiple mappings.

    Used by .{safe_,}substitute() to combine the mapping and keyword
    arguments.
    """
    def __init__(self, primary, secondary):
        self._primary = primary
        self._secondary = secondary

    def __getitem__(self, key):
        try:
            return self._primary[key]
        except KeyError:
            return self._secondary[key]


class _TemplateMetaclass(type):
    pattern = r"""
    %(delim)s(?:
      (?P<escaped>%(delim)s) |   # Escape sequence of two delimiters
      (?P<named>%(id)s)      |   # delimiter and a Python identifier
      {(?P<braced>%(id)s)}   |   # delimiter and a braced identifier
      (?P<invalid>)              # Other ill-formed delimiter exprs
    )
    """

    def __init__(cls, name, bases, dct):
        super(_TemplateMetaclass, cls).__init__(name, bases, dct)
        if 'pattern' in dct:
            pattern = cls.pattern
        else:
            pattern = _TemplateMetaclass.pattern % {
                'delim' : re.escape(cls.delimiter),
                'id'    : cls.idpattern,
                }
        cls.pattern = re.compile(pattern, re.IGNORECASE | re.VERBOSE)


class Template(object):
    """A string class for supporting $-substitutions."""
    __metaclass__ = _TemplateMetaclass

    delimiter = '$'
    idpattern = r'[_a-z][_a-z0-9]*'

    def __init__(self, template):
        self.template = template

    # Search for $$, $identifier, ${identifier}, and any bare $'s

    def _invalid(self, mo):
        i = mo.start('invalid')
        lines = self.template[:i].splitlines(True)
        if not lines:
            colno = 1
            lineno = 1
        else:
            colno = i - len(''.join(lines[:-1]))
            lineno = len(lines)
        raise ValueError('Invalid placeholder in string: line %d, col %d' %
                         (lineno, colno))

    def substitute(self, *args, **kws):
        if len(args) > 1:
            raise TypeError('Too many positional arguments')
        if not args:
            mapping = kws
        elif kws:
            mapping = _multimap(kws, args[0])
        else:
            mapping = args[0]
        # Helper function for .sub()
        def convert(mo):
            # Check the most common path first.
            named = mo.group('named') or mo.group('braced')
            if named is not None:
                val = mapping[named]
                # We use this idiom instead of str() because the latter will
                # fail if val is a Unicode containing non-ASCII characters.
                return '%s' % (val,)
            if mo.group('escaped') is not None:
                return self.delimiter
            if mo.group('invalid') is not None:
                self._invalid(mo)
            raise ValueError('Unrecognized named group in pattern',
                             self.pattern)
        return self.pattern.sub(convert, self.template)

    def safe_substitute(self, *args, **kws):
        if len(args) > 1:
            raise TypeError('Too many positional arguments')
        if not args:
            mapping = kws
        elif kws:
            mapping = _multimap(kws, args[0])
        else:
            mapping = args[0]
        # Helper function for .sub()
        def convert(mo):
            named = mo.group('named')
            if named is not None:
                try:
                    # We use this idiom instead of str() because the latter
                    # will fail if val is a Unicode containing non-ASCII
                    return '%s' % (mapping[named],)
                except KeyError:
                    return self.delimiter + named
            braced = mo.group('braced')
            if braced is not None:
                try:
                    return '%s' % (mapping[braced],)
                except KeyError:
                    return self.delimiter + '{' + braced + '}'
            if mo.group('escaped') is not None:
                return self.delimiter
            if mo.group('invalid') is not None:
                return self.delimiter
            raise ValueError('Unrecognized named group in pattern',
                             self.pattern)
        return self.pattern.sub(convert, self.template)

########NEW FILE########
__FILENAME__ = template
"""
Backport of string.Template from python2.4+
"""

import re as _re

class _multimap:
    """Helper class for combining multiple mappings.

    Used by .{safe_,}substitute() to combine the mapping and keyword
    arguments.
    """
    def __init__(self, primary, secondary):
        self._primary = primary
        self._secondary = secondary

    def __getitem__(self, key):
        try:
            return self._primary[key]
        except KeyError:
            return self._secondary[key]


class _TemplateMetaclass(type):
    pattern = r"""
    %(delim)s(?:
      (?P<escaped>%(delim)s) |   # Escape sequence of two delimiters
      (?P<named>%(id)s)      |   # delimiter and a Python identifier
      {(?P<braced>%(braced)s)}   |   # delimiter and a braced identifier
      (?P<invalid>)              # Other ill-formed delimiter exprs
    )
    """

    def __init__(cls, name, bases, dct):
        super(_TemplateMetaclass, cls).__init__(name, bases, dct)
        if 'pattern' in dct:
            pattern = cls.pattern
        else:
            pattern = _TemplateMetaclass.pattern % {
                'delim' : _re.escape(cls.delimiter),
                'id'    : cls.idpattern,
                'braced': r'[_a-z][_a-z0-9-]*'
                }
        cls.pattern = _re.compile(pattern, _re.IGNORECASE | _re.VERBOSE)


class Template(object):
    """A string class for supporting $-substitutions."""
    __metaclass__ = _TemplateMetaclass

    delimiter = '$'
    idpattern = r'[_a-z][_a-z0-9]*'

    def __init__(self, template):
        self.template = template

    # Search for $$, $identifier, ${identifier}, and any bare $'s

    def _invalid(self, mo):
        i = mo.start('invalid')
        lines = self.template[:i].splitlines(True)
        if not lines:
            colno = 1
            lineno = 1
        else:
            colno = i - len(''.join(lines[:-1]))
            lineno = len(lines)
        raise ValueError('Invalid placeholder in string: line %d, col %d' %
                         (lineno, colno))

    def substitute(self, *args, **kws):
        if len(args) > 1:
            raise TypeError('Too many positional arguments')
        if not args:
            mapping = kws
        elif kws:
            mapping = _multimap(kws, args[0])
        else:
            mapping = args[0]
        # Helper function for .sub()
        def convert(mo):
            # Check the most common path first.
            named = mo.group('named') or mo.group('braced')
            if named is not None:
                val = mapping[named]
                # We use this idiom instead of str() because the latter will
                # fail if val is a Unicode containing non-ASCII characters.
                return '%s' % (val,)
            if mo.group('escaped') is not None:
                return self.delimiter
            if mo.group('invalid') is not None:
                self._invalid(mo)
            raise ValueError('Unrecognized named group in pattern',
                             self.pattern)
        return self.pattern.sub(convert, self.template)

    def safe_substitute(self, *args, **kws):
        if len(args) > 1:
            raise TypeError('Too many positional arguments')
        if not args:
            mapping = kws
        elif kws:
            mapping = _multimap(kws, args[0])
        else:
            mapping = args[0]
        # Helper function for .sub()
        def convert(mo):
            named = mo.group('named')
            if named is not None:
                try:
                    # We use this idiom instead of str() because the latter
                    # will fail if val is a Unicode containing non-ASCII
                    return '%s' % (mapping[named],)
                except KeyError:
                    return self.delimiter + named
            braced = mo.group('braced')
            if braced is not None:
                try:
                    return '%s' % (mapping[braced],)
                except KeyError:
                    return self.delimiter + '{' + braced + '}'
            if mo.group('escaped') is not None:
                return self.delimiter
            if mo.group('invalid') is not None:
                return self.delimiter
            raise ValueError('Unrecognized named group in pattern',
                             self.pattern)
        return self.pattern.sub(convert, self.template)

########NEW FILE########
__FILENAME__ = compression
# $Id$

import sys,os
import tempfile
from shutil import rmtree
import pkg_resources
import helpers as _helpers 
import re


class Compression(object):
    def __init__(self):
        self.log = _helpers.get_logger(__name__)
        try:
            import tarfile as tarfile
            global tarfile
        except ImportError, e:
            self.log.warn(
                "tarfile module missing, gzip compression not available"
                )
            return None
        
        
class GzipCompression(Compression):
    def __init__(self):
        self.log = _helpers.get_logger(__name__)
        Compression.__init__(self)
        try:
            import gzip as gzip
            global gzip
        except ImportError, e:
            self.log.warn(
                "gzip module missing, gzip compression not available"
                )
            return None        
       
       
class BZ2Compression(Compression):
    def __init__(self):
        Compression.__init__(self)        
        try:
            import bz2
            from bz2 import BZ2File
            global bz2
        except ImportError, e:
            self.log.warn(
                'bz2 module missing, bzip2 compression not available'
                )     
            return None
            
            
class IOCompressionStream(Compression):
    def __init__(self, **kwargs):
        self.log = _helpers.get_logger(__name__)
        Compression.__init__(self)
        self.stream = None
        self.output_path = kwargs.get('output_path', None)
        
        assert not os.path.exists(self.output_path), \
            '%s already exists!' % self.output_path
        _helpers.ensure_dir(os.path.dirname(self.output_path))
        
    def open_stream(self):
        """
        Open a compressed stream for writing.  Must be subclassed.
        """
        raise "compression.IOCompressionStream.open_stream must be subclassed."
    
    def close_stream(self):
        """
        Close a compression stream.
        """
        raise "compression.IOCompressionStream.close_stream must be subclassed."
    
        
class GzipIOCompressionStream(IOCompressionStream,GzipCompression):      
    def __init__(self, **kwargs):
        self.log = _helpers.get_logger(__name__)
        IOCompressionStream.__init__(self, **kwargs)
        GzipCompression.__init__(self)
           
    def open_stream(self):
        """
        Open a Gzip compression stream for writing.  Returns an IO file
        handle object.
        """
        self.stream = gzip.open(self.output_path, 'w')
        self.log.debug('%s gzip io stream opened for writing' % self.stream)
        return self.stream
        
    def close_stream(self):
        """
        Close a Gzip compression stream.
        """
        # FIX ME: not sure what to catch here
        self.stream.close()
        self.log.debug('%s gzip io stream closed.' % self.stream)
        return True 
   
        
class BZ2IOCompressionStream(IOCompressionStream,BZ2Compression):      
    def __init__(self, **kwargs):
        self.log = _helpers.get_logger(__name__)
        IOCompressionStream.__init__(self, **kwargs)
        BZ2Compression.__init__(self)
        
    def open_stream(self):
        """
        Open a Bzip2 compression stream for writing.  Returns an IO file
        handle object.
        """
        self.stream = bz2.BZ2File(self.output_path, 'w')
        self.log.debug(
            '%s bzip2 io stream opened for writing' % self.output_path
            )
        return self.stream
        
    def close_stream(self):
        """
        Close a Bzip2 compression stream.
        """
        # FIX ME: not sure what to catch here
        self.stream.close()
        self.log.debug('%s bzip2 io stream closed.' % self.output_path)
        return True
                
                
class FileCompression(Compression):
    def __init__(self, **kwargs):
        Compression.__init__(self)
        self.log = _helpers.get_logger(__name__)
            
        self.source_path = kwargs.get('source_path', None)
        self.dest_dir = kwargs.get(
            'dest_dir', '%s/' % os.path.dirname(self.source_path)
            )
        self.compressed_path = None
        self.remove_source = kwargs.get('remove_source', False)
        self.compress_success = None
        self.decompress_success = None
        self.compressed_path = None
        self.decompressed_path = None
        
        # validations
        assert self.source_path, '%s missing'
        assert isinstance(self.remove_source, bool), \
            "remove_source must be True/False"

        assert os.path.exists(self.source_path), \
            '%s does not exist, skipping compression' % self.source_path
        
        # real paths please
        self.source_path = os.path.realpath(self.source_path)
        self.dest_dir = os.path.realpath(self.dest_dir)
        
        _helpers.ensure_dir(self.dest_dir)
            
    def compress(self):
        """
        Call all methods to perform compression.
        """
        self._pre_compress()
        self._compress_path()
        self._post_compress()
        
        if self.compress_success:
            return self.compressed_path
        else:
            return None
                 
    def decompress(self):
        """
        Call all methods to perform decompression.
        """
        self._pre_decompress()
        self._decompress_path()
        self._post_decompress()
        
        if self.decompress_success:
            return self.decompressed_path
        else:
            return None
          
    def _pre_compress(self):
        """
        This method is run before compression.
        """
        pass
        
    def _post_compress(self):
        """
        This method is run after compression.
        """
        if self.remove_source:
            self._remove_source_path()
    
    def _pre_decompress(self):
        """
        This method is run before decompression.
        """
        pass
    
    def _post_decompress(self):
        """
        This method is run after decompression.
        """
        if self.remove_source:
            self._remove_source_path()
        
    def _compress_path(self):
        """
        Compress directories or files.  Must be subclassed.
        """
        self.log.warn('_compress_path must be subclassed')

    def _decompress_path(self):
        """
        De-compress directories or files.  Must be subclassed.
        """
        pass
        
    def _remove_source_path(self):
        # FIX ME: need better checks here...  once we have a config to check
        # only delete if the file exists within the holland path or something?
        assert self.dest_dir != '/', 'trying to remove / (root)?'
        #try:
        #    rmtree(self.source_path)
        #    self.log.info('removed path %s' % self.source_path)
        #except IOError, e:
        #    self.log.error('failed to remove %s: %s' % (self.source_path, e))
        self.log.warn(
            'FIX ME -> compression.Compression._remove_source_path need ' +\
            'to properly write this method.'
            )
        if os.path.isfile(self.source_path):
            os.remove(self.source_path)
        elif os.path.isdir(self.source_path):
            rmtree(self.source_path)
        self.log.debug('removed path %s' % self.source_path)
           
              
class GzipFileCompression(FileCompression,GzipCompression):
    def __init__(self, **kwargs):
        FileCompression.__init__(self, **kwargs)
        GzipCompression.__init__(self)
        self.log = _helpers.get_logger(__name__)
        
    def _compress_path(self):
        """
        Compress directories or files using Gzip/Zlib libraries.
        """
        if os.path.isfile(self.source_path):
            self.compressed_path = os.path.join(
                self.dest_dir, "%s.gz" % os.path.basename(self.source_path)
                )
            try:
                f_in = open(self.source_path, "r")
                f_out = gzip.open(self.compressed_path, "w")
                f_out.write(f_in.read())
                f_in.close()
                f_out.close()
                self.log.debug(
                    "%s gzip'd as %s" % ( self.source_path, 
                                          self.compressed_path )
                    )
                self.compress_success = True
                
            except IOError, e:
                self.log.debug("failed to gzip %s" % self.source_path)
                
        elif os.path.isdir(self.source_path):
            self.compressed_path = os.path.join(
                self.dest_dir, "%s.tar.gz" % \
                    os.path.basename(self.source_path)
                )
            try:
                t = tarfile.open(name=self.compressed_path, mode = 'w:gz')
                t.add(self.source_path)
                t.close()
                self.log.debug(
                        "%s gzip'd as %s" % ( self.source_path, 
                                              self.compressed_path )
                        )
                self.compress_success = True
                
            except IOError, e:
                self.log.debug("failed to gzip %s" % self.source_path)
        
        else:
            self.log.warn(
                '%s is not a regular file/directory.  ignoring compression' %\
                self.source_path
                )
         
    def _decompress_path(self):
        """
        De-compress directories or files using Gzip/Zlib libraries.
        """
        self.decompressed_path = os.path.join(
                self.dest_dir, os.path.basename(self.source_path)
                )
        
        if self.decompressed_path.endswith('\.tar.gz'):
            self.decompressed_path = self.decompressed_path.split('.gz')[0]        
        elif self.decompressed_path.endswith('\.gz'):
            self.decompressed_path = self.decompressed_path.split('.gz')[0]
        elif self.decompressed_path.endswith('\.gz'):
            self.decompressed_path = self.decompressed_path.split('.gzip')[0]
            
        
        self.decompressed_path = _helpers.protected_path(
            self.decompressed_path
            )

        try:
            f_in = gzip.open(self.source_path, "r")
            f_out = open(self.decompressed_path, "w")
            f_out.write(f_in.read())
            f_in.close()
            f_out.close()
            
            # is it a tar?
            if tarfile.is_tarfile(self.decompressed_path):    
                tar_file = self.decompressed_path
                self.decompressed_path = self.decompressed_path.split('.tar')[0]
            
                self.decompressed_path = _helpers.protected_path(
                    self.decompressed_path
                    )

                try:
                    t = tarfile.open(name=tar_file, mode = 'r:')
                    t.extractall(self.decompressed_path)
                    t.close()
                    os.remove(tar_file)
        
                except IOError, e:
                    self.log.error(
                        "failed to untar %s (%s)" %\
                            (self.source_path, e)
                        )
                    
            self.log.debug(
                "%s gunzip'd as %s" % ( self.source_path, 
                                        self.decompressed_path )
                )
            self.decompress_success = True
            
        except IOError, e:
            self.log.error("failed to gunzip %s (%s)" % (self.source_path, e))
        
    
class BZ2FileCompression(FileCompression,BZ2Compression):
    def __init__(self, **kwargs):
        FileCompression.__init__(self, **kwargs)
        BZ2Compression.__init__(self)
        self.log = _helpers.get_logger(__name__)
                
    def _compress_path(self):
        """
        Compress directories or files using bz2(Bzip2) libraries.
        """
        if os.path.isfile(self.source_path):
            self.compressed_path = os.path.join(
                self.dest_dir, "%s.bz2" % os.path.basename(self.source_path)
                )
            try:
                f_in = open(self.source_path, "r")
                f_out = bz2.BZ2File(self.compressed_path, "w")
                f_out.write(f_in.read())
                f_in.close()
                f_out.close()
                self.log.debug(
                    "%s bzip2'd as %s" % ( self.source_path, 
                                          self.compressed_path )
                    )
                self.compress_success = True
                
            except IOError, e:
                self.log.error("failed to bzip2 %s" % self.source_path)
                
        elif os.path.isdir(self.source_path):
            self.compressed_path = os.path.join(
                self.dest_dir, "%s.tar.bz2" % \
                    os.path.basename(self.source_path)
                )
            try:
                t = tarfile.open(name=self.compressed_path, mode = 'w:bz2')
                t.add(self.source_path)
                t.close()
                self.log.debug(
                        "%s bzip2'd as %s" % ( self.source_path, 
                                              self.compressed_path )
                        )
                self.compress_success = True
                
            except IOError, e:
                self.log.error("failed to bzip2 %s" % self.source_path)
        
        else:
            self.log.warn(
                '%s is not a regular file/directory.  ignoring compression' %\
                self.source_path
                )
        
    def _decompress_path(self):
        """
        De-compress directories or files using bz2(Bzip2) libraries.
        """
        self.decompressed_path = os.path.join(
                self.dest_dir, '%s.bz2' % os.path.basename(self.source_path)
                )
                
        if self.decompressed_path.endswith('\.bz2'):
            self.decompressed_path = self.decompressed_path.split('.bz2')[0]
        elif self.decompressed_path.endswith('\.bzip2'):
            self.decompressed_path = self.decompressed_path.split('.bzip2')[0]
            
        
        self.decompressed_path = _helpers.protected_path(
            self.decompressed_path
            )

        try:
            f_in = bz2.BZ2File(self.source_path, "r")
            f_out = open(self.decompressed_path, "w")
            f_out.write(f_in.read())
            f_in.close()
            f_out.close()
            
            # is it a tar?
            if tarfile.is_tarfile(self.decompressed_path):    
                tar_file = self.decompressed_path
                self.decompressed_path = self.decompressed_path.split('.tar')[0]
            
                self.decompressed_path = _helpers.protected_path(
                    self.decompressed_path
                    )

                try:
                    t = tarfile.open(name=tar_file, mode = 'r:')
                    t.extractall(self.decompressed_path)
                    t.close()
                    os.remove(tar_file)
        
                except IOError, e:
                    self.log.error(
                        "failed to untar %s (%s)" %\
                            (self.source_path, e)
                        )
                    
            self.log.debug(
                "%s bunzip'd as %s" % ( self.source_path, 
                                        self.decompressed_path )
                )
            self.decompress_success = True
            
        except IOError, e:
            self.log.error("failed to bunzip %s (%s)" % (self.source_path, e))
    
########NEW FILE########
__FILENAME__ = compression_util
    
def get_compression_stream(**kwargs):
    """
    Open a compressed data stream.  Expects the following params:
    
        output_path = output file path
        mode = compression mode - gzip, bzip2, ...
    
    Returns:
    
        stream = compression.IOCompressionStream object
    
    Example:
    
        >>> stream = helpers.get_compression_stream(
                output_path='/tmp/myfile.gz', mode='gzip'
                )
        >>> stream.write('data... data... data...')
        >>> stream.close()
        
    """
    log = get_logger(__name__)
    output_path = kwargs.get('output_path', None)
    mode = kwargs.get('mode')
    
    assert mode in ['gzip', 'bzip2'], \
        "%s is not a supported compression mechanism."
    
    if mode == 'gzip':
        c = compression.GzipIOCompressionStream(output_path=output_path) 

    elif mode == 'bzip2':
        c = compression.BZ2IOCompressionStream(output_path=output_path) 
    c.open_stream()    
    return c.stream
       
def compress_path(**kwargs):
    """
    Compress a file or directory path.  Expects the following params:
    
        source_path = source directory or file to compress
        dest_dir = destination directory to save the compressed file
        remove_source = True/False - removes original source if True
        mode = compression mode - gzip, bzip2, ...
        
    Returns:
        
        string  - compressed file path on success
        None    - on failure
    """
    log = get_logger(__name__)
    source_path = kwargs.get('source_path', None)
    dest_dir = kwargs.get('dest_dir', os.path.dirname(source_path))
    mode = kwargs.get('mode', 'gzip').lower()
    remove_source = kwargs.get('remove_source', False)

    assert mode in ['gzip', 'bzip2'], \
        "%s is not a supported compression mechanism."
    
    if mode == 'gzip':
        c = compression.GzipFileCompression(
            source_path=source_path, dest_dir=dest_dir, 
            remove_source=remove_source
            )        

    elif mode == 'bzip2':
        c = compression.BZ2FileCompression(
            source_path=source_path, dest_dir=dest_dir, 
            remove_source=remove_source
            )        

    if c.compress():
        return c.compressed_path
    else:
        log.error('compressing path %s seems to have failed' % source_path)
        return None
    
def decompress_path(**kwargs):
    """
    Decompress a file or directory path.  Accepts the following params:
    
        source_path = source directory or file to decompress
        dest_dir = destination directory to save the decompressed file/dir
        remove_source = True/False - removes original source if True
        mode = gzip, bzip2, ...
        
    Returns:
        
        string  - decompressed file or directory path on success
        None    - on failure
        
    """
    log = get_logger(__name__)
    source_path = kwargs.get('source_path', None)
    dest_dir = kwargs.get('dest_dir', os.path.dirname(source_path))
    mode = kwargs.get('mode', None)
    remove_source = kwargs.get('remove_source', False)

    if not mode:
        if source_path.endswith('.gz') or source_path.endswith('gzip'):
            mode = 'gzip'
        elif source_path.endswith('.bzip2') or source_path.endswith('bz2'):
            mode = 'bzip2'    
    
    assert str(mode).lower() in ['gzip', 'bzip2'], \
        "%s is not a supported compression mechanism." % mode
    
    if mode == 'gzip':
        c = compression.GzipFileCompression(
            source_path=source_path, dest_dir=dest_dir, 
            remove_source=remove_source
            )        
    elif mode == 'bzip2':
        c = compression.BZ2FileCompression(
            source_path=source_path, dest_dir=dest_dir, 
            remove_source=remove_source
            )
    if c.decompress():
        return c.decompressed_path
    else:
        log.error('decompressing path %s seems to have failed' % source_path)
        return None
        

########NEW FILE########
__FILENAME__ = config
from holland.lib.configobj import ConfigObj
from holland.lib.validate import Validator
from checks import checks

from holland.helpers.log import get_logger, get_logging
from holland.helpers.providers import load_provider_resource, list_providers
import os
import pkg_resources as pkgr

config = ConfigObj()
spec = ConfigObj()

def extract_params(section):
    global config
    params = {}

    if section not in config:
        return params
    
    for option in config[section]:
        params[option] = config[section][option]

    return params

def get_config(**kwargs):
    global config
    global spec
    config_file = os.path.abspath(kwargs.get("config_file",
                                             "/etc/holland/holland.conf"))
    include_folders = kwargs.get("include_folders", ["providers",
                                                     "helpers",
                                                     "backupsets"])
    
    log = get_log(**kwargs)
    
    config_dir = os.path.join(*os.path.split(config_file)[:-1])
    log.debug("Detected configuration directory %s" % config_dir)
    
    log.debug("Getting initial config from %s" % config_file)
    config.merge(get_config_from_file(config_file))
    
    log.debug("Looking for config files")
    for folder in include_folders:
        folder_path = os.path.join(config_dir, folder)
        
        if not os.path.exists(folder_path):
            continue
        
        log.debug("Searching %s for config files" % folder_path)
        for included_config in get_configs_from_dir(dir=folder_path):
            log.debug("Merging %s into config" % included_config)
            config.merge(included_config)

    get_spec()

    log.debug("Reinitializing config with configspec")
    config = ConfigObj(config, configspec=spec)
    
    vtor = Validator()
    for check in checks:
        log.debug("Adding %s check to validator" % check)
        vtor.functions[check] = checks[check]
    
    log.debug("Validating config")
    errors = config.validate(vtor, preserve_errors=True)
    
    # Strip out unknown sections and keywords
    log.debug("Striping out unknown sections and keywords")
    known_sections = spec.keys()
    for section in config.keys():
        if section not in known_sections:
            log.warn("Section %s is not known, removing" % section)
            del config[section]
            continue
        for keyword,value in config[section].items():
            if isinstance(value, dict) and spec.get(section, {}).has_key('__many__'):
                continue
            if keyword not in spec.get(section, {}).keys():
                log.warn("Keyword %s in section %s is not known, removing" % \
                         (keyword, section))
                del config[section][keyword]
    log.debug("Going forth with config: %s" % config)


def get_configs_from_dir(dir):
    configs = []
    log = get_log()
    
    for root, dirs, files in os.walk(os.path.abspath(dir)):
        for config_file in files:
            config_file = os.path.join(root, config_file)
            log.debug("Found file %s" % config_file)
            configs.append(get_config_from_file(config_file))
    return configs

def get_config_from_file(file):
    return ConfigObj(file)

def get_spec():
    global spec
    log = get_log()

    log.debug("Looking for configspecs")
    for file_spec in pkgr.resource_listdir("holland", "validators"):
        if file_spec.endswith("configspec"):
            log.debug("Found configspec %s" % file_spec)
            file_stream = pkgr.resource_stream("holland",
                                               "validators/" + file_spec)
            log.debug("Merging %s into spec" % file_spec)
            spec.merge(ConfigObj(file_stream,list_values=False))
    
    log.debug("Searching providers for configspecs")
    for provider_spec in get_specs_from_providers():
        spec.merge(provider_spec)

def get_specs_from_providers():
    specs = []
    log = get_log()
    
    providers = list_providers()
    for provider in providers.keys():
        log.debug("Looking for configspec from %s" % providers[provider][0])
        provider_spec = get_provider_spec(providers[provider][0])
        specs.append(ConfigObj(provider_spec,list_values=False))
        
    return specs

def get_connect_params(**kwargs):
    # generate dictionary of params to pass to MySQLdb connect
    section = kwargs.get("section", "mysql_connect")
    return extract_params(section)

def setup_bootstrap_logging(**kwargs):
    # When we first startup logging needs the config, but the config needs
    # logging.  This will setup enough of logging to log the config parsing.
    # No validation or checking is preformed, we just need a log file to
    # write to.
    config_file = kwargs.get("config_file", "/etc/holland/holland.conf")
    
    log_config = ConfigObj(os.path.abspath(config_file), file_error=True)
    log_file = log_config["logging"]["log_filename"]
    
    logging = get_logging()
    log = logging.getLogger("holland")
    
    log.setLevel(logging.DEBUG)
    
    bootstrap_logging = logging.FileHandler(log_file)
    bootstrap_logging.name = "bootstrap"
    bootstrap_logging.setLevel(logging.DEBUG)
    
    formatter = logging.Formatter(
                            "%(asctime)s %(name)s %(levelname)s: %(message)s")
    
    bootstrap_logging.setFormatter(formatter)
    log.addHandler(bootstrap_logging)

    
def get_log(**kwargs):
    log = get_logger("holland")

    if len(log.handlers) == 0:
        setup_bootstrap_logging(**kwargs)

    return get_logger(__name__)

def get_log_config(**kwargs):
    # generate dictionary of params to pass to setup_logger
    global config
    section = kwargs.get("section", "logging")
    force_verbose = kwargs.get("stdout_verbose", False)
    
    if not config:
        get_config(**kwargs)
        
    params = extract_params(section)
    
    if force_verbose and params.has_key("stdout_verbose"):
        if not params["stdout_verbose"]:
            params["stdout_verbose"] = force_verbose

    return params

def get_provider_spec(provider_name):
    """
    Find the configspec from the provider
    This will throw a LookupError if no configspec is found, or the 
    provider does not exist
    """
    cfgspec = load_provider_resource(provider_name, 
                                    'validators/%s.configspec' % provider_name)
    return cfgspec.strip().splitlines()    

def rel_path(base, target):
    path = []
    dir, file = os.path.split(os.path.abspath(target))
    path.append(file)
    while dir != os.path.abspath(base):
        dir, file = os.path.split(dir)
        path.append(file)
    path.reverse()
    return os.path.join(*path)

########NEW FILE########
__FILENAME__ = log
try:
    import logging
    import logging.handlers as handlers
except ImportError:
    import holland.backports.logging as logging
    import holland.backports.logging.handlers as handlers

def get_logging():
    logging.handlers = handlers
    return logging

def get_logger(namespace):
    """
    Get logging namespace.  
    
    Example:
    
        log = holland.helpers.log.get_logger(__name__)
    """
    return logging.getLogger(namespace)


########NEW FILE########
__FILENAME__ = log_init
import sys
from types import StringTypes, FileType
import logging
import logging.config

DEFAULT_LEVEL   = logging.NOTSET
DEFAULT_FORMAT  = "%(asctime)s %(name)s %(levelname)s: %(message)s"
DEFAULT_DATEFMT = "%Y-%m-%d %H:%M:%S"

def _convert_strlevel(name):
    level = {
        'CRITICAL' : logging.CRITICAL,
        'ERROR' : logging.ERROR,
        'WARNING' : logging.WARNING,
        'INFO' : logging.INFO,
        'DEBUG' : logging.DEBUG,
    }
    return level.get(name.upper(), logging.WARNING)

def initialize_logging(**kwargs):
    # clear existing root handlers
    root_logger = logging.getLogger()
    map(root_logger.removeHandler, root_logger.handlers)

    # basic setup
    level = kwargs.get('level', DEFAULT_LEVEL)
    if isinstance(level, StringTypes):
        level = _convert_strlevel(level)
    format = kwargs.get('format', DEFAULT_FORMAT)
    datefmt = kwargs.get('datefmt', DEFAULT_DATEFMT)
    output = kwargs.get('output', sys.stderr)
    logging_config = kwargs.get('config_file')

    if logging_config:
        logging.config.fileConfig(logging_config)

    if isinstance(output, StringTypes):
        logging.basicConfig(level=level, format=format, datefmt=datefmt, filename=output)
    elif isinstance(output, FileType):
        logging.basicConfig(level=level, format=format, datefmt=datefmt, stream=output)
    else:
        logging.basicConfig(level=level, format=format, datefmt=datefmt)

def next_loglevel(current_level, increment=1):
    log_levels = [
        logging.CRITICAL,
        logging.ERROR,
        logging.WARNING,
        logging.INFO,
        logging.DEBUG
    ]
    idx = log_levels.index(current_level)
    idx = (idx + increment)
    if idx > len(log_levels):
        idx = len(log_levels) - 1
    return log_levels[idx]

def get_logger():
    return logging.getLogger(__name__)

# Original logging function
def setup_logger(**kwargs):
    verbose = kwargs.get("stdout_verbose", False)
    filename = kwargs.get("log_filename", "/var/log/holland/holland.log")
    max_size = kwargs.get("log_max_size", 1073741824)
    keep_count = kwargs.get("log_backup_count", 4)

    log_dir = os.path.dirname(filename)
    h.ensure_dir(log_dir)

    log = h.get_logger('holland')

    # See if there is a bootstrap handler and remove it
    if len(log.handlers) == 1:
        if hasattr(log.handlers[0], "name"):
            if log.handlers[0].name == "bootstrap":
                log.removeHandler(log.handlers[0])

    logging = h.get_logging()
    log.setLevel(logging.DEBUG)

    console = logging.StreamHandler()
    if verbose:
        console.setLevel(logging.DEBUG)
    else:
        console.setLevel(logging.INFO)

    file = logging.handlers.RotatingFileHandler(filename=filename,
                                                maxBytes=max_size,
                                                backupCount=keep_count)
    file.setLevel(logging.DEBUG)
    cformatter = logging.Formatter("%(levelname)-8s:  %(message)s")
    fformatter = logging.Formatter("%(asctime)s %(name)s %(levelname)s: %(message)s")

    console.setFormatter(cformatter)
    file.setFormatter(fformatter)

    log.addHandler(console)
    log.addHandler(file)

########NEW FILE########
__FILENAME__ = example
import logging

LOG = logging.getLogger(__name__)

# Specification for this plugin
# See: http://www.voidspace.org.uk/python/validate.html
CONFIGSPEC = """
[example]
foo_param = boolean(default=no)
""".splitlines()

class ExamplePlugin(object):
    """An example backup plugin for holland"""

    def __init__(self, name, config, target_directory, dry_run=False):
        """Createa new ExamplePlugin instance

        :param name: unique name of this backup
        :param config: dictionary config for this plugin
        :param target_directory: str path, under which backup data should be
                                 stored
        :param dry_run: boolean flag indicating whether this should be a real
                        backup run or whether this backup should only go
                        through the motions
        """
        self.name = name
        self.config = config
        self.target_directory = target_directory
        self.dry_run = dry_run
        LOG.info("Validating config")
        self.config.validate_config(CONFIGSPEC)

    def estimate_backup_size(self):
        """Estimate the size (in bytes) of the backup this plugin would
        produce, if run.

        :returns: int. size in bytes
        """
        return 0

    def backup(self):
        """
        Do what is necessary to perform and validate a successful backup.
        """
        if self.dry_run:
            LOG.info("[Dry run] Example Plugin - test backup run")
        else:
            LOG.info("Example plugin - real backup run")

    def info(self):
        """Provide extra information about the backup this plugin produced

        :returns: str. A textual string description the backup referenced by
                       `self.config`
        """
        return "Example plugin"

########NEW FILE########
__FILENAME__ = test_plugin
import time
from holland.backup.example import ExamplePlugin
from nose.tools import *

# Config mock, so we don't have to import holland.core
class MockConfig(object):
    def validate(self, spec):
        pass

def test_example_plugin():
    name = 'example/' + time.strftime('%Y%m%d_%H%M%S')
    target_directory = '/tmp/example_backup/'
    config = MockConfig()
    dry_run = False

    plugin = ExamplePlugin(name, config, target_directory, dry_run)
    assert_equals(plugin.estimate_backup_size(), 0)
    plugin.backup()

    dry_run = True

    plugin = ExamplePlugin(name, config, target_directory, dry_run)
    plugin.backup()

    ok_(isinstance(plugin.info(), basestring))

########NEW FILE########
__FILENAME__ = base
"""Main driver"""

import sys
import csv
import errno
import logging
from holland.core.exceptions import BackupError
from holland.lib.safefilename import encode
from holland.backup.mysqldump.command import ALL_DATABASES, MySQLDumpError
from holland.backup.mysqldump.mock.env import MockEnvironment

LOG = logging.getLogger(__name__)

def dry_run(*args, **kwargs):
    """Run a backup in no-op mode"""
    env = MockEnvironment()
    try:
        env.replace_environment()
        start(*args, **kwargs)
    finally:
        env.restore_environment()

def run(mysqldump, config): pass

def start(mysqldump,
          schema=None,
          lock_method='auto-detect',
          file_per_database=True,
          open_stream=open,
          compression_ext=''):
    """Run a mysqldump backup"""

    if not schema and file_per_database:
        raise BackupError("file_per_database specified without a valid schema")

    if not schema:
        target_databases = ALL_DATABASES
    else:

        if len(schema.databases) == 0:
            raise BackupError("No databases found to backup")

        if not file_per_database and not [x for x in schema.excluded_databases]:
            target_databases = ALL_DATABASES
        else:
            target_databases = [db for db in schema.databases
                                    if not db.excluded]
            write_manifest(schema, open_stream, compression_ext)

    if file_per_database:
        flush_logs = '--flush-logs' in mysqldump.options
        if flush_logs:
            mysqldump.options.remove('--flush-logs')
        last = len(target_databases)
        for count, db in enumerate(target_databases):
            more_options = [mysqldump_lock_option(lock_method, [db])]
            # add --flush-logs only to the last mysqldump run
            if flush_logs and count == last:
                more_options.append('--flush-logs')
            db_name = encode(db.name)[0]
            if db_name != db.name:
                LOG.warning("Encoding file-name for database %s to %s", db.name, db_name)
            try:
                stream = open_stream('%s.sql' % db_name, 'w')
            except (IOError, OSError), exc:
                raise BackupError("Failed to open output stream %s: %s" %
                                  ('%s.sql' + compression_ext, str(exc)))
            try:
                mysqldump.run([db.name], stream, more_options)
            finally:
                try:
                    stream.close()
                except (IOError, OSError), exc:
                    if exc.errno != errno.EPIPE:
                        LOG.error("%s", str(exc))
                        raise BackupError(str(exc))
    else:
        more_options = [mysqldump_lock_option(lock_method, target_databases)]
        try:
            stream = open_stream('all_databases.sql', 'w')
        except (IOError, OSError), exc:
            raise BackupError("Failed to open output stream %s: %s" %
                              'all_databases.sql' + compression_ext, exc)
        try:
            if target_databases is not ALL_DATABASES:
                target_databases = [db.name for db in target_databases]
            mysqldump.run(target_databases, stream, more_options)
        finally:
            try:
                stream.close()
            except (IOError, OSError), exc:
                if exc.errno != errno.EPIPE:
                    LOG.error("%s", str(exc))
                    raise BackupError(str(exc))

def write_manifest(schema, open_stream, ext):
    """Write real database names => encoded names to MANIFEST.txt"""
    manifest_fileobj = open_stream('MANIFEST.txt', 'w', method='none')
    try:
        manifest = csv.writer(manifest_fileobj,
                              dialect=csv.excel_tab,
                              lineterminator="\n",
                              quoting=csv.QUOTE_MINIMAL)
        for database in schema.databases:
            if database.excluded:
                continue
            name = database.name
            encoded_name = encode(name)[0]
            manifest.writerow([name.encode('utf-8'), encoded_name + '.sql' + ext])
    finally:
        manifest_fileobj.close()
        LOG.info("Wrote backup manifest %s", manifest_fileobj.name)

def mysqldump_lock_option(lock_method, databases):
    """Choose the mysqldump option to use for locking
    given the requested lock-method and the set of databases to
    be backed up
    """
    if lock_method == 'auto-detect':
        return mysqldump_autodetect_lock(databases)
    else:
        valid_methods = {
            'flush-lock'            : '--lock-all-tables',
            'lock-tables'           : '--lock-tables',
            'single-transaction'    : '--single-transaction',
            'none'                  : '--skip-lock-tables',
        }
        try:
            return valid_methods[lock_method]
        except KeyError:
            raise BackupError("Invalid mysqldump lock method %r" % \
                              lock_method)

def mysqldump_autodetect_lock(databases):
    """Auto-detect if we can do a transactional or
    non-transactional backup with mysqldump
    """

    if databases == ALL_DATABASES:
        return '--lock-all-tables'

    for database in databases:
        if database.excluded:
            continue
        for table in database.tables:
            if table.excluded:
                continue
            if not table.is_transactional:
                return '--lock-tables'
    else:
        return '--single-transaction'

########NEW FILE########
__FILENAME__ = command
"""mysqldump support"""

import os
import re
import errno
import logging
import subprocess
from tempfile import TemporaryFile

LOG = logging.getLogger(__name__)

ALL_DATABASES = object()

def check_master_data(version, arg):
    """Validate --master-data against a mysqldump version"""
    if version < (4, 1, 8) and arg:
        raise MyOptionError("--master-data only takes an argument in MySQL "
                            ">= 4.1.8")
    else:
        if arg:
            try:
                value = int(arg)
                assert value in (1, 2)
            except ValueError:
                raise MyOptionError("Invalid argument to --master-data: %r" % \
                                    arg)
            except AssertionError:
                raise MyOptionError("Argument to --master-data must be 1 or 2 "
                                    "not %r" % arg)

class MySQLDumpError(Exception):
    """Excepton class for MySQLDump errors"""

class MyOptionError(Exception):
    """Exception class for MySQL Option validation"""

class MyOptionChecker(object):
    """Container for adding and validating multiple options"""
    OPTION_ARG_CRE = re.compile(r'^(--[^=]+)(?:=(.+))?$', re.UNICODE)

    def __init__(self, version):
        self.version = version
        self._options = {}

    def check_option(self, option):
        """Check an option"""
        try:
            option, arg = self.OPTION_ARG_CRE.search(option).groups()
        except AttributeError:
            raise MyOptionError("Unparseable option '%s'" % option)

        option = option.replace('_', '-')

        if option not in self._options:
            raise MyOptionError("User supplied option '%s'" % option)
        my_option = self._options[option]
        my_option.check(self.version, arg)

    def add_option(self, my_option):
        """Add an option to this validator"""
        self._options[my_option.option] = my_option


class MyOption(object):
    """General MySQL command option"""
    def __init__(self, option, min_version=None, arg=None):
        self.option = option
        self.min_version = min_version
        self.arg = arg

    def check(self, version, arg=None):
        """Check this option against the particular mysql version
        and given the particular argument to the option.
        """
        self.check_version(version)
        self.check_arg(version, arg)

    def check_arg(self, version, arg):
        """Check the given argument against this option.
        """
        if isinstance(self.arg, basestring):
            return re.match(self.arg, arg, re.UNICODE) is not None
        elif callable(self.arg):
            return self.arg(version, arg)
        elif arg:
            raise MyOptionError("Invalid arg constraint %r" % self.arg)

    def check_version(self, version):
        """Check the given command version against the version required by
        this option"""
        if self.min_version and version < self.min_version:
            raise MyOptionError("Option %r requires minimum version %s" % \
                                (self.option,
                                 '.'.join([str(x) for x in self.min_version])
                                )
                            )

# XXX: support --skip-* for all options as well
MYSQLDUMP_OPTIONS = [
    # boolean options
    MyOption('--flush-logs'),
    MyOption('--flush-privileges', min_version=(5,0,26)),
    MyOption('--force'),
    MyOption('--hex-blob', min_version=(4,1,8)),
    MyOption('--add-drop-database'),
    MyOption('--no-autocommit'),
    MyOption('--delete-master-logs'),
    MyOption('--compress'),
    MyOption('--order-by-primary', min_version=(4,1,8)),
    MyOption('--insert-ignore', min_version=(4,1,12)),
    MyOption('--routines', min_version=(5,0,13)),
    MyOption('--events', min_version=(5,1,8)),
    MyOption('--max-allowed-packet', arg='\w'),

    # options that take arguments
    MyOption('--default-character-set', arg='\w'),
    MyOption('--master-data', arg=check_master_data),

    # lock modes
    MyOption('--single-transaction', (4,0,2)),
    MyOption('--lock-all-tables', min_version=(4,1,8)),
    MyOption('--lock-tables'),

    # misc
    MyOption('--skip-dump-date', min_version=(5,1,23)),
]

def mysqldump_version(command):
    """Return the version of the given mysqldump command"""
    args = [
        command,
        '--no-defaults',
        '--version',
    ]
    list2cmdline = subprocess.list2cmdline
    cmdline = list2cmdline(args)
    LOG.debug("Executing: %s", cmdline)
    try:
        process = subprocess.Popen(args,
                                   stdout=subprocess.PIPE,
                                   stderr=subprocess.STDOUT,
                                   close_fds=True)
        stdout, _ = process.communicate()
    except OSError, exc:
        if exc.errno == errno.ENOENT:
            raise MySQLDumpError("'%s' does not exist" % command)
        else:
            raise MySQLDumpError("Error[%d:%s] when trying to run '%s'" % \
                    (exc.errno, errno.errorcode[exc.errno], command))

    if process.returncode != 0:
        LOG.error("%s exited with non-zero status[%d]",
                  cmdline, process.returncode)
        for line in stdout.splitlines():
            LOG.error("! %s", line)
    try:
        return tuple([int(digit) for digit in
                        re.search(r'(\d+)[.](\d+)[.](\d+)', stdout).groups()])
    except AttributeError, exc:
        LOG.debug("%s provided output %r", cmdline, stdout)
        raise MySQLDumpError("Failed to determine mysqldump version for %s" % \
                             command)

class MySQLDump(object):
    """mysqldump command runner"""
    def __init__(self,
                 defaults_file,
                 cmd_path='mysqldump',
                 extra_defaults=False):
        if not os.path.exists(cmd_path):
            raise MySQLDumpError("'%s' does not exist" % cmd_path)
        self.cmd_path = cmd_path
        self.defaults_file = defaults_file
        self.extra_defaults = extra_defaults
        self.version = mysqldump_version(cmd_path)
        self.version_str = u'.'.join([str(digit) for digit in self.version])
        self.mysqldump_optcheck = MyOptionChecker(self.version)
        for optspec in MYSQLDUMP_OPTIONS:
            self.mysqldump_optcheck.add_option(optspec)
        self.options = []

    def add_option(self, option):
        """Add an option to this mysqldump instance, to be used
        when mysqldump is actually run via the instances .run() method
        """
        if option in self.options:
            LOG.warn("mysqldump option '%s' already requested.", option)
        self.options.append(option)
        self.mysqldump_optcheck.check_option(option)

    def run(self, databases, stream, additional_options=None):
        """Run mysqldump with the options configured on this instance"""
        if not hasattr(stream, 'fileno'):
            raise MySQLDumpError("Invalid output stream")

        if not databases:
            raise MySQLDumpError("No databases specified to backup")

        args = [ self.cmd_path, ]

        if self.defaults_file:
            if self.extra_defaults:
                args.append('--defaults-extra-file=%s' % self.defaults_file)
            else:
                args.append('--defaults-file=%s' % self.defaults_file)

        args.extend([str(opt) for opt in self.options])

        if additional_options:
            args.extend(additional_options)

        if databases is ALL_DATABASES:
            args.append('--all-databases')
        else:
            if len(databases) > 1:
                args.append('--databases')
            args.extend(databases)

        LOG.info("Executing: %s", subprocess.list2cmdline(args))
	errlog = TemporaryFile()
        pid = subprocess.Popen(args, 
                               stdout=stream.fileno(), 
                               stderr=errlog.fileno(), 
                               close_fds=True)
        status = pid.wait()
        try:
            errlog.flush()
            errlog.seek(0)
            for line in errlog:
                LOG.error("%s[%d]: %s", self.cmd_path, pid.pid, line.rstrip())
        finally:
            errlog.close()
	if status != 0:
            raise MySQLDumpError("mysqldump exited with non-zero status %d" % \
                                 pid.returncode)

########NEW FILE########
__FILENAME__ = env
import logging
from mocker import Mocker, ANY, CONTAINS, ARGS, KWARGS, MATCH
import storage
from _subprocess import PopenMock

LOG = logging.getLogger(__name__)

class MockEnvironment(object):
    def __init__(self):
        self.mocker = Mocker()
        _setup_fileio(self.mocker)
        _setup_mysqlclient(self.mocker)
        _setup_subprocess(self.mocker)

    def replace_environment(self):
        self.mocker.replay()

    def restore_environment(self):
        # restore MySQLClient
        # restore normal file io
        # restore normal subprocess
        self.mocker.restore()

def _setup_fileio(mocker):
    """Patch __builtin__.open.

    Allow read access normally, but all writes go
    through the storage mock layer.
    """
    def is_writemode(param):
        """Check whether a open() mode will allow a write operation"""
        return 'a' in param or 'w' in param

    # this doesn't handle r+ properly, but we don't use it in mysqldump
    _open = mocker.replace('__builtin__.open')
    fileobj = _open(ANY, MATCH(is_writemode), ANY)
    mocker.count(min=0,max=None)
    mocker.call(storage.open)

    fileobj = _open(ANY, MATCH(is_writemode))
    mocker.count(min=0,max=None)
    mocker.call(storage.open)

    _mkdir = mocker.replace('os.mkdir')
    _mkdir(ARGS)
    mocker.count(min=0,max=None)

def _setup_mysqlclient(mocker):
    """Patch any dangerous methods on MySQLClient

    Used for dry-run support.  Currently avoid
    doing stop_slave or start_slave in dry-run mode
    but allow all other (read) operations.
    """
    connect = mocker.replace("holland.lib.mysql.client.base.connect")
    client = connect(ARGS, KWARGS)

    client.connect()
    mocker.count(min=0,max=None)
    client.disconnect()
    mocker.count(min=0,max=None)
    # replace stop_slave, start_slave
    client.stop_slave(ARGS, KWARGS)
    mocker.count(min=0,max=None)
    client.start_slave()
    mocker.count(min=0,max=None)

def match_mysqldump(param):
    LOG.debug("match_mysqldump %r", param)
    if param and 'mysqldump' in param[0]:
        return "--version" in param
    return False

def _setup_subprocess(mocker):
    """Patch subprocess.Popen with a Mock object.

    In dry-run mode, don't run any commands - just
    fake it.
    """
    popen = mocker.replace("subprocess.Popen")
    pid = popen(MATCH(match_mysqldump), KWARGS)
    mocker.count(min=0,max=None)
    mocker.passthrough()

    pid = popen(ARGS, KWARGS)
    mocker.count(min=0,max=None)
    mocker.call(PopenMock)

########NEW FILE########
__FILENAME__ = io
"""Mock IO access"""
from storage import replace_builtins

def mock_io(mocker):
    replace_builtins()

########NEW FILE########
__FILENAME__ = mocker
"""
Copyright (c) 2007  Gustavo Niemeyer <gustavo@niemeyer.net>

Graceful platform for test doubles in Python (mocks, stubs, fakes, and dummies).
"""
import __builtin__
import tempfile
import unittest
import inspect
import shutil
import types
import sys
import os
import gc


if sys.version_info < (2, 4):
    from sets import Set as set # pragma: nocover


__all__ = ["Mocker", "expect", "IS", "CONTAINS", "IN", "MATCH",
           "ANY", "ARGS", "KWARGS"]


__author__ = "Gustavo Niemeyer <gustavo@niemeyer.net>"
__license__ = "PSF License"
__version__ = "0.10.1"


ERROR_PREFIX = "[Mocker] "


# --------------------------------------------------------------------
# Exceptions

class MatchError(AssertionError):
    """Raised when an unknown expression is seen in playback mode."""


# --------------------------------------------------------------------
# Helper for chained-style calling.

class expect(object):
    """This is a simple helper that allows a different call-style.

    With this class one can comfortably do chaining of calls to the
    mocker object responsible by the object being handled. For instance::

        expect(obj.attr).result(3).count(1, 2)

    Is the same as::

        obj.attr
        mocker.result(3)
        mocker.count(1, 2)

    """

    def __init__(self, mock, attr=None):
        self._mock = mock
        self._attr = attr

    def __getattr__(self, attr):
        return self.__class__(self._mock, attr)

    def __call__(self, *args, **kwargs):
        getattr(self._mock.__mocker__, self._attr)(*args, **kwargs)
        return self


# --------------------------------------------------------------------
# Extensions to Python's unittest.

class MockerTestCase(unittest.TestCase):
    """unittest.TestCase subclass with Mocker support.

    @ivar mocker: The mocker instance.

    This is a convenience only.  Mocker may easily be used with the
    standard C{unittest.TestCase} class if wanted.

    Test methods have a Mocker instance available on C{self.mocker}.
    At the end of each test method, expectations of the mocker will
    be verified, and any requested changes made to the environment
    will be restored.

    In addition to the integration with Mocker, this class provides
    a few additional helper methods.
    """

    expect = expect

    def __init__(self, methodName="runTest"):
        # So here is the trick: we take the real test method, wrap it on
        # a function that do the job we have to do, and insert it in the
        # *instance* dictionary, so that getattr() will return our
        # replacement rather than the class method.
        test_method = getattr(self, methodName, None)
        if test_method is not None:
            def test_method_wrapper():
                try:
                    result = test_method()
                except:
                    raise
                else:
                    if (self.mocker.is_recording() and
                        self.mocker.get_events()):
                        raise RuntimeError("Mocker must be put in replay "
                                           "mode with self.mocker.replay()")
                    if (hasattr(result, "addCallback") and
                        hasattr(result, "addErrback")):
                        def verify(result):
                            self.mocker.verify()
                            return result
                        result.addCallback(verify)
                    else:
                        self.mocker.verify()
                    return result
            # Copy all attributes from the original method..
            for attr in dir(test_method):
                # .. unless they're present in our wrapper already.
                if not hasattr(test_method_wrapper, attr) or attr == "__doc__":
                    setattr(test_method_wrapper, attr,
                            getattr(test_method, attr))
            setattr(self, methodName, test_method_wrapper)

        # We could overload run() normally, but other well-known testing
        # frameworks do it as well, and some of them won't call the super,
        # which might mean that cleanup wouldn't happen.  With that in mind,
        # we make integration easier by using the following trick.
        run_method = self.run
        def run_wrapper(*args, **kwargs):
            try:
                return run_method(*args, **kwargs)
            finally:
                self.__cleanup()
        self.run = run_wrapper

        self.mocker = Mocker()

        self.__cleanup_funcs = []
        self.__cleanup_paths = []

        super(MockerTestCase, self).__init__(methodName)

    def __cleanup(self):
        for path in self.__cleanup_paths:
            if os.path.isfile(path):
                os.unlink(path)
            elif os.path.isdir(path):
                shutil.rmtree(path)
        self.mocker.restore()
        for func, args, kwargs in self.__cleanup_funcs:
            func(*args, **kwargs)

    def addCleanup(self, func, *args, **kwargs):
        self.__cleanup_funcs.append((func, args, kwargs))

    def makeFile(self, content=None, suffix="", prefix="tmp", basename=None,
                 dirname=None, path=None):
        """Create a temporary file and return the path to it.

        @param content: Initial content for the file.
        @param suffix: Suffix to be given to the file's basename.
        @param prefix: Prefix to be given to the file's basename.
        @param basename: Full basename for the file.
        @param dirname: Put file inside this directory.

        The file is removed after the test runs.
        """
        if path is not None:
            self.__cleanup_paths.append(path)
        elif basename is not None:
            if dirname is None:
                dirname = tempfile.mkdtemp()
                self.__cleanup_paths.append(dirname)
            path = os.path.join(dirname, basename)
        else:
            fd, path = tempfile.mkstemp(suffix, prefix, dirname)
            self.__cleanup_paths.append(path)
            os.close(fd)
            if content is None:
                os.unlink(path)
        if content is not None:
            file = open(path, "w")
            file.write(content)
            file.close()
        return path

    def makeDir(self, suffix="", prefix="tmp", dirname=None, path=None):
        """Create a temporary directory and return the path to it.

        @param suffix: Suffix to be given to the file's basename.
        @param prefix: Prefix to be given to the file's basename.
        @param dirname: Put directory inside this parent directory.

        The directory is removed after the test runs.
        """
        if path is not None:
            os.makedirs(path)
        else:
            path = tempfile.mkdtemp(suffix, prefix, dirname)
        self.__cleanup_paths.append(path)
        return path

    def failUnlessIs(self, first, second, msg=None):
        """Assert that C{first} is the same object as C{second}."""
        if first is not second:
            raise self.failureException(msg or "%r is not %r" % (first, second))

    def failIfIs(self, first, second, msg=None):
        """Assert that C{first} is not the same object as C{second}."""
        if first is second:
            raise self.failureException(msg or "%r is %r" % (first, second))

    def failUnlessIn(self, first, second, msg=None):
        """Assert that C{first} is contained in C{second}."""
        if first not in second:
            raise self.failureException(msg or "%r not in %r" % (first, second))

    def failUnlessStartsWith(self, first, second, msg=None):
        """Assert that C{first} starts with C{second}."""
        if first[:len(second)] != second:
            raise self.failureException(msg or "%r doesn't start with %r" %
                                               (first, second))

    def failIfStartsWith(self, first, second, msg=None):
        """Assert that C{first} doesn't start with C{second}."""
        if first[:len(second)] == second:
            raise self.failureException(msg or "%r starts with %r" %
                                               (first, second))

    def failUnlessEndsWith(self, first, second, msg=None):
        """Assert that C{first} starts with C{second}."""
        if first[len(first)-len(second):] != second:
            raise self.failureException(msg or "%r doesn't end with %r" %
                                               (first, second))

    def failIfEndsWith(self, first, second, msg=None):
        """Assert that C{first} doesn't start with C{second}."""
        if first[len(first)-len(second):] == second:
            raise self.failureException(msg or "%r ends with %r" %
                                               (first, second))

    def failIfIn(self, first, second, msg=None):
        """Assert that C{first} is not contained in C{second}."""
        if first in second:
            raise self.failureException(msg or "%r in %r" % (first, second))

    def failUnlessApproximates(self, first, second, tolerance, msg=None):
        """Assert that C{first} is near C{second} by at most C{tolerance}."""
        if abs(first - second) > tolerance:
            raise self.failureException(msg or "abs(%r - %r) > %r" %
                                        (first, second, tolerance))

    def failIfApproximates(self, first, second, tolerance, msg=None):
        """Assert that C{first} is far from C{second} by at least C{tolerance}.
        """
        if abs(first - second) <= tolerance:
            raise self.failureException(msg or "abs(%r - %r) <= %r" %
                                        (first, second, tolerance))

    def failUnlessMethodsMatch(self, first, second):
        """Assert that public methods in C{first} are present in C{second}.

        This method asserts that all public methods found in C{first} are also
        present in C{second} and accept the same arguments.  C{first} may
        have its own private methods, though, and may not have all methods
        found in C{second}.  Note that if a private method in C{first} matches
        the name of one in C{second}, their specification is still compared.

        This is useful to verify if a fake or stub class have the same API as
        the real class being simulated.
        """
        first_methods = dict(inspect.getmembers(first, inspect.ismethod))
        second_methods = dict(inspect.getmembers(second, inspect.ismethod))
        for name, first_method in first_methods.items():
            first_argspec = inspect.getargspec(first_method)
            first_formatted = inspect.formatargspec(*first_argspec)

            second_method = second_methods.get(name)
            if second_method is None:
                if name[:1] == "_":
                    continue # First may have its own private methods.
                raise self.failureException("%s.%s%s not present in %s" %
                    (first.__name__, name, first_formatted, second.__name__))

            second_argspec = inspect.getargspec(second_method)
            if first_argspec != second_argspec:
                second_formatted = inspect.formatargspec(*second_argspec)
                raise self.failureException("%s.%s%s != %s.%s%s" %
                    (first.__name__, name, first_formatted,
                     second.__name__, name, second_formatted))


    assertIs = failUnlessIs
    assertIsNot = failIfIs
    assertIn = failUnlessIn
    assertNotIn = failIfIn
    assertStartsWith = failUnlessStartsWith
    assertNotStartsWith = failIfStartsWith
    assertEndsWith = failUnlessEndsWith
    assertNotEndsWith = failIfEndsWith
    assertApproximates = failUnlessApproximates
    assertNotApproximates = failIfApproximates
    assertMethodsMatch = failUnlessMethodsMatch

    # The following are missing in Python < 2.4.
    assertTrue = unittest.TestCase.failUnless
    assertFalse = unittest.TestCase.failIf

    # The following is provided for compatibility with Twisted's trial.
    assertIdentical = assertIs
    assertNotIdentical = assertIsNot
    failUnlessIdentical = failUnlessIs
    failIfIdentical = failIfIs


# --------------------------------------------------------------------
# Mocker.

class classinstancemethod(object):

    def __init__(self, method):
        self.method = method

    def __get__(self, obj, cls=None):
        def bound_method(*args, **kwargs):
            return self.method(cls, obj, *args, **kwargs)
        return bound_method


class MockerBase(object):
    """Controller of mock objects.

    A mocker instance is used to command recording and replay of
    expectations on any number of mock objects.

    Expectations should be expressed for the mock object while in
    record mode (the initial one) by using the mock object itself,
    and using the mocker (and/or C{expect()} as a helper) to define
    additional behavior for each event.  For instance::

        mock = mocker.mock()
        mock.hello()
        mocker.result("Hi!")
        mocker.replay()
        assert mock.hello() == "Hi!"
        mock.restore()
        mock.verify()

    In this short excerpt a mock object is being created, then an
    expectation of a call to the C{hello()} method was recorded, and
    when called the method should return the value C{10}.  Then, the
    mocker is put in replay mode, and the expectation is satisfied by
    calling the C{hello()} method, which indeed returns 10.  Finally,
    a call to the L{restore()} method is performed to undo any needed
    changes made in the environment, and the L{verify()} method is
    called to ensure that all defined expectations were met.

    The same logic can be expressed more elegantly using the
    C{with mocker:} statement, as follows::

        mock = mocker.mock()
        mock.hello()
        mocker.result("Hi!")
        with mocker:
            assert mock.hello() == "Hi!"

    Also, the MockerTestCase class, which integrates the mocker on
    a unittest.TestCase subclass, may be used to reduce the overhead
    of controlling the mocker.  A test could be written as follows::

        class SampleTest(MockerTestCase):

            def test_hello(self):
                mock = self.mocker.mock()
                mock.hello()
                self.mocker.result("Hi!")
                self.mocker.replay()
                self.assertEquals(mock.hello(), "Hi!")
    """

    _recorders = []

    # For convenience only.
    on = expect

    class __metaclass__(type):
        def __init__(self, name, bases, dict):
            # Make independent lists on each subclass, inheriting from parent.
            self._recorders = list(getattr(self, "_recorders", ()))

    def __init__(self):
        self._recorders = self._recorders[:]
        self._events = []
        self._recording = True
        self._ordering = False
        self._last_orderer = None

    def is_recording(self):
        """Return True if in recording mode, False if in replay mode.

        Recording is the initial state.
        """
        return self._recording

    def replay(self):
        """Change to replay mode, where recorded events are reproduced.

        If already in replay mode, the mocker will be restored, with all
        expectations reset, and then put again in replay mode.

        An alternative and more comfortable way to replay changes is
        using the 'with' statement, as follows::

            mocker = Mocker()
            <record events>
            with mocker:
                <reproduce events>

        The 'with' statement will automatically put mocker in replay
        mode, and will also verify if all events were correctly reproduced
        at the end (using L{verify()}), and also restore any changes done
        in the environment (with L{restore()}).

        Also check the MockerTestCase class, which integrates the
        unittest.TestCase class with mocker.
        """
        if not self._recording:
            for event in self._events:
                event.restore()
        else:
            self._recording = False
        for event in self._events:
            event.replay()

    def restore(self):
        """Restore changes in the environment, and return to recording mode.

        This should always be called after the test is complete (succeeding
        or not).  There are ways to call this method automatically on
        completion (e.g. using a C{with mocker:} statement, or using the
        L{MockerTestCase} class.
        """
        if not self._recording:
            self._recording = True
            for event in self._events:
                event.restore()

    def reset(self):
        """Reset the mocker state.

        This will restore environment changes, if currently in replay
        mode, and then remove all events previously recorded.
        """
        if not self._recording:
            self.restore()
        self.unorder()
        del self._events[:]

    def get_events(self):
        """Return all recorded events."""
        return self._events[:]

    def add_event(self, event):
        """Add an event.

        This method is used internally by the implementation, and
        shouldn't be needed on normal mocker usage.
        """
        self._events.append(event)
        if self._ordering:
            orderer = event.add_task(Orderer(event.path))
            if self._last_orderer:
                orderer.add_dependency(self._last_orderer)
            self._last_orderer = orderer
        return event

    def verify(self):
        """Check if all expectations were met, and raise AssertionError if not.

        The exception message will include a nice description of which
        expectations were not met, and why.
        """
        errors = []
        for event in self._events:
            try:
                event.verify()
            except AssertionError, e:
                error = str(e)
                if not error:
                    raise RuntimeError("Empty error message from %r"
                                       % event)
                errors.append(error)
        if errors:
            message = [ERROR_PREFIX + "Unmet expectations:", ""]
            for error in errors:
                lines = error.splitlines()
                message.append("=> " + lines.pop(0))
                message.extend([" " + line for line in lines])
                message.append("")
            raise AssertionError(os.linesep.join(message))

    def mock(self, spec_and_type=None, spec=None, type=None,
             name=None, count=True):
        """Return a new mock object.

        @param spec_and_type: Handy positional argument which sets both
                     spec and type.
        @param spec: Method calls will be checked for correctness against
                     the given class.
        @param type: If set, the Mock's __class__ attribute will return
                     the given type.  This will make C{isinstance()} calls
                     on the object work.
        @param name: Name for the mock object, used in the representation of
                     expressions.  The name is rarely needed, as it's usually
                     guessed correctly from the variable name used.
        @param count: If set to false, expressions may be executed any number
                     of times, unless an expectation is explicitly set using
                     the L{count()} method.  By default, expressions are
                     expected once.
        """
        if spec_and_type is not None:
            spec = type = spec_and_type
        return Mock(self, spec=spec, type=type, name=name, count=count)

    def proxy(self, object, spec=True, type=True, name=None, count=True,
              passthrough=True):
        """Return a new mock object which proxies to the given object.
 
        Proxies are useful when only part of the behavior of an object
        is to be mocked.  Unknown expressions may be passed through to
        the real implementation implicitly (if the C{passthrough} argument
        is True), or explicitly (using the L{passthrough()} method
        on the event).

        @param object: Real object to be proxied, and replaced by the mock
                       on replay mode.  It may also be an "import path",
                       such as C{"time.time"}, in which case the object
                       will be the C{time} function from the C{time} module.
        @param spec: Method calls will be checked for correctness against
                     the given object, which may be a class or an instance
                     where attributes will be looked up.  Defaults to the
                     the C{object} parameter.  May be set to None explicitly,
                     in which case spec checking is disabled.  Checks may
                     also be disabled explicitly on a per-event basis with
                     the L{nospec()} method.
        @param type: If set, the Mock's __class__ attribute will return
                     the given type.  This will make C{isinstance()} calls
                     on the object work.  Defaults to the type of the
                     C{object} parameter.  May be set to None explicitly.
        @param name: Name for the mock object, used in the representation of
                     expressions.  The name is rarely needed, as it's usually
                     guessed correctly from the variable name used.
        @param count: If set to false, expressions may be executed any number
                     of times, unless an expectation is explicitly set using
                     the L{count()} method.  By default, expressions are
                     expected once.
        @param passthrough: If set to False, passthrough of actions on the
                            proxy to the real object will only happen when
                            explicitly requested via the L{passthrough()}
                            method.
        """
        if isinstance(object, basestring):
            if name is None:
                name = object
            import_stack = object.split(".")
            attr_stack = []
            while import_stack:
                module_path = ".".join(import_stack)
                try:
                    object = __import__(module_path, {}, {}, [""])
                except ImportError:
                    attr_stack.insert(0, import_stack.pop())
                    if not import_stack:
                        raise
                    continue
                else:
                    for attr in attr_stack:
                        object = getattr(object, attr)
                    break
        if spec is True:
            spec = object
        if type is True:
            type = __builtin__.type(object)
        return Mock(self, spec=spec, type=type, object=object,
                    name=name, count=count, passthrough=passthrough)

    def replace(self, object, spec=True, type=True, name=None, count=True,
                passthrough=True):
        """Create a proxy, and replace the original object with the mock.

        On replay, the original object will be replaced by the returned
        proxy in all dictionaries found in the running interpreter via
        the garbage collecting system.  This should cover module
        namespaces, class namespaces, instance namespaces, and so on.

        @param object: Real object to be proxied, and replaced by the mock
                       on replay mode.  It may also be an "import path",
                       such as C{"time.time"}, in which case the object
                       will be the C{time} function from the C{time} module.
        @param spec: Method calls will be checked for correctness against
                     the given object, which may be a class or an instance
                     where attributes will be looked up.  Defaults to the
                     the C{object} parameter.  May be set to None explicitly,
                     in which case spec checking is disabled.  Checks may
                     also be disabled explicitly on a per-event basis with
                     the L{nospec()} method.
        @param type: If set, the Mock's __class__ attribute will return
                     the given type.  This will make C{isinstance()} calls
                     on the object work.  Defaults to the type of the
                     C{object} parameter.  May be set to None explicitly.
        @param name: Name for the mock object, used in the representation of
                     expressions.  The name is rarely needed, as it's usually
                     guessed correctly from the variable name used.
        @param passthrough: If set to False, passthrough of actions on the
                            proxy to the real object will only happen when
                            explicitly requested via the L{passthrough()}
                            method.
        """
        mock = self.proxy(object, spec, type, name, count, passthrough)
        event = self._get_replay_restore_event()
        event.add_task(ProxyReplacer(mock))
        return mock

    def patch(self, object, spec=True):
        """Patch an existing object to reproduce recorded events.

        @param object: Class or instance to be patched.
        @param spec: Method calls will be checked for correctness against
                     the given object, which may be a class or an instance
                     where attributes will be looked up.  Defaults to the
                     the C{object} parameter.  May be set to None explicitly,
                     in which case spec checking is disabled.  Checks may
                     also be disabled explicitly on a per-event basis with
                     the L{nospec()} method.

        The result of this method is still a mock object, which can be
        used like any other mock object to record events.  The difference
        is that when the mocker is put on replay mode, the *real* object
        will be modified to behave according to recorded expectations.

        Patching works in individual instances, and also in classes.
        When an instance is patched, recorded events will only be
        considered on this specific instance, and other instances should
        behave normally.  When a class is patched, the reproduction of
        events will be considered on any instance of this class once
        created (collectively).

        Observe that, unlike with proxies which catch only events done
        through the mock object, *all* accesses to recorded expectations
        will be considered;  even these coming from the object itself
        (e.g. C{self.hello()} is considered if this method was patched).
        While this is a very powerful feature, and many times the reason
        to use patches in the first place, it's important to keep this
        behavior in mind.

        Patching of the original object only takes place when the mocker
        is put on replay mode, and the patched object will be restored
        to its original state once the L{restore()} method is called
        (explicitly, or implicitly with alternative conventions, such as
        a C{with mocker:} block, or a MockerTestCase class).
        """
        if spec is True:
            spec = object
        patcher = Patcher()
        event = self._get_replay_restore_event()
        event.add_task(patcher)
        mock = Mock(self, object=object, patcher=patcher,
                    passthrough=True, spec=spec)
        object.__mocker_mock__ = mock
        return mock

    def act(self, path):
        """This is called by mock objects whenever something happens to them.

        This method is part of the implementation between the mocker
        and mock objects.
        """
        if self._recording:
            event = self.add_event(Event(path))
            for recorder in self._recorders:
                recorder(self, event)
            return Mock(self, path)
        else:
            # First run events that may run, then run unsatisfied events, then
            # ones not previously run. We put the index in the ordering tuple
            # instead of the actual event because we want a stable sort
            # (ordering between 2 events is undefined).
            events = self._events
            order = [(events[i].satisfied()*2 + events[i].has_run(), i)
                     for i in range(len(events))]
            order.sort()
            postponed = None
            for weight, i in order:
                event = events[i]
                if event.matches(path):
                    if event.may_run(path):
                        return event.run(path)
                    elif postponed is None:
                        postponed = event
            if postponed is not None:
                return postponed.run(path)
            raise MatchError(ERROR_PREFIX + "Unexpected expression: %s" % path)

    def get_recorders(cls, self):
        """Return recorders associated with this mocker class or instance.

        This method may be called on mocker instances and also on mocker
        classes.  See the L{add_recorder()} method for more information.
        """
        return (self or cls)._recorders[:]
    get_recorders = classinstancemethod(get_recorders)

    def add_recorder(cls, self, recorder):
        """Add a recorder to this mocker class or instance.

        @param recorder: Callable accepting C{(mocker, event)} as parameters.

        This is part of the implementation of mocker.

        All registered recorders are called for translating events that
        happen during recording into expectations to be met once the state
        is switched to replay mode.

        This method may be called on mocker instances and also on mocker
        classes.  When called on a class, the recorder will be used by
        all instances, and also inherited on subclassing.  When called on
        instances, the recorder is added only to the given instance.
        """
        (self or cls)._recorders.append(recorder)
        return recorder
    add_recorder = classinstancemethod(add_recorder)

    def remove_recorder(cls, self, recorder):
        """Remove the given recorder from this mocker class or instance.

        This method may be called on mocker classes and also on mocker
        instances.  See the L{add_recorder()} method for more information.
        """
        (self or cls)._recorders.remove(recorder)
    remove_recorder = classinstancemethod(remove_recorder)

    def result(self, value):
        """Make the last recorded event return the given value on replay.
        
        @param value: Object to be returned when the event is replayed.
        """
        self.call(lambda *args, **kwargs: value)

    def generate(self, sequence):
        """Last recorded event will return a generator with the given sequence.

        @param sequence: Sequence of values to be generated.
        """
        def generate(*args, **kwargs):
            for value in sequence:
                yield value
        self.call(generate)

    def throw(self, exception):
        """Make the last recorded event raise the given exception on replay.

        @param exception: Class or instance of exception to be raised.
        """
        def raise_exception(*args, **kwargs):
            raise exception
        self.call(raise_exception)

    def call(self, func):
        """Make the last recorded event cause the given function to be called.

        @param func: Function to be called.

        The result of the function will be used as the event result.
        """
        self._events[-1].add_task(FunctionRunner(func))

    def count(self, min, max=False):
        """Last recorded event must be replayed between min and max times.

        @param min: Minimum number of times that the event must happen.
        @param max: Maximum number of times that the event must happen.  If
                    not given, it defaults to the same value of the C{min}
                    parameter.  If set to None, there is no upper limit, and
                    the expectation is met as long as it happens at least
                    C{min} times.
        """
        event = self._events[-1]
        for task in event.get_tasks():
            if isinstance(task, RunCounter):
                event.remove_task(task)
        event.add_task(RunCounter(min, max))

    def is_ordering(self):
        """Return true if all events are being ordered.

        See the L{order()} method.
        """
        return self._ordering

    def unorder(self):
        """Disable the ordered mode.
        
        See the L{order()} method for more information.
        """
        self._ordering = False
        self._last_orderer = None

    def order(self, *path_holders):
        """Create an expectation of order between two or more events.

        @param path_holders: Objects returned as the result of recorded events.

        By default, mocker won't force events to happen precisely in
        the order they were recorded.  Calling this method will change
        this behavior so that events will only match if reproduced in
        the correct order.

        There are two ways in which this method may be used.  Which one
        is used in a given occasion depends only on convenience.

        If no arguments are passed, the mocker will be put in a mode where
        all the recorded events following the method call will only be met
        if they happen in order.  When that's used, the mocker may be put
        back in unordered mode by calling the L{unorder()} method, or by
        using a 'with' block, like so::

            with mocker.ordered():
                <record events>

        In this case, only expressions in <record events> will be ordered,
        and the mocker will be back in unordered mode after the 'with' block.

        The second way to use it is by specifying precisely which events
        should be ordered.  As an example::

            mock = mocker.mock()
            expr1 = mock.hello()
            expr2 = mock.world
            expr3 = mock.x.y.z
            mocker.order(expr1, expr2, expr3)

        This method of ordering only works when the expression returns
        another object.

        Also check the L{after()} and L{before()} methods, which are
        alternative ways to perform this.
        """
        if not path_holders:
            self._ordering = True
            return OrderedContext(self)

        last_orderer = None
        for path_holder in path_holders:
            if type(path_holder) is Path:
                path = path_holder
            else:
                path = path_holder.__mocker_path__
            for event in self._events:
                if event.path is path:
                    for task in event.get_tasks():
                        if isinstance(task, Orderer):
                            orderer = task
                            break
                    else:
                        orderer = Orderer(path)
                        event.add_task(orderer)
                    if last_orderer:
                        orderer.add_dependency(last_orderer)
                    last_orderer = orderer
                    break

    def after(self, *path_holders):
        """Last recorded event must happen after events referred to.

        @param path_holders: Objects returned as the result of recorded events
                             which should happen before the last recorded event

        As an example, the idiom::

            expect(mock.x).after(mock.y, mock.z)

        is an alternative way to say::

            expr_x = mock.x
            expr_y = mock.y
            expr_z = mock.z
            mocker.order(expr_y, expr_x)
            mocker.order(expr_z, expr_x)

        See L{order()} for more information.
        """
        last_path = self._events[-1].path
        for path_holder in path_holders:
            self.order(path_holder, last_path)

    def before(self, *path_holders):
        """Last recorded event must happen before events referred to.

        @param path_holders: Objects returned as the result of recorded events
                             which should happen after the last recorded event

        As an example, the idiom::

            expect(mock.x).before(mock.y, mock.z)

        is an alternative way to say::

            expr_x = mock.x
            expr_y = mock.y
            expr_z = mock.z
            mocker.order(expr_x, expr_y)
            mocker.order(expr_x, expr_z)

        See L{order()} for more information.
        """
        last_path = self._events[-1].path
        for path_holder in path_holders:
            self.order(last_path, path_holder)

    def nospec(self):
        """Don't check method specification of real object on last event.

        By default, when using a mock created as the result of a call to
        L{proxy()}, L{replace()}, and C{patch()}, or when passing the spec
        attribute to the L{mock()} method, method calls on the given object
        are checked for correctness against the specification of the real
        object (or the explicitly provided spec).

        This method will disable that check specifically for the last
        recorded event.
        """
        event = self._events[-1]
        for task in event.get_tasks():
            if isinstance(task, SpecChecker):
                event.remove_task(task)

    def passthrough(self, result_callback=None):
        """Make the last recorded event run on the real object once seen.

        @param result_callback: If given, this function will be called with
            the result of the *real* method call as the only argument.

        This can only be used on proxies, as returned by the L{proxy()}
        and L{replace()} methods, or on mocks representing patched objects,
        as returned by the L{patch()} method.
        """
        event = self._events[-1]
        if event.path.root_object is None:
            raise TypeError("Mock object isn't a proxy")
        event.add_task(PathExecuter(result_callback))

    def __enter__(self):
        """Enter in a 'with' context.  This will run replay()."""
        self.replay()
        return self

    def __exit__(self, type, value, traceback):
        """Exit from a 'with' context.

        This will run restore() at all times, but will only run verify()
        if the 'with' block itself hasn't raised an exception.  Exceptions
        in that block are never swallowed.
        """
        self.restore()
        if type is None:
            self.verify()
        return False

    def _get_replay_restore_event(self):
        """Return unique L{ReplayRestoreEvent}, creating if needed.

        Some tasks only want to replay/restore.  When that's the case,
        they shouldn't act on other events during replay.  Also, they
        can all be put in a single event when that's the case.  Thus,
        we add a single L{ReplayRestoreEvent} as the first element of
        the list.
        """
        if not self._events or type(self._events[0]) != ReplayRestoreEvent:
            self._events.insert(0, ReplayRestoreEvent())
        return self._events[0]


class OrderedContext(object):

    def __init__(self, mocker):
        self._mocker = mocker

    def __enter__(self):
        return None

    def __exit__(self, type, value, traceback):
        self._mocker.unorder()


class Mocker(MockerBase):
    __doc__ = MockerBase.__doc__

# Decorator to add recorders on the standard Mocker class.
recorder = Mocker.add_recorder


# --------------------------------------------------------------------
# Mock object.

class Mock(object):

    def __init__(self, mocker, path=None, name=None, spec=None, type=None,
                 object=None, passthrough=False, patcher=None, count=True):
        self.__mocker__ = mocker
        self.__mocker_path__ = path or Path(self, object)
        self.__mocker_name__ = name
        self.__mocker_spec__ = spec
        self.__mocker_object__ = object
        self.__mocker_passthrough__ = passthrough
        self.__mocker_patcher__ = patcher
        self.__mocker_replace__ = False
        self.__mocker_type__ = type
        self.__mocker_count__ = count

    def __mocker_act__(self, kind, args=(), kwargs={}, object=None):
        if self.__mocker_name__ is None:
            self.__mocker_name__ = find_object_name(self, 2)
        action = Action(kind, args, kwargs, self.__mocker_path__)
        path = self.__mocker_path__ + action
        if object is not None:
            path.root_object = object
        try:
            return self.__mocker__.act(path)
        except MatchError, exception:
            root_mock = path.root_mock
            if (path.root_object is not None and
                root_mock.__mocker_passthrough__):
                return path.execute(path.root_object)
            # Reinstantiate to show raise statement on traceback, and
            # also to make the traceback shown shorter.
            raise MatchError(str(exception))
        except AssertionError, e:
            lines = str(e).splitlines()
            message = [ERROR_PREFIX + "Unmet expectation:", ""]
            message.append("=> " + lines.pop(0))
            message.extend([" " + line for line in lines])
            message.append("")
            raise AssertionError(os.linesep.join(message))

    def __getattribute__(self, name):
        if name.startswith("__mocker_"):
            return super(Mock, self).__getattribute__(name)
        if name == "__class__":
            if self.__mocker__.is_recording() or self.__mocker_type__ is None:
                return type(self)
            return self.__mocker_type__
        return self.__mocker_act__("getattr", (name,))

    def __setattr__(self, name, value):
        if name.startswith("__mocker_"):
            return super(Mock, self).__setattr__(name, value)
        return self.__mocker_act__("setattr", (name, value))

    def __delattr__(self, name):
        return self.__mocker_act__("delattr", (name,))

    def __call__(self, *args, **kwargs):
        return self.__mocker_act__("call", args, kwargs)

    def __contains__(self, value):
        return self.__mocker_act__("contains", (value,))

    def __getitem__(self, key):
        return self.__mocker_act__("getitem", (key,))

    def __setitem__(self, key, value):
        return self.__mocker_act__("setitem", (key, value))

    def __delitem__(self, key):
        return self.__mocker_act__("delitem", (key,))

    def __len__(self):
        # MatchError is turned on an AttributeError so that list() and
        # friends act properly when trying to get length hints on
        # something that doesn't offer them.
        try:
            result = self.__mocker_act__("len")
        except MatchError, e:
            raise AttributeError(str(e))
        if type(result) is Mock:
            return 0
        return result

    def __nonzero__(self):
        try:
            return self.__mocker_act__("nonzero")
        except MatchError, e:
            return True

    def __iter__(self):
        # XXX On py3k, when next() becomes __next__(), we'll be able
        #     to return the mock itself because it will be considered
        #     an iterator (we'll be mocking __next__ as well, which we
        #     can't now).
        result = self.__mocker_act__("iter")
        if type(result) is Mock:
            return iter([])
        return result

    # When adding a new action kind here, also add support for it on
    # Action.execute() and Path.__str__().


def find_object_name(obj, depth=0):
    """Try to detect how the object is named on a previous scope."""
    try:
        frame = sys._getframe(depth+1)
    except:
        return None
    for name, frame_obj in frame.f_locals.iteritems():
        if frame_obj is obj:
            return name
    self = frame.f_locals.get("self")
    if self is not None:
        try:
            items = list(self.__dict__.iteritems())
        except:
            pass
        else:
            for name, self_obj in items:
                if self_obj is obj:
                    return name
    return None


# --------------------------------------------------------------------
# Action and path.

class Action(object):

    def __init__(self, kind, args, kwargs, path=None):
        self.kind = kind
        self.args = args
        self.kwargs = kwargs
        self.path = path
        self._execute_cache = {}

    def __repr__(self):
        if self.path is None:
            return "Action(%r, %r, %r)" % (self.kind, self.args, self.kwargs)
        return "Action(%r, %r, %r, %r)" % \
               (self.kind, self.args, self.kwargs, self.path)

    def __eq__(self, other):
        return (self.kind == other.kind and
                self.args == other.args and
                self.kwargs == other.kwargs)

    def __ne__(self, other):
        return not self.__eq__(other)

    def matches(self, other):
        return (self.kind == other.kind and
                match_params(self.args, self.kwargs, other.args, other.kwargs))

    def execute(self, object):
        # This caching scheme may fail if the object gets deallocated before
        # the action, as the id might get reused.  It's somewhat easy to fix
        # that with a weakref callback.  For our uses, though, the object
        # should never get deallocated before the action itself, so we'll
        # just keep it simple.
        if id(object) in self._execute_cache:
            return self._execute_cache[id(object)]
        execute = getattr(object, "__mocker_execute__", None)
        if execute is not None:
            result = execute(self, object)
        else:
            kind = self.kind
            if kind == "getattr":
                result = getattr(object, self.args[0])
            elif kind == "setattr":
                result = setattr(object, self.args[0], self.args[1])
            elif kind == "delattr":
                result = delattr(object, self.args[0])
            elif kind == "call":
                result = object(*self.args, **self.kwargs)
            elif kind == "contains":
                result = self.args[0] in object
            elif kind == "getitem":
                result = object[self.args[0]]
            elif kind == "setitem":
                result = object[self.args[0]] = self.args[1]
            elif kind == "delitem":
                del object[self.args[0]]
                result = None
            elif kind == "len":
                result = len(object)
            elif kind == "nonzero":
                result = bool(object)
            elif kind == "iter":
                result = iter(object)
            else:
                raise RuntimeError("Don't know how to execute %r kind." % kind)
        self._execute_cache[id(object)] = result
        return result


class Path(object):

    def __init__(self, root_mock, root_object=None, actions=()):
        self.root_mock = root_mock
        self.root_object = root_object
        self.actions = tuple(actions)
        self.__mocker_replace__ = False

    def parent_path(self):
        if not self.actions:
            return None
        return self.actions[-1].path
    parent_path = property(parent_path)
 
    def __add__(self, action):
        """Return a new path which includes the given action at the end."""
        return self.__class__(self.root_mock, self.root_object,
                              self.actions + (action,))

    def __eq__(self, other):
        """Verify if the two paths are equal.
        
        Two paths are equal if they refer to the same mock object, and
        have the actions with equal kind, args and kwargs.
        """
        if (self.root_mock is not other.root_mock or
            self.root_object is not other.root_object or
            len(self.actions) != len(other.actions)):
            return False
        for action, other_action in zip(self.actions, other.actions):
            if action != other_action:
                return False
        return True

    def matches(self, other):
        """Verify if the two paths are equivalent.
        
        Two paths are equal if they refer to the same mock object, and
        have the same actions performed on them.
        """
        if (self.root_mock is not other.root_mock or
            len(self.actions) != len(other.actions)):
            return False
        for action, other_action in zip(self.actions, other.actions):
            if not action.matches(other_action):
                return False
        return True

    def execute(self, object):
        """Execute all actions sequentially on object, and return result.
        """
        for action in self.actions:
            object = action.execute(object)
        return object

    def __str__(self):
        """Transform the path into a nice string such as obj.x.y('z')."""
        result = self.root_mock.__mocker_name__ or "<mock>"
        for action in self.actions:
            if action.kind == "getattr":
                result = "%s.%s" % (result, action.args[0])
            elif action.kind == "setattr":
                result = "%s.%s = %r" % (result, action.args[0], action.args[1])
            elif action.kind == "delattr":
                result = "del %s.%s" % (result, action.args[0])
            elif action.kind == "call":
                args = [repr(x) for x in action.args]
                items = list(action.kwargs.iteritems())
                items.sort()
                for pair in items:
                    args.append("%s=%r" % pair)
                result = "%s(%s)" % (result, ", ".join(args))
            elif action.kind == "contains":
                result = "%r in %s" % (action.args[0], result)
            elif action.kind == "getitem":
                result = "%s[%r]" % (result, action.args[0])
            elif action.kind == "setitem":
                result = "%s[%r] = %r" % (result, action.args[0],
                                          action.args[1])
            elif action.kind == "delitem":
                result = "del %s[%r]" % (result, action.args[0])
            elif action.kind == "len":
                result = "len(%s)" % result
            elif action.kind == "nonzero":
                result = "bool(%s)" % result
            elif action.kind == "iter":
                result = "iter(%s)" % result
            else:
                raise RuntimeError("Don't know how to format kind %r" %
                                   action.kind)
        return result


class SpecialArgument(object):
    """Base for special arguments for matching parameters."""

    def __init__(self, object=None):
        self.object = object

    def __repr__(self):
        if self.object is None:
            return self.__class__.__name__
        else:
            return "%s(%r)" % (self.__class__.__name__, self.object)

    def matches(self, other):
        return True

    def __eq__(self, other):
        return type(other) == type(self) and self.object == other.object


class ANY(SpecialArgument):
    """Matches any single argument."""

ANY = ANY()


class ARGS(SpecialArgument):
    """Matches zero or more positional arguments."""

ARGS = ARGS()


class KWARGS(SpecialArgument):
    """Matches zero or more keyword arguments."""

KWARGS = KWARGS()


class IS(SpecialArgument):

    def matches(self, other):
        return self.object is other

    def __eq__(self, other):
        return type(other) == type(self) and self.object is other.object


class CONTAINS(SpecialArgument):

    def matches(self, other):
        try:
            other.__contains__
        except AttributeError:
            try:
                iter(other)
            except TypeError:
                # If an object can't be iterated, and has no __contains__
                # hook, it'd blow up on the test below.  We test this in
                # advance to prevent catching more errors than we really
                # want.
                return False
        return self.object in other


class IN(SpecialArgument):

    def matches(self, other):
        return other in self.object


class MATCH(SpecialArgument):

    def matches(self, other):
        return bool(self.object(other))

    def __eq__(self, other):
        return type(other) == type(self) and self.object is other.object


def match_params(args1, kwargs1, args2, kwargs2):
    """Match the two sets of parameters, considering special parameters."""

    has_args = ARGS in args1
    has_kwargs = KWARGS in args1

    if has_kwargs:
        args1 = [arg1 for arg1 in args1 if arg1 is not KWARGS]
    elif len(kwargs1) != len(kwargs2):
        return False

    if not has_args and len(args1) != len(args2):
        return False

    # Either we have the same number of kwargs, or unknown keywords are
    # accepted (KWARGS was used), so check just the ones in kwargs1.
    for key, arg1 in kwargs1.iteritems():
        if key not in kwargs2:
            return False
        arg2 = kwargs2[key]
        if isinstance(arg1, SpecialArgument):
            if not arg1.matches(arg2):
                return False
        elif arg1 != arg2:
            return False

    # Keywords match.  Now either we have the same number of
    # arguments, or ARGS was used.  If ARGS wasn't used, arguments
    # must match one-on-one necessarily.
    if not has_args:
        for arg1, arg2 in zip(args1, args2):
            if isinstance(arg1, SpecialArgument):
                if not arg1.matches(arg2):
                    return False
            elif arg1 != arg2:
                return False
        return True

    # Easy choice. Keywords are matching, and anything on args is accepted.
    if (ARGS,) == args1:
        return True

    # We have something different there. If we don't have positional
    # arguments on the original call, it can't match.
    if not args2:
        # Unless we have just several ARGS (which is bizarre, but..).
        for arg1 in args1:
            if arg1 is not ARGS:
                return False
        return True

    # Ok, all bets are lost.  We have to actually do the more expensive
    # matching.  This is an algorithm based on the idea of the Levenshtein
    # Distance between two strings, but heavily hacked for this purpose.
    args2l = len(args2)
    if args1[0] is ARGS:
        args1 = args1[1:]
        array = [0]*args2l
    else:
        array = [1]*args2l
    for i in range(len(args1)):
        last = array[0]
        if args1[i] is ARGS:
            for j in range(1, args2l):
                last, array[j] = array[j], min(array[j-1], array[j], last)
        else:
            array[0] = i or int(args1[i] != args2[0])
            for j in range(1, args2l):
                last, array[j] = array[j], last or int(args1[i] != args2[j])
        if 0 not in array:
            return False
    if array[-1] != 0:
        return False
    return True


# --------------------------------------------------------------------
# Event and task base.

class Event(object):
    """Aggregation of tasks that keep track of a recorded action.

    An event represents something that may or may not happen while the
    mocked environment is running, such as an attribute access, or a
    method call.  The event is composed of several tasks that are
    orchestrated together to create a composed meaning for the event,
    including for which actions it should be run, what happens when it
    runs, and what's the expectations about the actions run.
    """

    def __init__(self, path=None):
        self.path = path
        self._tasks = []
        self._has_run = False

    def add_task(self, task):
        """Add a new task to this taks."""
        self._tasks.append(task)
        return task

    def remove_task(self, task):
        self._tasks.remove(task)

    def get_tasks(self):
        return self._tasks[:]

    def matches(self, path):
        """Return true if *all* tasks match the given path."""
        for task in self._tasks:
            if not task.matches(path):
                return False
        return bool(self._tasks)

    def has_run(self):
        return self._has_run

    def may_run(self, path):
        """Verify if any task would certainly raise an error if run.

        This will call the C{may_run()} method on each task and return
        false if any of them returns false.
        """
        for task in self._tasks:
            if not task.may_run(path):
                return False
        return True

    def run(self, path):
        """Run all tasks with the given action.

        @param path: The path of the expression run.

        Running an event means running all of its tasks individually and in
        order.  An event should only ever be run if all of its tasks claim to
        match the given action.

        The result of this method will be the last result of a task
        which isn't None, or None if they're all None.
        """
        self._has_run = True
        result = None
        errors = []
        for task in self._tasks:
            try:
                task_result = task.run(path)
            except AssertionError, e:
                error = str(e)
                if not error:
                    raise RuntimeError("Empty error message from %r" % task)
                errors.append(error)
            else:
                if task_result is not None:
                    result = task_result
        if errors:
            message = [str(self.path)]
            if str(path) != message[0]:
                message.append("- Run: %s" % path)
            for error in errors:
                lines = error.splitlines()
                message.append("- " + lines.pop(0))
                message.extend(["  " + line for line in lines])
            raise AssertionError(os.linesep.join(message))
        return result

    def satisfied(self):
        """Return true if all tasks are satisfied.

        Being satisfied means that there are no unmet expectations.
        """
        for task in self._tasks:
            try:
                task.verify()
            except AssertionError:
                return False
        return True

    def verify(self):
        """Run verify on all tasks.

        The verify method is supposed to raise an AssertionError if the
        task has unmet expectations, with a one-line explanation about
        why this item is unmet.  This method should be safe to be called
        multiple times without side effects.
        """
        errors = []
        for task in self._tasks:
            try:
                task.verify()
            except AssertionError, e:
                error = str(e)
                if not error:
                    raise RuntimeError("Empty error message from %r" % task)
                errors.append(error)
        if errors:
            message = [str(self.path)]
            for error in errors:
                lines = error.splitlines()
                message.append("- " + lines.pop(0))
                message.extend(["  " + line for line in lines])
            raise AssertionError(os.linesep.join(message))

    def replay(self):
        """Put all tasks in replay mode."""
        self._has_run = False
        for task in self._tasks:
            task.replay()

    def restore(self):
        """Restore the state of all tasks."""
        for task in self._tasks:
            task.restore()


class ReplayRestoreEvent(Event):
    """Helper event for tasks which need replay/restore but shouldn't match."""

    def matches(self, path):
        return False


class Task(object):
    """Element used to track one specific aspect on an event.

    A task is responsible for adding any kind of logic to an event.
    Examples of that are counting the number of times the event was
    made, verifying parameters if any, and so on.
    """

    def matches(self, path):
        """Return true if the task is supposed to be run for the given path.
        """
        return True

    def may_run(self, path):
        """Return false if running this task would certainly raise an error."""
        return True

    def run(self, path):
        """Perform the task item, considering that the given action happened.
        """

    def verify(self):
        """Raise AssertionError if expectations for this item are unmet.

        The verify method is supposed to raise an AssertionError if the
        task has unmet expectations, with a one-line explanation about
        why this item is unmet.  This method should be safe to be called
        multiple times without side effects.
        """

    def replay(self):
        """Put the task in replay mode.

        Any expectations of the task should be reset.
        """

    def restore(self):
        """Restore any environmental changes made by the task.

        Verify should continue to work after this is called.
        """


# --------------------------------------------------------------------
# Task implementations.

class OnRestoreCaller(Task):
    """Call a given callback when restoring."""

    def __init__(self, callback):
        self._callback = callback

    def restore(self):
        self._callback()


class PathMatcher(Task):
    """Match the action path against a given path."""

    def __init__(self, path):
        self.path = path

    def matches(self, path):
        return self.path.matches(path)

def path_matcher_recorder(mocker, event):
    event.add_task(PathMatcher(event.path))

Mocker.add_recorder(path_matcher_recorder)


class RunCounter(Task):
    """Task which verifies if the number of runs are within given boundaries.
    """

    def __init__(self, min, max=False):
        self.min = min
        if max is None:
            self.max = sys.maxint
        elif max is False:
            self.max = min
        else:
            self.max = max
        self._runs = 0

    def replay(self):
        self._runs = 0

    def may_run(self, path):
        return self._runs < self.max

    def run(self, path):
        self._runs += 1
        if self._runs > self.max:
            self.verify()

    def verify(self):
        if not self.min <= self._runs <= self.max:
            if self._runs < self.min:
                raise AssertionError("Performed fewer times than expected.")
            raise AssertionError("Performed more times than expected.")


class ImplicitRunCounter(RunCounter):
    """RunCounter inserted by default on any event.

    This is a way to differentiate explicitly added counters and
    implicit ones.
    """

def run_counter_recorder(mocker, event):
    """Any event may be repeated once, unless disabled by default."""
    if event.path.root_mock.__mocker_count__:
        event.add_task(ImplicitRunCounter(1))

Mocker.add_recorder(run_counter_recorder)

def run_counter_removal_recorder(mocker, event):
    """
    Events created by getattr actions which lead to other events
    may be repeated any number of times. For that, we remove implicit
    run counters of any getattr actions leading to the current one.
    """
    parent_path = event.path.parent_path
    for event in mocker.get_events()[::-1]:
        if (event.path is parent_path and
            event.path.actions[-1].kind == "getattr"):
            for task in event.get_tasks():
                if type(task) is ImplicitRunCounter:
                    event.remove_task(task)

Mocker.add_recorder(run_counter_removal_recorder)


class MockReturner(Task):
    """Return a mock based on the action path."""

    def __init__(self, mocker):
        self.mocker = mocker

    def run(self, path):
        return Mock(self.mocker, path)

def mock_returner_recorder(mocker, event):
    """Events that lead to other events must return mock objects."""
    parent_path = event.path.parent_path
    for event in mocker.get_events():
        if event.path is parent_path:
            for task in event.get_tasks():
                if isinstance(task, MockReturner):
                    break
            else:
                event.add_task(MockReturner(mocker))
            break

Mocker.add_recorder(mock_returner_recorder)


class FunctionRunner(Task):
    """Task that runs a function everything it's run.

    Arguments of the last action in the path are passed to the function,
    and the function result is also returned.
    """

    def __init__(self, func):
        self._func = func

    def run(self, path):
        action = path.actions[-1]
        return self._func(*action.args, **action.kwargs)


class PathExecuter(Task):
    """Task that executes a path in the real object, and returns the result."""

    def __init__(self, result_callback=None):
        self._result_callback = result_callback

    def get_result_callback(self):
        return self._result_callback

    def run(self, path):
        result = path.execute(path.root_object)
        if self._result_callback is not None:
            self._result_callback(result)
        return result


class Orderer(Task):
    """Task to establish an order relation between two events.

    An orderer task will only match once all its dependencies have
    been run.
    """

    def __init__(self, path):
        self.path = path
        self._run = False 
        self._dependencies = []

    def replay(self):
        self._run = False

    def has_run(self):
        return self._run

    def may_run(self, path):
        for dependency in self._dependencies:
            if not dependency.has_run():
                return False
        return True

    def run(self, path):
        for dependency in self._dependencies:
            if not dependency.has_run():
                raise AssertionError("Should be after: %s" % dependency.path)
        self._run = True

    def add_dependency(self, orderer):
        self._dependencies.append(orderer)

    def get_dependencies(self):
        return self._dependencies


class SpecChecker(Task):
    """Task to check if arguments of the last action conform to a real method.
    """

    def __init__(self, method):
        self._method = method
        self._unsupported = False

        if method:
            try:
                self._args, self._varargs, self._varkwargs, self._defaults = \
                    inspect.getargspec(method)
            except TypeError:
                self._unsupported = True
            else:
                if self._defaults is None:
                    self._defaults = ()
                if type(method) is type(self.run):
                    self._args = self._args[1:]

    def get_method(self):
        return self._method

    def _raise(self, message):
        spec = inspect.formatargspec(self._args, self._varargs,
                                     self._varkwargs, self._defaults)
        raise AssertionError("Specification is %s%s: %s" %
                             (self._method.__name__, spec, message))

    def verify(self):
        if not self._method:
            raise AssertionError("Method not found in real specification")

    def may_run(self, path):
        try:
            self.run(path)
        except AssertionError:
            return False
        return True

    def run(self, path):
        if not self._method:
            raise AssertionError("Method not found in real specification")
        if self._unsupported:
            return # Can't check it. Happens with builtin functions. :-(
        action = path.actions[-1]
        obtained_len = len(action.args)
        obtained_kwargs = action.kwargs.copy()
        nodefaults_len = len(self._args) - len(self._defaults)
        for i, name in enumerate(self._args):
            if i < obtained_len and name in action.kwargs:
                self._raise("%r provided twice" % name)
            if (i >= obtained_len and i < nodefaults_len and
                name not in action.kwargs):
                self._raise("%r not provided" % name)
            obtained_kwargs.pop(name, None)
        if obtained_len > len(self._args) and not self._varargs:
            self._raise("too many args provided")
        if obtained_kwargs and not self._varkwargs:
            self._raise("unknown kwargs: %s" % ", ".join(obtained_kwargs))

def spec_checker_recorder(mocker, event):
    spec = event.path.root_mock.__mocker_spec__
    if spec:
        actions = event.path.actions
        if len(actions) == 1:
            if actions[0].kind == "call":
                method = getattr(spec, "__call__", None)
                event.add_task(SpecChecker(method))
        elif len(actions) == 2:
            if actions[0].kind == "getattr" and actions[1].kind == "call":
                method = getattr(spec, actions[0].args[0], None)
                event.add_task(SpecChecker(method))

Mocker.add_recorder(spec_checker_recorder)


class ProxyReplacer(Task):
    """Task which installs and deinstalls proxy mocks.

    This task will replace a real object by a mock in all dictionaries
    found in the running interpreter via the garbage collecting system.
    """

    def __init__(self, mock):
        self.mock = mock
        self.__mocker_replace__ = False

    def replay(self):
        global_replace(self.mock.__mocker_object__, self.mock)

    def restore(self):
        global_replace(self.mock, self.mock.__mocker_object__)


def global_replace(remove, install):
    """Replace object 'remove' with object 'install' on all dictionaries."""
    for referrer in gc.get_referrers(remove):
        if (type(referrer) is dict and
            referrer.get("__mocker_replace__", True)):
            for key, value in referrer.items():
                if value is remove:
                    referrer[key] = install


class Undefined(object):

    def __repr__(self):
        return "Undefined"

Undefined = Undefined()


class Patcher(Task):

    def __init__(self):
        super(Patcher, self).__init__()
        self._monitored = {} # {kind: {id(object): object}}
        self._patched = {}

    def is_monitoring(self, obj, kind):
        monitored = self._monitored.get(kind)
        if monitored:
            if id(obj) in monitored:
                return True
            cls = type(obj)
            if issubclass(cls, type):
                cls = obj
            bases = set([id(base) for base in cls.__mro__])
            bases.intersection_update(monitored)
            return bool(bases)
        return False

    def monitor(self, obj, kind):
        if kind not in self._monitored:
            self._monitored[kind] = {}
        self._monitored[kind][id(obj)] = obj

    def patch_attr(self, obj, attr, value):
        original = obj.__dict__.get(attr, Undefined)
        self._patched[id(obj), attr] = obj, attr, original
        setattr(obj, attr, value)

    def get_unpatched_attr(self, obj, attr):
        cls = type(obj)
        if issubclass(cls, type):
            cls = obj
        result = Undefined
        for mro_cls in cls.__mro__:
            key = (id(mro_cls), attr)
            if key in self._patched:
                result = self._patched[key][2]
                if result is not Undefined:
                    break
            elif attr in mro_cls.__dict__:
                result = mro_cls.__dict__.get(attr, Undefined)
                break
        if isinstance(result, object) and hasattr(type(result), "__get__"):
            if cls is obj:
                obj = None
            return result.__get__(obj, cls)
        return result

    def _get_kind_attr(self, kind):
        if kind == "getattr":
            return "__getattribute__"
        return "__%s__" % kind

    def replay(self):
        for kind in self._monitored:
            attr = self._get_kind_attr(kind)
            seen = set()
            for obj in self._monitored[kind].itervalues():
                cls = type(obj)
                if issubclass(cls, type):
                    cls = obj
                if cls not in seen:
                    seen.add(cls)
                    unpatched = getattr(cls, attr, Undefined)
                    self.patch_attr(cls, attr,
                                    PatchedMethod(kind, unpatched,
                                                  self.is_monitoring))
                    self.patch_attr(cls, "__mocker_execute__",
                                    self.execute)

    def restore(self):
        for obj, attr, original in self._patched.itervalues():
            if original is Undefined:
                delattr(obj, attr)
            else:
                setattr(obj, attr, original)
        self._patched.clear()

    def execute(self, action, object):
        attr = self._get_kind_attr(action.kind)
        unpatched = self.get_unpatched_attr(object, attr)
        try:
            return unpatched(*action.args, **action.kwargs)
        except AttributeError:
            if action.kind == "getattr":
                # The normal behavior of Python is to try __getattribute__,
                # and if it raises AttributeError, try __getattr__.   We've
                # tried the unpatched __getattribute__ above, and we'll now
                # try __getattr__.
                try:
                    __getattr__ = unpatched("__getattr__")
                except AttributeError:
                    pass
                else:
                    return __getattr__(*action.args, **action.kwargs)
            raise


class PatchedMethod(object):

    def __init__(self, kind, unpatched, is_monitoring):
        self._kind = kind
        self._unpatched = unpatched
        self._is_monitoring = is_monitoring

    def __get__(self, obj, cls=None):
        object = obj or cls
        if not self._is_monitoring(object, self._kind):
            return self._unpatched.__get__(obj, cls)
        def method(*args, **kwargs):
            if self._kind == "getattr" and args[0].startswith("__mocker_"):
                return self._unpatched.__get__(obj, cls)(args[0])
            mock = object.__mocker_mock__
            return mock.__mocker_act__(self._kind, args, kwargs, object)
        return method

    def __call__(self, obj, *args, **kwargs):
        # At least with __getattribute__, Python seems to use *both* the
        # descriptor API and also call the class attribute directly.  It
        # looks like an interpreter bug, or at least an undocumented
        # inconsistency.
        return self.__get__(obj)(*args, **kwargs)


def patcher_recorder(mocker, event):
    mock = event.path.root_mock
    if mock.__mocker_patcher__ and len(event.path.actions) == 1:
        patcher = mock.__mocker_patcher__
        patcher.monitor(mock.__mocker_object__, event.path.actions[0].kind)

Mocker.add_recorder(patcher_recorder)

########NEW FILE########
__FILENAME__ = my
"""Mock various MySQL junk"""
import os
import logging
import MySQLdb
from mocker import *
from hldump.mysql.option import parse_options

def mock_mysql(mocker):
    mock_mysql_config(mocker)
    mock_mysql_option(mocker)
    mock_mysqldb_connect(mocker)
    mock_mysql_client(mocker)

def mock_mysql_client(mocker):
    client_cls = mocker.replace('hldump.mysql.client.MySQLClient')
    client = client_cls(ARGS, KWARGS)
    client.stop_slave()
    mocker.result(None)
    mocker.count(min=0,max=None)
    client.start_slave()
    mocker.result(None)
    mocker.count(min=0,max=None)

def mock_mysql_config(mocker):
    obj = mocker.replace('hldump.core.mysql_config')
    file = obj(ANY)
    mocker.call(lambda x: _mysql_config(mocker, x))

def _mysql_config(mocker, options):
    mysql_keys = ['user','password','host','port','socket']
    mysql_options = { 'client' : dict([(key, value)
                          for key, value in options.items()
                              if key in mysql_keys])
                    }

    if options['defaults_file']:
        defaults_file = os.path.expanduser(options['defaults_file'])
        defaults_file = os.path.abspath(defaults_file)
        logging.info("defaults-file %s", defaults_file)
        my_cnf = parse_options(open(defaults_file, 'r'))
        mysql_options.update(my_cnf)

    mocker.mysql_options = mysql_options
    return 'my.mock.cnf'

def mock_mysql_option(mocker):
    obj = mocker.replace('hldump.mysql.option.write_options')
    obj(ARGS,KWARGS)
    mocker.count(min=0,max=None)

def mock_mysqldb_connect(mocker):
    # What we want to happen is that when we're called with:
    # MySQLdb.connect(read_default_file='my.mock.cnf')
    # instead we run:
    # MySQLdb.connect(user=..., password=..., etc.)
    # once mysql_config is called, we'll have a mysql_config object
    # right on the mocker instance
    obj = mocker.replace('MySQLdb.connect')
    conn = obj(KWARGS, read_default_file=CONTAINS('my.mock.cnf'))
    mocker.count(min=0,max=None)
    mocker.call(lambda *args, **kwargs: _mysqldb_connect(mocker, *args, **kwargs))
   
def _mysqldb_connect(mocker, *args, **kwargs):
    # pull in the options from mocker.mysql_options if they're available or throw an exception
    logging.info("_mysqldb_connect(*%r, **%r)", args, kwargs)
    kwargs = dict(kwargs)

    kwargs.pop('read_default_file', None)
    kwargs.pop('read_default_group', None)

    if not hasattr(mocker, 'mysql_options'):
        mocker.mysql_options = {}
    mocker.mysql_options.setdefault('client', {})
    for key, value in mocker.mysql_options['client'].items():
        if not value:
            continue
        if key not in ('user', 'password', 'host', 'port', 'socket'):
            continue
        if key == 'password':
            key = 'passwd'
        kwargs[key] = value
    print "MySQLdb.connect(**%r)" % kwargs
    return MySQLdb.connect(**kwargs)

if __name__ == '__main__':
    mocker = Mocker()
    mock_mysql(mocker)

########NEW FILE########
__FILENAME__ = popen
"""Mock subprocess.Popen"""

from mocker import *

def _debug_wait(*args, **kwargs):
    print "Waiting(args=%r, kwargs=%r)" % (args, kwargs)
    return 0

def mock_subprocess(mocker):
    popen = mocker.replace('subprocess.Popen')
    pid = popen(ARGS, KWARGS)
    mocker.count(min=0,max=None)

    pid.poll()
    mocker.count(min=0,max=None)
    mocker.result(0)

    pid.wait()
    mocker.count(min=0,max=None)
    mocker.result(0)

    foo = pid.returncode
    mocker.count(min=0,max=None)
    mocker.result(0)

    mock_subprocess_stdin(mocker, pid)
    mock_subprocess_stdout(mocker, pid)
    mock_subprocess_stderr(mocker, pid)

def mock_subprocess_stdin(mocker, pid):
    # mock stdin, stdout, stderr as iterate file-like objects
    pid.stdin.write(ANY)
    mocker.count(min=0,max=None)
    mocker.call(lambda s: len(s))

    pid.stdin.close()
    mocker.count(min=0,max=None)

def mock_subprocess_stdout(mocker, pid):
    pid.stdout.read(ANY)
    mocker.count(min=0, max=None)
    mocker.result('')
    iter(pid.stdout)
    mocker.count(min=0, max=None)
    mocker.generate('')

    pid.stdout.fileno()
    mocker.count(min=0,max=None)
    mocker.result(-1)

    pid.stdout.close()
    mocker.count(min=0,max=None)
    mocker.result(-1)

def mock_subprocess_stderr(mocker, pid):
    pid.stderr.read(ANY)
    mocker.count(min=0, max=None)
    mocker.result('')
    iter(pid.stderr)
    mocker.count(min=0, max=None)
    mocker.generate('')

    pid.stderr.fileno()
    mocker.count(min=0,max=None)
    mocker.result(-1)

########NEW FILE########
__FILENAME__ = storage
import __builtin__

__all__ = (
    'file', 'open',
    'replace_builtins', 'restore_builtins',
    'original_file', 'original_open'
)

from warnings import warn

original_file = __builtin__.file
original_open = __builtin__.open


# bad for introspection?
DEFAULT = object()

# switch to a regex?
READ_MODES = ('r', 'rb', 'rU')
WRITE_MODES = ('w', 'wb', 'a', 'ab')

MIXED_MODES = ('r+', 'r+b', 'w+', 'w+b', 'a+', 'a+b')
ALL_MODES = READ_MODES + WRITE_MODES + MIXED_MODES
READ_MODES += MIXED_MODES
WRITE_MODES += MIXED_MODES


# would need something a little less basic if os.read
# is implemented...
_fileno_counter = 2
def get_new_fileno():
    global _fileno_counter
    _fileno_counter += 1
    return _fileno_counter


class file(object):
    """file(name[, mode]) -> file object

Open a file.  The mode can be 'r', 'w' or 'a' for reading (default),
writing or appending.  The file will be created if it doesn't exist
when opened for writing or appending; it will be truncated when
opened for writing.  Add a 'b' to the mode for binary files.
Add a '+' to the mode to allow simultaneous reading and writing.
The preferred way to open a file is with the builtin open() function."""

    def mode(self):
        "file mode, one of r(+)(b), w(+)(b) or a(+)(b)"
        return self._mode
    mode = property(mode)

    def name(self):
        "file name"
        return self._name
    name = property(name)

    def closed(self):
        "True if the file is closed"
        return self._closed
    closed = property(closed)

    def encoding(self):
        "file encoding"
        return None
    encoding = property(encoding)

    def errors(self):
        "Unicode error handler"
        return None
    errors = property(errors)

    def newlines(self):
        "end-of-line convention used in this file"
        return None
    newlines = property(newlines)

    _closed = False
    _mode = 'r'
    
    def __init__(self, name, mode='r'):
        "x.__init__(...) initializes x; see x.__class__.__doc__ for signature"
        if not isinstance(name, basestring):
            raise TypeError('File name argument must be str got: %s' %
                             type(name))
        if not isinstance(mode, basestring):
            raise TypeError('File mode argument must be str got: %s' % 
                            type(mode))
            
        self._name = name
        self._mode = mode
        self._position = 0
        self._data = ''
        self._closed = False
        self._binary = mode.endswith('b')
        self._fileno = get_new_fileno()
        self._in_iter = False
        self._softspace = 0
        
        if mode not in ALL_MODES:
            raise ValueError("The only supported modes are r(+)(b), w(+)(b) "
                             "and a(+)(b), not %r" % mode)
        if name == '':
            raise IOError("No such file or directory: ''")
        
        if mode in READ_MODES and mode[0] not in ('a', 'w'):
            self._open_read()
        elif mode in WRITE_MODES:
            if mode.startswith('a'):
                self._open_append()
            else:
                self._open_write()
        else:
            # double check and remove this branch!
            raise AssertionError('whoops - not possible, surely??')
    
    
    def _open_read(self):
        if not backend.CheckForFile(self.name):
            raise IOError('No such file or directory: %r' % self.name)
        data = backend.LoadFile(self.name)
        if not self._binary:
            data = data.replace('\r\n', '\n')
        self._data = data
    
        
    def _open_write(self):
        backend.SaveFile(self.name, '')
        
    
    def _open_append(self):
        if backend.CheckForFile(self.name):
            self._open_read()
            self._position = len(self._data)
        else:
            self._open_write()
    
    
    def _check_int_argument(self, arg):
        if isinstance(arg, float):
            arg = int(arg)
            warn(DeprecationWarning('Integer argument expected got float'))
        elif not isinstance(arg, (int, long)):
            raise TypeError('Integer argument expected. Got %s' % type(arg))
        return arg


    def read(self, size=DEFAULT):
        """read([size]) -> read at most size bytes, returned as a string.

        If the size argument is negative or omitted, read until EOF is reached.
        Notice that when in non-blocking mode, less data than what was requested
        may be returned, even if no size parameter was given."""
        if self.mode not in READ_MODES:
            raise IOError('Bad file descriptor')
        if self.closed:
            raise ValueError('I/O operation on closed file')
        if self._in_iter:
            raise ValueError('Mixing iteration and read methods would '
                             'lose data')
        
        pos = self._position
        if pos == 0 and self.mode not in WRITE_MODES:
            # can't do this when we are in a mixed read / write mode like r+
            # could do this on every read, not just when pos is 0?
            self._open_read()
        
        if size is DEFAULT:
            size = len(self._data)
        else:
            size = self._check_int_argument(size)
            if size < 0:
                size = len(self._data)
        
        data = self._data[pos: pos + size]
        self._position += len(data)
        return data
    
    
    def write(self, data):
        """write(str) -> None.  Write string str to file.

Note that due to buffering, flush() or close() may be needed before
the file on disk reflects the data written."""
        if self.mode not in WRITE_MODES:
            raise IOError('Bad file descriptor')
        if self.closed:
            raise ValueError('I/O operation on closed file')

        self._softspace = 0
        
        if not data:
            return
        if not self._binary:
            data = data.replace('\n', '\r\n')
        
        position = self._position
        start = self._data[:position]
        padding = (position - len(start)) * '\x00'
        end = self._data[position + len(data):]
        self._data = start + padding + data + end
        self._position = position + len(data)
    
        
    def close(self):
        """close() -> None or (perhaps) an integer.  Close the file.

Sets data attribute .closed to True.  A closed file cannot be used for
further I/O operations.  close() may be called more than once without
error.  Some kinds of file objects (for example, opened by popen())
may return an exit status upon closing."""
        if self.closed:
            return
        self._closed = True
        if self.mode in WRITE_MODES:
            backend.SaveFile(self.name, self._data)


    def __repr__(self):
        "x.__repr__() <==> repr(x)"
        state = 'open'
        if self.closed:
            state = 'closed'
        return '<%s file %r mode %r>' % (state, self.name, self.mode)
        
    
    def __del__(self):
        "alias for close()"
        self.close()

        
    def seek(self, position, whence=0):
        """seek(offset[, whence]) -> None.  Move to new file position.

Argument offset is a byte count.  Optional argument whence defaults to
0 (offset from start of file, offset should be >= 0); other values are 1
(move relative to current position, positive or negative), and 2 (move
relative to end of file, usually negative, although many platforms allow
seeking beyond the end of a file).  If the file is opened in text mode,
only offsets returned by tell() are legal.  Use of other offsets causes
undefined behavior.
Note that not all file objects are seekable."""
        position = self._check_int_argument(position)
        whence = self._check_int_argument(whence)
        if not 0 <= whence <= 2:
            raise IOError('Invalid Argument')
        
        if whence == 1:
            position = self._position + position
        elif whence == 2:
            position = len(self._data) + position
            
        if position < 0:
            raise IOError('Invalid Argument')
        self._in_iter = False
        self._position = position

        
    def tell(self):
        "tell() -> current file position, an integer (may be a long integer)."
        return self._position


    def flush(self):
        "flush() -> None.  Flush the internal I/O buffer."
        if self.mode not in WRITE_MODES:
            raise IOError('Bad file descriptor')
        backend.SaveFile(self.name, self._data)


    def isatty(self):
        "isatty() -> true or false.  True if the file is connected to a tty device."
        return False

    
    def fileno(self):
        """fileno() -> integer "file descriptor".

This is needed for lower-level file interfaces, such os.read()."""
        return self._fileno

    
    def __iter__(self):
        "x.__iter__() <==> iter(x)"
        return self

    
    def next(self):
        "x.next() -> the next value, or raise StopIteration"
        if self.mode in WRITE_MODES:
            raise IOError('Bad file descriptor')
        self._in_iter = True
        if self._position >= len(self._data):
            raise StopIteration
        return self.readline()
    
    
    def readline(self, size=DEFAULT):
        """readline([size]) -> next line from the file, as a string.

Retain newline.  A non-negative size argument limits the maximum
number of bytes to return (an incomplete line may be returned then).
Return an empty string at EOF."""
        if self.mode in WRITE_MODES:
            raise IOError('Bad file descriptor')
        
        if size is not DEFAULT:
            size = self._check_int_argument(size)
            if size < 0:
                # treat negative integers the same as DEFAULT
                size = DEFAULT
        
        if self._position >= len(self._data):
            return ''
        
        position = self._position
        remaining = self._data[position:]
        poz = remaining.find('\n')
        
        if poz == -1:
            if size is DEFAULT or size > len(remaining):
                self._position = len(self._data)
                return remaining
            actual = remaining[:size]
            self._position += len(actual)
            return actual
        
        if size is DEFAULT:
            self._position = position + poz + 1
            return remaining[:poz + 1]
        
        actual = remaining[:poz + 1]
        if len(actual) <= size:
            self._position += len(actual)
            return actual
        self._position += size
        return actual[:size]


    def readlines(self, size=DEFAULT):
        """readlines([size]) -> list of strings, each a line from the file.

Call readline() repeatedly and return a list of the lines so read.
The optional size argument, if given, is an approximate bound on the
total number of bytes in the lines returned."""
        if self.mode in WRITE_MODES:
            raise IOError('Bad file descriptor')
        
        # argument actually ignored
        if size is not DEFAULT:
            self._check_int_argument(size)

        result = list(self)
        self._in_iter = False
        return result
    
    
    def xreadlines(self):
        """xreadlines() -> returns self.

For backward compatibility. File objects now include the performance
optimizations previously implemented in the xreadlines module."""
        return self

    
    def _set_softspace(self, value):
        self._softspace = self._check_int_argument(value)
    
    def _get_softspace(self):
        return self._softspace
    
    softspace = property(_get_softspace, _set_softspace, 
                         doc="flag indicating that a space needs to be printed; used by print")
    
    
    def truncate(self, size=DEFAULT):
        """truncate([size]) -> None.  Truncate the file to at most size bytes.

Size defaults to the current file position, as returned by tell()."""
        if self.mode in READ_MODES:
            raise IOError('Bad file descriptor')
        if size is not DEFAULT:
            size = self._check_int_argument(size)
            if size < 0:
                raise IOError('Invalid argument')
        else:
            size = self._position
        data = self._data[:size]
        self._data = data + (size - len(data)) * '\x00'
        self.flush()
        
    
    def writelines(self, sequence):
        """writelines(sequence_of_strings) -> None.  Write the strings to the file.

Note that newlines are not added.  The sequence can be any iterable object
producing strings. This is equivalent to calling write() for each string."""
        if self.mode not in WRITE_MODES:
            raise IOError('Bad file descriptor')
        
        if getattr(sequence, '__iter__', None) is None:
            raise TypeError('writelines() requires an iterable argument')
        
        for line in sequence:
            self.write(line)
    
            
    def __enter__(self):
        "__enter__() -> self."
        return self
    

    def __exit__(self, *excinfo):
        "__exit__(*excinfo) -> None.  Closes the file."
        self.close()

    

def open(name, mode='r', bufsize=None):
    """open(name[, mode]) -> file object

Open a file using the file() type, returns a file object.  This is the
preferred way to open a file."""
    return file(name, mode)

def mkdir(path, mode=None):
    """mkdir(path [, mode=0777])"""

def replace_builtins():
    "replace file and open in the builtin module"
    __builtin__.file =  file
    __builtin__.open = open

def restore_builtins():
    "restore the original file and open to the builtin module"
    __builtin__.file =  original_file
    __builtin__.open = original_open

    
_store = {}

class backend(object):
    "Example backend."
    
    def CheckForFile(filename):
        return filename in _store
    CheckForFile = staticmethod(CheckForFile)

    def DeleteFile(filename):
        del _store[filename]
    DeleteFile = staticmethod(DeleteFile)

    def LoadFile(filename):
        return _store[filename]
    LoadFile = staticmethod(LoadFile)

    def SaveFile(filename, data):
        _store[filename] = data
    SaveFile = staticmethod(SaveFile)

########NEW FILE########
__FILENAME__ = _subprocess
from storage import original_open as open

class PopenMock(object):
    """subprocess.Popen mock object implementation
    
    This does not support the new features in 2.6
    (kill, send_signal, terminate)
    """
    def __init__(self, *args, **kwargs):
        self.pid = -1
        self.stdin = open("/dev/null", "r")
        self.stdout = open("/dev/null", "r+")
        self.stderr = open("/dev/null", "r+")
        self.returncode = None
        self.universal_newlines = False

    def communicate(self, input=None):
        """Interact with the process"""
        return ('', '')

    def wait(self):
        """Wait for this process to finish.

        This mock objects always returns 0 for the status
        """
        if self.returncode is None:
            self.returncode = 0
        return self.returncode

    def poll(self):
        if self.returncode is None:
            self.returncode = 0
        return self.returncode

########NEW FILE########
__FILENAME__ = option
"""MySQL option files support

http://dev.mysql.com/doc/refman/5.1/en/option-files.html
"""
import os
import re
import codecs
import logging
from holland.backup.mysqldump.util import INIConfig, BasicConfig
from holland.backup.mysqldump.util.config import update_config
from holland.backup.mysqldump.util.ini import ParsingError

LOG = logging.getLogger(__name__)

def merge_options(path,
                  *defaults_files,
                  **kwargs):
    defaults_config = INIConfig()
    defaults_config._new_namespace('client')
    for config in defaults_files:
        _my_config = load_options(config)
        update_config(defaults_config, _my_config)

    for key in ('user', 'password', 'socket', 'host', 'port'):
        if kwargs.get(key) is not None:
            defaults_config['client'][key] = kwargs[key]
    write_options(defaults_config, path)

def load_options(filename):
    """Load mysql option file from filename"""
    filename = os.path.abspath(os.path.expanduser(filename))
    cfg = INIConfig()
    try:
        cfg._readfp(open(filename, 'r'))
    except ParsingError, exc:
        LOG.debug("Skipping unparsable lines")
        for lineno, line in exc.errors:
            LOG.debug("Ignored line %d: %s", lineno, line.rstrip())

    return client_sections(cfg)

def unquote(value):
    """Remove quotes from a string."""
    if len(value) > 1 and value[0] == '"' and value[-1] == '"':
            value = value[1:-1]

    # substitute meta characters per:
    # http://dev.mysql.com/doc/refman/5.0/en/option-files.html
    MYSQL_META = {
        'b' : "\b",
        't' : "\t",
        'n' : "\n",
        'r' : "\r",
        '\\': "\\",
        's' : " ",
        '"' : '"',
    }
    return re.sub(r'\\(["btnr\\s])',
                  lambda m: MYSQL_META[m.group(1)],
                  value)

def quote(value):
    """Added quotes around a value"""

    return '"' + value.replace('"', '\\"') + '"'

def client_sections(config):
    """Create a copy of config with only valid client auth sections

    This includes [client], [mysql] and [mysqldump] with only options
    related to mysql authentication.
    """

    clean_cfg = INIConfig()
    clean_cfg._new_namespace('client')
    valid_sections = ['client', 'mysql', 'holland']
    for section in valid_sections:
        if section in config:
            clean_section = client_keys(config[section])
            update_config(clean_cfg.client, clean_section)
    return clean_cfg

def client_keys(config):
    """Create a copy of option_section with non-authentication options
    stripped out.

    Authentication options supported are:
    user, password, host, port, and socket
    """

    clean_namespace = BasicConfig()
    update_config(clean_namespace, config)
    valid_keys = ['user', 'password', 'host', 'port', 'socket']
    for key in config:
        if key not in valid_keys:
            del clean_namespace[key]
        else:
            clean_namespace[key] = unquote(config[key])
    return clean_namespace

def write_options(config, filename):
    quoted_config = INIConfig()
    update_config(quoted_config, config)
    for section in config:
        for key in config[section]:
            if '"' in config[section][key]:
                config[section][key] = quote(config[section][key])

    if isinstance(filename, basestring):
        filename = codecs.open(filename, 'w', 'utf8')
    data = unicode(config)
    print >>filename, data
    filename.close()

########NEW FILE########
__FILENAME__ = _legacy
import re
import logging
from subprocess import Popen, PIPE, list2cmdline
from exc import BackupError

def mysqldump_args(databases, defaults_file, extra_options):
    argv = [
        'mysqldump',
    ]

    if defaults_file:
        argv.append('--defaults-file=%s' % defaults_file)

    if extra_options:
        argv.extend(extra_options)

    argv.append('--databases')
    argv.extend(databases)

    return argv

def mysqldump(result_file,
              databases=None,
              defaults_file=None,
              extra_options=None):
    validate_extra_options(extra_options)
    collapse_extra_options(extra_options)
    args = mysqldump_args(databases, defaults_file, extra_options)
    logging.info("%s", list2cmdline(args))
    pid = Popen(args,
                stdout=result_file.fileno(),
                stderr=PIPE,
                close_fds=True)
    while pid.poll() is None:
        line = pid.stderr.readline()
        if not line:
            # eof
            break
        logging.error("[mysqldump:error] %s", line.rstrip())
    pid.wait()
    logging.info("mysqldump finished")
    if pid.returncode != 0:
        logging.debug("pid.returncode = %r", pid.returncode)
        raise BackupError("mysqldump failure[%d]" % pid.returncode)


def validate_extra_options(mysqldump_options):
    if not mysqldump_options:
        return

    for opt in mysqldump_options:
        validate_one_option(opt)

def collapse_extra_options(options):
    result = []
    for opt in options:
        if opt in result:
            logging.warning("Removing duplicate option %r", opt)
        else:
            result.append(opt)
    del options[:]
    options.extend(result)

# map patterns to actions
# passthrough => OK
# raise UnsupportedOption => not support by us
# raise InvalidOption => Not supported by mysqldump
def validate_one_option(opt):
    valid_options = [
        r'--flush-logs$',
        r'--routines$',
        r'--events$',
        r'--single-transaction$',
        r'--lock-all-tables$',
        r'--lock-tables$',
        r'--default-character-set=\w+$',
        r'--master-data(=\d)?$',
        r'--flush-privileges$'
    ]

    for opt_check in valid_options:
        if re.match(opt_check, opt):
            break
        logging.debug("%r did not match %r", opt, opt_check)
    else:
        raise BackupError("Invalid option %s" % opt)

########NEW FILE########
__FILENAME__ = plugin
"""Command Line Interface"""

import os
import re
import codecs
import logging
from holland.core.exceptions import BackupError
from holland.lib.compression import open_stream, lookup_compression
from holland.lib.mysql import MySQLSchema, connect, MySQLError
from holland.lib.mysql import include_glob, exclude_glob, \
                              include_glob_qualified, \
                              exclude_glob_qualified
from holland.lib.mysql import DatabaseIterator, MetadataTableIterator, \
                              SimpleTableIterator
from holland.backup.mysqldump.base import start
from holland.backup.mysqldump.util import INIConfig, update_config
from holland.backup.mysqldump.util.ini import OptionLine, CommentLine
from holland.lib.mysql.option import load_options, \
                                     write_options, \
                                     build_mysql_config
from holland.backup.mysqldump.command import MySQLDump, MySQLDumpError, \
                                             MyOptionError
from holland.backup.mysqldump.mock import MockEnvironment

LOG = logging.getLogger(__name__)

# We validate our config against the following spec
CONFIGSPEC = """
[mysqldump]
extra-defaults      = boolean(default=no)
mysql-binpath       = force_list(default=list())

lock-method         = option('flush-lock', 'lock-tables', 'single-transaction', 'auto-detect', 'none', default='auto-detect')

databases           = force_list(default=list('*'))
exclude-databases   = force_list(default=list())

tables              = force_list(default=list("*"))
exclude-tables      = force_list(default=list())

engines             = force_list(default=list("*"))
exclude-engines     = force_list(default=list())

exclude-invalid-views = boolean(default=no)

flush-logs           = boolean(default=no)
flush-privileges    = boolean(default=yes)
dump-routines       = boolean(default=yes)
dump-events         = boolean(default=yes)
stop-slave          = boolean(default=no)
max-allowed-packet  = string(default=128M)
bin-log-position    = boolean(default=no)

file-per-database   = boolean(default=yes)

additional-options  = force_list(default=list())

estimate-method = string(default='plugin')

[compression]
method = option('none', 'gzip', 'gzip-rsyncable', 'pigz', 'bzip2', 'pbzip2', 'lzma', 'lzop', default='gzip')
options = string(default="")
inline = boolean(default=yes)
level  = integer(min=0, max=9, default=1)

[mysql:client]
defaults-extra-file = force_list(default=list('~/.my.cnf'))
user                = string(default=None)
password            = string(default=None)
socket              = string(default=None)
host                = string(default=None)
port                = integer(min=0, default=None)
""".splitlines()

class MySQLDumpPlugin(object):
    """MySQLDump Backup Plugin interface for Holland"""
    CONFIGSPEC = CONFIGSPEC

    def __init__(self, name, config, target_directory, dry_run=False):
        self.name = name
        self.config = config
        self.target_directory = target_directory
        self.dry_run = dry_run
        self.config.validate_config(self.CONFIGSPEC) # -> ValidationError

        # Setup a discovery shell to find schema items
        # This will iterate over items during the estimate
        # or backup phase, which will call schema.refresh()
        self.schema = MySQLSchema()
        config = self.config['mysqldump']
        self.schema.add_database_filter(include_glob(*config['databases']))
        self.schema.add_database_filter(
                exclude_glob(*config['exclude-databases'])
        )

        self.schema.add_table_filter(include_glob_qualified(*config['tables']))
        self.schema.add_table_filter(exclude_glob_qualified(*config['exclude-tables']))
        self.schema.add_engine_filter(include_glob(*config['engines']))
        self.schema.add_engine_filter(exclude_glob(*config['exclude-engines']))

        self.mysql_config = build_mysql_config(self.config['mysql:client'])
        self.client = connect(self.mysql_config['client'])

    def estimate_backup_size(self):
        """Estimate the size of the backup this plugin will generate"""
        LOG.info("Estimating size of mysqldump backup")
        estimate_method = self.config['mysqldump']['estimate-method']

        if estimate_method.startswith('const:'):
            try:
                return parse_size(estimate_method[6:])
            except ValueError, exc:
                raise BackupError(str(exc))

        if estimate_method != 'plugin':
            raise BackupError("Invalid estimate-method '%s'" % estimate_method)

        try:
            db_iter = DatabaseIterator(self.client)
            tbl_iter = MetadataTableIterator(self.client)
            try:
                self.client.connect()
                self.schema.refresh(db_iter=db_iter, tbl_iter=tbl_iter)
            except MySQLError, exc:
                LOG.error("Failed to estimate backup size")
                LOG.error("[%d] %s", *exc.args)
                raise BackupError("MySQL Error [%d] %s" % exc.args)
            return sum([db.size for db in self.schema.databases])
        finally:
            self.client.disconnect()

    def _fast_refresh_schema(self):
        # determine if we can skip expensive table metadata lookups entirely
        # and just worry about finding database names
        # However, with lock-method=auto-detect we must look at table engines
        # to determine what lock method to use
        config = self.config['mysqldump']
        fast_iterate = config['lock-method'] != 'auto-detect' and \
                        not config['exclude-invalid-views']

        try:
            db_iter = DatabaseIterator(self.client)
            tbl_iter = SimpleTableIterator(self.client, record_engines=True)
            try:
                self.client.connect()
                self.schema.refresh(db_iter=db_iter,
                                    tbl_iter=tbl_iter,
                                    fast_iterate=fast_iterate)
            except MySQLError, exc:
                LOG.debug("MySQLdb error [%d] %s", exc_info=True, *exc.args)
                raise BackupError("MySQL Error [%d] %s" % exc.args)
        finally:
            self.client.disconnect()

    def backup(self):
        """Run a MySQL backup"""

        if self.schema.timestamp is None:
            self._fast_refresh_schema()

        mock_env = None
        if self.dry_run:
            mock_env = MockEnvironment()
            mock_env.replace_environment()
            LOG.info("Running in dry-run mode.")


        try:
            if self.config['mysqldump']['stop-slave']:
                self.client = connect(self.mysql_config['client'])
                if self.client.show_status('Slave_running', session=None) != 'ON':
                    raise BackupError("stop-slave enabled, but replication is "
                                  "either not configured or the slave is not "
                                  "running.")
                self.config.setdefault('mysql:replication', {})
                _stop_slave(self.client, self.config['mysql:replication'])
            self._backup()
        finally:
            if self.config['mysqldump']['stop-slave'] and \
                'mysql:replication' in self.config:
                _start_slave(self.client, self.config['mysql:replication'])
            if mock_env:
                mock_env.restore_environment()

    def _backup(self):
        """Real backup method.  May raise BackupError exceptions"""
        config = self.config['mysqldump']

        # setup defaults_file with ignore-table exclusions
        defaults_file = os.path.join(self.target_directory, 'my.cnf')
        write_options(self.mysql_config, defaults_file)
        if config['exclude-invalid-views']:
            LOG.info("* Finding and excluding invalid views...")
            definitions_path = os.path.join(self.target_directory,
                                            'invalid_views.sql')
            exclude_invalid_views(self.schema, self.client, definitions_path)
        add_exclusions(self.schema, defaults_file)

        # find the path to the mysqldump command
        mysqldump_bin = find_mysqldump(path=config['mysql-binpath'])
        LOG.info("Using mysqldump executable: %s", mysqldump_bin)

        # setup the mysqldump environment
        extra_defaults = config['extra-defaults']
        try:
            mysqldump = MySQLDump(defaults_file, 
                                  mysqldump_bin, 
                                  extra_defaults=extra_defaults)
        except MySQLDumpError, exc:
            raise BackupError(str(exc))

        LOG.info("mysqldump version %s", '.'.join([str(digit)
                for digit in mysqldump.version]))
        options = collect_mysqldump_options(config, mysqldump, self.client)
        validate_mysqldump_options(mysqldump, options)

        os.mkdir(os.path.join(self.target_directory, 'backup_data'))

        if self.config['compression']['method'] != 'none' and \
            self.config['compression']['level'] > 0:
            try:
                cmd, ext = lookup_compression(self.config['compression']['method'])
            except OSError, exc:
                raise BackupError("Unable to load compression method '%s': %s" %
                                  (self.config['compression']['method'], exc))
            LOG.info("Using %s compression level %d with args %s",
                     self.config['compression']['method'],
                     self.config['compression']['level'],
                     self.config['compression']['options'])
        else:
            LOG.info("Not compressing mysqldump output")
            cmd = ''
            ext = ''

        try:
            start(mysqldump=mysqldump,
                  schema=self.schema,
                  lock_method=config['lock-method'],
                  file_per_database=config['file-per-database'],
                  open_stream=self._open_stream,
                  compression_ext=ext)
        except MySQLDumpError, exc:
            raise BackupError(str(exc))

    def _open_stream(self, path, mode, method=None):
        """Open a stream through the holland compression api, relative to
        this instance's target directory
        """
        path = os.path.join(self.target_directory, 'backup_data', path)
        compression_method = method or self.config['compression']['method']
        compression_level = self.config['compression']['level']
        compression_options = self.config['compression']['options']
        stream = open_stream(path,
                             mode,
                             compression_method,
                             compression_level,
                             extra_args=compression_options)
        return stream

    def info(self):
        """Summarize information about this backup"""
        import textwrap
        return textwrap.dedent("""
        lock-method         = %s
        file-per-database   = %s

        Options used:
        flush-logs          = %s
        flush-privileges    = %s
        routines            = %s
        events              = %s

        Schema Filters:
        databases           = %s
        exclude-databases   = %s
        tables              = %s
        exclude-tables      = %s
        """).strip() % (
            self.config['mysqldump']['lock-method'],
            self.config['mysqldump']['file-per-database'] and 'yes' or 'no',
            self.config['mysqldump']['flush-logs'],
            self.config['mysqldump']['flush-privileges'],
            self.config['mysqldump']['dump-routines'],
            self.config['mysqldump']['dump-events'],
            ','.join(self.config['mysqldump']['databases']),
            ','.join(self.config['mysqldump']['exclude-databases']),
            ','.join(self.config['mysqldump']['tables']),
            ','.join(self.config['mysqldump']['exclude-tables'])
        )


def find_mysqldump(path=None):
    """Find a usable mysqldump binary in path or ENV[PATH]"""
    search_path = ':'.join(path) or os.environ.get('PATH', '')
    for _path in search_path.split(':'):
        if os.path.isfile(_path):
            return os.path.realpath(_path)
        if os.path.exists(os.path.join(_path, 'mysqldump')):
            return os.path.realpath(os.path.join(_path, 'mysqldump'))
    raise BackupError("Failed to find mysqldump in %s" % search_path)

def collect_mysqldump_options(config, mysqldump, client):
    """Do intelligent collection of mysqldump options from the config
    and add any additional options for further validation"""
    options = []
    if config['flush-logs']:
        options.append('--flush-logs')
    if config['flush-privileges']:
        if mysqldump.version < (5,0,26):
            LOG.warning("--flush privileges is available only for mysqldump "
                        "in 5.0.26+")
        else:
            options.append('--flush-privileges')
    if config['dump-routines']:
        if mysqldump.version < (5, 0, 13):
            LOG.warning("--routines is not available before mysqldump 5.0.13+")
        else:
            if mysqldump.version < (5, 0, 20):
                LOG.warning("mysqldump will not dump DEFINER values before "
                            "version 5.0.20.  You are running mysqldump from "
                            "version %s", mysqldump.version_str)
            options.append('--routines')
    if config['dump-events']:
        if mysqldump.version < (5, 1, 8):
            LOG.warning("--events only available for mysqldump 5.1.8+. skipping")
        else:
            options.append('--events')
    if config['max-allowed-packet']:
        options.append('--max-allowed-packet=' + config['max-allowed-packet'])
    if config['bin-log-position']:
        if client.show_variable('log_bin') != 'ON':
            raise BackupError("bin-log-position requested but "
                              "bin-log on server not active")
        options.append('--master-data=2')
    options.extend(config['additional-options'])
    return options

def validate_mysqldump_options(mysqldump, options):
    """Validate and add the requested options to the mysqldump instance"""
    error = False
    options = [opt for opt in options if opt]
    for option in options:
        try:
            mysqldump.add_option(option)
            LOG.info("Using mysqldump option %s", option)
        except MyOptionError, exc:
            LOG.warning(str(exc))


def _stop_slave(client, config=None):
    """Stop MySQL replication"""
    try:
        client.stop_slave(sql_thread_only=True)
        LOG.info("Stopped slave")
    except MySQLError, exc:
        raise BackupError("Failed to stop slave[%d]: %s" % exc.args)
    if config is not None:
        try:
            slave_info = client.show_slave_status()
            if slave_info:
                # update config with replication info
                config['slave_master_log_pos'] = slave_info['exec_master_log_pos']
                config['slave_master_log_file'] = slave_info['relay_master_log_file']
        except MySQLError, exc:
            raise BackupError("Failed to acquire slave status[%d]: %s" % \
                                exc.args)
        try:
            master_info = client.show_master_status()
            if master_info:
                config['master_log_file'] = master_info['file']
                config['master_log_pos'] = master_info['position']
        except MySQLError, exc:
            raise BackupError("Failed to acquire master status [%d] %s" % exc.args)

    LOG.info("MySQL Replication has been stopped.")

def _start_slave(client, config=None):
    """Start MySQL replication"""
    try:
        slave_info = client.show_slave_status()
        if slave_info and slave_info['exec_master_log_pos'] != config['slave_master_log_pos']:
            LOG.warning("ALERT! Slave position changed during backup")
    except MySQLError, exc:
        LOG.warning("Failed to sanity check replication[%d]: %s",
                         *exc.args)

    try:
        master_info = client.show_master_status()
        if master_info and master_info['position'] != config['master_log_pos']:
            LOG.warning("Sanity check on master status failed.  "
                    "Previously recorded %s:%s but currently found %s:%s",
                    config['master_log_file'], config['master_log_pos'],
                    master_info['file'], master_info['position'])
            LOG.warning("ALERT! Binary log position changed during backup!")
    except MySQLError, exc:
        LOG.warning("Failed to sanity check master status. [%d] %s", *exc.args)

    try:
        client.start_slave()
        LOG.info("Restarted slave")
    except MySQLError, exc:
        raise BackupError("Failed to restart slave [%d] %s" % exc.args)

def exclude_invalid_views(schema, client, definitions_file):
    """Flag invalid MySQL views as excluded to skip them during a mysqldump
    """
    sqlf = open(definitions_file, 'w')
    LOG.info("* Invalid and excluded views will be saved to %s",
            definitions_file)
    cursor = client.cursor()
    try:
        print >>sqlf, "--"
        print >>sqlf, "-- DDL of Invalid Views"
        print >>sqlf, "-- Created automatically by Holland"
        print >>sqlf, "--"
        print >>sqlf
        for db in schema.databases:
            if db.excluded:
                continue
            for table in db.tables:
                if table.excluded:
                    continue
                if table.engine != 'view':
                    continue
                LOG.debug("Testing view %s.%s", db.name, table.name)
                invalid_view = False
                try:
                    cursor.execute('SHOW FIELDS FROM `%s`.`%s`' %
                                    (db.name, table.name))
                    # check for missing definers that would bork
                    # lock-tables
                    for _, error_code, msg in client.show_warnings():
                        if error_code == 1449: # ER_NO_SUCH_USER
                            raise MySQLError(error_code, msg)
                except MySQLError, exc:
                    # 1356 = View references invalid table(s)...
                    if exc.args[0] in (1356, 1142, 1143, 1449):
                        invalid_view = True
                    else:
                        raise
                if invalid_view:
                    LOG.warning("* Excluding invalid view `%s`.`%s`: [%d] %s",
                                db.name, table.name, *exc.args)
                    table.excluded = True
                    view_definition = client.show_create_view(db.name,
                                                              table.name,
                                                              use_information_schema=True)
                    if view_definition is None:
                        LOG.error("!!! Failed to retrieve view definition for "
                                  "`%s`.`%s`", db.name, table.name)
                        LOG.warning("!!! View definition for `%s`.`%s` will "
                                    "not be included in this backup", db.name,
                                    table.name)
                        continue

                    LOG.info("* Saving view definition for "
                                 "`%s`.`%s`",
                                 db.name, table.name)
                    print >>sqlf, "--"
                    print >>sqlf, "-- Current View: `%s`.`%s`" % \
                    (db.name, table.name)
                    print >>sqlf, "--"
                    print >>sqlf
                    print >>sqlf, view_definition + ';'
                    print >>sqlf
    finally:
        sqlf.close()

def add_exclusions(schema, config):
    """Given a MySQLSchema add --ignore-table options in a [mysqldump]
    section for any excluded tables.

    """

    exclusions = []
    for db in schema.databases:
        if db.excluded:
            continue
        for table in db.tables:
            if table.excluded:
                LOG.info("Excluding table %s.%s", table.database, table.name)
                exclusions.append("ignore-table = " + table.database + '.' + table.name)

    if not exclusions:
        return

    try:
        my_cnf = codecs.open(config, 'a', 'utf8')
        print >>my_cnf
        print >>my_cnf, "[mysqldump]"
        for excl in exclusions:
            print >>my_cnf, excl
        my_cnf.close()
    except IOError, exc:
        LOG.error("Failed to write ignore-table exclusions to %s", config)
        raise

def parse_size(units_string):
    """Parse a MySQL-like size string into bytes

    >> parse_size('4G')
    4294967296
    """

    units_string = str(units_string)

    units = "kKmMgGtTpPeE"

    match = re.match(r'^(\d+(?:[.]\d+)?)([%s])$' % units, units_string)
    if not match:
        raise ValueError("Invalid constant size syntax %r" % units_string)
    number, unit = match.groups()
    unit = unit.upper()

    exponent = "KMGTPE".find(unit)

    return int(float(number) * 1024 ** (exponent + 1))

########NEW FILE########
__FILENAME__ = compat
# Copyright (c) 2001, 2002, 2003 Python Software Foundation
# Copyright (c) 2004-2008 Paramjit Oberoi <param.cs.wisc.edu>
# All Rights Reserved.  See LICENSE-PSF & LICENSE for details.

"""Compatibility interfaces for ConfigParser

Interfaces of ConfigParser, RawConfigParser and SafeConfigParser
should be completely identical to the Python standard library
versions.  Tested with the unit tests included with Python-2.3.4

The underlying INIConfig object can be accessed as cfg.data
"""

import re
from ConfigParser import DuplicateSectionError,    \
                  NoSectionError, NoOptionError,   \
                  InterpolationMissingOptionError, \
                  InterpolationDepthError,         \
                  InterpolationSyntaxError,        \
                  DEFAULTSECT, MAX_INTERPOLATION_DEPTH

# These are imported only for compatiability.
# The code below does not reference them directly.
from ConfigParser import Error, InterpolationError, \
                  MissingSectionHeaderError, ParsingError

import ini

class RawConfigParser(object):
    def __init__(self, defaults=None, dict_type=dict):
        if dict_type != dict:
            raise ValueError('Custom dict types not supported')
        self.data = ini.INIConfig(defaults=defaults, optionxformsource=self)

    def optionxform(self, optionstr):
        return optionstr.lower()

    def defaults(self):
        d = {}
        secobj = self.data._defaults
        for name in secobj._options:
            d[name] = secobj._compat_get(name)
        return d

    def sections(self):
        """Return a list of section names, excluding [DEFAULT]"""
        return list(self.data)

    def add_section(self, section):
        """Create a new section in the configuration.

        Raise DuplicateSectionError if a section by the specified name
        already exists.  Raise ValueError if name is DEFAULT or any of
        its case-insensitive variants.
        """
        # The default section is the only one that gets the case-insensitive
        # treatment - so it is special-cased here.
        if section.lower() == "default":
            raise ValueError, 'Invalid section name: %s' % section

        if self.has_section(section):
            raise DuplicateSectionError(section)
        else:
            self.data._new_namespace(section)

    def has_section(self, section):
        """Indicate whether the named section is present in the configuration.

        The DEFAULT section is not acknowledged.
        """
        try:
            self.data[section]
            return True
        except KeyError:
            return False

    def options(self, section):
        """Return a list of option names for the given section name."""
        try:
            return list(self.data[section])
        except KeyError:
            raise NoSectionError(section)

    def read(self, filenames):
        """Read and parse a filename or a list of filenames.

        Files that cannot be opened are silently ignored; this is
        designed so that you can specify a list of potential
        configuration file locations (e.g. current directory, user's
        home directory, systemwide directory), and all existing
        configuration files in the list will be read.  A single
        filename may also be given.
        """
        files_read = []
        if isinstance(filenames, basestring):
            filenames = [filenames]
        for filename in filenames:
            try:
                fp = open(filename)
            except IOError:
                continue
            files_read.append(filename)
            self.data._readfp(fp)
            fp.close()
        return files_read

    def readfp(self, fp, filename=None):
        """Like read() but the argument must be a file-like object.

        The `fp' argument must have a `readline' method.  Optional
        second argument is the `filename', which if not given, is
        taken from fp.name.  If fp has no `name' attribute, `<???>' is
        used.
        """
        self.data._readfp(fp)

    def get(self, section, option, vars=None):
        if not self.has_section(section):
            raise NoSectionError(section)
        if vars is not None and option in vars:
            value = vars[option]
        try:
            sec = self.data[section]
            return sec._compat_get(option)
        except KeyError:
            raise NoOptionError(option, section)

    def items(self, section):
        try:
            ans = []
            for opt in self.data[section]:
                ans.append((opt, self.get(section, opt)))
            return ans
        except KeyError:
            raise NoSectionError(section)

    def getint(self, section, option):
        return int(self.get(section, option))

    def getfloat(self, section, option):
        return float(self.get(section, option))

    _boolean_states = {'1': True, 'yes': True, 'true': True, 'on': True,
                       '0': False, 'no': False, 'false': False, 'off': False}

    def getboolean(self, section, option):
        v = self.get(section, option)
        if v.lower() not in self._boolean_states:
            raise ValueError, 'Not a boolean: %s' % v
        return self._boolean_states[v.lower()]

    def has_option(self, section, option):
        """Check for the existence of a given option in a given section."""
        try:
            sec = self.data[section]
        except KeyError:
            raise NoSectionError(section)
        try:
            sec[option]
            return True
        except KeyError:
            return False

    def set(self, section, option, value):
        """Set an option."""
        try:
            self.data[section][option] = value
        except KeyError:
            raise NoSectionError(section)

    def write(self, fp):
        """Write an .ini-format representation of the configuration state."""
        fp.write(str(self.data))

    def remove_option(self, section, option):
        """Remove an option."""
        try:
            sec = self.data[section]
        except KeyError:
            raise NoSectionError(section)
        try:
            sec[option]
            del sec[option]
            return 1
        except KeyError:
            return 0

    def remove_section(self, section):
        """Remove a file section."""
        if not self.has_section(section):
            return False
        del self.data[section]
        return True


class ConfigDict(object):
    """Present a dict interface to a ini section."""

    def __init__(self, cfg, section, vars):
        self.cfg = cfg
        self.section = section
        self.vars = vars

    def __getitem__(self, key):
        try:
            return RawConfigParser.get(self.cfg, self.section, key, self.vars)
        except (NoOptionError, NoSectionError):
            raise KeyError(key)


class ConfigParser(RawConfigParser):

    def get(self, section, option, raw=False, vars=None):
        """Get an option value for a given section.

        All % interpolations are expanded in the return values, based on the
        defaults passed into the constructor, unless the optional argument
        `raw' is true.  Additional substitutions may be provided using the
        `vars' argument, which must be a dictionary whose contents overrides
        any pre-existing defaults.

        The section DEFAULT is special.
        """
        if section != DEFAULTSECT and not self.has_section(section):
            raise NoSectionError(section)

        option = self.optionxform(option)
        value = RawConfigParser.get(self, section, option, vars)

        if raw:
            return value
        else:
            d = ConfigDict(self, section, vars)
            return self._interpolate(section, option, value, d)

    def _interpolate(self, section, option, rawval, vars):
        # do the string interpolation
        value = rawval
        depth = MAX_INTERPOLATION_DEPTH
        while depth:                    # Loop through this until it's done
            depth -= 1
            if "%(" in value:
                try:
                    value = value % vars
                except KeyError, e:
                    raise InterpolationMissingOptionError(
                        option, section, rawval, e.args[0])
            else:
                break
        if value.find("%(") != -1:
            raise InterpolationDepthError(option, section, rawval)
        return value

    def items(self, section, raw=False, vars=None):
        """Return a list of tuples with (name, value) for each option
        in the section.

        All % interpolations are expanded in the return values, based on the
        defaults passed into the constructor, unless the optional argument
        `raw' is true.  Additional substitutions may be provided using the
        `vars' argument, which must be a dictionary whose contents overrides
        any pre-existing defaults.

        The section DEFAULT is special.
        """
        if section != DEFAULTSECT and not self.has_section(section):
            raise NoSectionError(section)
        if vars is None:
            options = list(self.data[section])
        else:
            options = []
            for x in self.data[section]:
                if x not in vars:
                    options.append(x)
            options.extend(vars.keys())

        if "__name__" in options:
            options.remove("__name__")

        d = ConfigDict(self, section, vars)
        if raw:
            return [(option, d[option])
                    for option in options]
        else:
            return [(option, self._interpolate(section, option, d[option], d))
                    for option in options]


class SafeConfigParser(ConfigParser):
    _interpvar_re = re.compile(r"%\(([^)]+)\)s")
    _badpercent_re = re.compile(r"%[^%]|%$")

    def set(self, section, option, value):
        if not isinstance(value, basestring):
            raise TypeError("option values must be strings")
        # check for bad percent signs:
        # first, replace all "good" interpolations
        tmp_value = self._interpvar_re.sub('', value)
        # then, check if there's a lone percent sign left
        m = self._badpercent_re.search(tmp_value)
        if m:
            raise ValueError("invalid interpolation syntax in %r at "
                             "position %d" % (value, m.start()))

        ConfigParser.set(self, section, option, value)

    def _interpolate(self, section, option, rawval, vars):
        # do the string interpolation
        L = []
        self._interpolate_some(option, L, rawval, section, vars, 1)
        return ''.join(L)

    _interpvar_match = re.compile(r"%\(([^)]+)\)s").match

    def _interpolate_some(self, option, accum, rest, section, map, depth):
        if depth > MAX_INTERPOLATION_DEPTH:
            raise InterpolationDepthError(option, section, rest)
        while rest:
            p = rest.find("%")
            if p < 0:
                accum.append(rest)
                return
            if p > 0:
                accum.append(rest[:p])
                rest = rest[p:]
            # p is no longer used
            c = rest[1:2]
            if c == "%":
                accum.append("%")
                rest = rest[2:]
            elif c == "(":
                m = self._interpvar_match(rest)
                if m is None:
                    raise InterpolationSyntaxError(option, section,
                        "bad interpolation variable reference %r" % rest)
                var = m.group(1)
                rest = rest[m.end():]
                try:
                    v = map[var]
                except KeyError:
                    raise InterpolationMissingOptionError(
                        option, section, rest, var)
                if "%" in v:
                    self._interpolate_some(option, accum, v,
                                           section, map, depth + 1)
                else:
                    accum.append(v)
            else:
                raise InterpolationSyntaxError(
                    option, section,
                    "'%' must be followed by '%' or '(', found: " + repr(rest))

########NEW FILE########
__FILENAME__ = config
class ConfigNamespace(object):
    """Abstract class representing the interface of Config objects.

    A ConfigNamespace is a collection of names mapped to values, where
    the values may be nested namespaces.  Values can be accessed via
    container notation - obj[key] - or via dotted notation - obj.key.
    Both these access methods are equivalent.

    To minimize name conflicts between namespace keys and class members,
    the number of class members should be minimized, and the names of
    all class members should start with an underscore.

    Subclasses must implement the methods for container-like access,
    and this class will automatically provide dotted access.

    """

    # Methods that must be implemented by subclasses

    def __getitem__(self, key):
        return NotImplementedError(key)

    def __setitem__(self, key, value):
        raise NotImplementedError(key, value)

    def __delitem__(self, key):
        raise NotImplementedError(key)

    def __iter__(self):
        return NotImplementedError()

    def _new_namespace(self, name):
        raise NotImplementedError(name)

    # Machinery for converting dotted access into contained access
    #
    # To distinguish between accesses of class members and namespace
    # keys, we first call object.__getattribute__().  If that succeeds,
    # the name is assumed to be a class member.  Otherwise it is
    # treated as a namespace key.
    #
    # Therefore, member variables should be defined in the class,
    # not just in the __init__() function.  See BasicNamespace for
    # an example.

    def __getattr__(self, name):
        try:
            return self.__getitem__(name)
        except KeyError:
            return Undefined(name, self)

    def __setattr__(self, name, value):
        try:
            object.__getattribute__(self, name)
            object.__setattr__(self, name, value)
        except AttributeError:
            self.__setitem__(name, value)

    def __delattr__(self, name):
        try:
            object.__getattribute__(self, name)
            object.__delattr__(self, name)
        except AttributeError:
            self.__delitem__(name)

    def __getstate__(self):
        return self.__dict__

    def __setstate__(self, state):
        self.__dict__.update(state)

class Undefined(object):
    """Helper class used to hold undefined names until assignment.

    This class helps create any undefined subsections when an
    assignment is made to a nested value.  For example, if the
    statement is "cfg.a.b.c = 42", but "cfg.a.b" does not exist yet.
    """

    def __init__(self, name, namespace):
        object.__setattr__(self, 'name', name)
        object.__setattr__(self, 'namespace', namespace)

    def __setattr__(self, name, value):
        obj = self.namespace._new_namespace(self.name)
        obj[name] = value


# ---- Basic implementation of a ConfigNamespace

class BasicConfig(ConfigNamespace):
    """Represents a hierarchical collection of named values.

    Values are added using dotted notation:

    >>> n = BasicConfig()
    >>> n.x = 7
    >>> n.name.first = 'paramjit'
    >>> n.name.last = 'oberoi'

    ...and accessed the same way, or with [...]:

    >>> n.x
    7
    >>> n.name.first
    'paramjit'
    >>> n.name.last
    'oberoi'
    >>> n['x']
    7
    >>> n['name']['first']
    'paramjit'

    Iterating over the namespace object returns the keys:

    >>> l = list(n)
    >>> l.sort()
    >>> l
    ['name', 'x']

    Values can be deleted using 'del' and printed using 'print'.

    >>> n.aaa = 42
    >>> del n.x
    >>> print n
    aaa = 42
    name.first = paramjit
    name.last = oberoi

    Nested namepsaces are also namespaces:

    >>> isinstance(n.name, ConfigNamespace)
    True
    >>> print n.name
    first = paramjit
    last = oberoi
    >>> sorted(list(n.name))
    ['first', 'last']

    Finally, values can be read from a file as follows:

    >>> from StringIO import StringIO
    >>> sio = StringIO('''
    ... # comment
    ... ui.height = 100
    ... ui.width = 150
    ... complexity = medium
    ... have_python
    ... data.secret.password = goodness=gracious me
    ... ''')
    >>> n = BasicConfig()
    >>> n._readfp(sio)
    >>> print n
    complexity = medium
    data.secret.password = goodness=gracious me
    have_python
    ui.height = 100
    ui.width = 150
    """

    # this makes sure that __setattr__ knows this is not a namespace key
    _data = None

    def __init__(self):
        self._data = {}

    def __getitem__(self, key):
        return self._data[key]

    def __setitem__(self, key, value):
        self._data[key] = value

    def __delitem__(self, key):
        del self._data[key]

    def __iter__(self):
        return iter(self._data)

    def __str__(self, prefix=''):
        lines = []
        keys = self._data.keys()
        keys.sort()
        for name in keys:
            value = self._data[name]
            if isinstance(value, ConfigNamespace):
                lines.append(value.__str__(prefix='%s%s.' % (prefix,name)))
            else:
                if value is None:
                    lines.append('%s%s' % (prefix, name))
                else:
                    lines.append('%s%s = %s' % (prefix, name, value))
        return '\n'.join(lines)

    def _new_namespace(self, name):
        obj = BasicConfig()
        self._data[name] = obj
        return obj

    def _readfp(self, fp):
        while True:
            line = fp.readline()
            if not line:
                break

            line = line.strip()
            if not line: continue
            if line[0] == '#': continue
            data = line.split('=', 1)
            if len(data) == 1:
                name = line
                value = None
            else:
                name = data[0].strip()
                value = data[1].strip()
            name_components = name.split('.')
            ns = self
            for n in name_components[:-1]:
                try:
                    ns = ns[n]
                    if not isinstance(ns, ConfigNamespace):
                        raise TypeError('value-namespace conflict', n)
                except KeyError:
                    ns = ns._new_namespace(n)
            ns[name_components[-1]] = value


# ---- Utility functions

def update_config(target, source):
    """Imports values from source into target.

    Recursively walks the <source> ConfigNamespace and inserts values
    into the <target> ConfigNamespace.  For example:

    >>> n = BasicConfig()
    >>> n.playlist.expand_playlist = True
    >>> n.ui.display_clock = True
    >>> n.ui.display_qlength = True
    >>> n.ui.width = 150
    >>> print n
    playlist.expand_playlist = True
    ui.display_clock = True
    ui.display_qlength = True
    ui.width = 150

    >>> from iniparse import ini
    >>> i = ini.INIConfig()
    >>> update_config(i, n)
    >>> print i
    [playlist]
    expand_playlist = True
    <BLANKLINE>
    [ui]
    display_clock = True
    display_qlength = True
    width = 150

    """
    for name in source:
        value = source[name]
        if isinstance(value, ConfigNamespace):
            try:
                myns = target[name]
                if not isinstance(myns, ConfigNamespace):
                    raise TypeError('value-namespace conflict')
            except KeyError:
                myns = target._new_namespace(name)
            update_config(myns, value)
        else:
            target[name] = value




########NEW FILE########
__FILENAME__ = ini
"""Access and/or modify INI files

* Compatiable with ConfigParser
* Preserves order of sections & options
* Preserves comments/blank lines/etc
* More conveninet access to data

Example:

    >>> from StringIO import StringIO
    >>> sio = StringIO('''# configure foo-application
    ... [foo]
    ... bar1 = qualia
    ... bar2 = 1977
    ... [foo-ext]
    ... special = 1''')

    >>> cfg = INIConfig(sio)
    >>> print cfg.foo.bar1
    qualia
    >>> print cfg['foo-ext'].special
    1
    >>> cfg.foo.newopt = 'hi!'

    >>> print cfg
    # configure foo-application
    [foo]
    bar1 = qualia
    bar2 = 1977
    newopt = hi!
    [foo-ext]
    special = 1

"""

# An ini parser that supports ordered sections/options
# Also supports updates, while preserving structure
# Backward-compatiable with ConfigParser
try:
    set
except NameError:
    from sets import Set as set

import re
from ConfigParser import DEFAULTSECT, ParsingError, MissingSectionHeaderError

import config

class LineType(object):
    line = None

    def __init__(self, line=None):
        if line is not None:
            self.line = line.strip('\n')

    # Return the original line for unmodified objects
    # Otherwise construct using the current attribute values
    def __str__(self):
        if self.line is not None:
            return self.line
        else:
            return self.to_string()

    # If an attribute is modified after initialization
    # set line to None since it is no longer accurate.
    def __setattr__(self, name, value):
        if hasattr(self,name):
            self.__dict__['line'] = None
        self.__dict__[name] = value

    def to_string(self):
        raise Exception('This method must be overridden in derived classes')


class SectionLine(LineType):
    regex =  re.compile(r'^\['
                        r'(?P<name>[^]]+)'
                        r'\]\s*'
                        r'((?P<csep>;|#)(?P<comment>.*))?$')

    def __init__(self, name, comment=None, comment_separator=None,
                             comment_offset=-1, line=None):
        super(SectionLine, self).__init__(line)
        self.name = name
        self.comment = comment
        self.comment_separator = comment_separator
        self.comment_offset = comment_offset

    def to_string(self):
        out = '[' + self.name + ']'
        if self.comment is not None:
            # try to preserve indentation of comments
            out = (out+' ').ljust(self.comment_offset)
            out = out + self.comment_separator + self.comment
        return out

    def parse(cls, line):
        m = cls.regex.match(line.rstrip())
        if m is None:
            return None
        return cls(m.group('name'), m.group('comment'),
                   m.group('csep'), m.start('csep'),
                   line)
    parse = classmethod(parse)


class OptionLine(LineType):
    def __init__(self, name, value, separator=' = ', comment=None,
                 comment_separator=None, comment_offset=-1, line=None):
        super(OptionLine, self).__init__(line)
        self.name = name
        self.value = value
        self.separator = separator
        self.comment = comment
        self.comment_separator = comment_separator
        self.comment_offset = comment_offset

    def to_string(self):
        out = '%s%s%s' % (self.name, self.separator, self.value)
        if self.comment is not None:
            # try to preserve indentation of comments
            out = (out+' ').ljust(self.comment_offset)
            out = out + self.comment_separator + self.comment
        return out

    regex = re.compile(r'^(?P<name>[^:=\s[][^:=]*)'
                       r'(?P<sep>[:=]\s*)'
                       r'(?P<value>.*)$')

    def parse(cls, line):
        m = cls.regex.match(line.rstrip())
        if m is None:
            return None

        name = m.group('name').rstrip()
        value = m.group('value')
        sep = m.group('name')[len(name):] + m.group('sep')

        # comments are not detected in the regex because
        # ensuring total compatibility with ConfigParser
        # requires that:
        #     option = value    ;comment   // value=='value'
        #     option = value;1  ;comment   // value=='value;1  ;comment'
        #
        # Doing this in a regex would be complicated.  I
        # think this is a bug.  The whole issue of how to
        # include ';' in the value needs to be addressed.
        # Also, '#' doesn't mark comments in options...

        coff = value.find(';')
        if coff != -1 and value[coff-1].isspace():
            comment = value[coff+1:]
            csep = value[coff]
            value = value[:coff].rstrip()
            coff = m.start('value') + coff
        else:
            comment = None
            csep = None
            coff = -1

        return cls(name, value, sep, comment, csep, coff, line)
    parse = classmethod(parse)


class CommentLine(LineType):
    regex = re.compile(r'^(?P<csep>[;#]|[rR][eE][mM])'
                       r'(?P<comment>.*)$')

    def __init__(self, comment='', separator='#', line=None):
        super(CommentLine, self).__init__(line)
        self.comment = comment
        self.separator = separator

    def to_string(self):
        return self.separator + self.comment

    def parse(cls, line):
        m = cls.regex.match(line.rstrip())
        if m is None:
            return None
        return cls(m.group('comment'), m.group('csep'), line)
    parse = classmethod(parse)


class EmptyLine(LineType):
    # could make this a singleton
    def to_string(self):
        return ''

    value = property(lambda _: '')

    def parse(cls, line):
        if line.strip(): return None
        return cls(line)
    parse = classmethod(parse)


class ContinuationLine(LineType):
    regex = re.compile(r'^\s+(?P<value>.*)$')

    def __init__(self, value, value_offset=None, line=None):
        super(ContinuationLine, self).__init__(line)
        self.value = value
        if value_offset is None:
            value_offset = 8
        self.value_offset = value_offset

    def to_string(self):
        return ' '*self.value_offset + self.value

    def parse(cls, line):
        m = cls.regex.match(line.rstrip())
        if m is None:
            return None
        return cls(m.group('value'), m.start('value'), line)
    parse = classmethod(parse)


class LineContainer(object):
    def __init__(self, d=None):
        self.contents = []
        self.orgvalue = None
        if d:
            if isinstance(d, list): self.extend(d)
            else: self.add(d)

    def add(self, x):
        self.contents.append(x)

    def extend(self, x):
        for i in x: self.add(i)

    def get_name(self):
        return self.contents[0].name

    def set_name(self, data):
        self.contents[0].name = data

    def get_value(self):
        if self.orgvalue is not None:
            return self.orgvalue
        elif len(self.contents) == 1:
            return self.contents[0].value
        else:
            return '\n'.join([('%s' % x.value) for x in self.contents
                              if not isinstance(x, CommentLine)])

    def set_value(self, data):
        self.orgvalue = data
        lines = ('%s' % data).split('\n')

        # If there is an existing ContinuationLine, use its offset
        value_offset = None
        for v in self.contents:
            if isinstance(v, ContinuationLine):
                value_offset = v.value_offset
                break

        # Rebuild contents list, preserving initial OptionLine
        self.contents = self.contents[0:1]
        self.contents[0].value = lines[0]
        del lines[0]
        for line in lines:
            if line.strip():
                self.add(ContinuationLine(line, value_offset))
            else:
                self.add(EmptyLine())

    name = property(get_name, set_name)
    value = property(get_value, set_value)

    def __str__(self):
        s = [x.__str__() for x in self.contents]
        return '\n'.join(s)

    def finditer(self, key):
        for x in self.contents[::-1]:
            if hasattr(x, 'name') and x.name==key:
                yield x

    def find(self, key):
        for x in self.finditer(key):
            return x
        raise KeyError(key)


def _make_xform_property(myattrname, srcattrname=None):
    private_attrname = myattrname + 'value'
    private_srcname = myattrname + 'source'
    if srcattrname is None:
        srcattrname = myattrname

    def getfn(self):
        srcobj = getattr(self, private_srcname)
        if srcobj is not None:
            return getattr(srcobj, srcattrname)
        else:
            return getattr(self, private_attrname)

    def setfn(self, value):
        srcobj = getattr(self, private_srcname)
        if srcobj is not None:
            setattr(srcobj, srcattrname, value)
        else:
            setattr(self, private_attrname, value)

    return property(getfn, setfn)


class INISection(config.ConfigNamespace):
    _lines = None
    _options = None
    _defaults = None
    _optionxformvalue = None
    _optionxformsource = None
    _compat_skip_empty_lines = set()
    def __init__(self, lineobj, defaults = None,
                       optionxformvalue=None, optionxformsource=None):
        self._lines = [lineobj]
        self._defaults = defaults
        self._optionxformvalue = optionxformvalue
        self._optionxformsource = optionxformsource
        self._options = {}

    _optionxform = _make_xform_property('_optionxform')

    def _compat_get(self, key):
        # identical to __getitem__ except that _compat_XXX
        # is checked for backward-compatible handling
        if key == '__name__':
            return self._lines[-1].name
        if self._optionxform: key = self._optionxform(key)
        try:
            value = self._options[key].value
            del_empty = key in self._compat_skip_empty_lines
        except KeyError:
            if self._defaults and key in self._defaults._options:
                value = self._defaults._options[key].value
                del_empty = key in self._defaults._compat_skip_empty_lines
            else:
                raise
        if del_empty:
            value = re.sub('\n+', '\n', value)
        return value

    def __getitem__(self, key):
        if key == '__name__':
            return self._lines[-1].name
        if self._optionxform: key = self._optionxform(key)
        try:
            return self._options[key].value
        except KeyError:
            if self._defaults and key in self._defaults._options:
                return self._defaults._options[key].value
            else:
                raise

    def __setitem__(self, key, value):
        if self._optionxform: xkey = self._optionxform(key)
        else: xkey = key
        if xkey in self._compat_skip_empty_lines:
            self._compat_skip_empty_lines.remove(xkey)
        if xkey not in self._options:
            # create a dummy object - value may have multiple lines
            obj = LineContainer(OptionLine(key, ''))
            self._lines[-1].add(obj)
            self._options[xkey] = obj
        # the set_value() function in LineContainer
        # automatically handles multi-line values
        self._options[xkey].value = value

    def __delitem__(self, key):
        if self._optionxform: key = self._optionxform(key)
        if key in self._compat_skip_empty_lines:
            self._compat_skip_empty_lines.remove(key)
        for l in self._lines:
            remaining = []
            for o in l.contents:
                if isinstance(o, LineContainer):
                    n = o.name
                    if self._optionxform: n = self._optionxform(n)
                    if key != n: remaining.append(o)
                else:
                    remaining.append(o)
            l.contents = remaining
        del self._options[key]

    def __iter__(self):
        d = set()
        for l in self._lines:
            for x in l.contents:
                if isinstance(x, LineContainer):
                    if self._optionxform:
                        ans = self._optionxform(x.name)
                    else:
                        ans = x.name
                    if ans not in d:
                        yield ans
                        d.add(ans)
        if self._defaults:
            for x in self._defaults:
                if x not in d:
                    yield x
                    d.add(x)

    def _new_namespace(self, name):
        raise Exception('No sub-sections allowed', name)


def make_comment(line):
    return CommentLine(line.rstrip('\n'))


def readline_iterator(f):
    """iterate over a file by only using the file object's readline method"""

    have_newline = False
    while True:
        line = f.readline()

        if not line:
            if have_newline:
                yield ""
            return

        if line.endswith('\n'):
            have_newline = True
        else:
            have_newline = False

        yield line


def lower(x):
    return x.lower()


class INIConfig(config.ConfigNamespace):
    _data = None
    _sections = None
    _defaults = None
    _optionxformvalue = None
    _optionxformsource = None
    _sectionxformvalue = None
    _sectionxformsource = None
    _parse_exc = None
    _bom = False
    def __init__(self, fp=None, defaults=None, parse_exc=True,
                 optionxformvalue=lower, optionxformsource=None,
                 sectionxformvalue=None, sectionxformsource=None):
        self._data = LineContainer()
        self._parse_exc = parse_exc
        self._optionxformvalue = optionxformvalue
        self._optionxformsource = optionxformsource
        self._sectionxformvalue = sectionxformvalue
        self._sectionxformsource = sectionxformsource
        self._sections = {}
        if defaults is None: defaults = {}
        self._defaults = INISection(LineContainer(), optionxformsource=self)
        for name, value in defaults.iteritems():
            self._defaults[name] = value
        if fp is not None:
            self._readfp(fp)

    _optionxform = _make_xform_property('_optionxform', 'optionxform')
    _sectionxform = _make_xform_property('_sectionxform', 'optionxform')

    def __getitem__(self, key):
        if key == DEFAULTSECT:
            return self._defaults
        if self._sectionxform: key = self._sectionxform(key)
        return self._sections[key]

    def __setitem__(self, key, value):
        raise Exception('Values must be inside sections', key, value)

    def __delitem__(self, key):
        if self._sectionxform: key = self._sectionxform(key)
        for line in self._sections[key]._lines:
            self._data.contents.remove(line)
        del self._sections[key]

    def __iter__(self):
        d = set()
        d.add(DEFAULTSECT)
        for x in self._data.contents:
            if isinstance(x, LineContainer):
                if x.name not in d:
                    yield x.name
                    d.add(x.name)

    def _new_namespace(self, name):
        if self._data.contents:
            self._data.add(EmptyLine())
        obj = LineContainer(SectionLine(name))
        self._data.add(obj)
        if self._sectionxform: name = self._sectionxform(name)
        if name in self._sections:
            ns = self._sections[name]
            ns._lines.append(obj)
        else:
            ns = INISection(obj, defaults=self._defaults,
                            optionxformsource=self)
            self._sections[name] = ns
        return ns

    def __str__(self):
        if self._bom:
            fmt = u'\ufeff%s'
        else:
            fmt = '%s'
        return fmt % self._data.__str__()

    __unicode__ = __str__

    _line_types = [EmptyLine, CommentLine,
                   SectionLine, OptionLine,
                   ContinuationLine]

    def _parse(self, line):
        for linetype in self._line_types:
            lineobj = linetype.parse(line)
            if lineobj:
                return lineobj
        else:
            # can't parse line
            return None

    def _readfp(self, fp):
        cur_section = None
        cur_option = None
        cur_section_name = None
        cur_option_name = None
        pending_lines = []
        pending_empty_lines = False
        try:
            fname = fp.name
        except AttributeError:
            fname = '<???>'
        linecount = 0
        exc = None
        line = None

        for line in readline_iterator(fp):
            # Check for BOM on first line
            if linecount == 0 and isinstance(line, unicode):
                if line[0] == u'\ufeff':
                    line = line[1:]
                    self._bom = True

            lineobj = self._parse(line)
            linecount += 1

            if not cur_section and not isinstance(lineobj,
                                (CommentLine, EmptyLine, SectionLine)):
                if self._parse_exc:
                    raise MissingSectionHeaderError(fname, linecount, line)
                else:
                    lineobj = make_comment(line)

            if lineobj is None:
                if self._parse_exc:
                    if exc is None: exc = ParsingError(fname)
                    exc.append(linecount, line)
                lineobj = make_comment(line)

            if isinstance(lineobj, ContinuationLine):
                if cur_option:
                    if pending_lines:
                        cur_option.extend(pending_lines)
                        pending_lines = []
                        if pending_empty_lines:
                            optobj._compat_skip_empty_lines.add(cur_option_name)
                            pending_empty_lines = False
                    cur_option.add(lineobj)
                else:
                    # illegal continuation line - convert to comment
                    if self._parse_exc:
                        if exc is None: exc = ParsingError(fname)
                        exc.append(linecount, line)
                    lineobj = make_comment(line)

            if isinstance(lineobj, OptionLine):
                if pending_lines:
                    cur_section.extend(pending_lines)
                    pending_lines = []
                    pending_empty_lines = False
                cur_option = LineContainer(lineobj)
                cur_section.add(cur_option)
                if self._optionxform:
                    cur_option_name = self._optionxform(cur_option.name)
                else:
                    cur_option_name = cur_option.name
                if cur_section_name == DEFAULTSECT:
                    optobj = self._defaults
                else:
                    optobj = self._sections[cur_section_name]
                optobj._options[cur_option_name] = cur_option

            if isinstance(lineobj, SectionLine):
                self._data.extend(pending_lines)
                pending_lines = []
                pending_empty_lines = False
                cur_section = LineContainer(lineobj)
                self._data.add(cur_section)
                cur_option = None
                cur_option_name = None
                if cur_section.name == DEFAULTSECT:
                    self._defaults._lines.append(cur_section)
                    cur_section_name = DEFAULTSECT
                else:
                    if self._sectionxform:
                        cur_section_name = self._sectionxform(cur_section.name)
                    else:
                        cur_section_name = cur_section.name
                    if cur_section_name not in self._sections:
                        self._sections[cur_section_name] = \
                                INISection(cur_section, defaults=self._defaults,
                                           optionxformsource=self)
                    else:
                        self._sections[cur_section_name]._lines.append(cur_section)

            if isinstance(lineobj, (CommentLine, EmptyLine)):
                pending_lines.append(lineobj)
                if isinstance(lineobj, EmptyLine):
                    pending_empty_lines = True

        self._data.extend(pending_lines)
        if line and line[-1]=='\n':
            self._data.add(EmptyLine())

        if exc:
            raise exc


########NEW FILE########
__FILENAME__ = test_main
import locale
from holland.backup.mysqldump.mock.env import MockEnvironment
from holland.backup.mysqldump.core import start

__test__ = False

STD_OPTIONS = {
    'stop_slave' : True,
    'defaults_file' : '~/.my.cnf',
    'extra_options' : '',
    'file_per_database' : False,
    'compression' : None,
    'exclude_tables' : ['mysql.user']
}

mock_env = MockEnvironment()

def setup():
    locale.setlocale(locale.LC_ALL, '')
    mock_env.replace_environment()

def teardown():
    mock_env.restore_environment()

def test_start():
    opts = dict(STD_OPTIONS)
    start(opts)

def test_start_trx():
    opts = dict(STD_OPTIONS)
    opts['include_databases'] = ['employees']
    start(opts)

def test_start_multiple():
    opts = dict(STD_OPTIONS)
    opts['file_per_database'] = True
    start(opts)

def test_start_mysqlauth():
    opts = dict(STD_OPTIONS)
    opts['socket'] = '/var/lib/mysql/mysql.sock'
    start(opts)

def test_start_no_mycnf():
    opts = dict(STD_OPTIONS)
    opts['defaults_file'] = None
    opts['user'] = 'root'
    opts['password'] = None
    opts['socket'] = '/var/lib/mysql/mysql.sock'
    start(opts)

def test_start_exclude_engines():
    opts = dict(STD_OPTIONS)
    opts['exclude_engines'] = ['myisam']
    start(opts)

########NEW FILE########
__FILENAME__ = test_mocker
import unittest
import tempfile
import inspect
import shutil
import sys
import os
import gc

from types import ModuleType

try:
    import coverage
except ImportError:
    coverage = None
else:
    # Start coverage check before importing from mocker, to get all of it.
    coverage.erase()
    coverage.start()

from holland.backup.mysqldump.mock.mocker import \
    MockerBase, Mocker, Mock, Event, Task, Action, Path, recorder, expect, \
    PathMatcher, path_matcher_recorder, RunCounter, ImplicitRunCounter, \
    run_counter_recorder, run_counter_removal_recorder, MockReturner, \
    mock_returner_recorder, FunctionRunner, Orderer, SpecChecker, \
    spec_checker_recorder, match_params, ANY, IS, CONTAINS, IN, MATCH, ARGS, \
    KWARGS, MatchError, PathExecuter, ProxyReplacer, Patcher, Undefined, \
    PatchedMethod, MockerTestCase, ReplayRestoreEvent, OnRestoreCaller


class TestCase(unittest.TestCase):
    """Python 2.3 lacked a couple of useful aliases."""
    
    assertTrue = unittest.TestCase.failUnless
    assertFalse = unittest.TestCase.failIf


class CleanMocker(MockerBase):
    """Just a better name for MockerBase in a testing context."""


class IntegrationTest(TestCase):

    def setUp(self):
        self.mocker = Mocker()

    def tearDown(self):
        self.mocker.restore()

    def test_count(self):
        obj = self.mocker.mock()
        obj.x
        self.mocker.count(2, 3)

        self.mocker.replay()
        obj.x
        self.assertRaises(AssertionError, self.mocker.verify)
        obj.x
        self.mocker.verify()
        obj.x
        self.mocker.verify()
        self.assertRaises(AssertionError, getattr, obj, "x")

    def test_order(self):
        obj = self.mocker.mock()

        with_manager = self.mocker.order()
        with_manager.__enter__()
        obj.x
        obj.y
        obj.z
        with_manager.__exit__(None, None, None)

        self.mocker.replay()
        self.assertRaises(AssertionError, getattr, obj, "y")
        self.assertRaises(AssertionError, getattr, obj, "z")

        self.mocker.replay()
        obj.x
        self.assertRaises(AssertionError, getattr, obj, "z")

        self.mocker.replay()
        obj.x
        obj.y
        obj.z

    def test_spec_and_type(self):
        class C(object):
            def m(self, a): pass
        
        obj = self.mocker.mock(C)

        obj.m(1)
        obj.m(a=1)
        obj.m(1, 2)
        obj.m(b=2)
        obj.x()
        obj.y()
        self.mocker.nospec()
        obj.z()

        self.mocker.replay()

        self.assertTrue(isinstance(obj, C))

        obj.m(1)
        obj.m(a=1)
        obj.y()
        self.assertRaises(AssertionError, obj.m, 1, 2)
        self.assertRaises(AssertionError, obj.m, b=2)
        self.assertRaises(AssertionError, obj.x)
        self.assertRaises(AssertionError, obj.z)

    def test_result(self):
        obj = self.mocker.mock()
        obj.x
        self.mocker.result(42)
        self.mocker.replay()
        self.assertEquals(obj.x, 42)

    def test_throw(self):
        obj = self.mocker.mock()
        obj.x()
        self.mocker.throw(ValueError)
        self.mocker.replay()
        self.assertRaises(ValueError, obj.x)

    def test_call(self):
        calls = []
        def func(arg):
            calls.append(arg)
            return 42
        obj = self.mocker.mock()
        obj.x(24)
        self.mocker.call(func)
        self.mocker.replay()
        self.assertEquals(obj.x(24), 42)
        self.assertEquals(calls, [24])

    def test_call_result(self):
        calls = []
        def func(arg):
            calls.append(arg)
            return arg
        obj = self.mocker.mock()
        obj.x(24)
        self.mocker.call(func)
        self.mocker.result(42)
        self.mocker.replay()
        self.assertEquals(obj.x(24), 42)
        self.assertEquals(calls, [24])

    def test_generate(self):
        obj = self.mocker.mock()
        obj.x(24)
        self.mocker.generate([1, 2, 3])
        self.mocker.replay()
        result = obj.x(24)
        def g(): yield None
        self.assertEquals(type(result), type(g()))
        self.assertEquals(list(result), [1, 2, 3])

    def test_proxy(self):
        class C(object):
            def sum(self, *args):
                return sum(args)
        
        obj = self.mocker.proxy(C())
        expect(obj.multiply(2, 3)).result(6).nospec()
        expect(obj.sum(0, 0)).result(1)
        expect(obj.sum(0, 0)).passthrough()

        self.mocker.replay()

        self.assertEquals(obj.multiply(2, 3), 6) # Mocked.
        self.assertRaises(AttributeError, obj.multiply) # Passed through.

        self.assertEquals(obj.sum(2, 3), 5) # Passed through.
        self.assertEquals(obj.sum(0, 0), 1) # Mocked.
        self.assertEquals(obj.sum(0, 0), 0) # Passed through explicitly.
        self.assertRaises(AssertionError, obj.sum, 0, 0) # Seen twice.

    def test_replace_install_and_restore(self):
        module = self.mocker.replace("calendar")
        import calendar
        self.assertTrue(calendar is not module)
        self.mocker.replay()
        import calendar
        self.assertTrue(calendar is module)
        self.mocker.restore()
        import calendar
        self.assertTrue(calendar is not module)

    def test_replace_os_path_join(self):
        path = self.mocker.replace("os.path")
        expect(path.join(ARGS)).call(lambda *args: "-".join(args))
        expect(path.join("e", ARGS)).passthrough()
        self.mocker.replay()
        import os
        self.assertEquals(os.path.join("a", "b", "c"), "a-b-c")
        self.assertNotEquals(os.path.join("e", "f", "g"), "e-f-g")

    def test_replace_os_path_isfile(self):
        path = self.mocker.replace("os.path")
        expect(path.isfile("unexistent")).result(True)
        expect(path.isfile(ANY)).passthrough().count(2)
        self.mocker.replay()
        import os
        self.assertFalse(os.path.isfile("another-unexistent"))
        self.assertTrue(os.path.isfile("unexistent"))
        self.assertFalse(os.path.isfile("unexistent"))

    def test_patch_with_spec(self):
        class C(object):
            def method(self, a, b):
                pass
        mock = self.mocker.patch(C)
        mock.method(1, 2)
        mock.method(1)
        self.mocker.replay()
        mock.method(1, 2)
        self.assertRaises(AssertionError, mock.method, 1)

    def test_patch_with_spec_and_unexistent(self):
        class C(object):
            pass
        mock = self.mocker.patch(C)
        mock.method(1, 2)
        self.mocker.count(0)
        self.mocker.replay()
        self.assertRaises(AssertionError, self.mocker.verify)

    def test_mock_iter(self):
        """
        list() uses len() as a hint. When we mock iter(), it shouldn't
        explode due to the lack of len().
        """
        mock = self.mocker.mock()
        iter(mock)
        self.mocker.result(iter([1, 2, 3]))
        self.mocker.replay()
        self.assertEquals(list(mock), [1, 2, 3])
        self.mocker.verify()

    def test_replace_builtin_function(self):
        """
        Inspection doesn't work on builtin functions, but proxying should
        work even then (without spec enforcement in these cases).
        """
        from zlib import adler32
        mock = self.mocker.proxy(adler32)
        mock()
        self.mocker.result(42)
        self.mocker.replay()
        self.assertEquals(mock(), 42)


class ExpectTest(TestCase):

    def setUp(self):
        self.mocker = CleanMocker()

    def test_calling_mocker(self):
        obj = self.mocker.mock()
        expect(obj.attr).result(123)
        self.mocker.replay()
        self.assertEquals(obj.attr, 123)

    def test_chaining(self):
        obj = self.mocker.mock()
        expect(obj.attr).result(123).result(42)
        self.mocker.replay()
        self.assertEquals(obj.attr, 42)


class MockerTestCaseTest(TestCase):

    def setUp(self):
        self.test = MockerTestCase("shortDescription")

    def tearDown(self):
        self.test.mocker.restore()
        # Run it so that any cleanups are performed.
        self.test.run()

    def test_has_mocker(self):
        self.assertEquals(type(self.test.mocker), Mocker)

    def test_has_expect(self):
        self.assertTrue(self.test.expect is expect)

    def test_attributes_are_the_same(self):
        class MyTest(MockerTestCase):
            def test_method(self):
                pass
            test_method.foo = "bar"
        test = MyTest("test_method")
        self.assertEquals(getattr(test.test_method, "im_class", None), MyTest)
        self.assertEquals(getattr(test.test_method, "foo", None), "bar")

    def test_constructor_is_the_same(self):
        self.assertEquals(inspect.getargspec(TestCase.__init__),
                          inspect.getargspec(MockerTestCase.__init__))

    def test_docstring_is_the_same(self):
        class MyTest(MockerTestCase):
            def test_method(self):
                """Hello there!"""
        self.assertEquals(MyTest("test_method").test_method.__doc__,
                          "Hello there!")

    def test_short_description_is_the_same(self):
        class MyTest(MockerTestCase):
            def test_method(self):
                """Hello there!"""
        class StandardTest(TestCase):
            def test_method(self):
                """Hello there!"""

        self.assertEquals(MyTest("test_method").shortDescription(),
                          StandardTest("test_method").shortDescription())

    def test_missing_method_raises_the_same_error(self):
        class MyTest(TestCase):
            pass

        try:
            MyTest("unexistent_method").run()
        except Exception, e:
            expected_error = e

        class MyTest(MockerTestCase):
            pass
        
        try:
            MyTest("unexistent_method").run()
        except Exception, e:
            self.assertEquals(str(e), str(expected_error))
            self.assertEquals(type(e), type(expected_error))

    def test_raises_runtime_error_if_not_in_replay_mode_with_events(self):
        class MyTest(MockerTestCase):
            def test_method(self):
                pass

        test = MyTest("test_method")

        # That's fine.
        test.test_method()

        test.mocker.add_event(Event())

        # That's not.
        self.assertRaises(RuntimeError, test.test_method)

        test.mocker.replay()

        # Fine again.
        test.test_method()

    def test_mocker_is_verified_and_restored_after_test_method_is_run(self):
        calls = []
        class MyEvent(Event):
            def verify(self):
                calls.append("verify")
            def restore(self):
                calls.append("restore")
        class MyTest(MockerTestCase):
            def test_method(self):
                self.mocker.add_event(MyEvent())
                self.mocker.replay()
            def test_method_raising(self):
                self.mocker.add_event(MyEvent())
                self.mocker.replay()
                raise AssertionError("BOOM!")

        result = unittest.TestResult()
        MyTest("test_method").run(result)

        self.assertEquals(calls, ["verify", "restore"])
        self.assertTrue(result.wasSuccessful())

        del calls[:]

        result = unittest.TestResult()
        MyTest("test_method_raising").run(result)

        self.assertEquals(calls, ["restore"])
        self.assertEquals(len(result.errors), 0)
        self.assertEquals(len(result.failures), 1)
        self.assertTrue("BOOM!" in result.failures[0][1])

    def test_expectation_failure_acts_appropriately(self):
        class MyTest(MockerTestCase):
            def test_method(self):
                mock = self.mocker.mock()
                mock.x
                self.mocker.replay()
        
        result = unittest.TestResult()
        MyTest("test_method").run(result)

        self.assertEquals(len(result.errors), 0)
        self.assertEquals(len(result.failures), 1)
        self.assertTrue("mock.x" in result.failures[0][1])

    def test_add_cleanup(self):
        stash = []
        def func(a, b):
            stash.append((a, b))

        class MyTest(MockerTestCase):
            def tearDown(self):
                self.addCleanup(func, 3, b=4)
            def test_method(self):
                self.addCleanup(func, 1, b=2)
                stash.append(stash[:])

        MyTest("test_method").run()

        self.assertEquals(stash, [[], (1, 2), (3, 4)])

    def test_twisted_trial_deferred_support(self):
        calls = []
        callbacks = []
        errbacks = []
        deferreds = []
        class Deferred(object):
            def addCallback(self, callback):
                callbacks.append(callback)
            def addErrback(self, errback):
                errbacks.append(errback)
        class MyEvent(Event):
            def verify(self):
                calls.append("verify")
            def restore(self):
                calls.append("restore")
        class MyTest(MockerTestCase):
            def test_method(self):
                self.mocker.add_event(MyEvent())
                self.mocker.replay()
                deferred = Deferred()
                deferreds.append(deferred)
                return deferred

        result = unittest.TestResult()
        test = MyTest("test_method")
        deferred = test.test_method()

        self.assertEquals(deferred, deferreds[-1])
        self.assertEquals(calls, [])
        self.assertEquals(len(callbacks), 1)
        self.assertEquals(callbacks[-1]("foo"), "foo")
        self.assertEquals(calls, ["verify"])


    def test_fail_unless_is_raises_on_mismatch(self):
        try:
            self.test.failUnlessIs([], [])
        except AssertionError, e:
            self.assertEquals(str(e), "[] is not []")
        else:
            self.fail("AssertionError not raised")

    def test_fail_unless_is_uses_msg(self):
        try:
            self.test.failUnlessIs([], [], "oops!")
        except AssertionError, e:
            self.assertEquals(str(e), "oops!")
        else:
            self.fail("AssertionError not raised")

    def test_fail_unless_is_succeeds(self):
        obj = []
        try:
            self.test.failUnlessIs(obj, obj)
        except AssertionError:
            self.fail("AssertionError shouldn't be raised")

    def test_fail_if_is_raises_on_mismatch(self):
        obj = []
        try:
            self.test.failIfIs(obj, obj)
        except AssertionError, e:
            self.assertEquals(str(e), "[] is []")
        else:
            self.fail("AssertionError not raised")

    def test_fail_if_is_uses_msg(self):
        obj = []
        try:
            self.test.failIfIs(obj, obj, "oops!")
        except AssertionError, e:
            self.assertEquals(str(e), "oops!")
        else:
            self.fail("AssertionError not raised")

    def test_fail_if_is_succeeds(self):
        try:
            self.test.failIfIs([], [])
        except AssertionError:
            self.fail("AssertionError shouldn't be raised")

    def test_fail_unless_in_raises_on_mismatch(self):
        try:
            self.test.failUnlessIn(1, [])
        except AssertionError, e:
            self.assertEquals(str(e), "1 not in []")
        else:
            self.fail("AssertionError not raised")

    def test_fail_unless_in_uses_msg(self):
        try:
            self.test.failUnlessIn(1, [], "oops!")
        except AssertionError, e:
            self.assertEquals(str(e), "oops!")
        else:
            self.fail("AssertionError not raised")

    def test_fail_unless_in_succeeds(self):
        try:
            self.test.failUnlessIn(1, [1])
        except AssertionError:
            self.fail("AssertionError shouldn't be raised")

    def test_fail_if_in_raises_on_mismatch(self):
        try:
            self.test.failIfIn(1, [1])
        except AssertionError, e:
            self.assertEquals(str(e), "1 in [1]")
        else:
            self.fail("AssertionError not raised")

    def test_fail_if_in_uses_msg(self):
        try:
            self.test.failIfIn(1, [1], "oops!")
        except AssertionError, e:
            self.assertEquals(str(e), "oops!")
        else:
            self.fail("AssertionError not raised")

    def test_fail_if_in_succeeds(self):
        try:
            self.test.failIfIn(1, [])
        except AssertionError:
            self.fail("AssertionError shouldn't be raised")

    def test_fail_unless_starts_with_raises_on_mismatch(self):
        try:
            self.test.failUnlessStartsWith("abc", "def")
        except AssertionError, e:
            self.assertEquals(str(e), "'abc' doesn't start with 'def'")
        else:
            self.fail("AssertionError not raised")

    def test_fail_unless_starts_with_uses_msg(self):
        try:
            self.test.failUnlessStartsWith("abc", "def", "oops!")
        except AssertionError, e:
            self.assertEquals(str(e), "oops!")
        else:
            self.fail("AssertionError not raised")

    def test_fail_unless_starts_with_succeeds(self):
        try:
            self.test.failUnlessStartsWith("abcdef", "abc")
        except AssertionError:
            self.fail("AssertionError shouldn't be raised")

    def test_fail_unless_starts_with_works_with_non_strings(self):
        self.test.failUnlessStartsWith([1, 2, 3], [1, 2])
        self.assertRaises(AssertionError,
                          self.test.failUnlessStartsWith, [1, 2, 3], [4, 5, 6])

    def test_fail_if_starts_with_raises_on_mismatch(self):
        try:
            self.test.failIfStartsWith("abcdef", "abc")
        except AssertionError, e:
            self.assertEquals(str(e), "'abcdef' starts with 'abc'")
        else:
            self.fail("AssertionError not raised")

    def test_fail_if_starts_with_uses_msg(self):
        try:
            self.test.failIfStartsWith("abcdef", "abc", "oops!")
        except AssertionError, e:
            self.assertEquals(str(e), "oops!")
        else:
            self.fail("AssertionError not raised")

    def test_fail_if_starts_with_succeeds(self):
        try:
            self.test.failIfStartsWith("abc", "def")
        except AssertionError:
            self.fail("AssertionError shouldn't be raised")

    def test_fail_if_starts_with_works_with_non_strings(self):
        self.test.failIfStartsWith([1, 2, 3], [4, 5, 6])
        self.assertRaises(AssertionError,
                          self.test.failIfStartsWith, [1, 2, 3], [1, 2])

    def test_fail_unless_ends_with_raises_on_mismatch(self):
        try:
            self.test.failUnlessEndsWith("abc", "def")
        except AssertionError, e:
            self.assertEquals(str(e), "'abc' doesn't end with 'def'")
        else:
            self.fail("AssertionError not raised")

    def test_fail_unless_ends_with_uses_msg(self):
        try:
            self.test.failUnlessEndsWith("abc", "def", "oops!")
        except AssertionError, e:
            self.assertEquals(str(e), "oops!")
        else:
            self.fail("AssertionError not raised")

    def test_fail_unless_ends_with_succeeds(self):
        try:
            self.test.failUnlessEndsWith("abcdef", "def")
        except AssertionError:
            self.fail("AssertionError shouldn't be raised")

    def test_fail_unless_ends_with_works_with_non_strings(self):
        self.test.failUnlessEndsWith([1, 2, 3], [2, 3])
        self.assertRaises(AssertionError,
                          self.test.failUnlessEndsWith, [1, 2, 3], [4, 5, 6])

    def test_fail_if_ends_with_raises_on_mismatch(self):
        try:
            self.test.failIfEndsWith("abcdef", "def")
        except AssertionError, e:
            self.assertEquals(str(e), "'abcdef' ends with 'def'")
        else:
            self.fail("AssertionError not raised")

    def test_fail_if_ends_with_uses_msg(self):
        try:
            self.test.failIfEndsWith("abcdef", "def", "oops!")
        except AssertionError, e:
            self.assertEquals(str(e), "oops!")
        else:
            self.fail("AssertionError not raised")

    def test_fail_if_ends_with_succeeds(self):
        try:
            self.test.failIfEndsWith("abc", "def")
        except AssertionError:
            self.fail("AssertionError shouldn't be raised")

    def test_fail_if_ends_with_works_with_non_strings(self):
        self.test.failIfEndsWith([1, 2, 3], [4, 5, 6])
        self.assertRaises(AssertionError,
                          self.test.failIfEndsWith, [1, 2, 3], [2, 3])

    def test_fail_unless_approximates_raises_on_mismatch(self):
        try:
            self.test.failUnlessApproximates(1, 2, 0.999)
        except AssertionError, e:
            self.assertEquals(str(e), "abs(1 - 2) > 0.999")
        else:
            self.fail("AssertionError not raised")

    def test_fail_unless_approximates_uses_msg(self):
        try:
            self.test.failUnlessApproximates(1, 2, 0.999, "oops!")
        except AssertionError, e:
            self.assertEquals(str(e), "oops!")
        else:
            self.fail("AssertionError not raised")

    def test_fail_unless_approximates_succeeds(self):
        try:
            self.test.failUnlessApproximates(1, 2, 1)
        except AssertionError:
            self.fail("AssertionError shouldn't be raised")

    def test_fail_if_approximates_raises_on_mismatch(self):
        try:
            self.test.failIfApproximates(1, 2, 1)
        except AssertionError, e:
            self.assertEquals(str(e), "abs(1 - 2) <= 1")
        else:
            self.fail("AssertionError not raised")

    def test_fail_if_approximates_uses_msg(self):
        try:
            self.test.failIfApproximates(1, 2, 1, "oops!")
        except AssertionError, e:
            self.assertEquals(str(e), "oops!")
        else:
            self.fail("AssertionError not raised")

    def test_fail_if_approximates_succeeds(self):
        try:
            self.test.failIfApproximates(1, 2, 0.999)
        except AssertionError:
            self.fail("AssertionError shouldn't be raised")

    def test_fail_unless_methods_match_raises_on_different_method(self):
        class Fake(object):
            def method(self, a): pass
        class Real(object):
            def method(self, b): pass
        try:
            self.test.failUnlessMethodsMatch(Fake, Real)
        except AssertionError, e:
            self.assertEquals(str(e), "Fake.method(self, a) != "
                                      "Real.method(self, b)")
        else:
            self.fail("AssertionError not raised")

    def test_fail_unless_methods_match_raises_on_missing_method(self):
        class Fake(object):
            def method(self, a): pass
        class Real(object):
            pass
        try:
            self.test.failUnlessMethodsMatch(Fake, Real)
        except AssertionError, e:
            self.assertEquals(str(e), "Fake.method(self, a) not present "
                                      "in Real")
        else: self.fail("AssertionError not raised")

    def test_fail_unless_methods_match_succeeds_on_missing_priv_method(self):
        class Fake(object):
            def _method(self, a): pass
        class Real(object):
            pass
        try:
            self.test.failUnlessMethodsMatch(Fake, Real)
        except AssertionError, e:
            self.fail("AssertionError shouldn't be raised")

    def test_fail_unless_methods_match_raises_on_different_priv_method(self):
        class Fake(object):
            def _method(self, a): pass
        class Real(object):
            def _method(self, b): pass
        try:
            self.test.failUnlessMethodsMatch(Fake, Real)
        except AssertionError, e:
            self.assertEquals(str(e), "Fake._method(self, a) != "
                                      "Real._method(self, b)")
        else:
            self.fail("AssertionError not raised")

    def test_fail_unless_methods_match_succeeds(self):
        class Fake(object):
            def method(self, a): pass
        class Real(object):
            def method(self, a): pass
        obj = []
        try:
            self.test.failUnlessMethodsMatch(Fake, Real)
        except AssertionError:
            self.fail("AssertionError shouldn't be raised")

    def test_aliases(self):
        get_method = MockerTestCase.__dict__.get

        self.assertEquals(get_method("assertIs"),
                          get_method("failUnlessIs"))

        self.assertEquals(get_method("assertIsNot"),
                          get_method("failIfIs"))

        self.assertEquals(get_method("assertIn"),
                          get_method("failUnlessIn"))

        self.assertEquals(get_method("assertNotIn"),
                          get_method("failIfIn"))

        self.assertEquals(get_method("assertStartsWith"),
                          get_method("failUnlessStartsWith"))

        self.assertEquals(get_method("assertNotStartsWith"),
                          get_method("failIfStartsWith"))

        self.assertEquals(get_method("assertEndsWith"),
                          get_method("failUnlessEndsWith"))

        self.assertEquals(get_method("assertNotEndsWith"),
                          get_method("failIfEndsWith"))

        self.assertEquals(get_method("assertApproximates"),
                          get_method("failUnlessApproximates"))

        self.assertEquals(get_method("assertNotApproximates"),
                          get_method("failIfApproximates"))

        self.assertEquals(get_method("assertMethodsMatch"),
                          get_method("failUnlessMethodsMatch"))

    def test_twisted_trial_aliases(self):
        get_method = MockerTestCase.__dict__.get

        self.assertEquals(get_method("assertIdentical"),
                          get_method("assertIs"))

        self.assertEquals(get_method("assertNotIdentical"),
                          get_method("assertIsNot"))

        self.assertEquals(get_method("failUnlessIdentical"),
                          get_method("failUnlessIs"))

        self.assertEquals(get_method("failIfIdentical"),
                          get_method("failIfIs"))

    def test_missing_python23_aliases(self):
        self.assertEquals(MockerTestCase.assertTrue.im_func,
                          MockerTestCase.failUnless.im_func)

        self.assertEquals(MockerTestCase.assertFalse.im_func,
                          MockerTestCase.failIf.im_func)

    def test_make_file_returns_writable_filename(self):
        filename = self.test.makeFile()
        self.assertFalse(os.path.isfile(filename))
        open(filename, "w").write("Is writable!")

    def test_make_file_creates_file(self):
        filename = self.test.makeFile("")
        self.assertEquals(os.path.getsize(filename), 0)

    def test_make_file_cleansup_on_success(self):
        filename = self.test.makeFile()
        self.test.run()
        self.assertEquals(os.path.isfile(filename), False)

    def test_make_file_cleansup_on_failure(self):
        class MyTest(MockerTestCase):
            def test_method(self):
                raise AssertionError("BOOM!")
        test = MyTest("test_method")
        filename = test.makeFile()
        test.run()
        self.assertEquals(os.path.isfile(filename), False)

    def test_make_file_with_content(self):
        filename = self.test.makeFile("content")
        self.assertEquals(open(filename).read(), "content")

    def test_make_file_with_prefix(self):
        filename = self.test.makeFile(prefix="prefix-")
        self.assertTrue(os.path.basename(filename).startswith("prefix-"))

    def test_make_file_with_suffix(self):
        filename = self.test.makeFile(suffix="-suffix")
        self.assertTrue(os.path.basename(filename).endswith("-suffix"))

    def test_make_file_with_dirname(self):
        dirname = tempfile.mkdtemp()
        try:
            filename = self.test.makeFile(dirname=dirname)
            self.assertEquals(os.path.dirname(filename), dirname)
        finally:
            shutil.rmtree(dirname)

    def test_make_file_with_basename(self):
        filename = self.test.makeFile(basename="basename")
        self.assertEquals(os.path.basename(filename), "basename")
        self.test.run()
        self.assertFalse(os.path.exists(filename))

    def test_make_file_with_basename_and_dirname(self):
        dirname = tempfile.mkdtemp()
        try:
            filename = self.test.makeFile(dirname=dirname, basename="basename")
            self.assertEquals(os.path.dirname(filename), dirname)
            self.assertEquals(os.path.basename(filename), "basename")
        finally:
            shutil.rmtree(dirname)

    def test_make_file_with_path(self):
        path = tempfile.mktemp()
        try:
            filename = self.test.makeFile("", path=path)
            self.assertEquals(filename, path)
            self.assertEquals(os.path.getsize(filename), 0)
            self.test.run()
            self.assertFalse(os.path.exists(filename))
        finally:
            if os.path.isfile(path):
                os.unlink(path)

    def test_make_dir_returns_dirname(self):
        dirname = self.test.makeDir()
        self.assertEquals(os.path.isdir(dirname), True)

    def test_make_dir_cleansup_on_success(self):
        dirname = self.test.makeDir()
        self.test.run()
        self.assertEquals(os.path.isdir(dirname), False)

    def test_make_dir_cleansup_on_failure(self):
        class MyTest(MockerTestCase):
            def test_method(self):
                raise AssertionError("BOOM!")
        test = MyTest("test_method")
        dirname = test.makeDir()
        test.run()
        self.assertEquals(os.path.isdir(dirname), False)

    def test_make_dir_with_prefix(self):
        dirname = self.test.makeDir(prefix="prefix-")
        self.assertTrue(os.path.basename(dirname).startswith("prefix-"))

    def test_make_dir_with_suffix(self):
        dirname = self.test.makeDir(suffix="-suffix")
        self.assertTrue(os.path.basename(dirname).endswith("-suffix"))

    def test_make_dir_with_dirname(self):
        dirname = tempfile.mkdtemp()
        try:
            path = self.test.makeDir(dirname=dirname)
            self.assertEquals(os.path.dirname(path), dirname)
        finally:
            if os.path.exists(dirname):
                shutil.rmtree(dirname)

    def test_make_dir_with_path(self):
        path = tempfile.mktemp()
        try:
            self.assertEquals(self.test.makeDir(path=path), path)
            self.assertEquals(os.path.isdir(path), True)
            self.test.run()
            self.assertEquals(os.path.isdir(path), False)
        finally:
            if os.path.exists(path):
                shutil.rmtree(path)


class MockerTest(TestCase):

    def setUp(self):
        self.recorded = []
        self.mocker = CleanMocker()
        def recorder(mocker, event):
            self.recorded.append((mocker, event))
        self.mocker.add_recorder(recorder)

        self.action = Action("getattr", ("attr",), {},
                             Path(Mock(self.mocker, name="mock")))
        self.path = self.action.path + self.action

    def test_default_is_recording(self):
        self.assertTrue(self.mocker.is_recording())

    def test_replay(self):
        calls = []
        event = self.mocker.add_event(Event())
        task = event.add_task(Task())
        task.replay = lambda: calls.append("replay")
        task.restore = lambda: calls.append("restore")
        self.mocker.replay()
        self.assertFalse(self.mocker.is_recording())
        self.assertEquals(calls, ["replay"])
        self.mocker.replay()
        self.assertEquals(calls, ["replay", "restore", "replay"])

    def test_restore(self):
        calls = []
        event = self.mocker.add_event(Event())
        task = event.add_task(Task())
        task.replay = lambda: calls.append("replay")
        task.restore = lambda: calls.append("restore")
        self.mocker.replay()
        self.mocker.restore()
        self.mocker.restore()
        self.assertTrue(self.mocker.is_recording())
        self.assertEquals(calls, ["replay", "restore"])

    def test_reset(self):
        calls = []
        event = self.mocker.add_event(Event())
        task = event.add_task(Task())
        task.restore = lambda: calls.append("restore")
        self.mocker.replay()
        self.mocker.reset()
        self.mocker.reset()
        self.assertEquals(calls, ["restore"])
        self.assertEquals(self.mocker.get_events(), [])

    def test_reset_removes_ordering(self):
        self.mocker.order()
        self.mocker.reset()
        self.assertFalse(self.mocker.is_ordering())

    def test_verify(self):
        class MyEvent(object):
            def __init__(self, id, failed):
                self.id = id
                self.failed = failed
            def verify(self):
                if self.failed:
                    raise AssertionError("%d failed\n- Line 1\n- Line 2\n"
                                         % self.id)

        self.mocker.add_event(MyEvent(1, True))
        self.mocker.add_event(MyEvent(2, False))
        self.mocker.add_event(MyEvent(3, True))

        try:
            self.mocker.verify()
        except AssertionError, e:
            message = os.linesep.join(["[Mocker] Unmet expectations:",
                                       "",
                                       "=> 1 failed",
                                       " - Line 1",
                                       " - Line 2",
                                       "",
                                       "=> 3 failed",
                                       " - Line 1",
                                       " - Line 2",
                                       ""])
            self.assertEquals(str(e), message)
        else:
            self.fail("AssertionError not raised")

    def test_verify_errors_need_good_messages(self):
        class MyEvent(object):
            def verify(self):
                raise AssertionError()
        self.mocker.add_event(MyEvent())
        self.assertRaises(RuntimeError, self.mocker.verify)

    def test_mocker_as_context_manager(self):
        calls = []
        throw = False
        class MyEvent(Event):
            def verify(self):
                calls.append("verify")
                if throw:
                    raise AssertionError("Some problem")
            def replay(self):
                calls.append("replay")
            def restore(self):
                calls.append("restore")

        event = MyEvent()
        self.mocker.add_event(event)

        self.assertEquals(calls, [])

        mocker = self.mocker.__enter__()
        self.assertTrue(mocker is self.mocker)
        self.assertEquals(calls, ["replay"])

        # Verify without errors.
        del calls[:]
        result = self.mocker.__exit__(None, None, None)
        self.assertEquals(result, False)
        self.assertEquals(calls, ["restore", "verify"])

        throw = True

        # Verify raising an error.
        self.mocker.replay()
        del calls[:]
        self.assertRaises(AssertionError,
                          self.mocker.__exit__, None, None, None)
        self.assertEquals(calls, ["restore", "verify"])

        # An exception happened in the 'with' block.  Verify won't raise.
        self.mocker.replay()
        del calls[:]
        result = self.mocker.__exit__(AssertionError, None, None)
        self.assertEquals(result, False)
        self.assertEquals(calls, ["restore"])

    def test_add_recorder_on_instance(self):
        obj1 = object()
        obj2 = object()
        mocker = CleanMocker()
        self.assertEquals(mocker.add_recorder(obj1), obj1)
        self.assertEquals(mocker.add_recorder(obj2), obj2)
        self.assertEquals(mocker.get_recorders(), [obj1, obj2])
        mocker = CleanMocker()
        self.assertEquals(mocker.add_recorder(obj1), obj1)
        self.assertEquals(mocker.get_recorders(), [obj1])

    def test_add_recorder_on_class(self):
        class MyMocker(CleanMocker):
            pass
        obj1 = object()
        obj2 = object()
        self.assertEquals(MyMocker.add_recorder(obj1), obj1)
        self.assertEquals(MyMocker.add_recorder(obj2), obj2)
        mocker = MyMocker()
        self.assertEquals(mocker.get_recorders(), [obj1, obj2])
        mocker = MyMocker()
        self.assertEquals(mocker.get_recorders(), [obj1, obj2])

    def test_add_recorder_on_subclass(self):
        class MyMocker1(CleanMocker):
            pass
        obj1 = object()
        MyMocker1.add_recorder(obj1)
        class MyMocker2(MyMocker1):
            pass
        obj2 = object()
        MyMocker2.add_recorder(obj2)
        self.assertEquals(MyMocker1.get_recorders(), [obj1])
        self.assertEquals(MyMocker2.get_recorders(), [obj1, obj2])

    def test_remove_recorder_on_instance(self):
        obj1 = object()
        obj2 = object()
        obj3 = object()
        class MyMocker(CleanMocker):
            pass
        MyMocker.add_recorder(obj1)
        MyMocker.add_recorder(obj2)
        MyMocker.add_recorder(obj3)
        mocker = MyMocker()
        mocker.remove_recorder(obj2)
        self.assertEquals(mocker.get_recorders(), [obj1, obj3])
        self.assertEquals(MyMocker.get_recorders(), [obj1, obj2, obj3])

    def test_remove_recorder_on_class(self):
        class MyMocker(CleanMocker):
            pass
        obj1 = object()
        obj2 = object()
        self.assertEquals(MyMocker.add_recorder(obj1), obj1)
        self.assertEquals(MyMocker.add_recorder(obj2), obj2)
        MyMocker.remove_recorder(obj1)
        self.assertEquals(MyMocker.get_recorders(), [obj2])

    def test_mock(self):
        mock = self.mocker.mock()
        self.assertEquals(mock.__mocker_name__, None)
        self.assertEquals(mock.__mocker_spec__, None)
        self.assertEquals(mock.__mocker_type__, None)
        self.assertEquals(mock.__mocker_count__, True)

    def test_mock_with_name(self):
        mock = self.mocker.mock(name="name")
        self.assertEquals(mock.__mocker_name__, "name")

    def test_mock_with_spec(self):
        class C(object): pass
        mock = self.mocker.mock(spec=C)
        self.assertEquals(mock.__mocker_spec__, C)

    def test_mock_with_type(self):
        class C(object): pass
        mock = self.mocker.mock(type=C)
        self.assertEquals(mock.__mocker_type__, C)

    def test_mock_with_spec_and_type(self):
        class C(object): pass
        mock = self.mocker.mock(C)
        self.assertEquals(mock.__mocker_spec__, C)
        self.assertEquals(mock.__mocker_type__, C)

    def test_mock_with_count(self):
        class C(object): pass
        mock = self.mocker.mock(count=False)
        self.assertEquals(mock.__mocker_count__, False)

    def test_proxy(self):
        original = object()
        mock = self.mocker.proxy(original)
        self.assertEquals(type(mock), Mock)
        self.assertEquals(mock.__mocker_object__, original)
        self.assertEquals(mock.__mocker_path__.root_object, original)
        self.assertEquals(mock.__mocker_count__, True)

    def test_proxy_with_count(self):
        original = object()
        mock = self.mocker.proxy(original, count=False)
        self.assertEquals(mock.__mocker_count__, False)

    def test_proxy_with_spec(self):
        original = object()
        class C(object): pass
        mock = self.mocker.proxy(original, C)
        self.assertEquals(mock.__mocker_object__, original)
        self.assertEquals(mock.__mocker_spec__, C)

    def test_proxy_with_type(self):
        original = object()
        class C(object): pass
        mock = self.mocker.proxy(original, type=C)
        self.assertEquals(mock.__mocker_type__, C)

    def test_proxy_spec_defaults_to_the_object_itself(self):
        original = object()
        mock = self.mocker.proxy(original)
        self.assertEquals(mock.__mocker_spec__, original)

    def test_proxy_type_defaults_to_the_object_type(self):
        original = object()
        mock = self.mocker.proxy(original)
        self.assertEquals(mock.__mocker_type__, object)

    def test_proxy_with_spec_and_type_none(self):
        original = object()
        mock = self.mocker.proxy(original, spec=None, type=None)
        self.assertEquals(mock.__mocker_spec__, None)
        self.assertEquals(mock.__mocker_type__, None)

    def test_proxy_with_passthrough_false(self):
        original = object()
        class C(object): pass
        mock = self.mocker.proxy(original, C, passthrough=False)
        self.assertEquals(mock.__mocker_object__, original)
        self.assertEquals(mock.__mocker_spec__, C)
        self.assertEquals(mock.__mocker_passthrough__, False)

    def test_proxy_with_submodule_string(self):
        from os import path
        module = self.mocker.proxy("os.path")
        self.assertEquals(type(module), Mock)
        self.assertEquals(type(module.__mocker_object__), ModuleType)
        self.assertEquals(module.__mocker_name__, "os.path")
        self.assertEquals(module.__mocker_object__, path)

    def test_proxy_with_module_function_string(self):
        mock = self.mocker.proxy("os.path.join.func_name")
        self.assertEquals(mock.__mocker_object__, "join")

    def test_proxy_with_string_and_name(self):
        module = self.mocker.proxy("os.path", name="mock")
        self.assertEquals(module.__mocker_name__, "mock")

    def test_proxy_with_unexistent_module(self):
        self.assertRaises(ImportError, self.mocker.proxy, "unexistent.module")

    def test_replace(self):
        from os import path
        obj = object()
        proxy = self.mocker.replace(obj, spec=object, name="obj", count=False,
                                    passthrough=False)
        self.assertEquals(type(proxy), Mock)
        self.assertEquals(type(proxy.__mocker_object__), object)
        self.assertEquals(proxy.__mocker_object__, obj)
        self.assertEquals(proxy.__mocker_spec__, object)
        self.assertEquals(proxy.__mocker_name__, "obj")
        self.assertEquals(proxy.__mocker_count__, False)
        (event,) = self.mocker.get_events()
        self.assertEquals(type(event), ReplayRestoreEvent)
        (task,) = event.get_tasks()
        self.assertEquals(type(task), ProxyReplacer)
        self.assertTrue(task.mock is proxy)
        self.assertTrue(task.mock.__mocker_object__ is obj)
        self.assertTrue(proxy is not obj)

    def test_replace_with_submodule_string(self):
        from os import path
        module = self.mocker.replace("os.path")
        self.assertEquals(type(module), Mock)
        self.assertEquals(type(module.__mocker_object__), ModuleType)
        self.assertEquals(module.__mocker_name__, "os.path")
        self.assertEquals(module.__mocker_object__, path)
        (event,) = self.mocker.get_events()
        (task,) = event.get_tasks()
        self.assertEquals(type(task), ProxyReplacer)
        self.assertTrue(task.mock is module)
        self.assertTrue(task.mock.__mocker_object__ is path)
        self.assertTrue(module is not path)

    def test_replace_with_module_function_string(self):
        mock = self.mocker.replace("os.path.join.func_name")
        self.assertEquals(mock.__mocker_object__, "join")

    def test_replace_with_string_and_name(self):
        module = self.mocker.replace("os.path", name="mock")
        self.assertEquals(module.__mocker_name__, "mock")

    def test_replace_with_type(self):
        original = object()
        class C(object): pass
        mock = self.mocker.replace(original, type=C)
        self.assertEquals(mock.__mocker_type__, C)

    def test_replace_spec_defaults_to_the_object_itself(self):
        original = object()
        mock = self.mocker.replace(original)
        self.assertEquals(mock.__mocker_spec__, original)

    def test_replace_type_defaults_to_the_object_type(self):
        original = object()
        mock = self.mocker.replace(original)
        self.assertEquals(mock.__mocker_type__, object)

    def test_replace_with_spec_and_type_none(self):
        original = object()
        mock = self.mocker.replace(original, spec=None, type=None)
        self.assertEquals(mock.__mocker_spec__, None)
        self.assertEquals(mock.__mocker_type__, None)

    def test_replace_with_passthrough_false(self):
        original = object()
        class C(object): pass
        mock = self.mocker.replace(original, passthrough=False)
        self.assertEquals(mock.__mocker_passthrough__, False)

    def test_add_and_get_event(self):
        self.mocker.add_event(41)
        self.assertEquals(self.mocker.add_event(42), 42)
        self.assertEquals(self.mocker.get_events(), [41, 42])

    def test_recording(self):
        obj = self.mocker.mock()
        obj.attr()

        self.assertEquals(len(self.recorded), 2)

        action1 = Action("getattr", ("attr",), {})
        action2 = Action("call", (), {})

        mocker1, event1 = self.recorded[0]
        self.assertEquals(mocker1, self.mocker)
        self.assertEquals(type(event1), Event)
        self.assertTrue(event1.path.matches(Path(obj, None, [action1])))

        mocker2, event2 = self.recorded[1]
        self.assertEquals(mocker2, self.mocker)
        self.assertEquals(type(event2), Event)
        self.assertTrue(event2.path.matches(Path(obj, None,
                                                 [action1, action2])))

        self.assertEquals(self.mocker.get_events(), [event1, event2])

    def test_recording_result_path(self):
        obj = self.mocker.mock()
        result = obj.attr()
        path = Path(obj, None, [Action("getattr", ("attr",), {}),
                                Action("call", (), {})])
        self.assertTrue(result.__mocker_path__.matches(path))

    def test_replaying_no_events(self):
        self.mocker.replay()
        try:
            self.mocker.act(self.path)
        except AssertionError, e:
            pass
        else:
            self.fail("AssertionError not raised")
        self.assertEquals(str(e), "[Mocker] Unexpected expression: mock.attr")

    def test_replaying_matching(self):
        calls = []
        class MyTask(Task):
            def matches(_, path):
                calls.append("matches")
                self.assertTrue(self.path.matches(path))
                return True
            def run(_, path):
                calls.append("run")
                self.assertTrue(self.path.matches(path))
                return "result"
        event = Event()
        event.add_task(MyTask())
        self.mocker.add_event(event)
        self.mocker.replay()
        self.assertEquals(self.mocker.act(self.path), "result")
        self.assertEquals(calls, ["matches", "run"])

    def test_replaying_none_matching(self):
        calls = []
        class MyTask(Task):
            def matches(_, path):
                self.assertTrue(self.path.matches(path))
                calls.append("matches")
                return False
        event = Event()
        event.add_task(MyTask())
        self.mocker.add_event(event)
        self.mocker.replay()
        self.assertRaises(AssertionError, self.mocker.act, self.path)
        self.assertEquals(calls, ["matches"])

    def test_replay_order(self):
        """
        When playing back, the precedence of events is as follows:

        1. Events with may_run() true
        2. Events with satisfied() false
        3. Events with has_run() false

        """
        class MyTaskBase(Task):
            postpone = 2
            def may_run(self, path):
                if not self.postpone:
                    return True
                self.postpone -= 1
            def run(self, path):
                return self.__class__.__name__
        class MyTask1(MyTaskBase): pass
        class MyTask2(MyTaskBase): pass
        class MyTask3(MyTaskBase):
            raised = False
            def verify(self):
                if not self.postpone and not self.raised:
                    self.raised = True
                    raise AssertionError("An error")
        class MyTask4(MyTaskBase):
            postpone = 0
        class MyTask5(MyTaskBase):
            postpone = 1

        event1 = self.mocker.add_event(Event())
        event1.add_task(MyTask1())
        event2 = self.mocker.add_event(Event())
        event2.add_task(MyTask2())
        event3 = self.mocker.add_event(Event())
        event3.add_task(MyTask3())
        event4 = self.mocker.add_event(Event())
        event4.add_task(MyTask4())
        event5 = self.mocker.add_event(Event())
        event5.add_task(MyTask5())
        self.mocker.replay()

        # Labels: [M]ay run, [S]atisfied, [H]as run

        # State: 1=S 2=S 3= 4=MS 5=S
        self.assertEquals(self.mocker.act(self.path), "MyTask4")
        # State: 1=S 2=S 3= 4=MSH 5=S
        self.assertEquals(self.mocker.act(self.path), "MyTask4")
        # State: 1=MS 2=MS 3=M 4=MSH 5=MS
        self.assertEquals(self.mocker.act(self.path), "MyTask3")
        # State: 1=MS 2=MS 3=MSH 4=MSH 5=MS
        self.assertEquals(self.mocker.act(self.path), "MyTask1")
        # State: 1=MSH 2=MS 3=MSH 4=MSH 5=MS
        self.assertEquals(self.mocker.act(self.path), "MyTask2")
        # State: 1=MSH 2=MSH 3=MSH 4=MSH 5=MS
        self.assertEquals(self.mocker.act(self.path), "MyTask5")
        # State: 1=MSH 2=MSH 3=MSH 4=MSH 5=MSH
        self.assertEquals(self.mocker.act(self.path), "MyTask1")

    def test_recorder_decorator(self):
        result = recorder(42)
        try:
            self.assertEquals(result, 42)
            self.assertEquals(Mocker.get_recorders()[-1], 42)
            self.assertEquals(MockerBase.get_recorders(), [])
        finally:
            Mocker.remove_recorder(42)

    def test_result(self):
        event1 = self.mocker.add_event(Event())
        event2 = self.mocker.add_event(Event())
        self.mocker.result(123)
        self.assertEquals(event2.run(self.path), 123)

    def test_throw(self):
        class MyException(Exception): pass
        event1 = self.mocker.add_event(Event())
        event2 = self.mocker.add_event(Event())
        self.mocker.throw(MyException)
        self.assertRaises(MyException, event2.run, self.path)

    def test_call(self):
        event1 = self.mocker.add_event(Event())
        event2 = self.mocker.add_event(Event())
        self.mocker.call(lambda *args, **kwargs: 123)
        self.assertEquals(event2.run(self.path), 123)

    def test_count(self):
        event1 = self.mocker.add_event(Event())
        event2 = self.mocker.add_event(Event())
        event2.add_task(ImplicitRunCounter(1))
        self.mocker.count(2, 3)
        self.assertEquals(len(event1.get_tasks()), 0)
        (task,) = event2.get_tasks()
        self.assertEquals(type(task), RunCounter)
        self.assertEquals(task.min, 2)
        self.assertEquals(task.max, 3)
        self.mocker.count(4)
        self.assertEquals(len(event1.get_tasks()), 0)
        (task,) = event2.get_tasks()
        self.assertEquals(type(task), RunCounter)
        self.assertEquals(task.min, 4)
        self.assertEquals(task.max, 4)

    def test_order(self):
        mock1 = self.mocker.mock()
        mock2 = self.mocker.mock()
        mock3 = self.mocker.mock()
        mock4 = self.mocker.mock()
        result1 = mock1.attr1(1)
        result2 = mock2.attr2(2)
        result3 = mock3.attr3(3)
        result4 = mock4.attr4(4)

        # Try to spoil the logic which decides which task to reuse.
        other_task = Task()
        for event in self.mocker.get_events():
            event.add_task(other_task)

        self.mocker.order(result1, result2, result3)
        self.mocker.order(result1, result4)
        self.mocker.order(result2, result4)
        events = self.mocker.get_events()
        self.assertEquals(len(events), 8)

        self.assertEquals(events[0].get_tasks(), [other_task])
        other_task_, task1 = events[1].get_tasks()
        self.assertEquals(type(task1), Orderer)
        self.assertEquals(task1.path, events[1].path)
        self.assertEquals(task1.get_dependencies(), [])
        self.assertEquals(other_task_, other_task)

        self.assertEquals(events[2].get_tasks(), [other_task])
        other_task_, task3 = events[3].get_tasks()
        self.assertEquals(type(task3), Orderer)
        self.assertEquals(task3.path, events[3].path)
        self.assertEquals(task3.get_dependencies(), [task1])
        self.assertEquals(other_task_, other_task)

        self.assertEquals(events[4].get_tasks(), [other_task])
        other_task_, task5 = events[5].get_tasks()
        self.assertEquals(type(task5), Orderer)
        self.assertEquals(task5.path, events[5].path)
        self.assertEquals(task5.get_dependencies(), [task3])
        self.assertEquals(other_task_, other_task)

        self.assertEquals(events[6].get_tasks(), [other_task])
        other_task_, task7 = events[7].get_tasks()
        self.assertEquals(type(task7), Orderer)
        self.assertEquals(task7.path, events[7].path)
        self.assertEquals(task7.get_dependencies(), [task1, task3])
        self.assertEquals(other_task_, other_task)

    def test_after(self):
        mock1 = self.mocker.mock()
        mock2 = self.mocker.mock()
        mock3 = self.mocker.mock()
        result1 = mock1.attr1(1)
        result2 = mock2.attr2(2)
        result3 = mock3.attr3(3)

        # Try to spoil the logic which decides which task to reuse.
        other_task = Task()
        for event in self.mocker.get_events():
            event.add_task(other_task)

        self.mocker.after(result1, result2)

        events = self.mocker.get_events()
        self.assertEquals(len(events), 6)

        self.assertEquals(events[0].get_tasks(), [other_task])
        other_task_, task1 = events[1].get_tasks()
        self.assertEquals(type(task1), Orderer)
        self.assertEquals(task1.path, events[1].path)
        self.assertEquals(task1.get_dependencies(), [])
        self.assertEquals(other_task_, other_task)

        self.assertEquals(events[2].get_tasks(), [other_task])
        other_task_, task3 = events[3].get_tasks()
        self.assertEquals(type(task3), Orderer)
        self.assertEquals(task3.path, events[3].path)
        self.assertEquals(task3.get_dependencies(), [])
        self.assertEquals(other_task_, other_task)

        self.assertEquals(events[4].get_tasks(), [other_task])
        other_task_, task5 = events[5].get_tasks()
        self.assertEquals(type(task5), Orderer)
        self.assertEquals(task5.path, events[5].path)
        self.assertEquals(task5.get_dependencies(), [task1, task3])
        self.assertEquals(other_task_, other_task)

    def test_before(self):
        mock1 = self.mocker.mock()
        mock2 = self.mocker.mock()
        mock3 = self.mocker.mock()
        result1 = mock1.attr1(1)
        result2 = mock2.attr2(2)
        result3 = mock3.attr3(3)

        # Try to spoil the logic which decides which task to reuse.
        other_task = Task()
        for event in self.mocker.get_events():
            event.add_task(other_task)

        self.mocker.before(result1, result2)

        events = self.mocker.get_events()
        self.assertEquals(len(events), 6)

        self.assertEquals(events[4].get_tasks(), [other_task])
        other_task_, task5 = events[5].get_tasks()
        self.assertEquals(type(task5), Orderer)
        self.assertEquals(task5.path, events[5].path)
        self.assertEquals(task5.get_dependencies(), [])
        self.assertEquals(other_task_, other_task)

        self.assertEquals(events[0].get_tasks(), [other_task])
        other_task_, task1 = events[1].get_tasks()
        self.assertEquals(type(task1), Orderer)
        self.assertEquals(task1.path, events[1].path)
        self.assertEquals(task1.get_dependencies(), [task5])
        self.assertEquals(other_task_, other_task)

        self.assertEquals(events[2].get_tasks(), [other_task])
        other_task_, task3 = events[3].get_tasks()
        self.assertEquals(type(task3), Orderer)
        self.assertEquals(task3.path, events[3].path)
        self.assertEquals(task3.get_dependencies(), [task5])
        self.assertEquals(other_task_, other_task)

    def test_default_ordering(self):
        self.assertEquals(self.mocker.is_ordering(), False)

    def test_order_without_arguments(self):
        self.mocker.order()
        self.assertEquals(self.mocker.is_ordering(), True)

    def test_order_with_context_manager(self):
        with_manager = self.mocker.order()
        self.assertEquals(self.mocker.is_ordering(), True)
        with_manager.__enter__()
        self.assertEquals(self.mocker.is_ordering(), True)
        with_manager.__exit__(None, None, None)
        self.assertEquals(self.mocker.is_ordering(), False)

    def test_unorder(self):
        self.mocker.order()
        self.mocker.unorder()
        self.assertEquals(self.mocker.is_ordering(), False)

    def test_ordered_events(self):
        mock = self.mocker.mock()

        # Ensure that the state is correctly reset between
        # different ordered blocks.
        self.mocker.order()

        mock.a

        self.mocker.unorder()
        self.mocker.order()

        mock.x.y.z

        events = self.mocker.get_events()

        (task1,) = events[1].get_tasks()
        (task2,) = events[2].get_tasks()
        (task3,) = events[3].get_tasks()

        self.assertEquals(type(task1), Orderer)
        self.assertEquals(type(task2), Orderer)
        self.assertEquals(type(task3), Orderer)

        self.assertEquals(task1.path, events[1].path)
        self.assertEquals(task2.path, events[2].path)
        self.assertEquals(task3.path, events[3].path)

        self.assertEquals(task1.get_dependencies(), [])
        self.assertEquals(task2.get_dependencies(), [task1])
        self.assertEquals(task3.get_dependencies(), [task2])

    def test_nospec(self):
        event1 = self.mocker.add_event(Event())
        event2 = self.mocker.add_event(Event())
        task1 = event1.add_task(SpecChecker(None))
        task2 = event2.add_task(Task())
        task3 = event2.add_task(SpecChecker(None))
        task4 = event2.add_task(Task())
        self.mocker.nospec()
        self.assertEquals(event1.get_tasks(), [task1])
        self.assertEquals(event2.get_tasks(), [task2, task4])

    def test_passthrough(self):
        obj = object()
        mock = self.mocker.proxy(obj)
        event1 = self.mocker.add_event(Event(Path(mock, obj)))
        event2 = self.mocker.add_event(Event(Path(mock, obj)))
        self.mocker.passthrough()
        self.assertEquals(event1.get_tasks(), [])
        (task,) = event2.get_tasks()
        self.assertEquals(type(task), PathExecuter)

    def test_passthrough_fails_on_unproxied(self):
        mock = self.mocker.mock()
        event1 = self.mocker.add_event(Event(Path(mock)))
        event2 = self.mocker.add_event(Event(Path(mock)))
        self.assertRaises(TypeError, self.mocker.passthrough)

    def test_passthrough(self):
        obj = object()
        mock = self.mocker.proxy(obj)
        event = self.mocker.add_event(Event(Path(mock, obj)))
        result_callback = object()
        self.mocker.passthrough(result_callback)
        (task,) = event.get_tasks()
        self.assertEquals(task.get_result_callback(), result_callback)

    def test_on(self):
        obj = self.mocker.mock()
        self.mocker.on(obj.attr).result(123)
        self.mocker.replay()
        self.assertEquals(obj.attr, 123)

    def test_patch(self):
        class C(object): pass
        mock = self.mocker.patch(C)
        self.assertEquals(type(C.__mocker_mock__), Mock)
        self.assertTrue(C.__mocker_mock__ is mock)
        self.assertTrue(mock.__mocker_object__ is C)
        self.assertEquals(type(mock.__mocker_patcher__), Patcher)
        self.assertEquals(mock.__mocker_passthrough__, True)
        self.assertEquals(mock.__mocker_spec__, C)
        (event,) = self.mocker.get_events()
        self.assertEquals(type(event), ReplayRestoreEvent)
        (task,) = event.get_tasks()
        self.assertTrue(task is mock.__mocker_patcher__)

    def test_patch_without_spec(self):
        class C(object): pass
        mock = self.mocker.patch(C, spec=None)
        self.assertEquals(mock.__mocker_spec__, None)


class ActionTest(TestCase):

    def setUp(self):
        self.mock = Mock(None, name="mock")

    def test_create(self):
        objects = [object() for i in range(4)]
        action = Action(*objects)
        self.assertEquals(action.kind, objects[0])
        self.assertEquals(action.args, objects[1])
        self.assertEquals(action.kwargs, objects[2])
        self.assertEquals(action.path, objects[3])

    def test_repr(self):
        self.assertEquals(repr(Action("kind", "args", "kwargs")),
                          "Action('kind', 'args', 'kwargs')")
        self.assertEquals(repr(Action("kind", "args", "kwargs", "path")),
                          "Action('kind', 'args', 'kwargs', 'path')")

    def test_execute_unknown(self):
        self.assertRaises(RuntimeError, Action("unknown", (), {}).execute, None)

    def test_execute_getattr(self):
        class C(object):
            pass
        obj = C()
        obj.attr = C()
        action = Action("getattr", ("attr",), {})
        self.assertEquals(action.execute(obj), obj.attr)

    def test_execute_setattr(self):
        class C(object):
            pass
        obj = C()
        action = Action("setattr", ("attr", "value"), {})
        action.execute(obj)
        self.assertEquals(getattr(obj, "attr", None), "value")

    def test_execute_delattr(self):
        class C(object):
            pass
        obj = C()
        obj.attr = "value"
        action = Action("delattr", ("attr",), {})
        action.execute(obj)
        self.assertEquals(getattr(obj, "attr", None), None)

    def test_execute_call(self):
        obj = lambda a, b: a+b
        action = Action("call", (1,), {"b": 2})
        self.assertEquals(action.execute(obj), 3)

    def test_execute_contains(self):
        obj = ["a"]
        action = Action("contains", ("a",), {})
        self.assertEquals(action.execute(obj), True)
        action = Action("contains", ("b",), {})
        self.assertEquals(action.execute(obj), False)

    def test_execute_getitem(self):
        obj = {"a": 1}
        action = Action("getitem", ("a",), {})
        self.assertEquals(action.execute(obj), 1)
        action = Action("getitem", ("b",), {})
        self.assertRaises(KeyError, action.execute, obj)

    def test_execute_setitem(self):
        obj = {}
        action = Action("setitem", ("a", 1), {})
        action.execute(obj)
        self.assertEquals(obj, {"a": 1})

    def test_execute_delitem(self):
        obj = {"a": 1, "b": 2}
        action = Action("delitem", ("a",), {})
        action.execute(obj)
        self.assertEquals(obj, {"b": 2})

    def test_execute_len(self):
        obj = [1, 2, 3]
        action = Action("len", (), {})
        self.assertEquals(action.execute(obj), 3)

    def test_execute_nonzero(self):
        obj = []
        action = Action("nonzero", (), {})
        self.assertEquals(action.execute(obj), False)
        obj = [1]
        action = Action("nonzero", (), {})
        self.assertEquals(action.execute(obj), True)

    def test_execute_iter(self):
        obj = [1, 2, 3]
        action = Action("iter", (), {})
        result = action.execute(obj)
        self.assertEquals(type(result), type(iter(obj)))
        self.assertEquals(list(result), obj)

    def test_execute_caching(self):
        values = iter(range(10))
        obj = lambda: values.next()
        action = Action("call", (), {})
        self.assertEquals(action.execute(obj), 0)
        self.assertEquals(action.execute(obj), 0)
        obj = lambda: values.next()
        self.assertEquals(action.execute(obj), 1)

    def test_equals(self):
        obj1 = object()
        obj2 = object()

        self.assertEquals(Action("kind", (), {}, obj1),
                          Action("kind", (), {}, obj2))
        self.assertNotEquals(Action("kind", (), {}, obj1),
                             Action("dnik", (), {}, obj2))
        self.assertNotEquals(Action("kind", (), {}, obj1),
                             Action("kind", (1,), {}, obj2))
        self.assertNotEquals(Action("kind", (), {}, obj1),
                             Action("kind", (), {"a": 1}, obj2))
        self.assertNotEquals(Action("kind", (ANY,), {}, obj1),
                             Action("kind", (1,), {}, obj2))
        self.assertEquals(Action("kind", (CONTAINS(1),), {}, obj1),
                          Action("kind", (CONTAINS(1),), {}, obj2))

    def test_matches(self):
        obj1 = object()
        obj2 = object()

        action1 = Action("kind", (), {}, obj1)
        action2 = Action("kind", (), {}, obj2)
        self.assertTrue(action1.matches(action2))

        action1 = Action("kind", (), {}, obj1)
        action2 = Action("dnik", (), {}, obj2)
        self.assertFalse(action1.matches(action2))

        action1 = Action("kind", (), {}, obj1)
        action2 = Action("kind", (1,), {}, obj2)
        self.assertFalse(action1.matches(action2))

        action1 = Action("kind", (), {}, obj1)
        action2 = Action("kind", (), {"a": 1}, obj2)
        self.assertFalse(action1.matches(action2))

        action1 = Action("kind", (ARGS,), {}, obj1)
        action2 = Action("kind", (), {}, obj2)
        self.assertTrue(action1.matches(action2))

        action1 = Action("kind", (ARGS,), {"a": 1}, obj1)
        action2 = Action("kind", (), {}, obj2)
        self.assertFalse(action1.matches(action2))


class PathTest(TestCase):

    def setUp(self):
        class StubMocker(object):
            def act(self, path):
                pass
        self.mocker = StubMocker()
        self.mock = Mock(self.mocker, name="obj")
        self.object = object()

    def test_create(self):
        mock = object()
        path = Path(mock)
        self.assertEquals(path.root_mock, mock)
        self.assertEquals(path.root_object, None)
        self.assertEquals(path.actions, ())

    def test_create_with_object(self):
        mock = object()
        path = Path(mock, self.object)
        self.assertEquals(path.root_mock, mock)
        self.assertEquals(path.root_object, self.object)

    def test_create_with_actions(self):
        mock = object()
        path = Path(mock, self.object, [1,2,3])
        self.assertEquals(path.root_mock, mock)
        self.assertEquals(path.root_object, self.object)
        self.assertEquals(path.actions, (1,2,3))

    def test_add(self):
        mock = object()
        path = Path(mock, self.object, [1,2,3])
        result = path + 4
        self.assertTrue(result is not path)
        self.assertEquals(result.root_mock, mock)
        self.assertEquals(result.root_object, self.object)
        self.assertEquals(result.actions, (1,2,3,4))

    def test_parent_path(self):
        path1 = Path(self.mock)
        path2 = path1 + Action("getattr", ("attr",), {}, path1)
        path3 = path2 + Action("getattr", ("attr",), {}, path2)

        self.assertEquals(path1.parent_path, None)
        self.assertEquals(path2.parent_path, path1)
        self.assertEquals(path3.parent_path, path2)

    def test_equals(self):
        mock = object()
        obj = object()
        obj1 = object()
        obj2 = object()

        # Not the *same* mock.
        path1 = Path([], obj, [])
        path2 = Path([], obj, [])
        self.assertNotEquals(path1, path2)

        # Not the *same* object.
        path1 = Path(mock, [], [])
        path2 = Path(mock, [], [])
        self.assertNotEquals(path1, path2)

        path1 = Path(mock, obj, [Action("kind", (), {}, obj1)])
        path2 = Path(mock, obj, [Action("kind", (), {}, obj2)])
        self.assertEquals(path1, path2)

        path1 = Path(mock, obj, [Action("kind", (), {}, obj1)])
        path2 = Path(mock, obj, [Action("dnik", (), {}, obj2)])
        self.assertNotEquals(path1, path2)

        path1 = Path(mock, obj, [Action("kind", (), {}, obj1)])
        path2 = Path(object(), obj, [Action("kind", (), {}, obj2)])
        self.assertNotEquals(path1, path2)

        path1 = Path(mock, obj, [Action("kind", (), {}, obj1)])
        path2 = Path(mock, obj, [Action("kind", (1,), {}, obj2)])
        self.assertNotEquals(path1, path2)

        path1 = Path(mock, obj, [Action("kind", (), {}, obj1)])
        path2 = Path(mock, obj, [Action("kind", (), {"a": 1}, obj2)])
        self.assertNotEquals(path1, path2)

        path1 = Path(mock, obj, [Action("kind", (), {}, obj1)])
        path2 = Path(mock, obj, [])
        self.assertNotEquals(path1, path2)

        path1 = Path(mock, obj, [Action("kind", (ANY,), {}, obj1)])
        path2 = Path(mock, obj, [Action("kind", (1,), {}, obj2)])
        self.assertNotEquals(path1, path2)

        path1 = Path(mock, obj, [Action("kind", (CONTAINS(1),), {}, obj1)])
        path2 = Path(mock, obj, [Action("kind", (CONTAINS(1),), {}, obj2)])
        self.assertEquals(path1, path2)

    def test_matches(self):
        obj = object()
        mock = object()
        obj1 = object()
        obj2 = object()

        # Not the *same* mock.
        path1 = Path([], obj, [])
        path2 = Path([], obj, [])
        self.assertFalse(path1.matches(path2))

        path1 = Path(mock, obj1, [])
        path2 = Path(mock, obj2, [])
        self.assertTrue(path1.matches(path2))

        path1 = Path(mock, obj, [Action("kind", (), {}, obj1)])
        path2 = Path(mock, obj, [Action("kind", (), {}, obj2)])
        self.assertTrue(path1.matches(path2))

        path1 = Path(mock, obj, [Action("kind", (), {}, obj1)])
        path2 = Path(mock, obj, [Action("dnik", (), {}, obj2)])
        self.assertFalse(path1.matches(path2))

        path1 = Path(mock, obj, [Action("kind", (), {}, obj1)])
        path2 = Path(object(), [Action("kind", (), {}, obj2)])
        self.assertFalse(path1.matches(path2))

        path1 = Path(mock, obj, [Action("kind", (), {}, obj1)])
        path2 = Path(mock, obj, [Action("kind", (1,), {}, obj2)])
        self.assertFalse(path1.matches(path2))

        path1 = Path(mock, obj, [Action("kind", (), {}, obj1)])
        path2 = Path(mock, obj, [Action("kind", (), {"a": 1}, obj2)])
        self.assertFalse(path1.matches(path2))

        path1 = Path(mock, obj, [Action("kind", (), {}, obj1)])
        path2 = Path(mock, obj, [])
        self.assertFalse(path1.matches(path2))

        path1 = Path(mock, obj, [Action("kind", (ARGS,), {}, obj1)])
        path2 = Path(mock, obj, [Action("kind", (), {}, obj2)])
        self.assertTrue(path1.matches(path2))

        path1 = Path(mock, obj, [Action("kind", (ARGS,), {"a": 1}, obj1)])
        path2 = Path(mock, obj, [Action("kind", (), {}, obj2)])
        self.assertFalse(path1.matches(path2))

    def test_str(self):
        path = Path(self.mock, [])
        self.assertEquals(str(path), "obj")

    def test_str_unnamed(self):
        mock = Mock(self.mocker)
        path = Path(mock, [])
        self.assertEquals(str(path), "<mock>")

    def test_str_auto_named(self):
        named_mock = Mock(self.mocker)
        named_mock.attr
        path = Path(named_mock, [])
        self.assertEquals(str(path), "named_mock")

    def test_str_getattr(self):
        path = Path(self.mock, None, [Action("getattr", ("attr",), {})])
        self.assertEquals(str(path), "obj.attr")

        path += Action("getattr", ("x",), {})
        self.assertEquals(str(path), "obj.attr.x")

    def test_str_getattr_call(self):
        path = Path(self.mock, None, [Action("getattr", ("x",), {}),
                                      Action("getattr", ("y",), {}),
                                      Action("call", ("z",), {})])
        self.assertEquals(str(path), "obj.x.y('z')")

    def test_str_setattr(self):
        path = Path(self.mock, None,
                    [Action("setattr", ("attr", "value"), {})])
        self.assertEquals(str(path), "obj.attr = 'value'")

    def test_str_delattr(self):
        path = Path(self.mock, None, [Action("delattr", ("attr",), {})])
        self.assertEquals(str(path), "del obj.attr")

    def test_str_call(self):
        path = Path(self.mock, None, [Action("call", (), {})])
        self.assertEquals(str(path), "obj()")

        path = Path(self.mock, None,
                    [Action("call", (1, "2"), {"a": 3, "b": "4"})])
        self.assertEquals(str(path), "obj(1, '2', a=3, b='4')")

    def test_str_contains(self):
        path = Path(self.mock, None, [Action("contains", ("value",), {})])
        self.assertEquals(str(path), "'value' in obj")

    def test_str_getitem(self):
        path = Path(self.mock, None, [Action("getitem", ("key",), {})])
        self.assertEquals(str(path), "obj['key']")

    def test_str_setitem(self):
        path = Path(self.mock, None, [Action("setitem", ("key", "value"), {})])
        self.assertEquals(str(path), "obj['key'] = 'value'")

    def test_str_delitem(self):
        path = Path(self.mock, None, [Action("delitem", ("key",), {})])
        self.assertEquals(str(path), "del obj['key']")

    def test_str_len(self):
        path = Path(self.mock, None, [Action("len", (), {})])
        self.assertEquals(str(path), "len(obj)")

    def test_str_nonzero(self):
        path = Path(self.mock, None, [Action("nonzero", (), {})])
        self.assertEquals(str(path), "bool(obj)")

    def test_str_iter(self):
        path = Path(self.mock, None, [Action("iter", (), {})])
        self.assertEquals(str(path), "iter(obj)")

    def test_str_raises_on_unknown(self):
        path = Path(self.mock, None, [Action("unknown", (), {})])
        self.assertRaises(RuntimeError, str, path)

    def test_execute(self):
        class C(object):
            pass
        obj = C()
        obj.x = C()
        obj.x.y = lambda a, b: a+b
        path = Path(self.mock, None, [Action("getattr", ("x",), {}),
                                      Action("getattr", ("y",), {}),
                                      Action("call", (1,), {"b": 2})])
        self.assertEquals(path.execute(obj), 3)


class MatchParamsTest(TestCase):

    def true(self, *args):
        self.assertTrue(match_params(*args), repr(args))

    def false(self, *args):
        self.assertFalse(match_params(*args), repr(args))
    
    def test_any_repr(self):
        self.assertEquals(repr(ANY), "ANY")

    def test_any_equals(self):
        self.assertEquals(ANY, ANY)
        self.assertNotEquals(ANY, ARGS)
        self.assertNotEquals(ANY, object())

    def test_any_matches(self):
        self.assertTrue(ANY.matches(1))
        self.assertTrue(ANY.matches(42))
        self.assertTrue(ANY.matches(object()))

    def test_is_repr(self):
        self.assertEquals(repr(IS("obj")), "IS('obj')")

    def test_is_equals(self):
        l1 = []
        l2 = []
        self.assertNotEquals(IS(l1), l2)
        self.assertEquals(IS(l1), IS(l1))
        self.assertNotEquals(IS(l1), IS(l2))

    def test_is_matches(self):
        l1 = []
        l2 = []
        self.assertTrue(IS(l1).matches(l1))
        self.assertFalse(IS(l1).matches(l2))
        self.assertFalse(IS(l1).matches(ANY))

    def test_contains_repr(self):
        self.assertEquals(repr(CONTAINS("obj")), "CONTAINS('obj')")

    def test_contains_equals(self):
        self.assertEquals(CONTAINS([1]), CONTAINS([1]))
        self.assertNotEquals(CONTAINS(1), CONTAINS([1]))

    def test_contains_matches(self):
        self.assertTrue(CONTAINS(1).matches([1]))
        self.assertFalse(CONTAINS([1]).matches([1]))
        self.assertFalse(CONTAINS(1).matches(object()))

    def test_contains_matches_with_contains(self):
        """Can't be iterated, but has contains hook."""
        class C(object):
            def __contains__(self, value):
                return True
        self.assertTrue(CONTAINS(1).matches(C()))

    def test_in_repr(self):
        self.assertEquals(repr(IN("obj")), "IN('obj')")

    def test_in_equals(self):
        self.assertEquals(IN([1]), IN([1]))
        self.assertNotEquals(IN([1]), IN(1))

    def test_in_matches(self):
        self.assertTrue(IN([1]).matches(1))
        self.assertFalse(IN([1]).matches([1]))
        self.assertFalse(IN([1]).matches(object()))

    def test_match_repr(self):
        self.assertEquals(repr(MATCH("obj")), "MATCH('obj')")

    def test_match_equals(self):
        obj1, obj2 = [], []
        self.assertEquals(MATCH(obj1), MATCH(obj1))
        self.assertNotEquals(MATCH(obj1), MATCH(obj2))

    def test_match_matches(self):
        self.assertTrue(MATCH(lambda x: x > 10).matches(15))
        self.assertFalse(MATCH(lambda x: x > 10).matches(5))

    def test_normal(self):
        self.true((), {}, (), {})
        self.true((1, 2), {"a": 3}, (1, 2), {"a": 3})
        self.false((1,), {}, (), {})
        self.false((), {}, (1,), {})
        self.false((1, 2), {"a": 3}, (1, 2), {"a": 4})
        self.false((1, 2), {"a": 3}, (1, 3), {"a": 3})

    def test_any(self):
        self.true((1, 2), {"a": ANY}, (1, 2), {"a": 4})
        self.true((1, ANY), {"a": 3}, (1, 3), {"a": 3})
        self.false((ANY,), {}, (), {})

    def test_special_args_matching(self):
        self.true((1, IN([2])), {}, (1, 2), {})
        self.true((1, 2), {"a": IN([3])}, (1, 2), {"a": 3})
        self.false((1, IN([2])), {}, (1, 3), {})
        self.false((1, 2), {"a": IN([3])}, (1, 2), {"a": 4})

    def test_args_alone(self):
        self.true((ARGS,), {}, (), {})
        self.true((ARGS,), {}, (1, 2), {})
        self.false((ARGS,), {}, (1, 2), {"a": 2})
        self.false((ARGS,), {}, (), {"a": 2})
        self.true((ARGS,), {"a": 1}, (), {"a": 1})
        self.true((ARGS,), {"a": 1}, (1, 2), {"a": 1})
        self.false((ARGS,), {"a": 1}, (), {"a": 1, "b": 2})
        self.false((ARGS,), {"a": 1}, (1, 2), {"a": 1, "b": 2})
        self.false((ARGS,), {"a": 1}, (), {})

    def test_kwargs_alone(self):
        self.true((KWARGS,), {}, (), {})
        self.false((KWARGS,), {}, (1, 2), {})
        self.false((KWARGS,), {}, (1, 2), {"a": 2})
        self.true((KWARGS,), {}, (), {"a": 2})
        self.true((KWARGS,), {"a": 1}, (), {"a": 1})
        self.false((KWARGS,), {"a": 1}, (1, 2), {"a": 1})
        self.true((KWARGS,), {"a": 1}, (), {"a": 1, "b": 2})
        self.false((KWARGS,), {"a": 1}, (1, 2), {"a": 1, "b": 2})
        self.false((KWARGS,), {"a": 1}, (), {})

    def test_args_kwargs(self):
        self.true((ARGS, KWARGS), {}, (), {})
        self.true((ARGS, KWARGS), {}, (1, 2), {})
        self.true((ARGS, KWARGS), {}, (1, 2), {"a": 2})
        self.true((ARGS, KWARGS), {}, (), {"a": 2})
        self.true((ARGS, KWARGS), {"a": 1}, (), {"a": 1})
        self.true((ARGS, KWARGS), {"a": 1}, (1, 2), {"a": 1})
        self.true((ARGS, KWARGS), {"a": 1}, (), {"a": 1, "b": 2})
        self.true((ARGS, KWARGS), {"a": 1}, (1, 2), {"a": 1, "b": 2})
        self.false((ARGS, KWARGS), {"a": 1}, (), {})

    def test_args_at_start(self):
        self.true((ARGS, 3, 4), {}, (3, 4), {})
        self.true((ARGS, 3, 4), {}, (1, 2, 3, 4), {})
        self.true((ARGS, 3, 4), {"a": 1}, (3, 4), {"a": 1})
        self.false((ARGS, 3, 4), {"a": 1}, (1, 2, 3, 4), {"a": 1, "b": 2})
        self.false((ARGS, 3, 4), {}, (), {})
        self.false((ARGS, 3, 4), {}, (3, 5), {})
        self.false((ARGS, 3, 4), {}, (5, 5), {})
        self.false((ARGS, 3, 4), {}, (3, 4, 5), {})
        self.false((ARGS, 3, 4), {"a": 1}, (), {})
        self.false((ARGS, 3, 4), {"a": 1}, (3, 4), {})
        self.false((ARGS, 3, 4), {"a": 1}, (3, 4), {"b": 2})

    def test_args_at_end(self):
        self.true((1, 2, ARGS), {}, (1, 2), {})
        self.true((1, 2, ARGS), {}, (1, 2, 3, 4), {})
        self.true((1, 2, ARGS), {"a": 1}, (1, 2), {"a": 1})
        self.false((1, 2, ARGS), {"a": 1}, (1, 2, 3, 4), {"a": 1, "b": 2})
        self.false((1, 2, ARGS), {}, (), {})
        self.false((1, 2, ARGS), {}, (1, 3), {})
        self.false((1, 2, ARGS), {}, (3, 3), {})
        self.false((1, 2, ARGS), {"a": 1}, (), {})
        self.false((1, 2, ARGS), {"a": 1}, (1, 2), {})
        self.false((1, 2, ARGS), {"a": 1}, (1, 2), {"b": 2})

    def test_args_at_middle(self):
        self.true((1, ARGS, 4), {}, (1, 4), {})
        self.true((1, ARGS, 4), {}, (1, 2, 3, 4), {})
        self.true((1, ARGS, 4), {"a": 1}, (1, 4), {"a": 1})
        self.false((1, ARGS, 4), {"a": 1}, (1, 2, 3, 4), {"a": 1, "b": 2})
        self.false((1, ARGS, 4), {}, (), {})
        self.false((1, ARGS, 4), {}, (1, 5), {})
        self.false((1, ARGS, 4), {}, (5, 5), {})
        self.false((1, ARGS, 4), {"a": 1}, (), {})
        self.false((1, ARGS, 4), {"a": 1}, (1, 4), {})
        self.false((1, ARGS, 4), {"a": 1}, (1, 4), {"b": 2})

    def test_args_multiple(self):
        self.true((ARGS, 3, ARGS, 6, ARGS), {},
                  (1, 2, 3, 4, 5, 6), {})
        self.true((ARGS, ARGS, ARGS), {}, (1, 2, 3, 4, 5, 6), {})
        self.true((ARGS, ARGS, ARGS), {},  (), {})
        self.false((ARGS, 3, ARGS, 6, ARGS), {},
                   (1, 2, 3, 4, 5), {})
        self.false((ARGS, 3, ARGS, 6, ARGS), {},
                   (1, 2, 4, 5, 6), {})


class MockTest(TestCase):

    def setUp(self):
        self.paths = []
        class StubMocker(object):
            _recording = True
            def is_recording(self):
                return self._recording
            def replay(self):
                self._recording = False
            def act(_, path):
                self.paths.append(path)
                return 42
        self.StubMocker = StubMocker
        self.mocker = StubMocker()
        self.mock = Mock(self.mocker)

    def test_default_attributes(self):
        self.assertEquals(self.mock.__mocker__, self.mocker)
        self.assertEquals(self.mock.__mocker_path__, Path(self.mock))
        self.assertEquals(self.mock.__mocker_name__, None)
        self.assertEquals(self.mock.__mocker_spec__, None)
        self.assertEquals(self.mock.__mocker_type__, None)
        self.assertEquals(self.mock.__mocker_object__, None)
        self.assertEquals(self.mock.__mocker_passthrough__, False)
        self.assertEquals(self.mock.__mocker_patcher__, None)
        self.assertEquals(self.mock.__mocker_replace__, False)
        self.assertEquals(self.mock.__mocker_count__, True)

    def test_path(self):
        path = object()
        self.assertEquals(Mock(self.mocker, path).__mocker_path__, path)

    def test_object(self):
        mock = Mock(self.mocker, object="foo")
        self.assertEquals(mock.__mocker_object__, "foo")
        self.assertEquals(mock.__mocker_path__.root_object, "foo")

    def test_passthrough(self):
        mock = Mock(self.mocker, object="foo", passthrough=True)
        self.assertEquals(mock.__mocker_object__, "foo")
        self.assertEquals(mock.__mocker_passthrough__, True)

    def test_spec(self):
        C = object()
        self.assertEquals(Mock(self.mocker, spec=C).__mocker_spec__, C)

    def test_class_without_type(self):
        mock = Mock(self.mocker)
        self.assertEquals(mock.__class__, Mock)
        self.mocker.replay()
        self.assertEquals(mock.__class__, Mock)

    def test_class_with_type_when_recording(self):
        class C(object): pass
        mock = Mock(self.mocker, type=C)
        self.assertEquals(mock.__mocker_type__, C)
        self.assertEquals(mock.__class__, Mock)
        self.assertEquals(isinstance(mock, Mock), True)

    def test_class_with_type_when_replaying(self):
        class C(object): pass
        mock = Mock(self.mocker, type=C)
        self.mocker.replay()
        self.assertEquals(mock.__mocker_type__, C)
        self.assertEquals(mock.__class__, C)
        self.assertEquals(isinstance(mock, C), True)

    def test_auto_naming(self):
        named_mock = self.mock
        named_mock.attr
        another_name = named_mock
        named_mock = None # Can't find this one anymore.
        another_name.attr
        self.assertEquals(another_name.__mocker_name__, "named_mock")

    def test_auto_naming_on_self(self):
        self.named_mock = self.mock
        del self.mock
        self.named_mock.attr
        self.assertEquals(self.named_mock.__mocker_name__, "named_mock")

    def test_auto_naming_on_bad_self(self):
        self_ = self
        self = object() # No __dict__
        self_.named_mock = self_.mock
        self_.named_mock.attr
        self_.assertEquals(self_.named_mock.__mocker_name__, None)

    def test_auto_naming_without_getframe(self):
        getframe = sys._getframe
        sys._getframe = None
        try:
            self.named_mock = self.mock
            self.named_mock.attr
            self.assertEquals(self.named_mock.__mocker_name__, None)
        finally:
            sys._getframe = getframe

    def test_getattr(self):
        self.assertEquals(self.mock.attr, 42)
        (path,) = self.paths
        self.assertEquals(type(path), Path)
        self.assertTrue(path.parent_path is self.mock.__mocker_path__)
        self.assertEquals(path, self.mock.__mocker_path__ + 
                                Action("getattr", ("attr",), {}))

    def test_setattr(self):
        self.mock.attr = 24
        (path,) = self.paths
        self.assertEquals(type(path), Path)
        self.assertTrue(path.parent_path is self.mock.__mocker_path__)
        self.assertEquals(path, self.mock.__mocker_path__ + 
                                Action("setattr", ("attr", 24), {}))

    def test_delattr(self):
        del self.mock.attr
        (path,) = self.paths
        self.assertEquals(type(path), Path)
        self.assertTrue(path.parent_path is self.mock.__mocker_path__)
        self.assertEquals(path, self.mock.__mocker_path__ + 
                                Action("delattr", ("attr",), {}))

    def test_call(self):
        self.mock(1, a=2)
        (path,) = self.paths
        self.assertEquals(type(path), Path)
        self.assertTrue(path.parent_path is self.mock.__mocker_path__)
        self.assertEquals(path, self.mock.__mocker_path__ + 
                                Action("call", (1,), {"a": 2}))

    def test_contains(self):
        self.assertEquals("value" in self.mock, True) # True due to 42.
        (path,) = self.paths
        self.assertEquals(type(path), Path)
        self.assertTrue(path.parent_path is self.mock.__mocker_path__)
        self.assertEquals(path, self.mock.__mocker_path__ + 
                                Action("contains", ("value",), {}))

    def test_getitem(self):
        self.assertEquals(self.mock["key"], 42)
        (path,) = self.paths
        self.assertEquals(type(path), Path)
        self.assertTrue(path.parent_path is self.mock.__mocker_path__)
        self.assertEquals(path, self.mock.__mocker_path__ + 
                                Action("getitem", ("key",), {}))

    def test_setitem(self):
        self.mock["key"] = "value"
        (path,) = self.paths
        self.assertEquals(type(path), Path)
        self.assertTrue(path.parent_path is self.mock.__mocker_path__)
        self.assertEquals(path, self.mock.__mocker_path__ + 
                                Action("setitem", ("key", "value"), {}))

    def test_delitem(self):
        del self.mock["key"]
        (path,) = self.paths
        self.assertEquals(type(path), Path)
        self.assertTrue(path.parent_path is self.mock.__mocker_path__)
        self.assertEquals(path, self.mock.__mocker_path__ + 
                                Action("delitem", ("key",), {}))

    def test_len(self):
        self.assertEquals(len(self.mock), 42)
        (path,) = self.paths
        self.assertEquals(type(path), Path)
        self.assertTrue(path.parent_path is self.mock.__mocker_path__)
        self.assertEquals(path, self.mock.__mocker_path__ + 
                                Action("len", (), {}))

    def test_len_with_mock_result(self):
        self.mocker.act = lambda path: Mock(self.mocker)
        self.assertEquals(len(self.mock), 0)

    def test_len_transforms_match_error_to_attribute_error(self):
        """
        list() uses len() as a hint. When we mock iter(), it shouldn't
        explode due to the lack of len().
        """
        def raise_error(path):
            raise MatchError("Kaboom!")

        self.mocker.act = raise_error
        try:
            len(self.mock)
        except AttributeError, e:
            self.assertEquals(str(e), "Kaboom!")
        except MatchError:
            self.fail("Expected AttributeError, not MatchError.")
        else:
            self.fail("AttributeError not raised.")

    def test_nonzero(self):
        self.assertEquals(bool(self.mock), True) # True due to 42.
        (path,) = self.paths
        self.assertEquals(type(path), Path)
        self.assertTrue(path.parent_path is self.mock.__mocker_path__)
        self.assertEquals(path, self.mock.__mocker_path__ + 
                                Action("nonzero", (), {}))

    def test_nonzero_returns_true_on_match_error(self):
        """
        When an object doesn't define a boolean behavior explicitly, it
        should be handled as a true value by default, as Python usually
        does.
        """
        def raise_error(path):
            raise MatchError("Kaboom!")
        self.mocker.act = raise_error
        self.assertEquals(bool(self.mock), True)

    def test_iter(self):
        result_mock = Mock(self.mocker)
        self.mocker.act = lambda path: self.paths.append(path) or result_mock
        result = iter(self.mock)
        self.assertEquals(type(result), type(iter([])))
        self.assertEquals(list(result), [])
        (path,) = self.paths
        self.assertEquals(type(path), Path)
        self.assertTrue(path.parent_path is self.mock.__mocker_path__)
        self.assertEquals(path, self.mock.__mocker_path__ + 
                                Action("iter", (), {}))

    def test_passthrough_on_unexpected(self):
        class StubMocker(object):
            def act(self, path):
                if path.actions[-1].args == ("x",):
                    raise MatchError
                return 42
        class C(object):
            x = 123
            y = 321

        mock = Mock(StubMocker(), object=C())
        self.assertRaises(MatchError, getattr, mock, "x", 42)
        self.assertEquals(mock.y, 42)

        mock = Mock(StubMocker(), passthrough=True)
        self.assertRaises(MatchError, getattr, mock, "x", 42)
        self.assertEquals(mock.y, 42)

        mock = Mock(StubMocker(), object=C(), passthrough=True)
        self.assertEquals(mock.x, 123)
        self.assertEquals(mock.y, 42)

        mock = Mock(StubMocker(), passthrough=True)
        act = mock.__mocker_act__
        self.assertEquals(act("getattr", ("x",), 42, object=C()), 123)
        self.assertEquals(act("getattr", ("y",), 42, object=C()), 42)

    def test_act_with_object(self):
        obj = object()
        self.mock.__mocker_act__("kind", object=obj)
        (path,) = self.paths
        self.assertEquals(type(path), Path)
        self.assertTrue(path.parent_path is self.mock.__mocker_path__)
        self.assertTrue(path.root_object is obj)

    def test_reraise_assertion(self):
        class StubMocker(object):
            def act(self, path):
                message = os.linesep.join(["An", "- error", "- happened"])
                raise AssertionError(message)
        mock = Mock(StubMocker())
        try:
            mock.__mocker_act__("kind")
        except AssertionError, e:
            message = os.linesep.join(["[Mocker] Unmet expectation:",
                                       "",
                                       "=> An",
                                       " - error",
                                       " - happened",
                                       ""])
            self.assertEquals(str(e), message)
        else:
            self.fail("AssertionError not raised")

    def test_action_execute_and_path_str(self):
        """Check for kind support on Action.execute() and Path.__str__()."""
        mocker = Mocker()
        check = []
        for name, attr in Mock.__dict__.iteritems():
            if not name.startswith("__mocker_") and hasattr(attr, "__call__"):
                mock = mocker.mock()
                args = ["arg"] * (attr.func_code.co_argcount - 1)
                try:
                    attr(mock, *args)
                except:
                    pass
                else:
                    path = mocker.get_events()[-1].path
                    check.append((path, path.actions[-1]))

        for path, action in check:
            kind = action.kind

            try:
                str(path)
            except RuntimeError:
                self.fail("Kind %r not supported by Path.__str__()" % kind)

            try:
                action.execute(object())
            except RuntimeError:
                self.fail("Kind %r not supported by Action.execute()" % kind)
            except:
                pass


class EventTest(TestCase):

    def setUp(self):
        self.event = Event()

    def test_default_path(self):
        self.assertEquals(self.event.path, None)

    def test_path(self):
        path = object()
        event = Event(path)
        self.assertEquals(event.path, path)

    def test_add_and_get_tasks(self):
        task1 = self.event.add_task(Task())
        task2 = self.event.add_task(Task())
        self.assertEquals(self.event.get_tasks(), [task1, task2])

    def test_remove_task(self):
        task1 = self.event.add_task(Task())
        task2 = self.event.add_task(Task())
        task3 = self.event.add_task(Task())
        self.event.remove_task(task2)
        self.assertEquals(self.event.get_tasks(), [task1, task3])

    def test_default_matches(self):
        self.assertEquals(self.event.matches(None), False)

    def test_default_run(self):
        self.assertEquals(self.event.run(None), None)

    def test_default_satisfied(self):
        self.assertEquals(self.event.satisfied(), True)

    def test_default_verify(self):
        self.assertEquals(self.event.verify(), None)

    def test_default_replay(self):
        self.assertEquals(self.event.replay(), None)

    def test_default_restore(self):
        self.assertEquals(self.event.restore(), None)

    def test_matches_false(self):
        task1 = self.event.add_task(Task())
        task1.matches = lambda path: True
        task2 = self.event.add_task(Task())
        task2.matches = lambda path: False
        task3 = self.event.add_task(Task())
        task3.matches = lambda path: True
        self.assertEquals(self.event.matches(None), False)

    def test_matches_true(self):
        task1 = self.event.add_task(Task())
        task1.matches = lambda path: True
        task2 = self.event.add_task(Task())
        task2.matches = lambda path: True
        self.assertEquals(self.event.matches(None), True)

    def test_matches_argument(self):
        calls = []
        task = self.event.add_task(Task())
        task.matches = lambda path: calls.append(path)
        self.event.matches(42)
        self.assertEquals(calls, [42])

    def test_run(self):
        calls = []
        task1 = self.event.add_task(Task())
        task1.run = lambda path: calls.append(path) or True
        task2 = self.event.add_task(Task())
        task2.run = lambda path: calls.append(path) or False
        task3 = self.event.add_task(Task())
        task3.run = lambda path: calls.append(path) or None
        self.assertEquals(self.event.run(42), False)
        self.assertEquals(calls, [42, 42, 42])

    def test_run_errors(self):
        class MyTask(object):
            def __init__(self, id, failed):
                self.id = id
                self.failed = failed
            def run(self, path):
                if self.failed:
                    raise AssertionError("%d failed" % self.id)
        event = Event("i.am.a.path")
        event.add_task(MyTask(1, True))
        event.add_task(MyTask(2, False))
        event.add_task(MyTask(3, True))

        try:
            event.run("i.am.a.path")
        except AssertionError, e:
            message = os.linesep.join(["i.am.a.path",
                                       "- 1 failed",
                                       "- 3 failed"])
            self.assertEquals(str(e), message)
        else:
            self.fail("AssertionError not raised")

    def test_run_errors_with_different_path_representation(self):
        """When the path representation isn't the same it's shown up."""
        class MyTask(object):
            def __init__(self, id, failed):
                self.id = id
                self.failed = failed
            def run(self, path):
                if self.failed:
                    raise AssertionError("%d failed" % self.id)
        event = Event("i.am.a.path")
        event.add_task(MyTask(1, True))
        event.add_task(MyTask(2, False))
        event.add_task(MyTask(3, True))

        try:
            event.run(42)
        except AssertionError, e:
            message = os.linesep.join(["i.am.a.path",
                                       "- Run: 42", # <==
                                       "- 1 failed",
                                       "- 3 failed"])
            self.assertEquals(str(e), message)
        else:
            self.fail("AssertionError not raised")

    def test_run_errors_need_good_messages(self):
        class MyTask(Task):
            def run(self, path):
                raise AssertionError()
        self.event.add_task(MyTask())
        self.assertRaises(RuntimeError, self.event.run, 42)

    def test_has_run(self):
        self.assertFalse(self.event.has_run())
        self.event.run(None)
        self.assertTrue(self.event.has_run())

    def test_has_run_reset_on_replay(self):
        self.event.run(None)
        self.event.replay()
        self.assertFalse(self.event.has_run())

    def test_may_run(self):
        calls = []
        task1 = Task()
        task1.may_run = lambda path: calls.append((1, path)) or True
        task2 = Task()
        task2.may_run = lambda path: calls.append((2, path))

        self.assertEquals(self.event.may_run(42), True)

        self.event.add_task(task1)
        self.assertEquals(self.event.may_run(42), True)
        self.assertEquals(calls, [(1, 42)])

        del calls[:]
        self.event.add_task(task2)
        self.event.add_task(task1) # Should return on first false.
        self.assertEquals(self.event.may_run(42), False)
        self.assertEquals(calls, [(1, 42), (2, 42)])

    def test_satisfied_false(self):
        def raise_error():
            raise AssertionError("An error")
        task1 = self.event.add_task(Task())
        task2 = self.event.add_task(Task())
        task2.verify = raise_error
        task3 = self.event.add_task(Task())
        self.assertEquals(self.event.satisfied(), False)

    def test_satisfied_true(self):
        task1 = self.event.add_task(Task())
        task1.satisfied = lambda: True
        task2 = self.event.add_task(Task())
        task2.satisfied = lambda: True
        self.assertEquals(self.event.satisfied(), True)

    def test_verify(self):
        class MyTask(object):
            def __init__(self, id, failed):
                self.id = id
                self.failed = failed
            def verify(self):
                if self.failed:
                    raise AssertionError("%d failed" % self.id)
        event = Event("i.am.a.path")
        event.add_task(MyTask(1, True))
        event.add_task(MyTask(2, False))
        event.add_task(MyTask(3, True))

        try:
            event.verify()
        except AssertionError, e:
            message = os.linesep.join(["i.am.a.path",
                                       "- 1 failed",
                                       "- 3 failed"])
            self.assertEquals(str(e), message)
        else:
            self.fail("AssertionError not raised")

    def test_verify_errors_need_good_messages(self):
        class MyTask(Task):
            def verify(self):
                raise AssertionError()
        self.event.add_task(MyTask())
        self.assertRaises(RuntimeError, self.event.verify)

    def test_replay(self):
        calls = []
        task1 = self.event.add_task(Task())
        task2 = self.event.add_task(Task())
        task1.replay = lambda: calls.append("task1")
        task2.replay = lambda: calls.append("task2")
        self.event.replay()
        self.assertEquals(calls, ["task1", "task2"])

    def test_restore(self):
        calls = []
        task1 = self.event.add_task(Task())
        task2 = self.event.add_task(Task())
        task1.restore = lambda: calls.append("task1")
        task2.restore = lambda: calls.append("task2")
        self.event.restore()
        self.assertEquals(calls, ["task1", "task2"])


class ReplayRestoreEventTest(TestCase):

    def setUp(self):
        self.event = ReplayRestoreEvent()

    def test_never_matches(self):
        self.assertEquals(self.event.matches(None), False)
        self.event.add_task(Task())
        self.assertEquals(self.event.matches(None), False)


class TaskTest(TestCase):

    def setUp(self):
        self.task = Task()

    def test_default_matches(self):
        self.assertEquals(self.task.matches(None), True)

    def test_default_may_run(self):
        self.assertEquals(self.task.may_run(None), True)

    def test_default_run(self):
        self.assertEquals(self.task.run(None), None)

    def test_default_verify(self):
        self.assertEquals(self.task.verify(), None)

    def test_default_replay(self):
        self.assertEquals(self.task.replay(), None)

    def test_default_restore(self):
        self.assertEquals(self.task.restore(), None)


class OnRestoreCallerTest(TestCase):

    def setUp(self):
        self.mocker = CleanMocker()
        self.mock = self.mocker.mock()

    def test_is_task(self):
        self.assertTrue(isinstance(OnRestoreCaller(None), Task))

    def test_restore(self):
        calls = []
        task = OnRestoreCaller(lambda: calls.append("callback"))
        self.assertEquals(calls, [])
        task.restore()
        self.assertEquals(calls, ["callback"])
        task.restore()
        self.assertEquals(calls, ["callback", "callback"])


class PathMatcherTest(TestCase):

    def setUp(self):
        self.mocker = CleanMocker()
        self.mock = self.mocker.mock()

    def test_is_task(self):
        self.assertTrue(isinstance(PathMatcher(None), Task))

    def test_create(self):
        path = object()
        task = PathMatcher(path)
        self.assertEquals(task.path, path)

    def test_matches(self):
        path = Path(self.mock, None, [Action("getattr", ("attr1",), {})])
        task = PathMatcher(path)
        action = Action("getattr", (), {}, Path(self.mock))
        self.assertFalse(task.matches(action.path + action))
        action = Action("getattr", ("attr1",), {}, Path(self.mock))
        self.assertTrue(task.matches(action.path + action))

    def test_recorder(self):
        path = Path(self.mock, [Action("call", (), {})])
        event = Event(path)
        path_matcher_recorder(self.mocker, event)
        (task,) = event.get_tasks()
        self.assertEquals(type(task), PathMatcher)
        self.assertTrue(task.path is path)

    def test_is_standard_recorder(self):
        self.assertTrue(path_matcher_recorder in Mocker.get_recorders())


class RunCounterTest(TestCase):

    def setUp(self):
        self.mocker = CleanMocker()
        self.mock = self.mocker.mock()
        self.action = Action("getattr", ("attr",), {}, Path(self.mock))
        self.path = Path(self.mock, [self.action])
        self.event = Event(self.path)

    def test_is_task(self):
        self.assertTrue(isinstance(RunCounter(1), Task))

    def test_create_one_argument(self):
        task = RunCounter(2)
        self.assertEquals(task.min, 2)
        self.assertEquals(task.max, 2)

    def test_create_min_max(self):
        task = RunCounter(2, 3)
        self.assertEquals(task.min, 2)
        self.assertEquals(task.max, 3)

    def test_create_unbounded(self):
        task = RunCounter(2, None)
        self.assertEquals(task.min, 2)
        self.assertEquals(task.max, sys.maxint)

    def test_run_one_argument(self):
        task = RunCounter(2)
        task.run(self.path)
        task.run(self.path)
        self.assertRaises(AssertionError, task.run, self.path)

    def test_run_two_arguments(self):
        task = RunCounter(1, 2)
        task.run(self.path)
        task.run(self.path)
        self.assertRaises(AssertionError, task.run, self.path)

    def test_may_run(self):
        task = RunCounter(1)
        self.assertEquals(task.may_run(None), True)
        task.run(self.path)
        self.assertEquals(task.may_run(None), False)

    def test_verify(self):
        task = RunCounter(2)
        self.assertRaises(AssertionError, task.verify)
        task.run(self.path)
        self.assertRaises(AssertionError, task.verify)
        task.run(self.path)
        task.verify()
        self.assertRaises(AssertionError, task.run, self.path)
        self.assertRaises(AssertionError, task.verify)

    def test_verify_two_arguments(self):
        task = RunCounter(1, 2)
        self.assertRaises(AssertionError, task.verify)
        task.run(self.path)
        task.verify()
        task.run(self.path)
        task.verify()
        self.assertRaises(AssertionError, task.run, self.path)
        self.assertRaises(AssertionError, task.verify)

    def test_verify_unbound(self):
        task = RunCounter(1, None)
        self.assertRaises(AssertionError, task.verify)
        task.run(self.path)
        task.verify()
        task.run(self.path)
        task.verify()

    def test_reset_on_replay(self):
        task = RunCounter(1, 1)
        task.run(self.path)
        self.assertRaises(AssertionError, task.run, self.path)
        task.replay()
        self.assertRaises(AssertionError, task.verify)
        task.run(self.path)
        self.assertRaises(AssertionError, task.run, self.path)

    def test_recorder(self):
        run_counter_recorder(self.mocker, self.event)
        (task,) = self.event.get_tasks()
        self.assertEquals(type(task), ImplicitRunCounter)
        self.assertTrue(task.min == 1)
        self.assertTrue(task.max == 1)

    def test_recorder_wont_record_when_count_is_false(self):
        self.mock.__mocker_count__ = False
        run_counter_recorder(self.mocker, self.event)
        self.assertEquals(self.event.get_tasks(), [])

    def test_removal_recorder(self):
        """
        Events created by getattr actions which lead to other events
        may be repeated any number of times.
        """
        path1 = Path(self.mock)
        path2 = path1 + Action("getattr", ("attr",), {}, path1)
        path3 = path2 + Action("getattr", ("attr",), {}, path2)
        path4 = path3 + Action("call", (), {}, path3)
        path5 = path4 + Action("call", (), {}, path4)

        event3 = self.mocker.add_event(Event(path3))
        event2 = self.mocker.add_event(Event(path2))
        event5 = self.mocker.add_event(Event(path5))
        event4 = self.mocker.add_event(Event(path4))

        event2.add_task(RunCounter(1))
        event2.add_task(ImplicitRunCounter(1))
        event2.add_task(RunCounter(1))
        event3.add_task(RunCounter(1))
        event3.add_task(ImplicitRunCounter(1))
        event3.add_task(RunCounter(1))
        event4.add_task(RunCounter(1))
        event4.add_task(ImplicitRunCounter(1))
        event4.add_task(RunCounter(1))
        event5.add_task(RunCounter(1))
        event5.add_task(ImplicitRunCounter(1))
        event5.add_task(RunCounter(1))
        
        # First, when the previous event isn't a getattr.

        run_counter_removal_recorder(self.mocker, event5)

        self.assertEquals(len(event2.get_tasks()), 3)
        self.assertEquals(len(event3.get_tasks()), 3)
        self.assertEquals(len(event4.get_tasks()), 3)
        self.assertEquals(len(event5.get_tasks()), 3)

        # Now, for real.

        run_counter_removal_recorder(self.mocker, event4)

        self.assertEquals(len(event2.get_tasks()), 3)
        self.assertEquals(len(event3.get_tasks()), 2)
        self.assertEquals(len(event4.get_tasks()), 3)
        self.assertEquals(len(event5.get_tasks()), 3)

        task1, task2 = event3.get_tasks()
        self.assertEquals(type(task1), RunCounter)
        self.assertEquals(type(task2), RunCounter)

    def test_removal_recorder_with_obj(self):

        self.mocker.add_recorder(run_counter_recorder)
        self.mocker.add_recorder(run_counter_removal_recorder)

        obj = self.mocker.mock()

        obj.x.y()()

        events = self.mocker.get_events()
        self.assertEquals(len(events), 4)
        self.assertEquals(len(events[0].get_tasks()), 0)
        self.assertEquals(len(events[1].get_tasks()), 0)
        self.assertEquals(len(events[2].get_tasks()), 1)
        self.assertEquals(len(events[3].get_tasks()), 1)

    def test_reset_on_replay_with_mock(self):
        mock = self.mocker.mock()
        mock()
        self.mocker.count(1)
        self.mocker.replay()
        mock()
        self.assertRaises(AssertionError, mock)
        self.mocker.replay()
        mock()
        self.assertRaises(AssertionError, mock)

    def test_is_standard_recorder(self):
        self.assertTrue(run_counter_recorder in Mocker.get_recorders())
        self.assertTrue(run_counter_removal_recorder in Mocker.get_recorders())


class MockReturnerTest(TestCase):

    def setUp(self):
        self.mocker = CleanMocker()
        self.mock = self.mocker.mock()
        self.action = Action("getattr", ("attr",), {}, Path(self.mock))
        self.path = Path(self.mock, [self.action])
        self.event = Event(self.path)

    def test_is_task(self):
        self.assertTrue(isinstance(MockReturner(self.mocker), Task))

    def test_create(self):
        task = MockReturner(self.mocker)
        mock = task.run(self.path)
        self.assertTrue(isinstance(mock, Mock))
        self.assertEquals(mock.__mocker__, self.mocker)
        self.assertTrue(mock.__mocker_path__.matches(self.path))

    def test_recorder(self):
        path1 = Path(self.mock)
        path2 = path1 + Action("getattr", ("attr",), {}, path1)
        path3 = path2 + Action("getattr", ("attr",), {}, path2)
        path4 = path3 + Action("call", (), {}, path3)

        event2 = self.mocker.add_event(Event(path2))
        event3 = self.mocker.add_event(Event(path3))
        event4 = self.mocker.add_event(Event(path4))

        self.assertEquals(len(event2.get_tasks()), 0)
        self.assertEquals(len(event3.get_tasks()), 0)
        self.assertEquals(len(event4.get_tasks()), 0)

        # Calling on 4 should add it only to the parent.

        mock_returner_recorder(self.mocker, event4)

        self.assertEquals(len(event2.get_tasks()), 0)
        self.assertEquals(len(event3.get_tasks()), 1)
        self.assertEquals(len(event4.get_tasks()), 0)

        (task,) = event3.get_tasks()
        self.assertEquals(type(task), MockReturner)
        self.assertEquals(task.mocker, self.mocker)

        # Calling on it again shouldn't do anything.

        mock_returner_recorder(self.mocker, event4)

        self.assertEquals(len(event2.get_tasks()), 0)
        self.assertEquals(len(event3.get_tasks()), 1)
        self.assertEquals(len(event4.get_tasks()), 0)

    def test_is_standard_recorder(self):
        self.assertTrue(mock_returner_recorder in Mocker.get_recorders())


class FunctionRunnerTest(TestCase):

    def setUp(self):
        self.mocker = CleanMocker()
        self.mock = self.mocker.mock()
        self.action = Action("call", (1, 2), {"c": 3}, Path(self.mock))
        self.path = Path(self.mock, None, [self.action])
        self.event = Event(self.path)

    def test_is_task(self):
        self.assertTrue(isinstance(FunctionRunner(None), Task))

    def test_run(self):
        task = FunctionRunner(lambda *args, **kwargs: repr((args, kwargs)))
        result = task.run(self.path)
        self.assertEquals(result, "((1, 2), {'c': 3})")


class PathExecuterTest(TestCase):

    def setUp(self):
        self.mocker = CleanMocker()

    def test_is_task(self):
        self.assertTrue(isinstance(PathExecuter(), Task))

    def test_run(self):
        class C(object):
            pass
        obj = C()
        obj.x = C()
        obj.x.y = lambda a, b: a+b

        path = Path(None, obj, [Action("getattr", ("x",), {}),
                                Action("getattr", ("y",), {}),
                                Action("call", (1,), {"b": 2})])

        task = PathExecuter()
        self.assertEquals(task.run(path), 3)

    def test_run_with_result_callback(self):
        class C(object):
            def x(self, arg):
                return 41 + arg
        obj = C()

        path = Path(None, obj, [Action("getattr", ("x",), {}),
                                Action("call", (1,), {})])

        calls = []
        result_callback = lambda result: calls.append(result)
        task = PathExecuter(result_callback)
        self.assertEquals(task.get_result_callback(), result_callback)
        self.assertEquals(task.run(path), 42)
        self.assertEquals(calls, [42])


class OrdererTest(TestCase):

    def setUp(self):
        self.mocker = CleanMocker()
        self.mock = self.mocker.mock()
        self.action = Action("call", (1, 2, Path(self.mock)), {"c": 3})
        self.path = Path(self.mock, [self.action])

    def test_is_task(self):
        self.assertTrue(isinstance(Orderer(self.path), Task))

    def test_path(self):
        self.assertEquals(Orderer(self.path).path, self.path)

    def test_has_run(self):
        orderer = Orderer(self.path)
        self.assertFalse(orderer.has_run())
        orderer.run(self.path)
        self.assertTrue(orderer.has_run())

    def test_reset_on_replay(self):
        orderer = Orderer(self.path)
        orderer.run(self.path)
        orderer.replay()
        self.assertFalse(orderer.has_run())

    def test_reset_on_replay_with_mock(self):
        self.mocker.add_recorder(path_matcher_recorder)
        mock = self.mocker.mock()
        self.mocker.order(mock(1), mock(2))
        self.mocker.replay()
        mock(1)
        mock(2)
        self.mocker.replay()
        self.assertRaises(AssertionError, mock, 2)

    def test_add_and_get_dependencies(self):
        orderer = Orderer(self.path)
        orderer.add_dependency(1)
        orderer.add_dependency(2)
        self.assertEquals(orderer.get_dependencies(), [1, 2])

    def test_may_run(self):
        orderer1 = Orderer(self.path)
        orderer2 = Orderer(self.path)
        orderer2.add_dependency(orderer1)
        self.assertFalse(orderer2.may_run(None))
        self.assertTrue(orderer1.may_run(None))
        orderer1.run(self.path)
        self.assertTrue(orderer2.may_run(None))

    def test_run_with_missing_dependency(self):
        orderer1 = Orderer("path1")
        orderer2 = Orderer("path2")
        orderer2.add_dependency(orderer1)
        try:
            orderer2.run(None)
        except AssertionError, e:
            self.assertEquals(str(e), "Should be after: path1")
        else:
            self.fail("AssertionError not raised")


class SpecCheckerTest(TestCase):

    def setUp(self):
        class C(object):
            def __call__(self, a, b, c=3): pass
            def normal(self, a, b, c=3): pass
            def varargs(self, a, b, c=3, *args): pass
            def varkwargs(self, a, b, c=3, **kwargs): pass
            def varargskwargs(self, a, b, c=3, *args, **kwargs): pass
            def klass(cls, a, b, c=3): pass
            klass = classmethod(klass)
            def static(a, b, c=3): pass
            static = staticmethod(static)
            def noargs(self): pass
            def klassnoargs(cls): pass
            klassnoargs = classmethod(klassnoargs)
            def staticnoargs(): pass
            staticnoargs = staticmethod(staticnoargs)
        self.cls = C
        self.mocker = CleanMocker()
        self.mock = self.mocker.mock(self.cls)

    def path(self, *args, **kwargs):
        action = Action("call", args, kwargs, Path(self.mock))
        return action.path + action

    def good(self, method_names, args_expr):
        if type(method_names) is not list:
            method_names = [method_names]
        for method_name in method_names:
            task = SpecChecker(getattr(self.cls, method_name, None))
            path = eval("self.path(%s)" % args_expr)
            self.assertEquals(task.may_run(path), True)
            try:
                task.run(path)
            except AssertionError:
                self.fail("AssertionError raised with self.cls.%s(%s)"
                          % (method_name, args_expr))

    def bad(self, method_names, args_expr):
        if type(method_names) is not list:
            method_names = [method_names]
        for method_name in method_names:
            task = SpecChecker(getattr(self.cls, method_name, None))
            path = eval("self.path(%s)" % args_expr)
            self.assertEquals(task.may_run(path), False)
            try:
                task.run(path)
            except AssertionError:
                pass
            else:
                self.fail("AssertionError not raised with self.cls.%s(%s)"
                          % (method_name, args_expr))

    def test_get_method(self):
        task = SpecChecker(self.cls.noargs)
        self.assertEquals(task.get_method(), self.cls.noargs)

    def test_is_standard_recorder(self):
        self.assertTrue(spec_checker_recorder in Mocker.get_recorders())

    def test_is_task(self):
        self.assertTrue(isinstance(SpecChecker(self.cls.normal), Task))

    def test_error_message(self):
        task = SpecChecker(self.cls.normal)
        try:
            task.run(self.path(1))
        except AssertionError, e:
            self.assertEquals(str(e), "Specification is normal(a, b, c=3): "
                                      "'b' not provided")
        else:
            self.fail("AssertionError not raised")

    def test_verify_unexistent_method(self):
        task = SpecChecker(None)
        try:
            task.verify()
        except AssertionError, e:
            self.assertEquals(str(e), "Method not found in real specification")
        else:
            self.fail("AssertionError not raised")

    def test_unsupported_object_for_getargspec(self):
        from zlib import adler32
        # If that fails, this test has to change because either adler32 has
        # changed, or the implementation of getargspec has changed.
        self.assertRaises(TypeError, inspect.getargspec, adler32)
        try:
            task = SpecChecker(adler32)
            task.run(self.path("asd"))
        except TypeError, e:
            self.fail("TypeError: %s" % str(e))

    def test_recorder(self):
        self.mocker.add_recorder(spec_checker_recorder)
        obj = self.mocker.mock(spec=self.cls)
        obj.noargs()
        getattr, call = self.mocker.get_events()
        self.assertEquals(getattr.get_tasks(), [])
        (task,) = call.get_tasks()
        self.assertEquals(type(task), SpecChecker)
        self.assertEquals(task.get_method(), self.cls.noargs)

    def test_recorder_with_unexistent_method(self):
        self.mocker.add_recorder(spec_checker_recorder)
        obj = self.mocker.mock(spec=self.cls)
        obj.unexistent()
        getattr, call = self.mocker.get_events()
        self.assertEquals(getattr.get_tasks(), [])
        (task,) = call.get_tasks()
        self.assertEquals(type(task), SpecChecker)
        self.assertEquals(task.get_method(), None)

    def test_recorder_second_action_isnt_call(self):
        self.mocker.add_recorder(spec_checker_recorder)
        obj = self.mocker.mock(spec=self.cls)
        obj.noargs.x
        event1, event2 = self.mocker.get_events()
        self.assertEquals(event1.get_tasks(), [])
        self.assertEquals(event2.get_tasks(), [])

    def test_recorder_first_action_isnt_getattr(self):
        self.mocker.add_recorder(spec_checker_recorder)
        obj = self.mocker.mock(spec=self.cls)
        obj.__mocker_act__("anyother", ("attr",))()
        event1, event2 = self.mocker.get_events()
        self.assertEquals(event1.get_tasks(), [])
        self.assertEquals(event2.get_tasks(), [])

    def test_recorder_more_than_two_actions(self):
        self.mocker.add_recorder(spec_checker_recorder)
        obj = self.mocker.mock(spec=self.cls)
        obj.noargs().x
        event1, event2, event3 = self.mocker.get_events()
        self.assertEquals(len(event1.get_tasks()), 0)
        self.assertEquals(len(event2.get_tasks()), 1)
        self.assertEquals(len(event3.get_tasks()), 0)

    def test_recorder_with_call_on_object(self):
        self.mocker.add_recorder(spec_checker_recorder)
        obj = self.mocker.mock(spec=self.cls)
        obj()
        (call,) = self.mocker.get_events()
        (task,) = call.get_tasks()
        self.assertEquals(type(task), SpecChecker)
        self.assertEquals(task.get_method(), self.cls.__call__)

    def test_recorder_more_than_one_action_with_direct_call(self):
        self.mocker.add_recorder(spec_checker_recorder)
        obj = self.mocker.mock(spec=self.cls)
        obj().x
        event1, event2 = self.mocker.get_events()
        self.assertEquals(len(event1.get_tasks()), 1)
        self.assertEquals(len(event2.get_tasks()), 0)

    def test_noargs(self):
        methods = ["noargs", "klassnoargs", "staticnoargs"]
        self.good(methods, "")
        self.bad(methods, "1")
        self.bad(methods, "a=1")

    def test_args_and_kwargs(self):
        methods = ["__call__", "normal", "varargs", "varkwargs",
                   "varargskwargs", "static", "klass"]
        self.good(methods, "1, 2")
        self.good(methods, "1, 2, 3")
        self.good(methods, "1, b=2")
        self.good(methods, "1, b=2, c=3")
        self.good(methods, "a=1, b=2")
        self.good(methods, "a=1, b=2, c=3")

    def test_too_much(self):
        methods = ["__call__", "normal", "static", "klass"]
        self.bad(methods, "1, 2, 3, 4")
        self.bad(methods, "1, 2, d=4")

    def test_missing(self):
        methods = ["__call__", "normal", "varargs", "varkwargs",
                   "varargskwargs", "static", "klass"]
        self.bad(methods, "")
        self.bad(methods, "1")
        self.bad(methods, "c=3")
        self.bad(methods, "a=1")
        self.bad(methods, "b=2, c=3")

    def test_duplicated_argument(self):
        methods = ["__call__", "normal", "varargs", "varkwargs",
                   "varargskwargs", "static", "klass"]
        self.bad(methods, "1, 2, b=2")

    def test_varargs(self):
        self.good("varargs", "1, 2, 3, 4")
        self.bad("varargs", "1, 2, 3, 4, d=3")

    def test_varkwargs(self):
        self.good("varkwargs", "1, 2, d=3")
        self.bad("varkwargs", "1, 2, 3, 4, d=3")

    def test_varargskwargs(self):
        self.good("varargskwargs", "1, 2, 3, 4, d=3")

    def test_unexistent(self):
        self.bad("unexistent", "")


class ProxyReplacerTest(TestCase):

    def setUp(self):
        self.mocker = CleanMocker()
        import calendar
        self.mock = Mock(self.mocker, object=calendar)
        self.task = ProxyReplacer(self.mock)

    def tearDown(self):
        self.task.restore()

    def test_is_task(self):
        self.assertTrue(isinstance(ProxyReplacer(None), Task))

    def test_mock(self):
        mock = object()
        task = ProxyReplacer(mock)
        self.assertEquals(task.mock, mock)

    def test_defaults_to_not_installed(self):
        import calendar
        self.assertEquals(type(calendar), ModuleType)

    def test_install(self):
        self.task.replay()
        import calendar
        self.assertEquals(type(calendar), Mock)
        self.assertTrue(calendar is self.mock)

    def test_install_protects_mock(self):
        self.task.replay()
        self.assertEquals(type(self.mock.__mocker_object__), ModuleType)

    def test_install_protects_path(self):
        self.task.replay()
        self.assertEquals(type(self.mock.__mocker_path__.root_object),
                          ModuleType)

    def test_deinstall_protects_task(self):
        self.task.replay()
        self.task.restore()
        self.assertEquals(type(self.task.mock), Mock)

    def test_install_protects_anything_with_mocker_replace_false(self):
        class C(object):
            def __init__(self):
                import calendar
                self.calendar = calendar
                self.__mocker_replace__ = False
        obj = C()
        self.task.replay()
        self.assertEquals(type(self.mock.__mocker_path__.root_object),
                          ModuleType)

    def test_install_on_object(self):
        class C(object):
            def __init__(self):
                import calendar
                self.calendar = calendar
        obj = C()
        self.task.replay()
        self.assertEquals(type(obj.calendar), Mock)
        self.assertTrue(obj.calendar is self.mock)

    def test_install_on_submodule(self):
        from os import path
        import os
        mock = Mock(self.mocker, object=path)
        task = ProxyReplacer(mock)
        task.replay()
        try:
            self.assertEquals(type(os.path), Mock)
            self.assertTrue(os.path is mock)
        finally:
            task.restore()

    def test_uninstall_on_restore(self):
        self.task.replay()
        self.task.restore()
        import calendar
        self.assertEquals(type(calendar), ModuleType)
        self.assertEquals(calendar.__name__, "calendar")

    def test_uninstall_from_object(self):
        class C(object):
            def __init__(self):
                import calendar
                self.calendar = calendar
        obj = C()
        self.task.replay()
        self.task.restore()
        self.assertEquals(type(obj.calendar), ModuleType)
        self.assertEquals(obj.calendar.__name__, "calendar")

    def test_uninstall_from_submodule(self):
        from os import path
        import os
        mock = Mock(self.mocker, object=path)
        task = ProxyReplacer(mock)
        self.assertEquals(type(os.path), ModuleType)
        task.replay()
        task.restore()
        self.assertEquals(type(os.path), ModuleType)


class PatcherTest(TestCase):

    def setUp(self):
        self.mocker = Mocker()
        self.patcher = Patcher()
        self.C = type("C", (object,), {})
        self.D = type("D", (self.C,), {})
        self.E = type("E", (), {})

        class MockStub(object):
            def __mocker_act__(self, kind, args=(), kwargs={}, object=None):
                return (kind, args, kwargs, object)

        self.MockStub = MockStub

    def test_is_task(self):
        self.assertTrue(isinstance(Patcher(), Task))

    def test_undefined_repr(self):
        self.assertEquals(repr(Undefined), "Undefined")

    def test_is_monitoring_unseen_class_kind(self):
        self.assertFalse(self.patcher.is_monitoring(self.C, "kind"))

    def test_monitor_class(self):
        self.patcher.monitor(self.C, "kind")
        self.assertTrue(self.patcher.is_monitoring(self.C, "kind"))

    def test_monitor_subclass(self):
        self.patcher.monitor(self.C, "kind")
        self.assertTrue(self.patcher.is_monitoring(self.D, "kind"))

    def test_monitor_unknown_class(self):
        self.patcher.monitor(self.C, "kind")
        self.assertFalse(self.patcher.is_monitoring(self.E, "kind"))

    def test_is_monitoring_unseen_instance(self):
        obj = self.E()
        self.patcher.monitor(self.C, "kind")
        self.assertFalse(self.patcher.is_monitoring(obj, "kind"))

    def test_is_monitoring_instance_explicitly_monitored(self):
        obj = self.C()
        self.patcher.monitor(obj, "kind")
        self.assertTrue(self.patcher.is_monitoring(obj, "kind"))

    def test_is_monitoring_instance_monitored_by_class(self):
        obj = self.D()
        self.patcher.monitor(self.D, "kind")
        self.assertTrue(self.patcher.is_monitoring(obj, "kind"))

    def test_patch_attr(self):
        self.patcher.patch_attr(self.C, "attr", "patch")
        self.assertEquals(self.C.__dict__.get("attr"), "patch")

    def test_patch_attr_and_restore(self):
        self.patcher.patch_attr(self.C, "attr", "patch")
        self.patcher.restore()
        self.assertTrue("attr" not in self.C.__dict__)

    def test_patch_attr_and_restore_to_original(self):
        self.C.attr = "original"
        self.patcher.patch_attr(self.C, "attr", "patch")
        self.patcher.restore()
        self.assertEquals(self.C.__dict__.get("attr"), "original")

    def test_get_unpatched_attr_unpatched_undefined(self):
        self.assertEquals(self.patcher.get_unpatched_attr(self.C, "attr"),
                          Undefined)

    def test_get_unpatched_attr_unpatched(self):
        self.C.attr = "original"
        self.assertEquals(self.patcher.get_unpatched_attr(self.C, "attr"),
                          "original")

    def test_get_unpatched_attr_defined_on_superclass(self):
        self.C.attr = "original"
        self.assertEquals(self.patcher.get_unpatched_attr(self.D, "attr"),
                          "original")

    def test_get_unpatched_attr_defined_on_superclass_patched_on_sub(self):
        self.C.attr = "original"
        self.patcher.patch_attr(self.D, "attr", "patch")
        self.assertEquals(self.patcher.get_unpatched_attr(self.D, "attr"),
                          "original")

    def test_get_unpatched_attr_patched_originally_undefined(self):
        self.patcher.patch_attr(self.C, "attr", "patch")
        self.assertEquals(self.patcher.get_unpatched_attr(self.C, "attr"),
                          Undefined)

    def test_get_unpatched_attr_patched(self):
        self.C.attr = "original"
        self.patcher.patch_attr(self.C, "attr", "patch")
        self.assertEquals(self.patcher.get_unpatched_attr(self.C, "attr"),
                          "original")

    def test_get_unpatched_attr_on_instance_originally_undefined(self):
        self.assertEquals(self.patcher.get_unpatched_attr(self.C(), "attr"),
                          Undefined)

    def test_get_unpatched_attr_on_instance(self):
        self.C.attr = "original"
        self.assertEquals(self.patcher.get_unpatched_attr(self.D(), "attr"),
                          "original")

    def test_get_unpatched_attr_on_instance_defined_on_superclass(self):
        self.C.attr = "original"
        self.patcher.patch_attr(self.C, "attr", "patch")
        self.assertEquals(self.patcher.get_unpatched_attr(self.D(), "attr"),
                          "original")

    def test_get_unpatched_attr_on_instance_with_descriptor(self):
        self.C.attr = property(lambda self: "original")
        self.patcher.patch_attr(self.C, "attr", "patch")
        self.assertEquals(self.patcher.get_unpatched_attr(self.D(), "attr"),
                          "original")

    def test_get_unpatched_attr_on_subclass_with_descriptor(self):
        calls = []
        class Property(object):
            def __get__(self, obj, cls):
                calls.append((obj, cls))
                return "original"
        self.C.attr = Property()
        self.patcher.patch_attr(self.C, "attr", "patch")
        self.assertEquals(self.patcher.get_unpatched_attr(self.D, "attr"),
                          "original")
        self.assertEquals(calls, [(None, self.D)])

    def test_get_unpatched_attr_on_instance_with_fake_descriptor(self):
        class BadProperty(object):
            def __init__(self):
                # On real, __get__ must be on the class, not on the instance.
                self.__get__ = lambda self, obj, cls=None: "original"
        prop = BadProperty()
        self.C.attr = prop
        self.patcher.patch_attr(self.C, "attr", "patch")
        self.assertEquals(self.patcher.get_unpatched_attr(self.D(), "attr"),
                          prop)

    def test_replay_with_monitored_class(self):
        self.patcher.monitor(self.C, "call")
        self.patcher.replay()
        self.assertEquals(type(self.C.__dict__["__call__"]), PatchedMethod)

    def test_replay_with_monitored_instance(self):
        self.patcher.monitor(self.C(), "call")
        self.patcher.replay()
        self.assertEquals(type(self.C.__dict__["__call__"]), PatchedMethod)

    def test_replay_getattr(self):
        self.patcher.monitor(self.C, "getattr")
        self.patcher.replay()
        self.assertEquals(type(self.C.__dict__["__getattribute__"]),
                          PatchedMethod)

    def test_restore(self):
        self.patcher.monitor(self.C, "call")
        self.patcher.replay()
        self.patcher.restore()
        self.assertTrue("__call__" not in self.C.__dict__)

    def test_restore_twice_does_nothing(self):
        self.patcher.monitor(self.C, "call")
        self.patcher.replay()
        self.patcher.restore()
        self.C.__call__ = "original"
        self.patcher.restore()
        self.assertTrue(self.C.__dict__.get("__call__"), "original")

    def test_patched_call_on_instance(self):
        self.patcher.monitor(self.C, "call")
        obj = self.C()
        obj.__mocker_mock__ = self.MockStub()
        self.patcher.replay()
        result = obj(1, a=2)
        self.assertEquals(result, ("call", (1,), {"a": 2}, obj))

    def test_patched_call_on_class(self):
        self.patcher.monitor(self.C, "call")
        self.C.__mocker_mock__ = self.MockStub()
        self.patcher.replay()
        obj = self.C()
        result = obj(1, a=2)
        self.assertEquals(result, ("call", (1,), {"a": 2}, obj))

    def test_patched_call_on_class_edge_case(self):
        """Only "getattr" kind should passthrough on __mocker_* arguments."""
        self.patcher.monitor(self.C, "call")
        self.C.__mocker_mock__ = self.MockStub()
        self.patcher.replay()
        obj = self.C()
        result = obj("__mocker_mock__")
        self.assertEquals(result, ("call", ("__mocker_mock__",), {}, obj))

    def test_patched_getattr_on_class(self):
        self.patcher.monitor(self.C, "getattr")
        self.C.__mocker_mock__ = self.MockStub()
        self.patcher.replay()
        obj = self.C()
        result = obj.attr
        self.assertEquals(result, ("getattr", ("attr",), {}, obj))

    def test_patched_getattr_on_unmonitored_object(self):
        obj1 = self.C()
        obj1.__mocker_mock__ = self.MockStub()
        self.patcher.monitor(obj1, "getattr")
        obj2 = self.C()
        obj2.attr = "original"
        self.patcher.replay()
        self.assertEquals(obj1.attr, ("getattr", ("attr",), {}, obj1))
        self.assertEquals(obj2.attr, "original")

    def test_patched_getattr_on_different_instances(self):
        def build_getattr(original):
            def __getattribute__(self, name):
                if name == "attr":
                    return original
                return object.__getattribute__(self, name)
            return __getattribute__
        self.C.__getattribute__ = build_getattr("originalC")
        self.D.__getattribute__ = build_getattr("originalD")

        class MockStub(object):
            def __init__(self, id):
                self.id = id
            def __mocker_act__(self, kind, args=(), kwargs={}, object=None):
                return self.id

        obj1, obj2, obj3, obj4, obj5, obj6 = [self.C() for i in range(6)]
        obj7, obj8, obj9 = [self.D() for i in range(3)]

        obj2.__mocker_mock__ = MockStub(2)
        self.patcher.monitor(obj2, "getattr")
        obj5.__mocker_mock__ = MockStub(5)
        self.patcher.monitor(obj5, "getattr")
        obj8.__mocker_mock__ = MockStub(8)
        self.patcher.monitor(obj8, "getattr")

        self.patcher.replay()
        self.assertEquals(obj1.attr, "originalC")
        self.assertEquals(obj2.attr, 2)
        self.assertEquals(obj3.attr, "originalC")
        self.assertEquals(obj4.attr, "originalC")
        self.assertEquals(obj5.attr, 5)
        self.assertEquals(obj6.attr, "originalC")
        self.assertEquals(obj7.attr, "originalD")
        self.assertEquals(obj8.attr, 8)
        self.assertEquals(obj9.attr, "originalD")

    def test_patched_getattr_execute_getattr(self):
        class C(object):
            def __getattribute__(self, attr):
                if attr == "attr":
                    return "original"
        action = Action("getattr", ("attr",), {})
        obj = C()
        self.patcher.monitor(obj, "getattr")
        self.patcher.replay()
        self.assertEquals(self.patcher.execute(action, obj), "original")

    def test_execute_getattr_on_unexistent(self):
        action = Action("getattr", ("attr",), {})
        obj = self.C()
        self.patcher.monitor(obj, "getattr")
        self.patcher.replay()
        self.assertRaises(AttributeError, self.patcher.execute, action, obj)

    def test_patched_real_getattr_on_different_instances(self):
        def build_getattr(original):
            def __getattr__(self, name):
                if name == "attr":
                    return original
                return object.__getattr__(self, name)
            return __getattr__
        self.C.__getattr__ = build_getattr("originalC")
        self.D.__getattr__ = build_getattr("originalD")

        class MockStub(object):
            def __init__(self, id):
                self.id = id
            def __mocker_act__(self, kind, args=(), kwargs={}, object=None):
                return self.id

        obj1, obj2, obj3, obj4, obj5, obj6 = [self.C() for i in range(6)]
        obj7, obj8, obj9 = [self.D() for i in range(3)]

        obj2.__mocker_mock__ = MockStub(2)
        self.patcher.monitor(obj2, "getattr")
        obj5.__mocker_mock__ = MockStub(5)
        self.patcher.monitor(obj5, "getattr")
        obj8.__mocker_mock__ = MockStub(8)
        self.patcher.monitor(obj8, "getattr")

        self.patcher.replay()
        self.assertEquals(obj1.attr, "originalC")
        self.assertEquals(obj2.attr, 2)
        self.assertEquals(obj3.attr, "originalC")
        self.assertEquals(obj4.attr, "originalC")
        self.assertEquals(obj5.attr, 5)
        self.assertEquals(obj6.attr, "originalC")
        self.assertEquals(obj7.attr, "originalD")
        self.assertEquals(obj8.attr, 8)
        self.assertEquals(obj9.attr, "originalD")

    def test_patched_real_getattr_execute_getattr(self):
        class C(object):
            def __getattr__(self, attr):
                if attr == "attr":
                    return "original"
        action = Action("getattr", ("attr",), {})
        obj = C()
        self.patcher.monitor(obj, "getattr")
        self.patcher.replay()
        self.assertEquals(self.patcher.execute(action, obj), "original")

    def test_execute_call(self):
        class C(object):
            def __call__(self, *args, **kwargs):
                return (args, kwargs)
        action = Action("call", (1,), {"a": 2})
        obj = C()
        self.patcher.monitor(obj, "call")
        self.patcher.replay()
        self.assertEquals(self.patcher.execute(action, obj), ((1,), {"a": 2}))

    def test_recorder_class_getattr(self):
        self.C.method = lambda: None
        mock = self.mocker.patch(self.C)
        mock.method()
        self.mocker.result("mocked")
        self.mocker.replay()
        self.assertEquals(self.C().method(), "mocked")
        self.assertRaises(AssertionError, self.C().method)

    def test_recorder_instance_getattr(self):
        self.C.attr = "original"
        obj1 = self.C()
        obj2 = self.C()
        mock = self.mocker.patch(obj1)
        mock.attr
        self.mocker.result("mocked")
        self.mocker.replay()
        self.assertEquals(obj1.attr, "mocked")
        self.assertRaises(AssertionError, getattr, obj1, "attr")
        self.assertEquals(obj2.attr, "original")
        self.assertRaises(AttributeError, getattr, obj1, "unexistent")

    def test_recorder_passthrough(self):
        class C(object):
            def __init__(self):
                self.result = "original" # Value on object's dictionary.
            def method(self):
                return self.result
        mock = self.mocker.patch(C)
        mock.method()
        self.mocker.passthrough()
        self.mocker.replay()
        obj = C()
        self.assertEquals(obj.method(), "original")
        self.assertRaises(AssertionError, obj.method)

########NEW FILE########
__FILENAME__ = test_storage
import gc
import unittest
import __builtin__

import holland.backup.mysqldump.mock.storage as storage

dict_backend = storage.backend
FILE = '/myfile.txt'


class TestFileType(unittest.TestCase):
    def setUp(self):
        gc.collect()
        gc.collect()

    def test_patching(self):
        storage.replace_builtins()
        try:
            self.assertEqual(__builtin__.file, storage.file)
            self.assertEqual(__builtin__.open, storage.open)
        finally:
            storage.restore_builtins()

        self.assertEqual(__builtin__.file, storage.original_file)
        self.assertNotEqual(storage.original_file, storage.file)
        self.assertEqual(__builtin__.open, storage.original_open)
        self.assertNotEqual(storage.original_open, storage.open)


    def test_file_simple_read_and_write(self):
        source_data = 'Some text\nwith newlines\n'
        
        handle = storage.file(FILE, 'w')
        self.assertEquals(handle.mode, 'w')
        handle.write(source_data)
        handle.close()
        
        handle = storage.file(FILE,'r')
        data = handle.read()
        self.assertEquals(handle.mode, 'r')
        handle.close()
        self.assertEqual(data, source_data)
        
        handle = storage.file(FILE)
        self.assertEquals(handle.mode, 'r')
        data = handle.read()
        handle.close()
        self.assertEqual(data, source_data)
        
        
    def test_open_simple_read_and_write(self):
        source_data = 'Some text\nwith newlines\n'
        
        handle = storage.open(FILE, 'w')
        self.assertTrue(isinstance(handle, storage.file))
        self.assertEquals(handle.mode, 'w')
        handle.write(source_data)
        handle.close()
        
        handle = storage.open(FILE, 'r')
        self.assertTrue(isinstance(handle, storage.file))
        data = handle.read()
        self.assertEquals(handle.mode, 'r')
        handle.close()
        self.assertEqual(data, source_data)
        
        handle = storage.open(FILE)
        self.assertTrue(isinstance(handle, storage.file))
        self.assertEquals(handle.mode, 'r')
        data = handle.read()
        handle.close()
        self.assertEqual(data, source_data)
    
    
    def test_read_to_write(self):
        handle = storage.file(FILE, 'w')
        handle.write('some new data')
        handle.close()
        
        h = storage.file(FILE)
        self.assertRaises(IOError, h.write, 'foobar')

        
    def test_write_to_read(self):
        h = storage.file(FILE, 'w')
        self.assertRaises(IOError, h.read)
    
    
    def test_multiple_writes(self):
        handle = storage.file(FILE, 'w')
        handle.write('foo')
        handle.write('bar')
        handle.close()
        
        self.assertEqual(storage.file(FILE).read(), 'foobar')

        
    def test_repr(self):
        write = storage.file(FILE, 'w')
        read = storage.file(FILE, 'r')
        string = '<open file %r mode %r>'
        self.assertEqual(repr(read), string % (FILE, 'r'))
        self.assertEqual(repr(write), string % (FILE, 'w'))
        
        string = '<closed file %r mode %r>'
        write.close()
        read.close()
        self.assertEqual(repr(read), string % (FILE, 'r'))
        self.assertEqual(repr(write), string % (FILE, 'w'))
    
    
    def test_invalid_mode(self):
        self.assertRaises(ValueError, storage.file, 'filename', 'q')
        self.assertRaises(ValueError, storage.file, 'filename', '')
        self.assertRaises(TypeError, storage.file, 'filename', None)
        self.assertRaises(TypeError, storage.file, 'filename', 3)
    
        
    def test_open_nonexistent_file(self):
        self.assertRaises(IOError, storage.file, FILE + FILE, 'r')
    
    
    def test_open_write_deletes_and_creates(self):
        handle = storage.file(FILE, 'w')
        self.assertEqual(storage.file(FILE).read(), '')
        handle.write('foobar')
        handle.close()
        
        handle = storage.file(FILE, 'w')
        self.assertEqual(storage.file(FILE).read(), '')
        
    
    def test_gc_closes(self):
        handle = storage.file(FILE, 'w')
        handle.write('some new data')
        del handle
        gc.collect()
        gc.collect()
        self.assertEqual(storage.file(FILE).read(), 'some new data')
    

    def foo_closed(self):
        handle = storage.file(FILE, 'w')
        self.assertFalse(handle.closed)
        
        handle.close()
        self.assertTrue(handle.closed)
        
        self.assertRaises(ValueError, handle.write, 'foo')
        
        handle = storage.file(FILE)
        self.assertFalse(handle.closed)
        
        handle.close()
        self.assertTrue(handle.closed)
        
        self.assertRaises(ValueError, handle.read)
        
        
    def test_read_seek_tell(self):
        storage._store.clear()
        h = storage.file(FILE, 'w')
        h.write('foobar')
        h.close()
        
        h = storage.file(FILE)
        self.assertEqual(h.tell(), 0)
        
        self.assertEqual(h.read(0), '')
        self.assertEqual(h.read(1), 'f')
        self.assertEqual(h.tell(), 1)
        h.seek(0)
        self.assertEqual(h.tell(), 0)
        self.assertEqual(h.read(1), 'f')
        
        self.assertEqual(h.read(), 'oobar')
        self.assertEqual(h.read(), '')
        
        h.seek(0)
        self.assertEqual(h.read(100), 'foobar')
        
        h.seek(1000)
        self.assertEqual(h.tell(), 1000)
        self.assertEqual(h.read(100), '')
        
        self.assertRaises(IOError, h.seek, -1)
        self.assertRaises(TypeError, h.seek, None)
        
        # test deprecation warning for float value?


    def test_write_seek_tell(self):
        h = storage.file(FILE, 'w')
        self.assertEqual(h.tell(), 0)
        
        h.write('f')
        self.assertEqual(h.tell(), 1)
        h.seek(2)
        self.assertEqual(h.tell(), 2)
        h.write('g')
        h.close()
        self.assertEqual(storage.file(FILE).read(), 'f\x00g')
        
        h = storage.file(FILE, 'w')
        h.write('f')
        h.seek(0)
        h.write('g')
        h.close()
        self.assertEqual(storage.file(FILE).read(), 'g')
        
        h = storage.file(FILE, 'w')
        h.seek(1000)
        h.write('g')
        h.close()
        
        expected = '\x00' * 1000 + 'g'
        self.assertEqual(storage.file(FILE).read(), expected)
        
        self.assertRaises(IOError, h.seek, -1)
        self.assertRaises(TypeError, h.seek, None)
        
        
    def test_flush(self):
        h = storage.file(FILE, 'w')
        h.write('foo')
        
        read_handle = storage.file(FILE)
        self.assertEqual(read_handle.read(), '')
        
        h.flush()
        self.assertEqual(read_handle.read(), 'foo')
        h.close()
        
        self.assertRaises(IOError, read_handle.flush)
        read_handle.close()


    def test_read_write_binary(self):
        h = storage.file(FILE, 'w')
        h.write('foo\nbar\n')
        h.close()
    
        h = storage.file(FILE)
        self.assertEqual(h.read(), 'foo\nbar\n')
        h.close()
        
        h = storage.file(FILE, 'rb')
        self.assertEqual(h.read(), 'foo\r\nbar\r\n')
        h.close()
        
        h = storage.file(FILE, 'wb')
        h.write('foo\nbar\n')
        h.close()
        
        h = storage.file(FILE, 'rb')
        self.assertEqual(h.read(), 'foo\nbar\n')
        h.close()
        
        h = storage.file(FILE, 'w')
        h.write('foo\nbar\n')
        h.close()
        
        h = storage.file(FILE, 'rb')
        self.assertEqual(h.read(), 'foo\r\nbar\r\n')
        h.close()

    
    def test_assorted_members(self):
        h = storage.file(FILE, 'w')
        self.assertEquals(h.encoding, None)
        self.assertEqual(h.errors, None)
        self.assertEqual(h.newlines, None)
        self.assertFalse(h.isatty())
        h.close()
        
        h = storage.file(FILE)
        self.assertEquals(h.encoding, None)
        self.assertEqual(h.errors, None)
        self.assertEqual(h.newlines, None)
        self.assertFalse(h.isatty())
        h.close()
    
    
    def test_fileno(self):
        h = storage.file(FILE, 'w')
        h2 = storage.file(FILE)
        fileno1 = h.fileno()
        fileno2 = h2.fileno()
        
        self.assertTrue(isinstance(fileno1, int))
        self.assertTrue(isinstance(fileno2, int))
        
        self.assertTrue(fileno1 > 2)
        self.assertTrue(fileno2 > 2)
        
        self.assertNotEqual(fileno1, fileno2)
        
        h.close()
        h2.close()
        
    
    def test__iter__(self):
        # not as hard as you might think
        h = storage.file(FILE, 'w')
        i = h.__iter__()
        self.assertTrue(h is i)
        h.close()
    
    
    def test_next(self):
        h = storage.file(FILE, 'w')
        h.write('foo\nbar\nbaz\n')
        self.assertRaises(IOError, h.next)
        h.close()
        
        h = storage.file(FILE)
        self.assertEqual(h.next(), 'foo\n')
        
        self.assertRaises(ValueError, h.read)
        
        self.assertEqual(h.next(), 'bar\n')
        self.assertEqual(h.next(), 'baz\n')
        self.assertRaises(StopIteration, h.next)
        h.close()
        
        h = storage.file(FILE)
        self.assertEqual(h.next(), 'foo\n')
        h.seek(1)
        self.assertEqual(h.next(), 'oo\n')
        
        h.seek(3)
        self.assertEqual(h.read(4), '\nbar')
        self.assertEqual(h.next(), '\n')
    
    
    def test_readline(self):
        h = storage.file(FILE, 'w')
        h.write('foo\nbar\nbaz\n')
        self.assertRaises(IOError, h.readline)
        h.close()
        
        h = storage.file(FILE)
        self.assertEqual(h.readline(), 'foo\n')
        self.assertEqual(h.tell(), 4)
        h.seek(0)
        self.assertEqual(h.readline(), 'foo\n')
        
        self.assertRaises(TypeError, h.readline, None)
        self.assertEqual(h.readline(0), '')
        self.assertEqual(h.readline(1), 'b')
        self.assertEqual(h.readline(100), 'ar\n')
        self.assertEqual(h.tell(), 8)
        self.assertEqual(h.readline(-1), 'baz\n')
        self.assertEqual(h.tell(), 12)
        self.assertEqual(h.readline(), '')
        self.assertEqual(h.tell(), 12)
        h.close()
    
    
    def test_readlines(self):
        h = storage.file(FILE, 'w')
        h.write('foo\nbar\nbaz\n')
        self.assertRaises(IOError, h.readlines)
        h.close()
        
        h = storage.file(FILE)
        self.assertEqual(h.readlines(), ['foo\n', 'bar\n', 'baz\n'])
        self.assertEqual(h.readlines(), [])
        h.seek(0)
        
        self.assertRaises(TypeError, h.readline, None)
        self.assertEqual(h.readlines(0), ['foo\n', 'bar\n', 'baz\n'])
        h.close()

    def test_xreadlines(self):
        h = storage.file(FILE, 'w')
        self.assertTrue(h.xreadlines() is h)
        h.close()
        
        
    def test_softspace(self):
        h = storage.file(FILE, 'w')
        h.softspace = 1
        h.write('blam')
        self.assertEqual(h.softspace, 0)
        
        def set_softspace():
            h.softspace = 'kablooie'
        self.assertRaises(TypeError, set_softspace)
        
        h.close()
    
    
    def test_truncate(self):
        h = storage.file(FILE, 'w')
        h.write('kabloooie')
        
        self.assertRaises(IOError, storage.file(FILE).truncate)
        self.assertRaises(TypeError, h.truncate, 'foo')
        self.assertRaises(IOError, h.truncate, -1)
        
        h.seek(3)
        h.truncate()
        self.assertEqual(storage.file(FILE).read(), 'kab')
        
        h.truncate(10)
        self.assertEqual(storage.file(FILE).read(), 'kab\x00\x00\x00\x00\x00\x00\x00')
        self.assertEqual(h.tell(), 3)
        
        h.truncate(2)
        self.assertEqual(h.tell(), 3)
        h.close()


    def test_writelines(self):
        h = storage.file(FILE, 'w')
        
        
        self.assertRaises(IOError, storage.file(FILE).writelines, [])
        self.assertRaises(TypeError, h.writelines, object())
        
        h.write('blah')
        h.writelines(['\n','q', 'w', 'e'])
        h.close()
        
        f = storage.file(FILE)
        data = f.read()
        f.close()
        self.assertEqual(data, 'blah\nqwe')
    
    def test_append_mode(self):
        source_data = 'Some text\nwith newlines\n'
        storage._store.clear()
        handle = storage.open(FILE, 'ab')
        try:
            f = storage.open(FILE)
            try:
                self.assertEqual(f.read(), '')
            finally:
                f.close()
            
            handle.write(source_data)
        finally:
            handle.close()
        
        handle = storage.open(FILE, 'rb')
        try:
            self.assertEqual(handle.read(), source_data)
        finally:
            handle.close()
        
        handle = storage.open(FILE, 'a')
        try:
            self.assertEqual(handle.tell(), len(source_data))
            handle.write(source_data[::-1])
        finally:
            handle.close()
        
        handle = storage.open(FILE)
        try:
            self.assertEqual(handle.read(), source_data + source_data[::-1])
        finally:
            handle.close()
        
    
    def test_read_write_mode(self):
        #self.assertRaises(IOError, storage.file, FILE, 'r+')

        h = storage.file(FILE, 'w')
        h.__enter__()
        try:
            h.write('foo')
        finally:
            h.__exit__()
        
        h = storage.file(FILE, 'r+')
        h.__enter__()
        try:
            self.assertEqual(h.tell(), 0)
            self.assertEqual(h.read(), 'foo')
            self.assertEqual(h.tell(), 3)
            h.write('bar')
            self.assertEqual(h.tell(), 6)
            h.seek(0)
            self.assertEqual(h.read(), 'foobar')
        finally:
            h.__exit__()
        
    def test_write_read_mode(self):
        f = storage.file(FILE, 'w')
        try:
            f.write('foo')
        finally:
            f.close()
        
        h = storage.file(FILE, 'w+')
        try:
            f = storage.file(FILE)
            try:
                self.assertEqual(f.read(), '')
            finally:
                f.close()
            
            self.assertEqual(h.read(), '')
            self.assertEqual(h.tell(), 0)
            h.write('foo')
            self.assertEqual(h.tell(), 3)
            self.assertEqual(h.read(), '')
            
            h.seek(0)
            self.assertEqual(h.read(), 'foo')
        finally:
            h.close()
    
    
    def test_append_read_mode(self):
        f = storage.file(FILE, 'w')
        try:
            f.write('foo')
        finally:
            f.close()
        
        h = storage.file(FILE, 'a+')
        try:
            f = storage.file(FILE)
            try:
                self.assertEqual(f.read(), 'foo')
            finally:
                f.close()
            
            self.assertEqual(h.tell(), 3)
            self.assertEqual(h.read(), '')
            h.write('bar')
            self.assertEqual(h.tell(), 6)
            self.assertEqual(h.read(), '')
            
            h.seek(0)
            self.assertEqual(h.read(), 'foobar')
        finally:
            h.close()


    def test_seek_with_whence(self):
        data = 'foo bar baz'
        h = storage.file(FILE, 'w')
        try:
            h.write(data)
        finally:
            h.close()
        
        h = storage.file(FILE)
        self.assertRaises(IOError, h.seek, 0, 3)
        self.assertRaises(IOError, h.seek, -1, 0)
        self.assertRaises(IOError, h.seek, 0, -1)
        self.assertRaises(TypeError, h.seek, 0, None)
        
        h.seek(3, 0)
        self.assertEqual(h.tell(), 3)
        
        h.seek(-3, 1)
        self.assertEqual(h.tell(), 0)
        self.assertRaises(IOError, h.seek, -1, 1)
        
        h.seek(3, 1)
        self.assertEqual(h.tell(), 3)
        
        h.seek(0, 2)
        self.assertEqual(h.tell(), len(data))
        
        h.seek(-2, 2)
        self.assertEqual(h.tell(), len(data) - 2)
        
        h.seek(2, 2)
        self.assertEqual(h.tell(), len(data) + 2)
        self.assertRaises(IOError, h.seek, -(len(data) + 1), 2)
        
    
    def test_read_only_attributes(self):
        def setter(attribute, value):
            return lambda: setattr(h, attribute, value)
            
        h = storage.file(FILE, 'w')
        try:
            self.assertRaises(AttributeError, setter('mode', 'w'))
            self.assertRaises(AttributeError, setter('name', 'foo2'))
            self.assertRaises(AttributeError, setter('closed', True))
            self.assertRaises(AttributeError, setter('encoding', 'ascii'))
            self.assertRaises(AttributeError, setter('errors', None))
            self.assertRaises(AttributeError, setter('newlines', 'foo'))
        finally:
            h.close()


    def test_read_negative(self):
        h = storage.file(FILE, 'w')
        try:
            h.write('foo bar baz')
        finally:
            h.close()

        h = storage.file(FILE)
        try:
            self.assertEqual(h.read(-3), 'foo bar baz')
        finally:
            h.close()
    
    
    def test_invalid_name(self):
        self.assertRaises(IOError, storage.file, '')
        self.assertRaises(IOError, storage.file, '', 'w')
        self.assertRaises(TypeError, storage.file, None)
        self.assertRaises(TypeError, storage.file, None, 'w')
        self.assertRaises(TypeError, storage.file, 3)

        
    def test_default_backend(self):
        storage.backend = dict_backend
        storage._store.clear()

        self.assertRaises(IOError, storage.file, FILE)
        
        h = storage.file(FILE, 'w')
        h.__enter__()
        try:
            h.write('foo')
        finally:
            h.__exit__()
        
        h = storage.file(FILE)
        h.__enter__()
        try:
            self.assertEqual(h.read(), 'foo')
        finally:
            h.__exit__()


"""
Differences from standard file type:

* Attempting to set the read-only attributes (like mode, name etc) raises an AttributeError
  rather than a TypeError
* The exception messages are not all identical (some are better!)
* Strict about modes. Unrecognised modes always raise an exception

  (NOTE: The exception method that the standard file type does throw is:
   "ValueError: mode string must begin with one of 'r', 'w', 'a' or 'U', not 'z'")
   
* The deprecated readinto is not implemented

TODO:

* The buffering argument to the constructor is not implemented
* The IOError exceptions raised don't have an associated errno
* encoding, errors and newlines do nothing
* Behavior of tell() and seek() for text mode files may be incorrect (it
  should treat '\n' as '\r\n')
* Behaves like Windows, writes '\n' as '\r\n' unless in binary mode. A global
  flag to control this?
* Universal modes not supported
* Missing __format__ method needed when we move to 2.6
* Implementations of os and os.path that work with storage_backend
"""

########NEW FILE########
__FILENAME__ = test__subprocess
from holland.backup.mysqldump.mock._subprocess import PopenMock
from holland.backup.mysqldump.mock import MockEnvironment
from nose.tools import assert_equals

def test_popen_communicate():
    pid = PopenMock(['echo', 'foo'], close_fds=True)
    result = pid.communicate()
    assert_equals(result, ('',''))

########NEW FILE########
__FILENAME__ = test_client
import sys
from holland.backup.mysqldump.mock.mocker import Mocker, ANY, ARGS, KWARGS
from holland.backup.mysqldump.mysql.client import MySQLClient, flatten_list
from nose.tools import assert_equals

mocker =  None
def _db_setup():
    global mocker
    mocker = Mocker()
    connect = mocker.replace('MySQLdb.connect')
    mock = connect(ARGS, KWARGS)
    mocker.result(mock)
    cursor = mock.cursor()
    cursor.execute(ANY)
    cursor.close()
    mocker.replay()

def _db_teardown():
    global mocker
    mocker.verify()
    mocker.restore()
    mocker = None

def test_flush_tables():
    client = MySQLClient(read_default_group='client')
    client.flush_tables()
test_flush_tables.setup = _db_setup
test_flush_tables.teardown = _db_teardown

def test_flush_tables_with_read_lock():
    client = MySQLClient(read_default_group='client')
    client.flush_tables_with_read_lock()
test_flush_tables_with_read_lock.setup = _db_setup
test_flush_tables_with_read_lock.teardown = _db_teardown

def test_unlock_tables():
    client = MySQLClient(read_default_group='client')
    client.unlock_tables()
test_unlock_tables.setup = _db_setup
test_unlock_tables.teardown = _db_teardown

def test_stop_slave():
    client = MySQLClient(read_default_group='client')
    client.stop_slave()
test_stop_slave.setup = _db_setup
test_stop_slave.teardown = _db_teardown

def test_start_slave():
    client = MySQLClient(read_default_group='client')
    client.start_slave()
test_start_slave.setup = _db_setup
test_start_slave.teardown = _db_teardown

def test_flatten_list():
    start = ['aaa', ['bb', 'ccccc']]
    expected = ['aaa', 'bb', 'ccccc']
    assert_equals(flatten_list(start), expected)

def test_show_databases():
    mocker = Mocker()
    connect = mocker.replace('MySQLdb.connect')
    link = connect(ARGS, KWARGS)
    mocker.result(link)
    cursor = link.cursor()
    cursor.execute('SHOW DATABASES')
    cursor.fetchall()
    mocker.result(['mysql', 'test'])
    cursor.close()
    mocker.replay()
    try:
        client = MySQLClient()
        databases = client.show_databases()
        assert_equals(databases, ['mysql', 'test'])
    finally:
        mocker.restore()

def test_show_tables():
    mocker = Mocker()
    connect = mocker.replace('MySQLdb.connect')
    link = connect(ARGS, KWARGS)
    mocker.result(link)
    cursor = link.cursor()
    cursor.execute('SHOW TABLES FROM `mysql`')
    iter(cursor)
    mocker.generate(['user', 'db'])
    cursor.close()
    mocker.replay()

    try:
        client = MySQLClient()
        tables = client.show_tables('mysql')
        assert_equals(tables, ['user', 'db'])
    finally:
        mocker.restore()

def test_50_metadata():
    mocker = Mocker()
    connect = mocker.replace('MySQLdb.connect')
    link = connect(ARGS, KWARGS)
    mocker.result(link)
    link.get_server_info()
    mocker.result('5.0.87')
    cursor = link.cursor()
    cursor.execute('SHOW TABLE STATUS FROM `mysql`')
    cursor.description
    fields = [('name',),('engine',),('data_length',),('index_length',)]
    mocker.result(fields)
    iter(cursor)
    results = [
        ('user', 'myisam',1024L,1024L),
        ('db', 'myisam', 1024L, 1024L),
    ]
    mocker.generate(results)
    cursor.close()
    mocker.replay()
    try:
        client = MySQLClient(read_default_file='/etc/my.cnf')
        expected_metadata = [
            { 'database' : 'mysql',
              'name' : 'user',
              'engine' : 'myisam',
              'data_size' : 1024L,
              'index_size' : 1024L,
              'is_transactional' : False
            },
            { 'database' : 'mysql',
              'name' : 'db',
              'engine' : 'myisam',
              'data_size' : 1024L,
              'index_size' : 1024L,
              'is_transactional' : False
            },
        ]
        md = client.show_table_metadata('mysql')
        assert_equals(md, expected_metadata)
        mocker.verify()
    finally:
        mocker.restore()

def test_51_metadata():
    mocker = Mocker()
    connect = mocker.replace('MySQLdb.connect')
    link = connect(ARGS, KWARGS)
    mocker.result(link)
    link.get_server_info()
    mocker.result('5.1.99')
    cursor = link.cursor()
    sql = ("SELECT TABLE_SCHEMA AS `database`, "
           "          TABLE_NAME AS `name`, "
           "          DATA_LENGTH AS `data_size`, "
           "          INDEX_LENGTH AS `index_size`, "
           "          ENGINE AS `engine`, "
           "          TRANSACTIONS AS `is_transactional` "
           "FROM INFORMATION_SCHEMA.TABLES "
           "JOIN INFORMATION_SCHEMA.ENGINES USING (ENGINE) "
           "WHERE TABLE_SCHEMA = %s")
    cursor.execute(sql, ('mysql'))
    cursor.description
    fields = [('database',),('name',),('data_size',),('index_size',),('engine',),('is_transactional',)]
    mocker.result(fields)
    cursor.fetchall()
    results = [
        ('mysql', 'user', 1024L,1024L, 'MyISAM', False),
        ('mysql', 'db', 1024L, 1024L, 'MyISAM', False),
    ]
    mocker.result(results)
    cursor.close()
    mocker.replay()
    try:
        client = MySQLClient(read_default_file='/etc/my.cnf')
        expected_metadata = [
            { 'database' : 'mysql',
              'name' : 'user',
              'engine' : 'MyISAM',
              'data_size' : 1024L,
              'index_size' : 1024L,
              'is_transactional' : False,
            },
            { 'database' : 'mysql',
              'name' : 'db',
              'engine' : 'MyISAM',
              'data_size' : 1024L,
              'index_size' : 1024L,
              'is_transactional' : False,
            },
        ]
        md = client.show_table_metadata('mysql')
        assert_equals(md, expected_metadata)
        mocker.verify()
    finally:
        mocker.restore()

def test_show_slave_status():
    mocker = Mocker()
    connect = mocker.replace('MySQLdb.connect')
    link = connect(ARGS, KWARGS)
    mocker.result(link)
    cursor = link.cursor()
    sql = 'SHOW SLAVE STATUS'
    cursor.execute(sql)
    cursor.description
    fields = (
        ('Slave_IO_State',),
        ('Master_Host',),
        ('Master_User',),
        ('Master_Port',),
        ('Connect_Retry',),
        ('Master_Log_File',),
        ('Read_Master_Log_Pos',),
        ('Relay_Log_File',),
        ('Relay_Log_Pos',),
        ('Relay_Master_Log_File',),
        ('Slave_IO_Running',),
        ('Slave_SQL_Running',),
        ('Replicate_Do_DB',),
        ('Replicate_Ignore_DB',),
        ('Replicate_Do_Table',),
        ('Replicate_Ignore_Table',),
        ('Replicate_Wild_Do_Table',),
        ('Replicate_Wild_Ignore_Table',),
        ('Last_Errno',),
        ('Last_Error',),
        ('Skip_Counter',),
        ('Exec_Master_Log_Pos',),
        ('Relay_Log_Space',),
        ('Until_Condition',),
        ('Until_Log_File',),
        ('Until_Log_Pos',),
        ('Master_SSL_Allowed',),
        ('Master_SSL_CA_File',),
        ('Master_SSL_CA_Path',),
        ('Master_SSL_Cert',),
        ('Master_SSL_Cipher',),
        ('Master_SSL_Key',),
        ('Seconds_Behind_Master',),
        ('Master_SSL_Verify_Server_Cert',),
        ('Last_IO_Errno',),
        ('Last_IO_Error',),
        ('Last_SQL_Errno',),
        ('Last_SQL_Error',),
    )
    mocker.result(fields)
    cursor.fetchone()
    result = (
        'Waiting for master to send event',
        '127.0.0.1',
        'msandbox',
        23351L,
        60L,
        'mysql-bin.000004',
        106L,
        'mysql_sandbox23353-relay-bin.001724',
        251L,
        'mysql-bin.000004',
        'Yes',
        'Yes',
        '',
        '',
        '',
        '',
        '',
        '',
        0L,
        '',
        0L,
        106L,
        564L,
        'None',
        '',
        0L,
        'No',
        '',
        '',
        '',
        '',
        '',
        0L,
        'No',
        0L,
        '',
        0L,
        ''
    )
    mocker.result(result)
    cursor.close()
    mocker.replay()
    try:
        client = MySQLClient(read_default_file='/etc/my.cnf')
        expected_status = dict(zip([f[0].lower() for f in fields], result))
        slave_status = client.show_slave_status()
        assert_equals(slave_status, expected_status)
        mocker.verify()
    finally:
        mocker.restore()

def test_show_slave_status_noslave():
    mocker = Mocker()
    connect = mocker.replace('MySQLdb.connect')
    link = connect(ARGS, KWARGS)
    mocker.result(link)
    cursor = link.cursor()
    sql = 'SHOW SLAVE STATUS'
    cursor.execute(sql)
    cursor.description
    fields = (
        ('Slave_IO_State',),
        ('Master_Host',),
        ('Master_User',),
        ('Master_Port',),
        ('Connect_Retry',),
        ('Master_Log_File',),
        ('Read_Master_Log_Pos',),
        ('Relay_Log_File',),
        ('Relay_Log_Pos',),
        ('Relay_Master_Log_File',),
        ('Slave_IO_Running',),
        ('Slave_SQL_Running',),
        ('Replicate_Do_DB',),
        ('Replicate_Ignore_DB',),
        ('Replicate_Do_Table',),
        ('Replicate_Ignore_Table',),
        ('Replicate_Wild_Do_Table',),
        ('Replicate_Wild_Ignore_Table',),
        ('Last_Errno',),
        ('Last_Error',),
        ('Skip_Counter',),
        ('Exec_Master_Log_Pos',),
        ('Relay_Log_Space',),
        ('Until_Condition',),
        ('Until_Log_File',),
        ('Until_Log_Pos',),
        ('Master_SSL_Allowed',),
        ('Master_SSL_CA_File',),
        ('Master_SSL_CA_Path',),
        ('Master_SSL_Cert',),
        ('Master_SSL_Cipher',),
        ('Master_SSL_Key',),
        ('Seconds_Behind_Master',),
        ('Master_SSL_Verify_Server_Cert',),
        ('Last_IO_Errno',),
        ('Last_IO_Error',),
        ('Last_SQL_Errno',),
        ('Last_SQL_Error',),
    )
    mocker.result(fields)
    cursor.fetchone()
    result = None
    mocker.result(result)
    cursor.close()
    mocker.replay()
    try:
        client = MySQLClient(read_default_file='/etc/my.cnf')
        expected_status = None
        slave_status = client.show_slave_status()
        assert_equals(slave_status, expected_status)
        mocker.verify()
    finally:
        mocker.restore()

def test_status():
    mocker = Mocker()
    connect = mocker.replace('MySQLdb.connect')
    link = connect(ARGS, KWARGS)
    mocker.result(link)
    cursor = link.cursor()
    cursor.execute('SHOW GLOBAL STATUS LIKE %s', ('Bytes_sent',))
    cursor.fetchone()
    mocker.result(('Bytes_sent', 978L))
    cursor.close()
    mocker.replay()

    try:
        client = MySQLClient()
        assert_equals(client.show_status('Bytes_sent'), 978)
    finally:
        mocker.restore()

def test_variable():
    mocker = Mocker()
    connect = mocker.replace('MySQLdb.connect')
    link = connect(ARGS, KWARGS)
    mocker.result(link)
    cursor = link.cursor()
    sql = 'SHOW SESSION VARIABLES LIKE %s'
    cursor.execute(sql, ('sql_log_bin',))
    mocker.result(1L)
    cursor.fetchone()
    mocker.result(('sql_log_bin', 'ON'))
    cursor.close()
    mocker.replay()

    try:
        client = MySQLClient()
        assert_equals(client.show_variable('sql_log_bin', session=True), 'ON')
    finally:
        mocker.restore()

def test_variable_nomatch():
    mocker = Mocker()
    connect = mocker.replace('MySQLdb.connect')
    link = connect(ARGS, KWARGS)
    mocker.result(link)
    cursor = link.cursor()
    sql = 'SHOW SESSION VARIABLES LIKE %s'
    cursor.execute(sql, ('postgresql',))
    mocker.result(0L)
    cursor.fetchone()
    mocker.result(None)
    cursor.close()
    mocker.replay()

    try:
        client = MySQLClient()
        assert_equals(client.show_variable('postgresql', session=True), None)
    finally:
        mocker.restore()

########NEW FILE########
__FILENAME__ = test_option
import logging
from StringIO import StringIO
from textwrap import dedent
from holland.backup.mysqldump.mysql.option import *
from holland.backup.mysqldump.mock.storage import replace_builtins, restore_builtins
from nose.tools import ok_, assert_equals, assert_raises

__test__ = False

def test_parse_section():
    assert_equals(parse_section('[client]', 1, None), 'client')
    assert_raises(SyntaxError, parse_section, 'client]', 1, None)

def test_parse_single_option():
    assert_equals(parse_single_option('password="foobarbaz"',1,None), ('password', 'foobarbaz'))
    assert_raises(SyntaxError, parse_single_option, "password = 'foo", 1, None)

def test_parse_bad_single_option():
    assert_raises(SyntaxError, parse_single_option, "'password' = foo", 1, None)

def test_parse_options():
    option_file = StringIO(dedent("""
        # Sample option file

        [client]
        user = "root"
        password = "Ziya8ln12"
        """).strip()
    )
    result = parse_options(option_file)
    ok_('client' in result)
    ok_('user' in result['client'])
    ok_('password' in result['client'])
    assert_equals(result['client']['password'], 'Ziya8ln12')
    assert_equals(result['client']['user'], 'root')

def test_parse_invalid_options():
    option_file = StringIO(dedent("""
        # Option with with no section

        user = "root"
        password = "Blah blah blah"
        """).strip()
    )
    assert_raises(SyntaxError, parse_options, option_file)

def test_unquote():
    assert_equals(unquote('"foo"'), 'foo')
    assert_equals(unquote('foo'), 'foo')

STD_DEFAULTS_FILE = StringIO("""
# A sample defaults file

[client]
user = "root"
password = "some other password"

[mysqldump]
single-transaction
all-databases
""")


def test_write_options():
    replace_builtins()
    try:
        write_options(SAMPLE_OPTIONS, '/mysql_defaults.conf')
        stream = open('/mysql_defaults.conf')
        assert_equals(parse_options(stream), SAMPLE_OPTIONS)
    finally:
        restore_builtins()
    ok_(not os.path.exists('/mysql_defaults.conf'),
            "Fake filesystem failure (FFF)")

def test_write_options_stream():
    try:
        stream_out = StringIO()
        write_options(SAMPLE_OPTIONS, stream_out)
        stream_in = StringIO(stream_out.getvalue())
        assert_equals(parse_options(stream_in), SAMPLE_OPTIONS)
    finally:
        pass

EMPTY_CONF = StringIO("""
[client]
""")

def test_write_options_empty_section():
    assert_equals(parse_options(EMPTY_CONF), {})
    output = StringIO()
    write_options({ 'client' : {} }, output)
    assert_equals(output.getvalue(), "")

def test_parse_options_multivalue():
    mysqldump_cnf = StringIO("""
    [mysqldump]
    ignore-table = mysql.user
    ignore-table = mysql.db
    ignore-table = mysql.proc
    ignore-table = mysql.time_zone
    """)
    assert_equals(parse_options(mysqldump_cnf), { 'mysqldump' : { 'ignore-table' : 'mysql.time_zone' } })

def test_write_options_multivalue():
    mysqldump_options = {
        'mysqldump' : {
            'ignore-table' : ['mysql.user', 'mysql.db', 'mysql.proc', 'mysql.timezone']
        }
    }
    expected = dedent("""
    [mysqldump]
    ignore-table = mysql.user
    ignore-table = mysql.db
    ignore-table = mysql.proc
    ignore-table = mysql.timezone
    """).lstrip()
    result = StringIO()
    write_options(mysqldump_options, result)
    assert_equals(result.getvalue(), expected)

def test_client_sections():
    mixed_cnf = StringIO("""
    [client]
    user = root
    password = "foo bar baz"
    default-character-set = utf8

    [mysqldump]
    single-transaction
    compact
    """)

    options_dict = parse_options(mixed_cnf)
    client_config = client_sections(options_dict)
    assert_equals(client_config, { 'client' : { 'user' : 'root', 'password' : 'foo bar baz' } })

def test_merge_option_dicts():
    global_my_cnf = StringIO("""
    [client]
    user = backup
    host = "192.168.100.250"
    port = 3306
    """)

    local_my_cnf = StringIO("""
    [client]
    password = "bar"
    """)

    global_config = parse_options(global_my_cnf)
    local_config = parse_options(local_my_cnf)
    assert_equals(merge_option_dicts(local_config, global_config),
                    { 'client' : {
                        'user' : 'backup',
                        'host' : '192.168.100.250',
                        'port' : '3306',
                        'password' : 'bar' 
                        }
                    })

def test_merge_option_dicts_disjoint_sections():
    global_my_cnf = StringIO("""
    [holland]
    user = backup
    host = "192.168.100.250"
    port = 3306
    """)

    local_my_cnf = StringIO("""
    [client]
    password = "bar"
    """)

    global_config = parse_options(global_my_cnf)
    local_config = parse_options(local_my_cnf)
    assert_equals(merge_option_dicts(local_config, global_config),
                    { 'holland' : {
                        'user' : 'backup',
                        'host' : '192.168.100.250',
                        'port' : '3306',
                      },
                      'client' : {
                        'password' : 'bar',
                      }
                    })

########NEW FILE########
__FILENAME__ = test_mysqldump
from holland.backup.mysqldump.mysql.dump import *
from nose.tools import assert_equals, assert_raises

__test__ = False

def test_collapse_extra_options():
    redundant_options = [
        '--master-data=2',
        '--master-data=2',
        '--master-data=2',
        '--single-transaction',
        '--single-transaction',
    ]

    collapse_extra_options(redundant_options)
    assert_equals(redundant_options, ['--master-data=2', '--single-transaction'])

def test_validate_invalid_option():
    assert_raises(Exception, validate_one_option, '--result-file=/etc/passwd')

########NEW FILE########
__FILENAME__ = test_compat
from holland.backup.mysqldump.util import compat as ConfigParser
import StringIO
import unittest
import UserDict

from test import test_support

class SortedDict(UserDict.UserDict):
    def items(self):
        result = self.data.items()
        result.sort()
        return result

    def keys(self):
        result = self.data.keys()
        result.sort()
        return result

    def values(self):
        result = self.items()
        return [i[1] for i in values]

    def iteritems(self): return iter(self.items())
    def iterkeys(self): return iter(self.keys())
    __iter__ = iterkeys
    def itervalues(self): return iter(self.values())

class TestCaseBase(unittest.TestCase):
    __test__ = False

    def newconfig(self, defaults=None):
        if defaults is None:
            self.cf = self.config_class()
        else:
            self.cf = self.config_class(defaults)
        return self.cf

    def fromstring(self, string, defaults=None):
        cf = self.newconfig(defaults)
        sio = StringIO.StringIO(string)
        cf.readfp(sio)
        return cf

    def test_basic(self):
        cf = self.fromstring(
            "[Foo Bar]\n"
            "foo=bar\n"
            "[Spacey Bar]\n"
            "foo = bar\n"
            "[Commented Bar]\n"
            "foo: bar ; comment\n"
            "[Long Line]\n"
            "foo: this line is much, much longer than my editor\n"
            "   likes it.\n"
            "[Section\\with$weird%characters[\t]\n"
            "[Internationalized Stuff]\n"
            "foo[bg]: Bulgarian\n"
            "foo=Default\n"
            "foo[en]=English\n"
            "foo[de]=Deutsch\n"
            "[Spaces]\n"
            "key with spaces : value\n"
            "another with spaces = splat!\n"
            )
        L = cf.sections()
        L.sort()
        eq = self.assertEqual
        eq(L, [r'Commented Bar',
               r'Foo Bar',
               r'Internationalized Stuff',
               r'Long Line',
               r'Section\with$weird%characters[' '\t',
               r'Spaces',
               r'Spacey Bar',
               ])

        # The use of spaces in the section names serves as a
        # regression test for SourceForge bug #583248:
        # http://www.python.org/sf/583248
        eq(cf.get('Foo Bar', 'foo'), 'bar')
        eq(cf.get('Spacey Bar', 'foo'), 'bar')
        eq(cf.get('Commented Bar', 'foo'), 'bar')
        eq(cf.get('Spaces', 'key with spaces'), 'value')
        eq(cf.get('Spaces', 'another with spaces'), 'splat!')

        self.failIf('__name__' in cf.options("Foo Bar"),
                    '__name__ "option" should not be exposed by the API!')

        # Make sure the right things happen for remove_option();
        # added to include check for SourceForge bug #123324:
        self.failUnless(cf.remove_option('Foo Bar', 'foo'),
                        "remove_option() failed to report existance of option")
        self.failIf(cf.has_option('Foo Bar', 'foo'),
                    "remove_option() failed to remove option")
        self.failIf(cf.remove_option('Foo Bar', 'foo'),
                    "remove_option() failed to report non-existance of option"
                    " that was removed")

        self.assertRaises(ConfigParser.NoSectionError,
                          cf.remove_option, 'No Such Section', 'foo')

        eq(cf.get('Long Line', 'foo'),
           'this line is much, much longer than my editor\nlikes it.')

    def test_case_sensitivity(self):
        cf = self.newconfig()
        cf.add_section("A")
        cf.add_section("a")
        L = cf.sections()
        L.sort()
        eq = self.assertEqual
        eq(L, ["A", "a"])
        cf.set("a", "B", "value")
        eq(cf.options("a"), ["b"])
        eq(cf.get("a", "b"), "value",
           "could not locate option, expecting case-insensitive option names")
        self.failUnless(cf.has_option("a", "b"))
        cf.set("A", "A-B", "A-B value")
        for opt in ("a-b", "A-b", "a-B", "A-B"):
            self.failUnless(
                cf.has_option("A", opt),
                "has_option() returned false for option which should exist")
        eq(cf.options("A"), ["a-b"])
        eq(cf.options("a"), ["b"])
        cf.remove_option("a", "B")
        eq(cf.options("a"), [])

        # SF bug #432369:
        cf = self.fromstring(
            "[MySection]\nOption: first line\n\tsecond line\n")
        eq(cf.options("MySection"), ["option"])
        eq(cf.get("MySection", "Option"), "first line\nsecond line")

        # SF bug #561822:
        cf = self.fromstring("[section]\nnekey=nevalue\n",
                             defaults={"key":"value"})
        self.failUnless(cf.has_option("section", "Key"))


    def test_default_case_sensitivity(self):
        cf = self.newconfig({"foo": "Bar"})
        self.assertEqual(
            cf.get("DEFAULT", "Foo"), "Bar",
            "could not locate option, expecting case-insensitive option names")
        cf = self.newconfig({"Foo": "Bar"})
        self.assertEqual(
            cf.get("DEFAULT", "Foo"), "Bar",
            "could not locate option, expecting case-insensitive defaults")

    def test_parse_errors(self):
        self.newconfig()
        self.parse_error(ConfigParser.ParsingError,
                         "[Foo]\n  extra-spaces: splat\n")
        self.parse_error(ConfigParser.ParsingError,
                         "[Foo]\n  extra-spaces= splat\n")
        self.parse_error(ConfigParser.ParsingError,
                         "[Foo]\noption-without-value\n")
        self.parse_error(ConfigParser.ParsingError,
                         "[Foo]\n:value-without-option-name\n")
        self.parse_error(ConfigParser.ParsingError,
                         "[Foo]\n=value-without-option-name\n")
        self.parse_error(ConfigParser.MissingSectionHeaderError,
                         "No Section!\n")

    def parse_error(self, exc, src):
        sio = StringIO.StringIO(src)
        self.assertRaises(exc, self.cf.readfp, sio)

    def test_query_errors(self):
        cf = self.newconfig()
        self.assertEqual(cf.sections(), [],
                         "new ConfigParser should have no defined sections")
        self.failIf(cf.has_section("Foo"),
                    "new ConfigParser should have no acknowledged sections")
        self.assertRaises(ConfigParser.NoSectionError,
                          cf.options, "Foo")
        self.assertRaises(ConfigParser.NoSectionError,
                          cf.set, "foo", "bar", "value")
        self.get_error(ConfigParser.NoSectionError, "foo", "bar")
        cf.add_section("foo")
        self.get_error(ConfigParser.NoOptionError, "foo", "bar")

    def get_error(self, exc, section, option):
        try:
            self.cf.get(section, option)
        except exc, e:
            return e
        else:
            self.fail("expected exception type %s.%s"
                      % (exc.__module__, exc.__name__))

    def test_boolean(self):
        cf = self.fromstring(
            "[BOOLTEST]\n"
            "T1=1\n"
            "T2=TRUE\n"
            "T3=True\n"
            "T4=oN\n"
            "T5=yes\n"
            "F1=0\n"
            "F2=FALSE\n"
            "F3=False\n"
            "F4=oFF\n"
            "F5=nO\n"
            "E1=2\n"
            "E2=foo\n"
            "E3=-1\n"
            "E4=0.1\n"
            "E5=FALSE AND MORE"
            )
        for x in range(1, 5):
            self.failUnless(cf.getboolean('BOOLTEST', 't%d' % x))
            self.failIf(cf.getboolean('BOOLTEST', 'f%d' % x))
            self.assertRaises(ValueError,
                              cf.getboolean, 'BOOLTEST', 'e%d' % x)

    def test_weird_errors(self):
        cf = self.newconfig()
        cf.add_section("Foo")
        self.assertRaises(ConfigParser.DuplicateSectionError,
                          cf.add_section, "Foo")

    def test_write(self):
        cf = self.fromstring(
            "[Long Line]\n"
            "foo: this line is much, much longer than my editor\n"
            "   likes it.\n"
            "[DEFAULT]\n"
            "foo: another very\n"
            " long line"
            )
        output = StringIO.StringIO()
        cf.write(output)
        self.assertEqual(
            output.getvalue(),
            "[Long Line]\n"
            "foo: this line is much, much longer than my editor\n"
            "   likes it.\n"
            "[DEFAULT]\n"
            "foo: another very\n"
            " long line"
            )

    def test_set_string_types(self):
        cf = self.fromstring("[sect]\n"
                             "option1=foo\n")
        # Check that we don't get an exception when setting values in
        # an existing section using strings:
        class mystr(str):
            pass
        cf.set("sect", "option1", "splat")
        cf.set("sect", "option1", mystr("splat"))
        cf.set("sect", "option2", "splat")
        cf.set("sect", "option2", mystr("splat"))
        try:
            unicode
        except NameError:
            pass
        else:
            cf.set("sect", "option1", unicode("splat"))
            cf.set("sect", "option2", unicode("splat"))

    def test_read_returns_file_list(self):
        file1 = test_support.findfile("cfgparser.1")
        # check when we pass a mix of readable and non-readable files:
        cf = self.newconfig()
        parsed_files = cf.read([file1, "nonexistant-file"])
        self.assertEqual(parsed_files, [file1])
        self.assertEqual(cf.get("Foo Bar", "foo"), "newbar")
        # check when we pass only a filename:
        cf = self.newconfig()
        parsed_files = cf.read(file1)
        self.assertEqual(parsed_files, [file1])
        self.assertEqual(cf.get("Foo Bar", "foo"), "newbar")
        # check when we pass only missing files:
        cf = self.newconfig()
        parsed_files = cf.read(["nonexistant-file"])
        self.assertEqual(parsed_files, [])
        # check when we pass no files:
        cf = self.newconfig()
        parsed_files = cf.read([])
        self.assertEqual(parsed_files, [])

    # shared by subclasses
    def get_interpolation_config(self):
        return self.fromstring(
            "[Foo]\n"
            "bar=something %(with1)s interpolation (1 step)\n"
            "bar9=something %(with9)s lots of interpolation (9 steps)\n"
            "bar10=something %(with10)s lots of interpolation (10 steps)\n"
            "bar11=something %(with11)s lots of interpolation (11 steps)\n"
            "with11=%(with10)s\n"
            "with10=%(with9)s\n"
            "with9=%(with8)s\n"
            "with8=%(With7)s\n"
            "with7=%(WITH6)s\n"
            "with6=%(with5)s\n"
            "With5=%(with4)s\n"
            "WITH4=%(with3)s\n"
            "with3=%(with2)s\n"
            "with2=%(with1)s\n"
            "with1=with\n"
            "\n"
            "[Mutual Recursion]\n"
            "foo=%(bar)s\n"
            "bar=%(foo)s\n"
            "\n"
            "[Interpolation Error]\n"
            "name=%(reference)s\n",
            # no definition for 'reference'
            defaults={"getname": "%(__name__)s"})

    def check_items_config(self, expected):
        cf = self.fromstring(
            "[section]\n"
            "name = value\n"
            "key: |%(name)s| \n"
            "getdefault: |%(default)s|\n"
            "getname: |%(__name__)s|",
            defaults={"default": "<default>"})
        L = list(cf.items("section"))
        L.sort()
        self.assertEqual(L, expected)


class ConfigParserTestCase(TestCaseBase):
    config_class = ConfigParser.ConfigParser

    def test_interpolation(self):
        cf = self.get_interpolation_config()
        eq = self.assertEqual
        eq(cf.get("Foo", "getname"), "Foo")
        eq(cf.get("Foo", "bar"), "something with interpolation (1 step)")
        eq(cf.get("Foo", "bar9"),
           "something with lots of interpolation (9 steps)")
        eq(cf.get("Foo", "bar10"),
           "something with lots of interpolation (10 steps)")
        self.get_error(ConfigParser.InterpolationDepthError, "Foo", "bar11")

    def test_interpolation_missing_value(self):
        cf = self.get_interpolation_config()
        e = self.get_error(ConfigParser.InterpolationError,
                           "Interpolation Error", "name")
        self.assertEqual(e.reference, "reference")
        self.assertEqual(e.section, "Interpolation Error")
        self.assertEqual(e.option, "name")

    def test_items(self):
        self.check_items_config([('default', '<default>'),
                                 ('getdefault', '|<default>|'),
                                 ('getname', '|section|'),
                                 ('key', '|value|'),
                                 ('name', 'value')])

    def test_set_nonstring_types(self):
        cf = self.newconfig()
        cf.add_section('non-string')
        cf.set('non-string', 'int', 1)
        cf.set('non-string', 'list', [0, 1, 1, 2, 3, 5, 8, 13, '%('])
        cf.set('non-string', 'dict', {'pi': 3.14159, '%(': 1,
                                      '%(list)': '%(list)'})
        cf.set('non-string', 'string_with_interpolation', '%(list)s')
        self.assertEqual(cf.get('non-string', 'int', raw=True), 1)
        self.assertRaises(TypeError, cf.get, 'non-string', 'int')
        self.assertEqual(cf.get('non-string', 'list', raw=True),
                         [0, 1, 1, 2, 3, 5, 8, 13, '%('])
        self.assertRaises(TypeError, cf.get, 'non-string', 'list')
        self.assertEqual(cf.get('non-string', 'dict', raw=True),
                         {'pi': 3.14159, '%(': 1, '%(list)': '%(list)'})
        self.assertRaises(TypeError, cf.get, 'non-string', 'dict')
        self.assertEqual(cf.get('non-string', 'string_with_interpolation',
                                raw=True), '%(list)s')
        self.assertRaises(ValueError, cf.get, 'non-string',
                          'string_with_interpolation', raw=False)


class RawConfigParserTestCase(TestCaseBase):
    config_class = ConfigParser.RawConfigParser

    def test_interpolation(self):
        cf = self.get_interpolation_config()
        eq = self.assertEqual
        eq(cf.get("Foo", "getname"), "%(__name__)s")
        eq(cf.get("Foo", "bar"),
           "something %(with1)s interpolation (1 step)")
        eq(cf.get("Foo", "bar9"),
           "something %(with9)s lots of interpolation (9 steps)")
        eq(cf.get("Foo", "bar10"),
           "something %(with10)s lots of interpolation (10 steps)")
        eq(cf.get("Foo", "bar11"),
           "something %(with11)s lots of interpolation (11 steps)")

    def test_items(self):
        self.check_items_config([('default', '<default>'),
                                 ('getdefault', '|%(default)s|'),
                                 ('getname', '|%(__name__)s|'),
                                 ('key', '|%(name)s|'),
                                 ('name', 'value')])

    def test_set_nonstring_types(self):
        cf = self.newconfig()
        cf.add_section('non-string')
        cf.set('non-string', 'int', 1)
        cf.set('non-string', 'list', [0, 1, 1, 2, 3, 5, 8, 13])
        cf.set('non-string', 'dict', {'pi': 3.14159})
        self.assertEqual(cf.get('non-string', 'int'), 1)
        self.assertEqual(cf.get('non-string', 'list'),
                         [0, 1, 1, 2, 3, 5, 8, 13])
        self.assertEqual(cf.get('non-string', 'dict'), {'pi': 3.14159})


class SafeConfigParserTestCase(ConfigParserTestCase):
    config_class = ConfigParser.SafeConfigParser

    def test_safe_interpolation(self):
        # See http://www.python.org/sf/511737
        cf = self.fromstring("[section]\n"
                             "option1=xxx\n"
                             "option2=%(option1)s/xxx\n"
                             "ok=%(option1)s/%%s\n"
                             "not_ok=%(option2)s/%%s")
        self.assertEqual(cf.get("section", "ok"), "xxx/%s")
        self.assertEqual(cf.get("section", "not_ok"), "xxx/xxx/%s")

    def test_set_malformatted_interpolation(self):
        cf = self.fromstring("[sect]\n"
                             "option1=foo\n")

        self.assertEqual(cf.get('sect', "option1"), "foo")

        self.assertRaises(ValueError, cf.set, "sect", "option1", "%foo")
        self.assertRaises(ValueError, cf.set, "sect", "option1", "foo%")
        self.assertRaises(ValueError, cf.set, "sect", "option1", "f%oo")

        self.assertEqual(cf.get('sect', "option1"), "foo")

    def test_set_nonstring_types(self):
        cf = self.fromstring("[sect]\n"
                             "option1=foo\n")
        # Check that we get a TypeError when setting non-string values
        # in an existing section:
        self.assertRaises(TypeError, cf.set, "sect", "option1", 1)
        self.assertRaises(TypeError, cf.set, "sect", "option1", 1.0)
        self.assertRaises(TypeError, cf.set, "sect", "option1", object())
        self.assertRaises(TypeError, cf.set, "sect", "option2", 1)
        self.assertRaises(TypeError, cf.set, "sect", "option2", 1.0)
        self.assertRaises(TypeError, cf.set, "sect", "option2", object())

    def test_add_section_default_1(self):
        cf = self.newconfig()
        self.assertRaises(ValueError, cf.add_section, "default")

    def test_add_section_default_2(self):
        cf = self.newconfig()
        self.assertRaises(ValueError, cf.add_section, "DEFAULT")

class SortedTestCase(RawConfigParserTestCase):

    __test__ = False

    def newconfig(self, defaults=None):
        self.cf = self.config_class(defaults=defaults, dict_type=SortedDict)
        return self.cf

    def test_sorted(self):
        self.fromstring("[b]\n"
                        "o4=1\n"
                        "o3=2\n"
                        "o2=3\n"
                        "o1=4\n"
                        "[a]\n"
                        "k=v\n")
        output = StringIO.StringIO()
        self.cf.write(output)
        self.assertEquals(output.getvalue(),
                          "[a]\n"
                          "k = v\n\n"
                          "[b]\n"
                          "o1 = 4\n"
                          "o2 = 3\n"
                          "o3 = 2\n"
                          "o4 = 1\n\n")

class suite(unittest.TestSuite):
    def __init__(self):
        unittest.TestSuite.__init__(self, [
                unittest.makeSuite(RawConfigParserTestCase, 'test'),
                unittest.makeSuite(ConfigParserTestCase, 'test'),
                unittest.makeSuite(SafeConfigParserTestCase, 'test'),
                #unittest.makeSuite(SortedTestCase, 'test'),
    ])

########NEW FILE########
__FILENAME__ = test_fuzz
import re
import os
import random
import unittest
import ConfigParser
from StringIO import StringIO
from holland.backup.mysqldump.util import compat, ini

# TODO:
#  tabs
#  substitutions

def random_string(maxlen=200):
    length = random.randint(0, maxlen)
    s = []
    for i in range(length):
        s.append(chr(random.randint(32, 126)))

    return ''.join(s)

def random_space(maxlen=10):
    length = random.randint(0, maxlen)
    return ' '*length

def random_ini_file():
    num_lines = random.randint(0, 100)
    lines = []
    for i in range(num_lines):
        x = random.random()
        if x < 0.1:
            # empty line
            lines.append(random_space())
        elif x < 0.3:
            # comment
            sep = random.choice(['#', ';'])
            lines.append(sep + random_string())
        elif x < 0.5:
            # section
            if random.random() < 0.1:
                name = 'DEFAULT'
            else:
                name = random_string()
                name = re.sub(']', '' , name)
            l = '[' + name + ']'
            if random.randint(0,1):
                l += random_space()
            if random.randint(0,1):
                sep = random.choice(['#', ';'])
                l += sep + random_string()
            lines.append(l)
        elif x < 0.7:
            # option
            name = random_string()
            name = re.sub(':|=| |\[', '', name)
            sep = random.choice([':', '='])
            l = name + random_space() + sep + random_space() + random_string()
            if random.randint(0,1):
                l += ' ' + random_space() + ';'  +random_string()
            lines.append(l)
        elif x < 0.9:
            # continuation
            lines.append(' ' + random_space() + random_string())
        else:
            # junk
            lines.append(random_string())

    return '\n'.join(lines)

class test_fuzz(unittest.TestCase):
    def test_fuzz(self):
        random.seed(42)
        try:
            num_iter = int(os.environ['INIPARSE_FUZZ_ITERATIONS'])
        except (KeyError, ValueError):
            num_iter = 100
        for fuzz_iter in range(num_iter):
            try:
                # parse random file with errors disabled
                s = random_ini_file()
                c = ini.INIConfig(parse_exc=False)
                c._readfp(StringIO(s))
                # check that file is preserved, except for
                # commenting out erroneous lines
                l1 = s.split('\n')
                l2 = str(c).split('\n')
                self.assertEqual(len(l1), len(l2))
                good_lines = []
                for i in range(len(l1)):
                    try:
                        self.assertEqual(l1[i], l2[i])
                        good_lines.append(l1[i])
                    except AssertionError:
                        self.assertEqual('#'+l1[i], l2[i])
                # parse the good subset of the file
                # using ConfigParser
                s = '\n'.join(good_lines)
                cc = compat.RawConfigParser()
                cc.readfp(StringIO(s))
                cc_py = ConfigParser.RawConfigParser()
                cc_py.readfp(StringIO(s))
                # compare the two configparsers
                self.assertEqualSorted(cc_py.sections(), cc.sections())
                self.assertEqualSorted(cc_py.defaults().items(), cc.defaults().items())
                for sec in cc_py.sections():
                    self.assertEqualSorted(cc_py.options(sec), cc.options(sec))
                    for opt in cc_py.options(sec):
                        self.assertEqual(cc_py.get(sec, opt), cc.get(sec, opt))
            except AssertionError:
                fname = 'fuzz-test-iter-%d.ini' % fuzz_iter
                print 'Fuzz test failed at iteration', fuzz_iter
                print 'Writing out failing INI file as', fname
                f = open(fname, 'w')
                f.write(s)
                f.close()
                raise

    def assertEqualSorted(self, l1, l2):
        l1.sort()
        l2.sort()
        self.assertEqual(l1, l2)

class suite(unittest.TestSuite):
    def __init__(self):
        unittest.TestSuite.__init__(self, [
                unittest.makeSuite(test_fuzz, 'test'),
    ])

########NEW FILE########
__FILENAME__ = test_ini
import unittest
from StringIO import StringIO

from holland.backup.mysqldump.util import ini
from holland.backup.mysqldump.util import compat
from holland.backup.mysqldump.util import config

class test_section_line(unittest.TestCase):
    invalid_lines = [
        '# this is a comment',
        '; this is a comment',
        '  [sections must start on column1]',
        '[incomplete',
        '[ no closing ]brackets]',
        'ice-cream = mmmm',
        'ic[e-c]ream = mmmm',
        '[ice-cream] = mmmm',
        '-$%^',
    ]
    def test_invalid(self):
        for l in self.invalid_lines:
            p = ini.SectionLine.parse(l)
            self.assertEqual(p, None)

    lines = [
        ('[section]' ,          ('section', None, None, -1)),
        ('[se\ct%[ion\t]' ,     ('se\ct%[ion\t', None, None, -1)),
        ('[sec tion]  ; hi' ,   ('sec tion', ' hi', ';', 12)),
        ('[section]  #oops!' ,  ('section', 'oops!', '#', 11)),
        ('[section]   ;  ' ,    ('section', '', ';', 12)),
        ('[section]      ' ,    ('section', None, None, -1)),
    ]
    def test_parsing(self):
        for l in self.lines:
            p = ini.SectionLine.parse(l[0])
            self.assertNotEqual(p, None)
            self.assertEqual(p.name, l[1][0])
            self.assertEqual(p.comment, l[1][1])
            self.assertEqual(p.comment_separator, l[1][2])
            self.assertEqual(p.comment_offset, l[1][3])

    def test_printing(self):
        for l in self.lines:
            p = ini.SectionLine.parse(l[0])
            self.assertEqual(str(p), l[0])
            self.assertEqual(p.to_string(), l[0].strip())

    indent_test_lines = [
        ('[oldname]             ; comment', 'long new name',
         '[long new name]       ; comment'),
        ('[oldname]             ; comment', 'short',
         '[short]               ; comment'),
        ('[oldname]             ; comment', 'really long new name',
         '[really long new name] ; comment'),
    ]
    def test_preserve_indentation(self):
        for l in self.indent_test_lines:
            p = ini.SectionLine.parse(l[0])
            p.name = l[1]
            self.assertEqual(str(p), l[2])

class test_option_line(unittest.TestCase):
    lines = [
        ('option = value', 'option', ' = ', 'value', None, None, -1),
        ('option:   value', 'option', ':   ', 'value', None, None, -1),
        ('option=value', 'option', '=', 'value', None, None, -1),
        ('op[ti]on=value', 'op[ti]on', '=', 'value', None, None, -1),

        ('option = value # no comment', 'option', ' = ', 'value # no comment',
                                         None, None, -1),
        ('option = value     ;', 'option', ' = ', 'value',
                                         ';', '', 19),
        ('option = value     ; comment', 'option', ' = ', 'value',
                                         ';', ' comment', 19),
        ('option = value;1   ; comment', 'option', ' = ', 'value;1   ; comment',
                                         None, None, -1),
        ('op;ti on = value      ;; comm;ent', 'op;ti on', ' = ', 'value',
                                         ';', '; comm;ent', 22),
    ]
    def test_parsing(self):
        for l in self.lines:
            p = ini.OptionLine.parse(l[0])
            self.assertEqual(p.name, l[1])
            self.assertEqual(p.separator, l[2])
            self.assertEqual(p.value, l[3])
            self.assertEqual(p.comment_separator, l[4])
            self.assertEqual(p.comment, l[5])
            self.assertEqual(p.comment_offset, l[6])

    invalid_lines = [
        '  option = value',
        '# comment',
        '; comment',
        '[section 7]',
        '[section=option]',
        'option',
    ]
    def test_invalid(self):
        for l in self.invalid_lines:
            p = ini.OptionLine.parse(l)
            self.assertEqual(p, None)

    print_lines = [
        'option = value',
        'option= value',
        'option : value',
        'option: value  ',
        'option   =    value  ',
        'option = value   ;',
        'option = value;2    ;; 4 5',
        'option = value     ; hi!',
    ]
    def test_printing(self):
        for l in self.print_lines:
            p = ini.OptionLine.parse(l)
            self.assertEqual(str(p), l)
            self.assertEqual(p.to_string(), l.rstrip())

    indent_test_lines = [
        ('option = value   ;comment', 'newoption', 'newval',
         'newoption = newval ;comment'),
        ('option = value       ;comment', 'newoption', 'newval',
         'newoption = newval   ;comment'),
    ]
    def test_preserve_indentation(self):
        for l in self.indent_test_lines:
            p = ini.OptionLine.parse(l[0])
            p.name = l[1]
            p.value = l[2]
            self.assertEqual(str(p), l[3])

class test_comment_line(unittest.TestCase):
    invalid_lines = [
        '[section]',
        'option = value ;comment',
        '  # must start on first column',
    ]
    def test_invalid(self):
        for l in self.invalid_lines:
            p = ini.CommentLine.parse(l)
            self.assertEqual(p, None)

    lines = [
        '#this is a comment',
        ';; this is also a comment',
        '; so is this   ',
        'Rem and this',
        'remthis too!'
    ]
    def test_parsing(self):
        for l in self.lines:
            p = ini.CommentLine.parse(l)
            self.assertEqual(str(p), l)
            self.assertEqual(p.to_string(), l.rstrip())

class test_other_lines(unittest.TestCase):
    def test_empty(self):
        for s in ['asdf', '; hi', '  #rr', '[sec]', 'opt=val']:
            self.assertEqual(ini.EmptyLine.parse(s), None)
        for s in ['', '  ', '\t  \t']:
            self.assertEqual(str(ini.EmptyLine.parse(s)), s)

    def test_continuation(self):
        for s in ['asdf', '; hi', '[sec]', 'a=3']:
            self.assertEqual(ini.ContinuationLine.parse(s), None)
        for s in [' asdfasd ', '\t mmmm']:
            self.assertEqual(ini.ContinuationLine.parse(s).value,
                             s.strip())
            self.assertEqual(ini.ContinuationLine.parse(s).to_string(),
                             s.rstrip().replace('\t',' '))


class test_ini(unittest.TestCase):
    s1 = """
[section1]
help = me
I'm  = desperate     ; really!

[section2]
# comment and empty line before the first option

just = what?
just = kidding

[section1]
help = yourself
but = also me
"""

    def test_basic(self):
        sio = StringIO(self.s1)
        p = ini.INIConfig(sio)
        self.assertEqual(str(p), self.s1)
        self.assertEqual(p._data.find('section1').find('but').value, 'also me')
        self.assertEqual(p._data.find('section1').find('help').value, 'yourself')
        self.assertEqual(p._data.find('section2').find('just').value, 'kidding')

        itr = p._data.finditer('section1')
        v = itr.next()
        self.assertEqual(v.find('help').value, 'yourself')
        self.assertEqual(v.find('but').value, 'also me')
        v = itr.next()
        self.assertEqual(v.find('help').value, 'me')
        self.assertEqual(v.find('I\'m').value, 'desperate')
        self.assertRaises(StopIteration, itr.next)

        self.assertRaises(KeyError, p._data.find, 'section')
        self.assertRaises(KeyError, p._data.find('section2').find, 'ahem')

    def test_lookup(self):
        sio = StringIO(self.s1)
        p = ini.INIConfig(sio)
        self.assertEqual(p.section1.help, 'yourself')
        self.assertEqual(p.section1.but, 'also me')
        self.assertEqual(getattr(p.section1, 'I\'m'), 'desperate')
        self.assertEqual(p.section2.just, 'kidding')

        self.assertEqual(p.section1.just.__class__, config.Undefined)
        self.assertEqual(p.section2.help.__class__, config.Undefined)

    def test_order(self):
        sio = StringIO(self.s1)
        p = ini.INIConfig(sio)
        self.assertEqual(list(p), ['section1','section2'])
        self.assertEqual(list(p.section1), ['help', "i'm", 'but'])
        self.assertEqual(list(p.section2), ['just'])

    def test_delete(self):
        sio = StringIO(self.s1)
        p = ini.INIConfig(sio)
        del p.section1.help
        self.assertEqual(list(p.section1), ["i'm", 'but'])
        self.assertEqual(str(p), """
[section1]
I'm  = desperate     ; really!

[section2]
# comment and empty line before the first option

just = what?
just = kidding

[section1]
but = also me
""")
        del p.section2
        self.assertEqual(str(p), """
[section1]
I'm  = desperate     ; really!


[section1]
but = also me
""")

    def check_order(self, c):
        sio = StringIO(self.s1)
        c = c({'pi':'3.14153'})
        c.readfp(sio)
        self.assertEqual(c.sections(), ['section1','section2'])
        self.assertEqual(c.options('section1'), ['help', "i'm", 'but', 'pi'])
        self.assertEqual(c.items('section1'), [
            ('help', 'yourself'),
            ("i'm", 'desperate'),
            ('but', 'also me'),
            ('pi', '3.14153'),
        ])

    def test_compat_order(self):
        self.check_order(compat.RawConfigParser)
        self.check_order(compat.ConfigParser)

    inv = (
("""
# values must be in a section
value = 5
""",
"""
# values must be in a section
#value = 5
"""),
("""
# continuation lines only allowed after options
[section]
op1 = qwert
    yuiop
op2 = qwert

    yuiop
op3 = qwert
# yup
    yuiop

[another section]
    hmmm
""",
"""
# continuation lines only allowed after options
[section]
op1 = qwert
    yuiop
op2 = qwert

    yuiop
op3 = qwert
# yup
    yuiop

[another section]
#    hmmm
"""))

    def test_invalid(self):
        for (org, mod) in self.inv:
            ip = ini.INIConfig(StringIO(org), parse_exc=False)
            self.assertEqual(str(ip), mod)

    # test multi-line values
    s2 = (
"""
[section]
option =
  foo
  bar

  baz
  yam
"""
)

    s3 = (
"""
[section]
option =
  foo
  bar
  mum

  baz
  yam
"""
)

    def test_option_continuation(self):
        ip = ini.INIConfig(StringIO(self.s2))
        self.assertEqual(str(ip), self.s2)
        value = ip.section.option.split('\n')
        value.insert(3, 'mum')
        ip.section.option = '\n'.join(value)
        self.assertEqual(str(ip), self.s3)

    s5 = (
"""
[section]
option =
  foo
  bar
"""
)

    s6 = (
"""
[section]
option =


  foo



another = baz
"""
)

    def test_option_continuation_single(self):
        ip = ini.INIConfig(StringIO(self.s5))
        self.assertEqual(str(ip), self.s5)
        ip.section.option = '\n'.join(['', '', '', 'foo', '', '', ''])
        ip.section.another = 'baz'
        self.assertEqual(str(ip), self.s6)


class suite(unittest.TestSuite):
    def __init__(self):
        unittest.TestSuite.__init__(self, [
                unittest.makeSuite(test_section_line, 'test'),
                unittest.makeSuite(test_option_line, 'test'),
                unittest.makeSuite(test_comment_line, 'test'),
                unittest.makeSuite(test_other_lines, 'test'),
                unittest.makeSuite(test_ini, 'test'),
        ])

########NEW FILE########
__FILENAME__ = test_misc
import unittest
import pickle
import ConfigParser
from textwrap import dedent
from StringIO import StringIO
from holland.backup.mysqldump.util import compat, ini

class CaseSensitiveConfigParser(compat.ConfigParser):
    """Case Sensitive version of ConfigParser"""
    def optionxform(self, option):
        """Use str()"""
        return str(option)

class test_optionxform_override(unittest.TestCase):
    def test_derivedclass(self):
        c = CaseSensitiveConfigParser()
        c.add_section('foo')
        c.set('foo', 'bar', 'a')
        c.set('foo', 'Bar', 'b')
        self.assertEqual(c.get('foo', 'bar'), 'a')
        self.assertEqual(c.get('foo', 'Bar'), 'b')

    def test_assignment(self):
        c = compat.ConfigParser()
        c.optionxform = str
        c.add_section('foo')
        c.set('foo', 'bar', 'a')
        c.set('foo', 'Bar', 'b')
        self.assertEqual(c.get('foo', 'bar'), 'a')
        self.assertEqual(c.get('foo', 'Bar'), 'b')

    def test_dyanamic(self):
        c = compat.ConfigParser()
        c.optionxform = str
        c.add_section('foo')
        c.set('foo', 'bar', 'a')
        c.set('foo', 'Bar', 'b')
        c.set('foo', 'BAR', 'c')
        c.optionxform = str.upper
        self.assertEqual(c.get('foo', 'Bar'), 'c')
        c.optionxform = str.lower
        self.assertEqual(c.get('foo', 'Bar'), 'a')
        c.optionxform = str
        self.assertEqual(c.get('foo', 'Bar'), 'b')


class OnlyReadline:
    def __init__(self, s):
        self.sio = StringIO(s)

    def readline(self):
        return self.sio.readline()

class test_readline(unittest.TestCase):
    """Test that the file object passed to readfp only needs to
    support the .readline() method.  As of Python-2.4.4, this is
    true of the standard librariy's ConfigParser, and so other
    code uses that to guide what is sufficiently file-like."""

    test_strings = [
"""\
[foo]
bar=7
baz=8""",
"""\
[foo]
bar=7
baz=8
""",
"""\
[foo]
bar=7
baz=8
    """]

    def test_readline_iniconfig(self):
        for s in self.test_strings:
            fp = OnlyReadline(s)
            c = ini.INIConfig()
            c._readfp(fp)
            self.assertEqual(s, str(c))

    def test_readline_configparser(self):
        for s in self.test_strings:
            fp = OnlyReadline(s)
            c = compat.ConfigParser()
            c.readfp(fp)
            ss = StringIO()
            c.write(ss)
            self.assertEqual(s, ss.getvalue())


class test_multiline_with_comments(unittest.TestCase):
    """Test that multiline values are allowed to span comments."""

    s = """\
[sec]
opt = 1
 2

# comment
 3"""

    def test_read(self):
        c = ini.INIConfig()
        c._readfp(StringIO(self.s))
        self.assertEqual(c.sec.opt, '1\n2\n\n3')

    def test_write(self):
        c = ini.INIConfig()
        c._readfp(StringIO(self.s))
        c.sec.opt = 'xyz'
        self.assertEqual(str(c), """\
[sec]
opt = xyz""")

class test_empty_file(unittest.TestCase):
    """Test if it works with an blank file"""

    s = ""

    def test_read(self):
        c = ini.INIConfig()
        c._readfp(StringIO(self.s))
        self.assertEqual(str(c), '')

    def test_write(self):
        c = ini.INIConfig()
        c._readfp(StringIO(self.s))
        c.sec.opt = 'xyz'
        self.assertEqual(str(c), """\
[sec]
opt = xyz""")

class test_custom_dict(unittest.TestCase):
    def test_custom_dict_not_supported(self):
        self.assertRaises(ValueError, compat.RawConfigParser, None, 'foo')

class test_compat(unittest.TestCase):
    """Miscellaneous compatibility tests."""

    s = dedent("""\
        [DEFAULT]
        pi = 3.1415
        three = 3
        poet = e e

             cummings
        NH =
         live free

         or die

        [sec]
        opt = 6
        three = 3.0
        no-three = one
         two

         four
        longopt = foo
         bar

        # empty line should not be part of value
         baz

         bat

        """)

    def do_test(self, c):
        # default section is not acknowledged
        self.assertEqual(c.sections(), ['sec'])
        # options in the default section are merged with other sections
        self.assertEqual(sorted(c.options('sec')),
                         ['longopt', 'nh', 'no-three', 'opt', 'pi', 'poet', 'three'])

        # empty lines are stripped from multi-line values
        self.assertEqual(c.get('sec', 'poet').split('\n'),
                         ['e e', 'cummings'])
        self.assertEqual(c.get('DEFAULT', 'poet').split('\n'),
                         ['e e', 'cummings'])
        self.assertEqual(c.get('sec', 'longopt').split('\n'),
                         ['foo', 'bar', 'baz', 'bat'])
        self.assertEqual(c.get('sec', 'NH').split('\n'),
                         ['', 'live free', 'or die'])

        # check that empy-line stripping happens on all access paths
        # defaults()
        self.assertEqual(c.defaults(), {
            'poet': 'e e\ncummings',
            'nh': '\nlive free\nor die',
            'pi': '3.1415',
            'three': '3',
        })
        # items()
        l = c.items('sec')
        l.sort()
        self.assertEqual(l, [
            ('longopt', 'foo\nbar\nbaz\nbat'),
            ('nh', '\nlive free\nor die'),
            ('no-three', 'one\ntwo\nfour'),
            ('opt', '6'),
            ('pi', '3.1415'),
            ('poet', 'e e\ncummings'),
            ('three', '3.0'),
        ])

        # empty lines are preserved on explicitly set values
        c.set('sec', 'longopt', '\n'.join(['a', 'b', '', 'c', '', '', 'd']))
        c.set('DEFAULT', 'NH', '\nlive free\n\nor die')
        self.assertEqual(c.get('sec', 'longopt').split('\n'),
                         ['a', 'b', '', 'c', '', '', 'd'])
        self.assertEqual(c.get('sec', 'NH').split('\n'),
                         ['', 'live free', '', 'or die'])
        self.assertEqual(c.defaults(), {
            'poet': 'e e\ncummings',
            'nh': '\nlive free\n\nor die',
            'pi': '3.1415',
            'three': '3',
        })
        # items()
        l = c.items('sec')
        l.sort()
        self.assertEqual(l, [
            ('longopt', 'a\nb\n\nc\n\n\nd'),
            ('nh', '\nlive free\n\nor die'),
            ('no-three', 'one\ntwo\nfour'),
            ('opt', '6'),
            ('pi', '3.1415'),
            ('poet', 'e e\ncummings'),
            ('three', '3.0'),
        ])

        # empty line special magic goes away after remove_option()
        self.assertEqual(c.get('sec', 'no-three').split('\n'),
                         ['one', 'two','four'])
        c.remove_option('sec', 'no-three')
        c.set('sec', 'no-three', 'q\n\nw')
        self.assertEqual(c.get('sec', 'no-three'), 'q\n\nw')
        c.remove_option('sec', 'no-three')
    do_test.__test__ = False

    def do_configparser_test(self, cfg_class):
        c = cfg_class()
        c.readfp(StringIO(self.s))
        self.do_test(c)
        o = StringIO()
        c.write(o)
        self.assertEqual(o.getvalue().split('\n'), [
            '[DEFAULT]',
            'poet = e e',
            '\tcummings',
            'nh = ',
            '\tlive free',
            '\t',
            '\tor die',
            'pi = 3.1415',
            'three = 3',
            '',
            '[sec]',
            'opt = 6',
            'longopt = a',
            '\tb',
            '\t',
            '\tc',
            '\t',
            '\t',
            '\td',
            'three = 3.0',
            '',
            ''])

    do_configparser_test.__test__ = False

    def test_py_rawcfg(self):
        self.do_configparser_test(ConfigParser.RawConfigParser)

    def test_py_cfg(self):
        self.do_configparser_test(ConfigParser.ConfigParser)

    def test_py_safecfg(self):
        self.do_configparser_test(ConfigParser.SafeConfigParser)

    def do_compat_test(self, cfg_class):
        c = cfg_class()
        c.readfp(StringIO(self.s))
        self.do_test(c)
        o = StringIO()
        c.write(o)
        self.assertEqual(o.getvalue().split('\n'), [
            '[DEFAULT]',
            'pi = 3.1415',
            'three = 3',
            'poet = e e',
            '',
            '     cummings',
            'NH =',
            ' live free',
            '',
            ' or die',
            '',
            '[sec]',
            'opt = 6',
            'three = 3.0',
            'longopt = a',
            ' b',
            '',
            ' c',
            '',
            '',
            ' d',
            '',
            ''])
    do_compat_test.__test__ = False

    def test_py_rawcfg(self):
        self.do_compat_test(compat.RawConfigParser)

    def test_py_cfg(self):
        self.do_compat_test(compat.ConfigParser)

    def test_py_safecfg(self):
        self.do_compat_test(compat.SafeConfigParser)

class test_pickle(unittest.TestCase):
    s = dedent("""\
        [DEFAULT]
        pi = 3.1415
        three = 3
        poet = e e

             cummings
        NH =
         live free

         or die

        [sec]
        opt = 6
        three = 3.0
        no-three = one
         two

         four

        james = bond
        """)

    def do_compat_checks(self, c):
        self.assertEqual(c.sections(), ['sec'])
        self.assertEqual(sorted(c.options('sec')),
                         ['james', 'nh', 'no-three', 'opt', 'pi', 'poet', 'three'])
        self.assertEqual(c.defaults(), {
            'poet': 'e e\ncummings',
            'nh': '\nlive free\nor die',
            'pi': '3.1415',
            'three': '3',
        })
        l = c.items('sec')
        l.sort()
        self.assertEqual(l, [
            ('james', 'bond'),
            ('nh', '\nlive free\nor die'),
            ('no-three', 'one\ntwo\nfour'),
            ('opt', '6'),
            ('pi', '3.1415'),
            ('poet', 'e e\ncummings'),
            ('three', '3.0'),
        ])
        self.do_ini_checks(c.data)

    def do_ini_checks(self, c):
        self.assertEqual(list(c), ['sec'])
        self.assertEqual(sorted(c['sec']), ['james', 'nh', 'no-three', 'opt', 'pi', 'poet', 'three'])
        self.assertEqual(c._defaults['pi'], '3.1415')
        self.assertEqual(c.sec.opt, '6')
        self.assertEqual(c.sec.three, '3.0')
        self.assertEqual(c.sec['no-three'], 'one\ntwo\n\nfour')
        self.assertEqual(c.sec.james, 'bond')
        self.assertEqual(c.sec.pi, '3.1415')
        self.assertEqual(c.sec.poet, 'e e\n\ncummings')
        self.assertEqual(c.sec.NH, '\nlive free\n\nor die')
        self.assertEqual(str(c), self.s)

    def test_compat(self):
        for cfg_class in (compat.ConfigParser, compat.RawConfigParser, compat.SafeConfigParser):
            c = cfg_class()
            c.readfp(StringIO(self.s))
            self.do_compat_checks(c)
            p = pickle.dumps(c)
            c = None
            c2 = pickle.loads(p)
            self.do_compat_checks(c2)

    def test_ini(self):
        c = ini.INIConfig()
        c._readfp(StringIO(self.s))
        self.do_ini_checks(c)
        p = pickle.dumps(c)
        c = None
        c2 = pickle.loads(p)
        self.do_ini_checks(c2)

class suite(unittest.TestSuite):
    def __init__(self):
        unittest.TestSuite.__init__(self, [
                unittest.makeSuite(test_optionxform_override, 'test'),
                unittest.makeSuite(test_readline, 'test'),
                unittest.makeSuite(test_multiline_with_comments, 'test'),
                unittest.makeSuite(test_empty_file, 'test'),
                unittest.makeSuite(test_custom_dict, 'test'),
                unittest.makeSuite(test_compat, 'test'),
                unittest.makeSuite(test_pickle, 'test'),
    ])

########NEW FILE########
__FILENAME__ = test_unicode
import unittest
from StringIO import StringIO
from holland.backup.mysqldump.util import compat, ini

class test_unicode(unittest.TestCase):
    """Test files read in unicode-mode."""

    s1 = u"""\
[foo]
bar = fish
    """

    s2 = u"""\
\ufeff[foo]
bar = mammal
baz = Marc-Andr\202
    """

    def basic_tests(self, s, strable):
        f = StringIO(s)
        i = ini.INIConfig(f)
        self.assertEqual(unicode(i), s)
        self.assertEqual(type(i.foo.bar), unicode)
        if strable:
            self.assertEqual(str(i), str(s))
        else:
            self.assertRaises(UnicodeEncodeError, lambda: str(i))
        return i
    basic_tests.__test__ = False

    def test_ascii(self):
        i = self.basic_tests(self.s1, strable=True)
        self.assertEqual(i.foo.bar, 'fish')

    def test_unicode_without_bom(self):
        i = self.basic_tests(self.s2[1:], strable=False)
        self.assertEqual(i.foo.bar, 'mammal')
        self.assertEqual(i.foo.baz, u'Marc-Andr\202')

    def test_unicode_with_bom(self):
        i = self.basic_tests(self.s2, strable=False)
        self.assertEqual(i.foo.bar, 'mammal')
        self.assertEqual(i.foo.baz, u'Marc-Andr\202')

class suite(unittest.TestSuite):
    def __init__(self):
        unittest.TestSuite.__init__(self, [
                unittest.makeSuite(test_unicode, 'test'),
    ])

########NEW FILE########
__FILENAME__ = mysqlhotcopy
"""
Plugin for the Holland backup framework
to backup MySQL databases using mysqldump
"""

import os
import glob
import subprocess
import tempfile
import shutil
import logging

from holland.lib.mysql.client import connect, PassiveMySQLClient
from holland.lib.mysql.schema import MySQLSchema, \
                                     DatabaseIterator, \
                                     TableIterator
from holland.lib.mysql.option import make_mycnf
from holland.lib.archive import create_archive
from holland.core.util.path import format_bytes, disk_free
from holland.core.config.configobj import ConfigObj, ParseError
from holland.core.exceptions import BackupError

LOG = logging.getLogger(__name__)

# We validate our config against the following spec
CONFIGSPEC = """# MySQL Hotcopy Plugin Configuration
# This plugin backs up raw data files of non-transactional engines
# It's not so much "hot" as "warm" - tables should be locked during
# backup for consistency.
# This plugin also backs up .frm files for all table types

[mysqlhotcopy]
# Directories where mysql commands can be found
mysql-binpath       = string(default=None)
# How should tables be locked?
# flush-lock: global read lock (FLUSH TABLES WITH READ LOCK)
# lock-tables: lock only the tables being backed up
# default lock-tables. Use flush-lock if bin-log-position is set
lock-method         = option('flush-lock', 'lock-tables', 'none', default='lock-tables')
# Names of databases to backup
databases           = coerced_list(default=list('*'))
# Names of databases to exclude
exclude-databases   = coerced_list(default=list())
# Names of tables to backup
tables              = coerced_list(default=list('*'))
# Names of tables to exclude
exclude-tables      = coerced_list(default=list())
# Only backup the 2K header of MyISAM indexes
# (makes for faster backups sometimes, table must be repaired on restore)
partial-indexes     = boolean(default=false)
# How should tables be archived?
# dir - into a directory
# tar - into a tar archive
# zip - into a zip archive
# dir or zip offer constant time lookup and provides faster per-table restores
# tar probably gets somewhat better overall compression
archive-method      = option(dir,tar,zip,default="dir")
# stop the slave before running backups
stop-slave          = boolean(default=false)
# record the binary log position
bin-log-position    = boolean(default=false)

# Compression method
# Only applicable to certain archive types
# (e.g. zip only supports 'zlib' internal compression)
[compression]
method              = option('none','gzip', 'gzip-rsyncable', 'pigz','bzip2','pbzip2','lzma','lzop',default='gzip')
inline              = boolean(default=false)
level               = integer(default=1,min=0,max=9)
bin-path            = string(default=None)

# MySQL connection information
[mysql:client]
default-extra-files = coerced_list(default=list('~/.my.cnf'))
user                = string(default=None)
password            = string(default=None)
socket              = string(default=None)
host                = string(default=None)
port                = integer(default=None)
""".splitlines()

# Used for our surrogate connection
CLIENT_TIMEOUT = 28800

class MySQLHotcopy(object):
    """
    Plugin for backing up MySQL MyISAM tables
    """
    def __init__(self, name, config, target_directory, dry_run=False):
        self.name = name
        self.config = config
        self.target_directory = target_directory
        if dry_run:
            LOG.info("Dry-run mode")
        self.dry_run = dry_run
        self.config.validate_config(CONFIGSPEC)
        # Setup MySQL connection objects
        mycnf_cfg = self.config['mysql:client']
        self.mycnf = build_mysql_config(mycnf_cfg)
        self.mycnf.filename = os.path.join(self.target_directory, 'my.cnf')
        self.mysqlclient = holland.lib.mysql.connect(self.mycnf)

    def estimate_backup_size(self):
        total = 0
        datadir = self.mysqlclient.show_variable('datadir')
        for cpath in self._find_files(datadir):
            st = os.stat(cpath)
            if cpath.endswith('.MYI') and self.config.lookup('mysqlhotcopy.partial-indexes'):
                total += min(st.st_size, 2048)
            else:
                total += st.st_size
        return total

    def backup(self):
        """
        Start a backup.  This attempts one or more mysqldump
        runs.  On error, a BackupError exception will be thrown.
        """

        if self.config.lookup('mysqlhotcopy.stop-slave'):
            self.mysqlclient.stop_slave()

        if self.config.lookup('mysqlhotcopy.bin-log-position'):
            #ensure mysql:replication section exists
            self.config.setdefault('mysql:replication', {})
            # Log slave data if we can:
            is_slave = self.mysqlclient.is_slave_running()
            if is_slave:
                slave_status = self.mysqlclient.show_slave_status()
                self.config['mysql:replication']['slave_master_log_file'] = slave_status['Relay_Master_Log_File']
                self.config['mysql:replication']['slave_master_log_pos'] = slave_status['Exec_Master_Log_Pos']
            master_data = self.mysqlclient.show_master_status()
            if not master_data and not is_slave:
                LOG.error("bin-log-position requested, but this server is neither a master nor a slave")
                raise BackupError("Failboat: replication not configured")
            self.config['mysql:replication']['master_log_file'] = master_data[0]
            self.config['mysql:replication']['master_log_pos'] = master_data[1]
            LOG.info("Writing master status to %s", self.mycnf.path)
            if not self.dry_run:
                self.config.write()

        # trap exceptions so we make sure to restart the slave, if we stopped it
        # if the slave was already stopped, we will raise an exception when we
        # try to stop it (above)
        error = None
        try:
            self._backup()
        except Exception, e:
            error = e

        if self.config.lookup('mysqlhotcopy.stop-slave'):
            self.mysqlclient.start_slave()

        if error:
            raise e

    def _backup(self):
        datadir = self.mysqlclient.show_variable('datadir')
        archive_method = self.config.lookup('mysqlhotcopy.archive-method')
        if not self.dry_run:
            archive = create_archive(archive_method, os.path.join(self.target_directory, 'backup_data'))
        LOG.info("Creating backup_data %s archive", archive_method)

        if self.config.lookup('mysqlhotcopy.lock-method') == 'flush-lock':
            if not self.dry_run:
                self.mysqlclient.flush_tables_with_read_lock(extra_flush=True)
        elif self.config.lookup('mysqlhotcopy.lock-method') == 'lock-tables':
            tables = [x for x in self._find_tables() if x not in [('mysql', 'general_log'), ('mysql', 'slow_log')]]
            quoted_tables = map(lambda x: '`' + '`.`'.join(x) +
                                '`', tables)
            if not self.dry_run:
                self.mysqlclient.lock_tables(quoted_tables)
                self.mysqlclient.flush_tables()

        LOG.info("Starting Backup")
        error = None
        try:
            if self.config.lookup('mysqlhotcopy.partial-indexes'):
                LOG.info("Only archiving partial indexes")
            for cpath in self._find_files(datadir):
                rpath = os.sep.join(cpath.split(os.sep)[-2:])
                if self.config.lookup('mysqlhotcopy.partial-indexes') \
                    and rpath.endswith('.MYI'):
                    if not self.dry_run:
                        partial_data = open(cpath, 'r').read(2048)
                        archive.add_string(partial_data, rpath)
                    LOG.debug("%s [partial]", rpath)
                else:
                    LOG.debug("%s", rpath)
                    if not self.dry_run:
                        archive.add_file(cpath, rpath)
        except Exception, e:
            error = e
            LOG.error("Failed to archive data file. %s", e)

        if not self.dry_run:
            self.mysqlclient.unlock_tables()
            archive.close()
        if error:
            raise e

    def cleanup(self):
        """
        Finish a backup.
        """
        pass

    def _find_files(self, datadir):
        for db, tbl in self._find_tables():
            base_path = os.path.join(datadir, db, tbl)
            if not os.path.exists(base_path + '.frm'):
                LOG.debug("ARGH %s does not exist", base_path + '.frm')
                if self.mysqlclient.server_version > (5, 1, 0):
                    LOG.debug("Checking for weird encoding...")
                    db = self.mysqlclient.encode_as_filename(db)
                    tbl = self.mysqlclient.encode_as_filename(tbl)
                    LOG.debug("Encoded db: %r", db)
                    LOG.debug("Encoded tbl: %r", tbl)
                base_path = os.path.join(datadir, db, tbl)
                if os.path.exists(base_path + '.frm'):
                    LOG.debug("Encoded path %s DOES exist", base_path)
                else:
                    LOG.debug("FAIL: encodes path %s also does not exist", base_path)
            # Escape any accidental glob patterns
            for c in ('[', ']', '*', '?'):
                base_path = base_path.replace(c, '\\' + c)
            for name in glob.glob(base_path + '.*'):
                yield name


provider = MySQLHotcopy

########NEW FILE########
__FILENAME__ = test_hotcopy
import os
import unittest
import holland.backup.mysqlhotcopy

class TestArchive(unittest.TestCase):
    def setUp(self):
        pass

    def tearDown(self):
        pass

    def test_hotcopy(self):
        pass

########NEW FILE########
__FILENAME__ = innodb
"""Perform InnoDB recovery against a MySQL data directory"""

import os
import time
import signal
import logging
from cStringIO import StringIO
from subprocess import Popen, STDOUT, list2cmdline
from holland.core.exceptions import BackupError
from _mysqld import locate_mysqld_exe, generate_server_config, MySQLServer

LOG = logging.getLogger(__name__)

class InnodbRecoveryAction(object):
    def __init__(self, mysqld_config):
        self.mysqld_config = mysqld_config
        if 'datadir' not in mysqld_config:
            raise BackupError("datadir must be set for InnodbRecovery")

    def __call__(self, event, snapshot_fsm, snapshot):
        LOG.info("Starting InnoDB recovery")

        mysqld_exe = locate_mysqld_exe(self.mysqld_config)
        LOG.info("Bootstrapping with %s", mysqld_exe)

        mycnf_path = os.path.join(self.mysqld_config['datadir'], 
                                  'my.innodb_recovery.cnf')
        self.mysqld_config['log-error'] = 'innodb_recovery.log'
        my_conf = generate_server_config(self.mysqld_config,
                                         mycnf_path)
        
        mysqld = MySQLServer(mysqld_exe, my_conf)
        mysqld.start(bootstrap=True)

        while mysqld.poll() is None:
            if signal.SIGINT in snapshot_fsm.sigmgr.pending:
                mysqld.kill(signal.SIGKILL)
            time.sleep(0.5)
        LOG.info("%s has stopped", mysqld_exe)

        if mysqld.returncode != 0:
            datadir = self.mysqld_config['datadir']
            for line in open(os.path.join(datadir, 'innodb_recovery.log'), 'r'):
                LOG.error("%s", line.rstrip())
            raise BackupError("%s exited with non-zero status (%s) during "
                              "InnoDB recovery" % (mysqld_exe, mysqld.returncode))
        else:
            LOG.info("%s ran successfully", mysqld_exe)

########NEW FILE########
__FILENAME__ = lock
import logging

LOG = logging.getLogger(__name__)

class FlushAndLockMySQLAction(object):
    def __init__(self, client, extra_flush=True):
        self.client = client
        self.extra_flush = extra_flush

    def __call__(self, event, snapshot_fsm, snapshot_vol):
        if event == 'pre-snapshot':
            if self.extra_flush:
                LOG.debug("Executing FLUSH TABLES")
                self.client.flush_tables()
            LOG.debug("Executing FLUSH TABLES WITH READ LOCK")
            LOG.info("Acquiring read-lock and flushing tables")
            self.client.flush_tables_with_read_lock()
        elif event == 'post-snapshot':
            LOG.info("Releasing read-lock")
            self.client.unlock_tables()

########NEW FILE########
__FILENAME__ = mysqldump
"""Dispatch to the holland mysqldump plugin"""

import os
import time
import signal
import logging
from holland.lib.mysql import connect, MySQLError, PassiveMySQLClient
from _mysqld import generate_server_config, MySQLServer, locate_mysqld_exe

LOG = logging.getLogger(__name__)

class MySQLDumpDispatchAction(object):
    def __init__(self, mysqldump_plugin, mysqld_config):
        self.mysqldump_plugin = mysqldump_plugin
        self.mysqld_config = mysqld_config

    def __call__(self, event, snapshot_fsm, snapshot):
        LOG.info("Handing-off to mysqldump plugin")
        datadir = self.mysqld_config['datadir']
        # find a mysqld executable to use
        mysqld_exe = locate_mysqld_exe(self.mysqld_config)

        self.mysqld_config['log-error'] = 'holland_lvm.log'
        socket = os.path.join(datadir, 'holland_mysqldump.sock')
        self.mysqld_config['socket'] = socket
        # patch up socket in plugin
        self.mysqldump_plugin.config['mysql:client']['socket'] = socket
        self.mysqldump_plugin.mysql_config['client']['socket'] = socket
        # set pidfile (careful to not overwrite current one)
        self.mysqld_config['pid-file'] = os.path.join(datadir,
                                                      'holland_lvm.pid')
        mycnf_path = os.path.join(datadir, 'my.bootstrap.cnf')
        # generate a my.cnf to pass to the mysqld bootstrap
        my_conf = generate_server_config(self.mysqld_config, mycnf_path)

        # log-bin is disabled to avoid conflict with the normal mysqld process
        self.mysqldump_plugin.config['mysqldump']['bin-log-position'] = False

        mysqld = MySQLServer(mysqld_exe, my_conf)
        mysqld.start(bootstrap=False)
        LOG.info("Waiting for %s to start", mysqld_exe)

        try:
            wait_for_mysqld(self.mysqldump_plugin.mysql_config['client'],
                            mysqld)
            LOG.info("%s accepting connections on unix socket %s", mysqld_exe, socket)
            self.mysqldump_plugin.backup()
        finally:
            mysqld.kill(signal.SIGKILL) # DIE DIE DIE
            mysqld.stop() # we dont' really care about the exit code, if mysqldump ran smoothly :)

def wait_for_mysqld(config, mysqld):
    client = connect(config, PassiveMySQLClient)
    LOG.debug("connect via client %r", client)
    while mysqld.process.poll() is None:
        try:
            client.connect()
            client.ping()
            LOG.debug("Ping succeeded")
        except MySQLError:
            time.sleep(0.75)
            continue
        else:
            break
    client.disconnect()

########NEW FILE########
__FILENAME__ = replication
import logging
from holland.core.exceptions import BackupError
from holland.lib.mysql import MySQLError

LOG = logging.getLogger(__name__)

class RecordMySQLReplicationAction(object):
    def __init__(self, client, config):
        self.client = client
        self.config = config

    def __call__(self, event, snapshot_fsm, snapshot_vol):
        record_master_status(self.client, self.config)
        record_slave_status(self.client, self.config)

def record_master_status(client, config):
    try:
        LOG.debug("Executing SHOW MASTER STATUS")
        master_status = client.show_master_status()
        if master_status:
            binlog = master_status['file']
            position = master_status['position']
            config['master_log_file'] = binlog
            config['master_log_pos'] = position
            LOG.info("Recorded binlog = %s position = %s",
                    binlog, position)
        else:
            LOG.info("This MySQL server does not have binary logs enabled. "
                     "Nothing to record from SHOW MASTER STATUS.")
    except MySQLError, exc:
        raise BackupError("MySQL error while acquiring master replication "
                          "status [%d] %s" % exc.args)

def record_slave_status(client, config):
    try:
        LOG.debug("Executing SHOW SLAVE STATUS")
        slave_status = client.show_slave_status()
        if slave_status:
            binlog = slave_status['relay_master_log_file']
            position = slave_status['exec_master_log_pos']
            config['slave_master_log_file'] = binlog
            config['slave_master_log_pos'] = position
            LOG.info("Recorded slave replication status: "
                     "master_binlog = %s master_position = %s",
                    binlog, position)
        else:
            LOG.info("This MySQL server is not a slave. "
                     "Nothing to record from SHOW SLAVE STATUS")
    except MySQLError, exc:
        raise BackupError("MySQL error while acquiring slave replication "
                          "status [%d] %s" % exc.args)

########NEW FILE########
__FILENAME__ = _mysqld
"""Common mysqld bootstrapping functionality"""

import os
import signal
import logging
from cStringIO import StringIO
from subprocess import Popen, STDOUT, list2cmdline
from holland.core.exceptions import BackupError
from holland.lib.which import which, WhichError

LOG = logging.getLogger(__name__)

def locate_mysqld_exe(config):
    mysqld_candidates = config.pop('mysqld-exe')
    for candidate in mysqld_candidates:
        if os.path.isabs(candidate):
            path = [os.path.dirname(candidate)]
            candidate = os.path.basename(candidate)
        else:
            path = None # use environ[PATH]
        try:
            LOG.debug("Searching for %s on path %s",
                      candidate, path or os.environ['PATH'])
            return which(candidate, path)
        except WhichError:
            LOG.debug("mysqld path %s does not exist - skipping", candidate)
    raise BackupError("Failed to find mysqld binary")

class MySQLServer(object):
    def __init__(self, mysqld_exe, defaults_file):
        self.mysqld_exe = mysqld_exe
        self.defaults_file = defaults_file
        self.returncode = None
        self.process = None

    def start(self, bootstrap=False):
        args = [
            self.mysqld_exe,
            '--defaults-file=%s' % self.defaults_file,
        ]
        if bootstrap:
            args += ['--bootstrap']
        self.returncode = None
        LOG.info("Starting %s", list2cmdline(args))
        self.process = Popen(args,
                             preexec_fn=os.setsid,
                             stdin=open('/dev/null', 'r'),
                             stdout=open('/dev/null', 'w'),
                             stderr=STDOUT,
                             close_fds=True)
    def stop(self):
        LOG.info("Stopping %s", self.mysqld_exe)
        if self.process:
            #os.kill(self.process.pid, signal.SIGTERM)
            LOG.info("Waiting for MySQL to stop")
            self.process.wait()
            LOG.info("%s stopped", self.mysqld_exe)
            self.returncode = self.process.returncode
            self.process = None

    def poll(self):
        self.returncode = self.process.poll()
        return self.returncode

    def kill(self, signum):
        os.kill(self.process.pid, signum)

    def kill_safe(self, signum):
        try:
            self.kill(signum)
        except OSError:
            pass

    def restart(self):
        self.stop()
        self.start()

def generate_server_config(config, path):
    conf_data = StringIO()
    valid_params = [
        'innodb-buffer-pool-size',
        'innodb-log-file-size',
        'innodb-log-group-home-dir',
        'innodb-data-home-dir',
        'innodb-data-file-path',
        'innodb-fast-shutdown',
        'open-files-limit',
        'key-buffer-size',
        'tmpdir',
        'user',
        'datadir',
        'log-error',
        'socket',
        'pid-file',
        'port',
    ]
    print >>conf_data, "[mysqld]"
    for key, value in config.iteritems():
        if key.replace('_', '-') not in valid_params:
            LOG.warning("Ignoring mysqld config parameter %s", key)
            continue
        print >>conf_data, "%s = %s" % (key, value)
    print >>conf_data, "# not used for --bootstrap but here for completeness"
    print >>conf_data, "port = 3307"
    print >>conf_data, "loose-skip-ndbcluster"
    print >>conf_data, "skip-networking"
    print >>conf_data, "skip-slave-start"
    print >>conf_data, "skip-log-bin"
    text = conf_data.getvalue()
    LOG.debug("Generating config: %s", text)
    open(path, 'w').write(text)
    return path

########NEW FILE########
__FILENAME__ = tar
import os
import time
import shlex
import signal
import logging
from subprocess import list2cmdline, Popen, CalledProcessError
from holland.core.exceptions import BackupError

LOG = logging.getLogger(__name__)

class TarArchiveAction(object):
    def __init__(self, snap_datadir, archive_stream, config):
        self.snap_datadir = snap_datadir
        self.archive_stream = archive_stream
        self.config = config

    def __call__(self, event, snapshot_fsm, snapshot_vol):
        argv = [
            'tar',
            '--create',
            '--file', '-',
            '--verbose',
            '--totals',
            '--directory', self.snap_datadir,
            '.'
        ]

        pre_args = self.config['pre-args']
        if pre_args:
            LOG.info("Adding tar pre-args: %s", pre_args)
            pre_args = [arg.decode('utf8')
                        for arg in shlex.split(pre_args.encode('utf8'))]
            for option in pre_args:
                argv.insert(-3, option)
        for param in self.config['exclude']:
            argv.insert(-1, "--exclude")
            argv.insert(-1, os.path.join('.', param))
        post_args = self.config['post-args']
        if post_args:
            LOG.info("Adding tar post-args: %s", post_args)
            post_args = [arg.decode('utf8')
                         for arg in shlex.split(post_args.encode('utf8'))]
            for option in post_args:
                argv.append(option)
        LOG.info("Running: %s > %s", list2cmdline(argv), self.archive_stream.name)

        archive_dirname = os.path.dirname(self.archive_stream.name)
        if pre_args or post_args:
            warning_readme = os.path.join(archive_dirname, "NONSTD_TAR.txt")
            warning_log = open('warning_readme', 'w')
            print >>warning_log, ("This tar file was generated with non-std "
                                  "args:")
            print >>warning_log, list2cmdline(argv)
        archive_log = os.path.join(archive_dirname, 'archive.log')
        process = Popen(argv,
                        preexec_fn=os.setsid,
                        stdout=self.archive_stream,
                        stderr=open(archive_log, 'w'),
                        close_fds=True)
        while process.poll() is None:
            if signal.SIGINT in snapshot_fsm.sigmgr.pending:
                os.kill(process.pid, signal.SIGKILL)
            time.sleep(0.5)

        try:
            self.archive_stream.close()
        except IOError, exc:
            LOG.error("tar output stream %s failed: %s",
                      self.archive_stream.name, exc)
            raise BackupError(str(exc))

        if signal.SIGINT in snapshot_fsm.sigmgr.pending:
            raise KeyboardInterrupt("Interrupted")

        if process.returncode != 0:
            LOG.error("tar exited with non-zero status: %d",
                      process.returncode)
            LOG.error("Tailing up to the last 10 lines of archive.log for "
                      "troubleshooting:")
            for line in open(archive_log, 'r').readlines()[-10:]:
                LOG.error(" ! %s", line.rstrip())
            raise CalledProcessError(process.returncode, "tar")

########NEW FILE########
__FILENAME__ = common

"""Utility functions to help out the mysql-lvm plugin"""
import os
import errno
import shutil
import tempfile
import logging
from holland.core.exceptions import BackupError
from holland.core.util.fmt import format_bytes
from holland.lib.mysql import PassiveMySQLClient, MySQLError, \
                              build_mysql_config, connect
from holland.lib.lvm import Snapshot, parse_bytes

LOG = logging.getLogger(__name__)

def connect_simple(config):
    """Create a MySQLClientConnection given a mysql:client config
    section from a holland mysql backupset
    """
    try:
        mysql_config = build_mysql_config(config)
        LOG.debug("mysql_config => %r", mysql_config)
        connection = connect(mysql_config['client'], PassiveMySQLClient)
        connection.connect()
        return connection
    except MySQLError, exc:
        raise BackupError("[%d] %s" % exc.args)

def cleanup_tempdir(path):
    LOG.info("Removing temporary mountpoint %s", path)
    shutil.rmtree(path)

def build_snapshot(config, logical_volume, suppress_tmpdir=False):
    """Create a snapshot process for running through the various steps
    of creating, mounting, unmounting and removing a snapshot
    """
    snapshot_name = config['snapshot-name'] or \
                    logical_volume.lv_name + '_snapshot'
    extent_size = int(logical_volume.vg_extent_size)
    snapshot_size = config['snapshot-size']
    if not snapshot_size:
        snapshot_size = min(int(logical_volume.vg_free_count),
                            (int(logical_volume.lv_size)*0.2) / extent_size,
                            (15*1024**3) / extent_size,
                           )
        LOG.info("Auto-sizing snapshot-size to %s (%d extents)",
                 format_bytes(snapshot_size*extent_size),
                 snapshot_size)
        if snapshot_size < 1:
            raise BackupError("Insufficient free extents on %s "
                              "to create snapshot (free extents = %s)" %
                              (logical_volume.device_name(),
                              logical_volume.vg_free_count))
    else:
        try:
            _snapshot_size = snapshot_size
            snapshot_size = parse_bytes(snapshot_size) / extent_size
            LOG.info("Using requested snapshot-size %s "
                     "rounded by extent-size %s to %s.",
                     _snapshot_size,
                     format_bytes(extent_size),
                     format_bytes(snapshot_size*extent_size))
            if snapshot_size < 1:
                raise BackupError("Requested snapshot-size (%s) is "
                                  "less than 1 extent" % _snapshot_size)
            if snapshot_size > int(logical_volume.vg_free_count):
                LOG.info("Snapshot size requested %s, but only %s available.",
                         config['snapshot-size'],
                         format_bytes(int(logical_volume.vg_free_count)*extent_size, precision=4))
                LOG.info("Truncating snapshot-size to %d extents (%s)",
                         int(logical_volume.vg_free_count),
                         format_bytes(int(logical_volume.vg_free_count)*extent_size, precision=4))
                snapshot_size = int(logical_volume.vg_free_count)
        except ValueError, exc:
            raise BackupError("Problem parsing snapshot-size %s" % exc)

    mountpoint = config['snapshot-mountpoint']
    tempdir = False
    if not mountpoint:
        tempdir = True
        if not suppress_tmpdir:
            mountpoint = tempfile.mkdtemp()
    else:
        try:
            os.makedirs(mountpoint)
            LOG.info("Created mountpoint %s", mountpoint)
        except OSError, exc:
            # silently ignore if the mountpoint already exists
            if exc.errno != errno.EEXIST:
                raise BackupError("Failure creating snapshot mountpoint: %s" %
                                  str(exc))
    snapshot = Snapshot(snapshot_name, int(snapshot_size), mountpoint)
    if tempdir:
        snapshot.register('finish',
                          lambda *args, **kwargs: cleanup_tempdir(mountpoint))
    return snapshot

def log_final_snapshot_size(event, snapshot):
    """Log the final size of the snapshot before it is removed"""
    snapshot.reload()
    snap_percent = float(snapshot.snap_percent)/100
    snap_size = float(snapshot.lv_size)
    LOG.info("Final LVM snapshot size for %s is %s",
        snapshot.device_name(), format_bytes(snap_size*snap_percent))

########NEW FILE########
__FILENAME__ = innodb
"""
Path utility functions to inspect MySQL files
"""
import os
import logging
from os.path import isabs, join, realpath, abspath, splitext
from holland.core.util.path import relpath
try:
    from operator import itemgetter
except ImportError:
    def itemgetter(*items):
        if len(items) == 1:
            item = items[0]
            def g(obj):
                return obj[item]
        else:
            def g(obj):
                return tuple(obj[item] for item in items)
        return g

LOG = logging.getLogger(__name__)

class MySQLPathInfo(tuple):
    """Named tuple whose attributes describe the important
    file paths for the files in a MySQL instance.
    """

    __slots__ = ()

    _fields = ('datadir', 'innodb_log_group_home_dir', 'innodb_log_files_in_group', 'innodb_data_home_dir', 'innodb_data_file_path', 'abs_tablespace_paths')

    def __new__(_cls, datadir, innodb_log_group_home_dir, innodb_log_files_in_group, innodb_data_home_dir, innodb_data_file_path, abs_tablespace_paths):
        return tuple.__new__(_cls, (datadir, innodb_log_group_home_dir, innodb_log_files_in_group, innodb_data_home_dir, innodb_data_file_path, abs_tablespace_paths))

    #@classmethod
    def _make(cls, iterable, new=tuple.__new__, len=len):
        'Make a new MySQLPathInfo object from a sequence or iterable'
        result = new(cls, iterable)
        if len(result) != 6:
            raise TypeError('Expected 6 arguments, got %d' % len(result))
        return result
    _make = classmethod(_make)

    def __repr__(self):
        return 'MySQLPathInfo(datadir=%r, innodb_log_group_home_dir=%r, innodb_log_files_in_group=%r, innodb_data_home_dir=%r, innodb_data_file_path=%r, abs_tablespace_paths=%r)' % self

    def _asdict(t):
        'Return a new dict which maps field names to their values'
        return {'datadir': t[0], 'innodb_log_group_home_dir': t[1], 'innodb_log_files_in_group': t[2], 'innodb_data_home_dir': t[3], 'innodb_data_file_path': t[4], 'abs_tablespace_paths': t[5]}

    def _replace(_self, **kwds):
        'Return a new MySQLPathInfo object replacing specified fields with new values'
        result = _self._make(map(kwds.pop, ('datadir', 'innodb_log_group_home_dir', 'innodb_log_files_in_group', 'innodb_data_home_dir', 'innodb_data_file_path', 'abs_tablespace_paths'), _self))
        if kwds:
            raise ValueError('Got unexpected field names: %r' % kwds.keys())
        return result

    def __getnewargs__(self):
        return tuple(self)

    datadir = property(itemgetter(0))
    innodb_log_group_home_dir = property(itemgetter(1))
    innodb_log_files_in_group = property(itemgetter(2))
    innodb_data_home_dir = property(itemgetter(3))
    innodb_data_file_path = property(itemgetter(4))
    abs_tablespace_paths = property(itemgetter(5))

    #@classmethod
    def from_mysql(cls, mysql):
        """Create a MySQLPathInfo instance from a live MySQL connection"""
        ibd_homedir = mysql.show_variable('innodb_data_home_dir')
        abs_tablespace_paths = bool(ibd_homedir == '')
        return cls(
            datadir=mysql.show_variable('datadir'),
            innodb_log_group_home_dir=mysql.show_variable('innodb_log_group_home_dir'),
            innodb_log_files_in_group=mysql.show_variable('innodb_log_files_in_group'),
            innodb_data_home_dir=ibd_homedir,
            innodb_data_file_path=mysql.show_variable('innodb_data_file_path'),
            abs_tablespace_paths=abs_tablespace_paths
        )
    from_mysql = classmethod(from_mysql)

    def get_innodb_logdir(self):
        """Determine the directory for innodb's log files"""
        if isabs(self.innodb_log_group_home_dir):
            logdir = self.innodb_log_group_home_dir
        else:
            logdir = join(self.datadir, self.innodb_log_group_home_dir)

        return abspath(realpath(logdir))

    def get_innodb_datadir(self):
        """Determine the base directory for innodb shared tablespaces"""
        ibd_home_dir = self.innodb_data_home_dir or ''
        if not ibd_home_dir or not isabs(ibd_home_dir):
            ibd_home_dir = join(self.datadir, ibd_home_dir)

        return abspath(realpath(ibd_home_dir))

    def walk_innodb_shared_tablespaces(self):
        """Iterate over InnoDB shared tablespace paths"""
        ibd_homedir = self.get_innodb_datadir()
        ibd_data_file_path = self.innodb_data_file_path

        for spec in ibd_data_file_path.split(';'):
            tblspc_path = spec.split(':')[0]
            if not self.abs_tablespace_paths or not isabs(tblspc_path):
                tblspc_path = join(ibd_homedir, tblspc_path)
            yield abspath(realpath(tblspc_path))

    def walk_innodb_logs(self):
        """Iterate over InnoDB redo log paths"""
        basedir = self.get_innodb_logdir()
        for logid in xrange(self.innodb_log_files_in_group):
            yield join(basedir, 'ib_logfile' + str(logid))

    #@staticmethod
    def remap_path(path, mountpoint):
        """Remap a path to a new mountpoint

        >>> remap_path('/mnt/raid10/foo/bar/baz', '/mnt/snapshot')
        '/mnt/snapshot/foo/bar/baz'
        """
        rpath = relpath(path, getmount(path))
        return os.path.join(mountpoint, rpath)
    remap_path = staticmethod(remap_path)

    def remap_tablespaces(self, mountpoint):
        """Remap innodb-data-file-path paths to a new mountpoint

        innodb-data-file-path = /mnt/raid/ibdata/ibdata1:10M:autoextend
        >>> remap_tablespaces('/mnt/snapshot/')
        '/mnt/snapshot/ibdata/ibdata1:10M:autoextend'
        """
        innodb_data_home_dir = self.innodb_data_home_dir
        innodb_data_file_path = self.innodb_data_file_path
        basedir = self.get_innodb_datadir()
        spec_list = []
        for spec in innodb_data_file_path.split(';'):
            name, rest = spec.split(':', 1)
            if innodb_data_home_dir == '' and isabs(name):
                name = self.remap_path(name, mountpoint)
            spec = ':'.join([name, rest])
            spec_list.append(spec)
        return ';'.join(spec_list)



from holland.core.util.path import getmount
from holland.core.exceptions import BackupError
def is_subdir(path, start):
    """Check if path is a subdirectory or some starting path"""
    path = os.path.abspath(path)
    start = os.path.abspath(start)
    end = os.path.basename(path)
    while path != start and end:
        path, end = os.path.split(path)
    return path == start

def check_innodb(pathinfo, ensure_subdir_of_datadir=False):
    is_unsafe_for_lvm = False
    is_unsafe_for_physical_backups = False
    datadir = realpath(pathinfo.datadir)
    datadir_mp = getmount(datadir)
    for tablespace in pathinfo.walk_innodb_shared_tablespaces():
        space_mp = getmount(tablespace)
        if space_mp != datadir_mp:
            LOG.error("InnoDB shared tablespace %s is not on the same "
                      "filesystem as the datadir %s", tablespace, datadir)
            is_unsafe_for_lvm = True
        if ensure_subdir_of_datadir and not is_subdir(tablespace, datadir):
            LOG.error("InnoDB shared tablespace %s is not within a "
                      "subdirectory of the datadir %s.", tablespace, datadir)
            is_unsafe_for_physical_backups = True
    ib_logdir = pathinfo.get_innodb_logdir()
    ib_logdir_mp = getmount(ib_logdir)

    if ib_logdir_mp != datadir_mp:
        LOG.error("innodb-log-group-home-dir %s is not on the same filesystem "
                  "as the MySQL datadir %s", ib_logdir, datadir)
        is_unsafe_for_lvm = True
    if ensure_subdir_of_datadir and not is_subdir(ib_logdir, datadir):
            LOG.error("innodb-log-group-home-dir %s is not a subdirectory of "
                      "the datadir %s.", ib_logdir, datadir)
            is_unsafe_for_physical_backups = True

    if is_unsafe_for_lvm:
        raise BackupError("One or more InnoDB file paths are not on the same "
                          "logical volume as the datadir.  This is unsafe for "
                          "LVM snapshot backups.")
    if is_unsafe_for_physical_backups:
        raise BackupError("One or more InnoDB files are not contained within "
                          "the MySQL datadir. A consistent filesystem backup "
                          "is not supported with this configuration in the "
                          "current plugin version.")

########NEW FILE########
__FILENAME__ = plugin
"""MySQL LVM snapshot backups"""

import os
import tempfile
import logging
from holland.lib.lvm import LogicalVolume, CallbackFailuresError, \
                            LVMCommandError, relpath, getmount
from holland.lib.mysql.client import MySQLError
from holland.core.util.fmt import format_bytes
from holland.core.util.path import directory_size
from holland.core.exceptions import BackupError
from holland.backup.mysql_lvm.plugin.common import build_snapshot, \
                                                   connect_simple
from holland.backup.mysql_lvm.plugin.mysqldump.util import setup_actions
from holland.backup.mysqldump import MySQLDumpPlugin

LOG = logging.getLogger(__name__)

CONFIGSPEC = """
[mysql-lvm]
# default: mysql lv + _snapshot
snapshot-name = string(default=None)

# default: minimum of 20% of mysql lv or mysql vg free size
snapshot-size = string(default=None)

# default: temporary directory
snapshot-mountpoint = string(default=None)

# default: flush tables with read lock by default
lock-tables = boolean(default=yes)

# default: do an extra (non-locking) flush tables before
#          run flush tables with read lock
extra-flush-tables = boolean(default=yes)

[mysqld]
mysqld-exe              = force_list(default=list('mysqld', '/usr/libexec/mysqld'))
user                    = string(default='mysql')
innodb-buffer-pool-size = string(default=128M)
key-buffer-size         = string(default=16M)
tmpdir                  = string(default=None)

""".splitlines() + MySQLDumpPlugin.CONFIGSPEC

class MysqlDumpLVMBackup(object):
    """A Holland Backup plugin suitable for performing LVM snapshots of a 
    filesystem underlying a live MySQL instance.

    This plugin produces tar archives of a MySQL data directory.
    """
    CONFIGSPEC = CONFIGSPEC

    def __init__(self, name, config, target_directory, dry_run=False):
        self.config = config
        self.config.validate_config(self.CONFIGSPEC)
        LOG.debug("Validated config: %r", self.config)
        self.name = name
        self.target_directory = target_directory
        self.dry_run = dry_run
        self.client = connect_simple(self.config['mysql:client'])
        self.mysqldump_plugin = MySQLDumpPlugin(name, config, target_directory, dry_run)

    def estimate_backup_size(self):
        """Estimate the backup size this plugin will produce

        This is currently the total directory size of the MySQL datadir
        """

        return self.mysqldump_plugin.estimate_backup_size()

    def configspec(self):
        """INI Spec for the configuration values this plugin supports"""
        return self.CONFIGSPEC
    
    def backup(self):
        """Run a backup by running through a LVM snapshot against the device
        the MySQL datadir resides on
        """
        # connect to mysql and lookup what we're supposed to snapshot
        self.client.connect()
        datadir = os.path.realpath(self.client.show_variable('datadir'))
        LOG.info("Backing up %s via snapshot", datadir)
        # lookup the logical volume mysql's datadir sits on
        try:
             volume = LogicalVolume.lookup_from_fspath(datadir)
        except LookupError, exc:
            raise BackupError("Failed to lookup logical volume for %s: %s" %
                              (datadir, str(exc)))


        # create a snapshot manager
        snapshot = build_snapshot(self.config['mysql-lvm'], volume,
                                  suppress_tmpdir=self.dry_run)
        # calculate where the datadirectory on the snapshot will be located
        rpath = relpath(datadir, getmount(datadir))
        snap_datadir = os.path.abspath(os.path.join(snapshot.mountpoint or
        '/tmp', rpath))
        # setup actions to perform at each step of the snapshot process
        setup_actions(snapshot=snapshot,
                      config=self.config,
                      client=self.client,
                      datadir=snap_datadir,
                      spooldir=self.target_directory,
                      plugin=self.mysqldump_plugin)

        if self.config['mysqldump']['bin-log-position']:
            LOG.warn("bin-log-position is not supported with mysqldump-lvm.")
            LOG.warn("Replication status will be saved to the "
                     "[mysql:replication] section in %s",
                    self.config.filename)
            self.config['mysqldump']['bin-log-position'] = False

        if self.dry_run:
            self._dry_run(volume, snapshot, datadir)
            # do the normal mysqldump dry-run
            return self.mysqldump_plugin.backup()

        try:
            snapshot.start(volume)
        except CallbackFailuresError, exc:
            # XXX: one of our actions failed.  Log this better
            for callback, error in exc.errors:
                LOG.error("%s", error)
            raise BackupError("Error occurred during snapshot process. Aborting.")
        except LVMCommandError, exc:
            # Something failed in the snapshot process
            raise BackupError(str(exc))

    def _dry_run(self, volume, snapshot, datadir):
        """Implement dry-run for LVM snapshots.
        """
        LOG.info("* Would snapshot source volume %s/%s as %s/%s (size=%s)",
             volume.vg_name,
             volume.lv_name,
             volume.vg_name,
             snapshot.name,
             format_bytes(snapshot.size*int(volume.vg_extent_size)))
        LOG.info("* Would mount on %s",
             snapshot.mountpoint or 'generated temporary directory')

        snapshot_mountpoint = snapshot.mountpoint or tempfile.gettempdir()
        if getmount(self.target_directory) == getmount(datadir):
            LOG.error("Backup directory %s is on the same filesystem as "
                      "the source logical volume %s.",
                      self.target_directory, volume.device_name())
            LOG.error("This will result in very poor performance and "
                      "has a high potential for failed backups.")
            raise BackupError("Improper backup configuration for LVM.")

########NEW FILE########
__FILENAME__ = util
"""Utility functions to help out the mysql-lvm plugin"""
import os
import shutil
import tempfile
import logging
from holland.core.exceptions import BackupError
from holland.core.util.fmt import format_bytes
from holland.lib.mysql import PassiveMySQLClient, MySQLError, \
                              build_mysql_config, connect
from holland.lib.lvm import Snapshot, parse_bytes
from holland.backup.mysql_lvm.actions import FlushAndLockMySQLAction, \
                                             RecordMySQLReplicationAction, \
                                             MySQLDumpDispatchAction
from holland.backup.mysql_lvm.plugin.common import log_final_snapshot_size, \
                                                   connect_simple
from holland.backup.mysql_lvm.plugin.innodb import MySQLPathInfo, check_innodb

LOG = logging.getLogger(__name__)

def setup_actions(snapshot, config, client, datadir, spooldir, plugin):
    """Setup actions for a LVM snapshot based on the provided
    configuration.

    Optional actions:
        * MySQL locking
        * Recording MySQL replication
    """

    if config['mysql-lvm']['lock-tables']:
        extra_flush = config['mysql-lvm']['extra-flush-tables']
        act = FlushAndLockMySQLAction(client, extra_flush)
        snapshot.register('pre-snapshot', act, priority=100)
        snapshot.register('post-snapshot', act, priority=100)
    if config['mysql-lvm'].get('replication', True):
        repl_cfg = config.setdefault('mysql:replication', {})
        act = RecordMySQLReplicationAction(client, repl_cfg)
        snapshot.register('pre-snapshot', act, 0)

    mysqld_config = dict(config['mysqld'])
    mysqld_config['datadir'] = datadir
    if not mysqld_config['tmpdir']:
        mysqld_config['tmpdir'] = tempfile.gettempdir()

    if client.show_variable('have_innodb') == 'YES':
        pathinfo = MySQLPathInfo.from_mysql(client)
        check_innodb(pathinfo)

        ib_log_size = client.show_variable('innodb_log_file_size')
        if ib_log_size:
            mysqld_config['innodb-log-file-size'] = ib_log_size

        ibd_home_dir = pathinfo.innodb_data_home_dir
        if ibd_home_dir:
            # innodb_data_home_dir is set to something
            ibd_home_dir = pathinfo.remap_path(pathinfo.get_innodb_datadir(),
                                               snapshot.mountpoint)
            mysqld_config['innodb-data-home-dir'] = ibd_home_dir
            LOG.info("Remapped innodb-data-home-dir from %s to %s for snapshot",
                     pathinfo.get_innodb_datadir(), ibd_home_dir)

        ibd_file_path = pathinfo.innodb_data_file_path
        if ibd_file_path:
            ibd_file_path = pathinfo.remap_tablespaces(snapshot.mountpoint)
            mysqld_config['innodb-data-file-path'] = ibd_file_path
            if ibd_file_path != pathinfo.innodb_data_file_path:
                LOG.info("Remapped innodb-data-file-path from %s to %s for snapshot",
                         pathinfo.innodb_data_file_path, ibd_file_path)
                if not ibd_home_dir:
                    LOG.info("Remapped one or more tablespaces but "
                             "innodb-data-home-dir is not set. Setting "
                             "innodb-data-home-dir = '' to support absolute "
                             "tablespace paths on snapshot.")
                    mysqld_config['innodb-data-home-dir'] = ""

        ib_logdir = pathinfo.innodb_log_group_home_dir
        if ib_logdir and ib_logdir != './':
            ib_logdir = pathinfo.remap_path(pathinfo.get_innodb_logdir(),
                                            snapshot.mountpoint)
            mysqld_config['innodb-log-group-home-dir'] = ib_logdir
            LOG.info("Remapped innodb-log-group-home-dir from %s to %s for snapshot",
                     pathinfo.get_innodb_logdir(), ib_logdir)

    act = MySQLDumpDispatchAction(plugin, mysqld_config)
    snapshot.register('post-mount', act, priority=100)

    errlog_src = os.path.join(datadir, 'holland_lvm.log')
    errlog_dst = os.path.join(spooldir, 'holland_lvm.log')
    snapshot.register('pre-unmount',
                      lambda *args, **kwargs: shutil.copyfile(errlog_src,
                                                              errlog_dst)
                     )

    snapshot.register('pre-remove', log_final_snapshot_size)

########NEW FILE########
__FILENAME__ = plugin
"""MySQL LVM snapshot backups"""

import os
import logging
import tempfile
from holland.core.util.path import directory_size, format_bytes
from holland.core.exceptions import BackupError
from holland.lib.lvm import LogicalVolume, CallbackFailuresError, \
                            LVMCommandError, relpath, getmount
from holland.lib.mysql.client import MySQLError
from holland.backup.mysql_lvm.plugin.common import build_snapshot, \
                                                   connect_simple
from holland.backup.mysql_lvm.plugin.raw.util import setup_actions

LOG = logging.getLogger(__name__)

CONFIGSPEC = """
[mysql-lvm]
# default: mysql lv + _snapshot
snapshot-name = string(default=None)

# default: minimum of 20% of mysql lv or mysql vg free size
snapshot-size = string(default=None)

# default: temporary directory
snapshot-mountpoint = string(default=None)

# default: no
innodb-recovery = boolean(default=no)

# ignore errors due to strange innodb configurations
force-innodb-backup = boolean(default=no)

# default: flush tables with read lock by default
lock-tables = boolean(default=yes)

# default: do an extra (non-locking) flush tables before
#          run flush tables with read lock
extra-flush-tables = boolean(default=yes)

[mysqld]
mysqld-exe              = force_list(default=list('mysqld', '/usr/libexec/mysqld'))
user                    = string(default='mysql')
innodb-buffer-pool-size = string(default=128M)
tmpdir                  = string(default=None)

[tar]
exclude = force_list(default='mysql.sock')
post-args = string(default=None)
pre-args = string(default=None)

[compression]
method = option('none', 'gzip', 'gzip-rsyncable', 'pigz', 'bzip2', 'pbzip2', 'lzop', default='gzip')
options = string(default="")
level = integer(min=0, max=9, default=1)

[mysql:client]
# default: ~/.my.cnf
defaults-file = string(default='~/.my.cnf')
defaults-extra-file = force_list(default=list('~/.my.cnf'))

# default: current user
user = string(default=None)

# default: none
password = string(default=None)

# default: localhost
host = string(default=None)

# default: 3306
port = integer(default=None)
# default: none
socket = string(default=None)
""".splitlines()

class MysqlLVMBackup(object):
    """A Holland Backup plugin suitable for performing LVM snapshots of a 
    filesystem underlying a live MySQL instance.

    This plugin produces tar archives of a MySQL data directory.
    """
    CONFIGSPEC = CONFIGSPEC

    def __init__(self, name, config, target_directory, dry_run=False):
        self.config = config
        self.config.validate_config(self.configspec())
        LOG.debug("Validated config: %r", self.config)
        self.name = name
        self.target_directory = target_directory
        self.dry_run = dry_run
        self.client = connect_simple(self.config['mysql:client'])

    def estimate_backup_size(self):
        """Estimate the backup size this plugin will produce

        This is currently the total directory size of the MySQL datadir
        """
        try:
            self.client.connect()
            datadir = self.client.show_variable('datadir')
            self.client.disconnect()
        except MySQLError, exc:
            raise BackupError("[%d] %s" % exc.args)
        return directory_size(datadir)

    def configspec(self):
        """INI Spec for the configuration values this plugin supports"""
        return self.CONFIGSPEC

    def backup(self):
        """Run a backup by running through a LVM snapshot against the device
        the MySQL datadir resides on
        """
        # connect to mysql and lookup what we're supposed to snapshot
        try:
            self.client.connect()
            datadir = os.path.realpath(self.client.show_variable('datadir'))
        except MySQLError, exc:
            raise BackupError("[%d] %s" % exc.args)

        LOG.info("Backing up %s via snapshot", datadir)
        # lookup the logical volume mysql's datadir sits on

        try:
            volume = LogicalVolume.lookup_from_fspath(datadir)
        except LookupError, exc:
            raise BackupError("Failed to lookup logical volume for %s: %s" %
                              (datadir, str(exc)))


        # create a snapshot manager
        snapshot = build_snapshot(self.config['mysql-lvm'], volume,
                                  suppress_tmpdir=self.dry_run)
        # calculate where the datadirectory on the snapshot will be located
        rpath = relpath(datadir, getmount(datadir))
        snap_datadir = os.path.abspath(os.path.join(snapshot.mountpoint, rpath))
        # setup actions to perform at each step of the snapshot process
        setup_actions(snapshot=snapshot,
                      config=self.config,
                      client=self.client,
                      snap_datadir=snap_datadir,
                      spooldir=self.target_directory)

        if self.dry_run:
            return self._dry_run(volume, snapshot, datadir)

        try:
            snapshot.start(volume)
        except CallbackFailuresError, exc:
            # XXX: one of our actions failed.  Log this better
            for callback, error in exc.errors:
                LOG.error("%s", error)
            raise BackupError("Error occurred during snapshot process. Aborting.")
        except LVMCommandError, exc:
            # Something failed in the snapshot process
            raise BackupError(str(exc))

    def _dry_run(self, volume, snapshot, datadir):
        """Implement dry-run for LVM snapshots.
        """
        LOG.info("* Would snapshot source volume %s/%s as %s/%s (size=%s)",
             volume.vg_name,
             volume.lv_name,
             volume.vg_name,
             snapshot.name,
             format_bytes(snapshot.size*int(volume.vg_extent_size)))
        LOG.info("* Would mount on %s",
             snapshot.mountpoint or 'generated temporary directory')

        snapshot_mountpoint = snapshot.mountpoint or tempfile.gettempdir()
        if getmount(self.target_directory) == getmount(datadir):
            LOG.error("Backup directory %s is on the same filesystem as "
                      "the source logical volume %s.",
                      self.target_directory, volume.device_name())
            LOG.error("This will result in very poor performance and "
                      "has a high potential for failed backups.")
            raise BackupError("Improper backup configuration for LVM.")

########NEW FILE########
__FILENAME__ = util
"""Utility functions to help out the mysql-lvm plugin"""
import os
import shutil
import tempfile
import logging
from holland.core.backup import BackupError
from holland.lib.compression import open_stream
from holland.backup.mysql_lvm.actions import FlushAndLockMySQLAction, \
                                             RecordMySQLReplicationAction, \
                                             InnodbRecoveryAction, \
                                             TarArchiveAction
from holland.backup.mysql_lvm.plugin.common import log_final_snapshot_size, \
                                                   connect_simple
from holland.backup.mysql_lvm.plugin.innodb import MySQLPathInfo, check_innodb

LOG = logging.getLogger(__name__)

def setup_actions(snapshot, config, client, snap_datadir, spooldir):
    """Setup actions for a LVM snapshot based on the provided
    configuration.

    Optional actions:
        * MySQL locking
        * InnoDB recovery
        * Recording MySQL replication
    """
    mysql = connect_simple(config['mysql:client'])
    if mysql.show_variable('have_innodb') == 'YES':
        try:
            pathinfo = MySQLPathInfo.from_mysql(mysql)
        finally:
            mysql.close()
        try:
            check_innodb(pathinfo, ensure_subdir_of_datadir=True)
        except BackupError:
            if not config['mysql-lvm']['force-innodb-backup']:
                raise

    if config['mysql-lvm']['lock-tables']:
        extra_flush = config['mysql-lvm']['extra-flush-tables']
        act = FlushAndLockMySQLAction(client, extra_flush)
        snapshot.register('pre-snapshot', act, priority=100)
        snapshot.register('post-snapshot', act, priority=100)
    if config['mysql-lvm'].get('replication', True):
        repl_cfg = config.setdefault('mysql:replication', {})
        act = RecordMySQLReplicationAction(client, repl_cfg)
        snapshot.register('pre-snapshot', act, 0)
    if config['mysql-lvm']['innodb-recovery']:
        mysqld_config = dict(config['mysqld'])
        mysqld_config['datadir'] = snap_datadir
        if not mysqld_config['tmpdir']:
            mysqld_config['tmpdir'] = tempfile.gettempdir()
        ib_log_size = client.show_variable('innodb_log_file_size')
        mysqld_config['innodb-log-file-size'] = ib_log_size
        act = InnodbRecoveryAction(mysqld_config)
        snapshot.register('post-mount', act, priority=100)

    try:
        archive_stream = open_stream(os.path.join(spooldir, 'backup.tar'),
                                     'w',
                                     method=config['compression']['method'],
                                     level=config['compression']['level'],
                                     extra_args=config['compression']['options'])
    except OSError, exc:
        raise BackupError("Unable to create archive file '%s': %s" %
                          (os.path.join(spooldir, 'backup.tar'), exc))
    act = TarArchiveAction(snap_datadir, archive_stream, config['tar'])
    snapshot.register('post-mount', act, priority=50)

    snapshot.register('pre-remove', log_final_snapshot_size)

########NEW FILE########
__FILENAME__ = lvm_helper
"""Help setup lvm volumes"""
import os
from holland.backup.lvm.pylvm.api import lvcreate, lvremove, run_cmd

VGNAME = os.environ['VGNAME']
LVNAME = 'pylvm_test'
LVSIZE = '64M'
MKFS = os.getenv('MKFS', '/sbin/mkfs.ext3')

DEVICE = os.path.join(os.sep, 'dev', VGNAME, LVNAME)

def lv_setup():
    lvcreate(LVNAME, LVSIZE, VGNAME)
    run_cmd(MKFS, DEVICE)

def lv_teardown():
    lvremove(DEVICE)


MOUNTPOINT = None

def setup():
    global MOUNTPOINT
    import tempfile
    MOUNTPOINT = tempfile.mkdtemp()

def teardown():
    if os.path.exists(MOUNTPOINT or ''):
        import shutil
        shutil.rmtree(MOUNTPOINT)

setup()

########NEW FILE########
__FILENAME__ = test_fmt
import nose.tools
from holland.backup.lvm.pylvm.fmt import *

def test_prohibited_lvm_name():
    # 'snapshot' is prohibited
    nose.tools.assert_raises(SyntaxError, validate_name, 'snapshot')
    # _mlog or _image anywhere in a name is probihited
    nose.tools.assert_raises(SyntaxError, validate_name, 'foo_mlog')
    nose.tools.assert_raises(SyntaxError, validate_name, 'foo_mimage_bar')
    # names may not start with a '-'
    nose.tools.assert_raises(SyntaxError, validate_name, '-foo_bar_baz')

def test_valid_names():
    validate_name('mysql_snapshot')
    validate_name('mysql-snapshot')
    validate_name('09mysql19-snapshot-test')

########NEW FILE########
__FILENAME__ = test_lvm_api
import os
from holland.backup.lvm.pylvm.api import *
from nose.tools import *
from lvm_helper import *

def test_pvs():
    for pv in pvs():
        if pv['vg_name'] == VGNAME:
            break
    else:
        pv = None
    assert pv is not None, "Could not find Physical Volume for tested volume group %r" % VGNAME
    pv_again, = pvs(pv['pv_name'])
    ok_(pv_again['pv_name'] == pv['pv_name'])

def test_lvm_dict():
    info = "LVM2_PV_NAME='/dev/sdb' LVM2_VG_NAME='dba' LVM2_PV_FMT='lvm2' LVM2_PV_ATTR='a-' LVM2_PV_SIZE='135.50G' LVM2_PV_FREE='4.50G'"
    dinfo, = lvm_dict(info)
    assert 'pv_name' in dinfo

def test_lvremove():
    lvremove('%s/pylvm_test' % VGNAME)
    assert_raises(LVMError, lvs, '%s/pylvm_test' % VGNAME)
test_lvremove.setup = lv_setup

def test_lvremove_invalid():
    assert_raises(LVMError, lvremove, 'foobarbaz')

def test_lvsnapshot():
    lvsnapshot(lv_name='%s/pylvm_test' % VGNAME,
               snapshot_name='pylvm_test_snapshot',
               snapshot_size='64M')
    lvremove('%s/pylvm_test_snapshot' % VGNAME)
test_lvsnapshot.setup = lv_setup
test_lvsnapshot.teardown = lv_teardown

def test_mount_unmount():
    device_path = os.path.join(os.sep, 'dev', VGNAME, 'pylvm_test')
    pre_mount_dev = os.stat(MOUNTPOINT).st_dev
    mount(device_path, MOUNTPOINT)
    post_mount_dev = os.stat(MOUNTPOINT).st_dev
    ok_(pre_mount_dev != post_mount_dev, "Mount did not change device!")

    pre_unmount_dev = os.stat(MOUNTPOINT).st_dev
    unmount(MOUNTPOINT)
    post_unmount_dev = os.stat(MOUNTPOINT).st_dev
    ok_(pre_mount_dev != post_mount_dev, "Unmount did not change path device!")
test_mount_unmount.setup = lv_setup
test_mount_unmount.teardown = lv_teardown

def test_lv_exists():
    lv_setup()
    device_path = os.path.join(os.sep, 'dev', VGNAME, 'pylvm_test')
    ok_(os.path.exists(device_path), "lvcreate succeeded, but could not find logical volume!")
test_lv_exists.teardown = lv_teardown

########NEW FILE########
__FILENAME__ = test_lvm_objects
import os
from holland.backup.lvm.pylvm.objects import *
from nose.tools import *
from lvm_helper import *

def test_find_lv():
    lv, = LogicalVolume.find('%s/%s' % (VGNAME, LVNAME))
    assert lv.lv_name == LVNAME
    assert lv.volume_group.vg_name == VGNAME
    lv = LogicalVolume.find_one("%s/%s" % (VGNAME, LVNAME))
    assert lv.lv_name == LVNAME
    assert lv.volume_group.vg_name == VGNAME
    assert_raises(LVMError, LogicalVolume.find_one, '%s/foo-bar-baz' % VGNAME)
test_find_lv.setup = lv_setup
test_find_lv.teardown = lv_teardown

def test_lv_is_mounted():
    lv, = LogicalVolume.find('%s/%s' % (VGNAME, LVNAME))
    lv.mount(MOUNTPOINT)
    assert lv.is_mounted() is True
    lv.unmount()
    assert lv.is_mounted() is False
test_lv_is_mounted.setup = lv_setup
test_lv_is_mounted.teardown = lv_teardown

def test_find_vg():
    # find, finds a list
    vg, = VolumeGroup.find(VGNAME)
    assert vg.vg_name == VGNAME
    assert LVNAME in [x.lv_name for x in vg.lvs] 

    # find_one returns first matching
    vg = VolumeGroup.find_one(VGNAME)
    assert vg.vg_name == VGNAME
    assert LVNAME in [x.lv_name for x in vg.lvs]
    
    # Also check that missing groups raise appropriate errors
    assert_raises(LVMError, VolumeGroup.find_one, 'foo-bar-baz')

test_find_vg.setup = lv_setup
test_find_vg.teardown = lv_teardown

def test_lv_str_repr():
    lv, = LogicalVolume.find('%s/%s' % (VGNAME, LVNAME))
    assert str(lv).startswith('LogicalVolume(')
    assert repr(lv) == str(lv)
test_lv_str_repr.setup = lv_setup
test_lv_str_repr.teardown = lv_teardown

def test_vg_str_repr():
    vg, = VolumeGroup.find(VGNAME)
    assert str(vg) == repr(vg)
test_vg_str_repr.setup = lv_setup
test_vg_str_repr.teardown = lv_teardown

def test_lvobj_remove_mounted():
    lv, = LogicalVolume.find('%s/%s' % (VGNAME, LVNAME))
    snapshot = lv.snapshot()
    snapshot.mount(MOUNTPOINT)
    assert_raises(AssertionError, snapshot.remove)
    snapshot.unmount()
    snapshot.remove()
test_lvobj_remove_mounted.setup = lv_setup
test_lvobj_remove_mounted.teardown = lv_teardown

def test_lvobj_remove():
    lv, = LogicalVolume.find('%s/%s' % (VGNAME, LVNAME))
    snapshot_lv = lv.snapshot()
    ok_(snapshot_lv.exists())
    snapshot_lv.remove()
    ok_(not snapshot_lv.exists())

test_lvobj_remove.setup = lv_setup
test_lvobj_remove.teardown = lv_teardown

########NEW FILE########
__FILENAME__ = test_runcmd
from nose.tools import *
from holland.backup.lvm.pylvm.api import run_cmd, which, LVMError

def test_fail_cmd():
    assert_raises(LVMError, run_cmd, '/bin/false')

def test_lvm_warning_cmd():
    assert_raises(EnvironmentError, run_cmd, 'python', '-c', 'import sys; print >>sys.stderr, "WARNING: TEST"')

def test_which_notfound():
    assert_raises(LVMError, which, '/bin/foo_bar_baz_9876543210')

########NEW FILE########
__FILENAME__ = test_units
import holland.backup.lvm.pylvm.fmt
from nose.tools import *

def test_parse_size():
    ok_(holland.backup.lvm.pylvm.fmt.parse_size('42G') == (42 * 1024 ** 3))
    ok_(holland.backup.lvm.pylvm.fmt.parse_size('1024') == holland.backup.lvm.pylvm.fmt.parse_size('1024M'))
    ok_(holland.backup.lvm.pylvm.fmt.parse_size('42G') == holland.backup.lvm.pylvm.fmt.parse_size('43008M'))
    assert_raises(SyntaxError, holland.backup.lvm.pylvm.fmt.parse_size, '42Q')

def test_format_size():
    ok_(holland.backup.lvm.pylvm.fmt.format_size(holland.backup.lvm.pylvm.fmt.parse_size('42G'), 0) == '42G')
    ok_(holland.backup.lvm.pylvm.fmt.format_size(1024, 0) == '1K')
    ok_(holland.backup.lvm.pylvm.fmt.format_size(1024 ** 7, 0) == '1024E')
    assert_raises(ValueError, holland.backup.lvm.pylvm.fmt.format_size, 512)

def test_validate_size():
    ok_(holland.backup.lvm.pylvm.fmt.validate_size('12G') == '12.0000G')

    # Test that a string with an invalid unit generates a syntax error
    assert_raises(SyntaxError, holland.backup.lvm.pylvm.fmt.validate_size, '12Q')
    # Test that a valid unit that is otherwise unusable fails (e.g. < 1K)
    assert_raises(ValueError, holland.backup.lvm.pylvm.fmt.validate_size, '0.5K')

########NEW FILE########
__FILENAME__ = test_argparse
# -*- coding: utf-8 -*-

# Copyright  2006-2009 Steven J. Bethard <steven.bethard@gmail.com>.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not
# use this file except in compliance with the License. You may obtain a copy
# of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import codecs
import os
import shutil
import sys
import textwrap
import tempfile
import unittest
import holland.backup.lvm.cli.argparse as argparse

__test__ = False

try:
    from StringIO import StringIO
except ImportError:
    from io import StringIO

try:
    set
except NameError:
    from sets import Set as set

try:
    sorted
except NameError:

    def sorted(iterable, reverse=False):
        result = list(iterable)
        result.sort()
        if reverse:
            result.reverse()
        return result

# silence Python 2.6 buggy warnings about Exception.message
if sys.version_info[:2] == (2, 6):
    import warnings
    warnings.filterwarnings(
        action='ignore',
        message='BaseException.message has been deprecated as of Python 2.6',
        category=DeprecationWarning)


class TestCase(unittest.TestCase):

    def assertEqual(self, obj1, obj2):
        if obj1 != obj2:
            print('')
            print(repr(obj1))
            print(repr(obj2))
            print(obj1)
            print(obj2)
        super(TestCase, self).assertEqual(obj1, obj2)


class TempDirMixin(object):

    def setUp(self):
        self.temp_dir = tempfile.mkdtemp()
        self.old_dir = os.getcwd()
        os.chdir(self.temp_dir)

    def tearDown(self):
        os.chdir(self.old_dir)
        while True:
            try:
                shutil.rmtree(self.temp_dir)
            except WindowsError:
                continue
            else:
                break


class Sig(object):

    def __init__(self, *args, **kwargs):
        self.args = args
        self.kwargs = kwargs


class NS(object):

    def __init__(self, **kwargs):
        self.__dict__.update(kwargs)

    def __repr__(self):
        sorted_items = sorted(self.__dict__.items())
        kwarg_str = ', '.join(['%s=%r' % tup for tup in sorted_items])
        return '%s(%s)' % (type(self).__name__, kwarg_str)

    def __eq__(self, other):
        return vars(self) == vars(other)

    def __ne__(self, other):
        return not (self == other)


class ArgumentParserError(Exception):

    def __init__(self, message, error_code):
        Exception.__init__(self, message)
        self.message = message
        self.error_code = error_code


def stderr_to_parser_error(func, *args, **kwargs):
    # if this is being called recursively and stderr is already being
    # redirected, simply call the function and let the enclosing function
    # catch the exception
    if isinstance(sys.stderr, StringIO):
        return func(*args, **kwargs)

    # if this is not being called recursively, redirect stderr and
    # use it as the ArgumentParserError message
    old_stderr = sys.stderr
    sys.stderr = StringIO()
    try:
        try:
            return func(*args, **kwargs)
        except SystemExit:
            code = sys.exc_info()[1].code
            message = sys.stderr.getvalue()
            raise ArgumentParserError(message, code)
    finally:
        sys.stderr = old_stderr


class ErrorRaisingArgumentParser(argparse.ArgumentParser):

    def parse_args(self, *args, **kwargs):
        parse_args = super(ErrorRaisingArgumentParser, self).parse_args
        return stderr_to_parser_error(parse_args, *args, **kwargs)

    def exit(self, *args, **kwargs):
        exit = super(ErrorRaisingArgumentParser, self).exit
        return stderr_to_parser_error(exit, *args, **kwargs)

    def error(self, *args, **kwargs):
        error = super(ErrorRaisingArgumentParser, self).error
        return stderr_to_parser_error(error, *args, **kwargs)


class ParserTesterMetaclass(type):
    """Adds parser tests using the class attributes.

    Classes of this type should specify the following attributes:

    argument_signatures -- a list of Sig objects which specify
        the signatures of Argument objects to be created
    failures -- a list of args lists that should cause the parser
        to fail
    successes -- a list of (initial_args, options, remaining_args) tuples
        where initial_args specifies the string args to be parsed,
        options is a dict that should match the vars() of the options
        parsed out of initial_args, and remaining_args should be any
        remaining unparsed arguments
    """

    def __init__(cls, name, bases, bodydict):
        if name == 'ParserTestCase':
            return

        # default parser signature is empty
        if not hasattr(cls, 'parser_signature'):
            cls.parser_signature = Sig()

        # ---------------------------------------
        # functions for adding optional arguments
        # ---------------------------------------
        def no_groups(parser, argument_signatures):
            """Add all arguments directly to the parser"""
            for sig in argument_signatures:
                parser.add_argument(*sig.args, **sig.kwargs)

        def one_group(parser, argument_signatures):
            """Add all arguments under a single group in the parser"""
            group = parser.add_argument_group('foo')
            for sig in argument_signatures:
                group.add_argument(*sig.args, **sig.kwargs)

        def many_groups(parser, argument_signatures):
            """Add each argument in its own group to the parser"""
            for i, sig in enumerate(argument_signatures):
                group = parser.add_argument_group('foo:%i' % i)
                group.add_argument(*sig.args, **sig.kwargs)

        # --------------------------
        # functions for parsing args
        # --------------------------
        def listargs(parser, args):
            """Parse the args by passing in a list"""
            return parser.parse_args(args)

        def sysargs(parser, args):
            """Parse the args by defaulting to sys.argv"""
            old_sys_argv = sys.argv
            sys.argv = [old_sys_argv[0]] + args
            try:
                return parser.parse_args()
            finally:
                sys.argv = old_sys_argv

        # class that holds the combination of one optional argument
        # addition method and one arg parsing method
        class AddTests(object):

            def __init__(self, tester_cls, add_arguments, parse_args):
                self._add_arguments = add_arguments
                self._parse_args = parse_args

                add_arguments_name = self._add_arguments.__name__
                parse_args_name = self._parse_args.__name__
                for test_func in [self.test_failures, self.test_successes]:
                    func_name = test_func.__name__
                    names = func_name, add_arguments_name, parse_args_name
                    test_name = '_'.join(names)

                    def wrapper(self, test_func=test_func):
                        test_func(self)
                    try:
                        wrapper.__name__ = test_name
                    except TypeError:
                        pass
                    setattr(tester_cls, test_name, wrapper)

            def _get_parser(self, tester):
                args = tester.parser_signature.args
                kwargs = tester.parser_signature.kwargs
                parser = ErrorRaisingArgumentParser(*args, **kwargs)
                self._add_arguments(parser, tester.argument_signatures)
                return parser

            def test_failures(self, tester):
                parser = self._get_parser(tester)
                for args_str in tester.failures:
                    args = args_str.split()
                    raises = tester.assertRaises
                    raises(ArgumentParserError, parser.parse_args, args)

            def test_successes(self, tester):
                parser = self._get_parser(tester)
                for args, expected_ns in tester.successes:
                    if isinstance(args, str):
                        args = args.split()
                    result_ns = self._parse_args(parser, args)
                    tester.assertEqual(expected_ns, result_ns)

        # add tests for each combination of an optionals adding method
        # and an arg parsing method
        for add_arguments in [no_groups, one_group, many_groups]:
            for parse_args in [listargs, sysargs]:
                AddTests(cls, add_arguments, parse_args)

bases = TestCase,
ParserTestCase = ParserTesterMetaclass('ParserTestCase', bases, {})

# ===============
# Optionals tests
# ===============

class TestOptionalsSingleDash(ParserTestCase):
    """Test an Optional with a single-dash option string"""

    argument_signatures = [Sig('-x')]
    failures = ['-x', 'a', '--foo', '-x --foo', '-x -y']
    successes = [
        ('', NS(x=None)),
        ('-x a', NS(x='a')),
        ('-xa', NS(x='a')),
        ('-x -1', NS(x='-1')),
        ('-x-1', NS(x='-1')),
    ]


class TestOptionalsSingleDashCombined(ParserTestCase):
    """Test an Optional with a single-dash option string"""

    argument_signatures = [
        Sig('-x', action='store_true'),
        Sig('-yyy', action='store_const', const=42),
        Sig('-z'),
    ]
    failures = ['a', '--foo', '-xa', '-x --foo', '-x -z', '-z -x',
                '-yx', '-yz a', '-yyyx', '-yyyza', '-xyza']
    successes = [
        ('', NS(x=False, yyy=None, z=None)),
        ('-x', NS(x=True, yyy=None, z=None)),
        ('-za', NS(x=False, yyy=None, z='a')),
        ('-z a', NS(x=False, yyy=None, z='a')),
        ('-xza', NS(x=True, yyy=None, z='a')),
        ('-xz a', NS(x=True, yyy=None, z='a')),
        ('-x -za', NS(x=True, yyy=None, z='a')),
        ('-x -z a', NS(x=True, yyy=None, z='a')),
        ('-y', NS(x=False, yyy=42, z=None)),
        ('-yyy', NS(x=False, yyy=42, z=None)),
        ('-x -yyy -za', NS(x=True, yyy=42, z='a')),
        ('-x -yyy -z a', NS(x=True, yyy=42, z='a')),
    ]


class TestOptionalsSingleDashLong(ParserTestCase):
    """Test an Optional with a multi-character single-dash option string"""

    argument_signatures = [Sig('-foo')]
    failures = ['-foo', 'a', '--foo', '-foo --foo', '-foo -y', '-fooa']
    successes = [
        ('', NS(foo=None)),
        ('-foo a', NS(foo='a')),
        ('-foo -1', NS(foo='-1')),
        ('-fo a', NS(foo='a')),
        ('-f a', NS(foo='a')),
    ]


class TestOptionalsSingleDashSubsetAmbiguous(ParserTestCase):
    """Test Optionals where option strings are subsets of each other"""

    argument_signatures = [Sig('-f'), Sig('-foobar'), Sig('-foorab')]
    failures = ['-f', '-foo', '-fo', '-foo b', '-foob', '-fooba', '-foora']
    successes = [
        ('', NS(f=None, foobar=None, foorab=None)),
        ('-f a', NS(f='a', foobar=None, foorab=None)),
        ('-fa', NS(f='a', foobar=None, foorab=None)),
        ('-foa', NS(f='oa', foobar=None, foorab=None)),
        ('-fooa', NS(f='ooa', foobar=None, foorab=None)),
        ('-foobar a', NS(f=None, foobar='a', foorab=None)),
        ('-foorab a', NS(f=None, foobar=None, foorab='a')),
    ]


class TestOptionalsSingleDashAmbiguous(ParserTestCase):
    """Test Optionals that partially match but are not subsets"""

    argument_signatures = [Sig('-foobar'), Sig('-foorab')]
    failures = ['-f', '-f a', '-fa', '-foa', '-foo', '-fo', '-foo b']
    successes = [
        ('', NS(foobar=None, foorab=None)),
        ('-foob a', NS(foobar='a', foorab=None)),
        ('-foor a', NS(foobar=None, foorab='a')),
        ('-fooba a', NS(foobar='a', foorab=None)),
        ('-foora a', NS(foobar=None, foorab='a')),
        ('-foobar a', NS(foobar='a', foorab=None)),
        ('-foorab a', NS(foobar=None, foorab='a')),
    ]


class TestOptionalsNumeric(ParserTestCase):
    """Test an Optional with a short opt string"""

    argument_signatures = [Sig('-1', dest='one')]
    failures = ['-1', 'a', '-1 --foo', '-1 -y', '-1 -1', '-1 -2']
    successes = [
        ('', NS(one=None)),
        ('-1 a', NS(one='a')),
        ('-1a', NS(one='a')),
        ('-1-2', NS(one='-2')),
    ]


class TestOptionalsDoubleDash(ParserTestCase):
    """Test an Optional with a double-dash option string"""

    argument_signatures = [Sig('--foo')]
    failures = ['--foo', '-f', '-f a', 'a', '--foo -x', '--foo --bar']
    successes = [
        ('', NS(foo=None)),
        ('--foo a', NS(foo='a')),
        ('--foo=a', NS(foo='a')),
        ('--foo -2.5', NS(foo='-2.5')),
        ('--foo=-2.5', NS(foo='-2.5')),
    ]


class TestOptionalsDoubleDashPartialMatch(ParserTestCase):
    """Tests partial matching with a double-dash option string"""

    argument_signatures = [
        Sig('--badger', action='store_true'),
        Sig('--bat'),
    ]
    failures = ['--bar', '--b', '--ba', '--b=2', '--ba=4', '--badge 5']
    successes = [
        ('', NS(badger=False, bat=None)),
        ('--bat X', NS(badger=False, bat='X')),
        ('--bad', NS(badger=True, bat=None)),
        ('--badg', NS(badger=True, bat=None)),
        ('--badge', NS(badger=True, bat=None)),
        ('--badger', NS(badger=True, bat=None)),
    ]


class TestOptionalsSingleDoubleDash(ParserTestCase):
    """Test an Optional with single- and double-dash option strings"""

    argument_signatures = [
        Sig('-f', action='store_true'),
        Sig('--bar'),
        Sig('-baz', action='store_const', const=42),
    ]
    failures = ['--bar', '-fbar', '-fbaz', '-bazf', '-b B', 'B']
    successes = [
        ('', NS(f=False, bar=None, baz=None)),
        ('-f', NS(f=True, bar=None, baz=None)),
        ('--ba B', NS(f=False, bar='B', baz=None)),
        ('-f --bar B', NS(f=True, bar='B', baz=None)),
        ('-f -b', NS(f=True, bar=None, baz=42)),
        ('-ba -f', NS(f=True, bar=None, baz=42)),
    ]


class TestOptionalsAlternatePrefixChars(ParserTestCase):
    """Test an Optional with a double-dash option string"""

    parser_signature = Sig(prefix_chars='+:/', add_help=False)
    argument_signatures = [
        Sig('+f', action='store_true'),
        Sig('::bar'),
        Sig('/baz', action='store_const', const=42),
    ]
    failures = ['--bar', '-fbar', '-b B', 'B', '-f', '--bar B', '-baz']
    successes = [
        ('', NS(f=False, bar=None, baz=None)),
        ('+f', NS(f=True, bar=None, baz=None)),
        ('::ba B', NS(f=False, bar='B', baz=None)),
        ('+f ::bar B', NS(f=True, bar='B', baz=None)),
        ('+f /b', NS(f=True, bar=None, baz=42)),
        ('/ba +f', NS(f=True, bar=None, baz=42)),
    ]


class TestOptionalsShortLong(ParserTestCase):
    """Test a combination of single- and double-dash option strings"""

    argument_signatures = [
        Sig('-v', '--verbose', '-n', '--noisy', action='store_true'),
    ]
    failures = ['--x --verbose', '-N', 'a', '-v x']
    successes = [
        ('', NS(verbose=False)),
        ('-v', NS(verbose=True)),
        ('--verbose', NS(verbose=True)),
        ('-n', NS(verbose=True)),
        ('--noisy', NS(verbose=True)),
    ]


class TestOptionalsDest(ParserTestCase):
    """Tests various means of setting destination"""

    argument_signatures = [Sig('--foo-bar'), Sig('--baz', dest='zabbaz')]
    failures = ['a']
    successes = [
        ('--foo-bar f', NS(foo_bar='f', zabbaz=None)),
        ('--baz g', NS(foo_bar=None, zabbaz='g')),
        ('--foo-bar h --baz i', NS(foo_bar='h', zabbaz='i')),
        ('--baz j --foo-bar k', NS(foo_bar='k', zabbaz='j')),
    ]


class TestOptionalsDefault(ParserTestCase):
    """Tests specifying a default for an Optional"""

    argument_signatures = [Sig('-x'), Sig('-y', default=42)]
    failures = ['a']
    successes = [
        ('', NS(x=None, y=42)),
        ('-xx', NS(x='x', y=42)),
        ('-yy', NS(x=None, y='y')),
    ]


class TestOptionalsNargsDefault(ParserTestCase):
    """Tests not specifying the number of args for an Optional"""

    argument_signatures = [Sig('-x')]
    failures = ['a', '-x']
    successes = [
        ('', NS(x=None)),
        ('-x a', NS(x='a')),
    ]


class TestOptionalsNargs1(ParserTestCase):
    """Tests specifying the 1 arg for an Optional"""

    argument_signatures = [Sig('-x', nargs=1)]
    failures = ['a', '-x']
    successes = [
        ('', NS(x=None)),
        ('-x a', NS(x=['a'])),
    ]


class TestOptionalsNargs3(ParserTestCase):
    """Tests specifying the 3 args for an Optional"""

    argument_signatures = [Sig('-x', nargs=3)]
    failures = ['a', '-x', '-x a', '-x a b', 'a -x', 'a -x b']
    successes = [
        ('', NS(x=None)),
        ('-x a b c', NS(x=['a', 'b', 'c'])),
    ]


class TestOptionalsNargsOptional(ParserTestCase):
    """Tests specifying an Optional arg for an Optional"""

    argument_signatures = [
        Sig('-w', nargs='?'),
        Sig('-x', nargs='?', const=42),
        Sig('-y', nargs='?', default='spam'),
        Sig('-z', nargs='?', type=int, const='42', default='84'),
    ]
    failures = ['2']
    successes = [
        ('', NS(w=None, x=None, y='spam', z=84)),
        ('-w', NS(w=None, x=None, y='spam', z=84)),
        ('-w 2', NS(w='2', x=None, y='spam', z=84)),
        ('-x', NS(w=None, x=42, y='spam', z=84)),
        ('-x 2', NS(w=None, x='2', y='spam', z=84)),
        ('-y', NS(w=None, x=None, y=None, z=84)),
        ('-y 2', NS(w=None, x=None, y='2', z=84)),
        ('-z', NS(w=None, x=None, y='spam', z=42)),
        ('-z 2', NS(w=None, x=None, y='spam', z=2)),
    ]


class TestOptionalsNargsZeroOrMore(ParserTestCase):
    """Tests specifying an args for an Optional that accepts zero or more"""

    argument_signatures = [
        Sig('-x', nargs='*'),
        Sig('-y', nargs='*', default='spam'),
    ]
    failures = ['a']
    successes = [
        ('', NS(x=None, y='spam')),
        ('-x', NS(x=[], y='spam')),
        ('-x a', NS(x=['a'], y='spam')),
        ('-x a b', NS(x=['a', 'b'], y='spam')),
        ('-y', NS(x=None, y=[])),
        ('-y a', NS(x=None, y=['a'])),
        ('-y a b', NS(x=None, y=['a', 'b'])),
    ]


class TestOptionalsNargsOneOrMore(ParserTestCase):
    """Tests specifying an args for an Optional that accepts one or more"""

    argument_signatures = [
        Sig('-x', nargs='+'),
        Sig('-y', nargs='+', default='spam'),
    ]
    failures = ['a', '-x', '-y', 'a -x', 'a -y b']
    successes = [
        ('', NS(x=None, y='spam')),
        ('-x a', NS(x=['a'], y='spam')),
        ('-x a b', NS(x=['a', 'b'], y='spam')),
        ('-y a', NS(x=None, y=['a'])),
        ('-y a b', NS(x=None, y=['a', 'b'])),
    ]


class TestOptionalsChoices(ParserTestCase):
    """Tests specifying the choices for an Optional"""

    argument_signatures = [
        Sig('-f', choices='abc'),
        Sig('-g', type=int, choices=range(5))]
    failures = ['a', '-f d', '-fad', '-ga', '-g 6']
    successes = [
        ('', NS(f=None, g=None)),
        ('-f a', NS(f='a', g=None)),
        ('-f c', NS(f='c', g=None)),
        ('-g 0', NS(f=None, g=0)),
        ('-g 03', NS(f=None, g=3)),
        ('-fb -g4', NS(f='b', g=4)),
    ]


class TestOptionalsRequired(ParserTestCase):
    """Tests the an optional action that is required"""

    argument_signatures = [
        Sig('-x', type=int, required=True),
    ]
    failures = ['a', '']
    successes = [
        ('-x 1', NS(x=1)),
        ('-x42', NS(x=42)),
    ]


class TestOptionalsActionStore(ParserTestCase):
    """Tests the store action for an Optional"""

    argument_signatures = [Sig('-x', action='store')]
    failures = ['a', 'a -x']
    successes = [
        ('', NS(x=None)),
        ('-xfoo', NS(x='foo')),
    ]


class TestOptionalsActionStoreConst(ParserTestCase):
    """Tests the store_const action for an Optional"""

    argument_signatures = [Sig('-y', action='store_const', const=object)]
    failures = ['a']
    successes = [
        ('', NS(y=None)),
        ('-y', NS(y=object)),
    ]


class TestOptionalsActionStoreFalse(ParserTestCase):
    """Tests the store_false action for an Optional"""

    argument_signatures = [Sig('-z', action='store_false')]
    failures = ['a', '-za', '-z a']
    successes = [
        ('', NS(z=True)),
        ('-z', NS(z=False)),
    ]


class TestOptionalsActionStoreTrue(ParserTestCase):
    """Tests the store_true action for an Optional"""

    argument_signatures = [Sig('--apple', action='store_true')]
    failures = ['a', '--apple=b', '--apple b']
    successes = [
        ('', NS(apple=False)),
        ('--apple', NS(apple=True)),
    ]


class TestOptionalsActionAppend(ParserTestCase):
    """Tests the append action for an Optional"""

    argument_signatures = [Sig('--baz', action='append')]
    failures = ['a', '--baz', 'a --baz', '--baz a b']
    successes = [
        ('', NS(baz=None)),
        ('--baz a', NS(baz=['a'])),
        ('--baz a --baz b', NS(baz=['a', 'b'])),
    ]


class TestOptionalsActionAppendWithDefault(ParserTestCase):
    """Tests the append action for an Optional"""

    argument_signatures = [Sig('--baz', action='append', default=['X'])]
    failures = ['a', '--baz', 'a --baz', '--baz a b']
    successes = [
        ('', NS(baz=['X'])),
        ('--baz a', NS(baz=['X', 'a'])),
        ('--baz a --baz b', NS(baz=['X', 'a', 'b'])),
    ]


class TestOptionalsActionAppendConst(ParserTestCase):
    """Tests the append_const action for an Optional"""

    argument_signatures = [
        Sig('-b', action='append_const', const=Exception),
        Sig('-c', action='append', dest='b'),
    ]
    failures = ['a', '-c', 'a -c', '-bx', '-b x']
    successes = [
        ('', NS(b=None)),
        ('-b', NS(b=[Exception])),
        ('-b -cx -b -cyz', NS(b=[Exception, 'x', Exception, 'yz'])),
    ]


class TestOptionalsActionAppendConstWithDefault(ParserTestCase):
    """Tests the append_const action for an Optional"""

    argument_signatures = [
        Sig('-b', action='append_const', const=Exception, default=['X']),
        Sig('-c', action='append', dest='b'),
    ]
    failures = ['a', '-c', 'a -c', '-bx', '-b x']
    successes = [
        ('', NS(b=['X'])),
        ('-b', NS(b=['X', Exception])),
        ('-b -cx -b -cyz', NS(b=['X', Exception, 'x', Exception, 'yz'])),
    ]


class TestOptionalsActionCount(ParserTestCase):
    """Tests the count action for an Optional"""

    argument_signatures = [Sig('-x', action='count')]
    failures = ['a', '-x a', '-x b', '-x a -x b']
    successes = [
        ('', NS(x=None)),
        ('-x', NS(x=1)),
    ]


# ================
# Positional tests
# ================

class TestPositionalsNargsNone(ParserTestCase):
    """Test a Positional that doesn't specify nargs"""

    argument_signatures = [Sig('foo')]
    failures = ['', '-x', 'a b']
    successes = [
        ('a', NS(foo='a')),
    ]


class TestPositionalsNargs1(ParserTestCase):
    """Test a Positional that specifies an nargs of 1"""

    argument_signatures = [Sig('foo', nargs=1)]
    failures = ['', '-x', 'a b']
    successes = [
        ('a', NS(foo=['a'])),
    ]


class TestPositionalsNargs2(ParserTestCase):
    """Test a Positional that specifies an nargs of 2"""

    argument_signatures = [Sig('foo', nargs=2)]
    failures = ['', 'a', '-x', 'a b c']
    successes = [
        ('a b', NS(foo=['a', 'b'])),
    ]


class TestPositionalsNargsZeroOrMore(ParserTestCase):
    """Test a Positional that specifies unlimited nargs"""

    argument_signatures = [Sig('foo', nargs='*')]
    failures = ['-x']
    successes = [
        ('', NS(foo=[])),
        ('a', NS(foo=['a'])),
        ('a b', NS(foo=['a', 'b'])),
    ]


class TestPositionalsNargsZeroOrMoreDefault(ParserTestCase):
    """Test a Positional that specifies unlimited nargs and a default"""

    argument_signatures = [Sig('foo', nargs='*', default='bar')]
    failures = ['-x']
    successes = [
        ('', NS(foo='bar')),
        ('a', NS(foo=['a'])),
        ('a b', NS(foo=['a', 'b'])),
    ]


class TestPositionalsNargsOneOrMore(ParserTestCase):
    """Test a Positional that specifies one or more nargs"""

    argument_signatures = [Sig('foo', nargs='+')]
    failures = ['', '-x']
    successes = [
        ('a', NS(foo=['a'])),
        ('a b', NS(foo=['a', 'b'])),
    ]


class TestPositionalsNargsOptional(ParserTestCase):
    """Tests an Optional Positional"""

    argument_signatures = [Sig('foo', nargs='?')]
    failures = ['-x', 'a b']
    successes = [
        ('', NS(foo=None)),
        ('a', NS(foo='a')),
    ]


class TestPositionalsNargsOptionalDefault(ParserTestCase):
    """Tests an Optional Positional with a default value"""

    argument_signatures = [Sig('foo', nargs='?', default=42)]
    failures = ['-x', 'a b']
    successes = [
        ('', NS(foo=42)),
        ('a', NS(foo='a')),
    ]


class TestPositionalsNargsOptionalConvertedDefault(ParserTestCase):
    """Tests an Optional Positional with a default value
    that needs to be converted to the appropriate type.
    """

    argument_signatures = [
        Sig('foo', nargs='?', type=int, default='42'),
    ]
    failures = ['-x', 'a b', '1 2']
    successes = [
        ('', NS(foo=42)),
        ('1', NS(foo=1)),
    ]


class TestPositionalsNargsNoneNone(ParserTestCase):
    """Test two Positionals that don't specify nargs"""

    argument_signatures = [Sig('foo'), Sig('bar')]
    failures = ['', '-x', 'a', 'a b c']
    successes = [
        ('a b', NS(foo='a', bar='b')),
    ]


class TestPositionalsNargsNone1(ParserTestCase):
    """Test a Positional with no nargs followed by one with 1"""

    argument_signatures = [Sig('foo'), Sig('bar', nargs=1)]
    failures = ['', '--foo', 'a', 'a b c']
    successes = [
        ('a b', NS(foo='a', bar=['b'])),
    ]


class TestPositionalsNargs2None(ParserTestCase):
    """Test a Positional with 2 nargs followed by one with none"""

    argument_signatures = [Sig('foo', nargs=2), Sig('bar')]
    failures = ['', '--foo', 'a', 'a b', 'a b c d']
    successes = [
        ('a b c', NS(foo=['a', 'b'], bar='c')),
    ]


class TestPositionalsNargsNoneZeroOrMore(ParserTestCase):
    """Test a Positional with no nargs followed by one with unlimited"""

    argument_signatures = [Sig('foo'), Sig('bar', nargs='*')]
    failures = ['', '--foo']
    successes = [
        ('a', NS(foo='a', bar=[])),
        ('a b', NS(foo='a', bar=['b'])),
        ('a b c', NS(foo='a', bar=['b', 'c'])),
    ]


class TestPositionalsNargsNoneOneOrMore(ParserTestCase):
    """Test a Positional with no nargs followed by one with one or more"""

    argument_signatures = [Sig('foo'), Sig('bar', nargs='+')]
    failures = ['', '--foo', 'a']
    successes = [
        ('a b', NS(foo='a', bar=['b'])),
        ('a b c', NS(foo='a', bar=['b', 'c'])),
    ]


class TestPositionalsNargsNoneOptional(ParserTestCase):
    """Test a Positional with no nargs followed by one with an Optional"""

    argument_signatures = [Sig('foo'), Sig('bar', nargs='?')]
    failures = ['', '--foo', 'a b c']
    successes = [
        ('a', NS(foo='a', bar=None)),
        ('a b', NS(foo='a', bar='b')),
    ]


class TestPositionalsNargsZeroOrMoreNone(ParserTestCase):
    """Test a Positional with unlimited nargs followed by one with none"""

    argument_signatures = [Sig('foo', nargs='*'), Sig('bar')]
    failures = ['', '--foo']
    successes = [
        ('a', NS(foo=[], bar='a')),
        ('a b', NS(foo=['a'], bar='b')),
        ('a b c', NS(foo=['a', 'b'], bar='c')),
    ]


class TestPositionalsNargsOneOrMoreNone(ParserTestCase):
    """Test a Positional with one or more nargs followed by one with none"""

    argument_signatures = [Sig('foo', nargs='+'), Sig('bar')]
    failures = ['', '--foo', 'a']
    successes = [
        ('a b', NS(foo=['a'], bar='b')),
        ('a b c', NS(foo=['a', 'b'], bar='c')),
    ]


class TestPositionalsNargsOptionalNone(ParserTestCase):
    """Test a Positional with an Optional nargs followed by one with none"""

    argument_signatures = [Sig('foo', nargs='?', default=42), Sig('bar')]
    failures = ['', '--foo', 'a b c']
    successes = [
        ('a', NS(foo=42, bar='a')),
        ('a b', NS(foo='a', bar='b')),
    ]


class TestPositionalsNargs2ZeroOrMore(ParserTestCase):
    """Test a Positional with 2 nargs followed by one with unlimited"""

    argument_signatures = [Sig('foo', nargs=2), Sig('bar', nargs='*')]
    failures = ['', '--foo', 'a']
    successes = [
        ('a b', NS(foo=['a', 'b'], bar=[])),
        ('a b c', NS(foo=['a', 'b'], bar=['c'])),
    ]


class TestPositionalsNargs2OneOrMore(ParserTestCase):
    """Test a Positional with 2 nargs followed by one with one or more"""

    argument_signatures = [Sig('foo', nargs=2), Sig('bar', nargs='+')]
    failures = ['', '--foo', 'a', 'a b']
    successes = [
        ('a b c', NS(foo=['a', 'b'], bar=['c'])),
    ]


class TestPositionalsNargs2Optional(ParserTestCase):
    """Test a Positional with 2 nargs followed by one optional"""

    argument_signatures = [Sig('foo', nargs=2), Sig('bar', nargs='?')]
    failures = ['', '--foo', 'a', 'a b c d']
    successes = [
        ('a b', NS(foo=['a', 'b'], bar=None)),
        ('a b c', NS(foo=['a', 'b'], bar='c')),
    ]


class TestPositionalsNargsZeroOrMore1(ParserTestCase):
    """Test a Positional with unlimited nargs followed by one with 1"""

    argument_signatures = [Sig('foo', nargs='*'), Sig('bar', nargs=1)]
    failures = ['', '--foo', ]
    successes = [
        ('a', NS(foo=[], bar=['a'])),
        ('a b', NS(foo=['a'], bar=['b'])),
        ('a b c', NS(foo=['a', 'b'], bar=['c'])),
    ]


class TestPositionalsNargsOneOrMore1(ParserTestCase):
    """Test a Positional with one or more nargs followed by one with 1"""

    argument_signatures = [Sig('foo', nargs='+'), Sig('bar', nargs=1)]
    failures = ['', '--foo', 'a']
    successes = [
        ('a b', NS(foo=['a'], bar=['b'])),
        ('a b c', NS(foo=['a', 'b'], bar=['c'])),
    ]


class TestPositionalsNargsOptional1(ParserTestCase):
    """Test a Positional with an Optional nargs followed by one with 1"""

    argument_signatures = [Sig('foo', nargs='?'), Sig('bar', nargs=1)]
    failures = ['', '--foo', 'a b c']
    successes = [
        ('a', NS(foo=None, bar=['a'])),
        ('a b', NS(foo='a', bar=['b'])),
    ]


class TestPositionalsNargsNoneZeroOrMore1(ParserTestCase):
    """Test three Positionals: no nargs, unlimited nargs and 1 nargs"""

    argument_signatures = [
        Sig('foo'),
        Sig('bar', nargs='*'),
        Sig('baz', nargs=1),
    ]
    failures = ['', '--foo', 'a']
    successes = [
        ('a b', NS(foo='a', bar=[], baz=['b'])),
        ('a b c', NS(foo='a', bar=['b'], baz=['c'])),
    ]


class TestPositionalsNargsNoneOneOrMore1(ParserTestCase):
    """Test three Positionals: no nargs, one or more nargs and 1 nargs"""

    argument_signatures = [
        Sig('foo'),
        Sig('bar', nargs='+'),
        Sig('baz', nargs=1),
    ]
    failures = ['', '--foo', 'a', 'b']
    successes = [
        ('a b c', NS(foo='a', bar=['b'], baz=['c'])),
        ('a b c d', NS(foo='a', bar=['b', 'c'], baz=['d'])),
    ]


class TestPositionalsNargsNoneOptional1(ParserTestCase):
    """Test three Positionals: no nargs, optional narg and 1 nargs"""

    argument_signatures = [
        Sig('foo'),
        Sig('bar', nargs='?', default=0.625),
        Sig('baz', nargs=1),
    ]
    failures = ['', '--foo', 'a']
    successes = [
        ('a b', NS(foo='a', bar=0.625, baz=['b'])),
        ('a b c', NS(foo='a', bar='b', baz=['c'])),
    ]


class TestPositionalsNargsOptionalOptional(ParserTestCase):
    """Test two optional nargs"""

    argument_signatures = [
        Sig('foo', nargs='?'),
        Sig('bar', nargs='?', default=42),
    ]
    failures = ['--foo', 'a b c']
    successes = [
        ('', NS(foo=None, bar=42)),
        ('a', NS(foo='a', bar=42)),
        ('a b', NS(foo='a', bar='b')),
    ]


class TestPositionalsNargsOptionalZeroOrMore(ParserTestCase):
    """Test an Optional narg followed by unlimited nargs"""

    argument_signatures = [Sig('foo', nargs='?'), Sig('bar', nargs='*')]
    failures = ['--foo']
    successes = [
        ('', NS(foo=None, bar=[])),
        ('a', NS(foo='a', bar=[])),
        ('a b', NS(foo='a', bar=['b'])),
        ('a b c', NS(foo='a', bar=['b', 'c'])),
    ]


class TestPositionalsNargsOptionalOneOrMore(ParserTestCase):
    """Test an Optional narg followed by one or more nargs"""

    argument_signatures = [Sig('foo', nargs='?'), Sig('bar', nargs='+')]
    failures = ['', '--foo']
    successes = [
        ('a', NS(foo=None, bar=['a'])),
        ('a b', NS(foo='a', bar=['b'])),
        ('a b c', NS(foo='a', bar=['b', 'c'])),
    ]


class TestPositionalsChoicesString(ParserTestCase):
    """Test a set of single-character choices"""

    argument_signatures = [Sig('spam', choices=set('abcdefg'))]
    failures = ['', '--foo', 'h', '42', 'ef']
    successes = [
        ('a', NS(spam='a')),
        ('g', NS(spam='g')),
    ]


class TestPositionalsChoicesInt(ParserTestCase):
    """Test a set of integer choices"""

    argument_signatures = [Sig('spam', type=int, choices=range(20))]
    failures = ['', '--foo', 'h', '42', 'ef']
    successes = [
        ('4', NS(spam=4)),
        ('15', NS(spam=15)),
    ]


class TestPositionalsActionAppend(ParserTestCase):
    """Test the 'append' action"""

    argument_signatures = [
        Sig('spam', action='append'),
        Sig('spam', action='append', nargs=2),
    ]
    failures = ['', '--foo', 'a', 'a b', 'a b c d']
    successes = [
        ('a b c', NS(spam=['a', ['b', 'c']])),
    ]

# ========================================
# Combined optionals and positionals tests
# ========================================

class TestOptionalsNumericAndPositionals(ParserTestCase):
    """Tests negative number args when numeric options are present"""

    argument_signatures = [
        Sig('x', nargs='?'),
        Sig('-4', dest='y', action='store_true'),
    ]
    failures = ['-2', '-315']
    successes = [
        ('', NS(x=None, y=False)),
        ('a', NS(x='a', y=False)),
        ('-4', NS(x=None, y=True)),
        ('-4 a', NS(x='a', y=True)),
    ]


class TestOptionalsAlmostNumericAndPositionals(ParserTestCase):
    """Tests negative number args when almost numeric options are present"""

    argument_signatures = [
        Sig('x', nargs='?'),
        Sig('-k4', dest='y', action='store_true'),
    ]
    failures = ['-k3']
    successes = [
        ('', NS(x=None, y=False)),
        ('-2', NS(x='-2', y=False)),
        ('a', NS(x='a', y=False)),
        ('-k4', NS(x=None, y=True)),
        ('-k4 a', NS(x='a', y=True)),
    ]


class TestEmptyAndSpaceContainingArguments(ParserTestCase):

    argument_signatures = [
        Sig('x', nargs='?'),
        Sig('-y', '--yyy', dest='y'),
    ]
    failures = ['-y']
    successes = [
        ([''], NS(x='', y=None)),
        (['a badger'], NS(x='a badger', y=None)),
        (['-a badger'], NS(x='-a badger', y=None)),
        (['-y', ''], NS(x=None, y='')),
        (['-y', 'a badger'], NS(x=None, y='a badger')),
        (['-y', '-a badger'], NS(x=None, y='-a badger')),
        (['--yyy=a badger'], NS(x=None, y='a badger')),
        (['--yyy=-a badger'], NS(x=None, y='-a badger')),
    ]


class TestNargsZeroOrMore(ParserTestCase):
    """Tests specifying an args for an Optional that accepts zero or more"""

    argument_signatures = [Sig('-x', nargs='*'), Sig('y', nargs='*')]
    failures = []
    successes = [
        ('', NS(x=None, y=[])),
        ('-x', NS(x=[], y=[])),
        ('-x a', NS(x=['a'], y=[])),
        ('-x a -- b', NS(x=['a'], y=['b'])),
        ('a', NS(x=None, y=['a'])),
        ('a -x', NS(x=[], y=['a'])),
        ('a -x b', NS(x=['b'], y=['a'])),
    ]


class TestOptionLike(ParserTestCase):
    """Tests options that may or may not be arguments"""

    argument_signatures = [
        Sig('-x', type=float),
        Sig('-3', type=float, dest='y'),
        Sig('z', nargs='*'),
    ]
    failures = ['-x', '-y2.5', '-xa', '-x -a',
                '-x -3', '-x -3.5', '-3 -3.5',
                '-x -2.5', '-x -2.5 a', '-3 -.5',
                'a x -1', '-x -1 a', '-3 -1 a']
    successes = [
        ('', NS(x=None, y=None, z=[])),
        ('-x 2.5', NS(x=2.5, y=None, z=[])),
        ('-x 2.5 a', NS(x=2.5, y=None, z=['a'])),
        ('-3.5', NS(x=None, y=0.5, z=[])),
        ('-3-.5', NS(x=None, y=-0.5, z=[])),
        ('-3 .5', NS(x=None, y=0.5, z=[])),
        ('a -3.5', NS(x=None, y=0.5, z=['a'])),
        ('a', NS(x=None, y=None, z=['a'])),
        ('a -x 1', NS(x=1.0, y=None, z=['a'])),
        ('-x 1 a', NS(x=1.0, y=None, z=['a'])),
        ('-3 1 a', NS(x=None, y=1.0, z=['a'])),
    ]


class TestDefaultSuppress(ParserTestCase):
    """Test actions with suppressed defaults"""

    argument_signatures = [
        Sig('foo', nargs='?', default=argparse.SUPPRESS),
        Sig('bar', nargs='*', default=argparse.SUPPRESS),
        Sig('--baz', action='store_true', default=argparse.SUPPRESS),
    ]
    failures = ['-x']
    successes = [
        ('', NS()),
        ('a', NS(foo='a')),
        ('a b', NS(foo='a', bar=['b'])),
        ('--baz', NS(baz=True)),
        ('a --baz', NS(foo='a', baz=True)),
        ('--baz a b', NS(foo='a', bar=['b'], baz=True)),
    ]


class TestParserDefaultSuppress(ParserTestCase):
    """Test actions with a parser-level default of SUPPRESS"""

    parser_signature = Sig(argument_default=argparse.SUPPRESS)
    argument_signatures = [
        Sig('foo', nargs='?'),
        Sig('bar', nargs='*'),
        Sig('--baz', action='store_true'),
    ]
    failures = ['-x']
    successes = [
        ('', NS()),
        ('a', NS(foo='a')),
        ('a b', NS(foo='a', bar=['b'])),
        ('--baz', NS(baz=True)),
        ('a --baz', NS(foo='a', baz=True)),
        ('--baz a b', NS(foo='a', bar=['b'], baz=True)),
    ]


class TestParserDefault42(ParserTestCase):
    """Test actions with a parser-level default of 42"""

    parser_signature = Sig(argument_default=42, version='1.0')
    argument_signatures = [
        Sig('foo', nargs='?'),
        Sig('bar', nargs='*'),
        Sig('--baz', action='store_true'),
    ]
    failures = ['-x']
    successes = [
        ('', NS(foo=42, bar=42, baz=42)),
        ('a', NS(foo='a', bar=42, baz=42)),
        ('a b', NS(foo='a', bar=['b'], baz=42)),
        ('--baz', NS(foo=42, bar=42, baz=True)),
        ('a --baz', NS(foo='a', bar=42, baz=True)),
        ('--baz a b', NS(foo='a', bar=['b'], baz=True)),
    ]


class TestArgumentsFromFile(TempDirMixin, ParserTestCase):
    """Test reading arguments from a file"""

    def setUp(self):
        super(TestArgumentsFromFile, self).setUp()
        file_texts = [
            ('hello', 'hello world!\n'),
            ('recursive', '-a\n'
                          'A\n'
                          '@hello'),
            ('invalid', '@no-such-path\n'),
        ]
        for path, text in file_texts:
            file = open(path, 'w')
            file.write(text)
            file.close()

    parser_signature = Sig(fromfile_prefix_chars='@')
    argument_signatures = [
        Sig('-a'),
        Sig('x'),
        Sig('y', nargs='+'),
    ]
    failures = ['', '-b', 'X', '@invalid', '@missing']
    successes = [
        ('X Y', NS(a=None, x='X', y=['Y'])),
        ('X -a A Y Z', NS(a='A', x='X', y=['Y', 'Z'])),
        ('@hello X', NS(a=None, x='hello world!', y=['X'])),
        ('X @hello', NS(a=None, x='X', y=['hello world!'])),
        ('-a B @recursive Y Z', NS(a='A', x='hello world!', y=['Y', 'Z'])),
        ('X @recursive Z -a B', NS(a='B', x='X', y=['hello world!', 'Z'])),
    ]


# =====================
# Type conversion tests
# =====================

class TestFileTypeRepr(TestCase):

    def test_r(self):
        type = argparse.FileType('r')
        self.assertEqual("FileType('r')", repr(type))

    def test_wb_1(self):
        type = argparse.FileType('wb', 1)
        self.assertEqual("FileType('wb', 1)", repr(type))


class RFile(object):
    seen = {}

    def __init__(self, name):
        self.name = name

    def __eq__(self, other):
        if other in self.seen:
            text = self.seen[other]
        else:
            text = self.seen[other] = other.read()
            other.close()
        if not isinstance(text, str):
            text = text.decode('ascii')
        return self.name == other.name == text


class TestFileTypeR(TempDirMixin, ParserTestCase):
    """Test the FileType option/argument type for reading files"""

    def setUp(self):
        super(TestFileTypeR, self).setUp()
        for file_name in ['foo', 'bar']:
            file = open(os.path.join(self.temp_dir, file_name), 'w')
            file.write(file_name)
            file.close()

    argument_signatures = [
        Sig('-x', type=argparse.FileType()),
        Sig('spam', type=argparse.FileType('r')),
    ]
    failures = ['-x', '-x bar']
    successes = [
        ('foo', NS(x=None, spam=RFile('foo'))),
        ('-x foo bar', NS(x=RFile('foo'), spam=RFile('bar'))),
        ('bar -x foo', NS(x=RFile('foo'), spam=RFile('bar'))),
        ('-x - -', NS(x=sys.stdin, spam=sys.stdin)),
    ]


class TestFileTypeRB(TempDirMixin, ParserTestCase):
    """Test the FileType option/argument type for reading files"""

    def setUp(self):
        super(TestFileTypeRB, self).setUp()
        for file_name in ['foo', 'bar']:
            file = open(os.path.join(self.temp_dir, file_name), 'w')
            file.write(file_name)
            file.close()

    argument_signatures = [
        Sig('-x', type=argparse.FileType('rb')),
        Sig('spam', type=argparse.FileType('rb')),
    ]
    failures = ['-x', '-x bar']
    successes = [
        ('foo', NS(x=None, spam=RFile('foo'))),
        ('-x foo bar', NS(x=RFile('foo'), spam=RFile('bar'))),
        ('bar -x foo', NS(x=RFile('foo'), spam=RFile('bar'))),
        ('-x - -', NS(x=sys.stdin, spam=sys.stdin)),
    ]


class WFile(object):
    seen = set()

    def __init__(self, name):
        self.name = name

    def __eq__(self, other):
        if other not in self.seen:
            text = 'Check that file is writable.'
            if 'b' in other.mode:
                text = text.encode('ascii')
            other.write(text)
            other.close()
            self.seen.add(other)
        return self.name == other.name


class TestFileTypeW(TempDirMixin, ParserTestCase):
    """Test the FileType option/argument type for writing files"""

    argument_signatures = [
        Sig('-x', type=argparse.FileType('w')),
        Sig('spam', type=argparse.FileType('w')),
    ]
    failures = ['-x', '-x bar']
    successes = [
        ('foo', NS(x=None, spam=WFile('foo'))),
        ('-x foo bar', NS(x=WFile('foo'), spam=WFile('bar'))),
        ('bar -x foo', NS(x=WFile('foo'), spam=WFile('bar'))),
        ('-x - -', NS(x=sys.stdout, spam=sys.stdout)),
    ]


class TestFileTypeWB(TempDirMixin, ParserTestCase):

    argument_signatures = [
        Sig('-x', type=argparse.FileType('wb')),
        Sig('spam', type=argparse.FileType('wb')),
    ]
    failures = ['-x', '-x bar']
    successes = [
        ('foo', NS(x=None, spam=WFile('foo'))),
        ('-x foo bar', NS(x=WFile('foo'), spam=WFile('bar'))),
        ('bar -x foo', NS(x=WFile('foo'), spam=WFile('bar'))),
        ('-x - -', NS(x=sys.stdout, spam=sys.stdout)),
    ]


class TestTypeCallable(ParserTestCase):
    """Test some callables as option/argument types"""

    argument_signatures = [
        Sig('--eggs', type=complex),
        Sig('spam', type=float),
    ]
    failures = ['a', '42j', '--eggs a', '--eggs 2i']
    successes = [
        ('--eggs=42 42', NS(eggs=42, spam=42.0)),
        ('--eggs 2j -- -1.5', NS(eggs=2j, spam=-1.5)),
        ('1024.675', NS(eggs=None, spam=1024.675)),
    ]


class TestTypeUserDefined(ParserTestCase):
    """Test a user-defined option/argument type"""

    class MyType(TestCase):

        def __init__(self, value):
            self.value = value

        def __eq__(self, other):
            return (type(self), self.value) == (type(other), other.value)

    argument_signatures = [
        Sig('-x', type=MyType),
        Sig('spam', type=MyType),
    ]
    failures = []
    successes = [
        ('a -x b', NS(x=MyType('b'), spam=MyType('a'))),
        ('-xf g', NS(x=MyType('f'), spam=MyType('g'))),
    ]


class TestTypeClassicClass(ParserTestCase):
    """Test a classic class type"""

    class C:

        def __init__(self, value):
            self.value = value

        def __eq__(self, other):
            return (type(self), self.value) == (type(other), other.value)

    argument_signatures = [
        Sig('-x', type=C),
        Sig('spam', type=C),
    ]
    failures = []
    successes = [
        ('a -x b', NS(x=C('b'), spam=C('a'))),
        ('-xf g', NS(x=C('f'), spam=C('g'))),
    ]


class TestTypeRegistration(TestCase):
    """Test a user-defined type by registering it"""

    def test(self):

        def get_my_type(string):
            return 'my_type{%s}' % string

        parser = argparse.ArgumentParser()
        parser.register('type', 'my_type', get_my_type)
        parser.add_argument('-x', type='my_type')
        parser.add_argument('y', type='my_type')

        self.assertEqual(parser.parse_args('1'.split()),
                         NS(x=None, y='my_type{1}'))
        self.assertEqual(parser.parse_args('-x 1 42'.split()),
                         NS(x='my_type{1}', y='my_type{42}'))


# ============
# Action tests
# ============

class TestActionUserDefined(ParserTestCase):
    """Test a user-defined option/argument action"""

    class OptionalAction(argparse.Action):

        def __call__(self, parser, namespace, value, option_string=None):
            try:
                # check destination and option string
                assert self.dest == 'spam', 'dest: %s' % self.dest
                assert option_string == '-s', 'flag: %s' % option_string
                # when option is before argument, badger=2, and when
                # option is after argument, badger=<whatever was set>
                expected_ns = NS(spam=0.25)
                if value in [0.125, 0.625]:
                    expected_ns.badger = 2
                elif value in [2.0]:
                    expected_ns.badger = 84
                else:
                    raise AssertionError('value: %s' % value)
                assert expected_ns == namespace, ('expected %s, got %s' %
                                                  (expected_ns, namespace))
            except AssertionError:
                e = sys.exc_info()[1]
                raise ArgumentParserError('opt_action failed: %s' % e)
            setattr(namespace, 'spam', value)

    class PositionalAction(argparse.Action):

        def __call__(self, parser, namespace, value, option_string=None):
            try:
                assert option_string is None, ('option_string: %s' %
                                               option_string)
                # check destination
                assert self.dest == 'badger', 'dest: %s' % self.dest
                # when argument is before option, spam=0.25, and when
                # option is after argument, spam=<whatever was set>
                expected_ns = NS(badger=2)
                if value in [42, 84]:
                    expected_ns.spam = 0.25
                elif value in [1]:
                    expected_ns.spam = 0.625
                elif value in [2]:
                    expected_ns.spam = 0.125
                else:
                    raise AssertionError('value: %s' % value)
                assert expected_ns == namespace, ('expected %s, got %s' %
                                                  (expected_ns, namespace))
            except AssertionError:
                e = sys.exc_info()[1]
                raise ArgumentParserError('arg_action failed: %s' % e)
            setattr(namespace, 'badger', value)

    argument_signatures = [
        Sig('-s', dest='spam', action=OptionalAction,
            type=float, default=0.25),
        Sig('badger', action=PositionalAction,
            type=int, nargs='?', default=2),
    ]
    failures = []
    successes = [
        ('-s0.125', NS(spam=0.125, badger=2)),
        ('42', NS(spam=0.25, badger=42)),
        ('-s 0.625 1', NS(spam=0.625, badger=1)),
        ('84 -s2', NS(spam=2.0, badger=84)),
    ]


class TestActionRegistration(TestCase):
    """Test a user-defined action supplied by registering it"""

    class MyAction(argparse.Action):

        def __call__(self, parser, namespace, values, option_string=None):
            setattr(namespace, self.dest, 'foo[%s]' % values)

    def test(self):

        parser = argparse.ArgumentParser()
        parser.register('action', 'my_action', self.MyAction)
        parser.add_argument('badger', action='my_action')

        self.assertEqual(parser.parse_args(['1']), NS(badger='foo[1]'))
        self.assertEqual(parser.parse_args(['42']), NS(badger='foo[42]'))


# ================
# Subparsers tests
# ================

class TestAddSubparsers(TestCase):
    """Test the add_subparsers method"""

    def assertArgumentParserError(self, *args, **kwargs):
        self.assertRaises(ArgumentParserError, *args, **kwargs)

    def _get_parser(self, subparser_help=False):
        # create a parser with a subparsers argument
        parser = ErrorRaisingArgumentParser(
            prog='PROG', description='main description')
        parser.add_argument(
            '--foo', action='store_true', help='foo help')
        parser.add_argument(
            'bar', type=float, help='bar help')

        # check that only one subparsers argument can be added
        subparsers = parser.add_subparsers(help='command help')
        self.assertArgumentParserError(parser.add_subparsers)

        # add first sub-parser
        parser1_kwargs = dict(description='1 description')
        if subparser_help:
            parser1_kwargs['help'] = '1 help'
        parser1 = subparsers.add_parser('1', **parser1_kwargs)
        parser1.add_argument('-w', type=int, help='w help')
        parser1.add_argument('x', choices='abc', help='x help')

        # add second sub-parser
        parser2_kwargs = dict(description='2 description')
        if subparser_help:
            parser2_kwargs['help'] = '2 help'
        parser2 = subparsers.add_parser('2', **parser2_kwargs)
        parser2.add_argument('-y', choices='123', help='y help')
        parser2.add_argument('z', type=complex, nargs='*', help='z help')

        # return the main parser
        return parser

    def setUp(self):
        self.parser = self._get_parser()
        self.command_help_parser = self._get_parser(subparser_help=True)

    def test_parse_args_failures(self):
        # check some failure cases:
        for args_str in ['', 'a', 'a a', '0.5 a', '0.5 1',
                         '0.5 1 -y', '0.5 2 -w']:
            args = args_str.split()
            self.assertArgumentParserError(self.parser.parse_args, args)

    def test_parse_args(self):
        # check some non-failure cases:
        self.assertEqual(
            self.parser.parse_args('0.5 1 b -w 7'.split()),
            NS(foo=False, bar=0.5, w=7, x='b'),
        )
        self.assertEqual(
            self.parser.parse_args('0.25 --foo 2 -y 2 3j -- -1j'.split()),
            NS(foo=True, bar=0.25, y='2', z=[3j, -1j]),
        )
        self.assertEqual(
            self.parser.parse_args('--foo 0.125 1 c'.split()),
            NS(foo=True, bar=0.125, w=None, x='c'),
        )

    def test_dest(self):
        parser = ErrorRaisingArgumentParser()
        parser.add_argument('--foo', action='store_true')
        subparsers = parser.add_subparsers(dest='bar')
        parser1 = subparsers.add_parser('1')
        parser1.add_argument('baz')
        self.assertEqual(NS(foo=False, bar='1', baz='2'),
                         parser.parse_args('1 2'.split()))

    def test_help(self):
        self.assertEqual(self.parser.format_usage(),
                         'usage: PROG [-h] [--foo] bar {1,2} ...\n')
        self.assertEqual(self.parser.format_help(), textwrap.dedent('''\
            usage: PROG [-h] [--foo] bar {1,2} ...

            main description

            positional arguments:
              bar         bar help
              {1,2}       command help

            optional arguments:
              -h, --help  show this help message and exit
              --foo       foo help
            '''))

    def test_parser_command_help(self):
        self.assertEqual(self.command_help_parser.format_usage(),
                         'usage: PROG [-h] [--foo] bar {1,2} ...\n')
        self.assertEqual(self.command_help_parser.format_help(),
                         textwrap.dedent('''\
            usage: PROG [-h] [--foo] bar {1,2} ...

            main description

            positional arguments:
              bar         bar help
              {1,2}       command help
                1         1 help
                2         2 help

            optional arguments:
              -h, --help  show this help message and exit
              --foo       foo help
            '''))

    def test_subparser_title_help(self):
        parser = ErrorRaisingArgumentParser(prog='PROG',
                                            description='main description')
        parser.add_argument('--foo', action='store_true', help='foo help')
        parser.add_argument('bar', help='bar help')
        subparsers = parser.add_subparsers(title='subcommands',
                                           description='command help',
                                           help='additional text')
        parser1 = subparsers.add_parser('1')
        parser2 = subparsers.add_parser('2')
        self.assertEqual(parser.format_usage(),
                         'usage: PROG [-h] [--foo] bar {1,2} ...\n')
        self.assertEqual(parser.format_help(), textwrap.dedent('''\
            usage: PROG [-h] [--foo] bar {1,2} ...

            main description

            positional arguments:
              bar         bar help

            optional arguments:
              -h, --help  show this help message and exit
              --foo       foo help

            subcommands:
              command help

              {1,2}       additional text
            '''))

    def _test_subparser_help(self, args_str, expected_help):
        try:
            self.parser.parse_args(args_str.split())
        except ArgumentParserError:
            err = sys.exc_info()[1]
            if err.message != expected_help:
                print(repr(expected_help))
                print(repr(err.message))
            self.assertEqual(err.message, expected_help)

    def test_subparser1_help(self):
        self._test_subparser_help('5.0 1 -h', textwrap.dedent('''\
            usage: PROG bar 1 [-h] [-w W] {a,b,c}

            1 description

            positional arguments:
              {a,b,c}     x help

            optional arguments:
              -h, --help  show this help message and exit
              -w W        w help
            '''))

    def test_subparser2_help(self):
        self._test_subparser_help('5.0 2 -h', textwrap.dedent('''\
            usage: PROG bar 2 [-h] [-y {1,2,3}] [z [z ...]]

            2 description

            positional arguments:
              z           z help

            optional arguments:
              -h, --help  show this help message and exit
              -y {1,2,3}  y help
            '''))

# ============
# Groups tests
# ============

class TestPositionalsGroups(TestCase):
    """Tests that order of group positionals matches construction order"""

    def test_nongroup_first(self):
        parser = ErrorRaisingArgumentParser()
        parser.add_argument('foo')
        group = parser.add_argument_group('g')
        group.add_argument('bar')
        parser.add_argument('baz')
        expected = NS(foo='1', bar='2', baz='3')
        result = parser.parse_args('1 2 3'.split())
        self.failUnlessEqual(expected, result)

    def test_group_first(self):
        parser = ErrorRaisingArgumentParser()
        group = parser.add_argument_group('xxx')
        group.add_argument('foo')
        parser.add_argument('bar')
        parser.add_argument('baz')
        expected = NS(foo='1', bar='2', baz='3')
        result = parser.parse_args('1 2 3'.split())
        self.failUnlessEqual(expected, result)

    def test_interleaved_groups(self):
        parser = ErrorRaisingArgumentParser()
        group = parser.add_argument_group('xxx')
        parser.add_argument('foo')
        group.add_argument('bar')
        parser.add_argument('baz')
        group = parser.add_argument_group('yyy')
        group.add_argument('frell')
        expected = NS(foo='1', bar='2', baz='3', frell='4')
        result = parser.parse_args('1 2 3 4'.split())
        self.failUnlessEqual(expected, result)

# ===================
# Parent parser tests
# ===================

class TestParentParsers(TestCase):
    """Tests that parsers can be created with parent parsers"""

    def assertArgumentParserError(self, *args, **kwargs):
        self.assertRaises(ArgumentParserError, *args, **kwargs)

    def setUp(self):
        self.wxyz_parent = ErrorRaisingArgumentParser(add_help=False)
        self.wxyz_parent.add_argument('--w')
        x_group = self.wxyz_parent.add_argument_group('x')
        x_group.add_argument('-y')
        self.wxyz_parent.add_argument('z')

        self.abcd_parent = ErrorRaisingArgumentParser(add_help=False)
        self.abcd_parent.add_argument('a')
        self.abcd_parent.add_argument('-b')
        c_group = self.abcd_parent.add_argument_group('c')
        c_group.add_argument('--d')

        self.w_parent = ErrorRaisingArgumentParser(add_help=False)
        self.w_parent.add_argument('--w')

        self.z_parent = ErrorRaisingArgumentParser(add_help=False)
        self.z_parent.add_argument('z')

        # parents with mutually exclusive groups
        self.ab_mutex_parent = ErrorRaisingArgumentParser(add_help=False)
        group = self.ab_mutex_parent.add_mutually_exclusive_group()
        group.add_argument('-a', action='store_true')
        group.add_argument('-b', action='store_true')

    def test_single_parent(self):
        parser = ErrorRaisingArgumentParser(parents=[self.wxyz_parent])
        self.assertEqual(parser.parse_args('-y 1 2 --w 3'.split()),
                         NS(w='3', y='1', z='2'))

    def test_single_parent_mutex(self):
        self._test_mutex_ab(self.ab_mutex_parent.parse_args)
        parser = ErrorRaisingArgumentParser(parents=[self.ab_mutex_parent])
        self._test_mutex_ab(parser.parse_args)

    def test_single_granparent_mutex(self):
        parents = [self.ab_mutex_parent]
        parser = ErrorRaisingArgumentParser(add_help=False, parents=parents)
        parser = ErrorRaisingArgumentParser(parents=[parser])
        self._test_mutex_ab(parser.parse_args)

    def _test_mutex_ab(self, parse_args):
        self.assertEqual(parse_args([]), NS(a=False, b=False))
        self.assertEqual(parse_args(['-a']), NS(a=True, b=False))
        self.assertEqual(parse_args(['-b']), NS(a=False, b=True))
        self.assertArgumentParserError(parse_args, ['-a', '-b'])
        self.assertArgumentParserError(parse_args, ['-b', '-a'])
        self.assertArgumentParserError(parse_args, ['-c'])
        self.assertArgumentParserError(parse_args, ['-a', '-c'])
        self.assertArgumentParserError(parse_args, ['-b', '-c'])

    def test_multiple_parents(self):
        parents = [self.abcd_parent, self.wxyz_parent]
        parser = ErrorRaisingArgumentParser(parents=parents)
        self.assertEqual(parser.parse_args('--d 1 --w 2 3 4'.split()),
                         NS(a='3', b=None, d='1', w='2', y=None, z='4'))

    def test_multiple_parents_mutex(self):
        parents = [self.ab_mutex_parent, self.wxyz_parent]
        parser = ErrorRaisingArgumentParser(parents=parents)
        self.assertEqual(parser.parse_args('-a --w 2 3'.split()),
                         NS(a=True, b=False, w='2', y=None, z='3'))
        self.assertArgumentParserError(
            parser.parse_args, '-a --w 2 3 -b'.split())
        self.assertArgumentParserError(
            parser.parse_args, '-a -b --w 2 3'.split())

    def test_conflicting_parents(self):
        self.assertRaises(
            argparse.ArgumentError,
            argparse.ArgumentParser,
            parents=[self.w_parent, self.wxyz_parent])

    def test_conflicting_parents_mutex(self):
        self.assertRaises(
            argparse.ArgumentError,
            argparse.ArgumentParser,
            parents=[self.abcd_parent, self.ab_mutex_parent])

    def test_same_argument_name_parents(self):
        parents = [self.wxyz_parent, self.z_parent]
        parser = ErrorRaisingArgumentParser(parents=parents)
        self.assertEqual(parser.parse_args('1 2'.split()),
                         NS(w=None, y=None, z='2'))

    def test_subparser_parents(self):
        parser = ErrorRaisingArgumentParser()
        subparsers = parser.add_subparsers()
        abcde_parser = subparsers.add_parser('bar', parents=[self.abcd_parent])
        abcde_parser.add_argument('e')
        self.assertEqual(parser.parse_args('bar -b 1 --d 2 3 4'.split()),
                         NS(a='3', b='1', d='2', e='4'))

    def test_subparser_parents_mutex(self):
        parser = ErrorRaisingArgumentParser()
        subparsers = parser.add_subparsers()
        parents = [self.ab_mutex_parent]
        abc_parser = subparsers.add_parser('foo', parents=parents)
        c_group = abc_parser.add_argument_group('c_group')
        c_group.add_argument('c')
        parents = [self.wxyz_parent, self.ab_mutex_parent]
        wxyzabe_parser = subparsers.add_parser('bar', parents=parents)
        wxyzabe_parser.add_argument('e')
        self.assertEqual(parser.parse_args('foo -a 4'.split()),
                         NS(a=True, b=False, c='4'))
        self.assertEqual(parser.parse_args('bar -b  --w 2 3 4'.split()),
                         NS(a=False, b=True, w='2', y=None, z='3', e='4'))
        self.assertArgumentParserError(
            parser.parse_args, 'foo -a -b 4'.split())
        self.assertArgumentParserError(
            parser.parse_args, 'bar -b -a 4'.split())

    def test_parent_help(self):
        parents = [self.abcd_parent, self.wxyz_parent]
        parser = ErrorRaisingArgumentParser(parents=parents)
        parser_help = parser.format_help()
        self.assertEqual(parser_help, textwrap.dedent('''\
            usage: %s [-h] [-b B] [--d D] [--w W] [-y Y] a z

            positional arguments:
              a
              z

            optional arguments:
              -h, --help  show this help message and exit
              -b B
              --w W

            c:
              --d D

            x:
              -y Y
        ''' % sys.argv[0]))

    def test_groups_parents(self):
        parent = ErrorRaisingArgumentParser(add_help=False)
        g = parent.add_argument_group(title='g', description='gd')
        g.add_argument('-w')
        g.add_argument('-x')
        m = parent.add_mutually_exclusive_group()
        m.add_argument('-y')
        m.add_argument('-z')
        parser = ErrorRaisingArgumentParser(parents=[parent])

        self.assertRaises(ArgumentParserError, parser.parse_args,
            ['-y', 'Y', '-z', 'Z'])

        parser_help = parser.format_help()
        self.assertEqual(parser_help, textwrap.dedent('''\
            usage: %s [-h] [-w W] [-x X] [-y Y | -z Z]

            optional arguments:
              -h, --help  show this help message and exit
              -y Y
              -z Z

            g:
              gd

              -w W
              -x X
        ''' % sys.argv[0]))

# ==============================
# Mutually exclusive group tests
# ==============================

class TestMutuallyExclusiveGroupErrors(TestCase):

    def test_invalid_add_argument_group(self):
        parser = ErrorRaisingArgumentParser()
        raises = self.assertRaises
        raises(TypeError, parser.add_mutually_exclusive_group, title='foo')

    def test_invalid_add_argument(self):
        parser = ErrorRaisingArgumentParser()
        group = parser.add_mutually_exclusive_group()
        add_argument = group.add_argument
        raises = self.assertRaises
        raises(ValueError, add_argument, '--foo', required=True)
        raises(ValueError, add_argument, 'bar')
        raises(ValueError, add_argument, 'bar', nargs='+')
        raises(ValueError, add_argument, 'bar', nargs=1)
        raises(ValueError, add_argument, 'bar', nargs=argparse.PARSER)


class MEMixin(object):

    def test_failures_when_not_required(self):
        parse_args = self.get_parser(required=False).parse_args
        error = ArgumentParserError
        for args_string in self.failures:
            self.assertRaises(error, parse_args, args_string.split())

    def test_failures_when_required(self):
        parse_args = self.get_parser(required=True).parse_args
        error = ArgumentParserError
        for args_string in self.failures + ['']:
            self.assertRaises(error, parse_args, args_string.split())

    def test_successes_when_not_required(self):
        parse_args = self.get_parser(required=False).parse_args
        successes = self.successes + self.successes_when_not_required
        for args_string, expected_ns in successes:
            actual_ns = parse_args(args_string.split())
            self.assertEqual(actual_ns, expected_ns)

    def test_successes_when_required(self):
        parse_args = self.get_parser(required=True).parse_args
        for args_string, expected_ns in self.successes:
            actual_ns = parse_args(args_string.split())
            self.assertEqual(actual_ns, expected_ns)

    def test_usage_when_not_required(self):
        format_usage = self.get_parser(required=False).format_usage
        expected_usage = self.usage_when_not_required
        self.assertEqual(format_usage(), textwrap.dedent(expected_usage))

    def test_usage_when_required(self):
        format_usage = self.get_parser(required=True).format_usage
        expected_usage = self.usage_when_required
        self.assertEqual(format_usage(), textwrap.dedent(expected_usage))

    def test_help_when_not_required(self):
        format_help = self.get_parser(required=False).format_help
        help = self.usage_when_not_required + self.help
        self.assertEqual(format_help(), textwrap.dedent(help))

    def test_help_when_required(self):
        format_help = self.get_parser(required=True).format_help
        help = self.usage_when_required + self.help
        self.assertEqual(format_help(), textwrap.dedent(help))


class TestMutuallyExclusiveSimple(MEMixin, TestCase):

    def get_parser(self, required=None):
        parser = ErrorRaisingArgumentParser(prog='PROG')
        group = parser.add_mutually_exclusive_group(required=required)
        group.add_argument('--bar', help='bar help')
        group.add_argument('--baz', nargs='?', const='Z', help='baz help')
        return parser

    failures = ['--bar X --baz Y', '--bar X --baz']
    successes = [
        ('--bar X', NS(bar='X', baz=None)),
        ('--bar X --bar Z', NS(bar='Z', baz=None)),
        ('--baz Y', NS(bar=None, baz='Y')),
        ('--baz', NS(bar=None, baz='Z')),
    ]
    successes_when_not_required = [
        ('', NS(bar=None, baz=None)),
    ]

    usage_when_not_required = '''\
        usage: PROG [-h] [--bar BAR | --baz [BAZ]]
        '''
    usage_when_required = '''\
        usage: PROG [-h] (--bar BAR | --baz [BAZ])
        '''
    help = '''\

        optional arguments:
          -h, --help   show this help message and exit
          --bar BAR    bar help
          --baz [BAZ]  baz help
        '''


class TestMutuallyExclusiveLong(MEMixin, TestCase):

    def get_parser(self, required=None):
        parser = ErrorRaisingArgumentParser(prog='PROG')
        parser.add_argument('--abcde', help='abcde help')
        parser.add_argument('--fghij', help='fghij help')
        group = parser.add_mutually_exclusive_group(required=required)
        group.add_argument('--klmno', help='klmno help')
        group.add_argument('--pqrst', help='pqrst help')
        return parser

    failures = ['--klmno X --pqrst Y']
    successes = [
        ('--klmno X', NS(abcde=None, fghij=None, klmno='X', pqrst=None)),
        ('--abcde Y --klmno X',
            NS(abcde='Y', fghij=None, klmno='X', pqrst=None)),
        ('--pqrst X', NS(abcde=None, fghij=None, klmno=None, pqrst='X')),
        ('--pqrst X --fghij Y',
            NS(abcde=None, fghij='Y', klmno=None, pqrst='X')),
    ]
    successes_when_not_required = [
        ('', NS(abcde=None, fghij=None, klmno=None, pqrst=None)),
    ]

    usage_when_not_required = '''\
    usage: PROG [-h] [--abcde ABCDE] [--fghij FGHIJ]
                [--klmno KLMNO | --pqrst PQRST]
    '''
    usage_when_required = '''\
    usage: PROG [-h] [--abcde ABCDE] [--fghij FGHIJ]
                (--klmno KLMNO | --pqrst PQRST)
    '''
    help = '''\

    optional arguments:
      -h, --help     show this help message and exit
      --abcde ABCDE  abcde help
      --fghij FGHIJ  fghij help
      --klmno KLMNO  klmno help
      --pqrst PQRST  pqrst help
    '''


class TestMutuallyExclusiveFirstSuppressed(MEMixin, TestCase):

    def get_parser(self, required):
        parser = ErrorRaisingArgumentParser(prog='PROG')
        group = parser.add_mutually_exclusive_group(required=required)
        group.add_argument('-x', help=argparse.SUPPRESS)
        group.add_argument('-y', action='store_false', help='y help')
        return parser

    failures = ['-x X -y']
    successes = [
        ('-x X', NS(x='X', y=True)),
        ('-x X -x Y', NS(x='Y', y=True)),
        ('-y', NS(x=None, y=False)),
    ]
    successes_when_not_required = [
        ('', NS(x=None, y=True)),
    ]

    usage_when_not_required = '''\
        usage: PROG [-h] [-y]
        '''
    usage_when_required = '''\
        usage: PROG [-h] -y
        '''
    help = '''\

        optional arguments:
          -h, --help  show this help message and exit
          -y          y help
        '''


class TestMutuallyExclusiveManySuppressed(MEMixin, TestCase):

    def get_parser(self, required):
        parser = ErrorRaisingArgumentParser(prog='PROG')
        group = parser.add_mutually_exclusive_group(required=required)
        add = group.add_argument
        add('--spam', action='store_true', help=argparse.SUPPRESS)
        add('--badger', action='store_false', help=argparse.SUPPRESS)
        add('--bladder', help=argparse.SUPPRESS)
        return parser

    failures = [
        '--spam --badger',
        '--badger --bladder B',
        '--bladder B --spam',
    ]
    successes = [
        ('--spam', NS(spam=True, badger=True, bladder=None)),
        ('--badger', NS(spam=False, badger=False, bladder=None)),
        ('--bladder B', NS(spam=False, badger=True, bladder='B')),
        ('--spam --spam', NS(spam=True, badger=True, bladder=None)),
    ]
    successes_when_not_required = [
        ('', NS(spam=False, badger=True, bladder=None)),
    ]

    usage_when_required = usage_when_not_required = '''\
        usage: PROG [-h]
        '''
    help = '''\

        optional arguments:
          -h, --help  show this help message and exit
        '''


class TestMutuallyExclusiveOptionalAndPositional(MEMixin, TestCase):

    def get_parser(self, required):
        parser = ErrorRaisingArgumentParser(prog='PROG')
        group = parser.add_mutually_exclusive_group(required=required)
        group.add_argument('--foo', action='store_true', help='FOO')
        group.add_argument('--spam', help='SPAM')
        group.add_argument('badger', nargs='*', default='X', help='BADGER')
        return parser

    failures = [
        '--foo --spam S',
        '--spam S X',
        'X --foo',
        'X Y Z --spam S',
        '--foo X Y',
    ]
    successes = [
        ('--foo', NS(foo=True, spam=None, badger='X')),
        ('--spam S', NS(foo=False, spam='S', badger='X')),
        ('X', NS(foo=False, spam=None, badger=['X'])),
        ('X Y Z', NS(foo=False, spam=None, badger=['X', 'Y', 'Z'])),
    ]
    successes_when_not_required = [
        ('', NS(foo=False, spam=None, badger='X')),
    ]

    usage_when_not_required = '''\
        usage: PROG [-h] [--foo | --spam SPAM | badger [badger ...]]
        '''
    usage_when_required = '''\
        usage: PROG [-h] (--foo | --spam SPAM | badger [badger ...])
        '''
    help = '''\

        positional arguments:
          badger       BADGER

        optional arguments:
          -h, --help   show this help message and exit
          --foo        FOO
          --spam SPAM  SPAM
        '''


class TestMutuallyExclusiveOptionalsMixed(MEMixin, TestCase):

    def get_parser(self, required):
        parser = ErrorRaisingArgumentParser(prog='PROG')
        parser.add_argument('-x', action='store_true', help='x help')
        group = parser.add_mutually_exclusive_group(required=required)
        group.add_argument('-a', action='store_true', help='a help')
        group.add_argument('-b', action='store_true', help='b help')
        parser.add_argument('-y', action='store_true', help='y help')
        group.add_argument('-c', action='store_true', help='c help')
        return parser

    failures = ['-a -b', '-b -c', '-a -c', '-a -b -c']
    successes = [
        ('-a', NS(a=True, b=False, c=False, x=False, y=False)),
        ('-b', NS(a=False, b=True, c=False, x=False, y=False)),
        ('-c', NS(a=False, b=False, c=True, x=False, y=False)),
        ('-a -x', NS(a=True, b=False, c=False, x=True, y=False)),
        ('-y -b', NS(a=False, b=True, c=False, x=False, y=True)),
        ('-x -y -c', NS(a=False, b=False, c=True, x=True, y=True)),
    ]
    successes_when_not_required = [
        ('', NS(a=False, b=False, c=False, x=False, y=False)),
        ('-x', NS(a=False, b=False, c=False, x=True, y=False)),
        ('-y', NS(a=False, b=False, c=False, x=False, y=True)),
    ]

    usage_when_required = usage_when_not_required = '''\
        usage: PROG [-h] [-x] [-a] [-b] [-y] [-c]
        '''
    help = '''\

        optional arguments:
          -h, --help  show this help message and exit
          -x          x help
          -a          a help
          -b          b help
          -y          y help
          -c          c help
        '''


class TestMutuallyExclusiveOptionalsAndPositionalsMixed(MEMixin, TestCase):

    def get_parser(self, required):
        parser = ErrorRaisingArgumentParser(prog='PROG')
        parser.add_argument('x', help='x help')
        parser.add_argument('-y', action='store_true', help='y help')
        group = parser.add_mutually_exclusive_group(required=required)
        group.add_argument('a', nargs='?', help='a help')
        group.add_argument('-b', action='store_true', help='b help')
        group.add_argument('-c', action='store_true', help='c help')
        return parser

    failures = ['X A -b', '-b -c', '-c X A']
    successes = [
        ('X A', NS(a='A', b=False, c=False, x='X', y=False)),
        ('X -b', NS(a=None, b=True, c=False, x='X', y=False)),
        ('X -c', NS(a=None, b=False, c=True, x='X', y=False)),
        ('X A -y', NS(a='A', b=False, c=False, x='X', y=True)),
        ('X -y -b', NS(a=None, b=True, c=False, x='X', y=True)),
    ]
    successes_when_not_required = [
        ('X', NS(a=None, b=False, c=False, x='X', y=False)),
        ('X -y', NS(a=None, b=False, c=False, x='X', y=True)),
    ]

    usage_when_required = usage_when_not_required = '''\
        usage: PROG [-h] [-y] [-b] [-c] x [a]
        '''
    help = '''\

        positional arguments:
          x           x help
          a           a help

        optional arguments:
          -h, --help  show this help message and exit
          -y          y help
          -b          b help
          -c          c help
        '''

# =================================================
# Mutually exclusive group in parent parser tests
# =================================================

class MEPBase(object):

    def get_parser(self, required=None):
        parent = super(MEPBase, self).get_parser(required=required)
        parser = ErrorRaisingArgumentParser(
            prog=parent.prog, add_help=False, parents=[parent])
        return parser


class TestMutuallyExclusiveGroupErrorsParent(
    MEPBase, TestMutuallyExclusiveGroupErrors):
    pass


class TestMutuallyExclusiveSimpleParent(
    MEPBase, TestMutuallyExclusiveSimple):
    pass


class TestMutuallyExclusiveLongParent(
    MEPBase, TestMutuallyExclusiveLong):
    pass


class TestMutuallyExclusiveFirstSuppressedParent(
    MEPBase, TestMutuallyExclusiveFirstSuppressed):
    pass


class TestMutuallyExclusiveManySuppressedParent(
    MEPBase, TestMutuallyExclusiveManySuppressed):
    pass


class TestMutuallyExclusiveOptionalAndPositionalParent(
    MEPBase, TestMutuallyExclusiveOptionalAndPositional):
    pass


class TestMutuallyExclusiveOptionalsMixedParent(
    MEPBase, TestMutuallyExclusiveOptionalsMixed):
    pass


class TestMutuallyExclusiveOptionalsAndPositionalsMixedParent(
    MEPBase, TestMutuallyExclusiveOptionalsAndPositionalsMixed):
    pass

# =================
# Set default tests
# =================

class TestSetDefaults(TestCase):

    def test_set_defaults_no_args(self):
        parser = ErrorRaisingArgumentParser()
        parser.set_defaults(x='foo')
        parser.set_defaults(y='bar', z=1)
        self.assertEqual(NS(x='foo', y='bar', z=1),
                         parser.parse_args([]))
        self.assertEqual(NS(x='foo', y='bar', z=1),
                         parser.parse_args([], NS()))
        self.assertEqual(NS(x='baz', y='bar', z=1),
                         parser.parse_args([], NS(x='baz')))
        self.assertEqual(NS(x='baz', y='bar', z=2),
                         parser.parse_args([], NS(x='baz', z=2)))

    def test_set_defaults_with_args(self):
        parser = ErrorRaisingArgumentParser()
        parser.set_defaults(x='foo', y='bar')
        parser.add_argument('-x', default='xfoox')
        self.assertEqual(NS(x='xfoox', y='bar'),
                         parser.parse_args([]))
        self.assertEqual(NS(x='xfoox', y='bar'),
                         parser.parse_args([], NS()))
        self.assertEqual(NS(x='baz', y='bar'),
                         parser.parse_args([], NS(x='baz')))
        self.assertEqual(NS(x='1', y='bar'),
                         parser.parse_args('-x 1'.split()))
        self.assertEqual(NS(x='1', y='bar'),
                         parser.parse_args('-x 1'.split(), NS()))
        self.assertEqual(NS(x='1', y='bar'),
                         parser.parse_args('-x 1'.split(), NS(x='baz')))

    def test_set_defaults_subparsers(self):
        parser = ErrorRaisingArgumentParser()
        parser.set_defaults(x='foo')
        subparsers = parser.add_subparsers()
        parser_a = subparsers.add_parser('a')
        parser_a.set_defaults(y='bar')
        self.assertEqual(NS(x='foo', y='bar'),
                         parser.parse_args('a'.split()))

    def test_set_defaults_parents(self):
        parent = ErrorRaisingArgumentParser(add_help=False)
        parent.set_defaults(x='foo')
        parser = ErrorRaisingArgumentParser(parents=[parent])
        self.assertEqual(NS(x='foo'), parser.parse_args([]))

    def test_set_defaults_same_as_add_argument(self):
        parser = ErrorRaisingArgumentParser()
        parser.set_defaults(w='W', x='X', y='Y', z='Z')
        parser.add_argument('-w')
        parser.add_argument('-x', default='XX')
        parser.add_argument('y', nargs='?')
        parser.add_argument('z', nargs='?', default='ZZ')

        # defaults set previously
        self.assertEqual(NS(w='W', x='XX', y='Y', z='ZZ'),
                         parser.parse_args([]))

        # reset defaults
        parser.set_defaults(w='WW', x='X', y='YY', z='Z')
        self.assertEqual(NS(w='WW', x='X', y='YY', z='Z'),
                         parser.parse_args([]))

    def test_set_defaults_same_as_add_argument_group(self):
        parser = ErrorRaisingArgumentParser()
        parser.set_defaults(w='W', x='X', y='Y', z='Z')
        group = parser.add_argument_group('foo')
        group.add_argument('-w')
        group.add_argument('-x', default='XX')
        group.add_argument('y', nargs='?')
        group.add_argument('z', nargs='?', default='ZZ')


        # defaults set previously
        self.assertEqual(NS(w='W', x='XX', y='Y', z='ZZ'),
                         parser.parse_args([]))

        # reset defaults
        parser.set_defaults(w='WW', x='X', y='YY', z='Z')
        self.assertEqual(NS(w='WW', x='X', y='YY', z='Z'),
                         parser.parse_args([]))

# =====================
# Help formatting tests
# =====================

class TestHelpFormattingMetaclass(type):

    def __init__(cls, name, bases, bodydict):
        if name == 'HelpTestCase':
            return

        class AddTests(object):

            def __init__(self, test_class, func_suffix):
                self.func_suffix = func_suffix

                for test_func in [self.test_format,
                                  self.test_print,
                                  self.test_print_file]:
                    test_name = '%s_%s' % (test_func.__name__, func_suffix)

                    def test_wrapper(self, test_func=test_func):
                        test_func(self)
                    try:
                        test_wrapper.__name__ = test_name
                    except TypeError:
                        pass
                    setattr(test_class, test_name, test_wrapper)

            def _get_parser(self, tester):
                parser = argparse.ArgumentParser(
                    *tester.parser_signature.args,
                    **tester.parser_signature.kwargs)
                for argument_sig in tester.argument_signatures:
                    parser.add_argument(*argument_sig.args,
                                        **argument_sig.kwargs)
                group_signatures = tester.argument_group_signatures
                for group_sig, argument_sigs in group_signatures:
                    group = parser.add_argument_group(*group_sig.args,
                                                      **group_sig.kwargs)
                    for argument_sig in argument_sigs:
                        group.add_argument(*argument_sig.args,
                                           **argument_sig.kwargs)
                return parser

            def _test(self, tester, parser_text):
                expected_text = getattr(tester, self.func_suffix)
                expected_text = textwrap.dedent(expected_text)
                if expected_text != parser_text:
                    print(repr(expected_text))
                    print(repr(parser_text))
                    for char1, char2 in zip(expected_text, parser_text):
                        if char1 != char2:
                            print('first diff: %r %r' % (char1, char2))
                            break
                tester.assertEqual(expected_text, parser_text)

            def test_format(self, tester):
                parser = self._get_parser(tester)
                format = getattr(parser, 'format_%s' % self.func_suffix)
                self._test(tester, format())

            def test_print(self, tester):
                parser = self._get_parser(tester)
                print_ = getattr(parser, 'print_%s' % self.func_suffix)
                oldstderr = sys.stderr
                sys.stderr = StringIO()
                try:
                    print_()
                    parser_text = sys.stderr.getvalue()
                finally:
                    sys.stderr = oldstderr
                self._test(tester, parser_text)

            def test_print_file(self, tester):
                parser = self._get_parser(tester)
                print_ = getattr(parser, 'print_%s' % self.func_suffix)
                sfile = StringIO()
                print_(sfile)
                parser_text = sfile.getvalue()
                self._test(tester, parser_text)

        # add tests for {format,print}_{usage,help,version}
        for func_suffix in ['usage', 'help', 'version']:
            AddTests(cls, func_suffix)

bases = TestCase,
HelpTestCase = TestHelpFormattingMetaclass('HelpTestCase', bases, {})


class TestHelpBiggerOptionals(HelpTestCase):
    """Make sure that argument help aligns when options are longer"""

    parser_signature = Sig(prog='PROG', description='DESCRIPTION',
                           epilog='EPILOG', version='0.1')
    argument_signatures = [
        Sig('-x', action='store_true', help='X HELP'),
        Sig('--y', help='Y HELP'),
        Sig('foo', help='FOO HELP'),
        Sig('bar', help='BAR HELP'),
    ]
    argument_group_signatures = []
    usage = '''\
        usage: PROG [-h] [-v] [-x] [--y Y] foo bar
        '''
    help = usage + '''\

        DESCRIPTION

        positional arguments:
          foo            FOO HELP
          bar            BAR HELP

        optional arguments:
          -h, --help     show this help message and exit
          -v, --version  show program's version number and exit
          -x             X HELP
          --y Y          Y HELP

        EPILOG
    '''
    version = '''\
        0.1
        '''


class TestHelpBiggerOptionalGroups(HelpTestCase):
    """Make sure that argument help aligns when options are longer"""

    parser_signature = Sig(prog='PROG', description='DESCRIPTION',
                           epilog='EPILOG', version='0.1')
    argument_signatures = [
        Sig('-x', action='store_true', help='X HELP'),
        Sig('--y', help='Y HELP'),
        Sig('foo', help='FOO HELP'),
        Sig('bar', help='BAR HELP'),
    ]
    argument_group_signatures = [
        (Sig('GROUP TITLE', description='GROUP DESCRIPTION'), [
            Sig('baz', help='BAZ HELP'),
            Sig('-z', nargs='+', help='Z HELP')]),
    ]
    usage = '''\
        usage: PROG [-h] [-v] [-x] [--y Y] [-z Z [Z ...]] foo bar baz
        '''
    help = usage + '''\

        DESCRIPTION

        positional arguments:
          foo            FOO HELP
          bar            BAR HELP

        optional arguments:
          -h, --help     show this help message and exit
          -v, --version  show program's version number and exit
          -x             X HELP
          --y Y          Y HELP

        GROUP TITLE:
          GROUP DESCRIPTION

          baz            BAZ HELP
          -z Z [Z ...]   Z HELP

        EPILOG
    '''
    version = '''\
        0.1
        '''


class TestHelpBiggerPositionals(HelpTestCase):
    """Make sure that help aligns when arguments are longer"""

    parser_signature = Sig(usage='USAGE', description='DESCRIPTION')
    argument_signatures = [
        Sig('-x', action='store_true', help='X HELP'),
        Sig('--y', help='Y HELP'),
        Sig('ekiekiekifekang', help='EKI HELP'),
        Sig('bar', help='BAR HELP'),
    ]
    argument_group_signatures = []
    usage = '''\
        usage: USAGE
        '''
    help = usage + '''\

        DESCRIPTION

        positional arguments:
          ekiekiekifekang  EKI HELP
          bar              BAR HELP

        optional arguments:
          -h, --help       show this help message and exit
          -x               X HELP
          --y Y            Y HELP
        '''

    version = ''


class TestHelpReformatting(HelpTestCase):
    """Make sure that text after short names starts on the first line"""

    parser_signature = Sig(
        prog='PROG',
        description='   oddly    formatted\n'
                    'description\n'
                    '\n'
                    'that is so long that it should go onto multiple '
                    'lines when wrapped')
    argument_signatures = [
        Sig('-x', metavar='XX', help='oddly\n'
                                     '    formatted -x help'),
        Sig('y', metavar='yyy', help='normal y help'),
    ]
    argument_group_signatures = [
        (Sig('title', description='\n'
                                  '    oddly formatted group\n'
                                  '\n'
                                  'description'),
         [Sig('-a', action='store_true',
              help=' oddly \n'
                   'formatted    -a  help  \n'
                   '    again, so long that it should be wrapped over '
                   'multiple lines')]),
    ]
    usage = '''\
        usage: PROG [-h] [-x XX] [-a] yyy
        '''
    help = usage + '''\

        oddly formatted description that is so long that it should go onto \
multiple
        lines when wrapped

        positional arguments:
          yyy         normal y help

        optional arguments:
          -h, --help  show this help message and exit
          -x XX       oddly formatted -x help

        title:
          oddly formatted group description

          -a          oddly formatted -a help again, so long that it should \
be wrapped
                      over multiple lines
        '''
    version = ''


class TestHelpWrappingShortNames(HelpTestCase):
    """Make sure that text after short names starts on the first line"""

    parser_signature = Sig(prog='PROG', description= 'D\nD' * 30)
    argument_signatures = [
        Sig('-x', metavar='XX', help='XHH HX' * 20),
        Sig('y', metavar='yyy', help='YH YH' * 20),
    ]
    argument_group_signatures = [
        (Sig('ALPHAS'), [
            Sig('-a', action='store_true', help='AHHH HHA' * 10)]),
    ]
    usage = '''\
        usage: PROG [-h] [-x XX] [-a] yyy
        '''
    help = usage + '''\

        D DD DD DD DD DD DD DD DD DD DD DD DD DD DD DD DD DD DD DD DD DD DD \
DD DD DD
        DD DD DD DD D

        positional arguments:
          yyy         YH YHYH YHYH YHYH YHYH YHYH YHYH YHYH YHYH YHYH YHYH \
YHYH YHYH
                      YHYH YHYH YHYH YHYH YHYH YHYH YHYH YH

        optional arguments:
          -h, --help  show this help message and exit
          -x XX       XHH HXXHH HXXHH HXXHH HXXHH HXXHH HXXHH HXXHH HXXHH \
HXXHH HXXHH
                      HXXHH HXXHH HXXHH HXXHH HXXHH HXXHH HXXHH HXXHH HXXHH HX

        ALPHAS:
          -a          AHHH HHAAHHH HHAAHHH HHAAHHH HHAAHHH HHAAHHH HHAAHHH \
HHAAHHH
                      HHAAHHH HHAAHHH HHA
        '''
    version = ''


class TestHelpWrappingLongNames(HelpTestCase):
    """Make sure that text after long names starts on the next line"""

    parser_signature = Sig(usage='USAGE', description= 'D D' * 30,
                           version='V V'*30)
    argument_signatures = [
        Sig('-x', metavar='X' * 25, help='XH XH' * 20),
        Sig('y', metavar='y' * 25, help='YH YH' * 20),
    ]
    argument_group_signatures = [
        (Sig('ALPHAS'), [
            Sig('-a', metavar='A' * 25, help='AH AH' * 20),
            Sig('z', metavar='z' * 25, help='ZH ZH' * 20)]),
    ]
    usage = '''\
        usage: USAGE
        '''
    help = usage + '''\

        D DD DD DD DD DD DD DD DD DD DD DD DD DD DD DD DD DD DD DD DD DD DD \
DD DD DD
        DD DD DD DD D

        positional arguments:
          yyyyyyyyyyyyyyyyyyyyyyyyy
                                YH YHYH YHYH YHYH YHYH YHYH YHYH YHYH YHYH \
YHYH YHYH
                                YHYH YHYH YHYH YHYH YHYH YHYH YHYH YHYH YHYH YH

        optional arguments:
          -h, --help            show this help message and exit
          -v, --version         show program's version number and exit
          -x XXXXXXXXXXXXXXXXXXXXXXXXX
                                XH XHXH XHXH XHXH XHXH XHXH XHXH XHXH XHXH \
XHXH XHXH
                                XHXH XHXH XHXH XHXH XHXH XHXH XHXH XHXH XHXH XH

        ALPHAS:
          -a AAAAAAAAAAAAAAAAAAAAAAAAA
                                AH AHAH AHAH AHAH AHAH AHAH AHAH AHAH AHAH \
AHAH AHAH
                                AHAH AHAH AHAH AHAH AHAH AHAH AHAH AHAH AHAH AH
          zzzzzzzzzzzzzzzzzzzzzzzzz
                                ZH ZHZH ZHZH ZHZH ZHZH ZHZH ZHZH ZHZH ZHZH \
ZHZH ZHZH
                                ZHZH ZHZH ZHZH ZHZH ZHZH ZHZH ZHZH ZHZH ZHZH ZH
        '''
    version = '''\
        V VV VV VV VV VV VV VV VV VV VV VV VV VV VV VV VV VV VV VV VV VV VV \
VV VV VV
        VV VV VV VV V
        '''


class TestHelpUsage(HelpTestCase):
    """Test basic usage messages"""

    parser_signature = Sig(prog='PROG')
    argument_signatures = [
        Sig('-w', nargs='+', help='w'),
        Sig('-x', nargs='*', help='x'),
        Sig('a', help='a'),
        Sig('b', help='b', nargs=2),
        Sig('c', help='c', nargs='?'),
    ]
    argument_group_signatures = [
        (Sig('group'), [
            Sig('-y', nargs='?', help='y'),
            Sig('-z', nargs=3, help='z'),
            Sig('d', help='d', nargs='*'),
            Sig('e', help='e', nargs='+'),
        ])
    ]
    usage = '''\
        usage: PROG [-h] [-w W [W ...]] [-x [X [X ...]]] [-y [Y]] [-z Z Z Z]
                    a b b [c] [d [d ...]] e [e ...]
        '''
    help = usage + '''\

        positional arguments:
          a               a
          b               b
          c               c

        optional arguments:
          -h, --help      show this help message and exit
          -w W [W ...]    w
          -x [X [X ...]]  x

        group:
          -y [Y]          y
          -z Z Z Z        z
          d               d
          e               e
        '''
    version = ''


class TestHelpOnlyUserGroups(HelpTestCase):
    """Test basic usage messages"""

    parser_signature = Sig(prog='PROG', add_help=False)
    argument_signatures = []
    argument_group_signatures = [
        (Sig('xxxx'), [
            Sig('-x', help='x'),
            Sig('a', help='a'),
        ]),
        (Sig('yyyy'), [
            Sig('b', help='b'),
            Sig('-y', help='y'),
        ]),
    ]
    usage = '''\
        usage: PROG [-x X] [-y Y] a b
        '''
    help = usage + '''\

        xxxx:
          -x X  x
          a     a

        yyyy:
          b     b
          -y Y  y
        '''
    version = ''


class TestHelpUsageLongProg(HelpTestCase):
    """Test usage messages where the prog is long"""

    parser_signature = Sig(prog='P' * 60)
    argument_signatures = [
        Sig('-w', metavar='W'),
        Sig('-x', metavar='X'),
        Sig('a'),
        Sig('b'),
    ]
    argument_group_signatures = []
    usage = '''\
        usage: PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP
               [-h] [-w W] [-x X] a b
        '''
    help = usage + '''\

        positional arguments:
          a
          b

        optional arguments:
          -h, --help  show this help message and exit
          -w W
          -x X
        '''
    version = ''


class TestHelpUsageLongProgOptionsWrap(HelpTestCase):
    """Test usage messages where the prog is long and the optionals wrap"""

    parser_signature = Sig(prog='P' * 60)
    argument_signatures = [
        Sig('-w', metavar='W' * 25),
        Sig('-x', metavar='X' * 25),
        Sig('-y', metavar='Y' * 25),
        Sig('-z', metavar='Z' * 25),
        Sig('a'),
        Sig('b'),
    ]
    argument_group_signatures = []
    usage = '''\
        usage: PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP
               [-h] [-w WWWWWWWWWWWWWWWWWWWWWWWWW] \
[-x XXXXXXXXXXXXXXXXXXXXXXXXX]
               [-y YYYYYYYYYYYYYYYYYYYYYYYYY] [-z ZZZZZZZZZZZZZZZZZZZZZZZZZ]
               a b
        '''
    help = usage + '''\

        positional arguments:
          a
          b

        optional arguments:
          -h, --help            show this help message and exit
          -w WWWWWWWWWWWWWWWWWWWWWWWWW
          -x XXXXXXXXXXXXXXXXXXXXXXXXX
          -y YYYYYYYYYYYYYYYYYYYYYYYYY
          -z ZZZZZZZZZZZZZZZZZZZZZZZZZ
        '''
    version = ''


class TestHelpUsageLongProgPositionalsWrap(HelpTestCase):
    """Test usage messages where the prog is long and the positionals wrap"""

    parser_signature = Sig(prog='P' * 60, add_help=False)
    argument_signatures = [
        Sig('a' * 25),
        Sig('b' * 25),
        Sig('c' * 25),
    ]
    argument_group_signatures = []
    usage = '''\
        usage: PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP
               aaaaaaaaaaaaaaaaaaaaaaaaa bbbbbbbbbbbbbbbbbbbbbbbbb
               ccccccccccccccccccccccccc
        '''
    help = usage + '''\

        positional arguments:
          aaaaaaaaaaaaaaaaaaaaaaaaa
          bbbbbbbbbbbbbbbbbbbbbbbbb
          ccccccccccccccccccccccccc
        '''
    version = ''


class TestHelpUsageOptionalsWrap(HelpTestCase):
    """Test usage messages where the optionals wrap"""

    parser_signature = Sig(prog='PROG')
    argument_signatures = [
        Sig('-w', metavar='W' * 25),
        Sig('-x', metavar='X' * 25),
        Sig('-y', metavar='Y' * 25),
        Sig('-z', metavar='Z' * 25),
        Sig('a'),
        Sig('b'),
        Sig('c'),
    ]
    argument_group_signatures = []
    usage = '''\
        usage: PROG [-h] [-w WWWWWWWWWWWWWWWWWWWWWWWWW] \
[-x XXXXXXXXXXXXXXXXXXXXXXXXX]
                    [-y YYYYYYYYYYYYYYYYYYYYYYYYY] \
[-z ZZZZZZZZZZZZZZZZZZZZZZZZZ]
                    a b c
        '''
    help = usage + '''\

        positional arguments:
          a
          b
          c

        optional arguments:
          -h, --help            show this help message and exit
          -w WWWWWWWWWWWWWWWWWWWWWWWWW
          -x XXXXXXXXXXXXXXXXXXXXXXXXX
          -y YYYYYYYYYYYYYYYYYYYYYYYYY
          -z ZZZZZZZZZZZZZZZZZZZZZZZZZ
        '''
    version = ''


class TestHelpUsagePositionalsWrap(HelpTestCase):
    """Test usage messages where the positionals wrap"""

    parser_signature = Sig(prog='PROG')
    argument_signatures = [
        Sig('-x'),
        Sig('-y'),
        Sig('-z'),
        Sig('a' * 25),
        Sig('b' * 25),
        Sig('c' * 25),
    ]
    argument_group_signatures = []
    usage = '''\
        usage: PROG [-h] [-x X] [-y Y] [-z Z]
                    aaaaaaaaaaaaaaaaaaaaaaaaa bbbbbbbbbbbbbbbbbbbbbbbbb
                    ccccccccccccccccccccccccc
        '''
    help = usage + '''\

        positional arguments:
          aaaaaaaaaaaaaaaaaaaaaaaaa
          bbbbbbbbbbbbbbbbbbbbbbbbb
          ccccccccccccccccccccccccc

        optional arguments:
          -h, --help            show this help message and exit
          -x X
          -y Y
          -z Z
        '''
    version = ''


class TestHelpUsageOptionalsPositionalsWrap(HelpTestCase):
    """Test usage messages where the optionals and positionals wrap"""

    parser_signature = Sig(prog='PROG')
    argument_signatures = [
        Sig('-x', metavar='X' * 25),
        Sig('-y', metavar='Y' * 25),
        Sig('-z', metavar='Z' * 25),
        Sig('a' * 25),
        Sig('b' * 25),
        Sig('c' * 25),
    ]
    argument_group_signatures = []
    usage = '''\
        usage: PROG [-h] [-x XXXXXXXXXXXXXXXXXXXXXXXXX] \
[-y YYYYYYYYYYYYYYYYYYYYYYYYY]
                    [-z ZZZZZZZZZZZZZZZZZZZZZZZZZ]
                    aaaaaaaaaaaaaaaaaaaaaaaaa bbbbbbbbbbbbbbbbbbbbbbbbb
                    ccccccccccccccccccccccccc
        '''
    help = usage + '''\

        positional arguments:
          aaaaaaaaaaaaaaaaaaaaaaaaa
          bbbbbbbbbbbbbbbbbbbbbbbbb
          ccccccccccccccccccccccccc

        optional arguments:
          -h, --help            show this help message and exit
          -x XXXXXXXXXXXXXXXXXXXXXXXXX
          -y YYYYYYYYYYYYYYYYYYYYYYYYY
          -z ZZZZZZZZZZZZZZZZZZZZZZZZZ
        '''
    version = ''


class TestHelpUsageOptionalsOnlyWrap(HelpTestCase):
    """Test usage messages where there are only optionals and they wrap"""

    parser_signature = Sig(prog='PROG')
    argument_signatures = [
        Sig('-x', metavar='X' * 25),
        Sig('-y', metavar='Y' * 25),
        Sig('-z', metavar='Z' * 25),
    ]
    argument_group_signatures = []
    usage = '''\
        usage: PROG [-h] [-x XXXXXXXXXXXXXXXXXXXXXXXXX] \
[-y YYYYYYYYYYYYYYYYYYYYYYYYY]
                    [-z ZZZZZZZZZZZZZZZZZZZZZZZZZ]
        '''
    help = usage + '''\

        optional arguments:
          -h, --help            show this help message and exit
          -x XXXXXXXXXXXXXXXXXXXXXXXXX
          -y YYYYYYYYYYYYYYYYYYYYYYYYY
          -z ZZZZZZZZZZZZZZZZZZZZZZZZZ
        '''
    version = ''


class TestHelpUsagePositionalsOnlyWrap(HelpTestCase):
    """Test usage messages where there are only positionals and they wrap"""

    parser_signature = Sig(prog='PROG', add_help=False)
    argument_signatures = [
        Sig('a' * 25),
        Sig('b' * 25),
        Sig('c' * 25),
    ]
    argument_group_signatures = []
    usage = '''\
        usage: PROG aaaaaaaaaaaaaaaaaaaaaaaaa bbbbbbbbbbbbbbbbbbbbbbbbb
                    ccccccccccccccccccccccccc
        '''
    help = usage + '''\

        positional arguments:
          aaaaaaaaaaaaaaaaaaaaaaaaa
          bbbbbbbbbbbbbbbbbbbbbbbbb
          ccccccccccccccccccccccccc
        '''
    version = ''


class TestHelpVariableExpansion(HelpTestCase):
    """Test that variables are expanded properly in help messages"""

    parser_signature = Sig(prog='PROG')
    argument_signatures = [
        Sig('-x', type='int',
            help='x %(prog)s %(default)s %(type)s %%'),
        Sig('-y', action='store_const', default=42, const='XXX',
            help='y %(prog)s %(default)s %(const)s'),
        Sig('--foo', choices='abc',
            help='foo %(prog)s %(default)s %(choices)s'),
        Sig('--bar', default='baz', choices=[1, 2], metavar='BBB',
            help='bar %(prog)s %(default)s %(dest)s'),
        Sig('spam', help='spam %(prog)s %(default)s'),
        Sig('badger', default=0.5, help='badger %(prog)s %(default)s'),
    ]
    argument_group_signatures = [
        (Sig('group'), [
            Sig('-a', help='a %(prog)s %(default)s'),
            Sig('-b', default=-1, help='b %(prog)s %(default)s'),
        ])
    ]
    usage = ('''\
        usage: PROG [-h] [-x X] [-y] [--foo {a,b,c}] [--bar BBB] [-a A] [-b B]
                    spam badger
        ''')
    help = usage + '''\

        positional arguments:
          spam           spam PROG None
          badger         badger PROG 0.5

        optional arguments:
          -h, --help     show this help message and exit
          -x X           x PROG None int %
          -y             y PROG 42 XXX
          --foo {a,b,c}  foo PROG None a, b, c
          --bar BBB      bar PROG baz bar

        group:
          -a A           a PROG None
          -b B           b PROG -1
        '''
    version = ''


class TestHelpVariableExpansionUsageSupplied(HelpTestCase):
    """Test that variables are expanded properly when usage= is present"""

    parser_signature = Sig(prog='PROG', usage='%(prog)s FOO')
    argument_signatures = []
    argument_group_signatures = []
    usage = ('''\
        usage: PROG FOO
        ''')
    help = usage + '''\

        optional arguments:
          -h, --help  show this help message and exit
        '''
    version = ''


class TestHelpVariableExpansionNoArguments(HelpTestCase):
    """Test that variables are expanded properly with no arguments"""

    parser_signature = Sig(prog='PROG', add_help=False)
    argument_signatures = []
    argument_group_signatures = []
    usage = ('''\
        usage: PROG
        ''')
    help = usage
    version = ''


class TestHelpSuppressUsage(HelpTestCase):
    """Test that items can be suppressed in usage messages"""

    parser_signature = Sig(prog='PROG', usage=argparse.SUPPRESS)
    argument_signatures = [
        Sig('--foo', help='foo help'),
        Sig('spam', help='spam help'),
    ]
    argument_group_signatures = []
    help = '''\
        positional arguments:
          spam        spam help

        optional arguments:
          -h, --help  show this help message and exit
          --foo FOO   foo help
        '''
    usage = ''
    version = ''


class TestHelpSuppressOptional(HelpTestCase):
    """Test that optional arguments can be suppressed in help messages"""

    parser_signature = Sig(prog='PROG', add_help=False)
    argument_signatures = [
        Sig('--foo', help=argparse.SUPPRESS),
        Sig('spam', help='spam help'),
    ]
    argument_group_signatures = []
    usage = '''\
        usage: PROG spam
        '''
    help = usage + '''\

        positional arguments:
          spam  spam help
        '''
    version = ''


class TestHelpSuppressOptionalGroup(HelpTestCase):
    """Test that optional groups can be suppressed in help messages"""

    parser_signature = Sig(prog='PROG')
    argument_signatures = [
        Sig('--foo', help='foo help'),
        Sig('spam', help='spam help'),
    ]
    argument_group_signatures = [
        (Sig('group'), [Sig('--bar', help=argparse.SUPPRESS)]),
    ]
    usage = '''\
        usage: PROG [-h] [--foo FOO] spam
        '''
    help = usage + '''\

        positional arguments:
          spam        spam help

        optional arguments:
          -h, --help  show this help message and exit
          --foo FOO   foo help
        '''
    version = ''


class TestHelpSuppressPositional(HelpTestCase):
    """Test that positional arguments can be suppressed in help messages"""

    parser_signature = Sig(prog='PROG')
    argument_signatures = [
        Sig('--foo', help='foo help'),
        Sig('spam', help=argparse.SUPPRESS),
    ]
    argument_group_signatures = []
    usage = '''\
        usage: PROG [-h] [--foo FOO]
        '''
    help = usage + '''\

        optional arguments:
          -h, --help  show this help message and exit
          --foo FOO   foo help
        '''
    version = ''


class TestHelpRequiredOptional(HelpTestCase):
    """Test that required options don't look optional"""

    parser_signature = Sig(prog='PROG')
    argument_signatures = [
        Sig('--foo', required=True, help='foo help'),
    ]
    argument_group_signatures = []
    usage = '''\
        usage: PROG [-h] --foo FOO
        '''
    help = usage + '''\

        optional arguments:
          -h, --help  show this help message and exit
          --foo FOO   foo help
        '''
    version = ''


class TestHelpAlternatePrefixChars(HelpTestCase):
    """Test that options display with different prefix characters"""

    parser_signature = Sig(prog='PROG', prefix_chars='^;', add_help=False)
    argument_signatures = [
        Sig('^^foo', action='store_true', help='foo help'),
        Sig(';b', ';;bar', help='bar help'),
    ]
    argument_group_signatures = []
    usage = '''\
        usage: PROG [^^foo] [;b BAR]
        '''
    help = usage + '''\

        optional arguments:
          ^^foo              foo help
          ;b BAR, ;;bar BAR  bar help
        '''
    version = ''


class TestHelpNoHelpOptional(HelpTestCase):
    """Test that the --help argument can be suppressed help messages"""

    parser_signature = Sig(prog='PROG', add_help=False)
    argument_signatures = [
        Sig('--foo', help='foo help'),
        Sig('spam', help='spam help'),
    ]
    argument_group_signatures = []
    usage = '''\
        usage: PROG [--foo FOO] spam
        '''
    help = usage + '''\

        positional arguments:
          spam       spam help

        optional arguments:
          --foo FOO  foo help
        '''
    version = ''


class TestHelpVersionOptional(HelpTestCase):
    """Test that the --version argument can be suppressed help messages"""

    parser_signature = Sig(prog='PROG', version='1.0')
    argument_signatures = [
        Sig('--foo', help='foo help'),
        Sig('spam', help='spam help'),
    ]
    argument_group_signatures = []
    usage = '''\
        usage: PROG [-h] [-v] [--foo FOO] spam
        '''
    help = usage + '''\

        positional arguments:
          spam           spam help

        optional arguments:
          -h, --help     show this help message and exit
          -v, --version  show program's version number and exit
          --foo FOO      foo help
        '''
    version = '''\
        1.0
        '''


class TestHelpNone(HelpTestCase):
    """Test that no errors occur if no help is specified"""

    parser_signature = Sig(prog='PROG')
    argument_signatures = [
        Sig('--foo'),
        Sig('spam'),
    ]
    argument_group_signatures = []
    usage = '''\
        usage: PROG [-h] [--foo FOO] spam
        '''
    help = usage + '''\

        positional arguments:
          spam

        optional arguments:
          -h, --help  show this help message and exit
          --foo FOO
        '''
    version = ''


class TestHelpTupleMetavar(HelpTestCase):
    """Test specifying metavar as a tuple"""

    parser_signature = Sig(prog='PROG')
    argument_signatures = [
        Sig('-w', help='w', nargs='+', metavar=('W1', 'W2')),
        Sig('-x', help='x', nargs='*', metavar=('X1', 'X2')),
        Sig('-y', help='y', nargs=3, metavar=('Y1', 'Y2', 'Y3')),
        Sig('-z', help='z', nargs='?', metavar=('Z1', )),
    ]
    argument_group_signatures = []
    usage = '''\
        usage: PROG [-h] [-w W1 [W2 ...]] [-x [X1 [X2 ...]]] [-y Y1 Y2 Y3] \
[-z [Z1]]
        '''
    help = usage + '''\

        optional arguments:
          -h, --help        show this help message and exit
          -w W1 [W2 ...]    w
          -x [X1 [X2 ...]]  x
          -y Y1 Y2 Y3       y
          -z [Z1]           z
        '''
    version = ''


class TestHelpRawText(HelpTestCase):
    """Test the RawTextHelpFormatter"""

    parser_signature = Sig(
        prog='PROG', formatter_class=argparse.RawTextHelpFormatter,
        description='Keep the formatting\n'
                    '    exactly as it is written\n'
                    '\n'
                    'here\n')

    argument_signatures = [
        Sig('--foo', help='    foo help should also\n'
                          'appear as given here'),
        Sig('spam', help='spam help'),
    ]
    argument_group_signatures = [
        (Sig('title', description='    This text\n'
                                  '  should be indented\n'
                                  '    exactly like it is here\n'),
         [Sig('--bar', help='bar help')]),
    ]
    usage = '''\
        usage: PROG [-h] [--foo FOO] [--bar BAR] spam
        '''
    help = usage + '''\

        Keep the formatting
            exactly as it is written

        here

        positional arguments:
          spam        spam help

        optional arguments:
          -h, --help  show this help message and exit
          --foo FOO       foo help should also
                      appear as given here

        title:
              This text
            should be indented
              exactly like it is here

          --bar BAR   bar help
        '''
    version = ''


class TestHelpRawDescription(HelpTestCase):
    """Test the RawTextHelpFormatter"""

    parser_signature = Sig(
        prog='PROG', formatter_class=argparse.RawDescriptionHelpFormatter,
        description='Keep the formatting\n'
                    '    exactly as it is written\n'
                    '\n'
                    'here\n')

    argument_signatures = [
        Sig('--foo', help='  foo help should not\n'
                          '    retain this odd formatting'),
        Sig('spam', help='spam help'),
    ]
    argument_group_signatures = [
        (Sig('title', description='    This text\n'
                                  '  should be indented\n'
                                  '    exactly like it is here\n'),
         [Sig('--bar', help='bar help')]),
    ]
    usage = '''\
        usage: PROG [-h] [--foo FOO] [--bar BAR] spam
        '''
    help = usage + '''\

        Keep the formatting
            exactly as it is written

        here

        positional arguments:
          spam        spam help

        optional arguments:
          -h, --help  show this help message and exit
          --foo FOO   foo help should not retain this odd formatting

        title:
              This text
            should be indented
              exactly like it is here

          --bar BAR   bar help
        '''
    version = ''


class TestHelpArgumentDefaults(HelpTestCase):
    """Test the ArgumentDefaultsHelpFormatter"""

    parser_signature = Sig(
        prog='PROG', formatter_class=argparse.ArgumentDefaultsHelpFormatter,
        description='description')

    argument_signatures = [
        Sig('--foo', help='foo help - oh and by the way, %(default)s'),
        Sig('--bar', action='store_true', help='bar help'),
        Sig('spam', help='spam help'),
        Sig('badger', nargs='?', default='wooden', help='badger help'),
    ]
    argument_group_signatures = [
        (Sig('title', description='description'),
         [Sig('--baz', type=int, default=42, help='baz help')]),
    ]
    usage = '''\
        usage: PROG [-h] [--foo FOO] [--bar] [--baz BAZ] spam [badger]
        '''
    help = usage + '''\

        description

        positional arguments:
          spam        spam help
          badger      badger help (default: wooden)

        optional arguments:
          -h, --help  show this help message and exit
          --foo FOO   foo help - oh and by the way, None
          --bar       bar help (default: False)

        title:
          description

          --baz BAZ   baz help (default: 42)
        '''
    version = ''

# =====================================
# Optional/Positional constructor tests
# =====================================

class TestInvalidArgumentConstructors(TestCase):
    """Test a bunch of invalid Argument constructors"""

    def assertTypeError(self, *args, **kwargs):
        parser = argparse.ArgumentParser()
        self.assertRaises(TypeError, parser.add_argument,
                          *args, **kwargs)

    def assertValueError(self, *args, **kwargs):
        parser = argparse.ArgumentParser()
        self.assertRaises(ValueError, parser.add_argument,
                          *args, **kwargs)

    def test_invalid_keyword_arguments(self):
        self.assertTypeError('-x', bar=None)
        self.assertTypeError('-y', callback='foo')
        self.assertTypeError('-y', callback_args=())
        self.assertTypeError('-y', callback_kwargs={})

    def test_missing_destination(self):
        self.assertTypeError()
        for action in ['append', 'store']:
            self.assertTypeError(action=action)

    def test_invalid_option_strings(self):
        self.assertValueError('--')
        self.assertValueError('---')

    def test_invalid_action(self):
        self.assertValueError('-x', action='foo')
        self.assertValueError('foo', action='baz')
        parser = argparse.ArgumentParser()
        try:
            parser.add_argument("--foo", action="store-true")
        except ValueError:
            e = sys.exc_info()[1]
            expected = 'unknown action'
            msg = 'expected %r, found %r' % (expected, e)
            self.failUnless(expected in str(e), msg)

    def test_multiple_dest(self):
        parser = argparse.ArgumentParser()
        parser.add_argument(dest='foo')
        try:
            parser.add_argument('bar', dest='baz')
        except ValueError:
            e = sys.exc_info()[1]
            expected = 'dest supplied twice for positional argument'
            msg = 'expected %r, found %r' % (expected, e)
            self.failUnless(expected in str(e), msg)

    def test_no_argument_actions(self):
        for action in ['store_const', 'store_true', 'store_false',
                       'append_const', 'count']:
            for attrs in [dict(type=int), dict(nargs='+'),
                          dict(choices='ab')]:
                self.assertTypeError('-x', action=action, **attrs)

    def test_no_argument_no_const_actions(self):
        # options with zero arguments
        for action in ['store_true', 'store_false', 'count']:

            # const is always disallowed
            self.assertTypeError('-x', const='foo', action=action)

            # nargs is always disallowed
            self.assertTypeError('-x', nargs='*', action=action)

    def test_more_than_one_argument_actions(self):
        for action in ['store', 'append']:

            # nargs=0 is disallowed
            self.assertValueError('-x', nargs=0, action=action)
            self.assertValueError('spam', nargs=0, action=action)

            # const is disallowed with non-optional arguments
            for nargs in [1, '*', '+']:
                self.assertValueError('-x', const='foo',
                                      nargs=nargs, action=action)
                self.assertValueError('spam', const='foo',
                                      nargs=nargs, action=action)

    def test_required_const_actions(self):
        for action in ['store_const', 'append_const']:

            # nargs is always disallowed
            self.assertTypeError('-x', nargs='+', action=action)

    def test_parsers_action_missing_params(self):
        self.assertTypeError('command', action='parsers')
        self.assertTypeError('command', action='parsers', prog='PROG')
        self.assertTypeError('command', action='parsers',
                             parser_class=argparse.ArgumentParser)

    def test_required_positional(self):
        self.assertTypeError('foo', required=True)

    def test_user_defined_action(self):

        class Success(Exception):
            pass

        class Action(object):

            def __init__(self,
                         option_strings,
                         dest,
                         const,
                         default,
                         required=False):
                if dest == 'spam':
                    if const is Success:
                        if default is Success:
                            raise Success()

            def __call__(self, *args, **kwargs):
                pass

        parser = argparse.ArgumentParser()
        self.assertRaises(Success, parser.add_argument, '--spam',
                          action=Action, default=Success, const=Success)
        self.assertRaises(Success, parser.add_argument, 'spam',
                          action=Action, default=Success, const=Success)

# ================================
# Actions returned by add_argument
# ================================

class TestActionsReturned(TestCase):

    def test_dest(self):
        parser = argparse.ArgumentParser()
        action = parser.add_argument('--foo')
        self.assertEqual(action.dest, 'foo')
        action = parser.add_argument('-b', '--bar')
        self.assertEqual(action.dest, 'bar')
        action = parser.add_argument('-x', '-y')
        self.assertEqual(action.dest, 'x')

    def test_misc(self):
        parser = argparse.ArgumentParser()
        action = parser.add_argument('--foo', nargs='?', const=42,
                                     default=84, type=int, choices=[1, 2],
                                     help='FOO', metavar='BAR', dest='baz')
        self.assertEqual(action.nargs, '?')
        self.assertEqual(action.const, 42)
        self.assertEqual(action.default, 84)
        self.assertEqual(action.type, int)
        self.assertEqual(action.choices, [1, 2])
        self.assertEqual(action.help, 'FOO')
        self.assertEqual(action.metavar, 'BAR')
        self.assertEqual(action.dest, 'baz')


# ================================
# Argument conflict handling tests
# ================================

class TestConflictHandling(TestCase):

    def test_bad_type(self):
        self.assertRaises(ValueError, argparse.ArgumentParser,
                          conflict_handler='foo')

    def test_conflict_error(self):
        parser = argparse.ArgumentParser()
        parser.add_argument('-x')
        self.assertRaises(argparse.ArgumentError,
                          parser.add_argument, '-x')
        parser.add_argument('--spam')
        self.assertRaises(argparse.ArgumentError,
                          parser.add_argument, '--spam')

    def test_resolve_error(self):
        get_parser = argparse.ArgumentParser
        parser = get_parser(prog='PROG', conflict_handler='resolve')

        parser.add_argument('-x', help='OLD X')
        parser.add_argument('-x', help='NEW X')
        self.assertEqual(parser.format_help(), textwrap.dedent('''\
            usage: PROG [-h] [-x X]

            optional arguments:
              -h, --help  show this help message and exit
              -x X        NEW X
            '''))

        parser.add_argument('--spam', metavar='OLD_SPAM')
        parser.add_argument('--spam', metavar='NEW_SPAM')
        self.assertEqual(parser.format_help(), textwrap.dedent('''\
            usage: PROG [-h] [-x X] [--spam NEW_SPAM]

            optional arguments:
              -h, --help       show this help message and exit
              -x X             NEW X
              --spam NEW_SPAM
            '''))


# =============================
# Help and Version option tests
# =============================

class TestOptionalsHelpVersionActions(TestCase):
    """Test the help and version actions"""

    def _get_error_message(self, func, *args, **kwargs):
        try:
            func(*args, **kwargs)
        except ArgumentParserError:
            err = sys.exc_info()[1]
            return err.message
        else:
            self.assertRaises(ArgumentParserError, func, *args, **kwargs)

    def assertPrintHelpExit(self, parser, args_str):
        self.assertEqual(
            parser.format_help(),
            self._get_error_message(parser.parse_args, args_str.split()))

    def assertPrintVersionExit(self, parser, args_str):
        self.assertEqual(
            parser.format_version(),
            self._get_error_message(parser.parse_args, args_str.split()))

    def assertArgumentParserError(self, parser, *args):
        self.assertRaises(ArgumentParserError, parser.parse_args, args)

    def test_version(self):
        parser = ErrorRaisingArgumentParser(version='1.0')
        self.assertPrintHelpExit(parser, '-h')
        self.assertPrintHelpExit(parser, '--help')
        self.assertPrintVersionExit(parser, '-v')
        self.assertPrintVersionExit(parser, '--version')

    def test_version_format(self):
        parser = ErrorRaisingArgumentParser(prog='PPP', version='%(prog)s 3.5')
        msg = self._get_error_message(parser.parse_args, ['-v'])
        self.assertEqual('PPP 3.5\n', msg)

    def test_version_no_help(self):
        parser = ErrorRaisingArgumentParser(add_help=False, version='1.0')
        self.assertArgumentParserError(parser, '-h')
        self.assertArgumentParserError(parser, '--help')
        self.assertPrintVersionExit(parser, '-v')
        self.assertPrintVersionExit(parser, '--version')

    def test_version_action(self):
        parser = ErrorRaisingArgumentParser(prog='XXX')
        parser.add_argument('-V', action='version', version='%(prog)s 3.7')
        msg = self._get_error_message(parser.parse_args, ['-V'])
        self.assertEqual('XXX 3.7\n', msg)

    def test_no_help(self):
        parser = ErrorRaisingArgumentParser(add_help=False)
        self.assertArgumentParserError(parser, '-h')
        self.assertArgumentParserError(parser, '--help')
        self.assertArgumentParserError(parser, '-v')
        self.assertArgumentParserError(parser, '--version')

    def test_alternate_help_version(self):
        parser = ErrorRaisingArgumentParser()
        parser.add_argument('-x', action='help')
        parser.add_argument('-y', action='version')
        self.assertPrintHelpExit(parser, '-x')
        self.assertPrintVersionExit(parser, '-y')
        self.assertArgumentParserError(parser, '-v')
        self.assertArgumentParserError(parser, '--version')

    def test_help_version_extra_arguments(self):
        parser = ErrorRaisingArgumentParser(version='1.0')
        parser.add_argument('-x', action='store_true')
        parser.add_argument('y')

        # try all combinations of valid prefixes and suffixes
        valid_prefixes = ['', '-x', 'foo', '-x bar', 'baz -x']
        valid_suffixes = valid_prefixes + ['--bad-option', 'foo bar baz']
        for prefix in valid_prefixes:
            for suffix in valid_suffixes:
                format = '%s %%s %s' % (prefix, suffix)
            self.assertPrintHelpExit(parser, format % '-h')
            self.assertPrintHelpExit(parser, format % '--help')
            self.assertPrintVersionExit(parser, format % '-v')
            self.assertPrintVersionExit(parser, format % '--version')


# ======================
# str() and repr() tests
# ======================

class TestStrings(TestCase):
    """Test str()  and repr() on Optionals and Positionals"""

    def assertStringEqual(self, obj, result_string):
        for func in [str, repr]:
            self.assertEqual(func(obj), result_string)

    def test_optional(self):
        option = argparse.Action(
            option_strings=['--foo', '-a', '-b'],
            dest='b',
            type='int',
            nargs='+',
            default=42,
            choices=[1, 2, 3],
            help='HELP',
            metavar='METAVAR')
        string = (
            "Action(option_strings=['--foo', '-a', '-b'], dest='b', "
            "nargs='+', const=None, default=42, type='int', "
            "choices=[1, 2, 3], help='HELP', metavar='METAVAR')")
        self.assertStringEqual(option, string)

    def test_argument(self):
        argument = argparse.Action(
            option_strings=[],
            dest='x',
            type=float,
            nargs='?',
            default=2.5,
            choices=[0.5, 1.5, 2.5],
            help='H HH H',
            metavar='MV MV MV')
        string = (
            "Action(option_strings=[], dest='x', nargs='?', "
            "const=None, default=2.5, type=%r, choices=[0.5, 1.5, 2.5], "
            "help='H HH H', metavar='MV MV MV')" % float)
        self.assertStringEqual(argument, string)

    def test_namespace(self):
        ns = argparse.Namespace(foo=42, bar='spam')
        string = "Namespace(bar='spam', foo=42)"
        self.assertStringEqual(ns, string)

    def test_parser(self):
        parser = argparse.ArgumentParser(prog='PROG')
        string = (
            "ArgumentParser(prog='PROG', usage=None, description=None, "
            "version=None, formatter_class=%r, conflict_handler='error', "
            "add_help=True)" % argparse.HelpFormatter)
        self.assertStringEqual(parser, string)

# ===============
# Namespace tests
# ===============

class TestNamespace(TestCase):

    def test_constructor(self):
        ns = argparse.Namespace()
        self.assertRaises(AttributeError, getattr, ns, 'x')

        ns = argparse.Namespace(a=42, b='spam')
        self.assertEqual(ns.a, 42)
        self.assertEqual(ns.b, 'spam')

    def test_equality(self):
        ns1 = argparse.Namespace(a=1, b=2)
        ns2 = argparse.Namespace(b=2, a=1)
        ns3 = argparse.Namespace(a=1)
        ns4 = argparse.Namespace(b=2)

        self.assertEqual(ns1, ns2)
        self.assertNotEqual(ns1, ns3)
        self.assertNotEqual(ns1, ns4)
        self.assertNotEqual(ns2, ns3)
        self.assertNotEqual(ns2, ns4)
        self.failUnless(ns1 != ns3)
        self.failUnless(ns1 != ns4)
        self.failUnless(ns2 != ns3)
        self.failUnless(ns2 != ns4)


# ===================
# File encoding tests
# ===================

class TestEncoding(TestCase):

    def _test_module_encoding(self, path):
        if path.endswith('.pyc'):
            path = path[:-1]
        codecs.open(path, 'r', 'utf8').read()

    def test_argparse_module_encoding(self):
        self._test_module_encoding(argparse.__file__)

    def test_test_argparse_module_encoding(self):
        self._test_module_encoding(__file__)

# ===================
# ArgumentError tests
# ===================

class TestArgumentError(TestCase):

    def test_argument_error(self):
        msg = "my error here"
        error = argparse.ArgumentError(None, msg)
        self.failUnlessEqual(str(error), msg)

# =======================
# ArgumentTypeError tests
# =======================

class TestArgumentError(TestCase):

    def test_argument_type_error(self):

        def spam(string):
            raise argparse.ArgumentTypeError('spam!')

        parser = ErrorRaisingArgumentParser(prog='PROG', add_help=False)
        parser.add_argument('x', type=spam)
        try:
            parser.parse_args(['XXX'])
        except ArgumentParserError:
            expected = 'usage: PROG x\nPROG: error: argument x: spam!\n'
            msg = str(sys.exc_info()[1])
            self.failUnlessEqual(expected, msg)
        else:
            self.fail()

# ======================
# parse_known_args tests
# ======================

class TestParseKnownArgs(TestCase):

    def test_optionals(self):
        parser = argparse.ArgumentParser()
        parser.add_argument('--foo')
        args, extras = parser.parse_known_args('--foo F --bar --baz'.split())
        self.failUnlessEqual(NS(foo='F'), args)
        self.failUnlessEqual(['--bar', '--baz'], extras)

    def test_mixed(self):
        parser = argparse.ArgumentParser()
        parser.add_argument('-v', nargs='?', const=1, type=int)
        parser.add_argument('--spam', action='store_false')
        parser.add_argument('badger')

        argv = ["B", "C", "--foo", "-v", "3", "4"]
        args, extras = parser.parse_known_args(argv)
        self.failUnlessEqual(NS(v=3, spam=True, badger="B"), args)
        self.failUnlessEqual(["C", "--foo", "4"], extras)

# ============================
# from argparse import * tests
# ============================

class TestImportStar(TestCase):

    def test(self):
        for name in argparse.__all__:
            self.failUnless(hasattr(argparse, name))


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_snapshot
import os
import sys
import logging
from holland.backup.lvm.snapshot import SnapshotLifecycle
from holland.backup.lvm.pylvm import unmount, LVMError, lvremove
from nose.tools import *

# XXX: Setup a LV under ENV[LVM_TEST_VG]
# XXX: Mount LV under ENV[LVM_TEST_MP]
def _test_run():
    fsm = SnapshotLifecycle('/mnt/lvm_test')
    fsm.run()

def test_lvmfsm_misconfigured():
    """Test attempting to snapshot a directory not on an lvm device"""
    fsm = SnapshotLifecycle(target_directory='/home')
    assert_raises(LVMError, fsm.run)

def _test_lv_notfound():
    fsm = SnapshotLifecycle()
    fsm.lvname = 'dba/epicfail' # This LV shouldn't exist
    assert_raises(TypeError, fsm.run)

fileobj = None
def _do_naughty_things(*args):
    global fileobj
    logging.debug("Opening .foo on the mounted snapshot to break unmounting")
    fileobj = open('/tmp/mysnapshot/.foo', 'w')

def _cleanup_naughtiness():
    if not fileobj: return
    fileobj.close()
    unmount('/tmp/mysnapshot')
    lvremove('/dev/vg_test/lv_test_snapshot')

def test_snapshot_error():
    fsm = SnapshotLifecycle(target_directory='/mnt/lvm_test',
                            snapshot_mountpoint='/tmp/mysnapshot/')
    # be naughty and chdir to the snapshot after its mounted
    # this will cause a failure on the unmount phase
    fsm.add_callback('backup', _do_naughty_things)
    assert_raises(LVMError, fsm.run)
test_snapshot_error.teardown = _cleanup_naughtiness

def test_overallocated_snapshot():
    fsm = SnapshotLifecycle(target_directory='/mnt/lvm_test',
                            snapshot_mountpoint='/tmp/mysnapshot/',
                            snapshot_size='768M')
    assert_raises(EnvironmentError, fsm.run)

def test_bad_snapshot_mountpoint():
    fsm = SnapshotLifecycle(target_directory='/mnt/lvm_test',
                            snapshot_mountpoint='/tmp/foo/bar/baz')
    assert_raises(EnvironmentError, fsm.run)

def _do_remount(snapshot):
    snapshot.mount('/tmp/mysnapshot')

def test_bad_remove():
    fsm = SnapshotLifecycle(target_directory='/mnt/lvm_test',
                            snapshot_mountpoint='/tmp/mysnapshot')
    fsm.add_callback('preremove', _do_remount)
    assert_raises(AssertionError, fsm.run)

def test_good_snapshot():
    fsm = SnapshotLifecycle(target_directory='/mnt/lvm_test',
                            snapshot_mountpoint='/tmp/mysnapshot')
    fsm.run()

########NEW FILE########
__FILENAME__ = test_subprocess
import unittest
from test import test_support
import holland.backup.lvm.py23.subprocess23 as subprocess
import sys
import signal
import os
import tempfile
import time
import re

__test__ = False

mswindows = (sys.platform == "win32")

#
# Depends on the following external programs: Python
#

if mswindows:
    SETBINARY = ('import msvcrt; msvcrt.setmode(sys.stdout.fileno(), '
                                                'os.O_BINARY);')
else:
    SETBINARY = ''

# In a debug build, stuff like "[6580 refs]" is printed to stderr at
# shutdown time.  That frustrates tests trying to check stderr produced
# from a spawned Python process.
def remove_stderr_debug_decorations(stderr):
    return re.sub(r"\[\d+ refs\]\r?\n?$", "", stderr)

class ProcessTestCase(unittest.TestCase):
    def setUp(self):
        # Try to minimize the number of children we have so this test
        # doesn't crash on some buildbots (Alphas in particular).
        if hasattr(test_support, "reap_children"):
            test_support.reap_children()

    def tearDown(self):
        # Try to minimize the number of children we have so this test
        # doesn't crash on some buildbots (Alphas in particular).
        if hasattr(test_support, "reap_children"):
            test_support.reap_children()

    def mkstemp(self):
        """wrapper for mkstemp, calling mktemp if mkstemp is not available"""
        if hasattr(tempfile, "mkstemp"):
            return tempfile.mkstemp()
        else:
            fname = tempfile.mktemp()
            return os.open(fname, os.O_RDWR|os.O_CREAT), fname

    #
    # Generic tests
    #
    def test_call_seq(self):
        # call() function with sequence argument
        rc = subprocess.call([sys.executable, "-c",
                              "import sys; sys.exit(47)"])
        self.assertEqual(rc, 47)

    def test_check_call_zero(self):
        # check_call() function with zero return code
        rc = subprocess.check_call([sys.executable, "-c",
                                    "import sys; sys.exit(0)"])
        self.assertEqual(rc, 0)

    def test_check_call_nonzero(self):
        # check_call() function with non-zero return code
        try:
            subprocess.check_call([sys.executable, "-c",
                                   "import sys; sys.exit(47)"])
        except subprocess.CalledProcessError, e:
            self.assertEqual(e.returncode, 47)
        else:
            self.fail("Expected CalledProcessError")

    def test_call_kwargs(self):
        # call() function with keyword args
        newenv = os.environ.copy()
        newenv["FRUIT"] = "banana"
        rc = subprocess.call([sys.executable, "-c",
                          'import sys, os;' \
                          'sys.exit(os.getenv("FRUIT")=="banana")'],
                        env=newenv)
        self.assertEqual(rc, 1)

    def test_stdin_none(self):
        # .stdin is None when not redirected
        p = subprocess.Popen([sys.executable, "-c", 'print "banana"'],
                         stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        p.wait()
        self.assertEqual(p.stdin, None)

    def test_stdout_none(self):
        # .stdout is None when not redirected
        p = subprocess.Popen([sys.executable, "-c",
                             'print "    this bit of output is from a '
                             'test of stdout in a different '
                             'process ..."'],
                             stdin=subprocess.PIPE, stderr=subprocess.PIPE)
        p.wait()
        self.assertEqual(p.stdout, None)

    def test_stderr_none(self):
        # .stderr is None when not redirected
        p = subprocess.Popen([sys.executable, "-c", 'print "banana"'],
                         stdin=subprocess.PIPE, stdout=subprocess.PIPE)
        p.wait()
        self.assertEqual(p.stderr, None)

    def test_executable(self):
        p = subprocess.Popen(["somethingyoudonthave",
                              "-c", "import sys; sys.exit(47)"],
                             executable=sys.executable)
        p.wait()
        self.assertEqual(p.returncode, 47)

    def test_stdin_pipe(self):
        # stdin redirection
        p = subprocess.Popen([sys.executable, "-c",
                         'import sys; sys.exit(sys.stdin.read() == "pear")'],
                        stdin=subprocess.PIPE)
        p.stdin.write("pear")
        p.stdin.close()
        p.wait()
        self.assertEqual(p.returncode, 1)

    def test_stdin_filedes(self):
        # stdin is set to open file descriptor
        tf = tempfile.TemporaryFile()
        d = tf.fileno()
        os.write(d, "pear")
        os.lseek(d, 0, 0)
        p = subprocess.Popen([sys.executable, "-c",
                         'import sys; sys.exit(sys.stdin.read() == "pear")'],
                         stdin=d)
        p.wait()
        self.assertEqual(p.returncode, 1)

    def test_stdin_fileobj(self):
        # stdin is set to open file object
        tf = tempfile.TemporaryFile()
        tf.write("pear")
        tf.seek(0)
        p = subprocess.Popen([sys.executable, "-c",
                         'import sys; sys.exit(sys.stdin.read() == "pear")'],
                         stdin=tf)
        p.wait()
        self.assertEqual(p.returncode, 1)

    def test_stdout_pipe(self):
        # stdout redirection
        p = subprocess.Popen([sys.executable, "-c",
                          'import sys; sys.stdout.write("orange")'],
                         stdout=subprocess.PIPE)
        self.assertEqual(p.stdout.read(), "orange")

    def test_stdout_filedes(self):
        # stdout is set to open file descriptor
        tf = tempfile.TemporaryFile()
        d = tf.fileno()
        p = subprocess.Popen([sys.executable, "-c",
                          'import sys; sys.stdout.write("orange")'],
                         stdout=d)
        p.wait()
        os.lseek(d, 0, 0)
        self.assertEqual(os.read(d, 1024), "orange")

    def test_stdout_fileobj(self):
        # stdout is set to open file object
        tf = tempfile.TemporaryFile()
        p = subprocess.Popen([sys.executable, "-c",
                          'import sys; sys.stdout.write("orange")'],
                         stdout=tf)
        p.wait()
        tf.seek(0)
        self.assertEqual(tf.read(), "orange")

    def test_stderr_pipe(self):
        # stderr redirection
        p = subprocess.Popen([sys.executable, "-c",
                          'import sys; sys.stderr.write("strawberry")'],
                         stderr=subprocess.PIPE)
        self.assertEqual(remove_stderr_debug_decorations(p.stderr.read()),
                         "strawberry")

    def test_stderr_filedes(self):
        # stderr is set to open file descriptor
        tf = tempfile.TemporaryFile()
        d = tf.fileno()
        p = subprocess.Popen([sys.executable, "-c",
                          'import sys; sys.stderr.write("strawberry")'],
                         stderr=d)
        p.wait()
        os.lseek(d, 0, 0)
        self.assertEqual(remove_stderr_debug_decorations(os.read(d, 1024)),
                         "strawberry")

    def test_stderr_fileobj(self):
        # stderr is set to open file object
        tf = tempfile.TemporaryFile()
        p = subprocess.Popen([sys.executable, "-c",
                          'import sys; sys.stderr.write("strawberry")'],
                         stderr=tf)
        p.wait()
        tf.seek(0)
        self.assertEqual(remove_stderr_debug_decorations(tf.read()),
                         "strawberry")

    def test_stdout_stderr_pipe(self):
        # capture stdout and stderr to the same pipe
        p = subprocess.Popen([sys.executable, "-c",
                          'import sys;' \
                          'sys.stdout.write("apple");' \
                          'sys.stdout.flush();' \
                          'sys.stderr.write("orange")'],
                         stdout=subprocess.PIPE,
                         stderr=subprocess.STDOUT)
        output = p.stdout.read()
        stripped = remove_stderr_debug_decorations(output)
        self.assertEqual(stripped, "appleorange")

    def test_stdout_stderr_file(self):
        # capture stdout and stderr to the same open file
        tf = tempfile.TemporaryFile()
        p = subprocess.Popen([sys.executable, "-c",
                          'import sys;' \
                          'sys.stdout.write("apple");' \
                          'sys.stdout.flush();' \
                          'sys.stderr.write("orange")'],
                         stdout=tf,
                         stderr=tf)
        p.wait()
        tf.seek(0)
        output = tf.read()
        stripped = remove_stderr_debug_decorations(output)
        self.assertEqual(stripped, "appleorange")

    def test_stdout_filedes_of_stdout(self):
        # stdout is set to 1 (#1531862).
        cmd = r"import sys, os; sys.exit(os.write(sys.stdout.fileno(), '.\n'))"
        rc = subprocess.call([sys.executable, "-c", cmd], stdout=1)
        self.assertEquals(rc, 2)

    def test_cwd(self):
        tmpdir = os.getenv("TEMP", "/tmp")
        # We cannot use os.path.realpath to canonicalize the path,
        # since it doesn't expand Tru64 {memb} strings. See bug 1063571.
        cwd = os.getcwd()
        os.chdir(tmpdir)
        tmpdir = os.getcwd()
        os.chdir(cwd)
        p = subprocess.Popen([sys.executable, "-c",
                          'import sys,os;' \
                          'sys.stdout.write(os.getcwd())'],
                         stdout=subprocess.PIPE,
                         cwd=tmpdir)
        normcase = os.path.normcase
        self.assertEqual(normcase(p.stdout.read()), normcase(tmpdir))

    def test_env(self):
        newenv = os.environ.copy()
        newenv["FRUIT"] = "orange"
        p = subprocess.Popen([sys.executable, "-c",
                          'import sys,os;' \
                          'sys.stdout.write(os.getenv("FRUIT"))'],
                         stdout=subprocess.PIPE,
                         env=newenv)
        self.assertEqual(p.stdout.read(), "orange")

    def test_communicate_stdin(self):
        p = subprocess.Popen([sys.executable, "-c",
                              'import sys; sys.exit(sys.stdin.read() == "pear")'],
                             stdin=subprocess.PIPE)
        p.communicate("pear")
        self.assertEqual(p.returncode, 1)

    def test_communicate_stdout(self):
        p = subprocess.Popen([sys.executable, "-c",
                              'import sys; sys.stdout.write("pineapple")'],
                             stdout=subprocess.PIPE)
        (stdout, stderr) = p.communicate()
        self.assertEqual(stdout, "pineapple")
        self.assertEqual(stderr, None)

    def test_communicate_stderr(self):
        p = subprocess.Popen([sys.executable, "-c",
                              'import sys; sys.stderr.write("pineapple")'],
                             stderr=subprocess.PIPE)
        (stdout, stderr) = p.communicate()
        self.assertEqual(stdout, None)
        # When running with a pydebug build, the # of references is outputted
        # to stderr, so just check if stderr at least started with "pinapple"
        self.assert_(stderr.startswith("pineapple"))

    def test_communicate(self):
        p = subprocess.Popen([sys.executable, "-c",
                          'import sys,os;' \
                          'sys.stderr.write("pineapple");' \
                          'sys.stdout.write(sys.stdin.read())'],
                         stdin=subprocess.PIPE,
                         stdout=subprocess.PIPE,
                         stderr=subprocess.PIPE)
        (stdout, stderr) = p.communicate("banana")
        self.assertEqual(stdout, "banana")
        self.assertEqual(remove_stderr_debug_decorations(stderr),
                         "pineapple")

    # This test is Linux specific for simplicity to at least have
    # some coverage.  It is not a platform specific bug.
    if os.path.isdir('/proc/%d/fd' % os.getpid()):
        # Test for the fd leak reported in http://bugs.python.org/issue2791.
        def test_communicate_pipe_fd_leak(self):
            fd_directory = '/proc/%d/fd' % os.getpid()
            num_fds_before_popen = len(os.listdir(fd_directory))
            p = subprocess.Popen([sys.executable, '-c', 'print()'],
                                 stdout=subprocess.PIPE)
            p.communicate()
            num_fds_after_communicate = len(os.listdir(fd_directory))
            del p
            num_fds_after_destruction = len(os.listdir(fd_directory))
            self.assertEqual(num_fds_before_popen, num_fds_after_destruction)
            self.assertEqual(num_fds_before_popen, num_fds_after_communicate)

    def test_communicate_returns(self):
        # communicate() should return None if no redirection is active
        p = subprocess.Popen([sys.executable, "-c",
                              "import sys; sys.exit(47)"])
        (stdout, stderr) = p.communicate()
        self.assertEqual(stdout, None)
        self.assertEqual(stderr, None)

    def test_communicate_pipe_buf(self):
        # communicate() with writes larger than pipe_buf
        # This test will probably deadlock rather than fail, if
        # communicate() does not work properly.
        x, y = os.pipe()
        if mswindows:
            pipe_buf = 512
        else:
            pipe_buf = os.fpathconf(x, "PC_PIPE_BUF")
        os.close(x)
        os.close(y)
        p = subprocess.Popen([sys.executable, "-c",
                          'import sys,os;'
                          'sys.stdout.write(sys.stdin.read(47));' \
                          'sys.stderr.write("xyz"*%d);' \
                          'sys.stdout.write(sys.stdin.read())' % pipe_buf],
                         stdin=subprocess.PIPE,
                         stdout=subprocess.PIPE,
                         stderr=subprocess.PIPE)
        string_to_write = "abc"*pipe_buf
        (stdout, stderr) = p.communicate(string_to_write)
        self.assertEqual(stdout, string_to_write)

    def test_writes_before_communicate(self):
        # stdin.write before communicate()
        p = subprocess.Popen([sys.executable, "-c",
                          'import sys,os;' \
                          'sys.stdout.write(sys.stdin.read())'],
                         stdin=subprocess.PIPE,
                         stdout=subprocess.PIPE,
                         stderr=subprocess.PIPE)
        p.stdin.write("banana")
        (stdout, stderr) = p.communicate("split")
        self.assertEqual(stdout, "bananasplit")
        self.assertEqual(remove_stderr_debug_decorations(stderr), "")

    def test_universal_newlines(self):
        p = subprocess.Popen([sys.executable, "-c",
                          'import sys,os;' + SETBINARY +
                          'sys.stdout.write("line1\\n");'
                          'sys.stdout.flush();'
                          'sys.stdout.write("line2\\r");'
                          'sys.stdout.flush();'
                          'sys.stdout.write("line3\\r\\n");'
                          'sys.stdout.flush();'
                          'sys.stdout.write("line4\\r");'
                          'sys.stdout.flush();'
                          'sys.stdout.write("\\nline5");'
                          'sys.stdout.flush();'
                          'sys.stdout.write("\\nline6");'],
                         stdout=subprocess.PIPE,
                         universal_newlines=1)
        stdout = p.stdout.read()
        if hasattr(file, 'newlines'):
            # Interpreter with universal newline support
            self.assertEqual(stdout,
                             "line1\nline2\nline3\nline4\nline5\nline6")
        else:
            # Interpreter without universal newline support
            self.assertEqual(stdout,
                             "line1\nline2\rline3\r\nline4\r\nline5\nline6")

    def test_universal_newlines_communicate(self):
        # universal newlines through communicate()
        p = subprocess.Popen([sys.executable, "-c",
                          'import sys,os;' + SETBINARY +
                          'sys.stdout.write("line1\\n");'
                          'sys.stdout.flush();'
                          'sys.stdout.write("line2\\r");'
                          'sys.stdout.flush();'
                          'sys.stdout.write("line3\\r\\n");'
                          'sys.stdout.flush();'
                          'sys.stdout.write("line4\\r");'
                          'sys.stdout.flush();'
                          'sys.stdout.write("\\nline5");'
                          'sys.stdout.flush();'
                          'sys.stdout.write("\\nline6");'],
                         stdout=subprocess.PIPE, stderr=subprocess.PIPE,
                         universal_newlines=1)
        (stdout, stderr) = p.communicate()
        if hasattr(file, 'newlines'):
            # Interpreter with universal newline support
            self.assertEqual(stdout,
                             "line1\nline2\nline3\nline4\nline5\nline6")
        else:
            # Interpreter without universal newline support
            self.assertEqual(stdout, "line1\nline2\rline3\r\nline4\r\nline5\nline6")

    def test_no_leaking(self):
        # Make sure we leak no resources
        if not hasattr(test_support, "is_resource_enabled") \
               or test_support.is_resource_enabled("subprocess") and not mswindows:
            max_handles = 1026 # too much for most UNIX systems
        else:
            max_handles = 65
        for i in range(max_handles):
            p = subprocess.Popen([sys.executable, "-c",
                    "import sys;sys.stdout.write(sys.stdin.read())"],
                    stdin=subprocess.PIPE,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE)
            data = p.communicate("lime")[0]
            self.assertEqual(data, "lime")


    def test_list2cmdline(self):
        self.assertEqual(subprocess.list2cmdline(['a b c', 'd', 'e']),
                         '"a b c" d e')
        self.assertEqual(subprocess.list2cmdline(['ab"c', '\\', 'd']),
                         'ab\\"c \\ d')
        self.assertEqual(subprocess.list2cmdline(['a\\\\\\b', 'de fg', 'h']),
                         'a\\\\\\b "de fg" h')
        self.assertEqual(subprocess.list2cmdline(['a\\"b', 'c', 'd']),
                         'a\\\\\\"b c d')
        self.assertEqual(subprocess.list2cmdline(['a\\\\b c', 'd', 'e']),
                         '"a\\\\b c" d e')
        self.assertEqual(subprocess.list2cmdline(['a\\\\b\\ c', 'd', 'e']),
                         '"a\\\\b\\ c" d e')
        self.assertEqual(subprocess.list2cmdline(['ab', '']),
                         'ab ""')


    def test_poll(self):
        p = subprocess.Popen([sys.executable,
                          "-c", "import time; time.sleep(1)"])
        count = 0
        while p.poll() is None:
            time.sleep(0.1)
            count += 1
        # We expect that the poll loop probably went around about 10 times,
        # but, based on system scheduling we can't control, it's possible
        # poll() never returned None.  It "should be" very rare that it
        # didn't go around at least twice.
        self.assert_(count >= 2)
        # Subsequent invocations should just return the returncode
        self.assertEqual(p.poll(), 0)


    def test_wait(self):
        p = subprocess.Popen([sys.executable,
                          "-c", "import time; time.sleep(2)"])
        self.assertEqual(p.wait(), 0)
        # Subsequent invocations should just return the returncode
        self.assertEqual(p.wait(), 0)


    def test_invalid_bufsize(self):
        # an invalid type of the bufsize argument should raise
        # TypeError.
        try:
            subprocess.Popen([sys.executable, "-c", "pass"], "orange")
        except TypeError:
            pass
        else:
            self.fail("Expected TypeError")

    #
    # POSIX tests
    #
    if not mswindows:
        def test_exceptions(self):
            # catched & re-raised exceptions
            try:
                p = subprocess.Popen([sys.executable, "-c", ""],
                                 cwd="/this/path/does/not/exist")
            except OSError, e:
                # The attribute child_traceback should contain "os.chdir"
                # somewhere.
                self.assertNotEqual(e.child_traceback.find("os.chdir"), -1)
            else:
                self.fail("Expected OSError")

        def _suppress_core_files(self):
            """Try to prevent core files from being created.
            Returns previous ulimit if successful, else None.
            """
            try:
                import resource
                old_limit = resource.getrlimit(resource.RLIMIT_CORE)
                resource.setrlimit(resource.RLIMIT_CORE, (0,0))
                return old_limit
            except (ImportError, ValueError, resource.error):
                return None

        def _unsuppress_core_files(self, old_limit):
            """Return core file behavior to default."""
            if old_limit is None:
                return
            try:
                import resource
                resource.setrlimit(resource.RLIMIT_CORE, old_limit)
            except (ImportError, ValueError, resource.error):
                return

        def test_run_abort(self):
            # returncode handles signal termination
            old_limit = self._suppress_core_files()
            try:
                p = subprocess.Popen([sys.executable,
                                      "-c", "import os; os.abort()"])
            finally:
                self._unsuppress_core_files(old_limit)
            p.wait()
            self.assertEqual(-p.returncode, signal.SIGABRT)

        def test_preexec(self):
            # preexec function
            p = subprocess.Popen([sys.executable, "-c",
                              'import sys,os;' \
                              'sys.stdout.write(os.getenv("FRUIT"))'],
                             stdout=subprocess.PIPE,
                             preexec_fn=lambda: os.putenv("FRUIT", "apple"))
            self.assertEqual(p.stdout.read(), "apple")

        def test_args_string(self):
            # args is a string
            f, fname = self.mkstemp()
            os.write(f, "#!/bin/sh\n")
            os.write(f, "exec %s -c 'import sys; sys.exit(47)'\n" %
                        sys.executable)
            os.close(f)
            os.chmod(fname, 0700)
            p = subprocess.Popen(fname)
            p.wait()
            os.remove(fname)
            self.assertEqual(p.returncode, 47)

        def test_invalid_args(self):
            # invalid arguments should raise ValueError
            self.assertRaises(ValueError, subprocess.call,
                              [sys.executable,
                               "-c", "import sys; sys.exit(47)"],
                              startupinfo=47)
            self.assertRaises(ValueError, subprocess.call,
                              [sys.executable,
                               "-c", "import sys; sys.exit(47)"],
                              creationflags=47)

        def test_shell_sequence(self):
            # Run command through the shell (sequence)
            newenv = os.environ.copy()
            newenv["FRUIT"] = "apple"
            p = subprocess.Popen(["echo $FRUIT"], shell=1,
                                 stdout=subprocess.PIPE,
                                 env=newenv)
            self.assertEqual(p.stdout.read().strip(), "apple")

        def test_shell_string(self):
            # Run command through the shell (string)
            newenv = os.environ.copy()
            newenv["FRUIT"] = "apple"
            p = subprocess.Popen("echo $FRUIT", shell=1,
                                 stdout=subprocess.PIPE,
                                 env=newenv)
            self.assertEqual(p.stdout.read().strip(), "apple")

        def test_call_string(self):
            # call() function with string argument on UNIX
            f, fname = self.mkstemp()
            os.write(f, "#!/bin/sh\n")
            os.write(f, "exec %s -c 'import sys; sys.exit(47)'\n" %
                        sys.executable)
            os.close(f)
            os.chmod(fname, 0700)
            rc = subprocess.call(fname)
            os.remove(fname)
            self.assertEqual(rc, 47)


    #
    # Windows tests
    #
    if mswindows:
        def test_startupinfo(self):
            # startupinfo argument
            # We uses hardcoded constants, because we do not want to
            # depend on win32all.
            STARTF_USESHOWWINDOW = 1
            SW_MAXIMIZE = 3
            startupinfo = subprocess.STARTUPINFO()
            startupinfo.dwFlags = STARTF_USESHOWWINDOW
            startupinfo.wShowWindow = SW_MAXIMIZE
            # Since Python is a console process, it won't be affected
            # by wShowWindow, but the argument should be silently
            # ignored
            subprocess.call([sys.executable, "-c", "import sys; sys.exit(0)"],
                        startupinfo=startupinfo)

        def test_creationflags(self):
            # creationflags argument
            CREATE_NEW_CONSOLE = 16
            sys.stderr.write("    a DOS box should flash briefly ...\n")
            subprocess.call(sys.executable +
                                ' -c "import time; time.sleep(0.25)"',
                            creationflags=CREATE_NEW_CONSOLE)

        def test_invalid_args(self):
            # invalid arguments should raise ValueError
            self.assertRaises(ValueError, subprocess.call,
                              [sys.executable,
                               "-c", "import sys; sys.exit(47)"],
                              preexec_fn=lambda: 1)
            self.assertRaises(ValueError, subprocess.call,
                              [sys.executable,
                               "-c", "import sys; sys.exit(47)"],
                              close_fds=True)

        def test_shell_sequence(self):
            # Run command through the shell (sequence)
            newenv = os.environ.copy()
            newenv["FRUIT"] = "physalis"
            p = subprocess.Popen(["set"], shell=1,
                                 stdout=subprocess.PIPE,
                                 env=newenv)
            self.assertNotEqual(p.stdout.read().find("physalis"), -1)

        def test_shell_string(self):
            # Run command through the shell (string)
            newenv = os.environ.copy()
            newenv["FRUIT"] = "physalis"
            p = subprocess.Popen("set", shell=1,
                                 stdout=subprocess.PIPE,
                                 env=newenv)
            self.assertNotEqual(p.stdout.read().find("physalis"), -1)

        def test_call_string(self):
            # call() function with string argument on Windows
            rc = subprocess.call(sys.executable +
                                 ' -c "import sys; sys.exit(47)"')
            self.assertEqual(rc, 47)


def test_main():
    test_support.run_unittest(ProcessTestCase)
    if hasattr(test_support, "reap_children"):
        test_support.reap_children()

if __name__ == "__main__":
    test_main()

########NEW FILE########
__FILENAME__ = test_util
import os
from holland.backup.lvm.util import *
from nose.tools import *

def test_format_bytes():
    ok_(format_bytes(1024,0) == '1KB')
    ok_(format_bytes(0) == '0.00Bytes')

def test_relpath():
    ok_(relpath('/tmp/mysql/data', '/tmp/mysql') == 'data')
    ok_(relpath('/tmp/mysql', '/tmp/mysql') == '.')
    assert_raises(ValueError, relpath, None)

def test_getdevice():
    assert_equal(getdevice('/tmp'),
                 os.getenv('TMPDEV', ''),
                 msg="getdevice(/tmp) = %r but expected %r" % \
                    (getdevice('/tmp'), os.getenv('TMPDEV'))
                )
    ok_(getdevice('/NoSuchDirectory') is None)

def test_getmount():
    ok_(getmount('/tmp/mysql') == getmount('/tmp'))
    ok_(getmount('/') == '/')

########NEW FILE########
__FILENAME__ = base
# -*- coding: utf-8 -*-
"""Backup functions for pg_dump"""

# Python stdlib
import os
import shlex
import tempfile
import logging
import subprocess

# 3rd party Postgres db connector
import psycopg2 as dbapi
import psycopg2.extensions

# holland-core has a few nice utilities such as format_bytes
from holland.core.util.fmt import format_bytes
# Holland general compression functions
from holland.lib.compression import open_stream
# holland-common safefilename encoding
from holland.lib.safefilename import encode as encode_safe

LOG = logging.getLogger(__name__)

class PgError(Exception):
    """Raised when any error associated with Postgres occurs"""

def get_connection(config, db='template1'):
    psycopg2.extensions.register_type(psycopg2.extensions.UNICODE)
    args = {}
    # remap pgauth parameters to what psycopg2.connect accepts
    remap = { 'hostname' : 'host', 'username' : 'user' }
    for key in ('hostname', 'port', 'username', 'password'):
        value = config['pgauth'].get(key)
        key = remap.get(key, key)
        if value is not None:
            args[key] = value
    connection = dbapi.connect(database=db, **args)
    if not connection:
        raise PgError("Failed to connect to the Postgres database.")

    if config["pgdump"]["role"]:
        try:
            cursor = connection.cursor()
            cursor.execute("SET ROLE %s" % config["pgdump"]["role"])
        except:
            raise PgError("Failed to set role to " + config["pgdump"]["role"])

    global ver
    ver = connection.get_parameter_status('server_version')
    LOG.info("Server version " + ver)

    return connection

def get_db_size(dbname, connection):
    try:
        cursor = connection.cursor()
        cursor.execute("SELECT pg_database_size('%s')" % dbname)
        size = int(cursor.fetchone()[0])
        LOG.info("DB %s size %s", dbname, format_bytes(size))
        return size
    except:
        raise PgError("Could not detmine database size.")

def legacy_get_db_size(dbname, connection):
    cursor = connection.cursor()
    cursor.execute('SELECT SUM(relpages*8192) FROM pg_class')
    size = int(cursor.fetchone()[0])
    LOG.info("DB %s size %s", dbname, format_bytes(size))
    cursor.close()
    return size

def pg_databases(config, connection):
    """Find the databases available in the Postgres cluster specified
    in config['pgpass']
    """
    cursor = connection.cursor()
    cursor.execute("SELECT datname FROM pg_database WHERE not datistemplate and datallowconn")
    databases = [db for db, in cursor]
    cursor.close()
    logging.debug("pg_databases() -> %r", databases)
    return databases

def run_pgdump(dbname, output_stream, connection_params, format='custom', env=None):
    """Run pg_dump for the given database and write to the specified output
    stream.

    :param db: database name
    :type db: str
    :param output_stream: a file-like object - must have a fileno attribute
                          that is a real, open file descriptor
    """
    args = [ 'pg_dump' ] + connection_params + [
        '--format', format,
        dbname
    ]

    LOG.info('%s > %s', subprocess.list2cmdline(args),
                        output_stream.name)

    stderr = tempfile.TemporaryFile()
    returncode = subprocess.call(args,
                                 stdout=output_stream,
                                 stderr=stderr,
                                 env=env,
                                 close_fds=True)
    stderr.flush()
    stderr.seek(0)
    for line in stderr:
        LOG.error('%s', line.rstrip())
    stderr.close()

    if returncode != 0:
        raise OSError("%s failed." %
                      subprocess.list2cmdline(args))

def backup_globals(backup_directory, config, connection_params, env=None):
    """Backup global Postgres data that wouldn't otherwise
    be captured by pg_dump.

    Runs pg_dumpall -g > $backup_dir/globals.sql

    :param backup_directory: directory to save pg_dump output to
    :param config: PgDumpPlugin config dictionary
    :raises: OSError, PgError on error
    """

    path = os.path.join(backup_directory, 'global.sql')
    zopts = config['compression']
    output_stream = open_stream(path, 'w', 
                                method=zopts['method'],
                                level=zopts['level'],
                                extra_args=zopts['options'])

    args = [
        'pg_dumpall',
        '-g',
    ] + connection_params

    LOG.info('%s > %s', subprocess.list2cmdline(args),
                        output_stream.name)
    stderr = tempfile.TemporaryFile()
    returncode = subprocess.call(args,
                                 stdout=output_stream,
                                 stderr=stderr,
                                 env=env,
                                 close_fds=True)
    output_stream.close()
    stderr.flush()
    stderr.seek(0)
    for line in stderr:
        LOG.error('%s', line.rstrip())
    stderr.close()

    if returncode != 0:
        raise PgError("pg_dumpall command exited with failure code %d." %
                      returncode)

def generate_manifest(backups, path):
    manifest = open(os.path.join(path, 'MANIFEST'), 'w')
    for dbname, dumpfile in backups:
        try:
            print >>manifest, "%s\t%s" % (dbname.encode('utf8'),
                                          os.path.basename(dumpfile))
        except UnicodeError, exc:
            LOG.error("Failed to encode dbname %s: %s", dbname, exc)
    manifest.close()

def pgauth2args(config):
    args = []
    remap = { 'hostname' : 'host' }
    for param in ('hostname', 'port', 'username'):
        value = config['pgauth'].get(param)
        key = remap.get(param, param)
        if value is not None:
            args.extend(['--%s' % key, str(value)])

    # FIXME: --role only works on 8.4+
    if config['pgdump']['role']:
        if ver >= '8.4':
            args.extend(['--role', config['pgdump']['role']])
        else:
            raise PgError("The --role option is available only in Postgres versions 8.4 and higher.")

    return args

def pg_extra_options(config):
    args = []
    # normal compression doesn't make sense with --format=custom
    # use pg_dump's builtin --compress option instead
    if config['pgdump']['format'] == 'custom':
        LOG.info("Ignore compression method, since custom format is in use.")
        config['compression']['method'] = 'none'
        args += ['--compress',
                 str(config['compression']['level'])]
    additional_options = config['pgdump']['additional-options']
    if additional_options:
        # XXX: we may want to check these options more carefully and warn as appropriate.
        additional_options = additional_options.encode('utf8')
        args += shlex.split(additional_options)
    return args

def generate_pgpassfile(backup_directory, password):
    fileobj = open(os.path.join(backup_directory, 'pgpass'), 'w')
    # pgpass should always be 0600
    os.chmod(fileobj.name, 0600)
    fileobj.write('*:*:*:*:%s' % password)
    fileobj.close()
    return fileobj.name

def backup_pgsql(backup_directory, config, databases):
    """Backup databases in a Postgres instance

    :param backup_directory: directory to save pg_dump output to
    :param config: PgDumpPlugin config dictionary
    :raises: OSError, PgError on error
    """
    connection_params = pgauth2args(config)
    extra_options = pg_extra_options(config)

    pgenv = dict(os.environ)

    if config['pgauth']['password'] is not None:
        pgpass_file = generate_pgpassfile(backup_directory,
                                          config['pgauth']['password'])
        if 'PGPASSFILE' in pgenv:
            LOG.warn("Overriding PGPASSFILE in environment with %s because "
                     "a password is specified.",
                      pgpass_file)
        pgenv['PGPASSFILE'] = pgpass_file

    backup_globals(backup_directory, config, connection_params, env=pgenv)

    ext_map = {
        'custom' : '.dump',
        'plain' : '.sql',
        'tar' : '.tar',
    }


    backups = []
    for dbname in databases:
        format = config['pgdump']['format']

        dump_name, _ = encode_safe(dbname)
        if dump_name != dbname:
            LOG.warn("Encoded database %s as filename %s", dbname, dump_name)

        filename = os.path.join(backup_directory, dump_name + ext_map[format])

        zopts = config['compression']
        stream = open_stream(filename, 'w',
                             method=zopts['method'],
                             level=zopts['level'],
                             extra_args=zopts['options'])

        backups.append((dbname, stream.name))

        run_pgdump(dbname=dbname,
                   output_stream=stream,
                   connection_params=connection_params + extra_options,
                   format=format,
                   env=pgenv)

        stream.close()

    generate_manifest(backups, backup_directory)

def dry_run(databases, config):
    args = pgauth2args(config)

    LOG.info("pg_dumpall -g")
    for db in databases:
        LOG.info("pg_dump %s --format %s %s",
                 subprocess.list2cmdline(args),
                 config['pgdump']['format'],
                 db)

########NEW FILE########
__FILENAME__ = interface
# -*- coding: utf-8 -*-
"""
Plugin for the Holland backup framework
to backup Postgres databases using pg_dump
and pg_dumpall
"""

import os
import sys
import logging
from tempfile import NamedTemporaryFile
from holland.core.exceptions import BackupError
from holland.backup.pgdump.base import backup_pgsql, dry_run, \
                                       PgError, \
                                       dbapi, \
                                       pg_databases, \
                                       get_connection, get_db_size

LOG = logging.getLogger(__name__)

# This is a specification of what our configuration must include
# values are validate.py functions.  See:
# http://www.voidspace.org.uk/python/validate.html

# NOTE: this configuration isn't actually obeyed by the implementation
#       These are just various options that *might* be useful for pg_dump
#       to support (namely database/schema/table inclusion/exclusion).
# Anyone who picks up this plugin will likely want to trim or add to this
# as it makes sense
CONFIGSPEC = """
[pgdump]
format = option('plain','tar','custom', default='custom')
role = string(default=None)
additional-options = string(default=None)

[compression]
method = option('gzip', 'gzip-rsyncable', 'bzip2', 'pbzip2', 'lzop', 'lzma', 'pigz', 'none', default='gzip')
level = integer(min=0, default=1)
options = string(default="")

[pgauth]
username = string(default=None)
password = string(default=None)
hostname = string(default=None)
port = integer(default=None)
""".splitlines()

class PgDump(object):
    """
    Postgres pg_dump backups
    """

    def __init__(self, name, config, target_directory, dry_run=False):
        """Create a new PgDump instance

        :param name: unique name of this backup (e.g. pg_dump/20100101_000000)
        :param target_directory: where backup files should be stored
        :param dry_run: boolean flag indicating whether we should only go
                        through the motions of a backup without actually
                        performing the heavy weight steps.
        """
        self.name = name
        self.config = config
        self.target_directory = target_directory
        self.dry_run = dry_run
        self.config.validate_config(CONFIGSPEC)

        self.connection = get_connection(self.config)
        self.databases = pg_databases(self.config, self.connection)
        LOG.info("Found databases: %s", ','.join(self.databases))

    def estimate_backup_size(self):
        """Estimate the size (in bytes) of the backup this plugin would
        produce, if run.


        :returns: int. size in bytes
        """

        totalestimate = 0
        for db in self.databases:
            try:
                totalestimate += get_db_size(db, self.connection)
            except dbapi.DatabaseError, exc:
                if exc.pgcode != '42883': # 'missing function'
                    raise BackupError("Failed to estimate database size for "
                                      "%s: %s" % (db, exc))
                totalestimate += self._estimate_legacy_size(db)

        return totalestimate

    def _estimate_legacy_size(self, db):
        try:
            connection = get_connection(self.config, db)
            size = legacy_get_db_size(db, connection)
            connection.close()
            return size
        except dbapi.DatabaseError, exc:
            raise BackupError("Failed to estimate database size for %s: %s" %
                              (db, exc))

    def backup(self):
        """
        Start a backup.
        """

        if self.dry_run:
            # Very simply dry run information
            # enough to know that:
            # 1) We can connect to Postgres using pgpass data
            # 2) The exact databases we would dump
            dry_run(self.databases, self.config)
            return

        # First run a pg_dumpall -g and save the globals
        # Then run a pg_dump for each database we find
        backup_dir = os.path.join(self.target_directory, 'data')

        # put everything in data/
        try:
            os.mkdir(backup_dir)
        except OSError, exc:
            raise BackupError("Failed to create backup directory %s" % backup_dir)

        try:
            backup_pgsql(backup_dir, self.config, self.databases)
        except (OSError, PgError), exc:
            LOG.debug("Failed to backup Postgres. %s",
                          str(exc), exc_info=True)
            raise BackupError(str(exc))

    def configspec(cls):
        """Provide a specification for the configuration dictionary this
        plugin accepts.
        """
        return CONFIGSPEC
    configspec = classmethod(configspec)

    def info(self):
        """Provide extra information about a backup

        :returns: str. Descriptive text about the backup
        """

        return ""

########NEW FILE########
__FILENAME__ = random
import logging
import os
LOG = logging.getLogger(__name__)

CONFIGSPEC="""
[random]
bytes = integer(default=50)
""".splitlines()

class RandomPlugin(object):
    """Back up randomness"""

    def __init__(self, name, config, target_directory, dry_run=False):
        """Create new RandomPlugin instance"""

        self.name = name
        self.config = config
        self.target_directory = target_directory
        self.dry_run = dry_run
        LOG.info("Validating Config")
        self.config.validate_config(CONFIGSPEC)
        self.bytes = self.config['random']['bytes']

    def estimate_backup_size(self):
        return self.bytes

    def backup(self):
        rand = open("/dev/random", "r")
        bytesleft = self.bytes
        data = ''
        while bytesleft > 0:
            r = rand.read(bytesleft)
            data += r
            bytesleft -= len(r)
            LOG.info("Read %d bytes from /dev/random" % len(r))

        outfile = os.path.join(self.target_directory, 'random_data')
	f = open(outfile, "w")
	f.write(data)
        f.close()
        LOG.info("Wrote to "+outfile)


########NEW FILE########
__FILENAME__ = sqlite
"""SQLite backup plugin for Holland."""

import os
import logging
from subprocess import Popen, PIPE

from holland.lib.compression import open_stream
from holland.core.exceptions import BackupError

LOG = logging.getLogger(__name__)

CONFIGSPEC="""
[sqlite]
databases = force_list(default=list())
binary = string(default=/usr/bin/sqlite3)

[compression]
method = option('none', 'gzip', 'gzip-rsyncable', 'pigz', 'bzip2', 'pbzip2', 'lzop', default='gzip')
inline = boolean(default=yes)
level = integer(min=0, max=9, default=1)
""".splitlines()

class SQLitePlugin(object):
    def __init__(self, name, config, target_directory, dry_run=False):
        self.name = name
        self.config = config
        self.target_directory = target_directory
        self.dry_run = dry_run
        self.invalid_databases = []
        self.databases = []
        
        LOG.info("Validating config: %s", self.name)
        self.config.validate_config(CONFIGSPEC)
        LOG.debug("Validated config: %r", self.config)
        
        self.sqlite_bin = self.config['sqlite']['binary']
        self.check()
        
    def info(self):
        return "SQLite backup plugin for Holland."
        
    def check(self):
        LOG.info("Checking that SQLite backups can run.")
        if not os.path.exists(self.sqlite_bin):
            raise BackupError, \
                "SQLite binary [%s] doesn't exist!" % self.sqlite_bin    
                
        for db in self.config['sqlite']['databases']:
            # sometimes picks up empty string ('')
            if not db:
                continue
                
            path = os.path.abspath(os.path.expanduser(db))
            if not os.path.exists(path):
                LOG.error("SQLite database [%s] doesn't exist!" % path)
                self.invalid_databases.append(db)
                continue
            
            process = Popen([self.sqlite_bin, path, '.schema'], 
                            stdin=open('/dev/null', 'r'), 
                            stdout=open('/dev/null', 'w'), 
                            stderr=PIPE)
            _, stderroutput = process.communicate()
            
            if process.returncode != 0:
                LOG.error(stderroutput)
                self.invalid_databases.append(db)
            else:
                self.databases.append(db)

        if len(self.databases) == 0 and len(self.invalid_databases) == 0:
            raise BackupError, "No SQLite databases to backup!"
            
    def estimate_backup_size(self):
        """
        Return total estimated size of all databases we are backing up (does 
        not account for post-compression).
        """
        total_size = 0
        for db in self.databases:
            if db in self.invalid_databases:
                continue
            path = os.path.abspath(os.path.expanduser(db))
            total_size += os.path.getsize(path)
        return total_size

    def backup(self):
        """
        Use the internal '.dump' functionality built into SQLite to dump the 
        pure ASCII SQL Text and write that to disk.
        """
        
        zopts = (self.config['compression']['method'], 
                 int(self.config['compression']['level']))
        LOG.info("SQLite binary is [%s]" % self.sqlite_bin)         
        for db in self.databases:
            path = os.path.abspath(os.path.expanduser(db))
            
            if db in self.invalid_databases:
                LOG.warn("Skipping invalid SQLite database at [%s]" % path)
                continue
            
            if self.dry_run:
                LOG.info("Backing up SQLite database at [%s] (dry run)" % path)
                dest = open('/dev/null', 'w')
            else:
                LOG.info("Backing up SQLite database at [%s]" % path)
                dest = os.path.join(self.target_directory, '%s.sql' % \
                                    os.path.basename(path))                    
                dest = open_stream(dest, 'w', *zopts)
                
            process = Popen([self.sqlite_bin, path, '.dump'], 
                            stdin=open('/dev/null', 'r'), stdout=dest, 
                            stderr=PIPE)
            _, stderroutput = process.communicate()
            dest.close()

            if process.returncode != 0:
              LOG.error(stderroutput)
              raise BackupError("SQLite '.dump' of [%s] failed" % path)

        # Raise for invalid databases after we successfully backup the others
        if len(self.invalid_databases) > 0:
            raise BackupError, "Invalid database(s): %s" % self.invalid_databases
            

########NEW FILE########
__FILENAME__ = test_sqlite

import os
import time
import shutil
from tempfile import mkdtemp
from configobj import ConfigObj
from nose.tools import ok_, assert_equals, with_setup

from holland.backup.sqlite import SQLitePlugin
from holland.lib.which import which, WhichError


class MockConfig(ConfigObj):
    def validate_config(self, *args, **kw):
        pass
        
config = MockConfig()
config['sqlite'] = {
    'databases' : [os.path.join(os.path.dirname(__file__), 'sqlite.db')]
    }
config['compression'] = {
    'method': 'gzip', 
    'inline': 'yes', 
    'level': 1
    }

try:
    config['sqlite']['binary'] = which('sqlite')    
except WhichError, e:
    try:
        config['sqlite']['binary'] = which('sqlite3')
    except WhichError, e:
        raise Exception, "Unable to find sqlite binary"
    
    
def setup_func():
    "set up test fixtures"
    config['tmpdir'] = mkdtemp()

def teardown_func():
    "tear down test fixtures"
    if os.path.exists(config['tmpdir']):
        shutil.rmtree(config['tmpdir'])
    
@with_setup(setup_func, teardown_func)    
def test_sqlite_dry_run():
    name = 'sqlite/' + time.strftime('%Y%m%d_%H%M%S')
    dry_run = True
    plugin = SQLitePlugin(name, config, config['tmpdir'], dry_run)
    plugin.backup()

@with_setup(setup_func, teardown_func)
def test_sqlite_plugin():
    name = 'sqlite/' + time.strftime('%Y%m%d_%H%M%S')
    dry_run = False
    plugin = SQLitePlugin(name, config, config['tmpdir'], dry_run)
    assert_equals(plugin.estimate_backup_size(), 2048)
    plugin.backup()

@with_setup(setup_func, teardown_func)
def test_sqlite_info():
    name = 'sqlite/' + time.strftime('%Y%m%d_%H%M%S')
    dry_run = False
    plugin = SQLitePlugin(name, config, config['tmpdir'], dry_run)
    ok_(isinstance(plugin.info(), basestring))
    
########NEW FILE########
__FILENAME__ = mysql
"""
holland.backup.xtrabackup.mysql
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Simple mysql client wrapper
"""
import MySQLdb

class MySQL(object):
    MySQLError = MySQLdb.MySQLError

    def __init__(self, *args, **kwargs):
        self._connection = MySQLdb.connect(*args, **kwargs)

    def execute(self, sql, *args):
        cursor = self.cursor()
        try:
            return cursor.execute(sql, args)
        finally:
            cursor.close()

    def scalar(self, sql, *args):
        cursor = self.cursor()
        try:
            if cursor.execute(sql, args):
                return cursor.fetchone()[0]
            else:
                return None
        finally:
            cursor.close()

    def first(self, sql, *args):
        cursor = self.cursor()
        try:
            cursor.execute(sql, args)
            return cursor.fetchone()
        finally:
            cursor.close()

    def cursor(self):
        return self._connection.cursor()

    def from_defaults(cls, defaults_file):
        return cls(read_default_file=defaults_file)
    from_defaults = classmethod(from_defaults)

    def var(self, var, scope='SESSION'):
        scope = scope.upper()
        if scope not in ('SESSION', 'GLOBAL'):
            raise BackupError("Invalid variable scope used")
        var = var.replace('%', '\\%').replace('_', '\\_')
        sql = "SHOW %s VARIABLES LIKE '%s'" % (scope, var)
        try:
            return self.first(sql)[1]
        except IndexError:
            return None

    def close(self):
        try:
            return self._connection.close()
        finally:
            self._connection = None

########NEW FILE########
__FILENAME__ = plugin
"""
holland.mysql.xtrabackup
~~~~~~~~~~~~~~~~~~~~~~~

Xtrabackup backup strategy plugin
"""

import sys
import logging
from os.path import join
from holland.core.backup import BackupError
from holland.core.util.path import directory_size
from holland.lib.compression import open_stream
from holland.backup.xtrabackup.mysql import MySQL
from holland.backup.xtrabackup import util

LOG = logging.getLogger(__name__)

CONFIGSPEC = """
[xtrabackup]
global-defaults     = string(default='/etc/my.cnf')
innobackupex        = string(default='innobackupex-1.5.1')
ibbackup            = string(default=None)
stream              = string(default=tar)
apply-logs          = boolean(default=yes)
slave-info          = boolean(default=no)
safe-slave-backup   = boolean(default=no)
no-lock             = boolean(default=no)
tmpdir              = string(default=None)
additional-options  = force_list(default=list())
pre-command         = string(default=None)

[compression]
method              = option('none', 'gzip', 'gzip-rsyncable', 'pigz', 'bzip2', 'pbzip2', 'lzma', 'lzop', default=gzip)
inline              = boolean(default=yes)
options             = string(default="")
level               = integer(min=0, max=9, default=1)

[mysql:client]
defaults-extra-file = force_list(default=list('~/.my.cnf'))
user                = string(default=None)
password            = string(default=None)
socket              = string(default=None)
host                = string(default=None)
port                = integer(min=0, default=None)
""".splitlines()

class XtrabackupPlugin(object):
    #: control connection to mysql server
    mysql = None

    #: path to the my.cnf generated by this plugin
    defaults_path = None

    def __init__(self, name, config, target_directory, dry_run=False):
        self.name = name
        self.config = config
        self.config.validate_config(CONFIGSPEC)
        self.target_directory = target_directory
        self.dry_run = dry_run

        defaults_path = join(self.target_directory, 'my.cnf')
        client_opts = self.config['mysql:client']
        includes = [self.config['xtrabackup']['global-defaults']] + \
                   client_opts['defaults-extra-file']
        util.generate_defaults_file(defaults_path, includes, client_opts)
        self.defaults_path = defaults_path

    def estimate_backup_size(self):
        try:
            client = MySQL.from_defaults(self.defaults_path)
        except MySQL.MySQLError, exc:
            raise BackupError('Failed to connect to MySQL [%d] %s' % exc.args)
        try:
            try:
                datadir = client.var('datadir')
                return directory_size(datadir)
            except MySQL.MySQLError, exc:
                raise BackupError("Failed to find mysql datadir: [%d] %s" %
                                  exc.args)
            except OSError, exc:
                raise BackupError('Failed to calculate directory size: [%d] %s'
                                  % (exc.errno, exc.strerror))
        finally:
            client.close()

    def open_xb_logfile(self):
        """Open a file object to the log output for xtrabackup"""
        path = join(self.target_directory, 'xtrabackup.log')
        try:
            return open(path, 'a')
        except IOError, exc:
            raise BackupError('[%d] %s' % (exc.errno, exc.strerror))

    def open_xb_stdout(self):
        """Open the stdout output for a streaming xtrabackup run"""
        config = self.config['xtrabackup']
        backup_directory = self.target_directory
        stream = util.determine_stream_method(config['stream'])
        if stream:
            # XXX: bounce through compression
            if stream == 'tar':
                archive_path = join(backup_directory, 'backup.tar')
                zconfig = self.config['compression']
                try:
                    return open_stream(archive_path, 'w',
                                       method=zconfig['method'],
                                       level=zconfig['level'],
                                       extra_args=zconfig['options'])
                except OSError, exc:
                    raise BackupError("Unable to create output file: %s" % exc)
            elif stream == 'xbstream':
                archive_path = join(backup_directory, 'backup.xb')
                return open(archive_path, 'w')
            else:
                raise BackupError("Unknown stream method '%s'" % stream)
        else:
            return open('/dev/null', 'w')


    def dryrun(self):
        from subprocess import Popen, list2cmdline, PIPE, STDOUT
        xb_cfg = self.config['xtrabackup']
        args = util.build_xb_args(xb_cfg, self.target_directory,
                self.defaults_path)
        LOG.info("* xtrabackup command: %s", list2cmdline(args))
        args = [
            'xtrabackup',
            '--defaults-file=' + self.defaults_path,
            '--help'
        ]
        cmdline = list2cmdline(args)
        LOG.info("* Verifying generated config '%s'", self.defaults_path)
        LOG.debug("* Verifying via command: %s", cmdline)
        try:
            process = Popen(args, stdout=PIPE, stderr=STDOUT, close_fds=True)
        except OSError, exc:
            raise BackupError("Failed to find xtrabackup binary")
        stdout = process.stdout.read()
        process.wait()
        # Note: xtrabackup --help will exit with 1 usually
        if process.returncode != 1:
            LOG.error("! %s failed. Output follows below.", cmdline)
            for line in stdout.splitlines():
                LOG.error("! %s", line)
            raise BackupError("%s exited with failure status [%d]" %
                              (cmdline, process.returncode))

    def backup(self):
        if self.dry_run:
            self.dryrun()
            return
        xb_cfg = self.config['xtrabackup']
        backup_directory = self.target_directory
        tmpdir = util.evaluate_tmpdir(xb_cfg['tmpdir'], backup_directory)
        # innobackupex --tmpdir does not affect xtrabackup
        util.add_xtrabackup_defaults(self.defaults_path, tmpdir=tmpdir)
        args = util.build_xb_args(xb_cfg, backup_directory, self.defaults_path)
        util.execute_pre_command(xb_cfg['pre-command'],
                                 backup_directory=backup_directory)
        stderr = self.open_xb_logfile()
        try:
            stdout = self.open_xb_stdout()
            exc = None
            try:
                try:
                    util.run_xtrabackup(args, stdout, stderr)
                except Exception, exc:
                    LOG.info("!! %s", exc)
                    for line in open(join(self.target_directory, 'xtrabackup.log'), 'r'):
                        LOG.error("    ! %s", line.rstrip())
                    raise
            finally:
                try:
                    stdout.close()
                except IOError, e:
                    LOG.error("Error when closing %s: %s", stdout.name, e)
                    if exc is None:
                        raise
        finally:
            stderr.close()
        if xb_cfg['apply-logs']:
            util.apply_xtrabackup_logfile(xb_cfg, args[-1])


########NEW FILE########
__FILENAME__ = util
"""
holland.backup.xtrabackup.util
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Utility methods used by the xtrabackup plugin
"""

import codecs
import tempfile
import logging
from string import Template
from os.path import join, isabs, expanduser
from subprocess import Popen, PIPE, STDOUT, list2cmdline
from holland.core.backup import BackupError
from holland.lib.which import which, WhichError

LOG = logging.getLogger(__name__)

def generate_defaults_file(defaults_file, include=(), auth_opts=None):
    """Generate a mysql options file

    :param defaults_file: path where options should be written
    :param include: ordered list of additional defaults files to include
    :param auth_opts: dictionary of client options.  may include:
                      user, password, host, port, socket
    """
    LOG.info("* Generating mysql option file: %s", defaults_file)
    try:
        fileobj = codecs.open(defaults_file, 'a', encoding='utf8')
        try:
            for path in include:
                path = expanduser(path)
                LOG.info("  + Added !include %s", path)
                print >>fileobj, '!include ' + path

            if auth_opts:
                need_client_section = True
                for key in ('user', 'password', 'host', 'port', 'socket'):
                    value = auth_opts.get(key)
                    if value is None:
                        continue
                    if need_client_section:
                        LOG.info("  + Added [client] section with credentials from [mysql:client] section")
                        print >>fileobj, "[client]"
                        need_client_section = False
                    print >>fileobj, '%s = %s' % (key, value)
        finally:
            fileobj.close()
    except IOError, exc:
        raise BackupError("Failed to create %s: [%d] %s" %
                          (defaults_file, exc.errno, exc.strerror))

    return defaults_file

def run_xtrabackup(args, stdout, stderr):
    """Run xtrabackup"""
    cmdline = list2cmdline(args)
    LOG.info("Executing: %s", cmdline)
    LOG.info("  > %s 2 > %s", stdout.name, stderr.name)
    try:
        process = Popen(args, stdout=stdout, stderr=stderr, close_fds=True)
    except OSError, exc:
        # Failed to find innobackupex executable
        raise BackupError("%s failed: %s" % (args[0], exc.strerror))

    try:
        process.wait()
    except KeyboardInterrupt:
        raise BackupError("Interrupted")
    except SystemExit:
        raise BackupError("Terminated")

    if process.returncode != 0:
        # innobackupex exited with non-zero status
        raise BackupError("innobackupex exited with failure status [%d]" %
                          process.returncode)

def apply_xtrabackup_logfile(xb_cfg, backupdir):
    """Apply xtrabackup_logfile via innobackupex --apply-log [options]"""
    # run ${innobackupex} --apply-log ${backupdir}
    # only applies when streaming is not used
    stream_method = determine_stream_method(xb_cfg['stream'])
    if stream_method is not None:
        LOG.warning("Skipping --prepare/--apply-logs since backup is streamed")
        return

    if '--compress' in xb_cfg['additional-options']:
        LOG.warning("Skipping --apply-logs since --compress option appears "
                    "to have been used.")
        return

    innobackupex = xb_cfg['innobackupex']
    if not isabs(innobackupex):
        try:
            innobackupex = which(innobackupex)
        except WhichError:
            raise BackupError("Failed to find innobackupex script")
    args = [
        innobackupex,
        '--apply-log',
        backupdir
    ]

    cmdline = list2cmdline(args)
    LOG.info("Executing: %s", cmdline)
    try:
        process = Popen(args, stdout=PIPE, stderr=STDOUT, close_fds=True)
    except OSError, exc:
        raise BackupError("Failed to run %s: [%d] %s",
                          cmdline, exc.errno, exc.strerror)

    for line in process.stdout:
        LOG.info("%s", line.rstrip())
    process.wait()
    if process.returncode != 0:
        raise BackupError("%s returned failure status [%d]" %
                          (cmdline, process.returncode))

def determine_stream_method(stream):
    """Calculate the stream option from the holland config"""
    stream = stream.lower()
    if stream in ('yes', '1', 'true', 'tar', 'tar4ibd'):
        return 'tar'
    if stream in ('xbstream',):
        return 'xbstream'
    if stream in ('no', '0', 'false'):
        return None
    raise BackupError("Invalid xtrabackup stream method '%s'" % stream)

def evaluate_tmpdir(tmpdir=None, basedir=None):
    """Evaluate the tmpdir option"""
    if tmpdir is None:
        return basedir
    if not tmpdir:
        return tempfile.gettempdir()
    if basedir:
        return tmpdir.replace('{backup_directory}', basedir)
    return tmpdir


def execute_pre_command(pre_command, **kwargs):
    """Execute a pre-command"""
    if not pre_command:
        return

    pre_command = Template(pre_command).safe_substitute(**kwargs)
    LOG.info("Executing pre-command: %s", pre_command)
    try:
        process = Popen(pre_command,
                        stdout=PIPE,
                        stderr=STDOUT,
                        shell=True,
                        close_fds=True)
    except OSError, exc:
        # missing executable
        raise BackupError("pre-command %s failed: %s" %
                          (pre_command, exc.strerror))

    for line in process.stdout:
        LOG.info("  >> %s", process.pid, line)
    returncode = process.wait()
    if returncode != 0:
        raise BackupError("pre-command exited with failure status [%d]" %
                          returncode)

def add_xtrabackup_defaults(defaults_path, **kwargs):
    if not kwargs:
        return
    fileobj = open(defaults_path, 'a')
    try:
        try:
            # spurious newline for readability
            print >>fileobj
            print >>fileobj, "[xtrabackup]"
            for key, value in kwargs.iteritems():
                print >>fileobj, "%s = %s" % (key, value)
        except IOError, exc:
            raise BackupError("Error writing xtrabackup defaults to %s" %
                              defaults_path)
    finally:
        fileobj.close()
def build_xb_args(config, basedir, defaults_file=None):
    """Build the commandline for xtrabackup"""
    innobackupex = config['innobackupex']
    if not isabs(innobackupex):
        try:
            innobackupex = which(innobackupex)
        except WhichError:
            raise BackupError("Failed to find innobackupex script")

    ibbackup = config['ibbackup']
    stream = determine_stream_method(config['stream'])
    tmpdir = evaluate_tmpdir(config['tmpdir'], basedir)
    slave_info = config['slave-info']
    safe_slave_backup = config['safe-slave-backup']
    no_lock = config['no-lock']
    # filter additional options to remove any empty values
    extra_opts = filter(None, config['additional-options'])

    args = [
        innobackupex,
    ]
    if defaults_file:
        args.append('--defaults-file=' + defaults_file)
    if ibbackup:
        args.append('--ibbackup=' + ibbackup)
    if stream:
        args.append('--stream=' + stream)
    else:
        basedir = join(basedir, 'data')
    if tmpdir:
        args.append('--tmpdir=' + tmpdir)
    if slave_info:
        args.append('--slave-info')
    if safe_slave_backup:
        args.append('--safe-slave-backup')
    if no_lock:
        args.append('--no-lock')
    args.append('--no-timestamp')
    if extra_opts:
        args.extend(extra_opts)
    if basedir:
        args.append(basedir)
    return args

########NEW FILE########
__FILENAME__ = test_plugin
import time
from holland.backup.example import ExamplePlugin
from nose.tools import *

# Config mock, so we don't have to import holland.core
class MockConfig(object):
    def validate(self, spec):
        pass

def test_example_plugin():
    name = 'example/' + time.strftime('%Y%m%d_%H%M%S')
    target_directory = '/tmp/example_backup/'
    config = MockConfig()
    dry_run = False

    plugin = ExamplePlugin(name, config, target_directory, dry_run)
    assert_equals(plugin.estimate_backup_size(), 0)
    plugin.backup()

    dry_run = True

    plugin = ExamplePlugin(name, config, target_directory, dry_run)
    plugin.backup()

    ok_(isinstance(plugin.info(), basestring))

########NEW FILE########
__FILENAME__ = dir_archive
import os
import shutil

class DirArchive(object):
    """
    Read, write, access directory archives.  Treats a directory like an 
    archive.
    """
    def __init__(self, path, mode=None):
        """
        Initialize a DirArchive.
        
        Arguments:
        
        path -- Path to the archive directory
        mode -- Archive mode.  Default: None (unused, here for compatiblity)
        """
        self.path = path
        self.mode = mode
        if not os.path.exists(path):
            os.makedirs(path)

    def add_file(self, path, name):
        """
        Add a file to the archive.
        
        Arguments:
        
        path -- Path to file for which to add to archive.
        name -- Name of dest file
        """
        target_path = os.path.join(self.path, name)
        target_dir = os.path.dirname(target_path)
        if not os.path.exists(target_dir):
            os.makedirs(target_dir)
        shutil.copy2(path, target_path)

    def add_string(self, string, name):
        """
        Add a string to the archive, saved as a file.
        
        Arguments:
        
        string  -- String to add to archive.
        name    -- Name of file to create string as.
        """
        target_path = os.path.join(self.path, name)
        target_dir = os.path.dirname(target_path)
        if not os.path.exists(target_dir):
            os.makedirs(target_dir)
        fileobj = open(target_path, 'w')
        print >> fileobj, string
        fileobj.close()

    def list(self):
        """
        List members of the archive.
        """
        result = []
        top = self.path
        size = len(top.split(os.sep))
        for root, dirs, files in os.walk(top, topdown=False):
            for name in files:
                path = os.path.join(root, name)
                result.append(os.sep.join(path.split(os.sep)[size:]))
            for name in dirs:
                path = os.path.join(root, name)
                result.append(os.sep.join(path.split(os.sep)[size:]))
        return result

    def extract(self, name, dest):
        """
        Extract a member from the archive.
        
        Arguments:
        
        name -- Name of the member to extract.
        dest -- Destination path to extract the member to.
        """
        target_src = os.path.join(self.path, name)
        shutil.copy2(target_src, dest)

    def close(self):
        """
        Close archive.
        """
        import subprocess
        status = subprocess.call(['gzip', '-1', '--recursive', self.path])
        if status != 0:
            LOGGER.error("Failed to compress %r" % self.path)
            
if __name__ == '__main__':
    import time
    now = time.time()
    xv = DirArchive('backup/')
    xv.add_string("[mysqldump]\nignore-table=mysql.user\n", "my.cnf")
    xv.add_string("blah", "test/test.MYD")
    xv.add_file("user.frm", "mysql/user.frm")
    xv.add_file("user.MYD", "mysql/user.MYD")
    xv.add_file("user.MYI", "mysql/user.MYI")
    xv.close()
    print (time.time() - now), "seconds"

########NEW FILE########
__FILENAME__ = tar_archive
import os
import pwd
import grp
import time
import tarfile

try:
    from cStringIO import StringIO
except ImportError:
    from StringIO import StringIO

def _make_tarinfo(name, size):
    tarinfo = tarfile.TarInfo(name=name)
    tarinfo.size = size
    tarinfo.mtime = time.time()
    tarinfo.mode = 0660
    tarinfo.type = tarfile.REGTYPE
    tarinfo.uid = os.geteuid()
    tarinfo.gid = os.getegid()
    tarinfo.uname = pwd.getpwuid(os.geteuid()).pw_name
    tarinfo.gname = grp.getgrgid(os.getegid()).gr_name
    return tarinfo

class TarArchive(object):
    """
    Read, write, access Tar archives.
    """
    def __init__(self, path, mode='w:gz'):
        """
        Initialize a TarArchive.
        
        Arguments:
        
        path -- Path to the archive file
        mode -- Archive mode.  Default: w:gz (write + gzip) (see tarfile)
        """
        self.path = path
        self.mode = mode
        self.archive = tarfile.open(path, mode)

    def add_file(self, path, name):
        """
        Add a file to the archive.
        
        Arguments:
        
        path -- Path to file for which to add to archive.
        name -- Name of file (for tarinfo)
        """
        fileobj = open(path, 'r')
        size = os.fstat(fileobj.fileno()).st_size
        tarinfo = _make_tarinfo(name, size)
        self.archive.addfile(tarinfo, fileobj)
        fileobj.close()

    def add_string(self, string, name):
        """
        Add a string to the archive (fake file).
        
        Arguments:
        
        string  -- String to add to the archive.
        name    -- Name of the file to save string as.
        """
        tarinfo = _make_tarinfo(name, len(string))
        self.archive.addfile(tarinfo, StringIO(string))

    def list(self):
        """
        List contents of the archive.  Returns a list of member names.
        """
        result = []
        for member in self.archive.getmembers():
            result.append(member.name)
        return result

    def extract(self, name, dest):
        """
        Extract a member from an archive to 'dest' path.
        
        Arguments:
        
        name -- Name of the member in the archive to extract.
        dest -- Path to extract member to.
        """
        self.archive.extract(name, dest)

    def close(self):
        """
        Close archive.
        """
        self.archive.close()

if __name__ == '__main__':
    now = time.time()
    xv = TarArchive('foo.tgz', 'w:gz')
    xv.add_string("[mysqldump]\nignore-table=mysql.user\n", "my.cnf")
    xv.add_string("blah", "test/test.MYD")
    xv.add_file("user.frm", "mysql/user.frm")
    xv.add_file("user.MYD", "mysql/user.MYD")
    xv.add_file("user.MYI", "mysql/user.MYI")
    xv.close()
    print (time.time() - now), "seconds"

########NEW FILE########
__FILENAME__ = zip_archive
import time
import zipfile
import logging

LOGGER = logging.getLogger(__name__)

class ZipArchive(object):
    """
    Read, write, access Zip archives using zipfile.
    """
    def __init__(self, path, mode='w'):
        """
        Initialize a ZipArchive.
        
        Arguments:
        
        path -- Path to the archive file
        mode -- Archive mode.  Default: w (write) (see zipfile)
        """
        self.path = path
        self.mode = mode
        self.archive = zipfile.ZipFile(path, 
                                       mode, 
                                       zipfile.ZIP_DEFLATED, 
                                       True)

    def add_file(self, path, name):
        """
        Add a file to the archive.
        
        Arguments:
        
        path -- Path to file for which to add to archive.
        name -- Name of file to save in the archive
        """
        self.archive.write(path, name, zipfile.ZIP_DEFLATED)

    def add_string(self, str, name):
        """
        Add a string to the archive (fake file).
        
        Arguments:
        
        string  -- String to add to the archive.
        name    -- Name of the file to save string as.
        """
        self.archive.writestr(name, str)

    def list(self):
        """
        List contents of the archive.  Returns a list of member names.
        """
        result = []
        for member in self.archive.namelist():
            result.append(member)
        return result

    def extract(self, name, dest):
        """
        Extract a member from an archive to 'dest' path.
        
        Arguments:
        
        name -- Name of the member in the archive to extract.
        dest -- Path to extract member to.
        """
        self.archive.extract(name, dest)

    def close(self):
        """
        Close archive.
        """
        self.archive.close()

if __name__ == '__main__':
    now = time.time()
    xv = ZipArchive('foo.zip', 'w')
    xv.add_string("[mysqldump]\nignore-table=mysql.user\n", "my.cnf")
    xv.add_string("blah", "test/test.MYD")
    xv.add_file("user.frm", "mysql/user.frm")
    xv.add_file("user.MYD", "mysql/user.MYD")
    xv.add_file("user.MYI", "mysql/user.MYI")
    xv.close()
    print (time.time() - now), "seconds"

########NEW FILE########
__FILENAME__ = compression
import os
import logging
import errno
import subprocess
import which
import shlex
from tempfile import TemporaryFile

LOG = logging.getLogger(__name__)

#: This is a simple table of method_name : (command, extension)
#: mappings.
COMPRESSION_METHODS = {
    'gzip'  : ('gzip', '.gz'),
    'gzip-rsyncable' : ('gzip --rsyncable', '.gz'),
    'pigz'  : ('pigz', '.gz'),
    'bzip2' : ('bzip2', '.bz2'),
    'pbzip2': ('pbzip2', '.bz2'),
    'lzop'  : ('lzop', '.lzo'),
    'lzma'  : ('xz', '.xz'),
}

def lookup_compression(method):
    """
    Looks up the passed compression method in supported COMPRESSION_METHODS
    and returns a tuple in the form of ('command_name', 'file_extension').

    Arguments:

    method -- A string identifier of the compression method (i.e. 'gzip').
    """
    try:
        cmd, ext = COMPRESSION_METHODS[method]
        argv = shlex.split(cmd)
        try:
            return [which.which(argv[0])] + argv[1:], ext
        except which.WhichError, e:
            raise OSError("No command found for compression method '%s'" %
                    method)
    except KeyError:
        raise OSError("Unsupported compression method '%s'" % method)

class CompressionInput(object):
    """
    Class to create a compressed file descriptor for reading.  Functions like
    a standard file descriptor such as from open().
    """
    def __init__(self, path, mode, argv, bufsize=1024*1024):
        self.fileobj = open(path, 'r')
        self.pid = subprocess.Popen(argv + ['--decompress'],
                                    stdin=self.fileobj.fileno(),
                                    stdout=subprocess.PIPE,
                                    bufsize=bufsize)
        self.fd = self.pid.stdout.fileno()
        self.name = path
        self.closed = False

    def fileno(self):
        return self.fd

    def read(self, size):
        return os.read(self.fd, size)

    def next(self):
        return self.pid.stdout.next()

    def __iter__(self):
        return iter(self.pid.stdout)

    def close(self):
        import signal
        os.kill(self.pid.pid, signal.SIGTERM)
        self.fileobj.close()
        self.pid.stdout.close()
        self.pid.wait()
        self.closed = True


class CompressionOutput(object):
    """
    Class to create a compressed file descriptor for writing.  Functions like
    a standard file descriptor such as from open().
    """
    def __init__(self, path, mode, argv, level, inline):
        self.argv = argv
        self.level = level
        self.inline = inline
        if not inline:
            self.fileobj = open(os.path.splitext(path)[0], mode)
            self.fd = self.fileobj.fileno()
        else:
            self.fileobj = open(path, 'w')
            if level:
                argv += ['-%d' % level]
            LOG.debug("* Executing: %s", subprocess.list2cmdline(argv))
            self.stderr = TemporaryFile()
            self.pid = subprocess.Popen(argv,
                                        stdin=subprocess.PIPE,
                                        stdout=self.fileobj.fileno(),
                                        stderr=self.stderr)
            self.fd = self.pid.stdin.fileno()
        self.name = path
        self.closed = False

    def fileno(self):
        return self.fd

    def write(self, data):
        return os.write(self.fd, data)

    def close(self):
        self.closed = True
        if not self.inline:
            argv = list(self.argv)
            if self.level:
                argv += ['-%d' % self.level, '-']
            self.fileobj.close()
            self.fileobj = open(self.fileobj.name, 'r')
            cmp_f = open(self.name, 'w')
            LOG.debug("Running %r < %r[%d] > %r[%d]",
                         argv, self.fileobj.name, self.fileobj.fileno(),
                         cmp_f.name, cmp_f.fileno())
            pid = subprocess.Popen(args,
                                   stdin=self.fileobj.fileno(),
                                   stdout=cmp_f.fileno())
            status = pid.wait()
            os.unlink(self.fileobj.name)
        else:
            self.pid.stdin.close()
            status = self.pid.wait()
            stderr = self.stderr
            stderr.flush()
            stderr.seek(0)
            try:
                if status != 0:
                    for line in stderr:
                        if not line.strip(): continue
                        LOG.error("%s: %s", self.argv[0], line.rstrip())
                    raise IOError(errno.EPIPE,
                              "Compression program '%s' exited with status %d" %
                                (self.argv[0], status))
                else:
                    for line in stderr:
                        if not line.strip(): continue
                        LOG.info("%s: %s", self.argv[0], line.rstrip())
            finally:
                stderr.close()


def stream_info(path, method=None, level=None):
    """
    Determine compression command, and compressed path based on original path
    and compression method.  If method is not passed, or level is 0 the
    original path is returned.

    Arguments:

    path    -- Path to file to compress/decompress
    method  -- Compression method (i.e. 'gzip', 'bzip2', 'pbzip2', 'lzop')
    level   -- Compression level (0-9)
    """
    if not method or level == 0:
        return path

    argv, ext = lookup_compression(method)

    if not argv:
        raise IOError("Unknown compression method '%s'" % argv[0])

    if not path.endswith(ext):
        path += ext

    return argv, path

def _parse_args(value):
    """Convert a cmdline string to a list"""
    if isinstance(value, unicode):
        value = value.encode('utf8')
    return shlex.split(value)

def open_stream(path,
                mode,
                method=None,
                level=None,
                inline=True,
                extra_args=None):
    """
    Opens a compressed data stream, and returns a file descriptor type object
    that acts much like os.open() does.  If no method is passed, or the 
    compression level is 0, simply returns a file descriptor from open().

    Arguments:

    mode    -- File access mode (i.e. 'r' or 'w')
    method  -- Compression method (i.e. 'gzip', 'bzip2', 'pbzip2', 'lzop')
    level   -- Compression level
    inline  -- Boolean whether to compress inline, or after the file is written.
    """
    if not method or method == 'none' or level == 0:
        return open(path, mode)
    else:
        argv, path = stream_info(path, method)
        if extra_args:
            argv += _parse_args(extra_args)
        if mode == 'r':
            return CompressionInput(path, mode, argv=argv)
        elif mode == 'w':
            return CompressionOutput(path, mode, argv=argv, level=level,
                                     inline=inline)
        else:
            raise IOError("invalid mode: %s" % mode)

########NEW FILE########
__FILENAME__ = multidict
# (c) 2005 Ian Bicking and contributors; written for Paste: 
#   http://pythonpaste.org
# Licensed under the MIT license: 
#   http://www.opensource.org/licenses/mit-license.php
import cgi
import copy
import sys
from UserDict import DictMixin

class MultiDict(DictMixin):

    """
    An ordered dictionary that can have multiple values for each key.
    Adds the methods getall, getone, mixed, and add to the normal
    dictionary interface.
    """

    def __init__(self, *args, **kw):
        if len(args) > 1:
            raise TypeError(
                "MultiDict can only be called with one positional argument")
        if args:
            if hasattr(args[0], 'iteritems'):
                items = list(args[0].iteritems())
            elif hasattr(args[0], 'items'):
                items = args[0].items()
            else:
                items = list(args[0])
            self._items = items
        else:
            self._items = []
        self._items.extend(kw.iteritems())

    def __getitem__(self, key):
        for k, v in self._items:
            if k == key:
                return v
        raise KeyError(repr(key))

    def __setitem__(self, key, value):
        try:
            del self[key]
        except KeyError:
            pass
        self._items.append((key, value))

    def add(self, key, value):
        """
        Add the key and value, not overwriting any previous value.
        """
        self._items.append((key, value))

    def getall(self, key):
        """
        Return a list of all values matching the key (may be an empty list)
        """
        result = []
        for k, v in self._items:
            if key == k:
                result.append(v)
        return result

    def getone(self, key):
        """
        Get one value matching the key, raising a KeyError if multiple
        values were found.
        """
        v = self.getall(key)
        if not v:
            raise KeyError('Key not found: %r' % key)
        if len(v) > 1:
            raise KeyError('Multiple values match %r: %r' % (key, v))
        return v[0]

    def mixed(self):
        """
        Returns a dictionary where the values are either single
        values, or a list of values when a key/value appears more than
        once in this dictionary.  This is similar to the kind of
        dictionary often used to represent the variables in a web
        request.
        """
        result = {}
        multi = {}
        for key, value in self._items:
            if key in result:
                # We do this to not clobber any lists that are
                # *actual* values in this dictionary:
                if key in multi:
                    result[key].append(value)
                else:
                    result[key] = [result[key], value]
                    multi[key] = None
            else:
                result[key] = value
        return result

    def dict_of_lists(self):
        """
        Returns a dictionary where each key is associated with a
        list of values.
        """
        result = {}
        for key, value in self._items:
            if key in result:
                result[key].append(value)
            else:
                result[key] = [value]
        return result

    def __delitem__(self, key):
        items = self._items
        found = False
        for i in range(len(items)-1, -1, -1):
            if items[i][0] == key:
                del items[i]
                found = True
        if not found:
            raise KeyError(repr(key))

    def __contains__(self, key):
        for k, v in self._items:
            if k == key:
                return True
        return False

    has_key = __contains__

    def clear(self):
        self._items = []

    def copy(self):
        return MultiDict(self)

    def setdefault(self, key, default=None):
        for k, v in self._items:
            if key == k:
                return v
        self._items.append((key, default))
        return default

    def pop(self, key, *args):
        if len(args) > 1:
            raise TypeError, "pop expected at most 2 arguments, got "\
                              + repr(1 + len(args))
        for i in range(len(self._items)):
            if self._items[i][0] == key:
                v = self._items[i][1]
                del self._items[i]
                return v
        if args:
            return args[0]
        else:
            raise KeyError(repr(key))

    def popitem(self):
        return self._items.pop()

    def update(self, other=None, **kwargs):
        if other is None:
            pass
        elif hasattr(other, 'items'):
            self._items.extend(other.items())
        elif hasattr(other, 'keys'):
            for k in other.keys():
                self._items.append((k, other[k]))
        else:
            for k, v in other:
                self._items.append((k, v))
        if kwargs:
            self.update(kwargs)

    def __repr__(self):
        items = ', '.join(['(%r, %r)' % v for v in self._items])
        return '%s([%s])' % (self.__class__.__name__, items)

    def __len__(self):
        return len(self._items)

    ##
    ## All the iteration:
    ##

    def keys(self):
        return [k for k, v in self._items]

    def iterkeys(self):
        for k, v in self._items:
            yield k

    __iter__ = iterkeys

    def items(self):
        return self._items[:]

    def iteritems(self):
        return iter(self._items)

    def values(self):
        return [v for k, v in self._items]

    def itervalues(self):
        for k, v in self._items:
            yield v

class UnicodeMultiDict(DictMixin):
    """
    A MultiDict wrapper that decodes returned values to unicode on the
    fly. Decoding is not applied to assigned values.

    The key/value contents are assumed to be ``str``/``strs`` or
    ``str``/``FieldStorages`` (as is returned by the ``paste.request.parse_``
    functions).

    Can optionally also decode keys when the ``decode_keys`` argument is
    True.

    ``FieldStorage`` instances are cloned, and the clone's ``filename``
    variable is decoded. Its ``name`` variable is decoded when ``decode_keys``
    is enabled.

    """
    def __init__(self, multi=None, encoding=None, errors='strict',
                 decode_keys=False):
        self.multi = multi
        if encoding is None:
            encoding = sys.getdefaultencoding()
        self.encoding = encoding
        self.errors = errors
        self.decode_keys = decode_keys

    def _decode_key(self, key):
        if self.decode_keys:
            try:
                key = key.decode(self.encoding, self.errors)
            except AttributeError:
                pass
        return key

    def _decode_value(self, value):
        """
        Decode the specified value to unicode. Assumes value is a ``str`` or
        `FieldStorage`` object.

        ``FieldStorage`` objects are specially handled.
        """
        if isinstance(value, cgi.FieldStorage):
            # decode FieldStorage's field name and filename
            value = copy.copy(value)
            if self.decode_keys:
                value.name = value.name.decode(self.encoding, self.errors)
            value.filename = value.filename.decode(self.encoding, self.errors)
        else:
            try:
                value = value.decode(self.encoding, self.errors)
            except AttributeError:
                pass
        return value

    def __getitem__(self, key):
        return self._decode_value(self.multi.__getitem__(key))

    def __setitem__(self, key, value):
        self.multi.__setitem__(key, value)

    def add(self, key, value):
        """
        Add the key and value, not overwriting any previous value.
        """
        self.multi.add(key, value)

    def getall(self, key):
        """
        Return a list of all values matching the key (may be an empty list)
        """
        return [self._decode_value(v) for v in self.multi.getall(key)]

    def getone(self, key):
        """
        Get one value matching the key, raising a KeyError if multiple
        values were found.
        """
        return self._decode_value(self.multi.getone(key))

    def mixed(self):
        """
        Returns a dictionary where the values are either single
        values, or a list of values when a key/value appears more than
        once in this dictionary.  This is similar to the kind of
        dictionary often used to represent the variables in a web
        request.
        """
        unicode_mixed = {}
        for key, value in self.multi.mixed().iteritems():
            if isinstance(value, list):
                value = [self._decode_value(value) for value in value]
            else:
                value = self._decode_value(value)
            unicode_mixed[self._decode_key(key)] = value
        return unicode_mixed

    def dict_of_lists(self):
        """
        Returns a dictionary where each key is associated with a
        list of values.
        """
        unicode_dict = {}
        for key, value in self.multi.dict_of_lists().iteritems():
            value = [self._decode_value(value) for value in value]
            unicode_dict[self._decode_key(key)] = value
        return unicode_dict

    def __delitem__(self, key):
        self.multi.__delitem__(key)

    def __contains__(self, key):
        return self.multi.__contains__(key)

    has_key = __contains__

    def clear(self):
        self.multi.clear()

    def copy(self):
        return UnicodeMultiDict(self.multi.copy(), self.encoding, self.errors)

    def setdefault(self, key, default=None):
        return self._decode_value(self.multi.setdefault(key, default))

    def pop(self, key, *args):
        return self._decode_value(self.multi.pop(key, *args))

    def popitem(self):
        k, v = self.multi.popitem()
        return (self._decode_key(k), self._decode_value(v))

    def __repr__(self):
        items = ', '.join(['(%r, %r)' % v for v in self.items()])
        return '%s([%s])' % (self.__class__.__name__, items)

    def __len__(self):
        return self.multi.__len__()

    ##
    ## All the iteration:
    ##

    def keys(self):
        return [self._decode_key(k) for k in self.multi.iterkeys()]

    def iterkeys(self):
        for k in self.multi.iterkeys():
            yield self._decode_key(k)

    __iter__ = iterkeys

    def items(self):
        return [(self._decode_key(k), self._decode_value(v)) for \
                    k, v in self.multi.iteritems()]

    def iteritems(self):
        for k, v in self.multi.iteritems():
            yield (self._decode_key(k), self._decode_value(v))

    def values(self):
        return [self._decode_value(v) for v in self.multi.itervalues()]

    def itervalues(self):
        for v in self.multi.itervalues():
            yield self._decode_value(v)

__test__ = {
    'general': """
    >>> d = MultiDict(a=1, b=2)
    >>> d['a']
    1
    >>> d.getall('c')
    []
    >>> d.add('a', 2)
    >>> d['a']
    1
    >>> d.getall('a')
    [1, 2]
    >>> d['b'] = 4
    >>> d.getall('b')
    [4]
    >>> d.keys()
    ['a', 'a', 'b']
    >>> d.items()
    [('a', 1), ('a', 2), ('b', 4)]
    >>> d.mixed()
    {'a': [1, 2], 'b': 4}
    >>> MultiDict([('a', 'b')], c=2)
    MultiDict([('a', 'b'), ('c', 2)])
    """}

if __name__ == '__main__':
    import doctest
    doctest.testmod()

########NEW FILE########
__FILENAME__ = safefilename
# -*- coding: utf-8 -*-
#
#    Copyright  2007, 2008 Torsten Bronger <bronger@physik.rwth-aachen.de>
#
#    This file is part of the Bobcat program.
#
#    Bobcat is free software; you can use it, redistribute it and/or modify it
#    under the terms of the MIT license.
#
#    You should have received a copy of the MIT license with Bobcat.  If not,
#    see <http://bobcat.origo.ethz.ch/wiki/Licence>.
#

"""Codec class for safe filenames.  Safe filenames work on all important
filesystems, i.e., they don't contain special or dangerous characters, and
they don't assume that filenames are treated case-sensitively.

    >>> u"hallo".encode("safefilename")
    'hallo'
    >>> u"Hallo".encode("safefilename")
    '{h}allo'
    >>> u"MIT Thesis".encode("safefilename")
    '{mit}_{t}hesis'
    >>> u"Gesch\\u00e4ftsbrief".encode("safefilename")
    '{g}esch(e4)ftsbrief'

Of course, the mapping works in both directions as expected:

    >>> "{g}esch(e4)ftsbrief".decode("safefilename")
    u'Gesch\\xe4ftsbrief'
    >>> "{mit}_{t}hesis".decode("safefilename")
    u'MIT Thesis'

"""
from string import ascii_letters

safe_characters = ascii_letters + "0123456789-_+!$%&'@~#.,^"

def encode(filename, errors='strict'):
    """Convert Unicode strings to safe filenames.

    :Parameters:
      - `filename`: the input string to be converted into a safe filename
      - `errors`: the ``errors`` parameter known from standard ``str.encode``
        methods

    :type filename: unicode
    :type errors: str

    :Return:
      the safe filename

    :rtype: str
    """
    output = ""
    i = 0
    input_length = len(filename)
    while i < input_length:
        char = filename[i]
        if char in safe_characters:
            output += str(char)
        else:
            output += "(" + hex(ord(char))[2:] + ")"
        i += 1
    return output, input_length

def handle_problematic_characters(errors, filename, start, end, message):
    """Trivial helper routine in case something goes wrong in `decode`.

    :Parameters:
      - `errors`: the ``errors`` parameter known from standard ``str.encode``
        methods.  It is just passed by `decode`.
      - `filename`: the input to be converted to Unicode by `decode`
      - `start`: the starting position of the problematic area as an index in
        ``filename``
      - `end`: the ending position of the problematic area as an index in
        ``filename``.  Note that this obeys to the standard upper-limit
        notation in Python (range() etc).
      - `message`: error message describing the problem

    :type errors: str
    :type filename: str
    :type start: int
    :type end: int
    :type message: str

    :Return:
      the single character to be inserted into the output string

    :rtype: unicode
    """
    if errors == 'ignore':
        return u""
    elif errors == 'replace':
        return u"?"
    else:
        raise UnicodeDecodeError("safefilename", filename, start, end, message)

def decode(filename, errors='strict'):
    """Convert safe filenames to Unicode strings.

    :Parameters:
      - `filename`: the input string to be converted from a safe filename to an
        ordinary Unicode string
      - `errors`: the ``errors`` parameter known from standard ``str.encode``
        methods

    :type filename: str
    :type errors: str

    :Return:
      the plain Unicode string

    :rtype: unicode
    """
    filename = str(filename)
    input_length = len(filename)
    output = u""
    i = 0
    while i < input_length:
        char = filename[i]
        if char in safe_characters:
            output += char
        elif char == "(":
            end_position = filename.find(")", i)
            if end_position == -1:
                end_position = i+1
                while end_position < input_length and \
                        filename[end_position] in "0123456789abcdef" and \
                        end_position - i <= 8:
                    end_position += 1
                # In you want to implement StreamReaders: If the string
                # didn't start with this curly braces sequence, it should
                # return from here with a smaller value for consumed_length
                output += handle_problematic_characters(errors, filename, i, end_position,
                                                        "open parenthesis was never closed")
                i = end_position
                continue
            else:
                try:
                    output += unichr(int(filename[i+1:end_position], 16))
                except:
                    output += handle_problematic_characters(errors, filename, i, end_position+1,
                                                            "invalid data between parentheses")
            i = end_position
        else:
            output += handle_problematic_characters(errors, filename, i, i+1,
                                                    "invalid character '%s'" % char)
        i += 1
    return output, input_length

def registry(encoding):
    """Lookup function for the ``safefilename`` encoding.

    :Parameters:
      - `encoding`: the encoding which is looked for by the caller

    :type encoding: str

    :Return:
      a tuple containing (encoder, decoder, streamencoder, streamdecoder).  The
      latter two are set to ``None`` because they are not implemented (and not
      needed).

    :rtype: tuple
    """
    if encoding == "safefilename":
        return (encode, decode, None, None)
    else:
        return None

########NEW FILE########
__FILENAME__ = which
# Copyright (c) 2002-2005 ActiveState Corp.
# See LICENSE.txt for license details.
# Author:
#   Trent Mick (TrentM@ActiveState.com)
# Home:
#   http://trentm.com/projects/which/

r"""Find the full path to commands.

which(command, path=None, verbose=0, exts=None)
    Return the full path to the first match of the given command on the
    path.

whichall(command, path=None, verbose=0, exts=None)
    Return a list of full paths to all matches of the given command on
    the path.

whichgen(command, path=None, verbose=0, exts=None)
    Return a generator which will yield full paths to all matches of the
    given command on the path.
    
By default the PATH environment variable is searched (as well as, on
Windows, the AppPaths key in the registry), but a specific 'path' list
to search may be specified as well.  On Windows, the PATHEXT environment
variable is applied as appropriate.

If "verbose" is true then a tuple of the form
    (<fullpath>, <matched-where-description>)
is returned for each match. The latter element is a textual description
of where the match was found. For example:
    from PATH element 0
    from HKLM\SOFTWARE\...\perl.exe
"""

_cmdlnUsage = """
    Show the full path of commands.

    Usage:
        which [<options>...] [<command-name>...]

    Options:
        -h, --help      Print this help and exit.
        -V, --version   Print the version info and exit.

        -a, --all       Print *all* matching paths.
        -v, --verbose   Print out how matches were located and
                        show near misses on stderr.
        -q, --quiet     Just print out matches. I.e., do not print out
                        near misses.

        -p <altpath>, --path=<altpath>
                        An alternative path (list of directories) may
                        be specified for searching.
        -e <exts>, --exts=<exts>
                        Specify a list of extensions to consider instead
                        of the usual list (';'-separate list, Windows
                        only).

    Show the full path to the program that would be run for each given
    command name, if any. Which, like GNU's which, returns the number of
    failed arguments, or -1 when no <command-name> was given.

    Near misses include duplicates, non-regular files and (on Un*x)
    files without executable access.
"""

__revision__ = "$Id: which.py 430 2005-08-20 03:11:58Z trentm $"
__version_info__ = (1, 1, 0)
__version__ = '.'.join(map(str, __version_info__))

import os
import sys
import getopt
import stat


#---- exceptions

class WhichError(Exception):
    pass



#---- internal support stuff

def _getRegisteredExecutable(exeName):
    """Windows allow application paths to be registered in the registry."""
    registered = None
    if sys.platform.startswith('win'):
        if os.path.splitext(exeName)[1].lower() != '.exe':
            exeName += '.exe'
        import _winreg
        try:
            key = "SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\App Paths\\" +\
                  exeName
            value = _winreg.QueryValue(_winreg.HKEY_LOCAL_MACHINE, key)
            registered = (value, "from HKLM\\"+key)
        except _winreg.error:
            pass
        if registered and not os.path.exists(registered[0]):
            registered = None
    return registered

def _samefile(fname1, fname2):
    if sys.platform.startswith('win'):
        return ( os.path.normpath(os.path.normcase(fname1)) ==\
            os.path.normpath(os.path.normcase(fname2)) )
    else:
        return os.path.samefile(fname1, fname2)

def _cull(potential, matches, verbose=0):
    """Cull inappropriate matches. Possible reasons:
        - a duplicate of a previous match
        - not a disk file
        - not executable (non-Windows)
    If 'potential' is approved it is returned and added to 'matches'.
    Otherwise, None is returned.
    """
    for match in matches:  # don't yield duplicates
        if _samefile(potential[0], match[0]):
            if verbose:
                sys.stderr.write("duplicate: %s (%s)\n" % potential)
            return None
    else:
        if not stat.S_ISREG(os.stat(potential[0]).st_mode):
            if verbose:
                sys.stderr.write("not a regular file: %s (%s)\n" % potential)
        elif not os.access(potential[0], os.X_OK):
            if verbose:
                sys.stderr.write("no executable access: %s (%s)\n"\
                                 % potential)
        else:
            matches.append(potential)
            return potential

        
#---- module API

def whichgen(command, path=None, verbose=0, exts=None):
    """Return a generator of full paths to the given command.
    
    "command" is a the name of the executable to search for.
    "path" is an optional alternate path list to search. The default it
        to use the PATH environment variable.
    "verbose", if true, will cause a 2-tuple to be returned for each
        match. The second element is a textual description of where the
        match was found.
    "exts" optionally allows one to specify a list of extensions to use
        instead of the standard list for this system. This can
        effectively be used as an optimization to, for example, avoid
        stat's of "foo.vbs" when searching for "foo" and you know it is
        not a VisualBasic script but ".vbs" is on PATHEXT. This option
        is only supported on Windows.

    This method returns a generator which yields either full paths to
    the given command or, if verbose, tuples of the form (<path to
    command>, <where path found>).
    """
    matches = []
    if path is None:
        usingGivenPath = 0
        path = os.environ.get("PATH", "").split(os.pathsep)
        if sys.platform.startswith("win"):
            path.insert(0, os.curdir)  # implied by Windows shell
    else:
        usingGivenPath = 1

    # Windows has the concept of a list of extensions (PATHEXT env var).
    if sys.platform.startswith("win"):
        if exts is None:
            exts = os.environ.get("PATHEXT", "").split(os.pathsep)
            # If '.exe' is not in exts then obviously this is Win9x and
            # or a bogus PATHEXT, then use a reasonable default.
            for ext in exts:
                if ext.lower() == ".exe":
                    break
            else:
                exts = ['.COM', '.EXE', '.BAT']
        elif not isinstance(exts, list):
            raise TypeError("'exts' argument must be a list or None")
    else:
        if exts is not None:
            raise WhichError("'exts' argument is not supported on "\
                             "platform '%s'" % sys.platform)
        exts = []

    # File name cannot have path separators because PATH lookup does not
    # work that way.
    if os.sep in command or os.altsep and os.altsep in command:
        pass
    else:
        for i in range(len(path)):
            dirName = path[i]
            # On windows the dirName *could* be quoted, drop the quotes
            if sys.platform.startswith("win") and len(dirName) >= 2\
               and dirName[0] == '"' and dirName[-1] == '"':
                dirName = dirName[1:-1]
            for ext in ['']+exts:
                absName = os.path.abspath(
                    os.path.normpath(os.path.join(dirName, command+ext)))
                if os.path.isfile(absName):
                    if usingGivenPath:
                        fromWhere = "from given path element %d" % i
                    elif not sys.platform.startswith("win"):
                        fromWhere = "from PATH element %d" % i
                    elif i == 0:
                        fromWhere = "from current directory"
                    else:
                        fromWhere = "from PATH element %d" % (i-1)
                    match = _cull((absName, fromWhere), matches, verbose)
                    if match:
                        if verbose:
                            yield match
                        else:
                            yield match[0]
        match = _getRegisteredExecutable(command)
        if match is not None:
            match = _cull(match, matches, verbose)
            if match:
                if verbose:
                    yield match
                else:
                    yield match[0]


def which(command, path=None, verbose=0, exts=None):
    """Return the full path to the first match of the given command on
    the path.
    
    "command" is a the name of the executable to search for.
    "path" is an optional alternate path list to search. The default it
        to use the PATH environment variable.
    "verbose", if true, will cause a 2-tuple to be returned. The second
        element is a textual description of where the match was found.
    "exts" optionally allows one to specify a list of extensions to use
        instead of the standard list for this system. This can
        effectively be used as an optimization to, for example, avoid
        stat's of "foo.vbs" when searching for "foo" and you know it is
        not a VisualBasic script but ".vbs" is on PATHEXT. This option
        is only supported on Windows.

    If no match is found for the command, a WhichError is raised.
    """
    try:
        match = whichgen(command, path, verbose, exts).next()
    except StopIteration:
        raise WhichError("Could not find '%s' on the path." % command)
    return match


def whichall(command, path=None, verbose=0, exts=None):
    """Return a list of full paths to all matches of the given command
    on the path.  

    "command" is a the name of the executable to search for.
    "path" is an optional alternate path list to search. The default it
        to use the PATH environment variable.
    "verbose", if true, will cause a 2-tuple to be returned for each
        match. The second element is a textual description of where the
        match was found.
    "exts" optionally allows one to specify a list of extensions to use
        instead of the standard list for this system. This can
        effectively be used as an optimization to, for example, avoid
        stat's of "foo.vbs" when searching for "foo" and you know it is
        not a VisualBasic script but ".vbs" is on PATHEXT. This option
        is only supported on Windows.
    """
    return list( whichgen(command, path, verbose, exts) )

########NEW FILE########
__FILENAME__ = test_archive
import os
import shutil
import tempfile
from nose.tools import *
from holland.lib.archive import *

def setup_func():
    global tmpdir
    tmpdir = tempfile.mkdtemp()

def teardown_func():
    global tmpdir
    shutil.rmtree(tmpdir)
    
@with_setup(setup_func, teardown_func)
def test_dir_archive():
    global tmpdir
    axv = DirArchive(os.path.join(tmpdir, 'dir'))
    name_list = []
    for num in xrange(1, 16):
        fd, filename = tempfile.mkstemp(dir=tmpdir)
        os.close(fd)
        basename = os.path.basename(filename)
        axv.add_file(filename, basename)
        name_list.append(basename)

    for name in axv.list():
        ok_(name in name_list)

@with_setup(setup_func, teardown_func)
def test_tar_archive():
    global tmpdir
    axv = TarArchive(os.path.join(tmpdir, 'tar'))
    name_list = []

    for num in xrange(1, 16):
        fd, filename = tempfile.mkstemp(dir=tmpdir)
        os.close(fd)
        basename = os.path.basename(filename)
        axv.add_file(filename, basename)
        name_list.append(basename)

    for name in axv.list():
        ok_(name in name_list)
   
@with_setup(setup_func, teardown_func)
def test_zip_archive():
    global tmpdir
    axv = ZipArchive(os.path.join(tmpdir, 'zip'))
    name_list = []

    for num in xrange(1, 16):
        fd, filename = tempfile.mkstemp(dir=tmpdir)
        os.close(fd)
        basename = os.path.basename(filename)
        axv.add_file(filename, basename)
        name_list.append(basename)

    for name in axv.list():
        ok_(name in name_list)

########NEW FILE########
__FILENAME__ = test_compression
import os
import shutil
from nose.tools import *
from tempfile import mkdtemp

from holland.core.exceptions import ArgumentError
from holland.lib import compression

global tmpdir, config, plugin

def setup_func():
    global tmpdir
    tmpdir = mkdtemp()

def teardown_func():
    global tmpdir
    shutil.rmtree(tmpdir)
    
@with_setup(setup_func, teardown_func)
def test_lookup_compression():
    global tmpdir
    
    # should get something back, None if the method wasn't found
    cmd, ext = compression.lookup_compression('gzip')
    assert_not_equal(cmd, None)
    assert_not_equal(ext, None)
    
    # should get something back, None if the method wasn't found
    cmd, ext = compression.lookup_compression('bzip2')
    assert_not_equal(cmd, None)
    assert_not_equal(ext, None)
    
    # should get something back, None if the method wasn't found
    cmd, ext = compression.lookup_compression('lzop')
    assert_not_equal(cmd, None)
    assert_not_equal(ext, None)
    
    # this should fail
    cmd, ext = compression.lookup_compression('bogus_compression')
    assert_equal(cmd, None)
    assert_equal(ext, None)
    
@with_setup(setup_func, teardown_func)
def test_compression():
    global tmpdir
    
    # gzip - write it, read it, verify it
    f = compression.open_stream(os.path.join(tmpdir, 'gzip_foo'), 'w', 'gzip')
    f.write('foo')
    f.close()
    
    f = compression.open_stream(os.path.join(tmpdir, 'gzip_foo'), 'r', 'gzip')
    foo = f.read(3)
    f.close()

    ok_(foo == 'foo')
    
    # bzip2 - write it, read it, verify it
    f = compression.open_stream(os.path.join(tmpdir, 'bzip2_foo'), 'w', 'bzip2')
    f.write('foo')
    f.close()
    
    f = compression.open_stream(os.path.join(tmpdir, 'bzip2_foo'), 'r', 'bzip2')
    foo = f.read(3)
    f.close()

    ok_(foo == 'foo')
    
    # gzip - write it, read it, verify it
    f = compression.open_stream(os.path.join(tmpdir, 'lzop_foo'), 'w', 'lzop')
    f.write('foo')
    f.close()
    
    f = compression.open_stream(os.path.join(tmpdir, 'lzop_foo'), 'r', 'lzop')
    foo = f.read(3)
    f.close()

    ok_(foo == 'foo')
    
@raises(IOError)    
@with_setup(setup_func, teardown_func)
def test_compression_wrong_method():
    global tmpdir
    
    # gzip - write it, read it, verify it
    f = compression.open_stream(os.path.join(tmpdir, 'foo'), 'w', 'gzip')
    f.write('foo')
    f.close()
    
    f = compression.open_stream(os.path.join(tmpdir, 'foo'), 'r', 'bzip2')
    foo = f.read(3)
    f.close()

@raises(ArgumentError)
@with_setup(setup_func, teardown_func)
def test_compression_bad_mode():
    global tmpdir

    f = compression.open_stream(os.path.join(tmpdir, 'foo'), 'w', 'gzip')
    f.write('foo')
    f.close()
    
    f = compression.open_stream(os.path.join(tmpdir, 'foo'), 'bad', 'gzip')
    f.write('foo')
    f.close()
    
    
########NEW FILE########
__FILENAME__ = base
"""High-level Object Oriented LVM API"""

import os
import signal
import logging
from holland.lib.lvm.raw import pvs, vgs, lvs, lvsnapshot, lvremove, \
                                mount, umount, blkid
from holland.lib.lvm.util import getdevice, SignalManager
from holland.lib.lvm.errors import LVMCommandError

LOG = logging.getLogger(__name__)

class Volume(object):
    """Abstract Volume object for LVM Volume implementations

    This class should not directly be instantiated, but rather one
    of its subclasses such as PhysicalVolume, VolumeGroup or LogicalVolume
    """

    def __new__(cls, attributes=()):
        if cls is Volume:
            raise NotImplementedError('Volume is an abstract base class and '
                                      'should not be directly instantiated')
        return super(Volume, cls).__new__(cls)

    def __init__(self, attributes=()):
        self.attributes = dict(attributes)


    def __getattr__(self, key):
        try:
            return self.attributes[key]
        except KeyError:
            return super(Volume, self).__getattribute__(key)

    def reload(self):
        """Reload a Volume with underlying data, which may have changed"""

        raise NotImplementedError()

    def lookup(cls, pathspec):
        """Lookup a volume for the pathspec given

        This will always return the first volume and raise an error
        if multiple volumes are found.

        :returns: Volume instance
        """
        raise NotImplementedError()
    lookup = classmethod(lookup)

    def search(cls, pathspec=None):
        """Search for volumes for the pathspec given

        This will search for any volumes matching the pathspec and return
        an iterable to walk across the volumes found.

        :returns: iterable of Volume instances
        """
        raise NotImplementedError()
    search = classmethod(search)

    def __repr__(self):
        return '%s()' % (self.__class__.__name__,)

class PhysicalVolume(Volume):
    """LVM Physical Volume representation"""

    def reload(self):
        """Reload this PhysicalVolume"""
        self.attributes, = pvs(self.pv_name)

    def lookup(cls, pathspec):
        """Lookup a physical volume for the pathspec given

        This will always return the first volume found and raise an error
        if multiple volumes match ``pathspec``

        :returns: PhysicalVolume instance
        """
        try:
            volume, = pvs(pathspec)
            return cls(volume)
        except (ValueError, LVMCommandError):
            raise LookupError("No PhysicalVolume could be found for "
                              "pathspec %r" %
                              pathspec)
    lookup = classmethod(lookup)

    def search(cls, pathspec=None):
        """Search for volumes matching ``pathspec``

        This will search for any physical volumes matching ``pathspec`` and
        return an iterable that provides instance of PhysicalVolume

        :returns: iterable of PhysicalVolume instances
        """

        for volume in pvs(pathspec):
            yield cls(volume)
    search = classmethod(search)

    def __repr__(self):
        return "%s(device=%r)" % (self.__class__.__name__, self.pv_name)

class VolumeGroup(Volume):
    """LVM VolumeGroup representation"""

    def reload(self):
        """Reload this VolumeGroup"""
        self.attributes, = vgs(self.vg_name)

    def lookup(cls, pathspec):
        """Lookup a volume group for ``pathspec``

        This will always return the first volume group found and raise an error
        if multiple volumes match ``pathspec``

        :returns: VolumeGroup instance
        """
        try:
            volume, = vgs(pathspec)
            return cls(volume)
        except (LVMCommandError, ValueError):
            raise LookupError("No VolumeGroup could be found for pathspec %r" %
                              pathspec)
    lookup = classmethod(lookup)

    def search(cls, pathspec=None):
        """Search for volume groups matching ``pathspec``

        This will search for any volume groups matching ``pathspec`` and
        return an iterable that provides instance of VolumeGroup

        :returns: iterable of VolumeGroup instances
        """

        for volume in vgs(pathspec):
            yield cls(volume)
    search = classmethod(search)

    def __repr__(self):
        return '%s(vg_name=%s)' % (self.__class__.__name__, self.vg_name)

class LogicalVolume(Volume):
    """LVM Logical Volume representation"""

    def lookup(cls, pathspec):
        """Lookup a logical volume for ``pathspec``

        This will always return the first volume group found and raise an error
        if multiple volumes match ``pathspec``

        :returns: LogicalVolume instance
        """
        try:
            volume, = lvs(pathspec)
            return cls(volume)
        except (LVMCommandError, ValueError):
            #XX: Perhaps we should be more specific :)
            raise LookupError("No LogicalVolume could be found "
                              "for pathspec %r" %
                              pathspec)
    lookup = classmethod(lookup)

    def lookup_from_fspath(cls, path):
        """Lookup a logical volume for the filesystem path ``path``

        :returns: LogicalVolumeInstance
        """
        device = getdevice(path)
        return cls.lookup(device)
    lookup_from_fspath = classmethod(lookup_from_fspath)

    def search(cls, pathspec=None):
        """Search for logical volumes matching ``pathspec``

        This will search for any logical volumes matching ``pathspec`` and
        return an iterable that provides instances of LogicalVolume

        :returns: iterable of LogicalVolume instances
        """

        for volume in lvs(pathspec):
            yield cls(volume)
    search = classmethod(search)

    def reload(self):
        """Reload the data for this LogicalVolume"""
        self.attributes, = lvs(self.device_name())

    def snapshot(self, name, size):
        """Snapshot the current LogicalVolume instance and create a snapshot
        volume with the requested volume name and size

        :param name: name of the volume
        :param size: size of the snapshot
        :raises: LVMCommandError on error
        :returns: LogicalVolume that is a snapshot of this one on success
        """

        try:
            lvsnapshot(self.device_name(), name, size)
        except LVMCommandError, exc:
            for line in exc.error.splitlines():
                LOG.error("%s", line)
            raise
        return LogicalVolume.lookup(self.vg_name + '/' + name)

    def is_mounted(self):
        """Check if this logical volume is mounted

        :returns: True if mounted and false otherwise
        """
        real_device_path = os.path.realpath(self.device_name())
        for line in open('/proc/mounts', 'r'):
            dev = line.split()[0]
            if os.path.realpath(dev) == real_device_path:
                return True
        else:
            return False

    def mount(self, path, options=None):
        """Mount this volume on the specified path

        :param path: path where this volume should be mounted
        :param options: options to pass to mount
        """
        try:
            mount(self.device_name(), path, options)
        except LVMCommandError, exc:
            for line in exc.error.splitlines():
                LOG.error("%s", line)
            raise

    def unmount(self):
        """Unmount this volume, if mounted"""
        try:
            umount(self.device_name())
        except LVMCommandError, exc:
            for line in exc.error.splitlines():
                LOG.error("%s", line)
            raise

    def remove(self):
        """Remove this LogicalVolume

        The data on this object is not longer valid once this method
        successfully returns

        :raises: LVMCommandError on error
        """
        try:
            lvremove(self.device_name())
        except LVMCommandError, exc:
            for line in exc.error.splitlines():
                LOG.error("%s", line)
            raise

    def exists(self):
        """Check whether the volume currently exists

        :returns: bool. True if the volume exists or false otherwise
        """
        try:
            return self.lookup(self.device_name()) is not None
        except (LookupError, LVMCommandError):
            return False

    def volume_group(self):
        """Lookup this LogicalVolume's volume_group

        :returns: VolumeGroup
        """
        return VolumeGroup.lookup(self.vg_name)

    def device_name(self):
        """Lookup the canonical device name for the underlying locail volume

        :returns: device name string
        """
        return '/dev/' + self.vg_name + '/' + self.lv_name

    def filesystem(self):
        """Lookup the filesystem type for the underyling logical volume

        :returns: filesystem type name string
        """
        try:
            device_info, = blkid(self.device_name())
            LOG.debug("Looked up device_info => %r", device_info)
            return device_info['type']
        except (LVMCommandError, ValueError), exc:
            LOG.debug("Failed looking up filesystem for %s => %r",
                      self.device_name(), exc, exc_info=True)
            raise LookupError()

    def __repr__(self):
        return '%s(device=%r)' % (self.__class__.__name__, self.device_name())

########NEW FILE########
__FILENAME__ = constants
"""Constants used by various bits in the LVM API"""

PVS_ATTR = [
    'pv_fmt',
    'pv_uuid',
    'pv_size',
    'dev_size',
    'pv_free',
    'pv_used',
    'pv_name',
    'pv_attr',
    'pv_pe_count',
    'pv_pe_alloc_count',
    'pv_tags',
    # segment info introduces duplicate records
    # (with differing seginfo data)
    #'pvseg_start',
    #'pvseg_size',
    'pe_start',
    'vg_name'
]

VGS_ATTR = [
    'vg_fmt',
    'vg_uuid',
    'vg_name',
    'vg_attr',
    'vg_size',
    'vg_free',
    'vg_sysid',
    'vg_extent_size',
    'vg_extent_count',
    'vg_free_count',
    'max_lv',
    'max_pv',
    'pv_count',
    'lv_count',
    'snap_count',
    'vg_seqno',
    'vg_tags'
]

LVS_ATTR = [
    'lv_uuid',
    'lv_name',
    'lv_attr',
    'lv_major',
    'lv_minor',
    'lv_kernel_major',
    'lv_kernel_minor',
    'lv_size',
    'seg_count',
    'origin',
    'snap_percent',
    'copy_percent',
    'move_pv',
    'lv_tags',
# segment information break 1:1 mapping
#    'segtype',
#    'stripes',
#    'stripesize',
#    'chunksize',
#   'seg_start',
#   'seg_size',
#   'seg_tags',
#   'devices',
#    'regionsize',
    'mirror_log',
    'modules',
    'vg_name',
    'vg_extent_size',
    'vg_extent_count',
    'vg_free_count',
]

########NEW FILE########
__FILENAME__ = errors
"""Standard errors raised by holland.lib.lvm"""

class SnapshotError(Exception):
    """Error occurred during snapshotting process"""

class LVMCommandError(Exception):
    """Error occurred while running a lvm command
    
    :attribute cmd: The command that was being run
    :attribute status: exit status of the command
    :attribute error: stderr output of the command
    """

    def __init__(self, cmd, status, error):
        error = error and error.strip() or ''
        Exception.__init__(self, cmd, status, error)
        self.cmd = cmd
        self.status = status
        self.error = error

    def __str__(self):
        return "Command '%s' exited with status %d" % (self.cmd, self.status)

########NEW FILE########
__FILENAME__ = api
"""Low level command interface to LVM"""

import os
import csv
import logging
from subprocess import Popen, PIPE, list2cmdline, call
from cStringIO import StringIO

LOG = logging.getLogger(__name__)

class CalledProcessError(Exception):
    """This exception is raised when a process run by check_call() returns
    a non-zero exit status.  The exit status will be stored in the
    returncode attribute."""
    def __init__(self, returncode, cmd):
        self.returncode = returncode
        self.cmd = cmd
        Exception.__init__()

    def __str__(self):
        return "Command '%s' returned non-zero exit status %d" % \
                (self.cmd, self.returncode)   


def check_call(*popenargs, **kwargs):
    """Run command with arguments.  Wait for command to complete.  If
    the exit code was zero then return, otherwise raise
    CalledProcessError.  The CalledProcessError object will have the
    return code in the returncode attribute.

    The arguments are the same as for the Popen constructor.  Example:

    check_call(["ls", "-l"])
    """
    LOG.debug("%s", list2cmdline(popenargs[0]))
    retcode = call(*popenargs, **kwargs)
    cmd = kwargs.get("args")
    if cmd is None:
        cmd = popenargs[0]
    if retcode:
        raise CalledProcessError(retcode, cmd)
    return retcode


__all__ = [
    'is_lvm_device',
    'lvs',
    'vgs',
    'pvs',
    'lvcreate',
    'lvsnapshot',
    'lvremove',
    'mount',
    'unmount',
    'lvm_dict',
    'lvm_cmd',
]

PVS_ATTRIBUTES = [
    'pv_fmt',
    'pv_uuid',
    'pv_size',
    'dev_size',
    'pv_free',
    'pv_used',
    'pv_name',
    'pv_attr',
    'pv_pe_count',
    'pv_pe_alloc_count',
    'pv_tags',
    # segment info introduces duplicate records
    # (with differing seginfo data)
    #'pvseg_start',
    #'pvseg_size',
    'pe_start',
    'vg_name'
]

VGS_ATTRIBUTES = [
    'vg_fmt',
    'vg_uuid',
    'vg_name',
    'vg_attr',
    'vg_size',
    'vg_free',
    'vg_sysid',
    'vg_extent_size',
    'vg_extent_count',
    'vg_free_count',
    'max_lv',
    'max_pv',
    'pv_count',
    'lv_count',
    'snap_count',
    'vg_seqno',
    'vg_tags'
]

LVS_ATTRIBUTES = [
    'lv_uuid',
    'lv_name',
    'lv_attr',
    'lv_major',
    'lv_minor',
    'lv_kernel_major',
    'lv_kernel_minor',
    'lv_size',
    'seg_count',
    'origin',
    'snap_percent',
    'copy_percent',
    'move_pv',
    'lv_tags',
# segment information break 1:1 mapping
#    'segtype',
#    'stripes',
#    'stripesize',
#    'chunksize',
#   'seg_start',
#   'seg_size',
#   'seg_tags',
#   'devices',
#    'regionsize',
    'mirror_log',
    'modules',
    'vg_name'
]

def is_lvm_device(dev_path):
    """Check if a device path refers to an LVM physical volume
    :param dev_path: path to the device to check
    :returns: True if dev_path is a pv and False otherwise
    """
    args = [
        'pvs',
        '--noheadings',
        '--options', 'pv_name',
    ]
    data = str(lvm_cmd(*args))
    devices = [line.strip() for line in data.splitlines()]
    return dev_path in devices

def pvs(name=None):
    """List available physical volumes and return a dictionary of attributes"""
    args = [
        'pvs',
        '--noheadings',
        '--nosuffix',
        '--separator=,',
        '--units=b',
        '--options', ','.join(PVS_ATTRIBUTES)
    ]
    if name:
        args.append(name)
    data = lvm_cmd(*args)
    return lvm_dict(PVS_ATTRIBUTES, data)

def vgs(name=None):
    """List available volume groups and return a dictionary of attributes"""
    args = [
        'vgs',
        '--noheadings',
        '--nosuffix',
        '--separator=,',
        '--units=b',
        '--options', ','.join(VGS_ATTRIBUTES)
    ]
    if name:
        args.append(name)
    data = lvm_cmd(*args)
    return lvm_dict(VGS_ATTRIBUTES, data)

def lvs(name=None):
    """List available logical volumes and return a dictionary of attributes

    If a name parameter is specified, only matching logical volumes
    will be returned.
    """
    args = [
        'lvs',
        '--noheadings',
        '--nosuffix',
        '--separator=,',
        '--units=b',
        '--options', ','.join(LVS_ATTRIBUTES)
    ]
    if name:
        args.append(name)
    data = lvm_cmd(*args)
    return lvm_dict(LVS_ATTRIBUTES, data)

def lvcreate(lv_name, lv_size, vg_name):
    """Create a new logical volume in the specified volume group"""
    args = [
        'lvcreate',
        '--name', lv_name,
        '--size', str(lv_size),
        vg_name
    ]
    check_call(args)

def lvsnapshot(lv_name, snapshot_name, snapshot_extents):
    """Snapshot an existing logical volume using the specified
    snapshot name and snapshot size.

    snapshot_size should be a valid LVM size string.
    """
    args = [
        'lvcreate',
        '--snapshot',
        '--name', snapshot_name,
        '--extents', str(int(snapshot_extents)),
        lv_name
    ]
    check_call(args)

def lvremove(name):
    """Forcibly remove a logical volume.

    WARNING: This is dangerous and care should be taken before
    calling this method.
    """
    device = os.path.join(os.sep, 'dev', name)
    args = [
        'lvremove',
        '-f',
        device
    ]
    check_call(args)

def mount(device, path, options=None):
    """Mount the specified path or device"""
    #   mount exit status on linux defined by mount(1)
    #   0      success
    #   1      incorrect invocation or permissions
    #   2      system error (out of memory, cannot fork, no more loop devices)
    #   4      internal mount bug or missing nfs support in mount
    #   8      user interrupt
    #   16     problems writing or locking /etc/mtab
    #   32     mount failure
    #   64     some mount succeeded
    if not device:
        raise ValueError("No device specified to mount")
    if not path:
        raise ValueError("No path specified to mount")

    args = [
        'mount',
        device,
        path,
    ]
    if options:
        args.insert(1, options)
        args.insert(1, '-o')

    check_call(*args)

def unmount(path_or_device):
    """Unmount the specified path or device."""
    check_call(['umount', path_or_device])

def lvm_cmd(cmd, *args):
    """Run a LVM command and return the output.

    If a command returns non-zero status, an OSError is raised and the
    errno is set to the returncode of the command.

    For LVM, we also detect the string 'WARNING:' in output, which also
    tends to imply a failure.
    """
    pid = Popen([cmd] + list(args), stdout=PIPE, stderr=PIPE, close_fds=True)
    stdoutdata, stderrdata = pid.communicate()
    if pid.returncode != 0:
        raise CalledProcessError(pid.returncode, cmd)

    # WARNING: Likely means something is seriously broken - either we're not
    # root or the base LVM setup needs fixing. To err on safety we raise
    # an EnvironmentError in this case.
    if "WARNING:" in stderrdata:
        raise CalledProcessError(0, cmd)

    return stdoutdata

def lvm_dict(keys, values):
    """Convert LVM tool output into a dictionary"""
    stream = StringIO(values)
    kwargs = dict(delimiter=',', skipinitialspace=True)
    for row in csv.reader(stream, **kwargs):
        yield dict(zip(keys, row))

########NEW FILE########
__FILENAME__ = base
"""Object Oriented LVM interface"""

import os
import logging
from holland.backup.lvm.util.path import getmount, getdevice
from api import is_lvm_device, pvs, vgs, lvs, mount, unmount, \
                lvsnapshot, lvremove, LVMError
from fmt import format_size, parse_size

__all__ = [
    'PhysicalVolume',
    'VolumeGroup',
    'LogicalVolume',
    'LVMError'
]

class PhysicalVolume(object):
    def __init__(self, **kwargs):
        self.__dict__.update(kwargs)

    def find_one(cls, name):
        for pvinfo in pvs(name):
            return PhysicalVolume(**pvinfo)
    find_one = classmethod(find_one)

class VolumeGroup(object):
    """LVM Volume Group"""
    def __init__(self, **kwargs):
        # silence pylint
        self.vg_name = None
        self.__dict__.update(kwargs)

    def find_all(cls, name):
        """Find all volume groups matching the given name"""
        result = []
        for vginfo in vgs(name):
            logging.debug("Creating VolumeGroup from info %r", vginfo)
            result.append(VolumeGroup(**vginfo))
        return result
    find_all = classmethod(find_all)
    find = find_all

    def find_one(cls, name):
        """Find first LVM volume group matching the given name"""
        for vginfo in vgs(name):
            logging.debug("Creating VolumeGroup from info %r", vginfo)
            return VolumeGroup(**vginfo)
    find_one = classmethod(find_one)

    def lvs(self):
        """List logical volumes for this volume group"""
        lv_list = LogicalVolume.find(None)
        result = []
        for logical_volume in lv_list:
            if logical_volume.vg_name == self.vg_name:
                result.append(logical_volume)
        return result
    lvs = property(lvs)

    def __str__(self):
        """String representation of this volume group"""
        return repr(self)

    def __repr__(self):
        """Representation of this volume group"""
        attributes = ['%s=%r' % (key, value)
                        for key, value in self.__dict__.items()
                            if key != 'self']
        return 'VolumeGroup(%s)' % ','.join(attributes)

class LogicalVolume(object):
    """LVM Logical Volume"""

    # 15G max snapshot size in bytes
    # Used when no snapshot size is provided
    MAX_SNAPSHOT_SIZE = 15*1024**3

    def __init__(self, **kwargs):
        # silence pylint
        self.vg_name = None
        self.lv_name = None
        self.lv_attr = None
        self.__dict__.update(kwargs)

    def find_all(cls, name):
        """Find all logical volumes matching name"""
        result = []
        for lvinfo in lvs(name):
            result.append(LogicalVolume(**lvinfo))
        return result
    find_all = classmethod(find_all)
    find = find_all

    def find_one(cls, name):
        """Find the first logical volume matching name"""
        for lvinfo in lvs(name):
            return LogicalVolume(**lvinfo)
    find_one = classmethod(find_one)

    def find_mounted(cls, path):
        """Find a currently mounted logical volume by mountpoint path"""
        dev = getdevice(getmount(path))
        try:
            return cls.find_one(dev)
        except LVMError, exc:
            logging.debug("Failed to find logical volume for device %r (path=%r): %s", dev, path, exc)
            return None
    find_mounted = classmethod(find_mounted)

    def vgs(self):
        """Find volume group for this Logical Volume"""
        vg_list = VolumeGroup.find(self.vg_name)

        logging.debug("vg_list => %r", vg_list)
        #assert len(vg_list) != 0,
        #    "No volume group found for logical volume %r" % \
        # (self.lv_name)
        #assert len(vg_list) == 1,
        #    "More than one volume group found for logical volume %r" % \
        #   (self.vg_name)

        return vg_list[0]
    vgs = property(vgs)
    volume_group = vgs

    def device_path(self):
        """Find the canonical path for this logical volume

        This returns a string path of the form
        /dev/volume-group/logical-volume
        """
        return os.path.join(os.sep, 'dev',
                            self.volume_group.vg_name,
                            self.lv_name)
    device_path = property(device_path)

    def exists(self):
        """Check whether this logical volume still exists"""
        return os.path.exists(self.device_path)

    def is_mounted(self):
        """Check if this logical volume is mounted"""
        device = os.path.join('/dev', self.vg_name, self.lv_name)
        real_device_path = os.path.realpath(device)
        for line in open('/proc/mounts', 'r'):
            dev = line.split()[0]
            if os.path.realpath(dev) == real_device_path:
                return True
        else:
            return False

    def snapshot(self, name=None, size=None):
        """Snapshot this logical volume with specified name and size"""
        if not name:
            name = self.lv_name + '_snapshot'
        volume_group = self.volume_group
        if not size:
            vg_size = int(volume_group.vg_size)
            vg_free = int(volume_group.vg_free)
            size = min(vg_size*0.2, vg_free, self.MAX_SNAPSHOT_SIZE)
        else:
            size = parse_size(size)

        # floor division to avoid exceeding the actual available extents
        snapshot_extents = size // int(volume_group.vg_extent_size)

        if snapshot_extents == 0:
            raise LVMError("No free snapshot space on %s" % str(self))

        if snapshot_extents > volume_group.vg_free_count:
            raise LVMError(
                "Excessive snapshot size (%s) exceeds free extents (%d)." % \
                    (size, self.vg_free_count)
                )

        lvsnapshot(lv_name='/'.join([self.vg_name, self.lv_name]),
                   snapshot_name=name,
                   snapshot_extents=snapshot_extents)

        return LogicalVolume.find('/'.join([self.vg_name, name]))[0]

    def mount(self, path):
        """Mount this logical volume and the specified path"""
        device = '/'.join(['/dev', self.vg_name, self.lv_name])
        logging.info("mounting %s to %s", device, path)
        options = None
        if fs_type(device) == 'xfs':
            options = 'nouuid'
        mount(device, path, options)
        assert self.is_mounted() is True, ("Mount of %s to %s failed " + \
                                           "even though mount() did " + \
                                           "not raise an error" % \
                                           (device, path))

    def unmount(self):
        """Unmount this logical volume"""
        device = '/'.join(['/dev', self.vg_name, self.lv_name])
        unmount(device)
        assert self.is_mounted() is False, "Unmount of %s failed" % (device,)

    def remove(self):
        """Remove this logical volume, if it is a snapshot"""
        try:
            self.refresh()
        except:
            logging.error("Failed to refresh %s", self)

        assert self.lv_attr[0] in 'sS', "Only snapshots can be " + \
                                        "removed via this API"

        if self.lv_attr[0] == 'S':
            logging.fatal("Snapshot %s exceeded size of %s",
                            os.path.join('/dev/', self.vg_name, self.lv_name),
                            format_size(int(self.lv_size)))

        if self.is_mounted():
            raise AssertionError("Logical volume %r is mounted." %
                os.path.join(self.vg_name, self.lv_name))
        lvremove('/'.join([self.vg_name, self.lv_name]))

    def refresh(self):
        """Refresh this LogicalVolume to match the underlying volume"""
        for lv in lvs():
            if lv['lv_uuid'] == self.lv_uuid:
                self.__dict__.update(lv)
                return
        else:
            raise LVMError("No system logical volume found with uuid %s" % self.lv_uuid)

    def __str__(self):
        extra = self.lv_attr in 'sS' and '[snapshot]' or ''
        return os.path.join("%s/dev" % extra,
                            self.vg_name, self.lv_name)

    def __repr__(self):
        attributes = ['%s=%r' % (key, value)
                        for key, value in self.__dict__.items()
                            if key != 'self']
        return "LogicalVolume(%s)" % ','.join(attributes)

########NEW FILE########
__FILENAME__ = raw
"""Raw LVM command API"""

import os
import re
import csv
import logging
from cStringIO import StringIO
from subprocess import Popen, PIPE, STDOUT, list2cmdline

from holland.lib.lvm.constants import PVS_ATTR, VGS_ATTR, LVS_ATTR
from holland.lib.lvm.errors import LVMCommandError

LOG = logging.getLogger(__name__)

def pvs(*physical_volumes):
    """Report information about physical volumes

    :param volume_groups: volume groups to report on
    :returns: list of dicts of pvs parameters
    """
    pvs_args = [
        'pvs',
        '--unbuffered',
        '--noheadings',
        '--nosuffix',
        '--units=b',
        '--separator=,',
        '--options=%s' % ','.join(PVS_ATTR),
    ]
    pvs_args.extend(list(physical_volumes))
    process = Popen(pvs_args,
                    stdout=PIPE,
                    stderr=PIPE,
                    preexec_fn=os.setsid,
                    close_fds=True)
    stdout, stderr = process.communicate()

    if process.returncode != 0:
        raise LVMCommandError('pvs', process.returncode, stderr)

    return parse_lvm_format(PVS_ATTR, stdout)

def vgs(*volume_groups):
    """Report information about volume groups

    :param volume_groups: volume groups to report on
    :returns: list of dicts of vgs parameters
    """
    vgs_args = [
        'vgs',
        '--unbuffered',
        '--noheadings',
        '--nosuffix',
        '--units=b',
        '--separator=,',
        '--options=%s' % ','.join(VGS_ATTR),
    ]
    vgs_args.extend(list(volume_groups))
    process = Popen(vgs_args,
                    stdout=PIPE,
                    stderr=PIPE,
                    preexec_fn=os.setsid,
                    close_fds=True)
    stdout, stderr = process.communicate()

    if process.returncode != 0:
        raise LVMCommandError('vgs', process.returncode, stderr)

    return parse_lvm_format(VGS_ATTR, stdout)

def lvs(*volume_groups):
    """Report information about logical volumes

    `volume_groups` may refer to either an actual volume-group name or to a
    logical volume path to refer to a single logical volume

    :param volume_groups: volumes to report on
    :returns: list of dicts of lvs parameters
    """
    lvs_args = [
        'lvs',
        '--unbuffered',
        '--noheadings',
        '--nosuffix',
        '--units=b',
        '--separator=,',
        '--options=%s' % ','.join(LVS_ATTR),
    ]
    lvs_args.extend(list(volume_groups))
    process = Popen(lvs_args,
                    stdout=PIPE,
                    stderr=PIPE,
                    preexec_fn=os.setsid,
                    close_fds=True)
    stdout, stderr = process.communicate()

    if process.returncode != 0:
        raise LVMCommandError('lvs', process.returncode, stderr)

    return parse_lvm_format(LVS_ATTR, stdout)


def parse_lvm_format(keys, values):
    """Convert LVM tool output into a dictionary"""
    stream = StringIO(values)
    for row in csv.reader(stream, delimiter=',', skipinitialspace=True):
        yield dict(zip(keys, row))

def lvsnapshot(orig_lv_path, snapshot_name, snapshot_extents, chunksize=None):
    """Create a snapshot of an existing logical volume

    :param snapshot_lv_name: name of the snapshot
    :param orig_lv_path: path to the logical volume being snapshotted
    :param snapshot_extents: size to allocate to snapshot volume in extents
    :param chunksize: (optional) chunksize of the snapshot volume
    """
    lvcreate_args = [
        'lvcreate',
        '--snapshot',
        '--name', snapshot_name,
        '--extents', "%d" % snapshot_extents,
        orig_lv_path,
    ]

    if chunksize:
        lvcreate_args.insert(-1, '--chunksize')
        lvcreate_args.insert(-1, chunksize)

    LOG.debug("%s", list2cmdline(lvcreate_args))
    process = Popen(lvcreate_args,
                    stdout=PIPE,
                    stderr=PIPE,
                    preexec_fn=os.setsid,
                    close_fds=True)

    stdout, stderr = process.communicate()

    for line in stdout.splitlines():
        if not line:
            continue
        LOG.debug("lvcreate: %s", line)

    if process.returncode != 0:
        raise LVMCommandError(list2cmdline(lvcreate_args),
                              process.returncode,
                              str(stderr).strip())

def lvremove(lv_path):
    """Remove a logical volume

    :param lv_path: logical volume to remove
    :raises: LVMCommandError if lvremove returns with non-zero status
    """
    lvremove_args = [
        'lvremove',
        '--force',
        lv_path,
    ]

    process = Popen(lvremove_args,
                    stdout=PIPE,
                    stderr=PIPE,
                    preexec_fn=os.setsid,
                    close_fds=True)

    stdout, stderr = process.communicate()

    for line in str(stdout).splitlines():
        if not line:
            continue
        LOG.debug("%s : %s", list2cmdline(lvremove_args), line)

    if process.returncode != 0:
        raise LVMCommandError(list2cmdline(lvremove_args),
                              process.returncode,
                              str(stderr).strip())


## Filesystem utility functions
def blkid(*devices):
    """Locate/print block device attributes

    :param devices: devices to run blkid against
    :returns: iterable of dicts of blkid data
    """
    blkid_args = [
        'blkid',
        '-c', '/dev/null',
    ]

    blkid_args.extend([os.path.realpath(dev) for dev in devices])

    process = Popen(blkid_args,
                    stdout=PIPE,
                    stderr=PIPE,
                    close_fds=True)
    stdout, stderr = process.communicate()

    if process.returncode != 0:
        cmd_str = list2cmdline(blkid_args)
        raise LVMCommandError(cmd_str, process.returncode, stderr)

    return parse_blkid_format(stdout)

def parse_blkid_format(text):
    """Parse the blkid output format

    :returns: iterable of dicts containing blkid information
    """
    blkid_cre = re.compile(r'(?P<device>.+?)[:] (?P<values>.*)')
    values_cre = re.compile(r'(?P<LABEL>[A-Z_]+)[=]"(?P<VALUE>[^"]+)')
    for line in text.splitlines():
        device, values = blkid_cre.match(line).group('device', 'values')
        key_values = [(key.lower(), value) for key, value
                                            in values_cre.findall(values)]
        yield dict(key_values, device=device)

def mount(device, path, options=None, vfstype=None):
    """Mount a filesystem

    :raises: LVMCommandError
    """
    mount_args = [
        'mount',
    ]
    if options:
        mount_args.extend(['-o', options])
    if vfstype:
        mount_args.extend(['-t', vfstype])
    mount_args.extend([device, path])

    process = Popen(mount_args,
                    stdout=PIPE,
                    stderr=PIPE,
                    preexec_fn=os.setsid,
                    close_fds=True)
    stdout, stderr = process.communicate()

    if process.returncode != 0:
        cmd_str = list2cmdline(mount_args)
        raise LVMCommandError(cmd_str, process.returncode, stderr)

    return stdout

def umount(*path):
    """Unmount a file system

    :raises: LVMCommandError
    """
    process = Popen(['umount'] + list(path),
                    stdout=PIPE,
                    stderr=PIPE,
                    preexec_fn=os.setsid,
                    close_fds=True)

    stdout, stderr =  process.communicate()

    if process.returncode != 0:
        cmd_str = list2cmdline(['umount'] + list(path))
        raise LVMCommandError(cmd_str, process.returncode, stderr)

    return stdout

os.environ['PATH'] = '/sbin:/usr/sbin:' + os.environ['PATH']

########NEW FILE########
__FILENAME__ = snapshot
"""LVM Snapshot state machine"""

import sys
import signal
import logging
from holland.lib.lvm.errors import LVMCommandError
from holland.lib.lvm.util import SignalManager, format_bytes

LOG = logging.getLogger(__name__)

__all__ = [
    'Snapshot',
    'CallbackFailuresError',
]

class Snapshot(object):
    """Snapshot state machine"""
    def __init__(self, name, size, mountpoint):
        self.name = name
        self.size = size
        self.mountpoint = mountpoint
        self.callbacks = {}
        self.sigmgr = SignalManager()

    def start(self, volume):
        """Start the snapshot process to snapshot the logical volume
        that ``path`` exists on.

        """
        self.sigmgr.trap(signal.SIGINT, signal.SIGTERM, signal.SIGHUP)
        try:
            self._apply_callbacks('initialize', self)
        except CallbackFailuresError, exc:
            return self.error(None, exc)
        return self.create_snapshot(volume)

    def create_snapshot(self, logical_volume):
        """Create a snapshot for the given logical volume

        """

        try:
            self._apply_callbacks('pre-snapshot', self, None)
            snapshot = logical_volume.snapshot(self.name, self.size)
            LOG.info("Created snapshot volume %s", snapshot.device_name())
        except (LVMCommandError, CallbackFailuresError), exc:
            return self.error(None, exc)

        try:
            self._apply_callbacks('post-snapshot', self, snapshot)
        except CallbackFailuresError, exc:
            return self.error(snapshot, exc)

        return self.mount_snapshot(snapshot)

    def mount_snapshot(self, snapshot):
        """Mount the snapshot"""

        try:
            self._apply_callbacks('pre-mount', self, snapshot)
            options = None
            if snapshot.filesystem() == 'xfs':
                LOG.info("xfs filesystem detected on %s. "
                         "Using mount -o nouuid",
                         snapshot.device_name())
                options = 'nouuid'
            snapshot.mount(self.mountpoint, options)
            LOG.info("Mounted %s on %s",
                     snapshot.device_name(), self.mountpoint)
            self._apply_callbacks('post-mount', self, snapshot)
        except (CallbackFailuresError, LVMCommandError), exc:
            return self.error(snapshot, exc)

        return self.unmount_snapshot(snapshot)

    def unmount_snapshot(self, snapshot):
        """Unmount the snapshot"""
        try:
            self._apply_callbacks('pre-unmount', snapshot)
            snapshot.unmount()
            LOG.info("Unmounted %s", snapshot.device_name())
        except (CallbackFailuresError, LVMCommandError), exc:
            return self.error(snapshot, exc)

        try:
            self._apply_callbacks('post-unmount', snapshot)
        except CallbackFailuresError, exc:
            return self.error(snapshot, exc)

        return self.remove_snapshot(snapshot)

    def remove_snapshot(self, snapshot):
        """Remove the snapshot"""
        try:
            self._apply_callbacks('pre-remove', snapshot)
            snapshot.remove()
            LOG.info("Removed snapshot %s", snapshot.device_name())
        except (CallbackFailuresError, LVMCommandError), exc:
            return self.error(snapshot, exc)

        try:
            self._apply_callbacks('post-remove', snapshot)
        except (CallbackFailuresError), exc:
            return self.error(snapshot, exc)

        return self.finish()

    def finish(self):
        """Finish the snapshotting process"""
        pending = self.sigmgr.pending[:]
        self.sigmgr.restore()
        if signal.SIGINT in pending:
            raise KeyboardInterrupt()

        self._apply_callbacks('finish', self)
        if sys.exc_info()[1]:
            raise

    def error(self, snapshot, exc):
        """Handle an error during the snapshot process"""
        LOG.debug("Error encountered during snapshot processing: %s", exc)

        if snapshot and snapshot.exists():
            snapshot.reload()
            if 'S' in snapshot.lv_attr:
                LOG.error("Snapshot space (%s) exceeded. "
                          "Snapshot %s is no longer valid",
                          snapshot.device_name(),
                          format_bytes(int(snapshot.lv_size)))
            try:
                if snapshot.is_mounted():
                    snapshot.unmount()
                    LOG.info("Unmounting snapshot %s on cleanup",
                             snapshot.device_name())
                if snapshot.exists():
                    snapshot.remove()
                    LOG.info("Removed snapshot %s on cleanup",
                             snapshot.device_name())
            except LVMCommandError, exc:
                LOG.error("Failed to remove snapshot %s", exc)

        return self.finish()

    def register(self, event, callback, priority=100):
        """Register a callback for ``event`` with ``priority``

        """
        self.callbacks.setdefault(event, []).append((priority, callback))

    def unregister(self, event, callback):
        """Remove a previously registered callback"""
        pending = []
        for info in self.callbacks.get(event, []):
            if callback in info:
                pending.append(info)

        for item in pending:
            self.callbacks[event].remove(item)

    def _apply_callbacks(self, event, *args, **kwargs):
        """Apply callbacks for event"""
        callback_list = list(self.callbacks.get(event, []))
        callback_list.sort()
        callback_list.reverse()
        callback_list = [callback[1] for callback in callback_list]
        for callback in callback_list:
            try:
                LOG.debug("Calling %s", callback)
                callback(event, *args, **kwargs)
            except:
                LOG.debug("Callback %r failed for event %s",
                          callback, event, exc_info=True)
                exc = sys.exc_info()[1]
                raise CallbackFailuresError([(callback, exc)])


class CallbackFailuresError(Exception):
    """Error running callbacks"""

    def __init__(self, errors):
        Exception.__init__(self, errors)
        self.errors = errors


########NEW FILE########
__FILENAME__ = util
"""LVM formatting and validation methods"""

import os
import re
import signal
from math import log

__all__ = [
    'getmount',
    'getdevice',
    'relpath',
    'format_bytes',
    'parse_bytes',
    'SignalManager',
]

def getmount(path):
    """Return the mount point of a path
    
    :param path: path to find the mountpoint for

    :returns: str mounpoint path
    """

    path = os.path.realpath(path)

    while path != os.path.sep:
        if os.path.ismount(path):
            return path
        path = os.path.abspath(os.path.join(path, os.pardir))
    return path

def getdevice(mountpoint):
    """Return the device name for the given mountpoint
    
    This method should return the "top" device for the last device
    mounted on path, in case there are multiple stacked mounts

    :param mountpoint: mountpoint path to find the underlying device for

    :returns: str device path
    """

    if not os.path.exists(mountpoint):
        # don't return a device for a non-existent mountpoint
        return None

    mountpoint = getmount(mountpoint)

    # Read /proc/mounts in reverse order to get the right mountpoint
    # for a stacked mount, as these should be appended to the end

    # For py23 support 'reversed' doesn't exist, so call list.reverse()
    # explicitly
    proc_mounts_info = open('/etc/mtab', 'r').readlines()
    proc_mounts_info.reverse()

    for path in proc_mounts_info:
        device, mount = path.split()[0:2]
        # handle path with spaces - encoded in /etc/mtab
        mount = mount.decode('string_escape')
        mount = os.path.normpath(mount)
        if mount == mountpoint:
            return device

# Taken from posixpath in Python2.6
def relpath(path, start=os.curdir):
    """Return a relative version of a path"""

    if not path:
        raise ValueError("no path specified")

    start_list = [x for x in os.path.abspath(start).split(os.sep) if x]
    path_list = [x for x in os.path.abspath(path).split(os.sep) if x]

    # Work out how much of the filepath is shared by start and path.
    i = len(os.path.commonprefix([start_list, path_list]))

    rel_list = [os.pardir] * (len(start_list)-i) + path_list[i:]
    if not rel_list:
        return os.curdir
    return os.path.join(*rel_list)

def format_bytes(nbytes, precision=2):
    """Format an integer number of bytes to a human readable string."""

    if nbytes != 0:
        exponent = int(log(abs(nbytes), 1024))
    else:       
        exponent = 0

    return "%.*f%s" % (
        precision,
        float(nbytes) / (1024 ** exponent),
        ['Bytes','KB','MB','GB','TB','PB','EB','ZB','YB'][int(exponent)]
    )


def parse_bytes(units_string):
    """Parse an LVM size string into bytes
    
    :returns: integer number of bytes
    """

    units_string = str(units_string)

    units = "bBkKmMgGtTpPeE"

    match = re.match(r'^(\d+(?:[.]\d+)?)([%s]|)$' % units, units_string)
    if not match:
        raise ValueError("Invalid LVM Unit syntax %r" % units_string)
    number, unit = match.groups()
    if not unit:
        unit = 'M'
    unit = unit.upper()

    try:
        exponent = "BKMGTPE".index(unit)
    except ValueError:
        raise ValueError("Invalid unit %r. Must be B,K,M,G,T,P or E" % unit)


    return int(float(number) * 1024 ** (exponent))

class SignalManager(object):
    """Manage signals around critical sections"""

    def __init__(self):
        self.pending = []
        self._handlers = {}

    def trap(self, *signals):
        """Request the set of signals to be trapped """
        for sig in signals:
            prev = signal.signal(sig, self._trap_signal)
            self._handlers[sig] = prev

    def _trap_signal(self, signum, *args):
        """Trap a signal and note it in this instance's pending list"""
        args = args
        self.pending.append(signum)

    def restore(self):
        """Clear pending signals and release trapped signals, restoring the
        original handlers
        """
        del self.pending[:]
        for sig in self._handlers:
            signal.signal(sig, self._handlers[sig])
        self._handlers.clear()

########NEW FILE########
__FILENAME__ = constants
import tempfile

LOOP_DEV = '/dev/loop0'
IMG_FILE = '/tmp/hl_lvm.img'
IMG_SIZE = 128*1024**2
TEST_VG = 'holland'
TEST_LV = 'test_lv'
MNT_DIR = tempfile.mkdtemp()

########NEW FILE########
__FILENAME__ = test_base
import sys
import logging
from nose.tools import *
from holland.lib.lvm.base import *
from holland.lib.lvm.util import getdevice
from tests.constants import *

def test_basevolume():
    assert_raises(NotImplementedError, Volume)

    class Test(Volume):
        pass

    ok_(Test())

    volume = Test({ 'foo' : 'bar', 'baz' : 'biz' })
    assert_equals(volume.foo, 'bar')
    assert_equals(volume.baz, 'biz')
    assert_raises(AttributeError, volume.__getattr__, 'blah')

    assert_raises(NotImplementedError, volume.reload)
    assert_raises(NotImplementedError, Test.lookup, 'foo')
    assert_raises(NotImplementedError, Test.search, 'foo')

    assert_equals(repr(Test()), 'Test()')


class TestPhysicalVolume(object):
    def test_create(self):
        """Test creating a physical volume"""

    def test_reload(self):
        """Test reloading a PhysicalVolume"""
        pv = PhysicalVolume.lookup('/dev/loop0')
        assert_equals(pv.pv_name, '/dev/loop0')
        pv.reload()
        assert_equals(pv.pv_name, '/dev/loop0')

    def test_lookup(self):
        """Test looking up a single physical volume"""
        pv = PhysicalVolume.lookup('/dev/loop0')
        assert_equals(pv.pv_name, '/dev/loop0')

    def test_lookup_failure(self):
        """Test looking up an invalid pv"""
        assert_raises(LookupError, PhysicalVolume.lookup, '/dev/loop1')
        
    def test_search(self):
        """Test searching for a physical volume"""
        # stupid simple test to make sure we're returning an iterable
        # not just a Volume object
        result = PhysicalVolume.search('/dev/loop0')
        ok_(not isinstance(result, Volume))
        pv = result.next()
        ok_(isinstance(pv, PhysicalVolume), "not a physical volume? %r" % pv)
        assert_raises(StopIteration, result.next)

    def test_repr(self):
        pv = PhysicalVolume.lookup('/dev/loop0')
        assert_equals(repr(pv), "PhysicalVolume(device='/dev/loop0')")


class TestVolumeGroup(object):

    def test_create(self):
        vg = VolumeGroup({ 'vg_name' : 'dba', 'vg_extent_size' : 4*1024**2 })
        assert_equals(vg.vg_name, 'dba')
        assert_equals(vg.vg_extent_size, 4194304)

    def test_lookup(self):
        vg = VolumeGroup.lookup("holland")
        assert_equals(vg.vg_name, 'holland')
        assert_equals(vg.lv_count, '1') # only holland/test_lv is created
        assert_equals(vg.pv_count, '1') # only /dev/loopN is assigned

    def test_failing_lookup(self):
        assert_raises(LookupError, VolumeGroup.lookup, 'holland_missing')

    def test_search(self):
        for vg in VolumeGroup.search('holland'):
            assert_equals(vg.vg_name, 'holland')
            assert_equals(vg.lv_count, '1') # only holland/test_lv is created
            assert_equals(vg.pv_count, '1') # only /dev/loopN is assigned

    def test_reload(self):
        # XXX: not sure a good way to check this - do something to change the vg
        vg = VolumeGroup.lookup('holland')
        vg.reload()
        assert_equals(vg.vg_name, 'holland')

    def test_repr(self):
        vg = VolumeGroup.lookup('holland')
        assert_equals(repr(vg), 'VolumeGroup(vg_name=holland)')

class TestLogicalVolume(object):
    def test_create(self):
        """Test creating a LogicalVolume"""
        lv = LogicalVolume({ 'lv_name' : 'mysql', 'vg_name' : 'dba' })
        assert_equals(lv.lv_name, 'mysql')

    def test_happy_lookup(self):
        """Test a loading an existing lv"""
        lv = LogicalVolume.lookup('holland/test_lv')
        assert_equals(lv.vg_name, 'holland')
        assert_equals(lv.lv_name, 'test_lv')

    def test_sad_lookup(self):
        assert_raises(LookupError, LogicalVolume.lookup, 'holland/test_lv_missing')

    def test_look_from_fspath(self):
        lv = LogicalVolume.lookup_from_fspath(MNT_DIR)
        assert_equals(lv.vg_name, TEST_VG)
        assert_equals(lv.lv_name, TEST_LV)

    def test_search(self):
        for lv in LogicalVolume.search('%s/%s' % (TEST_VG, TEST_LV)):
            assert_equals(lv.vg_name, TEST_VG)
            assert_equals(lv.lv_name, TEST_LV)

    def test_reload(self):
        # XXX: need to test changing attributes and reloading
        lv = LogicalVolume.lookup(TEST_VG)
        lv.reload()
        assert_equals(lv.lv_name, TEST_LV)

    def test_snapshot(self):
        lv = LogicalVolume.lookup('%s/%s' % (TEST_VG, TEST_LV))
        snapshot = None
        try:
            snapshot = lv.snapshot(lv.lv_name + '_snapshot', 1)
            ok_(lv != snapshot)
            ok_('s' in snapshot.lv_attr)
            assert_equals(snapshot.lv_size, snapshot.vg_extent_size)
            snapshot.mount(MNT_DIR)
            ok_(snapshot.is_mounted())
            assert_equals(getdevice(MNT_DIR), os.path.realpath(snapshot.device_name()))
            lv.reload()
            snapshot.unmount()
            assert_equals(snapshot.is_mounted(), False)
            snapshot.remove()
            ok_(not snapshot.exists())
            snapshot = None
        finally:
            if snapshot and snapshot.is_mounted():
                snapshot.unmount()
            if snapshot and snapshot.exists():
                snapshot.remove()

    def test_filesystem(self):
        """Test looking up filesystem of lv"""
        lv = LogicalVolume.lookup('holland/test_lv')
        logging.warn("Loaded logical volume lv => %r", lv)
        ok_(lv.exists())
        assert_equals(lv.filesystem(), 'ext3')

    def test_bad_filesystem(self):
        """Test looking for filesystem of a lv that doesn't exist"""
        lv = LogicalVolume()
        lv.vg_name = 'holland'
        lv.lv_name = 'test_lv_missing' # <- doesn't exist in our setup
        assert_raises(LookupError, lv.filesystem)

    def test_volume_group(self):
        lv = LogicalVolume.lookup('%s/%s' % (TEST_VG, TEST_LV))
        vg = lv.volume_group()
        assert_equals(vg.vg_name, lv.vg_name)
        assert_equals(vg.vg_extent_size, lv.vg_extent_size)

    def test_repr(self):
        lv = LogicalVolume.lookup('holland/test_lv')
        assert_equals(repr(lv), 'LogicalVolume(device=\'/dev/holland/test_lv\')')

########NEW FILE########
__FILENAME__ = test_errors
from nose.tools import *
from holland.lib.lvm.errors import *

def test_errors():
    exc = LVMCommandError('cmd', -1, 'error message')
    assert_equal(exc.cmd, 'cmd')
    assert_equal(exc.status, -1)
    assert_equal(exc.error, 'error message')

########NEW FILE########
__FILENAME__ = test_raw
import os, sys
import shutil
import tempfile
import subprocess
from nose.tools import *
from holland.lib.lvm.raw import *
from tests.constants import *

def test_pvs():
    pvs(LOOP_DEV)

def test_vgs():
    vg, = vgs(TEST_VG)
    assert_equals(vg['vg_name'], TEST_VG)
    assert_equals(int(vg['pv_count']), 1)

def test_lvs():
    lv, = lvs('%s/%s' % (TEST_VG,TEST_LV))
    assert_equals(lv['vg_name'], TEST_VG)
    vg_extents = int(lv['vg_extent_count'])
    vg_extent_size = int(lv['vg_extent_size'])

    assert_equals(int(lv['lv_size']), IMG_SIZE / 2)

def ensure_snapshot_unmount():
    try:
        umount('/dev/%s/%s_snapshot' % (TEST_VG, TEST_LV))
    except:
        pass

def test_snapshot():
    lvsnapshot('%s/%s' % (TEST_VG, TEST_LV), '%s_snapshot' % TEST_LV , 4, '512K')
    assert_raises(LVMCommandError, lvsnapshot, '%s/%s' % (TEST_VG, TEST_LV), '%s_snapshot' % TEST_LV , 1)
    mount('/dev/%s/%s' % (TEST_VG, TEST_LV), '/mnt/tmp', options='noatime', vfstype='ext3')
    umount('/dev/%s/%s' % (TEST_VG, TEST_LV))
    lvremove('%s/%s_snapshot' % (TEST_VG, TEST_LV))
    assert_raises(LVMCommandError, lvremove, '%s/%s_snapshot' % (TEST_VG, TEST_LV)) # this should fail the 2nd time
test_snapshot.teardown = ensure_snapshot_unmount()

def test_blkid():
    info, = blkid('/dev/%s/%s' % (TEST_VG, TEST_LV))
    assert_equals(info['type'], 'ext3')

def test_bad_mount():
    assert_raises(LVMCommandError, mount, '/dev/%s/%s' % (TEST_VG, TEST_LV), os.path.join(MNT_DIR, 'missing'))

########NEW FILE########
__FILENAME__ = test_snapshot
import shutil
from nose.tools import *
from holland.lib.lvm import LogicalVolume
from holland.lib.lvm.snapshot import *
from tests.constants import *

class TestSnapshot(object):
    def setup(self):
        self.tmpdir = tempfile.mkdtemp()

    def teardown(self):
        shutil.rmtree(self.tmpdir)
 
    def test_snapshot_fsm(self):
        lv = LogicalVolume.lookup('%s/%s' % (TEST_VG, TEST_LV))
        name = lv.lv_name + '_snapshot'
        size = 1 # extent

        snapshot = Snapshot(name, size, self.tmpdir)
        snapshot.start(lv)

    def test_snapshot_fsm_with_callbacks(self):
        lv = LogicalVolume.lookup('%s/%s' % (TEST_VG, TEST_LV))
        name = lv.lv_name + '_snapshot'
        size = 1 # extent

        snapshot = Snapshot(name, size, self.tmpdir)
        def handle_event(event, *args, **kwargs):
            pass

        snapshot.register('pre-mount', handle_event)
        snapshot.register('post-mount', handle_event)
        snapshot.start(lv)

    def test_snapshot_fsm_with_failures(self):
        lv = LogicalVolume.lookup('%s/%s' % (TEST_VG, TEST_LV))
        name = lv.lv_name + '_snapshot'
        size = 1 # extent

        snapshot = Snapshot(name, size, self.tmpdir)

        def bad_callback(event, *args, **kwargs):
            raise Exception("Oooh nooo!")

        for evt in ('initialize', 'pre-snapshot', 'post-snapshot', 
                    'pre-mount', 'post-mount', 'pre-unmount', 'post-unmount',
                    'pre-remove', 'post-remove', 'finish'):
            snapshot.register(evt, bad_callback)
            assert_raises(CallbackFailuresError, snapshot.start, lv)
            snapshot.unregister(evt, bad_callback)
            if snapshot.sigmgr._handlers:
                raise Exception("WTF. sigmgr handlers still exist when checking event => %r", evt)

########NEW FILE########
__FILENAME__ = test_util
import os
import signal
from nose.tools import *
from holland.lib.lvm.util import *

def test_format_bytes():
    assert_equals(format_bytes(1024), '1.00KB')
    assert_equals(format_bytes(0), '0.00Bytes')

def test_getmount():
    assert_equals(getmount('/'), '/')
    assert_equals(getmount('/foobarbaz'), '/')

def test_getdevice():
    # XXX: bad hack
    dev = open('/etc/mtab', 'r').readline().split()[0].strip()
    assert_equals(getdevice('/'), dev)
    assert_equals(getdevice('/foobarbaz'), None)

def test_relpath():
    assert_raises(ValueError, relpath, '')
    assert_equals(relpath('/foo/bar/baz', '/foo/bar'), 'baz')
    assert_equals(relpath('/foo/bar/', '/foo/bar/'), os.curdir)

def test_signalmanager():
    sigmgr = SignalManager()
    sigmgr.trap(signal.SIGINT)
    os.kill(os.getpid(), signal.SIGINT)
    ok_(sigmgr.pending)
    assert_equals(sigmgr.pending[0], signal.SIGINT)
    sigmgr.restore()
    assert_raises(KeyboardInterrupt, os.kill, os.getpid(), signal.SIGINT)

def test_parsebytes():
    # bytes without units should be interpretted as MB
    bytes = parse_bytes('1024')
    assert_equals(bytes, 1024**3)
    # this should not be bytes
    ok_(bytes > 1024)
   
    bytes = parse_bytes('1024G')
    assert_equals(bytes, 1024**4)

########NEW FILE########
__FILENAME__ = mocker
"""
Copyright (c) 2007  Gustavo Niemeyer <gustavo@niemeyer.net>

Graceful platform for test doubles in Python (mocks, stubs, fakes, and dummies).
"""
import __builtin__
import tempfile
import unittest
import inspect
import shutil
import types
import sys
import os
import gc


if sys.version_info < (2, 4):
    from sets import Set as set # pragma: nocover


__all__ = ["Mocker", "expect", "IS", "CONTAINS", "IN", "MATCH",
           "ANY", "ARGS", "KWARGS"]


__author__ = "Gustavo Niemeyer <gustavo@niemeyer.net>"
__license__ = "PSF License"
__version__ = "0.10.1"


ERROR_PREFIX = "[Mocker] "


# --------------------------------------------------------------------
# Exceptions

class MatchError(AssertionError):
    """Raised when an unknown expression is seen in playback mode."""


# --------------------------------------------------------------------
# Helper for chained-style calling.

class expect(object):
    """This is a simple helper that allows a different call-style.

    With this class one can comfortably do chaining of calls to the
    mocker object responsible by the object being handled. For instance::

        expect(obj.attr).result(3).count(1, 2)

    Is the same as::

        obj.attr
        mocker.result(3)
        mocker.count(1, 2)

    """

    def __init__(self, mock, attr=None):
        self._mock = mock
        self._attr = attr

    def __getattr__(self, attr):
        return self.__class__(self._mock, attr)

    def __call__(self, *args, **kwargs):
        getattr(self._mock.__mocker__, self._attr)(*args, **kwargs)
        return self


# --------------------------------------------------------------------
# Extensions to Python's unittest.

class MockerTestCase(unittest.TestCase):
    """unittest.TestCase subclass with Mocker support.

    @ivar mocker: The mocker instance.

    This is a convenience only.  Mocker may easily be used with the
    standard C{unittest.TestCase} class if wanted.

    Test methods have a Mocker instance available on C{self.mocker}.
    At the end of each test method, expectations of the mocker will
    be verified, and any requested changes made to the environment
    will be restored.

    In addition to the integration with Mocker, this class provides
    a few additional helper methods.
    """

    expect = expect

    def __init__(self, methodName="runTest"):
        # So here is the trick: we take the real test method, wrap it on
        # a function that do the job we have to do, and insert it in the
        # *instance* dictionary, so that getattr() will return our
        # replacement rather than the class method.
        test_method = getattr(self, methodName, None)
        if test_method is not None:
            def test_method_wrapper():
                try:
                    result = test_method()
                except:
                    raise
                else:
                    if (self.mocker.is_recording() and
                        self.mocker.get_events()):
                        raise RuntimeError("Mocker must be put in replay "
                                           "mode with self.mocker.replay()")
                    if (hasattr(result, "addCallback") and
                        hasattr(result, "addErrback")):
                        def verify(result):
                            self.mocker.verify()
                            return result
                        result.addCallback(verify)
                    else:
                        self.mocker.verify()
                    return result
            # Copy all attributes from the original method..
            for attr in dir(test_method):
                # .. unless they're present in our wrapper already.
                if not hasattr(test_method_wrapper, attr) or attr == "__doc__":
                    setattr(test_method_wrapper, attr,
                            getattr(test_method, attr))
            setattr(self, methodName, test_method_wrapper)

        # We could overload run() normally, but other well-known testing
        # frameworks do it as well, and some of them won't call the super,
        # which might mean that cleanup wouldn't happen.  With that in mind,
        # we make integration easier by using the following trick.
        run_method = self.run
        def run_wrapper(*args, **kwargs):
            try:
                return run_method(*args, **kwargs)
            finally:
                self.__cleanup()
        self.run = run_wrapper

        self.mocker = Mocker()

        self.__cleanup_funcs = []
        self.__cleanup_paths = []

        super(MockerTestCase, self).__init__(methodName)

    def __cleanup(self):
        for path in self.__cleanup_paths:
            if os.path.isfile(path):
                os.unlink(path)
            elif os.path.isdir(path):
                shutil.rmtree(path)
        self.mocker.restore()
        for func, args, kwargs in self.__cleanup_funcs:
            func(*args, **kwargs)

    def addCleanup(self, func, *args, **kwargs):
        self.__cleanup_funcs.append((func, args, kwargs))

    def makeFile(self, content=None, suffix="", prefix="tmp", basename=None,
                 dirname=None, path=None):
        """Create a temporary file and return the path to it.

        @param content: Initial content for the file.
        @param suffix: Suffix to be given to the file's basename.
        @param prefix: Prefix to be given to the file's basename.
        @param basename: Full basename for the file.
        @param dirname: Put file inside this directory.

        The file is removed after the test runs.
        """
        if path is not None:
            self.__cleanup_paths.append(path)
        elif basename is not None:
            if dirname is None:
                dirname = tempfile.mkdtemp()
                self.__cleanup_paths.append(dirname)
            path = os.path.join(dirname, basename)
        else:
            fd, path = tempfile.mkstemp(suffix, prefix, dirname)
            self.__cleanup_paths.append(path)
            os.close(fd)
            if content is None:
                os.unlink(path)
        if content is not None:
            file = open(path, "w")
            file.write(content)
            file.close()
        return path

    def makeDir(self, suffix="", prefix="tmp", dirname=None, path=None):
        """Create a temporary directory and return the path to it.

        @param suffix: Suffix to be given to the file's basename.
        @param prefix: Prefix to be given to the file's basename.
        @param dirname: Put directory inside this parent directory.

        The directory is removed after the test runs.
        """
        if path is not None:
            os.makedirs(path)
        else:
            path = tempfile.mkdtemp(suffix, prefix, dirname)
        self.__cleanup_paths.append(path)
        return path

    def failUnlessIs(self, first, second, msg=None):
        """Assert that C{first} is the same object as C{second}."""
        if first is not second:
            raise self.failureException(msg or "%r is not %r" % (first, second))

    def failIfIs(self, first, second, msg=None):
        """Assert that C{first} is not the same object as C{second}."""
        if first is second:
            raise self.failureException(msg or "%r is %r" % (first, second))

    def failUnlessIn(self, first, second, msg=None):
        """Assert that C{first} is contained in C{second}."""
        if first not in second:
            raise self.failureException(msg or "%r not in %r" % (first, second))

    def failUnlessStartsWith(self, first, second, msg=None):
        """Assert that C{first} starts with C{second}."""
        if first[:len(second)] != second:
            raise self.failureException(msg or "%r doesn't start with %r" %
                                               (first, second))

    def failIfStartsWith(self, first, second, msg=None):
        """Assert that C{first} doesn't start with C{second}."""
        if first[:len(second)] == second:
            raise self.failureException(msg or "%r starts with %r" %
                                               (first, second))

    def failUnlessEndsWith(self, first, second, msg=None):
        """Assert that C{first} starts with C{second}."""
        if first[len(first)-len(second):] != second:
            raise self.failureException(msg or "%r doesn't end with %r" %
                                               (first, second))

    def failIfEndsWith(self, first, second, msg=None):
        """Assert that C{first} doesn't start with C{second}."""
        if first[len(first)-len(second):] == second:
            raise self.failureException(msg or "%r ends with %r" %
                                               (first, second))

    def failIfIn(self, first, second, msg=None):
        """Assert that C{first} is not contained in C{second}."""
        if first in second:
            raise self.failureException(msg or "%r in %r" % (first, second))

    def failUnlessApproximates(self, first, second, tolerance, msg=None):
        """Assert that C{first} is near C{second} by at most C{tolerance}."""
        if abs(first - second) > tolerance:
            raise self.failureException(msg or "abs(%r - %r) > %r" %
                                        (first, second, tolerance))

    def failIfApproximates(self, first, second, tolerance, msg=None):
        """Assert that C{first} is far from C{second} by at least C{tolerance}.
        """
        if abs(first - second) <= tolerance:
            raise self.failureException(msg or "abs(%r - %r) <= %r" %
                                        (first, second, tolerance))

    def failUnlessMethodsMatch(self, first, second):
        """Assert that public methods in C{first} are present in C{second}.

        This method asserts that all public methods found in C{first} are also
        present in C{second} and accept the same arguments.  C{first} may
        have its own private methods, though, and may not have all methods
        found in C{second}.  Note that if a private method in C{first} matches
        the name of one in C{second}, their specification is still compared.

        This is useful to verify if a fake or stub class have the same API as
        the real class being simulated.
        """
        first_methods = dict(inspect.getmembers(first, inspect.ismethod))
        second_methods = dict(inspect.getmembers(second, inspect.ismethod))
        for name, first_method in first_methods.items():
            first_argspec = inspect.getargspec(first_method)
            first_formatted = inspect.formatargspec(*first_argspec)

            second_method = second_methods.get(name)
            if second_method is None:
                if name[:1] == "_":
                    continue # First may have its own private methods.
                raise self.failureException("%s.%s%s not present in %s" %
                    (first.__name__, name, first_formatted, second.__name__))

            second_argspec = inspect.getargspec(second_method)
            if first_argspec != second_argspec:
                second_formatted = inspect.formatargspec(*second_argspec)
                raise self.failureException("%s.%s%s != %s.%s%s" %
                    (first.__name__, name, first_formatted,
                     second.__name__, name, second_formatted))


    assertIs = failUnlessIs
    assertIsNot = failIfIs
    assertIn = failUnlessIn
    assertNotIn = failIfIn
    assertStartsWith = failUnlessStartsWith
    assertNotStartsWith = failIfStartsWith
    assertEndsWith = failUnlessEndsWith
    assertNotEndsWith = failIfEndsWith
    assertApproximates = failUnlessApproximates
    assertNotApproximates = failIfApproximates
    assertMethodsMatch = failUnlessMethodsMatch

    # The following are missing in Python < 2.4.
    assertTrue = unittest.TestCase.failUnless
    assertFalse = unittest.TestCase.failIf

    # The following is provided for compatibility with Twisted's trial.
    assertIdentical = assertIs
    assertNotIdentical = assertIsNot
    failUnlessIdentical = failUnlessIs
    failIfIdentical = failIfIs


# --------------------------------------------------------------------
# Mocker.

class classinstancemethod(object):

    def __init__(self, method):
        self.method = method

    def __get__(self, obj, cls=None):
        def bound_method(*args, **kwargs):
            return self.method(cls, obj, *args, **kwargs)
        return bound_method


class MockerBase(object):
    """Controller of mock objects.

    A mocker instance is used to command recording and replay of
    expectations on any number of mock objects.

    Expectations should be expressed for the mock object while in
    record mode (the initial one) by using the mock object itself,
    and using the mocker (and/or C{expect()} as a helper) to define
    additional behavior for each event.  For instance::

        mock = mocker.mock()
        mock.hello()
        mocker.result("Hi!")
        mocker.replay()
        assert mock.hello() == "Hi!"
        mock.restore()
        mock.verify()

    In this short excerpt a mock object is being created, then an
    expectation of a call to the C{hello()} method was recorded, and
    when called the method should return the value C{10}.  Then, the
    mocker is put in replay mode, and the expectation is satisfied by
    calling the C{hello()} method, which indeed returns 10.  Finally,
    a call to the L{restore()} method is performed to undo any needed
    changes made in the environment, and the L{verify()} method is
    called to ensure that all defined expectations were met.

    The same logic can be expressed more elegantly using the
    C{with mocker:} statement, as follows::

        mock = mocker.mock()
        mock.hello()
        mocker.result("Hi!")
        with mocker:
            assert mock.hello() == "Hi!"

    Also, the MockerTestCase class, which integrates the mocker on
    a unittest.TestCase subclass, may be used to reduce the overhead
    of controlling the mocker.  A test could be written as follows::

        class SampleTest(MockerTestCase):

            def test_hello(self):
                mock = self.mocker.mock()
                mock.hello()
                self.mocker.result("Hi!")
                self.mocker.replay()
                self.assertEquals(mock.hello(), "Hi!")
    """

    _recorders = []

    # For convenience only.
    on = expect

    class __metaclass__(type):
        def __init__(self, name, bases, dict):
            # Make independent lists on each subclass, inheriting from parent.
            self._recorders = list(getattr(self, "_recorders", ()))

    def __init__(self):
        self._recorders = self._recorders[:]
        self._events = []
        self._recording = True
        self._ordering = False
        self._last_orderer = None

    def is_recording(self):
        """Return True if in recording mode, False if in replay mode.

        Recording is the initial state.
        """
        return self._recording

    def replay(self):
        """Change to replay mode, where recorded events are reproduced.

        If already in replay mode, the mocker will be restored, with all
        expectations reset, and then put again in replay mode.

        An alternative and more comfortable way to replay changes is
        using the 'with' statement, as follows::

            mocker = Mocker()
            <record events>
            with mocker:
                <reproduce events>

        The 'with' statement will automatically put mocker in replay
        mode, and will also verify if all events were correctly reproduced
        at the end (using L{verify()}), and also restore any changes done
        in the environment (with L{restore()}).

        Also check the MockerTestCase class, which integrates the
        unittest.TestCase class with mocker.
        """
        if not self._recording:
            for event in self._events:
                event.restore()
        else:
            self._recording = False
        for event in self._events:
            event.replay()

    def restore(self):
        """Restore changes in the environment, and return to recording mode.

        This should always be called after the test is complete (succeeding
        or not).  There are ways to call this method automatically on
        completion (e.g. using a C{with mocker:} statement, or using the
        L{MockerTestCase} class.
        """
        if not self._recording:
            self._recording = True
            for event in self._events:
                event.restore()

    def reset(self):
        """Reset the mocker state.

        This will restore environment changes, if currently in replay
        mode, and then remove all events previously recorded.
        """
        if not self._recording:
            self.restore()
        self.unorder()
        del self._events[:]

    def get_events(self):
        """Return all recorded events."""
        return self._events[:]

    def add_event(self, event):
        """Add an event.

        This method is used internally by the implementation, and
        shouldn't be needed on normal mocker usage.
        """
        self._events.append(event)
        if self._ordering:
            orderer = event.add_task(Orderer(event.path))
            if self._last_orderer:
                orderer.add_dependency(self._last_orderer)
            self._last_orderer = orderer
        return event

    def verify(self):
        """Check if all expectations were met, and raise AssertionError if not.

        The exception message will include a nice description of which
        expectations were not met, and why.
        """
        errors = []
        for event in self._events:
            try:
                event.verify()
            except AssertionError, e:
                error = str(e)
                if not error:
                    raise RuntimeError("Empty error message from %r"
                                       % event)
                errors.append(error)
        if errors:
            message = [ERROR_PREFIX + "Unmet expectations:", ""]
            for error in errors:
                lines = error.splitlines()
                message.append("=> " + lines.pop(0))
                message.extend([" " + line for line in lines])
                message.append("")
            raise AssertionError(os.linesep.join(message))

    def mock(self, spec_and_type=None, spec=None, type=None,
             name=None, count=True):
        """Return a new mock object.

        @param spec_and_type: Handy positional argument which sets both
                     spec and type.
        @param spec: Method calls will be checked for correctness against
                     the given class.
        @param type: If set, the Mock's __class__ attribute will return
                     the given type.  This will make C{isinstance()} calls
                     on the object work.
        @param name: Name for the mock object, used in the representation of
                     expressions.  The name is rarely needed, as it's usually
                     guessed correctly from the variable name used.
        @param count: If set to false, expressions may be executed any number
                     of times, unless an expectation is explicitly set using
                     the L{count()} method.  By default, expressions are
                     expected once.
        """
        if spec_and_type is not None:
            spec = type = spec_and_type
        return Mock(self, spec=spec, type=type, name=name, count=count)

    def proxy(self, object, spec=True, type=True, name=None, count=True,
              passthrough=True):
        """Return a new mock object which proxies to the given object.
 
        Proxies are useful when only part of the behavior of an object
        is to be mocked.  Unknown expressions may be passed through to
        the real implementation implicitly (if the C{passthrough} argument
        is True), or explicitly (using the L{passthrough()} method
        on the event).

        @param object: Real object to be proxied, and replaced by the mock
                       on replay mode.  It may also be an "import path",
                       such as C{"time.time"}, in which case the object
                       will be the C{time} function from the C{time} module.
        @param spec: Method calls will be checked for correctness against
                     the given object, which may be a class or an instance
                     where attributes will be looked up.  Defaults to the
                     the C{object} parameter.  May be set to None explicitly,
                     in which case spec checking is disabled.  Checks may
                     also be disabled explicitly on a per-event basis with
                     the L{nospec()} method.
        @param type: If set, the Mock's __class__ attribute will return
                     the given type.  This will make C{isinstance()} calls
                     on the object work.  Defaults to the type of the
                     C{object} parameter.  May be set to None explicitly.
        @param name: Name for the mock object, used in the representation of
                     expressions.  The name is rarely needed, as it's usually
                     guessed correctly from the variable name used.
        @param count: If set to false, expressions may be executed any number
                     of times, unless an expectation is explicitly set using
                     the L{count()} method.  By default, expressions are
                     expected once.
        @param passthrough: If set to False, passthrough of actions on the
                            proxy to the real object will only happen when
                            explicitly requested via the L{passthrough()}
                            method.
        """
        if isinstance(object, basestring):
            if name is None:
                name = object
            import_stack = object.split(".")
            attr_stack = []
            while import_stack:
                module_path = ".".join(import_stack)
                try:
                    object = __import__(module_path, {}, {}, [""])
                except ImportError:
                    attr_stack.insert(0, import_stack.pop())
                    if not import_stack:
                        raise
                    continue
                else:
                    for attr in attr_stack:
                        object = getattr(object, attr)
                    break
        if spec is True:
            spec = object
        if type is True:
            type = __builtin__.type(object)
        return Mock(self, spec=spec, type=type, object=object,
                    name=name, count=count, passthrough=passthrough)

    def replace(self, object, spec=True, type=True, name=None, count=True,
                passthrough=True):
        """Create a proxy, and replace the original object with the mock.

        On replay, the original object will be replaced by the returned
        proxy in all dictionaries found in the running interpreter via
        the garbage collecting system.  This should cover module
        namespaces, class namespaces, instance namespaces, and so on.

        @param object: Real object to be proxied, and replaced by the mock
                       on replay mode.  It may also be an "import path",
                       such as C{"time.time"}, in which case the object
                       will be the C{time} function from the C{time} module.
        @param spec: Method calls will be checked for correctness against
                     the given object, which may be a class or an instance
                     where attributes will be looked up.  Defaults to the
                     the C{object} parameter.  May be set to None explicitly,
                     in which case spec checking is disabled.  Checks may
                     also be disabled explicitly on a per-event basis with
                     the L{nospec()} method.
        @param type: If set, the Mock's __class__ attribute will return
                     the given type.  This will make C{isinstance()} calls
                     on the object work.  Defaults to the type of the
                     C{object} parameter.  May be set to None explicitly.
        @param name: Name for the mock object, used in the representation of
                     expressions.  The name is rarely needed, as it's usually
                     guessed correctly from the variable name used.
        @param passthrough: If set to False, passthrough of actions on the
                            proxy to the real object will only happen when
                            explicitly requested via the L{passthrough()}
                            method.
        """
        mock = self.proxy(object, spec, type, name, count, passthrough)
        event = self._get_replay_restore_event()
        event.add_task(ProxyReplacer(mock))
        return mock

    def patch(self, object, spec=True):
        """Patch an existing object to reproduce recorded events.

        @param object: Class or instance to be patched.
        @param spec: Method calls will be checked for correctness against
                     the given object, which may be a class or an instance
                     where attributes will be looked up.  Defaults to the
                     the C{object} parameter.  May be set to None explicitly,
                     in which case spec checking is disabled.  Checks may
                     also be disabled explicitly on a per-event basis with
                     the L{nospec()} method.

        The result of this method is still a mock object, which can be
        used like any other mock object to record events.  The difference
        is that when the mocker is put on replay mode, the *real* object
        will be modified to behave according to recorded expectations.

        Patching works in individual instances, and also in classes.
        When an instance is patched, recorded events will only be
        considered on this specific instance, and other instances should
        behave normally.  When a class is patched, the reproduction of
        events will be considered on any instance of this class once
        created (collectively).

        Observe that, unlike with proxies which catch only events done
        through the mock object, *all* accesses to recorded expectations
        will be considered;  even these coming from the object itself
        (e.g. C{self.hello()} is considered if this method was patched).
        While this is a very powerful feature, and many times the reason
        to use patches in the first place, it's important to keep this
        behavior in mind.

        Patching of the original object only takes place when the mocker
        is put on replay mode, and the patched object will be restored
        to its original state once the L{restore()} method is called
        (explicitly, or implicitly with alternative conventions, such as
        a C{with mocker:} block, or a MockerTestCase class).
        """
        if spec is True:
            spec = object
        patcher = Patcher()
        event = self._get_replay_restore_event()
        event.add_task(patcher)
        mock = Mock(self, object=object, patcher=patcher,
                    passthrough=True, spec=spec)
        object.__mocker_mock__ = mock
        return mock

    def act(self, path):
        """This is called by mock objects whenever something happens to them.

        This method is part of the implementation between the mocker
        and mock objects.
        """
        if self._recording:
            event = self.add_event(Event(path))
            for recorder in self._recorders:
                recorder(self, event)
            return Mock(self, path)
        else:
            # First run events that may run, then run unsatisfied events, then
            # ones not previously run. We put the index in the ordering tuple
            # instead of the actual event because we want a stable sort
            # (ordering between 2 events is undefined).
            events = self._events
            order = [(events[i].satisfied()*2 + events[i].has_run(), i)
                     for i in range(len(events))]
            order.sort()
            postponed = None
            for weight, i in order:
                event = events[i]
                if event.matches(path):
                    if event.may_run(path):
                        return event.run(path)
                    elif postponed is None:
                        postponed = event
            if postponed is not None:
                return postponed.run(path)
            raise MatchError(ERROR_PREFIX + "Unexpected expression: %s" % path)

    def get_recorders(cls, self):
        """Return recorders associated with this mocker class or instance.

        This method may be called on mocker instances and also on mocker
        classes.  See the L{add_recorder()} method for more information.
        """
        return (self or cls)._recorders[:]
    get_recorders = classinstancemethod(get_recorders)

    def add_recorder(cls, self, recorder):
        """Add a recorder to this mocker class or instance.

        @param recorder: Callable accepting C{(mocker, event)} as parameters.

        This is part of the implementation of mocker.

        All registered recorders are called for translating events that
        happen during recording into expectations to be met once the state
        is switched to replay mode.

        This method may be called on mocker instances and also on mocker
        classes.  When called on a class, the recorder will be used by
        all instances, and also inherited on subclassing.  When called on
        instances, the recorder is added only to the given instance.
        """
        (self or cls)._recorders.append(recorder)
        return recorder
    add_recorder = classinstancemethod(add_recorder)

    def remove_recorder(cls, self, recorder):
        """Remove the given recorder from this mocker class or instance.

        This method may be called on mocker classes and also on mocker
        instances.  See the L{add_recorder()} method for more information.
        """
        (self or cls)._recorders.remove(recorder)
    remove_recorder = classinstancemethod(remove_recorder)

    def result(self, value):
        """Make the last recorded event return the given value on replay.
        
        @param value: Object to be returned when the event is replayed.
        """
        self.call(lambda *args, **kwargs: value)

    def generate(self, sequence):
        """Last recorded event will return a generator with the given sequence.

        @param sequence: Sequence of values to be generated.
        """
        def generate(*args, **kwargs):
            for value in sequence:
                yield value
        self.call(generate)

    def throw(self, exception):
        """Make the last recorded event raise the given exception on replay.

        @param exception: Class or instance of exception to be raised.
        """
        def raise_exception(*args, **kwargs):
            raise exception
        self.call(raise_exception)

    def call(self, func):
        """Make the last recorded event cause the given function to be called.

        @param func: Function to be called.

        The result of the function will be used as the event result.
        """
        self._events[-1].add_task(FunctionRunner(func))

    def count(self, min, max=False):
        """Last recorded event must be replayed between min and max times.

        @param min: Minimum number of times that the event must happen.
        @param max: Maximum number of times that the event must happen.  If
                    not given, it defaults to the same value of the C{min}
                    parameter.  If set to None, there is no upper limit, and
                    the expectation is met as long as it happens at least
                    C{min} times.
        """
        event = self._events[-1]
        for task in event.get_tasks():
            if isinstance(task, RunCounter):
                event.remove_task(task)
        event.add_task(RunCounter(min, max))

    def is_ordering(self):
        """Return true if all events are being ordered.

        See the L{order()} method.
        """
        return self._ordering

    def unorder(self):
        """Disable the ordered mode.
        
        See the L{order()} method for more information.
        """
        self._ordering = False
        self._last_orderer = None

    def order(self, *path_holders):
        """Create an expectation of order between two or more events.

        @param path_holders: Objects returned as the result of recorded events.

        By default, mocker won't force events to happen precisely in
        the order they were recorded.  Calling this method will change
        this behavior so that events will only match if reproduced in
        the correct order.

        There are two ways in which this method may be used.  Which one
        is used in a given occasion depends only on convenience.

        If no arguments are passed, the mocker will be put in a mode where
        all the recorded events following the method call will only be met
        if they happen in order.  When that's used, the mocker may be put
        back in unordered mode by calling the L{unorder()} method, or by
        using a 'with' block, like so::

            with mocker.ordered():
                <record events>

        In this case, only expressions in <record events> will be ordered,
        and the mocker will be back in unordered mode after the 'with' block.

        The second way to use it is by specifying precisely which events
        should be ordered.  As an example::

            mock = mocker.mock()
            expr1 = mock.hello()
            expr2 = mock.world
            expr3 = mock.x.y.z
            mocker.order(expr1, expr2, expr3)

        This method of ordering only works when the expression returns
        another object.

        Also check the L{after()} and L{before()} methods, which are
        alternative ways to perform this.
        """
        if not path_holders:
            self._ordering = True
            return OrderedContext(self)

        last_orderer = None
        for path_holder in path_holders:
            if type(path_holder) is Path:
                path = path_holder
            else:
                path = path_holder.__mocker_path__
            for event in self._events:
                if event.path is path:
                    for task in event.get_tasks():
                        if isinstance(task, Orderer):
                            orderer = task
                            break
                    else:
                        orderer = Orderer(path)
                        event.add_task(orderer)
                    if last_orderer:
                        orderer.add_dependency(last_orderer)
                    last_orderer = orderer
                    break

    def after(self, *path_holders):
        """Last recorded event must happen after events referred to.

        @param path_holders: Objects returned as the result of recorded events
                             which should happen before the last recorded event

        As an example, the idiom::

            expect(mock.x).after(mock.y, mock.z)

        is an alternative way to say::

            expr_x = mock.x
            expr_y = mock.y
            expr_z = mock.z
            mocker.order(expr_y, expr_x)
            mocker.order(expr_z, expr_x)

        See L{order()} for more information.
        """
        last_path = self._events[-1].path
        for path_holder in path_holders:
            self.order(path_holder, last_path)

    def before(self, *path_holders):
        """Last recorded event must happen before events referred to.

        @param path_holders: Objects returned as the result of recorded events
                             which should happen after the last recorded event

        As an example, the idiom::

            expect(mock.x).before(mock.y, mock.z)

        is an alternative way to say::

            expr_x = mock.x
            expr_y = mock.y
            expr_z = mock.z
            mocker.order(expr_x, expr_y)
            mocker.order(expr_x, expr_z)

        See L{order()} for more information.
        """
        last_path = self._events[-1].path
        for path_holder in path_holders:
            self.order(last_path, path_holder)

    def nospec(self):
        """Don't check method specification of real object on last event.

        By default, when using a mock created as the result of a call to
        L{proxy()}, L{replace()}, and C{patch()}, or when passing the spec
        attribute to the L{mock()} method, method calls on the given object
        are checked for correctness against the specification of the real
        object (or the explicitly provided spec).

        This method will disable that check specifically for the last
        recorded event.
        """
        event = self._events[-1]
        for task in event.get_tasks():
            if isinstance(task, SpecChecker):
                event.remove_task(task)

    def passthrough(self, result_callback=None):
        """Make the last recorded event run on the real object once seen.

        @param result_callback: If given, this function will be called with
            the result of the *real* method call as the only argument.

        This can only be used on proxies, as returned by the L{proxy()}
        and L{replace()} methods, or on mocks representing patched objects,
        as returned by the L{patch()} method.
        """
        event = self._events[-1]
        if event.path.root_object is None:
            raise TypeError("Mock object isn't a proxy")
        event.add_task(PathExecuter(result_callback))

    def __enter__(self):
        """Enter in a 'with' context.  This will run replay()."""
        self.replay()
        return self

    def __exit__(self, type, value, traceback):
        """Exit from a 'with' context.

        This will run restore() at all times, but will only run verify()
        if the 'with' block itself hasn't raised an exception.  Exceptions
        in that block are never swallowed.
        """
        self.restore()
        if type is None:
            self.verify()
        return False

    def _get_replay_restore_event(self):
        """Return unique L{ReplayRestoreEvent}, creating if needed.

        Some tasks only want to replay/restore.  When that's the case,
        they shouldn't act on other events during replay.  Also, they
        can all be put in a single event when that's the case.  Thus,
        we add a single L{ReplayRestoreEvent} as the first element of
        the list.
        """
        if not self._events or type(self._events[0]) != ReplayRestoreEvent:
            self._events.insert(0, ReplayRestoreEvent())
        return self._events[0]


class OrderedContext(object):

    def __init__(self, mocker):
        self._mocker = mocker

    def __enter__(self):
        return None

    def __exit__(self, type, value, traceback):
        self._mocker.unorder()


class Mocker(MockerBase):
    __doc__ = MockerBase.__doc__

# Decorator to add recorders on the standard Mocker class.
recorder = Mocker.add_recorder


# --------------------------------------------------------------------
# Mock object.

class Mock(object):

    def __init__(self, mocker, path=None, name=None, spec=None, type=None,
                 object=None, passthrough=False, patcher=None, count=True):
        self.__mocker__ = mocker
        self.__mocker_path__ = path or Path(self, object)
        self.__mocker_name__ = name
        self.__mocker_spec__ = spec
        self.__mocker_object__ = object
        self.__mocker_passthrough__ = passthrough
        self.__mocker_patcher__ = patcher
        self.__mocker_replace__ = False
        self.__mocker_type__ = type
        self.__mocker_count__ = count

    def __mocker_act__(self, kind, args=(), kwargs={}, object=None):
        if self.__mocker_name__ is None:
            self.__mocker_name__ = find_object_name(self, 2)
        action = Action(kind, args, kwargs, self.__mocker_path__)
        path = self.__mocker_path__ + action
        if object is not None:
            path.root_object = object
        try:
            return self.__mocker__.act(path)
        except MatchError, exception:
            root_mock = path.root_mock
            if (path.root_object is not None and
                root_mock.__mocker_passthrough__):
                return path.execute(path.root_object)
            # Reinstantiate to show raise statement on traceback, and
            # also to make the traceback shown shorter.
            raise MatchError(str(exception))
        except AssertionError, e:
            lines = str(e).splitlines()
            message = [ERROR_PREFIX + "Unmet expectation:", ""]
            message.append("=> " + lines.pop(0))
            message.extend([" " + line for line in lines])
            message.append("")
            raise AssertionError(os.linesep.join(message))

    def __getattribute__(self, name):
        if name.startswith("__mocker_"):
            return super(Mock, self).__getattribute__(name)
        if name == "__class__":
            if self.__mocker__.is_recording() or self.__mocker_type__ is None:
                return type(self)
            return self.__mocker_type__
        return self.__mocker_act__("getattr", (name,))

    def __setattr__(self, name, value):
        if name.startswith("__mocker_"):
            return super(Mock, self).__setattr__(name, value)
        return self.__mocker_act__("setattr", (name, value))

    def __delattr__(self, name):
        return self.__mocker_act__("delattr", (name,))

    def __call__(self, *args, **kwargs):
        return self.__mocker_act__("call", args, kwargs)

    def __contains__(self, value):
        return self.__mocker_act__("contains", (value,))

    def __getitem__(self, key):
        return self.__mocker_act__("getitem", (key,))

    def __setitem__(self, key, value):
        return self.__mocker_act__("setitem", (key, value))

    def __delitem__(self, key):
        return self.__mocker_act__("delitem", (key,))

    def __len__(self):
        # MatchError is turned on an AttributeError so that list() and
        # friends act properly when trying to get length hints on
        # something that doesn't offer them.
        try:
            result = self.__mocker_act__("len")
        except MatchError, e:
            raise AttributeError(str(e))
        if type(result) is Mock:
            return 0
        return result

    def __nonzero__(self):
        try:
            return self.__mocker_act__("nonzero")
        except MatchError, e:
            return True

    def __iter__(self):
        # XXX On py3k, when next() becomes __next__(), we'll be able
        #     to return the mock itself because it will be considered
        #     an iterator (we'll be mocking __next__ as well, which we
        #     can't now).
        result = self.__mocker_act__("iter")
        if type(result) is Mock:
            return iter([])
        return result

    # When adding a new action kind here, also add support for it on
    # Action.execute() and Path.__str__().


def find_object_name(obj, depth=0):
    """Try to detect how the object is named on a previous scope."""
    try:
        frame = sys._getframe(depth+1)
    except:
        return None
    for name, frame_obj in frame.f_locals.iteritems():
        if frame_obj is obj:
            return name
    self = frame.f_locals.get("self")
    if self is not None:
        try:
            items = list(self.__dict__.iteritems())
        except:
            pass
        else:
            for name, self_obj in items:
                if self_obj is obj:
                    return name
    return None


# --------------------------------------------------------------------
# Action and path.

class Action(object):

    def __init__(self, kind, args, kwargs, path=None):
        self.kind = kind
        self.args = args
        self.kwargs = kwargs
        self.path = path
        self._execute_cache = {}

    def __repr__(self):
        if self.path is None:
            return "Action(%r, %r, %r)" % (self.kind, self.args, self.kwargs)
        return "Action(%r, %r, %r, %r)" % \
               (self.kind, self.args, self.kwargs, self.path)

    def __eq__(self, other):
        return (self.kind == other.kind and
                self.args == other.args and
                self.kwargs == other.kwargs)

    def __ne__(self, other):
        return not self.__eq__(other)

    def matches(self, other):
        return (self.kind == other.kind and
                match_params(self.args, self.kwargs, other.args, other.kwargs))

    def execute(self, object):
        # This caching scheme may fail if the object gets deallocated before
        # the action, as the id might get reused.  It's somewhat easy to fix
        # that with a weakref callback.  For our uses, though, the object
        # should never get deallocated before the action itself, so we'll
        # just keep it simple.
        if id(object) in self._execute_cache:
            return self._execute_cache[id(object)]
        execute = getattr(object, "__mocker_execute__", None)
        if execute is not None:
            result = execute(self, object)
        else:
            kind = self.kind
            if kind == "getattr":
                result = getattr(object, self.args[0])
            elif kind == "setattr":
                result = setattr(object, self.args[0], self.args[1])
            elif kind == "delattr":
                result = delattr(object, self.args[0])
            elif kind == "call":
                result = object(*self.args, **self.kwargs)
            elif kind == "contains":
                result = self.args[0] in object
            elif kind == "getitem":
                result = object[self.args[0]]
            elif kind == "setitem":
                result = object[self.args[0]] = self.args[1]
            elif kind == "delitem":
                del object[self.args[0]]
                result = None
            elif kind == "len":
                result = len(object)
            elif kind == "nonzero":
                result = bool(object)
            elif kind == "iter":
                result = iter(object)
            else:
                raise RuntimeError("Don't know how to execute %r kind." % kind)
        self._execute_cache[id(object)] = result
        return result


class Path(object):

    def __init__(self, root_mock, root_object=None, actions=()):
        self.root_mock = root_mock
        self.root_object = root_object
        self.actions = tuple(actions)
        self.__mocker_replace__ = False

    def parent_path(self):
        if not self.actions:
            return None
        return self.actions[-1].path
    parent_path = property(parent_path)
 
    def __add__(self, action):
        """Return a new path which includes the given action at the end."""
        return self.__class__(self.root_mock, self.root_object,
                              self.actions + (action,))

    def __eq__(self, other):
        """Verify if the two paths are equal.
        
        Two paths are equal if they refer to the same mock object, and
        have the actions with equal kind, args and kwargs.
        """
        if (self.root_mock is not other.root_mock or
            self.root_object is not other.root_object or
            len(self.actions) != len(other.actions)):
            return False
        for action, other_action in zip(self.actions, other.actions):
            if action != other_action:
                return False
        return True

    def matches(self, other):
        """Verify if the two paths are equivalent.
        
        Two paths are equal if they refer to the same mock object, and
        have the same actions performed on them.
        """
        if (self.root_mock is not other.root_mock or
            len(self.actions) != len(other.actions)):
            return False
        for action, other_action in zip(self.actions, other.actions):
            if not action.matches(other_action):
                return False
        return True

    def execute(self, object):
        """Execute all actions sequentially on object, and return result.
        """
        for action in self.actions:
            object = action.execute(object)
        return object

    def __str__(self):
        """Transform the path into a nice string such as obj.x.y('z')."""
        result = self.root_mock.__mocker_name__ or "<mock>"
        for action in self.actions:
            if action.kind == "getattr":
                result = "%s.%s" % (result, action.args[0])
            elif action.kind == "setattr":
                result = "%s.%s = %r" % (result, action.args[0], action.args[1])
            elif action.kind == "delattr":
                result = "del %s.%s" % (result, action.args[0])
            elif action.kind == "call":
                args = [repr(x) for x in action.args]
                items = list(action.kwargs.iteritems())
                items.sort()
                for pair in items:
                    args.append("%s=%r" % pair)
                result = "%s(%s)" % (result, ", ".join(args))
            elif action.kind == "contains":
                result = "%r in %s" % (action.args[0], result)
            elif action.kind == "getitem":
                result = "%s[%r]" % (result, action.args[0])
            elif action.kind == "setitem":
                result = "%s[%r] = %r" % (result, action.args[0],
                                          action.args[1])
            elif action.kind == "delitem":
                result = "del %s[%r]" % (result, action.args[0])
            elif action.kind == "len":
                result = "len(%s)" % result
            elif action.kind == "nonzero":
                result = "bool(%s)" % result
            elif action.kind == "iter":
                result = "iter(%s)" % result
            else:
                raise RuntimeError("Don't know how to format kind %r" %
                                   action.kind)
        return result


class SpecialArgument(object):
    """Base for special arguments for matching parameters."""

    def __init__(self, object=None):
        self.object = object

    def __repr__(self):
        if self.object is None:
            return self.__class__.__name__
        else:
            return "%s(%r)" % (self.__class__.__name__, self.object)

    def matches(self, other):
        return True

    def __eq__(self, other):
        return type(other) == type(self) and self.object == other.object


class ANY(SpecialArgument):
    """Matches any single argument."""

ANY = ANY()


class ARGS(SpecialArgument):
    """Matches zero or more positional arguments."""

ARGS = ARGS()


class KWARGS(SpecialArgument):
    """Matches zero or more keyword arguments."""

KWARGS = KWARGS()


class IS(SpecialArgument):

    def matches(self, other):
        return self.object is other

    def __eq__(self, other):
        return type(other) == type(self) and self.object is other.object


class CONTAINS(SpecialArgument):

    def matches(self, other):
        try:
            other.__contains__
        except AttributeError:
            try:
                iter(other)
            except TypeError:
                # If an object can't be iterated, and has no __contains__
                # hook, it'd blow up on the test below.  We test this in
                # advance to prevent catching more errors than we really
                # want.
                return False
        return self.object in other


class IN(SpecialArgument):

    def matches(self, other):
        return other in self.object


class MATCH(SpecialArgument):

    def matches(self, other):
        return bool(self.object(other))

    def __eq__(self, other):
        return type(other) == type(self) and self.object is other.object


def match_params(args1, kwargs1, args2, kwargs2):
    """Match the two sets of parameters, considering special parameters."""

    has_args = ARGS in args1
    has_kwargs = KWARGS in args1

    if has_kwargs:
        args1 = [arg1 for arg1 in args1 if arg1 is not KWARGS]
    elif len(kwargs1) != len(kwargs2):
        return False

    if not has_args and len(args1) != len(args2):
        return False

    # Either we have the same number of kwargs, or unknown keywords are
    # accepted (KWARGS was used), so check just the ones in kwargs1.
    for key, arg1 in kwargs1.iteritems():
        if key not in kwargs2:
            return False
        arg2 = kwargs2[key]
        if isinstance(arg1, SpecialArgument):
            if not arg1.matches(arg2):
                return False
        elif arg1 != arg2:
            return False

    # Keywords match.  Now either we have the same number of
    # arguments, or ARGS was used.  If ARGS wasn't used, arguments
    # must match one-on-one necessarily.
    if not has_args:
        for arg1, arg2 in zip(args1, args2):
            if isinstance(arg1, SpecialArgument):
                if not arg1.matches(arg2):
                    return False
            elif arg1 != arg2:
                return False
        return True

    # Easy choice. Keywords are matching, and anything on args is accepted.
    if (ARGS,) == args1:
        return True

    # We have something different there. If we don't have positional
    # arguments on the original call, it can't match.
    if not args2:
        # Unless we have just several ARGS (which is bizarre, but..).
        for arg1 in args1:
            if arg1 is not ARGS:
                return False
        return True

    # Ok, all bets are lost.  We have to actually do the more expensive
    # matching.  This is an algorithm based on the idea of the Levenshtein
    # Distance between two strings, but heavily hacked for this purpose.
    args2l = len(args2)
    if args1[0] is ARGS:
        args1 = args1[1:]
        array = [0]*args2l
    else:
        array = [1]*args2l
    for i in range(len(args1)):
        last = array[0]
        if args1[i] is ARGS:
            for j in range(1, args2l):
                last, array[j] = array[j], min(array[j-1], array[j], last)
        else:
            array[0] = i or int(args1[i] != args2[0])
            for j in range(1, args2l):
                last, array[j] = array[j], last or int(args1[i] != args2[j])
        if 0 not in array:
            return False
    if array[-1] != 0:
        return False
    return True


# --------------------------------------------------------------------
# Event and task base.

class Event(object):
    """Aggregation of tasks that keep track of a recorded action.

    An event represents something that may or may not happen while the
    mocked environment is running, such as an attribute access, or a
    method call.  The event is composed of several tasks that are
    orchestrated together to create a composed meaning for the event,
    including for which actions it should be run, what happens when it
    runs, and what's the expectations about the actions run.
    """

    def __init__(self, path=None):
        self.path = path
        self._tasks = []
        self._has_run = False

    def add_task(self, task):
        """Add a new task to this taks."""
        self._tasks.append(task)
        return task

    def remove_task(self, task):
        self._tasks.remove(task)

    def get_tasks(self):
        return self._tasks[:]

    def matches(self, path):
        """Return true if *all* tasks match the given path."""
        for task in self._tasks:
            if not task.matches(path):
                return False
        return bool(self._tasks)

    def has_run(self):
        return self._has_run

    def may_run(self, path):
        """Verify if any task would certainly raise an error if run.

        This will call the C{may_run()} method on each task and return
        false if any of them returns false.
        """
        for task in self._tasks:
            if not task.may_run(path):
                return False
        return True

    def run(self, path):
        """Run all tasks with the given action.

        @param path: The path of the expression run.

        Running an event means running all of its tasks individually and in
        order.  An event should only ever be run if all of its tasks claim to
        match the given action.

        The result of this method will be the last result of a task
        which isn't None, or None if they're all None.
        """
        self._has_run = True
        result = None
        errors = []
        for task in self._tasks:
            try:
                task_result = task.run(path)
            except AssertionError, e:
                error = str(e)
                if not error:
                    raise RuntimeError("Empty error message from %r" % task)
                errors.append(error)
            else:
                if task_result is not None:
                    result = task_result
        if errors:
            message = [str(self.path)]
            if str(path) != message[0]:
                message.append("- Run: %s" % path)
            for error in errors:
                lines = error.splitlines()
                message.append("- " + lines.pop(0))
                message.extend(["  " + line for line in lines])
            raise AssertionError(os.linesep.join(message))
        return result

    def satisfied(self):
        """Return true if all tasks are satisfied.

        Being satisfied means that there are no unmet expectations.
        """
        for task in self._tasks:
            try:
                task.verify()
            except AssertionError:
                return False
        return True

    def verify(self):
        """Run verify on all tasks.

        The verify method is supposed to raise an AssertionError if the
        task has unmet expectations, with a one-line explanation about
        why this item is unmet.  This method should be safe to be called
        multiple times without side effects.
        """
        errors = []
        for task in self._tasks:
            try:
                task.verify()
            except AssertionError, e:
                error = str(e)
                if not error:
                    raise RuntimeError("Empty error message from %r" % task)
                errors.append(error)
        if errors:
            message = [str(self.path)]
            for error in errors:
                lines = error.splitlines()
                message.append("- " + lines.pop(0))
                message.extend(["  " + line for line in lines])
            raise AssertionError(os.linesep.join(message))

    def replay(self):
        """Put all tasks in replay mode."""
        self._has_run = False
        for task in self._tasks:
            task.replay()

    def restore(self):
        """Restore the state of all tasks."""
        for task in self._tasks:
            task.restore()


class ReplayRestoreEvent(Event):
    """Helper event for tasks which need replay/restore but shouldn't match."""

    def matches(self, path):
        return False


class Task(object):
    """Element used to track one specific aspect on an event.

    A task is responsible for adding any kind of logic to an event.
    Examples of that are counting the number of times the event was
    made, verifying parameters if any, and so on.
    """

    def matches(self, path):
        """Return true if the task is supposed to be run for the given path.
        """
        return True

    def may_run(self, path):
        """Return false if running this task would certainly raise an error."""
        return True

    def run(self, path):
        """Perform the task item, considering that the given action happened.
        """

    def verify(self):
        """Raise AssertionError if expectations for this item are unmet.

        The verify method is supposed to raise an AssertionError if the
        task has unmet expectations, with a one-line explanation about
        why this item is unmet.  This method should be safe to be called
        multiple times without side effects.
        """

    def replay(self):
        """Put the task in replay mode.

        Any expectations of the task should be reset.
        """

    def restore(self):
        """Restore any environmental changes made by the task.

        Verify should continue to work after this is called.
        """


# --------------------------------------------------------------------
# Task implementations.

class OnRestoreCaller(Task):
    """Call a given callback when restoring."""

    def __init__(self, callback):
        self._callback = callback

    def restore(self):
        self._callback()


class PathMatcher(Task):
    """Match the action path against a given path."""

    def __init__(self, path):
        self.path = path

    def matches(self, path):
        return self.path.matches(path)

def path_matcher_recorder(mocker, event):
    event.add_task(PathMatcher(event.path))

Mocker.add_recorder(path_matcher_recorder)


class RunCounter(Task):
    """Task which verifies if the number of runs are within given boundaries.
    """

    def __init__(self, min, max=False):
        self.min = min
        if max is None:
            self.max = sys.maxint
        elif max is False:
            self.max = min
        else:
            self.max = max
        self._runs = 0

    def replay(self):
        self._runs = 0

    def may_run(self, path):
        return self._runs < self.max

    def run(self, path):
        self._runs += 1
        if self._runs > self.max:
            self.verify()

    def verify(self):
        if not self.min <= self._runs <= self.max:
            if self._runs < self.min:
                raise AssertionError("Performed fewer times than expected.")
            raise AssertionError("Performed more times than expected.")


class ImplicitRunCounter(RunCounter):
    """RunCounter inserted by default on any event.

    This is a way to differentiate explicitly added counters and
    implicit ones.
    """

def run_counter_recorder(mocker, event):
    """Any event may be repeated once, unless disabled by default."""
    if event.path.root_mock.__mocker_count__:
        event.add_task(ImplicitRunCounter(1))

Mocker.add_recorder(run_counter_recorder)

def run_counter_removal_recorder(mocker, event):
    """
    Events created by getattr actions which lead to other events
    may be repeated any number of times. For that, we remove implicit
    run counters of any getattr actions leading to the current one.
    """
    parent_path = event.path.parent_path
    for event in mocker.get_events()[::-1]:
        if (event.path is parent_path and
            event.path.actions[-1].kind == "getattr"):
            for task in event.get_tasks():
                if type(task) is ImplicitRunCounter:
                    event.remove_task(task)

Mocker.add_recorder(run_counter_removal_recorder)


class MockReturner(Task):
    """Return a mock based on the action path."""

    def __init__(self, mocker):
        self.mocker = mocker

    def run(self, path):
        return Mock(self.mocker, path)

def mock_returner_recorder(mocker, event):
    """Events that lead to other events must return mock objects."""
    parent_path = event.path.parent_path
    for event in mocker.get_events():
        if event.path is parent_path:
            for task in event.get_tasks():
                if isinstance(task, MockReturner):
                    break
            else:
                event.add_task(MockReturner(mocker))
            break

Mocker.add_recorder(mock_returner_recorder)


class FunctionRunner(Task):
    """Task that runs a function everything it's run.

    Arguments of the last action in the path are passed to the function,
    and the function result is also returned.
    """

    def __init__(self, func):
        self._func = func

    def run(self, path):
        action = path.actions[-1]
        return self._func(*action.args, **action.kwargs)


class PathExecuter(Task):
    """Task that executes a path in the real object, and returns the result."""

    def __init__(self, result_callback=None):
        self._result_callback = result_callback

    def get_result_callback(self):
        return self._result_callback

    def run(self, path):
        result = path.execute(path.root_object)
        if self._result_callback is not None:
            self._result_callback(result)
        return result


class Orderer(Task):
    """Task to establish an order relation between two events.

    An orderer task will only match once all its dependencies have
    been run.
    """

    def __init__(self, path):
        self.path = path
        self._run = False 
        self._dependencies = []

    def replay(self):
        self._run = False

    def has_run(self):
        return self._run

    def may_run(self, path):
        for dependency in self._dependencies:
            if not dependency.has_run():
                return False
        return True

    def run(self, path):
        for dependency in self._dependencies:
            if not dependency.has_run():
                raise AssertionError("Should be after: %s" % dependency.path)
        self._run = True

    def add_dependency(self, orderer):
        self._dependencies.append(orderer)

    def get_dependencies(self):
        return self._dependencies


class SpecChecker(Task):
    """Task to check if arguments of the last action conform to a real method.
    """

    def __init__(self, method):
        self._method = method
        self._unsupported = False

        if method:
            try:
                self._args, self._varargs, self._varkwargs, self._defaults = \
                    inspect.getargspec(method)
            except TypeError:
                self._unsupported = True
            else:
                if self._defaults is None:
                    self._defaults = ()
                if type(method) is type(self.run):
                    self._args = self._args[1:]

    def get_method(self):
        return self._method

    def _raise(self, message):
        spec = inspect.formatargspec(self._args, self._varargs,
                                     self._varkwargs, self._defaults)
        raise AssertionError("Specification is %s%s: %s" %
                             (self._method.__name__, spec, message))

    def verify(self):
        if not self._method:
            raise AssertionError("Method not found in real specification")

    def may_run(self, path):
        try:
            self.run(path)
        except AssertionError:
            return False
        return True

    def run(self, path):
        if not self._method:
            raise AssertionError("Method not found in real specification")
        if self._unsupported:
            return # Can't check it. Happens with builtin functions. :-(
        action = path.actions[-1]
        obtained_len = len(action.args)
        obtained_kwargs = action.kwargs.copy()
        nodefaults_len = len(self._args) - len(self._defaults)
        for i, name in enumerate(self._args):
            if i < obtained_len and name in action.kwargs:
                self._raise("%r provided twice" % name)
            if (i >= obtained_len and i < nodefaults_len and
                name not in action.kwargs):
                self._raise("%r not provided" % name)
            obtained_kwargs.pop(name, None)
        if obtained_len > len(self._args) and not self._varargs:
            self._raise("too many args provided")
        if obtained_kwargs and not self._varkwargs:
            self._raise("unknown kwargs: %s" % ", ".join(obtained_kwargs))

def spec_checker_recorder(mocker, event):
    spec = event.path.root_mock.__mocker_spec__
    if spec:
        actions = event.path.actions
        if len(actions) == 1:
            if actions[0].kind == "call":
                method = getattr(spec, "__call__", None)
                event.add_task(SpecChecker(method))
        elif len(actions) == 2:
            if actions[0].kind == "getattr" and actions[1].kind == "call":
                method = getattr(spec, actions[0].args[0], None)
                event.add_task(SpecChecker(method))

Mocker.add_recorder(spec_checker_recorder)


class ProxyReplacer(Task):
    """Task which installs and deinstalls proxy mocks.

    This task will replace a real object by a mock in all dictionaries
    found in the running interpreter via the garbage collecting system.
    """

    def __init__(self, mock):
        self.mock = mock
        self.__mocker_replace__ = False

    def replay(self):
        global_replace(self.mock.__mocker_object__, self.mock)

    def restore(self):
        global_replace(self.mock, self.mock.__mocker_object__)


def global_replace(remove, install):
    """Replace object 'remove' with object 'install' on all dictionaries."""
    for referrer in gc.get_referrers(remove):
        if (type(referrer) is dict and
            referrer.get("__mocker_replace__", True)):
            for key, value in referrer.items():
                if value is remove:
                    referrer[key] = install


class Undefined(object):

    def __repr__(self):
        return "Undefined"

Undefined = Undefined()


class Patcher(Task):

    def __init__(self):
        super(Patcher, self).__init__()
        self._monitored = {} # {kind: {id(object): object}}
        self._patched = {}

    def is_monitoring(self, obj, kind):
        monitored = self._monitored.get(kind)
        if monitored:
            if id(obj) in monitored:
                return True
            cls = type(obj)
            if issubclass(cls, type):
                cls = obj
            bases = set([id(base) for base in cls.__mro__])
            bases.intersection_update(monitored)
            return bool(bases)
        return False

    def monitor(self, obj, kind):
        if kind not in self._monitored:
            self._monitored[kind] = {}
        self._monitored[kind][id(obj)] = obj

    def patch_attr(self, obj, attr, value):
        original = obj.__dict__.get(attr, Undefined)
        self._patched[id(obj), attr] = obj, attr, original
        setattr(obj, attr, value)

    def get_unpatched_attr(self, obj, attr):
        cls = type(obj)
        if issubclass(cls, type):
            cls = obj
        result = Undefined
        for mro_cls in cls.__mro__:
            key = (id(mro_cls), attr)
            if key in self._patched:
                result = self._patched[key][2]
                if result is not Undefined:
                    break
            elif attr in mro_cls.__dict__:
                result = mro_cls.__dict__.get(attr, Undefined)
                break
        if isinstance(result, object) and hasattr(type(result), "__get__"):
            if cls is obj:
                obj = None
            return result.__get__(obj, cls)
        return result

    def _get_kind_attr(self, kind):
        if kind == "getattr":
            return "__getattribute__"
        return "__%s__" % kind

    def replay(self):
        for kind in self._monitored:
            attr = self._get_kind_attr(kind)
            seen = set()
            for obj in self._monitored[kind].itervalues():
                cls = type(obj)
                if issubclass(cls, type):
                    cls = obj
                if cls not in seen:
                    seen.add(cls)
                    unpatched = getattr(cls, attr, Undefined)
                    self.patch_attr(cls, attr,
                                    PatchedMethod(kind, unpatched,
                                                  self.is_monitoring))
                    self.patch_attr(cls, "__mocker_execute__",
                                    self.execute)

    def restore(self):
        for obj, attr, original in self._patched.itervalues():
            if original is Undefined:
                delattr(obj, attr)
            else:
                setattr(obj, attr, original)
        self._patched.clear()

    def execute(self, action, object):
        attr = self._get_kind_attr(action.kind)
        unpatched = self.get_unpatched_attr(object, attr)
        try:
            return unpatched(*action.args, **action.kwargs)
        except AttributeError:
            if action.kind == "getattr":
                # The normal behavior of Python is to try __getattribute__,
                # and if it raises AttributeError, try __getattr__.   We've
                # tried the unpatched __getattribute__ above, and we'll now
                # try __getattr__.
                try:
                    __getattr__ = unpatched("__getattr__")
                except AttributeError:
                    pass
                else:
                    return __getattr__(*action.args, **action.kwargs)
            raise


class PatchedMethod(object):

    def __init__(self, kind, unpatched, is_monitoring):
        self._kind = kind
        self._unpatched = unpatched
        self._is_monitoring = is_monitoring

    def __get__(self, obj, cls=None):
        object = obj or cls
        if not self._is_monitoring(object, self._kind):
            return self._unpatched.__get__(obj, cls)
        def method(*args, **kwargs):
            if self._kind == "getattr" and args[0].startswith("__mocker_"):
                return self._unpatched.__get__(obj, cls)(args[0])
            mock = object.__mocker_mock__
            return mock.__mocker_act__(self._kind, args, kwargs, object)
        return method

    def __call__(self, obj, *args, **kwargs):
        # At least with __getattribute__, Python seems to use *both* the
        # descriptor API and also call the class attribute directly.  It
        # looks like an interpreter bug, or at least an undocumented
        # inconsistency.
        return self.__get__(obj)(*args, **kwargs)


def patcher_recorder(mocker, event):
    mock = event.path.root_mock
    if mock.__mocker_patcher__ and len(event.path.actions) == 1:
        patcher = mock.__mocker_patcher__
        patcher.monitor(mock.__mocker_object__, event.path.actions[0].kind)

Mocker.add_recorder(patcher_recorder)

########NEW FILE########
__FILENAME__ = test_errors
from nose.tools import *
from holland.lib.lvm.errors import *

def test_errors():
    exc = LVMCommandError('cmd', -1, 'error message')
    assert_equal(exc.cmd, 'cmd')
    assert_equal(exc.status, -1)
    assert_equal(exc.error, 'error message')

########NEW FILE########
__FILENAME__ = test_util
import os
import signal
from nose.tools import *
from holland.lib.lvm.util import *

def test_format_bytes():
    assert_equals(format_bytes(1024), '1.00KB')
    assert_equals(format_bytes(0), '0.00Bytes')

def test_getmount():
    assert_equals(getmount('/'), '/')
    assert_equals(getmount('/foobarbaz'), '/')

def test_getdevice():
    # XXX: bad hack
    dev = open('/etc/mtab', 'r').readline().split()[0].strip()
    assert_equals(getdevice('/'), dev)
    assert_equals(getdevice('/foobarbaz'), None)

def test_relpath():
    assert_raises(ValueError, relpath, '')
    assert_equals(relpath('/foo/bar/baz', '/foo/bar'), 'baz')
    assert_equals(relpath('/foo/bar/', '/foo/bar/'), os.curdir)
    assert_equals(relpath('/var/lib/mysql', '/'), 'var/lib/mysql')

def test_signalmanager():
    sigmgr = SignalManager()
    sigmgr.trap(signal.SIGINT)
    os.kill(os.getpid(), signal.SIGINT)
    ok_(sigmgr.pending)
    assert_equals(sigmgr.pending[0], signal.SIGINT)
    sigmgr.restore()
    assert_raises(KeyboardInterrupt, os.kill, os.getpid(), signal.SIGINT)

def test_parsebytes():
    # bytes without units should be interpretted as MB
    bytes = parse_bytes('1024')
    assert_equals(bytes, 1024**3)
    # this should not be bytes
    ok_(bytes > 1024)
   
    bytes = parse_bytes('1024G')
    assert_equals(bytes, 1024**4)

########NEW FILE########
__FILENAME__ = test_base
import sys
from nose.tools import *
from holland.lib.lvm.base import *
from holland.lib.lvm.util import getdevice
from tests.constants import *

def test_basevolume():
    assert_raises(NotImplementedError, Volume)

    class Test(Volume):
        pass

    ok_(Test())

    volume = Test({ 'foo' : 'bar', 'baz' : 'biz' })
    assert_equals(volume.foo, 'bar')
    assert_equals(volume.baz, 'biz')
    assert_raises(AttributeError, volume.__getattr__, 'blah')

    assert_raises(NotImplementedError, volume.reload)
    assert_raises(NotImplementedError, Test.lookup, 'foo')
    assert_raises(NotImplementedError, Test.search, 'foo')

    assert_equals(repr(Test()), 'Test()')


class TestPhysicalVolume(object):
    def test_create(self):
        """Test creating a physical volume"""

    def test_reload(self):
        """Test reloading a PhysicalVolume"""
        pv = PhysicalVolume.lookup('/dev/loop0')
        assert_equals(pv.pv_name, '/dev/loop0')
        pv.reload()
        assert_equals(pv.pv_name, '/dev/loop0')

    def test_lookup(self):
        """Test looking up a single physical volume"""
        pv = PhysicalVolume.lookup('/dev/loop0')
        assert_equals(pv.pv_name, '/dev/loop0')

    def test_lookup_failure(self):
        """Test looking up an invalid pv"""
        assert_raises(LookupError, PhysicalVolume.lookup, '/dev/loop1')
        
    def test_search(self):
        """Test searching for a physical volume"""
        # stupid simple test to make sure we're returning an iterable
        # not just a Volume object
        result = PhysicalVolume.search('/dev/loop0')
        ok_(not isinstance(result, Volume))
        pv = result.next()
        ok_(isinstance(pv, PhysicalVolume), "not a physical volume? %r" % pv)
        assert_raises(StopIteration, result.next)

    def test_repr(self):
        pv = PhysicalVolume.lookup('/dev/loop0')
        assert_equals(repr(pv), "PhysicalVolume(device='/dev/loop0')")


class TestVolumeGroup(object):

    def test_create(self):
        vg = VolumeGroup({ 'vg_name' : 'dba', 'vg_extent_size' : 4*1024**2 })
        assert_equals(vg.vg_name, 'dba')
        assert_equals(vg.vg_extent_size, 4194304)

    def test_lookup(self):
        vg = VolumeGroup.lookup("holland")
        assert_equals(vg.vg_name, 'holland')
        assert_equals(vg.lv_count, '1') # only holland/test_lv is created
        assert_equals(vg.pv_count, '1') # only /dev/loopN is assigned

    def test_failing_lookup(self):
        assert_raises(LookupError, VolumeGroup.lookup, 'holland_missing')

    def test_search(self):
        for vg in VolumeGroup.search('holland'):
            assert_equals(vg.vg_name, 'holland')
            assert_equals(vg.lv_count, '1') # only holland/test_lv is created
            assert_equals(vg.pv_count, '1') # only /dev/loopN is assigned

    def test_reload(self):
        # XXX: not sure a good way to check this - do something to change the vg
        vg = VolumeGroup.lookup('holland')
        vg.reload()
        assert_equals(vg.vg_name, 'holland')

    def test_repr(self):
        vg = VolumeGroup.lookup('holland')
        assert_equals(repr(vg), 'VolumeGroup(vg_name=holland)')

class TestLogicalVolume(object):
    def test_create(self):
        """Test creating a LogicalVolume"""
        lv = LogicalVolume({ 'lv_name' : 'mysql', 'vg_name' : 'dba' })
        assert_equals(lv.lv_name, 'mysql')

    def test_happy_lookup(self):
        """Test a loading an existing lv"""
        lv = LogicalVolume.lookup('holland/test_lv')
        assert_equals(lv.vg_name, 'holland')
        assert_equals(lv.lv_name, 'test_lv')

    def test_sad_lookup(self):
        assert_raises(LookupError, LogicalVolume.lookup, 'holland/test_lv_missing')

    def test_look_from_fspath(self):
        lv = LogicalVolume.lookup_from_fspath(MNT_DIR)
        assert_equals(lv.vg_name, TEST_VG)
        assert_equals(lv.lv_name, TEST_LV)

    def test_search(self):
        for lv in LogicalVolume.search('%s/%s' % (TEST_VG, TEST_LV)):
            assert_equals(lv.vg_name, TEST_VG)
            assert_equals(lv.lv_name, TEST_LV)

    def test_reload(self):
        # XXX: need to test changing attributes and reloading
        lv = LogicalVolume.lookup(TEST_VG)
        lv.reload()
        assert_equals(lv.lv_name, TEST_LV)

    def test_snapshot(self):
        lv = LogicalVolume.lookup('%s/%s' % (TEST_VG, TEST_LV))
        snapshot = None
        try:
            snapshot = lv.snapshot(lv.lv_name + '_snapshot', 1)
            ok_(lv != snapshot)
            ok_('s' in snapshot.lv_attr)
            assert_equals(snapshot.lv_size, snapshot.vg_extent_size)
            snapshot.mount(MNT_DIR, options='nouuid')
            ok_(snapshot.is_mounted())
            assert_equals(getdevice(MNT_DIR), os.path.realpath(snapshot.device_name()))
            lv.reload()
            snapshot.unmount()
            assert_equals(snapshot.is_mounted(), False)
            snapshot.remove()
            ok_(not snapshot.exists())
            snapshot = None
        finally:
            if snapshot and snapshot.is_mounted():
                snapshot.unmount()
            if snapshot and snapshot.exists():
                snapshot.remove()

    def test_filesystem(self):
        """Test looking up filesystem of lv"""
        lv = LogicalVolume.lookup('holland/test_lv')
        assert_equals(lv.filesystem(), 'xfs')

    def test_bad_filesystem(self):
        """Test looking for filesystem of a lv that doesn't exist"""
        lv = LogicalVolume()
        lv.vg_name = 'holland'
        lv.lv_name = 'test_lv_missing' # <- doesn't exist in our setup
        assert_raises(LookupError, lv.filesystem)

    def test_volume_group(self):
        lv = LogicalVolume.lookup('%s/%s' % (TEST_VG, TEST_LV))
        vg = lv.volume_group()
        assert_equals(vg.vg_name, lv.vg_name)
        assert_equals(vg.vg_extent_size, lv.vg_extent_size)

    def test_repr(self):
        lv = LogicalVolume.lookup('holland/test_lv')
        assert_equals(repr(lv), 'LogicalVolume(device=\'/dev/holland/test_lv\')')

########NEW FILE########
__FILENAME__ = test_errors
from nose.tools import *
from holland.lib.lvm.errors import *

def test_errors():
    exc = LVMCommandError('cmd', -1, 'error message')
    assert_equal(exc.cmd, 'cmd')
    assert_equal(exc.status, -1)
    assert_equal(exc.error, 'error message')

########NEW FILE########
__FILENAME__ = test_raw
import os, sys
import shutil
import tempfile
import subprocess
from nose.tools import *
from holland.lib.lvm.raw import *
from tests.constants import *

def test_pvs():
    pvs(LOOP_DEV)

def test_vgs():
    vg, = vgs(TEST_VG)
    assert_equals(vg['vg_name'], TEST_VG)
    assert_equals(int(vg['pv_count']), 1)

def test_lvs():
    lv, = lvs('%s/%s' % (TEST_VG,TEST_LV))
    assert_equals(lv['vg_name'], TEST_VG)
    vg_extents = int(lv['vg_extent_count'])
    vg_extent_size = int(lv['vg_extent_size'])

    assert_equals(int(lv['lv_size']), IMG_SIZE / 2)

def ensure_snapshot_unmount():
    try:
        umount('/dev/%s/%s_snapshot' % (TEST_VG, TEST_LV))
    except:
        pass

def test_snapshot():
    lvsnapshot('%s/%s' % (TEST_VG, TEST_LV), '%s_snapshot' % TEST_LV , 4, '512K')
    assert_raises(LVMCommandError, lvsnapshot, '%s/%s' % (TEST_VG, TEST_LV), '%s_snapshot' % TEST_LV , 1)
    mount('/dev/%s/%s_snapshot' % (TEST_VG, TEST_LV), '/mnt/tmp', options='nouuid,noatime', vfstype='xfs')
    umount('/dev/%s/%s_snapshot' % (TEST_VG, TEST_LV))
    lvremove('%s/%s_snapshot' % (TEST_VG, TEST_LV))
    assert_raises(LVMCommandError, lvremove, '%s/%s_snapshot' % (TEST_VG, TEST_LV)) # this should fail the 2nd time
test_snapshot.teardown = ensure_snapshot_unmount()

def test_blkid():
    info, = blkid('/dev/%s/%s' % (TEST_VG, TEST_LV))
    assert_equals(info['type'], 'xfs')

def test_bad_mount():
    assert_raises(LVMCommandError, mount, '/dev/%s/%s' % (TEST_VG, TEST_LV), os.path.join(MNT_DIR, 'missing'))

########NEW FILE########
__FILENAME__ = test_snapshot
import shutil
from nose.tools import *
from holland.lib.lvm import LogicalVolume
from holland.lib.lvm.snapshot import *
from tests.constants import *

class TestSnapshot(object):
    def setup(self):
        self.tmpdir = tempfile.mkdtemp()

    def teardown(self):
        shutil.rmtree(self.tmpdir)
 
    def test_snapshot_fsm(self):
        lv = LogicalVolume.lookup('%s/%s' % (TEST_VG, TEST_LV))
        name = lv.lv_name + '_snapshot'
        size = 1 # extent

        snapshot = Snapshot(name, size, self.tmpdir)
        snapshot.start(lv)

    def test_snapshot_fsm_with_callbacks(self):
        lv = LogicalVolume.lookup('%s/%s' % (TEST_VG, TEST_LV))
        name = lv.lv_name + '_snapshot'
        size = 1 # extent

        snapshot = Snapshot(name, size, self.tmpdir)
        def handle_event(event, *args, **kwargs):
            pass

        snapshot.register('pre-mount', handle_event)
        snapshot.register('post-mount', handle_event)
        snapshot.start(lv)

    def test_snapshot_fsm_with_failures(self):
        lv = LogicalVolume.lookup('%s/%s' % (TEST_VG, TEST_LV))
        name = lv.lv_name + '_snapshot'
        size = 1 # extent

        snapshot = Snapshot(name, size, self.tmpdir)

        def bad_callback(event, *args, **kwargs):
            raise Exception("Oooh nooo!")

        for evt in ('initialize', 'pre-snapshot', 'post-snapshot', 
                    'pre-mount', 'post-mount', 'pre-unmount', 'post-unmount',
                    'pre-remove', 'post-remove', 'finish'):
            snapshot.register(evt, bad_callback)
            assert_raises(CallbackFailuresError, snapshot.start, lv)
            snapshot.unregister(evt, bad_callback)
            if snapshot.sigmgr._handlers:
                raise Exception("WTF. sigmgr handlers still exist when checking event => %r", evt)

########NEW FILE########
__FILENAME__ = test_util
import os
import signal
from nose.tools import *
from holland.lib.lvm.util import *

def test_format_bytes():
    assert_equals(format_bytes(1024), '1.00KB')
    assert_equals(format_bytes(0), '0.00Bytes')

def test_getmount():
    assert_equals(getmount('/'), '/')
    assert_equals(getmount('/foobarbaz'), '/')

def test_getdevice():
    # XXX: bad hack
    dev = open('/etc/mtab', 'r').readline().split()[0].strip()
    assert_equals(getdevice('/'), dev)
    assert_equals(getdevice('/foobarbaz'), None)

def test_relpath():
    assert_raises(ValueError, relpath, '')
    assert_equals(relpath('/foo/bar/baz', '/foo/bar'), 'baz')
    assert_equals(relpath('/foo/bar/', '/foo/bar/'), os.curdir)

def test_signalmanager():
    sigmgr = SignalManager()
    sigmgr.trap(signal.SIGINT)
    os.kill(os.getpid(), signal.SIGINT)
    ok_(sigmgr.pending)
    assert_equals(sigmgr.pending[0], signal.SIGINT)
    sigmgr.restore()
    assert_raises(KeyboardInterrupt, os.kill, os.getpid(), signal.SIGINT)

def test_parsebytes():
    # bytes without units should be interpretted as MB
    bytes = parse_bytes('1024')
    assert_equals(bytes, 1024**3)
    # this should not be bytes
    ok_(bytes > 1024)
   
    bytes = parse_bytes('1024G')
    assert_equals(bytes, 1024**4)

########NEW FILE########
__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# holland.lib.mysql documentation build configuration file, created by
# sphinx-quickstart on Sat Apr 18 21:00:42 2009.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# The contents of this file are pickled, so don't put values in the namespace
# that aren't pickleable (module imports are okay, they're removed automatically).
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os

# If your extensions are in another directory, add it here. If the directory
# is relative to the documentation root, use os.path.abspath to make it
# absolute, like shown here.
#sys.path.append(os.path.abspath('.'))

# General configuration
# ---------------------

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['.templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'holland.lib.mysql'
copyright = u'2009, Andrew Garner'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = '0.3'
# The full version, including alpha/beta/rc tags.
release = '0.3'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of documents that shouldn't be included in the build.
#unused_docs = []

# List of directories, relative to source directory, that shouldn't be searched
# for source files.
exclude_trees = []

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'


# Options for HTML output
# -----------------------

# The style sheet to use for HTML and HTML Help pages. A file of that name
# must exist either in Sphinx' static/ path, or in one of the custom paths
# given in html_static_path.
html_style = 'default.css'

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['.static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_use_modindex = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, the reST sources are included in the HTML build as _sources/<name>.
#html_copy_source = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# If nonempty, this is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = ''

# Output file base name for HTML help builder.
htmlhelp_basename = 'hollandlibmysqldoc'


# Options for LaTeX output
# ------------------------

# The paper size ('letter' or 'a4').
#latex_paper_size = 'letter'

# The font size ('10pt', '11pt' or '12pt').
#latex_font_size = '10pt'

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, document class [howto/manual]).
latex_documents = [
  ('index', 'hollandlibmysql.tex', ur'holland.lib.mysql Documentation',
   ur'Andrew Garner', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# Additional stuff for the LaTeX preamble.
#latex_preamble = ''

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_use_modindex = True

########NEW FILE########
__FILENAME__ = cli
# mycmdparser.py
# Utility class to parse info from various mysql cli tools
# Written by: Andrew Garner <andrew.garner@rackspace.com>

"""
This module includes utilities to parse mysql 
command line utilities such as mysql, mysqlcheck,
mysqld (server), etc.

Accepted options are available as dictionary attributes
on the mysqlcliParser.
Further attributes:
- Returns tuple (major, minor, rev) for cli utility
cli_version()
- Returns list of my.cnf locations this cli utility searches
  by default
mycnf_locations()
- Returns list of config groups within each my.cnf this utility 
  reads by default
mycnf_groups()
"""
import os
import re
import commands

#class CmdOption_OLD(object):
#  def __init__(self, short_option=None, 
#                     long_option=None,
#                     default_value=None, 
#                     arg_type=None, 
#                     arg_optional=True):
#    self.short_option = short_option
#    self.long_option = long_option 
#    self.default_value = default_value
#    self.arg_type = arg_type
#    self.arg_optional = arg_optional
#
#  def __str__(self):
#    return "--%-30s%s [%s]" % (self.long_option,
#                               self.arg_type and "=%-7s" % self.arg_type or " "*8, 
#                               self.default_value)

class CmdOption(object):
    def __init__(self, *args, **kwargs):
        self.short_option = kwargs.get('short_option', None)
        self.long_option = kwargs.get('long_option', None)
        self.default_value = kwargs.get('default_value', None)
        self.arg_type = kwargs.get('arg_type', None)
        self.arg_optional = kwargs.get('arg_optional', True)
                                                  
    def __str__(self):
        arg_type = self.arg_type and "=%-7s" % self.arg_type or " "*8
        return "--%-30s%s [%s]" % \
            (self.long_option, arg_type, self.default_value)
            
class MyCmdParser(dict):
    """
    Parse the --help --verbose output of a MySQL command and generate a 
    dictionary of accepted short and long options.
    """
    def __init__(self, cli_path):
        dict.__init__(self)
        self.cli_path = cli_path
        self.cli_info = self._run_cli_help()
        self.cli_version = self._parse_cli_version()
        self.mycnf_locations = self._parse_mycnf_locations()
        self.mycnf_groups = self._parse_mycnf_groups()
        self.cli_options = self._parse_cli_options()
        self.cli_defaults = self._parse_cli_defaults()
        for opt in self.cli_options:
            sopt, lopt, arg, optional = opt
            lopt = lopt.replace('_', '-')
            if arg is not None:
                default_value = self.cli_defaults.get(lopt)
            else:
                arg = None
                default_value = None
            opt = CmdOption(short_option=sopt, 
                            long_option=lopt, 
                            default_value=default_value,  
                            arg_optional=optional,
                            arg_type=arg)
            self[sopt] = opt
            self[lopt] = opt
            
        self.cli_options = self.values()

    def _run_cli_help(self):
        args = [self.cli_path,
                '--no-defaults',
                '--loose-user=nobody',
                '--help',
                '--verbose']
        cli_cmd = ' '.join(args)
        status, cli_output = commands.getstatusoutput(cli_cmd)

        if status != 0:
            raise IOError, cli_output
        
        return cli_output
    
    def _parse_cli_options(self):
        # Find all valid options
        optcre = re.compile(r'^  (?:-(?P<short_option>.), )?' +
                            r'--(?P<opt>[-a-zA-Z0-9_]+)' +
                            r'(?:(?:=(?P<type>#|name))|' +
                            r'(?:\[=(?P<opt_type>#|name)\]))?', re.M)
        valid_options = optcre.findall(self.cli_info)
        return valid_options

    def _parse_cli_default_value(self, value):
        if re.match('^\d+$', value):
            return int(value)
        if value == 'TRUE':
            return True
        if value == 'FALSE':
            return False
        if value == '(No default value)':
            return None
        return value

    def _parse_cli_defaults(self):
        defaults = {}
        defaults_section_cre = re.compile(r'\n(?:-+? -+?\n(.*))(\n|$)', 
                                          re.M|re.S)
        m = defaults_section_cre.search(self.cli_info)
        if m:
            defaults_cre = re.compile(
                r'^(?P<opt>[a-zA-Z_\-]+)(?:\s+(?P<value>.+?))?$')
            for line in m.groups()[0].splitlines():
                m = defaults_cre.match(line)
                if m:
                    key, value = m.groups()
                    key = key.replace('_', '-') 
                    value = self._parse_cli_default_value(value)
                    defaults[key] = value
        return defaults

    def _parse_cli_version(self):
        base_bin = os.path.basename(self.cli_path)
        vers_cre = re.compile(r'^.* Ver .*?(\d+\.\d+.\d+)', re.M)
        m = vers_cre.search(self.cli_info)
        if m:
            return tuple(map(int, m.groups()[0].split('.')))
        return None 

    def _parse_mycnf_locations(self):
        # Default options are read from the following files in the given order:
        # /etc/my.cnf ~/.my.cnf
        mycnf_loc_cre = re.compile(r'Default options.*order:\n([^\n]+)', re.M)
        m = mycnf_loc_cre.search(self.cli_info)
        if m:
            mycnf_locs = m.groups()[0].split()
            return mycnf_locs
        return None

    def _parse_mycnf_groups(self):
        # The following groups are read: mysql_cluster cli server cli-5.0
        mycnf_grp_cre = re.compile(r'^The.*groups are read: (.+)$', re.M)
        m = mycnf_grp_cre.search(self.cli_info)
        if m:
            return m.groups()[0].split()
        return None

########NEW FILE########
__FILENAME__ = base
"""MySQLdb.Connection wrappers"""

import sys
import re
import logging
from textwrap import dedent
import MySQLdb
import MySQLdb.connections

MySQLError = MySQLdb.MySQLError
ProgrammingError = MySQLdb.ProgrammingError
OperationalError = MySQLdb.OperationalError

LOG = logging.getLogger(__name__)

__all__ = [
    'connect',
    'MySQLClient',
    'PassiveMySQLClient',
    'AutoMySQLClient',
    'MySQLError',
    'ProgrammingError',
    'OperationalError',
]

def flatten_list(a_list):
    """Given a list of sequences, return a flattened list

    >>> flatten_list([['a', 'b', 'c'], ['e', 'f', 'j']])
    ['a', 'b', 'c', 'e', 'f', 'j']
    >>> flatten_list([['aaaa', 'bbbb'], 'b', 'cc'])
    ['aaaa', 'bbbb', 'b', 'cc']
    """
    # isinstance check to ensure we're not iterating over characters
    # in a string
    return sum([isinstance(item, (list, tuple)) and list(item) or [item]
                    for item in a_list], [])

class MySQLClient(object):
    """MySQLdb Helper

    Provides common functions for reading meta-data
    from and performing administrative functions on
    a MySQL server.

    This class also behave as a MySQLdb.Connection
    object and can be used to perform arbitrary queries
    using the Python dbapi.
    """

    SCOPE = ['GLOBAL', 'SESSION']

    def __init__(self, *args, **kwargs):
        """Create a new MySQLClient instance

        This is a simple wrapper for MySQLdb.connect(*args, **kwargs)

        :param args: args tuple to pass to MySQLdb.connect
        :param kwargs: kwargs dict to pass to MySQLdb.connect
        """
        self._connection = MySQLdb.connect(*args, **kwargs)

    def flush_tables(self):
        """Flush MySQL server table data to disk


        Runs FLUSH TABLES
        Also flushes the query cache and closes all
        open tables:

        http://dev.mysql.com/doc/refman/5.0/en/flush.html
        """
        cursor = self.cursor()
        cursor.execute('FLUSH /*!40101 LOCAL */ TABLES')
        cursor.close()

    def flush_tables_with_read_lock(self):
        """Acquire MySQL server global read lock

        Runs FLUSH TABLES WITH READ LOCK
        """
        cursor = self.cursor()
        cursor.execute('FLUSH TABLES WITH READ LOCK')
        cursor.close()

    def unlock_tables(self):
        """Unlock any tables previously locked by this session

        Runs UNLOCK TABLES
        """
        cursor = self.cursor()
        cursor.execute('UNLOCK TABLES')
        cursor.close()

    def show_databases(self):
        """List available databases

        :returns: list of database names
        """
        sql = "SHOW DATABASES"
        cursor = self.cursor()
        cursor.execute(sql)
        # Flatten the list of lists containing the database names
        db_list = flatten_list(cursor.fetchall())
        cursor.close()
        return db_list

    def _show_table_metadata50(self, database):
        """MySQL 5.0 (and below) implement of show_table_metadata()

        This version uses SHOW TABLE STATUS and pulls out useful metadata
        :param database: database to extract metadata from
        :returns: list of dictionaries, one dictionary per table
        """
        sql = "SHOW TABLE STATUS FROM `%s`" % database.replace('`','``')
        cursor = self.cursor()
        try:
            cursor.execute(sql)
        except MySQLError, exc:
            LOG.error("MySQL reported an error while running %s. [%d] %s", 
                      sql, *exc.args)
            raise
        names = [info[0].lower() for info in cursor.description]
        result = []
        for row in cursor:
            row = dict(zip(names, row))
            row['database'] = database
            row['data_size'] = (row.pop('data_length') or 0)
            row['index_size'] = (row.pop('index_length') or 0)
            # coerce null engine to 'view' as necessary
            if row['engine'] is None:
                if row['comment'] == 'VIEW':
                    row['engine'] = 'VIEW'
                else:
                    row['engine'] = ''
                    if 'references invalid table' in (row['comment'] or ''):
                        LOG.warning("Invalid view %s.%s: %s", 
                                    row['database'], row['name'],
                                    row['comment'] or '')
                    if 'Incorrect key file' in (row['comment'] or ''):
                        LOG.warning("Invalid table %s.%s: %s",
                                    row['database'], row['name'],
                                    row['comment'] or '')
            row['is_transactional'] = row['engine'].lower() in ['view', 
                                                                'innodb']
            for key in row.keys():
                valid_keys = [
                    'database',
                    'name',
                    'data_size',
                    'index_size',
                    'engine',
                    'is_transactional'
                ]
                if key not in valid_keys:
                    row.pop(key)
            result.append(row)
        cursor.close()
        return result

    def _show_table_metadata51(self, database):
        """MySQL 5.1+ implementation of show_table_metadata

        This version uses the information schema primarily so
        we can identify whether an engine is transactional by
        examining the INFORMATION_SCHEMA.ENGINES table.

        :param database: database to extract metadata from
        :returns: list of dictionaries, one dictionary per table
        """
        sql = ("SELECT TABLE_SCHEMA AS `database`, "
               "          TABLE_NAME AS `name`, "
               "          COALESCE(DATA_LENGTH, 0) AS `data_size`, "
               "          COALESCE(INDEX_LENGTH, 0) AS `index_size`, "
               "          COALESCE(ENGINE, 'view') AS `engine`, "
               "          (TRANSACTIONS = 'YES' OR ENGINE IS NULL) AS `is_transactional` "
               "FROM INFORMATION_SCHEMA.TABLES "
               "LEFT JOIN INFORMATION_SCHEMA.ENGINES USING (ENGINE) "
               "WHERE TABLE_SCHEMA = %s")
        cursor = self.cursor()
        cursor.execute(sql, (database))
        names = [info[0] for info in cursor.description]
        all_rows = cursor.fetchall()
        result = [dict(zip(names, row)) for row in all_rows]
        cursor.close()
        return result

    def show_table_metadata(self, database):
        """Iterate over the table metadata for the specified database.

        :param database: database to extract metadata from
        :returns: list of dicts, one dict per table
        """
        try:
            if self.server_version() < (5,1):
                return self._show_table_metadata50(database)
            else:
                return self._show_table_metadata51(database)
        except MySQLError, exc:
            exc.args = (exc.args[0], exc.args[1].decode('utf8'))
            raise

    def show_tables(self, database, full=False):
        """List tables in the given database

        Runs SHOW TABLES FROM ``database`` and return a list of table
        names.

        If `full` is requested, then SHOW FULL TABLES FROM `database`
        will be run and a list of (name, kind) tuples will be returned
        where `kind` is a string matching either 'BASE TABLE' for a normal
        table or 'VIEW' if a table is actually a view.

        :param database: Required.  database name to list tables from
        :param full: Optional. include table type n the results
        :returns: list of table names
        """
        sql = "SHOW %sTABLES FROM `%s`" % \
            (['', 'FULL '][int(full)],
             database.replace('`','``'))
        cursor = self.cursor()
        cursor.execute(sql)
        try:
            if full:
                return [(table, kind) for table, kind in cursor]
            else:
                return [table for table in cursor]
        finally:
            cursor.close()

    def show_table_status(self, database):
        """SHOW TABLE STATUS

        :param database: database to extract table status from
        :returns: list of tuples
        """
        sql = "SHOW TABLE STATUS"
        cursor = self.cursor()
        cursor.execute(sql)
        try:
            return [row for row in cursor]
        finally:
            cursor.close()

    def show_create_view(self, schema, name, use_information_schema=True):
        """Attempt to retrieve the CREATE VIEW statement for a given view

        This method will return None if no view could be found or any of the
        view queries have failed.

        SHOW CREATE VIEW will fail if a view references a column that
        no longer exists.

        If SHOW CREATE VIEW fails, this method attempts to reconstruct the
        CREATE VIEW statement from INFORMATION_SCHEMA.VIEWS, as long as
        use_information_schema=True.  A view retrieved in this manner will
        be missing the ALGORITHM attribute which would otherwise be reported
        by SHOW CREATE VIEW.
        """
        cursor = self.cursor()
        try:
            try:
                if cursor.execute('SHOW CREATE VIEW `%s`.`%s`' %
                                  (schema, name)):
                    return cursor.fetchone()[1]
            except MySQLError, exc:
                LOG.warning("!!! SHOW CREATE VIEW failed for `%s`.`%s`. "
                            "The view likely references columns that no "
                            "longer exist in the underlying tables.",
                            schema, name)

            if not use_information_schema:
                return None

            LOG.warning("!!! Reconstructing view definition `%s`.`%s` from "
                        "INFORMATION_SCHEMA.VIEWS.  This definition will not "
                        "have an explicit ALGORITHM set.", schema, name)
            try:
                sql = dedent("""
                             SELECT CONCAT(
                                'CREATE DEFINER=', DEFINER,
                                ' SQL SECURITY ', SECURITY_TYPE,
                                ' VIEW ', TABLE_NAME,
                                ' AS ', VIEW_DEFINITION,
                                CASE
                                WHEN CHECK_OPTION <> 'NONE' THEN
                                    CONCAT(' WITH ',
                                           CHECK_OPTION,
                                           ' CHECK OPTION')
                                ELSE
                                    ''
                                END
                                )
                                FROM INFORMATION_SCHEMA.VIEWS
                                WHERE TABLE_SCHEMA = %s
                                AND TABLE_NAME = %s
                             """)
                if cursor.execute(sql, (schema, name)):
                    return cursor.fetchone()[0]
            except MySQLError, exc:
                LOG.debug("INFORMATION_SCHEMA.VIEWS(%s,%s) failed: [%d] %s ",
                        schema, name, *exc.args)
            return None
        finally:
            cursor.close()

    def show_create_table(self, database, table):
        """Fetch DDL for a table

        Runs SHOW CREATE TABLE `database`.`table` and
        returns only the DDL portion

        :param database: database the table is in
        :param table: name of the table
        :raises: MySQLError, if the table does not exist
        :returns: DDL string for the given string
        """

        sql = "SHOW CREATE TABLE `%s`.`%s`"
        database = database.replace('`', '``')
        table = table.replace('`', '``')
        cursor = self.cursor()
        if cursor.execute(sql % (database, table)):
            return cursor.fetchone()[1]
        cursor.close()

    def show_slave_status(self):
        """Fetch MySQL slave status

        :returns: slave status dict
        """
        sql = "SHOW SLAVE STATUS"
        cursor = self.cursor()
        cursor.execute(sql)
        keys = [col[0].lower() for col in cursor.description]
        slave_status = cursor.fetchone()
        cursor.close()

        if not slave_status:
            return None
        else:
            return dict(zip(keys, slave_status))

    def show_master_status(self):
        """Fetch MySQL master status"""
        sql = "SHOW MASTER STATUS"
        cursor = self.cursor()
        cursor.execute(sql)
        keys = [col[0].lower() for col in cursor.description]
        master_status = cursor.fetchone()
        cursor.close()

        if not master_status:
            return None
        else:
            return dict(zip(keys, master_status))

    def start_slave(self):
        """Run START SLAVE on the connected MySQL instance"""
        sql = "START SLAVE"
        cursor = self.cursor()
        result = cursor.execute(sql)
        cursor.close()
        return result

    def stop_slave(self, sql_thread_only=False):
        """Run STOP SLAVE on the connected MySQL instance"""
        sql = "STOP SLAVE"
        if sql_thread_only:
            sql += " SQL_THREAD"
        cursor = self.cursor()
        result = cursor.execute(sql)
        cursor.close()
        return result

    def show_status(self, key, session=False):
        """Fetch MySQL server status"""
        if session is not None:
            scope = self.SCOPE[session]
        else:
            # 4.1 support - GLOBAL/SESSION STATUS is not implemented
            scope = ''
        sql = 'SHOW %s STATUS LIKE ' % scope + '%s'
        cursor = self.cursor()
        cursor.execute(sql, (key,))
        key, value = cursor.fetchone()
        cursor.close()
        return value

    def show_variable(self, key, session=False):
        """Fetch MySQL server variable"""
        scope = self.SCOPE[session]
        sql = 'SHOW %s VARIABLES LIKE ' % scope + '%s'
        cursor = self.cursor()
        if cursor.execute(sql, (key,)):
            value = cursor.fetchone()[1]
        else:
            value = None
        cursor.close()
        return value

    def set_variable(self, key, value, session=True):
        """Set a MySQL server variable.

        This method defaults to setting the variable for the session
        rather than globally.
        """
        sql = "SET %(scope)s %(variable)s = %(value)r" % \
            { 'scope' : self.SCOPE[session],
              'variable' : key,
              'value' : value
            }
        cursor = self.cursor()
        cursor.execute(sql)
        cursor.close()
        return self.show_variable(key, session)

    def server_version(self):
        """
        server_version(self)
        returns a numeric tuple: major, minor, revision versions (respectively)
        """
        version = self.get_server_info()
        m = re.match(r'^(\d+)\.(\d+)\.(\d+)', version)
        if m:
            return tuple([int(v) for v in m.groups()])
        else:
            raise MySQLError("Could not match server version: %r" % version)

    def __getattr__(self, key):
        """Pass through to the underlying MySQLdb.Connection object"""
        return getattr(self._connection, key)


class PassiveMySQLClient(MySQLClient):
    """A client connection that defers the connection process until
    the connect method is called"""

    def __init__(self, *args, **kwargs):
        self._connection = None
        self._args = args
        self._kwargs = kwargs

    def connect(self):
        """Connect to MySQL using the connection parameters this instance
        was created with.

        :raises: `MySQLError`
        """
        self._connection = MySQLdb.connect(*self._args, **self._kwargs)

    def disconnect(self):
        """Disconnect this instance from MySQL"""
        try:
            if self._connection:
                self._connection.close()
        finally:
            self._connection = None


class AutoMySQLClient(PassiveMySQLClient):
    """A client connection that deferred the connection process until
    `connect()` is called or one of the standard `MySQLClient` methods
    is requested"""

    def __getattr__(self, key):
        if self._connection is None:
            getattr(MySQLdb.connections.Connection, key)
            LOG.debug("Connected to MySQL")
            self.connect()

        # ensure the connection is usable
        try:
            self._connection.ping()
        except MySQLError:
            LOG.info("Reconnecting to MySQL after failed ping")
            self.connect()

        return PassiveMySQLClient.__getattr__(self, key)

def connect(config, client_class=AutoMySQLClient):
    """Create a MySQLClient object from a dict

    :param config: dict-like object containing zero or more of
                   the keys:
                    user
                    password
                    host
                    port
                    socket
                    ssl
                    compress
    :returns: `MySQLClient` instance
    """

    # map standard my.cnf parameters to
    # what MySQLdb.connect expects
    # http://mysql-python.sourceforge.net/MySQLdb.html#mysqldb
    #FIXME: SSL is more complicated than just a single param string
    cnf_to_mysqldb = {
        'user' : 'user', # same
        'password' : 'passwd', # weird
        'host' : 'host', # same
        'port' : 'port',
        'socket' : 'unix_socket',
        'ssl' : 'ssl',
        'compress' : 'compress'
    }

    value_conv = {
        'port' : int,
        # XXX: MySQLdb doesn't handle unicode credentials well
        #      These are encoded to utf8 byte strings as a result
        'user' : lambda s: s.encode('utf8'),
        'password' : lambda s: s.encode('utf8'),
    }

    args = {}
    for key in config:
        # skip undefined values
        if config.get(key) is None:
            continue
        try:
            # normalize the value. port => int
            value = value_conv.get(key, unicode)(config[key])
            # convert my.cnf parameters to what MySQLdb expects
            args[cnf_to_mysqldb[key]] = value
        except KeyError:
            LOG.warn("Skipping unknown parameter %s", key)
    # also, always use utf8
    return client_class(charset='utf8', **args)

########NEW FILE########
__FILENAME__ = legacy
import re
import textwrap
import logging
import MySQLdb
from MySQLdb import OperationalError
from MySQLdb.constants.CLIENT import INTERACTIVE

__all__ = [
    'connect',
    'MySQLClient',
    'OperationalError',
    'ProgrammingError',
    'DatabaseError'
]

LOGGER = logging.getLogger(__name__)

class MySQLClient(object):
    def __init__(self, **kwargs):
        """
        Initialize a MySQLClient connections.  Keyword arguments are passed
        directly to MySQLdb.connect().  See MySQLdb for all known arguments.
        
        Possible Arguments:
        
        host        -- Name of host to connect to. 
        user        -- User to authenticate as. 
        passwd      -- Password to authenticate with.
        db          -- Database to use. 
        port        -- TCP port of MySQL server.
        unix_socket -- Location of UNIX socket.
        """
        self._conn = MySQLdb.connect(**kwargs)
    
    def quote_id(self, *args):
        """
        quote_id(self, *args)
        return a qualified list of quoted schema components
        ['test','bar`foo', 'column'] => "`test`.`bar``foo`.`columns`"
        """
        if not args: return None
        return '.'.join(map(lambda x: '`%s`' % x.replace('`','``'), args))

    def unquote_id(self, *args):
        result = []
        for arg in args:
            arg = arg[1:-1].replace('``', '`')
            result.append(arg)
        return result
 
    def quote(self, *args):
        """
        quote(self, *args)
        return a comma delimited string with each element in args quoted
        ['a', '\'b', 'c'] => "'a','''b','c'"
        """
        if not args: return None
        return ','.join(map(lambda x: "'%s'" % x.replace("'","''"), args))

    def show_databases(self):
        """
        Return a list of databases.
        """
        cursor = self.cursor()
        cursor.execute('SHOW DATABASES')
        result = [db for db, in cursor]
        cursor.close()
        return result
    
    def show_tables(self, db):
        """
        Return a list of tables for 'db'.
        
        Arguments:
        
        db -- The database name.
        """
        cursor = self.cursor()
        # probably should filter views
        cursor.execute('SHOW TABLES FROM %s' % self.quote_id(db))
        result = [tbl for tbl, in cursor]
        cursor.close()
        return result

    def show_table_status(self, db):
        """
        Return a the table status for 'db'.  Returns an iterable generator
        object.
        
        Arguments:
        
        db -- The database name.
        """
        cursor = self.cursor()
        cursor.execute('SHOW TABLE STATUS FROM %s' % self.quote_id(db))
        hdr = [d[0].lower() for d in cursor.description]
        while True:
            row = cursor.fetchone()
            if not row:
                break
            tbl_status = dict(zip(hdr, row))
            yield tbl_status
        cursor.close()

    def show_variable(self, name, session_only=False):
        """
        Returns the result of SHOW GLOBAL VARIABLIES LIKE '${name}' without
        any glob wild cards (only returns a single result (string)).
        
        Arguments:
        
        name         -- Name of the 'like' variable modifier
        session_only -- Boolean.  Only show session variables, rather than 
                        global.
        """
        cursor = self.cursor()
        if session_only:
            cursor.execute('SHOW SESSION VARIABLES LIKE %s', name)
        else:
            cursor.execute('SHOW GLOBAL VARIABLES LIKE %s', name)

        try:
            _, value = cursor.fetchone()
        except TypeError, e:
            value = None
        cursor.close()
        return value
    
    def show_variables_like(self, name, session_only=False):
        """
        Returns the result of SHOW GLOBAL VARIABLIES LIKE '%${name}%' with
        the glob wild card to return all matching variables.
        
        Arguments:
        
        name         -- Name of the 'like' variable modifier
        session_only -- Boolean.  Only show session variables, rather than 
                        global.
        """
        cursor = self.cursor()
        if session_only:
            cursor.execute('SHOW SESSION VARIABLES LIKE %s', name)
        else:
            cursor.execute('SHOW GLOBAL VARIABLES LIKE %s', name)

        variables = {}
        for row in cursor.fetchall():
            variables[row[0]] = row[1]
        cursor.close()
        return variables
    
    def set_variable(self, name, value, session=True):
        """
        Set a variable in the running server
        """
        cursor = self.cursor()
        name = self.quote_id(name)
        sql = 'SET ' + ['GLOBAL', 'SESSION'][session] + ' ' + name + ' = %s'
        cursor.execute(sql, value)
        if not session:
            LOGGER.debug("GLOBAL variable set: %s = %s" % (name, value))
        cursor.close()

    def set_wait_timeout(self, value):
        """
        Change the idle timeout for this connection.  This method is 
        deprecated, use MySQLClient.set_variable.
        
        If this connection is flagged as interactive interactive_timeout
        will be set, otherwise wait_timeout is set
        """
        if self.client_flag & INTERACTIVE:
            self.set_variable('interactive_timeout', value)
        else:
            self.set_variable('wait_timeout', value)

    def show_indexes(self, db, tbl):
        """
        Returns a dictionary of index for the database
        and table specified
        """
        cursor = self.cursor()
        sql = "SHOW INDEXES FROM %s" % self.quote_id(db, tbl)
        cursor.execute(sql)
        hdr = [d[0].lower() for d in cursor.description]
        info = {}
        for row in cursor.fetchall():
            row = dict(zip(hdr, row))
            info.setdefault(row.get('key_name'), [])\
                            .append(row.get('column_name'))
        cursor.close()
        return info

    def flush_logs(self):
        """
        Runs FLUSH LOGS
        """
        cursor = self.cursor()
        LOGGER.debug("Query: FLUSH LOGS executed.")
        cursor.execute('FLUSH LOGS')
        cursor.close()
    
    def flush_tables(self, table_list=None):
        """
        Runs FLUSH TABLES, by default flushes all tables.  Only flush specific
        tables by passing a list of database.table names.
        """
        if table_list:
            for db_and_table in table_list:
                db, table = db_and_table.split('.')
                cursor = self.cursor()
                LOGGER.debug('Query: FLUSH TABLES %s.%s' % (db, table))
                cursor.execute('FLUSH TABLES %s.%s' % (db, table))
        else:
            cursor = self.cursor()
            LOGGER.debug('Query: FLUSH TABLES')
            cursor.execute('FLUSH TABLES')
        cursor.close()
        
    def flush_tables_with_read_lock(self, extra_flush=False):
        """
        Runs FLUSH TABLES WITH READ LOCK
        """
        cursor = self.cursor()
        if extra_flush:
            LOGGER.debug('Query: FLUSH TABLES')   
            cursor.execute('FLUSH TABLES')
        LOGGER.debug('Query: FLUSH TABLES WITH READ LOCK')    
        cursor.execute('FLUSH TABLES WITH READ LOCK')
        cursor.close()

    def lock_tables(self, table_list=None):
        if not table_list:
            return
        query = 'LOCK TABLES ' + ' READ LOCAL, '.join(table_list)\
              + ' READ LOCAL'
        LOGGER.debug("Query: %s", query)
        cursor = self.cursor()
        cursor.execute(query)
        cursor.close()

    def unlock_tables(self):
        cursor = self.cursor()
        LOGGER.debug('Query: UNLOCK TABLES')
        cursor.execute('UNLOCK TABLES')
        cursor.close()
        
    def walk_databases(self):
        for db in self.show_databases():
            yield db

    def walk_tables(self, dbinclude=None):
        """
            walk_tables(self, include=None, exclude=None)
            Walks over the tables in the databases in include and returns
            (db, tbl_status) tuples where tbl_status is the dictionary from 
            a SHOW TABLE STATUS    row.
            if include is None, include all databases
                except those in exclude
            otherwise, only visit tables in the include list
                except those also in the exclude list
        """
        for db in self.show_databases():
            if db not in (dbinclude or ()):
                continue
            for tbl_status in self.show_table_status(db):
                tbl_status['db'] = db
                yield tbl_status

    def show_master_status(self):
        cursor = self.cursor()
        info = None
        if cursor.execute('SHOW MASTER STATUS'):
            info = cursor.fetchone()[0:2]
        cursor.close()
        return info

    def show_slave_status(self):
        cursor = self.cursor(MySQLdb.cursors.DictCursor)
        info = None
        cursor.execute('SHOW SLAVE STATUS')
        info = cursor.fetchone()
        cursor.close()
        return info

    def is_slave_running(self):
        info = self.show_slave_status()
        if not info:
            return False
        return (info.get('Slave_IO_Running', 'No') == 'Yes'
                and info.get('Slave_SQL_Running', 'No') == 'Yes')

    def start_slave(self):
        cursor = self.cursor()
        #FIXME: handle other warnings?
        LOGGER.debug("Query: START SLAVE")
        cursor.execute('START SLAVE')
        cursor.close()
    
    def stop_slave(self):
        if not self.is_slave_running():
            raise OperationalError("Slave is not running")
        cursor = self.cursor()
        cursor.execute('STOP SLAVE')
        messages = cursor.messages
        cursor.close()
        if messages:
            raise OperationalError("%s[%d]: %s" % messages[1])

    def show_transactional_engines(self):
        """
        show_transaction_engines(self)
        returns a list of engines with transactional capabilities suitable for
        mysqldump's --single-transaction flag
        """
        if self.server_version() < (5, 1, 2):
            # No access to an engines transactional status
            # before 5.1.2, so statically code the ones we
            # know about
            return ['innodb', 'berkelydb']
        else:
            cursor = self.cursor()
            cursor.execute("""SELECT Engine
                              FROM INFORMATION_SCHEMA.ENGINES
                              WHERE TRANSACTIONS = 'YES'""")
            result = [eng[0].lower() for eng in cursor.fetchall()]
            cursor.close()
            return result

    def server_version(self):
        """
        server_version(self)
        returns a numeric tuple: major, minor, revision versions (respectively)
        """
        version = self.get_server_info()
        m = re.match(r'^(\d+)\.(\d+)\.(\d+)', version)
        if m:
            return tuple(map(int, m.groups()))
        else:
            # TODO: make this prettier
            raise OperationalError("Could not match server version")

    def is_transactional(self, engine):
        if not engine:
            return False

        if not hasattr(self, '_txn_ngn_cache'):
            self._txn_ngn_cache = self.show_transactional_engines() + ['view']
        return engine.lower() in self._txn_ngn_cache

    def encode_as_filename(self, name):
        if self.server_version() < (5, 1, 2):
            raise OperationalError, \
                "MySQLClient.encode_as_filename not compatible with MySQL < 5.1."
                
        cursor = self.cursor()
        orig_charset = self.show_variable('character_set_results', 
                                           session_only=True)
        try:
            self.set_variable('character_set_results', 
                              'filename', 
                              session=True)
            cursor.execute('SELECT %s', name)
            filename, = [x for x, in cursor]
            cursor.close()
            self.set_variable('character_set_results', 
                              orig_charset, 
                              session=True)
        except OperationalError, e:
            # try again just to make sure
            self.set_variable('character_set_results', orig_charset, session=True)
            raise OperationalError, e
        return filename

    def show_encoded_dbs(self):
        if self.server_version() < (5, 1, 2):
            raise OperationalError, \
                "MySQLClient.show_encoded_dbs not compatible with MySQL < 5.1."
                
        charset_name = self.get_character_set_info()['name']
        self.set_character_set('binary')
        cursor = self.cursor()
        cursor.execute('''SELECT CONVERT(SCHEMA_NAME USING utf8) AS utf8_name,
                          CONVERT(SCHEMA_NAME USING filename) AS encoded_name
                          FROM INFORMATION_SCHEMA.SCHEMATA''')
        result = []
        for utf8_name, encoded_name in cursor:
            result.append((utf8_name, encoded_name))
        cursor.close()
        self.set_character_set(charset_name)
        return result

    def run_stmt(self, sql):
        cursor = self.cursor()
        cursor.execute(sql)
        cursor.close()

    # pass through to underlying connection object
    def __getattr__(self, key):
        return getattr(self._conn, key)

# map standard my.cnf parameters to
# what MySQLdb.connect expects
# http://mysql-python.sourceforge.net/MySQLdb.html#mysqldb
CNF_TO_MYSQLDB = {
    'user' : 'user', # same
    'password' : 'passwd', # weird
    'host' : 'host', # same
    'port' : 'port',
    'socket' : 'unix_socket',
    'ssl' : 'ssl',
    'compress' : 'compress'
}

def connect(**kwargs):
    args = {}
    for key in kwargs:
        if key in CNF_TO_MYSQLDB:
            args[CNF_TO_MYSQLDB[key]] = kwargs[key]
        else:
            LOGGER.warn("Skipping unknown parameter %s", key)
    return MySQLClient(use_unicode=True, charset='utf8', **args)

########NEW FILE########
__FILENAME__ = find
"""
Find schema names that adhere to particular patterns
"""
import sys
import fnmatch
import string
import logging

LOGGER = logging.getLogger(__name__)

class MySQLFind(object):
    def __init__(self, client, **kwargs):
        self.client = client
        self.dbinclude = list(kwargs.get('dbinclude') or ['*'])
        self.dbexclude = list(kwargs.get('dbexclude') or [])
        self.dbexclude.extend(['information_schema', 'lost+found'])
        self.tblinclude = []
        for pat in list(kwargs.get('tblinclude') or []):
            if '.' not in pat:
                pat = '*.' + pat
            self.tblinclude.append(pat)
        self.tblexclude = []
        for pat in list(kwargs.get('tblexclude') or []):
            if '.' not in pat:
                pat = '*.' + pat
            self.tblexclude.append(pat)
            
    def is_filtered(self, name, include_patterns, exclude_patterns):
        """
        Check if a string is filtered
        """
        # if a db.tbl name does not match an include pattern - filter it
        for pat in map(string.lower, include_patterns or ['*']):
            if fnmatch.fnmatch(name.lower(), pat):
                break
        else:
            return True

        for pat in map(string.lower, exclude_patterns or []):
            if fnmatch.fnmatch(name.lower(), pat):
                return True
        return False

    def find_databases(self):
        """
        Find databases that match the given patterns
        """
        self.filtered = False
        result = []
        for name in self.client.show_databases():
            if not self.is_filtered(name, self.dbinclude, self.dbexclude):
                result.append(name)
            elif name not in ['information_schema','lost+found']:
                self.filtered = True
        return result

    def find_table_status(self):
        self.filtered = False
        result = []
        for status in self.client.walk_tables(dbinclude=self.find_databases()):
            db = status['db']
            if self.is_filtered(db, self.dbinclude, self.dbexclude):
                continue
            tbl = db + '.' + status['name']
            if self.is_filtered(tbl, self.tblinclude, self.tblexclude):
                continue
            yield status
            
    def find_tables(self):
        """
        Find tables that match the given patterns
        """
        self.filtered = False
        result = []
        for db in self.find_databases():
            for tbl in self.client.show_tables(db):
                name = db + '.' + tbl
                if not self.is_filtered(name, 
                                        self.tblinclude, 
                                        self.tblexclude):
                    result.append(name)
                else:
                    self.filtered = True
        return result

    def find_non_transactional(self):
        for tbl_status in self.find_table_status():
            engine = tbl_status['engine'] or tbl_status['comment']

            if self.client.is_transactional(engine):
                continue
            yield tbl_status['db'] + '.' + tbl_status['name'], engine

    def find_excluded_tables(self):
        """
        Find tables that would be filtered by the 
        given patterns
        """
        result = []
        for db in self.find_databases():
            for tbl in self.client.show_tables(db):
                name = db + '.' + tbl
                if self.is_filtered(name, self.tblinclude, self.tblexclude):
                    result.append(name)
        return result

    def __repr__(self):
        return """DB Include: %s
        DB Exclude: %s
        TbL Include: %s
        Tbl Exclude: %s
        """ % (self.dbinclude, self.dbexclude, self.tblinclude, self.tblexclude)

########NEW FILE########
__FILENAME__ = base
"""MySQL option files support

http://dev.mysql.com/doc/refman/5.1/en/option-files.html
"""
import os, sys
import re
import errno
import codecs
import logging
import subprocess
from holland.lib.mysql.option.parser import OptionFile

LOG = logging.getLogger(__name__)

def merge_options(*defaults_files):
    """Merge multiple defaults files together"""
    defaults_config = dict(client={})
    def merge(dst_dict, src_dict):
        """Merge two dictionaries non-destructively"""
        for key, val in src_dict.items():
            if (key in dst_dict and isinstance(dst_dict[key], dict) and
                                isinstance(val, dict)):
                merge(dst_dict[key], val)
            else:
                dst_dict[key] = val

    for config in defaults_files:
        try:
            _my_config = load_options(config)
            merge(defaults_config, _my_config)
        except IOError:
            if not os.path.exists(config):
                LOG.warn("No such file or directory: '%s'", config)
            else:
                raise

    return { 'client' : defaults_config['client'] }

def load_options(path):
    """Load mysql option file from path"""
    options = OptionFile()
    options.read([path])
    return options

def quote(value):
    """Added quotes around a value"""

    return '"' + value.replace('"', '\\"') + '"'

def write_options(config, filename):
    if isinstance(filename, basestring):
        filename = codecs.open(filename, 'w', 'utf8')
    for section in config:
        print >>filename, "[%s]" % section
        for key in config[section]:
            value = unicode(config[section][key])
            print >>filename, "%s = %s" % (key, quote(value))
    filename.close()

def build_mysql_config(mysql_config):
    """Given a standard Holland [mysql:client] section build an in-memory
    config that represents the auth parameters, including those merged in from
    *defaults-extra-files*

    :param mysql_config: required.  This should be a dict object with the
                         zero or more of the following keys:
                           user (string)
                           password (string)
                           host (string)
                           socket (string)
                           port (integer)
                           defaults-extra-file (list)
    :type mysql_config: dict
    """
    defaults_config = merge_options(*mysql_config['defaults-extra-file'])

    if mysql_config.get('password') is not None:
        password = mysql_config['password']
        if password.startswith('file:'):
            password_file = password[len('file:'):]
            password = process_password_file(password_file)
            mysql_config['password'] = password
            LOG.info("Read password from file '%s'", password_file)

    for key in ('user', 'password', 'socket', 'host', 'port'):
        if key in mysql_config and mysql_config[key]:
            defaults_config['client'][key] = mysql_config[key]
    return defaults_config

def process_password_file(path):
    """Read the file at `path` and return the
    contents of the file

    :param path: file path to read a passwrod from
    :returns: password contained in `path`
    """
    try:
        # strip trailing whitespace
        password = open(path, 'r').read().rstrip()
        LOG.debug("Loaded password file %s", path)
        return password
    except IOError, exc:
        LOG.error("Failed to load password file %s: %s", path, str(exc))
        raise

########NEW FILE########
__FILENAME__ = legacy
"""
Support for parsing and writing my.cnf option
files
"""

import os
import re
import codecs
from types import StringTypes
try:
    from cStringIO import StringIO
except ImportError:
    from StringIO import StringIO
import logging

from holland.lib.multidict import MultiDict

LOGGER = logging.getLogger(__name__)

# map sections to lists of params that
# support multiple (repeated) entries
MULTI_KEY = {
    'mysqldump' : [
        'ignore-table',
    ],
    'mysqld' : [
        'replicate-do-db',
        'replicate-do-table',
        'replicate-ignore-db',
        'replicate-ignore-table',
        'replicate-rewrite-db',
        'replicate-wild-do-table',
        'replicate-wild-ignore-table',
    ]
}

class OptionFileParser(object):
    """
    Parser for my.cnf files

    The parse method on this object yields
    OptionFile instances populated with the
    groups and key/value pairs from the underlying
    my.cnf data.
    """


    def __init__(self):
        self.active_group = None

    def parse(self, obj):
        """
        Parse the given object and return an
        OptionFile instance

        If object is a string, this method will
        attempt to open a file of the same name
        and use that for input. Otherwise obj
        should be an iterable that returns lines
        of text representing a text file
        """
        optionobj = OptionFile()
        for line in obj:
            line = line.strip()
            # Skip blank lines
            if not line:
                continue
            # Skip comments
            elif line.startswith('#') or line.startswith(';'):
                continue
            # Groups are of the form [<name>]
            elif line.startswith('['):
                group_name = self._parse_group(line)
                if not group_name:
                    continue
                # Make sure the group exists
                if not group_name in optionobj:
                    optionobj[group_name] = MultiDict()
                # Set it as the active group
                self.active_group = group_name
            # Follow !include or !includedir directives
            elif line.startswith('!include'):
                for optf in self._parse_include(line):
                    optionobj.update(optf)
            # Anything else should be some sort of key[,value] pair
            # where value is optional
            else:
                result = self._parse_option_name(line)
                if not result:
                    continue
                if not self.active_group:
                    continue
                key, value = result
                optionobj[self.active_group].add(key, value)
        return optionobj

    def _parse_group(self, line):
        if not line.endswith(']'):
            return
        group_name = line[1:-1].strip()
        return group_name

    def _parse_include(self, line):
        directive, arg = line.split(None, 1)

        if directive == '!include':
            fileobj = open(arg, 'r')
            yield fileobj
        elif directive == '!includedir':
            if not os.path.isdir(arg):
                return
            for path in os.listdir(arg):
                path = os.path.join(arg, path)
                if not path.endswith('.cnf') and not path.endswith('.ini'):
                    continue
                if not os.path.isfile(path):
                    continue
                fileobj = open(path, 'r')
                yield self.parse(fileobj)

    def _unquote(self, value):
        if len(value) > 1 and value[0] == value[-1] and value[0] in ('"', "'"):
            value = value[1:-1]
        # support weird mysql conversions
        MYSQL_META = {
            'b' : "\b",
            't' : "\t",
            'n' : "\n",
            'r' : "\r",
            '\\': "\\",
            's' : " ",
            '"' : '"',
        }
        return re.sub(r'\\(["btnr\\s])',
                      lambda m: MYSQL_META[m.group(1)],
                      value)

    def _parse_option_name(self, line):
        if not self.active_group:
            return
        opt_parts = line.split('=', 1)
        # FIXME: Handle inline comments (value may be a quoted string with an embedded '#' in that case)
        if len(opt_parts) == 2:
            key, value = opt_parts
            value = self._unquote(value.strip())
        elif len(opt_parts) == 1:
            key, = opt_parts
            value = True
        key = key.strip().replace('_','-')
        return (key, value)

def canonicalize(optiondict):
    output_dict = MultiDict()
    for section in optiondict:
        if section not in output_dict:
            output_dict[section] = MultiDict()
        for key in optiondict[section]:
            if key in MULTI_KEY.get(section,[]):
                output_dict[section].add(key, optiondict[section][key])
            else:
                output_dict[section][key] = optiondict[section][key]
        output_dict[section].update(optiondict[section])
    return output_dict

class OptionFile(MultiDict):
    def add_section(self, name):
        if not name in self:
            self[name] = MultiDict()
            return True
        return False

    def __str__(self):
        result = StringIO()
        result = codecs.getwriter('utf8')(result)
        output_dict = MultiDict(self)
        for section in self:
            print >>result, "[%s]" % section
            sdict = MultiDict(output_dict.pop(section))
            for key in self[section]:
                if key in MULTI_KEY.get(section,[]):
                    val = sdict.pop(key)
                    print >>result, key,'=',val
                elif key in sdict:
                    value = sdict.getall(key)[-1]
                    del sdict[key]
                    if value is True:
                        print >>result, key
                    else:
                        value = value.replace('"', r'\"')
                        print >>result, key,'=','"%s"' % value
        return result.getvalue()

    def write(self, filename=None):
        if not filename:
            fd, filename = tempfile.mkstemp()
            os.close(fd)
        fileobj = open(filename, 'w')
        print >>fileobj, str(self)
        fileobj.close()
        return filename

def _scrub_cnf(optionfile):
    for key in optionfile:
        if key != 'client':
            LOGGER.debug("Dropping section %s", key)
            del optionfile[key]
    for key in optionfile.get('client', []):
        if key not in ['user','password','host','socket', 'port']:
            LOGGER.debug("Dropping %s from client section", key)
            del optionfile['client'][key]

def make_mycnf(*args, **kwargs):
    """
    Generate a mycnf from the input arguments

    If output is not specified, a temporary file will be created
    and removed on program termination. Each arg should be one of
    a string type, a dictionary or a fileobj.  Each arg will be
    processed in order, with earlier args having their my.cnf
    values overwritten/merged with later args
    """
    base_optionobj = OptionFile()
    parser = OptionFileParser()
    for input in args:
        if isinstance(input, StringTypes):
            try:
                optionobj = parser.parse(open(input, 'r'))
                for key in optionobj:
                    if key in base_optionobj:
                        base_optionobj[key].update(optionobj[key])
                    else:
                        base_optionobj[key] = MultiDict(optionobj[key])
            except IOError, e:
                LOGGER.debug("Failed to parse mysql config %r: %s", input, e)
        else:
            optionobj = parser.parse(input)
            for key in optionobj:
                if key in base_optionobj:
                    base_optionobj[key].update(optionobj[key])
                else:
                    base_optionobj[key] = MultiDict(optionobj[key])

    for key, value in kwargs.items():
        # remove any empty/unset values
        map(value.pop, [opt for opt, val in value.items() if val is None])
        # merge the passed in options - these take precedence
        if key not in base_optionobj:
            base_optionobj.setdefault(key, MultiDict())
        base_optionobj[key].update(value)
    # scrub the final product - we only support connection options and a 'client' section
    _scrub_cnf(base_optionobj)
    return base_optionobj

########NEW FILE########
__FILENAME__ = parser
"""MySQL option file support"""

import os
import re
import glob
import codecs

def expandpath(path):
    """Expand a path to an absolute path

    This will convert a relative path to an absolute path and also expand any
    user directories such as ~/ to the user's homedir.
    """
    return os.path.abspath(os.path.expanduser(path))

def remove_inline_comment(value):
    """Remove a MySQL inline comment from an option file line"""
    escaped = False
    quote = None
    for idx, char in enumerate(value):
        if char in ('"', "'") and not escaped:
            if not quote:
                quote = char
            elif quote == char:
                quote = None
        if not quote and char == '#':
            return value[0:idx]
        escaped = (quote and char == '\\' and not escaped)
    return value

def unquote_option_value(value):
    """Remove quotes from a string."""
    if len(value) > 1 and value[0] in ('"', "'") and value[0] == value[-1]:
        return value[1:-1]
    return value

def unescape_option_value(value):
    """Unescape an option value per MySQL supported escape sequences

    See: http://dev.mysql.com/doc/refman/5.0/en/option-files.html
    """
    meta_mapping = {
        'b' : "\b",
        't' : "\t",
        'n' : "\n",
        'r' : "\r",
        '\\': "\\",
        's' : " ",
        '"' : '"',
    }

    return re.sub(r'\\(["btnr\\s])',
                  lambda m: meta_mapping[m.group(1)],
                  value)


def unpack_option_value(value):
    """Process an option value according to MySQL's syntax rules"""
    value = remove_inline_comment(value)
    value = value.strip()
    value = unquote_option_value(value)
    value = unescape_option_value(value)
    return value

def resolve_option(item):
    """Expand an option prefix to the full name of the option"""
    known = [
        u'host',
        u'password',
        u'port',
        u'socket',
        u'user',
    ]
    candidates = [key for key in known if key.startswith(item)]

    if len(candidates) > 1:
        # mimic MySQL's error message
        raise ParseError("ambiguous option '%s' (%s)" %
                         (item, ','.join(candidates)))
    elif not candidates:
        return item

    return candidates[0]

def find_includes(include_directive):
    """Find includes for the given !include* directive"""
    directive, path = include_directive.split(None, 1)
    if directive == '!includedir':
        return glob.glob(os.path.join(path, '*.cnf')) + \
               glob.glob(os.path.join(path, '*.ini'))
    elif directive == '!include':
        return path
    raise ParseError("Invalid include directive %s" % include_directive)

class ParseError(Exception):
    "Exception raised when parsing an option file"


class OptionFile(dict):
    """Represent a MySQL option file"""

    KV_CRE = re.compile(r'(?P<key>[^=\s]+?)\s*(?:=\s*(?P<value>.*))?$')

    def read_options(self, iterable):
        """Parse lines from the data source specified by ``iterable``
        """
        section = None

        path = getattr(iterable, 'name', '<unknown>')

        for lineno, line in enumerate(iterable):
            line = line.strip()
            if line.startswith('!include'):
                paths = find_includes(line)
                self.process_includes(paths)
            elif not line:
                continue
            elif line.startswith('#') or line.startswith(';'):
                continue
            elif line.startswith("["):
                section = remove_inline_comment(line).strip()
                if section.startswith('[') and section.endswith(']'):
                    section = section[1:-1].lower()
                    self.setdefault(section, dict())
                else:
                    raise ParseError("Wrong group definition in config file: "
                                     "%s at line %d" % (path, lineno+1))
            else:
                key_value = self.parse_key_value(line)
                if key_value:
                    key, value = key_value
                    self[section][key] = value
                else:
                    raise ParseError(line, "%s:%s" % (path, lineno+1))

    def read(self, filenames):
        """Read and parse a list of option file paths

        :returns: list of paths successfully processed
        """
        processed = []
        for path in filenames:
            try:
                fileobj = codecs.open(expandpath(path), 'r', encoding='utf8')
            except IOError:
                continue
            try:
                self.read_options(fileobj)
                processed.append(path)
            finally:
                fileobj.close()
        return processed

    def parse_key_value(self, line):
        """Process a key/value directive according to MySQL syntax rules

        :returns: tuple if line is a valid key/value pair otherwise returns None
                  If this is a bare option such as 'no-auto-rehash' the value
                  element of the key/value tuple will be None
        """
        match = self.KV_CRE.match(line)
        if match:
            key, value = match.group('key', 'value')
            if value:
                value = unpack_option_value(value)
            else:
                key = remove_inline_comment(key)
            key = resolve_option(key)
            return key, value
        return None

    def process_includes(self, paths):
        """Call ``read_options()`` for every valid file in paths

        :returns: list of invalid paths that were skipped
        """
        skipped = []
        for path in paths:
            try:
                fileobj = codecs.open(expandpath(path), 'r', encoding='utf8')
            except IOError:
                skipped.append(path)
                continue
            try:
                # python2.3 does not support try/except/finally
                try:
                    self.read_options(fileobj)
                except IOError:
                    continue
            finally:
                fileobj.close()
        return skipped

########NEW FILE########
__FILENAME__ = base
"""Summarize a MySQL Schema"""

import time
import logging
from holland.lib.mysql.client import MySQLError

LOG = logging.getLogger(__name__)

class MySQLSchema(object):
    """A catalog summary of a MySQL Instance"""

    def __init__(self):
        self.databases = []
        self._database_filters = []
        self._table_filters = []
        self._engine_filters = []
        self.timestamp = None

    def excluded_tables(self):
        """Iterate over tables excluded in this schema"""
        for database in self.databases:
            for table in database:
                if table.excluded:
                    yield table
    excluded_tables = property(excluded_tables)

    def excluded_databases(self):
        """Iterate over databases excluded in this schema"""
        for database in self.databases:
            if database.excluded:
                yield database
    excluded_databases = property(excluded_databases)

    def add_database_filter(self, filterobj):
        """Add a database filter to this summary

        :param filterobj: a callable that returns True if a database
                          should be filtered by name
        :type filterobj: callable, such as `IncludeFilter` or `ExcludeFilter`
        """
        self._database_filters.append(filterobj)

    def add_table_filter(self, filterobj):
        """Add a table filter to this summary

        :param filterobj: a callable that returns True if a table
                          should be filtered by name
        :type filterobj: callable, such as `IncludeFilter` or `ExcludeFilter`
        """
        self._table_filters.append(filterobj)

    def add_engine_filter(self, filterobj):
        """Add an engine filter to this summary

        :param filterobj: a callable that returns True if a table
                          should be filtered by name
        :type filterobj: callable, such as `IncludeFilter` or `ExcludeFilter`
        """
        self._engine_filters.append(filterobj)

    def is_db_filtered(self, name):
        """Check if the database name is filtered by any database filters

        :param name: database name that should be checked against the list of
                     registered database filters.
        :type name: str
        :returns: True if the database named by `name` should be filtered
        """
        for _filter in self._database_filters:
            if _filter(name):
                return True

    def is_table_filtered(self, name):
        """Check if the table name is filtered by any table filters

        :param name: table name that should be checked against the list of
                     registered table filters.
        :type name: str
        :returns: True if the database named by `name` should be filtered
        """
        for _filter in self._table_filters:
            if _filter(name):
                return True

    def is_engine_filtered(self, name):
        """Check if the engine name is filtered by any engine filters

        :param name: engine name that should be checked against the list of
                     registered engine filters.
        :type name: str
        :returns: True if the table with the storage engine named by `name`
                  should be filtered
        """
        for _filter in self._engine_filters:
            if _filter(name):
                return True

    def refresh(self, db_iter, tbl_iter, fast_iterate=False):
        """Summarize the schema by walking over the given database and table
        iterators

        :param db_iter: Required. A `DatabaseIterator` instance that will
                        provide an iterator instance when called with no
                        arguments. This iterator must yield `Database`
                        instances.
        :param tbl_iter: Required. A `TableIterator` instance that will return
                         provide an iterator instance when called with a
                         database name. This iterator must yield `Table`
                         instances from the requested database.

        :param fast_iterate: Optional. Skips table iteration when there are no 
                             useful filters - include pattern = *, 
                             exclude pattern = ''
        """
        for database in db_iter():
            self.databases.append(database)
            if self.is_db_filtered(database.name):
                database.excluded = True
                continue

            # skip iterating over tables when:
            # 1) we are matching all tables (using default pattern)
            # 2) we are matching all engines (using default pattern)
            # 3) caller does not require table iteration
            if fast_iterate and (len(self._table_filters) == 2 and
                self._table_filters[0].patterns == ['.*\\..*$'] and
                self._table_filters[1].patterns == []) and \
                (len(self._engine_filters) == 2 and
                self._engine_filters[0].patterns == ['.*$'] and
                self._engine_filters[1].patterns == []):
                    # optimize case where we have no table level filters
                    continue

            try:
                for table in tbl_iter(database.name):
                    if self.is_table_filtered(table.database + '.' + table.name):
                        table.excluded = True
                    if self.is_engine_filtered(table.engine):
                        table.excluded = True
                    database.add_table(table)
            except MySQLError, exc:
                # mimic mysqldump behavior here and skip any databases that 
                # are not readable
                if exc.args[0] == 1018:
                    continue
                raise
        self.timestamp = time.time()


class Database(object):
    """Representation of a MySQL Database

    Only the name an whether this database is
    excluded is recorded"""

    __slots__ = ('name', 'excluded', 'tables')

    def __init__(self, name):
        self.name = name
        self.tables  = []
        self.excluded = False

    def add_table(self, tableobj):
        """Add the table object to this database

        :param tableobj: `Table` instance that should be added to this
                         `Database` instance
        """
        self.tables.append(tableobj)

    def excluded_tables(self):
        """List tables associated with this database that are flagged as
        excluded"""
        for tableobj in self.tables:
            if tableobj.excluded:
                yield tableobj

    def is_transactional(self):
        """Check if this database is safe to dump in --single-transaction
        mode
        """
        for tableobj in self.tables:
            if not tableobj.is_transactional:
                return False

    def size(self):
        """Size of all non-excluded objects in this database

        :returns: int. sum of all data and indexes of tables that are not
                  excluded from this database
        """
        return sum([table.size for table in self.tables
                    if not table.excluded
                        and table.engine not in ('mrg_myisam', 'federated') ])
    size = property(size)

    def __str__(self):
        return "Database(name=%r, table_count=%d, excluded=%r)" % \
                (self.name, len(self.tables), self.excluded)

    __repr__ = __str__

class Table(object):
    """Representation of a MySQL Table

    """
    __slots__ = ('database',
                 'name',
                 'data_size',
                 'index_size',
                 'engine',
                 'is_transactional',
                 'excluded',
                )

    def __init__(self, database,
                       name,
                       data_size,
                       index_size,
                       engine,
                       is_transactional):
        self.database = database
        self.name = name
        self.data_size = data_size
        self.index_size = index_size
        self.engine = engine
        self.is_transactional = is_transactional
        self.excluded = False

    def size(self):
        return self.data_size + self.index_size
    size = property(size)

    def __str__(self):
        return "%sTable(name=%r, data_size=%s, " + \
               "index_size=%s, engine=%s, txn=%s)" % \
                (self.excluded and "[EXCL]" or "",
                 self.name,
                 "%.2fMB" % (self.data_size / 1024.0**2),
                 "%.2fMB" % (self.index_size / 1024.0**2),
                 self.engine,
                 str(self.is_transactional)
                )

class DatabaseIterator(object):
    """Iterate over databases returns by a MySQLClient instance

    client must have a show_databases() method
    """
    STD_EXCLUSIONS = (
        'information_schema',
        'performance_schema',
        'lost+found',
    )
    def __init__(self, client):
        """Construct a new iterator to produce `Database` instances for the
        database requested by the __call__ method.

        :param client: `MySQLClient` instance to use to iterate over objects in
        the specified databasea
        """
        self.client = client

    def __call__(self):
        for name in self.client.show_databases():
            if name not in self.STD_EXCLUSIONS:
                yield Database(name)


class TableIterator(object):
    """Iterate over tables returned by the client instance

    client must have a show_table_metadata(database_name) method
    """
    def __init__(self, client):
        """Construct a new iterator to produce `Table` instances for the
        database requested by the __call__ method.

        :param client: `MySQLClient` instance to use to iterate over objects in
        the specified database
        """
        self.client = client

    def __call__(self, database):
        raise NotImplementedError()

class MetadataTableIterator(TableIterator):
    """Iterate over SHOW TABLE STATUS in the requested database
    and yield Table instances
    """

    def __call__(self, database):
        for metadata in self.client.show_table_metadata(database):
            yield Table(**metadata)

import re

class SimpleTableIterator(MetadataTableIterator):
    """Iterator over tables returns by the client instance

    Unlike a MetadataTableIterator, this will not lookup the table size
    but rather just uses SHOW DATABASES/SHOW TABLES/SHOW CREATE TABLE

    SHOW CREATE TABLE is only used for engine lookup in MySQL 5.0.
    """
    
    ENGINE_PCRE = re.compile(r'^[)].*ENGINE=(\S+)', re.M)

    def __init__(self, client, record_engines=False):
        """Construct a new iterator to produce `Table` instances for the
        database requested by the __call__ method.

        :param client: `MySQLClient` instance to use to iterate over objects in
        the specified database
        """
        self.client = client
        self.record_engines = record_engines

    def _faster_mysql51_metadata(self, database):
        sql = ("SELECT TABLE_SCHEMA AS `database`, "
               "          TABLE_NAME AS `name`, "
               "          0 AS `data_size`, "
               "          0 AS `index_size`, "
               "          COALESCE(ENGINE, 'view') AS `engine`, "
               "          TRANSACTIONS = 'YES' AS `is_transactional` "
               "FROM INFORMATION_SCHEMA.TABLES "
               "JOIN INFORMATION_SCHEMA.ENGINES USING (ENGINE) "
               "WHERE TABLE_SCHEMA = %s")
        cursor = self.client.cursor()
        try:
            cursor.execute(sql, database)
            return cursor.fetchall()
        finally:
            cursor.close()

    def _lookup_engine(self, database, table):
        ddl = self.client.show_create_table(database, table)
        match = self.ENGINE_PCRE.search(ddl)
        if match:
            return match.group(1)
        raise ValueError("Failed to lookup storage engine")

    def __call__(self, database):
        if self.client.server_version >= (5,1):
            for metadata in self._faster_mysql51_metadata():
                yield Table(**metadata)
        else:
            for table, kind in self.client.show_tables(database, full=True):
                metadata = [
                    ('database', database),
                    ('name', table),
                    ('data_size', 0),
                    ('index_size', 0),
                ]

                if kind == 'VIEW':
                    metadata.append(('engine', 'view'))
                    metadata.append(('is_transactional', 'yes'))
                else:
                    if self.record_engines:
                        engine = self._lookup_engine(database, table).lower()
                        metadata.append(('engine', engine))
                        metadata.append(('is_transactional', engine == 'innodb'))
                    else:
                        metadata.append(('engine', ''))
                        metadata.append(('is_transactional', False))
                yield Table(**dict(metadata))

########NEW FILE########
__FILENAME__ = filter
"""Simple Filter support"""

import re
import fnmatch

class BaseFilter(object):
    """Filter a string based on a list of regular expression or glob patterns.

    This should be inherited and the __call__ overriden with a real
    implementation
    """
    __slots__ = ('patterns', '_re_options')

    def __init__(self, patterns, case_insensitive=True):
        self.patterns = list(patterns)
        if case_insensitive:
            self._re_options = re.M|re.U|re.I
        else:
            self._re_options = re.M|re.U

    def add_glob(self, glob):
        """Add a glob pattern to this filter

        Internally all globs are converted to regular expressions using
        `fnmatch.translate()`

        :param glob: glob pattern to add
        :type glob: str
        """
        self.patterns.append(fnmatch.translate(glob))

    def add_regex(self, regex):
        """Add a regular expression pattern to this filter

        :param regex: regular expression pattern to add to this filter.
        :type regex: str
        """
        self.patterns.append(regex)

    def __call__(self, item):
        """Run this filter - return True if filtered and False otherwise.

        :param item: item to check against this filter
        :type item: str
        """
        raise NotImplementedError()

    def __repr__(self):
        return self.__class__.__name__ + '(patterns=%r)' % self.patterns

class IncludeFilter(BaseFilter):
    """Include only objects that match *all* assigned filters"""

    def __call__(self, item):
        for _pattern in self.patterns:
            if re.match(_pattern, item, self._re_options) is not None:
                return False
        else:
            return True

class ExcludeFilter(BaseFilter):
    """Exclude objects that match any filter"""

    def __call__(self, item):
        for _pattern in self.patterns:
            if re.match(_pattern, item, self._re_options) is not None:
                return True
        else:
            return False

def exclude_glob(*pattern):
    """Create an exclusion filter from a glob pattern"""
    result = []
    for pat in pattern:
        result.append(fnmatch.translate(pat))
    return ExcludeFilter(result)

def include_glob(*pattern):
    """Create an inclusion filter from glob patterns"""
    result = []
    for pat in pattern:
        result.append(fnmatch.translate(pat))
    return IncludeFilter(result)

def include_glob_qualified(*pattern):
    """Create an inclusion filter from glob patterns

    Additionally ensure the pattern is for a qualified table name.
    If not '.' is found in the name, this implies an implicit *. 
    before the name
    """
    result = []
    for pat in pattern:
        if '.' not in pat:
            pat = '*.' + pat
        result.append(pat)
    return include_glob(*result)

def exclude_glob_qualified(*pattern):
    """Create an exclusion filter from glob patterns

    Additionally ensure the pattern is for a qualified table name.
    If not '.' is found in the name, this implies an implicit *.
    before the name
    """
    result = []
    for pat in pattern:
        if '.' not in pat:
            pat = '*.' + pat
        result.append(pat)
    return exclude_glob(*result) 

########NEW FILE########
__FILENAME__ = std_table_status
# SHOW TABLE STATUS generated from 5.1.32
import datetime

STATUS_DATABASES = [
    'information_schema',
    'employees',
    'mysql',
    'test'
]

INFORMATION_SCHEMA_TABLE_STATUS = [  
    (   'CHARACTER_SETS',
        'MEMORY',
        10L,
        'Fixed',
        None,
        384L,
        0L,
        16604160L,
        0L,
        0L,
        None,
        None,
        None,
        None,
        'utf8_general_ci',
        None,
        'max_rows=43690',
        ''),
    (   'COLLATIONS',
        'MEMORY',
        10L,
        'Fixed',
        None,
        231L,
        0L,
        16704765L,
        0L,
        0L,
        None,
        None,
        None,
        None,
        'utf8_general_ci',
        None,
        'max_rows=72628',
        ''),
    (   'COLLATION_CHARACTER_SET_APPLICABILITY',
        'MEMORY',
        10L,
        'Fixed',
        None,
        195L,
        0L,
        16691610L,
        0L,
        0L,
        None,
        None,
        None,
        None,
        'utf8_general_ci',
        None,
        'max_rows=86037',
        ''),
    (   'COLUMNS',
        'MyISAM',
        10L,
        'Dynamic',
        None,
        0L,
        0L,
        281474976710655L,
        1024L,
        0L,
        None,
        datetime.datetime(2009, 5, 4, 14, 44, 31),
        datetime.datetime(2009, 5, 4, 14, 44, 31),
        None,
        'utf8_general_ci',
        None,
        'max_rows=4560',
        ''),
    (   'COLUMN_PRIVILEGES',
        'MEMORY',
        10L,
        'Fixed',
        None,
        2565L,
        0L,
        16757145L,
        0L,
        0L,
        None,
        None,
        None,
        None,
        'utf8_general_ci',
        None,
        'max_rows=6540',
        ''),
    (   'ENGINES',
        'MEMORY',
        10L,
        'Fixed',
        None,
        490L,
        0L,
        16709000L,
        0L,
        0L,
        None,
        None,
        None,
        None,
        'utf8_general_ci',
        None,
        'max_rows=34239',
        ''),
    (   'EVENTS',
        'MyISAM',
        10L,
        'Dynamic',
        None,
        0L,
        0L,
        281474976710655L,
        1024L,
        0L,
        None,
        datetime.datetime(2009, 5, 4, 14, 44, 31),
        datetime.datetime(2009, 5, 4, 14, 44, 31),
        None,
        'utf8_general_ci',
        None,
        'max_rows=618',
        ''),
    (   'FILES',
        'MEMORY',
        10L,
        'Fixed',
        None,
        2677L,
        0L,
        16758020L,
        0L,
        0L,
        None,
        None,
        None,
        None,
        'utf8_general_ci',
        None,
        'max_rows=6267',
        ''),
    (   'GLOBAL_STATUS',
        'MEMORY',
        10L,
        'Fixed',
        None,
        3268L,
        0L,
        16755036L,
        0L,
        0L,
        None,
        None,
        None,
        None,
        'utf8_general_ci',
        None,
        'max_rows=5133',
        ''),
    (   'GLOBAL_VARIABLES',
        'MEMORY',
        10L,
        'Fixed',
        None,
        3268L,
        0L,
        16755036L,
        0L,
        0L,
        None,
        None,
        None,
        None,
        'utf8_general_ci',
        None,
        'max_rows=5133',
        ''),
    (   'KEY_COLUMN_USAGE',
        'MEMORY',
        10L,
        'Fixed',
        None,
        4637L,
        0L,
        16762755L,
        0L,
        0L,
        None,
        None,
        None,
        None,
        'utf8_general_ci',
        None,
        'max_rows=3618',
        ''),
    (   'PARTITIONS',
        'MyISAM',
        10L,
        'Dynamic',
        None,
        0L,
        0L,
        281474976710655L,
        1024L,
        0L,
        None,
        datetime.datetime(2009, 5, 4, 14, 44, 31),
        datetime.datetime(2009, 5, 4, 14, 44, 31),
        None,
        'utf8_general_ci',
        None,
        'max_rows=5612',
        ''),
    (   'PLUGINS',
        'MyISAM',
        10L,
        'Dynamic',
        None,
        0L,
        0L,
        281474976710655L,
        1024L,
        0L,
        None,
        datetime.datetime(2009, 5, 4, 14, 44, 31),
        datetime.datetime(2009, 5, 4, 14, 44, 31),
        None,
        'utf8_general_ci',
        None,
        'max_rows=13025',
        ''),
    (   'PROCESSLIST',
        'MyISAM',
        10L,
        'Dynamic',
        None,
        0L,
        0L,
        281474976710655L,
        1024L,
        0L,
        None,
        datetime.datetime(2009, 5, 4, 14, 44, 31),
        datetime.datetime(2009, 5, 4, 14, 44, 31),
        None,
        'utf8_general_ci',
        None,
        'max_rows=23763',
        ''),
    (   'PROFILING',
        'MEMORY',
        10L,
        'Fixed',
        None,
        308L,
        0L,
        16562084L,
        0L,
        0L,
        None,
        None,
        None,
        None,
        'utf8_general_ci',
        None,
        'max_rows=54471',
        ''),
    (   'REFERENTIAL_CONSTRAINTS',
        'MEMORY',
        10L,
        'Fixed',
        None,
        4814L,
        0L,
        16767162L,
        0L,
        0L,
        None,
        None,
        None,
        None,
        'utf8_general_ci',
        None,
        'max_rows=3485',
        ''),
    (   'ROUTINES',
        'MyISAM',
        10L,
        'Dynamic',
        None,
        0L,
        0L,
        281474976710655L,
        1024L,
        0L,
        None,
        datetime.datetime(2009, 5, 4, 14, 44, 31),
        datetime.datetime(2009, 5, 4, 14, 44, 31),
        None,
        'utf8_general_ci',
        None,
        'max_rows=588',
        ''),
    (   'SCHEMATA',
        'MEMORY',
        10L,
        'Fixed',
        None,
        3464L,
        0L,
        16755368L,
        0L,
        0L,
        None,
        None,
        None,
        None,
        'utf8_general_ci',
        None,
        'max_rows=4843',
        ''),
    (   'SCHEMA_PRIVILEGES',
        'MEMORY',
        10L,
        'Fixed',
        None,
        2179L,
        0L,
        16767405L,
        0L,
        0L,
        None,
        None,
        None,
        None,
        'utf8_general_ci',
        None,
        'max_rows=7699',
        ''),
    (   'SESSION_STATUS',
        'MEMORY',
        10L,
        'Fixed',
        None,
        3268L,
        0L,
        16755036L,
        0L,
        0L,
        None,
        None,
        None,
        None,
        'utf8_general_ci',
        None,
        'max_rows=5133',
        ''),
    (   'SESSION_VARIABLES',
        'MEMORY',
        10L,
        'Fixed',
        None,
        3268L,
        0L,
        16755036L,
        0L,
        0L,
        None,
        None,
        None,
        None,
        'utf8_general_ci',
        None,
        'max_rows=5133',
        ''),
    (   'STATISTICS',
        'MEMORY',
        10L,
        'Fixed',
        None,
        2679L,
        0L,
        16770540L,
        0L,
        0L,
        None,
        None,
        None,
        None,
        'utf8_general_ci',
        None,
        'max_rows=6262',
        ''),
    (   'TABLE_STATUS',
        'MEMORY',
        10L,
        'Fixed',
        None,
        3545L,
        0L,
        16760760L,
        0L,
        0L,
        None,
        None,
        None,
        None,
        'utf8_general_ci',
        None,
        'max_rows=4732',
        ''),
    (   'TABLE_CONSTRAINTS',
        'MEMORY',
        10L,
        'Fixed',
        None,
        2504L,
        0L,
        16749256L,
        0L,
        0L,
        None,
        None,
        None,
        None,
        'utf8_general_ci',
        None,
        'max_rows=6700',
        ''),
    (   'TABLE_PRIVILEGES',
        'MEMORY',
        10L,
        'Fixed',
        None,
        2372L,
        0L,
        16748692L,
        0L,
        0L,
        None,
        None,
        None,
        None,
        'utf8_general_ci',
        None,
        'max_rows=7073',
        ''),
    (   'TRIGGERS',
        'MyISAM',
        10L,
        'Dynamic',
        None,
        0L,
        0L,
        281474976710655L,
        1024L,
        0L,
        None,
        datetime.datetime(2009, 5, 4, 14, 44, 31),
        datetime.datetime(2009, 5, 4, 14, 44, 31),
        None,
        'utf8_general_ci',
        None,
        'max_rows=569',
        ''),
    (   'USER_PRIVILEGES',
        'MEMORY',
        10L,
        'Fixed',
        None,
        1986L,
        0L,
        16759854L,
        0L,
        0L,
        None,
        None,
        None,
        None,
        'utf8_general_ci',
        None,
        'max_rows=8447',
        ''),
    (   'VIEWS',
        'MyISAM',
        10L,
        'Dynamic',
        None,
        0L,
        0L,
        281474976710655L,
        1024L,
        0L,
        None,
        datetime.datetime(2009, 5, 4, 14, 44, 31),
        datetime.datetime(2009, 5, 4, 14, 44, 31),
        None,
        'utf8_general_ci',
        None,
        'max_rows=6932',
        '')
]

EMPLOYEES_TABLE_STATUS = [   
    (   'departments',
        'InnoDB',
        10L,
        'Compact',
        9L,
        1820L,
        16384L,
        0L,
        16384L,
        38797312L,
        None,
        datetime.datetime(2009, 5, 4, 0, 34, 27),
        None,
        None,
        'latin1_swedish_ci',
        None,
        '',
        ''),
    (   'dept_emp',
        'InnoDB',
        10L,
        'Compact',
        332289L,
        36L,
        12075008L,
        0L,
        10518528L,
        38797312L,
        None,
        datetime.datetime(2009, 5, 4, 0, 34, 27),
        None,
        None,
        'latin1_swedish_ci',
        None,
        '',
        ''),
    (   'dept_manager',
        'InnoDB',
        10L,
        'Compact',
        24L,
        682L,
        16384L,
        0L,
        32768L,
        38797312L,
        None,
        datetime.datetime(2009, 5, 4, 0, 34, 27),
        None,
        None,
        'latin1_swedish_ci',
        None,
        '',
        ''),
    (   'employees',
        'InnoDB',
        10L,
        'Compact',
        299366L,
        50L,
        15220736L,
        0L,
        0L,
        38797312L,
        None,
        datetime.datetime(2009, 5, 4, 0, 34, 27),
        None,
        None,
        'latin1_swedish_ci',
        None,
        '',
        ''),
    (   'salaries',
        'InnoDB',
        10L,
        'Compact',
        2844513L,
        35L,
        100270080L,
        0L,
        36241408L,
        38797312L,
        None,
        datetime.datetime(2009, 5, 4, 0, 34, 27),
        None,
        None,
        'latin1_swedish_ci',
        None,
        '',
        ''),
    (   'titles',
        'InnoDB',
        10L,
        'Compact',
        443951L,
        46L,
        20512768L,
        0L,
        11059200L,
        38797312L,
        None,
        datetime.datetime(2009, 5, 4, 0, 34, 27),
        None,
        None,
        'latin1_swedish_ci',
        None,
        '',
        '')
]

MYSQL_TABLE_STATUS = [   
    (   'columns_priv',
        'MyISAM',
        10L,
        'Fixed',
        0L,
        0L,
        0L,
        227994731135631359L,
        4096L,
        0L,
        None,
        datetime.datetime(2009, 3, 24, 18, 3, 33),
        datetime.datetime(2009, 3, 24, 18, 3, 33),
        None,
        'utf8_bin',
        None,
        '',
        'Column privileges'),
    (   'db',
        'MyISAM',
        10L,
        'Fixed',
        0L,
        0L,
        880L,
        123848989752688639L,
        5120L,
        880L,
        None,
        datetime.datetime(2009, 3, 24, 18, 3, 32),
        datetime.datetime(2009, 3, 24, 18, 4, 7),
        datetime.datetime(2009, 3, 24, 18, 3, 35),
        'utf8_bin',
        None,
        '',
        'Database privileges'),
    (   'event',
        'MyISAM',
        10L,
        'Dynamic',
        0L,
        0L,
        0L,
        281474976710655L,
        2048L,
        0L,
        None,
        datetime.datetime(2009, 3, 24, 18, 3, 35),
        datetime.datetime(2009, 3, 24, 18, 3, 35),
        None,
        'utf8_general_ci',
        None,
        '',
        'Events'),
    (   'func',
        'MyISAM',
        10L,
        'Fixed',
        0L,
        0L,
        0L,
        162974011515469823L,
        1024L,
        0L,
        None,
        datetime.datetime(2009, 3, 24, 18, 3, 33),
        datetime.datetime(2009, 3, 24, 18, 3, 33),
        None,
        'utf8_bin',
        None,
        '',
        'User defined functions'),
    (   'general_log',
        'CSV',
        10L,
        'Dynamic',
        2L,
        0L,
        0L,
        0L,
        0L,
        0L,
        None,
        None,
        None,
        None,
        'utf8_general_ci',
        None,
        '',
        'General log'),
    (   'help_category',
        'MyISAM',
        10L,
        'Fixed',
        37L,
        581L,
        21497L,
        163536961468891135L,
        3072L,
        0L,
        None,
        datetime.datetime(2009, 3, 24, 18, 3, 33),
        datetime.datetime(2009, 3, 24, 18, 3, 35),
        None,
        'utf8_general_ci',
        None,
        '',
        'help categories'),
    (   'help_keyword',
        'MyISAM',
        10L,
        'Fixed',
        450L,
        197L,
        88650L,
        55450570411999231L,
        16384L,
        0L,
        None,
        datetime.datetime(2009, 3, 24, 18, 3, 33),
        datetime.datetime(2009, 3, 24, 18, 3, 35),
        None,
        'utf8_general_ci',
        None,
        '',
        'help keywords'),
    (   'help_relation',
        'MyISAM',
        10L,
        'Fixed',
        986L,
        9L,
        8874L,
        2533274790395903L,
        16384L,
        0L,
        None,
        datetime.datetime(2009, 3, 24, 18, 3, 33),
        datetime.datetime(2009, 3, 24, 18, 3, 35),
        None,
        'utf8_general_ci',
        None,
        '',
        'keyword-topic relation'),
    (   'help_topic',
        'MyISAM',
        10L,
        'Dynamic',
        503L,
        817L,
        411064L,
        281474976710655L,
        20480L,
        0L,
        None,
        datetime.datetime(2009, 3, 24, 18, 3, 33),
        datetime.datetime(2009, 3, 24, 18, 3, 35),
        None,
        'utf8_general_ci',
        None,
        '',
        'help topics'),
    (   'host',
        'MyISAM',
        10L,
        'Fixed',
        0L,
        0L,
        0L,
        110056715893866495L,
        2048L,
        0L,
        None,
        datetime.datetime(2009, 3, 24, 18, 3, 32),
        datetime.datetime(2009, 3, 24, 18, 3, 32),
        None,
        'utf8_bin',
        None,
        '',
        'Host privileges;  Merged with database privileges'),
    (   'ndb_binlog_index',
        'MyISAM',
        10L,
        'Dynamic',
        0L,
        0L,
        0L,
        281474976710655L,
        1024L,
        0L,
        None,
        datetime.datetime(2009, 3, 24, 18, 3, 35),
        datetime.datetime(2009, 3, 24, 18, 3, 35),
        None,
        'latin1_swedish_ci',
        None,
        '',
        ''),
    (   'plugin',
        'MyISAM',
        10L,
        'Fixed',
        0L,
        0L,
        0L,
        162411061562048511L,
        1024L,
        0L,
        None,
        datetime.datetime(2009, 3, 24, 18, 3, 33),
        datetime.datetime(2009, 3, 24, 18, 3, 33),
        None,
        'utf8_bin',
        None,
        '',
        'MySQL plugins'),
    (   'proc',
        'MyISAM',
        10L,
        'Dynamic',
        0L,
        0L,
        0L,
        281474976710655L,
        2048L,
        0L,
        None,
        datetime.datetime(2009, 3, 24, 18, 3, 34),
        datetime.datetime(2009, 3, 24, 18, 3, 34),
        None,
        'utf8_general_ci',
        None,
        '',
        'Stored Procedures'),
    (   'procs_priv',
        'MyISAM',
        10L,
        'Fixed',
        0L,
        0L,
        0L,
        239253730204057599L,
        4096L,
        0L,
        None,
        datetime.datetime(2009, 3, 24, 18, 3, 34),
        datetime.datetime(2009, 3, 24, 18, 3, 34),
        None,
        'utf8_bin',
        None,
        '',
        'Procedure privileges'),
    (   'servers',
        'MyISAM',
        10L,
        'Fixed',
        0L,
        0L,
        0L,
        433752939111120895L,
        1024L,
        0L,
        None,
        datetime.datetime(2009, 3, 24, 18, 3, 33),
        datetime.datetime(2009, 3, 24, 18, 3, 33),
        None,
        'utf8_general_ci',
        None,
        '',
        'MySQL Foreign Servers table'),
    (   'slow_log',
        'CSV',
        10L,
        'Dynamic',
        2L,
        0L,
        0L,
        0L,
        0L,
        0L,
        None,
        None,
        None,
        None,
        'utf8_general_ci',
        None,
        '',
        'Slow log'),
    (   'tables_priv',
        'MyISAM',
        10L,
        'Fixed',
        0L,
        0L,
        0L,
        239535205180768255L,
        4096L,
        0L,
        None,
        datetime.datetime(2009, 3, 24, 18, 3, 33),
        datetime.datetime(2009, 3, 24, 18, 3, 33),
        None,
        'utf8_bin',
        None,
        '',
        'Table privileges'),
    (   'time_zone',
        'MyISAM',
        10L,
        'Fixed',
        0L,
        0L,
        0L,
        1970324836974591L,
        1024L,
        0L,
        1L,
        datetime.datetime(2009, 3, 24, 18, 3, 34),
        datetime.datetime(2009, 3, 24, 18, 3, 34),
        None,
        'utf8_general_ci',
        None,
        '',
        'Time zones'),
    (   'time_zone_leap_second',
        'MyISAM',
        10L,
        'Fixed',
        0L,
        0L,
        0L,
        3659174697238527L,
        1024L,
        0L,
        None,
        datetime.datetime(2009, 3, 24, 18, 3, 34),
        datetime.datetime(2009, 3, 24, 18, 3, 34),
        None,
        'utf8_general_ci',
        None,
        '',
        'Leap seconds information for time zones'),
    (   'time_zone_name',
        'MyISAM',
        10L,
        'Fixed',
        0L,
        0L,
        0L,
        55450570411999231L,
        1024L,
        0L,
        None,
        datetime.datetime(2009, 3, 24, 18, 3, 33),
        datetime.datetime(2009, 3, 24, 18, 3, 33),
        None,
        'utf8_general_ci',
        None,
        '',
        'Time zone names'),
    (   'time_zone_transition',
        'MyISAM',
        10L,
        'Fixed',
        0L,
        0L,
        0L,
        4785074604081151L,
        1024L,
        0L,
        None,
        datetime.datetime(2009, 3, 24, 18, 3, 34),
        datetime.datetime(2009, 3, 24, 18, 3, 34),
        None,
        'utf8_general_ci',
        None,
        '',
        'Time zone transitions'),
    (   'time_zone_transition_type',
        'MyISAM',
        10L,
        'Fixed',
        0L,
        0L,
        0L,
        10696049115004927L,
        1024L,
        0L,
        None,
        datetime.datetime(2009, 3, 24, 18, 3, 34),
        datetime.datetime(2009, 3, 24, 18, 3, 34),
        None,
        'utf8_general_ci',
        None,
        '',
        'Time zone transition types'),
    (   'user',
        'MyISAM',
        10L,
        'Dynamic',
        2L,
        96L,
        440L,
        281474976710655L,
        2048L,
        248L,
        None,
        datetime.datetime(2009, 3, 24, 18, 3, 32),
        datetime.datetime(2009, 3, 24, 18, 4, 7),
        None,
        'utf8_bin',
        None,
        '',
        'Users and global privileges')
]
TEST_TABLE_STATUS = []

########NEW FILE########
__FILENAME__ = test_cli
import shutil
from nose.tools import *
from tempfile import mkdtemp

from holland.core.exceptions import ArgumentError
from holland.lib.mysql.cli import MyCmdParser

global tmpdir

def setup_func():
    global tmpdir
    tmpdir = mkdtemp()

def teardown_func():
    global tmpdir
    shutil.rmtree(tmpdir)

@with_setup(setup_func, teardown_func)
def test_mysqlcmdparser():
    global tmpdir

    cli = MyCmdParser('mysqldump')
    ok_(isinstance(cli.cli_version, tuple))
    ok_(cli.cli_options)
    ok_(cli.cli_defaults)
    ok_(cli.has_key('opt'))
    ok_(cli.has_key('single-transaction'))

@raises(IOError)
@with_setup(setup_func, teardown_func)
def test_mysqlcmdparser_bad_command():
    global tmpdir

    cli = MyCmdParser('_doesnt_exist_mysqldumb')
    ok_(isinstance(cli.cli_version, tuple))

@with_setup(setup_func, teardown_func)
def test_parse_mysqld():
    global tmpdir

    cli = MyCmdParser('/usr/libexec/mysqld')
    ok_(isinstance(cli.cli_version, tuple))
    ok_(cli.cli_options)
    ok_(cli.cli_defaults)
    ok_(cli.has_key('log-bin'))
    ok_(cli.has_key('binlog-do-db'))

########NEW FILE########
__FILENAME__ = test_client
"""
Test behavior of MySQLClient class
"""

__test__ = True

from MySQLdb import OperationalError, ProgrammingError
from tempfile import mkdtemp
import shutil
from nose.tools import *
from holland.lib.mysql.client.legacy import MySQLClient

global tmpdir, client

def setup_func():
    global tmpdir, client
    client = MySQLClient(read_default_group='client')
    tmpdir = mkdtemp()

def teardown_func():
    global tmpdir, client
    shutil.rmtree(tmpdir)

@with_setup(setup_func, teardown_func)
def test_quoting():
    global tmpdir, client
    names = ['test','bar`foo', 'column']
    eq_(client.quote_id(*names), "`test`.`bar``foo`.`column`")
    names = ["'test'","'bar`foo'", "'column'"]
    eq_(client.unquote_id(*names), ['test','bar`foo', 'column'])
    assert_not_equal(client.unquote_id(*names), names)
    strings = ['a', '\'b', 'c']
    eq_(client.quote(*strings), "'a','''b','c'")

@with_setup(setup_func, teardown_func)
def test_show_databases():
    global tmpdir, client
    res = client.show_databases()
    ok_('mysql' in res)

@with_setup(setup_func, teardown_func)
def test_show_tables():
    global tmpdir, client
    res = client.show_tables('mysql')
    # verify common mysql.XXX tables
    for table in ['user', 'db', 'columns_priv', 'host']:
        ok_(table in res)

@with_setup(setup_func, teardown_func)
def test_show_databases():
    global tmpdir, client
    res = client.show_table_status('mysql')
    for r in res:
        ok_(r.has_key('engine'))

@with_setup(setup_func, teardown_func)
def test_show_variables():
    global tmpdir, client
    res = client.show_variable('port')
    ok_(res == '3306')

@with_setup(setup_func, teardown_func)
def test_show_variable_bad():
    global tmpdir, client
    res = client.show_variable('portsafasfasdfasdffa')
    ok_(res == None)

@with_setup(setup_func, teardown_func)
def test_show_variables_like():
    global tmpdir, client
    res = client.show_variables_like('%version%')
    ok_(res.has_key('version'))
    ok_(res.has_key('protocol_version'))
    ok_(res.has_key('version_compile_machine'))

@with_setup(setup_func, teardown_func)
def test_set_variable():
    global tmpdir, client
    cur_max = int(client.show_variable('max_connections'))
    res = client.set_variable('max_connections', cur_max+10, session=False)
    new_max = client.show_variable('max_connections')
    ok_(cur_max+10 == int(new_max))

@raises(OperationalError)
@with_setup(setup_func, teardown_func)
def test_set_variable_wrong_scope():
    global tmpdir, client
    # max_connections is global variable...  setting session=True raises
    res = client.set_variable('max_connections', 100, session=True)

@raises(OperationalError)
@with_setup(setup_func, teardown_func)
def test_set_variable_bad_variable():
    global tmpdir, client
    res = client.set_variable('johnny_bad_var', 100, session=True)

@with_setup(setup_func, teardown_func)
def test_set_wait_timeout():
    global tmpdir, client
    cur_wait = int(client.show_variable('interactive_timeout'))
    res = client.set_variable('interactive_timeout', cur_wait+10, session=True)
    new_wait = client.show_variable('interactive_timeout', session_only=True)
    ok_(cur_wait+10 == int(new_wait))

@with_setup(setup_func, teardown_func)
def test_show_indexes():
    global tmpdir, client
    res = client.show_indexes('mysql', 'user')
    ok_(type(res) is dict)

@raises(ProgrammingError)
@with_setup(setup_func, teardown_func)
def test_show_indexes_bad_database():
    global tmpdir, client
    res = client.show_indexes('_bad_database_not_exist', 'user')
    ok_(type(res) is dict)

@with_setup(setup_func, teardown_func)
def test_flush_logs():
    global tmpdir, client
    res = client.flush_logs()

@with_setup(setup_func, teardown_func)
def test_flush_tables():
    global tmpdir, client
    # flush all
    client.flush_tables()
    # flush some
    client.flush_tables(table_list=['mysql.user', 'mysql.column_priv'])

@with_setup(setup_func, teardown_func)
def test_flush_tables_bad():
    global tmpdir, client
    # flush something that doesn't exist, doesn't raise anything
    client.flush_tables(table_list=['mysql_bad.user', 'mysql_bad.column_priv'])

@with_setup(setup_func, teardown_func)
def test_flush_tables_with_read_lock():
    global tmpdir, client
    client.flush_tables_with_read_lock()
    client.flush_tables_with_read_lock(extra_flush=True)

@with_setup(setup_func, teardown_func)
def test_lock_tables():
    global tmpdir, client
    client.lock_tables(table_list=['mysql.user', 'mysql.columns_priv'])

@with_setup(setup_func, teardown_func)
def test_unlock_tables():
    global tmpdir, client
    client.unlock_tables()

@with_setup(setup_func, teardown_func)
def test_walk_databases():
    global tmpdir, client
    for db in client.walk_databases():
        ok_(len(db) > 0)

@with_setup(setup_func, teardown_func)
def test_walk_tables():
    global tmpdir, client
    for table in client.walk_tables():
        ok_(type(table) == dict)

    for table in client.walk_tables(dbinclude=['mysql']):
        ok_(type(table) == dict)

@with_setup(setup_func, teardown_func)
def test_walk_tables_bad_db():
    global tmpdir, client

    for table in client.walk_tables(dbinclude=['Johnny_not_exist_mysql']):
        ok_(type(table) == dict)

@with_setup(setup_func, teardown_func)
def test_master_status():
    # FIXME: Need a master to be setup to get info on it
    global tmpdir, client
    res = client.show_master_status()

@with_setup(setup_func, teardown_func)
def test_slave_status():
    # FIXME: Need a slave to be setup to get info on it
    global tmpdir, client
    res = client.show_slave_status()

@with_setup(setup_func, teardown_func)
def test_is_slave_running():
    # FIXME: Need a slave to be setup
    global tmpdir, client
    res = client.is_slave_running()
    eq_(res, False)

@raises(OperationalError)
@with_setup(setup_func, teardown_func)
def test_start_slave():
    # FIXME: Need a slave to be setup
    global tmpdir, client
    res = client.start_slave()

@raises(OperationalError)
@with_setup(setup_func, teardown_func)
def test_stop_slave():
    # FIXME: Need a slave to be setup
    global tmpdir, client
    res = client.stop_slave()

@with_setup(setup_func, teardown_func)
def test_show_transactional_engines():
    global tmpdir, client
    res = client.show_transactional_engines()
    ok_('innodb' in res)

@with_setup(setup_func, teardown_func)
def test_server_version():
    global tmpdir, client
    res = client.server_version()
    ok_(type(int(res[0])) is int)
    ok_(type(int(res[1])) is int)
    ok_(type(int(res[2])) is int)

@with_setup(setup_func, teardown_func)
def test_is_transactional():
    global tmpdir, client
    res = client.is_transactional('innodb')
    eq_(res, True)
    res = client.is_transactional('myisam')
    eq_(res, False)
    res = client.is_transactional('johnny_engine_not_exist')
    eq_(res, False)

@with_setup(setup_func, teardown_func)
def test_encode_as_filename():
    global tmpdir, client
    if client.server_version() < (5, 1, 2):
        # This should throw an OperationalError
        try:
            res = client.encode_as_filename('latin')
            ok_(False)
        except OperationalError, e:
            ok_(True)
    else:
        res = client.encode_as_filename('latin')
        ok_(res)

@with_setup(setup_func, teardown_func)
def test_show_encoded_dbs():
    global tmpdir, client
    if client.server_version() < (5, 1, 2):
        # This should throw an OperationalError
        try:
            res = client.show_encoded_dbs()
            ok_(False)
        except OperationalError, e:
            ok_(True)
    else:
        res = client.show_encoded_dbs()
        ok_(type(res) == list)

########NEW FILE########
__FILENAME__ = test_option
import tempfile
import sys
from nose.tools import *
from holland.lib.mysql.option.legacy import OptionFile
from holland.lib.mysql.option.base import load_options, write_options

def test_load_options():
    fileobj = tempfile.NamedTemporaryFile()
    print >>fileobj, "[client]"
    print >>fileobj, "user = root"
    print >>fileobj, 'password = "foo"barbaz"'
    print >>fileobj, 'single-transaction=1'
    print >>fileobj
    print >>fileobj, '[mysqldump]'
    print >>fileobj, 'master-data = 2'
    print >>fileobj
    fileobj.flush()
    fileobj.seek(0)

    config = load_options(fileobj.name)
    assert_equals(config['client']['user'], 'root')
    assert_equals(config['client']['password'], 'foo"barbaz')

def test_write_options():
    fileobj = tempfile.NamedTemporaryFile()
    print >>fileobj, "[client]"
    print >>fileobj, "user = root"
    print >>fileobj, 'password = "foo"barbaz"'
    print >>fileobj, '[mysqldump]'
    print >>fileobj, 'master-data = 2'
    print >>fileobj
    fileobj.flush()
    fileobj.seek(0)

    config = load_options(fileobj.name)

    write_options(config, fileobj.name)

def test_load_options_with_errors():
    fileobj = tempfile.NamedTemporaryFile()
    print >>fileobj, "[client]"
    print >>fileobj, "user = root"
    print >>fileobj, 'password = "foo"barbaz"'
    print >>fileobj, 'single-transaction=1'
    print >>fileobj
    print >>fileobj, '[mysqldump]'
    print >>fileobj, 'master-data = 2'
    # ConfigObj won't support bare options like these
    # check that load_options skip them cleanly
    print >>fileobj, 'skip-dump-data'
    print >>fileobj, 'skip-lock-tables'
    print >>fileobj
    fileobj.flush()
    fileobj.seek(0)

    config = load_options(fileobj.name)

# configobj is quirky and will raise ParseError w/ one error
# and ConfigObjError w/ a errors attribute when there are
# multiple errors.  Let's test that we handle this
def test_load_options_with_one_error():
    fileobj = tempfile.NamedTemporaryFile()
    print >>fileobj, "[client]"
    print >>fileobj, "user = root"
    print >>fileobj, 'password = "foo"barbaz"'
    print >>fileobj, 'single-transaction=1'
    print >>fileobj
    print >>fileobj, '[mysqldump]'
    print >>fileobj, 'master-data = 2'
    # ConfigObj won't support bare options like these
    # check that load_options skip them cleanly
    print >>fileobj, 'skip-lock-tables'
    print >>fileobj
    fileobj.flush()
    fileobj.seek(0)

    config = load_options(fileobj.name)

########NEW FILE########
__FILENAME__ = build_debs
#!/usr/bin/env python

import sys, os
import time
import shutil
import logging
import subprocess
from os.path import join, dirname, abspath

config = {}
config['srcdir'] = abspath(join(dirname(__file__), '..'))
config['debian'] = join(config['srcdir'], 'contrib', 'debian')

def holland_version():
    holland_core_dir = join(config['srcdir'])
    args = ['python', 'setup.py', '--version']
    return subprocess.Popen(args, stdout=subprocess.PIPE, cwd=holland_core_dir).communicate()[0].strip()

def changelog_time():
    time_str = time.strftime('%a, %d %b %Y %H:%M:%S ')
    offset = ('-','+')[time.altzone > 0]
    offset += '%04d' % time.altzone
    return time_str + offset

def update_changelog():
    format = """\
holland (%(version)s-local-%(today)s) unstable; urgency=low

  * Non-maintainer upload

 -- %(name)s <%(email)s>  %(date)s
"""
    version = holland_version()
    entry = format % { 'version' : version,
                       'name' : 'Unknown',
                       'email': 'example@foo.com',
                       'today': time.strftime('%Y%M%d%H%M'),
                       'date' : changelog_time()
                     }
    src = join(config['debian'], 'changelog')
    changelog = open(src + '.new', 'w')
    print >>changelog, entry,
    shutil.copyfileobj(open(src), changelog)
    changelog.close()
    os.rename(src + '.new', src)
    logging.info("Updated %s with NMU changelog", src)
                        
def check_prereq():
    control_file = join(config['debian'], 'control')
    assert os.path.exists('/usr/bin/dpkg-checkbuilddeps'), \
        "dpkg-dev required to build the Holland debian packages"
    assert os.path.exists('/usr/bin/debuild'), \
        "devscripts required to build the Holland debian packages"

    args = ['dpkg-checkbuilddeps', control_file]
    logging.info("Checking prereqs. Running %s", subprocess.list2cmdline(args))
    ret = subprocess.call(args)
    return ret

def prep_tree():
    src = config['debian']
    dst = join(config['srcdir'], 'debian')
    
    if os.path.exists(dst):
        if os.path.islink(dst):
            os.unlink(dst)
        else:
            shutil.rmtree(dst)
 
    shutil.copytree(src, dst)
    logging.info("Copied %s to %s", src, dst)
    update_changelog()

def build_deb():
    args = [
        'debuild',
        '--no-tgz-check',
        '-rfakeroot',
        '-us',
        '-uc',
    ]
    logging.info("Running %s", subprocess.list2cmdline(args))
    return subprocess.call(args)

def cleanup_tree():
    args = [
        'debuild',
        'clean'
    ]
    logging.info("Running %s", subprocess.list2cmdline(args))
    subprocess.call(args)
    src = config['debian']
    dst = join(config['srcdir'], 'debian')
    if os.path.samefile(src, dst):
        os.unlink(dst)
        logging.info("Unlinked %s", dst)

def config_logging():
    logger = logging.getLogger()
    handler = logging.StreamHandler()
    handler.setLevel(logging.INFO)
    formatter = logging.Formatter("%(levelname)s: %(message)s")
    handler.setFormatter(formatter)
    logger.addHandler(handler)

def main():
    config_logging()
    if check_prereq() != 0:
        return 1
    try:
        try:
            prep_tree()
            return build_deb()
        except AssertionError, exc:
            logging.fatal("%s", exc)
            return 1
    finally:
        cleanup_tree()
    
if __name__ == '__main__':
    sys.exit(main())

########NEW FILE########
__FILENAME__ = build_rpms
#!/usr/bin/env python

import sys, os
import shutil
import re
import tarfile
from subprocess import Popen, PIPE
from optparse import OptionParser, IndentedHelpFormatter

VERSION='0.3'

config = {}
config['srcdir'] = os.getcwd()
config['topdir'] = os.path.join(os.environ['HOME'], 'holland-buildroot')
config['spec'] = './contrib/holland.spec'

def get_opts_args():
    fmt = IndentedHelpFormatter(
            indent_increment=4, max_help_position=32, width=77, short_first=1
            )
    parser = OptionParser(formatter=fmt, version=VERSION)
    parser.usage = """ devtools/build_rpms.py --(OPTIONS)"""

    parser.add_option('--topdir', action='store', dest='topdir',
                      help="tmp directory to build in.")
    parser.add_option('--just-source', action='store_true', dest='just_source',
                      help="just build the source rpm")
    parser.add_option('--clean', action='store_true', dest='clean',
                      help="remove directory after building (for testing)")
    parser.add_option('--with-plugin', action='append', dest='with_plugins',
                      default=[], metavar='PLUGIN',
                      help="Include additional plugins not built by default")
    parser.add_option('--tarball', action='store_true',
                      help='Use entire source tree rather than git archive.'
                           'Default when now executed from a git repository')
    (cli_opts, cli_args) = parser.parse_args()
    return (cli_opts, cli_args)

def prep_buildroot(cli_opts):
    version, dev_tag = get_holland_version()
    dirs = ['RPMS', 'SRPMS', 'BUILD', 'SPECS', 'SOURCES']
    for d in dirs:
        path = os.path.join(config['topdir'], d)
        if not os.path.exists(path):
            os.makedirs(path)
    f = open(config['spec'], 'r')
    data = f.read()
    f.close()

    data = re.sub('@@@VERSION@@@', version, data)

    f = open(os.path.join(config['topdir'], 'SPECS', 'holland.spec'), 'w')
    f.write(data)
    f.close()

    if cli_opts.tarball or not os.path.exists('.git'):
        print "creating source distribution %s/SOURCES/holland-%s.tar.gz" % \
              (config['topdir'], version)
        from tarfile import TarFile
        archive = tarfile.open('%s/SOURCES/holland-%s.tar.gz' %
                               (config['topdir'], version),
                               'w:gz')
        archive.add('.', arcname='holland-%s/' % version)
        archive.close()
    else:
        cmd = "git archive --prefix=holland-%s/ HEAD > %s/SOURCES/holland-%s.tar.gz" % \
              (version, config['topdir'], version)
        print cmd
        os.system(cmd)


def build_srpm():
    version, dev_tag = get_holland_version()
    if dev_tag:
        dev_option = "--define='src_dev_tag dev'"
    else:
        dev_option = ''

    cmd = "rpmbuild -bs %s/SPECS/holland.spec --define='_topdir %s' %s" % \
           (config['topdir'], config['topdir'], dev_option)
    retcode = os.system(cmd)
    return retcode

def build_rpms(with_extra):
    version, dev_tag = get_holland_version()
    if dev_tag:
        dev_option = "--define='src_dev_tag dev'"
    else:
        dev_option = ''

    with_extra = ' '.join(['--with %s' % extra for extra in with_extra])
    cmd = "rpmbuild -bb %s/SPECS/holland.spec --define='_topdir %s' %s %s" % \
           (config['topdir'], config['topdir'], dev_option, with_extra)
    print cmd
    retcode = os.system(cmd)
    return retcode


def get_holland_version():
    version = None
    dev_tag = None
    version = Popen(['python', 'setup.py', '--version'], stdout=PIPE, cwd=
                    os.path.join(config['srcdir']))\
                    .communicate()[0].strip('\n')
    try:
        if int(version.split('.')[-1])%2 != 0:
            dev_tag = 'dev'
    except ValueError:
        dev_tag = None

    if not version:
        raise Exception, "unable to determine holland version"
    return version, dev_tag


def exit(code=0, clean=False):
    if clean:
        if os.path.exists(config['topdir']):
            print "cleaning %s" % config['topdir']
            shutil.rmtree(config['topdir'])
    sys.exit(code)

def main():
    (cli_opts, cli_args) = get_opts_args()
    (version, dev_tag) = get_holland_version()

    if cli_opts.topdir:
        if not os.path.exists(os.path.abspath(cli_opts.topdir)):
            os.makedirs(os.path.abspath(cli_opts.topdir))
        config['topdir'] = os.path.abspath(cli_opts.topdir)

    prep_buildroot(cli_opts)
    retcode = build_srpm()
    if int(retcode) != 0:
        print
        print '-' * 77
        print
        print "Please correct the above errors"
        print
        exit(1, cli_opts.clean)

    if not cli_opts.just_source:
        retcode = build_rpms(cli_opts.with_plugins)

    print
    print '-' * 77
    print

    if int(retcode) == 0:
        print "Holland %s%s built in %s" % (version, dev_tag or '', config['topdir'])
        exit(0, cli_opts.clean)
    else:
        print "Holland %s%s build FAILED!  Files in %s" % (version, dev_tag,
                                                           config['topdir'])
        exit(1, cli_opts.clean)
    print

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = make_release
#!/usr/bin/env python
import sys
import shlex
import logging
import commands
from tarfile import TarFile
from optparse import OptionParser

def main(args=None):
    parser = OptionParser()
    parser.add_option('--version', default=None)
    opts, args = parser.parse_args(args)

    logging.basicConfig(level=logging.INFO,
                        format='[%(levelname)s] %(message)s')

    if not opts.version:
        version = commands.getoutput('python setup.py --version')
    else:
        version = opts.version

    name = 'holland-%s' % version

    status, output = commands.getstatusoutput('git archive --prefix=%s/ HEAD '
                                              '| gzip --fast > %s.tar.gz' %
                                              (name, name))
    if status != 0:
        logging.error("%s failed.", sys.argv[0])
        return 1
    else:
        logging.info("created archive %s.tar.gz", name)
        return 0


if __name__ == '__main__':
    sys.exit(main())

########NEW FILE########
__FILENAME__ = mkvirtenv
#!/usr/bin/env python

"""Setup a python virtual environment to test holland"""

import sys, os
import signal
import shutil
import logging
import subprocess
from optparse import OptionParser
from os.path import abspath, join, dirname, basename, expanduser
try:
    import curses
    curses.setupterm()
except ImportError:
    curses = None

from _virtualenv import create_environment

HOLLAND_ROOT = abspath(join(dirname(__file__), '..'))

def make_env(virtual_env_root):
    """Setup an environment dictionary suitable for passing to
    ``subprocess.Popen`` that allows commands to run correctly
    in the virtual environment specified by ``virtual_env_root``
    """
    env = dict(os.environ) # copy environment
    env['VIRTUAL_ENV'] = virtual_env_root
    env['PATH'] = ':'.join(
                            [
                                join(virtual_env_root, 'bin'),
                                os.environ.get('PATH', ''),
                            ]
                        )
    env['PS1'] = r'[holland-test]% '
    env['PROMPT'] = env['PS1']
    env['HOLLAND_CONFIG'] = join(virtual_env_root,
                                 'etc',
                                 'holland',
                                 'holland.conf')
    return env

def start_shell(virtual_env):
    """Start a shell in the virtual environment"""
    shell = os.environ.get('SHELL', '/bin/bash')
    logging.info("Starting shell in virtual environment %s - "
                 "use ctrl-d to exit", shell)
    args = [shell]
    if basename(shell) == 'zsh':
        args += ['--no-globalrcs']
    pid = subprocess.Popen(args, env=virtual_env)
    while True:
        try:
            if pid.wait() is not None:
                logging.info("Shell exited with status %d", pid.returncode)
                break
        except KeyboardInterrupt:
            logging.info("start_shell SIGTERM")
            pass
    return pid.returncode

def run_setup_develop(cwd, env):
    """Run python setup.py --develop in the specified working directory
    and with the provided environment dictionary
    """
    log_path = join(env['VIRTUAL_ENV'], 'holland_install.log')
    return subprocess.call(['python', 'setup.py', 'develop'],
                           stdout=open(log_path, 'a'),
                           stderr=subprocess.STDOUT,
                           cwd=cwd,
                           env=env)

def install_holland(virtual_env):
    """Install holland-core"""
    env = dict(virtual_env)
    holland_core = join(HOLLAND_ROOT)
    ret = run_setup_develop(cwd=holland_core, env=env)
    if ret != 0:
        logging.error("Failed to install holland-core")
    else:
        logging.info("Installed holland-core.")

def install_plugins(virtual_env, egg_env):
    """Install (active) holland plugins"""
    logging.info("Installing holland plugins")
    for plugin_dir in open(join(HOLLAND_ROOT, 'plugins', 'ACTIVE')):
        plugin_dir = plugin_dir.rstrip()
        if plugin_dir in egg_env:
            logging.info("%r found in test environment. Not installing.", 
                         plugin_dir)
            continue
        plugin_path = join(HOLLAND_ROOT, 'plugins', plugin_dir)
        ret = run_setup_develop(cwd=plugin_path, env=virtual_env)
        if ret != 0:
            logging.error("Failed to install plugin %s", plugin_dir)
        else:
            logging.info("Installed plugin %s", plugin_dir)

if curses:
    COLOR_NAMES = "BLACK RED GREEN YELLOW BLUE MAGENTA CYAN WHITE"
    COLORS = dict(zip(COLOR_NAMES.split(), xrange(8)))
    RESET = curses.tigetstr('sgr0')
    def colorize(record):
        levelno = record.levelno
        if(levelno>=40):
            color = COLORS['RED'] # red
        elif(levelno>=30):
            color = COLORS['YELLOW'] # yellow
        elif(levelno>=20):
            color = COLORS['GREEN'] # green 
        elif(levelno>=10):
            color = COLORS['MAGENTA'] 
        else:
            color = RESET # normal
        color = curses.tparm(curses.tigetstr('setaf'), color)
        record.levelname = color + record.levelname + RESET
        return record
else:
    colorize = lambda record: record

class ColorFormatter(logging.Formatter):
    def format(self, record):
        return logging.Formatter.format(self, colorize(record))

def setup_logging(debug):
    """Setup basic console logging"""
    root = logging.getLogger()
    root.setLevel(debug and logging.DEBUG or logging.INFO)
    handler = logging.StreamHandler()
    formatter = ColorFormatter(fmt='[%(levelname)s] %(message)s')
    handler.setFormatter(formatter)
    root.addHandler(handler)

def install_configs(env_root):
    """Install testing configs into virtual environment"""
    holland_etc = join(env_root, 'etc', 'holland')
    holland_bk_etc = join(env_root, 'etc', 'holland', 'backupsets')
    holland_pv_etc = join(env_root, 'etc', 'holland', 'providers')

    if os.path.exists(holland_etc):
        logging.info("An existing config already exists in %s. Not installing test configs.",
                        holland_etc)
        return
    # copytree doesn't create all dirs on python 2.4
    if not os.path.exists(join(env_root, 'etc')):
        os.makedirs(join(env_root, 'etc'))
    shutil.copytree(join(HOLLAND_ROOT, 'test_config'),
                    join(env_root, 'etc', 'holland'))

def find_egg_env(path):
    from pkg_resources import Environment
    return Environment([path])

def main(args=None):
    """Main script entry point"""
    oparser = OptionParser()
    oparser.add_option('--distribute', action='store_true',
                       default=False,
                       help='Use Distribute instead of Setuptools')
    oparser.add_option('--clear', action='store_true',
                       default=False,
                       help='Clear out the non-root install and start '
                            'from scratch')
    oparser.add_option('--no-site-packages', action='store_true',
                       default=False, dest='no_site_packages',
                       help="Don't use the systems site_packages")
    oparser.add_option('--debug', action='store_true')
    opts, args = oparser.parse_args(args)

    # this seems odd, but we want the oposite logic of no-site-packages
    if opts.no_site_packages:
        use_site_packages = False
    else:
        use_site_packages = True

    setup_logging(opts.debug)
    home_dir = os.environ.get('HOLLAND_HOME', expanduser('~/holland-test'))
    if home_dir in sys.executable:
        logging.error("Please exit your current virtual environment before trying to create another")
        return 1

    create_environment(home_dir, site_packages=use_site_packages, clear=opts.clear,
                       unzip_setuptools=False, use_distribute=opts.distribute)
    virtualenv = make_env(home_dir)
    egg_env = find_egg_env(os.path.join(home_dir, 'lib', 'python2.4', 'site-packages'))
    if 'holland' in egg_env:
        logging.info("'holland' found in environment. Not reinstalling.")
    else:
        install_holland(virtualenv)
    install_plugins(virtualenv, egg_env)

    install_configs(home_dir)
    result = start_shell(virtualenv)
    logging.info("Exiting virtual environment[%d]", result)
    return result

if __name__ == '__main__':
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        logging.warn("Interrupted")
        sys.exit(1)

########NEW FILE########
__FILENAME__ = _virtualenv

"""Create a "virtual" Python installation
"""

virtualenv_version = "1.5.1"

import sys
import os
import optparse
import re
import shutil
import logging
import tempfile
import distutils.sysconfig
try:
    import subprocess
except ImportError, e:
    if sys.version_info <= (2, 3):
        print 'ERROR: %s' % e
        print 'ERROR: this script requires Python 2.4 or greater; or at least the subprocess module.'
        print 'If you copy subprocess.py from a newer version of Python this script will probably work'
        sys.exit(101)
    else:
        raise
try:
    set
except NameError:
    from sets import Set as set

join = os.path.join
py_version = 'python%s.%s' % (sys.version_info[0], sys.version_info[1])

is_jython = sys.platform.startswith('java')
is_pypy = hasattr(sys, 'pypy_version_info')

if is_pypy:
    expected_exe = 'pypy-c'
elif is_jython:
    expected_exe = 'jython'
else:
    expected_exe = 'python'


REQUIRED_MODULES = ['os', 'posix', 'posixpath', 'nt', 'ntpath', 'genericpath',
                    'fnmatch', 'locale', 'encodings', 'codecs',
                    'stat', 'UserDict', 'readline', 'copy_reg', 'types',
                    're', 'sre', 'sre_parse', 'sre_constants', 'sre_compile',
                    'zlib']

REQUIRED_FILES = ['lib-dynload', 'config']

if sys.version_info[:2] >= (2, 6):
    REQUIRED_MODULES.extend(['warnings', 'linecache', '_abcoll', 'abc'])
if sys.version_info[:2] >= (2, 7):
    REQUIRED_MODULES.extend(['_weakrefset'])
if sys.version_info[:2] <= (2, 3):
    REQUIRED_MODULES.extend(['sets', '__future__'])
if is_pypy:
    # these are needed to correctly display the exceptions that may happen
    # during the bootstrap
    REQUIRED_MODULES.extend(['traceback', 'linecache'])

class Logger(object):

    """
    Logging object for use in command-line script.  Allows ranges of
    levels, to avoid some redundancy of displayed information.
    """

    DEBUG = logging.DEBUG
    INFO = logging.INFO
    NOTIFY = (logging.INFO+logging.WARN)/2
    WARN = WARNING = logging.WARN
    ERROR = logging.ERROR
    FATAL = logging.FATAL

    LEVELS = [DEBUG, INFO, NOTIFY, WARN, ERROR, FATAL]

    def __init__(self, consumers):
        self.consumers = consumers
        self.indent = 0
        self.in_progress = None
        self.in_progress_hanging = False

    def debug(self, msg, *args, **kw):
        self.log(self.DEBUG, msg, *args, **kw)
    def info(self, msg, *args, **kw):
        self.log(self.INFO, msg, *args, **kw)
    def notify(self, msg, *args, **kw):
        self.log(self.NOTIFY, msg, *args, **kw)
    def warn(self, msg, *args, **kw):
        self.log(self.WARN, msg, *args, **kw)
    def error(self, msg, *args, **kw):
        self.log(self.WARN, msg, *args, **kw)
    def fatal(self, msg, *args, **kw):
        self.log(self.FATAL, msg, *args, **kw)
    def log(self, level, msg, *args, **kw):
        if args:
            if kw:
                raise TypeError(
                    "You may give positional or keyword arguments, not both")
        args = args or kw
        rendered = None
        for consumer_level, consumer in self.consumers:
            if self.level_matches(level, consumer_level):
                if (self.in_progress_hanging
                    and consumer in (sys.stdout, sys.stderr)):
                    self.in_progress_hanging = False
                    sys.stdout.write('\n')
                    sys.stdout.flush()
                if rendered is None:
                    if args:
                        rendered = msg % args
                    else:
                        rendered = msg
                    rendered = ' '*self.indent + rendered
                if hasattr(consumer, 'write'):
                    consumer.write(rendered+'\n')
                else:
                    consumer(rendered)

    def start_progress(self, msg):
        assert not self.in_progress, (
            "Tried to start_progress(%r) while in_progress %r"
            % (msg, self.in_progress))
        if self.level_matches(self.NOTIFY, self._stdout_level()):
            sys.stdout.write(msg)
            sys.stdout.flush()
            self.in_progress_hanging = True
        else:
            self.in_progress_hanging = False
        self.in_progress = msg

    def end_progress(self, msg='done.'):
        assert self.in_progress, (
            "Tried to end_progress without start_progress")
        if self.stdout_level_matches(self.NOTIFY):
            if not self.in_progress_hanging:
                # Some message has been printed out since start_progress
                sys.stdout.write('...' + self.in_progress + msg + '\n')
                sys.stdout.flush()
            else:
                sys.stdout.write(msg + '\n')
                sys.stdout.flush()
        self.in_progress = None
        self.in_progress_hanging = False

    def show_progress(self):
        """If we are in a progress scope, and no log messages have been
        shown, write out another '.'"""
        if self.in_progress_hanging:
            sys.stdout.write('.')
            sys.stdout.flush()

    def stdout_level_matches(self, level):
        """Returns true if a message at this level will go to stdout"""
        return self.level_matches(level, self._stdout_level())

    def _stdout_level(self):
        """Returns the level that stdout runs at"""
        for level, consumer in self.consumers:
            if consumer is sys.stdout:
                return level
        return self.FATAL

    def level_matches(self, level, consumer_level):
        """
        >>> l = Logger()
        >>> l.level_matches(3, 4)
        False
        >>> l.level_matches(3, 2)
        True
        >>> l.level_matches(slice(None, 3), 3)
        False
        >>> l.level_matches(slice(None, 3), 2)
        True
        >>> l.level_matches(slice(1, 3), 1)
        True
        >>> l.level_matches(slice(2, 3), 1)
        False
        """
        if isinstance(level, slice):
            start, stop = level.start, level.stop
            if start is not None and start > consumer_level:
                return False
            if stop is not None or stop <= consumer_level:
                return False
            return True
        else:
            return level >= consumer_level

    #@classmethod
    def level_for_integer(cls, level):
        levels = cls.LEVELS
        if level < 0:
            return levels[0]
        if level >= len(levels):
            return levels[-1]
        return levels[level]

    level_for_integer = classmethod(level_for_integer)

logger = Logger([(Logger.level_for_integer(2), sys.stdout)])

def mkdir(path):
    if not os.path.exists(path):
        logger.info('Creating %s', path)
        os.makedirs(path)
    else:
        logger.info('Directory %s already exists', path)

def copyfile(src, dest, symlink=True):
    if not os.path.exists(src):
        # Some bad symlink in the src
        logger.warn('Cannot find file %s (bad symlink)', src)
        return
    if os.path.exists(dest):
        logger.debug('File %s already exists', dest)
        return
    if not os.path.exists(os.path.dirname(dest)):
        logger.info('Creating parent directories for %s' % os.path.dirname(dest))
        os.makedirs(os.path.dirname(dest))
    if symlink and hasattr(os, 'symlink'):
        logger.info('Symlinking %s', dest)
        os.symlink(os.path.abspath(src), dest)
    else:
        logger.info('Copying to %s', dest)
        if os.path.isdir(src):
            shutil.copytree(src, dest, True)
        else:
            shutil.copy2(src, dest)

def writefile(dest, content, overwrite=True):
    if not os.path.exists(dest):
        logger.info('Writing %s', dest)
        f = open(dest, 'wb')
        f.write(content)
        f.close()
        return
    else:
        f = open(dest, 'rb')
        c = f.read()
        f.close()
        if c != content:
            if not overwrite:
                logger.notify('File %s exists with different content; not overwriting', dest)
                return
            logger.notify('Overwriting %s with new content', dest)
            f = open(dest, 'wb')
            f.write(content)
            f.close()
        else:
            logger.info('Content %s already in place', dest)

def rmtree(dir):
    if os.path.exists(dir):
        logger.notify('Deleting tree %s', dir)
        shutil.rmtree(dir)
    else:
        logger.info('Do not need to delete %s; already gone', dir)

def make_exe(fn):
    if hasattr(os, 'chmod'):
        oldmode = os.stat(fn).st_mode & 07777
        newmode = (oldmode | 0555) & 07777
        os.chmod(fn, newmode)
        logger.info('Changed mode of %s to %s', fn, oct(newmode))

def _find_file(filename, dirs):
    for dir in dirs:
        if os.path.exists(join(dir, filename)):
            return join(dir, filename)
    return filename

def _install_req(py_executable, unzip=False, distribute=False):
    if not distribute:
        setup_fn = 'setuptools-0.6c11-py%s.egg' % sys.version[:3]
        project_name = 'setuptools'
        bootstrap_script = EZ_SETUP_PY
        source = None
    else:
        setup_fn = None
        source = 'distribute-0.6.14.tar.gz'
        project_name = 'distribute'
        bootstrap_script = DISTRIBUTE_SETUP_PY
        try:
            # check if the global Python has distribute installed or plain
            # setuptools
            import pkg_resources
            if not hasattr(pkg_resources, '_distribute'):
                location = os.path.dirname(pkg_resources.__file__)
                logger.notify("A globally installed setuptools was found (in %s)" % location)
                logger.notify("Use the --no-site-packages option to use distribute in "
                              "the virtualenv.")
        except ImportError:
            pass

    search_dirs = file_search_dirs()

    if setup_fn is not None:
        setup_fn = _find_file(setup_fn, search_dirs)

    if source is not None:
        source = _find_file(source, search_dirs)

    if is_jython and os._name == 'nt':
        # Jython's .bat sys.executable can't handle a command line
        # argument with newlines
        fd, ez_setup = tempfile.mkstemp('.py')
        os.write(fd, bootstrap_script)
        os.close(fd)
        cmd = [py_executable, ez_setup]
    else:
        cmd = [py_executable, '-c', bootstrap_script]
    if unzip:
        cmd.append('--always-unzip')
    env = {}
    remove_from_env = []
    if logger.stdout_level_matches(logger.DEBUG):
        cmd.append('-v')

    old_chdir = os.getcwd()
    if setup_fn is not None and os.path.exists(setup_fn):
        logger.info('Using existing %s egg: %s' % (project_name, setup_fn))
        cmd.append(setup_fn)
        if os.environ.get('PYTHONPATH'):
            env['PYTHONPATH'] = setup_fn + os.path.pathsep + os.environ['PYTHONPATH']
        else:
            env['PYTHONPATH'] = setup_fn
    else:
        # the source is found, let's chdir
        if source is not None and os.path.exists(source):
            os.chdir(os.path.dirname(source))
            # in this case, we want to be sure that PYTHONPATH is unset (not
            # just empty, really unset), else CPython tries to import the
            # site.py that it's in virtualenv_support
            remove_from_env.append('PYTHONPATH')
        else:
            logger.info('No %s egg found; downloading' % project_name)
        cmd.extend(['--always-copy', '-U', project_name])
    logger.start_progress('Installing %s...' % project_name)
    logger.indent += 2
    cwd = None
    if project_name == 'distribute':
        env['DONT_PATCH_SETUPTOOLS'] = 'true'

    def _filter_ez_setup(line):
        return filter_ez_setup(line, project_name)

    if not os.access(os.getcwd(), os.W_OK):
        cwd = tempfile.mkdtemp()
        if source is not None and os.path.exists(source):
            # the current working dir is hostile, let's copy the
            # tarball to a temp dir
            target = os.path.join(cwd, os.path.split(source)[-1])
            shutil.copy(source, target)
    try:
        call_subprocess(cmd, show_stdout=False,
                        filter_stdout=_filter_ez_setup,
                        extra_env=env,
                        remove_from_env=remove_from_env,
                        cwd=cwd)
    finally:
        logger.indent -= 2
        logger.end_progress()
        if os.getcwd() != old_chdir:
            os.chdir(old_chdir)
        if is_jython and os._name == 'nt':
            os.remove(ez_setup)

def file_search_dirs():
    here = os.path.dirname(os.path.abspath(__file__))
    dirs = ['.', here,
            join(here, 'virtualenv_support')]
    if os.path.splitext(os.path.dirname(__file__))[0] != 'virtualenv':
        # Probably some boot script; just in case virtualenv is installed...
        try:
            import virtualenv
        except ImportError:
            pass
        else:
            dirs.append(os.path.join(os.path.dirname(virtualenv.__file__), 'virtualenv_support'))
    return [d for d in dirs if os.path.isdir(d)]

def install_setuptools(py_executable, unzip=False):
    _install_req(py_executable, unzip)

def install_distribute(py_executable, unzip=False):
    _install_req(py_executable, unzip, distribute=True)

_pip_re = re.compile(r'^pip-.*(zip|tar.gz|tar.bz2|tgz|tbz)$', re.I)
def install_pip(py_executable):
    filenames = []
    for dir in file_search_dirs():
        filenames.extend([join(dir, fn) for fn in os.listdir(dir)
                          if _pip_re.search(fn)])
    filenames = [(os.path.basename(filename).lower(), i, filename) for i, filename in enumerate(filenames)]
    filenames.sort()
    filenames = [filename for basename, i, filename in filenames]
    if not filenames:
        filename = 'pip'
    else:
        filename = filenames[-1]
    easy_install_script = 'easy_install'
    if sys.platform == 'win32':
        easy_install_script = 'easy_install-script.py'
    cmd = [py_executable, join(os.path.dirname(py_executable), easy_install_script), filename]
    if filename == 'pip':
        logger.info('Installing pip from network...')
    else:
        logger.info('Installing %s' % os.path.basename(filename))
    logger.indent += 2
    def _filter_setup(line):
        return filter_ez_setup(line, 'pip')
    try:
        call_subprocess(cmd, show_stdout=False,
                        filter_stdout=_filter_setup)
    finally:
        logger.indent -= 2

def filter_ez_setup(line, project_name='setuptools'):
    if not line.strip():
        return Logger.DEBUG
    if project_name == 'distribute':
        for prefix in ('Extracting', 'Now working', 'Installing', 'Before',
                       'Scanning', 'Setuptools', 'Egg', 'Already',
                       'running', 'writing', 'reading', 'installing',
                       'creating', 'copying', 'byte-compiling', 'removing',
                       'Processing'):
            if line.startswith(prefix):
                return Logger.DEBUG
        return Logger.DEBUG
    for prefix in ['Reading ', 'Best match', 'Processing setuptools',
                   'Copying setuptools', 'Adding setuptools',
                   'Installing ', 'Installed ']:
        if line.startswith(prefix):
            return Logger.DEBUG
    return Logger.INFO

def main():
    parser = optparse.OptionParser(
        version=virtualenv_version,
        usage="%prog [OPTIONS] DEST_DIR")

    parser.add_option(
        '-v', '--verbose',
        action='count',
        dest='verbose',
        default=0,
        help="Increase verbosity")

    parser.add_option(
        '-q', '--quiet',
        action='count',
        dest='quiet',
        default=0,
        help='Decrease verbosity')

    parser.add_option(
        '-p', '--python',
        dest='python',
        metavar='PYTHON_EXE',
        help='The Python interpreter to use, e.g., --python=python2.5 will use the python2.5 '
        'interpreter to create the new environment.  The default is the interpreter that '
        'virtualenv was installed with (%s)' % sys.executable)

    parser.add_option(
        '--clear',
        dest='clear',
        action='store_true',
        help="Clear out the non-root install and start from scratch")

    parser.add_option(
        '--no-site-packages',
        dest='no_site_packages',
        action='store_true',
        help="Don't give access to the global site-packages dir to the "
             "virtual environment")

    parser.add_option(
        '--unzip-setuptools',
        dest='unzip_setuptools',
        action='store_true',
        help="Unzip Setuptools or Distribute when installing it")

    parser.add_option(
        '--relocatable',
        dest='relocatable',
        action='store_true',
        help='Make an EXISTING virtualenv environment relocatable.  '
        'This fixes up scripts and makes all .pth files relative')

    parser.add_option(
        '--distribute',
        dest='use_distribute',
        action='store_true',
        help='Use Distribute instead of Setuptools. Set environ variable '
        'VIRTUALENV_USE_DISTRIBUTE to make it the default ')

    parser.add_option(
        '--prompt=',
        dest='prompt',
        help='Provides an alternative prompt prefix for this environment')

    if 'extend_parser' in globals():
        extend_parser(parser)

    options, args = parser.parse_args()

    global logger

    if 'adjust_options' in globals():
        adjust_options(options, args)

    verbosity = options.verbose - options.quiet
    logger = Logger([(Logger.level_for_integer(2-verbosity), sys.stdout)])

    if options.python and not os.environ.get('VIRTUALENV_INTERPRETER_RUNNING'):
        env = os.environ.copy()
        interpreter = resolve_interpreter(options.python)
        if interpreter == sys.executable:
            logger.warn('Already using interpreter %s' % interpreter)
        else:
            logger.notify('Running virtualenv with interpreter %s' % interpreter)
            env['VIRTUALENV_INTERPRETER_RUNNING'] = 'true'
            file = __file__
            if file.endswith('.pyc'):
                file = file[:-1]
            popen = subprocess.Popen([interpreter, file] + sys.argv[1:], env=env)
            raise SystemExit(popen.wait())

    if not args:
        print 'You must provide a DEST_DIR'
        parser.print_help()
        sys.exit(2)
    if len(args) > 1:
        print 'There must be only one argument: DEST_DIR (you gave %s)' % (
            ' '.join(args))
        parser.print_help()
        sys.exit(2)

    home_dir = args[0]

    if os.environ.get('WORKING_ENV'):
        logger.fatal('ERROR: you cannot run virtualenv while in a workingenv')
        logger.fatal('Please deactivate your workingenv, then re-run this script')
        sys.exit(3)

    if 'PYTHONHOME' in os.environ:
        logger.warn('PYTHONHOME is set.  You *must* activate the virtualenv before using it')
        del os.environ['PYTHONHOME']

    if options.relocatable:
        make_environment_relocatable(home_dir)
        return

    create_environment(home_dir, site_packages=not options.no_site_packages, clear=options.clear,
                       unzip_setuptools=options.unzip_setuptools,
                       use_distribute=options.use_distribute,
                       prompt=options.prompt)
    if 'after_install' in globals():
        after_install(options, home_dir)

def call_subprocess(cmd, show_stdout=True,
                    filter_stdout=None, cwd=None,
                    raise_on_returncode=True, extra_env=None,
                    remove_from_env=None):
    cmd_parts = []
    for part in cmd:
        if len(part) > 40:
            part = part[:30]+"..."+part[-5:]
        if ' ' in part or '\n' in part or '"' in part or "'" in part:
            part = '"%s"' % part.replace('"', '\\"')
        cmd_parts.append(part)
    cmd_desc = ' '.join(cmd_parts)
    if show_stdout:
        stdout = None
    else:
        stdout = subprocess.PIPE
    logger.debug("Running command %s" % cmd_desc)
    if extra_env or remove_from_env:
        env = os.environ.copy()
        if extra_env:
            env.update(extra_env)
        if remove_from_env:
            for varname in remove_from_env:
                env.pop(varname, None)
    else:
        env = None
    try:
        proc = subprocess.Popen(
            cmd, stderr=subprocess.STDOUT, stdin=None, stdout=stdout,
            cwd=cwd, env=env)
    except Exception, e:
        logger.fatal(
            "Error %s while executing command %s" % (e, cmd_desc))
        raise
    all_output = []
    if stdout is not None:
        stdout = proc.stdout
        while 1:
            line = stdout.readline()
            if not line:
                break
            line = line.rstrip()
            all_output.append(line)
            if filter_stdout:
                level = filter_stdout(line)
                if isinstance(level, tuple):
                    level, line = level
                logger.log(level, line)
                if not logger.stdout_level_matches(level):
                    logger.show_progress()
            else:
                logger.info(line)
    else:
        proc.communicate()
    proc.wait()
    if proc.returncode:
        if raise_on_returncode:
            if all_output:
                logger.notify('Complete output from command %s:' % cmd_desc)
                logger.notify('\n'.join(all_output) + '\n----------------------------------------')
            raise OSError(
                "Command %s failed with error code %s"
                % (cmd_desc, proc.returncode))
        else:
            logger.warn(
                "Command %s had error code %s"
                % (cmd_desc, proc.returncode))


def create_environment(home_dir, site_packages=True, clear=False,
                       unzip_setuptools=False, use_distribute=False,
                       prompt=None):
    """
    Creates a new environment in ``home_dir``.

    If ``site_packages`` is true (the default) then the global
    ``site-packages/`` directory will be on the path.

    If ``clear`` is true (default False) then the environment will
    first be cleared.
    """
    home_dir, lib_dir, inc_dir, bin_dir = path_locations(home_dir)

    py_executable = os.path.abspath(install_python(
        home_dir, lib_dir, inc_dir, bin_dir,
        site_packages=site_packages, clear=clear))

    install_distutils(home_dir)

    if use_distribute or os.environ.get('VIRTUALENV_USE_DISTRIBUTE'):
        install_distribute(py_executable, unzip=unzip_setuptools)
    else:
        install_setuptools(py_executable, unzip=unzip_setuptools)

    install_pip(py_executable)

    install_activate(home_dir, bin_dir, prompt)

def path_locations(home_dir):
    """Return the path locations for the environment (where libraries are,
    where scripts go, etc)"""
    # XXX: We'd use distutils.sysconfig.get_python_inc/lib but its
    # prefix arg is broken: http://bugs.python.org/issue3386
    if sys.platform == 'win32':
        # Windows has lots of problems with executables with spaces in
        # the name; this function will remove them (using the ~1
        # format):
        mkdir(home_dir)
        if ' ' in home_dir:
            try:
                import win32api
            except ImportError:
                print 'Error: the path "%s" has a space in it' % home_dir
                print 'To handle these kinds of paths, the win32api module must be installed:'
                print '  http://sourceforge.net/projects/pywin32/'
                sys.exit(3)
            home_dir = win32api.GetShortPathName(home_dir)
        lib_dir = join(home_dir, 'Lib')
        inc_dir = join(home_dir, 'Include')
        bin_dir = join(home_dir, 'Scripts')
    elif is_jython:
        lib_dir = join(home_dir, 'Lib')
        inc_dir = join(home_dir, 'Include')
        bin_dir = join(home_dir, 'bin')
    elif is_pypy:
        lib_dir = home_dir
        inc_dir = join(home_dir, 'include')
        bin_dir = join(home_dir, 'bin')
    else:
        lib_dir = join(home_dir, 'lib', py_version)
        inc_dir = join(home_dir, 'include', py_version)
        bin_dir = join(home_dir, 'bin')
    return home_dir, lib_dir, inc_dir, bin_dir


def change_prefix(filename, dst_prefix):
    prefixes = [sys.prefix]
    if hasattr(sys, 'real_prefix'):
        prefixes.append(sys.real_prefix)
    prefixes = map(os.path.abspath, prefixes)
    filename = os.path.abspath(filename)
    for src_prefix in prefixes:
        if filename.startswith(src_prefix):
            _, relpath = filename.split(src_prefix, 1)
            assert relpath[0] == os.sep
            relpath = relpath[1:]
            return join(dst_prefix, relpath)
    assert False, "Filename %s does not start with any of these prefixes: %s" % \
        (filename, prefixes)

def copy_required_modules(dst_prefix):
    import imp
    for modname in REQUIRED_MODULES:
        if modname in sys.builtin_module_names:
            logger.info("Ignoring built-in bootstrap module: %s" % modname)
            continue
        try:
            f, filename, _ = imp.find_module(modname)
        except ImportError:
            logger.info("Cannot import bootstrap module: %s" % modname)
        else:
            if f is not None:
                f.close()
            dst_filename = change_prefix(filename, dst_prefix)
            copyfile(filename, dst_filename)
            if filename.endswith('.pyc'):
                pyfile = filename[:-1]
                if os.path.exists(pyfile):
                    copyfile(pyfile, dst_filename[:-1])


def install_python(home_dir, lib_dir, inc_dir, bin_dir, site_packages, clear):
    """Install just the base environment, no distutils patches etc"""
    if sys.executable.startswith(bin_dir):
        print 'Please use the *system* python to run this script'
        return

    if clear:
        rmtree(lib_dir)
        ## FIXME: why not delete it?
        ## Maybe it should delete everything with #!/path/to/venv/python in it
        logger.notify('Not deleting %s', bin_dir)

    if hasattr(sys, 'real_prefix'):
        logger.notify('Using real prefix %r' % sys.real_prefix)
        prefix = sys.real_prefix
    else:
        prefix = sys.prefix
    mkdir(lib_dir)
    fix_lib64(lib_dir)
    stdlib_dirs = [os.path.dirname(os.__file__)]
    if sys.platform == 'win32':
        stdlib_dirs.append(join(os.path.dirname(stdlib_dirs[0]), 'DLLs'))
    elif sys.platform == 'darwin':
        stdlib_dirs.append(join(stdlib_dirs[0], 'site-packages'))
    if hasattr(os, 'symlink'):
        logger.info('Symlinking Python bootstrap modules')
    else:
        logger.info('Copying Python bootstrap modules')
    logger.indent += 2
    try:
        # copy required files...
        for stdlib_dir in stdlib_dirs:
            if not os.path.isdir(stdlib_dir):
                continue
            for fn in os.listdir(stdlib_dir):
                if fn != 'site-packages' and os.path.splitext(fn)[0] in REQUIRED_FILES:
                    copyfile(join(stdlib_dir, fn), join(lib_dir, fn))
        # ...and modules
        copy_required_modules(home_dir)
    finally:
        logger.indent -= 2
    mkdir(join(lib_dir, 'site-packages'))
    import site
    site_filename = site.__file__
    if site_filename.endswith('.pyc'):
        site_filename = site_filename[:-1]
    elif site_filename.endswith('$py.class'):
        site_filename = site_filename.replace('$py.class', '.py')
    site_filename_dst = change_prefix(site_filename, home_dir)
    site_dir = os.path.dirname(site_filename_dst)
    writefile(site_filename_dst, SITE_PY)
    writefile(join(site_dir, 'orig-prefix.txt'), prefix)
    site_packages_filename = join(site_dir, 'no-global-site-packages.txt')
    if not site_packages:
        writefile(site_packages_filename, '')
    else:
        if os.path.exists(site_packages_filename):
            logger.info('Deleting %s' % site_packages_filename)
            os.unlink(site_packages_filename)

    if is_pypy:
        stdinc_dir = join(prefix, 'include')
    else:
        stdinc_dir = join(prefix, 'include', py_version)
    if os.path.exists(stdinc_dir):
        copyfile(stdinc_dir, inc_dir)
    else:
        logger.debug('No include dir %s' % stdinc_dir)

    if sys.exec_prefix != prefix:
        if sys.platform == 'win32':
            exec_dir = join(sys.exec_prefix, 'lib')
        elif is_jython:
            exec_dir = join(sys.exec_prefix, 'Lib')
        else:
            exec_dir = join(sys.exec_prefix, 'lib', py_version)
        for fn in os.listdir(exec_dir):
            copyfile(join(exec_dir, fn), join(lib_dir, fn))

    if is_jython:
        # Jython has either jython-dev.jar and javalib/ dir, or just
        # jython.jar
        for name in 'jython-dev.jar', 'javalib', 'jython.jar':
            src = join(prefix, name)
            if os.path.exists(src):
                copyfile(src, join(home_dir, name))
        # XXX: registry should always exist after Jython 2.5rc1
        src = join(prefix, 'registry')
        if os.path.exists(src):
            copyfile(src, join(home_dir, 'registry'), symlink=False)
        copyfile(join(prefix, 'cachedir'), join(home_dir, 'cachedir'),
                 symlink=False)

    mkdir(bin_dir)
    py_executable = join(bin_dir, os.path.basename(sys.executable))
    if 'Python.framework' in prefix:
        if re.search(r'/Python(?:-32|-64)*$', py_executable):
            # The name of the python executable is not quite what
            # we want, rename it.
            py_executable = os.path.join(
                    os.path.dirname(py_executable), 'python')

    logger.notify('New %s executable in %s', expected_exe, py_executable)
    if sys.executable != py_executable:
        ## FIXME: could I just hard link?
        executable = sys.executable
        if sys.platform == 'cygwin' and os.path.exists(executable + '.exe'):
            # Cygwin misreports sys.executable sometimes
            executable += '.exe'
            py_executable += '.exe'
            logger.info('Executable actually exists in %s' % executable)
        shutil.copyfile(executable, py_executable)
        make_exe(py_executable)
        if sys.platform == 'win32' or sys.platform == 'cygwin':
            pythonw = os.path.join(os.path.dirname(sys.executable), 'pythonw.exe')
            if os.path.exists(pythonw):
                logger.info('Also created pythonw.exe')
                shutil.copyfile(pythonw, os.path.join(os.path.dirname(py_executable), 'pythonw.exe'))
        if is_pypy:
            # make a symlink python --> pypy-c
            python_executable = os.path.join(os.path.dirname(py_executable), 'python')
            logger.info('Also created executable %s' % python_executable)
            copyfile(py_executable, python_executable)

    if os.path.splitext(os.path.basename(py_executable))[0] != expected_exe:
        secondary_exe = os.path.join(os.path.dirname(py_executable),
                                     expected_exe)
        py_executable_ext = os.path.splitext(py_executable)[1]
        if py_executable_ext == '.exe':
            # python2.4 gives an extension of '.4' :P
            secondary_exe += py_executable_ext
        if os.path.exists(secondary_exe):
            logger.warn('Not overwriting existing %s script %s (you must use %s)'
                        % (expected_exe, secondary_exe, py_executable))
        else:
            logger.notify('Also creating executable in %s' % secondary_exe)
            shutil.copyfile(sys.executable, secondary_exe)
            make_exe(secondary_exe)

    if 'Python.framework' in prefix:
        logger.debug('MacOSX Python framework detected')

        # Make sure we use the the embedded interpreter inside
        # the framework, even if sys.executable points to
        # the stub executable in ${sys.prefix}/bin
        # See http://groups.google.com/group/python-virtualenv/
        #                              browse_thread/thread/17cab2f85da75951
        original_python = os.path.join(
            prefix, 'Resources/Python.app/Contents/MacOS/Python')
        shutil.copy(original_python, py_executable)

        # Copy the framework's dylib into the virtual
        # environment
        virtual_lib = os.path.join(home_dir, '.Python')

        if os.path.exists(virtual_lib):
            os.unlink(virtual_lib)
        copyfile(
            os.path.join(prefix, 'Python'),
            virtual_lib)

        # And then change the install_name of the copied python executable
        try:
            call_subprocess(
                ["install_name_tool", "-change",
                 os.path.join(prefix, 'Python'),
                 '@executable_path/../.Python',
                 py_executable])
        except:
            logger.fatal(
                "Could not call install_name_tool -- you must have Apple's development tools installed")
            raise

        # Some tools depend on pythonX.Y being present
        py_executable_version = '%s.%s' % (
            sys.version_info[0], sys.version_info[1])
        if not py_executable.endswith(py_executable_version):
            # symlinking pythonX.Y > python
            pth = py_executable + '%s.%s' % (
                    sys.version_info[0], sys.version_info[1])
            if os.path.exists(pth):
                os.unlink(pth)
            os.symlink('python', pth)
        else:
            # reverse symlinking python -> pythonX.Y (with --python)
            pth = join(bin_dir, 'python')
            if os.path.exists(pth):
                os.unlink(pth)
            os.symlink(os.path.basename(py_executable), pth)

    if sys.platform == 'win32' and ' ' in py_executable:
        # There's a bug with subprocess on Windows when using a first
        # argument that has a space in it.  Instead we have to quote
        # the value:
        py_executable = '"%s"' % py_executable
    cmd = [py_executable, '-c', 'import sys; print sys.prefix']
    logger.info('Testing executable with %s %s "%s"' % tuple(cmd))
    proc = subprocess.Popen(cmd,
                            stdout=subprocess.PIPE)
    proc_stdout, proc_stderr = proc.communicate()
    proc_stdout = os.path.normcase(os.path.abspath(proc_stdout.strip()))
    if proc_stdout != os.path.normcase(os.path.abspath(home_dir)):
        logger.fatal(
            'ERROR: The executable %s is not functioning' % py_executable)
        logger.fatal(
            'ERROR: It thinks sys.prefix is %r (should be %r)'
            % (proc_stdout, os.path.normcase(os.path.abspath(home_dir))))
        logger.fatal(
            'ERROR: virtualenv is not compatible with this system or executable')
        if sys.platform == 'win32':
            logger.fatal(
                'Note: some Windows users have reported this error when they installed Python for "Only this user".  The problem may be resolvable if you install Python "For all users".  (See https://bugs.launchpad.net/virtualenv/+bug/352844)')
        sys.exit(100)
    else:
        logger.info('Got sys.prefix result: %r' % proc_stdout)

    pydistutils = os.path.expanduser('~/.pydistutils.cfg')
    if os.path.exists(pydistutils):
        logger.notify('Please make sure you remove any previous custom paths from '
                      'your %s file.' % pydistutils)
    ## FIXME: really this should be calculated earlier
    return py_executable

def install_activate(home_dir, bin_dir, prompt=None):
    if sys.platform == 'win32' or is_jython and os._name == 'nt':
        files = {'activate.bat': ACTIVATE_BAT,
                 'deactivate.bat': DEACTIVATE_BAT}
        if os.environ.get('OS') == 'Windows_NT' and os.environ.get('OSTYPE') == 'cygwin':
            files['activate'] = ACTIVATE_SH
    else:
        files = {'activate': ACTIVATE_SH}

        # suppling activate.fish in addition to, not instead of, the
        # bash script support.
        files['activate.fish'] = ACTIVATE_FISH

        # same for csh/tcsh support...
        files['activate.csh'] = ACTIVATE_CSH



    files['activate_this.py'] = ACTIVATE_THIS
    vname = os.path.basename(os.path.abspath(home_dir))
    for name, content in files.items():
        content = content.replace('__VIRTUAL_PROMPT__', prompt or '')
        content = content.replace('__VIRTUAL_WINPROMPT__', prompt or '(%s)' % vname)
        content = content.replace('__VIRTUAL_ENV__', os.path.abspath(home_dir))
        content = content.replace('__VIRTUAL_NAME__', vname)
        content = content.replace('__BIN_NAME__', os.path.basename(bin_dir))
        writefile(os.path.join(bin_dir, name), content)

def install_distutils(home_dir):
    distutils_path = change_prefix(distutils.__path__[0], home_dir)
    mkdir(distutils_path)
    ## FIXME: maybe this prefix setting should only be put in place if
    ## there's a local distutils.cfg with a prefix setting?
    home_dir = os.path.abspath(home_dir)
    ## FIXME: this is breaking things, removing for now:
    #distutils_cfg = DISTUTILS_CFG + "\n[install]\nprefix=%s\n" % home_dir
    writefile(os.path.join(distutils_path, '__init__.py'), DISTUTILS_INIT)
    writefile(os.path.join(distutils_path, 'distutils.cfg'), DISTUTILS_CFG, overwrite=False)

def fix_lib64(lib_dir):
    """
    Some platforms (particularly Gentoo on x64) put things in lib64/pythonX.Y
    instead of lib/pythonX.Y.  If this is such a platform we'll just create a
    symlink so lib64 points to lib
    """
    if [p for p in distutils.sysconfig.get_config_vars().values()
        if isinstance(p, basestring) and 'lib64' in p]:
        logger.debug('This system uses lib64; symlinking lib64 to lib')
        assert os.path.basename(lib_dir) == 'python%s' % sys.version[:3], (
            "Unexpected python lib dir: %r" % lib_dir)
        lib_parent = os.path.dirname(lib_dir)
        assert os.path.basename(lib_parent) == 'lib', (
            "Unexpected parent dir: %r" % lib_parent)
        copyfile(lib_parent, os.path.join(os.path.dirname(lib_parent), 'lib64'))

def resolve_interpreter(exe):
    """
    If the executable given isn't an absolute path, search $PATH for the interpreter
    """
    if os.path.abspath(exe) != exe:
        paths = os.environ.get('PATH', '').split(os.pathsep)
        for path in paths:
            if os.path.exists(os.path.join(path, exe)):
                exe = os.path.join(path, exe)
                break
    if not os.path.exists(exe):
        logger.fatal('The executable %s (from --python=%s) does not exist' % (exe, exe))
        sys.exit(3)
    return exe

############################################################
## Relocating the environment:

def make_environment_relocatable(home_dir):
    """
    Makes the already-existing environment use relative paths, and takes out
    the #!-based environment selection in scripts.
    """
    home_dir, lib_dir, inc_dir, bin_dir = path_locations(home_dir)
    activate_this = os.path.join(bin_dir, 'activate_this.py')
    if not os.path.exists(activate_this):
        logger.fatal(
            'The environment doesn\'t have a file %s -- please re-run virtualenv '
            'on this environment to update it' % activate_this)
    fixup_scripts(home_dir)
    fixup_pth_and_egg_link(home_dir)
    ## FIXME: need to fix up distutils.cfg

OK_ABS_SCRIPTS = ['python', 'python%s' % sys.version[:3],
                  'activate', 'activate.bat', 'activate_this.py']

def fixup_scripts(home_dir):
    # This is what we expect at the top of scripts:
    shebang = '#!%s/bin/python' % os.path.normcase(os.path.abspath(home_dir))
    # This is what we'll put:
    new_shebang = '#!/usr/bin/env python%s' % sys.version[:3]
    activate = "import os; activate_this=os.path.join(os.path.dirname(__file__), 'activate_this.py'); execfile(activate_this, dict(__file__=activate_this)); del os, activate_this"
    if sys.platform == 'win32':
        bin_suffix = 'Scripts'
    else:
        bin_suffix = 'bin'
    bin_dir = os.path.join(home_dir, bin_suffix)
    home_dir, lib_dir, inc_dir, bin_dir = path_locations(home_dir)
    for filename in os.listdir(bin_dir):
        filename = os.path.join(bin_dir, filename)
        if not os.path.isfile(filename):
            # ignore subdirs, e.g. .svn ones.
            continue
        f = open(filename, 'rb')
        lines = f.readlines()
        f.close()
        if not lines:
            logger.warn('Script %s is an empty file' % filename)
            continue
        if not lines[0].strip().startswith(shebang):
            if os.path.basename(filename) in OK_ABS_SCRIPTS:
                logger.debug('Cannot make script %s relative' % filename)
            elif lines[0].strip() == new_shebang:
                logger.info('Script %s has already been made relative' % filename)
            else:
                logger.warn('Script %s cannot be made relative (it\'s not a normal script that starts with %s)'
                            % (filename, shebang))
            continue
        logger.notify('Making script %s relative' % filename)
        lines = [new_shebang+'\n', activate+'\n'] + lines[1:]
        f = open(filename, 'wb')
        f.writelines(lines)
        f.close()

def fixup_pth_and_egg_link(home_dir, sys_path=None):
    """Makes .pth and .egg-link files use relative paths"""
    home_dir = os.path.normcase(os.path.abspath(home_dir))
    if sys_path is None:
        sys_path = sys.path
    for path in sys_path:
        if not path:
            path = '.'
        if not os.path.isdir(path):
            continue
        path = os.path.normcase(os.path.abspath(path))
        if not path.startswith(home_dir):
            logger.debug('Skipping system (non-environment) directory %s' % path)
            continue
        for filename in os.listdir(path):
            filename = os.path.join(path, filename)
            if filename.endswith('.pth'):
                if not os.access(filename, os.W_OK):
                    logger.warn('Cannot write .pth file %s, skipping' % filename)
                else:
                    fixup_pth_file(filename)
            if filename.endswith('.egg-link'):
                if not os.access(filename, os.W_OK):
                    logger.warn('Cannot write .egg-link file %s, skipping' % filename)
                else:
                    fixup_egg_link(filename)

def fixup_pth_file(filename):
    lines = []
    prev_lines = []
    f = open(filename)
    prev_lines = f.readlines()
    f.close()
    for line in prev_lines:
        line = line.strip()
        if (not line or line.startswith('#') or line.startswith('import ')
            or os.path.abspath(line) != line):
            lines.append(line)
        else:
            new_value = make_relative_path(filename, line)
            if line != new_value:
                logger.debug('Rewriting path %s as %s (in %s)' % (line, new_value, filename))
            lines.append(new_value)
    if lines == prev_lines:
        logger.info('No changes to .pth file %s' % filename)
        return
    logger.notify('Making paths in .pth file %s relative' % filename)
    f = open(filename, 'w')
    f.write('\n'.join(lines) + '\n')
    f.close()

def fixup_egg_link(filename):
    f = open(filename)
    link = f.read().strip()
    f.close()
    if os.path.abspath(link) != link:
        logger.debug('Link in %s already relative' % filename)
        return
    new_link = make_relative_path(filename, link)
    logger.notify('Rewriting link %s in %s as %s' % (link, filename, new_link))
    f = open(filename, 'w')
    f.write(new_link)
    f.close()

def make_relative_path(source, dest, dest_is_directory=True):
    """
    Make a filename relative, where the filename is dest, and it is
    being referred to from the filename source.

        >>> make_relative_path('/usr/share/something/a-file.pth',
        ...                    '/usr/share/another-place/src/Directory')
        '../another-place/src/Directory'
        >>> make_relative_path('/usr/share/something/a-file.pth',
        ...                    '/home/user/src/Directory')
        '../../../home/user/src/Directory'
        >>> make_relative_path('/usr/share/a-file.pth', '/usr/share/')
        './'
    """
    source = os.path.dirname(source)
    if not dest_is_directory:
        dest_filename = os.path.basename(dest)
        dest = os.path.dirname(dest)
    dest = os.path.normpath(os.path.abspath(dest))
    source = os.path.normpath(os.path.abspath(source))
    dest_parts = dest.strip(os.path.sep).split(os.path.sep)
    source_parts = source.strip(os.path.sep).split(os.path.sep)
    while dest_parts and source_parts and dest_parts[0] == source_parts[0]:
        dest_parts.pop(0)
        source_parts.pop(0)
    full_parts = ['..']*len(source_parts) + dest_parts
    if not dest_is_directory:
        full_parts.append(dest_filename)
    if not full_parts:
        # Special case for the current directory (otherwise it'd be '')
        return './'
    return os.path.sep.join(full_parts)



############################################################
## Bootstrap script creation:

def create_bootstrap_script(extra_text, python_version=''):
    """
    Creates a bootstrap script, which is like this script but with
    extend_parser, adjust_options, and after_install hooks.

    This returns a string that (written to disk of course) can be used
    as a bootstrap script with your own customizations.  The script
    will be the standard virtualenv.py script, with your extra text
    added (your extra text should be Python code).

    If you include these functions, they will be called:

    ``extend_parser(optparse_parser)``:
        You can add or remove options from the parser here.

    ``adjust_options(options, args)``:
        You can change options here, or change the args (if you accept
        different kinds of arguments, be sure you modify ``args`` so it is
        only ``[DEST_DIR]``).

    ``after_install(options, home_dir)``:

        After everything is installed, this function is called.  This
        is probably the function you are most likely to use.  An
        example would be::

            def after_install(options, home_dir):
                subprocess.call([join(home_dir, 'bin', 'easy_install'),
                                 'MyPackage'])
                subprocess.call([join(home_dir, 'bin', 'my-package-script'),
                                 'setup', home_dir])

        This example immediately installs a package, and runs a setup
        script from that package.

    If you provide something like ``python_version='2.4'`` then the
    script will start with ``#!/usr/bin/env python2.4`` instead of
    ``#!/usr/bin/env python``.  You can use this when the script must
    be run with a particular Python version.
    """
    filename = __file__
    if filename.endswith('.pyc'):
        filename = filename[:-1]
    f = open(filename, 'rb')
    content = f.read()
    f.close()
    py_exe = 'python%s' % python_version
    content = (('#!/usr/bin/env %s\n' % py_exe)
               + '## WARNING: This file is generated\n'
               + content)
    return content.replace('##EXT' 'END##', extra_text)

##EXTEND##

##file site.py
SITE_PY = """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""".decode("base64").decode("zlib")

##file ez_setup.py
EZ_SETUP_PY = """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""".decode("base64").decode("zlib")

##file distribute_setup.py
DISTRIBUTE_SETUP_PY = """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""".decode("base64").decode("zlib")

##file activate.sh
ACTIVATE_SH = """
eJytVU1v4jAQPW9+xTT0ANVS1GsrDlRFAqmFqmG72m0rY5IJsRRslDiktNr/vuMQ8tFQpNU2B4I9
H36eeW/SglkgYvBFiLBKYg0LhCRGD1KhA7BjlUQuwkLIHne12HCNNpz5kVrBgsfBmdWCrUrA5VIq
DVEiQWjwRISuDreW5eE+CtodeLeAnhZEGKMGFXqAciMiJVcoNWx4JPgixDjzEj48QVeCfcqmtzfs
cfww+zG4ZfeD2ciGF7gCHaDMPM1jtvuHXAsPfF2rSGeOxV4iDY5GUGb3xVEYv2aj6WQ0vRseAlMY
G5DKsAawwnQUXt2LQOYlzZoYByqhonqoqfxZf4BLD97i4DukgXADCPgGgdOLTK5arYxZB1xnrc9T
EQFcHoZEAa1gSQioo/TPV5FZrDlxJA+NzwF+Ek1UonOzFnKZp6k5mgLBqSkuuAGXS4whJb5xz/xs
wXCHjiVerAk5eh9Kfz1wqOldtVv9dkbscfjgjKeTA8XPrtaNauX5rInOxaHuOReNtpFjo1/OxdFG
5eY9hJ3L3jqcPJbATggXAemDLZX0MNZRYjSDH7C1wMHQh73DyYfTu8a0F9v+6D8W6XNnF1GEIXW/
JrSKPOtnW1YFat9mrLJkzLbyIlTvYzV0RGXcaTBfVLx7jF2PJ2wyuBsydpm7VSVa4C4Zb6pFO2TR
huypCEPwuQjNftUrNl6GsYZzuFrrLdC9iJjQ3omAPBbcI2lsU77tUD43kw1NPZhTrnZWzuQKLomx
Rd4OXM1ByExVVkmoTwfBJ7Lt10Iq1Kgo23Bmd8Ib1KrGbsbO4Pp2yO4fpnf3s6MnZiwuiJuls1/L
Pu4yUCvhpA+vZaJvWWDTr0yFYYyVnHMqCEq+QniuYX225xmnzRENjbXACF3wkCYNVZ1mBwxoR9Iw
WAo3/36oSOTfgjwEEQKt15e9Xpqm52+oaXxszmnE9GLl65RH2OMmS6+u5acKxDmlPgj2eT5/gQOX
LLK0j1y0Uwbmn438VZkVpqlfNKa/YET/53j+99G8H8tUhr9ZSXs2
""".decode("base64").decode("zlib")

##file activate.fish
ACTIVATE_FISH = """
eJydVm1v4jgQ/s6vmA1wBxUE7X2stJVYlVWR2lK13d6d9laRk0yIr8HmbIe0++tvnIQQB9pbXT5A
Ys/LM55nZtyHx5RrSHiGsMm1gRAh1xhDwU0Kng8hFzMWGb5jBv2E69SDs0TJDdj3MxilxmzPZzP7
pVPMMl+q9bjXh1eZQ8SEkAZULoAbiLnCyGSvvV6SC7IoBcS4Nw0wjcFbvJDcjiuTswzFDpiIQaHJ
lQAjQUi1YRmUboC2uZJig8J4PaCnT5IaDcgsbm/CjinOwgx1KcUTMEhhTgV4g2B1fRk8Le8fv86v
g7v545UHpZB9rKnp+gXsMhxLunIIpwVQxP/l9c/Hq9Xt1epm4R27bva6AJqN92G4YhbMG2i+LB+u
grv71c3dY7B6WtzfLy9bePbp0taDTXSwJQJszUnnp0y57mvpPcrF7ZODyhswtd59+/jdgw+fwBNS
xLSscksUPIDqwwNmCez3PpxGeyBYg6HE0YdcWBxcKczYzuVJi5Wu915vn5oWePCCoPUZBN5B7IgV
MCi54ZDLG7TUZ0HweXkb3M5vFmSpFm/gthhBx0UrveoPpv9AJ9unIbQYdUoe21bKg2q48sPFGVwu
H+afrxd1qvclaNlRFyh1EQ2sSccEuNAGWQwysfVpz1tPajUqbqJUnEcIJkWo6OXDaodK8ZiLdbmM
L1wb+9H0D+pcyPSrX5u5kgWSygRYXCnJUi/KKcuU4cqsAyTKZBiissLc7NFwizvjxtieKBVCIdWz
fzilzPaYyljZN0cGN1v7NnaIPNCGmVy3GKuJaQ6iVjE1Qfm+36hglErwmnAD8hu0dDy4uICBA8ZV
pQr/q/+O0KFW2kjelu9Dgb9SDBsWV4F4x5CswgS0zBVlk5tDMP5bVtUGpslbm81Lu2sdKq7uNMGh
MVQ4fy9xhogC1lS5guhISa0DlBWv0O8odT6/LP+4WZzDV6FzIkEqC0uolGZSZoMnlpxplmD2euaT
O4hkTpPnbztDccey0bhjDaBIqaWQa0uwEtQEwtyU56i4fq54F9IE3ORR6mKriODM4XOYZwaVYLYz
7SPbKkz4i7VkB6/Ot1upDE3znNqYKpM8raa0Bx8vfvntJ32UENsM4aI6gJL+jJwhxhh3jVIDOcpi
m0r2hmEtS8XXXNBk71QCDXTBNhhPiHX2LtHkrVIlhoEshH/EZgdq53Eirqs5iFKMnkOmqZTtr3Xq
djvPTWZT4S3NT5aVLgurMPUWI07BRVYqkQrmtCKohNY8qu9EdACoT6ki0a66XxVF4f9AQ3W38yO5
mWmZmIIpnDFrbXakvKWeZhLwhvrbUH8fahhqD0YUcBDJjEBMQwiznE4y5QbHrbhHBOnUAYzb2tVN
jJa65e+eE2Ya30E2GurxUP8ssA6e/wOnvo3V78d3vTcvMB3n7l3iX1JXWqk=
""".decode("base64").decode("zlib")

##file activate.csh
ACTIVATE_CSH = """
eJx9U11vmzAUffevOCVRu+UB9pws29Kl0iq1aVWllaZlcgxciiViItsQdb9+xiQp+dh4QOB7Pu49
XHqY59IgkwVhVRmLmFAZSrGRNkdgykonhFiqSCRW1sJSmJg8wCDT5QrucRCyHn6WFRKhVGmhKwVp
kUpNiS3emup3TY6XIn7DVNQyJUwlrgthJD6n/iCNv72uhCzCpFx9CRkThRQGKe08cWXJ9db/yh/u
pvzl9mn+PLnjj5P5D1yM8QmXlzBkSdXwZ0H/BBc0mEo5FE5qI2jKhclHOOvy9HD/OO/6YO1mX9vx
sY0H/tPIV0dtqel0V7iZvWyNg8XFcBA0ToEqVeqOdNUEQFvN41SumAv32VtJrakQNSmLWmgp4oJM
yDoBHgoydtoEAs47r5wHHnUal5vbJ8oOI+9wI86vb2d8Nrm/4Xy4RZ8R85E4uTZPB5EZPnTaaAGu
E59J8BE2J8XgrkbLeXMlVoQxznEYFYY8uFFdxsKQRx90Giwx9vSueHP1YNaUSFG4vTaErNSYuBOF
lXiVyXa9Sy3JdClEyK1dD6Nos9mEf8iKlOpmqSNTZnYjNEWiUYn2pKNB3ttcLJ3HmYYXy6Un76f7
r8rRsC1TpTJj7f19m5sUf/V3Ir+x/yjtLu8KjLX/CmN/AcVGUUo=
""".decode("base64").decode("zlib")

##file activate.bat
ACTIVATE_BAT = """
eJyFUkEKgzAQvAfyhz0YaL9QEWpRqlSjWGspFPZQTevFHOr/adQaU1GaUzI7Mzu7ZF89XhKkEJS8
qxaKMMsvboQ+LxxE44VICSW1gEa2UFaibqoS0iyJ0xw2lIA6nX5AHCu1jpRsv5KRjknkac9VLVug
sX9mtzxIeJDE/mg4OGp47qoLo3NHX2jsMB3AiDht5hryAUOEifoTdCXbSh7V0My2NMq/Xbh5MEjU
ZT63gpgNT9lKOJ/CtHsvT99re3pX303kydn4HeyOeAg5cjf2EW1D6HOPkg9NGKhu
""".decode("base64").decode("zlib")

##file deactivate.bat
DEACTIVATE_BAT = """
eJxzSE3OyFfIT0vj4spMU0hJTcvMS01RiPf3cYkP8wwKCXX0iQ8I8vcNCFHQ4FIAguLUEgWIgK0q
FlWqXJpcICVYpGzx2BAZ4uHv5+Hv6wq1BWINXBTdKriEKkI1DhW2QAfhttcxxANiFZCBbglQSJUL
i2dASrm4rFz9XLgAwJNbyQ==
""".decode("base64").decode("zlib")

##file distutils-init.py
DISTUTILS_INIT = """
eJytV92L4zYQf9dfMU0ottuse/TeFkKh3MvC0Ydy0IdlMVpbTtR1JCMpm+T++s5Y/pBs53oPZ1hQ
pPnSb34zo5WnVhsH2jLpV/Y2Li/cKKkOFoYN3Za6ErAdFtKC0g44vEvjzrwR6h1Oujo3YgdWw0VA
yRWcLUo6cBpqqSpwRwHWVY18ZRB9W3jq3HDlfoIvqK7NG2gF7a297VANvZ3O1sGrQI/eDe5yB0ZY
WQkLUpHxhVX09NDe3FGr31BL1lJUD9f8ln+FShpROm1ujOFS8ZOAPUKRt9wd836Hjqw7O9nYgvYD
iX+1VOlMPPXQ5EVRy0YURbaDZDSQZEzWo7rS5kSLNHaQwX4RRLrQGe1nj92Fh1zltEhHDDZfEO0g
O6MraHn5xg8IpYOfLfC2FdxYShLC64EES4A0uuROYhq49Zs368RpMvTHJmOiscKHUXRXKIpcKiuM
Sz/sYHa7TkxcRYkkEhN8HZaxKCJXFFJJh+baW5JluRG8SjM20JHEA9qWWtXywBjbbvF2rjzC61k2
VSGuDibTUGlhVeLgTekLHPEP73wQrrscUsUGrPCGjkTCC1JXXyw8EJWP3FSUZY8IiSCCRp97dnfO
RUUx5a0RtbxSzLX/3XBXYxIpyQka/fh74pGrjQ5QzUt9OnFV5dMV+otOG5gQjctxozNTNtzaSSiN
JHqu0FeJmsqRN/KrKHRLGbaQWtHUgRB9FDfu5giN4eZWIDqWCv8vrcTjrNZgRXQPzy+RmGjQpLRI
EKz0UqQLlR28ciusM8jn7PtcLPZy2zbSDeyyos0iO+ybBgPyRvSk/CEFm8IndQebz8iXTRbbjhDP
5xh7iJfBrKd/Nenjj6Jvgp2B+W7AnP102BXH5IZWPV3tI2MUOvXowpdS12IIXhLLP0lKyeuZrpEv
pFhPqHg3JFTd1cceVp0EsPgGU0wFO2u4iyYRoFYfEm9kG/RZcUUBm87t9mFtx9iCtC9kx4Rt4R8a
OdgzSt40vtyFecAZZ8BfCOhCrC8djMGPFaz2Vlt5TSZCk053+37wbLDLRXfZ+F45NtdVpVWdudSC
xgODI8EsiLoTl5aO0lhoigX7GHZDHAY4LxoMIu1gXPYPksmFquxF4uRKZhEnKzXu82HESb+LlNQz
Fh/RvFJVuhK+Ee5slBdj30FcRGdJ5rhKxtkyKxWcGoV/WOCYKqkNDYJ5fNQVx3g400tpJBS2FSU+
Tco9ss8nZ08dtscGQfSby87b73fOw+4UgrEMNnY6uMzYvSDxPVPpsij6+l0/ZPfuH0Iz010giY34
HpL0ZLyLJB4ukaQRU+GwptO7yIZCQE33B0K9iCqO6X+AR4n7wAeH68DPkJzpTsD3x+/cj9LIVHC2
An1wmv7CzWHoqR02vb0VL73siP+3nkX0YbQ0l9f6WDyOm24cj3rxO2MMip6kpcu6VCefn/789PR3
0v0fg21sFIp70rj9PCi8YDRDXFucym/43qN+iENh1Jy/dIIIqF3OIkDvBMsdx+huWv8Kz73vl8g5
WQ3JOGqwu3lb4dfKKbvLigXDQsb8B/xt39Q=
""".decode("base64").decode("zlib")

##file distutils.cfg
DISTUTILS_CFG = """
eJxNj00KwkAMhfc9xYNuxe4Ft57AjYiUtDO1wXSmNJnK3N5pdSEEAu8nH6lxHVlRhtDHMPATA4uH
xJ4EFmGbvfJiicSHFRzUSISMY6hq3GLCRLnIvSTnEefN0FIjw5tF0Hkk9Q5dRunBsVoyFi24aaLg
9FDOlL0FPGluf4QjcInLlxd6f6rqkgPu/5nHLg0cXCscXoozRrP51DRT3j9QNl99AP53T2Q=
""".decode("base64").decode("zlib")

##file activate_this.py
ACTIVATE_THIS = """
eJyNUlGL2zAMfvevEBlHEujSsXsL9GGDvW1jD3sZpQQ3Ua7aJXawnbT595Ocpe0dO5ghseVP+vRJ
VpIkn2cYPZknwAvWLXWYhRP5Sk4baKgOWRWNqtpdgTyH2Y5wpq5Tug406YAgKEzkwqg7NBPwR86a
Hk0olPopaK0NHJHzYQPnE5rI0o8+yBUwiBfyQcT8mMPJGiAT0A0O+b8BY4MKJ7zPcSSzHaKrSpJE
qeDmUgGvVbPCS41DgO+6xy/OWbfAThMn/OQ9ukDWRCSLiKzk1yrLjWapq6NnvHUoHXQ4bYPdrsVX
4lQMc/q6ZW975nmSK+oH6wL42a9H65U6aha342Mh0UVDzrD87C1bH73s16R5zsStkBZDp0NrXQ+7
HaRnMo8f06UBnljKoOtn/YT+LtdvSyaT/BtIv9KR60nF9f3qmuYKO4//T9ItJMsjPfgUHqKwCZ3n
xu/Lx8M/UvCLTxW7VULHxB1PRRbrYfvWNY5S8it008jOjcleaMqVBDnUXcWULV2YK9JEQ92OfC96
1Tv4ZicZZZ7GpuEpZbbeQ7DxquVx5hdqoyFSSmXwfC90f1Dc7hjFs/tK99I0fpkI8zSLy4tSy+sI
3vMWehjQNJmE5VePlZbL61nzX3S93ZcfDqznnkb9AZ3GWJU=
""".decode("base64").decode("zlib")

if __name__ == '__main__':
    main()

## TODO:
## Copy python.exe.manifest
## Monkeypatch distutils.sysconfig

########NEW FILE########
__FILENAME__ = deprecrated_test_sysutils_helper
import sys, os
import platform
import unittest
import tempfile
import string
import random
import logging
import logging.handlers as handlers
import md5
from shutil import rmtree

# FIXME: This used to test holland.helpers which was
#        a collection of utility methods. These have
#        since been forked off into various plugins
#        or merged into holland.core.util. This
#        should be updated to test holland.core.util
#        and any other tests added to the appropriate
#        plugin egg's test suite.

#class TestSysUtilsHelper(unittest.TestCase):
# hack: disabling test until I fix
# many of these functions have been shuffled around
# into individual plugins and need merged into those
# test cases
class Test(object):
    """
    A test class for testing the sysutils helper
    """
    
    def setUp(self):
        self.log = logging.getLogger('holland')
        file = logging.FileHandler(filename='/dev/null')
        self.log.addHandler(file)
        
    
    def test_ensure_dir(self):
        # No arguments
        self.assertRaises(TypeError, h.ensure_dir);
        # Directory that already exists
        self.assertEqual(h.ensure_dir('/tmp'), True)
        # File that already exists
        self.assertEqual(h.ensure_dir('/dev/null'), True)
        # Directory that does not exist
        self.assertEqual(h.ensure_dir('/tmp/testdir'), True)
        # Directory that cannot be created
        self.assertRaises(OSError, h.ensure_dir, '/dev/null/dir')
        # Cleanup
        os.rmdir('/tmp/testdir')


    def test_protected_path(self):
        # file
        fd,file_path = tempfile.mkstemp(prefix='holland-test-')
        safe_path = h.protected_path(file_path)
        expected_path = "%s.0" % file_path
        self.assertEquals(safe_path == expected_path, True)
        
        # dir
        dir_path = tempfile.mkdtemp(prefix='holland-test-')
        safe_path = h.protected_path(dir_path)
        expected_path = "%s.0" % dir_path
        self.assertEquals(safe_path == expected_path, True)
        
        # clean up
        os.remove(file_path)
        rmtree(dir_path)
    
    def test_get_compression_stream(self):
        for c_mode in ['gzip', 'bzip2']:
            fd,file_path = tempfile.mkstemp(prefix='holland-test-')
            dir_path = tempfile.mkdtemp(prefix='holland-test-dir')        
            file_path = os.path.realpath(file_path)
            os.remove(file_path)
            dir_path = os.path.realpath(dir_path)
            data = ''
            for i in xrange(1024**2):
                data = data + random.choice(string.letters)
        
            stream = h.get_compression_stream(output_path=file_path, mode=c_mode)
            stream.write(data)
            stream.close()
        
            new_file_path = h.decompress_path(
                source_path=file_path, dest_dir=dir_path, mode=c_mode
                )

            f = open(new_file_path, 'r')
            a = md5.new(f.read()).digest()
            b = md5.new(data).digest()
            self.assertEqual(a == b, True)    
            f.close()

            # clean up 
            os.remove(new_file_path)
            rmtree(dir_path)
        
    def test_compress_path(self):
        # Test to see if a file can be gzipped and ungzipped
        # (and it returns the same md5sum)
        fd,file_path = tempfile.mkstemp(prefix='holland-test-')
        dir_path = tempfile.mkdtemp(prefix='holland-test-dir')        
        file_path = os.path.realpath(file_path)
        dir_path = os.path.realpath(dir_path)
        
        # Create and compress the file
        handle = os.fdopen(fd, 'w')
        for i in xrange(1024**2):
            handle.write(random.choice(string.letters))
        handle.close()
        comp_path = h.compress_path(
            source_path = file_path, dest_dir = dir_path, 
            remove_source = False, mode = 'gzip'
            )
                                  
        self.assertEqual(comp_path != None, True)
        
        # Uncompress the file and compare to original
        uncomp_path = h.decompress_path(
            source_path = comp_path, dest_dir = dir_path, 
            remove_source = False, mode = 'gzip'
            )
        self.assertEqual(uncomp_path != None, True)

        original_file = file(file_path)
        uncompressed_file = file(uncomp_path)
        
        a = md5.new(original_file.read()).digest()
        b = md5.new(uncompressed_file.read()).digest()
        self.assertEqual(a == b, True)
        
    
    # Platform-specific tests
    #   FIX ME:
    #       Tests are incomplete and have not been tested on Linux platform
    
    def test_mount_info(self):
        self.assertRaises(TypeError, h.mount_info)
        if platform.system() != 'Linux':
            print "Skipping Test For This Platform (%s)" % platform.system()
            return False
            
    def test_which(self):
        # No arguments given
        self.assertRaises(TypeError, h.which)
        if platform.system() == 'Windows':
            print "Skipping Test For This Platform (%s)" % platform.system()
            return False
        # Common utility test
        self.assertEqual(h.which('ls'), '/bin/ls')
        # Not found test
        self.assertRaises(OSError, h.which, 'notacommand')
        
    # FIX ME: Incomplete Test
    def test_relpath(self):
        # No arguments given
        self.assertRaises(TypeError, h.relpath)
        if platform.system() == 'Windows':
            print "Skipping Test For This Platform (%s)" % platform.system()
            return False
        # Same Path
        self.assertEqual(h.relpath('test', 'test'), '')        
        # Empty Path
        self.assertEqual(h.relpath('', ''), '')
        # Sub-Path
        self.assertEqual(h.relpath('/tmp/test', '/test'), None)
        
    # End of platform-specific tests

    def test_format_bytes(self):
        # No arguments given
        self.assertRaises(TypeError, h.format_bytes)
        # 0 bytes
        self.assertEqual(h.format_bytes(0), '0.00B')
        # 1b
        self.assertEqual(h.format_bytes(1), '1.00B')
        # 1KiB
        self.assertEqual(h.format_bytes(1024), '1.00KiB')
        # Remaing test for other units.
        # Note the + 2 since we ran the '1b' and '1KiB' tests above
        # and these were taken from the array in the original function
        units = ['MiB','GiB','TiB','PiB','EiB','ZiB','YiB']
        for unit in units:
            power = units.index(unit) + 2
            self.assertEqual(h.format_bytes(1024**power),
                            '1.00' + unit)
        # Negative Bytes
        self.assertRaises(ArithmeticError, h.format_bytes, -1);


    def tearDown(self):
        pass


def suite():
    suite = unittest.TestSuite()
    suite.addTest(unittest.makeSuite(TestSysUtilsHelper))
    return suite

if __name__ == '__main__':
    unittest.main()
    unittest.TextTestRunner(verbosity=3).run(suite())

########NEW FILE########
__FILENAME__ = test_bootstrap
import os
import shutil
import tempfile
import unittest
from holland.core.config import setup_config
from holland.core.log import clear_root_handlers
from holland.core.util.bootstrap import bootstrap, setup_logging
from holland.core.util.fmt import format_loglevel
from optparse import OptionParser

class TestBootstrap(unittest.TestCase):
    def setUp(self):
        clear_root_handlers()
        # This is tested in test_config.py
        self.tmpdir = tempfile.mkdtemp()
        log_file = os.path.join(self.tmpdir, 'holland.log')
        test_cfg = """
        [holland]
        plugin_dirs = /usr/share/holland/plugins
        backupsets = default
        umask = 0007
        path = /bin:/usr/bin:/usr/local/bin:/usr/local/sbin:/usr/local/mysql/bin

        [logging]
        level = critical
        filename = %s
        """ % (log_file)
        path = os.path.join(self.tmpdir, 'holland.conf')
        open(path, 'w').write(test_cfg)
        setup_config(path)

    def test_log_level(self):
        """
        Test for issue #1323 - ensure log level is set according to holland.conf
        """
        p = OptionParser()
        p.add_option('--verbose', '-v', action='store_true', default=False)
        p.add_option('--quiet', '-q', action='store_true', default=True)
        p.add_option('--log-level', '-l', type='choice',
                     choices=['critical', 'error', 'warning', 'info', 'debug'])
        opts, args = p.parse_args(['test'])
        setup_logging(opts)
        import logging
        self.assertEquals(logging.getLogger().getEffectiveLevel(), logging.CRITICAL)

    def test_backupset(self):
        pass

    def test_provider(self):
        pass

    def tearDown(self):
        pass

########NEW FILE########
__FILENAME__ = test_config
import os
import shutil
import tempfile
import unittest
from holland.core.config import hollandcfg, setup_config

class TestHollandConfig(unittest.TestCase):
    def setUp(self):
        test_cfg = """
        [holland]
        plugin_dirs = /usr/share/holland/plugins
        backupsets = default
        umask = 0007
        path = /bin:/usr/bin:/usr/local/bin:/usr/local/sbin:/usr/local/mysql/bin

        [logging]
        level = info
        filename = /dev/null
        """
        self.tmpdir = tempfile.mkdtemp()
        path = os.path.join(self.tmpdir, 'holland.conf')
        open(path, 'w').write(test_cfg)
        setup_config(path)

    def test_globalconfig(self):
        import logging
        cfgentry_tests = {
            'holland.plugin-dirs' : ['/usr/share/holland/plugins'],
            'holland.umask' : int('0007', 8),
            'holland.path' : '/bin:/usr/bin:/usr/local/bin:/usr/local/sbin:/usr/local/mysql/bin',

            'logging.level' : logging.INFO,
            'logging.filename' : '/dev/null'
        }

        for key, value in cfgentry_tests.items():
            self.assertEqual(hollandcfg.lookup(key), value)

    def test_backupset(self):
        pass

    def test_provider(self):
        pass

    def tearDown(self):
        shutil.rmtree(self.tmpdir)

########NEW FILE########
__FILENAME__ = run
import os, sys
import shlex, subprocess
import shutil
import ConfigParser

# Functions
def execute(cmd, config):
    returnCode = subprocess.call(cmd, 
        stdout=config.get('global', 'output_log'), 
        stderr=config.get('global', 'error_log'))
    if returnCode != 0:
        print "Failure is not an option! Except when it is"
        sys.exit(returnCode)
    return

# Setup
pwd = os.getcwd()
config = ConfigParser.ConfigParser()
config.read('config/test.conf')
outputLog = open(config.get('global', 'output_log'), 'w')
errorLog = open(config.get('global', 'error_log'), 'w')

# Create a sandbox
subprocess.call([
    'make_sandbox', 
    config.get('sandbox','tarball'),
    '--upper_directory=' + pwd + '/' + config.get('sandbox','upper_directory'),
    '--sandbox_directory=' + config.get('sandbox','sandbox_directory'),
    '--datadir_from=' + config.get('sandbox', 'datadir_from'),
    '--sandbox_port=' + config.get('sandbox', 'port'),
    '--db_user=' + config.get('sandbox', 'user'),
    '--db_password=' + config.get('sandbox', 'password'),
    '--no_confirm'
    ], 
    stdout=outputLog,
    stderr=errorLog)
subprocess.call([pwd + '/sandbox/mysql/start'], stdout=outputLog, stderr=errorLog)

# Run Maatkit before backup
maatkit_output = open('results/before-restore.mkt', 'w')
subprocess.call(shlex.split('mk-table-checksum localhost' +
    ' --port=' + config.get('sandbox', 'port') +
    ' --user=' + config.get('sandbox', 'user') +
    ' --password=' + config.get('sandbox', 'password')),
    stdout=maatkit_output,
    stderr=errorLog)
maatkit_output.close()

# Setup Holland virtual environment
if not os.path.exists(config.get('global', 'holland_install_dir')):
    os.mkdir(config.get('global', 'holland_install_dir'))
holland_path = pwd + '/' + config.get('global', 'holland_install_dir')
status = subprocess.call(['virtualenv', holland_path]) 
os.environ['PATH'] = os.path.join(holland_path, 'bin') + ':' + os.environ['PATH']
subprocess.call(['python', 'setup.py', 'install'],
    cwd=config.get('global', 'holland_repository'),
    stdout=outputLog,
    stderr=errorLog)
plugin_dir=config.get('global', 'holland_repository') + '/plugins'
for plugin in open(plugin_dir + '/ACTIVE', 'r'):
    subprocess.call(['python', 'setup.py', 'install'], 
        cwd=plugin_dir + '/' + plugin.strip(),
        stdout=outputLog,
        stderr=errorLog)

# Copy Config Files
running_config_dir=config.get('global', 'holland_install_dir') + '/etc/holland'
if not os.path.exists(config.get('global', 'holland_install_dir') + '/etc'):
    os.mkdir(config.get('global', 'holland_install_dir') + '/etc')
if not os.path.exists(config.get('global', 'holland_install_dir') + '/etc/holland'):
    os.mkdir(config.get('global', 'holland_install_dir') + '/etc/holland')
shutil.copytree(
    config.get('global', 'backupset_config_dir'),
    running_config_dir + '/backupsets')
shutil.copytree(
    config.get('global', 'provider_config_dir'),
    running_config_dir + '/providers')

# Generate holland.conf from main test config
holland_config = ConfigParser.ConfigParser()
holland_config.add_section('holland')
for item, value in config.items('holland'):
    holland_config.set('holland', item, value)
holland_config_file = open(running_config_dir + '/holland.conf', 'w')
holland_config.write(holland_config_file)
holland_config_file.close()

# Back some shit up, w00!
holland_path = pwd + '/' + config.get('global', 'holland_install_dir')
subprocess.call(['virtualenv', holland_path])
subprocess.call(shlex.split(
    'bin/holland --config-file=' + 
    'etc/holland/holland.conf bk'),
    cwd=holland_path)



# Cleanup
subprocess.call([pwd + '/sandbox/mysql/stop'])
shutil.rmtree(pwd + '/sandbox/mysql')
shutil.rmtree(pwd + '/' + config.get('global', 'holland_install_dir'))
outputLog.close() 
errorLog.close()


########NEW FILE########
__FILENAME__ = tomsay
#! /usr/bin/python

import sys

tom = """              \       _____
               \     /     \\
                \   | (o|o) |
                 \  |   _\  |
                    | '---' |
                     \_____/
           _\___    ___| |____       
             |_/\  /   '-'    \    @ @@ @
               \ \|  .GOOGLE.  |     @@@
                \   /       |  |   @
                 \ /|       |  |   @@
                    |_______|_/  @
                    |       |_\/
                    |  .-.  |
                    |  | |  |
                    '._| |_.'
                    /__| |__\\
"""

MSG_WIDTH = 40

def split_lines(words):
    full_lines = len(words)/MSG_WIDTH
    line = 0
    msg_list = ['']
    for word in words:
        line_len = len(msg_list[line])
        if line_len > MSG_WIDTH:
            msg_list.append('')
            line = line + 1
        msg_list[line] = ' '.join((msg_list[line], word)).strip()
    return msg_list

def add_spaces(length, string):
    return string + (' ' * (length - len(string)))

def build_bubble(msg):
    lines = split_lines(msg)
    longest_line = 0
    for line in lines:
        if len(line) > longest_line:
            longest_line = len(line)
    s = [' ' + ('_'*longest_line) + '_' + ' ']
    for line in lines:
        if line == lines[0]:
            s.append("/ " + add_spaces(longest_line, line) + " \\")
        elif line == lines[-1]:
            s.append("\\ " + add_spaces(longest_line, line) + " /")
        else:
            s.append("| " + add_spaces(longest_line, line) + " |")
    s.append(' ' + ('-'*longest_line) + '-' + ' ')
    return '\n'.join(s)


def main():
    msg = sys.argv[1:]
    bubble = build_bubble(msg)
    print bubble + '\n' + tom + '\n'

if __name__ == "__main__":
    main()

########NEW FILE########

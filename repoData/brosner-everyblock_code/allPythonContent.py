__FILENAME__ = admin
from django.contrib.admin import ModelAdmin, site
from ebblog.blog.models import Entry

class EntryAdmin(ModelAdmin):
    list_display = ('pub_date', 'headline', 'author')

site.register(Entry, EntryAdmin)

########NEW FILE########
__FILENAME__ = feeds
from django.contrib.syndication.feeds import Feed
from django.utils.feedgenerator import Rss201rev2Feed
from ebblog.blog.models import Entry

# RSS feeds powered by Django's syndication framework use MIME type
# 'application/rss+xml'. That's unacceptable to us, because that MIME type
# prompts users to download the feed in some browsers, which is confusing.
# Here, we set the MIME type so that it doesn't do that prompt.
class CorrectMimeTypeFeed(Rss201rev2Feed):
    mime_type = 'application/xml'

# This is a django.contrib.syndication.feeds.Feed subclass whose feed_type
# is set to our preferred MIME type.
class BlogFeed(Feed):
    feed_type = CorrectMimeTypeFeed

class BlogEntryFeed(Feed):
    title = ""
    link = ""
    description = ""

    def items(self):
        return Entry.objects.order_by('-pub_date')[:10]

    def item_link(self, item):
        return item.url()

    def item_pubdate(self, item):
        return item.pub_date

########NEW FILE########
__FILENAME__ = models
from django.db import models

class Entry(models.Model):
    pub_date = models.DateTimeField()
    author = models.CharField(max_length=32, help_text='Use the full name, e.g., "John Lennon".')
    slug = models.CharField(max_length=32)
    headline = models.CharField(max_length=255)
    summary = models.TextField(help_text='Use plain text (no HTML).')
    body = models.TextField(help_text='Use raw HTML, including &lt;p&gt; tags.')

    class Meta:
        verbose_name_plural = 'entries'

    def __unicode__(self):
        return self.headline

    def url(self):
        return "/%s/%s/" % (self.pub_date.strftime("%Y/%b/%d").lower(), self.slug)

########NEW FILE########
__FILENAME__ = manage
#!/usr/bin/env python
from django.core.management import execute_manager
import settings_devel

if __name__ == "__main__":
    execute_manager(settings_devel)

########NEW FILE########
__FILENAME__ = settings
import os

DATABASE_ENGINE = 'sqlite3'
DATABASE_NAME = '/tmp/ebblog.db'
DATABASE_USER = ''
DATABASE_HOST = ''
DATABASE_PORT = ''

ROOT_URLCONF = 'ebblog.urls'

INSTALLED_APPS = (
    'django.contrib.admin',
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'ebblog.blog',
)

TEMPLATE_DIRS = (
    os.path.normpath(os.path.join(os.path.dirname(__file__), 'templates')),
)

########NEW FILE########
__FILENAME__ = urls
from django.conf.urls.defaults import *
from django.contrib.syndication.views import feed as feed_view
from django.views.generic import date_based, list_detail
from django.contrib import admin
from ebblog.blog.models import Entry
from ebblog.blog import feeds

admin.autodiscover()

info_dict = {
    'queryset': Entry.objects.order_by('pub_date'),
    'date_field': 'pub_date',
}

FEEDS = {
    'rss': feeds.BlogEntryFeed,
}

urlpatterns = patterns('',
    (r'^(?P<year>\d{4})/(?P<month>[a-z]{3})/(?P<day>\w{1,2})/(?P<slug>\w+)/$', date_based.object_detail, dict(info_dict, slug_field='slug')),
    (r'^(?P<year>\d{4})/(?P<month>[a-z]{3})/(?P<day>\w{1,2})/$', date_based.archive_day, info_dict),
    (r'^(?P<year>\d{4})/(?P<month>[a-z]{3})/$', date_based.archive_month, info_dict),
    (r'^(?P<year>\d{4})/$', date_based.archive_year, info_dict),
    (r'^(rss)/$', feed_view, {'feed_dict': FEEDS}),
    (r'^archives/', list_detail.object_list, {'queryset': Entry.objects.order_by('-pub_date'), 'template_name': 'blog/archive.html'}),
    (r'^$', date_based.archive_index, dict(info_dict, template_name='homepage.html')),
    ('^admin/', include(admin.site.urls)),
)

########NEW FILE########
__FILENAME__ = auto_purge
from ebdata.blobs.models import Page, IgnoredDateline
from ebdata.nlp.datelines import guess_datelines
from ebpub.streets.models import Suburb
import re

def dateline_should_be_purged(dateline):
    dateline = dateline.upper()
    try:
        IgnoredDateline.objects.get(dateline=dateline)
        return True
    except IgnoredDateline.DoesNotExist:
        pass
    try:
        Suburb.objects.get(normalized_name=dateline)
        return True
    except Suburb.DoesNotExist:
        pass
    return False

def all_relevant_datelines():
    """
    Prints all datelines that are in articles but not in ignored_datelines,
    for all unharvested Pages in the system.
    """
    seen = {}
    for page in Page.objects.filter(has_addresses__isnull=True, is_pdf=False):
        for bit in page.mine_page():
            for dateline in guess_datelines(bit):
                dateline = dateline.upper()
                if dateline not in seen and not dateline_should_be_purged(dateline):
                    print dateline
                    seen[dateline] = 1

def page_should_be_purged(paragraph_list):
    """
    Returns a tuple of (purge, reason). purge is True if the given list of
    strings can be safely purged. reason is a string.
    """
    datelines = []
    for para in paragraph_list:
        datelines.extend(guess_datelines(para))
    if datelines:
        dateline_text = ', '.join([str(d) for d in datelines])
        if not [d for d in datelines if not dateline_should_be_purged(d)]:
            return (True, 'Dateline(s) %s safe to purge' % dateline_text)
        else:
            return (False, 'Dateline(s) %s found but not safe to purge' % dateline_text)
    return (False, 'No datelines')

########NEW FILE########
__FILENAME__ = create_seeds
from ebdata.blobs.models import Seed
from ebpub.db.models import Schema

def create_rss_seed(url, base_url, rss_full_entry, pretty_name, guess_article_text=True, strip_noise=False):
    if rss_full_entry:
        guess_article_text = strip_noise = False
    if 'www.' in base_url:
        normalize_www = 2
    else:
        normalize_www = 1
    Seed.objects.create(
        url=url,
        base_url=base_url,
        delay=3,
        depth=1,
        is_crawled=False,
        is_rss_feed=True,
        is_active=True,
        rss_full_entry=rss_full_entry,
        normalize_www=normalize_www,
        pretty_name=pretty_name,
        schema=Schema.objects.get(slug='news-articles'),
        autodetect_locations=True,
        guess_article_text=guess_article_text,
        strip_noise=strip_noise,
        city='',
    )

########NEW FILE########
__FILENAME__ = geotagging
from django.conf import settings
from ebdata.blobs.auto_purge import page_should_be_purged
from ebdata.blobs.models import Page
from ebdata.nlp.addresses import parse_addresses
from ebpub.db.models import NewsItem, SchemaField, Lookup
from ebpub.geocoder import SmartGeocoder, AmbiguousResult, DoesNotExist, InvalidBlockButValidStreet
from ebpub.geocoder.parser.parsing import normalize, ParsingError
from ebpub.streets.models import Suburb
from ebpub.utils.text import slugify, smart_excerpt
import datetime
import time


def save_locations_for_page(p):
    """
    Given a Page object, this function parses the text, finds all valid
    locations and creates a NewsItem for each location.
    """
    paragraph_list = p.auto_excerpt()
    do_purge, no_purge_reason = page_should_be_purged(paragraph_list)
    robot_report = [no_purge_reason]
    if do_purge:
        p.set_no_locations(geocoded_by='confidentrobot')
    else:
        if p.seed.autodetect_locations:
            if not p.article_headline:
                return
            if not p.article_date:
                return

            # Add a paragraph of the article's headline so that we find any/all
            # addresses in the headline, too.
            paragraph_list = [p.article_headline] + paragraph_list

            locations, location_report = auto_locations(paragraph_list, p.seed.city)
            if location_report:
                robot_report.append(location_report)

            if locations:
                # Check for existing NewsItems with this exact pub_date,
                # headline, location_name and source.
                do_geotag = True
                try:
                    source_schemafield = SchemaField.objects.get(schema__id=p.seed.schema_id, name='source')
                except SchemaField.DoesNotExist:
                    pass
                else:
                    existing_newsitems = NewsItem.objects.filter(schema__id=p.seed.schema_id,
                        pub_date=p.article_date, title=p.article_headline,
                        location_name=locations[0][0]).by_attribute(source_schemafield, p.seed.pretty_name, is_lookup=True).count()
                    if existing_newsitems:
                        robot_report.append('article appears to exist already')
                        do_geotag = False
                if do_geotag:
                    geotag_page(p.id, p.seed.pretty_name, p.seed.schema, p.url,
                        locations, p.article_headline, p.article_date)
            p.has_addresses = bool(locations)
            p.when_geocoded = datetime.datetime.now()
            p.geocoded_by = 'robot'
        p.robot_report = '; '.join(robot_report)[:255]
    p.save()

def geotag_page(page_id, source, schema, url, data_tuples, article_headline, article_date):
    """
    Given a Page ID and a list of (location, wkt, excerpt, block) tuples
    representing the addresses in the page, creates a NewsItem for each
    address. Returns a list of all created NewsItems.
    """
    if not data_tuples:
        return
    if not source:
        raise ValueError('Provide a source')
    if not url:
        raise ValueError('Provide a URL')
    if not article_headline:
        raise ValueError('Provide an article headline')
    if not article_date:
        raise ValueError('Provide an article date')
    if not isinstance(article_date, datetime.date):
        article_date = datetime.date(*time.strptime(article_date, '%Y-%m-%d')[:3])

    # If this schema has a "source" SchemaField, then get or create it.
    try:
        sf = SchemaField.objects.get(schema__id=schema.id, name='source')
    except SchemaField.DoesNotExist:
        source = None
    else:
        try:
            source = Lookup.objects.get(schema_field__id=sf.id, code=source)
        except Lookup.DoesNotExist:
            source = Lookup.objects.create(
                schema_field_id=sf.id,
                name=source,
                code=source,
                slug=slugify(source)[:32],
                description=''
            )
    ni_list = []
    for location, wkt, excerpt, block in data_tuples:
        description = excerpt = excerpt.replace('\n', ' ')
        if source is not None:
            # u'\u2014' is an em dash.
            description = u'%s \u2014 %s' % (source.name, description)
        ni = NewsItem.objects.create(
            schema=schema,
            title=article_headline,
            description=description,
            url=url,
            pub_date=article_date,
            item_date=article_date,
            location=wkt,
            location_name=location,
            block=block,
        )
        atts = {'page_id': page_id, 'excerpt': excerpt}
        if source is not None:
            atts['source'] = source.id
        ni.attributes = atts
        ni_list.append(ni)
    return ni_list

def auto_locations(paragraph_list, default_city=''):
    """
    Given a list of strings, detects all valid, unique addresses and returns a
    tuple (result, report), where result is a list of tuples in the format
    (address, wkt, excerpt, block) and report is a string of what happened.

    If default_city is given, it will be used in the geocoding for detected
    addresses that don't specify a city.
    """
    result, report = [], []
    addresses_seen = set()
    geocoder = SmartGeocoder()
    for para in paragraph_list:
        for addy, city in parse_addresses(para):
            # Skip addresses if they have a city that's a known suburb.
            if city and Suburb.objects.filter(normalized_name=normalize(city)).count():
                report.append('got suburb "%s, %s"' % (addy, city))
                continue

            # Try geocoding the address. If a city was provided, first try
            # geocoding with the city, then fall back to just the address
            # (without the city).
            point = None
            attempts = [addy]
            if default_city:
                attempts.insert(0, '%s, %s' % (addy, default_city))
            if city and city.lower() != default_city.lower():
                attempts.insert(0, '%s, %s' % (addy, city))
            for attempt in attempts:
                try:
                    point = geocoder.geocode(attempt)
                    break
                except AmbiguousResult:
                    report.append('got ambiguous address "%s"' % attempt)
                    # Don't try any other address attempts, because they only
                    # get *more* ambiguous. Plus, the subsequent attempts could
                    # be incorrect. For example, with this:
                    #    addy = '100 Broadway'
                    #    city = 'Manhattan'
                    #    default_city = 'Brooklyn'
                    # There are multiple "100 Broadway" addresses in Manhattan,
                    # so geocoding should fail at this point. It should not
                    # roll back to try the default_city (Brooklyn).
                    break
                except (DoesNotExist, InvalidBlockButValidStreet):
                    report.append('got nonexistent address "%s"' % attempt)
                except ParsingError:
                    report.append('got parsing error "%s"' % attempt)
            if point is None:
                continue # This address could not be geocoded.

            if point['address'] in addresses_seen:
                continue
            if len(para) > 300:
                try:
                    excerpt = smart_excerpt(para, addy)
                except ValueError:
                    excerpt = para
            else:
                excerpt = para
            result.append((addy, point['point'], excerpt, point['block']))
            addresses_seen.add(point['address'])
    return (result, '; '.join(report))

def save_locations_for_ungeocoded_pages():
    for p in Page.objects.filter(when_geocoded__isnull=True).iterator():
        save_locations_for_page(p)

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    save_locations_for_ungeocoded_pages()

########NEW FILE########
__FILENAME__ = manual
"""
Helper functions for manually adding news article NewsItems.
"""

from ebdata.blobs.models import Seed, Page
from ebdata.retrieval import UnicodeRetriever
from ebpub.db.models import Schema
from ebpub.geocoder import SmartGeocoder
from geotagging import geotag_page # relative import
import datetime

def add_newsitem(seed_url, seed_name, url, article_headline, article_date, name_excerpts):
    schema = Schema.objects.get(slug='news-articles')
    geocoder = SmartGeocoder()
    try:
        s = Seed.objects.get(url=seed_url)
    except Seed.DoesNotExist:
        s = Seed.objects.create(
            url=seed_url,
            base_url=seed_url,
            delay=0,
            depth=0,
            is_crawled=False,
            is_rss_feed=False,
            is_active='t',
            rss_full_entry=False,
            normalize_www=3,
            pretty_name=seed_name,
            schema=schema,
            autodetect_locations=True,
            guess_article_text=False,
            strip_noise=False,
            city='',
        )
    try:
        p = Page.objects.get(url=url)
    except Page.DoesNotExist:
        html = UnicodeRetriever().get_html(url)
        p = Page.objects.create(
            seed=s,
            url=url,
            scraped_url=url,
            html=html,
            when_crawled=datetime.datetime.now(),
            is_article=True,
            is_pdf=False,
            is_printer_friendly=False,
            article_headline=article_headline,
            article_date=article_date,
            has_addresses=None,
            when_geocoded=None,
            geocoded_by='',
            times_skipped=0,
            robot_report=''
        )
    data_tuples = []
    for location_name, excerpt in name_excerpts:
        point = geocoder.geocode(location_name) # Let exceptions bubble up.
        data_tuples.append((location_name, point['point'], excerpt, point['block']))
    return geotag_page(p.id, seed_name, schema, url, data_tuples, article_headline, article_date)

########NEW FILE########
__FILENAME__ = models
from ebpub.db.models import Schema
from django.db import models
import datetime

class Seed(models.Model):
    url = models.CharField(max_length=512)
    base_url = models.CharField(max_length=512) # e.g., 'http://www.suntimes.com/'
    delay = models.SmallIntegerField()
    depth = models.SmallIntegerField()
    is_crawled = models.BooleanField()
    is_rss_feed = models.BooleanField()
    is_active = models.BooleanField()
    rss_full_entry = models.BooleanField() # If True, then an RSS <entry> contains the whole article.
    normalize_www = models.SmallIntegerField() # 1 = Remove www, 2 = Add www, 3 = Ignore subdomain
    pretty_name = models.CharField(max_length=128) # e.g., 'Chicago Sun-Times'
    schema = models.ForeignKey(Schema) # news-articles, missed-connections, etc.

    # If True, then Pages from this Seed will be automatically address-detected.
    autodetect_locations = models.BooleanField()

    # If True, then robot will use templatemaker.articletext.article_text() to
    # determine Page excerpts.
    guess_article_text = models.BooleanField()

    # If True, then robot will use templatemaker.clean.strip_template() to
    # determine Page excerpts.
    strip_noise = models.BooleanField()

    # An uppercase string of the city that this seed covers -- e.g., 'BROOKLYN'.
    # If given, this city will be used to disambiguate addresses in automatic
    # geocoding.
    city = models.CharField(max_length=64, blank=True)

    def __unicode__(self):
        return self.url

class PageManager(models.Manager):
    def increment_skip(self, page_id):
        # Use this to increment the 'times_skipped' column atomically.
        # I.e., it's better to use this than to call save() on Page objects,
        # because that introduces the possibility of clashes.
        from django.db import connection
        cursor = connection.cursor()
        cursor.execute("UPDATE %s SET times_skipped = times_skipped + 1 WHERE id = %%s" % Page._meta.db_table, (page_id,))
        connection._commit()

    def next_ungeocoded(self, seed_id):
        "Returns the next ungeocoded Page for the given seed_id."
        try:
            return self.select_related().filter(has_addresses__isnull=True, is_article=True, seed__id=seed_id).order_by('times_skipped', 'when_crawled')[0]
        except IndexError:
            raise self.model.DoesNotExist

class Page(models.Model):
    seed = models.ForeignKey(Seed)

    # The publicly displayed URL for this page.
    url = models.CharField(max_length=512, db_index=True)

    # The URL that we actually scraped for this page (possibly a
    # printer-friendly version).
    scraped_url = models.CharField(max_length=512)

    html = models.TextField()
    when_crawled = models.DateTimeField()

    # Whether this page is an "article," as opposed to some sort of index page.
    is_article = models.NullBooleanField()

    # Whether this page is the extracted text of a PDF.
    is_pdf = models.BooleanField()

    # Whether this page is the printer-friendly version.
    is_printer_friendly = models.BooleanField()

    article_headline = models.CharField(max_length=255, blank=True)
    article_date = models.DateField(blank=True, null=True)

    # True = addresses were found
    # False = addresses were not found
    # None = page has not yet been examined
    has_addresses = models.NullBooleanField()

    when_geocoded = models.DateTimeField(blank=True, null=True)
    geocoded_by = models.CharField(max_length=32, blank=True)

    # The number of times this page has been "skipped" in the blob geocoder.
    times_skipped = models.SmallIntegerField()

    robot_report = models.CharField(max_length=255, blank=True)

    objects = PageManager()

    def __unicode__(self):
        return u'%s scraped %s' % (self.url, self.when_crawled)

    def set_no_locations(self, geocoded_by='robot'):
        """
        Marks this Page as geocoded with no locations. Does NOT save it.
        """
        self.has_addresses = False
        self.when_geocoded = datetime.datetime.now()
        self.geocoded_by = geocoded_by
    set_no_locations.alters_data = True

    def mine_page(self):
        """
        Runs templatemaker on this Page and returns the raw mined content, as
        a list of strings.
        """
        from ebdata.templatemaker.webmining import mine_page
        try:
            other_page = self.companion_page()
        except IndexError:
            return [self.html]
        return mine_page(self.html, [other_page.html])

    def auto_excerpt(self):
        """
        Attempts to detect the text of this page (ignoring all navigation and
        other clutter), returning a list of strings. Each string represents a
        paragraph.
        """
        from ebdata.textmining.treeutils import make_tree
        tree = make_tree(self.html)
        if self.seed.rss_full_entry:
            from ebdata.templatemaker.textlist import html_to_paragraph_list
            paras = html_to_paragraph_list(tree)
        else:
            if self.seed.strip_noise:
                from ebdata.templatemaker.clean import strip_template
                try:
                    html2 = self.companion_page().html
                except IndexError:
                    pass
                else:
                    tree2 = make_tree(html2)
                    strip_template(tree, tree2)
            if self.seed.guess_article_text:
                from ebdata.templatemaker.articletext import article_text
                paras = article_text(tree)
            else:
                from ebdata.templatemaker.textlist import html_to_paragraph_list
                paras = html_to_paragraph_list(tree)
        return paras

    def companion_page(self):
        """
        Returns another Page for self.seed, for use in a templatemaker
        duplicate-detection algorithm. Raises IndexError if none exist.
        """
        # To avoid the problem of site redesigns affecting the layout, get an
        # example page that was crawled *just before* the current Page.
        try:
            return Page.objects.filter(seed__id=self.seed_id, is_article=True,
                when_crawled__lt=self.when_crawled, is_pdf=False).order_by('-when_crawled')[0]
        except IndexError:
            # If no pages were crawled directly before this one, then get the page
            # that was crawled directly *after* this one.
            return Page.objects.filter(seed__id=self.seed_id, is_article=True,
                when_crawled__gt=self.when_crawled, is_pdf=False).order_by('when_crawled')[0]

    def newsitems(self):
        """
        Returns a list of {excerpt, location_name} dictionaries for every
        location found in this Page, or an empty list if it has no addresses.
        """
        from ebpub.db.models import Attribute, SchemaField
        if not self.has_addresses:
            return []
        # First, figure out the SchemaFields.
        real_names = dict([(sf.name, sf.real_name.encode('utf8')) for sf in SchemaField.objects.filter(schema__id=self.seed.schema_id, name__in=('excerpt', 'page_id'))])
        return [{'id': att.news_item_id, 'url': att.news_item.item_url_with_domain(), 'excerpt': getattr(att, real_names['excerpt']), 'location_name': att.news_item.location_name} \
            for att in Attribute.objects.select_related().filter(**{real_names['page_id']: self.id, 'schema__id': self.seed.schema_id})]

# Datelines that should be ignored by the blob updater.
class IgnoredDateline(models.Model):
    dateline = models.CharField(max_length=255, unique=True)

    def __unicode__(self):
        return self.dateline

########NEW FILE########
__FILENAME__ = scrapers
"""
Generic scrapers that create Pages based on some common Web site patterns.
"""

from django.conf import settings
from django.utils.html import strip_tags
from ebdata.blobs.geotagging import save_locations_for_page
from ebdata.blobs.models import Seed, Page
from ebdata.retrieval import UnicodeRetriever, RetrievalError
from ebdata.retrieval import log # Register the logging hooks.
from ebpub.utils.dates import parse_date
import datetime
import logging

class NoPagesYet(Exception):
    pass

class NoSeedYet(Exception):
    pass

class SpecializedCrawler(object):
    """
    Base class for Page crawlers.
    """

    schema = None
    seed_url = None
    date_headline_re = None
    date_format = None
    retriever = None

    def __init__(self):
        try:
            self.seed = Seed.objects.get(url=self.seed_url)
        except Seed.DoesNotExist:
            raise NoSeedYet('You need to add a Seed with the URL %r' % self.seed_url)
        self.logger = logging.getLogger('eb.retrieval.%s.%s' % (settings.SHORT_NAME, self.schema))
        if self.retriever is None:
            self.retriever = UnicodeRetriever(cache=None, sleep=self.seed.delay)

    def save_page(self, unique_id):
        """
        Downloads the page with the given unique ID (possibly a numeric ID, or
        a URL) and saves it as a Page object. Returns the Page object, or None
        if the page couldn't be found.

        The page won't be retrieved/saved if it's already in the database. In
        this case, the existing Page object will be returned.
        """
        self.logger.debug('save_page(%s)', unique_id)
        retrieval_url = self.retrieval_url(unique_id)
        public_url = self.public_url(unique_id)

        try:
            p = Page.objects.get(seed__id=self.seed.id, url=public_url)
        except Page.DoesNotExist:
            pass
        else:
            self.logger.debug('Skipping already-saved URL %s', public_url)
            return p

        try:
            html = self.retriever.get_html(retrieval_url).strip()
        except (RetrievalError, UnicodeDecodeError):
            return None
        if not html:
            self.logger.debug('Got empty page for %s', retrieval_url)
            return None
        self.logger.debug('Got VALID page for %s', retrieval_url)

        m = self.date_headline_re.search(html)
        if not m:
            self.logger.debug('Could not find date/headline on %s', retrieval_url)
            return None
        article_date, article_headline = m.groupdict()['article_date'], m.groupdict()['article_headline']
        try:
            article_date = parse_date(article_date, self.date_format)
        except ValueError:
            self.logger.debug('Got unparseable date %r on %s', article_date, retrieval_url)
            return None
        article_headline = strip_tags(article_headline)
        if len(article_headline) > 255:
            article_headline = article_headline[:252] + '...'

        p = Page.objects.create(
            seed=self.seed,
            url=public_url,
            scraped_url=retrieval_url,
            html=html,
            when_crawled=datetime.datetime.now(),
            is_article=True,
            is_pdf=False,
            is_printer_friendly=False,
            article_headline=article_headline,
            article_date=article_date,
            has_addresses=None,
            when_geocoded=None,
            geocoded_by='',
            times_skipped=0,
            robot_report='',
        )
        self.logger.debug('Created Page ID %s' % p.id)
        save_locations_for_page(p)
        return p

    ######################################
    # METHODS SUBCLASSES SHOULD OVERRIDE #
    ######################################

    def public_url(self, unique_id):
        "Given the ID value, returns the URL that we should publish."
        raise NotImplementedError()

    def retrieval_url(self, unique_id):
        "Given the ID value, returns the URL that we should scrape."
        return self.public_url(unique_id)

class IncrementalCrawler(SpecializedCrawler):
    """
    Crawler that populates the blobs.Page table by incrementing IDs.

    This is a very "dumb" but effective technique for crawling sites such
    as cityofchicago.org whose pages have incremental ID numbers.

    LIMITATIONS/ASSUMPTIONS:

    * This assumes that the URL for each retrieved page is in the same format,
      such that ordering by the URL will result in the highest ID.
    * This assumes that a Seed exists with url=self.seed_url.
    * Before running update(), at least one Page with the given seed must
      exist. Otherwise the retriever won't know what the latest page is!
    """

    max_blanks = 10

    ##################################################
    # METHODS SUBCLASSES SHOULD NOT HAVE TO OVERRIDE #
    ##################################################

    def max_id(self):
        "Returns the ID of the latest page we've already crawled."
        try:
            latest_page = Page.objects.filter(seed__id=self.seed.id).order_by('-url')[0]
        except IndexError:
            raise NoPagesYet('Seed ID %s has no pages yet' % self.seed.id)
        return int(self.id_for_url(latest_page.url))

    def update(self):
        """
        Determines the ID of the latest page we've already crawled, and crawls
        until self.max_blanks blank pages are reached.
        """
        current_id = self.max_id()
        num_blanks = 0
        while num_blanks < self.max_blanks:
            current_id += 1
            page = self.save_page(current_id)
            if page:
                num_blanks = 0
            else:
                num_blanks += 1

    def save_id_range(self, first_id, last_id):
        """
        Downloads and saves Pages for the given ID range, inclusive. Pages
        won't be saved if they're already in the database.
        """
        for id_value in range(int(first_id), int(last_id)+1):
            self.save_page(id_value)

    ######################################
    # METHODS SUBCLASSES SHOULD OVERRIDE #
    ######################################

    def id_for_url(self, url):
        "Given a URL, returns its ID value. This can be either a string or int."
        raise NotImplementedError()

class PageAreaCrawler(SpecializedCrawler):
    """
    Crawler that finds specific links on a given index page (seed_url)
    and creates a blobs.Page for each link that hasn't yet been created.
    """

    ##################################################
    # METHODS SUBCLASSES SHOULD NOT HAVE TO OVERRIDE #
    ##################################################

    def update(self):
        seed_html = self.retriever.get_html(self.seed_url)
        for url in self.get_links(seed_html):
            self.save_page(url)

    def public_url(self, unique_id):
        return unique_id

    ######################################
    # METHODS SUBCLASSES SHOULD OVERRIDE #
    ######################################

    def get_links(self, html):
        """
        Given the seed HTML, returns the list of links.
        """
        raise NotImplementedError()

########NEW FILE########
__FILENAME__ = update_feeds
"""
RSS-feed retriever
"""

from ebdata.blobs.geotagging import save_locations_for_page
from ebdata.blobs.models import Seed, Page
from ebdata.retrieval import UnicodeRetriever
from ebdata.retrieval import log # Register the logging hooks.
from ebdata.templatemaker.htmlutils import printer_friendly_link
from ebdata.textmining.treeutils import make_tree
from ebpub.utils.dates import parse_date
import feedparser
import cgi
import datetime
import logging
import re
import time
import urllib
import urlparse

strip_tags = lambda x: re.sub(r'<[^>]*>', ' ', x).replace('&nbsp;', ' ')
server_authority_re = re.compile('^(?:([^\@]+)\@)?([^\:]+)(?:\:(.+))?$')
url_collapse_re = re.compile('([^/]+/\.\./?|/\./|//|/\.$|/\.\.$)')

def remove_query_string(url):
    bits = urlparse.urlparse(url)
    return urlparse.urlunparse(bits[:4] + ('',) + bits[5:])

def add_query_string(url, new_values):
    bits = urlparse.urlparse(url)
    qs = cgi.parse_qs(bits[4], keep_blank_values=True)
    qs.update(new_values)
    return urlparse.urlunparse(bits[:4] + (urllib.urlencode(qs, doseq=True),) + bits[5:])

def normalize_url(base_href, url, normalize_www_flag):
    """
    Normalizes the given URL:
        * Joins it with base_href if it doesn't already have a domain.
        * Lowercases the scheme (WWW.GOOGLE.COM -> www.google.com).
        * Removes the port (80 or 443) if it's default.
        * Collapses '../' and './'.
        * Alphabetizes the query string by its keys.
        * If it ends in '/index.html', removes the 'index.html'.
        * Normalizes the 'www.' subdomain according to normalize_www_flag.
    Returns None if the URL is invalid.

    normalize_www_flag should be either 1, 2 or 3:
        * 1 = Remove the 'www.' subdomain, if it exists.
        * 2 = Add a 'www.' subdomain, if a subdomain doesn't exist.
        * 3 = Don't touch the subdomain.
    """
    # Inspired by http://www.mnot.net/python/urlnorm.py -- BSD license.
    url = urlparse.urljoin(base_href, url)
    scheme, authority, path, parameters, query, fragment = urlparse.urlparse(url)
    scheme = scheme.lower()
    if '.' not in authority:
        return None
    if authority:
        userinfo, host, port = server_authority_re.match(authority).groups()
        if host[-1] == '.':
            host = host[:-1]

        # Normalize the www subdomain, if necessary.
        if normalize_www_flag == 1 and host.startswith('www.'):
            host = host[4:]
        elif normalize_www_flag == 2 and host.count('.') == 1:
            host = 'www.' + host

        authority = host.lower()
        if userinfo:
            authority = "%s@%s" % (userinfo, authority)
        if port and port != {'http': '80', 'https': '443'}.get(scheme):
            authority = "%s:%s" % (authority, port)

    if scheme.startswith('http'):
        last_path = path
        while 1:
            path = url_collapse_re.sub('/', path, 1)
            if last_path == path:
                break
            last_path = path
    if not path:
        path = '/'
    if path.endswith('/index.html'):
        path = path[:-10] # Trim trailing "index.html".
    if query:
        # Reorder the query string to alphabetize the keys.
        query_bits = sorted(cgi.parse_qsl(query, keep_blank_values=True))
        query = '&'.join(['%s=%s' % (k, v) for k, v in query_bits])
    return urlparse.urlunparse((scheme, authority, path, parameters, query, ''))

try:
    # any() built-in only in Python >= 2.5
    any
except NameError:
    def any(iterable):
        for element in iterable:
            if element:
                return True
        return False

class FeedUpdater(object):
    def __init__(self, seed, retriever, logger):
        self.seed = seed
        self.retriever = retriever
        self.logger = logger

    def update(self):
        try:
            feed = feedparser.parse(self.seed.url)
        except UnicodeDecodeError:
            self.logger.info('UnicodeDecodeError on %r', self.seed.url)
            return
        for entry in feed['entries']:
            if 'feedburner_origlink' in entry:
                url = entry['feedburner_origlink']
            elif 'pheedo_origLink' in entry:
                url = entry['pheedo_origLink']
            elif 'link' in entry:
                url = entry['link']
            else:
                continue # Skip entries with no link.

            try:
                url = normalize_url(self.seed.base_url, url, self.seed.normalize_www)
            except Exception:
                self.logger.warn('Problem normalizing URL: %r, %r, %r', self.seed.base_url, url, self.seed.normalize_www)
                continue

            if not url:
                self.logger.info('Skipping article with empty URL: %r, %r', self.seed.base_url, url)
                continue

            if len(url) > 512:
                self.logger.warning('Skipping long URL %s', url)
                continue

            article_date = entry.get('updated_parsed') and datetime.date(*entry['updated_parsed'][:3]) or None
            if article_date and article_date > datetime.date.today():
                # Skip articles in the future, because sometimes articles show
                # up in the feed before they show up on the site, and we don't
                # want to retrieve the article until it actually exists.
                self.logger.info('Skipping article_date %s, which is in the future', article_date)
                continue

            url = self.normalize_url(url)

            try:
                title = entry['title']
            except KeyError:
                self.logger.debug('Skipping %s due to missing title', url)
                continue

            if not self.download_page(url, title):
                self.logger.debug('Skipping %s due to download_page()', url)
                continue

            # If we've already retrieved the page, there's no need to retrieve
            # it again.
            try:
                Page.objects.filter(url=url)[0]
            except IndexError:
                pass
            else:
                self.logger.debug('URL %s has already been retrieved', url)
                continue

            # If this seed contains the full content in the RSS feed <summary>,
            # then we just use it instead of downloading the contents.
            if self.seed.rss_full_entry:
                is_printer_friendly = False
                try:
                    html = entry['summary']
                except KeyError:
                    html = entry['description']
            else:
                is_printer_friendly = False
                html = None
                time.sleep(self.seed.delay)

                # First, try deducing for the printer-friendly page, given the URL.
                print_url = self.get_printer_friendly_url(url)
                if print_url is not None:
                    try:
                        html = self.get_article_page(print_url)
                        is_printer_friendly = True
                    except Exception, e:
                        self.logger.info('Error retrieving supposedly accurate printer-friendly page %s: %s', print_url, e)

                # If a printer-friendly page didn't exist, get the real page.
                if html is None:
                    try:
                        html = self.get_article_page(url)
                    except Exception, e:
                        self.logger.info('Error retrieving %s: %s', url, e)
                        continue

                    # If a page was downloaded, try looking for a printer-friendly
                    # link, and download that.
                    print_page = self.get_printer_friendly_page(html, url)
                    if print_page is not None:
                        is_printer_friendly = True
                        html = print_page

                new_html = self.scrape_article_from_page(html)
                if new_html is not None:
                    html = new_html

                if article_date is None:
                    article_date = self.scrape_article_date_from_page(html)

            if not html.strip():
                self.logger.debug('Got empty HTML page')
                continue

            article_headline = strip_tags(title)
            if len(article_headline) > 252:
                article_headline = article_headline[252:] + '...'
            p = Page.objects.create(
                seed=self.seed,
                url=url,
                scraped_url=(is_printer_friendly and print_url or url),
                html=html,
                when_crawled=datetime.datetime.now(),
                is_article=True,
                is_pdf=False,
                is_printer_friendly=is_printer_friendly,
                article_headline=article_headline,
                article_date=article_date,
                has_addresses=None,
                when_geocoded=None,
                geocoded_by='',
                times_skipped=0,
                robot_report='',
            )
            self.logger.info('Created %s story %r', self.seed.base_url, article_headline)
            save_locations_for_page(p)

    def normalize_url(self, url):
        """
        Given the article URL, returns a normalized version of the URL.
        """
        return url

    def download_page(self, url, article_headline):
        """
        Given the URL and headline from RSS, returns True if this page should
        be downloaded, and False if it can be skipped.
        """
        return True

    def get_article_page(self, url):
        return self.retriever.get_html(url)

    def get_printer_friendly_url(self, url):
        """
        Given a story URL, returns the printer-friendly URL, or None if it
        can't be determined.
        """
        return None

    def get_printer_friendly_page(self, html, url):
        """
        Parses the given detail page and returns the printer-friendly page, or
        None if not found.
        """
        print_link = printer_friendly_link(make_tree(html))
        if print_link:
            print_link = urlparse.urljoin(url, print_link)
            try:
                return self.get_article_page(print_link)
            except Exception, e:
                self.logger.debug('Error retrieving printer-friendly page %s: %s', url, e)
                return None
        else:
            return None

    def scrape_article_from_page(self, html):
        """
        Parses the given detail page and returns the article as a string, or
        None if it can't be found.
        """
        return html

    def scrape_article_date_from_page(self, html):
        """
        Parses the given detail page and returns the article date as a
        datetime.date, or None if it can't be found.
        """
        return None

def update(seed_id=None):
    """
    Retrieves and saves every new item for every Seed that is an RSS feed.
    """
    retriever = UnicodeRetriever(cache=None)
    logger = logging.getLogger('eb.retrieval.blob_rss')
    qs = Seed.objects.filter(is_rss_feed=True, is_active=True)
    if seed_id is not None:
        qs = qs.filter(id=seed_id)
    for seed in qs:
        updater = FeedUpdater(seed, retriever, logger)
        updater.update()

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    update()

########NEW FILE########
__FILENAME__ = addresses
import re

# Regex notes:
#   * This is *not* a case-insensitive regex, because we assume
#     capitalized words are special (street names).
#   * All data matched by capturing parentheses is concatenated together, so
#     if you don't want to include something in the resulting string, don't
#     capture it.

# STREET_NAME is a fragment of a regular expression that is used in several
# places in our "real" regular expression (ADDRESSES_RE) below. The one tricky
# thing about it is that it includes a "CAPTURE_START" placeholder instead of
# a capturing opening parenthesis. This lets us create two versions of the
# regex -- STREET_NAME_CAPTURE and STREET_NAME_NOCAPTURE.
STREET_NAME = r"""
    # Here, we define some common false positives and tell the regex to ignore them.
    (?!
        [Aa][Ss][Ss][Oo][Cc][Ii][Aa][Tt][Ee][Dd]\ [Pp][Rr][Ee][Ss][Ss] # associated press
        |
        [Uu][Nn][Ii][Vv][Ee][Rr][Ss][Ii][Tt][Yy]\ [Oo][Ff]             # university of
    )
    # DIRECTION
    %(CAPTURE_START)s
        (?:
            [NSEWnsew]\.?
            |
            (?:
                [Nn][Oo][Rr][Tt][Hh] |
                [Ss][Oo][Uu][Tt][Hh] |
                [Ee][Aa][Ss][Tt] |
                [Ww][Ee][Ss][Tt] |
                [Nn][Oo][Rr][Tt][Hh][Ee][Aa][Ss][Tt] |
                [Ee][Aa][Ss][Tt][Ww][Ee][Ss][Tt] |
                [Ss][Oo][Uu][Tt][Hh][Ee][Aa][Ss][Tt] |
                [Ss][Oo][Uu][Tt][Hh][Ww][Ee][Ss][Tt]
            )
            |
            (?:
                N\.?W | S\.?W | N\.?E | S\.?E
            )\.?
        )
        \ +                                        # space (but not newline)
    )?
    (?:
        # STREET NAME
        %(CAPTURE_START)s
            # Numbered street names with a suffix ("3rd", "4th").
            \d+(?:st|ST|nd|ND|rd|RD|th|TH|d|D)

            |

            # Or, numbered street names without a suffix ("3", "4")
            # but with a street type.
            \d+
            (?=
                \ +
                (?:Ave|Avenue|Blvd|Boulevard|Bvd|Cir|Circle|Court|Ct|Dr|Drive|
                   Lane|Ln|Parkway|Pkwy|Place|Plaza|Pl|Plz|Point|Pt|Pts|Rd|Rte|
                   Sq|Sqs|Street|Streets|St|Sts|Terrace|Ter|Terr|Trl|Way|Wy
                )
                \b
            )

            |

            # Or, street names that don't start with numbers.
            (?:
                # Optional prefixes --
                # "St", as in "St Louis"
                # "Dr. Martin", as in "Dr. Martin Luther King"
                (?:
                    [Ss][Tt]\.?
                    |
                    [Dd][Rr]\.?\ [Mm][Aa][Rr][Tt][Ii][Nn]
                )
                \ +
            )?
            (?:
                Mass\.(?=\ +[Aa]ve)  # Special case: "Mass." abbr. for "Massachussetts Ave."
                                     # Needs to be special-cased because of the period.
                |
                (?:Avenue|Ave\.?)\ +[A-Z]       # Special case: "Avenue X"
                |
                [A-Z][a-z][A-Za-z]*  # One initial-capped word
                |
                [A-Z]\b              # Single-letter street name (e.g., K St. in DC)
                (?!\.\w)             # Avoid '20 U.S.A.'
            )
        )
        (?:
            # Here, we list the options with street suffixes first, so that
            # the suffix abbreviations are treated as the last part of the
            # street name, to avoid overeagerly capturing "123 Main St. The".
            %(CAPTURE_START)s
                \ +(?:Ave|Blvd|Bvd|Cir|Ct|Dr|Ln|Pkwy|Pl|Plz|Pt|Pts|Rd|Rte|Sq|Sqs|St|Sts|Ter|Terr|Trl|Wy)\.
                |
                \ +[A-Z][a-z][A-Za-z]*\ (?:Ave|Blvd|Bvd|Cir|Ct|Dr|Ln|Pkwy|Pl|Plz|Pt|Pts|Rd|Rte|Sq|Sqs|St|Sts|Ter|Terr|Trl|Wy)\.
                |
                (?:,?\ Jr\.?,?|\ +[A-Z][a-z][A-Za-z]*){2}\ +(?:Ave|Blvd|Bvd|Cir|Ct|Dr|Ln|Pkwy|Pl|Plz|Pt|Pts|Rd|Rte|Sq|Sqs|St|Sts|Ter|Terr|Trl|Wy)\.
                |
                (?:,?\ Jr\.?,?|\ +[A-Z][a-z][A-Za-z]*){3}\ +(?:Ave|Blvd|Bvd|Cir|Ct|Dr|Ln|Pkwy|Pl|Plz|Pt|Pts|Rd|Rte|Sq|Sqs|St|Sts|Ter|Terr|Trl|Wy)\.
                |
                (?:,?\ Jr\.?,?|\ +[A-Z][a-z][A-Za-z]*){4}\ +(?:Ave|Blvd|Bvd|Cir|Ct|Dr|Ln|Pkwy|Pl|Plz|Pt|Pts|Rd|Rte|Sq|Sqs|St|Sts|Ter|Terr|Trl|Wy)\.
                |
                (?:,?\ Jr\.?,?|\ +[A-Z][a-z][A-Za-z]*){5}\ +(?:Ave|Blvd|Bvd|Cir|Ct|Dr|Ln|Pkwy|Pl|Plz|Pt|Pts|Rd|Rte|Sq|Sqs|St|Sts|Ter|Terr|Trl|Wy)\.
                |
                (?:,?\ Jr\.?,?|\ +[A-Z][a-z][A-Za-z]*){1,5}
            )?
            # OPTIONAL POST-DIR
            (?:
                # Standard post-dir format
                %(CAPTURE_START)s
                    ,?\s(?:N\.?E|S\.?E|N\.?W|S\.?W|N|S|E|W)\.?
                )
                # Avoid greedily capturing more letters, like
                # '123 Main St, New England' to '123 Main St, N'
                (?![A-Za-z])

                |

                # Or, a special-case for DC quadrants, to find stuff like:
                # "600 H Street in NE Washington"
                # "600 H Street in the NE quadrant"
                # "600 H Street in northeast DC"

                # Note that this is NOT captured, so that it's excluded from
                # the final output.
                ,?
                \s in
                %(CAPTURE_START)s
                    \s
                )
                (?:
                    (?:the|far) \s
                )?

                %(CAPTURE_START)s
                    (?:NE|SE|NW|SW|[Nn]ortheast|[Ss]outheast|[Nn]orthwest|[Ss]outhwest)
                    (?=
                        \s (?:quadrant|D\.?C\.?|Washington)
                    )
                )
            )?
        )?
    )
"""
STREET_NAME_CAPTURE = STREET_NAME % {'CAPTURE_START': '('}
STREET_NAME_NOCAPTURE = STREET_NAME % {'CAPTURE_START': '(?:'}

ADDRESSES_RE = re.compile(r"""(?x)
    (?<!-|/|:|,|\.|\$) # These various characters are not allowed before an address/intersection.
    \b

    # Ignore things that look like dates -- e.g., "21 May 2009".
    # This is a problem e.g. in cases where there's a May Street.
    (?!
        \d+\s+
        (?:January|February|March|April|May|June|July|August|September|October|November|December)
        ,?\s+
        \d\d\d\d
    )

    # Ignore intersections that are prefixed by "University of", like
    # "University of Texas at Austin". This is a common false positive.
    (?<!
        [Uu][Nn][Ii][Vv][Ee][Rr][Ss][Ii][Tt][Yy]\s[Oo][Ff]\s
    )

    (?:
        # SEGMENT ("FOO BETWEEN BAR AND BAZ")
        (?:
            %(STREET_NAME_CAPTURE)s (,?\ + between \ +) %(STREET_NAME_CAPTURE)s (,?\ + and \ +) %(STREET_NAME_CAPTURE)s
            |
            %(STREET_NAME_CAPTURE)s (,?\ + from \ +) %(STREET_NAME_CAPTURE)s (,?\ + to \ +) %(STREET_NAME_CAPTURE)s
        )

        |

        # BLOCK/ADDRESS
        (?:
            (
                (?:
                    (?:\d+|[Ff][Ii][Rr][Ss][Tt])[-\ ]
                        (?:(?:[Nn][Oo][Rr][Tt][Hh]|[Ss][Oo][Uu][Tt][Hh]|[Ee][Aa][Ss][Tt]|[Ww][Ee][Ss][Tt])\ )?
                    [Bb][Ll][Oo][Cc][Kk]\ [Oo][Ff]
                    |
                    \d+\ *-\ *\d+
                    |
                    \d+
                )
                \ +
            )
            %(STREET_NAME_CAPTURE)s

            # ignore the intersection in parenthesis so that it's not picked
            # up as a separate location. We do this by consuming the string
            # but *not* capturing it.
            (?:
                \ +
                \(?
                between
                \ +
                %(STREET_NAME_NOCAPTURE)s
                \ +
                and
                \ +
                %(STREET_NAME_NOCAPTURE)s
                \)?
            )?
        )

        |

        # INTERSECTION
        (?:
            # Common intersection prefixes. They're included here so that the
            # regex doesn't include them as part of the street name.
            (?:
                (?:
                    [Nn]ear |
                    [Aa]t |
                    [Oo]n |
                    [Tt]o |
                    [Aa]round |
                    [Ii]ntersection\ of |
                    [Cc]orner\ of |
                    [Aa]rea\ of |
                    [Aa]reas?\ surrounding |
                    vicinity\ of |
                    ran\ down |
                    running\ down |
                    crossed
                )
                \ +
            )?
            \b
            (?:%(STREET_NAME_CAPTURE)s)
            (\ +)
            (
                (?:
                    [Aa][Nn][Dd] |
                    [Aa][Tt] |
                    [Nn][Ee][Aa][Rr] |
                    & |
                    [Aa][Rr][Oo][Uu][Nn][Dd] |
                    [Tt][Oo][Ww][Aa][Rr][Dd][Ss]? |
                    [Oo][Ff][Ff] |
                    (?:[Jj][Uu][Ss][Tt]\ )?(?:[Nn][Oo][Rr][Tt][Hh]|[Ss][Oo][Uu][Tt][Hh]|[Ee][Aa][Ss][Tt]|[Ww][Ee][Ss][Tt])\ [Oo][Ff] |
                    (?:[Jj][Uu][Ss][Tt]\ )?[Pp][Aa][Ss][Tt]
                )
                \ +
            )
            (?:%(STREET_NAME_CAPTURE)s)
        )
    )

    # OPTIONAL CITY SUFFIX
    (?:
        (?:
            ,?\s+in |
            ,
        )
        \s+

        # CITY NAME
        (
            [A-Z][a-z][A-Za-z]*                   # One initial-capped word
            (?:
                ,?\ Jr\.?,?
                |
                \ [A-Z][a-z][A-Za-z]*
                |
                -[A-Za-z]+                        # Hyphenated words (e.g. "Croton-on-Hudson" in NY)
            ){0,4}  # Initial-capped words
        )
    )?
    """ % {'STREET_NAME_CAPTURE': STREET_NAME_CAPTURE, 'STREET_NAME_NOCAPTURE': STREET_NAME_NOCAPTURE})

def parse_addresses(text):
    """
    Returns a list of all addresses found in the given string, as tuples in the
    format (address, city).
    """
    # This assumes the last parenthetical grouping in ADDRESSES_RE is the city.
    return [(''.join(bits[:-1]), bits[-1]) for bits in ADDRESSES_RE.findall(text)]

def tag_addresses(text, pre='<addr>', post='</addr>'):
    """
    "Tags" any addresses in the given string by surrounding them with pre and post.
    Returns the resulting string.

    Note that only the addresses are tagged, not the cities (if cities exist).
    """
    def _re_handle_address(m):
        bits = m.groups()
        return pre + ''.join(filter(None, bits[:-1])) + (bits[-1] and (', %s' % bits[-1]) or '') + post
    return ADDRESSES_RE.sub(_re_handle_address, text)

########NEW FILE########
__FILENAME__ = datelines
import re

dateline_re = re.compile(ur"""
    (?:
        (?:                                                     # Either a newline, or a
            ^                                                   # <p> / <div>, followed by tags/space
            |
            </?\s*(?:[Pp]|[Dd][Ii][Vv])[^>]*>
        )
        (?:<[^>]*>|\s)*                                         # The start of a line
    )
    (?:\(\d\d?-\d\d?\)\s+\d\d?:\d\d\s+[PMCE][SD]T\s+)?          # Optional timestamp -- e.g., "(07-17) 13:09 PDT"
    ([A-Z][A-Z.]*[A-Z.,](?:\s+[A-Z][A-Za-z.]*[A-Za-z.,]){0,4})  # The dateline itself
    (?:                                                         # Optional parenthetical news outlet
        \s+
        \(
            [-A-Za-z0-9]{1,15}
            (?:\s+[-A-Za-z0-9]{1,15}){0,4}
        \)
    )?
    \s*                                                         # Optional space before dash
    (?:\xa0--\xa0|--|\x97|\u2015|&\#8213;|&\#151;|&\#x97;|)     # Dash (or emdash)
    """, re.MULTILINE | re.VERBOSE)

def guess_datelines(text):
    """
    Given some text (with or without HTML), returns a list of the dateline(s)
    in it. Returns an empty list if none are found.
    """
    return dateline_re.findall(text)

########NEW FILE########
__FILENAME__ = places
import re
from ebpub.db.models import Location
from ebpub.streets.models import Place, Misspelling

def phrase_tagger(phrases, pre='<span>', post='</span>'):
    # Sort the phrases and then reverse them so, for example, Lake View East
    # will come before Lake View in the regex, and will match more greedily.
    # Use the decorate-sort-undecorate pattern and then list.reverse() to
    # avoid the overhead of calling a custom comparison function.
    decorated = [(len(p), p) for p in phrases]
    decorated.sort()
    decorated.reverse()
    phrases = [i[1] for i in decorated]
    # use a closure here to cache the value for phrases
    def tag_phrases(text):
        """
        Returns text with any matches from phrases wrapped with pre and post.
        """
        # If no phrases were provided, just return the text we received.
        if len(phrases) == 0:
            return text

        def _re_handle_match(m):
            output = (m.group(1) or '') + m.group(2) + (m.group(3) or '')
            if m.group(1) and m.group(3):
                return output
            return pre + output + post
        phrases_re = '|'.join([r'\b%s\b' % re.escape(p) for p in phrases])

        # In addition to identifying every phrase, this regex grabs the "pre"
        # and "post" before the phrase, optionally. Then the _re_handle_match()
        # function checks whether the "pre" and "post" were provided. If both
        # were found, that means this phrase was already tagged (perhaps by
        # tag_addresses(), and thus the new tags aren't inserted. Note that
        # this assumes that each tagging of the text (whether it's
        # tag_addresses(), place_tagger() or location_tagger()) uses a
        # consistent "pre" and "post".
        return re.sub('(?i)(%s[^<]*)?(%s)([^<]*%s)?' % \
            (re.escape(pre), phrases_re, re.escape(post)), _re_handle_match, text)

    return tag_phrases

def place_tagger(pre='<addr>', post='</addr>'):
    phrases = [p['pretty_name'] for p in Place.objects.values('pretty_name').order_by('-pretty_name')]
    return phrase_tagger(phrases, pre, post)

def location_tagger(pre='<addr>', post='</addr>'):
    location_qs = Location.objects.values('name').order_by('-name').exclude(location_type__slug__in=('boroughs', 'cities'))
    locations = [p['name'] for p in location_qs]
    misspellings = [m['incorrect'] for m in Misspelling.objects.values('incorrect').order_by('-incorrect')]
    return phrase_tagger(locations + misspellings, pre, post)

########NEW FILE########
__FILENAME__ = datelines
from ebdata.nlp.datelines import guess_datelines
import unittest

class DatelineTestCase(unittest.TestCase):
    def assertDatelines(self, text, expected):
        self.assertEqual(guess_datelines(text), expected)

    def test_basic1(self):
        self.assertDatelines('CHICAGO -- Something happened', ['CHICAGO'])

    def test_basic2(self):
        self.assertDatelines('CHICAGO-- Something happened', ['CHICAGO'])

    def test_basic3(self):
        self.assertDatelines('CHICAGO --Something happened', ['CHICAGO'])

    def test_basic4(self):
        self.assertDatelines('CHICAGO--Something happened', ['CHICAGO'])

    def test_lowercase1(self):
        self.assertDatelines('chicago -- Something happened', [])

    def test_lowercase2(self):
        self.assertDatelines('That was in Chicago -- where something happened', [])

    def test_emdash1(self):
        self.assertDatelines('CHICAGO\x97Something happened', ['CHICAGO'])

    def test_emdash2(self):
        self.assertDatelines('CHICAGO \x97Something happened', ['CHICAGO'])

    def test_emdash3(self):
        self.assertDatelines('CHICAGO  \x97Something happened', ['CHICAGO'])

    def test_emdash4(self):
        self.assertDatelines(u'CHICAGO \u2015 Something happened', ['CHICAGO'])

    def test_emdash5(self):
        self.assertDatelines(u'CHICAGO\xa0--\xa0Something happened', ['CHICAGO'])

    def test_emdash6(self):
        self.assertDatelines(u'CHICAGO \xa0--\xa0 Something happened', ['CHICAGO'])

    def test_html_entity_dash1(self):
        self.assertDatelines('CHICAGO &#8213; Something happened', ['CHICAGO'])

    def test_html_entity_dash2(self):
        self.assertDatelines('CHICAGO &#151; Something happened', ['CHICAGO'])

    def test_html_entity_dash3(self):
        self.assertDatelines('CHICAGO &#x97; Something happened', ['CHICAGO'])

    def test_multi_word_dateline1(self):
        self.assertDatelines('SAN FRANCISCO -- Something happened', ['SAN FRANCISCO'])

    def test_multi_word_dateline2(self):
        self.assertDatelines('SOUTH SAN FRANCISCO -- Something happened', ['SOUTH SAN FRANCISCO'])

    def test_comma_periods(self):
        self.assertDatelines('CHESTERFIELD, S.C. -- Something happened', ['CHESTERFIELD, S.C.'])

    def test_comma_no_space(self):
        self.assertDatelines('CHESTERFIELD,S.C. -- Something happened', [])

    def test_lowercase(self):
        self.assertDatelines('Lowercase -- Something happened', [])

    def test_start_of_line1(self):
        self.assertDatelines('blah blah\nCHICAGO -- Something happened', ['CHICAGO'])

    def test_start_of_line2(self):
        self.assertDatelines('blah blah\n<b>CHICAGO -- Something happened', ['CHICAGO'])

    def test_start_of_line_p(self):
        self.assertDatelines('<div>BY ASSOCIATED PRESS</div><p>CHICAGO -- Something happened', ['CHICAGO'])

    def test_start_of_line_div(self):
        self.assertDatelines('<div>BY ASSOCIATED PRESS</div><div>CHICAGO -- Something happened', ['CHICAGO'])

    def test_start_of_line_div_then_more1(self):
        self.assertDatelines('<div><span>CHICAGO -- </span>Something happened', ['CHICAGO'])

    def test_start_of_line_div_then_more2(self):
        self.assertDatelines('<br><div><span>CHICAGO -- </span>Something happened', ['CHICAGO'])

    def test_start_of_line_div_then_more3(self):
        self.assertDatelines('<div> Reporting<br>Rafael Romo </div> <span> CHICAGO -- </span> Something happened', ['CHICAGO'])

    def test_second_word_lowercase(self):
        self.assertDatelines('Associated Press Writer<br><br><div>KANDAHAR, Afghanistan -- Something happened', ['KANDAHAR, Afghanistan'])

    def test_news_outlet_1(self):
        self.assertDatelines('CHICAGO (Aurora Beacon News) -- Something happened', ['CHICAGO'])

    def test_news_outlet_2(self):
        self.assertDatelines('CHICAGO (STNG) -- Something happened', ['CHICAGO'])

    def test_news_outlet_3(self):
        self.assertDatelines('CHICAGO (CBS) -- Something happened', ['CHICAGO'])

    def test_news_outlet_4(self):
        self.assertDatelines('CHICAGO (ABC) -- Something happened', ['CHICAGO'])

    def test_news_outlet_5(self):
        self.assertDatelines('CHICAGO (FOX) -- Something happened', ['CHICAGO'])

    def test_news_outlet_6(self):
        self.assertDatelines('CHICAGO (AP) -- Something happened', ['CHICAGO'])

    def test_news_outlet_7(self):
        self.assertDatelines('CHICAGO (Associated Press) -- Something happened', ['CHICAGO'])

    def test_news_outlet_8(self):
        self.assertDatelines('CHICAGO (Post-Tribune) -- Something happened', ['CHICAGO'])

    def test_news_outlet_9(self):
        self.assertDatelines('CHICAGO (Chicago Sun-Times) -- Something happened', ['CHICAGO'])

    def test_news_outlet_10(self):
        self.assertDatelines('CHICAGO (Chicago Tribune) -- Something happened', ['CHICAGO'])

    def test_news_outlet_11(self):
        self.assertDatelines('CHICAGO (Sports Network) -- Something happened', ['CHICAGO'])

    def test_news_outlet_12(self):
        self.assertDatelines('CHICAGO (BCN) -- Something happened', ['CHICAGO'])

    def test_timestamp_prefix1(self):
        self.assertDatelines('(07-17) 13:09 PDT BERKELEY -- Something happened', ['BERKELEY'])

    def test_timestamp_prefix2(self):
        self.assertDatelines('(07-17) 13:09 MDT BERKELEY -- Something happened', ['BERKELEY'])

    def test_timestamp_prefix3(self):
        self.assertDatelines('(07-17) 13:09 CDT BERKELEY -- Something happened', ['BERKELEY'])

    def test_timestamp_prefix4(self):
        self.assertDatelines('(07-17) 13:09 EDT BERKELEY -- Something happened', ['BERKELEY'])

    def test_timestamp_prefix5(self):
        self.assertDatelines('(07-17) 13:09 PST BERKELEY -- Something happened', ['BERKELEY'])

    def test_timestamp_prefix6(self):
        self.assertDatelines('(07-17) 13:09 MST BERKELEY -- Something happened', ['BERKELEY'])

    def test_timestamp_prefix7(self):
        self.assertDatelines('(07-17) 13:09 CST BERKELEY -- Something happened', ['BERKELEY'])

    def test_timestamp_prefix8(self):
        self.assertDatelines('(07-17) 13:09 EST BERKELEY -- Something happened', ['BERKELEY'])

if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = tests
# -*- coding: utf-8 -*-
from ebdata.nlp.addresses import parse_addresses
from ebdata.nlp.places import phrase_tagger
import unittest

class AddressParsing(unittest.TestCase):
    def assertParses(self, text, expected):
        self.assertEqual(parse_addresses(text), expected)

class MixedCaseAddressParsing(AddressParsing):
    def test_empty(self):
        self.assertParses('', [])

    def test_nomatch1(self):
        self.assertParses('Hello there', [])

    def test_nomatch2(self):
        self.assertParses('Call 321-FUN-TIMES', [])

    def test_nomatch3(self):
        self.assertParses('Call 321-Fun-Times', [])

    def test_nomatch4(self):
        self.assertParses('Call 321-Fun Times', [])

    def test_address_basic(self):
        self.assertParses('123 Main St.', [('123 Main St.', '')])

    def test_address_basic_in_sentence(self):
        self.assertParses('I live at 123 Main St., you know.', [('123 Main St.', '')])

    def test_address_basic_with_followup_sentence(self):
        self.assertParses('I live at 123 Main St. The other person lives elsewhere.', [('123 Main St.', '')])

    def test_address_no_suffix(self):
        self.assertParses('123 Main', [('123 Main', '')])

    def test_address_no_suffix_period(self):
        self.assertParses('Lives at 123 Main.', [('123 Main', '')])

    def test_address_multiple_spaces1(self):
        self.assertParses('123  Main St.', [('123  Main St.', '')])

    def test_address_multiple_spaces2(self):
        self.assertParses('123 Main  St.', [('123 Main  St.', '')])

    def test_address_dir_n(self):
        self.assertParses('123 N. Main St.', [('123 N. Main St.', '')])

    def test_address_dir_s(self):
        self.assertParses('123 S. Main St.', [('123 S. Main St.', '')])

    def test_address_dir_e(self):
        self.assertParses('123 E. Main St.', [('123 E. Main St.', '')])

    def test_address_dir_w(self):
        self.assertParses('123 W. Main St.', [('123 W. Main St.', '')])

    def test_address_dir_ne(self):
        self.assertParses('123 NE. Main St.', [('123 NE. Main St.', '')])

    def test_address_dir_nw(self):
        self.assertParses('123 NW. Main St.', [('123 NW. Main St.', '')])

    def test_address_dir_se(self):
        self.assertParses('123 SE. Main St.', [('123 SE. Main St.', '')])

    def test_address_dir_sw(self):
        self.assertParses('123 SW. Main St.', [('123 SW. Main St.', '')])

    def test_address_dir_ne_no_period(self):
        self.assertParses('123 NE Main St.', [('123 NE Main St.', '')])

    def test_address_dir_nw_no_period(self):
        self.assertParses('123 NW Main St.', [('123 NW Main St.', '')])

    def test_address_dir_se_no_period(self):
        self.assertParses('123 SE Main St.', [('123 SE Main St.', '')])

    def test_address_dir_sw_no_period(self):
        self.assertParses('123 SW Main St.', [('123 SW Main St.', '')])

    def test_address_dir_northeast(self):
        self.assertParses('123 Northeast Main St.', [('123 Northeast Main St.', '')])

    def test_address_dir_northwest(self):
        self.assertParses('123 Northwest Main St.', [('123 Northwest Main St.', '')])

    def test_address_dir_southeast(self):
        self.assertParses('123 Southeast Main St.', [('123 Southeast Main St.', '')])

    def test_address_dir_southwest(self):
        self.assertParses('123 Southwest Main St.', [('123 Southwest Main St.', '')])

    def test_address_dir_no_period(self):
        self.assertParses('123 N Main St.', [('123 N Main St.', '')])

    def test_address_coincidence(self):
        self.assertParses('My Favorite Number Is 123 And I Love It.', [('123 And', '')])

    def test_block_basic(self):
        self.assertParses('100 block of Main Street', [('100 block of Main Street', '')])

    def test_block_zero(self):
        self.assertParses('0 block of Main Street', [('0 block of Main Street', '')])

    def test_block_first(self):
        self.assertParses('first block of Main Street', [('first block of Main Street', '')])

    def test_block_first_cap(self):
        self.assertParses('First block of Main Street', [('First block of Main Street', '')])

    def test_block_with_direction_first_north(self):
        self.assertParses('800 North Block of Lawrence Avenue', [('800 North Block of Lawrence Avenue', '')])

    def test_block_with_direction_first_south(self):
        self.assertParses('800 South Block of Lawrence Avenue', [('800 South Block of Lawrence Avenue', '')])

    def test_block_with_direction_first_east(self):
        self.assertParses('800 East Block of Lawrence Avenue', [('800 East Block of Lawrence Avenue', '')])

    def test_block_with_direction_first_west(self):
        self.assertParses('800 West Block of Lawrence Avenue', [('800 West Block of Lawrence Avenue', '')])

    def test_block_no_suffix(self):
        self.assertParses('1000 block of Western', [('1000 block of Western', '')])

    def test_block_cap(self):
        self.assertParses('100 Block of Main Street', [('100 Block of Main Street', '')])

    def test_block_hyphen(self):
        self.assertParses('5700-block of South Indiana', [('5700-block of South Indiana', '')])

    def test_block_period_not_included(self):
        self.assertParses('It happened on the 1000 block of Western.', [('1000 block of Western', '')])

    def test_block_comma_not_included(self):
        self.assertParses('It happened on the 1000 block of Western, officials said', [('1000 block of Western', '')])

    def test_block_quote_not_included(self):
        self.assertParses('"It happened on the 1000 block of Western" they said.', [('1000 block of Western', '')])

    def test_block_no_double_zeroes(self):
        self.assertParses('Henry lives on the 140 block of Park Hill Avenue, right?', [('140 block of Park Hill Avenue', '')])

    def test_block_direction(self):
        self.assertParses('100 Block of N. Main Street', [('100 Block of N. Main Street', '')])

    def test_suffix_period_included(self):
        self.assertParses('The event at 1358 W. Leland Ave. was fun.', [('1358 W. Leland Ave.', '')])

    def test_multi_word_street_name(self):
        self.assertParses('Residents of 3200 N. Lake Shore Drive were happy.', [('3200 N. Lake Shore Drive', '')])

    def test_prefix_mc(self):
        self.assertParses('I live at 926 E. McLemore.', [('926 E. McLemore', '')])

    def test_prefix_st(self):
        self.assertParses('I live at 926 N. St. Louis.', [('926 N. St. Louis', '')])

    def test_prefix_st_no_period(self):
        self.assertParses('I live at 926 N. St Louis.', [('926 N. St Louis', '')])

    def test_prefix_st_no_period_suffix(self):
        self.assertParses('I live at 926 N. St Louis St.', [('926 N. St Louis St.', '')])

    def test_prefix_saint(self):
        self.assertParses('I live at 926 N. Saint Louis St.', [('926 N. Saint Louis St.', '')])

    def test_newlines_excluded1(self):
        self.assertParses('The number 926\nIs cool', [])

    def test_newlines_excluded2(self):
        self.assertParses('I live at 123\nMain St.', [])

    def test_address_range1(self):
        self.assertParses('10-12 Main St.', [('10-12 Main St.', '')])

    def test_address_range2(self):
        self.assertParses('10-12 N. Main St.', [('10-12 N. Main St.', '')])

    def test_address_range3(self):
        self.assertParses('0-100 Main St.', [('0-100 Main St.', '')])

    def test_address_range4(self):
        self.assertParses('0-100 N. Main St.', [('0-100 N. Main St.', '')])

    def test_pre_number_quote(self):
        self.assertParses('The address is "123 Main St."', [('123 Main St.', '')])

    def test_pre_number_dash(self):
        self.assertParses('I-90 Edens', [])

    def test_pre_number_dollar_sign(self):
        self.assertParses('Hawaii Gas Passes $5 Mark', [])

    def test_pre_number_letter(self):
        self.assertParses('A123 Main St.', [])

    def test_pre_number_slash(self):
        self.assertParses('Chicago 24/7 Crime', [])

    def test_pre_number_colon(self):
        self.assertParses('Happened at about 6:30 Wednesday night', [])

    def test_pre_number_comma(self):
        self.assertParses('That is worth more than $3,000 American dollars', [])

    def test_pre_number_period(self):
        self.assertParses('He received a 3.0 Grade Point Average', [])

    def test_mlk1(self):
        self.assertParses('3624 S. Dr. Martin Luther King Jr. Memorial Drive', [('3624 S. Dr. Martin Luther King Jr. Memorial Drive', '')])

    def test_mlk2(self):
        self.assertParses('3624 S. Dr. Martin Luther King, Jr., Memorial Drive', [('3624 S. Dr. Martin Luther King, Jr., Memorial Drive', '')])

    def test_mlk3(self):
        self.assertParses('3624 S. Dr. Martin Luther King, Jr. Memorial Drive', [('3624 S. Dr. Martin Luther King, Jr. Memorial Drive', '')])

    def test_mlk4(self):
        self.assertParses('3624 S. Martin Luther King, Jr., Memorial Drive', [('3624 S. Martin Luther King, Jr., Memorial Drive', '')])

    def test_mlk5(self):
        self.assertParses('3624 S. Dr. Martin Luther King Drive', [('3624 S. Dr. Martin Luther King Drive', '')])

    def test_mlk6(self):
        self.assertParses('3624 S. Dr. Martin Drive', [('3624 S. Dr. Martin Drive', '')])

    def test_junior1(self):
        self.assertParses('3624 S. John Hancock Jr. Road', [('3624 S. John Hancock Jr. Road', '')])

    def test_junior2(self):
        self.assertParses('3624 S. John Hancock, Jr., Road', [('3624 S. John Hancock, Jr., Road', '')])

    def test_numeric_street1(self):
        self.assertParses('330 West 95th Street', [('330 West 95th Street', '')])

    def test_numeric_street2(self):
        self.assertParses('the Continental, located at 330 West 95th Street.', [('330 West 95th Street', '')])

    def test_suffix_ave(self):
        self.assertParses('The man at 123 Main Ave. was cool.', [('123 Main Ave.', '')])

    def test_suffix_blvd(self):
        self.assertParses('The man at 123 Main Blvd. was cool.', [('123 Main Blvd.', '')])

    def test_suffix_bvd(self):
        self.assertParses('The man at 123 Main Bvd. was cool.', [('123 Main Bvd.', '')])

    def test_suffix_cir(self):
        self.assertParses('The man at 123 Main Cir. was cool.', [('123 Main Cir.', '')])

    def test_suffix_ct(self):
        self.assertParses('The man at 123 Main Ct. was cool.', [('123 Main Ct.', '')])

    def test_suffix_dr(self):
        self.assertParses('The man at 123 Main Dr. was cool.', [('123 Main Dr.', '')])

    def test_suffix_ln(self):
        self.assertParses('The man at 123 Main Ln. was cool.', [('123 Main Ln.', '')])

    def test_suffix_pkwy(self):
        self.assertParses('The man at 123 Main Pkwy. was cool.', [('123 Main Pkwy.', '')])

    def test_suffix_pl(self):
        self.assertParses('The man at 123 Main Pl. was cool.', [('123 Main Pl.', '')])

    def test_suffix_plz(self):
        self.assertParses('The man at 123 Main Plz. was cool.', [('123 Main Plz.', '')])

    def test_suffix_pt(self):
        self.assertParses('The man at 123 Main Pt. was cool.', [('123 Main Pt.', '')])

    def test_suffix_pts(self):
        self.assertParses('The man at 123 Main Pts. was cool.', [('123 Main Pts.', '')])

    def test_suffix_rd(self):
        self.assertParses('The man at 123 Main Rd. was cool.', [('123 Main Rd.', '')])

    def test_suffix_rte(self):
        self.assertParses('The man at 123 Main Rte. was cool.', [('123 Main Rte.', '')])

    def test_suffix_sq(self):
        self.assertParses('The man at 123 Main Sq. was cool.', [('123 Main Sq.', '')])

    def test_suffix_sqs(self):
        self.assertParses('The man at 123 Main Sqs. was cool.', [('123 Main Sqs.', '')])

    def test_suffix_st(self):
        self.assertParses('The man at 123 Main St. was cool.', [('123 Main St.', '')])

    def test_suffix_sts(self):
        self.assertParses('The man at 123 Main Sts. was cool.', [('123 Main Sts.', '')])

    def test_suffix_ter(self):
        self.assertParses('The man at 123 Main Ter. was cool.', [('123 Main Ter.', '')])

    def test_suffix_terr(self):
        self.assertParses('The man at 123 Main Terr. was cool.', [('123 Main Terr.', '')])

    def test_suffix_trl(self):
        self.assertParses('The man at 123 Main Trl. was cool.', [('123 Main Trl.', '')])

    def test_suffix_wy(self):
        self.assertParses('The man at 123 Main Wy. was cool.', [('123 Main Wy.', '')])

    def test_suffix_unknown_no_period(self):
        # If the suffix is unknown, the period isn't included
        self.assertParses('The man at 123 Main Wacky. was cool.', [('123 Main Wacky', '')])

    def test_postdir_n(self):
        self.assertParses('It happened at the garden, 1075 Lake Blvd. N.', [('1075 Lake Blvd. N.', '')])

    def test_postdir_s(self):
        self.assertParses('It happened at the garden, 1075 Lake Blvd. S.', [('1075 Lake Blvd. S.', '')])

    def test_postdir_e(self):
        self.assertParses('It happened at the garden, 1075 Lake Blvd. E.', [('1075 Lake Blvd. E.', '')])

    def test_postdir_w(self):
        self.assertParses('It happened at the garden, 1075 Lake Blvd. W.', [('1075 Lake Blvd. W.', '')])

    def test_postdir_nw(self):
        self.assertParses('It happened at the garden, 1075 Lake Blvd. NW.', [('1075 Lake Blvd. NW.', '')])

    def test_postdir_ne(self):
        self.assertParses('It happened at the garden, 1075 Lake Blvd. NE.', [('1075 Lake Blvd. NE.', '')])

    def test_postdir_sw(self):
        self.assertParses('It happened at the garden, 1075 Lake Blvd. SW.', [('1075 Lake Blvd. SW.', '')])

    def test_postdir_se(self):
        self.assertParses('It happened at the garden, 1075 Lake Blvd. SE.', [('1075 Lake Blvd. SE.', '')])

    def test_postdir_period_nw(self):
        self.assertParses('It happened at the garden, 1075 Lake Blvd. N.W.', [('1075 Lake Blvd. N.W.', '')])

    def test_postdir_period_ne(self):
        self.assertParses('It happened at the garden, 1075 Lake Blvd. N.E.', [('1075 Lake Blvd. N.E.', '')])

    def test_postdir_period_sw(self):
        self.assertParses('It happened at the garden, 1075 Lake Blvd. S.W.', [('1075 Lake Blvd. S.W.', '')])

    def test_postdir_period_se(self):
        self.assertParses('It happened at the garden, 1075 Lake Blvd. S.E.', [('1075 Lake Blvd. S.E.', '')])

    def test_postdir_one_word_street(self):
        self.assertParses('9421 Wabash SW', [('9421 Wabash SW', '')])

    def test_postdir_one_word_street_numbered(self):
        self.assertParses('9421 18th SW', [('9421 18th SW', '')])

    def test_postdir_two_word_street(self):
        self.assertParses('9421 Home Run SW', [('9421 Home Run SW', '')])

    def test_postdir_two_word_street_numbered(self):
        self.assertParses('9421 18th St. SW', [('9421 18th St. SW', '')])

    def test_postdir_central_park_w(self):
        self.assertParses('It happened at 32 West Central Park Avenue W.', [('32 West Central Park Avenue W.', '')])

    def test_postdir_with_in_prefix_washington_dc01(self):
        self.assertParses('It happened on the 600 block of H Street in northeast D.C. and stuff.', [('600 block of H Street northeast', '')])

    def test_postdir_with_in_prefix_washington_dc02(self):
        self.assertParses('It happened on the 600 block of H Street in northeast Washington.', [('600 block of H Street northeast', '')])

    def test_postdir_with_in_prefix_washington_dc03(self):
        self.assertParses('It happened on the 600 block of H Street in NE Washington.', [('600 block of H Street NE', '')])

    def test_postdir_with_in_prefix_washington_dc04(self):
        self.assertParses('It happened on the 600 block of H Street in the NE quadrant', [('600 block of H Street NE', '')])

    def test_postdir_with_in_prefix_washington_dc05(self):
        self.assertParses('It happened on the 600 block of H Street in the NE quadrant', [('600 block of H Street NE', '')])

    def test_postdir_with_in_prefix_washington_dc06(self):
        self.assertParses('It happened on the 600 block of H Street, in northeast D.C. and stuff.', [('600 block of H Street northeast', '')])

    def test_postdir_with_in_prefix_washington_dc07(self):
        self.assertParses('It happened on the 600 block of H Street, in northeast Washington.', [('600 block of H Street northeast', '')])

    def test_postdir_with_in_prefix_washington_dc08(self):
        self.assertParses('It happened on the 600 block of H Street, in NE Washington.', [('600 block of H Street NE', '')])

    def test_postdir_with_in_prefix_washington_dc09(self):
        self.assertParses('It happened on the 600 block of H Street, in the NE quadrant', [('600 block of H Street NE', '')])

    def test_postdir_with_in_prefix_washington_dc10(self):
        self.assertParses('It happened on the 600 block of H Street, in the NE quadrant', [('600 block of H Street NE', '')])

    def test_postdir_with_in_prefix_washington_dc11(self):
        self.assertParses('It happened on the 600 block of H Street, in far northeast Washington.', [('600 block of H Street northeast', '')])

    def test_postdir_seattle(self):
        self.assertParses('Sunday, August 24 at Camp Long, 5200 35th Ave. SW.', [('5200 35th Ave. SW.', '')])

    def test_postdir_comma(self):
        self.assertParses('It happened at 123 Main St., NE, yesterday.', [('123 Main St., NE', '')])

    def test_postdir_not_greedy1(self):
        self.assertParses('at 1620 S. Jackson St. Executive Director Hilary Stern said', [('1620 S. Jackson St.', '')])

    def test_postdir_not_greedy2(self):
        self.assertParses('at 1620 S. Jackson St., Executive Director Hilary Stern said', [('1620 S. Jackson St.', 'Executive Director Hilary Stern')])

    def test_postdir_neighborhood(self):
        self.assertParses('Start at Prezza, 24 Fleet St., North End, 6:30 p.m. $50.', [('Start at Prezza', ''), ('24 Fleet St.', 'North End')])

    def test_one_letter_street(self):
        self.assertParses('It happened at 77 K St.', [('77 K St.', '')])

    def test_one_letter_street_postdir(self):
        self.assertParses('It happened at 77 K St. NE.', [('77 K St. NE.', '')])

    def test_one_letter_street_avenue_x(self):
        self.assertParses('It happened at 1823 Avenue X.', [('1823 Avenue X', '')])

    def test_one_letter_street_avenue_x_abbreviated1(self):
        self.assertParses('It happened at 1823 Ave. X.', [('1823 Ave. X', '')])

    def test_one_letter_street_avenue_x_abbreviated2(self):
        self.assertParses('It happened at 1823 Ave X.', [('1823 Ave X', '')])

    def test_one_letter_street_avenue_x_control(self):
        self.assertParses('It happened at 1823 Main Ave. X marks the spot.', [('1823 Main Ave.', '')])

    def test_one_letter_sanity_check(self):
        self.assertParses('More than 77 ATLASES.', [])

    def test_one_letter_sanity_check2(self):
        self.assertParses('Home prices in 20 U.S. metropolitan areas dropped 15.8 percent in May', [])

    def test_abbreviation_mass(self):
        self.assertParses('It happened at 472 Mass. Ave.', [('472 Mass. Ave.', '')])

class FalsePositives(AddressParsing):
    def test_false_positive_st(self):
        self.assertParses('Copyright 2004-2007 Gothamist', [('2004-2007 Gothamist', '')])

    def test_associated_press(self):
        self.assertParses('Copyright 2008 Associated Press', [])

    def test_university_of_texas1(self):
        self.assertParses('She attends University of Texas at Austin.', [])

    def test_university_of_texas2(self):
        self.assertParses('She attends University Of Texas at Austin.', [])

    def test_university_of_texas3(self):
        self.assertParses('She attends UNIVERSITY OF TEXAS at Austin.', [])

    def test_date1(self):
        self.assertParses('Posted: Friday, 22 May 2009 1:45PM', [])

    def test_date2(self):
        self.assertParses('Posted: Friday, 22 May, 2009 1:45PM', [])

    def test_date3(self):
        self.assertParses('It is scheduled for 22 May 2009.', [])

    def test_date4(self):
        self.assertParses('It is scheduled for 22 October 2009.', [])

class NumberedStreets(AddressParsing):
    def test_block(self):
        self.assertParses('1500 block of 16th Avenue', [('1500 block of 16th Avenue', '')])

    def test_address(self):
        self.assertParses('1500 16th Avenue', [('1500 16th Avenue', '')])

    def test_street_dir1(self):
        self.assertParses('1500 N. 16th Avenue', [('1500 N. 16th Avenue', '')])

    def test_street_number_only(self):
        self.assertParses('327 E. 93 St.', [('327 E. 93 St.', '')])

    def test_suffix_missing1(self):
        self.assertParses('327 E. 93rd', [('327 E. 93rd', '')])

    def test_suffix_missing2(self):
        self.assertParses('327 East 93rd', [('327 East 93rd', '')])

    def test_suffix_missing_plus_period(self):
        self.assertParses('I live at 327 E. 93rd. Where do you live?', [('327 E. 93rd', '')])

    def test_suffix_no_dir(self):
        self.assertParses('327 93rd', [('327 93rd', '')])

    def test_d_suffix(self):
        self.assertParses('327 93d', [('327 93d', '')])

    def test_street_number_only_no_dir(self):
        self.assertParses('327 93 St.', [('327 93 St.', '')])

    def test_street_number_only_suffix_missing(self):
        self.assertParses('327 E. 93', [('327 E', '')]) # TODO: This test passes but is incorrect behavior.

    def test_false_positive1(self):
        self.assertParses('150 61 Year Olds', [('61 Year Olds', '')])

class Intersections(AddressParsing):
    def test_and(self):
        self.assertParses('Near Ashland Ave. and Division St.', [('Ashland Ave. and Division St.', '')])

    def test_at(self):
        self.assertParses('Near Ashland Ave. at Division St.', [('Ashland Ave. at Division St.', '')])

    def test_prefix_at(self):
        self.assertParses('At Ashland Ave. and Division St.', [('Ashland Ave. and Division St.', '')])

    def test_prefix_on(self):
        self.assertParses('building on Ashland Ave. and Division St.', [('Ashland Ave. and Division St.', '')])

    def test_prefix_around(self):
        self.assertParses('Around Ashland Ave. and Division St.', [('Ashland Ave. and Division St.', '')])

    def test_prefix_corner_of(self):
        self.assertParses('At the corner of Ashland Ave. and Division St.', [('Ashland Ave. and Division St.', '')])

    def test_area_of(self):
        self.assertParses('In the area of Ashland Ave. and Division St.', [('Ashland Ave. and Division St.', '')])

    def test_prefix_area_surrounding(self):
        self.assertParses('In the area surrounding Ashland Ave. and Division St.', [('Ashland Ave. and Division St.', '')])

    def test_prefix_areas_surrounding(self):
        self.assertParses('In the areas surrounding Ashland Ave. and Division St.', [('Ashland Ave. and Division St.', '')])

    def test_prefix_located_on(self):
        self.assertParses('Holy Family Parish located on Roosevelt and May streets', [('Roosevelt and May', '')])

    def test_prefix_vicinity_of(self):
        self.assertParses('Holy Family Parish located at the vicinity of Roosevelt and May streets', [('Roosevelt and May', '')])

    def test_prefix_ran_down(self):
        self.assertParses('The man punched the people as he ran down Washington Street near Dearborn Street, said police.', [('Washington Street near Dearborn Street', '')])

    def test_prefix_running_down(self):
        self.assertParses('The man punched the people while running down Washington Street near Dearborn Street, said police.', [('Washington Street near Dearborn Street', '')])

    def test_to(self):
        self.assertParses('On November 28th, at 10:31pm, officers from District 2 responded to George Street and Langdon Street for a report of a person shot.', [('George Street and Langdon Street', '')])

    def test_that(self):
        self.assertParses('The firm that Microsoft and Apple both tried to buy.', [('Microsoft and Apple', '')])

    def test_directionals1(self):
        self.assertParses('Near N. Ashland Ave. at W. Division St.', [('N. Ashland Ave. at W. Division St.', '')])

    def test_directionals2(self):
        self.assertParses('Near N Ashland Ave. at W Division St.', [('N Ashland Ave. at W Division St.', '')])

    def test_directionals3(self):
        self.assertParses('The el station at N Ashland Ave. at W Division St.', [('N Ashland Ave. at W Division St.', '')])

    def test_address_confusion(self):
        self.assertParses('Around 1200 N. Ashland Ave. at Division St.', [('1200 N. Ashland Ave.', '')])

    def test_intersection1(self):
        self.assertParses('at the intersection of Ashland Ave. and Division St. earlier today', [('Ashland Ave. and Division St.', '')])

    def test_intersection2(self):
        self.assertParses('at the intersection of Ashland Ave. & Division St. earlier today', [('Ashland Ave. & Division St.', '')])

    def test_intersection3(self):
        self.assertParses('at the intersection of Ashland Ave. at Division St. earlier today', [('Ashland Ave. at Division St.', '')])

    def test_intersection4(self):
        self.assertParses('at the intersection of Ashland and Division.', [('Ashland and Division', '')])

    def test_intersection5(self):
        self.assertParses('at the intersection of Ashland near Division.', [('Ashland near Division', '')])

    def test_toward(self):
        self.assertParses('running on Ashland toward Division.', [('Ashland toward Division', '')])

    def test_toward2(self):
        self.assertParses('running on Ashland towards Division.', [('Ashland towards Division', '')])

    def test_north_of(self):
        self.assertParses('on Pulaski Road north of West Lake Street about 3:30 p.m.', [('Pulaski Road north of West Lake Street', '')])

    def test_south_of(self):
        self.assertParses('on Pulaski Road south of West Lake Street about 3:30 p.m.', [('Pulaski Road south of West Lake Street', '')])

    def test_east_of(self):
        self.assertParses('on Pulaski Road east of West Lake Street about 3:30 p.m.', [('Pulaski Road east of West Lake Street', '')])

    def test_west_of(self):
        self.assertParses('on Pulaski Road west of West Lake Street about 3:30 p.m.', [('Pulaski Road west of West Lake Street', '')])

    def test_just_north_of(self):
        self.assertParses('on Pulaski Road just north of West Lake Street about 3:30 p.m.', [('Pulaski Road just north of West Lake Street', '')])

    def test_just_south_of(self):
        self.assertParses('on Pulaski Road just south of West Lake Street about 3:30 p.m.', [('Pulaski Road just south of West Lake Street', '')])

    def test_just_east_of(self):
        self.assertParses('on Pulaski Road just east of West Lake Street about 3:30 p.m.', [('Pulaski Road just east of West Lake Street', '')])

    def test_just_west_of(self):
        self.assertParses('on Pulaski Road just west of West Lake Street about 3:30 p.m.', [('Pulaski Road just west of West Lake Street', '')])

    def test_past(self):
        self.assertParses('on Pulaski Road past West Lake Street about 3:30 p.m.', [('Pulaski Road past West Lake Street', '')])

    def test_just_past(self):
        self.assertParses('on Pulaski Road just past West Lake Street about 3:30 p.m.', [('Pulaski Road just past West Lake Street', '')])

    def test_around(self):
        self.assertParses('on Pulaski Road around West Lake Street about 3:30 p.m.', [('Pulaski Road around West Lake Street', '')])

    def test_crossed(self):
        self.assertParses('as she crossed 122nd Street at Broadway at 3 p.m. while driving', [('122nd Street at Broadway', '')])

    def test_off(self):
        self.assertParses('waiting for a bus on Woodland Road off Eastway Drive late Saturday', [('Woodland Road off Eastway Drive', '')])

    def test_postdir1(self):
        self.assertParses('at Ashland and Division NE', [('Ashland and Division NE', '')])

    def test_postdir2(self):
        self.assertParses('at the corner of 12th St and Maryland Avenue, NE, one block away.', [('12th St and Maryland Avenue, NE', '')])

    def test_postdir3(self):
        self.assertParses('It is Rain City Yoga on Roosevelt and 50th. Earlier this year, the cafe closed.', [('Roosevelt and 50th', '')])

    def test_list_of_intersections(self):
        self.assertParses('The testing will occur in the areas of 18th St & Mission, 22nd St. & Valencia, 23rd St and Folsom, and 18th St and Bryant.', [('18th St & Mission', ''), ('22nd St. & Valencia', ''), ('23rd St and Folsom', ''), ('18th St and Bryant', '')])

    def test_address_multiple_spaces(self):
        self.assertParses('In the area of 18th  St and Mission  Rd', [('18th  St and Mission  Rd', '')])

    def test_one_letter_street_avenue_x(self):
        self.assertParses('At the intersection of Avenue X and Avenue Y.', [('Avenue X and Avenue Y', '')])

    def test_one_letter_street_avenue_x_abbreviated(self):
        self.assertParses('At the intersection of Ave. X and Ave. Y.', [('Ave. X and Ave. Y', '')])

    def test_ignore_intersection_after_between1(self):
        self.assertParses('1060 E 47th St between Ellis and Greenwood Aves', [('1060 E 47th St', '')])

    def test_ignore_intersection_after_between2(self):
        self.assertParses('1060 E 47th St (between Ellis and Greenwood Aves)', [('1060 E 47th St', '')])

    def test_ignore_intersection_after_between_control(self):
        self.assertParses('E 47th St between Ellis and Greenwood Aves', [('E 47th St between Ellis and Greenwood Aves', '')])

class SegmentParsing(AddressParsing):
    def test_basic01(self):
        self.assertParses('Wabash between Adams and Jackson', [('Wabash between Adams and Jackson', '')])

    def test_basic02(self):
        self.assertParses('Wabash from Adams to Jackson', [('Wabash from Adams to Jackson', '')])

    def test_comma01(self):
        self.assertParses('Wabash, between Adams and Jackson', [('Wabash, between Adams and Jackson', '')])

    def test_comma02(self):
        self.assertParses('Wabash, from Adams to Jackson', [('Wabash, from Adams to Jackson', '')])

    def test_comma03(self):
        self.assertParses('Wabash, between Adams, and Jackson', [('Wabash, between Adams, and Jackson', '')])

    def test_comma04(self):
        self.assertParses('Wabash, from Adams, to Jackson', [('Wabash, from Adams, to Jackson', '')])

    def test_withcity01(self):
        self.assertParses('Wabash between Adams and Jackson, Chicago', [('Wabash between Adams and Jackson', 'Chicago')])

    def test_withcity02(self):
        self.assertParses('Wabash between Adams and Jackson in Chicago', [('Wabash between Adams and Jackson', 'Chicago')])

class CityAddressParsing(AddressParsing):
    def test_comma1(self):
        self.assertParses('3000 S. Wabash Ave., Chicago', [('3000 S. Wabash Ave.', 'Chicago')])

    def test_comma2(self):
        self.assertParses('3000 Wabash Ave., Chicago', [('3000 Wabash Ave.', 'Chicago')])

    def test_comma3(self):
        self.assertParses('3000 Wabash Ave.,    Chicago', [('3000 Wabash Ave.', 'Chicago')])

    def test_in1(self):
        self.assertParses('3000 S. Wabash Ave. in Chicago', [('3000 S. Wabash Ave.', 'Chicago')])

    def test_in2(self):
        self.assertParses('3000 Wabash Ave. in Chicago', [('3000 Wabash Ave.', 'Chicago')])

    def test_in_comma1(self):
        self.assertParses('3000 Wabash Ave., in Chicago', [('3000 Wabash Ave.', 'Chicago')])

    def test_in_comma2(self):
        self.assertParses('3000 Wabash Ave.,    in Chicago', [('3000 Wabash Ave.', 'Chicago')])

    def test_multiple(self):
        self.assertParses('3000 Wabash Ave. in Chicago and 123 Main St. in Boston', [('3000 Wabash Ave.', 'Chicago'), ('123 Main St.', 'Boston')])

    def test_intersection1(self):
        self.assertParses('at Adams and Wabash in Chicago', [('Adams and Wabash', 'Chicago')])

    def test_intersection2(self):
        self.assertParses('at Adams and Wabash, Chicago', [('Adams and Wabash', 'Chicago')])

    def test_postdir_comma1(self):
        self.assertParses('3000 Wabash Ave., SW, Chicago', [('3000 Wabash Ave., SW', 'Chicago')])

    def test_postdir_comma2(self):
        self.assertParses('3000 Wabash Ave., SW, in Chicago', [('3000 Wabash Ave., SW', 'Chicago')])

    def test_abbreviation_mass(self):
        self.assertParses('It happened at 472 Mass. Ave. in Cambridge', [('472 Mass. Ave.', 'Cambridge')])

    def test_hyphen(self):
        self.assertParses('Happened at 121 Maple Street, Croton-on-Hudson', [('121 Maple Street', 'Croton-on-Hudson')])

class EdgeCases(AddressParsing):
    def test_uppercase_named_street(self):
        self.assertParses('2826 S. WENTWORTH', [('2826 S. WENTWORTH', '')])

class PhraseTagger(unittest.TestCase):
    def test_double_matching(self):
        # Make sure matching behaves as greedily as possible
        places = ['Lake View', 'Lake View East']
        text = 'In Lake View East today, a Lake View man...'
        tag = phrase_tagger(places)
        self.assertEqual(tag(text), 'In <span>Lake View East</span> today, a <span>Lake View</span> man...')

    def test_empty_phrases(self):
        # Make sure an empty phrase list doesn't result in matching everything
        phrases = []
        text = 'In Lake View East today, a Lake View man...'
        tag = phrase_tagger(phrases)
        self.assertEqual(tag(text), 'In Lake View East today, a Lake View man...')

    def test_matched_phrases_begin(self):
        # Don't try to re-highlight things that have already been highlighted
        phrases = ['South Chicago']
        text = 'on the <addr>South Chicago Ave on the 7400 block</addr>...'
        tag = phrase_tagger(phrases, pre='<addr>', post='</addr>')
        self.assertEqual(tag(text), text)

    def test_matched_phrases_end(self):
        # Don't try to re-highlight things that have already been highlighted
        phrases = ['South Chicago']
        text = 'on the <addr>7400 block of South Chicago</addr>...'
        tag = phrase_tagger(phrases, pre='<addr>', post='</addr>')
        self.assertEqual(tag(text), text)

    def test_matched_phrases_middle(self):
        # Don't try to re-highlight things that have already been highlighted
        phrases = ['South Chicago']
        text = 'on the <addr>7400 block of South Chicago Ave</addr>...'
        tag = phrase_tagger(phrases, pre='<addr>', post='</addr>')
        self.assertEqual(tag(text), text)

if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = dbf
"""
Functions that deal with DBF files.
"""

import struct, datetime, decimal, itertools

# From http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/362715
def reader(f, strip_values=False):
    """Returns an iterator over records in a Xbase DBF file.

    The first row returned contains the field names.
    The second row contains field specs: (type, size, decimal places).
    Subsequent rows contain the data records.
    If a record is marked as deleted, it is skipped.

    File should be opened for binary reads.

    If strip_values is True, then all string values will be stripped of leading
    and trailing whitespace.

    Example usage:

        f = open('/path/to/somefile.dbf', 'rb')
        db = list(reader(f))
        f.close()
        for record in db:
            print record
        fieldnames, fieldspecs, records = db[0], db[1], db[2:]
    """
    # See DBF format spec at:
    #     http://www.pgts.com.au/download/public/xbase.htm#DBF_STRUCT

    numrec, lenheader = struct.unpack('<xxxxLH22x', f.read(32))    
    numfields = (lenheader - 33) // 32

    fields = []
    for fieldno in xrange(numfields):
        name, typ, size, deci = struct.unpack('<11sc4xBB14x', f.read(32))
        name = name.replace('\0', '')       # eliminate NULs from string   
        fields.append((name, typ, size, deci))
    yield [field[0] for field in fields]
    yield [tuple(field[1:]) for field in fields]

    terminator = f.read(1)
    if terminator != '\r':
        raise ValueError('Got unhandled terminator %r' % terminator)

    fields.insert(0, ('DeletionFlag', 'C', 1, 0))
    fmt = ''.join(['%ds' % fieldinfo[2] for fieldinfo in fields])
    fmtsiz = struct.calcsize(fmt)
    for i in xrange(numrec):
        record = struct.unpack(fmt, f.read(fmtsiz))
        if record[0] != ' ':
            continue                        # deleted record
        result = []
        for (name, typ, size, deci), value in itertools.izip(fields, record):
            if name == 'DeletionFlag':
                continue
            if typ == "N":
                value = value.replace('\0', '').lstrip()
                if value == '':
                    value = 0
                elif deci:
                    value = decimal.Decimal(value)
                else:
                    value = int(value)
            elif typ == 'D':
                try:
                    y, m, d = int(value[:4]), int(value[4:6]), int(value[6:8])
                except ValueError:
                    value = None
                else:
                    value = datetime.date(y, m, d)
            elif typ == 'L':
                value = (value in 'YyTt' and 'T') or (value in 'NnFf' and 'F') or '?'
            elif strip_values:
                value = value.strip()
            result.append(value)
        yield result

def dict_reader(f, strip_values=False):
    """
    Returns an iterator that yields a dictionary for every record in the given
    file-like object.
    
    Make sure the given object is a DBF file and was opened in binary mode --
    open('foo', 'rb').
    """
    r = reader(f, strip_values)
    fields = r.next()
    specs = r.next()
    for record in r:
        yield dict(zip(fields, record))

def writer(f, fieldnames, fieldspecs, records):
    """ Return a string suitable for writing directly to a binary dbf file.

    File f should be open for writing in a binary mode.

    Fieldnames should be no longer than ten characters and not include \x00.
    Fieldspecs are in the form (type, size, deci) where
        type is one of:
            C for ascii character data
            M for ascii character memo data (real memo fields not supported)
            D for datetime objects
            N for ints or decimal objects
            L for logical values 'T', 'F', or '?'
        size is the field width
        deci is the number of decimal places in the provided decimal object
    Records can be an iterable over the records (sequences of field values).
    """
    # header info
    ver = 3
    now = datetime.datetime.now()
    yr, mon, day = now.year-1900, now.month, now.day
    numrec = len(records)
    numfields = len(fieldspecs)
    lenheader = numfields * 32 + 33
    lenrecord = sum(field[1] for field in fieldspecs) + 1
    hdr = struct.pack('<BBBBLHH20x', ver, yr, mon, day, numrec, lenheader, lenrecord)
    f.write(hdr)
                      
    # field specs
    for name, (typ, size, deci) in itertools.izip(fieldnames, fieldspecs):
        name = name.ljust(11, '\x00')
        fld = struct.pack('<11sc4xBB14x', name, typ, size, deci)
        f.write(fld)

    # terminator
    f.write('\r')

    # records
    for record in records:
        f.write(' ')                        # deletion flag
        for (typ, size, deci), value in itertools.izip(fieldspecs, record):
            if value is None:
                value = ' ' * size
            elif typ == "N":
                value = str(value).rjust(size, ' ')
            elif typ == 'D':
                value = value.strftime('%Y%m%d')
            elif typ == 'L':
                value = str(value)[0].upper()
            else:
                value = str(value)[:size].ljust(size, ' ')
            assert len(value) == size
            f.write(value)

    # End of file
    f.write('\x1A')

if __name__ == "__main__":
    import sys
    from pprint import pprint
    f = open(sys.argv[1], 'rb')
    for record in dict_reader(f, True):
        pprint(record)

########NEW FILE########
__FILENAME__ = excel
"""
Utilities for reading Microsoft Excel files.
"""

import xlrd
import datetime

class ExcelDictReader(object):
    """
    Provides an API that lets you iterate over every row in an Excel worksheet,
    much like csv.DictReader. This assumes that the worksheet is a simple table
    with a single header row at the top.

    header_row_num is the zero-indexed row number of the headers. (Note that
    you can specify the headers manually by using the "custom_headers"
    argument.)

    start_row_num is the zero-indexed row number of where the data starts.

    use_last_header_if_duplicate, either True or False, dictates the behavior
    to use in the case of duplicate column headers. If True, then the *last*
    column's value will be used. If False, then the *first* column's value will
    be used. Note that there's no way to access the other column, either way.

    custom_headers, if given, will be used instead of the values in
    header_row_num. If you provide custom_headers, the value of header_row_num
    will be ignored.

    Example usage:
        reader = ExcelDictReader('/path/to/my.xls', 0, 0, 1)
        for row in reader:
            print row

    This yields dictionaries like:
        {'header1': 'value1', 'header2': 'value2'}
    """
    def __init__(self, filename, sheet_index=0, header_row_num=0, start_row_num=0,
            use_last_header_if_duplicate=True, custom_headers=None):
        self.workbook = xlrd.open_workbook(filename)
        self.sheet_index = sheet_index
        self.header_row, self.start_row = header_row_num, start_row_num
        self.use_last_header_if_duplicate = use_last_header_if_duplicate
        self.custom_headers = custom_headers

    def __iter__(self):
        worksheet = self.workbook.sheet_by_index(self.sheet_index)
        if self.custom_headers:
            headers = self.custom_headers
        else:
            headers = [v.value.strip() for v in worksheet.row(self.header_row)]
        for row_num in xrange(self.start_row, worksheet.nrows):
            data_dict = {}
            for i, cell in enumerate(worksheet.row(row_num)):
                value = cell.value

                # Clean up the value. The xlrd library doesn't convert date
                # values to Python objects automatically, so we have to do that
                # here. Also, strip whitespace from any text field.
                # cell.ctype is documented here:
                # http://www.lexicon.net/sjmachin/xlrd.html#xlrd.Cell-class
                if cell.ctype == 3:
                    try:
                        value = datetime.datetime(*xlrd.xldate_as_tuple(value, self.workbook.datemode))
                    except ValueError:
                        # The datetime module raises ValueError for invalid
                        # dates, like the year 0. Rather than skipping the
                        # value (which would lose data), we just keep it as
                        # a string.
                        pass
                elif cell.ctype == 1:
                    value = value.strip()

                # Only append the value to the dictionary if 
                if self.use_last_header_if_duplicate or headers[i] not in data_dict:
                    data_dict[headers[i]] = value

            yield data_dict

########NEW FILE########
__FILENAME__ = mdb
"""
Utilities for reading data from Microsoft Access MDB files.

These require the mdbtools binaries, available here:
    http://mdbtools.sourceforge.net/
    http://prdownloads.sourceforge.net/mdbtools/mdbtools-0.5.tar.gz
    sudo apt-get install mdbtools
"""

import csv
from subprocess import Popen, PIPE

def list_tables(filename):
    """
    Returns a list of all the table names in the given MDB filename.
    """
    # Tell it to delimit the names with the pipe character.
    output = Popen(["mdb-tables", "-d", '|', "-t", "table", filename], stdout=PIPE).communicate()[0]
    return [t.strip() for t in output.split('|') if t.strip()]

class TableReader(csv.DictReader):
    """
    Like csv.DictReader, but it takes the MDB filename and table name.
    
    Example usage:
        for row in TableReader('mydb.mdb', 'some_table'):
            print row
    """
    def __init__(self, filename, table_name):
        f = Popen(['mdb-export', '-D', '%Y-%m-%d', filename, table_name], stdout=PIPE).stdout
        csv.DictReader.__init__(self, f)

########NEW FILE########
__FILENAME__ = pdftotext
"""
Utilities for reading data from PDF files.

These require the pdftotext binary, available in the Xpdf package:
    http://www.foolabs.com/xpdf/download.html
"""

import os

PDFTOTEXT_BINARY = 'pdftotext'

def pdf_to_text(filename, keep_layout=True, raw=False):
    """
    Returns the text of the PDF with the given filename on the local filesystem.
    """
    if keep_layout and raw:
        raise ValueError('The "keep_layout" and "raw" arguments may not be used together')
    options = []
    if keep_layout:
        options.append('-layout')
    if raw:
        options.append('-raw')
    cmd = "%s %s '%s' -" % (PDFTOTEXT_BINARY, ' '.join(options), filename)
    return os.popen(cmd).read()

def pdfstring_to_text(pdf_string, keep_layout=True, raw=False):
    """
    Returns the text of the given PDF (provided as a string).
    """
    import os
    from tempfile import mkstemp
    fd, name = mkstemp()
    fp = os.fdopen(fd, 'wb')
    fp.write(pdf_string)
    fp.close()
    try:
        result = pdf_to_text(name, keep_layout, raw)
    finally:
        os.unlink(name)
    return result

########NEW FILE########
__FILENAME__ = unicodecsv
"""
Support for reading CSV as Unicode objects.

This module is necessary because Python's csv library doesn't support reading
Unicode strings.

This code is mostly copied from the Python documentation:
http://www.python.org/doc/2.5.2/lib/csv-examples.html
The changes we've made are to implement a DictReader instead of a normal
Reader.
"""

import csv
import codecs

class UTF8Recoder:
    """
    Iterator that reads an encoded stream and reencodes the input to UTF-8.
    """
    def __init__(self, f, encoding):
        self.reader = codecs.getreader(encoding)(f)

    def __iter__(self):
        return self

    def next(self):
        return self.reader.next().encode('utf-8')

class UnicodeDictReader:
    """
    A CSV dict reader which will iterate over lines in the CSV file "f",
    which is encoded in the given encoding. Results will always be Unicode
    objects instead of bytestrings.
    """
    def __init__(self, f, fieldnames, dialect=csv.excel, encoding='utf-8', **kwargs):
        f = UTF8Recoder(f, encoding)
        self.fieldnames = fieldnames
        self.reader = csv.reader(f, dialect=dialect, **kwargs)

    def next(self):
        row = self.reader.next()
        row = [unicode(s, 'utf-8') for s in row]
        return dict(zip(self.fieldnames, row))

    def __iter__(self):
        return self

########NEW FILE########
__FILENAME__ = log
# This module is registered by ebdata.retrieval.__init__ so that
# client code can simply use 'import logging' instead of having to set up
# the logging infrastructure on a case-by-case basis.
#
# Client code should use this like so:
#     import logging
#     # logger name should start with 'eb.retrieval.' to use this.
#     logger = logging.getLogger('eb.retrieval.scrapers.chicago.crime')
#     logger.warn('...')
#     logger.info('...')

from django.conf import settings
import logging
import logging.handlers

# Set up the file handler.
logfile = logging.FileHandler(settings.SCRAPER_LOGFILE_NAME)
logfile.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s %(name)-12s: %(levelname)-8s %(message)s')
logfile.setFormatter(formatter)

# Set up the e-mail handler.
class EbdataSMTPHandler(logging.handlers.SMTPHandler):
    def getSubject(self, record):
        return '%s[Log error]: %s' % (settings.EMAIL_SUBJECT_PREFIX, record.name)

emailer = EbdataSMTPHandler(settings.EMAIL_HOST, "example@example.com",
    [a[1] for a in settings.ADMINS], '')
emailer.setLevel(logging.WARNING)

# Add the handler to the 'eb.retrieval' logger. This means that only loggers
# whose names begin with 'eb.retrieval.' will get this functionality.
eb_root_logger = logging.getLogger('eb.retrieval')
eb_root_logger.addHandler(logfile)
eb_root_logger.addHandler(emailer)

# Set the logger's threshold to INFO. By default, it seems to ignore everything
# under the level WARNING.
eb_root_logger.setLevel(logging.DEBUG)

########NEW FILE########
__FILENAME__ = log_debug
"""
Import this file to send all 'eb.retrieval' messages (of any level, including
logger.DEBUG) to the console. This is convenient for debugging.
"""

from ebdata.retrieval import log
import logging

# Send all DEBUG messages to the console.
printer = logging.StreamHandler()
printer.setLevel(logging.DEBUG)
eb_root_logger = logging.getLogger('eb.retrieval')
eb_root_logger.addHandler(printer)

# Remove the e-mail handler.
eb_root_logger.removeHandler(log.emailer)

########NEW FILE########
__FILENAME__ = models
from django.db import models
from ebpub.db.models import NewsItem, Schema

LIST_PAGE = 1
DETAIL_PAGE = 2
PAGE_TYPE_CHOICES = (
    (LIST_PAGE, 'list'),
    (DETAIL_PAGE, 'detail')
)

class ScrapedPage(models.Model):
    page_type = models.SmallIntegerField(choices=PAGE_TYPE_CHOICES)
    schema = models.ForeignKey(Schema)
    when_crawled = models.DateTimeField()
    url = models.URLField()
    html = models.TextField()

    def __unicode__(self):
        return u'HTML from %s - %s' % (self.url, self.when_crawled)

class NewsItemHistoryManager(models.Manager):
    def save_page_if_needed(self, page):
        """Call save() on a Page object if it hasn't already been saved."""
        if page is not None and page.pk is None:
            page.save()

    def record_history(self, news_item, page):
        """Associates a page with a NewsItem."""
        self.save_page_if_needed(page)
        self.create(news_item=news_item, page=page)

class NewsItemHistory(models.Model):
    """Essentially a ManyToMany relation between ScrapedPage and NewsItem"""
    news_item = models.ForeignKey(NewsItem)
    page = models.ForeignKey(ScrapedPage)
    objects = NewsItemHistoryManager()

########NEW FILE########
__FILENAME__ = retrievers
"""
Retriever classes.

A Retriever class simply knows how to retrieve a resource off of the Web. It
knows nothing about *scraping*, i.e., parsing the contents of Web pages.
"""

import httplib2
from Cookie import SimpleCookie, CookieError
from urllib import urlencode
from urlparse import urljoin
import logging
import time
import socket

class RetrievalError(Exception):
    "Couldn't retrieve data"
    pass

class PageNotFoundError(RetrievalError):
    "Couldn't retrieve data"
    pass


class Default:
    # Used to determine whether a default argument was given to Retriever.__init__().
    pass

LOG_ENTRY_FMT = "%(timestamp)s\t%(method)s\t%(uri)s\t%(status)s\t%(elapsed)s\t%(size)s"

class Retriever(object):
    'HTTP client.'
    def __init__(self, user_agent=None, cache=Default, timeout=20, sleep=0):
        # Use cache=None to explicitly turn off caching.
        # If you don't provide cache, then it will cache in
        # settings.HTTP_CACHE, or '/tmp/eb_scraper_cache' if
        # the setting is undefined.
        # sleep should be the number of seconds to sleep between requests.
        from django.conf import settings
        if cache is Default:
            cache = getattr(settings, 'HTTP_CACHE', '/tmp/eb_scraper_cache')
        self.h = httplib2.Http(cache, timeout=timeout)
        self.h.force_exception_to_status_code = False
        self.h.follow_redirects = False
        self.user_agent = user_agent or 'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.0)'
        self._cookies = SimpleCookie()
        self.logger = logging.getLogger('eb.retrieval.retriever')
        self.sleep = sleep

        # Keep track of whether we've downloaded any pages yet.
        # This makes sure we don't sleep before the very first requested page.
        self.page_downloaded = False

    def clear_cookies(self):
        self._cookies = SimpleCookie()

    def get_html_and_headers(self, uri, data=None, headers=None, send_cookies=True, follow_redirects=True, raise_on_error=True):
        "Retrieves the resource and returns a tuple of (content, header dictionary)."
        # Sleep, if necessary, but only if a page has already been downloaded
        # with this retriever. (We don't want to sleep before the very first
        # request that a retriever makes, because that would be unnecessary.)
        if self.sleep and self.page_downloaded:
            self.logger.debug('Sleeping for %s seconds', self.sleep)
            time.sleep(self.sleep)
        self.page_downloaded = True

        # Prepare the request.
        if not headers:
            headers = {}
        headers['user-agent'] = headers.get('user-agent', self.user_agent)
        if send_cookies and self._cookies:
            # Some broken ASP.NET servers put "\r\n" in there, so we replace
            # that with semicolon to get proper behavior.
            headers['Cookie'] = self._cookies.output(attrs=[], header='').strip().replace('\r\n', ';')
        method = data and "POST" or "GET"
        body = data and urlencode(data) or None
        if method == "POST" and body:
            headers.setdefault('Content-Type', 'application/x-www-form-urlencoded')

        # Get the response.
        resp_headers = None
        for attempt_number in range(3):
            self.logger.debug('Attempt %s: %s %s', attempt_number + 1, method, uri)
            if data:
                self.logger.debug('%r', data)
            try:
                resp_headers, content = self.h.request(uri, method, body=body, headers=headers)
                if resp_headers['status'] == '500':
                    self.logger.debug("Request got a 500 error: %s %s", method, uri)
                    continue # Try again.
                break
            except socket.timeout:
                self.logger.debug("Request timed out after %s seconds: %s %s", self.h.timeout, method, uri)
                continue # Try again.
            except socket.error, e:
                self.logger.debug("Got socket error: %s", e)
                continue # Try again.
            except httplib2.ServerNotFoundError:
                raise RetrievalError("Could not %s %r: server not found" % (method, uri))
        if resp_headers is None:
            raise RetrievalError("Request timed out 3 times: %s %s" % (method, uri))

        # Raise RetrievalError if necessary.
        if raise_on_error and resp_headers['status'] in ('400', '408', '500'):
            raise RetrievalError("Could not %s %r: HTTP status %s" % (method, uri, resp_headers['status']))
        if raise_on_error and resp_headers['status'] == '404':
            raise PageNotFoundError("Could not %s %r: HTTP status %s" % (method, uri, resp_headers['status']))

        # Set any received cookies.
        if 'set-cookie' in resp_headers:
            try:
                self._cookies.load(resp_headers['set-cookie'])
            except CookieError:
                # Skip invalid cookies.
                pass

        # Handle redirects that weren't caught by httplib2 for whatever reason.
        if follow_redirects and resp_headers['status'] in ('301', '302', '303'):
            try:
                new_location = resp_headers['location']
            except KeyError:
                raise RetrievalError('Got redirect, but the response was missing a "location" header. Headers were: %r' % resp_headers)
            self.logger.debug('Got %s redirect', resp_headers['status'])

            # Some broken Web apps send relative URLs in their "Location"
            # headers in redirects. Detect that and use urljoin() to get a full
            # URL.
            if not new_location.startswith('http://') and not new_location.startswith('https://'):
                new_location = urljoin(uri, new_location)
            # Clear the POST data, if any, so that we do a GET request.
            if data:
                data = {}
                del headers['Content-Type']
            return Retriever.get_html_and_headers(self, new_location, data, headers, send_cookies)

        return content, resp_headers

    def get_html(self, uri, data=None, headers=None, send_cookies=True, follow_redirects=True, raise_on_error=True):
        "Retrieves the resource and returns it as raw HTML."
        return self.get_html_and_headers(uri, data, headers, send_cookies, follow_redirects, raise_on_error)[0]

    def get_to_file(self, *args, **kwargs):
        """
        Downloads the given URI and saves it to a temporary file. Returns the
        full filename of the temporary file.
        """
        import os
        from tempfile import mkstemp
        fd, name = mkstemp()
        fp = os.fdopen(fd, 'wb')
        fp.write(self.get_html(*args, **kwargs))
        fp.close()
        return name

class UnicodeRetriever(Retriever):
    """
    Like Retriever, but get_html() returns a Unicode object instead of a
    bytestring. It uses the chardet module to determine the encoding to use.
    """
    def __init__(self, *args, **kwargs):
        # errors can be 'strict', 'ignore' or 'replace'. See Python docs.
        self.error_handling = kwargs.pop('errors', 'strict')
        Retriever.__init__(self, *args, **kwargs)

    def get_html_and_headers(self, *args, **kwargs):
        encoding, content, headers = self.get_encoding_html_and_headers(*args, **kwargs)
        return content.decode(encoding, self.error_handling), headers

    def get_encoding_html_and_headers(self, *args, **kwargs):
        """
        Returns a tuple of (encoding, html_bytestring, headers).

        This is useful if you don't know whether you want to decode the string
        until *after* calling this method. (Perhaps you want to inspect the
        headers.)
        """
        import chardet
        content, headers = Retriever.get_html_and_headers(self, *args, **kwargs)
        guess = chardet.detect(content)
        # Maybe this should take into account guess['confidence']?
        return guess['encoding'], content, headers

########NEW FILE########
__FILENAME__ = base
from ebdata.retrieval import Retriever
import datetime
import logging

class ScraperBroken(Exception):
    "Something changed in the underlying HTML and broke the scraper."
    pass

class BaseScraper(object):
    """
    Base class for all scrapers in ebdata.retrieval.scrapers.
    """
    logname = 'basescraper'
    sleep = 0

    def __init__(self, use_cache=True):
        if not use_cache:
            self.retriever = Retriever(cache=None, sleep=self.sleep)
        else:
            self.retriever = Retriever(sleep=self.sleep)
        self.logger = logging.getLogger('eb.retrieval.%s' % self.logname)
        self.start_time = datetime.datetime.now()

    def update(self):
        'Run the scraper.'
        raise NotImplementedError()

    def get_html(self, *args, **kwargs):
        return self.retriever.get_html(*args, **kwargs)

    @classmethod
    def parse_html(cls, html):
        from lxml import etree
        from cStringIO import StringIO
        return etree.parse(StringIO(html), etree.HTMLParser())

########NEW FILE########
__FILENAME__ = list_detail
from base import BaseScraper, ScraperBroken

class SkipRecord(Exception):
    "Exception that signifies a detail record should be skipped over."
    pass

class StopScraping(Exception):
    "Exception that signifies scraping should stop."
    pass

class ListDetailScraper(BaseScraper):
    """
    A screen-scraper optimized for list-detail types of sites.

    A list-detail site is a site that displays a list of records, which might
    be paginated. Each record might have its own page -- a "detail" page -- or
    the list page might display all available information for that record.

    To use this class, subclass it and implement the following:

        * list_pages()
        * Either parse_list() or parse_list_re
        * existing_record()
        * save()

    If the scraped site does not have detail pages, implement the following:

        * has_detail = False

    If the scraped site has detail pages, implement the following:

        * detail_required()
        * get_detail()
        * Either parse_detail() or parse_detail_re

    These are additional, optional hooks:

        * clean_list_record()
        * clean_detail_record()
    """

    ################################
    # MAIN METHODS FOR OUTSIDE USE #
    ################################

    def display_data(self):
        """
        Retrieves all pages, parses them and prints the data as Python
        dictionaries to standard output.

        This is mainly useful for debugging.
        """
        from pprint import pprint
        for d in self.raw_data():
            pprint(d)

    def raw_data(self):
        """
        Iterator that yields *all* current raw data for this scraper,
        regardless of whether it's existing or not.

        Each record is represented as a {'list', 'detail'} dictionary,
        where `list` is the clean list record and `detail` is the clean
        detail record.
        """
        for page in self.list_pages():
            for list_record in self.parse_list(page):
                try:
                    list_record = self.clean_list_record(list_record)
                except SkipRecord:
                    continue
                if self.has_detail:
                    try:
                        page = self.get_detail(list_record)
                        detail_record = self.parse_detail(page, list_record)
                        detail_record = self.clean_detail_record(detail_record)
                    except SkipRecord:
                        continue
                else:
                    detail_record = None
                yield {'list': list_record, 'detail': detail_record}

    def xml_data(self):
        """
        Iterator that yields *all* current raw data for this scraper,
        regardless of whether it's existing or not, as serialized XML.
        """
        from xml.sax.saxutils import escape
        yield u'<data>'
        for d in self.raw_data():
            yield u'<object>'
            for datatype in ('list', 'detail'):
                for k, v in d[datatype].items():
                    if not isinstance(v, basestring):
                        v = str(v)
                    yield u'  <att name="%s-%s">%s</att>' % (datatype[0], k, escape(v))
            yield u'</object>'
        yield u'</data>'

    def update(self):
        """
        The main scraping method. This retrieves all pages, parses them and
        saves the data.

        Subclasses should not have to override this method.
        """
        self.num_skipped = 0
        self.logger.info("update() started")
        try:
            for page in self.list_pages():
                try:
                    self.update_from_string(page)
                except StopScraping:
                    break
        finally:
            self.logger.info("update() finished")

    def update_from_string(self, page):
        """
        For scrapers with has_detail=False, runs the equivalent of update() on
        the given string.

        This is useful if you've got cached versions of HTML that you want to
        parse.

        Subclasses should not have to override this method.
        """
        for list_record in self.parse_list(page):
            try:
                list_record = self.clean_list_record(list_record)
            except SkipRecord, e:
                self.num_skipped += 1
                self.logger.debug("Skipping list record for %r: %s " % (list_record, e))
                continue
            except ScraperBroken, e:
                # Re-raise the ScraperBroken with some addtional helpful information.
                raise ScraperBroken('%r -- %s' % (list_record, e))
            self.logger.debug("Clean list record: %r" % list_record)

            old_record = self.existing_record(list_record)
            self.logger.debug("Existing record: %r" % old_record)

            if self.has_detail and self.detail_required(list_record, old_record):
                self.logger.debug("Detail page is required")
                try:
                    page = self.get_detail(list_record)
                    detail_record = self.parse_detail(page, list_record)
                    detail_record = self.clean_detail_record(detail_record)
                except SkipRecord, e:
                    self.num_skipped += 1
                    self.logger.debug("Skipping detail record for list %r: %s" % (list_record, e))
                    continue
                except ScraperBroken, e:
                    # Re-raise the ScraperBroken with some addtional helpful information.
                    raise ScraperBroken('%r -- %s' % (list_record, e))
                self.logger.debug("Clean detail record: %r" % detail_record)
            else:
                self.logger.debug("Detail page is not required")
                detail_record = None

            self.save(old_record, list_record, detail_record)

    def update_from_dir(self, dirname):
        """
        For scrapers with has_detail=False, runs the equivalent of update() on
        every file in the given directory, in sorted order.

        This is useful if you've got cached versions of HTML that you want to
        parse.

        Subclasses should not have to override this method.
        """
        import os
        filenames = os.listdir(dirname)
        filenames.sort()
        for filename in filenames:
            full_name = os.path.join(dirname, filename)
            self.logger.info("Reading from file %s" % full_name)
            page = open(full_name).read()
            self.update_from_string(page)

    ####################################################
    # INTERNAL METHODS THAT SUBCLASSES SHOULD OVERRIDE #
    ####################################################

    parse_list_re = None
    parse_detail_re = None
    has_detail = True

    def list_pages(self):
        """
        Iterator that yields list pages, as strings.

        Usually, this will only yield a single string, but it might yield
        multiple pages if the list is paginated.
        """
        raise NotImplementedError()

    def parse_list(self, page):
        """
        Given the full HTML of a list page, yields a dictionary of data for
        each record on the page.

        You can either implement this method or define a parse_list_re
        attribute. If you define a parse_list_re attribute, it should be set
        to a compiled regular-expression that finds all the records on a list
        page and uses named groups.
        """
        if self.parse_list_re is not None:
            count = 0
            for record in self.parse_list_re.finditer(page):
                yield record.groupdict()
                count += 1
            if count == 0:
                self.logger.info('%s.parse_list_re found NO records', self.__class__.__name__)
        else:
            raise NotImplementedError()

    def call_cleaners(self, record):
        """
        Given a dictionary returned by parse_list() or parse_detail(),
        calls any method defined whose name match a pattern based on a
        key in dictionary. The value at the key and the entire record
        are passed in as positional arguments. The patten is
        "_clean_KEY".

        For example, if the record contains a key "restaurant",
        call_cleaners() will call a method _clean_restaurant() if it
        exists.

        The _clean_KEY() callable should return a value that will
        replace the value at the key in the dictionary.

        It is up to the subclass's clean_list_record() and
        clean_detail_record() to call call_cleaners().
        """
        for key, value in record.items():
            meth_name = "_clean_%s" % key
            if hasattr(self, meth_name):
                method = getattr(self, meth_name)
                if callable(method):
                    record[key] = method(value, record)
        return record

    def clean_list_record(self, record):
        """
        Given a dictionary as returned by parse_list(), performs any
        necessary cleanup of the data and returns a dictionary.

        For example, this could convert date strings to datetime objects.
        """
        return record

    def existing_record(self, record):
        """
        Given a cleaned list record as returned by clean_list_record(), returns
        the existing record from the data store, if it exists.

        If an existing record doesn't exist, this should return None.
        """
        raise NotImplementedError()

    def detail_required(self, list_record, old_record):
        """
        Given a cleaned list record and the old record (which might be None),
        returns True if the scraper should download the detail page for this
        record.
        """
        raise NotImplementedError()

    def get_detail(self, record):
        """
        Given a cleaned list record as returned by clean_list_record, retrieves
        and returns the HTML for the record's detail page.
        """
        raise NotImplementedError()

    def parse_detail(self, page, list_record):
        """
        Given the full HTML of a detail page, returns a dictionary of data for
        the record represented on that page.

        You can either implement this method or define a parse_detail_re
        attribute. If you define a parse_detail_re attribute, it should be set
        to a compiled regular-expression that parses the record on a detail
        page and uses named groups.
        """
        if self.parse_detail_re is not None:
            m = self.parse_detail_re.search(page)
            if m:
                self.logger.debug('Got a match for parse_detail_re')
                return m.groupdict()
            self.logger.debug('Did not get a match for parse_detail_re')
            return {}
        else:
            raise NotImplementedError()

    def clean_detail_record(self, record):
        """
        Given a dictionary as returned by parse_detail(), performs any
        necessary cleanup of the data and returns a dictionary.

        For example, this could convert date strings to datetime objects.
        """
        return record

    def save(self, old_record, list_record, detail_record):
        """
        Saves the given record to storage.

        list_record and detail_record are both dictionaries representing the
        data from the list page and detail page, respectively. If the scraped
        site does not have detail pages, detail_record will be None.

        old_record is the existing record, as returned by existing_record(). It
        will be None if there is no existing record.
        """
        raise NotImplementedError()

class RssListDetailScraper(ListDetailScraper):
    """
    A ListDetailScraper for sites whose lists are RSS feeds.

    Subclasses should not have to implement parse_list() or get_detail().
    """
    def parse_list(self, page):
        # The page is an RSS feed, so use feedparser to parse it.
        import feedparser
        self.logger.debug("Parsing RSS feed with feedparser")
        feed = feedparser.parse(page)
        for entry in feed['entries']:
            yield entry

    def get_detail(self, record):
        # Assume that the detail page is accessible via the <link> for this
        # entry.
        return self.get_html(record['link'])

########NEW FILE########
__FILENAME__ = newsitem_list_detail
from django.conf import settings
from django.db import transaction
from ebdata.retrieval.scrapers.list_detail import ListDetailScraper
from ebdata.retrieval.utils import locations_are_close
from ebpub.db.models import Schema, NewsItem, Lookup, DataUpdate, field_mapping
from ebpub.geocoder import SmartGeocoder, GeocodingException, ParsingError
from ebpub.utils.text import address_to_block
import datetime

class NewsItemListDetailScraper(ListDetailScraper):
    """
    A ListDetailScraper that saves its data into the NewsItem table.

    Subclasses are required to set the `schema_slugs` attribute.

    self.schemas lazily loads the list of Schema objects the first time it's
    accessed. It is a dictionary in the format {slug: Schema}.

    self.schema is available if schema_slugs has only one element. It's the
    Schema object.

    self.lookups lazily loads a dictionary of all SchemaFields with
    lookup=True. The dictionary is in the format {name: schemafield}. If
    schema_slug has more than one element, self.lookups is a dictionary in the
    format {schema_slug: {name: schemafield}}.

    self.schema_field_mapping lazily loads a dictionary of each SchemaField,
    mapping the name to the real_name. If schema_slug has more than one element,
    self.schema_field_mapping is a dictionary in the format
    {schema_slug: {name: real_name}}.
    """
    schema_slugs = None
    logname = None

    def __init__(self, *args, **kwargs):
        if self.logname is None:
            self.logname = '%s.%s' % (settings.SHORT_NAME, self.schema_slugs[0])
        super(NewsItemListDetailScraper, self).__init__(*args, **kwargs)
        self._schema_cache = None
        self._schemas_cache = None
        self._lookups_cache = None
        self._schema_fields_cache = None
        self._schema_field_mapping_cache = None
        self._geocoder = SmartGeocoder()

    # schemas, schema, lookups and schema_field_mapping are all lazily loaded
    # so that this scraper can be run (in raw_data(), xml_data() or
    # display_data()) without requiring a valid database to be set up.

    def _get_schemas(self):
        if self._schemas_cache is None:
            self._schemas_cache = dict([(s, Schema.objects.get(slug=s)) for s in self.schema_slugs])
        return self._schemas_cache
    schemas = property(_get_schemas)

    def _get_schema(self):
        if self._schema_cache is None:
            if len(self.schema_slugs) > 1:
                raise AttributeError('self.schema is only available if len(schema_slugs) == 1')
            self._schema_cache = self.schemas[self.schema_slugs[0]]
        return self._schema_cache
    schema = property(_get_schema)

    def _get_lookups(self):
        if self._lookups_cache is None:
            lc = dict([(s.slug, dict([(sf.name, sf) for sf in s.schemafield_set.filter(is_lookup=True)])) for s in self.schemas.values()])
            if len(self.schema_slugs) == 1:
                lc = lc[self.schema_slugs[0]]
            self._lookups_cache = lc
        return self._lookups_cache
    lookups = property(_get_lookups)

    def _get_schema_fields(self):
        if self._schema_fields_cache is None:
            sfs = dict([(s.slug, dict([(sf.name, sf) for sf in s.schemafield_set.all()])) for s in self.schemas.values()])
            if len(self.schema_slugs) == 1:
                sfs = sfs[self.schema_slugs[0]]
            self._schema_fields_cache = sfs
        return self._schema_fields_cache
    schema_fields = property(_get_schema_fields)

    def _get_schema_field_mapping(self):
        if self._schema_field_mapping_cache is None:
            schema_objs = self.schemas.values()
            mapping = field_mapping([s.id for s in schema_objs])
            fm = dict([(s.slug, mapping[s.id]) for s in schema_objs])
            if len(self.schema_slugs) == 1:
                fm = fm[self.schema_slugs[0]]
            self._schema_field_mapping_cache = fm
        return self._schema_field_mapping_cache
    schema_field_mapping = property(_get_schema_field_mapping)

    def get_or_create_lookup(self, schema_field_name, name, code, description='', schema=None, make_text_slug=True):
        """
        Returns the Lookup instance matching the given Schema slug, SchemaField
        name and Lookup.code, creating it (with the given name/code/description)
        if it doesn't already exist.

        If make_text_slug is True, then a slug will be created from the given
        name. If it's False, then the slug will be the Lookup's ID.
        """
        if len(self.schema_slugs) > 1:
            sf = self.lookups[schema][schema_field_name]
        else:
            sf = self.lookups[schema_field_name]
        return Lookup.objects.get_or_create_lookup(sf, name, code, description, make_text_slug, self.logger)

    @transaction.commit_on_success
    def create_newsitem(self, attributes, **kwargs):
        """
        Creates and saves a NewsItem with the given kwargs. Returns the new
        NewsItem.

        kwargs MUST have the following keys:
            title
            item_date
            location_name
        For any other kwargs whose values aren't provided, this will use
        sensible defaults.

        kwargs may optionally contain a 'convert_to_block' boolean. If True,
        this will convert the given kwargs['location_name'] to a block level
        but will use the real (non-block-level) address for geocoding and Block
        association.

        attributes is a dictionary to use to populate this NewsItem's Attribute
        object.
        """
        block = location = None
        if 'location' not in kwargs:
            location = self.geocode(kwargs['location_name'])
            if location:
                block = location['block']
                location = location['point']
        if kwargs.pop('convert_to_block', False):
            kwargs['location_name'] = address_to_block(kwargs['location_name'])
            # If the exact address couldn't be geocoded, try using the
            # normalized location name.
            if location is None:
                location = self.geocode(kwargs['location_name'])
                if location:
                    block = location['block']
                    location = location['point']

        # Normally we'd just use "schema = kwargs.get('schema', self.schema)",
        # but self.schema will be evaluated even if the key is found in
        # kwargs, which raises an error when using multiple schemas.
        schema = kwargs.get('schema', None)
        schema = schema or self.schema

        ni = NewsItem.objects.create(
            schema=schema,
            title=kwargs['title'],
            description=kwargs.get('description', ''),
            url=kwargs.get('url', ''),
            pub_date=kwargs.get('pub_date', self.start_time),
            item_date=kwargs['item_date'],
            location=kwargs.get('location', location),
            location_name=kwargs['location_name'],
            location_object=kwargs.get('location_object', None),
            block=kwargs.get('block', block),
        )
        ni.attributes = attributes
        self.num_added += 1
        self.logger.info(u'Created NewsItem %s (total created in this scrape: %s)', ni.id, self.num_added)
        return ni

    @transaction.commit_on_success
    def update_existing(self, newsitem, new_values, new_attributes):
        """
        Given an existing NewsItem and dictionaries new_values and
        new_attributes, determines which values and attributes have changed
        and saves the object and/or its attributes if necessary.
        """
        newsitem_updated = False
        # First, check the NewsItem's values.
        for k, v in new_values.items():
            if getattr(newsitem, k) != v:
                self.logger.info('ID %s %s changed from %r to %r' % (newsitem.id, k, getattr(newsitem, k), v))
                setattr(newsitem, k, v)
                newsitem_updated = True
        if newsitem_updated:
            newsitem.save()
        # Next, check the NewsItem's attributes.
        for k, v in new_attributes.items():
            if newsitem.attributes[k] != v:
                self.logger.info('ID %s %s changed from %r to %r' % (newsitem.id, k, newsitem.attributes[k], v))
                newsitem.attributes[k] = v
                newsitem_updated = True
        if newsitem_updated:
            self.num_changed += 1
            self.logger.debug('Total changed in this scrape: %s', self.num_changed)
        else:
            self.logger.debug('No changes to NewsItem %s detected', newsitem.id)

    def update(self):
        """
        Updates the Schema.last_updated fields after scraping is done.
        """
        self.num_added = 0
        self.num_changed = 0
        update_start = datetime.datetime.now()

        # We use a try/finally here so that the DataUpdate object is created
        # regardless of whether the scraper raised an exception.
        try:
            got_error = True
            super(NewsItemListDetailScraper, self).update()
            got_error = False
        finally:
            # Rollback, in case the database is in an aborted transaction. his
            # avoids the "psycopg2.ProgrammingError: current transaction is aborted,
            # commands ignored until end of transaction block" error.
            from django.db import connection
            connection._rollback()

            update_finish = datetime.datetime.now()

            # Clear the Schema cache, in case the schemas have been updated in the
            # database since we started the scrape.
            self._schemas_cache = self._schema_cache = None

            for s in self.schemas.values():
                s.last_updated = datetime.date.today()
                s.save()
                DataUpdate.objects.create(
                    schema=s,
                    update_start=update_start,
                    update_finish=update_finish,
                    num_added=self.num_added,
                    num_changed=self.num_changed,
                    # None of our scrapers delete records yet, but we have the
                    # plumbing in place here in case future scrapers need to do
                    # that.
                    num_deleted=0,
                    num_skipped=self.num_skipped,
                    got_error=got_error,
                )

    def geocode(self, location_name):
        """
        Tries to geocode the given location string, returning a Point object
        or None.
        """
        try:
            return self._geocoder.geocode(location_name)
        except (GeocodingException, ParsingError):
            return None

    def safe_location(self, location_name, geom, max_distance=200):
        """
        Returns a location (geometry) to use, given a location_name and
        geometry. This is used for data sources that publish both a geometry
        and a location_name -- we double-check that the geometry is within
        a certain `max_distance` from the geocoded location_name.

        If there's a discrepancy or if the location_name can't be geocoded,
        this returns None.
        """
        location = self.geocode(location_name)
        if location is None:
            return None
        location_point = location['point']
        if not location_point:
            return None
        location_point.srid = 4326
        is_close, distance = locations_are_close(location_point, geom, max_distance)
        if not is_close:
            return None
        return geom

########NEW FILE########
__FILENAME__ = new_newsitem_list_detail
from ebdata.retrieval.models import ScrapedPage, NewsItemHistory, LIST_PAGE, DETAIL_PAGE
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper as BaseScraper
from ebdata.retrieval.scrapers.list_detail import SkipRecord
from ebdata.retrieval.scrapers.base import ScraperBroken
from ebpub.db.models import NewsItem
import datetime

class NewsItemListDetailScraper(BaseScraper):
    def get_page(self, *args, **kwargs):
        """
        Calls NewsItemScraper's get_html method and returns an unsaved ``Page``
        object wrapping the html.
        """
        schema = kwargs.get('schema', None)
        schema = schema or self.schema
        html = super(NewsItemListDetailScraper, self).get_html(*args, **kwargs)
        return ScrapedPage(url=args[0], when_crawled=datetime.datetime.now(), html=html, schema=schema)

    def update_from_string(self, list_page):
        """
        For scrapers with has_detail=False, runs the equivalent of update() on
        the given string.

        This is useful if you've got cached versions of HTML that you want to
        parse.

        Subclasses should not have to override this method.
        """
        # TODO: Setting the page type should probably happen somewhere else.
        list_page.page_type = LIST_PAGE
        self.num_skipped = 0
        for list_record in self.parse_list(list_page.html):
            try:
                list_record = self.clean_list_record(list_record)
            except SkipRecord, e:
                self.num_skipped += 1
                self.logger.debug("Skipping list record for %r: %s " % (list_record, e))
                continue
            except ScraperBroken, e:
                # Re-raise the ScraperBroken with some addtional helpful information.
                raise ScraperBroken('%r -- %s' % (list_record, e))
            self.logger.debug("Clean list record: %r" % list_record)

            old_record = self.existing_record(list_record)
            self.logger.debug("Existing record: %r" % old_record)

            if self.has_detail and self.detail_required(list_record, old_record):
                self.logger.debug("Detail page is required")
                try:
                    detail_page = self.get_detail(list_record)
                    # TODO: Setting the page type should probably happen somewhere else.
                    detail_page.page_type = DETAIL_PAGE
                    detail_record = self.parse_detail(detail_page.html, list_record)
                    detail_record = self.clean_detail_record(detail_record)
                except SkipRecord, e:
                    self.num_skipped += 1
                    self.logger.debug("Skipping detail record for list %r: %s" % (list_record, e))
                    continue
                except ScraperBroken, e:
                    # Re-raise the ScraperBroken with some addtional helpful information.
                    raise ScraperBroken('%r -- %s' % (list_record, e))
                self.logger.debug("Clean detail record: %r" % detail_record)
            else:
                self.logger.debug("Detail page is not required")
                detail_page = None
                detail_record = None

            self.save(old_record, list_record, detail_record, list_page, detail_page)

    def create_newsitem(self, attributes, list_page=None, detail_page=None, **kwargs):
        """
        Creates and saves a NewsItem with the given kwargs. Returns the new
        NewsItem.

        kwargs MUST have the following keys:
            title
            item_date
            location_name
        For any other kwargs whose values aren't provided, this will use
        sensible defaults.

        attributes is a dictionary to use to populate this NewsItem's Attribute
        object.
        """
        block = location = None
        if 'location' not in kwargs:
            location = self.geocode(kwargs['location_name'])
            if location:
                block = location['block']
                location = location['point']

        # Normally we'd just use "schema = kwargs.get('schema', self.schema)",
        # but self.schema will be evaluated even if the key is found in
        # kwargs, which raises an error when using multiple schemas.
        schema = kwargs.get('schema', None)
        schema = schema or self.schema

        ni = NewsItem.objects.create(
            schema=schema,
            title=kwargs['title'],
            description=kwargs.get('description', ''),
            url=kwargs.get('url', ''),
            pub_date=kwargs.get('pub_date', self.start_time),
            item_date=kwargs['item_date'],
            location=kwargs.get('location', location),
            location_name=kwargs['location_name'],
            location_object=kwargs.get('location_object', None),
            block=kwargs.get('block', block)
        )
        ni.attributes = attributes
        if list_page is not None:
            NewsItemHistory.objects.record_history(ni, list_page)
        if detail_page is not None:
            NewsItemHistory.objects.record_history(ni, detail_page)
        self.logger.info(u'Created NewsItem ID %s' % ni.id)
        self.num_added += 1
        return ni

    def update_existing(self, newsitem, new_values, new_attributes, list_page=None, detail_page=None):
        """
        Given an existing NewsItem and dictionaries new_values and
        new_attributes, determines which values and attributes have changed
        and saves the object and/or its attributes if necessary.
        """
        # First, check the NewsItem's values.
        newsitem_updated = False
        for k, v in new_values.items():
            if getattr(newsitem, k) != v:
                self.logger.info('ID %s %s changed from %r to %r' % (newsitem.id, k, getattr(newsitem, k), v))
                setattr(newsitem, k, v)
                newsitem_updated = True
        if newsitem_updated:
            if list_page is not None:
                NewsItemHistory.objects.record_history(newsitem, list_page)
            if detail_page is not None:
                NewsItemHistory.objects.record_history(newsitem, detail_page)
            newsitem.save()
            self.num_changed += 1
        # Next, check the NewsItem's attributes.
        for k, v in new_attributes.items():
            if newsitem.attributes[k] != v:
                self.logger.info('ID %s %s changed from %r to %r' % (newsitem.id, k, newsitem.attributes[k], v))
                newsitem.attributes[k] = v

########NEW FILE########
__FILENAME__ = config
"""
Sample config file for the updaterdaemon
"""

def hourly(*minutes):
    def handle(dt):
        return dt.minute in minutes
    return handle

def multiple_hourly(*hour_minutes):
    # hour_minutes is a list of tuples in the format (hour, minute)
    hour_minutes = set(hour_minutes)
    def handle(dt):
        return (dt.hour, dt.minute) in hour_minutes
    return handle

def daily(hour, minute):
    def handle(dt):
        return dt.hour == hour and dt.minute == minute
    return handle

def weekly(weekday, hour, minute):
    # weekday -- 0=Monday, 6=Sunday
    def handle(dt):
        return dt.weekday() == weekday and dt.hour == hour and dt.minute == minute
    return handle

TASKS = (
    # time_callback, function_to_run, params_for_function, settings_file_name
    #
    # Example:
    # (daily(12, 0), run_some_function, {'kwargs': 'foo'}, {'DJANGO_SETTINGS_MODULE': 'foo.settings'})
)

########NEW FILE########
__FILENAME__ = runner
from ebdata.utils.daemon import Daemon
import datetime
import os
import sys
import time

class EveryMinuteDaemon(Daemon):
    """
    A daemon that calls handle_time() every minute.
    """
    def run(self):
        while 1:
            # Calculate the next minute. We don't care about handling the
            # current minute, because if we did that, it would be handled
            # twice if this program were stopped and restarted during that
            # minute.
            next_minute = datetime.datetime.now() + datetime.timedelta(minutes=1)
            next_minute = next_minute.replace(second=0, microsecond=0)

            # Sleep until the next minute. Add 5 seconds to the sleep time to
            # avoid edge cases and off-by-one errors. Messy but effective.
            sleep_delta = next_minute - datetime.datetime.now()
            time.sleep(sleep_delta.seconds + 5)

            # Call the hook.
            self.handle_time(next_minute)

    def handle_time(self, timestamp):
        pass

class EveryTwoSecondsDaemon(Daemon):
    """
    A daemon that calls handle_time() every two seconds.

    This is useful for debugging -- just replace EveryMinuteDaemon with
    EveryTwoSecondsDaemon in your subclass.
    """
    def run(self):
        while 1:
            self.handle_time(datetime.datetime.now())
            time.sleep(2)

    def handle_time(self, timestamp):
        pass

class UpdaterDaemon(EveryMinuteDaemon):
    def __init__(self, config, *args, **kwargs):
        super(UpdaterDaemon, self).__init__(*args, **kwargs)
        self.config = config

    def handle_time(self, timestamp):
        # Get the tasks for the given timestamp, and run any that need to be
        # run. Reload the config to take into account any changes that might
        # have been made.
        reload(self.config)
        for check, func, kwargs, env in self.config.TASKS:
            if check(timestamp):
                # Fork a child process and grandchild process, and kill the
                # child process immediately so that it doesn't block.
                # For more on this technique, see the final paragraph at
                # http://www.faqs.org/faqs/unix-faq/faq/part3/section-13.html
                try:
                    pid = os.fork()
                except OSError, e:
                    sys.stderr.write("fork failed: %d (%s)\n" % (e.errno, e.strerror))
                    os._exit(1)
                if pid == 0: # child
                    try:
                        pid2 = os.fork()
                    except OSError, e:
                        sys.stderr.write("inner fork failed: %d (%s)\n" % (e.errno, e.strerror))
                        os._exit(1)
                    if pid2 == 0: # child
                        os.environ.update(env)
                        from django.conf import settings

                        # Log the function call and PID.
                        sys.stdout.write('%s\t%s\t%r\t%s\n' % (datetime.datetime.now(), func.func_name, kwargs, os.getpid()))
                        sys.stdout.flush()

                        try:
                            func(**kwargs)
                        except Exception, e:
                            from django.core.mail import mail_admins
                            import traceback
                            traceback_string = '\n'.join(traceback.format_exception(*sys.exc_info()))
                            sys.stderr.write("ERROR AT %s\n" % datetime.datetime.now())
                            sys.stderr.write(traceback_string)
                            sys.stderr.write("\n========================================\n")
                            subject = '%s %s' % (func.func_name, str(kwargs).replace('\n', ' '))
                            try:
                                mail_admins(subject, traceback_string)
                            except Exception, e:
                                sys.stderr.write("Got error mailing admins: %s\n" % e)
                            # Don't call sys.exit() for this,
                            # because we're in a child process.
                            os._exit(1)
                        sys.stdout.flush()
                    os._exit(0)
                else: # parent
                    os.waitpid(pid, 0)

if __name__ == "__main__":
    from ebdata.retrieval.updaterdaemon import config
    daemon = UpdaterDaemon(config, '/tmp/updaterdaemon.pid',
        stdout='/tmp/updaterdaemon.log',
        stderr='/tmp/updaterdaemon.err')
    daemon.run_from_command_line(sys.argv[1:])

########NEW FILE########
__FILENAME__ = utils
"""
Utilities useful for scraping.
"""

import re
import htmlentitydefs
from ebgeo.utils.geodjango import smart_transform
from django.contrib.gis.geos import GEOSGeometry

multispace_re = re.compile(r'\s\s+')

def norm_dict_space(d, *keys):
    """
    >>> d = {'name': '  john  smith ',
    ...      ' address': ' 123  main st'}
    >>> norm_dict_space(d, 'name', 'address')
    >>> d
    {'name': 'john smith', 'address': '123 main st'}
    """
    for key in keys:
        d[key] = multispace_re.sub(' ', d[key]).strip()

def obj_dict_merge(obj, update_dict, ignore_attrs=None):
    """Updates the attributes of obj with the values in update_dict.

    Takes a list of attributes to ignore.

    Returns True if any of obj's attributes were updated, False otherwise.
    """
    if not ignore_attrs:
        ignore_attrs = []
    changed = False
    for attr in obj.__dict__.keys():
        if attr not in ignore_attrs and update_dict.has_key(attr):
            update_val = update_dict[attr]
            if getattr(obj, attr) != update_val:
                setattr(obj, attr, update_val)
                changed = True
    return changed, obj

# From http://effbot.org/zone/re-sub.htm#unescape-html
def convert_entities(text):
    """
    Converts HTML entities in the given string (e.g., '&#28;' or '&nbsp;') to
    their corresponding characters.
    """
    NAMED_ENTITY_SPECIAL_CASES = {
        'apos': u"'",
    }
    def fixup(m):
        text = m.group(0)
        if text[:2] == "&#":
            # character reference
            try:
                if text[:3] == "&#x":
                    return unichr(int(text[3:-1], 16))
                else:
                    return unichr(int(text[2:-1]))
            except ValueError:
                pass
        else:
            # named entity
            entity = text[1:-1]
            try:
                text = unichr(htmlentitydefs.name2codepoint[entity])
            except KeyError:
                try:
                    return NAMED_ENTITY_SPECIAL_CASES[entity]
                except KeyError:
                    pass
        return text # leave as is
    return re.sub("&#?\w+;", fixup, text)

def locations_are_close(geom_a, geom_b, max_distance=200):
    """
    Verifies that two locations are within a certain distance from
    each other. Returns a tuple of (is_close, distance), where
    is_close is True only if the locations are within max_distance.

    Assumes max_distance is in meters.
    """
    # Both geometries must be GEOSGeometry for the distance method.
    if not (isinstance(geom_a, GEOSGeometry) and isinstance(geom_b, GEOSGeometry)):
        raise ValueError, 'both geometries must be GEOSGeometry instances'
    carto_srid = 3395 # SRS in meters
    geom_a = smart_transform(geom_a, carto_srid)
    geom_b = smart_transform(geom_b, carto_srid)
    distance = geom_a.distance(geom_b)
    return (distance < max_distance), distance

########NEW FILE########
__FILENAME__ = articletext
from ebdata.retrieval.utils import convert_entities
from lxml import etree
import decimal
import re

is_punctuated = re.compile(ur"""
    [\.\!\?]
    (?:\s+|"|'|\xe2\x80\x9d|\u201d|\u2019|\)){0,3}
    \s*
    $
""", re.VERBOSE).search

def article_text_sections(tree):
    """
    Given an HTML tree of a news article (or blog entry permalink), deduces
    which part of it is text and returns a list of lists of strings, with each
    string representing a paragraph and each list of strings representing a
    "section" of the page.
    """

    # The basic algorithm here is to combine all text within the same block
    # (e.g., a <div>).

    MIN_NUM_PARAGRAPHS = 3
    MIN_NUM_PUNCTUATED = 3

    # In order for a paragraph to be counted toward MIN_NUM_PUNCTUATED, it must
    # have this number of characters.
    MIN_CHARS_IN_PARAGRAPH = 30

    # If this many paragraphs with MIN_CHARS_IN_PARAGRAPH are included in the
    # section, then the section will be included, regardless of failing
    # MIN_PERCENTAGE_PUNCTUATED.
    NUM_PARAGRAPHS_SAFE_GUESS = 6

    # In order for a section to be included in the result, at least this
    # percentage of paragraphs in the section must be punctuated.
    MIN_PERCENTAGE_PUNCTUATED = decimal.Decimal('.5')

    block_tags = set(['blockquote', 'dd', 'div', 'dt', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'h7', 'h8', 'li', 'p', 'td', 'th', 'tr'])
    drop_tags_only = set(['a', 'abbr', 'acronym', 'b', 'center', 'dir', 'dl', 'em', 'font', 'form', 'hr', 'i', 'label', 'menu', 'ol', 'pre', 'small', 'span', 'strong', 'sub', 'sup', 'table', 'tbody', 'tfoot', 'thead', 'topic', 'u', 'ul', 'wbr'])
    drop_tags_and_contents = set(['applet', 'area', 'button', 'embed', 'img', 'iframe', 'head', 'input', 'link', 'map', 'meta', 'noscript', 'object', 'option', 'script', 'select', 'spacer', 'style', 'textarea', 'title'])
    layout_tags = set(['div', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'h7', 'h8', 'td', 'th', 'tr'])
    is_open_tag = re.compile('^<[^/][^>]+>$').search
    is_close_tag = re.compile('^</[^>]+>$').search
    ignored_paragraphs = set(['del.icio.us', 'digg', 'email', 'e-mail editor', 'e-mail story', 'no comments', 'print', 'print article', 'printer-friendly', 'printer version', 'reprints'])

    elements_to_drop = []
    for element in tree.getiterator():
        if not isinstance(element.tag, basestring): # If it's a comment...
            element.drop_tag()
            continue
        if element.text and '\n' in element.text:
            element.text = element.text.replace('\n', ' ')
        if element.tail and '\n' in element.tail:
            element.tail = element.tail.replace('\n', ' ')
        if element.tag in block_tags:
            element.text = '\n' + (element.text or '')
            element.tail = '\n' + (element.tail or '')
        elif element.tag == 'br':
            element.tail = '\n' + (element.tail or '')
            element.drop_tag()
        elif element.tag in drop_tags_only:
            element.drop_tag()
        elif element.tag in drop_tags_and_contents:
            elements_to_drop.append(element)
        elif element.tag not in ('html', 'body'): # Unknown tag!
            element.drop_tag()
    for e in elements_to_drop:
        e.drop_tree()

    for element in tree.getiterator():
        if element.tag in block_tags:
            if element.tag in layout_tags:
                element.text = '\n<%s>\n%s\n' % (element.tag, (element.text or ''))
                element.tail = '\n</%s>\n%s\n' % (element.tag, (element.tail or ''))
            element.drop_tag()

    try:
        tree.body
    except IndexError:
        # In some cases, the article is missing a <body> tag, and tree.body
        # will result in an IndexError. Just skip these.
        return []

    new_html = etree.tostring(tree.body, method='html')
    new_html = convert_entities(new_html)
    lines = re.split(r'\s*\n+\s*', new_html.strip())
    result = []
    sections = []
    for line in lines:
        if is_open_tag(line):
            result.append([])
        elif is_close_tag(line):
            last_bit = result.pop()
            if len(last_bit) >= MIN_NUM_PARAGRAPHS:
                sections.append(last_bit)
        else: # It's text, not a tag.
            try:
                result[-1].append(line)
            except IndexError: # No tags seen yet.
                result.append([line])

    # Cut out the sections that don't contain enough punctuated sentences.
    final_sections = []
    for section in sections:
        count = 0
        to_delete = []
        for i, paragraph in enumerate(section):
            if paragraph.lower() in ignored_paragraphs:
                to_delete.append(i)
            elif is_punctuated(paragraph) and len(paragraph) >= MIN_CHARS_IN_PARAGRAPH:
                count += 1
        percent_punctuated = decimal.Decimal(count) / decimal.Decimal(len(section))
        if count >= NUM_PARAGRAPHS_SAFE_GUESS or (count >= MIN_NUM_PUNCTUATED and percent_punctuated >= MIN_PERCENTAGE_PUNCTUATED):
            for i in reversed(to_delete): # Delete in reverse so that index order is preserved.
                del section[i]
            final_sections.append(section)
    return final_sections

def article_text(tree):
    """
    Simple wrapper around article_text_sections() that "flattens" sections into
    a single section.
    """
    result = []
    for section in article_text_sections(tree):
        result.extend(section)
    return result

if __name__ == "__main__":
    from ebdata.retrieval import UnicodeRetriever
    from ebdata.textmining.treeutils import make_tree
    import sys
    html = UnicodeRetriever().get_html(sys.argv[1])
    lines = article_text(make_tree(html))
    print lines

########NEW FILE########
__FILENAME__ = brain
from listdiff import Hole # relative import
import re

class Brain(list):
    def _each_member(self):
        for s in self:
            yield isinstance(s, Hole), s

    def as_text(self, custom_marker='{{ HOLE }}'):
        """
        Returns a display-friendly version of the Brain, using the
        given custom_marker to mark template holes.
        """
        output = []
        for is_hole, member in self._each_member():
            if is_hole:
                output.append(custom_marker)
            else:
                output.append(member)
        return ''.join(output)

    def concise(self):
        """
        Returns the brain as a list with all consecutive strings combined into
        a single string.
        """
        last_one_was_string = False
        output = []
        for is_hole, member in self._each_member():
            if is_hole:
                output.append(member)
            else:
                if last_one_was_string:
                    output[-1] += member
                else:
                    output.append(member)
            last_one_was_string = not is_hole
        return output

    def num_holes(self):
        """
        Returns the number of holes in this Brain.
        """
        return len([member for is_hole, member in self._each_member() if is_hole])

    def match_regex(self):
        """
        Returns a regular expression (as a string) that matches strings
        formatted with this Brain.
        """
        regex = ['^(?s)']
        for is_hole, member in self._each_member():
            if is_hole:
                regex.append(member.regex())
            else:
                regex.append(re.escape(member))
        regex.append('$')
        return ''.join(regex)

    def serialize(self):
        """
        Returns a serialized string representing this Brain.
        """
        import cPickle as pickle
        import base64
        return base64.encodestring(pickle.dumps(self, protocol=2))

    def from_serialized(cls, serialized_string):
        """
        Class method that returns a Brain instance for the given serialized
        string (as returned by Brain.serialize()).
        """
        import cPickle as pickle
        import base64
        return pickle.loads(base64.decodestring(serialized_string))
    from_serialized = classmethod(from_serialized)

########NEW FILE########
__FILENAME__ = clean
from ebdata.textmining.treeutils import make_tree_and_preprocess
from listdiff import longest_common_substring # relative import
from lxml import etree

elements_with_ids = etree.XPath('//*[normalize-space(@id)!=""]')

def identical_elements(list1, list2, debug):
    """
    Returns a list of (elements_to_delete, elements_whose_tail_should_be_removed)
    tuples.

    list1 and list2 should both be lists of etree Elements.
    """
    # list1 and list2 are lists of etree Elements.
    if list1 == list2 == []:
        if debug:
            print "identical_elements() got empty lists"
        return []
    hash_list1 = [(el.tag, el.attrib.get('id'), el.attrib.get('class')) for el in list1]
    hash_list2 = [(el.tag, el.attrib.get('id'), el.attrib.get('class')) for el in list2]
    best_size, offset1, offset2 = longest_common_substring(hash_list1, hash_list2)
    if debug:
        print "Got these two lists:\n  %r\n  %r\nMatch:\n  %r" % (hash_list1, hash_list2, hash_list1[offset1:offset1+best_size])
    if best_size == 0:
        return []
    result = []
    if offset1 > 0 and offset2 > 0:
        # There's leftover stuff on the left side of BOTH lists.
        if debug:
            print "Leftovers on left of BOTH"
        result.extend(identical_elements(list1[:offset1], list2[:offset2], debug))
    for i in range(best_size):
        child1, child2 = list1[offset1+i], list2[offset2+i]
        if debug:
            print "Children:\n  %r\n  %r" % (child1, child2)
            print '%r    %r' % (etree.tostring(child1, method='html'), etree.tostring(child2, method='html'))
        if child1.tag == child2.tag and dict(child1.attrib) == dict(child2.attrib) and child1.text == child2.text and list(child1) == list(child2):
            if debug:
                print "Identical!"
            tail_removals = []
            if child1.tail == child2.tail:
                tail_removals.append(child1)
            # If the previous sibling's tails are equal, remove those.
            if i > 0 and list1[offset1+i-1].tail == list2[offset2+i-1].tail:
                tail_removals.append(list1[offset1+i-1])
            result.append(([child1, child2], tail_removals))
        else:
            if debug:
                print "No matches; descending into children"
            result.extend(identical_elements(list(child1), list(child2), debug))
    if (offset1 + best_size < len(list1)) and (offset2 + best_size < len(list2)):
        # There's leftover stuff on the right side of BOTH lists.
        if debug:
            print "Leftovers on right of BOTH"
        result.extend(identical_elements(list1[offset1+best_size:], list2[offset2+best_size:], debug))
    return result

def strip_template(tree1, tree2, check_ids=True, debug=False):
    """
    Given two etree trees, determines the duplicate/redundant elements in
    both and strips those redundancies from both trees (in place).

    If check_ids is True, then this will also check for duplicate elements
    by ID. This helps to find duplicates at different levels of the tree --
    by default (without check_ids), this function only finds duplicates if
    they're at the same position in the HTML tree.

    Returns the number of redundant elements that have been removed.
    """
    # TODO:
    #    Solve the sidebar problem -- delete them

    # Assemble a list of trees to compare. Obviously, first we just compare the
    # given trees -- but if check_ids is True, then we also compare the
    # subtrees containing "id" attributes.
    tree_pairs = [(tree1, tree2)]
    if check_ids:
        ids2 = dict([(el.get('id'), el) for el in elements_with_ids(tree2)])
        other_pairs = [(el.getparent(), ids2[el.get('id')].getparent()) for el in elements_with_ids(tree1) if el.get('id') in ids2]
        tree_pairs.extend(other_pairs)

    # Run the algorithm multiple times until no similarities remain. This is
    # sort of inelegant, but it works.
    num_removed = 0
    for tree1, tree2 in tree_pairs:
        if debug:
            print 'NEW TREE PAIR:\n  %r\n  %r' % (tree1, tree2)
        while 1:
            if debug:
                print 'New round'
            if tree1 is None and tree2 is None:
                break
            result = identical_elements(list(tree1), list(tree2), debug)
            if debug:
                print "strip_template() result:\n%r" % result
            if not result:
                break
            for drops, tail_removals in result:
                for removal in tail_removals:
                    removal.tail = ''
                for drop in drops:
                    drop.drop_tree()
            num_removed += len(result)
    return num_removed

def clean_page(html, other_page):
    """
    Wrapper around the various cleaning functions. This accepts and returns
    strings instead of trees.
    """
    tree1 = make_tree_and_preprocess(html)
    tree2 = make_tree_and_preprocess(other_page)
    strip_template(tree1, tree2)
    # drop_useless_tags(tree1)
    # remove_empty_tags(tree1, ('div', 'span', 'td', 'tr', 'table'))
    return etree.tostring(tree1, method='html'), etree.tostring(tree2, method='html')

########NEW FILE########
__FILENAME__ = hole
import re

class Hole(object):
    capture = True # Designates whether the Hole captures something in regex().
    def __eq__(self, other):
        "A Hole is equal to any other Hole (but not subclasses)."
        return type(other) is self.__class__

    def __repr__(self):
        return '<%s>' % self.__class__.__name__

    def regex(self):
        return '(.*?)'

class OrHole(Hole):
    "A Hole that can contain one of a set of values."
    capture = True
    def __init__(self, *choices):
        self.choices = choices

    def __eq__(self, other):
        "An OrHole is equal to another one if its choices are the same."
        return type(other) is self.__class__ and self.choices == other.choices

    def __repr__(self):
        return '<%s: %r>' % (self.__class__.__name__, self.choices)

    def regex(self):
        return '(%s)' % '|'.join(re.escape(choice) for choice in self.choices)

class RegexHole(Hole):
    """
    A Hole that contains data that matches the given regex. It's up to the
    caller to determine whether the data should be grouped.
    """
    def __init__(self, regex_string, capture):
        self.regex_string = regex_string
        self.capture = capture

    def __eq__(self, other):
        "A RegexHole is equal to another one if its regex_string is the same."
        return type(other) is self.__class__ and self.regex_string == other.regex_string and self.capture == other.capture

    def __repr__(self):
        return '<%s: %s>' % (self.__class__.__name__, self.regex_string)

    def regex(self):
        return self.regex_string

class IgnoreHole(Hole):
    """
    A Hole that contains an arbitrary amount of data but should be ignored.
    I.e., its contents are *not* included in extract().
    """
    capture = False
    def regex(self):
        return '.*?' # No parenthesis!

########NEW FILE########
__FILENAME__ = htmlutils
"""
Utilities for manipulating lxml.html trees.
"""

import re
from lxml import etree
from lxml.html import Element, builder as E

# All text that isn't within an <a> tag.
non_linked_text = etree.XPath("descendant-or-self::*[name()!='a']/text()")

# All text that's within an <a> tag.
linked_text = etree.XPath("descendant-or-self::*[name()='a']/text()")

# All <a> elements whose href and text contain the string 'print'.
# The "translate(x, 'PRINT', 'print')
printer_links = etree.XPath("//a[contains(translate(@href, 'PRINT', 'print'), 'print') or contains(@href, 'pf')][contains(translate(text(), 'PRINT', 'print'), 'print')]")

def percent_linked_text(tree):
    """
    Returns a float representing the percentage of all text within this tree
    that is linked (e.g., is within an <a> tag).
    """
    links_yes = len(''.join([bit.strip() for bit in linked_text(tree) if bit.strip()]))
    links_no = len(''.join([bit.strip() for bit in non_linked_text(tree) if bit.strip()]))
    try:
        return 1.0 * links_yes / (links_yes + links_no)
    except ZeroDivisionError:
        return 0.0

def is_printer_link(href, link_text):
    """
    Helper function that picks up some of the logic that the `printer_links`
    XPath expression can't handle. Returns True if the given URL and link text
    probably are a print link.
    """
    if not re.search(r'(?i)\b(?:print|printer)\b', link_text):
        return False
    if re.search(r'(?i)print[\s-]*(?:edition|advertising|ads)\b', link_text):
        return False
    if re.search(r'(?i)\s*javascript:', href):
        return False
    return True

def printer_friendly_link(tree):
    """
    Returns the 'printer-friendly' URL for the given HTML tree.

    This works by looking for any link that has 'print' in both the link text
    and the URL.

    Returns None if it can't find such a link.
    """
    a_tags = [a for a in printer_links(tree) if is_printer_link(a.attrib['href'], a.text_content())]
    if a_tags:
        # GOTCHA: If there are multiple links, we use the first one.
        link = a_tags[0].attrib['href'].strip()
        if not link.startswith('javascript:'):
            return link
    return None

def remove_empty_tags(tree, ignore_tags):
    """
    Removes all empty tags in the given etree, editing it in place. A tag is
    considered empty if it has no contents (text or tags).

    This works in a reductive manner. If the removal of an empty tag causes its
    parent to become empty, then the parent will be removed, too, recursively.

    ignore_tags should be a tuple of tag names to ignore (i.e., any empty
    tag with a tag name in ignore_tags will not be removed). Each tag name in
    this list should be lowercase.

    The <body> and <html> tags are never removed.
    """
    ignore_tags += ('body', 'html')
    child_removed = False
    for element in tree:
        # The "element.getparent() is not None" check ensures that we don't
        # cause the AssertionError in drop_tree().
        if element.tag not in ignore_tags and (element.text is None or not element.text.strip()) \
                and not list(element) and element.getparent() is not None:
            element.drop_tree()
            child_removed = True
        else:
            remove_empty_tags(element, ignore_tags)
    if child_removed:
        parent = tree.getparent()
        if parent is not None:
            remove_empty_tags(parent, ignore_tags)

def brs_to_paragraphs(tree, inline_tags=None):
    """
    Return an lxml tree with all <br> elements stripped and paragraphs put in
    place where necessary.
    """
    # add these tags to p's that we're currently building, any other tags will
    # close the current p
    inline_tags = inline_tags or ['a']

    # if this tree doesn't have any child elements, just return it as is
    if len(tree) == 0:
        return tree

    # if this tree doesn't contain any <br> tags, we don't need to touch it
    if tree.find('.//br') is None:
        return tree

    # XXX: We're building a whole new tree here and leaving out any attributes.
    # A) That might be a little slower and more memory intensive than modifying
    # the tree in place, and B) we're dropping any attributes on block elements.
    # The latter is probably fine for current use, but certainly not ideal.
    new_tree = Element(tree.tag)

    # if this tree starts out with text, create a new paragraph for it, and
    # add it to the tree
    if tree.text:
        p = E.P()
        p.text = tree.text
        new_tree.append(p)

    for e in tree:
        if e.tag == 'br':
            # avoid adding empty p elements
            if e.tail is None:
                continue
            # start a new p
            p = E.P()
            p.text = e.tail
            new_tree.append(p)
        # if this is a block tag, and it has trailing text, that text needs to
        # go into a new paragraph... only if the tail has actual content and
        # not just whitespace though.
        elif e.tail and re.match('[^\s]', e.tail) and e.tag not in inline_tags:
            p = E.P()
            p.text = e.tail
            e.tail = ''
            new_tree.append(e)
            new_tree.append(p)
        # keep inline tags inside the current paragraph
        elif e.tag in inline_tags:
            p.append(e)
        else:
            new_tree.append(brs_to_paragraphs(e))

    return new_tree

########NEW FILE########
__FILENAME__ = listdiff
from hole import Hole

def listdiff(list1, list2):
    """
    Given two lists, returns a "diff" list, with Hole instances inserted
    as necessary.
    """
    hole = Hole()

    # Special case.
    if list1 == list2 == []:
        return []

    best_size, offset1, offset2 = longest_common_substring(list1, list2)

    result = []

    if best_size == 0:
        result.append(hole)
    if offset1 > 0 and offset2 > 0:
        # There's leftover stuff on the left side of BOTH lists.
        result.extend(listdiff(list1[:offset1], list2[:offset2]))
    elif offset1 > 0 or offset2 > 0:
        # There's leftover stuff on the left side of ONLY ONE of the lists.
        result.append(hole)
    if best_size > 0:
        result.extend(list1[offset1:offset1+best_size])
        if (offset1 + best_size < len(list1)) and (offset2 + best_size < len(list2)):
            # There's leftover stuff on the right side of BOTH lists.
            result.extend(listdiff(list1[offset1+best_size:], list2[offset2+best_size:]))
        elif (offset1 + best_size < len(list1)) or (offset2 + best_size < len(list2)):
            # There's leftover stuff on the right side of ONLY ONE of the lists.
            result.append(hole)
    return result

# NOTE: This is a "longest common substring" algorithm, not a
# "longest common subsequence" algorithm. The difference is that longest common
# subsequence does not require the bits to be contiguous.
#
# The longest common subsequence of "foolish" and "fools" is "fools".
# The longest common substring of "foolish" and "fools" is "fool".
try:
    from listdiffc import longest_common_subsequence as longest_common_substring
except ImportError:
    def longest_common_substring(seq1, seq2):
        """
        Given two sequences, calculates the longest common substring and returns
        a tuple of:
            (LCS length, LCS offset in seq1, LCS offset in seq2)
        """
        best_size, offset1, offset2 = half_longest_match(seq1, seq2)
        best_size, offset2, offset1 = half_longest_match(seq2, seq1, best_size, offset2, offset1)
        return best_size, offset1, offset2

    def half_longest_match(seq1, seq2, best_size=0, offset1=-1, offset2=-1):
        """
        Implements "one half" of the longest common substring algorithm.
        """
        len1 = len(seq1)
        len2 = len(seq2)
        i = 0 # seq2 index
        current_size = 0
        while i < len2:
            if best_size >= len2 - i:
                break # Short circuit
            j = i
            k = 0
            while k < len1 and j < len2:
                if seq1[k] == seq2[j]:
                    current_size += 1
                    if current_size >= best_size:
                        new_offset1 = k - current_size + 1
                        new_offset2 = j - current_size + 1
                        if current_size > best_size or (new_offset1 <= offset1 and new_offset2 <= offset2):
                            offset1 = new_offset1
                            offset2 = new_offset2
                        best_size = current_size
                else:
                    current_size = 0
                j += 1
                k += 1
            i += 1
            current_size = 0
        return best_size, offset1, offset2

########NEW FILE########
__FILENAME__ = sst
"""
Template implementation that uses the Site Style Tree concept, as described
in this paper:

    L. Yi and B. Liu. Eliminating noisy information in web pages for data mining.
    In ACM Conf. on Knowledge Discovery and Data Mining (SIGKDD), 2003.
    http://citeseer.ist.psu.edu/yi03eliminating.html
"""

from ebdata.templatemaker.listdiff import longest_common_substring
from ebdata.textmining.treeutils import make_tree_and_preprocess
from lxml import etree
import time

class NoMatch(Exception):
    pass

def element_hash_strict(el):
    """
    Returns a hash of the given etree Element, such that it can be used
    in a longest_common_substring comparison against another tree.
    """
    # <br> tags should never be marked as the same as other <br> tags, so use
    # the current time to introduce enough entropy. Note that we use '%.10f'
    # instead of str(time.time()) because str() rounds the number to two
    # decimal places, resulting in identical results for subsequent tags.
    if el.tag == 'br':
        return '%.10f' % time.time()

    attrs = sorted(dict(el.attrib).items())
    return (el.tag, attrs, el.text, el.tail)

def element_hash_loose(el):
    if el.tag == 'br':
        return '%.10f' % time.time()
    if el.tag in ('h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'a', 'title'):
        return el.tag
    return (el.tag, el.text)

def tree_diff_children(list1, list2, hash_func, algorithm):
    # list1 and list2 are lists of etree Elements.
    if list1 == list2 == []:
        return []
    # Try to find the longest common substring, according to hash_func().
    # First we use element_hash_strict(), but then we use element_hash_loose()
    # as a fallback.
    best_size, offset1, offset2 = longest_common_substring([hash_func(el) for el in list1], [hash_func(el) for el in list2])
    result = []
    if best_size == 0:
        if hash_func == element_hash_strict:
            result.extend(tree_diff_children(list1, list2, element_hash_loose, algorithm))
        else:
            result.append(etree.Element('MULTITAG_HOLE'))
    if offset1 > 0 and offset2 > 0:
        # There's leftover stuff on the left side of BOTH lists.
        result.extend(tree_diff_children(list1[:offset1], list2[:offset2], element_hash_strict, algorithm))
    elif offset1 > 0 or offset2 > 0:
        # There's leftover stuff on the left side of ONLY ONE of the lists.
        result.append(etree.Element('MULTITAG_HOLE'))
    if best_size > 0:
        for i in range(best_size):
            child = tree_diff(list1[offset1+i], list2[offset2+i], algorithm)
            result.append(child)
        if (offset1 + best_size < len(list1)) and (offset2 + best_size < len(list2)):
            # There's leftover stuff on the right side of BOTH lists.
            result.extend(tree_diff_children(list1[offset1+best_size:], list2[offset2+best_size:], element_hash_strict, algorithm))
        elif (offset1 + best_size < len(list1)) or (offset2 + best_size < len(list2)):
            # There's leftover stuff on the right side of ONLY ONE of the lists.
            result.append(etree.Element('MULTITAG_HOLE'))
    return result

def tree_diff(tree1, tree2, algorithm=1):
    """
    Returns a "diff" of the two etree objects, using these placeholders in case
    of differences:
        TEXT_HOLE -- used when the 'text' differs
        TAIL_HOLE -- used when the 'tail' differs
        ATTRIB_HOLE -- used when an attribute value (or existence) differs
        MULTITAG_HOLE -- used when an element's children differ

    This assumes tree1 and tree2 share the same root tag, e.g. "<html>".
    """
    # Copy the element (but not its children).
    result = etree.Element(tree1.tag)
    result.text = (tree1.text != tree2.text) and 'TEXT_HOLE' or tree1.text
    result.tail = (tree1.tail != tree2.tail) and 'TAIL_HOLE' or tree1.tail
    attrs1, attrs2 = dict(tree1.attrib), dict(tree2.attrib)
    for k1, v1 in attrs1.items():
        if attrs2.pop(k1, None) == v1:
            result.attrib[k1] = v1
        else:
            result.attrib[k1] = 'ATTRIB_HOLE'
    for k2 in attrs2.keys():
        result.attrib[k2] = 'ATTRIB_HOLE'
    if algorithm == 1:
        for child in tree_diff_children(list(tree1), list(tree2), element_hash_strict, algorithm):
            result.append(child)
    elif algorithm == 2:
        if [child.tag for child in tree1] == [child.tag for child in tree2]:
            for i, child in enumerate(tree1):
                diff_child = tree_diff(child, tree2[i], algorithm)
                result.append(diff_child)
        else:
            result.append(etree.Element('MULTITAG_HOLE'))
    else:
        raise ValueError('Got invalid algorithm: %r' % algorithm)
    return result

def tree_extract_children(list1, list2, hash_func, algorithm):
    # list1 and list2 are lists of etree Elements.
    if list1 == list2 == []:
        return []
    best_size, offset1, offset2 = longest_common_substring([hash_func(el) for el in list1], [hash_func(el) for el in list2])
    result = []
    if best_size == 0:
        if [el.tag for el in list1] == ['MULTITAG_HOLE']:
            data = ''.join([etree.tostring(child, method='html') for child in list2])
            result.append({'type': 'multitag', 'value': data, 'tag': None})
        elif hash_func == element_hash_strict:
            result.extend(tree_extract_children(list1, list2, element_hash_loose, algorithm))
        else:
            raise NoMatch('Brain tag had children %r, but sample had %r' % (list1, list2))
    if offset1 > 0 and offset2 > 0:
        # There's leftover stuff on the left side of BOTH lists.
        result.extend(tree_extract_children(list1[:offset1], list2[:offset2], element_hash_strict, algorithm))
    elif offset1 > 0:
        # There's leftover stuff on the left side of ONLY the brain.
        if [el.tag for el in list1[:offset1]] == ['MULTITAG_HOLE']:
            result.append({'type': 'multitag', 'value': '', 'tag': None})
        else:
            raise NoMatch('Brain tag had children %r, but sample had %r' % (list1[:offset1], list2))
    elif offset2 > 0:
        # There's leftover stuff on the left side of ONLY the sample.
        raise NoMatch('Brain tag had children %r, but sample had %r' % (list1, list2))
    if best_size > 0:
        for i in range(best_size):
            child_result = tree_extract(list1[offset1+i], list2[offset2+i], algorithm)
            result.extend(child_result)
        if (offset1 + best_size < len(list1)) or (offset2 + best_size < len(list2)):
            # There's leftover stuff on the right side of EITHER list.
            child_result = tree_extract_children(list1[offset1+best_size:], list2[offset2+best_size:], element_hash_strict, algorithm)
            result.extend(child_result)
    return result

def tree_extract(brain, sample, algorithm):
    """
    Given two etrees -- a brain (the result of a tree_diff()) and a sample
    to extract from -- this returns a list of raw data from the sample.

    Each element in the resulting list is a dict of {type, value}, where:
        type is either 'attrib', 'text', 'multitag' or 'tail'
        value is a string of the raw data
    """
    result = []

    # Extract ATTRIB_HOLE.
    brain_attrs = sorted(dict(brain.attrib).items()) # Sort, to be deterministic in output.
    sample_attrs = dict(sample.attrib)
    for k, brain_value in brain_attrs:
        if brain_value == 'ATTRIB_HOLE':
            result.append({'type': 'attrib', 'value': sample_attrs.pop(k, ''), 'tag': brain.tag})
        else:
            sample_value = sample_attrs.pop(k, None)
            if brain_value != sample_value:
                raise NoMatch('<%s> %r attribute had different values: %r and %r' % (brain.tag, k, brain_value, sample_value))
    if sample_attrs:
        # If any attributes are left in sample_attrs, they weren't in the brain.
        raise NoMatch('<%s> attributes exist in sample but not in brain: %r' % (brain.tag, sample_attrs))

    # Extract TEXT_HOLE.
    if brain.text == 'TEXT_HOLE':
        result.append({'type': 'text', 'value': sample.text, 'tag': brain.tag})
    elif brain.text != sample.text:
        raise NoMatch('<%s> text had different values: %r and %r' % (brain.tag, brain.text, sample.text))

    # Extract MULTITAG_HOLE.
    brain_children = [child.tag for child in brain]
    sample_children = [child.tag for child in sample]
    if 'MULTITAG_HOLE' in brain_children:
        if algorithm == 1:
            multitag_result = tree_extract_children(list(brain), list(sample), element_hash_strict, algorithm)
            result.extend(multitag_result)
        elif algorithm == 2:
            data = ''.join([etree.tostring(child) for child in sample])
            result.append({'type': 'multitag', 'value': data, 'tag': None})
        else:
            ValueError('Got invalid algorithm: %r' % algorithm)
    elif brain_children == sample_children:
        for i, child in enumerate(brain):
            child_result = tree_extract(child, sample[i], algorithm)
            result.extend(child_result)
    else:
        raise NoMatch('Brain <%s> tag had children %r, but sample had %s' % \
            (brain.tag, brain_children, sample_children))

    # Extract TAIL_HOLE.
    if brain.tail == 'TAIL_HOLE':
        # Note that we use brain.getparent() here to get the tag that contains the tail text.
        result.append({'type': 'tail', 'value': sample.tail, 'tag': brain.getparent().tag})
    elif brain.tail != sample.tail:
        raise NoMatch('<%s> tail had different values: %r and %r' % (brain.tag, brain.tail, sample.tail))

    return result

class Template(object):
    def __init__(self, algorithm=1):
        # algorithm can be either 1 or 2.
        #     1 -- Smarter algorithm that removes more noise, but might fail.
        #     2 -- Dumber algorithm that doesn't remove as much noise, but it
        #          never fails.
        self.htmltree = None
        self.algorithm = algorithm

    def learn(self, html):
        tree = make_tree_and_preprocess(html)
        if self.htmltree is None:
            self.htmltree = tree
        else:
            self.htmltree = tree_diff(self.htmltree, tree, self.algorithm)

    def as_text(self):
        return etree.tostring(self.htmltree, method='html')

    def extract(self, html):
        tree = make_tree_and_preprocess(html)
        if self.htmltree is None:
            raise ValueError('This template has not learned anything yet.')
        return tree_extract(self.htmltree, tree, self.algorithm)

def extract(html, other_pages):
    """
    Given an HTML page string and list of other pages, creates a Template
    and extracts the data from the page.
    """
    # First try algorithm 1, because it's more effective. But if it fails,
    # fall back to algorithm 2.
    for algorithm in (1, 2):
        t = Template(algorithm=algorithm)
        for sample in [html] + other_pages:
            t.learn(sample)
        try:
            return t.extract(html)
        except NoMatch:
            if algorithm == 1:
                continue
            else:
                raise
    raise NoMatch('Reached end of extract() without having gotten a match')

########NEW FILE########
__FILENAME__ = template
from brain import Brain # relative import
from listdiff import listdiff # relative import
import re

class NoMatch(Exception):
    pass

class Template(object):
    def __init__(self, brain=None):
        if isinstance(brain, str):
            self.brain = Brain.from_serialized(brain)
        else:
            self.brain = brain

    def tokenize(self, text):
        """
        Returns a list of tokens for the given text. This list may include
        Hole instances.
        """
        return list(text)

    def learn(self, *texts):
        """
        Learns the given Sample String(s).
        """
        brain = self.brain
        for text in texts:
            tokens = self.tokenize(text)
            if brain is None:
                brain = Brain(tokens)
            else:
                brain = Brain(listdiff(brain, tokens))
        self.brain = brain

    def as_text(self, custom_marker='{{ HOLE }}'):
        """
        Returns a display-friendly version of the template, using the
        given custom_marker to mark template holes.
        """
        return self.brain.as_text(custom_marker)

    def num_holes(self):
        """
        Returns the number of holes in this template.
        """
        return self.brain.num_holes()

    def extract(self, text):
        """
        Given a bunch of text that is marked up using this template, extracts
        the data.

        Returns a tuple of the raw data, in the order in which it appears in
        the template. If the text doesn't match the template, raises NoMatch.
        """
        regex = self.brain.match_regex()
        m = re.search(regex, text)
        if m:
            return m.groups()
        raise NoMatch()

########NEW FILE########
__FILENAME__ = articletext
from ebdata.templatemaker.articletext import is_punctuated
import os.path
import unittest

class AutoTestMetaclass(type):
    """
    Metaclass that adds a test method for every pair in TEST_DATA.
    """
    def __new__(cls, name, bases, attrs):
        def make_test_func(input_value, expected):
            return lambda self: self.assertAutotest(input_value, expected)
        for i, (html, expected) in enumerate(attrs['TEST_DATA']):
            func = make_test_func(html, expected)
            func.__doc__ = repr(html)
            attrs['test_%03d' % i] = func # Use '%03d' to make tests run in order, because unittest uses string ordering.
        return type.__new__(cls, name, bases, attrs)

class PunctuatedTestCase(unittest.TestCase):
    __metaclass__ = AutoTestMetaclass

    TEST_DATA = (
        (u'This is a sentence.', True),
        (u'This is a sentence?', True),
        (u'This is a sentence!', True),
        (u'This is a sentence.  ', True),
        (u'This is a sentence?  ', True),
        (u'This is a sentence!  ', True),
        (u'Not a sentence', False),
        (u'Not. A! Sentence? Correct', False),
        (u'"This is a quoted sentence."', True),
        (u'"This is a quoted sentence." ', True),
        (u'"This is a quoted sentence. " ', True),
        (u'"This is a quoted sentence?"', True),
        (u'"This is a quoted sentence?" ', True),
        (u'"This is a quoted sentence ?" ', True),
        (u'"This is a quoted sentence!"', True),
        (u'"This is a quoted sentence!" ', True),
        (u'"This is a quoted sentence! " ', True),
        (u'This is a sentence (yeah).', True),
        (u'This is a sentence. (Yeah.)', True),
        (u'This is a sentence. (Yeah?)', True),
        (u'This is a sentence. (Yeah!)', True),
        (u'This is a sentence. ("Quoted.")', True),
        (u'This is a sentence. ("Quoted?")', True),
        (u'This is a sentence. ("Quoted!")', True),
        (u'This is a sentence. (\'Single-quoted.\')', True),
        (u'This is a sentence. (\'Single-quoted?\')', True),
        (u'This is a sentence. (\'Single-quoted!\')', True),
        (u'This is a sentence. ("\'Double-quoted.\'")', True),
        (u'This is a sentence. ("\'Double-quoted?\'")', True),
        (u'This is a sentence. ("\'Double-quoted!\'")', True),
        (u'This is a sentence. (\'"Double-quoted."\')', True),
        (u'This is a sentence. (\'"Double-quoted?"\')', True),
        (u'This is a sentence. (\'"Double-quoted!"\')', True),
        (u'He "said, \'It\'s going to reveal what else she has done\'."', True),
        (u'He \u201csaid, \u2018It\u2019s going to reveal what else she has done\u2019.\u201d', True),
    )

    def assertAutotest(self, sentence, expected):
        self.assertEqual(bool(is_punctuated(sentence)), expected)

if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = brain
from ebdata.templatemaker.brain import Brain
from ebdata.templatemaker.hole import Hole, OrHole, IgnoreHole
import unittest

class BrainTestCase(unittest.TestCase):
    def assertAsText(self, brain, marker, expected):
        """
        Asserts that Brain(brain).as_text(marker) == expected.
        """
        b = Brain(brain)
        if marker is not None:
            self.assertEqual(b.as_text(marker), expected)
        else:
            self.assertEqual(b.as_text(), expected)

    def assertNumHoles(self, brain, expected):
        """
        Asserts that Brain(brain).num_holes() == expected.
        """
        b = Brain(brain)
        self.assertEqual(b.num_holes(), expected)

    def assertRegex(self, brain, expected):
        """
        Asserts that Brain(brain).match_regex() == expected.
        """
        b = Brain(brain)
        self.assertEqual(b.match_regex(), expected)

    def test_as_text_empty1(self):
        self.assertAsText([], None, '')

    def test_as_text_empty2(self):
        self.assertAsText([], 'marker', '')

    def test_as_text1(self):
        self.assertAsText(['1', Hole(), '2', Hole(), '3'], None, '1{{ HOLE }}2{{ HOLE }}3')

    def test_as_text2(self):
        self.assertAsText(['1', Hole(), '2', Hole(), '3'], '!', '1!2!3')

    def test_num_holes_empty(self):
        self.assertNumHoles([], 0)

    def test_num_holes1(self):
        self.assertNumHoles(['a', 'b', 'c'], 0)

    def test_num_holes2(self):
        self.assertNumHoles(['a', Hole(), 'c'], 1)

    def test_num_holes3(self):
        self.assertNumHoles(['a', Hole(), 'c', Hole()], 2)

    def test_regex_empty(self):
        self.assertRegex([], '^(?s)$')

    def test_regex_noholes(self):
        self.assertRegex(['a', 'b', 'c'], '^(?s)abc$')

    def test_regex_special_chars(self):
        self.assertRegex(['^$?.*'], r'^(?s)\^\$\?\.\*$')

    def test_regex_holes1(self):
        self.assertRegex(['a', Hole(), 'b'], '^(?s)a(.*?)b$')

    def test_regex_holes2(self):
        self.assertRegex(['a', OrHole('b', 'c'), 'd', IgnoreHole()], '^(?s)a(b|c)d.*?$')

class BrainEmptyTestCase(unittest.TestCase):
    def assertConcise(self, brain, expected):
        """
        Asserts that Brain(brain).concise() == expected.
        """
        b = Brain(brain)
        self.assertEqual(b.concise(), expected)

    def test_empty(self):
        self.assertConcise([], [])

    def test_basic1(self):
        self.assertConcise(['a'], ['a'])

    def test_basic2(self):
        self.assertConcise(['a', 'b'], ['ab'])

    def test_basic3(self):
        self.assertConcise(['a', Hole(), 'b'], ['a', Hole(), 'b'])

    def test_basic4(self):
        self.assertConcise([Hole(), 'a', Hole(), 'b'], [Hole(), 'a', Hole(), 'b'])

    def test_basic5(self):
        self.assertConcise([Hole(), 'a', Hole(), 'b', Hole()],
            [Hole(), 'a', Hole(), 'b', Hole()])

    def test_basic6(self):
        self.assertConcise([Hole(), 'a', 'b', 'c', Hole(), 'd', 'e', 'f', Hole(), 'g'],
            [Hole(), 'abc', Hole(), 'def', Hole(), 'g'])

    def test_long_strings(self):
        self.assertConcise(['this is ', 'a test', Hole(), 'of the ', 'emergency ', 'broadcast system', Hole()],
            ['this is a test', Hole(), 'of the emergency broadcast system', Hole()])

class BrainSerialization(unittest.TestCase):
    def assertSerializes(self, brain):
        """
        Serializes and unserializes the given brain, asserting that a round
        trip works properly.
        """
        b = Brain(brain)
        self.assertEqual(b, Brain.from_serialized(b.serialize()))

    def test_empty(self):
        self.assertSerializes([])

    def test_integer(self):
        self.assertSerializes([1, 2, 3])

    def test_string(self):
        self.assertSerializes(['abc', 'd', 'e', 'fg hi jklmnop'])

    def test_hole1(self):
        self.assertSerializes([Hole()])

    def test_hole2(self):
        self.assertSerializes([Hole(), Hole(), Hole()])

    def test_hole_and_strings(self):
        self.assertSerializes([Hole(), 'abc', Hole(), 'def', Hole()])

    def test_format1(self):
        self.assertEqual(Brain([]).serialize(), 'gAJjZXZlcnlibG9jay50ZW1wbGF0ZW1ha2VyLmJyYWluCkJyYWluCnEBKYFxAn1xA2Iu\n')

    def test_format2(self):
        self.assertEqual(Brain([Hole(), 'abc', Hole()]).serialize(), 'gAJjZXZlcnlibG9jay50ZW1wbGF0ZW1ha2VyLmJyYWluCkJyYWluCnEBKYFxAihjZXZlcnlibG9j\nay50ZW1wbGF0ZW1ha2VyLmhvbGUKSG9sZQpxAymBcQR9cQViVQNhYmNxBmgDKYFxB31xCGJlfXEJ\nYi4=\n')

    def test_format_input1(self):
        self.assertEqual(Brain([]), Brain.from_serialized('gAJjZXZlcnlibG9jay50ZW1wbGF0ZW1ha2VyLmJyYWluCkJyYWluCnEBKYFxAn1xA2Iu\n'))

    def test_format_input2(self):
        self.assertEqual(Brain([Hole(), 'abc', Hole()]), Brain.from_serialized('gAJjZXZlcnlibG9jay50ZW1wbGF0ZW1ha2VyLmJyYWluCkJyYWluCnEBKYFxAihjZXZlcnlibG9j\nay50ZW1wbGF0ZW1ha2VyLmhvbGUKSG9sZQpxAymBcQR9cQViVQNhYmNxBmgDKYFxB31xCGJlfXEJ\nYi4=\n'))

if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = clean
from ebdata.templatemaker.clean import strip_template
from lxml import etree
from lxml.html import document_fromstring
import unittest

class StripTemplateTestCase(unittest.TestCase):
    def assertStrips(self, html1, html2, expected, num_removals, check_ids=False):
        """
        Asserts that strip_template(html1, html2) will result in the expected
        HTML string, and that the return value is num_removals.
        """
        # The test strings should *not* have <html> and <body> tags, for the
        # sake of brevity.
        tree1 = document_fromstring('<html><body>%s</body></html>' % html1)
        tree2 = document_fromstring('<html><body>%s</body></html>' % html2)
        expected = '<html><body>%s</body></html>' % expected

        got_removals = strip_template(tree1, tree2, check_ids=check_ids)
        got_tree = etree.tostring(tree1, method='html')
        self.assertEqual(got_tree, expected)
        self.assertEqual(got_removals, num_removals)

    def test_noop(self):
        self.assertStrips(
            '<p>Foo</p>',
            '<div>Bar</div>',
            '<p>Foo</p>',
            0,
        )

    def test_header(self):
        self.assertStrips(
            '<p>Header</p><h1>Headline 1</h1>',
            '<p>Header</p><h1>Headline 2</h1>',
            '<h1>Headline 1</h1>',
            1,
        )

    def test_footer(self):
        self.assertStrips(
            '<h1>Headline 1</h1><p>Footer</p>',
            '<h1>Headline 2</h1><p>Footer</p>',
            '<h1>Headline 1</h1>',
            1,
        )

    def test_header_and_footer(self):
        self.assertStrips(
            '<p>Header</p><h1>Headline 1</h1><p>Footer</p>',
            '<p>Header</p><h1>Headline 2</h1><p>Footer</p>',
            '<h1>Headline 1</h1>',
            2,
        )

    def test_header_same_tag(self):
        self.assertStrips(
            '<p>Header</p><p>Article 1</p>',
            '<p>Header</p><p>Article 2</p>',
            '<p>Article 1</p>',
            1,
        )

    def test_footer_same_tag(self):
        self.assertStrips(
            '<p>Article 1</p><p>Footer</p>',
            '<p>Article 2</p><p>Footer</p>',
            '<p>Article 1</p>',
            1,
        )

    def test_header_and_footer_same_tag(self):
        self.assertStrips(
            '<p>Header</p><p>Article 1</p><p>Footer</p>',
            '<p>Header</p><p>Article 2</p><p>Footer</p>',
            '<p>Article 1</p>',
            2,
        )

    def test_nested_1level(self):
        self.assertStrips(
            '<ul><li>News</li></ul><h1>Headline 1</h1>',
            '<ul><li>News</li></ul><h1>Headline 2</h1>',
            '<h1>Headline 1</h1>',
            2,
        )

    def test_nested_2level(self):
        self.assertStrips(
            '<div id="nav"><ul><li>News</li></ul></div><h1>Headline 1</h1>',
            '<div id="nav"><ul><li>News</li></ul></div><h1>Headline 2</h1>',
            '<h1>Headline 1</h1>',
            3,
        )

    def test_header_tail_same(self):
        self.assertStrips(
            '<p>Header</p> Tail <h1>Headline 1</h1>',
            '<p>Header</p> Tail <h1>Headline 2</h1>',
            '<h1>Headline 1</h1>',
            1,
        )

    def test_header_tail_different(self):
        self.assertStrips(
            '<p>Header</p> Tail1 <h1>Headline 1</h1>',
            '<p>Header</p> Tail2 <h1>Headline 2</h1>',
            ' Tail1 <h1>Headline 1</h1>',
            1,
        )

    def test_footer_head_same(self):
        self.assertStrips(
            '<h1>Headline 1</h1> Head <p>Footer</p>',
            '<h1>Headline 2</h1> Head <p>Footer</p>',
            '<h1>Headline 1</h1>',
            1,
        )

    def test_footer_head_different(self):
        self.assertStrips(
            '<h1>Headline 1</h1> Head1 <p>Footer</p>',
            '<h1>Headline 2</h1> Head2 <p>Footer</p>',
            '<h1>Headline 1</h1> Head1 ',
            1,
        )

    def test_same_tags_different_attributes1(self):
        self.assertStrips(
            '<p style="color: red;">Header</p><h1>Headline 1</h1>',
            '<p                    >Header</p><h1>Headline 2</h1>',
            '<p style="color: red;">Header</p><h1>Headline 1</h1>',
            0,
        )

    def test_same_tags_different_attributes2(self):
        self.assertStrips(
            '<p style="color: red;">Header</p><h1>Headline 1</h1>',
            '<p                    >Header</p><h1>Headline 1</h1>',
            '<p style="color: red;">Header</p>',
            1,
        )

    def test_different_level(self):
        """
        If data is identical but at a different level in the tree,
        strip_template() will not find it.
        """
        self.assertStrips(
            '<div><p>Foo</p><p>Bar</p></div>',
            '<p>Foo</p><p>Bar</p>',
            '<div><p>Foo</p><p>Bar</p></div>',
            0,
        )

    def test_ids_header(self):
        # This would be detected with check_ids=False, but this test makes sure
        # it doesn't break anything to use check_ids=True.
        self.assertStrips(
            '<p id="header">Header</p><h1>Headline 1</h1>',
            '<p id="header">Header</p><h1>Headline 2</h1>',
            '<h1>Headline 1</h1>',
            1,
            check_ids=True,
        )

    def test_ids_footer(self):
        self.assertStrips(
            '<h1>Headline 1</h1><p id="footer">Footer</p>',
            '<h1>Headline 2</h1><p id="footer">Footer</p>',
            '<h1>Headline 1</h1>',
            1,
            check_ids=True,
        )

    def test_ids_header_and_footer(self):
        self.assertStrips(
            '<p id="footer">Header</p><h1>Headline 1</h1><p id="footer">Footer</p>',
            '<p id="footer">Header</p><h1>Headline 2</h1><p id="footer">Footer</p>',
            '<h1>Headline 1</h1>',
            2,
            check_ids=True,
        )

    def test_ids_different_level1(self):
        self.assertStrips(
            '<div><p id="first">Foo</p><p id="second">Bar</p></div>',
            '<p id="first">Foo</p><p id="second">Bar</p>',
            '<div></div>',
            2,
            check_ids=True,
        )

    def test_ids_different_level2(self):
        self.assertStrips(
            '<div><p id="first">Foo</p><p id="second">Bar</p>Tail</div>',
            '<p id="first">Foo</p><p id="second">Bar</p>',
            '<div>Tail</div>',
            2,
            check_ids=True,
        )

    def test_ids_different_level3(self):
        self.assertStrips(
            '<div><p id="first">Foo</p><p id="second">Bar</p>Tail</div>',
            '<p id="first">Foo</p><p id="second">Bar</p>Tail',
            '<div></div>',
            2,
            check_ids=True,
        )

if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = hole
from ebdata.templatemaker.hole import Hole, OrHole, RegexHole, IgnoreHole
import unittest

class HoleEquality(unittest.TestCase):
    def test_equal_hole(self):
        self.assertEqual(Hole(), Hole())

    def test_nonequal_hole(self):
        self.assertNotEqual(Hole(), OrHole())

    def test_equal_orhole(self):
        self.assertEqual(OrHole('a', 'b'), OrHole('a', 'b'))

    def test_nonequal_orhole1(self):
        self.assertNotEqual(OrHole('a'), OrHole('a', 'b'))

    def test_nonequal_orhole2(self):
        self.assertNotEqual(OrHole('a'), OrHole('b'))

    def test_equal_regexhole1(self):
        self.assertEqual(RegexHole('\d\d', False), RegexHole('\d\d', False))

    def test_equal_regexhole2(self):
        self.assertEqual(RegexHole('(\d\d)', True), RegexHole('(\d\d)', True))

    def test_nonequal_regexhole1(self):
        self.assertNotEqual(RegexHole('\d\d', False), RegexHole('\d', False))

    def test_nonequal_regexhole2(self):
        self.assertNotEqual(RegexHole('\d', False), IgnoreHole())

    def test_nonequal_regexhole3(self):
        self.assertNotEqual(RegexHole('\d', False), Hole())

    def test_nonequal_regexhole4(self):
        self.assertNotEqual(RegexHole('\d\d', False), RegexHole('\d\d', True))

    def test_nonequal_regexhole5(self):
        self.assertNotEqual(RegexHole('\d\d', False), RegexHole('(\d\d)', False))

    def test_equal_ignorehole(self):
        self.assertEqual(IgnoreHole(), IgnoreHole())

    def test_nonequal_ignorehole1(self):
        self.assertNotEqual(IgnoreHole(), Hole())

    def test_nonequal_ignorehole2(self):
        self.assertNotEqual(IgnoreHole(), OrHole('a'))

class HoleRepr(unittest.TestCase):
    def test_hole(self):
        self.assertEqual(repr(Hole()), '<Hole>')

    def test_orhole(self):
        self.assertEqual(repr(OrHole(1, 2, 3, 4)), '<OrHole: (1, 2, 3, 4)>')

    def test_regexhole(self):
        self.assertEqual(repr(RegexHole('\d\d-\d\d', False)), '<RegexHole: \d\d-\d\d>')

    def test_ignorehole(self):
        self.assertEqual(repr(IgnoreHole()), '<IgnoreHole>')

class Regexes(unittest.TestCase):
    def test_hole(self):
        self.assertEqual(Hole().regex(), '(.*?)')

    def test_orhole1(self):
        self.assertEqual(OrHole('a', 'b').regex(), '(a|b)')

    def test_orhole2(self):
        self.assertEqual(OrHole('?', '.').regex(), '(\?|\.)')

    def test_regexhole(self):
        self.assertEqual(RegexHole('\d\d-\d\d', False).regex(), '\d\d-\d\d')

    def test_ignorehole(self):
        self.assertEqual(IgnoreHole().regex(), '.*?')

class HoleCapture(unittest.TestCase):
    def test_hole(self):
        self.assertEqual(Hole().capture, True)

    def test_orhole(self):
        self.assertEqual(OrHole('a', 'b').capture, True)

    def test_regexhole1(self):
        self.assertEqual(RegexHole('\d\d-\d\d', False).capture, False)

    def test_regexhole2(self):
        self.assertEqual(RegexHole('(\d\d-\d\d)', True).capture, True)

    def test_regexhole3(self):
        self.assertEqual(RegexHole('(\d\d-\d\d)', False).capture, False)

    def test_ignorehole(self):
        self.assertEqual(IgnoreHole().capture, False)

if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = htmlutils
from ebdata.templatemaker.htmlutils import percent_linked_text, printer_friendly_link
from ebdata.templatemaker.htmlutils import remove_empty_tags, brs_to_paragraphs
from ebdata.textmining.treeutils import preprocess
from lxml.html import document_fromstring
from lxml import etree
import unittest

class PercentLinkedTextTestCase(unittest.TestCase):
    def assertPercentLinked(self, html, expected):
        """
        Asserts that the given HTML string has the expected percentage of
        linked text.
        """
        tree = document_fromstring(html)
        self.assertEqual(percent_linked_text(tree), expected)

    def test_basic1(self):
        self.assertPercentLinked('<p><a href=".">Test</a></p>', 1.0)

    def test_basic2(self):
        self.assertPercentLinked('<p>Test</p>', 0.0)

    def test_basic3(self):
        self.assertPercentLinked('<p><a href=".">Test</a>Test</p>', 0.5)

    def test_basic4(self):
        self.assertPercentLinked('<p><a href=".">Test test</a>Test test</p>', 0.5)

    def test_basic5(self):
        self.assertPercentLinked('<p><a href=".">Test</a></p><p>Test2</p>', 4.0 / 9.0)

    def test_empty(self):
        self.assertPercentLinked('<p></p>', 0.0)

class PrinterFriendlyLinkTestCase(unittest.TestCase):
    def assertPrinterFriendlyLink(self, html, expected):
        """
        Asserts that the given HTML string has the expected printer-friendly
        URL.
        """
        tree = document_fromstring(html)
        self.assertEqual(printer_friendly_link(tree), expected)

    def test_empty1(self):
        self.assertPrinterFriendlyLink('<p></p>', None)

    def test_empty2(self):
        self.assertPrinterFriendlyLink('<p><a></a></p>', None)

    def test_empty3(self):
        self.assertPrinterFriendlyLink('<p><a href=""></a></p>', None)

    def test_empty4(self):
        self.assertPrinterFriendlyLink('<p><a href="/1/print/"></a></p>', None)

    def test_noprint1(self):
        self.assertPrinterFriendlyLink('<p><a href="">print</a></p>', None)

    def test_noprint2(self):
        self.assertPrinterFriendlyLink('<p><a href="foo">print</a></p>', None)

    def test_noprint3(self):
        self.assertPrinterFriendlyLink('<p><a href="print"></a></p>', None)

    def test_noprint4(self):
        self.assertPrinterFriendlyLink('<p><a href="print">foo</a></p>', None)

    def test_hit1(self):
        self.assertPrinterFriendlyLink('<p><a href="/1/print/">print</a></p>', '/1/print/')

    def test_hit2(self):
        self.assertPrinterFriendlyLink('<p><a href="/1/print/">printer</a></p>', '/1/print/')

    def test_hit3(self):
        self.assertPrinterFriendlyLink('<p><a href="/1/print/">printer-friendly</a></p>', '/1/print/')

    def test_hit4(self):
        self.assertPrinterFriendlyLink('<p><a href="/1/print/">this link happens to include the word print</a></p>', '/1/print/')

    def test_hit5(self):
        self.assertPrinterFriendlyLink('<p><a href="/1/printer/">printer-friendly</a></p>', '/1/printer/')

    def test_hit_with_child(self):
        self.assertPrinterFriendlyLink('<a href="/mediakit/print/"><img/>Print</a>', '/mediakit/print/')

    def test_case_insensitive1(self):
        self.assertPrinterFriendlyLink('<p><a href="/1/PRINT/">A PRINT VERSION</a></p>', '/1/PRINT/')

    def test_case_insensitive2(self):
        self.assertPrinterFriendlyLink('<p><a href="/1/Print/">Print version</a></p>', '/1/Print/')

    def test_multiple_links1(self):
        self.assertPrinterFriendlyLink('<a href="/1/print/">print 1</a><a href="/2/print/">print 2</a>', '/1/print/')

    def test_multiple_links_first_javascript(self):
        self.assertPrinterFriendlyLink('<a href="javascript:print();">print</a> <p><a href="/1/print/">print</a></p>', '/1/print/')

    def test_javascript1(self):
        self.assertPrinterFriendlyLink('<a href="javascript:print();">print</a>', None)

    def test_javascript2(self):
        self.assertPrinterFriendlyLink('<a href=" javascript:print(); "> print </a>', None)

    def test_false_positive_print_edition1(self):
        self.assertPrinterFriendlyLink('<a href="/news/printedition/front/">Print Edition</a>', None)

    def test_false_positive_print_edition2(self):
        self.assertPrinterFriendlyLink('<a href="/news/printedition/front/">Print-Edition</a>', None)

    def test_false_positive_print_edition3(self):
        self.assertPrinterFriendlyLink('<a href="/news/printedition/front/">Print edition</a>', None)

    def test_false_positive_print_edition4(self):
        self.assertPrinterFriendlyLink('<a href="/1/print/">Print edition</a>', None)

    def test_false_positive_reprint1(self):
        self.assertPrinterFriendlyLink('<a href="/services/site/la-reprint-request-splash,0,6731163.htmlstory">Reprint</a>', None)

    def test_false_positive_reprint2(self):
        self.assertPrinterFriendlyLink('<a href="/reprints">Reprints</a>', None)

    def test_false_positive_print_advertising1(self):
        self.assertPrinterFriendlyLink('<a href="/mediakit/print/">Print advertising</a>', None)

    def test_false_positive_print_advertising2(self):
        self.assertPrinterFriendlyLink('<a href="/mediakit/print/">Print ads</a>', None)

class PreprocessTestCase(unittest.TestCase):
    def assertPreprocess(self, html, expected, **kwargs):
        # The test strings should *not* have <html> and <body> tags, for the
        # sake of brevity.
        html = '<html><body>%s</body></html>' % html
        expected = '<html><body>%s</body></html>' % expected

        result_tree = preprocess(document_fromstring(html), **kwargs)
        got = etree.tostring(result_tree)
        self.assertEqual(got, expected)

    def test_comments1(self):
        self.assertPreprocess(
            '<h1><!-- test --></h1>',
            '<h1/>'
        )

    def test_comments2(self):
        self.assertPreprocess(
            '<h1>A<!-- test --></h1>',
            '<h1>A</h1>'
        )

    def test_comments3(self):
        self.assertPreprocess(
            '<h1><!-- test -->B</h1>',
            '<h1>B</h1>'
        )

    def test_comments4(self):
        self.assertPreprocess(
            '<h1>A<!-- test -->B</h1>',
            '<h1>AB</h1>'
        )

    def test_comments5(self):
        self.assertPreprocess(
            '<h1>A <!-- test -->B</h1>',
            '<h1>A B</h1>'
        )

    def test_comments6(self):
        self.assertPreprocess(
            '<h1><!-- <p> </p> --></h1>',
            '<h1/>'
        )

    def test_dropstyle1(self):
        self.assertPreprocess(
            '<style type="text/css">p { font-weight: 10px; }</style><p>Hello</p>',
            '<p>Hello</p>'
        )

    def test_dropstyle2(self):
        self.assertPreprocess(
            '<STYLE type="text/css">p { font-weight: 10px; }</STYLE><p>Hello</p>',
            '<p>Hello</p>'
        )

    def test_droplink1(self):
        self.assertPreprocess(
            '<link rel="stylesheet" /><p>Hello</p>',
            '<p>Hello</p>'
        )

    def test_dropmeta1(self):
        self.assertPreprocess(
            '<meta  /><p>Hello</p>',
            '<p>Hello</p>'
        )

    def test_dropscript1(self):
        self.assertPreprocess(
            '<script type="text/javascript">alert("hello");</script><p>Hello</p>',
            '<p>Hello</p>'
        )

    def test_dropnoscript1(self):
        self.assertPreprocess(
            '<noscript>Turn on JavaScript.</noscript><p>Hello</p>',
            '<p>Hello</p>'
        )

    def test_drop_tags1_control(self):
        self.assertPreprocess(
            '<b>Hello there</b>',
            '<b>Hello there</b>'
        )

    def test_drop_tags1(self):
        self.assertPreprocess(
            '<b>Hello there</b>',
            'Hello there',
            drop_tags=('b',),
        )

    def test_drop_tags_with_defaults(self):
        self.assertPreprocess(
            '<b>Hello there</b><style type="text/css">div { border: 1px; }</style>',
            'Hello there',
            drop_tags=('b',),
        )

    def test_drop_trees1_control(self):
        self.assertPreprocess(
            'That is <b>cool</b>',
            'That is <b>cool</b>'
        )

    def test_drop_trees1(self):
        self.assertPreprocess(
            'That is <b>cool</b>',
            'That is ',
            drop_trees=('b',)
        )

    def test_dropattrs1_control(self):
        self.assertPreprocess(
            '<div id="head">Hi</div>',
            '<div id="head">Hi</div>'
        )

    def test_dropattrs1(self):
        self.assertPreprocess(
            '<div id="head">Hi</div>',
            '<div>Hi</div>',
            drop_attrs=('id',)
        )

class RemoveEmptyTagsTestCase(unittest.TestCase):
    def assertTagsRemoved(self, html, expected, ignore_tags):
        """
        Asserts that removing all empty tags in `html` (except `ignore_tags`)
        will result in the string `expected`.
        """
        html = '<html><body>%s</body></html>' % html
        expected = '<html><body>%s</body></html>' % expected

        tree = document_fromstring(html)
        remove_empty_tags(tree, ignore_tags)
        self.assertEqual(etree.tostring(tree, method='html'), expected)

    def test_basic1(self):
        self.assertTagsRemoved('<p></p>', '', ())

    def test_basic2(self):
        self.assertTagsRemoved('<div></div>', '', ())

    def test_basic3(self):
        self.assertTagsRemoved('<br>', '', ())

    def test_basic4(self):
        self.assertTagsRemoved('a<p></p>b', 'ab', ())

    def test_basic5(self):
        self.assertTagsRemoved(' <p></p> ', '  ', ())

    def test_nested1(self):
        self.assertTagsRemoved('<div><p></p></div>', '', ())

    def test_nested2(self):
        self.assertTagsRemoved('<div><div><p></p></div></div>', '', ())

    def test_nested3(self):
        self.assertTagsRemoved('<div><div><p><br></p></div></div>', '', ())

    def test_nested4(self):
        self.assertTagsRemoved('<p><br></p>', '', ())

    def test_nested5(self):
        self.assertTagsRemoved('<div><p></p><p>Hey<span></span></p></div>', '<div><p>Hey</p></div>', ())

    def test_ignore1(self):
        self.assertTagsRemoved('<div></div>', '', ('br',))

    def test_ignore2(self):
        self.assertTagsRemoved('<br>', '<br>', ('br',))

    def test_ignore3(self):
        self.assertTagsRemoved('<p><br></p>', '<p><br></p>', ('br',))

    def test_wacky(self):
        self.assertTagsRemoved('<div><br/></div><br/>', '', ())

class BreakToParagraphTestCase(unittest.TestCase):
    def assertConverted(self, html, expected):
        html = '<html><body>%s</body></html>' % html
        expected = '<html><body>%s</body></html>' % expected

        tree = document_fromstring(html)
        tree = brs_to_paragraphs(tree)
        self.assertEqual(etree.tostring(tree, method='html'), expected)

    def test_basic1(self):
        self.assertConverted('<h1>Headline</h1>', '<h1>Headline</h1>')

    def test_basic2(self):
        self.assertConverted('<h1>Headline <span>Yo</span></h1>', '<h1>Headline <span>Yo</span></h1>')

    def test_basic3(self):
        self.assertConverted('First line<br>Second line', '<p>First line</p><p>Second line</p>')

    def test_basic4(self):
        self.assertConverted('<div>Hello there</div>', '<div>Hello there</div>')

    def test_empty(self):
        self.assertConverted('', '')

    def test_block_trailing_text(self):
        self.assertConverted('<div><h1>Headline</h1>Paragraph 1<br>Paragraph2</div>',
                             '<div><h1>Headline</h1><p>Paragraph 1</p><p>Paragraph2</p></div>')

    def test_initial_text(self):
        # make sure elements whose contents start with text get that text put into a <p>
        self.assertConverted('<div>Paragraph 1<br>Paragraph2</div>',
                             '<div><p>Paragraph 1</p><p>Paragraph2</p></div>')

    def test_consecutive_brs(self):
        # <br> tags with no intervening text shouldn't result in empty <p> tags
        self.assertConverted('<div>Paragraph 1<br><br><br>Paragraph 2</div>',
                             '<div><p>Paragraph 1</p><p>Paragraph 2</p></div>')

    def test_inline_links(self):
        # make sure inline <a> tags are kept in the <p> we build
        self.assertConverted('<div>Paragraph <a href="">1</a> is here.<br>Paragraph 2</div>',
                             '<div><p>Paragraph <a href="">1</a> is here.</p><p>Paragraph 2</p></div>')

    def test_trailing_whitespace(self):
        # make sure trailing whitespace doesn't get wrapped in p tags, and that
        # the element preceding the whitespace is handled correctly.
        self.assertConverted('<div><p>Paragraph 1<br>Paragraph2</p></div>   ',
                             '<div><p><p>Paragraph 1</p><p>Paragraph2</p></p></div>')

if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = listdiff
from ebdata.templatemaker.hole import Hole
from ebdata.templatemaker.listdiff import listdiff, longest_common_substring
import unittest

class LongestCommonSubstring(unittest.TestCase):
    def LCS(self, seq1, seq2):
        return longest_common_substring(seq1, seq2)

    def assertLCS(self, seq1, seq2, expected_length, expected_offset1, expected_offset2):
        best_size, offset1, offset2 = self.LCS(seq1, seq2)
        self.assertEqual(best_size, expected_length)
        self.assertEqual(offset1, expected_offset1)
        self.assertEqual(offset2, expected_offset2)

    def test_both_empty(self):
        self.assertLCS([], [], 0, -1, -1)

    def test_l1_empty(self):
        self.assertLCS([], ['a'], 0, -1, -1)

    def test_l2_empty(self):
        self.assertLCS(['a'], [], 0, -1, -1)

    def test_equal1(self):
        self.assertLCS(['a'], ['a'], 1, 0, 0)

    def test_equal2(self):
        self.assertLCS(['a', 'b', 'c'], ['a', 'b', 'c'], 3, 0, 0)

    def test_common1(self):
        self.assertLCS(['a', 'b', 'c'], ['b', 'c', 'a'], 2, 1, 0)

    def test_common2(self):
        self.assertLCS(['b', 'c', 'a'], ['a', 'b', 'c'], 2, 0, 1)

    def test_common3(self):
        self.assertLCS(['a', 'b', 'c', 'd'], ['a'], 1, 0, 0)

    def test_common4(self):
        self.assertLCS(['a', 'b', 'c', 'd'], ['b'], 1, 1, 0)

    def test_common5(self):
        self.assertLCS(['a', 'b', 'c', 'd'], ['c'], 1, 2, 0)

    def test_common6(self):
        self.assertLCS(['a', 'b', 'c', 'd'], ['d'], 1, 3, 0)

    def test_common7(self):
        self.assertLCS(['a', 'b', 'c', 'd'], ['c', 'd'], 2, 2, 0)

    def test_common8(self):
        self.assertLCS(['a', 'b', 'c', 'd'], ['f', 'c', 'd'], 2, 2, 1)

    def test_common9(self):
        self.assertLCS(['a'], ['a', 'b', 'c', 'd'], 1, 0, 0)

    def test_common10(self):
        self.assertLCS(['b'], ['a', 'b', 'c', 'd'], 1, 0, 1)

    def test_common11(self):
        self.assertLCS(['c'], ['a', 'b', 'c', 'd'], 1, 0, 2)

    def test_common12(self):
        self.assertLCS(['d'], ['a', 'b', 'c', 'd'], 1, 0, 3)

    def test_common13(self):
        self.assertLCS(['c', 'd'], ['a', 'b', 'c', 'd'], 2, 0, 2)

    def test_common14(self):
        self.assertLCS(['f', 'c', 'd'], ['a', 'b', 'c', 'd'], 2, 1, 2)

    def test_common15(self):
        self.assertLCS(['1', '2', '!', '4', '5'], ['1', '2', '3', '4', '5'], 2, 0, 0)

    def test_common16(self):
        self.assertLCS(['1', '2', '4', '5'], ['1', '2', '3', '4', '5'], 2, 0, 0)

    def test_common17(self):
        self.assertLCS(['1', '2', '3', '4', '5'], ['1', '2', '4', '5'], 2, 0, 0)

    def test_hole1(self):
        self.assertLCS([Hole()], [Hole()], 1, 0, 0)

    def test_hole2(self):
        self.assertLCS([Hole(), Hole()], [Hole(), Hole()], 2, 0, 0)

    def test_hole3(self):
        self.assertLCS([Hole(), 'a'], [Hole(), 'b'], 1, 0, 0)

    def test_hole4(self):
        self.assertLCS(['a', Hole(), 'b'], ['a', Hole(), 'b'], 3, 0, 0)

    def test_hole5(self):
        self.assertLCS(['b', Hole(), 'c'], ['a', Hole(), 'c'], 2, 1, 1)

    def test_hole6(self):
        self.assertLCS(['a', Hole(), 'b'], ['c', Hole(), 'd'], 1, 1, 1)

    def test_earliest1(self):
        "The LCS should be the earliest index in both strings."
        self.assertLCS(['b', 'a', 'c'], ['a', 'd', 'a'], 1, 1, 0)

    def test_earliest2(self):
        "The LCS should be the earliest index in both strings."
        self.assertLCS(['a', 'd', 'a'], ['b', 'a', 'c'], 1, 0, 1)

class ListdiffTestCase(unittest.TestCase):
    def assertListdiff(self, l1, l2, expected):
        self.assertEqual(listdiff(l1, l2), expected)

    def test_both_empty(self):
        self.assertListdiff([], [], [])

    def test_l1_empty(self):
        self.assertListdiff(
            [],
            ['a'],
            [Hole()],
        )

    def test_l2_empty(self):
        self.assertListdiff(
            ['a'],
            [],
            [Hole()],
        )

    def test_equal1(self):
        self.assertListdiff(
            ['a'],
            ['a'],
            ['a'],
        )

    def test_equal2(self):
        self.assertListdiff(
            ['a', 'b'],
            ['a', 'b'],
            ['a', 'b'],
        )

    def test_equal3(self):
        self.assertListdiff(
            ['a', 'b', 'c'],
            ['a', 'b', 'c'],
            ['a', 'b', 'c'],
        )

    def test_hole1(self):
        self.assertListdiff(
            ['Hello', ' ', 'John'],
            ['Hello', ' ', 'Fran'],
            ['Hello', ' ', Hole()],
        )

    def test_hole2(self):
        self.assertListdiff(
            ['Hello', ' ', 'John'],
            ['Goodbye', ' ', 'Fran'],
            [Hole(), ' ', Hole()],
        )

    def test_hole3(self):
        self.assertListdiff(
            ['a', 'b', 'c', 'd', 'e', 'f'],
            ['a', '_', 'c', '_', 'e', '_'],
            ['a', Hole(), 'c', Hole(), 'e', Hole()],
        )

    def test_hole4(self):
        self.assertListdiff(
            ['a', 'b', 'c', 'd', 'e', 'f'],
            ['_', 'b', '_', 'd', '_', 'f'],
            [Hole(), 'b', Hole(), 'd', Hole(), 'f'],
        )

    def test_hole5(self):
        self.assertListdiff(
            ['this', ' ', 'and', ' ', 'that'],
            ['foo', ' ', 'and', ' ', 'bar'],
            [Hole(), ' ', 'and', ' ', Hole()],
        )

    def test_hole6(self):
        self.assertListdiff(
            ['1', '2', '3', '4', '5'],
            ['1', '2', '4', '5'],
            ['1', '2', Hole(), '4', '5'],
        )

    def test_hole7(self):
        self.assertListdiff(
            ['1', '2', '4', '5'],
            ['1', '2', '3', '4', '5'],
            ['1', '2', Hole(), '4', '5'],
        )

    def test_hole8(self):
        self.assertListdiff(
            ['3', '4', '5'],
            ['4', '5'],
            [Hole(), '4', '5'],
        )

    def test_hole9(self):
        self.assertListdiff(
            ['4', '5'],
            ['5'],
            [Hole(), '5'],
        )

    def test_hole_input1(self):
        self.assertListdiff(
            [Hole()],
            [Hole()],
            [Hole()],
        )

    def test_hole_input2(self):
        self.assertListdiff(
            [],
            [Hole()],
            [Hole()],
        )

    def test_hole_input3(self):
        self.assertListdiff(
            [Hole()],
            [],
            [Hole()],
        )

    def test_hole_input4(self):
        self.assertListdiff(
            [Hole(), 'hello'],
            [Hole(), 'hello'],
            [Hole(), 'hello'],
        )

    def test_hole_input5(self):
        self.assertListdiff(
            [Hole(), 'person 1'],
            [Hole(), 'person 2'],
            [Hole(), Hole()],
        )

    def test_hole_input6(self):
        self.assertListdiff(
            [Hole(), 'person 1', ' test'],
            [Hole(), 'person 2', ' test'],
            [Hole(), Hole(), ' test'],
        )

    def test_hole_input7(self):
        self.assertListdiff(
            ['foo', Hole(), 'person 1 test'],
            ['foo', Hole(), 'person 2 test'],
            ['foo', Hole(), Hole()],
        )

if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = listdiffc
"""
These tests are identical to the ones in listdiff.py but use the C version of
longest_common_substring instead of the pure Python version.
"""

from ebdata.templatemaker.listdiffc import longest_common_subsequence as longest_common_substring
from listdiff import LongestCommonSubstring
import unittest

class LongestCommonSubstringC(LongestCommonSubstring):
    def LCS(self, seq1, seq2):
        return longest_common_substring(seq1, seq2)

del LongestCommonSubstring

if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = run_all
import glob

for filename in glob.glob('*.py'):
    exec 'from %s import *' % filename[:-3]

if __name__ == '__main__':
    import unittest
    unittest.main()

########NEW FILE########
__FILENAME__ = sst
from ebdata.templatemaker.sst import tree_diff, Template, NoMatch
from ebdata.textmining.treeutils import preprocess
from lxml import etree
from lxml.html import document_fromstring
import unittest

class TreeDiffTestCaseAlgorithm1(unittest.TestCase):
    algorithm = 1
    def assertTreeDiff(self, html1, html2, expected):
        """
        Asserts that the given HTML strings will produce a tree_diff of the
        expected HTML string.
        """
        # The test strings should *not* have <html> and <body> tags, for the
        # sake of brevity.
        tree1 = document_fromstring('<html><body>%s</body></html>' % html1)
        tree2 = document_fromstring('<html><body>%s</body></html>' % html2)
        expected = '<html><body>%s</body></html>' % expected

        result_tree = tree_diff(preprocess(tree1), preprocess(tree2), self.algorithm)
        got = etree.tostring(result_tree)
        self.assertEqual(got, expected)

    def test_same1(self):
        self.assertTreeDiff(
            '<h1>test</h1>',
            '<h1>test</h1>',
            '<h1>test</h1>'
        )

    def test_case_insensitive_tags(self):
        self.assertTreeDiff(
            '<h1>test</h1>',
            '<H1>test</H1>',
            '<h1>test</h1>'
        )

    def test_texthole1(self):
        self.assertTreeDiff(
            '<h1>Headline</h1>',
            '<h1>Different</h1>',
            '<h1>TEXT_HOLE</h1>'
        )

    def test_texthole2(self):
        self.assertTreeDiff(
            '<h1>Headline</h1><p>Para</p>',
            '<h1>Different</h1><p>Para</p>',
            '<h1>TEXT_HOLE</h1><p>Para</p>'
        )

    def test_texthole3(self):
        self.assertTreeDiff(
            '<h1>Headline</h1><p>Para</p><p>Final</p>',
            '<h1>Different</h1><p>Para</p><p>Diff</p>',
            '<h1>TEXT_HOLE</h1><p>Para</p><MULTITAG_HOLE/>'
        )

    def test_tailhole1(self):
        self.assertTreeDiff(
            '<p>That was <b>so</b> fun.</p>',
            '<p>That was <b>so</b> boring.</p>',
            '<p>That was <b>so</b>TAIL_HOLE</p>'
        )

    def test_attribhole1(self):
        self.assertTreeDiff(
            '<p id="foo">Hello</p>',
            '<p id="bar">Hello</p>',
            '<p id="ATTRIB_HOLE">Hello</p>'
        )

    def test_attribhole2(self):
        self.assertTreeDiff(
            '<p id="bar" class="1">Hello</p>',
            '<p id="bar" class="2">Hello</p>',
            '<p id="bar" class="ATTRIB_HOLE">Hello</p>'
        )

    def test_attribhole3(self):
        self.assertTreeDiff(
            '<p id="bar">Hello</p>',
            '<p>Hello</p>',
            '<p id="ATTRIB_HOLE">Hello</p>'
        )

    def test_attribhole4(self):
        self.assertTreeDiff(
            '<p>Hello</p>',
            '<p id="bar">Hello</p>',
            '<p id="ATTRIB_HOLE">Hello</p>'
        )

    def test_multitaghole1(self):
        self.assertTreeDiff(
            '<div><p>Yes</p><p>No</p></div>',
            '<div><p>Yes</p><p>No</p><p>Maybe</p></div>',
            '<div><p>Yes</p><p>No</p><MULTITAG_HOLE/></div>'
        )

    def test_multitaghole2(self):
        self.assertTreeDiff(
            '<div>Text <p>Yes</p><p>No</p></div>',
            '<div>Text <p>Yes</p><p>No</p><p>Maybe</p></div>',
            '<div>Text <p>Yes</p><p>No</p><MULTITAG_HOLE/></div>'
        )

    def test_multitaghole3(self):
        self.assertTreeDiff(
            '<div><p>Yes</p><p>No</p> Tail</div>',
            '<div><p>Yes</p><p>No</p><p>Maybe</p> Tail</div>',
            '<div><p>Yes</p><p>No</p>TAIL_HOLE<MULTITAG_HOLE/></div>'
        )

    def test_multitaghole4(self):
        self.assertTreeDiff(
            '<div><p>Yes</p><p>No</p></div>',
            '<div><p>Foo</p><p>Bar</p><p>Maybe</p></div>',
            '<div><MULTITAG_HOLE/></div>'
        )

    def test_multitaghole5(self):
        self.assertTreeDiff(
            '<div><p>Yes</p><p>No</p></div>',
            '<div><p>Yes</p><p>Bar</p><p>Maybe</p></div>',
            '<div><p>Yes</p><MULTITAG_HOLE/></div>'
        )

    def test_multitaghole6(self):
        self.assertTreeDiff(
            '<div><p>Yes</p><p>No</p></div>',
            '<div><p>Yes</p><p id="test">No</p><p>Maybe</p></div>',
            '<div><p>Yes</p><p id="ATTRIB_HOLE">No</p><MULTITAG_HOLE/></div>'
        )

    def test_same_level_p(self): 
        self.assertTreeDiff( 
            '<p>First 1</p>', 
            '<p>Second 1</p><p>Second 2</p>', 
            '<MULTITAG_HOLE/>' 
        ) 

    def test_same_level_h1(self): 
        self.assertTreeDiff( 
            '<h1>First 1</h1>', 
            '<h1>Second 1</h1><p>Second 2</p>', 
            '<h1>TEXT_HOLE</h1><MULTITAG_HOLE/>'
        ) 

    def test_same_level_h2(self): 
        self.assertTreeDiff( 
            '<h2>First 1</h2>', 
            '<h2>Second 1</h2><p>Second 2</p>', 
            '<h2>TEXT_HOLE</h2><MULTITAG_HOLE/>'
        ) 

    def test_same_level_h3(self): 
        self.assertTreeDiff( 
            '<h3>First 1</h3>', 
            '<h3>Second 1</h3><p>Second 2</p>', 
            '<h3>TEXT_HOLE</h3><MULTITAG_HOLE/>'
        ) 

    def test_same_level_h4(self): 
        self.assertTreeDiff( 
            '<h4>First 1</h4>', 
            '<h4>Second 1</h4><p>Second 2</p>', 
            '<h4>TEXT_HOLE</h4><MULTITAG_HOLE/>'
        ) 

    def test_same_level_h5(self): 
        self.assertTreeDiff( 
            '<h5>First 1</h5>', 
            '<h5>Second 1</h5><p>Second 2</p>', 
            '<h5>TEXT_HOLE</h5><MULTITAG_HOLE/>'
        ) 

    def test_same_level_h6(self): 
        self.assertTreeDiff( 
            '<h6>First 1</h6>', 
            '<h6>Second 1</h6><p>Second 2</p>', 
            '<h6>TEXT_HOLE</h6><MULTITAG_HOLE/>'
        ) 

    def test_same_level_a(self): 
        self.assertTreeDiff( 
            '<a>First 1</a>', 
            '<a>Second 1</a><p>Second 2</p>', 
            '<a>TEXT_HOLE</a><MULTITAG_HOLE/>'
        ) 

    def test_same_level1(self):
        self.assertTreeDiff(
            '<h1>Man seen</h1><p>By John Smith</p><p>A man was seen today.</p>',
            '<h1>Bird seen</h1><p>By John Smith</p><p>A bird was seen yesterday.</p>',
            '<h1>TEXT_HOLE</h1><p>By John Smith</p><MULTITAG_HOLE/>'
        )

    def test_same_level2(self):
        self.assertTreeDiff(
            '<p>By John Smith</p><h1>Man seen</h1><p>A man was seen today.</p>',
            '<p>By John Smith</p><h1>Bird seen</h1><p>A bird was seen yesterday.</p>',
            '<p>By John Smith</p><h1>TEXT_HOLE</h1><MULTITAG_HOLE/>'
        )

    def test_same_level3(self):
        self.assertTreeDiff(
            '<p>By John Smith</p><h1>Man seen</h1><p>A man was seen today.</p><p>The end.</p>',
            '<p>By John Smith</p><h1>Bird seen</h1><p>A bird was seen yesterday.</p><p>The end.</p>',
            '<p>By John Smith</p><h1>TEXT_HOLE</h1><MULTITAG_HOLE/><p>The end.</p>'
        )

    def test_confusing(self):
        # Note: The "~" are in here to make this more understandable by vertical alignment.
        self.assertTreeDiff(
            '<ul>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<li/>\n\t\t<li/>\n\t\t<li class="current"></li>\n\t\t<li/>\n\t</ul>'.replace('~', ''),
            '<ul><li class="current"></li>\n\t\t<li/>\n\t\t<li/>\n\t\t~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<li/>\n\t</ul>'.replace('~', ''),
            '<ul><MULTITAG_HOLE/>~~~~~~~~~~~~~~~<li/>\n\t\t<li/>\n\t\t<MULTITAG_HOLE/>~~~~~~~~~~~~~~~<li/>\n\t</ul>'.replace('~', ''),
        )

    def test_mixed1(self):
        self.assertTreeDiff(
            '<h1>Headline</h1><p>This thing</p><br/><div id="footer">Copyright 2006</div>',
            '<h1>Headline 2</h1><p id="first">This thing</p><br/><div id="footer">Copyright 2007</div>',
            '<h1>TEXT_HOLE</h1><p id="ATTRIB_HOLE">This thing</p><MULTITAG_HOLE/>'
        )

    def test_comments_ignored1(self):
        self.assertTreeDiff(
            '<h1><!-- comment --></h1>',
            '<h1></h1>',
            '<h1/>',
        )

    def test_comments_ignored2(self):
        self.assertTreeDiff(
            '<h1>A<!-- comment --></h1>',
            '<h1>A</h1>',
            '<h1>A</h1>',
        )

    def test_comments_ignored3(self):
        self.assertTreeDiff(
            '<h1><!-- comment -->A</h1>',
            '<h1>A</h1>',
            '<h1>A</h1>',
        )

    def test_comments_ignored4(self):
        self.assertTreeDiff(
            '<h1>A<!-- comment -->B</h1>',
            '<h1>AB</h1>',
            '<h1>AB</h1>',
        )

    def test_comments_ignored5(self):
        self.assertTreeDiff(
            '<h1>Title <!-- foo -->here</h1>',
            '<h1>Title here</h1>',
            '<h1>Title here</h1>',
        )

    def test_comments_ignored6(self):
        self.assertTreeDiff(
            '<h1>Title <!-- foo -->here</h1><!--<p>nothing</p>--><p><!--foo-->Paragraph here</p>',
            '<h1>Title here</h1><p>Paragraph here</p>',
            '<h1>Title here</h1><p>Paragraph here</p>',
        )

class TreeDiffTestCaseAlgorithm2(TreeDiffTestCaseAlgorithm1):
    """
    Like TreeDiffTestCaseAlgorithm1, but it uses algorithm 2. As such, it only
    needs to implement the methods whose outcome is different between the two
    algorithms.
    """
    algorithm = 2

    def test_texthole3(self):
        self.assertTreeDiff(
            '<h1>Headline</h1><p>Para</p><p>Final</p>',
            '<h1>Different</h1><p>Para</p><p>Diff</p>',
            '<h1>TEXT_HOLE</h1><p>Para</p><p>TEXT_HOLE</p>'
        )

    def test_multitaghole1(self):
        self.assertTreeDiff(
            '<div><p>Yes</p><p>No</p></div>',
            '<div><p>Yes</p><p>No</p><p>Maybe</p></div>',
            '<div><MULTITAG_HOLE/></div>'
        )

    def test_multitaghole2(self):
        self.assertTreeDiff(
            '<div>Text <p>Yes</p><p>No</p></div>',
            '<div>Text <p>Yes</p><p>No</p><p>Maybe</p></div>',
            '<div>Text <MULTITAG_HOLE/></div>'
        )

    def test_multitaghole3(self):
        self.assertTreeDiff(
            '<div><p>Yes</p><p>No</p> Tail</div>',
            '<div><p>Yes</p><p>No</p><p>Maybe</p> Tail</div>',
            '<div><MULTITAG_HOLE/></div>'
        )

    def test_multitaghole5(self):
        self.assertTreeDiff(
            '<div><p>Yes</p><p>No</p></div>',
            '<div><p>Yes</p><p>Bar</p><p>Maybe</p></div>',
            '<div><MULTITAG_HOLE/></div>'
        )

    def test_multitaghole6(self):
        self.assertTreeDiff(
            '<div><p>Yes</p><p>No</p></div>',
            '<div><p>Yes</p><p id="test">No</p><p>Maybe</p></div>',
            '<div><MULTITAG_HOLE/></div>'
        )
    def test_same_level_h1(self): 
        self.assertTreeDiff( 
            '<h1>First 1</h1>', 
            '<h1>Second 1</h1><p>Second 2</p>', 
            '<MULTITAG_HOLE/>'
        ) 

    def test_same_level_h2(self): 
        self.assertTreeDiff( 
            '<h2>First 1</h2>', 
            '<h2>Second 1</h2><p>Second 2</p>', 
            '<MULTITAG_HOLE/>'
        ) 

    def test_same_level_h3(self): 
        self.assertTreeDiff( 
            '<h3>First 1</h3>', 
            '<h3>Second 1</h3><p>Second 2</p>', 
            '<MULTITAG_HOLE/>'
        ) 

    def test_same_level_h4(self): 
        self.assertTreeDiff( 
            '<h4>First 1</h4>', 
            '<h4>Second 1</h4><p>Second 2</p>', 
            '<MULTITAG_HOLE/>'
        ) 

    def test_same_level_h5(self): 
        self.assertTreeDiff( 
            '<h5>First 1</h5>', 
            '<h5>Second 1</h5><p>Second 2</p>', 
            '<MULTITAG_HOLE/>'
        ) 

    def test_same_level_h6(self): 
        self.assertTreeDiff( 
            '<h6>First 1</h6>', 
            '<h6>Second 1</h6><p>Second 2</p>', 
            '<MULTITAG_HOLE/>'
        ) 

    def test_same_level_a(self): 
        self.assertTreeDiff( 
            '<a>First 1</a>', 
            '<a>Second 1</a><p>Second 2</p>', 
            '<MULTITAG_HOLE/>'
        ) 

    def test_same_level1(self):
        self.assertTreeDiff(
            '<h1>Man seen</h1><p>By John Smith</p><p>A man was seen today.</p>',
            '<h1>Bird seen</h1><p>By John Smith</p><p>A bird was seen yesterday.</p>',
            '<h1>TEXT_HOLE</h1><p>By John Smith</p><p>TEXT_HOLE</p>'
        )

    def test_same_level2(self):
        self.assertTreeDiff(
            '<p>By John Smith</p><h1>Man seen</h1><p>A man was seen today.</p>',
            '<p>By John Smith</p><h1>Bird seen</h1><p>A bird was seen yesterday.</p>',
            '<p>By John Smith</p><h1>TEXT_HOLE</h1><p>TEXT_HOLE</p>'
        )

    def test_same_level3(self):
        self.assertTreeDiff(
            '<p>By John Smith</p><h1>Man seen</h1><p>A man was seen today.</p><p>The end.</p>',
            '<p>By John Smith</p><h1>Bird seen</h1><p>A bird was seen yesterday.</p><p>The end.</p>',
            '<p>By John Smith</p><h1>TEXT_HOLE</h1><p>TEXT_HOLE</p><p>The end.</p>'
        )

    def test_confusing(self):
        # Note: The "~" are in here to make this more understandable by vertical alignment.
        self.assertTreeDiff(
            '<ul>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<li/>\n\t\t<li/>\n\t\t<li class="current"></li>\n\t\t<li/>\n\t</ul>'.replace('~', ''),
            '<ul><li class="current"></li>\n\t\t<li/>\n\t\t<li/>\n\t\t~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<li/>\n\t</ul>'.replace('~', ''),
            '<ul><li class="ATTRIB_HOLE"/>\n\t\t<li/>\n\t\t<li class="ATTRIB_HOLE"/>\n\t\t<li/>\n\t</ul>'.replace('~', ''),
        )

    def test_mixed1(self):
        self.assertTreeDiff(
            '<h1>Headline</h1><p>This thing</p><br/><div id="footer">Copyright 2006</div>',
            '<h1>Headline 2</h1><p id="first">This thing</p><br/><div id="footer">Copyright 2007</div>',
            '<h1>TEXT_HOLE</h1><p id="ATTRIB_HOLE">This thing</p><br/><div id="footer">TEXT_HOLE</div>'
        )

class TemplateExtractionTestCaseAlgorithm1(unittest.TestCase):
    algorithm = 1
    def assertExtracts(self, html_list, expected_data_list):
        """
        Creates a Template from every string in html_list, then extracts the
        data from each of those strings, asserting that the data matches
        expected_data_list.
        """
        t = Template(algorithm=self.algorithm)
        for html in html_list:
            t.learn(html)
        got_data_list = []
        for html in html_list:
            got_data_list.append(t.extract(html))
        self.assertEqual(got_data_list, expected_data_list)

    def assertNoMatch(self, html_list, sample):
        """
        Creates a Template from every string in html_list, then asserts that
        t.extract(sample) raises NoMatch.
        """
        t = Template()
        for html in html_list:
            t.learn(html)
        self.assertRaises(NoMatch, t.extract, sample)

    def test_same1(self):
        self.assertExtracts(
            ['<h1>test</h1>', '<h1>test</h1>'],
            [[], []],
        )

    def test_case_insensitive_tags(self):
        self.assertExtracts(
            ['<h1>test</h1>', '<H1>test</H1>'],
            [[], []],
        )

    def test_texthole1(self):
        self.assertExtracts(
            ['<h1>Headline</h1>', '<h1>Different</h1>'],
            [[{'type': 'text', 'value': 'Headline', 'tag': 'h1'}],
             [{'type': 'text', 'value': 'Different', 'tag': 'h1'}]]
        )

    def test_texthole2(self):
        self.assertExtracts(
            ['<h1>Headline</h1><p>Para</p>', '<h1>Different</h1><p>Para</p>'],
            [[{'type': 'text', 'value': 'Headline', 'tag': 'h1'}],
             [{'type': 'text', 'value': 'Different', 'tag': 'h1'}]]
        )

    def test_texthole3(self):
        self.assertExtracts(
            ['<h1>Headline</h1><p>Para</p><p>Final</p>', '<h1>Different</h1><p>Para</p><p>Diff</p>'],
            [[{'type': 'text', 'value': 'Headline', 'tag': 'h1'}, {'type': 'multitag', 'value': '<p>Final</p>', 'tag': None}],
             [{'type': 'text', 'value': 'Different', 'tag': 'h1'}, {'type': 'multitag', 'value': '<p>Diff</p>', 'tag': None}]]
        )

    def test_tailhole1(self):
        self.assertExtracts(
            ['<p>That was <b>so</b> fun.</p>', '<p>That was <b>so</b> boring.</p>'],
            [[{'type': 'tail', 'value': ' fun.', 'tag': 'p'}],
             [{'type': 'tail', 'value': ' boring.', 'tag': 'p'}]]
        )

    def test_tailhole2(self):
        self.assertExtracts(
            ['<p>That was <em><b>so</b></em> fun.</p>', '<p>That was <em><b>so</b></em> boring.</p>'],
            [[{'type': 'tail', 'value': ' fun.', 'tag': 'p'}],
             [{'type': 'tail', 'value': ' boring.', 'tag': 'p'}]]
        )

    def test_attribhole1(self):
        self.assertExtracts(
            ['<p id="foo">Hello</p>', '<p id="bar">Hello</p>'],
            [[{'type': 'attrib', 'value': 'foo', 'tag': 'p'}],
             [{'type': 'attrib', 'value': 'bar', 'tag': 'p'}]]
        )

    def test_attribhole2(self):
        self.assertExtracts(
            ['<p id="bar" class="1">Hello</p>', '<p id="bar" class="2">Hello</p>'],
            [[{'type': 'attrib', 'value': '1', 'tag': 'p'}],
             [{'type': 'attrib', 'value': '2', 'tag': 'p'}]]
        )

    def test_attribhole3(self):
        self.assertExtracts(
            ['<p id="bar">Hello</p>', '<p>Hello</p>'],
            [[{'type': 'attrib', 'value': 'bar', 'tag': 'p'}],
             [{'type': 'attrib', 'value': '', 'tag': 'p'}]]
        )

    def test_attribhole4(self):
        self.assertExtracts(
            ['<p>Hello</p>', '<p id="bar">Hello</p>'],
            [[{'type': 'attrib', 'value': '', 'tag': 'p'}],
             [{'type': 'attrib', 'value': 'bar', 'tag': 'p'}]]
        )

    def test_attribhole5(self):
        self.assertExtracts(
            ['<p class="klass" id="eyedee">Hello</p>', '<p id="eyedee" class="klass">Hello</p>'],
            [[], []]
        )

    def test_attribhole6(self):
        self.assertExtracts(
            ['<p class="klass" id="eyedee">Hello</p>', '<p id="eyedee2" class="klass2">Hello</p>'],
            [[{'type': 'attrib', 'value': 'klass', 'tag': 'p'}, {'type': 'attrib', 'value': 'eyedee', 'tag': 'p'}],
             [{'type': 'attrib', 'value': 'klass2', 'tag': 'p'}, {'type': 'attrib', 'value': 'eyedee2', 'tag': 'p'}]]
        )

    def test_attribhole7(self):
        self.assertExtracts(
            ['<p class="klass" id="eyedee">Hello</p>', '<p id="eyedee2" class="klass2" newatt="on">Hello</p>'],
            [[{'type': 'attrib', 'value': 'klass', 'tag': 'p'}, {'type': 'attrib', 'value': 'eyedee', 'tag': 'p'}, {'type': 'attrib', 'value': '', 'tag': 'p'}],
             [{'type': 'attrib', 'value': 'klass2', 'tag': 'p'}, {'type': 'attrib', 'value': 'eyedee2', 'tag': 'p'}, {'type': 'attrib', 'value': 'on', 'tag': 'p'}]]
        )

    def test_multitaghole1(self):
        self.assertExtracts(
            ['<div><p>Yes</p><p>No</p></div>', '<div><p>Yes</p><p>No</p><p>Maybe</p></div>'],
            [[{'type': 'multitag', 'value': '', 'tag': None}],
             [{'type': 'multitag', 'value': '<p>Maybe</p>', 'tag': None}]]
        )

    def test_multitaghole2(self):
        self.assertExtracts(
            ['<div>Text <p>Yes</p><p>No</p></div>', '<div>Text <p>Yes</p><p>No</p><p>Maybe</p></div>'],
            [[{'type': 'multitag', 'value': '', 'tag': None}],
             [{'type': 'multitag', 'value': '<p>Maybe</p>', 'tag': None}]]
        )

    def test_multitaghole3(self):
        self.assertExtracts(
            ['<div><p>Yes</p><p>No</p> Tail</div>',
             '<div><p>Yes</p><p>No</p><p>Maybe</p> Tail</div>'],
            [[{'type': 'multitag', 'value': '', 'tag': None}],
             [{'type': 'multitag', 'value': '<p>Maybe</p>', 'tag': None}]]
        )

    def test_multitaghole4(self):
        self.assertExtracts(
            ['<div><p>Yes</p><p>No</p></div>', '<div><p>Foo</p><p>Bar</p><p>Maybe</p></div>'],
            [[{'type': 'multitag', 'value': '<p>Yes</p><p>No</p>', 'tag': None}],
             [{'type': 'multitag', 'value': '<p>Foo</p><p>Bar</p><p>Maybe</p>', 'tag': None}]]
        )

    def test_multitaghole5(self):
        self.assertExtracts(
            ['<div><p>Yes</p><p>No</p></div>', '<div><p>Yes</p><p>Bar</p><p>Maybe</p></div>'],
            [[{'tag': None, 'type': 'multitag', 'value': '<p>No</p>'}],
             [{'tag': None, 'type': 'multitag', 'value': '<p>Bar</p><p>Maybe</p>'}]]
        )

    def test_multitaghole6(self):
        self.assertExtracts(
            ['<div><p>Yes</p><p>No</p></div>', '<div><p>Yes</p><p id="test">No</p><p>Maybe</p></div>'],
            [[{'tag': 'p', 'type': 'attrib', 'value': ''}, {'tag': None, 'type': 'multitag', 'value': ''}],
             [{'tag': 'p', 'type': 'attrib', 'value': 'test'}, {'tag': None, 'type': 'multitag', 'value': '<p>Maybe</p>'}]]
        )

    def test_same_level1(self):
        self.assertExtracts(
            ['<h1>Man seen</h1><p>By John Smith</p><p>A man was seen today.</p>',
             '<h1>Bird seen</h1><p>By John Smith</p><p>A bird was seen yesterday.</p>'],
            [[{'tag': 'h1', 'type': 'text', 'value': 'Man seen'}, {'tag': None, 'type': 'multitag', 'value': '<p>A man was seen today.</p>'}],
             [{'tag': 'h1', 'type': 'text', 'value': 'Bird seen'}, {'tag': None, 'type': 'multitag', 'value': '<p>A bird was seen yesterday.</p>'}]]
        )

    def test_same_level2(self):
        self.assertExtracts(
            ['<p>By John Smith</p><h1>Man seen</h1><p>A man was seen today.</p>',
             '<p>By John Smith</p><h1>Bird seen</h1><p>A bird was seen yesterday.</p>'],
            [[{'tag': 'h1', 'type': 'text', 'value': 'Man seen'}, {'tag': None, 'type': 'multitag', 'value': '<p>A man was seen today.</p>'}],
             [{'tag': 'h1', 'type': 'text', 'value': 'Bird seen'}, {'tag': None, 'type': 'multitag', 'value': '<p>A bird was seen yesterday.</p>'}]]
        )

    def test_same_level3(self):
        self.assertExtracts(
            ['<p>By John Smith</p><h1>Man seen</h1><p>A man was seen today.</p><p>The end.</p>',
             '<p>By John Smith</p><h1>Bird seen</h1><p>A bird was seen yesterday.</p><p>The end.</p>'],
            [[{'tag': 'h1', 'type': 'text', 'value': 'Man seen'}, {'tag': None, 'type': 'multitag', 'value': '<p>A man was seen today.</p>'}],
             [{'tag': 'h1', 'type': 'text', 'value': 'Bird seen'}, {'tag': None, 'type': 'multitag', 'value': '<p>A bird was seen yesterday.</p>'}]]
        )

    def test_confusing1(self):
        self.assertExtracts(
            ['<ul>\n\t\t<li></li>\n\t\t<li></li>\n\t\t<li class="current"></li>\n\t\t<li></li>\n\t</ul>',
             '<ul>\n\t\t<li class="current"></li>\n\t\t<li></li>\n\t\t<li></li>\n\t\t<li></li>\n\t</ul>',
             '<ul>\n\t\t<li></li>\n\t\t<li class="current"></li>\n\t\t<li></li>\n\t\t<li></li>\n\t</ul>'],
            [[{'tag': None, 'type': 'multitag', 'value': ''},
              {'tag': None, 'type': 'multitag', 'value': ''},
              {'tag': None, 'type': 'multitag', 'value': '<li class="current">\n\t\t'}],
             [{'tag': None, 'type': 'multitag', 'value': '<li class="current">\n\t\t'},
              {'tag': None, 'type': 'multitag', 'value': ''},
              {'tag': None, 'type': 'multitag', 'value': ''}],
             [{'tag': None, 'type': 'multitag', 'value': ''},
              {'tag': None, 'type': 'multitag', 'value': '<li class="current">\n\t\t'},
              {'tag': None, 'type': 'multitag', 'value': ''}]]
        )

    def test_confusing2(self):
        self.assertExtracts(
            ['<a>Test</a><hr><a>Foo</a> | <a>Bar</a>',
              '<b>bold:</b> <a>link1</a><input><a>link2</a>'],
            [[{'tag': None, 'type': 'multitag', 'value': ''},
              {'tag': 'a', 'type': 'text', 'value': 'Test'},
              {'tag': None, 'type': 'multitag', 'value': '<hr>'},
              {'tag': 'a', 'type': 'text', 'value': 'Foo'},
              {'tag': 'body', 'type': 'tail', 'value': ' | '},
              {'tag': None, 'type': 'multitag', 'value': '<a>Bar</a>'}],
             [{'tag': None, 'type': 'multitag', 'value': '<b>bold:</b> '},
              {'tag': 'a', 'type': 'text', 'value': 'link1'},
              {'tag': None, 'type': 'multitag', 'value': '<input>'},
              {'tag': 'a', 'type': 'text', 'value': 'link2'},
              {'tag': 'body', 'type': 'tail', 'value': None},
              {'tag': None, 'type': 'multitag', 'value': ''}]]
        )

    def test_mixed1(self):
        self.assertExtracts(
            ['<h1>Headline</h1><p>This thing</p><br/><div id="footer">Copyright 2006</div>',
             '<h1>Headline 2</h1><p id="first">This thing</p><br/><div id="footer">Copyright 2007</div>'],
            [[{'type': 'text', 'value': 'Headline', 'tag': 'h1'}, {'type': 'attrib', 'value': '', 'tag': 'p'}, {'type': 'multitag', 'value': '<br><div id="footer">Copyright 2006</div>', 'tag': None}],
             [{'type': 'text', 'value': 'Headline 2', 'tag': 'h1'}, {'type': 'attrib', 'value': 'first', 'tag': 'p'}, {'type': 'multitag', 'value': '<br><div id="footer">Copyright 2007</div>', 'tag': None}]]
        )

    def test_comments_ignored1(self):
        self.assertExtracts(
            ['<h1><!-- comment --></h1>', '<h1></h1>'],
            [[], []]
        )

    def test_comments_ignored2(self):
        self.assertExtracts(
            ['<h1>A<!-- comment --></h1>', '<h1>A</h1>'],
            [[], []]
        )

    def test_comments_ignored3(self):
        self.assertExtracts(
            ['<h1><!-- comment -->A</h1>', '<h1>A</h1>'],
            [[], []]
        )

    def test_comments_ignored4(self):
        self.assertExtracts(
            ['<h1>A<!-- comment -->B</h1>', '<h1>AB</h1>'],
            [[], []]
        )

    def test_comments_ignored5(self):
        self.assertExtracts(
            ['<h1>Title <!-- foo -->here</h1>', '<h1>Title here</h1>'],
            [[], []]
        )

    def test_comments_ignored6(self):
        self.assertExtracts(
            ['<h1>Title <!-- foo -->here</h1><!--<p>nothing</p>--><p><!--foo-->Paragraph here</p>',
             '<h1>Title here</h1><p>Paragraph here</p>'],
            [[], []]
        )

    def test_nomatch_texthole1(self):
        self.assertNoMatch(
            ['<h1>test</h1>', '<h1>test</h1>'],
            '<h1>bar</h1>',
        )

    def test_nomatch_texthole2(self):
        self.assertNoMatch(
            ['<h1>test</h1><p>Foo</p>', '<h1>test</h1><p>Bar</p>'],
            '<h1>bar</h1><p>Foo</p>',
        )

    def test_nomatch_multitaghole1(self):
        self.assertNoMatch(
            ['<div><p>1</p><p>2</p></div>', '<div><p>1</p><p>2</p></div>'],
            '<div><p>1</p><p>2</p><p>3</p></div>'
        )

    def test_nomatch_tailhole1(self):
        self.assertNoMatch(
            ['<p>This is <b>bolded</b>, right?</p>', '<p>This is <b>bolded</b>, right?</p>'],
            '<p>This is <b>bolded</b>, no?</p>'
        )

    def test_namespaced1(self): 
        self.assertExtracts( 
            ['<h1 foo:bar="ignore">Headline</h1>', '<h1>Different</h1>'], 
            [[{'type': 'text', 'value': 'Headline', 'tag': 'h1'}],
             [{'type': 'text', 'value': 'Different', 'tag': 'h1'}]] 
        ) 

    def test_namespaced2(self): 
        self.assertExtracts( 
            ['<h1>Headline</h1>', '<h1 foo:bar="ignore">Different</h1>'], 
            [[{'type': 'text', 'value': 'Headline', 'tag': 'h1'}],
             [{'type': 'text', 'value': 'Different', 'tag': 'h1'}]] 
        ) 

    def test_namespaced3(self): 
        self.assertExtracts( 
            ['<h1 foo:bar="ignore">Headline</h1>', '<h1 foo:bar="ignore">Different</h1>'], 
            [[{'type': 'text', 'value': 'Headline', 'tag': 'h1'}],
             [{'type': 'text', 'value': 'Different', 'tag': 'h1'}]] 
        ) 

    def test_namespaced4(self): 
        self.assertExtracts( 
            ['<h1 foo:bar="ignore" class="red">Headline</h1>', '<h1 foo:bar="ignore">Different</h1>'], 
            [[{'type': 'attrib', 'value': 'red', 'tag': 'h1'}, {'type': 'text', 'value': 'Headline', 'tag': 'h1'}],
             [{'type': 'attrib', 'value': '', 'tag': 'h1'}, {'type': 'text', 'value': 'Different', 'tag': 'h1'}]] 
        ) 

class TemplateExtractionTestCaseAlgorithm2(TemplateExtractionTestCaseAlgorithm1):
    algorithm = 2

    def test_texthole3(self):
        self.assertExtracts(
            ['<h1>Headline</h1><p>Para</p><p>Final</p>', '<h1>Different</h1><p>Para</p><p>Diff</p>'],
            [[{'tag': 'h1', 'type': 'text', 'value': 'Headline'},
              {'tag': 'p', 'type': 'text', 'value': 'Final'}],
             [{'tag': 'h1', 'type': 'text', 'value': 'Different'},
              {'tag': 'p', 'type': 'text', 'value': 'Diff'}]]
        )

    def test_multitaghole1(self):
        self.assertExtracts(
            ['<div><p>Yes</p><p>No</p></div>', '<div><p>Yes</p><p>No</p><p>Maybe</p></div>'],
            [[{'tag': None, 'type': 'multitag', 'value': '<p>Yes</p><p>No</p>'}],
             [{'tag': None, 'type': 'multitag', 'value': '<p>Yes</p><p>No</p><p>Maybe</p>'}]]
        )

    def test_multitaghole2(self):
        self.assertExtracts(
            ['<div>Text <p>Yes</p><p>No</p></div>', '<div>Text <p>Yes</p><p>No</p><p>Maybe</p></div>'],
            [[{'tag': None, 'type': 'multitag', 'value': '<p>Yes</p><p>No</p>'}],
             [{'tag': None, 'type': 'multitag', 'value': '<p>Yes</p><p>No</p><p>Maybe</p>'}]]
        )

    def test_multitaghole3(self):
        self.assertExtracts(
            ['<div><p>Yes</p><p>No</p> Tail</div>',
             '<div><p>Yes</p><p>No</p><p>Maybe</p> Tail</div>'],
            [[{'tag': None, 'type': 'multitag', 'value': '<p>Yes</p><p>No</p> Tail'}],
             [{'tag': None, 'type': 'multitag', 'value': '<p>Yes</p><p>No</p><p>Maybe</p> Tail'}]]
        )

    def test_multitaghole5(self):
        self.assertExtracts(
            ['<div><p>Yes</p><p>No</p></div>', '<div><p>Yes</p><p>Bar</p><p>Maybe</p></div>'],
            [[{'tag': None, 'type': 'multitag', 'value': '<p>Yes</p><p>No</p>'}],
             [{'tag': None, 'type': 'multitag', 'value': '<p>Yes</p><p>Bar</p><p>Maybe</p>'}]]
        )

    def test_multitaghole6(self):
        self.assertExtracts(
            ['<div><p>Yes</p><p>No</p></div>', '<div><p>Yes</p><p id="test">No</p><p>Maybe</p></div>'],
            [[{'tag': None, 'type': 'multitag', 'value': '<p>Yes</p><p>No</p>'}],
             [{'tag': None, 'type': 'multitag', 'value': '<p>Yes</p><p id="test">No</p><p>Maybe</p>'}]]
        )

    def test_same_level1(self):
        self.assertExtracts(
            ['<h1>Man seen</h1><p>By John Smith</p><p>A man was seen today.</p>',
             '<h1>Bird seen</h1><p>By John Smith</p><p>A bird was seen yesterday.</p>'],
            [[{'tag': 'h1', 'type': 'text', 'value': 'Man seen'},
              {'tag': 'p', 'type': 'text', 'value': 'A man was seen today.'}],
             [{'tag': 'h1', 'type': 'text', 'value': 'Bird seen'},
              {'tag': 'p', 'type': 'text', 'value': 'A bird was seen yesterday.'}]]
        )

    def test_same_level2(self):
        self.assertExtracts(
            ['<p>By John Smith</p><h1>Man seen</h1><p>A man was seen today.</p>',
             '<p>By John Smith</p><h1>Bird seen</h1><p>A bird was seen yesterday.</p>'],
            [[{'tag': 'h1', 'type': 'text', 'value': 'Man seen'},
              {'tag': 'p', 'type': 'text', 'value': 'A man was seen today.'}],
             [{'tag': 'h1', 'type': 'text', 'value': 'Bird seen'},
              {'tag': 'p', 'type': 'text', 'value': 'A bird was seen yesterday.'}]]
        )

    def test_same_level3(self):
        self.assertExtracts(
            ['<p>By John Smith</p><h1>Man seen</h1><p>A man was seen today.</p><p>The end.</p>',
             '<p>By John Smith</p><h1>Bird seen</h1><p>A bird was seen yesterday.</p><p>The end.</p>'],
            [[{'tag': 'h1', 'type': 'text', 'value': 'Man seen'},
              {'tag': 'p', 'type': 'text', 'value': 'A man was seen today.'}],
             [{'tag': 'h1', 'type': 'text', 'value': 'Bird seen'},
              {'tag': 'p', 'type': 'text', 'value': 'A bird was seen yesterday.'}]]
        )

    def test_confusing1(self):
        self.assertExtracts(
            ['<ul>\n\t\t<li></li>\n\t\t<li></li>\n\t\t<li class="current"></li>\n\t\t<li></li>\n\t</ul>',
             '<ul>\n\t\t<li class="current"></li>\n\t\t<li></li>\n\t\t<li></li>\n\t\t<li></li>\n\t</ul>',
             '<ul>\n\t\t<li></li>\n\t\t<li class="current"></li>\n\t\t<li></li>\n\t\t<li></li>\n\t</ul>'],
            [[{'tag': 'li', 'type': 'attrib', 'value': ''},
              {'tag': 'li', 'type': 'attrib', 'value': ''},
              {'tag': 'li', 'type': 'attrib', 'value': 'current'}],
             [{'tag': 'li', 'type': 'attrib', 'value': 'current'},
              {'tag': 'li', 'type': 'attrib', 'value': ''},
              {'tag': 'li', 'type': 'attrib', 'value': ''}],
             [{'tag': 'li', 'type': 'attrib', 'value': ''},
              {'tag': 'li', 'type': 'attrib', 'value': 'current'},
              {'tag': 'li', 'type': 'attrib', 'value': ''}]]
        )

    def test_confusing2(self):
        self.assertExtracts(
            ['<a>Test</a><hr><a>Foo</a> | <a>Bar</a>',
              '<b>bold:</b> <a>link1</a><input><a>link2</a>'],
            [[{'tag': None, 'type': 'multitag', 'value': '<a>Test</a><hr/><a>Foo</a> | <a>Bar</a>'}],
             [{'tag': None, 'type': 'multitag', 'value': '<b>bold:</b> <a>link1</a><input/><a>link2</a>'}]]
        )

    def test_mixed1(self):
        self.assertExtracts(
            ['<h1>Headline</h1><p>This thing</p><br/><div id="footer">Copyright 2006</div>',
             '<h1>Headline 2</h1><p id="first">This thing</p><br/><div id="footer">Copyright 2007</div>'],
            [[{'tag': 'h1', 'type': 'text', 'value': 'Headline'},
              {'tag': 'p', 'type': 'attrib', 'value': ''},
              {'tag': 'div', 'type': 'text', 'value': 'Copyright 2006'}],
             [{'tag': 'h1', 'type': 'text', 'value': 'Headline 2'},
              {'tag': 'p', 'type': 'attrib', 'value': 'first'},
              {'tag': 'div', 'type': 'text', 'value': 'Copyright 2007'}]]
        )

if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = template
import unittest
from ebdata.templatemaker import Template, NoMatch
from ebdata.templatemaker.brain import Brain
from ebdata.templatemaker.hole import Hole

class TemplatemakerTestCase(unittest.TestCase):
    def create_the_long_way(self, *inputs):
        """
        "Helper method that returns a Template with the given inputs.
        """
        t = Template()
        for i in inputs:
            t.learn(i)
        return t

    def create_the_short_way(self, *inputs):
        t = Template()
        t.learn(*inputs)
        return t

    def assertCreated(self, expected, *inputs):
        """
        Asserts that a Template with the given inputs would be
        rendered as_text('!') to the expected string.
        """
        t1 = self.create_the_long_way(*inputs)
        t2 = self.create_the_short_way(*inputs)
        self.assertEqual(t1.as_text('!'), expected)
        self.assertEqual(t2.as_text('!'), expected)

class TemplatemakerExtractTestCase(unittest.TestCase):
    """
    This class' tests assume that self.setUp() creates self.template.
    """
    def assertExtracts(self, text, expected):
        """
        Asserts that self.template.extract(text) returns expected.
        """
        self.assertEqual(self.template.extract(text), expected)

    def assertNoMatch(self, text):
        """
        Asserts that self.template.extract(text) raises NoMatch.
        """
        self.assertRaises(NoMatch, self.template.extract, text)

class Creation(TemplatemakerTestCase):
    def test_noop1(self):
        self.assertCreated('<title>123</title>', '<title>123</title>')

    def test_noop2(self):
        self.assertCreated('<title>123</title>', '<title>123</title>', '<title>123</title>')

    def test_noop3(self):
        self.assertCreated('<title>123</title>', '<title>123</title>', '<title>123</title>', '<title>123</title>')

    def test_one_char_start1(self):
        self.assertCreated('!2345', '12345', '_2345')

    def test_one_char_start2(self):
        self.assertCreated('!2345', '12345', '12345', '_2345')

    def test_one_char_start3(self):
        self.assertCreated('!2345', '12345', '_2345', '^2345')

    def test_one_char_end1(self):
        self.assertCreated('1234!', '12345', '1234_')

    def test_one_char_end2(self):
        self.assertCreated('1234!', '12345', '12345', '1234_')

    def test_one_char_end3(self):
        self.assertCreated('1234!', '12345', '1234_', '1234^')

    def test_one_char_middle1(self):
        self.assertCreated('12!45', '12345', '12_45')

    def test_one_char_middle2(self):
        self.assertCreated('12!45', '12345', '12345', '12_45')

    def test_one_char_middle3(self):
        self.assertCreated('12!45', '12345', '12_45', '12^45')

    def test_one_char_middle4(self):
        self.assertCreated('12!45', '12345', '1245')

    def test_multi_char_start1(self):
        self.assertCreated('!345', '12345', '_2345', '1_345')

    def test_multi_char_start2(self):
        self.assertCreated('!345', '12345', '1_345', '_2345')

    def test_multi_char_start3(self):
        self.assertCreated('!45', '12345', '_2345', '1_345', '12_45')

    def test_multi_char_start4(self):
        self.assertCreated('!5', '12345', '_2345', '1_345', '12_45', '123_5')

    def test_multi_char_end1(self):
        self.assertCreated('1234!', '12345', '1234_')

    def test_multi_char_end2(self):
        self.assertCreated('123!', '12345', '1234_', '123_5')

    def test_multi_char_end3(self):
        self.assertCreated('12!', '12345', '1234_', '123_5', '12_45')

    def test_multi_char_end4(self):
        self.assertCreated('1!', '12345', '1234_', '123_5', '12_45', '1_345')

    def test_empty(self):
        self.assertCreated('', '', '')

    def test_no_similarities1(self):
        self.assertCreated('!', 'a', 'b')

    def test_no_similarities2(self):
        self.assertCreated('!', 'ab', 'ba', 'ac', 'bc')

    def test_no_similarities3(self):
        self.assertCreated('!', 'abc', 'ab_', 'a_c', '_bc')

    def test_left_weight1(self):
        self.assertCreated('!a!', 'ab', 'ba') # NOT '!b!'

    def test_left_weight2(self):
        self.assertCreated('a!b!', 'abc', 'acb')

    def test_multihole1(self):
        self.assertCreated('!2!', '123', '_23', '12_')

    def test_multihole2(self):
        self.assertCreated('!2!4!', '12345', '_2_4_')

    def test_multihole3(self):
        self.assertCreated('!2!4!', '12345', '_2345', '12_45', '1234_')

    def test_multihole4(self):
        self.assertCreated('!2!456!8', '12345678', '_2_456_8')

    def test_multihole5(self):
        self.assertCreated('!2!456!8', '12345678', '_2345678', '12_45678', '123456_8')

    def test_multihole6(self):
        self.assertCreated('!e! there', 'hello there', 'goodbye there')

class ExtractNoHoles(TemplatemakerExtractTestCase):
    def setUp(self):
        self.template = Template(Brain(['hello']))

    def test_extracts_nothing(self):
        self.assertExtracts('hello', ())

    def test_no_match_empty(self):
        self.assertNoMatch('')

    def test_no_match_case_sensitive1(self):
        self.assertNoMatch('Hello')

    def test_no_match_case_sensitive2(self):
        self.assertNoMatch('HELLO')

    def test_no_match_invalid(self):
        self.assertNoMatch('goodbye')

    def test_no_match_spaces1(self):
        self.assertNoMatch('hello ')

    def test_no_match_spaces2(self):
        self.assertNoMatch(' hello')

    def test_no_match_spaces3(self):
        self.assertNoMatch(' hello ')

class ExtractOneHole(TemplatemakerExtractTestCase):
    def setUp(self):
        self.template = Template(Brain(['Hello, ', Hole(), '. How are you?']))

    def test_one_word(self):
        self.assertExtracts('Hello, Picasso. How are you?', ('Picasso',))

    def test_two_words(self):
        self.assertExtracts('Hello, Michael Jordan. How are you?', ('Michael Jordan',))

    def test_three_words(self):
        self.assertExtracts('Hello, Frank Lloyd Wright. How are you?', ('Frank Lloyd Wright',))

    def test_period(self):
        self.assertExtracts('Hello, Richard J. Daley. How are you?', ('Richard J. Daley',))

    def test_empty_value(self):
        self.assertExtracts('Hello, . How are you?', ('',))

    def test_no_match_empty(self):
        self.assertNoMatch('')

    def test_no_match_case_sensitive(self):
        self.assertNoMatch('hello, friend. how are you?')

    def test_no_match_invalid(self):
        self.assertNoMatch('foo')

    def test_no_match_slightly_off1(self):
        self.assertNoMatch('Hello, friend.')

    def test_no_match_slightly_off2(self):
        self.assertNoMatch('Hello. How are you?')

    def test_no_match_slightly_off3(self):
        self.assertNoMatch('Hello friend. How are you?') # No comma

class ExtractTwoHoles(TemplatemakerExtractTestCase):
    def setUp(self):
        self.template = Template(Brain(['<p>', Hole(), ' and ', Hole(), '</p>']))

    def test_basic1(self):
        self.assertExtracts('<p>this and that</p>', ('this', 'that'))

    def test_basic2(self):
        self.assertExtracts('<p>foo and bar</p>', ('foo', 'bar'))

    def test_multiple_ands(self):
        self.assertExtracts('<p>and and and</p>', ('and', 'and'))

    def test_spaces1(self):
        self.assertExtracts('<p> this  and  that </p>', (' this ', ' that '))

    def test_spaces2(self):
        self.assertExtracts('<p>  and  </p>', (' ', ' '))

    def test_dots(self):
        self.assertExtracts('<p>. and .</p>', ('.', '.'))

    def test_question_marks(self):
        self.assertExtracts('<p>? and ?</p>', ('?', '?'))

    def test_empty_values(self):
        self.assertExtracts('<p> and </p>', ('', ''))

    def test_one_empty_value_first(self):
        self.assertExtracts('<p> and that</p>', ('', 'that'))

    def test_one_empty_value_second(self):
        self.assertExtracts('<p>this and </p>', ('this', ''))

    def test_no_match_empty(self):
        self.assertNoMatch('')

    def test_no_match_case_sensitive(self):
        self.assertNoMatch('<P>this and that</P>')

    def test_no_match_invalid(self):
        self.assertNoMatch('foo')

    def test_no_match_slightly_off1(self):
        self.assertNoMatch('this and that')

    def test_no_match_slightly_off2(self):
        self.assertNoMatch('<p></p>')

    def test_no_match_slightly_off3(self):
        self.assertNoMatch('<p>and</p>')

class ExtractWithHoleAtStart(TemplatemakerExtractTestCase):
    def setUp(self):
        self.template = Template(Brain([Hole(), ' and bar']))

    def test_basic(self):
        self.assertExtracts('foo and bar', ('foo',))

    def test_and(self):
        self.assertExtracts('and and bar', ('and',))

    def test_empty_value(self):
        self.assertExtracts(' and bar', ('',))

    def test_space_value(self):
        self.assertExtracts('  and bar', (' ',))

    def test_large(self):
        self.assertExtracts('This and that and this and that and bar', ('This and that and this and that',))

    def test_no_match_empty(self):
        self.assertNoMatch('')

    def test_no_match_case_sensitive(self):
        self.assertNoMatch('foo AND BAR')

    def test_no_match_invalid(self):
        self.assertNoMatch('foo')

    def test_no_match_slightly_off1(self):
        self.assertNoMatch('foo and bar.')

    def test_no_match_slightly_off2(self):
        self.assertNoMatch('and bar')

    def test_no_match_slightly_off3(self):
        self.assertNoMatch('and bar ')

class ExtractWithHoleAtEnd(TemplatemakerExtractTestCase):
    def setUp(self):
        self.template = Template(Brain(['foo and ', Hole()]))

    def test_basic(self):
        self.assertExtracts('foo and bar', ('bar',))

    def test_and(self):
        self.assertExtracts('foo and and', ('and',))

    def test_empty_value(self):
        self.assertExtracts('foo and ', ('',))

    def test_space_value(self):
        self.assertExtracts('foo and  ', (' ',))

    def test_period(self):
        self.assertExtracts('foo and bar.', ('bar.',))

    def test_large(self):
        self.assertExtracts('foo and this and that and this and that', ('this and that and this and that',))

    def test_no_match_empty(self):
        self.assertNoMatch('')

    def test_no_match_case_sensitive(self):
        self.assertNoMatch('FOO AND bar')

    def test_no_match_invalid(self):
        self.assertNoMatch('foo')

    def test_no_match_slightly_off1(self):
        self.assertNoMatch('foo and')

    def test_no_match_slightly_off2(self):
        self.assertNoMatch(' foo and')

class Initialization(unittest.TestCase):
    def test_string(self):
        t = Template(brain='Y2NvcHlfcmVnCl9yZWNvbnN0cnVjdG9yCnAxCihjZXZlcnlibG9jay50ZW1wbGF0ZW1ha2VyLmJy\nYWluCkJyYWluCnAyCmNfX2J1aWx0aW5fXwpsaXN0CnAzCihscDQKZzEKKGNldmVyeWJsb2NrLnRl\nbXBsYXRlbWFrZXIuaG9sZQpIb2xlCnA1CmNfX2J1aWx0aW5fXwpvYmplY3QKcDYKTnRScDcKYVMn\nYWJjJwpwOAphZzEKKGc1Cmc2Ck50UnA5CmF0UnAxMAou\n')
        self.assertEqual(t.brain, [Hole(), 'abc', Hole()])

    def test_brain(self):
        t = Template(brain=[Hole(), 'abc', Hole()])
        self.assertEqual(t.brain, [Hole(), 'abc', Hole()])

    def test_none(self):
        t = Template(brain=None)
        self.assertEqual(t.brain, None)

if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = textlist
from ebdata.templatemaker.textlist import html_to_paragraph_list
from ebdata.textmining.treeutils import make_tree
import unittest

class AutoTextMetaclass(type):
    """
    Metaclass that adds a test method for every pair in TEST_DATA.
    """
    def __new__(cls, name, bases, attrs):
        TEST_DATA = (
            # input, expected output
            ('hello', ['hello']),
            ('hello\nthere', ['hello there']),
            ('hello\r\nthere', ['hello there']),

            ('<p>First</p><p>Second</p>', ['First', 'Second']),
            ('<p>First<p>Second</p>', ['First', 'Second']),
            ('First<p>Second</p>', ['First', 'Second']),
            ('First<p>Second', ['First', 'Second']),
            ('<p>First</p>Second', ['First', 'Second']),
            ('First</p>Second', ['First', 'Second']),
            ('First</p>Second</p>', ['First', 'Second']),
            ('First<p></p>Second', ['First', 'Second']),
            ('<p></p>First<p></p>Second', ['First', 'Second']),
            ('</p>First</p>Second', ['First', 'Second']),

            ('hello<br>there', ['hello', 'there']),
            ('hello<br>\nthere', ['hello', 'there']),
            ('hello<br><br>there', ['hello', 'there']),
            ('hello<br><br><br>there', ['hello', 'there']),
            ('hello<br><br><br><br>there', ['hello', 'there']),

            ('<p>hello<br>there</p>', ['hello', 'there']),
            ('<p>hello</p><br><p>there</p>', ['hello', 'there']),
            ('hello<br><p>there</p>', ['hello', 'there']),
            ('hello<br><p>there</p>', ['hello', 'there']),
        )
        def make_test_func(html, expected):
            return lambda self: self.assertConverts(html, expected)
        for i, (html, expected) in enumerate(TEST_DATA):
            func = make_test_func(html, expected)
            func.__doc__ = repr(html)
            attrs['test_%03d' % i] = func # Use '%03d' to make tests run in order, because unittest uses string ordering.
        return type.__new__(cls, name, bases, attrs)

class LocationTestCase(unittest.TestCase):
    __metaclass__ = AutoTextMetaclass

    def assertConverts(self, html, expected):
        self.assertEqual(html_to_paragraph_list(make_tree(html)), expected)

if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = webmining
from ebdata.templatemaker.webmining import mine_page
import unittest

class MinePageTestCase(unittest.TestCase):
    def assertMines(self, html, others, expected):
        """
        Asserts that the given HTML strings will produce a tree_diff of the
        expected HTML string.
        """
        got = mine_page(html, others)
        self.assertEqual(got, expected)

    def test_basic(self):
        self.assertMines(
            '<h1>Bird flies</h1>',
            ['<h1>Man walks</h1>'],
            ['Bird flies']
        )

    def test_convert_newlines(self):
        self.assertMines(
            '<p>The person\nfell down the stairs.</p>',
            ['<p>Foo</p>'],
            ['<p>The person fell down the stairs.</p>']
        )

    def test_convert_tabs(self):
        self.assertMines(
            '<p>The person\tfell down the stairs.</p>',
            ['<p>Foo</p>'],
            ['<p>The person fell down the stairs.</p>']
        )

    def test_convert_nbsp1(self):
        self.assertMines(
            '<p>The person&nbsp;fell down the stairs.</p>',
            ['<p>Foo</p>'],
            ['<p>The person fell down the stairs.</p>']
        )

    def test_convert_nbsp2(self):
        self.assertMines(
            '<p>The person&#160;fell down the stairs.</p>',
            ['<p>Foo</p>'],
            ['<p>The person fell down the stairs.</p>']
        )

    def test_drop_nonalpha_lines1(self):
        self.assertMines(
            '<h1>-</h1>',
            ['<h1>??</h1>'],
            []
        )

    def test_drop_nonalpha_lines2(self):
        self.assertMines(
            '<h1>1</h1>',
            ['<h1>-</h1>'],
            ['1']
        )

    def test_drop_nonalpha_lines3(self):
        self.assertMines(
            '<h1><br>-</h1>',
            ['<h1><br>??</h1>'],
            []
        )

if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = textlist
from ebdata.retrieval.utils import convert_entities
from lxml import etree
import re

def html_to_paragraph_list(tree):
    """
    Given an HTML tree, removes HTML tags and returns a list of strings, with
    each string representing a paragraph/block.
    """
    block_tags = set(['blockquote', 'dd', 'div', 'dt', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'h7', 'h8', 'li', 'p', 'td', 'th', 'tr'])
    drop_tags_only = set(['a', 'abbr', 'acronym', 'b', 'center', 'dir', 'dl', 'em', 'font', 'form', 'hr', 'i', 'label', 'menu', 'ol', 'pre', 'small', 'span', 'strong', 'sub', 'sup', 'table', 'tbody', 'tfoot', 'thead', 'topic', 'u', 'ul', 'wbr'])
    drop_tags_and_contents = set(['applet', 'area', 'button', 'embed', 'img', 'iframe', 'head', 'input', 'link', 'map', 'meta', 'noscript', 'object', 'option', 'script', 'select', 'spacer', 'style', 'textarea', 'title'])

    elements_to_drop = []
    for element in tree.getiterator():
        if not isinstance(element.tag, basestring): # If it's a comment...
            element.drop_tag()
            continue
        if element.text and '\n' in element.text:
            element.text = element.text.replace('\n', ' ')
        if element.tail and '\n' in element.tail:
            element.tail = element.tail.replace('\n', ' ')
        if element.tag in block_tags:
            element.text = '\n' + (element.text or '')
            element.tail = '\n' + (element.tail or '')
            element.drop_tag()
        elif element.tag == 'br':
            element.tail = '\n' + (element.tail or '')
            element.drop_tag()
        elif element.tag in drop_tags_only:
            element.drop_tag()
        elif element.tag in drop_tags_and_contents:
            elements_to_drop.append(element)
        elif element.tag not in ('html', 'body'): # Unknown tag!
            element.drop_tag()
    for e in elements_to_drop:
        e.drop_tree()

    try:
        tree.body
    except IndexError:
        return ''
    else:
        new_html = etree.tostring(tree.body, method='html')[6:-7] # strip <body> and </body>
        new_html = convert_entities(new_html)
        return re.split(r'\s*\n+\s*', new_html.strip())

########NEW FILE########
__FILENAME__ = webmining
from ebdata.templatemaker.htmlutils import remove_empty_tags, brs_to_paragraphs
from ebdata.templatemaker.sst import extract
from ebdata.textmining.treeutils import make_tree_and_preprocess, preprocess
from lxml import etree
import re

def mine_page(html, other_pages):
    result = []
    for hole in extract(html, other_pages):
        # Differences in attribute values aren't relevant.
        if hole['type'] == 'attrib' or not hole['value'] or not hole['value'].strip():
            continue

        # # Differences in links are likely navigation, and can be ignored.
        # if hole['type'] == 'text' and hole['tag'] == 'a':
        #     continue

        # If it's a multitag value, clean its HTML a bit.
        if hole['type'] == 'multitag':
            tree = make_tree_and_preprocess(hole['value'])

            # Drop a bunch of tags that can muck up the display.
            tree = preprocess(tree,
                drop_tags=('a', 'area', 'b', 'center', 'font', 'form', 'img', 'input', 'map', 'small', 'sub', 'sup', 'topic'),
                drop_trees=('applet', 'button', 'embed', 'iframe', 'object', 'select', 'textarea'),
                drop_attrs=('background', 'border', 'cellpadding', 'cellspacing', 'class', 'clear', 'id', 'rel', 'style', 'target'))

            remove_empty_tags(tree, ('br',))
            tree = brs_to_paragraphs(tree)

            # The [6:-7] cuts off the '<body>' and '</body>'.
            try:
                body = tree.body
            except IndexError:
                continue # lxml raises an IndexError if there's no <body>.

            # Skip bits that don't have at least one letter or number.
            # Note: If this code is ever internationalized, this will have to be
            # removed.
            if not re.search('[A-Za-z0-9]', body.text_content()):
                continue

            string = etree.tostring(body, method='html')[6:-7]
        else:
            string = hole['value']

            # Skip bits that don't have at least one letter or number.
            # Note: If this code is ever internationalized, this will have to be
            # removed.
            if not re.search('[A-Za-z0-9]', string):
                continue

        # Clean up newlines, tabs and &nbsp;.
        string = re.sub('[\n\t]', ' ', string.strip())
        string = string.replace('&nbsp;', ' ')
        string = string.replace('&#160;', ' ')

        result.append(string)
    return result

########NEW FILE########
__FILENAME__ = treeutils
"""
Unit tests for ebdata/textmining/treeutils.py
"""

from ebdata.textmining.treeutils import make_tree, preprocess
from lxml import etree
import unittest

class MakeTreeTestCase(unittest.TestCase):
    def assertMakeTree(self, html, expected):
        got = etree.tostring(make_tree(html), method='html')
        self.assertEqual(got, expected)

    def test_basic1(self):
        self.assertMakeTree('<html><body><h1>Hello</h1></body></html>', '<html><body><h1>Hello</h1></body></html>')

    def test_lxml_magic_behavior(self):
        # lxml sometimes reorders elements, depending on whether they're
        # block/inline.
        self.assertMakeTree('<html><body><h1><p><span>Hello</span></p></h1></body></html>',
            '<html><body><h1></h1><p><span>Hello</span></p></body></html>')

    def test_empty1(self):
        self.assertMakeTree('<html></html>', '<html></html>')

    def test_empty2(self):
        self.assertMakeTree('<html><body></body></html>', '<html><body></body></html>')

    def test_newlines(self):
        self.assertMakeTree('<html><body>\r\n\r\n\r\n</body></html>', '<html><body>\n\n\n</body></html>')

    def test_unicode_xml_declaration(self):
        self.assertMakeTree(u'<?xml version="1.0" encoding="utf-8"?><html><body><h1>Hello</h1></body></html>', '<html><body><h1>Hello</h1></body></html>')

class PreprocessTestCase(unittest.TestCase):
    def assertPreprocesses(self, html, expected, **kwargs):
        tree = make_tree(html)
        got = etree.tostring(preprocess(tree, **kwargs), method='html')
        self.assertEqual(got, expected)

    def test_comment1(self):
        self.assertPreprocesses('<html><body><!-- comment --></body></html>', '<html><body></body></html>')

    def test_comment2(self):
        self.assertPreprocesses('<html><body> <!--\n comment\n --> </body></html>', '<html><body>  </body></html>')

    def test_comment3(self):
        self.assertPreprocesses('<html><body> <!-- <p>Test</p> --> </body></html>', '<html><body>  </body></html>')

    def test_comment4(self):
        self.assertPreprocesses('<html><body> <!-- <p>Test</p> --> <!-- <p>Again</p> --> </body></html>', '<html><body>   </body></html>')

    def test_style1(self):
        self.assertPreprocesses('<html><head><style type="text/css">\n#foo { font: 11px verdana,sans-serif; }\n</style></head><body>Hi</body></html>', '<html><head></head><body>Hi</body></html>')

    def test_link1(self):
        self.assertPreprocesses('<html><head><link rel="stylesheet" src="/style.css"></head><body>Hi</body></html>', '<html><head></head><body>Hi</body></html>')

    def test_meta1(self):
        self.assertPreprocesses('<html><head><meta name="robots" content="noarchive"></head><body>Hi</body></html>', '<html><head></head><body>Hi</body></html>')

    def test_script1(self):
        self.assertPreprocesses('<html><head><script type="text/javascript">alert("hello");</script></head><body>Hi</body></html>', '<html><head></head><body>Hi</body></html>')

    def test_noscript1(self):
        self.assertPreprocesses('<html><body>Hi <noscript>You have no JavaScript</noscript> </body></html>', '<html><body>Hi  </body></html>')

    def test_droptags1(self):
        self.assertPreprocesses('<html><body><h1>Hello</h1></body></html>', '<html><body><h1>Hello</h1></body></html>')
        self.assertPreprocesses('<html><body><h1>Hello</h1></body></html>', '<html><body>Hello</body></html>', drop_tags=('h1',))

    def test_droptags2(self):
        self.assertPreprocesses('<html><body><div><p>Hello</p></div></body></html>', '<html><body><div><p>Hello</p></div></body></html>')
        self.assertPreprocesses('<html><body><div><p>Hello</p></div></body></html>', '<html><body><p>Hello</p></body></html>', drop_tags=('div',))
        self.assertPreprocesses('<html><body><div><p>Hello</p></div></body></html>', '<html><body><div>Hello</div></body></html>', drop_tags=('p',))

    def test_droptrees1(self):
        self.assertPreprocesses('<html><body><h1>Hello</h1></body></html>', '<html><body><h1>Hello</h1></body></html>')
        self.assertPreprocesses('<html><body><h1>Hello</h1></body></html>', '<html><body></body></html>', drop_trees=('h1',))

    def test_droptrees3(self):
        self.assertPreprocesses('<html><body><div><p>Hello</p></div></body></html>', '<html><body><div><p>Hello</p></div></body></html>')
        self.assertPreprocesses('<html><body><div><p>Hello</p></div></body></html>', '<html><body></body></html>', drop_trees=('div',))

    def test_droptrees4(self):
        self.assertPreprocesses('<html><body><div><p><span>Hello</span></p></div></body></html>', '<html><body><div><p><span>Hello</span></p></div></body></html>')
        self.assertPreprocesses('<html><body><div><p><span>Hello</span></p></div></body></html>', '<html><body></body></html>', drop_trees=('div',))

    def test_dropattrs(self):
        self.assertPreprocesses('<html><body><div id="foo" class="bar">Hi</div></body></html>', '<html><body><div id="foo" class="bar">Hi</div></body></html>')
        self.assertPreprocesses('<html><body><div id="foo" class="bar">Hi</div></body></html>', '<html><body><div class="bar">Hi</div></body></html>', drop_attrs=('id',))

    def test_drop_namespaced_attrs1(self):
        self.assertPreprocesses('<html><body><div dc:foo="foo">Hi</div></body></html>', '<html><body><div>Hi</div></body></html>')

    def test_drop_namespaced_attrs2(self):
        self.assertPreprocesses('<html><body><div dc:foo="foo" id="bar">Hi</div></body></html>', '<html><body><div id="bar">Hi</div></body></html>')

if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = treeutils
"""
Common utilities for creating and cleaning lxml HTML trees.
"""

from lxml.html import document_fromstring
import re

def make_tree(html):
    """
    Returns an lxml tree for the given HTML string (either Unicode or
    bytestring).

    This is better than lxml.html.document_fromstring because this takes care
    of a few known issues.
    """
    # Normalize newlines. Otherwise, "\r" gets converted to an HTML entity
    # by lxml.
    html = re.sub('\r\n', '\n', html)

    # Remove <?xml> declaration in Unicode objects, because it causes an error:
    # "ValueError: Unicode strings with encoding declaration are not supported."
    # Note that the error only occurs if the <?xml> tag has an "encoding"
    # attribute, but we remove it in all cases, as there's no downside to
    # removing it.
    if isinstance(html, unicode):
        html = re.sub(r'^\s*<\?xml\s+.*?\?>', '', html)

    return document_fromstring(html)

def make_tree_and_preprocess(html):
    """
    Returns an lxml tree for the given HTML string (either Unicode or
    bytestring). Also preprocesses the HTML to remove data that isn't relevant
    to text mining (see the docstring for preprocess()).
    """
    tree = make_tree(html)
    return preprocess(tree)

def preprocess(tree, drop_tags=(), drop_trees=(), drop_attrs=()):
    """
    Preprocesses a HTML etree to remove data that isn't relevant to text mining.
    The tree is edited in place, but it's also the return value, for
    convenience.

    Specifically, this does the following:
        * Removes all comments and their contents.
        * Removes these tags and their contents:
            <style>, <link>, <meta>, <script>, <noscript>, plus all of drop_trees.
        * For all tags in drop_tags, removes the tags (but keeps the contents).
        * Removes all namespaced attributes in all elements.
    """
    tags_to_drop = set(drop_tags)
    trees_to_drop = set(['style', 'link', 'meta', 'script', 'noscript'])
    for tag in drop_trees:
        trees_to_drop.add(tag)

    elements_to_drop = []
    for element in tree.getiterator():
        if element.tag in tags_to_drop or not isinstance(element.tag, basestring): # If it's a comment...
            element.drop_tag()
        elif element.tag in trees_to_drop:
            elements_to_drop.append(element)
        for attname in element.attrib.keys():
            if ':' in attname or attname in drop_attrs:
                del element.attrib[attname]
    for e in elements_to_drop:
        e.drop_tree()
    return tree

########NEW FILE########
__FILENAME__ = daemon
# The following is from http://www.jejik.com/articles/2007/02/a_simple_unix_linux_daemon_in_python/
# with slight formatting changes. The author of the code there has placed it in the public domain.

import atexit
import sys
import os
import time
from signal import SIGTERM
from optparse import OptionParser

class Daemon(object):
    """
    A generic daemon class.

    Usage: subclass the Daemon class and override the run() method
    """
    def __init__(self, pidfile, stdin='/dev/null', stdout='/dev/null', stderr='/dev/null'):
        self.stdin = stdin
        self.stdout = stdout
        self.stderr = stderr
        self.pidfile = pidfile

    def run_from_command_line(self, argv):
        """
        Given sys.argv, parses the command-line arguments and calls the
        appropriate method, failing appropriately in case of problems.
        """
        usage = "usage: %prog [options] start|stop|restart"
        parser = OptionParser(usage=usage)
        parser.add_option("-D", "--debug", help="run in debugging mode (run in the foreground)",
                          action="store_true", dest="debugging", default=False)
        (options, args) = parser.parse_args(argv)
        if len(args) == 1:
            if args[0] == 'start':
                self.start(options.debugging)
            elif args[0] == 'stop':
                self.stop()
            elif args[0] == 'restart':
                self.restart()
            else:
                parser.error("unknown command")
                sys.exit(2)
            sys.exit(0)
        else:
            parser.error("invalid command")
            sys.exit(2)

    def daemonize(self):
        """
        Do the UNIX double-fork magic. See Stevens' "Advanced
        Programming in the UNIX Environment" for details (ISBN 0201563177)
        """
        try:
            pid = os.fork()
            if pid > 0:
                # Exit the first parent.
                sys.exit(0)
        except OSError, e:
            sys.stderr.write("fork #1 failed: %d (%s)\n" % (e.errno, e.strerror))
            sys.exit(1)

        # Decouple from the parent environment.
        os.chdir("/")
        os.setsid()
        os.umask(0)

        # Do the second fork.
        try:
            pid = os.fork()
            if pid > 0:
                # Exit from the second parent.
                sys.exit(0)
        except OSError, e:
            sys.stderr.write("fork #2 failed: %d (%s)\n" % (e.errno, e.strerror))
            sys.exit(1)

        # Redirect standard file descriptors. Because the daemon has no
        # controlling terminal, we want to avoid side effects from reading and
        # writing to/from the standard file descriptors.
        sys.stdout.flush()
        sys.stderr.flush()
        si = file(self.stdin, 'r')
        so = file(self.stdout, 'a+')
        se = file(self.stderr, 'a+', 0)
        # Use os.dup2() with the fileno() instead of assigning directly to
        # sys.stdout because the former will also affect any C-level sys.stdout
        # calls.
        os.dup2(si.fileno(), sys.stdin.fileno())  # Essentially: sys.stdin = si
        os.dup2(so.fileno(), sys.stdout.fileno()) # Essentially: sys.stdout = so
        os.dup2(se.fileno(), sys.stderr.fileno()) # Essentially: sys.stderr = se

        # Write the pidfile.
        atexit.register(self.delpid)
        pid = str(os.getpid())
        open(self.pidfile, 'w+').write("%s\n" % pid)

    def get_pid_from_file(self):
        "Returns the pid value from the file."
        try:
            pf = open(self.pidfile, 'r')
            pid = int(pf.read().strip())
            pf.close()
        except IOError:
            pid = None
        return pid

    def delpid(self):
        os.remove(self.pidfile)

    def start(self, debug=False):
        """
        Starts the daemon.
        """
        # Check for a pidfile to see whether the daemon is already running.
        if self.get_pid_from_file():
            sys.stderr.write("pidfile %s already exists. Is the daemon already running?\n" % self.pidfile)
            sys.exit(1)

        # Don't detach from foreground if debugging.
        if not debug:
            self.daemonize()
        # Start the daemon.
        self.run()

    def stop(self):
        """
        Stops the daemon.
        """
        # Get the pid from the pidfile.
        pid = self.get_pid_from_file()

        if not pid:
            sys.stderr.write("pidfile %s does not exist. Is the daemon not running?\n" % self.pidfile)
            return # not an error in a restart

        # Try killing the daemon process.
        try:
            while 1:
                os.kill(pid, SIGTERM)
                time.sleep(0.1)
        except OSError, err:
            err = str(err)
            if err.find("No such process") > 0:
                os.remove(self.pidfile)
            else:
                print str(err)
                sys.exit(1)

    def restart(self):
        """
        Restarts the daemon.
        """
        self.stop()
        self.start()

    def run(self):
        """
        The daemon's logic.

        Subclasses should override this method.
        """
        raise NotImplementedError()

########NEW FILE########
__FILENAME__ = locator_calcs
#!/usr/bin/env python
import sys
from django.conf import settings
from ebgeo.maps.shortcuts import city_extent_in_map_srs
from ebgeo.maps.utils import extent_scale, center, extent_resolution, calculate_bounds
from ebgeo.maps.extent import transform_extent

LOCATOR_SIZE = (75, 75)

def get_citywide_bounds_for_size(city_slug, size=LOCATOR_SIZE):
    """
    Returns a 4-tuple extent in degrees decimal longitude and latitude
    the extent of a rectangle in (size[0], size[1]) pixels that is
    filled by the city.

    This is useful in generating locator map bounds because the extent
    of the city doesn't typically match the aspect ratio of the image.
    """
    extent = city_extent_in_map_srs(city_slug)
    resolution = extent_resolution(extent, size, settings.MAP_UNITS)
    new_extent = calculate_bounds(center(extent), resolution, size)
    # Convert back to lng/lat
    return transform_extent(new_extent, 4326, src_srs=900913)

def get_bounds_for_openlayers(city_slug, size=LOCATOR_SIZE):
    extent = get_citywide_bounds_for_size(city_slug, size)
    return 'new OpenLayers.Bounds(%.5f, %.5f, %.5f, %.5f)' % extent

def calc_locator_scale(city_slug):
    return extent_scale(city_extent_in_map_srs(city_slug),
                        size=LOCATOR_SIZE,
                        units=settings.MAP_UNITS)

def main(argv=None):
    if argv is None:
        argv = sys.argv[1:]

    if len(argv) < 2:
        print >> sys.stderr, 'Usage: %s {bounds|scale} city_slug [city_slug ...]' % sys.argv[0]
        return 1

    actions = {
        'bounds': get_bounds_for_openlayers,
        'scale': calc_locator_scale
    }

    for slug in argv[1:]:
        print slug, actions[argv[0]](slug)

if __name__ == '__main__':
    sys.exit(main())

########NEW FILE########
__FILENAME__ = bins
"""
Coarse-graining data, AKA "binning".

We implement two methods of binning data, equal-size and equal-weight.
There are others, including zero-centered, mean-centered, and
median-centered.

Note that while we intend for the bins to be expose an interchangeable
interface---that is, each method of binning exposes the same interface
and the method of finding which bin a particular value is in is the
same---the semantics of determining a bin differs slightly by
implementation, and by intent.

The reason is that the high and low boundaries of equal-weight bins
break on the values of the actual data used to create the bins, and
therefore are always values found in the data, while the boundaries of
equal-size bins break on computed values determined by the initial data,
and therefore might not be found in the data. This has two implications::

    1. Adjacent equal-weight bins may be disjunct, that is, for example,
       the high value of a bin may not be equal to or very nearly equal
       to the low value of the next bin. This implies that only values
       found in the binned data should be used to determine a bin, and
       not an arbitrary value.
    2. You can determine a bin for an arbitrary value with equal-size
       bins, provided the value is between the lowest and highest
       boundary.
"""

from __future__ import division

class Bin(object):
    def __init__(self, min, max, data, last=False):
        # min and max may, for example in the case of equal-size, be
        # different than the min/max of the list of data values
        self.min = min
        self.max = max
        self.data = list(data)
        self.last = last

    def __contains__(self, x):
        if x in self.data or \
           (self.last and self.min <= x <= self.max) or \
           (not self.last and self.min <= x < self.max):
            return True
        return False

    def add(self, value):
        self.data.append(value)

    def __str__(self):
        return "(%s, %s)" % (self.min, self.max)

    def __repr__(self):
        return "<Bin %s>" % self.__str__()
    
class Bins(object):
    def __init__(self, values, n=4):
        self.n = n
        self.bins = []
        self.bin_data(values)

    def bin_data(self, values):
        raise NotImplementedError()

    def bin_value(self, value):
        for bin in self.bins:
            if value in bin:
                bin.add(value)

    def __len__(self):
        return len(self.bins)

    def which_bin(self, value):
        for i, bin in enumerate(self.bins):
            if value in bin:
                return i
        return None

    def __str__(self):
        return "[%s]" % ", ".join([str(b) for b in self.bins])

    def __repr__(self):
        return "<Bins %s>" % self.__str__()

class EqualSize(Bins):
    """
    Creates bins of equal interval between min and max.

    >>> values = [10, 13, 17, 32, 35, 40, 60, 64, 67]
    >>> bins = EqualSize(values, 3)
    >>> bins
    <Bins [(10.0, 29.0), (29.0, 48.0), (48.0, 67.0)]>
    >>> bins.which_bin(10.0)
    0
    >>> bins.which_bin(15)
    0
    >>> bins.which_bin(29.0)
    1
    >>> bins.which_bin(30)
    1
    >>> bins.which_bin(48.0)
    2
    >>> bins.which_bin(55)
    2
    >>> bins.which_bin(67.0)
    2
    >>> bins.which_bin(0)
    >>> bins.which_bin(67.1)
    """
    def bin_data(self, values):
        min_val = min(values)
        max_val = max(values)
        interval = (max_val - min_val) / self.n
        for i in xrange(self.n):
            last = i == self.n-1
            b1, b2 = (min_val + (interval * i)), (min_val + (interval * (i+1)))
            bin = Bin(b1, b2, [], last)
            self.bins.append(bin)
        for v in values:
            self.bin_value(v)

class EqualWeight(Bins):
    """
    Creates bins of roughly equal count of values.

    >>> values = [10, 13, 17, 32, 35, 40, 60, 64, 67]
    >>> bins = EqualWeight(values, 3)
    >>> bins
    <Bins [(10, 17), (32, 40), (60, 67)]>
    >>> bins.which_bin(10)
    0
    >>> bins.which_bin(15)
    0
    >>> bins.which_bin(17)
    0
    >>> bins.which_bin(32)
    1
    >>> bins.which_bin(36)
    1
    >>> bins.which_bin(40)
    1
    >>> bins.which_bin(60)
    2
    >>> bins.which_bin(63)
    2
    >>> bins.which_bin(67)
    2
    >>> bins.which_bin(0)
    >>> bins.which_bin(18)
    >>> bins.which_bin(41)
    >>> bins.which_bin(68)
    """
    def bin_data(self, values):
        values = sorted(list(values))
        num_vals = len(values)
        for i in xrange(self.n):
            lo, hi = int(i/self.n*num_vals), int((i+1)/self.n*num_vals-1)
            b1, b2 = values[lo], values[hi]
            last = i == self.n-1
            bin = Bin(b1, b2, values[lo:hi+1], last)
            self.bins.append(bin)

    def in_bin(self, bin, value):
        return bin[0] <= value <= bin[1]

if __name__ == "__main__":
    import doctest
    doctest.testmod()

########NEW FILE########
__FILENAME__ = cached_image
from django.http import HttpResponse
from django.core.cache import cache

def set_cache_raw(key, data, timeout=None):
    """
    Works around a Django bug that tries to typecast a regular
    byte string as a Unicode string.
    """
    cache.set(key, (data,), timeout)

def get_cache_raw(key):
    """
    Corresponding `get` function to the `raw_cache_set` workaround
    """
    data = cache.get(key)
    if data is not None:
        return data[0]

class CachedImageResponse(HttpResponse):
    def __init__(self, key, image_gen_fn, expiry_seconds=None, mimetype='image/png'):

        img_bytes = get_cache_raw(key)

        if img_bytes is None:
            img_bytes = image_gen_fn()
            set_cache_raw(key, img_bytes, expiry_seconds)
        
        super(CachedImageResponse, self).__init__(content=img_bytes,
                                                  mimetype=mimetype)

########NEW FILE########
__FILENAME__ = colors
class Color(object):
    def __init__(self, r, g, b):
        for x in [r, g, b]:
            if not isinstance(x, int):
                raise RuntimeError, 'expected int got %s (%s)' % (type(x), x)
        self.r = r
        self.g = g
        self.b = b

    @classmethod
    def from_hex(cls, color_hex):
        """
        Parses a hex color code, like "#cc00ff"

        >>> Color.from_hex('#cc6699')
        <Color r(204), g(102), b(153)>
        >>> Color.from_hex('33ddee')
        <Color r(51), g(221), b(238)>
        >>> Color.from_hex('#c69')
        <Color r(204), g(102), b(153)>
        >>> Color.from_hex('c69')
        <Color r(204), g(102), b(153)>
        """
        if color_hex.startswith('#'):
            color_hex = color_hex[1:]
        if len(color_hex) == 3:
            # Shortcut hex: 'c69' -> 'cc6699'
            color_hex = ''.join((color_hex[i] * 2 for i in xrange(3)))
        r, g, b = map(lambda x: int(x, 16),
                      (color_hex[i*2:(i*2)+2] for i in xrange(3)))
        return cls(r, g, b)

    def to_hex(self):
        """
        >>> Color(r=204, g=102, b=153).to_hex()
        'cc6699'
        >>> Color(r=0, g=0, b=255).to_hex()
        '0000ff'
        """
        return '%02x%02x%02x' % (self.r, self.g, self.b)

    def __str__(self):
        return self.to_hex()

    def __repr__(self):
        return '<Color r(%s), g(%s), b(%s)>' % (self.r, self.g, self.b)

class ColorSpread(object):
    """
    Generates intermediate color values between two endpoints.

    Moves linearly in the color space.

    >>> spread = ColorSpread(Color.from_hex('#ff3399'), Color.from_hex('#33ff99'), 2)
    >>> for color in spread:
    ...     str(color)
    'ff3399'
    '999999'
    '33ff99'
    """
    def __init__(self, start, end, steps):
        self.start = start
        self.end = end
        self.steps = steps

    def __iter__(self):
        for i in xrange(self.steps + 1):
            if i == self.steps:
                yield self.end
                break
            color_parts = {}
            for comp in ['r', 'g', 'b']:
                start = getattr(self.start, comp)
                delta = getattr(self.end, comp) - start
                inc = int(1.0 * delta / self.steps)
                color_parts[comp] = start + (i * inc)
            yield Color(**color_parts)

def color_spread(start_hex, end_hex, steps):
    for color in ColorSpread(Color.from_hex(start_hex), Color.from_hex(end_hex), steps):
        yield color

if __name__ == '__main__':
    import doctest
    doctest.testmod()

########NEW FILE########
__FILENAME__ = constants
TILE_SIZE = 256

########NEW FILE########
__FILENAME__ = extent
from django.conf import settings
from django.contrib.gis.geos import Point
from django.contrib.gis.gdal import OGRGeometry, Envelope
from ebgeo.maps.utils import get_resolution, px_from_lnglat, lnglat_from_px
from ebpub.metros.allmetros import METRO_LIST

def transform_extent(extent, dest_srs, src_srs=4326):
    """
    Transforms an extent -- given as 4-tuple (min x, min y, max x,
    max x) -- to a target SRS.

    The `dest_srs' can be a SRID, WKT, or Proj.4 string. It is passed
    directly to the GEOS `transform()' method.
    """
    # lower-left (min x, min y)
    ll = Point(extent[0], extent[1], srid=src_srs)
    # upper-right (max x, max y)
    ur = Point(extent[2], extent[3], srid=src_srs)

    ll.transform(dest_srs)
    ur.transform(dest_srs)

    return (ll.x, ll.y, ur.x, ur.y)

def city_from_extent(extent):
    city_extents = dict([(m['short_name'], m['extent']) for m in METRO_LIST])

    env = OGRGeometry(Envelope(*extent).wkt)

    matches = []
    for slug, city_ext in city_extents.iteritems():
        city_env = OGRGeometry(Envelope(*city_ext).wkt)
        if city_env.intersects(env):
            matches.append((slug, city_env))

    if len(matches) == 1:
        return matches[0][0]
    elif len(matches) > 1:
        # Crudely select the intersecting city with the most overlap
        # TODO: get rid of this
        current_best_slug, current_max_area = None, float('-inf')
        for slug, city_env in matches:
            intersection = city_env.intersection(env)
            area = intersection.area
            if area > current_max_area:
                current_max_area = area
                current_best_slug = slug
        return current_best_slug

    # If we didn't find a match with a city extent, start expanding the buffer
    # around the city extents until we match one
    for i in xrange(6):
        for slug, city_ext in city_extents.iteritems():
            extent = buffer_extent(city_ext, 1, num_tiles=i+1)
            city_env = OGRGeometry(Envelope(*extent).wkt)
            if env.intersects(city_env):
                return slug

def buffer_extent(extent, zoom_level, num_tiles=6, tile_size=256, units='degrees'):
    """
    Buffers an extent by the size of num_tiles at a particular zoom level.
    """
    scale = settings.MAP_SCALES[zoom_level]
    resolution = get_resolution(scale, units)
    ll_px = px_from_lnglat((extent[0], extent[1]), resolution)
    ur_px = px_from_lnglat((extent[2], extent[3]), resolution)
    pixel_buf = num_tiles * tile_size
    # Note that the (0, 0) point for the lnglat_from_px function is upper-left,
    # so /addition/ of the buffer to the y component moves it in the negative
    # direction, and vice versa
    ll_px = (ll_px[0] - pixel_buf, ll_px[1] + pixel_buf)
    ur_px = (ur_px[0] + pixel_buf, ur_px[1] - pixel_buf)
    return lnglat_from_px(ll_px, resolution) + lnglat_from_px(ur_px, resolution)

########NEW FILE########
__FILENAME__ = mapserver
import sys
import os.path
from cStringIO import StringIO
from mapnik import *
from django.conf import settings
import PIL.Image
from ebgeo.maps import bins
from ebgeo.maps.constants import TILE_SIZE

def xml_path(maptype):
    path = os.path.join(sys.prefix, 'mapnik', '%s.xml' % maptype)
    return path

def get_mapserver(maptype):
    return {
        'main': MainMap,
        'locator': LocatorMap,
        'thematic': ThematicMap,
        'homepage': HomepageMap
    }[maptype]

class MapServer(Map):
    """
    A simple wrapper class around Mapnik's Map that provides a little
    friendlier interface to setting up a basic map and for common
    tasks.
    """
    def __init__(self, proj4, width=None, height=None):
        width = width or TILE_SIZE
        height = height or TILE_SIZE
        super(MapServer, self).__init__(width, height, '+init=epsg:900913')
        load_map(self, xml_path(self.maptype))

    def zoom_to_bbox(self, minx, miny, maxx, maxy):
        """
        Zooms map to bounding box - convenience method
        """
        return self.zoom_to_box(Envelope(minx, miny, maxx, maxy))

    def render_image(self, mimetype='image/png'):
        """
        Renders the map as an Mapnik image
        """
        img = Image(self.width, self.height)
        render(self, img)
        return img

    def get_graphic(self, mapnik_img, mimetype='image/png'):
        """
        Returns the raw bytes of graphic in the target format (PNG, JPG, GIF,
        etc.)
        """
        img = PIL.Image.fromstring('RGBA', (self.width, self.height), mapnik_img.tostring())
        buf = StringIO()
        if mimetype.find('/') != -1:
            format = mimetype.split('/')[1]
        else:
            format = mimetype
        img.save(buf, format)
        try:
            return buf.getvalue()
        finally:
            buf.close()

    def export_pdf(self, filename):
        """
        Renders map as a PDF, exporting to file given.
        """
        import cairo
        surface = cairo.PDFSurface(filename, self.width, self.height)
        render(self, surface)

    def create_layer(self, layer_name, style_name, postgis_table):
        """
        Convenience shortcut method for setting up a new layer with
        a defined style and PostGIS table name.
        """
        layer = Layer(layer_name)
        layer.datasource = PostGIS(host=settings.MAPS_POSTGIS_HOST, user=settings.MAPS_POSTGIS_USER, password=settings.MAPS_POSTGIS_PASS, dbname=settings.MAPS_POSTGIS_DB, table=postgis_table)
        layer.styles.append(style_name)
        return layer 

    def add_layer(self, layer_name, style_name, postgis_table, skip_if_missing=True):
        layer = self.create_layer(layer_name, style_name, postgis_table)
        self.layers.append(layer)

    def draw_map(self):
        raise NotImplementedError('subclasses must implement draw_map() method')

    def __call__(self, mimetype='image/png'):
        self.draw_map()
        img = self.render_image()
        return self.get_graphic(img, mimetype)

class MainMap(MapServer):
    maptype = 'main'

    def draw_map(self):
        self.add_layer('coastline', 'coastline', 'coastlines')
        self.add_layer('city', 'city-fill', 'cities')
        self.add_layer('major-water', 'water', 'water')
        self.add_layer('landmarks', 'landmarks', 'landmarks')
        self.add_layer('airports', 'airports', 'airports')
        self.add_layer('parks', 'parks', 'parks')

        # Streets
        streets = Layer('streets')
        streets.datasource = PostGIS(host=settings.MAPS_POSTGIS_HOST, user=settings.MAPS_POSTGIS_USER, password=settings.MAPS_POSTGIS_PASS, dbname=settings.MAPS_POSTGIS_DB, table='streets')
        # Add street styles -- order matters
        for style in [
            'road-fill',
            'arterial-fill',
            'highway-fill',
            'ramp-border',
            'ramp-fill',
            'interstate-border',
            'interstate-fill',
            'road-label',
            'arterial-label',
            'highway-label',
            'interstate-label'
        ]:
            streets.styles.append(style)
        self.layers.append(streets)

        self.add_layer('neighborhoods', 'neighborhoods', 'neighborhoods')
        self.add_layer('city-border', 'city-border', 'city')

class LocatorMap(MapServer):
    maptype = 'locator'

    def draw_map(self):
        self.add_layer('city', 'city-fill', 'cities')

class HomepageMap(LocatorMap):
    maptype = 'homepage'

# TODO: Move this somewhere else.
BINNING_METHOD = bins.EqualSize

# TODO: Move this to a config file, maybe subclass from a generic ColorTheme class.
class GreenTheme:
    no_value = '#D9FCC3'
    range = ['#D9FCC3', '#A0E673', '#5ACC2D', '#22944E', '#13552D']
    border = '#C0CCC4'

class ThematicMap(MapServer):
    """
    Generates a cloropleth or "thematic" map for a LocationType.

    Data values are given as a dict, and keys are ids of the Location objects
    that comprise the LocationType.
    """
    maptype = 'thematic'

    def __init__(self, location_type, theme_data, key_field, colors=None, num_bins=5, **kwargs):
        super(ThematicMap, self).__init__(**kwargs)
        self.location_type = location_type
        self.theme_data = theme_data
        self.key_field = key_field
        self.colors = colors or GreenTheme
        num_bins = num_bins or len(self.colors.range)
        self.bins = BINNING_METHOD(theme_data.values(), num_bins)

    def draw_map(self):
        style = Style()
        # Add a default Rule for features that aren't in the values list
        default_rule = Rule()
        default_rule.symbols.append(PolygonSymbolizer(Color(self.colors.no_value)))
        default_rule.symbols.append(LineSymbolizer(Color(self.colors.border), 1.0))
        style.rules.append(default_rule)
        # TODO: Instead of one rule per object, compose a filter
        # expression for the objects with the same value; also, contend
        # with string v. numeric in the DBF
        for key, value in self.theme_data.iteritems():
            rule = Rule()
            # The Mapnik C++ signature requires strings, not Unicode
            filter_exp = "[%s] = '%s'" % (self.key_field, str(key))
            rule.filter = Filter(filter_exp)
            color = self.colors.range[self.bins.which_bin(value)]
            rule.symbols.append(PolygonSymbolizer(Color(color)))
            rule.symbols.append(LineSymbolizer(Color(self.colors.border), 1.0))
            style.rules.append(rule)
        self.append_style('theme', style)
        layer = Layer('theme')
        layer.datasource = LocationDatasource(self.location_type)
        layer.styles.append('theme')
        self.layers.append(layer)

def LocationDatasource(location_type):
    """
    Use ebpub.db.Location objects as a datasource for Mapnik layers.
    """
    table_sql = """\
        (SELECT * FROM db_location WHERE location_type_id = %s) AS db_location
    """.strip() % (location_type.id,)
    host = settings.DATABASE_HOST and settings.DATABASE_HOST or settings.MAPS_POSTGIS_HOST
    port = settings.DATABASE_PORT and settings.DATABASE_PORT or 5432
    return PostGIS(host=host,
                   port=port,
                   dbname=settings.DATABASE_NAME,
                   user=settings.DATABASE_USER,
                   password=settings.DATABASE_PASSWORD,
                   table=str(table_sql), # Mapnik can't handle any Unicode
                   estimate_extent=True)

########NEW FILE########
__FILENAME__ = markers
from PIL import Image
from aggdraw import Draw, Pen, Brush

def make_marker(radius, fill_color, stroke_color, stroke_width, opacity=1.0):
    """
    Creates a map marker and returns a PIL image.

    radius
        In pixels

    fill_color
        Any PIL-acceptable color representation, but standard hex
        string is best

    stroke_color
        See fill_color

    stroke_width
        In pixels

    opacity
        Float between 0.0 and 1.0
    """
    # Double all dimensions for drawing. We'll resize back to the original
    # radius for final output -- it makes for a higher-quality image, especially
    # around the edges
    radius, stroke_width = radius * 2, stroke_width * 2
    diameter = radius * 2
    im = Image.new('RGBA', (diameter, diameter))
    draw = Draw(im)
    # Move in from edges half the stroke width, so that the stroke is not
    # clipped.
    half_stroke_w = (stroke_width / 2 * 1.0) + 1
    min_x, min_y = half_stroke_w, half_stroke_w
    max_x = diameter - half_stroke_w
    max_y = max_x
    bbox = (min_x, min_y, max_x, max_y)
    # Translate opacity into aggdraw's reference (0-255)
    opacity = int(opacity * 255)
    draw.ellipse(bbox,
                 Pen(stroke_color, stroke_width, opacity),
                 Brush(fill_color, opacity))
    draw.flush()
    # The key here is to resize using the ANTIALIAS filter, which is very
    # high-quality
    im = im.resize((diameter / 2, diameter / 2), Image.ANTIALIAS)
    return im

########NEW FILE########
__FILENAME__ = projections
'''
Definitions and utilities for projection support.

TODO: GeoDjango provides much more robust tools for setting up projection
objects from EPSG codes, PROJ4 and WKT, etc. In the long run we should use that
intead.
'''

EPSG_PROJ4 = {
    4326: '+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs',
    # spherical mercator
    900913: '+proj=merc +a=6378137 +b=6378137 +lat_ts=0.0 +lon_0=0.0 ' \
            '+x_0=0.0 +y_0=0 +k=1.0 +units=m ' \
            '+nadgrids=@null +no_defs',
    # USA_Contiguous_Albers_Equal_Area_Conic
    # cf. http://spatialreference.org/ref/epsg/102003/
    102003: '+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=37.5 +lon_0=-96 +x_0=0 ' \
            '+y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs'
}

def epsg_to_proj4(code):
    if code.lower().startswith('epsg:'):
        code = code.split(':')[1]
    return EPSG_PROJ4[int(code)]

########NEW FILE########
__FILENAME__ = shortcuts
from django.conf import settings
from TileCache.Service import Service
from TileCache.Layer import Tile
from ebgeo.maps.extent import transform_extent, buffer_extent
from ebgeo.maps.tile import get_tile_coords
from ebgeo.maps.mapserver import get_mapserver
from ebgeo.maps.utils import extent_scale
from ebpub.metros.allmetros import get_metro, METRO_DICT
from django.contrib.gis.gdal import SpatialReference

def get_eb_layer(name):
    svc = Service.load(settings.TILECACHE_CONFIG)
    return svc.layers[name]

def render_tile(name, z, x, y, source_srs=None, dest_srs=None, bbox=None,
                scales=None, units=None, extension='png'):
    """
    A shortcut for rendering a map tile using the EveryBlock settings.

    Useful for views and for rendering scripts. Main config options can be
    overriden.
    """
    layer = get_eb_layer(name)
    if source_srs is not None:
        layer.source_srs = source_srs
    if dest_srs is not None:
        layer.dest_srs = dest_srs
    if bbox is not None:
        layer.set_bbox(bbox)
    if scales is not None:
        layer.set_resolutions(scales, units)
    layer.extension = extension
    tile = Tile(layer, x, y, z)
    return layer.renderTile(tile)

def get_citywide_mapserver(maptype, size=(75,75), extension=None):
    map_srs = SpatialReference(settings.SPATIAL_REF_SYS)
    mapserver = get_mapserver(maptype)(map_srs.proj4, width=size[0], height=size[1])
    return mapserver

def render_locator_map(city_slug, size=(75,75), extension='png'):
    map_srs = SpatialReference(settings.SPATIAL_REF_SYS)
    bbox = city_extent_in_map_srs(city_slug)
    mapserver = get_mapserver('locator')(map_srs.proj4, width=size[0], height=size[1])
    mapserver.zoom_to_bbox(*bbox)
    return mapserver(extension)

def get_locator_scale(city_slug, size=(75,75)):
    bbox = transform_extent(get_metro(city_slug)['extent'], settings.SPATIAL_REF_SYS)
    return extent_scale(bbox, size, settings.MAP_UNITS)

def get_all_tile_coords(layer, cities=None, levels=(0, 5)):
    """
    A shortcut for getting all the tile coordinates for a layer.

    Can be optionally constrained by city or a list of cities (slugs).
    """
    if isinstance(layer, basestring):
        layer = get_eb_layer(layer)

    if cities is None:
        cities = METRO_DICT.keys()
    elif isinstance(cities, basestring):
        cities = [cities]

    for slug in cities:
        bboxes = []
        city_ext = transform_extent(get_metro(slug)['extent'], layer.dest_srs)
        for level in xrange(*levels):
            bboxes.append(buffer_extent(city_ext, level, units=settings.MAP_UNITS))
        for tile_coords in get_tile_coords(layer, levels=levels, bboxes=bboxes):
            yield tile_coords

def extent_in_map_srs(extent):
    """
    Returns an extent assumed to be in lng/lat in the target map SRS
    """
    return transform_extent(extent, settings.SPATIAL_REF_SYS)

def city_extent_in_map_srs(city_slug):
    return extent_in_map_srs(get_metro(city_slug)['extent'])


########NEW FILE########
__FILENAME__ = tess
from __future__ import division
import math
import os.path
from django.conf import settings
from django.contrib.gis.geos import Point
from django.contrib.gis.gdal import DataSource
from ebgeo.maps.extent import transform_extent
from ebgeo.utils.geodjango import reduce_layer_geom
from ebpub.metros.allmetros import get_metro

def tessellate(extent, radius):
    """
    Computes a tessellation of a given extent, with each tile covering
    a area inscribed by a circle with radius given. Returns an
    iterator which yields the center (x, y) coordinate.

    The algorithm covers the extent with squares with edges of size
    `radius`, then horizontally shifts every other row. The squares
    then have their top and bottom edges resized and shifted so the
    polygon becomes a regular hexagon.
    """
    # Hexagon math characteristics -- note that we use a vertically
    # oriented hexagon (standing on a vertex instead of an edge).
    height = 2 * radius
    width = math.sqrt(3) * radius
    h = math.sin(math.radians(30)) * radius # `h` is the sin(30) height
    r = width / 2 # `r` is the cos(30) width
    y_offset = -h

    extent_h = abs(extent[3] - extent[1])
    extent_w = abs(extent[2] - extent[0])

    # figure out number of rows
    n_rows = int(math.ceil(extent_h / height))

    # add rows to cover the gap created by the offset
    while (n_rows * (h + radius)) + y_offset < extent_h:
        n_rows += 1

    for row in xrange(n_rows):
        n_cols = int(math.ceil(extent_w / width))
        # if odd, offset x
        if row % 2 != 0:
            x_offset = -(width / 2)
            # add a col to cover the gap created by the offset
            if (n_cols * width) + x_offset < extent_w:
                n_cols += 1
        else:
            x_offset = 0
        for col in xrange(n_cols):
            # figure the centroid (x, y) of the hexagon
            x = ((col + 0.5) * width) + x_offset + extent[0]
            y = (row * (h + radius)) + (height / 2) + y_offset + extent[1]
            yield (x, y)

def cover_region(extent, radius):
    """
    Returns an iterator that covers a given region with circle buffer of given
    radius. `extent` should be given in terms of lat/lng, and `radius` should be
    in kilometers.

    The iterator yields (lng, lat) tuples -- the centroid of the buffer.
    """
    target_srs = 900913 # Spherical Mercator

    # Convert radius from km to meters
    radius_m = radius * 1000

    # Convert extent from lat/lng to a projected extent
    proj_ext = transform_extent(extent, target_srs)

    for (x, y) in tessellate(proj_ext, radius_m):
        pt = Point(x, y, srid=target_srs)
        pt.transform(4326)
        yield (pt.x, pt.y)

def cover_city(city_slug, radius):
    """
    An iterator that yields a centroid (lng, lat) for a circle buffer with
    radius given. The total buffers completely cover the city and no buffers
    that do not intersect with the city boundary are included.

    Radius is in kilometers.
    """
    def shapefile_path(city):
        return os.path.normpath(os.path.join(settings.SHAPEFILE_ROOT, city, 'city_4326'))

    ds = DataSource(shapefile_path(city_slug) + '.shp')
    city_geom = reduce_layer_geom(ds[0], 'union')
    city_geom.srid = 4326
    city_geom.transform(900913)

    for (x, y) in cover_region(get_metro(city_slug)['extent'], radius):
        pt = Point(x, y, srid=4326)
        pt.transform(900913)
        buf = pt.buffer(radius)
        if buf.intersects(city_geom.geos):
            yield (x, y)

def test_draw():
    import pylab
    from matplotlib.patches import RegularPolygon, Rectangle

    extent = (0, 0, 10, 10)
    radius = 1
    
    fig = pylab.figure()
    ax = fig.add_subplot(111)
    ax.add_artist(Rectangle((extent[0], extent[1]), extent[2] - extent[0],
                            extent[3] - extent[1]))

    for (x, y) in tessellate(extent, radius):
        ax.add_artist(RegularPolygon((x, y), 6, radius=radius,
                                     orientation=math.pi/2, alpha=0.5, facecolor='r'))

    pylab.show()

def test():
    print len(list(cover_region(get_metro('la')['extent'], 1.609)))

def test_cover_city():
    print len(list(cover_city('la', 1.609)))

if __name__ == '__main__':
    #test()
    test_cover_city()

########NEW FILE########
__FILENAME__ = tests
import unittest
from extent import transform_extent, city_from_extent
from tess import tessellate, cover_region, cover_city
from shortcuts import get_all_tile_coords, extent_in_map_srs, city_extent_in_map_srs, get_locator_scale

class ExtentTestCase(unittest.TestCase):
    def test_transform_extent(self):
        extent = (-87.9, 41.9, -87.8, 42.0)
        expected = (-9784983.241, 5146011.679, -9773851.292, 5160979.444)
        transformed = transform_extent(extent, 900913)
        for i in xrange(4):
            self.assertAlmostEqual(transformed[i], expected[i], places=3)

    def test_city_from_extent(self):
        extent = (-87.7433, 41.9243, -87.6927, 41.9511)
        self.assertEqual('chicago', city_from_extent(extent))

    def test_city_from_extent_non_unique_match(self):
        # Extent overlaps Philly and NYC, but moreso Philly
        extent = (-75.1087, 40.0048, -74.2267, 40.5173)
        self.assertEqual('philly', city_from_extent(extent))

    def test_city_from_extent_no_initial_overlap(self):
        # Extent to the south-west of San Jose
        extent = (-122.5669, 37.0204, -122.2787, 37.1912)
        self.assertEqual('sanjose', city_from_extent(extent))

class TessellateTestCase(unittest.TestCase):
    def _test_tessellation(self, fn, extent, radius, expected, places=3):
        actual = list(fn(extent, radius))
        for i, (x, y) in enumerate(actual):
            self.assertAlmostEqual(expected[i][0], x, places=places)
            self.assertAlmostEqual(expected[i][1], y, places=places)

    def test_tessellate(self):
        extent = (0, 0, 100, 100)
        radius = 30
        expected = [
            (25.980762113533157, 15.000000000000002),
            (77.942286340599466, 15.000000000000002),
            (0.0, 60.0),
            (51.961524227066306, 60.0),
            (103.92304845413261, 60.0),
            (25.980762113533157, 105.0),
            (77.942286340599466, 105.0)]
        self._test_tessellation(tessellate, extent, radius, expected)

    def test_cover_region(self):
        extent = (-87.7220, 41.9282, -87.7020, 41.9402)
        radius = 0.2 * 3 # in kilometers, 3 Chicago city blocks
        expected = [
            (-87.717332216860044, 41.930204961746526),
            (-87.707996650580171, 41.930204961746526),
            (-87.698661084300312, 41.930204961746526),
            (-87.721999999999966, 41.936219468889561),
            (-87.712664433720107, 41.936219468889561),
            (-87.703328867440248, 41.936219468889561),
            (-87.717332216860044, 41.942233408877819),
            (-87.707996650580171, 41.942233408877819),
            (-87.698661084300312, 41.942233408877819)]
        self._test_tessellation(cover_region, extent, radius, expected, places=5)

    def test_cover_city(self):
        expected = [
            (-87.551111071672324, 41.678109980749085),
            (-87.784500228668932, 41.778672990728793),
            (-87.628907457337846, 41.778672990728793),
            (-87.706703843003382, 41.879078553274987),
            (-87.784500228668932, 41.979326607041052)]
        self._test_tessellation(cover_city, 'chicago', 10, expected, places=5)

class ShortcutsTestCase(unittest.TestCase):
    def _compare_extents(self, expected, actual, places=3):
        for i, value in enumerate(actual):
            self.assertAlmostEqual(expected[i], value, places=places)
    
    def test_get_all_tile_coords(self):
        expected = [(68, 36, 0),
                    (73, 36, 0),
                    (78, 36, 0),
                    (68, 41, 0),
                    (73, 41, 0),
                    (78, 41, 0),
                    (68, 46, 0),
                    (73, 46, 0),
                    (78, 46, 0)]
        self.assertEqual(expected, list(get_all_tile_coords('main', 'chicago', levels=(0, 1))))

    def test_extent_in_map_srs(self):
        extent = (-87.721999999999994,
                  41.928199999999997,
                  -87.701999999999998,
                  41.940199999999997)

        expected = (-9765168.3713675346,
                    5150230.2126926854,
                    -9762941.9815516714,
                    5152025.8988631964)

        actual = extent_in_map_srs(extent)

        self._compare_extents(expected, actual)

    def test_city_extent_in_map_srs(self):
        expected = (-9789446.3730731141,
                    5107883.1729132505,
                    -9743141.6950437613,
                    5164430.3004190186)

        actual = city_extent_in_map_srs('chicago')

        self._compare_extents(expected, actual)

    def test_get_locator_scale(self):
        self.assertAlmostEqual(2137215,
                               get_locator_scale('chicago'),
                               places=0)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = tile
from TileCache.Layer import MetaLayer
from ebgeo.maps.mapserver import get_mapserver
from ebgeo.maps.utils import get_resolution
from ebgeo.maps.extent import transform_extent, city_from_extent

class EBLayer(MetaLayer):
    config_properties = [
        {'name': 'scales', 'description': 'Comma-delimited list of scales'},
        {'name': 'source_srs', 'description': 'Source spatial ref system ID (SRID)'},
        {'name': 'dest_srs', 'description': 'Destination, i.e., map\'s, spatial ref system ID (SRID)'},
    ] + MetaLayer.config_properties

    def __init__(self, name, source_srs, dest_srs, scales, **kwargs):
        MetaLayer.__init__(self, name, **kwargs)

        # Type-cast scales (if coming from tilecache config file)
        if isinstance(scales, basestring):
            scales = [float(s) for s in scales.split(',')]

        self.set_resolutions(scales)
        self.source_srs = source_srs
        self.dest_srs = dest_srs
        self.set_bbox(self.bbox)

    def set_resolutions(self, scales, units=None):
        if units is None:
            units = self.units
        self.resolutions = [get_resolution(s, units) for s in scales]

    def set_bbox(self, bbox):
        self.bbox = transform_extent(bbox, self.dest_srs)

    def renderTile(self, tile):
        """
        Overrides MetaLayer's renderTile method

        Returns the raw bytes of the rendered tile.
        """
        # The bbox will be in map projection units, not (necessarily) lat/lng
        tile_bbox = tile.bounds()

        width, height = tile.size()
        mapserver = get_mapserver(self.name)(self.dest_srs, width=width, height=height)
        mapserver.zoom_to_bbox(*tile_bbox)
        mimetype = 'image/%s' % self.extension
        # Calling the mapserver instance gives the raw bytestream
        # of the tile image
        tile.data = mapserver(mimetype)
        return tile.data

def get_tile_coords(layer, levels=(0, 5), bboxes=None):
    """
    A generator that yields tuples of tile grid coordinates.

    Yields a 3-tuple (x, y, z).

    Arguments:

        layer
            A TileCache.Layer (or subclass) instance

        levels
            A 2-tuple of the start and stop zoom levels

        bboxes
            A bounding box or list of bounding boxes to contrain tiles to.

            This should be in units of the target map projection
            (i.e., probably /not/ lat/lng)

            If a list is given, its length must match the number of levels being
            queried (determined by subtracting the stop (2nd element of the
            ``levels`` tuple) from the start (1st element))
    """
    if bboxes is None:
        bboxes = layer.bbox

    if isinstance(bboxes, list):
        if len(bboxes) != (levels[1] - levels[0]) or \
           False in [isinstance(x, tuple) for x in bboxes]:
            raise RuntimeError('list of bboxeses must match number of levels')
    else:
        # To match the semantics below, copy the bboxes for each level
        bboxes = [bboxes for _ in xrange(*levels)]

    for i, z in enumerate(xrange(*levels)):
        bbox = bboxes[i]
        bottomleft = layer.getClosestCell(z, bbox[0:2])
        topright = layer.getClosestCell(z, bbox[2:4])
        metaSize = layer.getMetaSize(z)
        for y in xrange(bottomleft[1], topright[1], metaSize[1]):
            for x in xrange(bottomleft[0], topright[0], metaSize[0]):
                yield (x, y, z)

########NEW FILE########
__FILENAME__ = tilecache_service
import re
import subprocess
import tempfile
from TileCache.Service import Service, Request, TileCacheException
from TileCache.Caches.Disk import Disk
import TileCache.Layer as Layer

request_pat = r'/(?P<version>\d{1,2}\.\d{1,3})/(?P<layername>[a-z]{1,64})/(?P<z>\d{1,10})/(?P<x>\d{1,10}),(?P<y>\d{1,10})\.(?P<extension>(?:png|jpg|gif))'
request_re = re.compile(request_pat)

class PostRenderingError(Exception):
    pass

class EBRequest(Request):
    def _parse_path(self, path):
        m = request_re.search(path)
        if not m:
            raise TileCacheException('unexpected request path format %r: should '
                                     'be of form /version/layername/z/x,y.ext' % path)
        else:
            return m.groups()

    def parse(self, fields, path, host):
        # /1.0/main/0/0,0.png -> /version/layername/z/x,y.ext
        version, layername, z, x, y, extension = self._parse_path(path)
        layer = self.getLayer(layername)
        return Layer.Tile(layer, int(x), int(y), int(z))

class EBService(Service):
    def dispatchRequest(self, params, path_info='/', req_method='GET',
                        host='http://example.com/'):
        tile = EBRequest(self).parse(params, path_info, host)
        if isinstance(tile, Layer.Tile):
            if req_method == 'DELETE':
                self.expireTile(tile)
                return ('text/plain', 'OK')
            else:
                return self.renderTile(tile)
        else:
            return (tile.format, tile.data)

class EBCache(Disk):
    def set(self, tile, data):
        data = optimize_png(data)
        Disk.set(self, tile, data)

def optimize_png(data):
    """
    Optimize a PNG's file size with optipng(1).
    """
    # Create a named temp file with the PNG data because optipng doesn't read
    # from stdin.
    temp_f = tempfile.NamedTemporaryFile(prefix='evb', suffix='.png')
    temp_f.file.write(data)
    temp_f.file.flush()
    try:
        rc = subprocess.call(['optipng', '-q', temp_f.name])
        if rc < 0:
            raise PostRenderingError('optipng was terminated by signal %s' % -rc)
        elif rc != 0:
            raise PostRenderingError('optipng returned non-zero %s' % rc)
    except OSError, e:
        PostRenderingError('optipng execution failed: %s' % e)
    opt_f = open(temp_f.name, 'r')
    try:
        return opt_f.read()
    finally:
        opt_f.close()
        temp_f.close()

########NEW FILE########
__FILENAME__ = urls
from django.conf.urls.defaults import *
from ebgeo.maps import views
from ebgeo.maps.tilecache_service import request_pat as tile_request_pat

urlpatterns = patterns('',
    (r'^tile%s' % tile_request_pat, views.get_tile),
    (r'^locator/(?P<version>\d+\.\d+)/(?P<city>\w{1,32})\.(?P<extension>(?:png|jpg|gif))$', views.locator_map),
    (r'^browser/export_pdf/$', views.export_pdf),
    (r'^marker_(?P<radius>\d{1,2})\.png$', views.get_marker),
)

########NEW FILE########
__FILENAME__ = utils
import math
from django.conf import settings

PAIR_SEP = "|"
KEY_VALUE_SEP = ":"

def encode_theme_data(data):
    """
    Encodes a dictionary suitable for passing theme data in a URI to the
    map server.

    Assumes URI encoding will be handled upstream.

    >>> d = {"logan-square": 54, "edgewater": 31, "the-loop": 44}
    >>> s = encode_theme_data(d)
    >>> s == 'the-loop:44|logan-square:54|edgewater:31'
    True
    """
    return PAIR_SEP.join(["%s%s%s" % (k, KEY_VALUE_SEP, v) for k, v in data.items()])

def decode_theme_data(s):
    """
    Decodes a string that's been encoding by encode_theme_data() and
    returns a dictionary.

    Assumes already URI-decoded.

    >>> d = decode_theme_data("the-loop:44|logan-square:54|edgewater:31")
    >>> d == {"logan-square": 54, "edgewater": 31, "the-loop": 44}
    True
    """
    pairs = [s.split(KEY_VALUE_SEP) for s in s.split(PAIR_SEP)]
    return dict([(k, float(v)) for k, v in pairs])

INCHES_PER_UNIT = {
    "inches": 1.0,
    "ft": 12.0,
    "mi": 63360.0,
    "m": 39.3701,
    "km": 39370.1,
    "dd": 4374754
}
INCHES_PER_UNIT["in"] = INCHES_PER_UNIT["inches"]
INCHES_PER_UNIT["degrees"] = INCHES_PER_UNIT["dd"]

DOTS_PER_INCH = 72

def normalize_scale(scale):
    """
    Ensures scale is in the 1/n representation.
    """
    return scale >= 1.0 and (1.0 / scale) or scale

def get_resolution(scale, units="degrees"):
    """
    Returns resolution from given scale and units.
    """
    return 1 / (normalize_scale(scale) * INCHES_PER_UNIT[units] * DOTS_PER_INCH)

def get_scale(resolution, units="degrees"):
    """
    Returns scale from given resolution and units.
    """
    return resolution * INCHES_PER_UNIT[units] * DOTS_PER_INCH

def px_from_lnglat(lnglat, resolution, extent=(-180, -90, 180, 90)):
    return (round(1/resolution * (lnglat[0] - extent[0])),
            round(1/resolution * (extent[3] - lnglat[1])))

def lnglat_from_px(px, resolution, extent=(-180, -90, 180, 90)):
    w = round(1/resolution * extent[2]) - round(1/resolution * extent[0])
    h = round(1/resolution * extent[3]) - round(1/resolution * extent[1])
    return ((px[0] - w / 2) * resolution,
            -(px[1] - h / 2) * resolution)

def km_per_lng_at_lat(lat):
    return 111.321 * math.cos(math.radians(lat))

def km_per_lat():
    return 111.0

def lng_per_km_at_lat(lat):
    return 1 / km_per_lng_at_lat(lat)

def lat_per_km():
    return 1 / km_per_lat()

def extent_resolution(extent, size, units='degrees'):
    width = extent[2] - extent[0]
    height = extent[3] - extent[1]
    return max(width / size[0], height / size[1])

def extent_scale(extent, size, units='degrees'):
    """
    Given an extent, return the scale at which it will fill the given
    size, a 2-tuple: (width, height)
    """
    resolution = extent_resolution(extent, size, units)
    return get_scale(resolution, units)

def get_scale_for_resolution(resolution, units='degrees'):
    resolutions = [get_resolution(s, units) for s in settings.MAP_SCALES]
    for i, res in enumerate(resolutions):
        if res < resolution:
            break
    i = max(0, i-1)
    return settings.MAP_SCALES[i]

def calculate_bounds(center, resolution, size):
    w_units = size[0] * resolution
    h_units = size[1] * resolution
    return (center[0] - w_units / 2,
            center[1] - h_units / 2,
            center[0] + w_units / 2,
            center[1] + h_units / 2)

def center(extent):
    return ((extent[2] - extent[0]) / 2 + extent[0],
            (extent[3] - extent[1]) / 2 + extent[1])

if __name__ == "__main__":
    import doctest
    doctest.testmod()

########NEW FILE########
__FILENAME__ = views
from cStringIO import StringIO
from django.conf import settings
from django.http import HttpResponse, Http404
from django.shortcuts import render_to_response
import mapnik
from ebgeo.maps.mapserver import get_mapserver
from ebgeo.maps.shortcuts import render_tile, render_locator_map
from ebgeo.maps.markers import make_marker
from ebgeo.maps.cached_image import CachedImageResponse

class TileResponse(object):
    def __init__(self, tile_bytes):
        self.tile_bytes = tile_bytes

    def __call__(self, extension='png'):
        if self.tile_bytes:
            return HttpResponse(self.tile_bytes, mimetype=('image/%s' % extension))
        else:
            raise Http404

def get_tile(request, version, layername, z, x, y, extension='png'):
    'Returns a map tile in the requested format'
    z, x, y = int(z), int(x), int(y)
    response = TileResponse(render_tile(layername, z, x, y, extension=extension))
    return response(extension)

def locator_map(request, version, city, extension='png'):
    'The 75x75 contextual locator map'
    response = TileResponse(render_locator_map(city))
    return response(extension)

def get_marker(request, radius):
    radius = int(radius)
    stroke_width = 1.0

    # Defaults
    fill_color = '#FF4600'
    stroke_color = '#C32700'
    opacity = 1.0

    if 'opacity' in request.GET:
        try:
            opacity = float(request.GET['opacity'])
        except ValueError:
            raise Http404
        else:
            if not (opacity >= 0.0 and opacity <= 1.0):
                raise Http404

    cache_key = 'marker-%s-%s-%s-%s-%s' % (radius, fill_color, stroke_color,
                                           stroke_width, opacity)
    def get_marker_bytes():
        img = make_marker(radius, fill_color, stroke_color, stroke_width, opacity)
        img_sio = StringIO()
        img.save(img_sio, 'PNG')
        return img_sio.getvalue()

    return CachedImageResponse(cache_key, get_marker_bytes)

########NEW FILE########
__FILENAME__ = bunch
class Bunch(object):
    """
    A bunch is a list of objects which knows its center point,
    determined as the average of its objects' points. It's a useful
    data structure for clustering.
    """
    __slots__ = ["objects", "center", "points"]

    def __init__(self, obj, point):
        self.objects = []
        self.points = []
        self.center = (0, 0)
        self.add_obj(obj, point)

    def add_obj(self, obj, point):
        self.objects.append(obj)
        self.points.append(point)
        self.update_center(point)

    def update_center(self, point):
        xs = [p[0] for p in self.points]
        ys = [p[1] for p in self.points]
        self.center = (sum(xs) * 1.0 / len(self.objects), sum(ys) * 1.0 / len(self.objects))

    def x(self):
        return self.center[0]
    x = property(x)

    def y(self):
        return self.center[1]
    y = property(y)
        
    def __repr__(self):
        objs = list.__repr__(self.objects[:3])
        if len(self.objects) > 3:
            objs = objs[:-1] + ", ...]"
        return u"<Bunch: %s, center: (%.3f, %.3f)>" % (objs, self.x, self.y)

########NEW FILE########
__FILENAME__ = cluster
"""
Map marker clustering

We need to know:

    + List of points (in lat/lng)

    + List of map resolutions

    + Size of buffer, and how to translate it into lat/lng
"""

import math
from bunch import Bunch # relative import

def euclidean_distance(a, b):
    """
    Calculates the Euclidean distance between two points.

    Assumes (x, y) pairs.
    """
    return math.hypot(a[0] - b[0], a[1] - b[1])

def buffer_cluster(objects, radius, dist_fn=euclidean_distance):
    """
    Clusters objects into bunches within a buffer by a given radius.

    Differs from k-means clustering in that the number of bunches is
    not known before the program is run or given as an argument: a
    "natural" number of bunches is returned, depending on whether a
    point falls within a buffer. The number of bunches is inversely
    proportional to the size of the buffer: the larger the buffer,
    the fewer number of bunches (but the larger the number of points
    contained in each bunch).

    Similar to k-means clustering in that it calculates a new center
    point for each bunch on each iteration, eventually arriving at
    a steady state.

    I'm just calling it 'buffer clustering': this may be called
    something else for real and there may be a better implementation,
    but I don't know better!

    ``objects`` is a dict with keys for ID some domain object, and 
    the values being 2-tuples representing their points on a 
    coordinate system.
    """
    bunches = []
    buffer = radius
    for key, point in objects.iteritems():
        bunched = False
        for bunch in bunches:
            if dist_fn(point, bunch.center) <= buffer:
                bunch.add_obj(key, point)
                bunched = True
                break
        if not bunched:
            bunches.append(Bunch(key, point))
    return bunches

########NEW FILE########
__FILENAME__ = json
from django.utils.simplejson import JSONEncoder
from bunch import Bunch # relative import

class ClusterJSON(JSONEncoder):
    def default(self, o):
        if isinstance(o, Bunch):
            return [o.objects, o.center]
        else:
            return JSONEncoder.default(self, o)

########NEW FILE########
__FILENAME__ = sample
sample_pts = {
    "547223": (-87.649755, 41.952132),
    "535915": (-87.654234, 41.943924),
    "506275": (-87.649563, 41.944182),
    "506273": (-87.653626, 41.943958),
    "381656": (-87.6493073829498, 41.9518546243512),
    "381655": (-87.6493280816326, 41.9506789387755),
    "381674": (-87.653336, 41.95436),
    "381673": (-87.653336, 41.95436),
    "381277": (-87.652891, 41.943069),
    "381276": (-87.652891, 41.943069),
    "381275": (-87.652891, 41.943069),
    "290247": (-87.653336, 41.95436),
    "540442": (-87.655889, 41.946749),
    "540441": (-87.655889, 41.946749),
    "535914": (-87.655889, 41.946749),
    "535913": (-87.655889, 41.946749),
    "522201": (-87.652891, 41.943069),
    "522200": (-87.652891, 41.943069),
    "522199": (-87.652891, 41.943069),
    "528481": (-87.655889, 41.946749),
    "528480": (-87.655889, 41.946749),
    "224057": (-87.6592931729398, 41.9431305762175),
    "381280": (-87.6572742854764, 41.9476417502965),
    "381279": (-87.6572742854764, 41.9476417502965),
    "381274": (-87.6505849680289, 41.94050887793),
    "369771": (-87.6555685046775, 41.9481200641759),
    "369770": (-87.6555685046775, 41.9481200641759),
    "369769": (-87.6555685046775, 41.9481200641759),
    "290241": (-87.6499393358209, 41.9529202856542),
    "381287": (-87.6603461831696, 41.9512279341246),
    "369773": (-87.6538410819671, 41.947521022968),
    "290088": (-87.6580857988953, 41.9496455042247),
    "227119": (-87.654063595462, 41.9507127602951),
    "210797": (-87.652894, 41.942568),
    "381278": (-87.653217, 41.942949),
    "252855": (-87.653626, 41.943958),
    "252959": (-87.6541082138935, 41.9540619759756),
    "252961": (-87.650398, 41.952847),
    "234434": (-87.6586411257973, 41.9407245242837),
    "211533": (-87.654234, 41.943924),
    "213523": (-87.6493073829498, 41.9518546243512),
    "210886": (-87.6553480058843, 41.946225401234),
    "210864": (-87.6553509907506, 41.9452477788242),
    "211816": (-87.655927, 41.948869),
    "210821": (-87.653217, 41.942949),
    "211339": (-87.6550617758789, 41.9428803524006),
    "211845": (-87.653305, 41.94815),
    "210804": (-87.652669, 41.942807),
    "214068": (-87.65474975, 41.95381475),
    "210764": (-87.6519398234194, 41.940949436066)
}

extent = (-87.87620574707032, 41.7274477421875, -87.49855071777344, 42.139435046875)

scale = 153600

size = (550, 600)

center = (-87.68737823242188, 41.93344139453125)

resolution = 0.0006866455078125

########NEW FILE########
__FILENAME__ = shortcuts
from ebgeo.utils import clustering

def cluster_newsitems(qs, radius=26):
    """
    A convenience function for clustering a newsitem queryset.
    """
    return clustering.cluster_scales(
        dict([(ni.id, (ni.location.centroid.x, ni.location.centroid.y))
              for ni in qs if ni.location]),
        radius)

########NEW FILE########
__FILENAME__ = tests
"""
Tests for developing map marker clustering module
"""

import random
from ebgeo.utils.clustering import cluster_by_scale, cluster_scales
from ebgeo.utils.clustering import cluster
from ebgeo.utils.clustering import sample
from ebgeo.utils.clustering import json

def gen_test_points(n=50, extent=(0,0,100,100), rand_seed=None):
    """
    Returns a list of n (x, y) pairs, distributed randomly throughout the extent.
    """
    if rand_seed:
        random.seed(rand_seed)
    return [(random.randint(extent[0], extent[2]), random.randint(extent[1], extent[3]))
            for i in xrange(n)]

def gen_test_objs(n=50, extent=(0,0,100,100), rand_seed=None):
    points = gen_test_points(n=n, extent=extent, rand_seed=rand_seed)
    return dict(zip(xrange(len(points)), points))

def print_bunches(bunches, *args, **kwargs):
    for i, bunch in enumerate(bunches):
        print "%3d: %d objects" % (i+1, len(bunch.objects))

def plot_bunches(bunches, buffer):
    import pylab
    from matplotlib.patches import Circle
    # Plot points
    pylab.plot([p[0] for p in points], [p[1] for p in points], 'r+')
    subplot = pylab.figure(1).axes[0]
    # Plot clusters
    for b in bunches:
        e = Circle((b.x, b.y), buffer, facecolor="green", alpha=0.4)
        subplot.add_artist(e)
    pylab.show()

def display(bunches, buffer, f=plot_bunches):
    f(bunches, buffer)

def timeit(label, f, *args, **kwargs):
    import time, sys
    start = time.time()
    ret_val = f(*args, **kwargs)
    print >> sys.stderr, "%s took %.4f seconds" % (label, time.time()-start)
    return ret_val

def randomize(L):
    import copy, random
    Lprime = copy.copy(L)
    random.shuffle(Lprime)
    return Lprime

def main():
    buffer = 20
    objs = gen_test_objs(rand_seed="foo")
    bunches = timeit("cluster", cluster.buffer_cluster, objs, buffer)
    display(bunches, buffer=buffer, f=print_bunches)

if __name__ == "__main__":
    #main()
    #for bunch in cluster_by_scale(sample.sample_pts, 51, 19200):
        #print "%3d: (%.4f, %.4f)" % (len(bunch.objects), bunch.x, bunch.y)
    from django.utils.simplejson import dumps
    print dumps(cluster_scales(sample.sample_pts, 26), cls=json.ClusterJSON)

########NEW FILE########
__FILENAME__ = correcting
class InvalidGeometry(Exception):
    pass

def correcting_layer(layer):
    """
    Generator for correcting invalid geometries of the layer's
    features. Yields 2-tuples (feature, geometry), where geometry is
    the corrected geometry. The original feature.geom is left
    preserved.

    Usage is simply to wrap an existing gdal.layer.Layer with this
    function and iterate over features.
    """
    for feature in layer:
        geom = feature.geom
        if not geom.geos.valid:
            # Note that the correction method is to buffer the
            # geometry with distance 0 -- this may not always work, so
            # check the resulting geometry before reassigning the
            # feature's geometry
            new_geom = geom.geos.buffer(0.0)
            if new_geom.valid:
                geom = new_geom.ogr
            else:
                raise InvalidGeometry()
        yield (feature, geom)

########NEW FILE########
__FILENAME__ = feature_reducer
from ebgeo.utils.geodjango import iterfeatures, getfield, itergeoms, linemerge
from collections import defaultdict
import ogr

DEBUG = True

class Reducer(object):
    """
    Reduces an OGR datasource by combining the geometries of features
    that are identified by common field values (``key_fields``)
    """
    def __init__(self, key_fields):
        """
        ``key_fields`` is a sequence consisting either or both of strings,
        which are names of the source datasource's fields, or 2-tuples,
        which consist of a field name for the destination datasource and a
        callable which takes a feature and a layer and returns the value
        of the field.
        """
        self.key_fields = key_fields
        # Create a list of destination field names; order is imporant
        # and the list comes in handy for creating the destinations'
        # fields.
        self.key_fieldnames = []
        for fieldname in self.key_fields:
            if isinstance(fieldname, tuple):
                fieldname = fieldname[0]
            self.key_fieldnames.append(fieldname)

    def _add_feature(self, layer, geom, fields):
        feature = ogr.Feature(feature_def=layer.GetLayerDefn())
        feature.SetGeometry(geom)
        for i, field in enumerate(fields):
            feature.SetField(i, field)
        layer.CreateFeature(feature)
        feature.Destroy()

    def reduce(self, src_layer, dst_layer):
        "Reduces a layer's features down to the destination layer"
        src_layer_defn = src_layer.GetLayerDefn()

        # This is the data structure which holds our new,
        # combined features
        reduced = defaultdict(list)

        count = 0
        for feature in iterfeatures(src_layer):
            count += 1
            # "Key fields" are a tuple of the values of the
            # fields of the feature which uniquely identify a
            # reduced-down feature. We start with a list to build
            # it initially from ``self.key_fields``.
            key_fields = []
            try:
                for fieldname in self.key_fields:
                    if isinstance(fieldname, basestring):
                        value = getfield(feature, fieldname)
                    elif isinstance(fieldname, tuple):
                        fieldname, func = fieldname
                        if not callable(func):
                            raise ValueError("2nd member of tuple must be a callable")
                        # Callable must take ``feature`` and ``layer``
                        # as positional args and return a string
                        value = func(feature, src_layer)
                    else:
                        raise ValueError("key_fields item must be the name of a field or a tuple")
                    if value is None:
                        value = ""
                    key_fields.append(value)
            except StopIteration:
                continue
            # Tuple-ize the key_fields so it can be a key in our
            # dictionary
            key_fields = tuple(key_fields)
            reduced[key_fields].append(feature.GetGeometryRef().Clone())
            feature.Destroy()
            if DEBUG and count % 100 == 0:
                print "Processed %s features" % count

        # Calculate width for each field
        widths = []
        reduced_keys = reduced.keys()
        for i in range(len(self.key_fieldnames)):
            widths.append(max([len(k[i]) for k in reduced_keys if k is not None]))

        # Create layer's fields
        for i, key_fieldname in enumerate(self.key_fieldnames):
            dst_fd = ogr.FieldDefn(key_fieldname, ogr.OFTString)
            dst_fd.SetWidth(widths[i])
            dst_layer.CreateField(dst_fd)

        # Create the new features
        for key_fields, geom_list in reduced.items():
            # Sew together adjacent LineStrings
            merged_geom = linemerge(geom_list)
            subgeom_count = merged_geom.GetGeometryCount()
            if DEBUG:
                print "Outputting %r with %s separate lines" % (key_fields[0], subgeom_count and subgeom_count or 1)
            if subgeom_count == 0:
                self._add_feature(dst_layer, merged_geom, key_fields)
            else:
                for geom in itergeoms(merged_geom):
                    self._add_feature(dst_layer, geom, key_fields)

########NEW FILE########
__FILENAME__ = geodjango
"""
Utility functions for working with GeoDjango GDAL and GEOS data
"""

from django.contrib.gis import geos
from django.contrib.gis.geos import Point, LineString, Polygon, GeometryCollection, MultiPoint, MultiLineString, MultiPolygon

def reduce_layer_geom(layer, method):
    """
    Iterates over all the geometries in an GDAL layer and successively
    applies given `method' to the geometries.
    """
    def reduction(x, y):
        return getattr(x, method)(y)
    
    return reduce(reduction, [feat.geom for feat in layer])

# TODO: remove this once line_merge is added to django.contrib.gis.geos
from django.contrib.gis.geos.libgeos import lgeos
from django.contrib.gis.geos.prototypes.topology import topology
geos_linemerge = topology(lgeos.GEOSLineMerge)
def line_merge(geom):
    return geom._topology(geos_linemerge(geom.ptr))

def flatten(geos_geom):
    """
    Flattens a GEOS geometry and returns a list of the component geometries.
    """
    def _flatten(geom, acc):
        if isinstance(geom, (Point, LineString, Polygon)):
            acc.append(geom)
            return acc
        elif isinstance(geom, (GeometryCollection, MultiPoint, MultiLineString, MultiPolygon)):
            subgeom_list = list(geom)
            if subgeom_list:
                acc.append(subgeom_list.pop(0))
                for subgeom in subgeom_list:
                    _flatten(subgeom, acc)
            return acc
        else:
            raise TypeError, 'not a recognized GEOSGeometry type'
    flattened = []
    _flatten(geos_geom, flattened)
    return flattened

def make_geomcoll(geom_list):
    """
    From a list of geometries, return a single GeometryCollection (or
    subclass) geometry.

    This flattens multi-point/linestring/polygon geometries in the
    list.
    """
    flattened = []

    for geom in geom_list:
        flattened.extend(flatten(geom))

    return GeometryCollection(flattened)

def make_multi(geom_list, collapse_single=False):
    if len(geom_list) == 1 and collapse_single:
        return geom_list[0]

    geom_types = set(g.geom_type for g in geom_list)

    if len(geom_types) > 1:
        raise ValueError, 'all geometries must be of the same geom_type'

    geom_type = geom_types.pop()

    valid_geom_types = ('Point', 'LineString', 'Polygon')

    if geom_type not in valid_geom_types:
        raise ValueError, 'geometries must be of type %s' % ', '.join(valid_geom_types)

    cls = getattr(geos, 'Multi%s' % geom_type)
    return cls(geom_list)

def smart_transform(geom, srid, clone=True):
    """
    Returns a new geometry transformed to the srid given. Assumes if
    the initial geom is lacking an SRS that it is EPSG 4326. (Hence the
    "smartness" of this function.) This fixes many silent bugs when
    transforming between SRSes when the geometry is missing this info.
    """
    if not geom.srs:
        geom.srid = 4326
    return geom.transform(srid, clone=clone)

########NEW FILE########
__FILENAME__ = progressbar
import sys
import time
import threading

lock = threading.RLock()

class ProgressBar(object):
    """
    12% [====                ]

    >>> pbar = ProgressBar(0, 99)
    >>> pbar(0)
    >>> pbar(1) # &c.
    """
    def __init__(self, min_val, max_val, width=40, stdout=None):
        self.min_val = min_val
        self.max_val = max_val
        self.width = width
        self.current = min_val
        self.char = '='
        if stdout is None:
            stdout = sys.stdout
        self.stdout = stdout

    def _percent_complete(self):
        return float(self.current - self.min_val) / (self.max_val - self.min_val)

    def bar(self):
        num_hash_marks = int(round(self.width * self._percent_complete()))
        bar = '[' + num_hash_marks * self.char + (self.width - num_hash_marks) * ' ' + ']'
        return '%3.f%% %s' % (self._percent_complete() * 100, bar)

    def __str__(self):
        return self.bar()

    def __call__(self, current):
        lock.acquire()
        try:
            self.current = current
            self.stdout.write('\r')
            self.stdout.write(str(self))
            self.stdout.flush()
        finally:
            lock.release()

class TimedProgressBar(ProgressBar):
    """
    ETA 00:06:28 12% [====                ]

    >>> pbar = TimedProgressBar(0, 99)
    >>> pbar.start()
    >>> pbar(0)
    >>> pbar(1) # &c.
    """
    def __init__(self, min_val, max_val, show_rate=True, width=40, stdout=None):
        ProgressBar.__init__(self, min_val, max_val, width, stdout)
        self.show_rate = True
        self.start_time = None
        self.last_time = None
        self.eta = (0, 0, 0) # (hours, minutes, seconds) est. remaining

    def start(self):
        self.start_time = self.last_time = time.time()

    def __str__(self):
        if self.show_rate:
            rate = ' (%.1f/sec)' % self.rate
        else:
            rate = ''
        return 'ETA %02d:%02d:%02d %s%s' % (self.eta[0], self.eta[1], self.eta[2],
                                            ProgressBar.__str__(self), rate)

    def __call__(self, current):
        if self.start_time is None:
            raise RuntimeError('must call .start()')
        lock.acquire()
        try:
            self.last_time = time.time()
            elapsed = self.last_time - self.start_time
            rate = (self.current - self.min_val) / elapsed
            self.rate = rate
            eta_seconds = (self.max_val - self.current) * rate
            hours = eta_seconds / 3600
            minutes = (eta_seconds % 3600) / 60
            seconds = eta_seconds % 60
            self.eta = (hours, minutes, seconds)
            ProgressBar.__call__(self, current)
        finally:
            lock.release()

if __name__ == '__main__':
    import random
    pb = TimedProgressBar(0, 99)
    pb.start()
    for i in xrange(100):
        time.sleep(1 + random.random()/2)
        pb(i)

########NEW FILE########
__FILENAME__ = shapeindex
"""
Functions for taking advantage of the quad-tree shapefile index produced by
Mapnik's bundled shapeindex utility, allowing for efficient selection of
features based on a given bounding box / extent.
"""

import os.path
import struct
from django.contrib.gis.gdal import Envelope, OGRGeometry

def read_shapeindex(f, extent):
    """
    Reads the binary index as written out by quadtree.hpp in
    mapnik/utils/shapeindex
    """
    # Read the header -- 16 bytes -- just a sanity check
    (name, zeropad) = struct.unpack('6s10s', f.read(16))
    assert name == 'mapnik' and zeropad == '\x00' * 10

    ids = []
    filter_env = OGRGeometry(Envelope(*extent).wkt)
    read_node(f, filter_env, ids)
    ids.sort()
    return ids

def read_node(f, filter_env, ids):
    """
    Follows query_node() in plugins/input/shape/shp_index.hpp
    """
    (offset,) = struct.unpack('i', f.read(4))
    envelope = OGRGeometry(Envelope(*struct.unpack('4d', f.read(32))).wkt)
    (shape_count,) = struct.unpack('i', f.read(4))
    if not envelope.intersects(filter_env):
        f.seek(offset + shape_count * 4 + 4, 1)
        return
    ids.extend(struct.unpack('%di' % shape_count, f.read(4 * shape_count)))
    (num_children,) = struct.unpack('i', f.read(4))
    for i in xrange(num_children):
        read_node(f, filter_env, ids)

def indexed_shapefile(ds, extent):
    shapefile = ds.name
    root, _ = os.path.splitext(shapefile)
    filename = root + '.index'
    ids = read_shapeindex(open(filename), extent)
    f = open(shapefile)
    layer = ds[0]
    for id in ids:
        f.seek(id)
        (record_number,) = struct.unpack('>i', f.read(4))
        yield layer[record_number - 1]

########NEW FILE########
__FILENAME__ = models
from django.db import models

class City(models.Model):
    city = models.CharField(max_length=64)
    state = models.CharField(max_length=2, blank=True)

    def __unicode__(self):
        return self.pretty_name

    def pretty_name(self):
        if self.state:
            return u'%s, %s' % (self.city, self.state)
        return self.city

    def url(self):
        return '/citypoll/%s/' % self.id

class Vote(models.Model):
    # The "normalized" version of the requested city.
    city = models.ForeignKey(City, blank=True, null=True)

    # The raw data that's submitted to us.
    city_text = models.CharField(max_length=64)
    email = models.CharField(max_length=128)
    notes = models.TextField()

    # Metadata about the submission.
    ip_address = models.CharField(max_length=225, blank=True) # Might be comma-separated list.
    date_received = models.DateTimeField()

    def __unicode__(self):
        return u'%s at %s' % (self.city_text, self.date_received)

########NEW FILE########
__FILENAME__ = normalize
from ebinternal.citypoll.models import Vote

def normalize_cities():
    total = 0
    for v in Vote.objects.distinct().filter(city__id__isnull=False).values('city_text', 'city'):
        total += Vote.objects.filter(city__id__isnull=True, city_text__iexact=v['city_text']).update(city=v['city'])
    return total

if __name__ == "__main__":
    print normalize_cities()

########NEW FILE########
__FILENAME__ = views
from django.db.models import Count
from django.http import Http404, HttpResponse
from django.shortcuts import render_to_response, get_object_or_404
from django.utils import simplejson as json
from ebinternal.citypoll.models import Vote, City
import datetime

def save_feedback(request):
    # Meant to be called as an Ajax request; no response.
    if request.META.get('CONTENT_TYPE') == 'application/json':
        data = json.loads(request.raw_post_data)
        city_text = data['city'].strip()
        email = data['email'].strip().lower()
    else:
        if not (request.POST.get('city') and request.POST.get('email')):
            raise Http404('City or email is missing')
        city_text = request.POST['city'].strip()
        email = request.POST['email'].strip().lower()

    ip_address = request.META.get('HTTP_X_FORWARDED_FOR', '') or request.META.get('REMOTE_ADDR', '')

    # Ignore duplicate feedbacks.
    try:
        v = Vote.objects.get(city_text=city_text, email=email)
    except Vote.DoesNotExist:
        v = Vote.objects.create(
            city=None,
            city_text=city_text,
            email=email,
            notes=request.POST.get('notes', '').strip(),
            ip_address=ip_address,
            date_received=datetime.datetime.now(),
        )
    return HttpResponse('')

def city_vote_list(request):
    c_list = City.objects.annotate(votes=Count('vote')).order_by('-votes')
    return render_to_response('citypoll/vote_list.html', {
        'city_list': c_list,
    })

def city_detail(request, city_id):
    c = get_object_or_404(City, id=city_id)
    return render_to_response('citypoll/city_detail.html', {
        'city': c,
        'vote_list': c.vote_set.order_by('-date_received'),
    })

########NEW FILE########
__FILENAME__ = models
from django.db import models

class Category(models.Model):
    name = models.CharField(max_length=64)

    def __unicode__(self):
        return self.name

class Feedback(models.Model):
    # Stuff that's added when the feedback is created.
    city = models.CharField(max_length=32, blank=True)
    page_url = models.CharField(max_length=255, blank=True)
    message = models.TextField()
    ip_address = models.CharField(max_length=225, blank=True) # Might be comma-separated list.
    email = models.CharField(max_length=255, blank=True)
    date_received = models.DateTimeField()

    # Stuff that's added by our staff.
    assigned_to = models.CharField(max_length=32, blank=True)
    date_responded = models.DateTimeField(blank=True, null=True)
    responder = models.CharField(max_length=32, blank=True)
    is_awesome = models.BooleanField()
    is_ignored = models.BooleanField()
    category = models.ForeignKey(Category)

    def __unicode__(self):
        return u'#%s: From %s' % (self.id, self.email or 'anonymous')

    def url(self):
        return '/feedback/%s/' % self.id

class Response(models.Model):
    feedback = models.ForeignKey(Feedback)
    date_sent = models.DateTimeField()
    to_email = models.CharField(max_length=255)
    from_email = models.CharField(max_length=255)
    message = models.TextField()

    def __unicode__(self):
        return unicode(self.id)

class CannedResponse(models.Model):
    name = models.CharField(max_length=128)
    message = models.TextField()

    def __unicode__(self):
        return self.name

########NEW FILE########
__FILENAME__ = views
from django import forms
from django.conf import settings
from django.core.mail import send_mail
from django.http import Http404, HttpResponse, HttpResponseRedirect
from django.shortcuts import get_object_or_404, render_to_response
from ebinternal.feedback.models import Category, Feedback, Response, CannedResponse
import datetime
import re

class CategoryForm(forms.Form):
    category = forms.ModelChoiceField(Category.objects.order_by('name'))

def save_feedback(request):
    # Meant to be called as an Ajax request; no response.
    if not request.POST.get('message', '').strip():
        raise Http404
    page_url = request.META.get('HTTP_REFERER', '').replace('\n', '').strip()
    m = re.search(r'^http://(.*?)\.', page_url)
    city = m and m.group(1) or ''
    ip_address = request.META.get('HTTP_X_FORWARDED_FOR', '') or request.META.get('REMOTE_ADDR', '')
    uncategorized, _ = Category.objects.get_or_create(name='Uncategorized')
    f = Feedback.objects.create(
        city=city,
        page_url=page_url,
        message=request.POST['message'],
        ip_address=ip_address[:255],
        email=request.POST.get('email', '').strip(),
        date_received=datetime.datetime.now(),
        assigned_to='',
        date_responded=None,
        responder='',
        is_awesome=False,
        is_ignored=False,
        category=uncategorized,
    )
    anon_note = f.email and 'S' or 'Anonymous s'
    subject = '%site feedback: %s %s' % (anon_note, f.city or 'Unknown city', f.page_url)
    message = u'Referrer: %s\nIP address: %s\nE-mail: %s\nReply here: %s\n\n%s' % \
        (f.page_url, f.ip_address, f.email, f.url(), f.message)
    send_mail(subject, message, settings.EB_FROM_EMAIL, [settings.EB_NOTIFICATION_EMAIL], fail_silently=True)
    return HttpResponse('')

def feedback_list(request):
    open_list = Feedback.objects.select_related().filter(date_responded__isnull=True).order_by('-date_received')
    closed_list = Feedback.objects.select_related().filter(date_responded__isnull=False).order_by('-date_received')[:30]
    return render_to_response('feedback/feedback_list.html', {
        'open_list': open_list,
        'closed_list': closed_list,
        'category_form': CategoryForm(auto_id=False),
    })

def feedback_detail(request, feedback_id):
    f = get_object_or_404(Feedback.objects.select_related(), id=feedback_id)
    remote_user = request.META.get('REMOTE_USER', '')
    if request.method == 'POST':
        if request.POST.get('claim', False):
            f.assigned_to = remote_user
            f.save()
            return HttpResponseRedirect(request.path)
        response_text = request.POST.get('response', '').strip()
        if response_text:
            f.date_responded = datetime.datetime.now()
            f.responder = remote_user
            if response_text:
                if not f.email.strip():
                    raise Http404('No e-mail address to send to')
                # Note that we send the message before creating the Response
                # object, in case the e-mail sending fails.
                subject = 'Response to your site feedback'
                message = u'%s\n\n\n---- You wrote: ----\n\n%s' % (response_text, f.message)
                send_mail(subject, message, settings.EB_FROM_EMAIL, [f.email], fail_silently=False)
                response = Response.objects.create(
                    feedback=f,
                    date_sent=f.date_responded,
                    to_email=f.email,
                    from_email=f.responder,
                    message=response_text,
                )
            f.save()
            return HttpResponseRedirect('../')
    staff_first_name, staff_full_name = settings.EB_STAFF[remote_user]
    return render_to_response('feedback/feedback_detail.html', {
        'feedback': f,
        'response_list': f.response_set.order_by('date_sent'),
        'canned_response_list': CannedResponse.objects.order_by('name'),
        'category_form': CategoryForm(auto_id=False),
        'staff_first_name': staff_first_name,
        'staff_full_name': staff_full_name,
    })

def feedback_ignore(request):
    f_id = request.POST.get('feedback_id')
    if not f_id.isdigit():
        raise Http404
    remote_user = request.META.get('REMOTE_USER', '')
    Feedback.objects.filter(id=f_id).update(date_responded=datetime.datetime.now(), responder=remote_user, is_ignored=True)
    return HttpResponse('')

def feedback_change_category(request):
    f_id = request.POST.get('feedback_id')
    c_id = request.POST.get('category_id')
    if not c_id.isdigit() or not f_id.isdigit():
        raise Http404
    Feedback.objects.filter(id=f_id).update(category=c_id)
    return HttpResponse('')

########NEW FILE########
__FILENAME__ = settings
import os

DEBUG = True

DATABASE_ENGINE = 'postgresql_psycopg2'
DATABASE_USER = ''
DATABASE_HOST = ''
DATABASE_NAME = 'internal'

INSTALLED_APPS = (
    'django.contrib.humanize',
    'ebinternal.citypoll',
    'ebinternal.feedback',
)

TEMPLATE_DIRS = (
    os.path.normpath(os.path.join(os.path.dirname(__file__), 'templates')),
)

ROOT_URLCONF = 'ebinternal.urls'

EB_MEDIA_ROOT = os.path.normpath(os.path.join(os.path.dirname(__file__), 'media'))
EB_MEDIA_URL = ''

# The "from" address of email sent from the feedback application.
EB_FROM_EMAIL = ''

# The email address where notification of new feedback should be sent.
EB_NOTIFICATION_EMAIL = ''

# A mapping from REMOTE_USERs to (name, full_name) tuples. It is assumed that
# the REMOTE_USER will be an email address.
EB_STAFF = {
    '': ('Staff', 'feedback@example.com'),
    # example
    #'john.doe@example.com': ('John', 'John Doe')
}

# This is used in mail signatures as well as for parsing out which page
# feedback came from.
EB_DOMAIN_NAME = 'example.com'
########NEW FILE########
__FILENAME__ = urls
from django.conf import settings
from django.conf.urls.defaults import *
from django.views.generic import simple
from ebinternal.citypoll import views as citypoll_views
from ebinternal.feedback import views as feedback_views
from everyblock.utils.redirecter import redirecter

if settings.DEBUG:
    urlpatterns = patterns('',
        (r'^(?P<path>(?:images|scripts|styles|openlayers).*)$', 'django.views.static.serve', {'document_root': settings.EB_MEDIA_ROOT}),
    )
else:
    urlpatterns = patterns('')

urlpatterns += patterns('',
    (r'^$', simple.direct_to_template, {'template': 'homepage.html'}),
    (r'^feedback/$', feedback_views.feedback_list),
    (r'^feedback/(\d{1,5})/$', feedback_views.feedback_detail),
    (r'^feedback/change-category/$', feedback_views.feedback_change_category),
    (r'^feedback/ignore/$', feedback_views.feedback_ignore),

    (r'^r/$', redirecter),

    (r'^citypoll/$', citypoll_views.city_vote_list),
    (r'^citypoll/(\d{1,5})/$', citypoll_views.city_detail),

    # This is public (not password-protected via auth).
    (r'^send-feedback/$', feedback_views.save_feedback),
    (r'^send-feedback/city/$', citypoll_views.save_feedback),
)

########NEW FILE########
__FILENAME__ = export_newsitems
"""
Exports newsitems to a CSV file.
"""

from ebpub.db.models import NewsItem, Location, Schema
import sys
import csv
from optparse import OptionParser

def export_newsitems(queryset, schema, out):
    ni_fields = [('title', 'title'),
                 ('description', 'description'),
                 ('location_name', 'location'),
                 ('url', 'URL'),
                 ('item_date', 'item date'),
                 ('pub_date', 'publication date')]
    s_fields = [(str(sf.name), str(sf.pretty_name)) for sf in schema.schemafield_set.all()]
    s_fields.sort()
    writer = csv.writer(out)
    writer.writerow([f[1] for f in ni_fields + s_fields])
    for ni in queryset:
        values = [getattr(ni, f[0]) for f in ni_fields]
        values += [ni.attributes[f[0]] for f in s_fields]
        writer.writerow(values)

def main():
    parser = OptionParser(usage='usage: %prog [options] <schema-slug>')
    parser.add_option('-l', '--location', dest='loc_slug', metavar="SLUG",
                      help='limit newsitems to those contained by location')
    parser.add_option('-f', '--filename', dest='out_file', metavar="FILE",
                      help='write output to this filename')

    (options, args) = parser.parse_args()

    if not args:
        parser.error('must give a schema slug')

    try:
        schema = Schema.objects.get(slug=args[0])
    except Schema.DoesNotExist:
        parser.error('unknown schema %r' % args[0])
        return 1

    niqs = NewsItem.objects.filter(schema=schema)

    if options.loc_slug:
        try:
            loc = Location.objects.get(slug=options.loc_slug)
        except Location.DoesNotExist:
            parser.error('unknown location %r' % options.loc_slug)
            return 1
        else:
            niqs = niqs.filter(location__within=loc.location)

    if options.out_file:
        f = open(options.out_file, 'w')
    else:
        f = sys.stdout

    export_newsitems(niqs, schema, f)

if __name__ == '__main__':
    sys.exit(main())
    

########NEW FILE########
__FILENAME__ = callbacks
# This is a lightweight framework for passing events that need to happen
# when users log in successfully. For example, there's a way to specify that
# an e-mail alert should be created for a given user as soon as he logs in.

from django.utils import simplejson
from ebpub.alerts.models import EmailAlert
import datetime

###############
# SERIALIZING #
###############

# We store compressed JSON in the PendingUserAction table.

def serialize(data):
    return simplejson.dumps(data)

def unserialize(data):
    return simplejson.loads(data)

#############
# CALLBACKS #
#############

def do_callback(callback_name, user, data):
    # callback_name is a key in CALLBACKS.
    # serialized_data is an unserialized Python object.
    try:
        callback = CALLBACKS[callback_name]
    except KeyError:
        return None
    return callback(user, data)

def create_alert(user, data):
    EmailAlert.objects.create(
        user_id=user.id,
        block_id=data['block_id'],
        location_id=data['location_id'],
        frequency=data['frequency'],
        radius=data['radius'],
        include_new_schemas=data['include_new_schemas'],
        schemas=data['schemas'],
        signup_date=datetime.datetime.now(),
        cancel_date=None,
        is_active=True,
    )
    return "Your e-mail alert was created successfully. Thanks for signing up!"

CALLBACKS = {
    'createalert': create_alert,
}

########NEW FILE########
__FILENAME__ = constants
# The key used in request.session to represent the user ID.
USER_SESSION_KEY = 'user_id'

# The key used in request.session to represent the user's e-mail.
EMAIL_SESSION_KEY = 'email'

########NEW FILE########
__FILENAME__ = context_processors
from django.conf import settings
from constants import EMAIL_SESSION_KEY # relative import

def user(request):
    # Makes 'USER' and 'USER_EMAIL' available in templates.
    if request.user:
        return {'DEBUG': settings.DEBUG, 'USER': request.user, 'USER_EMAIL': request.session[EMAIL_SESSION_KEY]}
    else:
        return {'DEBUG': settings.DEBUG, 'USER': None, 'USER_EMAIL': None}

########NEW FILE########
__FILENAME__ = forms
from django import forms
from ebpub.accounts.models import User

class UniqueEmailField(forms.EmailField):
    """
    Validates that the given value is an e-mail address and hasn't already
    been registered.
    """
    def clean(self, value):
        value = forms.EmailField.clean(self, value).lower() # Normalize to lowercase.
        if User.objects.filter(email=value).count():
            raise forms.ValidationError('This e-mail address is already registered.')
        return value

class EmailRegistrationForm(forms.Form):
    email = UniqueEmailField(label='Your e-mail address', widget=forms.TextInput(attrs={'size': 50}))

class BasePasswordForm(forms.Form):
    password1 = forms.CharField(label='Password', widget=forms.PasswordInput)
    password2 = forms.CharField(label='Password (again)', widget=forms.PasswordInput)

    def clean_password2(self):
        p1 = self.cleaned_data['password1']
        p2 = self.cleaned_data['password2']
        if p1 != p2:
            raise forms.ValidationError("The passwords didn't match! Try entering them again.")
        return p2

class PasswordRegistrationForm(BasePasswordForm):
    e = UniqueEmailField(widget=forms.HiddenInput)
    h = forms.CharField(widget=forms.HiddenInput)

class PasswordResetForm(BasePasswordForm):
    e = forms.EmailField(widget=forms.HiddenInput)
    h = forms.CharField(widget=forms.HiddenInput)

class PasswordResetRequestForm(forms.Form):
    email = forms.EmailField()

    def clean_email(self):
        email = self.cleaned_data['email'].lower()
        if not User.objects.filter(email=email).count():
            raise forms.ValidationError("This e-mail address isn't registered yet.")
        return email

class LoginForm(forms.Form):
    email = forms.EmailField()
    password = forms.CharField(widget=forms.PasswordInput)

    def __init__(self, request, *args, **kwargs):
        forms.Form.__init__(self, *args, **kwargs)
        self.request = request

    def clean(self):
        # Note that because this is the form-wide clean() method, any
        # validation errors raised here will not be tied to a particular field.
        # Instead, use form.non_field_errors() in the template.

        email = self.cleaned_data.get('email')
        password = self.cleaned_data.get('password')

        # Check that both email and password were valid. If they're not valid,
        # there's no need to run the following bit of validation.
        if email and password:
            self.user = User.objects.user_by_password(email.lower(), password)
            if self.user is None:
                raise forms.ValidationError("That e-mail and password combo isn't valid. Note that the password is case-sensitive.")
            elif not self.user.is_active:
                raise forms.ValidationError("This account is inactive.")

        if not self.request.session.test_cookie_worked():
            raise forms.ValidationError("Your Web browser doesn't appear to have cookies enabled. Enable cookies, then try again.")

        return self.cleaned_data

########NEW FILE########
__FILENAME__ = middleware
import constants # relative import

UNDEFINED = 123

class LazyUser(object):
    # This class is a transparent wrapper around User that hits the database
    # only after an attribute is accessed.
    def __init__(self, user_id):
        self.user_id = user_id
        self._user_cache = UNDEFINED

    def __getattr__(self, name):
        # Optimization: There's no need to hit the database if we're just
        # getting the ID.
        if name == 'id':
            return self.user_id

        if self._user_cache == UNDEFINED:
            from ebpub.accounts.models import User
            try:
                self._user_cache = User.objects.get(id=self.user_id)
            except User.DoesNotExist:
                self._user_cache = None
        return getattr(self._user_cache, name)

class LazyUserDescriptor(object):
    # This class uses a Python descriptor so that a LazyUser isn't
    # actually created until request.user is accessed.
    def __get__(self, request, obj_type=None):
        if not hasattr(request, '_cached_user'):
            try:
                user_id = request.session[constants.USER_SESSION_KEY]
            except KeyError:
                user = None
            else:
                user = LazyUser(user_id)
            request._cached_user = user
        return request._cached_user

class UserMiddleware(object):
    def process_request(self, request):
        request.__class__.user = LazyUserDescriptor()
        return None

########NEW FILE########
__FILENAME__ = models
from django.db import models
from ebpub.utils import multidb
import utils # relative import

class UserManager(multidb.Manager):
    # This method is necessary because ebpub.utils.multidb doesn't support
    # inserts or updates -- only reads.
    # TODO: Remove this once multidb gets that feature.
    def create_user(self, **kwargs):
        from django.db.backends.postgresql_psycopg2.base import DatabaseWrapper
        connection = DatabaseWrapper(self.database_settings)
        cursor = connection.cursor()
        opts = self.model._meta
        fields = [f for f in opts.fields if f.name != 'id']
        try:
            kwargs['password'] = utils.make_password_hash(kwargs['password'])
            values = [kwargs[f.name] for f in fields]
        except KeyError, e:
            raise ValueError('Missing field: %s' % e)
        cursor.execute("INSERT INTO %s (%s) VALUES (%s)" % \
            (opts.db_table, ','.join([f.column for f in fields]), ','.join(['%s' for i in xrange(len(fields))])),
            values)
        cursor.execute("SELECT CURRVAL('\"%s_id_seq\"')" % opts.db_table)
        user_id = cursor.fetchone()[0]
        connection._commit()
        connection.close()
        return User.objects.get(id=user_id)

    def set_password(self, user_id, raw_password):
        from django.db.backends.postgresql_psycopg2.base import DatabaseWrapper
        connection = DatabaseWrapper(self.database_settings)
        cursor = connection.cursor()
        password = utils.make_password_hash(raw_password)
        cursor.execute("UPDATE %s SET password=%%s WHERE id=%%s" % self.model._meta.db_table,
            (password, user_id))
        connection._commit()
        connection.close()

    def user_by_password(self, email, raw_password):
        """
        Returns a User object for the given e-mail and raw password. If the
        e-mail address exists but the password is incorrect, returns None.
        """
        try:
            user = self.get(email=email)
        except self.model.DoesNotExist:
            return None
        if user.check_password(raw_password):
            return user
        return None

class User(models.Model):
    email = models.EmailField(unique=True) # Stored in all-lowercase.

    # Password uses '[algo]$[salt]$[hexdigest]', just like Django's auth.User.
    password = models.CharField(max_length=128)

    # The SHORT_NAME for the user's metro when they created the account.
    main_metro = models.CharField(max_length=32)

    creation_date = models.DateTimeField()
    is_active = models.BooleanField()

    objects = UserManager('users')

    def __unicode__(self):
        return self.email

    def set_password(self, new_password):
        self.password = utils.make_password_hash(new_password)

    def check_password(self, raw_password):
        "Returns True if the given raw password is correct for this user."
        return utils.check_password_hash(raw_password, self.password)

# Note that this class does *not* use the multidb Manager.
# It's city-specific because pending user actions are city-specific.

class PendingUserAction(models.Model):
    email = models.EmailField(db_index=True) # Stored in all-lowercase.
    callback = models.CharField(max_length=50)
    data = models.TextField() # Serialized into JSON.
    action_date = models.DateTimeField() # When the action was created (so we can clear out expired ones).

    def __unicode__(self):
        return u'%s for %s' % (self.callback, self.email)

########NEW FILE########
__FILENAME__ = urls
from django.conf.urls.defaults import *
from ebpub.savedplaces import views as savedplaces_views
from ebpub.preferences import views as preferences_views
import views # relative import

urlpatterns = patterns('',
    (r'^dashboard/$', views.dashboard),
    (r'^login/$', views.login),
    (r'^logout/$', views.logout),
    (r'^register/$', views.register),
    (r'^password-change/$', views.request_password_change),
    (r'^email-sent/$', 'django.views.generic.simple.direct_to_template', {'template': 'accounts/email_sent.html'}),
    (r'^saved-places/add/$', savedplaces_views.ajax_save_place),
    (r'^saved-places/delete/$', savedplaces_views.ajax_remove_place),
    (r'^hidden-schemas/add/$', preferences_views.ajax_save_hidden_schema),
    (r'^hidden-schemas/delete/$', preferences_views.ajax_remove_hidden_schema),
    (r'^api/saved-places/$', savedplaces_views.json_saved_places),
    (r'^c/$', views.confirm_email),
    (r'^r/$', views.password_reset),
)

########NEW FILE########
__FILENAME__ = utils
from django import http
from django.conf import settings
from django.core.mail import SMTPConnection, EmailMultiAlternatives
from django.template.loader import render_to_string
from ebpub.metros.allmetros import get_metro
import constants # relative import
import random
import urllib

# In Python 2.5+, the sha library was deprecated in favor of hashlib.
try:
    import hashlib
    sha_constructor = hashlib.sha1
except ImportError:
    import sha
    sha_constructor = sha.new

###############################
# E-MAIL ADDRESS VERIFICATION #
###############################

# We use the same e-mail verification functions for account creation
# and password reset, but each takes a 'task' argument, which is either
# CREATE_TASK or RESET_TASK.

CREATE_TASK = 1
RESET_TASK = 2

def make_email_hash(email, task):
    salt = {CREATE_TASK: settings.PASSWORD_CREATE_SALT, RESET_TASK: settings.PASSWORD_RESET_SALT}[task]
    return sha_constructor(salt % (settings.SECRET_KEY, email)).hexdigest()[:6]

def verification_url(email, task):
    params = {'e': email, 'h': make_email_hash(email, task)}
    url = {CREATE_TASK: 'c', RESET_TASK: 'r'}[task]
    return '/accounts/%s/?%s' % (url, urllib.urlencode(params))

def send_verification_email(email, task):
    domain = get_metro()['short_name'] + '.' + settings.EB_DOMAIN
    url = 'http://%s%s' % (domain, verification_url(email, task))
    template_name = {CREATE_TASK: 'register', RESET_TASK: 'password_reset'}[task]
    text_content = render_to_string('accounts/%s_email.txt' % template_name, {'url': url, 'email': email})
    html_content = render_to_string('accounts/%s_email.html' % template_name, {'url': url, 'email': email})

    if settings.DEBUG:
        print text_content
        print html_content
    else:
        subject = {CREATE_TASK: 'Please confirm account', RESET_TASK: 'Password reset request'}[task]
        conn = SMTPConnection() # Use default settings.
        message = EmailMultiAlternatives(subject, text_content, settings.GENERIC_EMAIL_SENDER,
            [email], connection=conn)
        message.attach_alternative(html_content, 'text/html')
        message.send()

#############
# PASSWORDS #
#############

def make_password_hash(raw_password):
    salt = sha_constructor(str(random.random())).hexdigest()[:5]
    hsh = sha_constructor(salt + raw_password).hexdigest()
    return 'sha1$%s$%s' % (salt, hsh)

def check_password_hash(raw_password, password_hash):
    "Returns True if the raw_password matches password_hash."
    algo, salt, hsh = password_hash.split('$')
    correct_hash = sha_constructor(salt + raw_password).hexdigest()
    return hsh == correct_hash

##############
# LOGGING IN #
##############

def login(request, user):
    """
    Logs the given user into the given HttpRequest, setting the correct
    bits in the session.
    """
    if constants.USER_SESSION_KEY in request.session:
        if request.session[constants.USER_SESSION_KEY] != user.id:
            # To avoid reusing another user's session, create a new, empty
            # session if the existing session corresponds to a different
            # authenticated user.
            request.session.flush()
    else:
        request.session.cycle_key()

    # Set the session variables. Note that we save the user's e-mail address
    # in the session, despite the fact that this is redundant, so that we can
    # access it without having to do a database lookup.
    request.session[constants.USER_SESSION_KEY] = user.id
    request.session[constants.EMAIL_SESSION_KEY] = user.email

    if hasattr(request, 'user'):
        request.user = user

def login_required(view_func):
    """
    Decorator that requires login before a given view function can be
    accessed.
    """
    def inner_view(request, *args, **kwargs):
        if request.user is not None:
            return view_func(request, *args, **kwargs)
        request.session['next_url'] = request.path
        return http.HttpResponseRedirect('/accounts/login/')
    return inner_view

########NEW FILE########
__FILENAME__ = views
from django import http
from django.conf import settings
from django.template.loader import render_to_string
from ebpub.accounts import callbacks
from ebpub.accounts.models import User, PendingUserAction
from ebpub.alerts.models import EmailAlert
from ebpub.db.models import Schema
from ebpub.metros.allmetros import get_metro
from ebpub.preferences.models import HiddenSchema
from ebpub.savedplaces.models import SavedPlace
from ebpub.utils.view_utils import eb_render
import forms, utils # relative import
import datetime

###########################
# VIEWS FOR USER ACCOUNTS #
###########################

def login(request, custom_message=None, force_form=False, initial_email=None):
    # custom_message is a string to display at the top of the login form.
    # force_form is used when you want to force display of the original
    # form (regardless of whether it's a POST request).

    # If the user is already logged in, redirect to the dashboard.
    if request.user:
        return http.HttpResponseRedirect('/accounts/dashboard/')

    if request.method == 'POST' and not force_form:
        form = forms.LoginForm(request, request.POST)
        if form.is_valid():
            utils.login(request, form.user)
            if request.session.test_cookie_worked():
                request.session.delete_test_cookie()

            # If the session contains a 'pending_login' variable, it will be a
            # tuple of (callback_name, data), where data is an unserialized
            # Python object and callback_name corresponds to a callback in
            # ebpub/accounts/callbacks.py.
            if 'pending_login' in request.session:
                try:
                    callback, data = request.session['pending_login']
                    message = callbacks.do_callback(callback, form.user, data)
                except (TypeError, ValueError):
                    message = None

                # We're done with the callbacks and don't want to risk them
                # happening again, so we delete the session value.
                del request.session['pending_login']

                # Save the login message in the session so we can display it
                # for the user.
                if message:
                    request.session['login_message'] = message

            next_url = request.session.pop('next_url', '/accounts/dashboard/')
            return http.HttpResponseRedirect(next_url)
    else:
        form = forms.LoginForm(request, initial={'email': initial_email})
    request.session.set_test_cookie()
    custom_message = request.session.pop('login_message', custom_message)
    return eb_render(request, 'accounts/login_form.html', {'form': form, 'custom_message': custom_message})

def logout(request):
    if request.method == 'POST':
        request.session.flush()
        request.user = None

        # The `next_url` can be specified either as POST data or in the
        # session. If it's in the session, it can be trusted. If it's in
        # POST data, it can't be trusted, so we do a simple check that it
        # starts with a slash (so that people can't hack redirects to other
        # sites).
        if 'next_url' in request.POST and request.POST['next_url'].startswith('/'):
            next_url = request.POST['next_url']
        elif 'next_url' in request.session:
            next_url = request.session.pop('next_url')
        else:
            request.session['login_message'] = "You're logged out. You can log in again below."
            next_url = '/accounts/login/'

        return http.HttpResponseRedirect(next_url)
    return eb_render(request, 'accounts/logout_form.html')

@utils.login_required
def dashboard(request):
    custom_message = request.session.get('login_message')
    if 'login_message' in request.session:
        del request.session['login_message']

    alert_list = EmailAlert.active_objects.filter(user_id=request.user.id)
    saved_place_list = SavedPlace.objects.filter(user_id=request.user.id)
    hidden_schema_ids = HiddenSchema.objects.filter(user_id=request.user.id).values('schema_id')
    hidden_schema_ids = set([x['schema_id'] for x in hidden_schema_ids])

    schema_list = []
    for schema in Schema.public_objects.filter(is_special_report=False).order_by('plural_name'):
        schema_list.append({'schema': schema, 'is_hidden': schema.id in hidden_schema_ids})

    return eb_render(request, 'accounts/dashboard.html', {
        'custom_message': custom_message,
        'user': request.user,
        'alert_list': alert_list,
        'saved_place_list': saved_place_list,
        'schema_list': schema_list,
    })

####################################
# UTILITIES USED BY MULTIPLE VIEWS #
####################################

# These utilities encapsulate some logic used by both the registration
# workflow and the "I forgot my password" workflow.

class BadHash(Exception):
    def __init__(self, response):
        self.response = response

def send_confirmation_and_redirect(request, email, task):
    if settings.DEBUG:
        url = utils.verification_url(email, task)
        return http.HttpResponse('<a href="%s">Click here to simulate the e-mail confirmation</a>.' % url)
    else:
        utils.send_verification_email(email, task)
    return http.HttpResponseRedirect('/accounts/email-sent/')

def confirm_request_hash(request, task):
    if request.method == 'GET':
        d = request.GET
    elif request.method == 'POST':
        d = request.POST
    else:
        raise http.Http404('Invalid method')

    # Verify the hash.
    try:
        email, email_hash = d['e'], d['h']
        if email_hash != utils.make_email_hash(email, task):
            raise KeyError
    except KeyError:
        form_link = {utils.CREATE_TASK: '/accounts/register/', utils.RESET_TASK: '/accounts/password-change/'}[task]
        response = http.HttpResponseNotFound(render_to_string('accounts/hash_error.html', {'form_link': form_link}))
        raise BadHash(response)

    return email, email_hash

########################
# REGISTRATION PROCESS #
########################

# We want to avoid creating a database record until an e-mail address has been
# verified, so we use a hash of the e-mail address for security.

def register(request):
    # If the user is already logged in, redirect to the dashboard.
    if request.user:
        return http.HttpResponseRedirect('/accounts/dashboard/')

    if request.method == 'POST':
        form = forms.EmailRegistrationForm(request.POST)
        if form.is_valid():
            return send_confirmation_and_redirect(request, form.cleaned_data['email'], utils.CREATE_TASK)
    else:
        form = forms.EmailRegistrationForm()
    return eb_render(request, 'accounts/register_form_1.html', {'form': form})

def confirm_email(request):
    try:
        email, email_hash = confirm_request_hash(request, utils.CREATE_TASK)
    except BadHash, e:
        return e.response
    if request.method == 'POST':
        form = forms.PasswordRegistrationForm(request.POST)
        if form.is_valid():
            u = User.objects.create_user(
                email=form.cleaned_data['e'],
                password=form.cleaned_data['password1'],
                main_metro=get_metro()['short_name'],
                creation_date=datetime.datetime.now(),
                is_active=True,
            )
            utils.login(request, u)

            # Look for any PendingUserActions for this e-mail address and
            # execute the callbacks.
            for action in PendingUserAction.objects.filter(email=u.email):
                data = callbacks.unserialize(action.data)
                callbacks.do_callback(action.callback, u, data)
                action.delete()

            request.session['login_message'] = 'Your account was created! Thanks for signing up.'
            return http.HttpResponseRedirect('../dashboard/')
    else:
        form = forms.PasswordRegistrationForm(initial={'e': email, 'h': email_hash})
    return eb_render(request, 'accounts/register_form_2.html', {'form': form})

###################
# PASSWORD CHANGE #
###################

def request_password_change(request):
    if request.method == 'POST':
        form = forms.PasswordResetRequestForm(request.POST)
        if form.is_valid():
            return send_confirmation_and_redirect(request, form.cleaned_data['email'], utils.RESET_TASK)
    else:
        form = forms.PasswordResetRequestForm()
    return eb_render(request, 'accounts/request_password_change_form.html', {'form': form})

def password_reset(request):
    try:
        email, email_hash = confirm_request_hash(request, utils.RESET_TASK)
    except BadHash, e:
        return e.response
    if request.method == 'POST':
        form = forms.PasswordResetForm(request.POST)
        if form.is_valid():
            try:
                user = User.objects.get(is_active=True, email=email.lower())
            except User.DoesNotExist:
                # If we reach this point, then somebody managed to submit a
                # hash for a user that's not registered yet.
                raise http.Http404()
            User.objects.set_password(user.id, form.cleaned_data['password1'])
            request.session['login_message'] = 'Your password was changed successfully. Give it a shot by logging in below:'
            return http.HttpResponseRedirect('/accounts/login/')
    else:
        form = forms.PasswordResetForm(initial={'e': email, 'h': email_hash})
    return eb_render(request, 'accounts/password_change_form.html', {'form': form})

########NEW FILE########
__FILENAME__ = models
from django.db import models
from ebpub.db.models import Location
from ebpub.streets.models import Block

class ActiveAlertsManager(models.Manager):
    def get_query_set(self):
        return super(ActiveAlertsManager, self).get_query_set().filter(is_active=True)

class EmailAlert(models.Model):
    user_id = models.IntegerField()
    block = models.ForeignKey(Block, blank=True, null=True)
    location = models.ForeignKey(Location, blank=True, null=True)
    frequency = models.PositiveIntegerField() # 1=daily, 7=weekly
    radius = models.PositiveIntegerField(blank=True, null=True)

    # If True, schemas should be treated as an exclusion list instead of an
    # inclusion list. This allows people to exclude existing schemas, but
    # but still receive updates for new schemas when we add them later.
    include_new_schemas = models.BooleanField()

    # A comma-separated list of schema IDs. Semantics depend on the value of
    # include_new_schemas (see above comment).
    schemas = models.TextField()

    signup_date = models.DateTimeField()
    cancel_date = models.DateTimeField(blank=True, null=True)
    is_active = models.BooleanField()

    objects = models.Manager()
    active_objects = ActiveAlertsManager()

    def __unicode__(self):
        return u'User %s: %u' % (self.user_id, self.name())

    def unsubscribe_url(self):
        return '/alerts/unsubscribe/%s/' % self.id

    def name(self):
        if self.block:
            return u'%s block%s around %s' % (self.radius, (self.radius != 1 and 's' or ''), self.block.pretty_name)
        else:
            return self.location.pretty_name

    def pretty_frequency(self):
        return {1: 'daily', 7: 'weekly'}[self.frequency]

    def _get_user(self):
        if not hasattr(self, '_user_cache'):
            from ebpub.accounts.models import User
            try:
                self._user_cache = User.objects.get(id=self.user_id)
            except User.DoesNotExist:
                self._user_cache = None
        return self._user_cache
    user = property(_get_user)

########NEW FILE########
__FILENAME__ = sending
from django.conf import settings
from django.core.mail import SMTPConnection, EmailMultiAlternatives
from django.template.loader import render_to_string
from ebpub.alerts.models import EmailAlert
from ebpub.db.models import NewsItem
from ebpub.db.utils import populate_attributes_if_needed
from ebpub.db.views import make_search_buffer
from ebpub.streets.models import Block
import datetime

class NoNews(Exception):
    pass

def email_text_for_place(alert, place, place_name, place_url, newsitem_list, date, frequency):
    """
    Returns a tuple of (text, html) for the given args. `text` is the text-only
    e-mail, and `html` is the HTML version.
    """
    domain = '%s.%s' % (settings.SHORT_NAME, settings.EB_DOMAIN)
    context = {
        'place': place,
        'is_block': isinstance(place, Block),
        'block_radius': isinstance(place, Block) and alert.radius or None,
        'domain': domain,
        'email_address': alert.user.email,
        'place_name': place_name,
        'place_url': place_url,
        'newsitem_list': newsitem_list,
        'date': date,
        'frequency': frequency,
        'unsubscribe_url': alert.unsubscribe_url(),
    }
    return render_to_string('alerts/email.txt', context), render_to_string('alerts/email.html', context)

def email_for_subscription(alert, start_date, frequency):
    """
    Returns a (place_name, text, html) tuple for the given EmailAlert
    object and date.
    """
    start_datetime = datetime.datetime(start_date.year, start_date.month, start_date.day)
    yesterday = datetime.date.today() - datetime.timedelta(days=1)
    end_datetime = datetime.datetime.combine(yesterday, datetime.time(23, 59, 59, 9999)) # the end of yesterday
    # Order by schema__id to group schemas together.
    qs = NewsItem.objects.select_related().filter(schema__is_public=True, pub_date__range=(start_datetime, end_datetime)).order_by('-schema__importance', 'schema__id')
    if alert.include_new_schemas:
        if alert.schemas:
            qs = qs.exclude(schema__id__in=alert.schemas.split(','))
    else:
        if alert.schemas:
            qs = qs.filter(schema__id__in=alert.schemas.split(','))
    if alert.block:
        place_name, place_url = alert.block.pretty_name, alert.block.url()
        place = alert.block
        search_buffer = make_search_buffer(alert.block.location.centroid, alert.radius)
        qs = qs.filter(location__bboverlaps=search_buffer)
    elif alert.location:
        place_name, place_url = alert.location.name, alert.location.url()
        place = alert.location
        qs = qs.filter(newsitemlocation__location__id=alert.location.id)
    ni_list = list(qs)
    if not ni_list:
        raise NoNews
    schemas_used = set([ni.schema for ni in ni_list])
    populate_attributes_if_needed(ni_list, list(schemas_used))
    text, html = email_text_for_place(alert, place, place_name, place_url, ni_list, start_date, frequency)
    return place_name, text, html

def send_all(frequency):
    """
    Sends an e-mail to all subscribers in the system with data with the given frequency.
    """
    conn = SMTPConnection() # Use default settings.
    count = 0
    start_date = datetime.date.today() - datetime.timedelta(days=frequency)
    for alert in EmailAlert.active_objects.filter(frequency=frequency):
        try:
            place_name, text_content, html_content = email_for_subscription(alert, start_date, frequency)
        except NoNews:
            continue
        subject = 'Update: %s' % place_name
        message = EmailMultiAlternatives(subject, text_content, settings.GENERIC_EMAIL_SENDER,
            [alert.user.email], connection=conn)
        message.attach_alternative(html_content, 'text/html')
        message.send()
        count += 1

########NEW FILE########
__FILENAME__ = views
from django import forms, http
from django.shortcuts import get_object_or_404
from django.template.defaultfilters import capfirst
from ebpub.accounts import callbacks
from ebpub.accounts.models import User, PendingUserAction
from ebpub.accounts.utils import login_required, CREATE_TASK
from ebpub.accounts.views import login, send_confirmation_and_redirect
from ebpub.alerts.models import EmailAlert
from ebpub.db.models import Schema
from ebpub.db.views import generic_place_page, url_to_place, block_radius_value
from ebpub.streets.models import Block
from ebpub.utils.view_utils import eb_render
import datetime

FREQUENCY_CHOICES = (('1', 'Daily'), ('7', 'Weekly'))
RADIUS_CHOICES = (('1', '1 block'), ('3', '3 blocks'), ('8', '8 blocks'))

class SchemaMultipleChoiceField(forms.ModelMultipleChoiceField):
    def __init__(self, *args, **kwargs):
        kwargs['queryset'] = Schema.public_objects.filter(is_special_report=False).order_by('plural_name')
        super(SchemaMultipleChoiceField, self).__init__(*args, **kwargs)

    def label_from_instance(self, obj):
        return capfirst(obj.plural_name)

class LocationAlertForm(forms.Form):
    frequency = forms.ChoiceField(choices=FREQUENCY_CHOICES, widget=forms.RadioSelect)
    selected_schemas = SchemaMultipleChoiceField(widget=forms.CheckboxSelectMultiple)
    displayed_schemas = SchemaMultipleChoiceField(widget=forms.MultipleHiddenInput)
    include_new_schemas = forms.BooleanField(required=False)

    # This form is slightly complicated because the e-mail address doesn't need
    # to be entered if the user is logged in. The __init__() method takes an
    # `email_required` argument, which specifies whether the `email` field
    # should be included in the form.
    def __init__(self, *args, **kwargs):
        self.email_required = kwargs.pop('email_required', True)
        forms.Form.__init__(self, *args, **kwargs)
        if self.email_required:
            f = forms.EmailField(widget=forms.TextInput(attrs={'id': 'emailinput', 'class': 'textinput placeholder'}))
            self.fields['email'] = f

    def clean(self):
        # Normalize e-mail address to lower case.
        if self.cleaned_data.get('email'):
            self.cleaned_data['email'] = self.cleaned_data['email'].lower()

        # Set cleaned_data['schemas'], which we'll use later. Its value depends...
        if 'include_new_schemas' in self.cleaned_data and 'selected_schemas' in self.cleaned_data:
            if self.cleaned_data['include_new_schemas']:
                # Set it to the list of schemas to opt out of.
                self.cleaned_data['schemas'] = set(self.cleaned_data['displayed_schemas']) - set(self.cleaned_data['selected_schemas'])
            else:
                # Set it to the list of schemas to opt in to.
                self.cleaned_data['schemas'] = self.cleaned_data['selected_schemas']

        return self.cleaned_data

class BlockAlertForm(LocationAlertForm):
    radius = forms.ChoiceField(choices=RADIUS_CHOICES, widget=forms.RadioSelect)

def signup(request, *args, **kwargs):
    place = url_to_place(*args, **kwargs)
    schema_list = Schema.public_objects.filter(is_special_report=False).order_by('plural_name')
    if isinstance(place, Block):
        FormClass, type_code = BlockAlertForm, 'b'
    else:
        FormClass, type_code = LocationAlertForm, 'l'
    email_required = request.user is None
    if request.method == 'POST':
        form = FormClass(request.POST, email_required=email_required)
        if form.is_valid():
            return finish_signup(request, place, form.cleaned_data)
    else:
        schema_ids = [s.id for s in schema_list]
        form = FormClass(initial={
            'email': 'Enter your e-mail address',
            'radius': block_radius_value(request)[1],
            'frequency': '1',
            'include_new_schemas': True,
            'selected_schemas': schema_ids,
            'displayed_schemas': schema_ids,
        }, email_required=email_required)
    return generic_place_page(request, 'alerts/signup_form.html', place, {'form': form, 'schema_list': schema_list})

def finish_signup(request, place, data):
    # This is called from signup(), after `data` (the alert options) is
    # validated/cleaned. This is a separate function so signup() doesn't get
    # too unwieldy.

    # First, delete displayed_schemas and selected_schemas, because neither is
    # used in serialization. Also, convert `schemas` to be a string list of IDs
    # instead of the model objects, because that's what we end up storing in
    # the database.
    del data['displayed_schemas']
    del data['selected_schemas']
    data['schemas'] = ','.join([str(s.id) for s in data['schemas']])
    if isinstance(place, Block):
        data['block_id'] = place.id
        data['location_id'] = None
    else:
        data['block_id'] = None
        data['location_id'] = place.id
        data['radius'] = None

    if request.user:
        email = request.user.email
    else:
        email = data['email']

    if request.user:
        message = callbacks.create_alert(request.user, data)
        request.session['login_message'] = message
        return http.HttpResponseRedirect('/accounts/dashboard/')

    try:
        user = User.objects.get(email=email)
    except User.DoesNotExist:
        # We haven't seen this e-mail address yet, so send out a confirmation
        # e-mail to create the account. But first, save the user's alert
        # information so we can create the alert once the user confirms the
        # e-mail address. (We don't want to send the alert options in that
        # confirmation e-mail, because that's too much data to pass in a URL.)
        PendingUserAction.objects.create(
            email=email,
            callback='createalert',
            data=callbacks.serialize(data),
            action_date=datetime.datetime.now(),
        )
        return send_confirmation_and_redirect(request, email, CREATE_TASK)
    else:
        # This e-mail address already has an account, so show a password
        # confirmation screen.
        msg = "You already have an account with this e-mail address. " \
              "Please enter your password to confirm this alert subscription."
        request.session['pending_login'] = ('createalert', data)
        return login(request, custom_message=msg, force_form=True, initial_email=email)

@login_required
def unsubscribe(request, alert_id):
    a = get_object_or_404(EmailAlert.active_objects.all(), id=alert_id, user_id=request.user.id)
    if request.method == 'POST':
        EmailAlert.objects.filter(id=alert_id).update(cancel_date=datetime.datetime.now(), is_active=False)
        request.session['login_message'] = "We've unsubscribed you from the alert for %s" % a.name()
        return http.HttpResponseRedirect('/accounts/dashboard/')
    return eb_render(request, 'alerts/confirm_unsubscription.html', {'alert': a})

########NEW FILE########
__FILENAME__ = activate_schema
from django.db import connection, transaction
from ebpub.db.models import Schema
import sys

def set_schema_min_date(schema):
    """
    Sets the schema's min_date to the earliest item_date found in news items.
    """
    cursor = connection.cursor()
    cursor.execute("""
        update db_schema
              set min_date = (select min(item_date) from db_newsitem where schema_id=%s)
          where id=%s;
    """, (schema.id, schema.id))
    transaction.commit_unless_managed()

def fix_initial_pub_dates(schema):
    """
    Sets pub_date equal to item_date for the earliest import of the given schema.
    """
    cursor = connection.cursor()
    cursor.execute("""
        update db_newsitem
              set pub_date = item_date
         where pub_date = (select min(pub_date) from db_newsitem where schema_id=%s)
             and schema_id=%s;
    """, (schema.id, schema.id))
    transaction.commit_unless_managed()

def activate_schema(schema):
    """
    Fixes the given schema's min_date, its news item pub_dates, and makes it
    public.
    """
    fix_initial_pub_dates(schema)
    set_schema_min_date(schema)
    # Re-fetch the schema so we don't overwrite the previous changes.
    schema = Schema.objects.get(pk=schema.pk)
    schema.is_public = True
    schema.save()

if __name__ == '__main__':
    try:
        schema = Schema.objects.get(slug__exact=sys.argv[1])
    except Schema.DoesNotExist:
        print "Schema with slug %s could not be found." % sys.argv[1]
        sys.exit(-1)
    activate_schema(schema)
    print "%s: fixed schema.min_date, associated pub_dates, and set is_public=True." % schema.slug

########NEW FILE########
__FILENAME__ = add_location
#!/usr/bin/env python

import sys
from optparse import OptionParser
from django.db import connection
from django.contrib.gis.geos import fromstr
from ebpub.db.bin.alphabetize_locations import alphabetize_locations
from ebpub.db.models import Location, LocationType, NewsItem
from ebpub.geocoder.parser.parsing import normalize
from ebpub.utils.text import slugify
from ebpub.metros.allmetros import get_metro

def swallow_out(f, start_msg=None, end_msg=None):
    """
    Swallows the output of the wrapped function normally going to stdout,
    optionally printing a message before and after the function call.
    """
    def wrapped(*args, **kwargs):
        if start_msg:
            sys.stdout.write(start_msg)
            sys.stdout.flush()
        old_stdout = sys.stdout
        sys.stdout = open('/dev/null', 'w')
        f(*args, **kwargs)
        sys.stdout = old_stdout
        if end_msg:
            sys.stdout.write(end_msg)
            sys.stdout.flush()
    return wrapped

def populate_ni_loc(location):
    ni_count = NewsItem.objects.count()
    cursor = connection.cursor()
    i = 0
    while i < ni_count:
        print i
        cursor.execute("""
            INSERT INTO db_newsitemlocation (news_item_id, location_id)
            SELECT ni.id, loc.id FROM db_newsitem ni, db_location loc
            WHERE intersects(loc.location, ni.location)
                AND ni.id >= %s AND ni.id < %s
                AND loc.id = %s """, (i, i+200, location.id))
        connection._commit()
        i += 200

alphabetize_locations = swallow_out(alphabetize_locations, 'Re-alphabetizing locations ...', ' done.\n')
populate_ni_loc = swallow_out(populate_ni_loc, 'Populating newsitemlocations ...', ' done.\n')

def add_location(name, wkt, loc_type, source='UNKNOWN'):
    geom = fromstr(wkt, srid=4326)
    loc, created = Location.objects.get_or_create(
        name=name,
        slug=slugify(name),
        normalized_name=normalize(name),
        location_type=loc_type,
        location=geom,
        centroid=geom.centroid,
        display_order=0,
        city=get_metro()['city_name'].upper(),
        source=source
    )
    print '%s %s %s' % (created and 'Created' or 'Found', loc_type.name, name)
    return loc

def main(argv=None):
    if argv is None:
        argv = sys.argv[1:]

    usage = 'usage: %prog [options] name wkt'
    p = OptionParser(usage=usage)
    p.add_option('-l', '--location_type', dest='loc_type_slug',
                 default='neighborhoods', help='location type slug')
    p.add_option('-s', '--source', dest='source',
                 default='UNKNOWN', help='source of data')

    opts, args = p.parse_args(argv)

    if len(args) != 2:
        p.error('required arguments `name`, `wkt`')

    try:
        loc_type = LocationType.objects.get(slug=opts.loc_type_slug)
    except LocationType.DoesNotExist:
        p.error('unknown location type slug')

    location = add_location(args[0], args[1], loc_type, opts.source)

    alphabetize_locations(opts.loc_type_slug)
    populate_ni_loc(location)

if __name__ == '__main__':
    sys.exit(main())

########NEW FILE########
__FILENAME__ = alphabetize_locations
#!/usr/bin/env python
import sys
from ebpub.db.models import Location

def alphabetize_locations(location_type_slug=None):
    if location_type_slug is None:
        location_type_slug = 'neighborhoods'
    for i, loc in enumerate(Location.objects.filter(location_type__slug=location_type_slug).order_by('name').iterator()):
        print loc.name
        loc.display_order = i
        loc.save()

if __name__ == "__main__":
    location_type_slug = len(sys.argv[1:]) and sys.argv[1] or None
    sys.exit(alphabetize_locations(location_type_slug))

########NEW FILE########
__FILENAME__ = export_schema
#!/usr/bin/env python
from ebpub.db.models import Schema

fixbool = lambda x: bool(x) and 't' or 'f'

def get_value(obj, field):
    if field.get_internal_type() == 'BooleanField':
        return fixbool(getattr(obj, field.attname))
    else:
        return getattr(obj, field.attname)

def escape(value):
    if isinstance(value, (int, long)):
        return str(value)
    else:
        return "'%s'" % str(value).replace("'", "''")

def get_cols_vals_for_insert(model):
    # Don't copy the primary key id field or related fields
    fields = [f for f in model._meta.fields if f.get_internal_type() not in ('AutoField', 'ForeignKey')]
    cols = [f.column for f in fields]
    values = [escape(get_value(model, f)) for f in fields]
    return (cols, values)

def get_insert_sql(table, cols, values):
    cols_clause = '(' + ', '.join(cols) + ')'
    values_clause = '(' + ', '.join(values) + ')'
    return 'INSERT INTO %s %s VALUES %s;' % (table, cols_clause, values_clause)
    
def print_schema_creation(schema_slug):
    s = Schema.objects.get(slug=schema_slug)
    print "BEGIN;"
    print get_insert_sql(s._meta.db_table, *get_cols_vals_for_insert(s))
    for sf in s.schemafield_set.all():
        cols, vals = get_cols_vals_for_insert(sf)
        cols.insert(0, 'schema_id')
        vals.insert(0, "(SELECT id FROM %s WHERE slug='%s')" % (s._meta.db_table, s.slug))
        print get_insert_sql(sf._meta.db_table, cols, vals)
    print "COMMIT;"

if __name__ == "__main__":
    import sys
    print_schema_creation(sys.argv[1])

########NEW FILE########
__FILENAME__ = geocode_newsitems
from ebpub.db.models import NewsItem
from ebpub.geocoder import SmartGeocoder, GeocodingException, AmbiguousResult, InvalidBlockButValidStreet
from ebpub.geocoder.parser.parsing import ParsingError

def geocode(schema=None):
    """
    Geocode NewsItems with null locations.

    If ``schema`` is provided, only geocode NewsItems with that particular
    schema slug.
    """
    geocoder = SmartGeocoder()
    qs = NewsItem.objects.filter(location__isnull=True).order_by('-id')
    if schema is not None:
        print "Geocoding %s..." % schema
        qs = qs.filter(schema__slug=schema)
    else:
        print "Geocoding all ungeocoded newsitems..."

    geocoded_count = 0
    not_found_count = 0
    ambiguous_count = 0
    parsing_error_count = 0
    invalid_block_count = 0

    for ni in qs.iterator():
        loc_name = ni.location_name
        try:
            add = geocoder.geocode(loc_name)
        except InvalidBlockButValidStreet:
            print '      invalid block but valid street: %s' % loc_name
            invalid_block_count += 1
        except AmbiguousResult, e:
            print '      ambiguous: %s' % loc_name
            ambiguous_count += 1
        except GeocodingException, e:
            print '      not found: %s' % loc_name
            not_found_count += 1
        except ParsingError:
            print '      parse error: %s' % loc_name
            parsing_error_count += 1
        except:
            raise
        else:
            ni.location = add['point']
            ni.block = add['block']
            ni.save()
            print '%s (%s)' % (loc_name, ni.item_url())
            geocoded_count += 1

    print "------------------------------------------------------------------"
    print "Geocoded:       %s" % geocoded_count
    print "Not found:      %s" % not_found_count
    print "Ambiguous:      %s" % ambiguous_count
    print "Parse errors:   %s" % parsing_error_count
    print "Invalid blocks: %s" % invalid_block_count

if __name__ == "__main__":
    import sys
    try:
        schema_slug = sys.argv[1]
    except IndexError:
        geocode()
    else:
        geocode(schema_slug)

########NEW FILE########
__FILENAME__ = import_hoods
import os
import sys
import datetime
from optparse import OptionParser
from django.contrib.gis.gdal import DataSource
from django.db import connection
from ebpub.db.models import Location, LocationType, NewsItem
from ebpub.geocoder.parser.parsing import normalize
from ebpub.utils.text import slugify
from ebpub.metros.allmetros import get_metro

def populate_ni_loc(location):
    ni_count = NewsItem.objects.count()
    cursor = connection.cursor()
    i = 0
    while i < ni_count:
        cursor.execute("""
            INSERT INTO db_newsitemlocation (news_item_id, location_id)
            SELECT ni.id, loc.id FROM db_newsitem ni, db_location loc
            WHERE st_intersects(loc.location, ni.location)
                AND ni.id >= %s AND ni.id < %s
                AND loc.id = %s
        """, (i, i+200, location.id))
        connection._commit()
        i += 200

class NeighborhoodImporter(object):
    def __init__(self, layer):
        self.layer = layer
        metro = get_metro()
        self.metro_name = metro['metro_name'].upper()
        self.now = datetime.datetime.now()
        self.location_type, _ = LocationType.objects.get_or_create(
            name = 'neighborhood',
            plural_name = 'neighborhoods',
            scope = self.metro_name,
            slug = 'neighborhoods',
            is_browsable = True,
            is_significant = True,
        )
        unknown, created = Location.objects.get_or_create(
            name = 'Unknown neighborhood',
            normalized_name = 'UNKNOWN',
            slug = 'unknown',
            location_type = self.location_type,
            location = None,
            centroid = None,
            display_order = 0,
            city = self.metro_name,
            source = '',
            area = None,
            is_public = False
        )
        if not created:
            unknown.creation_date = self.now
            unknown.last_mod_date = self.now

    def save(self, name_field='name', source='UNKNOWN', verbose=True):
        hoods = []
        for feature in self.layer:
            name = feature.get(name_field)
            geom = feature.geom.transform(4326, True).geos
            if not geom.valid:
                geom = geom.buffer(0.0)
                if not geom.valid:
                    print >> sys.stderr, 'Warning: invalid geometry: %s' % name
            fields = dict(
                name = name,
                normalized_name = normalize(name),
                slug = slugify(name),
                location_type = self.location_type,
                location = geom,
                centroid = geom.centroid,
                city = self.metro_name,
                source = source,
                area = geom.transform(3395, True).area,
                is_public = True,
                display_order = 0, # This is overwritten in the next loop
            )
            hoods.append(fields)
        num_created = 0
        for i, hood_fields in enumerate(sorted(hoods, key=lambda h: h['name'])):
            kwargs = dict(hood_fields, defaults={'creation_date': self.now, 'last_mod_date': self.now, 'display_order': i})
            hood, created = Location.objects.get_or_create(**kwargs)
            if created:
                num_created += 1
            if verbose:
                print >> sys.stderr, '%s neighborhood %s' % (created and 'Created' or 'Already had', hood)
            if verbose:
                sys.stderr.write('Populating newsitem locations ... ')
            populate_ni_loc(hood)
            if verbose:
                sys.stderr.write('done.\n')
        return num_created

usage = 'usage: %prog [options] /path/to/shapefile'
optparser = OptionParser(usage=usage)

def parse_args(argv=None):
    if argv is None:
        argv = sys.argv[1:]

    optparser.add_option('-n', '--name-field', dest='name_field', default='name', help='field that contains neighborhood\'s name')
    optparser.add_option('-i', '--layer-index', dest='layer_id', default=0, help='index of layer in shapefile')
    optparser.add_option('-s', '--source', dest='source', default='UNKNOWN', help='source metadata of the shapefile')
    optparser.add_option('-v', '--verbose', dest='verbose', action='store_true', default=False, help='be verbose')

    return optparser.parse_args(argv)

def main():
    opts, args = parse_args()
    if len(args) != 1:
        optparser.error('must give path to shapefile')
    shapefile = args[0]
    if not os.path.exists(shapefile):
        optparser.error('file does not exist')
    ds = DataSource(shapefile)
    layer = ds[opts.layer_id]
    importer = NeighborhoodImporter(layer)
    num_created = importer.save(name_field=opts.name_field, source=opts.source, verbose=opts.verbose)
    if opts.verbose:
        print >> sys.stderr, 'Created %s neighborhoods.' % num_created

if __name__ == '__main__':
    sys.exit(main())

########NEW FILE########
__FILENAME__ = update_aggregates
#!/usr/bin/env python
from django.db import connection, transaction
from ebpub.db import constants
from ebpub.db.models import Schema, SchemaField, NewsItem, AggregateAll, AggregateDay, AggregateLocationDay, AggregateLocation, AggregateFieldLookup
from ebpub.db.utils import today
import datetime

def smart_update(cursor, new_values, table_name, field_names, comparable_fields, where, pk_name='id', dry_run=False):
    # new_values is a list of dictionaries, each with a value for each field in field_names.

    # Run a query to determine the current values in the DB.
    where = where.items()
    cursor.execute("""
        SELECT %s, %s
        FROM %s
        WHERE %s""" % (pk_name, ','.join(field_names), table_name,
            ' AND '.join(['%s=%%s' % k for k, v in where])), tuple([v for k, v in where]))
    old_values = dict([(tuple(row[1:len(comparable_fields)+1]), dict(zip((pk_name,)+field_names, row))) for row in cursor.fetchall()])
    for new_value in new_values:
        key = tuple([new_value[i] for i in comparable_fields])
        try:
            old_value = old_values.pop(key)
        except KeyError:
            print "INSERT INTO %s (%s) VALUES (%s)" % (table_name, ', '.join(field_names + tuple([i[0] for i in where])), ', '.join([str(new_value[i]) for i in field_names] + [str(i[1]) for i in where]))
            if not dry_run:
                cursor.execute("INSERT INTO %s (%s) VALUES (%s)" % \
                    (table_name, ', '.join(field_names + tuple([i[0] for i in where])), ','.join(['%s' for _ in tuple(field_names) + tuple(where)])),
                    tuple([new_value[i] for i in field_names] + [i[1] for i in where]))
        else:
            for k, v in new_value.items():
                if old_value[k] != v:
                    print "UPDATE %s SET %s WHERE %s=%s" % (table_name, ', '.join(['%s=%s' % (k, v) for k, v in new_value.items()]), pk_name, old_value[pk_name])
                    if not dry_run:
                        new_value_tuple = new_value.items()
                        cursor.execute("UPDATE %s SET %s WHERE %s=%%s" % \
                            (table_name, ', '.join(['%s=%%s' % k for k, v in new_value_tuple]), pk_name),
                            tuple([v for k, v in new_value_tuple] + [old_value[pk_name]]))
                    break
    for old_value in old_values.values():
        print "DELETE FROM %s WHERE %s = %s" % (table_name, pk_name, old_value[pk_name])
        if not dry_run:
            cursor.execute("DELETE FROM %s WHERE %s = %%s" % (table_name, pk_name), (old_value[pk_name],))

def update_aggregates(schema_id_or_slug, dry_run=False):
    """
    Updates all Aggregate* tables for the given schema_id/slug,
    deleting/updating the existing records if necessary.

    If dry_run is True, then the records won't be updated -- only the SQL
    will be output.
    """
    if not str(schema_id_or_slug).isdigit():
        schema_id = Schema.objects.get(slug=schema_id_or_slug).id
    else:
        schema_id = schema_id_or_slug
    cursor = connection.cursor()

    # AggregateAll
    cursor.execute("SELECT COUNT(*) FROM db_newsitem WHERE schema_id = %s", (schema_id,))
    new_values = [{'total': row[0]} for row in cursor.fetchall()]
    smart_update(cursor, new_values, AggregateAll._meta.db_table, ('total',), (), {'schema_id': schema_id}, dry_run=dry_run)

    # AggregateDay
    cursor.execute("""
        SELECT item_date, COUNT(*)
        FROM db_newsitem
        WHERE schema_id = %s
        GROUP BY 1""", (schema_id,))
    new_values = [{'date_part': row[0], 'total': row[1]} for row in cursor.fetchall()]
    smart_update(cursor, new_values, AggregateDay._meta.db_table, ('date_part', 'total'), ('date_part',), {'schema_id': schema_id}, dry_run=dry_run)

    # AggregateLocationDay
    cursor.execute("""
        SELECT nl.location_id, ni.item_date, loc.location_type_id, COUNT(*)
        FROM db_newsitemlocation nl, db_newsitem ni, db_location loc
        WHERE nl.news_item_id = ni.id
            AND ni.schema_id = %s
            AND nl.location_id = loc.id
        GROUP BY 1, 2, 3""", (schema_id,))
    new_values = [{'location_id': row[0], 'date_part': row[1], 'location_type_id': row[2], 'total': row[3]} for row in cursor.fetchall()]
    smart_update(cursor, new_values, AggregateLocationDay._meta.db_table, ('location_id', 'date_part', 'location_type_id', 'total'), ('location_id', 'date_part', 'location_type_id'), {'schema_id': schema_id}, dry_run=dry_run)

    # AggregateLocation
    # This query is a bit clever -- we just sum up the totals created in a
    # previous aggregate. It's a helpful optimization, because otherwise
    # the location query is way too slow.
    # Note that we calculate the total for the last 30 days that had at least
    # one news item -- *NOT* the last 30 days, period.
    # We add date_part <= current_date here to keep sparse items in the future
    # from throwing off counts for the previous 30 days.
    cursor.execute("SELECT date_part FROM %s WHERE schema_id = %%s AND date_part <= current_date ORDER BY date_part DESC LIMIT 1" % \
        AggregateLocationDay._meta.db_table, (schema_id,))
    try:
        end_date = cursor.fetchone()[0]
    except TypeError: # if cursor.fetchone() is None, there are no records.
        pass
    else:
        start_date = end_date - datetime.timedelta(days=30)
        cursor.execute("""
            SELECT location_id, location_type_id, SUM(total)
            FROM %s
            WHERE schema_id = %%s
                AND date_part BETWEEN %%s AND %%s
            GROUP BY 1, 2""" % AggregateLocationDay._meta.db_table,
                (schema_id, start_date, end_date))
        new_values = [{'location_id': row[0], 'location_type_id': row[1], 'total': row[2]} for row in cursor.fetchall()]
        smart_update(cursor, new_values, AggregateLocation._meta.db_table, ('location_id', 'location_type_id', 'total'), ('location_id', 'location_type_id'), {'schema_id': schema_id}, dry_run=dry_run)

    for sf in SchemaField.objects.filter(schema__id=schema_id, is_filter=True, is_lookup=True):
        try:
            end_date = NewsItem.objects.filter(schema__id=schema_id, item_date__lte=today()).values_list('item_date', flat=True).order_by('-item_date')[0]
        except IndexError:
            continue # There have been no NewsItems in the given date range.
        start_date = end_date - datetime.timedelta(days=constants.NUM_DAYS_AGGREGATE)

        if sf.is_many_to_many_lookup():
            # AggregateFieldLookup
            cursor.execute("""
                SELECT id, (
                    SELECT COUNT(*) FROM db_attribute a, db_newsitem ni
                    WHERE a.news_item_id = ni.id
                        AND a.schema_id = %%s
                        AND ni.schema_id = %%s
                        AND a.%s ~ ('[[:<:]]' || db_lookup.id || '[[:>:]]')
                        AND ni.item_date BETWEEN %%s AND %%s
                )
                FROM db_lookup
                WHERE schema_field_id = %%s""" % sf.real_name, (schema_id, schema_id, start_date, end_date, sf.id))
            new_values = [{'lookup_id': row[0], 'total': row[1]} for row in cursor.fetchall()]
            smart_update(cursor, new_values, AggregateFieldLookup._meta.db_table, ('lookup_id', 'total'), ('lookup_id',), {'schema_id': schema_id, 'schema_field_id': sf.id}, dry_run=dry_run)
        else:
            # AggregateFieldLookup
            cursor.execute("""
                SELECT a.%s, COUNT(*)
                FROM db_attribute a, db_newsitem ni
                WHERE a.news_item_id = ni.id
                    AND a.schema_id = %%s
                    AND ni.schema_id = %%s
                    AND %s IS NOT NULL
                    AND ni.item_date BETWEEN %%s AND %%s
                GROUP BY 1""" % (sf.real_name, sf.real_name), (schema_id, schema_id, start_date, end_date))
            new_values = [{'lookup_id': row[0], 'total': row[1]} for row in cursor.fetchall()]
            smart_update(cursor, new_values, AggregateFieldLookup._meta.db_table, ('lookup_id', 'total'), ('lookup_id',), {'schema_id': schema_id, 'schema_field_id': sf.id}, dry_run=dry_run)

    transaction.commit_unless_managed()

def update_all_aggregates(verbose=False):
    for s in Schema.objects.all():
        if verbose:
            print '... %s' % s.plural_name
        update_aggregates(s.id)

if __name__ == "__main__":
    import sys
    if len(sys.argv) == 2:
        update_aggregates(sys.argv[1])
    else:
        update_all_aggregates(verbose=True)

########NEW FILE########
__FILENAME__ = constants
# The number of days to use in sparklines.
NUM_DAYS_AGGREGATE = 30

# Number of results per page in the schema_filter view.
FILTER_PER_PAGE = 30

# Number of NewsItems to fetch for place_detail.
# Gotcha/caveat: If there are more than this number of items in a given day,
# then place_detail will ignore the day because of the smart_bunches()
# logic.
NUM_NEWS_ITEMS_PLACE_DETAIL = 1000

# Number of days to which we limit the NewsItems on place_detail.
LOCATION_DAY_OPTIMIZATION = 7

# Regular expression that parses block-page URLs. The last part of it is for
# the optional pre-directional and/or post-directional (for example,
# 'n', 'ne', 'n-w', '-sw').
BLOCK_URL_REGEX = r'(\d{1,6})-(\d{1,6})([nsew]{1,2})?(?:-([nsew]{1,2}))?'

########NEW FILE########
__FILENAME__ = feeds
from django.contrib.syndication.feeds import Feed
from django.contrib.syndication.views import feed as django_feed_view
from django.http import Http404, HttpResponsePermanentRedirect
from django.utils import simplejson
from django.utils.feedgenerator import Rss201rev2Feed
from ebpub.db.constants import BLOCK_URL_REGEX
from ebpub.db.models import NewsItem, Location
from ebpub.db.utils import populate_attributes_if_needed, today
from ebpub.db.views import make_search_buffer, url_to_block, BLOCK_RADIUS_CHOICES, BLOCK_RADIUS_DEFAULT
from ebpub.metros.allmetros import get_metro
from ebpub.streets.models import Block
import datetime
import re

# RSS feeds powered by Django's syndication framework use MIME type
# 'application/rss+xml'. That's unacceptable to us, because that MIME type
# prompts users to download the feed in some browsers, which is confusing.
# Here, we set the MIME type so that it doesn't do that prompt.
class CorrectMimeTypeFeed(Rss201rev2Feed):
    mime_type = 'application/xml'

# This is a django.contrib.syndication.feeds.Feed subclass whose feed_type
# is set to our preferred MIME type.
class EbpubFeed(Feed):
    feed_type = CorrectMimeTypeFeed

location_re = re.compile(r'^([-_a-z0-9]{1,32})/([-_a-z0-9]{1,32})$')

def bunch_by_date_and_schema(newsitem_list, date_cutoff):
    current_schema_date, current_list = None, []
    for ni in newsitem_list:
        ni_pub_date = ni.pub_date.date()

        # Remove collapsable newsitems that shouldn't be published in the
        # feed yet. See the lengthy comment in AbstractLocationFeed.items().
        if ni.schema.can_collapse and ni_pub_date >= date_cutoff:
            continue

        if current_schema_date != (ni.schema, ni_pub_date):
            if current_list:
                yield current_list
            current_schema_date = (ni.schema, ni_pub_date)
            current_list = [ni]
        else:
            current_list.append(ni)
    if current_list:
        yield current_list

class AbstractLocationFeed(EbpubFeed):
    "Abstract base class for location-specific RSS feeds."

    title_template = 'feeds/streets_title.html'
    description_template = 'feeds/streets_description.html'

    def items(self, obj):
        # Note that items() returns "packed" tuples instead of objects.
        # This is necessary because we return NewsItems and blog entries,
        # plus different types of NewsItems (bunched vs. unbunched).

        # Limit the feed to all NewsItems published in the last four days.
        # We *do* include items from today in this query, but we'll filter
        # those later in this method so that only today's *uncollapsed* items
        # (schema.can_collapse=False) will be included in the feed. We don't
        # want today's *collapsed* items to be included, because more items
        # might be added to the database before the day is finished, and
        # that would result in the RSS item being updated multiple times, which
        # is annoying.
        today_value = today()
        start_date = today_value - datetime.timedelta(days=4)
        end_date = today_value
        # Note: The pub_date__lt=end_date+(1 day) ensures that we don't miss
        # stuff that has a pub_date of the afternoon of end_date. A straight
        # pub_date__range would miss those items.
        qs = NewsItem.objects.select_related().filter(schema__is_public=True, pub_date__gte=start_date, pub_date__lt=end_date+datetime.timedelta(days=1)).extra(select={'pub_date_date': 'date(db_newsitem.pub_date)'}).order_by('-pub_date_date', 'schema__id', 'id')

        # Filter out ignored schemas -- those whose slugs are specified in
        # the "ignore" query-string parameter.
        if 'ignore' in self.request.GET:
            schema_slugs = self.request.GET['ignore'].split(',')
            qs = qs.exclude(schema__slug__in=schema_slugs)

        # Filter wanted schemas -- those whose slugs are specified in the
        # "only" query-string parameter.
        if 'only' in self.request.GET:
            schema_slugs = self.request.GET['only'].split(',')
            qs = qs.filter(schema__slug__in=schema_slugs)

        block_radius = self.request.GET.get('radius', BLOCK_RADIUS_DEFAULT)
        if block_radius not in BLOCK_RADIUS_CHOICES:
            raise Http404('Invalid radius')
        ni_list = list(self.newsitems_for_obj(obj, qs, block_radius))
        schema_list = list(set([ni.schema for ni in ni_list]))
        populate_attributes_if_needed(ni_list, schema_list)

        is_block = isinstance(obj, Block)

        # Note that this decorates the results by returning tuples instead of
        # NewsItems. This is necessary because we're bunching.
        for schema_group in bunch_by_date_and_schema(ni_list, today_value):
            schema = schema_group[0].schema
            if schema.can_collapse:
                yield ('newsitem', obj, schema, schema_group, is_block, block_radius)
            else:
                for newsitem in schema_group:
                    yield ('newsitem', obj, schema, newsitem, is_block, block_radius)

    def item_pubdate(self, item):
        if item[0] == 'newsitem':
            if item[2].can_collapse:
                return item[3][0].pub_date
            return item[3].pub_date
        else:
            raise NotImplementedError()

    def item_link(self, item):
        if item[0] == 'newsitem':
            if item[2].can_collapse:
                return item[1].url() + '#%s-%s' % (item[3][0].schema.slug, item[3][0].pub_date.strftime('%Y%m%d'))
            return item[3].item_url_with_domain()
        else:
            raise NotImplementedError()

    def newsitems_for_obj(self, obj, qs, block_radius):
        raise NotImplementedError('Subclasses must implement this.')

class BlockFeed(AbstractLocationFeed):
    def get_object(self, bits):
        # TODO: This duplicates the logic in the URLconf. Fix Django to allow
        # for RSS feed URL parsing in the URLconf.
        # See http://code.djangoproject.com/ticket/4720
        if get_metro()['multiple_cities']:
            street_re = re.compile(r'^([-a-z]{3,40})/([-a-z0-9]{1,64})/%s$' % BLOCK_URL_REGEX)
        else:
            street_re = re.compile(r'^()([-a-z0-9]{1,64})/%s$' % BLOCK_URL_REGEX)
        m = street_re.search('/'.join(bits))
        if not m:
            raise Block.DoesNotExist
        city_slug, street_slug, from_num, to_num, predir, postdir = m.groups()
        return url_to_block(city_slug, street_slug, from_num, to_num, predir, postdir)

    def title(self, obj):
        return u"EBPUB: %s" % obj.pretty_name

    def link(self, obj):
        return obj.url()

    def description(self, obj):
        return u"EBPUB %s" % obj.pretty_name

    def newsitems_for_obj(self, obj, qs, block_radius):
        search_buffer = make_search_buffer(obj.location.centroid, block_radius)
        return qs.filter(location__bboverlaps=search_buffer)

class LocationFeed(AbstractLocationFeed):
    def get_object(self, bits):
        m = location_re.search('/'.join(bits))
        if not m:
            raise Location.DoesNotExist
        type_slug, slug = m.groups()
        return Location.objects.select_related().get(location_type__slug=type_slug, slug=slug)

    def title(self, obj):
        return u"EBPUB: %s" % obj.name

    def link(self, obj):
        return obj.url()

    def description(self, obj):
        return u"EBPUB %s" % obj.name

    def newsitems_for_obj(self, obj, qs, block_radius):
        return qs.filter(newsitemlocation__location__id=obj.id)

FEEDS = {
    'streets': BlockFeed,
    'locations': LocationFeed,
}

def feed_view(request, *args, **kwargs):
    kwargs['feed_dict'] = FEEDS
    return django_feed_view(request, *args, **kwargs)

########NEW FILE########
__FILENAME__ = models
from django.contrib.gis.db import models
from django.contrib.gis.db.models import Count
from django.db import connection, transaction
from ebpub.streets.models import Block
from ebpub.utils.text import slugify
import datetime

def field_mapping(schema_id_list):
    """
    Given a list of schema IDs, returns a dictionary of dictionaries, mapping
    schema_ids to dictionaries mapping the fields' name->real_name.
    Example return value:
        {1: {u'crime_type': 'varchar01', u'crime_date', 'date01'},
         2: {u'permit_number': 'varchar01', 'to_date': 'date01'},
        }
    """
    # schema_fields = [{'schema_id': 1, 'name': u'crime_type', 'real_name': u'varchar01'},
    #                  {'schema_id': 1, 'name': u'crime_date', 'real_name': u'date01'}]
    result = {}
    for sf in SchemaField.objects.filter(schema__id__in=(schema_id_list)).values('schema', 'name', 'real_name'):
        result.setdefault(sf['schema'], {})[sf['name']] = sf['real_name']
    return result

class SchemaManager(models.Manager):
    def get_query_set(self):
        return super(SchemaManager, self).get_query_set().filter(is_public=True)

class Schema(models.Model):
    name = models.CharField(max_length=32)
    plural_name = models.CharField(max_length=32)
    indefinite_article = models.CharField(max_length=2) # 'a' or 'an'
    slug = models.CharField(max_length=32, unique=True)
    min_date = models.DateField() # the earliest available NewsItem.pub_date for this Schema
    last_updated = models.DateField()
    date_name = models.CharField(max_length=32) # human-readable name for the NewsItem.item_date field
    date_name_plural = models.CharField(max_length=32)
    importance = models.SmallIntegerField() # bigger number is more important
    is_public = models.BooleanField(db_index=True)
    is_special_report = models.BooleanField()

    # whether RSS feed should collapse many of these into one
    can_collapse = models.BooleanField()

    # whether a newsitem_detail page exists for NewsItems of this Schema
    has_newsitem_detail = models.BooleanField()

    # whether aggregate charts are allowed for this Schema
    allow_charting = models.BooleanField()

    # whether attributes should be preloaded for NewsItems of this Schema, in the list view
    uses_attributes_in_list = models.BooleanField()

    # number of records to show on place_overview
    number_in_overview = models.SmallIntegerField()

    objects = models.Manager()
    public_objects = SchemaManager()

    def __unicode__(self):
        return self.name

    def url(self):
        return '/%s/' % self.slug

    def icon_slug(self):
        if self.is_special_report:
            return 'special-report'
        return self.slug

class SchemaInfo(models.Model):
    schema = models.ForeignKey(Schema)
    short_description = models.TextField()
    summary = models.TextField()
    source = models.TextField()
    grab_bag_headline = models.CharField(max_length=128, blank=True)
    grab_bag = models.TextField(blank=True)
    short_source = models.CharField(max_length=128)
    update_frequency = models.CharField(max_length=64)
    intro = models.TextField()

    def __unicode__(self):
        return unicode(self.schema)

class SchemaField(models.Model):
    schema = models.ForeignKey(Schema)
    name = models.CharField(max_length=32)
    real_name = models.CharField(max_length=10) # 'varchar01', 'varchar02', etc.
    pretty_name = models.CharField(max_length=32) # human-readable name, for presentation
    pretty_name_plural = models.CharField(max_length=32) # plural human-readable name
    display = models.BooleanField() # whether to display value on the public site
    is_lookup = models.BooleanField() # whether the value is a foreign key to Lookup
    is_filter = models.BooleanField()
    is_charted = models.BooleanField() # whether schema_detail displays a chart for this field
    display_order = models.SmallIntegerField()
    is_searchable = models.BooleanField() # whether the value is searchable by content

    def __unicode__(self):
        return u'%s - %s' % (self.schema, self.name)

    def _get_slug(self):
        return self.name.replace('_', '-')
    slug = property(_get_slug)

    def _datatype(self):
        return self.real_name[:-2]
    datatype = property(_datatype)

    def is_type(self, *data_types):
        """
        Returns True if this SchemaField is of *any* of the given data types.

        Allowed values are 'varchar', 'date', 'time', 'datetime', 'bool', 'int'.
        """
        for t in data_types:
            if t == self.real_name[:-2]:
                return True
        return False

    def is_many_to_many_lookup(self):
        """
        Returns True if this SchemaField is a many-to-many lookup.
        """
        return self.is_lookup and not self.is_type('int')

    def all_lookups(self):
        if not self.is_lookup:
            raise ValueError('SchemaField.all_lookups() can only be called if is_lookup is True')
        return Lookup.objects.filter(schema_field__id=self.id).order_by('name')

    def browse_by_title(self):
        "Returns FOO in 'Browse by FOO', for this SchemaField."
        if self.is_type('bool'):
            return u'whether they %s' % self.pretty_name_plural
        return self.pretty_name

    def smart_pretty_name(self):
        """
        Returns the pretty name for this SchemaField, taking into account
        many-to-many fields.
        """
        if self.is_many_to_many_lookup():
            return self.pretty_name_plural
        return self.pretty_name

class SchemaFieldInfo(models.Model):
    schema = models.ForeignKey(Schema)
    schema_field = models.ForeignKey(SchemaField)
    help_text = models.TextField()

    def __unicode__(self):
        return unicode(self.schema_field)

class LocationType(models.Model):
    name = models.CharField(max_length=255) # e.g., "Ward" or "Congressional District"
    plural_name = models.CharField(max_length=64) # e.g., "Wards"
    scope = models.CharField(max_length=64) # e.g., "Chicago" or "U.S.A."
    slug = models.CharField(max_length=32, unique=True)
    is_browsable = models.BooleanField() # whether this is displayed on location_type_list
    is_significant = models.BooleanField() # whether this is used to display aggregates, etc.

    def __unicode__(self):
        return u'%s, %s' % (self.name, self.scope)

    def url(self):
        return '/locations/%s/' % self.slug

class Location(models.Model):
    name = models.CharField(max_length=255) # e.g., "35th Ward"
    normalized_name = models.CharField(max_length=255, db_index=True)
    slug = models.CharField(max_length=32, db_index=True)
    location_type = models.ForeignKey(LocationType)
    location = models.GeometryField(null=True)
    centroid = models.PointField(null=True)
    display_order = models.SmallIntegerField()
    city = models.CharField(max_length=255)
    source = models.CharField(max_length=64)
    area = models.FloatField(blank=True, null=True) # in square meters
    population = models.IntegerField(blank=True, null=True) # from the 2000 Census
    user_id = models.IntegerField(blank=True, null=True)
    is_public = models.BooleanField()
    description = models.TextField(blank=True)
    creation_date = models.DateTimeField(blank=True, null=True)
    last_mod_date = models.DateTimeField(blank=True, null=True)
    objects = models.GeoManager()

    class Meta:
        unique_together = (('slug', 'location_type'),)

    def __unicode__(self):
        return self.name

    def url(self):
        return '/locations/%s/%s/' % (self.location_type.slug, self.slug)

    def rss_url(self):
        return '/rss%s' % self.url()

    def alert_url(self):
        return '%salerts/' % self.url()

    def edit_url(self):
        return '/locations/%s/edit/%s/' % (self.location_type.slug, self.slug)

    # Give Location objects a "pretty_name" attribute for interoperability with
    # Block objects. (Parts of our app accept either a Block or Location.)
    def _get_name(self):
        return self.name
    pretty_name = property(_get_name)

    def _is_custom(self):
        return self.location_type.slug == 'custom'
    is_custom = property(_is_custom)

class AttributesDescriptor(object):
    """
    This class provides the functionality that makes the attributes available
    as `attributes` on a model instance.
    """
    def __get__(self, instance, instance_type=None):
        if instance is None:
            raise AttributeError("%s must be accessed via instance" % self.__class__.__name__)
        if not hasattr(instance, '_attributes_cache'):
            select_dict = field_mapping([instance.schema_id])[instance.schema_id]
            instance._attributes_cache = AttributeDict(instance.id, instance.schema_id, select_dict)
        return instance._attributes_cache

    def __set__(self, instance, value):
        if instance is None:
            raise AttributeError("%s must be accessed via instance" % self.__class__.__name__)
        if not isinstance(value, dict):
            raise ValueError('Only a dictionary is allowed')
        mapping = field_mapping([instance.schema_id])[instance.schema_id].items()
        values = [value.get(k, None) for k, v in mapping]
        cursor = connection.cursor()
        cursor.execute("""
            UPDATE %s
            SET %s
            WHERE news_item_id = %%s
            """ % (Attribute._meta.db_table, ','.join(['%s=%%s' % v for k, v in mapping])),
                values + [instance.id])
        # If no records were updated, that means the DB doesn't yet have a
        # row in the attributes table for this news item. Do an INSERT.
        if cursor.rowcount < 1:
            cursor.execute("""
                INSERT INTO %s (news_item_id, schema_id, %s)
                VALUES (%%s, %%s, %s)""" % (Attribute._meta.db_table, ','.join([v for k, v in mapping]), ','.join(['%s' for k in mapping])),
                [instance.id, instance.schema_id] + values)
        transaction.commit_unless_managed()

class AttributeDict(dict):
    """
    A dictionary-like object that serves as a wrapper around attributes for a
    given NewsItem.
    """
    def __init__(self, news_item_id, schema_id, mapping):
        dict.__init__(self)
        self.news_item_id = news_item_id
        self.schema_id = schema_id
        self.mapping = mapping # name -> real_name dictionary
        self.cached = False

    def __do_query(self):
        if not self.cached:
            atts = Attribute.objects.filter(news_item__id=self.news_item_id).extra(select=self.mapping).values(*self.mapping.keys())[0]
            self.update(atts)
            self.cached = True

    def get(self, *args, **kwargs):
        self.__do_query()
        return dict.get(self, *args, **kwargs)

    def __getitem__(self, name):
        self.__do_query()
        return dict.__getitem__(self, name)

    def __setitem__(self, name, value):
        cursor = connection.cursor()
        real_name = self.mapping[name]
        cursor.execute("""
            UPDATE %s
            SET %s = %%s
            WHERE news_item_id = %%s
            """ % (Attribute._meta.db_table, real_name), [value, self.news_item_id])
        # If no records were updated, that means the DB doesn't yet have a
        # row in the attributes table for this news item. Do an INSERT.
        if cursor.rowcount < 1:
            cursor.execute("""
                INSERT INTO %s (news_item_id, schema_id, %s)
                VALUES (%%s, %%s, %%s)""" % (Attribute._meta.db_table, real_name),
                [self.news_item_id, self.schema_id, value])
        transaction.commit_unless_managed()
        dict.__setitem__(self, name, value)

class NewsItemQuerySet(models.query.GeoQuerySet):
    def prepare_attribute_qs(self):
        clone = self._clone()
        if 'db_attribute' not in clone.query.extra_tables:
            clone.query.extra_tables += ('db_attribute',)
        clone.query.extra_where += ('db_newsitem.id = db_attribute.news_item_id',)
        return clone

    def by_attribute(self, schema_field, att_value, is_lookup=False):
        """
        Returns a QuerySet of NewsItems whose attribute value for the given
        SchemaField is att_value. If att_value is a list, this will do the
        equivalent of an "OR" search, returning all NewsItems that have an
        attribute value in the att_value list.

        This handles many-to-many lookups correctly behind the scenes.

        If is_lookup is True, then att_value is treated as the 'code' of a
        Lookup object, and the Lookup's ID will be retrieved for use in the
        query.
        """
        clone = self.prepare_attribute_qs()
        real_name = str(schema_field.real_name)
        if not isinstance(att_value, (list, tuple)):
            att_value = [att_value]
        if is_lookup:
            att_value = Lookup.objects.filter(schema_field__id=schema_field.id, code__in=att_value)
            if not att_value:
                # If the lookup values don't exist, then there aren't any
                # NewsItems with this attribute value. Note that we aren't
                # using QuerySet.none() here, because we want the result to
                # be a NewsItemQuerySet, and none() returns a normal QuerySet.
                clone.query.extra_where += ('1=0',)
                return clone
            att_value = [val.id for val in att_value]
        if schema_field.is_many_to_many_lookup():
            # We have to use a regular expression search to look for all rows
            # with the given att_value *somewhere* in the column. The [[:<:]]
            # thing is a word boundary.
            for value in att_value:
                if not str(value).isdigit():
                    raise ValueError('Only integer strings allowed for att_value in many-to-many SchemaFields')
            clone.query.extra_where += ("db_attribute.%s ~ '[[:<:]]%s[[:>:]]'" % (real_name, '|'.join([str(val) for val in att_value])),)
        elif None in att_value:
            if att_value != [None]:
                raise ValueError('by_attribute() att_value list cannot have more than one element if it includes None')
            clone.query.extra_where += ("db_attribute.%s IS NULL" % real_name,)
        else:
            clone.query.extra_where += ("db_attribute.%s IN (%s)" % (real_name, ','.join(['%s' for val in att_value])),)
            clone.query.extra_params += tuple(att_value)
        return clone

    def date_counts(self):
        """
        Returns a dictionary mapping {item_date: count}.
        """
        # TODO: values + annotate doesn't seem to play nice with GeoQuerySet
        # at the moment. This is the changeset where it broke:
        # http://code.djangoproject.com/changeset/10326
        from django.db.models.query import QuerySet
        qs = QuerySet.values(self, 'item_date').annotate(count=models.Count('id'))
        return dict([(v['item_date'], v['count']) for v in qs])

    def top_lookups(self, schema_field, count):
        """
        Returns a list of {lookup, count} dictionaries representing the top
        Lookups for this QuerySet.
        """
        real_name = "db_attribute." + str(schema_field.real_name)
        if schema_field.is_many_to_many_lookup():
            clone = self.prepare_attribute_qs().filter(schema__id=schema_field.schema_id)
            clone = clone.extra(where=[real_name + " ~ ('[[:<:]]' || db_lookup.id || '[[:>:]]')"])
            # We want to count the current queryset and get a single
            # row for injecting into the subsequent Lookup query, but
            # we don't want Django's aggregation support to
            # automatically group by fields that aren't relevant and
            # would cause multiple rows as a result. So we call
            # `values()' on a field that we're already filtering by,
            # in this case, schema, as essentially a harmless identify
            # function.
            clone = clone.values('schema').annotate(count=Count('schema'))
            qs = Lookup.objects.filter(schema_field__id=schema_field.id)
            qs = qs.extra(select={'lookup_id': 'id', 'item_count': clone.values('count').query})
        else:
            qs = self.prepare_attribute_qs().extra(select={'lookup_id': real_name})
            qs.query.group_by = [real_name]
            qs = qs.values('lookup_id').annotate(item_count=Count('id'))
        ids_and_counts = [(v['lookup_id'], v['item_count']) for v in qs.values('lookup_id', 'item_count').order_by('-item_count') if v['item_count']][:count]
        lookup_objs = Lookup.objects.in_bulk([i[0] for i in ids_and_counts])
        return [{'lookup': lookup_objs[i[0]], 'count': i[1]} for i in ids_and_counts]

    def text_search(self, schema_field, query):
        """
        Returns a QuerySet of NewsItems whose attribute for
        a given schema field matches a text search query.
        """
        clone = self.prepare_attribute_qs()
        query = query.lower()
        clone.query.extra_where += ("db_attribute." + str(schema_field.real_name) + " ILIKE %s",)
        clone.query.extra_params += ("%%%s%%" % query,)
        return clone

class NewsItemManager(models.GeoManager):
    def get_query_set(self):
        return NewsItemQuerySet(self.model)

    def by_attribute(self, *args, **kwargs):
        return self.get_query_set().by_attribute(*args, **kwargs)

    def text_search(self, *args, **kwargs):
        return self.get_query_set().text_search(*args, **kwargs)

    def date_counts(self, *args, **kwargs):
        return self.get_query_set().date_counts(*args, **kwargs)

    def top_lookups(self, *args, **kwargs):
        return self.get_query_set().top_lookups(*args, **kwargs)

class NewsItem(models.Model):
    schema = models.ForeignKey(Schema)
    title = models.CharField(max_length=255)
    description = models.TextField()
    url = models.TextField(blank=True)
    pub_date = models.DateTimeField(db_index=True)
    item_date = models.DateField(db_index=True)
    location = models.GeometryField(blank=True, null=True)
    location_name = models.CharField(max_length=255)
    location_object = models.ForeignKey(Location, blank=True, null=True)
    block = models.ForeignKey(Block, blank=True, null=True)
    objects = NewsItemManager()
    attributes = AttributesDescriptor()

    def __unicode__(self):
        return self.title

    def item_url(self):
        return '/%s/by-date/%s/%s/%s/%s/' % (self.schema.slug, self.item_date.year, self.item_date.month, self.item_date.day, self.id)

    def item_url_with_domain(self):
        from django.conf import settings
        return 'http://%s.%s%s' % (settings.SHORT_NAME, settings.EB_DOMAIN, self.item_url())

    def item_date_url(self):
        return '/%s/by-date/%s/%s/%s/' % (self.schema.slug, self.item_date.year, self.item_date.month, self.item_date.day)

    def location_url(self):
        if self.location_object_id is not None:
            return self.location_object.url()
        return None

    def attributes_for_template(self):
        """
        Return a list of AttributeForTemplate objects for this NewsItem. The
        objects are ordered by SchemaField.display_order.
        """
        fields = SchemaField.objects.filter(schema__id=self.schema_id).select_related().order_by('display_order')
        field_infos = dict([(obj.schema_field_id, obj.help_text) for obj in SchemaFieldInfo.objects.filter(schema__id=self.schema_id)])
        try:
            attribute_row = Attribute.objects.filter(news_item__id=self.id).values(*[f.real_name for f in fields])[0]
        except KeyError:
            return []
        return [AttributeForTemplate(f, attribute_row, field_infos.get(f.id, None)) for f in fields]

class AttributeForTemplate(object):
    def __init__(self, schema_field, attribute_row, help_text):
        self.sf = schema_field
        self.raw_value = attribute_row[schema_field.real_name]
        self.schema_slug = schema_field.schema.slug
        self.is_lookup = schema_field.is_lookup
        self.is_filter = schema_field.is_filter
        self.help_text = help_text
        if self.is_lookup:
            if self.raw_value == '':
                self.values = []
            elif self.sf.is_many_to_many_lookup():
                try:
                    id_values = map(int, self.raw_value.split(','))
                except ValueError:
                    self.values = []
                else:
                    lookups = Lookup.objects.in_bulk(id_values)
                    self.values = [lookups[i] for i in id_values]
            else:
                self.values = [Lookup.objects.get(id=self.raw_value)]
        else:
            self.values = [self.raw_value]

    def value_list(self):
        """
        Returns a list of {value, url} dictionaries representing each value for
        this attribute.
        """
        from django.utils.dateformat import format, time_format
        urls = [None]
        descriptions = [None]
        if self.is_filter:
            if self.is_lookup:
                urls = [look and '/%s/by-%s/%s/' % (self.schema_slug, self.sf.slug, look.slug) or None for look in self.values]
            elif isinstance(self.raw_value, datetime.date):
                urls = ['/%s/by-%s/%s/%s/%s/' % (self.schema_slug, self.sf.slug, self.raw_value.year, self.raw_value.month, self.raw_value.day)]
            elif self.raw_value in (True, False, None):
                urls = ['/%s/by-%s/%s/' % (self.schema_slug, self.sf.slug, {True: 'yes', False: 'no', None: 'na'}[self.raw_value])]
        if self.is_lookup:
            values = [val and val.name or 'None' for val in self.values]
            descriptions = [val and val.description or None for val in self.values]
        elif isinstance(self.raw_value, datetime.datetime):
            values = [format(self.raw_value, 'F j, Y, P')]
        elif isinstance(self.raw_value, datetime.date):
            values = [format(self.raw_value, 'F j, Y')]
        elif isinstance(self.raw_value, datetime.time):
            values = [time_format(self.raw_value, 'P')]
        elif self.raw_value is True:
            values = ['Yes']
        elif self.raw_value is False:
            values = ['No']
        elif self.raw_value is None:
            values = ['N/A']
        else:
            values = [self.raw_value]
        return [{'value': value, 'url': url, 'description': description} for value, url, description in zip(values, urls, descriptions)]

class Attribute(models.Model):
    news_item = models.ForeignKey(NewsItem, primary_key=True, unique=True)
    schema = models.ForeignKey(Schema)
    # All data-type field names must end in two digits, because the code assumes this.
    varchar01 = models.CharField(max_length=255, blank=True, null=True)
    varchar02 = models.CharField(max_length=255, blank=True, null=True)
    varchar03 = models.CharField(max_length=255, blank=True, null=True)
    varchar04 = models.CharField(max_length=255, blank=True, null=True)
    varchar05 = models.CharField(max_length=255, blank=True, null=True)
    date01 = models.DateField(blank=True, null=True)
    date02 = models.DateField(blank=True, null=True)
    date03 = models.DateField(blank=True, null=True)
    date04 = models.DateField(blank=True, null=True)
    date05 = models.DateField(blank=True, null=True)
    time01 = models.TimeField(blank=True, null=True)
    time02 = models.TimeField(blank=True, null=True)
    datetime01 = models.DateTimeField(blank=True, null=True)
    datetime02 = models.DateTimeField(blank=True, null=True)
    datetime03 = models.DateTimeField(blank=True, null=True)
    datetime04 = models.DateTimeField(blank=True, null=True)
    bool01 = models.NullBooleanField(blank=True)
    bool02 = models.NullBooleanField(blank=True)
    bool03 = models.NullBooleanField(blank=True)
    bool04 = models.NullBooleanField(blank=True)
    bool05 = models.NullBooleanField(blank=True)
    int01 = models.IntegerField(blank=True, null=True)
    int02 = models.IntegerField(blank=True, null=True)
    int03 = models.IntegerField(blank=True, null=True)
    int04 = models.IntegerField(blank=True, null=True)
    int05 = models.IntegerField(blank=True, null=True)
    int06 = models.IntegerField(blank=True, null=True)
    int07 = models.IntegerField(blank=True, null=True)
    text01 = models.TextField(blank=True, null=True)

    def __unicode__(self):
        return u'Attributes for news item %s' % self.news_item_id

class LookupManager(models.Manager):
    def get_or_create_lookup(self, schema_field, name, code=None, description='', make_text_slug=True, logger=None):
        """
        Returns the Lookup instance matching the given SchemaField, name and
        Lookup.code, creating it (with the given name/code/description) if it
        doesn't already exist.

        If make_text_slug is True, then a slug will be created from the given
        name. If it's False, then the slug will be the Lookup's ID.
        """
        def log_info(message):
            if logger is None:
                return
            logger.info(message)
        def log_warn(message):
            if logger is None:
                return
            logger.warn(message)
        code = code or name # code defaults to name if it wasn't provided
        try:
            obj = Lookup.objects.get(schema_field__id=schema_field.id, code=code)
        except Lookup.DoesNotExist:
            if make_text_slug:
                slug = slugify(name)
                if len(slug) > 32:
                    # Only bother to warn if we're actually going to use the slug.
                    if make_text_slug:
                        log_warn("Trimming slug %r to %r in order to fit 32-char limit." % (slug, slug[:32]))
                    slug = slug[:32]
            else:
                # To avoid integrity errors in the slug when creating the Lookup,
                # use a temporary dummy slug that's guaranteed not to be in use.
                # We'll change it back immediately afterward.
                slug = '__3029j3f029jf029jf029__'
            if len(name) > 255:
                old_name = name
                name = name[:250] + '...'
                # Save the full name in the description.
                if not description:
                    description = old_name
                log_warn("Trimming name %r to %r in order to fit 255-char limit." % (old_name, name))
            obj = Lookup(schema_field_id=schema_field.id, name=name, code=code, slug=slug, description=description)
            obj.save()
            if not make_text_slug:
                # Set the slug to the ID.
                obj.slug = obj.id
                obj.save()
            log_info('Created %s %r' % (schema_field.name, name))
        return obj

class Lookup(models.Model):
    schema_field = models.ForeignKey(SchemaField)
    name = models.CharField(max_length=255)
    # `code` is the optional internal code to use during retrieval.
    # For example, in scraping Chicago crimes, we use the crime type code
    # to find the appropriate crime type in this table. We can't use `name`
    # in that case, because we've massaged `name` to use a "prettier"
    # formatting than exists in the data source.
    code = models.CharField(max_length=255, blank=True)
    slug = models.CharField(max_length=32, db_index=True)
    description = models.TextField(blank=True)

    objects = LookupManager()

    class Meta:
        unique_together = (('slug', 'schema_field'),)

    def __unicode__(self):
        return u'%s - %s' % (self.schema_field, self.name)

class NewsItemLocation(models.Model):
    news_item = models.ForeignKey(NewsItem)
    location = models.ForeignKey(Location)

    class Meta:
        unique_together = (('news_item', 'location'),)

    def __unicode__(self):
        return u'%s - %s' % (self.news_item, self.location)

class AggregateBaseClass(models.Model):
    schema = models.ForeignKey(Schema)
    total = models.IntegerField()

    class Meta:
        abstract = True

class AggregateAll(AggregateBaseClass):
    # Total items in the schema.
    pass

class AggregateDay(AggregateBaseClass):
    # Total items in the schema with item_date on the given day
    date_part = models.DateField(db_index=True)

class AggregateLocation(AggregateBaseClass):
    # Total items in the schema in location, summed over that last 30 days
    location_type = models.ForeignKey(LocationType)
    location = models.ForeignKey(Location)

class AggregateLocationDay(AggregateBaseClass):
    # Total items in the schema in location with item_date on the given day
    location_type = models.ForeignKey(LocationType)
    location = models.ForeignKey(Location)
    date_part = models.DateField(db_index=True)

class AggregateFieldLookup(AggregateBaseClass):
    # Total items in the schema with schema_field's value = lookup
    schema_field = models.ForeignKey(SchemaField)
    lookup = models.ForeignKey(Lookup)

class SearchSpecialCase(models.Model):
    query = models.CharField(max_length=64, unique=True)
    redirect_to = models.CharField(max_length=255, blank=True)
    title = models.CharField(max_length=128, blank=True)
    body = models.TextField(blank=True)

    def __unicode__(self):
        return self.query

class DataUpdate(models.Model):
    # Keeps track of each time we update our data.
    schema = models.ForeignKey(Schema)
    update_start = models.DateTimeField()  # When the scraper/importer started running.
    update_finish = models.DateTimeField() # When the scraper/importer finished.
    num_added = models.IntegerField()
    num_changed = models.IntegerField()
    num_deleted = models.IntegerField()
    num_skipped = models.IntegerField()
    got_error = models.BooleanField()

    def __unicode__(self):
        return u'%s started on %s' % (self.schema.name, self.update_start)

    def total_time(self):
        return self.update_finish - self.update_start

########NEW FILE########
__FILENAME__ = dateutils
from django import template
import calendar

register = template.Library()

def days_in_month(value):
    # Given a datetime.date, returns the number of days in that month.
    return calendar.monthrange(value.year, value.month)[1]
register.filter('days_in_month', days_in_month)

########NEW FILE########
__FILENAME__ = eb
from ebpub.db.models import NewsItem, SchemaField
from ebpub.db.utils import populate_attributes_if_needed
from ebpub.utils.bunch import bunch, bunchlong, stride
from ebpub.metros.allmetros import METRO_LIST, get_metro
from django import template
from django.conf import settings
from django.template.defaultfilters import stringfilter
from django.template.loader import select_template
from django.conf import settings
import datetime

register = template.Library()

register.filter('bunch', bunch)
register.filter('bunchlong', bunchlong)
register.filter('stride', stride)

def METRO_NAME():
    return get_metro()['metro_name']
register.simple_tag(METRO_NAME)

def SHORT_NAME():
    return settings.SHORT_NAME
register.simple_tag(SHORT_NAME)

def STATE_ABBREV():
    return get_metro()['state']
register.simple_tag(STATE_ABBREV)

def EB_SUBDOMAIN():
    return '%s.%s' % (settings.SHORT_NAME, settings.EB_DOMAIN)
register.simple_tag(EB_SUBDOMAIN)

def isdigit(value):
    return value.isdigit()
isdigit = stringfilter(isdigit)
register.filter('isdigit', isdigit)

def lessthan(value, arg):
    return int(value) < int(arg)
register.filter('lessthan', lessthan)

def greaterthan(value, arg):
    return int(value) > int(arg)
register.filter('greaterthan', greaterthan)

def schema_plural_name(schema, value):
    if isinstance(value, (list, tuple)):
        value = len(value)
    return (value == 1) and schema.name or schema.plural_name
register.simple_tag(schema_plural_name)

def safe_id_sort(value, arg):
    """
    Like Django's built-in "dictsort", but sorts second by the ID attribute, to
    ensure sorts always end up the same.
    """
    var_resolve = template.Variable(arg).resolve
    decorated = [(var_resolve(item), item.id, item) for item in value]
    decorated.sort()
    return [item[2] for item in decorated]
safe_id_sort.is_safe = False
register.filter('safe_id_sort', safe_id_sort)

def safe_id_sort_reversed(value, arg):
    var_resolve = template.Variable(arg).resolve
    decorated = [(var_resolve(item), item.id, item) for item in value]
    decorated.sort()
    decorated.reverse()
    return [item[2] for item in decorated]
safe_id_sort_reversed.is_safe = False
register.filter('safe_id_sort_reversed', safe_id_sort_reversed)

def friendlydate(value):
    try: # Convert to a datetime.date, if it's a datetime.datetime.
        value = value.date()
    except AttributeError:
        pass
    today = datetime.date.today()
    if value == today:
        return 'today'
    elif value == today - datetime.timedelta(1):
        return 'yesterday'
    elif today - value <= datetime.timedelta(6):
        return value.strftime('%A')
    return '%s %s' % (value.strftime('%B'), value.day)
register.filter('friendlydate', friendlydate)

class GetMetroListNode(template.Node):
    def render(self, context):
        context['METRO_LIST'] = METRO_LIST
        return ''

def do_get_metro_list(parser, token):
    # {% get_metro_list %}
    return GetMetroListNode()
register.tag('get_metro_list', do_get_metro_list)

class GetMetroNode(template.Node):
    def render(self, context):
        context['METRO'] = get_metro()
        return ''

def do_get_metro(parser, token):
    # {% get_metro %}
    return GetMetroNode()
register.tag('get_metro', do_get_metro)

class GetNewsItemNode(template.Node):
    def __init__(self, newsitem_variable, context_var):
        self.variable = template.Variable(newsitem_variable)
        self.context_var = context_var

    def render(self, context):
        newsitem_id = self.variable.resolve(context)
        try:
            context[self.context_var] = NewsItem.objects.select_related().get(id=newsitem_id)
        except NewsItem.DoesNotExist:
            pass
        return ''

def do_get_newsitem(parser, token):
    # {% get_newsitem [id_or_var_containing_id] as [context_var] %}
    bits = token.split_contents()
    if len(bits) != 4:
        raise template.TemplateSyntaxError('%r tag requires 3 arguments' % bits[0])
    return GetNewsItemNode(bits[1], bits[3])
register.tag('get_newsitem', do_get_newsitem)

class GetNewerNewsItemNode(template.Node):
    def __init__(self, newsitem_variable, newsitem_list_variable, context_var):
        self.newsitem_var = template.Variable(newsitem_variable)
        self.newsitem_list_var = template.Variable(newsitem_list_variable)
        self.context_var = context_var

    def render(self, context):
        newsitem = self.newsitem_var.resolve(context)
        newsitem_list = self.newsitem_list_var.resolve(context)
        if newsitem_list and newsitem_list[0].item_date > newsitem.item_date:
            context[self.context_var] = newsitem_list[0]
        else:
            context[self.context_var] = None
        return ''

def do_get_newer_newsitem(parser, token):
    # {% get_more_recent_newsitem [newsitem] [comparison_list] as [context_var] %}
    bits = token.split_contents()
    if len(bits) != 5:
        raise template.TemplateSyntaxError('%r tag requires 4 arguments' % bits[0])
    return GetNewerNewsItemNode(bits[1], bits[2], bits[4])
register.tag('get_newer_newsitem', do_get_newer_newsitem)

class GetNewsItemListByAttributeNode(template.Node):
    def __init__(self, schema_id_variable, newsitem_id_variable, att_name, att_value_variable, context_var):
        self.schema_id_variable = template.Variable(schema_id_variable)
        self.newsitem_id_variable = template.Variable(newsitem_id_variable)
        self.att_name = att_name
        self.att_value_variable = template.Variable(att_value_variable)
        self.context_var = context_var

    def render(self, context):
        schema_id = self.schema_id_variable.resolve(context)
        newsitem_id = self.newsitem_id_variable.resolve(context)
        att_value = self.att_value_variable.resolve(context)
        sf = SchemaField.objects.select_related().get(schema__id=schema_id, name=self.att_name)
        ni_list = NewsItem.objects.select_related().filter(schema__id=schema_id).exclude(id=newsitem_id).by_attribute(sf, att_value).order_by('-item_date')
        populate_attributes_if_needed(ni_list, [sf.schema])

        # We're assigning directly to context.dicts[-1] so that the variable
        # gets set in the top-most context in the context stack. If we didn't
        # do this, the variable would only be available within the specific
        # {% block %} from which the template tag was called, because the
        # {% block %} implementation does a context.push() and context.pop().
        context.dicts[-1][self.context_var] = ni_list

        return ''

def do_get_newsitem_list_by_attribute(parser, token):
    # {% get_newsitem_list_by_attribute [schema_id] [newsitem_id_to_ignore] [att_name]=[value_or_var_containing_value] as [context_var] %}
    # {% get_newsitem_list_by_attribute schema.id newsitem.id business_id=attributes.business_id as other_licenses %}
    bits = token.split_contents()
    if len(bits) != 6:
        raise template.TemplateSyntaxError('%r tag requires 5 arguments' % bits[0])
    if bits[3].count('=') != 1:
        raise template.TemplateSyntaxError('%r tag third argument must contain 1 equal sign' % bits[0])
    att_name, att_value_variable = bits[3].split('=')
    return GetNewsItemListByAttributeNode(bits[1], bits[2], att_name, att_value_variable, bits[5])
register.tag('get_newsitem_list_by_attribute', do_get_newsitem_list_by_attribute)

class NewsItemListBySchemaNode(template.Node):
    def __init__(self, newsitem_list_variable, is_ungrouped):
        self.variable = template.Variable(newsitem_list_variable)
        self.is_ungrouped = is_ungrouped

    def render(self, context):
        ni_list = self.variable.resolve(context)

        # For convenience, the newsitem_list might just be a single newsitem,
        # in which case we turn it into a list.
        if isinstance(ni_list, NewsItem):
            ni_list = [ni_list]

        schema = ni_list[0].schema
        template_list = ['db/snippets/newsitem_list/%s.html' % schema.slug,
                         'db/snippets/newsitem_list.html']
        schema_template = select_template(template_list)
        return schema_template.render(template.Context({
            'is_grouped': not self.is_ungrouped,
            'schema': schema,
            'newsitem_list': ni_list,
            'num_newsitems': len(ni_list),
            'place': context.get('place'),
            'is_block': context.get('is_block'),
            'block_radius': context.get('block_radius'),
        }))

def do_newsitem_list_by_schema(parser, token):
    # {% newsitem_list_by_schema [newsitem_or_newsitem_list] [ungrouped?] %}
    bits = token.split_contents()
    if len(bits) not in (2, 3):
        raise template.TemplateSyntaxError('%r tag requires one or two arguments' % bits[0])
    if len(bits) == 3:
        if bits[2] != 'ungrouped':
            raise template.TemplateSyntaxError('Optional last argument to %r tag must be the string "ungrouped"' % bits[0])
        is_ungrouped = True
    else:
        is_ungrouped = False
    return NewsItemListBySchemaNode(bits[1], is_ungrouped)
register.tag('newsitem_list_by_schema', do_newsitem_list_by_schema)

def contains(value, arg):
    return arg in value
register.filter('contains', contains)

########NEW FILE########
__FILENAME__ = eb_filter
"""
Template tags for the custom filter.
"""

from django import template

register = template.Library()

def filter_breadcrumb_link(schema, filters, filter_):
    """
    {% filter_breadcrumb_link filters filter %}
    """
    output = []
    for f in filters:
        output.append(f['url'])
        if f == filter_:
            break
    return '%s%s/' % (schema.url(), '/'.join(output))
register.simple_tag(filter_breadcrumb_link)

class FilterUrlNode(template.Node):
    def __init__(self, schema_var, filterdict_var, additions, removals):
        self.schema_var = template.Variable(schema_var)
        self.filterdict_var = template.Variable(filterdict_var)
        self.additions = [(template.Variable(k), template.Variable(v)) for k, v in additions]
        self.removals = [template.Variable(a) for a in removals]

    def render(self, context):
        # Note that we do a copy() here so that we don't edit the dict in place.
        schema = self.schema_var.resolve(context)
        filterdict = self.filterdict_var.resolve(context).copy()
        for key, value in self.additions:
            filterdict[key.resolve(context)] = value.resolve(context)
        for key in self.removals:
            try:
                del filterdict[key.resolve(context)]
            except KeyError:
                pass
        urls = [d['url'] for d in filterdict.values()]
        if urls:
            return '%s%s/' % (schema.url(), '/'.join(urls))
        else:
            return '%sfilter/' % schema.url()

def do_filter_url(parser, token):
    """
    {% filter_url schema filter_dict %}
    {% filter_url schema filter_dict key value %}
    {% filter_url schema filter_dict key "value again" %}
    {% filter_url schema filter_dict "key" "value again" %}
    {% filter_url schema filter_dict -key_to_remove %}
    {% filter_url schema filter_dict -"key_to_remove" %}

    Outputs a string like 'filter1/foo/filter2/bar/' (with a trailing slash but
    not a leading slash).
    """
    bits = token.split_contents()
    additions, removals = [], []
    schema_var = bits[1]
    filterdict_var = bits[2]
    stack = bits[:2:-1]
    # TODO: This probably fails for removals of hard-coded strings that contain spaces.
    while stack:
        bit = stack.pop()
        if bit.startswith('-'):
            removals.append(bit[1:])
        else:
            try:
                next_bit = stack.pop()
            except IndexError:
                raise template.TemplateSyntaxError('Invalid argument length: %r' % token)
            additions.append((bit, next_bit))
    return FilterUrlNode(schema_var, filterdict_var, additions, removals)

register.tag('filter_url', do_filter_url)

########NEW FILE########
__FILENAME__ = eb_json
"""
Custom template tags for dealing with json.
"""

from django import template
from django.conf import settings
from django.utils import simplejson

register = template.Library()


def json_value(value, arg):
    data = simplejson.loads(value)
    try:
        return data[arg]
    except KeyError:
        if settings.DEBUG:
            raise
        return None

register.filter('json_value', json_value)

########NEW FILE########
__FILENAME__ = full_links
from django import template
import re

register = template.Library()

class FullLinksNode(template.Node):
    """
    Converts all <a href>s within {% full_links %} / {% end_full_links %} to
    use fully qualified URLs -- i.e., to start with 'http://'. Doesn't touch
    the ones that already start with 'http://'.
    """
    def __init__(self, nodelist, domain_var):
        self.nodelist = nodelist
        self.domain_var = template.Variable(domain_var)

    def render(self, context):
        domain = self.domain_var.resolve(context)
        output = self.nodelist.render(context)
        output = re.sub(r'(?i)(<a.*?\bhref=")/', r'\1http://%s/' % domain, output)
        return output

def do_full_links(parser, token):
    # {% full_links [domain] %} ... {% end_full_links %}
    args = token.contents.split()
    if len(args) != 2:
        raise template.TemplateSyntaxError("%r tag requires exactly one argument." % args[0])
    nodelist = parser.parse(('end_full_links',))
    parser.delete_first_token()
    return FullLinksNode(nodelist, args[1])
register.tag('full_links', do_full_links)

########NEW FILE########
__FILENAME__ = mapping
from django import template
from django.conf import settings
from ebpub.utils.stats import normalize

def tile_url_list():
    return settings.TILE_URL_LIST

register = template.Library()

@register.inclusion_tag("etc/mapping_includes.html")
def activate_mapping():
    return {
        "city": settings.SHORT_NAME,
        "tile_url_list": tile_url_list()
    }

MAX_MARKER_RADIUS = 18
MIN_MARKER_RADIUS = 3

get_norm_radius = normalize(MIN_MARKER_RADIUS, MAX_MARKER_RADIUS)

def _get_marker_radius(normalized_value):
    """
    Assumes `normalized_value` is in the range 0.0 and 1.0
    """
    return int(round(((MAX_MARKER_RADIUS - MIN_MARKER_RADIUS) * normalized_value) + MIN_MARKER_RADIUS))

@register.simple_tag
def get_marker_radius(normalized_value):
    return unicode(_get_marker_radius(normalized_value))

@register.simple_tag
def get_marker_url(normalized_value):
    """
    Returns a URL to a marker who's size is based on a normalized (i.e., between
    0.0 and 1.0) value.
    """
    radius = _get_marker_radius(normalized_value)
    if hasattr(settings, 'MARKER_URL_BASE'):
        return settings.MARKER_URL_BASE + 'marker_%d.png' % radius
    else:
        return '/images/mapmarkers/bubble/marker_%d.png' % radius

########NEW FILE########
__FILENAME__ = raw
from django import template

register = template.Library()

def raw(parser, token):
    # Whatever is between {% raw %} and {% endraw %} will be preserved as
    # raw, unrendered template code.
    # If 'silent' is passed in -- {% raw silent %} -- then the resulting output
    # will not contain the {% raw %} and {% endraw %} tags themselves.
    # Otherwise, the output will include an {% endraw %} at the start and
    # {% raw %} at the end, so that other parts of the page aren't vulnerable
    # to Django template escaping injection.
    silent = 'silent' in token.contents
    if silent:
        text = []
    else:
        text = ['{% endraw %}']
    parse_until = 'endraw'
    tag_mapping = {
        template.TOKEN_TEXT: ('', ''),
        template.TOKEN_VAR: ('{{', '}}'),
        template.TOKEN_BLOCK: ('{%', '%}'),
        template.TOKEN_COMMENT: ('{#', '#}'),
    }
    # By the time this template tag is called, the template system has already
    # lexed the template into tokens. Here, we loop over the tokens until
    # {% endraw %} and parse them to TextNodes. We have to add the start and
    # end bits (e.g. "{{" for variables) because those have already been
    # stripped off in a previous part of the template-parsing process.
    while parser.tokens:
        token = parser.next_token()
        if token.token_type == template.TOKEN_BLOCK and token.contents == parse_until:
            if not silent:
                text.append('{% raw silent %}')
            return template.TextNode(u''.join(text))
        start, end = tag_mapping[token.token_type]
        text.append(u'%s%s%s' % (start, token.contents, end))
    parser.unclosed_block_tag(parse_until)
raw = register.tag(raw)

########NEW FILE########
__FILENAME__ = tests
"""
Unit tests for db app.
"""

from django.test import TestCase
from ebpub.db.models import NewsItem, Attribute
import datetime

class ViewTestCase(TestCase):
    "Unit tests for views.py."
    fixtures = ('crimes',)

    def test_search(self):
        # response = self.client.get('')
        pass

    def test_newsitem_detail(self):
        # response = self.client.get('')
        pass

    def test_location_redirect(self):
        # redirect to neighborhoods by default
        response = self.client.get('/locations/')
        self.assertEqual(response.status_code, 301)
        self.assertEqual(response['Location'], 'http://testserver/locations/neighborhoods/')

    def test_location_type_detail(self):
        # response = self.client.get('')
        pass

    def test_location_detail(self):
        # response = self.client.get('')
        pass

    def test_schema_detail(self):
        response = self.client.get('/crime/')
        self.assertEqual(response.status_code, 200)
        response = self.client.get('/nonexistent/')
        self.assertEqual(response.status_code, 404)

    def test_schema_xy_detail(self):
        # response = self.client.get('')
        pass

    def test_filter_choices(self):
        # response = self.client.get('')
        pass

    def test_filter_detail(self):
        # response = self.client.get('')
        pass

    def test_filter_detail_month(self):
        # response = self.client.get('')
        pass

    def test_filter_detail_day(self):
        # response = self.client.get('')
        pass

class DatabaseExtensionsTestCase(TestCase):
    "Unit tests for the custom ORM stuff in models.py."
    fixtures = ('crimes',)

    def testAttributesLazilyLoaded(self):
        """
        Attributes are retrieved lazily the first time you access the
        `attributes` attribute.
        """
        # Turn DEBUG on and reset queries, so we can keep track of queries.
        # This is hackish.
        from django.conf import settings
        from django.db import connection
        connection.queries = []
        settings.DEBUG = True

        ni = NewsItem.objects.get(id=1)
        self.assertEquals(ni.attributes['case_number'], u'HM609859')
        self.assertEquals(ni.attributes['crime_date'], datetime.date(2006, 9, 19))
        self.assertEquals(ni.attributes['crime_time'], None)
        self.assertEquals(len(connection.queries), 3)

        connection.queries = []
        settings.DEBUG = False

    def testSetAllAttributesNonDict(self):
        """
        Setting `attributes` to something other than a dictionary will raise
        ValueError.
        """
        ni = NewsItem.objects.get(id=1)
        def setAttributeToNonDict():
            ni.attributes = 1
        self.assertRaises(ValueError, setAttributeToNonDict)

    def testSetAllAttributes1(self):
        """
        Attributes can be set by assigning a dictionary to the `attributes`
        attribute. As soon as `attributes` is assigned-to, the UPDATE query
        is executed in the database.
        """
        ni = NewsItem.objects.get(id=1)
        self.assertEquals(ni.attributes['case_number'], u'HM609859')
        ni.attributes = dict(ni.attributes, case_number=u'Hello')
        self.assertEquals(Attribute.objects.get(news_item__id=1).varchar01, u'Hello')

    def testSetAllAttributes2(self):
        """
        Setting attributes works even if you don't access them first.
        """
        ni = NewsItem.objects.get(id=1)
        ni.attributes = {
            u'arrests': False,
            u'beat_id': 214,
            u'block_id': 25916,
            u'case_number': u'Hello',
            u'crime_date': datetime.date(2006, 9, 19),
            u'crime_time': None,
            u'domestic': False,
            u'is_outdated': True,
            u'location_id': 66,
            u'police_id': None,
            u'status': u'',
            u'type_id': 97
        }
        self.assertEquals(Attribute.objects.get(news_item__id=1).varchar01, u'Hello')

    def testSetAllAttributesNull(self):
        """
        If you assign to NewsItem.attributes and the dictionary doesn't include
        a value for every field, a None/NULL will be inserted for values that
        aren't represented in the dictionary.
        """
        ni = NewsItem.objects.get(id=1)
        ni.attributes = {u'arrests': False}
        ni = NewsItem.objects.get(id=1)
        self.assertEquals(ni.attributes['arrests'], False)
        self.assertEquals(ni.attributes['beat_id'], None)
        self.assertEquals(ni.attributes['block_id'], None)
        self.assertEquals(ni.attributes['case_number'], None)
        self.assertEquals(ni.attributes['crime_date'], None)
        self.assertEquals(ni.attributes['crime_time'], None)
        self.assertEquals(ni.attributes['domestic'], None)
        self.assertEquals(ni.attributes['is_outdated'], None)
        self.assertEquals(ni.attributes['location_id'], None)
        self.assertEquals(ni.attributes['police_id'], None)
        self.assertEquals(ni.attributes['status'], None)
        self.assertEquals(ni.attributes['type_id'], None)

    def testSetSingleAttribute1(self):
        """
        Setting a single attribute will result in an immediate query setting
        just that attribute.
        """
        ni = NewsItem.objects.get(id=1)
        self.assertEquals(ni.attributes['case_number'], u'HM609859')
        ni.attributes['case_number'] = u'Hello'
        self.assertEquals(Attribute.objects.get(news_item__id=1).varchar01, u'Hello')

    def testSetSingleAttribute2(self):
        """
        Setting single attributes works even if you don't access them first.
        """
        ni = NewsItem.objects.get(id=1)
        ni.attributes['case_number'] = u'Hello'
        self.assertEquals(Attribute.objects.get(news_item__id=1).varchar01, u'Hello')

    def testSetSingleAttribute3(self):
        """
        Setting a single attribute will result in the value being cached.
        """
        ni = NewsItem.objects.get(id=1)
        self.assertEquals(ni.attributes['case_number'], u'HM609859')
        ni.attributes['case_number'] = u'Hello'
        self.assertEquals(ni.attributes['case_number'], u'Hello')

    def testSetSingleAttribute4(self):
        """
        Setting a single attribute will result in the value being cached, even
        if you don't access the attribute first.
        """
        ni = NewsItem.objects.get(id=1)
        ni.attributes['case_number'] = u'Hello'
        self.assertEquals(ni.attributes['case_number'], u'Hello')

    def testSetSingleAttributeNumQueries(self):
        """
        When setting an attribute, the system will only use a single query --
        i.e., it won't have to retrieve the attributes first simply because
        code accessed the NewsItem.attributes attribute.
        """
        # Turn DEBUG on and reset queries, so we can keep track of queries.
        # This is hackish.
        from django.conf import settings
        from django.db import connection
        connection.queries = []
        settings.DEBUG = True

        ni = NewsItem.objects.get(id=1)
        ni.attributes['case_number'] = u'Hello'
        self.assertEquals(len(connection.queries), 3)

        connection.queries = []
        settings.DEBUG = False

    def testBlankAttributes(self):
        """
        If a NewsItem has no attributes set, accessing NewsItem.attributes will
        return an empty dictionary.
        """
        Attribute.objects.filter(news_item__id=1).delete()
        ni = NewsItem.objects.get(id=1)
        self.assertEquals(ni.attributes, {})

    def testSetAttributesFromBlank(self):
        """
        When setting attributes on a NewsItem that doesn't have attributes yet,
        the underlying implementation will use an INSERT statement instead of
        an UPDATE.
        """
        Attribute.objects.filter(news_item__id=1).delete()
        ni = NewsItem.objects.get(id=1)
        ni.attributes = {
            u'arrests': False,
            u'beat_id': 214,
            u'block_id': 25916,
            u'case_number': u'Hello',
            u'crime_date': datetime.date(2006, 9, 19),
            u'crime_time': None,
            u'domestic': False,
            u'is_outdated': True,
            u'location_id': 66,
            u'police_id': None,
            u'status': u'',
            u'type_id': 97
        }
        self.assertEquals(Attribute.objects.get(news_item__id=1).varchar01, u'Hello')

    def testSetSingleAttributeFromBlank(self):
        """
        When setting a single attribute on a NewsItem that doesn't have
        attributes yet, the underlying implementation will use an INSERT
        statement instead of an UPDATE.
        """
        Attribute.objects.filter(news_item__id=1).delete()
        ni = NewsItem.objects.get(id=1)
        ni.attributes['case_number'] = u'Hello'
        self.assertEquals(Attribute.objects.get(news_item__id=1).varchar01, u'Hello')

    def testAttributeFromBlankSanity(self):
        """
        Sanity check for munging attribute data from blank.
        """
        Attribute.objects.filter(news_item__id=1).delete()
        ni = NewsItem.objects.get(id=1)
        self.assertEquals(ni.attributes, {})
        ni.attributes['case_number'] = u'Hello'
        self.assertEquals(ni.attributes['case_number'], u'Hello')
        self.assertEquals(Attribute.objects.get(news_item__id=1).varchar01, u'Hello')

########NEW FILE########
__FILENAME__ = utils
from django.conf import settings
import datetime

def smart_bunches(newsitem_list, max_days=5, max_items_per_day=100):
    """
    Helper function that takes a list of NewsItems, ordered descending by
    pub_date, and returns a list of NewsItems that's been optimized for
    display in timelines.

    Assumes each NewsItem has a pub_date_date attribute!

    The logic is:
        * Go backwards in time until there are 5 full days' worth of news
          (not necessarily 5 consecutive days).
        * If, for any day, there are more than 100 items, stop at that day
          (inclusive).
        * Any NewsItems in the list with a pub_date equal to the oldest
          pub_date in the list will be removed. This is because we cannot
          assume *all* of the items with that pub_date are in the list.
    """
    if newsitem_list:
        current_date = None
        days_seen = 0
        stop_at_next_day = False
        end_index = None
        oldest_pub_date = newsitem_list[-1].pub_date_date
        for i, ni in enumerate(newsitem_list):
            if ni.pub_date_date != current_date:
                days_seen += 1
                current_date = ni.pub_date_date
                items_in_current_day = 1
                if stop_at_next_day or days_seen > max_days or ni.pub_date_date == oldest_pub_date:
                    end_index = i
                    break
            else:
                items_in_current_day += 1
                if items_in_current_day > max_items_per_day:
                    stop_at_next_day = True
        if end_index is not None:
            del newsitem_list[end_index:]
    return newsitem_list

def populate_attributes_if_needed(newsitem_list, schema_list):
    """
    Helper function that takes a list of NewsItems and sets ni.attribute_values
    to a dictionary of attributes {field_name: value} for all NewsItems whose
    schemas have uses_attributes_in_list=True. This is accomplished with a
    minimal amount of database queries.

    The values in the attribute_values dictionary are Lookup instances in the
    case of Lookup fields. Otherwise, they're the direct values from the
    Attribute table.

    schema_list should be a list of all Schemas that are referenced in
    newsitem_list.

    Note that the list is edited in place; there is no return value.
    """
    from ebpub.db.models import Attribute, Lookup, SchemaField
    # To accomplish this, we determine which NewsItems in ni_list require
    # attribute prepopulation, and run a single DB query that loads all of the
    # attributes. Another way to do this would be to load all of the attributes
    # when loading the NewsItems in the first place (via a JOIN), but we want
    # to avoid joining such large tables.
    preload_schema_ids = set([s.id for s in schema_list if s.uses_attributes_in_list])
    if not preload_schema_ids:
        return
    preloaded_nis = [ni for ni in newsitem_list if ni.schema_id in preload_schema_ids]
    if not preloaded_nis:
        return
    # fmap = {schema_id: {'fields': [(name, real_name)], 'lookups': [real_name1, real_name2]}}
    fmap = {}
    attribute_columns_to_select = set(['news_item'])
    for sf in SchemaField.objects.filter(schema__id__in=[s.id for s in schema_list]).values('schema', 'name', 'real_name', 'is_lookup'):
        fmap.setdefault(sf['schema'], {'fields': [], 'lookups': []})['fields'].append((sf['name'], sf['real_name']))
        if sf['is_lookup']:
            fmap[sf['schema']]['lookups'].append(sf['real_name'])
        attribute_columns_to_select.add(str(sf['real_name']))

    att_dict = dict([(i['news_item'], i) for i in Attribute.objects.filter(news_item__id__in=[ni.id for ni in preloaded_nis]).values(*list(attribute_columns_to_select))])

    # Determine which Lookup objects need to be retrieved.
    lookup_ids = set()
    for ni in preloaded_nis:
        for real_name in fmap[ni.schema_id]['lookups']:
            value = att_dict[ni.id][real_name]
            if ',' in str(value):
                lookup_ids.update(value.split(','))
            else:
                lookup_ids.add(value)

    # Retrieve only the Lookups that are referenced in preloaded_nis.
    lookup_ids = [i for i in lookup_ids if i]
    if lookup_ids:
        lookup_objs = Lookup.objects.in_bulk(lookup_ids)
    else:
        lookup_objs = {}

    # Set 'attribute_values' for each NewsItem in preloaded_nis.
    for ni in preloaded_nis:
        att = att_dict[ni.id]
        att_values = {}
        for field_name, real_name in fmap[ni.schema_id]['fields']:
            value = att[real_name]
            if real_name in fmap[ni.schema_id]['lookups']:
                if real_name.startswith('int'):
                    value = lookup_objs[value]
                else: # Many-to-many lookups are comma-separated strings.
                    value = [lookup_objs[int(i)] for i in value.split(',') if i]
            att_values[field_name] = value
        ni.attribute_values = att_values

def populate_schema(newsitem_list, schema):
    for ni in newsitem_list:
        # TODO: This relies on undocumented Django APIs -- the "_schema_cache" name.
        ni._schema_cache = schema

def today():
    if settings.EB_TODAY_OVERRIDE:
        return settings.EB_TODAY_OVERRIDE
    return datetime.date.today()

########NEW FILE########
__FILENAME__ = views
from django import template
from django.conf import settings
from django.core.cache import cache
from django.http import Http404, HttpResponse, HttpResponseRedirect, HttpResponsePermanentRedirect
from django.shortcuts import render_to_response, get_object_or_404
from django.template.loader import select_template
from django.utils import dateformat, simplejson
from django.utils.datastructures import SortedDict
from django.db.models import Q
from ebgeo.utils.clustering.shortcuts import cluster_newsitems
from ebgeo.utils.clustering.json import ClusterJSON
from ebpub.db import constants
from ebpub.db.models import NewsItem, Schema, SchemaInfo, SchemaField, Lookup, LocationType, Location, SearchSpecialCase
from ebpub.db.models import AggregateDay, AggregateLocation, AggregateLocationDay, AggregateFieldLookup
from ebpub.db.utils import smart_bunches, populate_attributes_if_needed, populate_schema, today
from ebpub.utils.dates import daterange, parse_date
from ebpub.geocoder import SmartGeocoder, AmbiguousResult, DoesNotExist, GeocodingException, InvalidBlockButValidStreet
from ebpub.geocoder.parser.parsing import normalize, ParsingError
from ebpub.preferences.models import HiddenSchema
from ebpub.savedplaces.models import SavedPlace
from ebpub.streets.models import Street, City, Block, Intersection
from ebpub.streets.utils import full_geocode
from ebpub.utils.view_utils import eb_render
from ebpub.metros.allmetros import METRO_DICT, get_metro
import datetime
import random
import re
import urllib

BLOCK_RADIUS_COOKIE_NAME = 'radius'
BLOCK_RADIUS_CHOICES = {'1': .0015, '3': .0035, '8': .007} # number of blocks -> number of geographic degrees
BLOCK_RADIUS_DEFAULT = '8'
HIDE_ADS_COOKIE_NAME = 'h'
HIDE_SCHEMA_INTRO_COOKIE_NAME = 'schemaintro'
DEFAULT_LOCTYPE_SLUG = 'neighborhoods'
POPULATION_SCALE = 1000 # newsitems per POPULATION_SCALE people

################################
# HELPER FUNCTIONS (NOT VIEWS) #
################################

radius_url = lambda radius: '%s-block%s' % (radius, radius != '1' and 's' or '')

def has_staff_cookie(request):
    return request.COOKIES.get(settings.STAFF_COOKIE_NAME) == settings.STAFF_COOKIE_VALUE

def get_schema_manager(request):
    if has_staff_cookie(request):
        return Schema.objects
    else:
        return Schema.public_objects

def block_radius_value(request):
    # Returns a tuple of (xy_radius, block_radius, cookies_to_set).
    if 'radius' in request.GET and request.GET['radius'] in BLOCK_RADIUS_CHOICES:
        block_radius = request.GET['radius']
        cookies_to_set = {BLOCK_RADIUS_COOKIE_NAME: block_radius}
    else:
        if request.COOKIES.get(BLOCK_RADIUS_COOKIE_NAME) in BLOCK_RADIUS_CHOICES:
            block_radius = request.COOKIES[BLOCK_RADIUS_COOKIE_NAME]
        else:
            block_radius = BLOCK_RADIUS_DEFAULT
        cookies_to_set = {}
    return BLOCK_RADIUS_CHOICES[block_radius], block_radius, cookies_to_set

def get_date_chart_agg_model(schemas, start_date, end_date, agg_model, kwargs=None):
    kwargs = kwargs or {}
    counts = {}
    for agg in agg_model.objects.filter(schema__id__in=[s.id for s in schemas], date_part__range=(start_date, end_date), **kwargs):
        counts.setdefault(agg.schema_id, {})[agg.date_part] = agg.total
    return get_date_chart(schemas, start_date, end_date, counts)

def get_date_chart(schemas, start_date, end_date, counts):
    """
    Returns a list that's used to display a date chart for the given
    schemas. Note that start_date and end_date should be datetime.date objects,
    NOT datetime.datetime objects.

    counts should be a nested dictionary: {schema_id: {date: count}}

    The list will be in order given by the `schemas` parameter.
    """
    result = []
    for schema in schemas:
        if schema.id not in counts:
            result.append({
                'schema': schema,
                'dates': [{'date': d, 'count': 0} for d in daterange(start_date, end_date)],
                'max_count': 0,
                'total_count': 0,
                'latest_date': None,
            })
        else:
            dates = [{'date': d, 'count': counts[schema.id].get(d, 0)} for d in daterange(start_date, end_date)]
            nonzero_dates = [d['date'] for d in dates if d['count']]
            if nonzero_dates:
                latest_date = max(nonzero_dates)
            else:
                latest_date = None
            result.append({
                'schema': schema,
                'dates': dates,
                'max_count': max(d['count'] for d in dates),
                'total_count': sum(d['count'] for d in dates),
                'latest_date': latest_date,
            })
    return result

def url_to_place(*args, **kwargs):
    # Given args and kwargs captured from the URL, returns the place.
    # This relies on "place_type" being provided in the URLpattern.
    parse_func = kwargs['place_type'] == 'block' and url_to_block or url_to_location
    return parse_func(*args)

def url_to_block(city_slug, street_slug, from_num, to_num, predir, postdir):
    params = {
        'street_slug': street_slug,
        'predir': (predir and predir.upper() or ''),
        'postdir': (postdir and postdir.upper() or ''),
        'from_num': int(from_num),
        'to_num': int(to_num),
    }
    if city_slug:
        city = City.from_slug(city_slug).norm_name
        city_filter = Q(left_city=city) | Q(right_city=city)
    else:
        city_filter = Q()
    b_list = list(Block.objects.filter(city_filter, **params))

    if not b_list:
        raise Http404()

    return b_list[0]

def url_to_location(type_slug, slug):
    return get_object_or_404(Location.objects.select_related(), location_type__slug=type_slug, slug=slug)

def parse_pid(pid):
    """
    Returns a tuple of (place, block_radius, xy_radius), where block_radius and
    xy_radius are None for Locations.

    PID examples:
        'b:12;1' (block ID 12, 1-block radius)
        'l:32' (location ID 32)
    """
    try:
        place_type, place_id = pid.split(':')
        if place_type == 'b':
            place_id, block_radius = place_id.split('.')
        place_id = int(place_id)
    except (KeyError, ValueError):
        raise Http404('Invalid place')
    if place_type == 'b':
        try:
            xy_radius = BLOCK_RADIUS_CHOICES[block_radius]
        except KeyError:
            raise Http404('Invalid radius')
        return (get_object_or_404(Block, id=place_id), block_radius, xy_radius)
    elif place_type == 'l':
        return (get_object_or_404(Location, id=place_id), None, None)
    else:
        raise Http404

def has_clusters(cluster_dict):
    """
    Determines whether the cluster_dict has any actual clusters.

    Catches the case where a queryset has no items that have been geocoded.

    Cluster dicts have keys which are scales, and values which are lists of
    Bunch objects, so this function simply tests to see if any of the lists are
    not empty.
    """
    def any(iterable):
        for element in iterable:
            if not element:
                return False
        return True

    return any(cluster_dict.values())

def block_bbox(block, radius):
    """
    Assumes `block' has `wkt' attribute
    """
    try:
        from osgeo import ogr
    except ImportError:
        import ogr
    env = ogr.CreateGeometryFromWkt(block.wkt).Buffer(radius).GetEnvelope()
    return (env[0], env[2], env[1], env[3])

def make_search_buffer(geom, block_radius):
    """
    Returns a polygon of a buffer around a block's centroid. `geom'
    should be the centroid of the block. `block_radius' is number of
    blocks.
    """
    return geom.buffer(BLOCK_RADIUS_CHOICES[str(block_radius)]).envelope

##############
# AJAX VIEWS #
##############

def validate_address(request):
    # Validates that request.GET['address'] can be parsed with the address parser.
    if not request.GET.get('address', '').strip():
        raise Http404
    geocoder = SmartGeocoder()
    try:
        result = {'address': geocoder.geocode(request.GET['address'])['address']}
    except (DoesNotExist, ParsingError, InvalidBlockButValidStreet):
        result = {}
    except AmbiguousResult, e:
        if get_metro()['multiple_cities']:
            result = {'addresses': [add['address'] + ', ' + add['city'] for add in e.choices]}
        else:
            result = {'addresses': [add['address'] for add in e.choices]}
    return HttpResponse(simplejson.dumps(result), mimetype="application/javascript")

def ajax_wkt(request):
    # JSON -- returns a list of WKT strings for request.GET['q'].
    # If it can't be geocoded, the list is empty.
    # If it's ambiguous, the list has multiple elements.
    q = request.GET.get('q', '').strip()
    if not q:
        wkt_list = []
    else:
        try:
            result = full_geocode(q)
        except DoesNotExist:
            wkt_list = []
        except Exception:
            wkt_list = []
        else:
            if result['type'] == 'block':
                wkt_list = []
            elif result['type'] in ('location', 'place'):
                if result['ambiguous']:
                    wkt_list = [r.wkt for r in result['result']]
                else:
                    wkt_list = [result['result'].location.wkt]
            elif result['type'] == 'address':
                if result['ambiguous']:
                    wkt_list = [r['point'].wkt for r in result['result']]
                else:
                    wkt_list = [result['result']['point'].wkt]
            else:
                wkt_list = []
    return HttpResponse(simplejson.dumps(wkt_list), mimetype="application/javascript")

def ajax_map_popups(request):
    """
    JSON -- returns a list of lists for request.GET['q'] (a comma-separated
    string of NewsItem IDs).

    The structure of the inner lists is [newsitem_id, popup_html, schema_name]
    """
    try:
        newsitem_ids = map(int, request.GET['q'].split(','))
    except (KeyError, ValueError):
        raise Http404('Invalid query')
    if len(newsitem_ids) >= 400:
        raise Http404('Too many points') # Security measure.
    ni_list = list(NewsItem.objects.filter(id__in=newsitem_ids).select_related().order_by('schema__id'))
    populate_attributes_if_needed(ni_list, list(set([ni.schema for ni in ni_list])))
    result = []
    current_schema = current_template = None
    for ni in ni_list:
        schema = ni.schema
        if current_schema != schema:
            template_list = ['db/snippets/newsitem_list_ungrouped/%s.html' % schema.slug,
                             'db/snippets/newsitem_list/%s.html' % schema.slug,
                             'db/snippets/newsitem_list.html']
            current_template = select_template(template_list)
            current_schema = schema
        html = current_template.render(template.Context({'schema': schema, 'newsitem_list': [ni], 'num_newsitems': 1}))
        result.append([ni.id, html, schema.name[0].upper() + schema.name[1:]])
    return HttpResponse(simplejson.dumps(result), mimetype="application/javascript")

def ajax_place_newsitems(request):
    """
    JSON -- expects request.GET['pid'] and request.GET['s'] (a schema ID).
    """
    try:
        s = Schema.public_objects.get(id=int(request.GET['s']))
    except (KeyError, ValueError, Schema.DoesNotExist):
        raise Http404('Invalid Schema')
    place, block_radius, xy_radius = parse_pid(request.GET.get('pid', ''))
    if isinstance(place, Block):
        search_buffer = make_search_buffer(place.location.centroid, block_radius)
        newsitem_qs = NewsItem.objects.filter(location__bboverlaps=search_buffer)
    else:
        newsitem_qs = NewsItem.objects.filter(newsitemlocation__location__id=place.id)

    # Make the JSON output. Note that we have to call dumps() twice because the
    # bunches are a special case.
    ni_list = list(newsitem_qs.filter(schema__id=s.id).order_by('-item_date')[:50])
    bunches = simplejson.dumps(cluster_newsitems(ni_list, 26), cls=ClusterJSON)
    id_list = simplejson.dumps([ni.id for ni in ni_list])
    return HttpResponse('{"bunches": %s, "ids": %s}' % (bunches, id_list), mimetype="application/javascript")

def ajax_place_lookup_chart(request):
    """
    JSON -- expects request.GET['pid'] and request.GET['sf'] (a SchemaField ID).
    """
    try:
        sf = SchemaField.objects.select_related().get(id=int(request.GET['sf']), schema__is_public=True)
    except (KeyError, ValueError, SchemaField.DoesNotExist):
        raise Http404('Invalid SchemaField')
    place, block_radius, xy_radius = parse_pid(request.GET.get('pid', ''))
    qs = NewsItem.objects.filter(schema__id=sf.schema.id)
    filter_url = place.url()[1:]
    if isinstance(place, Block):
        search_buffer = make_search_buffer(place.location.centroid, block_radius)
        qs = qs.filter(location__bboverlaps=search_buffer)
        filter_url += radius_url(block_radius) + '/'
    else:
        qs = qs.filter(newsitemlocation__location__id=place.id)
    total_count = qs.count()
    top_values = qs.top_lookups(sf, 10)
    return render_to_response('db/snippets/lookup_chart.html', {
        'lookup': {'sf': sf, 'top_values': top_values},
        'total_count': total_count,
        'schema': sf.schema,
        'filter_url': filter_url,
    })

def ajax_place_date_chart(request):
    """
    JSON -- expects request.GET['pid'] and request.GET['s'] (a Schema ID).
    """
    try:
        s = Schema.public_objects.get(id=int(request.GET['s']))
    except (KeyError, ValueError, Schema.DoesNotExist):
        raise Http404('Invalid Schema')
    place, block_radius, xy_radius = parse_pid(request.GET.get('pid', ''))
    qs = NewsItem.objects.filter(schema__id=s.id)
    filter_url = place.url()[1:]
    if isinstance(place, Block):
        search_buffer = make_search_buffer(place.location.centroid, block_radius)
        qs = qs.filter(location__bboverlaps=search_buffer)
        filter_url += radius_url(block_radius) + '/'
    else:
        qs = qs.filter(newsitemlocation__location__id=place.id)
    # TODO: Ignore future dates
    end_date = qs.order_by('-item_date').values('item_date')[0]['item_date']
    start_date = end_date - datetime.timedelta(days=30)
    counts = qs.filter(schema__id=s.id, item_date__gte=start_date, item_date__lte=end_date).date_counts()
    date_chart = get_date_chart([s], end_date - datetime.timedelta(days=30), end_date, {s.id: counts})[0]
    return render_to_response('db/snippets/date_chart.html', {
        'schema': s,
        'date_chart': date_chart,
        'filter_url': filter_url,
    })

def ajax_location_type_list(request):
    loc_types = LocationType.objects.order_by('plural_name').values('id', 'slug', 'plural_name')
    response = HttpResponse(mimetype='application/javascript')
    simplejson.dump(list(loc_types), response)
    return response

def ajax_location_list(request, loc_type_id):
    locations = Location.objects.filter(location_type__pk=loc_type_id, is_public=True).order_by('display_order').values('id', 'slug', 'name')
    if not locations:
        raise Http404()
    response = HttpResponse(mimetype='application/javascript')
    simplejson.dump(list(locations), response)
    return response

def ajax_location(request, loc_id):
    try:
        location = Location.objects.get(pk=int(loc_id))
    except (ValueError, Location.DoesNotExist):
        raise Http404()
    loc_obj = dict([(k, getattr(location, k)) for k in ('name', 'area', 'id', 'normalized_name', 'slug', 'population')])
    loc_obj['wkt'] = location.location.wkt
    loc_obj['centroid'] = location.centroid.wkt
    response = HttpResponse(mimetype='application/javascript')
    simplejson.dump(loc_obj, response)
    return response

#########
# VIEWS #
#########

def homepage(request):
    end_date = today()
    start_date = end_date - datetime.timedelta(days=30)

    sparkline_schemas = list(Schema.public_objects.filter(allow_charting=True, is_special_report=False))

    # Order by slug to ensure case-insensitive ordering. (Kind of hackish.)
    lt_list = LocationType.objects.filter(is_significant=True).order_by('slug').extra(select={'count': 'select count(*) from db_location where is_public=True and location_type_id=db_locationtype.id'})
    street_count = Street.objects.count()
    more_schemas = Schema.public_objects.filter(allow_charting=False).order_by('name')

    # Get the public records.
    date_charts = get_date_chart_agg_model(sparkline_schemas, start_date, end_date, AggregateDay)
    empty_date_charts, non_empty_date_charts = [], []
    for chart in date_charts:
        if chart['total_count']:
            non_empty_date_charts.append(chart)
        else:
            empty_date_charts.append(chart)
    non_empty_date_charts.sort(lambda a, b: cmp(b['total_count'], a['total_count']))
    empty_date_charts.sort(lambda a, b: cmp(a['schema'].plural_name, b['schema'].plural_name))

    # Get the news articles.
    ni_list = NewsItem.objects.select_related().filter(schema__slug='news-articles', item_date__gt=start_date).order_by('-item_date')[:100]
    if ni_list:
        populate_schema(ni_list, ni_list[0].schema)
        populate_attributes_if_needed(ni_list, [ni_list[0].schema])

    article_bunches = cluster_newsitems(ni_list, 26)
    try:
        num_articles = [s for s in date_charts if s['schema'].slug == 'news-articles'][0]['total_count']
    except IndexError:
        num_articles = 0

    # Get the featured neighborhood.
    # This automatically chooses a neighborhood with at least 2 recent news articles.
    neighborhoods_with_articles = AggregateLocationDay.objects.filter(
        schema__slug='news-articles', location_type__slug='neighborhoods', total__gte=2,
        date_part__gte=end_date - datetime.timedelta(days=7),
        date_part__lt=end_date + datetime.timedelta(days=1))

    if neighborhoods_with_articles:
        # First try neighborhoods with 4 or more articles. Fall back to any
        # neighborhood with at least 2 articles.
        fn_ids = [x.location_id for x in neighborhoods_with_articles if x.total >= 4]
        if not fn_ids:
            fn_ids = [x.location_id for x in neighborhoods_with_articles]
        fn = Location.objects.get(id=random.choice(fn_ids))
    else:
        # If no neighborhoods have articles, just pick a random neighborhood.
        try:
            fn = Location.objects.filter(location_type__slug='neighborhoods', is_public=True)[0]
        except IndexError:
            # If no neighborhoods have been added to the DB yet, that's fine.
            fn = None

    if fn is not None:
        # Get the featured neighborhood news articles.
        qs = NewsItem.objects.filter(schema__slug='news-articles', newsitemlocation__location__id=fn.id, item_date__gte=start_date, item_date__lte=end_date)
        fn_article_count = qs.count()
        fn_all_articles = list(qs.select_related().order_by('-item_date')[:10])
        fn_articles = []
        if fn_all_articles:
            populate_attributes_if_needed(fn_all_articles, [fn_all_articles[0].schema])
            # Remove any articles whose headlines, excerpts or sources are duplicate.
            headlines, excerpts, sources = set(), set(), set()
            for a in fn_all_articles:
                # Calculate a hash of the excerpt by removing spaces. We use this
                # for duplicate comparison.
                excerpt_hash = re.sub(r'\s', '', a.attribute_values['excerpt'])
                if a.title not in headlines and excerpt_hash not in excerpts and a.attribute_values['source'].name not in sources:
                    headlines.add(a.title)
                    excerpts.add(excerpt_hash)
                    sources.add(a.attribute_values['source'].name)
                    fn_articles.append(a)
                    if len(fn_articles) == 3:
                        break

        # Get the featured neighborhood public records.
        fn_date_charts = get_date_chart_agg_model(sparkline_schemas, start_date, end_date, AggregateLocationDay, kwargs={'location__id': fn.id})
        fn_empty_date_charts, fn_non_empty_date_charts = [], []
        for chart in fn_date_charts:
            if chart['total_count']:
                fn_non_empty_date_charts.append(chart)
            else:
                fn_empty_date_charts.append(chart)
        fn_non_empty_date_charts.sort(lambda a, b: cmp(b['total_count'], a['total_count']))
        fn_empty_date_charts.sort(lambda a, b: cmp(a['schema'].plural_name, b['schema'].plural_name))
    else:
        fn_articles = fn_article_count = fn_non_empty_date_charts = fn_empty_date_charts = None

    return eb_render(request, 'homepage.html', {
        'location_type_list': lt_list,
        'street_count': street_count,
        'all_bunches': simplejson.dumps(article_bunches, cls=ClusterJSON),
        'newsitem_list': ni_list,
        'more_schemas': more_schemas,
        'featured_neighborhood': fn,
        'featured_neighborhood_articles': fn_articles,
        'featured_neighborhood_article_count': fn_article_count,
        'num_articles': num_articles,
        'non_empty_date_charts': non_empty_date_charts,
        'empty_date_charts': empty_date_charts,
        'fn_non_empty_date_charts': fn_non_empty_date_charts,
        'fn_empty_date_charts': fn_empty_date_charts,
    })

def search(request, schema_slug=''):
    "Performs a location search and redirects to the address/xy page."
    # Check whether a schema was provided.
    if schema_slug:
        try:
            schema = get_schema_manager(request).get(slug=schema_slug)
        except Schema.DoesNotExist:
            raise Http404('Schema does not exist')
        url_prefix = schema.url()[:-1]
    else:
        schema = None
        url_prefix = ''

    # Get the query.
    q = request.GET.get('q', '').strip()
    if not q:
        return HttpResponseRedirect(url_prefix + '/') # TODO: Do something better than redirecting.

    # For /search/?type=alert, we redirect results to the alert page, not the
    # place page.
    if request.GET.get('type', '') == 'alert':
        url_method = 'alert_url'
    else:
        url_method = 'url'

    # Try to geocode it using full_geocode().
    try:
        result = full_geocode(q, search_places=False)
    except: # TODO: Naked except clause.
        pass
    else:
        if result['ambiguous']:
            if result['type'] == 'block':
                return eb_render(request, 'db/search_invalid_block.html', {
                    'query': q,
                    'choices': result['result'],
                    'street_name': result['street_name'],
                    'block_number': result['block_number']
                })
            else:
                return eb_render(request, 'db/did_you_mean.html', {'query': q, 'choices': result['result']})
        elif result['type'] == 'location':
            return HttpResponseRedirect(url_prefix + getattr(result['result'], url_method)())
        elif result['type'] == 'address':
            # Block
            if result['result']['block']:
                return HttpResponseRedirect(url_prefix + getattr(result['result']['block'], url_method)())
            # Intersection
            try:
                intersection = Intersection.objects.get(id=result['result']['intersection_id'])
            except Intersection.DoesNotExist:
                pass
            else:
                return HttpResponseRedirect(url_prefix + getattr(intersection, url_method)())

    # Failing the geocoding, look in the special-case table.
    try:
        special_case = SearchSpecialCase.objects.get(query=normalize(q))
    except SearchSpecialCase.DoesNotExist:
        pass
    else:
        if special_case.redirect_to:
            return HttpResponseRedirect(special_case.redirect_to)
        else:
            return eb_render(request, 'db/search_special_case.html', {'query': q, 'special_case': special_case})

    # Failing that, display a list of ZIP codes if this looks like a ZIP.
    if re.search(r'^\s*\d{5}(?:-\d{4})?\s*$', q):
        z_list = Location.objects.filter(location_type__slug='zipcodes', is_public=True).select_related().order_by('name')
        if z_list:
            return eb_render(request, 'db/search_error_zip_list.html', {'query': q, 'zipcode_list': z_list})

    # Failing all of that, display the search error page.
    lt_list = LocationType.objects.filter(is_significant=True).order_by('name')
    return eb_render(request, 'db/search_error.html', {'query': q, 'locationtype_list': lt_list})

def newsitem_detail(request, schema_slug, year, month, day, newsitem_id):
    try:
        date = datetime.date(int(year), int(month), int(day))
    except ValueError:
        raise Http404('Invalid day')
    ni = get_object_or_404(NewsItem.objects.select_related(), id=newsitem_id)

    if ni.schema.slug != schema_slug or ni.item_date != date:
        raise Http404
    if not ni.schema.is_public and not has_staff_cookie(request):
        raise Http404('Not public')

    if not ni.schema.has_newsitem_detail:
        return HttpResponsePermanentRedirect(ni.url)

    atts = ni.attributes_for_template()
    has_location = ni.location is not None

    if has_location:
        locations_within = Location.objects.select_related().filter(location__intersects=ni.location)
    else:
        locations_within = ()

    hide_ads = (request.COOKIES.get(HIDE_ADS_COOKIE_NAME) == 't')

    templates_to_try = ('db/newsitem_detail/%s.html' % ni.schema.slug, 'db/newsitem_detail.html')
    if 'new' in request.GET: # TODO: Remove this after feature is implemented.
        templates_to_try = ('db/newsitem_detail_new.html',) + templates_to_try
    return eb_render(request, templates_to_try, {
        'newsitem': ni,
        'attribute_list': [att for att in atts if att.sf.display],
        'attribute_dict': dict((att.sf.name, att) for att in atts),
        'has_location': has_location,
        'locations_within': locations_within,
        'hide_ads': hide_ads,
    })

def schema_list(request):
    schemainfo_list = SchemaInfo.objects.select_related().filter(schema__is_public=True, schema__is_special_report=False).order_by('schema__plural_name')
    schemafield_list = list(SchemaField.objects.filter(is_filter=True).order_by('display_order'))
    browsable_locationtype_list = LocationType.objects.filter(is_significant=True)

    # Populate s_list, which contains a schema and schemafield list for each schema.
    s_list = []
    for s in schemainfo_list:
        s_list.append({
            'schema': s.schema,
            'schemainfo': s,
            'schemafield_list': [sf for sf in schemafield_list if sf.schema_id == s.schema_id],
        })

    return eb_render(request, 'db/schema_list.html', {
        'schema_list': s_list,
        'browsable_locationtype_list': browsable_locationtype_list,
    })

def schema_detail(request, slug):
    s = get_object_or_404(get_schema_manager(request), slug=slug)
    try:
        si = SchemaInfo.objects.get(schema__id=s.id)
    except SchemaInfo.DoesNotExist:
        si = None

    if s.is_special_report:
        return schema_detail_special_report(request, s, si)

    location_type_list = LocationType.objects.filter(is_significant=True).order_by('slug')

    if s.allow_charting:
        # For the date range, the end_date is the last non-future date
        # with at least one NewsItem.
        try:
            end_date = NewsItem.objects.filter(schema__id=s.id, item_date__lte=today()).values_list('item_date', flat=True).order_by('-item_date')[0]
        except IndexError:
            latest_dates = date_chart = ()
            start_date = end_date = None
        else:
            start_date = end_date - datetime.timedelta(days=constants.NUM_DAYS_AGGREGATE)
            date_chart = get_date_chart_agg_model([s], start_date, end_date, AggregateDay)[0]
            latest_dates = [date['date'] for date in date_chart['dates'] if date['count']]

        # Populate schemafield_list and lookup_list.
        schemafield_list = list(s.schemafield_set.filter(is_filter=True).order_by('display_order'))
        LOOKUP_MIN_DISPLAYED = 7
        LOOKUP_BUFFER = 4
        lookup_list = []
        for sf in schemafield_list:
            if not (sf.is_charted and sf.is_lookup):
                continue
            top_values = list(AggregateFieldLookup.objects.filter(schema_field__id=sf.id).select_related('lookup').order_by('-total')[:LOOKUP_MIN_DISPLAYED + LOOKUP_BUFFER])
            if len(top_values) == LOOKUP_MIN_DISPLAYED + LOOKUP_BUFFER:
                top_values = top_values[:LOOKUP_MIN_DISPLAYED]
                has_more = True
            else:
                has_more = False
            lookup_list.append({'sf': sf, 'top_values': top_values, 'has_more': has_more})

        location_chartfield_list = []

        unknowns = AggregateLocation.objects.filter(
                schema__id=s.id,
                location__slug='unknown').select_related('location')
        unknown_dict = dict([(u.location_type_id, u.total) for u in unknowns])

        # Populate location_chartfield_list.
        for lt in location_type_list:
            # Collect the locations in the location_type here so we don't have
            # to query them again when grouping them with the newsitem totals
            locations = dict([(loc.id, loc) for loc in lt.location_set.iterator()])

            ni_totals = AggregateLocation.objects.filter(
                schema__id=s.id,
                location_type__id=lt.id,
                location__is_public=True).select_related('location').order_by('-total')

            if ni_totals:
                location_chartfield_list.append({'location_type': lt, 'locations': ni_totals[:9], 'unknown': unknown_dict.get(lt.id, 0)})
        ni_list = ()
    else:
        latest_dates = schemafield_list = date_chart = lookup_list = location_chartfield_list = ()
        ni_list = list(NewsItem.objects.filter(schema__id=s.id).order_by('-item_date')[:30])
        populate_schema(ni_list, s)
        populate_attributes_if_needed(ni_list, [s])

    textsearch_sf_list = list(SchemaField.objects.filter(schema__id=s.id, is_searchable=True).order_by('display_order'))
    boolean_lookup_list = [sf for sf in SchemaField.objects.filter(schema__id=s.id, is_filter=True, is_lookup=False).order_by('display_order') if sf.is_type('bool')]

    templates_to_try = ('db/schema_detail/%s.html' % s.slug, 'db/schema_detail.html')

    # The HIDE_SCHEMA_INTRO_COOKIE_NAME cookie is a comma-separated list of
    # schema IDs for schemas whose intro text should *not* be displayed.
    hide_intro = str(s.id) in request.COOKIES.get(HIDE_SCHEMA_INTRO_COOKIE_NAME, '').split(',')

    return eb_render(request, templates_to_try, {
        'schema': s,
        'schemainfo': si,
        'schemafield_list': schemafield_list,
        'location_type_list': location_type_list,
        'date_chart': date_chart,
        'lookup_list': lookup_list,
        'location_chartfield_list': location_chartfield_list,
        'boolean_lookup_list': boolean_lookup_list,
        'search_list': textsearch_sf_list,
        'newsitem_list': ni_list,
        'latest_dates': latest_dates[-3:],
        'hide_intro': hide_intro,
        'hide_intro_cookie_name': HIDE_SCHEMA_INTRO_COOKIE_NAME,
        'start_date': s.min_date,
        'end_date': today(),
    })

def schema_detail_special_report(request, schema, schemainfo):
    ni_list = NewsItem.objects.filter(schema__id=schema.id)
    populate_schema(ni_list, schema)
    populate_attributes_if_needed(ni_list, [schema])
    bunches = cluster_newsitems(ni_list, 26)

    if schema.allow_charting:
        browsable_locationtype_list = LocationType.objects.filter(is_significant=True)
        schemafield_list = list(schema.schemafield_set.filter(is_filter=True).order_by('display_order'))
    else:
        browsable_locationtype_list = []
        schemafield_list = []

    templates_to_try = ('db/schema_detail/%s.html' % schema.slug, 'db/schema_detail_special_report.html')
    return eb_render(request, templates_to_try, {
        'schema': schema,
        'schemainfo': schemainfo,
        'newsitem_list': ni_list,
        'nothing_geocoded': not has_clusters(bunches),
        'all_bunches': simplejson.dumps(bunches, cls=ClusterJSON),
        'browsable_locationtype_list': browsable_locationtype_list,
        'schemafield_list': schemafield_list,
    })

def schema_about(request, slug):
    s = get_object_or_404(get_schema_manager(request), slug=slug)
    si = get_object_or_404(SchemaInfo, schema__id=s.id)
    return eb_render(request, 'db/schema_about.html', {'schemainfo': si, 'schema': s})

def schema_filter(request, slug, urlbits):
    # Due to the way our custom filter UI works, address, date and text
    # searches come in a query string instead of in the URL. Here, we validate
    # those searches and do a redirect so that the address and date are in
    # urlbits.
    if request.GET.get('address', '').strip():
        xy_radius, block_radius, cookies_to_set = block_radius_value(request)
        address = request.GET['address'].strip()
        result = None
        try:
            result = SmartGeocoder().geocode(address)
        except AmbiguousResult, e:
            address_choices = e.choices
        except (GeocodingException, ParsingError):
            address_choices = ()
        if result:
            if result['block']:
                new_url = request.path + result['block'].url()[1:] + radius_url(block_radius) + '/'
            elif result['intersection']:
                new_url = request.path + result['intersection'].url()[1:] + radius_url(block_radius) + '/'
            else:
                raise NotImplementedError('Reached invalid geocoding type: %r' % result)
            return HttpResponseRedirect(new_url)
        else:
            return eb_render(request, 'db/filter_bad_address.html', {
                'address_choices': address_choices,
                'address': address,
                'radius': block_radius,
                'radius_url': radius_url(block_radius),
            })
    if request.GET.get('start_date', '').strip() and request.GET.get('end_date', '').strip():
        try:
            start_date = parse_date(request.GET['start_date'], '%m/%d/%Y')
            end_date = parse_date(request.GET['end_date'], '%m/%d/%Y')
        except ValueError:
            return HttpResponseRedirect('../')
        if start_date.year < 1900 or end_date.year < 1900:
            # This prevents strftime from throwing a ValueError.
            raise Http404('Dates before 1900 are not supported.')
        new_url = request.path + '%s,%s' % (start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d')) + '/'
        return HttpResponseRedirect(new_url)
    if request.GET.get('textsearch', '').strip() and request.GET.get('q', '').strip():
        new_url = request.path + 'by-%s/%s/' % (request.GET['textsearch'], urllib.quote(request.GET['q']))
        return HttpResponseRedirect(new_url)

    s = get_object_or_404(get_schema_manager(request), slug=slug, is_special_report=False)
    if not s.allow_charting:
        return HttpResponsePermanentRedirect(s.url())
    filter_sf_list = list(SchemaField.objects.filter(schema__id=s.id, is_filter=True).order_by('display_order'))
    textsearch_sf_list = list(SchemaField.objects.filter(schema__id=s.id, is_searchable=True).order_by('display_order'))

    # Use SortedDict to preserve the display_order.
    filter_sf_dict = SortedDict([(sf.slug, sf) for sf in filter_sf_list] + [(sf.slug, sf) for sf in textsearch_sf_list])

    # Create the initial QuerySet of NewsItems.
    start_date = s.min_date
    end_date = today()
    qs = NewsItem.objects.filter(schema__id=s.id, item_date__lte=end_date).order_by('-item_date')

    lookup_descriptions = []

    # urlbits is a string describing the filters (or None, in the case of
    # "/filter/"). Cycle through them to see which ones are valid.
    urlbits = urlbits or ''
    urlbits = filter(None, urlbits.split('/')[::-1]) # Reverse them, so we can use pop().
    filters = SortedDict()
    date_filter_applied = location_filter_applied = False
    while urlbits:
        bit = urlbits.pop()

        # Date range
        if bit == 'by-date' or bit == 'by-pub-date':
            if date_filter_applied:
                raise Http404('Only one date filter can be applied')
            try:
                date_range = urlbits.pop()
            except IndexError:
                raise Http404('Missing date range')
            try:
                start_date, end_date = date_range.split(',')
                start_date = datetime.date(*map(int, start_date.split('-')))
                end_date = datetime.date(*map(int, end_date.split('-')))
            except (IndexError, ValueError, TypeError):
                raise Http404('Missing or invalid date range')
            if bit == 'by-date':
                date_field_name = 'item_date'
                label = s.date_name
            else:
                date_field_name = 'pub_date'
                label = 'date published'
            gte_kwarg = '%s__gte' % date_field_name
            lt_kwarg = '%s__lt' % date_field_name
            kwargs = {
                gte_kwarg: start_date,
                lt_kwarg: end_date+datetime.timedelta(days=1)
            }
            qs = qs.filter(**kwargs)
            if start_date == end_date:
                value = dateformat.format(start_date, 'N j, Y')
            else:
                value = u'%s \u2013 %s' % (dateformat.format(start_date, 'N j, Y'), dateformat.format(end_date, 'N j, Y'))
            filters['date'] = {'name': 'date', 'label': label, 'short_value': value, 'value': value, 'url': '%s/%s,%s' % (bit, start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'))}
            date_filter_applied = True

        # Lookup
        elif bit.startswith('by-'):
            sf_slug = bit[3:]
            try:
                # Pop it so that we can't get subsequent lookups for this SchemaField.
                sf = filter_sf_dict.pop(sf_slug)
            except KeyError:
                raise Http404('Invalid SchemaField slug')
            if sf.is_lookup:
                if urlbits:
                    look = get_object_or_404(Lookup, schema_field__id=sf.id, slug=urlbits.pop())
                    qs = qs.by_attribute(sf, look.id)
                    if look.description:
                        lookup_descriptions.append(look)
                    filters[sf.name] = {'name': sf.name, 'label': sf.pretty_name, 'short_value': look.name, 'value': look.name, 'url': 'by-%s/%s' % (sf.slug, look.slug)}
                else: # List of available lookups.
                    lookup_list = Lookup.objects.filter(schema_field__id=sf.id).order_by('name')
                    filters['lookup'] = {'name': 'lookup', 'label': None, 'value': 'By ' + sf.pretty_name, 'url': None}
                    return eb_render(request, 'db/filter_lookup_list.html', {
                        'schema': s,
                        'filters': filters,
                        'lookup_type': sf.pretty_name,
                        'lookup_list': lookup_list,
                    })
            elif sf.is_type('bool'): # Boolean field.
                if urlbits:
                    slug = urlbits.pop()
                    try:
                        real_val = {'yes': True, 'no': False, 'na': None}[slug]
                    except KeyError:
                        raise Http404('Invalid boolean field URL')
                    qs = qs.by_attribute(sf, real_val)
                    value = {True: 'Yes', False: 'No', None: 'N/A'}[real_val]
                    filters[sf.name] = {'name': sf.name, 'label': sf.pretty_name, 'short_value': value, 'value': u'%s%s: %s' % (sf.pretty_name[0].upper(), sf.pretty_name[1:], value), 'url': 'by-%s/%s' % (sf.slug, slug)}
                else:
                    filters['lookup'] = {'name': sf.name, 'label': None, 'value': u'By whether they ' + sf.pretty_name_plural, 'url': None}
                    return eb_render(request, 'db/filter_lookup_list.html', {
                        'schema': s,
                        'filters': filters,
                        'lookup_type': u'whether they ' + sf.pretty_name_plural,
                        'lookup_list': [{'slug': 'yes', 'name': 'Yes'}, {'slug': 'no', 'name': 'No'}, {'slug': 'na', 'name': 'N/A'}],
                    })
            else: # Text-search field.
                if not urlbits:
                    raise Http404('Text search lookup requires search params')
                query = urlbits.pop()
                qs = qs.text_search(sf, query)
                filters[sf.name] = {'name': sf.name, 'label': sf.pretty_name, 'short_value': query, 'value': query, 'url': 'by-%s/%s' % (sf.slug, query)}

        # Street/address
        elif bit.startswith('streets'):
            if location_filter_applied:
                raise Http404('Only one location filter can be applied')
            try:
                if get_metro()['multiple_cities']:
                    city_slug = urlbits.pop()
                else:
                    city_slug = ''
                street_slug = urlbits.pop()
                block_range = urlbits.pop()
            except IndexError:
                raise Http404()
            try:
                block_radius = urlbits.pop()
            except IndexError:
                xy_radius, block_radius, cookies_to_set = block_radius_value(request)
                return HttpResponseRedirect(request.path + radius_url(block_radius) + '/')
            m = re.search('^%s$' % constants.BLOCK_URL_REGEX, block_range)
            if not m:
                raise Http404('Invalid block URL')
            block = url_to_block(city_slug, street_slug, *m.groups())
            m = re.search(r'^(\d)-blocks?$', block_radius)
            if not m:
                raise Http404('Invalid block radius')
            block_radius = m.group(1)
            if block_radius not in BLOCK_RADIUS_CHOICES:
                raise Http404('Invalid block radius')
            search_buffer = make_search_buffer(block.location.centroid, block_radius)
            qs = qs.filter(location__bboverlaps=search_buffer)
            value = '%s block%s around %s' % (block_radius, (block_radius != '1' and 's' or ''), block.pretty_name)
            filters['location'] = {
                'name': 'location',
                'label': 'Area',
                'short_value': value,
                'value': value,
                'url': block.url()[1:] + radius_url(block_radius),
                'location_name': block.pretty_name,
                'location_object': block,
            }
            location_filter_applied = True

        # Location
        elif bit.startswith('locations'):
            if location_filter_applied:
                raise Http404('Only one location filter can be applied')
            if not urlbits:
                raise Http404()
            location_type_slug = urlbits.pop()
            if urlbits:
                loc = url_to_location(location_type_slug, urlbits.pop())
                qs = qs.filter(newsitemlocation__location__id=loc.id)
                filters['location'] = {
                    'name': 'location',
                    'label': loc.location_type.name,
                    'short_value': loc.name,
                    'value': loc.name,
                    'url': 'locations/%s/%s' % (location_type_slug, loc.slug),
                    'location_name': loc.name,
                    'location_object': loc,
                }
                location_filter_applied = True
            else: # List of available locations for this location type.
                lookup_list = Location.objects.filter(location_type__slug=location_type_slug, is_public=True).order_by('display_order')
                if not lookup_list:
                    raise Http404()
                location_type = lookup_list[0].location_type
                filters['location'] = {'name': 'location', 'label': None, 'value': 'By ' + location_type.name, 'url': None}
                return eb_render(request, 'db/filter_lookup_list.html', {
                    'schema': s,
                    'filters': filters,
                    'lookup_type': location_type.name,
                    'lookup_list': lookup_list,
                })

        else:
            raise Http404('Invalid filter type')

    # Get the list of top values for each lookup that isn't being filtered-by.
    # LOOKUP_MIN_DISPLAYED sets the number of records to display for each lookup
    # type. Normally, the UI displays a "See all" link, but the link is removed
    # if there are fewer than (LOOKUP_MIN_DISPLAYED + LOOKUP_BUFFER) records.
    LOOKUP_MIN_DISPLAYED = 7
    LOOKUP_BUFFER = 4
    lookup_list, boolean_lookup_list, search_list = [], [], []
    for sf in filter_sf_dict.values():
        if sf.is_searchable:
            search_list.append(sf)
        elif sf.is_type('bool'):
            boolean_lookup_list.append(sf)
        elif sf.is_lookup:
            top_values = AggregateFieldLookup.objects.filter(schema_field__id=sf.id).select_related('lookup').order_by('-total')[:LOOKUP_MIN_DISPLAYED+LOOKUP_BUFFER]
            if len(top_values) == LOOKUP_MIN_DISPLAYED + LOOKUP_BUFFER:
                top_values = top_values[:LOOKUP_MIN_DISPLAYED]
                has_more = True
            else:
                has_more = False
            lookup_list.append({'sf': sf, 'top_values': top_values, 'has_more': has_more})

    # Get the list of LocationTypes if a location filter has *not* been applied.
    if location_filter_applied:
        location_type_list = []
    else:
        location_type_list = LocationType.objects.filter(is_significant=True).order_by('slug')

    # Do the pagination. We don't use Django's Paginator class because it uses
    # SELECT COUNT(*), which we want to avoid.
    try:
        page = int(request.GET.get('page', '1'))
    except ValueError:
        raise Http404('Invalid page')
    idx_start = (page - 1) * constants.FILTER_PER_PAGE
    idx_end = page * constants.FILTER_PER_PAGE

    # Get one extra, so we can tell whether there's a next page.
    ni_list = list(qs[idx_start:idx_end+1])
    if page > 1 and not ni_list:
        raise Http404('No objects on page %s' % page)
    if len(ni_list) > constants.FILTER_PER_PAGE:
        has_next = True
        ni_list = ni_list[:-1]
    else:
        has_next = False
        idx_end = idx_start + len(ni_list)
    has_previous = page > 1

    populate_schema(ni_list, s)
    populate_attributes_if_needed(ni_list, [s])
    bunches = cluster_newsitems(ni_list, 26)

    return eb_render(request, 'db/filter.html', {
        'schema': s,
        'newsitem_list': ni_list,

        # Pagination stuff
        'has_next': has_next,
        'has_previous': has_previous,
        'page_number': page,
        'previous_page_number': page - 1,
        'next_page_number': page + 1,
        'page_start_index': idx_start + 1,
        'page_end_index': idx_end,

        'nothing_geocoded': not has_clusters(bunches),
        'all_bunches': simplejson.dumps(bunches, cls=ClusterJSON),
        'lookup_list': lookup_list,
        'boolean_lookup_list': boolean_lookup_list,
        'search_list': search_list,
        'location_type_list': location_type_list,
        'filters': filters,
        'date_filter_applied': date_filter_applied,
        'location_filter_applied': location_filter_applied,
        'lookup_descriptions': lookup_descriptions,
        'start_date': start_date,
        'end_date': end_date,
    })

def location_type_detail(request, slug):
    lt = get_object_or_404(LocationType, slug=slug)
    order_by = get_metro()['multiple_cities'] and ('city', 'display_order') or ('display_order',)
    loc_list = Location.objects.filter(location_type__id=lt.id, is_public=True).order_by(*order_by)
    lt_list = [{'location_type': i, 'is_current': i == lt} for i in LocationType.objects.filter(is_significant=True).order_by('plural_name')]
    return eb_render(request, 'db/location_type_detail.html', {
        'location_type': lt,
        'location_list': loc_list,
        'location_type_list': lt_list,
    })

def city_list(request):
    c_list = [City.from_norm_name(c['city']) for c in Street.objects.distinct().values('city').order_by('city')]
    return eb_render(request, 'db/city_list.html', {'city_list': c_list})

def street_list(request, city_slug):
    city = city_slug and City.from_slug(city_slug) or None
    kwargs = city_slug and {'city': city.norm_name} or {}
    streets = list(Street.objects.filter(**kwargs).order_by('street', 'suffix'))
    if not streets:
        raise Http404('This city has no streets')
    return eb_render(request, 'db/street_list.html', {
        'street_list': streets,
        'city': city,
    })

def block_list(request, city_slug, street_slug):
    city = city_slug and City.from_slug(city_slug) or None
    kwargs = {'street_slug': street_slug}
    if city_slug:
        city_filter = Q(left_city=city.norm_name) | Q(right_city=city.norm_name)
    else:
        city_filter = Q()
    blocks = Block.objects.filter(city_filter, **kwargs).order_by('postdir', 'predir', 'from_num', 'to_num')
    if not blocks:
        raise Http404
    return eb_render(request, 'db/block_list.html', {
        'block_list': blocks,
        'first_block': blocks[0],
        'city': city,
    })

def generic_place_page(request, template_name, place, extra_context=None):
    extra_context = extra_context or {}
    if place.location is None:
        is_block = False
        place_wkt = ''
    elif isinstance(place, Block):
        is_block = True
        place_wkt = ''
    else:
        is_block = False
        place_wkt = place.location.simplify(tolerance=0.001, preserve_topology=True)
    return eb_render(request, template_name, dict(place=place, is_block=is_block, place_wkt=place_wkt, **extra_context))

def place_detail(request, *args, **kwargs):
    schema_manager = get_schema_manager(request)
    place = url_to_place(*args, **kwargs)
    cookies_to_set, bbox = {}, None
    if place.location is None:
        newsitem_qs = NewsItem.objects.filter(newsitemlocation__location__id=place.id)
        bbox = get_metro()['extent']
        block_radius = None
        nearby_locations = []
        saved_place_lookup = {'location__id': place.id}
    elif isinstance(place, Block):
        xy_radius, block_radius, cookies_to_set = block_radius_value(request)
        search_buf = make_search_buffer(place.location.centroid, block_radius)
        newsitem_qs = NewsItem.objects.filter(location__bboverlaps=search_buf)
        nearby_locations = list(Location.objects.filter(location_type__is_significant=True, location__bboverlaps=search_buf).select_related())
        bbox = search_buf.extent
        saved_place_lookup = {'block__id': place.id}
    else:
        newsitem_qs = NewsItem.objects.filter(newsitemlocation__location__id=place.id)
        search_buf = make_search_buffer(place.location.centroid, 3)
        nearby_locations = Location.objects.filter(location_type__is_significant=True).select_related().exclude(id=place.id).order_by('location_type__id').filter(location__bboverlaps=search_buf)
        bbox = search_buf.extent
        block_radius = None
        saved_place_lookup = {'location__id': place.id}

    # Determine whether this is a saved place.
    is_saved = False
    if request.user:
        saved_place_lookup['user_id'] = request.user.id # TODO: request.user.id should not do a DB lookup
        is_saved = SavedPlace.objects.filter(**saved_place_lookup).count()

    if kwargs.get('detail_page', False):
        is_latest_page = True
        # Check the query string for the max date to use. Otherwise, fall
        # back to today.
        end_date = today()
        if 'start' in request.GET:
            try:
                end_date = parse_date(request.GET['start'], '%m/%d/%Y')
                is_latest_page = False
            except ValueError:
                raise Http404

        # As an optimization, limit the NewsItems to those published in the
        # last few days.
        start_date = end_date - datetime.timedelta(days=constants.LOCATION_DAY_OPTIMIZATION)
        ni_list = newsitem_qs.filter(pub_date__gt=start_date-datetime.timedelta(days=1), pub_date__lt=end_date+datetime.timedelta(days=1)).select_related()
        if not has_staff_cookie(request):
            ni_list = ni_list.filter(schema__is_public=True)
        ni_list = ni_list.extra(
            select={'pub_date_date': 'date(db_newsitem.pub_date)'},
            order_by=('-pub_date_date', '-schema__importance', 'schema')
        )[:constants.NUM_NEWS_ITEMS_PLACE_DETAIL]
        ni_list = smart_bunches(list(ni_list), max_days=5, max_items_per_day=100)
        schemas_used = list(set([ni.schema for ni in ni_list]))
        s_list = schema_manager.filter(is_special_report=False, allow_charting=True).order_by('plural_name')
        populate_attributes_if_needed(ni_list, schemas_used)
        bunches = cluster_newsitems(ni_list, 26)
        if ni_list:
            next_day = ni_list[-1].pub_date - datetime.timedelta(days=1)
        else:
            next_day = None

        hidden_schema_list = []
        if request.user is not None:
            hidden_schema_list = [o.schema for o in HiddenSchema.objects.filter(user_id=request.user.id)]

        context = {
            'newsitem_list': ni_list,
            'nothing_geocoded': not has_clusters(bunches),
            'all_bunches': simplejson.dumps(bunches, cls=ClusterJSON),
            'next_day': next_day,
            'is_latest_page': is_latest_page,
            'hidden_schema_list': hidden_schema_list,
        }
        template_name = 'db/place_detail.html'
    else:
        # Here, the goal is to get the latest nearby NewsItems for each
        # schema. A naive way to do this would be to run the query once for
        # each schema, but we improve on that by grabbing the latest 300
        # items of ANY schema and hoping that several of the schemas include
        # all of their recent items in that list. Then, for any remaining
        # schemas, we do individual queries as a last resort.
        # Note that we iterate over the 300 NewsItems as the outer loop rather
        # than iterating over the schemas as the outer loop, because there are
        # many more NewsItems than schemas.
        s_list = SortedDict([(s.id, [s, [], 0]) for s in schema_manager.filter(is_special_report=False).order_by('plural_name')])
        needed = set(s_list.keys())
        for ni in newsitem_qs.order_by('-item_date', '-id')[:300]: # Ordering by ID ensures consistency across page views.
            s_id = ni.schema_id
            if s_id in needed:
                s_list[s_id][1].append(ni)
                s_list[s_id][2] += 1
                if s_list[s_id][2] == s_list[s_id][0].number_in_overview:
                    needed.remove(s_id)
        sf_dict = {}
        for sf in SchemaField.objects.filter(is_lookup=True, is_charted=True, schema__is_public=True, schema__is_special_report=False).values('id', 'schema_id', 'pretty_name').order_by('schema__id', 'display_order'):
            sf_dict.setdefault(sf['schema_id'], []).append(sf)
        schema_blocks, all_newsitems = [], []
        for s, newsitems, _ in s_list.values():
            if s.id in needed:
                newsitems = list(newsitem_qs.filter(schema__id=s.id).order_by('-item_date', '-id')[:s.number_in_overview])
            populate_schema(newsitems, s)
            schema_blocks.append({
                'schema': s,
                'latest_newsitems': newsitems,
                'has_newsitems': bool(newsitems),
                'lookup_charts': sf_dict.get(s.id),
            })
            all_newsitems.extend(newsitems)
        s_list = [s[0] for s in s_list.values()]
        populate_attributes_if_needed(all_newsitems, s_list)
        s_list = [s for s in s_list if s.allow_charting]
        context = {'schema_blocks': schema_blocks}
        template_name = 'db/place_overview.html'

    context.update({
        'nearby_locations': nearby_locations,
        'block_radius': block_radius,
        'bbox': bbox,
        'filtered_schema_list': s_list,
        'filter_map_by_schema': 'filter' in request.GET,
        'is_saved': is_saved,
    })

    response = generic_place_page(request, template_name, place, context)
    if cookies_to_set:
        for k, v in cookies_to_set.items():
            response.set_cookie(k, v)
    return response

def feed_signup(request, *args, **kwargs):
    place = url_to_place(*args, **kwargs)
    s_list = get_schema_manager(request).filter(is_special_report=False).order_by('plural_name')
    return generic_place_page(request, 'db/feed_signup.html', place, {'schema_list': s_list})

########NEW FILE########
__FILENAME__ = base
from django.core.exceptions import ObjectDoesNotExist
from django.db.models import Q
from ebpub.geocoder.parser.parsing import normalize, parse, ParsingError
from ebpub.geocoder.models import GeocoderCache
from ebpub.streets.models import Block, StreetMisspelling, Intersection
import re

block_re = re.compile(r'^(\d+)[-\s]+(?:blk|block)\s+(?:of\s+)?(.*)$', re.IGNORECASE)
intersection_re = re.compile(r'(?<=.) (?:and|\&|at|near|@|around|towards?|off|/|(?:just )?(?:north|south|east|west) of|(?:just )?past) (?=.)', re.IGNORECASE)
# segment_re = re.compile(r'^.{1,40}?\b(?:between .{1,40}? and|from .{1,40}? to) .{1,40}?$', re.IGNORECASE) # TODO

class GeocodingException(Exception):
    pass

class AmbiguousResult(GeocodingException):
    def __init__(self, choices, message=None):
        self.choices = choices
        if message is None:
            message = "Address DB returned %s results" % len(choices)
        self.message = message

    def __str__(self):
        return self.message

class DoesNotExist(GeocodingException):
    pass

class UnparseableLocation(GeocodingException):
    pass

class InvalidBlockButValidStreet(GeocodingException):
    def __init__(self, block_number, street_name, block_list):
        self.block_number = block_number
        self.street_name = street_name
        self.block_list = block_list
    
class Address(dict):
    "A simple container class for representing a single street address."
    def __init__(self, *args, **kwargs):
        dict.__init__(self, *args, **kwargs)
        self._cache_hit = False

    @property
    def latitude(self):
        if self["point"]:
            return self["point"].lat
    lat = latitude

    @property
    def longitude(self):
        if self["point"]:
            return self["point"].lng
    lng = longitude

    def __unicode__(self):
        return u", ".join([self[k] for k in ["address", "city", "state", "zip"]])

    @classmethod
    def from_cache(cls, cached):
        """
        Builds an Address object from a GeocoderCache result object.
        """
        fields = {
            'address': cached.address,
            'city': cached.city,
            'state': cached.state,
            'zip': cached.zip,
            'point': cached.location,
            'intersection_id': cached.intersection_id,
        }
        try:
            block_obj = cached.block
        except ObjectDoesNotExist:
            fields.update({'block': None})
        else:
            fields.update({'block': block_obj})
        try:
            intersection_obj = cached.intersection
        except ObjectDoesNotExist:
            fields.update({'intersection': None})
        else:
            fields.update({'intersection': intersection_obj})
        obj = cls(fields)
        obj._cache_hit = True
        return obj

class Geocoder(object):
    """
    Generic Geocoder class.

    Subclasses must override the following attribute:

        _do_geocode(self, location_string)
            Actually performs the geocoding. The base class implementation of
            geocode() calls this behind the scenes.
    """
    def __init__(self, use_cache=True):
        self.use_cache = use_cache

    def geocode(self, location):
        """
        Geocodes the given location, handling caching behind the scenes.
        """
        location = normalize(location)
        result, cache_hit = None, False

        # Get the result (an Address instance), either from the cache or by
        # calling _do_geocode().
        if self.use_cache:
            try:
                cached = GeocoderCache.objects.filter(normalized_location=location)[0]
            except IndexError:
                pass
            else:
                result = Address.from_cache(cached)
                cache_hit = True

        if result is None:
            try:
                result = self._do_geocode(location)
            except AmbiguousResult, e:
                # If multiple results were found, check whether they have the
                # same point. If they all have the same point, don't raise the
                # AmbiguousResult exception -- just return the first one.
                # 
                # An edge case is if result['point'] is None. This could happen
                # if the geocoder found locations, not points. In that case,
                # just raise the AmbiguousResult.
                result = e.choices[0]
                if result['point'] is None:
                    raise
                for i in e.choices[1:]:
                    if i['point'] != result['point']:
                        raise

        # Save the result to the cache if it wasn't in there already.
        if not cache_hit and self.use_cache:
            GeocoderCache.populate(location, result)

        return result

class AddressGeocoder(Geocoder):
    def _do_geocode(self, location_string):
        # Parse the address.
        try:
            locations = parse(location_string)
        except ParsingError, e:
            raise

        all_results = []
        for loc in locations:
            loc_results = self._db_lookup(loc)
            # If none were found, maybe the street was misspelled. Check that.
            if not loc_results and loc['street']:
                try:
                    misspelling = StreetMisspelling.objects.get(incorrect=loc['street'])
                    loc['street'] = misspelling.correct
                except StreetMisspelling.DoesNotExist:
                    pass
                else:
                    loc_results = self._db_lookup(loc)
                # Next, try removing the street suffix, in case an incorrect
                # one was given.
                if not loc_results and loc['suffix']:
                    loc_results = self._db_lookup(dict(loc, suffix=None))
                # Next, try looking for the street, in case the street
                # exists but the address doesn't.
                if not loc_results and loc['number']:
                    kwargs = {'street': loc['street']}
                    sided_filters = []
                    if loc['city']:
                        city_filter = Q(left_city=loc['city']) | Q(right_city=loc['city'])
                        sided_filters.append(city_filter)
                    b_list = Block.objects.filter(*sided_filters, **kwargs).order_by('predir', 'from_num', 'to_num')
                    if b_list:
                        raise InvalidBlockButValidStreet(loc['number'], b_list[0].street_pretty_name, b_list)
            all_results.extend(loc_results)

        if not all_results:
            raise DoesNotExist("Geocoder db couldn't find this location: %r" % location_string)
        elif len(all_results) == 1:
            return all_results[0]
        else:
            raise AmbiguousResult(all_results)

    def _db_lookup(self, location):
        """
        Given a location dict as returned by parse(), looks up the address in
        the DB. Always returns a list of Address dictionaries (or an empty list
        if no results are found).
        """
        if not location['number']:
            return []

        # Query the blocks database.
        try:
            blocks = Block.objects.search(
                street=location['street'],
                number=location['number'],
                predir=location['pre_dir'],
                suffix=location['suffix'],
                postdir=location['post_dir'],
                city=location['city'],
                state=location['state'],
                zipcode=location['zip'],
            )
        except Exception, e:
            # TODO: replace with Block-specific exception
            raise Exception("Road segment db query failed: %r" % e)
        return [self._build_result(location, block, geocoded_pt) for block, geocoded_pt in blocks]

    def _build_result(self, location, block, geocoded_pt):
        return Address({
            'address': unicode(" ".join([str(s) for s in [location['number'], block.predir, block.street_pretty_name, block.postdir] if s])),
            'city': block.city.title(),
            'state': block.state,
            'zip': block.zip,
            'block': block,
            'intersection_id': None,
            'point': geocoded_pt,
            'url': block.url(),
            'wkt': str(block.location),
        })

class BlockGeocoder(AddressGeocoder):
    def _do_geocode(self, location_string):
        m = block_re.search(location_string)
        if not m:
            raise ParsingError("BlockGeocoder somehow got an address it can't parse: %r" % location_string)
        new_location_string = ' '.join(m.groups())
        return AddressGeocoder._do_geocode(self, new_location_string)

class IntersectionGeocoder(Geocoder):
    def _do_geocode(self, location_string):
        sides = intersection_re.split(location_string)
        if len(sides) != 2:
            raise ParsingError("Couldn't parse intersection: %r" % location_string)

        # Parse each side of the intersection to a list of possibilities.
        # Let the ParseError exception propagate, if it's raised.
        left_side = parse(sides[0])
        right_side = parse(sides[1])

        all_results = []
        seen_intersections = set()
        for street_a in left_side:
            street_a['street'] = StreetMisspelling.objects.make_correction(street_a['street'])
            for street_b in right_side:
                street_b['street'] = StreetMisspelling.objects.make_correction(street_b['street'])
                for result in self._db_lookup(street_a, street_b):
                    if result["intersection_id"] not in seen_intersections:
                        seen_intersections.add(result["intersection_id"])
                        all_results.append(result)

        if not all_results:
            raise DoesNotExist("Geocoder db couldn't find this intersection: %r" % location_string)
        elif len(all_results) == 1:
            return all_results.pop()
        else:
            raise AmbiguousResult(list(all_results), "Intersections DB returned %s results" % len(all_results))

    def _db_lookup(self, street_a, street_b):
        try:
            intersections = Intersection.objects.search(
                predir_a=street_a["pre_dir"],
                street_a=street_a["street"],
                suffix_a=street_a["suffix"],
                postdir_a=street_a["post_dir"],
                predir_b=street_b["pre_dir"],
                street_b=street_b["street"],
                suffix_b=street_b["suffix"],
                postdir_b=street_b["post_dir"]
            )
        except Exception, e:
            raise DoesNotExist("Intersection db query failed: %r" % e)
        return [self._build_result(i) for i in intersections]

    def _build_result(self, intersection):
        return Address({
            'address': intersection.pretty_name,
            'city': intersection.city,
            'state': intersection.state,
            'zip': intersection.zip,
            'intersection_id': intersection.id,
            'intersection': intersection,
            'block': None,
            'point': intersection.location,
            'url': intersection.url(),
            'wkt': str(intersection.location),
        })

# THIS IS NOT YET FINISHED
#
# class SegmentGeocoder(Geocoder):
#     def _do_geocode(self, location_string):
#         bits = segment_re.findall(location_string)
#         g = IntersectionGeocoder()
#         try:
#             point1 = g.geocode('%s and %s' % (bits[0], bits[1]))
#             point2 = g.geocode('%s and %s' % (bits[0], bits[2]))
#         except DoesNotExist, e:
#             raise DoesNotExist("Segment query failed: %r" % e)
#         # TODO: Make a line from the two points, and return that.

class SmartGeocoder(Geocoder):
    def _do_geocode(self, location_string):
        if intersection_re.search(location_string):
            geocoder = IntersectionGeocoder()
        elif block_re.search(location_string):
            geocoder = BlockGeocoder()
        else:
            geocoder = AddressGeocoder()
        return geocoder._do_geocode(location_string)

########NEW FILE########
__FILENAME__ = models
from django.contrib.gis.db import models
from ebpub.streets.models import Block
from ebpub.streets.models import Intersection

class GeocoderCache(models.Model):
    normalized_location = models.CharField(max_length=255, db_index=True)
    address = models.CharField(max_length=255)
    city = models.CharField(max_length=255)
    state = models.CharField(max_length=2)
    zip = models.CharField(max_length=10)
    location = models.PointField()
    block = models.ForeignKey(Block, blank=True, null=True)
    intersection = models.ForeignKey(Intersection, blank=True, null=True)
    generated_at = models.DateTimeField(auto_now_add=True)
    objects = models.GeoManager()

    def __unicode__(self):
        return self.normalized_location

    @classmethod
    def populate(cls, normalized_location, address):
        """
        Populates the cache from an Address object.
        """
        if address['point'] is None:
            return
        obj = cls()
        obj.normalized_location = normalized_location
        for field in ('address', 'city', 'state', 'zip'):
            setattr(obj, field, address[field])
        for relation in ['block', 'intersection_id']:
            if relation in address:
                setattr(obj, relation, address[relation])
        obj.location = address['point']
        obj.save()

########NEW FILE########
__FILENAME__ = cities
cities = {
    "CHICAGO": "CHI",
    "SAN FRANCISCO": ["SAN FRAN", "SF"],
    "NEW YORK": ["NY", "NYC"],
    "THE BRONX": ["BRONX"],
}

########NEW FILE########
__FILENAME__ = make_cf_tests
#!/usr/bin/env python
"""
Extracts location strings and geocoder results from Civic Footprint
application log.

Example of log is below:

Attempting to geocode 4155 N Wolcott, Chicago, IL
[#<struct Geocoder::Result latitude="41.957265", longitude="-87.676214", address="4155 N WOLCOTT AVE", city="CHICAGO", state="IL", zip="60613-2681", country="US", precision="address", warning="The exact location could not be found, here is the closest match: 4155 N Wolcott Ave, Chicago, IL 60613">]
--
Attempting to geocode 2450 E 91 ST          , Chicago IL
[#<struct Geocoder::Result latitude="41.730037", longitude="-87.564174", address="2450 E 91ST ST", city="CHICAGO", state="IL", zip="60617-3822", country="US", precision="address", warning="The exact location could not be found, here is the closest match: 2450 E 91st St, Chicago, IL 60617">]
--
Attempting to geocode 2038 damen ave chicago il
[#<struct Geocoder::Result latitude="41.854524", longitude="-87.676083", address="2038 S DAMEN AVE", city="CHICAGO", state="IL", zip="60608-2625", country="US", precision="address", warning="The exact location could not be found, here is the closest match: 2038 S Damen Ave, Chicago, IL 60608">, #<struct Geocoder::Result latitude="41.918759", longitude="-87.677699", address="2038 N DAMEN AVE", city="CHICAGO", state="IL", zip="60647-4564", country="US", precision="address", warning="The exact location could not be found, here is the closest match: 2038 N Damen Ave, Chicago, IL 60647">]
--
Attempting to geocode 29 W. division st. 
Geocoding error: unable to parse location
--
"""
import re, sys

location_re = re.compile(r"Attempting to geocode (.+)$")
find_result = re.compile(r"#<struct Geocoder::Result ([^>]+?)>").findall
find_ruby_pairs = re.compile(r'([a-z]+)="([^"]+)"').findall
keys_to_delete = "precision country warning latitude longitude".split()

def extract_tests(f):
    results = []
    seen = set()
    for line in f:
        line = line.strip()
        # Records are delimited with lines containing exactly the
        # text "--"
        if line == "--": continue
        m = location_re.match(line)
        if m:
            input = m.group(1)
            # Get the next line if we've got a location string
            line = f.next().strip()
            m = find_result(line)
            if m:
                if input not in seen:
                    seen.add(input)
                else:
                    continue
                output = []
                for r in m:
                    r = dict(find_ruby_pairs(r))
                    if r["precision"] != "address" or \
                       ("city" in r and r["city"].upper() != "CHICAGO"):
                       continue
                    r["point"] = (r["latitude"], r["longitude"])
                    for k in keys_to_delete:
                        if k in r: del r[k]
                    output.append(r)
                if output:
                    results.append((input, output))
    return results

if __name__ == "__main__":
    print "cf_addrs = {"
    for (l, r) in extract_tests(sys.stdin):
        print "    %r:" % l
        print "    %r," % r
    print "}"

########NEW FILE########
__FILENAME__ = numbered_streets
numbered_streets = {
    "1ST": ["FIRST", "1"],
    "2ND": ["SECOND", "2D", "2"],
    "3RD": ["THIRD", "3D", "3"],
    "4TH": ["FOURTH", "4"],
    "5TH": ["FIFTH", "5"],
    "6TH": ["SIXTH", "6"],
    "7TH": ["SEVENTH", "7"],
    "8TH": ["EIGHTH", "8"],
    "9TH": ["NINTH", "9"],
    "10TH": ["TENTH", "10"],
    "11TH": ["ELEVENTH", "11"],
    "12TH": ["TWELFTH", "TWELVTH", "12"],
    "13TH": ["THIRTEENTH", "13"],
    "14TH": ["FOURTEENTH", "14"],
    "15TH": ["FIFTEENTH", "15"],
    "16TH": ["SIXTEENTH", "16"],
    "17TH": ["SEVENTEENTH", "17"],
    "18TH": ["EIGHTEENTH", "18"],
    "19TH": ["NINTEENTH", "19"],
    "20TH": ["TWENTIETH", "20"],
    "21ST": "21",
    "22ND": "22",
    "23RD": ["23D", "23"],
    "24TH": "24",
    "25TH": "25",
    "26TH": "26",
    "27TH": "27",
    "28TH": "28",
    "29TH": "29",
    "30TH": ["THIRTIETH", "30"],
    "31ST": "31",
    "32ND": "32",
    "33RD": ["33D", "33"],
    "34TH": "34",
    "35TH": "35",
    "36TH": "36",
    "37TH": "37",
    "38TH": "38",
    "39TH": "39",
    "40TH": ["FORTIETH", "40"],
    "41ST": "41",
    "42ND": "42",
    "43RD": ["43D", "43"],
    "44TH": "44",
    "45TH": "45",
    "46TH": "46",
    "47TH": "47",
    "48TH": "48",
    "49TH": "49",
    "50TH": ["FIFTIETH", "50"],
    "51ST": "51",
    "52ND": "52",
    "53RD": ["53D", "53"],
    "54TH": "54",
    "55TH": "55",
    "56TH": "56",
    "57TH": "57",
    "58TH": "58",
    "59TH": "59",
    "60TH": ["SIXTYETH", "SIXTIETH", "60"],
    "61ST": "61",
    "62ND": "62",
    "63RD": ["63D", "63"],
    "64TH": "64",
    "65TH": "65",
    "66TH": "66",
    "67TH": "67",
    "68TH": "68",
    "69TH": "69",
    "70TH": ["SEVENTYITH", "SEVENTIETH", "70"],
    "71ST": "71",
    "72ND": "72",
    "73RD": ["73D", "73"],
    "74TH": "74",
    "75TH": "75",
    "76TH": "76",
    "77TH": "77",
    "78TH": "78",
    "79TH": "79",
    "80TH": ["EIGHTYITH", "EIGHTIETH", "80"],
    "81ST": "81",
    "82ND": "82",
    "83RD": ["83D", "83"],
    "84TH": "84",
    "85TH": "85",
    "86TH": "86",
    "87TH": "87",
    "88TH": "88",
    "89TH": "89",
    "90TH": ["NINETYITH", "NINETIETH", "90"],
    "91ST": "91",
    "92ND": "92",
    "93RD": ["93D", "93"],
    "94TH": "94",
    "95TH": "95",
    "96TH": "96",
    "97TH": "97",
    "98TH": "98",
    "99TH": "99",
    "100TH": "100",
    "101ST": "101",
    "102ND": "102",
    "103RD": ["103D", "103"],
    "104TH": "104",
    "105TH": "105",
    "106TH": "106",
    "107TH": "107",
    "108TH": "108",
    "109TH": "109",
    "110TH": "110",
    "111TH": "111",
    "112TH": "112",
    "113TH": "113",
    "114TH": "114",
    "115TH": "115",
    "116TH": "116",
    "117TH": "117",
    "118TH": "118",
    "119TH": "119",
    "120TH": "120",
    "121ST": "121",
    "122ND": "122",
    "123RD": ["123D", "123"],
    "124TH": "124",
    "125TH": "125",
    "126TH": "126",
    "127TH": "127",
    "128TH": "128",
    "129TH": "129",
    "130TH": "130",
    "131ST": "131",
    "132ND": "132",
    "133RD": ["133D", "133"],
    "134TH": "134",
    "135TH": "135",
    "136TH": "136",
    "137TH": "137",
    "138TH": "138",
    "139TH": "139",
    "140TH": "140",
    "141ST": "141",
    "142ND": "142",
    "143RD": ["143D", "143"],
    "144TH": "144",
    "145TH": "145",
    "146TH": "146",
    "147TH": "147",
    "148TH": "148",
    "149TH": "149",
    "150TH": "150",
    "151ST": "151",
    "152ND": "152",
    "153RD": ["153D", "153"],
    "154TH": "154",
    "155TH": "155",
    "156TH": "156",
    "157TH": "157",
    "158TH": "158",
    "159TH": "159",
    "160TH": "160",
    "161ST": "161",
    "162ND": "162",
    "163RD": ["163D", "163"],
    "164TH": "164",
    "165TH": "165",
    "166TH": "166",
    "167TH": "167",
    "168TH": "168",
    "169TH": "169",
    "170TH": "170",
    "171ST": "171",
    "172ND": "172",
    "173RD": ["173D", "173"],
    "174TH": "174",
    "175TH": "175",
    "176TH": "176",
    "177TH": "177",
    "178TH": "178",
    "179TH": "179",
    "180TH": "180",
    "181ST": "181",
    "182ND": "182",
    "183RD": ["183D", "183"],
    "184TH": "184",
    "185TH": "185",
    "186TH": "186",
    "187TH": "187",
    "188TH": "188",
    "189TH": "189",
    "190TH": "190",
    "191ST": "191",
    "192ND": "192",
    "193RD": ["193D", "193"],
    "194TH": "194",
    "195TH": "195",
    "196TH": "196",
    "197TH": "197",
    "198TH": "198",
    "199TH": "199",
    "200TH": "200",
    "201ST": "201",
    "202ND": "202",
    "203RD": ["203D", "203"],
    "204TH": "204",
    "205TH": "205",
    "206TH": "206",
    "207TH": "207",
    "208TH": "208",
    "209TH": "209",
    "210TH": "210",
    "211TH": "211",
    "212TH": "212",
    "213RD": "213",
    "214TH": "214",
    "215TH": "215",
    "216TH": "216",
    "217TH": "217",
    "218TH": "218",
    "219TH": "219",
    "220TH": "220",
    "221ST": "221",
    "222ND": "222",
    "223RD": ["223D", "223"],
    "224TH": "224",
    "225TH": "225",
    "226TH": "226",
    "227TH": "227",
    "228TH": "228",
    "229TH": "229",
    "230TH": "230",
    "231ST": "231",
    "232ND": "232",
    "233RD": ["233D", "233"],
    "234TH": "234",
    "235TH": "235",
    "236TH": "236",
    "237TH": "237",
    "238TH": "238",
    "239TH": "239",
    "240TH": "240",
    "241ST": "241",
    "242ND": "242",
    "243RD": ["243D", "243"],
    "244TH": "244",
    "245TH": "245",
    "246TH": "246",
    "247TH": "247",
    "248TH": "248",
    "249TH": "249",
    "250TH": "250",
    "251ST": "251",
    "252ND": "252",
    "253RD": ["253D", "253"],
    "254TH": "254",
    "255TH": "255",
    "256TH": "256",
    "257TH": "257",
    "258TH": "258",
    "259TH": "259",
    "260TH": "260",
    "261ST": "261",
    "262ND": "262",
    "263RD": ["263D", "263"],
    "264TH": "264",
    "265TH": "265",
    "266TH": "266",
    "267TH": "267",
    "268TH": "268",
    "269TH": "269",
    "270TH": "270",
    "271ST": "271",
    "272ND": "272",
    "273RD": ["273D", "273"],
    "274TH": "274",
    "275TH": "275",
    "276TH": "276",
    "277TH": "277",
    "278TH": "278",
    "279TH": "279",
    "280TH": "280",
    "281ST": "281",
    "282ND": "282",
    "283RD": ["283D", "283"],
    "284TH": "284",
    "285TH": "285",
    "286TH": "286",
    "287TH": "287",
    "288TH": "288",
    "289TH": "289",
    "290TH": "290",
    "291ST": "291",
    "292ND": "292",
    "293RD": ["293D", "293"],
    "294TH": "294",
    "295TH": "295",
    "296TH": "296",
    "297TH": "297",
    "298TH": "298",
    "299TH": "299",
}

########NEW FILE########
__FILENAME__ = parsing
import re
import string
from itertools import izip

# The following are all relative imports
from suffixes import suffixes
from states import states
from cities import cities
from numbered_streets import numbered_streets

class ParsingError(Exception):
    pass

#################
# STANDARDIZERS #
#################

DIRECTIONALS = {
    'N': 'NORTH',
    'NE': 'NORTHEAST',
    'E': 'EAST',
    'SE': 'SOUTHEAST',
    'S': 'SOUTH',
    'SW': 'SOUTHWEST',
    'W': 'WEST',
    'NW': 'NORTHWEST',
}

class Standardizer(object):
    """Replaces a suffix, directional, state, etc. with the preferred standard form.

    For example, given the text "avenu" for suffixes, returns "AVE".

    >>> suff_standardizer = Standardizer(suffixes)
    >>> suff_standardizer("avenu")
    'AVE'
    >>> dir_standardizer = Standardizer(DIRECTIONALS)
    >>> dir_standardizer("north")
    'N'
    >>> dir_standardizer("n")
    'N'
    """
    def __init__(self, d):
        self.replacement = {}
        for standard, options in d.items():
            standard = standard.upper()
            if isinstance(options, basestring):
                options = [options]
            for opt in options:
                self.replacement[opt.upper()] = standard
            # Also map the standard to itself.
            self.replacement[standard] = standard

    def __call__(self, s):
        if s.upper() in self.replacement:
            return self.replacement[s.upper()]
        else:
            return s

def number_standardizer(s):
    """
    Removes the second number in hyphenated addresses such as '123-02', as
    used in NYC. Note that this also removes the second number in address
    ranges.
    """
    m = re.search(r'^(\d+)[A-Z]?(?:-\d+[A-Z]?)?$', s)
    if not m:
        # We shouldn't reach this, but if the regex doesn't match, just return the input.
        return s
    return m.group(1)

dir_standardizer = Standardizer(DIRECTIONALS)

STANDARDIZERS = {
    'number': number_standardizer,
    'pre_dir': dir_standardizer,
    'street': Standardizer(numbered_streets),
    'suffix': Standardizer(suffixes),
    'post_dir': dir_standardizer,
    'city': Standardizer(cities),
    'state': Standardizer(states),
}

# Regex which matches all punctuation, except for dashes (which
# might be used in NYC addresses) and ampersands.
preserved_puncts = "-&"
punct = re.compile(r'[%s]' % re.escape("".join(set(string.punctuation) - set(preserved_puncts))))

half_addresses_re = re.compile(r'(?<=\s)[I1]/2(?=\s)')
multi_dash_re = re.compile(r'(?<=\d)\s*-+\s*(?=\d)')
zip_plus_4_re = re.compile(r'(?<=^\d{5})-\d{4}$')

def normalize(location):
    """
    Normalizes an address string for parsing, comparisons.

    >>> normalize(u"1972 n. dawson ave. chicago il")
    u'1972 N DAWSON AVE CHICAGO IL'
    >>> normalize(u"1972 n. dawson ave., chicago il")
    u'1972 N DAWSON AVE CHICAGO IL'
    >>> normalize(u"n kimball ave & w diversey ave")
    u'N KIMBALL AVE & W DIVERSEY AVE'
    """
    location = location.upper()
    location = half_addresses_re.sub('', location) # Strip "1/2" addresses.
    location = multi_dash_re.sub('-', location)
    location = punct.sub('', location) # Remove all punctuation except dashes, and ampersands.
    location = re.sub(r'\s+', ' ', location.strip()) # Strip/normalize whitespace.
    location = zip_plus_4_re.sub('', location) # Strip the +4 part of a ZIP+4.
    return location

def strip_unit(location):
    """
    Given an address string, strips the apartment number, suite number, etc.
    """
    return re.sub(r'(?i)(\s*,)?\s*(?:space\s+|suite\s+|ste\.?\s+|unit:?\s+|apt\.?\s+|\#\s*)[-\#0-9a-z]*$', '', location)

###########
# PARSING #
###########

def abbrev_regex(d, case_insensitive=True, matches_entirely=True):
    """
    Returns a regular expression pattern that matches an abbreviation.

    >>> suffixes = {
    ...     'av': ['ave', 'avenue'],
    ...     'st': ['str', 'street'],
    ...     'rd': 'road'
    ... }
    >>> regex = abbrev_regex(suffixes)
    >>> re.search(regex, "Ave")
    <_sre.SRE_Match object at ...>
    >>> re.search(regex, " Ave ") == None
    True
    >>> regex = abbrev_regex(suffixes, case_insensitive=False)
    >>> re.search(regex, "str")
    <_sre.SRE_Match object at ...>
    >>> re.search(regex, "Str") == None
    True
    >>> regex = abbrev_regex(suffixes, matches_entirely=False)
    >>> re.search(regex, " Road ")
    <_sre.SRE_Match object at ...>
    """
    alts = []
    for k, v in d.items():
        if isinstance(v, basestring):
            v = [v]
        alts.append(k)
        alts.extend(v)
    pattern = r"(?:%s)" % "|".join(alts)
    if matches_entirely:
        pattern = "^" + pattern + "$"
    if case_insensitive:
        pattern = "(?i)" + pattern
    return pattern

directional_re = re.compile(abbrev_regex(DIRECTIONALS))

TOKEN_REGEXES = {
    'number': re.compile(r'^\d+[A-Z]?(?:-\d+[A-Z]?)?$'),
    'pre_dir': directional_re,
    'street': re.compile(r'^[0-9]{1,3}(?:ST|ND|RD|TH)|[A-Z]{1,25}|[0-9]{1,3}$'),
    'suffix': re.compile(abbrev_regex(suffixes)),
    'post_dir': directional_re,

    # Cities are assumed to have at least three letters and at most 25 letters.
    # This is a safe assumption that comes from this page:
    # http://www.geographylists.com/list17f.html
    'city': re.compile(r'^[A-Z]{3,25}$'),

    # State words can have between 2 and 13 letters ('MASSACHUSETTS' is the
    # longest, with 13 letters). Note that this doesn't count states whose
    # names take up more than one word. This regex matches *single* words.
    'state': re.compile(r'^[A-Z]{2,13}$'),

    'zip': re.compile(r'^\d{5}(?:-\d{4})?$'),
}

class Location(dict):
    location_keys = ('number', 'pre_dir', 'street', 'suffix', 'post_dir', 'city', 'state', 'zip')

    def __init__(self, *args):
        super(Location, self).__init__(*args)
        for location_key in self.location_keys:
            if location_key not in self:
                self[location_key] = None

    def __repr__(self):
        return "{%s}" % ", ".join(["%r: %r" % (k, self[k]) for k in self.location_keys])

    def __setitem__(self, name, value):
        if name not in self.location_keys:
            raise AttributeError(repr(name))
        super(Location, self).__setitem__(name, value)

def address_combinations():
    """
    Generator that yields a list for every possible combination of address
    tokens. For example:
        ['number', 'pre_dir', 'street']
        ['number', 'street', 'city', 'state']
    """
    for number_times in (0, 1):
        for pre_dir_times in (0, 1):
            for street_times in (1, 2, 3, 4, 5):
                for suffix_times in (0, 1):
                    for post_dir_times in (0, 1):
                        for city_times in (0, 1, 2, 3, 4):
                            # If a city isn't given, then a state isn't allowed.
                            for state_times in (city_times == 0 and (0,) or (0, 1, 2)):
                                for zip_times in (0, 1):
                                    yield ['number'] * number_times + ['pre_dir'] * pre_dir_times + ['street'] * street_times + ['suffix'] * suffix_times + ['post_dir'] * post_dir_times + ['city'] * city_times + ['state'] * state_times + ['zip'] * zip_times

punc_split = re.compile(r"\S+").findall

def parse(location):
    s = strip_unit(normalize(location))
    tokens = punc_split(s)
    len_tokens = len(tokens)
    result_list = []

    for token_types in address_combinations():
        if len(token_types) == len_tokens:
            try:
                for token, token_type in izip(tokens, token_types):
                    if not TOKEN_REGEXES[token_type].match(token):
                        raise StopIteration() # Token regex didn't match.
            except StopIteration:
                continue

            # If we made it this far, then all of the tokens are valid.
            # Create the Location object.
            result = Location()
            for token, token_type in izip(tokens, token_types):
                if result[token_type]:
                    result[token_type] += ' ' + token
                else:
                    result[token_type] = token

            # Standardize all values.
            for key, value in result.items():
                if value and key in STANDARDIZERS:
                    result[key] = STANDARDIZERS[key](value)

            result_list.append(result)

    if not result_list:
        raise ParsingError("Failed to parse location %r" % location)
    return result_list

if __name__ == "__main__":
    import doctest
    doctest.testmod(optionflags=doctest.ELLIPSIS)

########NEW FILE########
__FILENAME__ = states
states = {
  'AL': 'ALABAMA',
  'AK': 'ALASKA',
  'AS': 'AMERICAN SAMOA',
  'AZ': 'ARIZONA',
  'AR': 'ARKANSAS',
  'CA': 'CALIFORNIA',
  'CO': 'COLORADO',
  'CT': 'CONNECTICUT',
  'DE': 'DELAWARE',
  'DC': 'DISTRICT OF COLUMBIA',
  'FM': 'FEDERATED STATES OF MICRONESIA',
  'FL': 'FLORIDA',
  'GA': 'GEORGIA',
  'GU': 'GUAM',
  'HI': 'HAWAII',
  'ID': 'IDAHO',
  'IL': 'ILLINOIS',
  'IN': 'INDIANA',
  'IA': 'IOWA',
  'KS': 'KANSAS',
  'KY': 'KENTUCKY',
  'LA': 'LOUISIANA',
  'ME': 'MAINE',
  'MH': 'MARSHALL ISLANDS',
  'MD': 'MARYLAND',
  'MA': 'MASSACHUSETTS',
  'MI': 'MICHIGAN',
  'MN': 'MINNESOTA',
  'MS': 'MISSISSIPPI',
  'MO': 'MISSOURI',
  'MT': 'MONTANA',
  'NE': 'NEBRASKA',
  'NV': 'NEVADA',
  'NH': 'NEW HAMPSHIRE',
  'NJ': 'NEW JERSEY',
  'NM': 'NEW MEXICO',
  'NY': 'NEW YORK',
  'NC': 'NORTH CAROLINA',
  'ND': 'NORTH DAKOTA',
  'MP': 'NORTHERN MARIANA ISLANDS',
  'OH': 'OHIO',
  'OK': 'OKLAHOMA',
  'OR': 'OREGON',
  'PW': 'PALAU',
  'PA': 'PENNSYLVANIA',
  'PR': 'PUERTO RICO',
  'RI': 'RHODE ISLAND',
  'SC': 'SOUTH CAROLINA',
  'SD': 'SOUTH DAKOTA',
  'TN': 'TENNESSEE',
  'TX': 'TEXAS',
  'UT': 'UTAH',
  'VT': 'VERMONT',
  'VI': 'VIRGIN ISLANDS',
  'VA': 'VIRGINIA',
  'WA': 'WASHINGTON',
  'WV': 'WEST VIRGINIA',
  'WI': 'WISCONSIN',
  'WY': 'WYOMING',
}

########NEW FILE########
__FILENAME__ = suffixes
suffixes = {
 'ALY': ['ALLEE', 'ALLY', 'ALLEY', 'ALY'],
 'ANX': ['ANEX', 'ANNX', 'ANX', 'ANNEX'],
 'ARC': ['ARC', 'ARCADE'],
 'AVE': ['AVEN', 'AVNUE', 'AVENU', 'AVN', 'AV', 'AVE', 'AVENUE'],
 'BCH': ['BCH', 'BEACH'],
 'BG': ['BURG'],
 'BGS': ['BURGS'],
 'BLF': ['BLUF', 'BLF', 'BLUFF'],
 'BLFS': ['BLUFFS'],
 'BLVD': ['BLVD', 'BOULV', 'BOUL', 'BOULEVARD', 'BL'],
 'BND': ['BEND', 'BND'],
 'BR': ['BRNCH', 'BR', 'BRANCH'],
 'BRG': ['BRG', 'BRIDGE', 'BRDGE'],
 'BRK': ['BRK', 'BROOK'],
 'BRKS': ['BROOKS'],
 'BTM': ['BTM', 'BOTTM', 'BOT', 'BOTTOM'],
 'BYP': ['BYPS', 'BYPA', 'BYPAS', 'BYP', 'BYPASS'],
 'BYU': ['BAYOU', 'BAYOO'],
 'CIR': ['CIRC', 'CRCLE', 'CIR', 'CIRCL', 'CIRCLE', 'CRCL'],
 'CIRS': ['CIRCLES'],
 'CLB': ['CLUB', 'CLB'],
 'CLF': ['CLF', 'CLIFF'],
 'CLFS': ['CLIFFS', 'CLFS'],
 'CMN': ['COMMON'],
 'COR': ['CORNER', 'COR'],
 'CORS': ['CORNERS', 'CORS'],
 'CP': ['CP', 'CAMP', 'CMP'],
 'CPE': ['CAPE', 'CPE'],
 'CRES': ['CRESENT',
          'CRECENT',
          'CRSENT',
          'CRSNT',
          'CRES',
          'CRSCNT',
          'CRESCENT'],
 'CRK': ['CK', 'CR', 'CREEK', 'CRK'],
 'CRSE': ['COURSE', 'CRSE'],
 'CRST': ['CREST'],
 'CSWY': ['CSWY', 'CAUSWAY', 'CAUSEWAY'],
 'CT': ['COURT', 'CRT', 'CT'],
 'CTR': ['CNTER', 'CTR', 'CENTRE', 'CEN', 'CENT', 'CNTR', 'CENTR', 'CENTER'],
 'CTRS': ['CENTERS'],
 'CTS': ['COURTS', 'CTS'],
 'CURV': ['CURVE'],
 'CV': ['COVE', 'CV'],
 'CVS': ['COVES'],
 'CYN': ['CANYON', 'CANYN', 'CNYN', 'CYN'],
 'DL': ['DALE', 'DL'],
 'DM': ['DAM', 'DM'],
 'DR': ['DRIV', 'DR', 'DRIVE', 'DRV'],
 'DRS': ['DRIVES'],
 'DV': ['DV', 'DVD', 'DIV', 'DIVIDE'],
 'EST': ['EST', 'ESTATE'],
 'ESTS': ['ESTATES', 'ESTS'],
 'EXPY': ['EXPY', 'EXPR', 'EXPRESS', 'EXPW', 'EXP', 'EXPWY', 'EXPRESSWAY'],
 'EXT': ['EXTN', 'EXT', 'EXTNSN', 'EXTENSION'],
 'EXTS': ['EXTS', 'EXTENSIONS'],
 'FALL': ['FALL'],
 'FLD': ['FIELD', 'FLD'],
 'FLDS': ['FIELDS', 'FLDS'],
 'FLS': ['FLS', 'FALLS'],
 'FLT': ['FLAT', 'FLT'],
 'FLTS': ['FLATS', 'FLTS'],
 'FRD': ['FRD', 'FORD'],
 'FRDS': ['FORDS'],
 'FRG': ['FORGE', 'FRG', 'FORG'],
 'FRGS': ['FORGES'],
 'FRK': ['FORK', 'FRK'],
 'FRKS': ['FORKS', 'FRKS'],
 'FRST': ['FRST', 'FOREST', 'FORESTS'],
 'FRY': ['FERRY', 'FRY', 'FRRY'],
 'FT': ['FRT', 'FT', 'FORT'],
 'FWY': ['FREEWAY', 'FRWAY', 'FRWY', 'FREEWY', 'FWY'],
 'GDN': ['GARDN', 'GRDN', 'GARDEN', 'GDN', 'GRDEN'],
 'GDNS': ['GDNS', 'GRDNS', 'GARDENS'],
 'GLN': ['GLEN', 'GLN'],
 'GLNS': ['GLENS'],
 'GRN': ['GRN', 'GREEN'],
 'GRNS': ['GREENS'],
 'GRV': ['GROVE', 'GRV', 'GROV'],
 'GRVS': ['GROVES'],
 'GTWY': ['GTWAY', 'GATWAY', 'GTWY', 'GATEWAY', 'GATEWY'],
 'HBR': ['HARBOR', 'HARBR', 'HARB', 'HRBOR', 'HBR'],
 'HBRS': ['HARBORS'],
 'HL': ['HILL', 'HL'],
 'HLS': ['HLS', 'HILLS'],
 'HOLW': ['HOLW', 'HOLWS', 'HLLW', 'HOLLOWS', 'HOLLOW'],
 'HTS': ['HTS', 'HEIGHT', 'HGTS', 'HT', 'HEIGHTS'],
 'HVN': ['HVN', 'HAVEN', 'HAVN'],
 'HWY': ['HIWY', 'HIGHWAY', 'HWY', 'HWAY', 'HIWAY', 'HIGHWY'],
 'INLT': ['INLT', 'INLET'],
 'IS': ['ISLAND', 'IS', 'ISLND'],
 'ISLE': ['ISLE', 'ISLES'],
 'ISS': ['ISS', 'ISLANDS', 'ISLNDS'],
 'JCT': ['JCT', 'JCTN', 'JUNCTION', 'JCTION', 'JUNCTN', 'JUNCTON'],
 'JCTS': ['JCTNS', 'JCTS', 'JUNCTIONS'],
 'KNL': ['KNL', 'KNOL', 'KNOLL'],
 'KNLS': ['KNOLLS', 'KNLS'],
 'KY': ['KY', 'KEY'],
 'KYS': ['KEYS', 'KYS'],
 'LAND': ['LAND'],
 'LCK': ['LOCK', 'LCK'],
 'LCKS': ['LOCKS', 'LCKS'],
 'LDG': ['LODGE', 'LDGE', 'LODG', 'LDG'],
 'LF': ['LF', 'LOAF'],
 'LGT': ['LIGHT', 'LGT'],
 'LGTS': ['LIGHTS'],
 'LK': ['LAKE', 'LK'],
 'LKS': ['LAKES', 'LKS'],
 'LN': ['LN', 'LANE', 'LANES', 'LA'],
 'LNDG': ['LNDNG', 'LNDG', 'LANDING'],
 'LOOP': ['LOOPS', 'LOOP'],
 'MALL': ['MALL'],
 'MDW': ['MEADOW', 'MDW'],
 'MDWS': ['MEDOWS', 'MEADOWS', 'MDWS'],
 'MEWS': ['MEWS'],
 'ML': ['ML', 'MILL'],
 'MLS': ['MLS', 'MILLS'],
 'MNR': ['MNR', 'MANOR'],
 'MNRS': ['MNRS', 'MANORS'],
 'MSN': ['MSN', 'MISSN', 'MISSION', 'MSSN'],
 'MT': ['MT', 'MOUNT', 'MNT'],
 'MTN': ['MOUNTAIN', 'MOUNTIN', 'MNTN', 'MNTAIN', 'MTN', 'MTIN'],
 'MTNS': ['MOUNTAINS', 'MNTNS'],
 'MTWY': ['MOTORWAY'],
 'NCK': ['NCK', 'NECK'],
 'OPAS': ['OVERPASS'],
 'ORCH': ['ORCHRD', 'ORCH', 'ORCHARD'],
 'OVAL': ['OVAL', 'OVL'],
 'PARK': ['PK', 'PARK', 'PARKS', 'PRK'],
 'PASS': ['PASS'],
 'PATH': ['PATH', 'PATHS'],
 'PIKE': ['PIKE', 'PIKES'],
 'PKWY': ['PKWAY', 'PKY', 'PARKWAYS', 'PKWY', 'PARKWY', 'PKWYS', 'PARKWAY'],
 'PL': ['PLACE', 'PL'],
 'PLN': ['PLAIN', 'PLN'],
 'PLNS': ['PLNS', 'PLAINS', 'PLAINES'],
 'PLZ': ['PLAZA', 'PLZ', 'PLZA'],
 'PNE': ['PINE'],
 'PNES': ['PINES', 'PNES'],
 'PR': ['PR', 'PRR', 'PRAIRIE', 'PRARIE'],
 'PRT': ['PRT', 'PORT'],
 'PRTS': ['PRTS', 'PORTS'],
 'PSGE': ['PASSAGE'],
 'PT': ['PT', 'POINT'],
 'PTS': ['POINTS', 'PTS'],
 'RADL': ['RADL', 'RAD', 'RADIEL', 'RADIAL'],
 'RAMP': ['RAMP'],
 'RD': ['RD', 'ROAD'],
 'RDG': ['RDG', 'RIDGE', 'RDGE'],
 'RDGS': ['RDGS', 'RIDGES'],
 'RDS': ['ROADS', 'RDS'],
 'RIV': ['RIV', 'RVR', 'RIVER', 'RIVR'],
 'RNCH': ['RANCHES', 'RANCH', 'RNCH', 'RNCHS'],
 'ROW': ['ROW'],
 'RPD': ['RAPID', 'RPD'],
 'RPDS': ['RPDS', 'RAPIDS'],
 'RST': ['RST', 'REST'],
 'RTE': ['ROUTE'],
 'RUE': ['RUE'],
 'RUN': ['RUN'],
 'SHL': ['SHL', 'SHOAL'],
 'SHLS': ['SHLS', 'SHOALS'],
 'SHR': ['SHOAR', 'SHORE', 'SHR'],
 'SHRS': ['SHORES', 'SHOARS', 'SHRS'],
 'SKWY': ['SKYWAY'],
 'SMT': ['SMT', 'SUMMIT', 'SUMITT', 'SUMIT'],
 'SPG': ['SPRING', 'SPNG', 'SPRNG', 'SPG'],
 'SPGS': ['SPRINGS', 'SPGS', 'SPRNGS', 'SPNGS'],
 'SPUR': ['SPUR', 'SPURS'],
 'SQ': ['SQR', 'SQ', 'SQUARE', 'SQU', 'SQRE'],
 'SQS': ['SQRS', 'SQUARES'],
 'ST': ['STRT', 'STREET', 'STR', 'ST'],
 'STA': ['STATN', 'STN', 'STATION', 'STA'],
 'STRA': ['STRAVE',
          'STRAV',
          'STRAVEN',
          'STRAVN',
          'STRVN',
          'STRAVENUE',
          'STRVNUE',
          'STRA'],
 'STRM': ['STREME', 'STRM', 'STREAM'],
 'STS': ['STREETS'],
 'TER': ['TER', 'TERRACE', 'TERR'],
 'TPKE': ['TURNPK', 'TRPK', 'TPK', 'TPKE', 'TURNPIKE', 'TRNPK'],
 'TRAK': ['TRACK', 'TRACKS', 'TRKS', 'TRK', 'TRAK'],
 'TRCE': ['TRCE', 'TRACES', 'TRACE'],
 'TRFY': ['TRFY', 'TRAFFICWAY'],
 'TRL': ['TRLS', 'TRAIL', 'TR', 'TRL', 'TRAILS'],
 'TRWY': ['THROUGHWAY'],
 'TUNL': ['TUNEL', 'TUNNEL', 'TUNLS', 'TUNL', 'TUNNL', 'TUNNELS'],
 'UN': ['UNION', 'UN'],
 'UNS': ['UNIONS'],
 'UPAS': ['UNDERPASS'],
 'VIA': ['VIADUCT', 'VDCT', 'VIA', 'VIADCT'],
 'VIS': ['VSTA', 'VIS', 'VISTA', 'VST', 'VIST'],
 'VL': ['VILLE', 'VL'],
 'VLG': ['VILLAG', 'VILLG', 'VILLIAGE', 'VLG', 'VILL', 'VILLAGE'],
 'VLGS': ['VLGS', 'VILLAGES'],
 'VLY': ['VLY', 'VALLEY', 'VALLY', 'VLLY'],
 'VLYS': ['VALLEYS', 'VLYS'],
 'VW': ['VW', 'VIEW'],
 'VWS': ['VWS', 'VIEWS'],
 'WALK': ['WALKS', 'WALK', 'WK'],
 'WALL': ['WALL'],
 'WAY': ['WY', 'WAY'],
 'WAYS': ['WAYS'],
 'WL': ['WELL'],
 'WLS': ['WLS', 'WELLS'],
 'XING': ['CROSSING', 'XING', 'CRSSNG', 'CRSSING'],
 'XRD': ['CROSSROAD']}

########NEW FILE########
__FILENAME__ = tests
"""
Tests for address parsing.

The LocationTestCase class contains both hand-written tests and a metaclass
that auto-generates tests based on some sample data.
"""

from ebpub.geocoder.parser.parsing import parse, address_combinations, ParsingError, Location
import unittest

class AutoLocationMetaclass(type):
    """
    Metaclass that adds a test method for every combination of test data
    (defined in TEST_DATA).
    """
    def __new__(cls, name, bases, attrs):
        TEST_DATA = (
            # token type, (one-word sample, two-word sample, three-word sample, ...)
            ('number', ('228',)),
            ('pre_dir', ('S',)),
            ('street', ('BROADWAY', 'OLD MILL', 'MARTIN LUTHER KING', 'MARTIN LUTHER KING JR', 'DR MARTIN LUTHER KING JR')),
            ('suffix', ('AVE',)),
            ('post_dir', ('S',)),
            ('city', ('CHICAGO', 'SAN FRANCISCO', 'NEW YORK CITY', 'OLD NEW YORK CITY')),
            ('state', ('IL', 'NEW HAMPSHIRE')),
            ('zip', ('60604',)),
        )
        for token_types in address_combinations():
            test_input = []
            expected = Location()
            for t_type, samples in TEST_DATA:
                count = token_types.count(t_type)
                if count:
                    test_input.append(samples[count-1])
                    expected[t_type] = samples[count-1]

            # Take the normalization into account.
            if expected['state'] == 'NEW HAMPSHIRE':
                expected['state'] = 'NH'

            location = ' '.join(test_input)
            func = lambda self: self.assertParseContains(location, expected)
            func.__doc__ = location
            attrs['test_%s' % '_'.join(token_types)] = func

        return type.__new__(cls, name, bases, attrs)

class LocationTestCase(unittest.TestCase):
    __metaclass__ = AutoLocationMetaclass

    # def assertParses(self, location, expected):
    #     try:
    #         actual = [dict(result) for result in parse(location)]
    #         try:
    #             self.assertEqual(actual, expected)
    #         except AssertionError, e:
    #             raise AssertionError("%r: %s" % (location, e))
    #     except ParsingError, e:
    #         self.fail(e)

    def assertParseContains(self, location, contains):
        # Because the parser is overly greedy and gives many possible
        # responses, it keeps the unit tests tidier and less brittle if we
        # just list one important parse result that we expect, rather than
        # listing every single test result.
        try:
            actual = [dict(result) for result in parse(location)]
        except ParsingError, e:
            self.fail(e)
        else:
            self.assert_(contains in actual, '%r not in %r' % (contains, actual))

    def test_saint_louis(self):
        self.assertParseContains('11466 S Saint Louis Ave, Chicago, IL, 60655',
            {'number': '11466', 'pre_dir': 'S', 'street': 'SAINT LOUIS', 'suffix': 'AVE', 'post_dir': None, 'city': 'CHICAGO', 'state': 'IL', 'zip': '60655'},
        )

    def test_st_louis_ave(self):
        self.assertParseContains('11466 S St Louis Ave',
            {'number': '11466', 'pre_dir': 'S', 'street': 'ST LOUIS', 'suffix': 'AVE', 'post_dir': None, 'city': None, 'state': None, 'zip': None},
        )

    def test_st_louis_st(self):
        self.assertParseContains('11466 S St Louis St',
            {'number': '11466', 'pre_dir': 'S', 'street': 'ST LOUIS', 'suffix': 'ST', 'post_dir': None, 'city': None, 'state': None, 'zip': None},
        )

    def test_numbered_street1(self):
        self.assertParseContains('2 W 111th Pl',
            {'number': '2', 'pre_dir': 'W', 'street': '111TH', 'suffix': 'PL', 'post_dir': None, 'city': None, 'state': None, 'zip': None},
        )

    def test_numbered_street2(self):
        self.assertParseContains('260 W 44th St',
            {'number': '260', 'pre_dir': 'W', 'street': '44TH', 'suffix': 'ST', 'post_dir': None, 'city': None, 'state': None, 'zip': None},
        )

    def test_numbered_street3(self):
        self.assertParseContains('260 W 44th, New York, NY 10036',
            {'number': '260', 'pre_dir': 'W', 'street': '44TH', 'suffix': None, 'post_dir': None, 'city': 'NEW YORK', 'state': 'NY', 'zip': '10036'},
        )

    def test_numbered_street4(self):
        self.assertParseContains('1 5th Ave, New York, NY 10003',
            {'number': '1', 'pre_dir': None, 'street': '5TH', 'suffix': 'AVE', 'post_dir': None, 'city': 'NEW YORK', 'state': 'NY', 'zip': '10003'},
        )

    def test_numbered_street5(self):
        self.assertParseContains('329 50 ST, MANHATTAN',
            {'number': '329', 'pre_dir': None, 'street': '50TH', 'suffix': 'ST', 'post_dir': None, 'city': 'MANHATTAN', 'state': None, 'zip': None},
        )

    def test_numbered_street6(self):
        self.assertParseContains('329 41 ST, MANHATTAN',
            {'number': '329', 'pre_dir': None, 'street': '41ST', 'suffix': 'ST', 'post_dir': None, 'city': 'MANHATTAN', 'state': None, 'zip': None},
        )

    def test_numbered_street7(self):
        self.assertParseContains('329 42 ST, MANHATTAN',
            {'number': '329', 'pre_dir': None, 'street': '42ND', 'suffix': 'ST', 'post_dir': None, 'city': 'MANHATTAN', 'state': None, 'zip': None},
        )

    def test_numbered_street8(self):
        self.assertParseContains('329 43 ST, MANHATTAN',
            {'number': '329', 'pre_dir': None, 'street': '43RD', 'suffix': 'ST', 'post_dir': None, 'city': 'MANHATTAN', 'state': None, 'zip': None},
        )

    def test_junior(self):
        self.assertParseContains('3624 S. John Hancock Jr. Road',
            {'number': '3624', 'pre_dir': 'S', 'street': 'JOHN HANCOCK JR', 'suffix': 'RD', 'post_dir': None, 'city': None, 'state': None, 'zip': None},
        )

    # The test_perl* tests were copied from the unit tests for the CPAN
    # Geo::StreetAddress module.
    def test_perl1(self):
        self.assertParseContains('1005 Gravenstein Hwy 95472',
            {'number': '1005', 'pre_dir': None, 'street': 'GRAVENSTEIN', 'suffix': 'HWY', 'post_dir': None, 'city': None, 'state': None, 'zip': '95472'},
        )

    def test_perl2(self):
        self.assertParseContains('1005 Gravenstein Hwy, 95472',
            {'number': '1005', 'pre_dir': None, 'street': 'GRAVENSTEIN', 'suffix': 'HWY', 'post_dir': None, 'city': None, 'state': None, 'zip': '95472'},
        )

    def test_perl3(self):
        self.assertParseContains('1005 Gravenstein Hwy N, 95472',
            {'number': '1005', 'pre_dir': None, 'street': 'GRAVENSTEIN', 'suffix': 'HWY', 'post_dir': 'N', 'city': None, 'state': None, 'zip': '95472'},
        )

    def test_perl4(self):
        self.assertParseContains('1005 Gravenstein Highway North, 95472',
            {'number': '1005', 'pre_dir': None, 'street': 'GRAVENSTEIN', 'suffix': 'HWY', 'post_dir': 'N', 'city': None, 'state': None, 'zip': '95472'},
        )

    def test_perl5(self):
        self.assertParseContains('1005 N Gravenstein Highway, Sebastopol, CA',
            {'number': '1005', 'pre_dir': 'N', 'street': 'GRAVENSTEIN', 'suffix': 'HWY', 'post_dir': None, 'city': 'SEBASTOPOL', 'state': 'CA', 'zip': None},
        )

    # def test_perl6(self):
    #     self.assertParses('1005 N Gravenstein Highway, Suite 500, Sebastopol, CA', [{
    #         'number': '1005',
    #         'pre_dir': 'N',
    #         'street': 'GRAVENSTEIN',
    #         'suffix': 'HWY',
    #         'post_dir': None,
    #         'city': 'SEBASTOPOL',
    #         'state': 'CA',
    #         'zip': None,
    #     }])
    # 
    # def test_perl7(self):
    #     self.assertParses('1005 N Gravenstein Hwy Suite 500 Sebastopol, CA', [{
    #         'number': '1005',
    #         'pre_dir': 'N',
    #         'street': 'GRAVENSTEIN',
    #         'suffix': 'HWY',
    #         'post_dir': None,
    #         'city': 'SEBASTOPOL',
    #         'state': 'CA',
    #         'zip': None,
    #     }])

    def test_perl8(self):
        self.assertParseContains('1005 N Gravenstein Highway, Sebastopol, CA, 95472',
            {'number': '1005', 'pre_dir': 'N', 'street': 'GRAVENSTEIN', 'suffix': 'HWY', 'post_dir': None, 'city': 'SEBASTOPOL', 'state': 'CA', 'zip': '95472'},
        )

    def test_perl9(self):
        self.assertParseContains('1005 N Gravenstein Highway Sebastopol CA 95472',
            {'number': '1005', 'pre_dir': 'N', 'street': 'GRAVENSTEIN', 'suffix': 'HWY', 'post_dir': None, 'city': 'SEBASTOPOL', 'state': 'CA', 'zip': '95472'},
        )

    def test_perl10(self):
        self.assertParseContains('1005 Gravenstein Hwy N Sebastopol CA',
            {'number': '1005', 'pre_dir': None, 'street': 'GRAVENSTEIN', 'suffix': 'HWY', 'post_dir': 'N', 'city': 'SEBASTOPOL', 'state': 'CA', 'zip': None},
        )

    def test_perl11(self):
        self.assertParseContains('1005 Gravenstein Hwy N, Sebastopol CA',
            {'number': '1005', 'pre_dir': None, 'street': 'GRAVENSTEIN', 'suffix': 'HWY', 'post_dir': 'N', 'city': 'SEBASTOPOL', 'state': 'CA', 'zip': None},
        )

    def test_perl13(self):
        self.assertParseContains('1005 Gravenstein Hwy, North Sebastopol CA',
            {'number': '1005', 'pre_dir': None, 'street': 'GRAVENSTEIN', 'suffix': 'HWY', 'post_dir': None, 'city': 'NORTH SEBASTOPOL', 'state': 'CA', 'zip': None},
        )

    def test_perl18(self):
        self.assertParseContains('1600 Pennsylvania Ave. Washington DC',
            {'number': '1600', 'pre_dir': None, 'street': 'PENNSYLVANIA', 'suffix': 'AVE', 'post_dir': None, 'city': 'WASHINGTON', 'state': 'DC', 'zip': None},
        )

    def test_perl21(self):
        self.assertParseContains('100 South St, Philadelphia, PA',
            {'number': '100', 'pre_dir': None, 'street': 'SOUTH', 'suffix': 'ST', 'post_dir': None, 'city': 'PHILADELPHIA', 'state': 'PA', 'zip': None},
        )

    def test_perl22(self):
        self.assertParseContains('100 S.E. Washington Ave, Minneapolis, MN',
            {'number': '100', 'pre_dir': 'SE', 'street': 'WASHINGTON', 'suffix': 'AVE', 'post_dir': None, 'city': 'MINNEAPOLIS', 'state': 'MN', 'zip': None},
        )

    def test_perl23(self):
        self.assertParseContains('3813 1/2 Some Road, Los Angeles, CA',
            {'number': '3813', 'pre_dir': None, 'street': 'SOME', 'suffix': 'RD', 'post_dir': None, 'city': 'LOS ANGELES', 'state': 'CA', 'zip': None},
        )

    def test_nyc_borough(self):
        self.assertParseContains("187 Bedord Ave, Brooklyn, NY",
            {'number': '187', 'pre_dir': None, 'street': 'BEDORD', 'suffix': 'AVE', 'post_dir': None, 'city': 'BROOKLYN', 'state': 'NY', 'zip': None},
        )

    def test_avenue_b_nyc(self):
        self.assertParseContains("51 Avenue B, New York, NY",
            {'number': '51', 'pre_dir': None, 'street': 'AVENUE B', 'suffix': None, 'post_dir': None, 'city': 'NEW YORK', 'state': 'NY', 'zip': None},
        )

    def test_e20th_st_nyc(self):
        self.assertParseContains("31 East 20th Street, New York, NY",
            {'number': '31', 'pre_dir': 'E', 'street': '20TH', 'suffix': 'ST', 'post_dir': None, 'city': 'NEW YORK', 'state': 'NY', 'zip': None},
        )

    def test_fifth_st_standardization(self):
        self.assertParseContains('175 Fifth St Brooklyn NY',
            {'number': '175', 'pre_dir': None, 'street': '5TH', 'suffix': 'ST', 'post_dir': None, 'city': 'BROOKLYN', 'state': 'NY', 'zip': None},
        )

    def test_bronx_standardization1(self):
        self.assertParseContains('123 Main St Bronx',
            {'number': '123', 'pre_dir': None, 'street': 'MAIN', 'suffix': 'ST', 'post_dir': None, 'city': 'THE BRONX', 'state': None, 'zip': None},
        )

    def test_bronx_standardization2(self):
        self.assertParseContains('123 Main St, The Bronx',
            {'number': '123', 'pre_dir': None, 'street': 'MAIN', 'suffix': 'ST', 'post_dir': None, 'city': 'THE BRONX', 'state': None, 'zip': None},
        )

    def test_broadway_simple(self):
        self.assertParseContains('321 BROADWAY, MANHATTAN',
            {'number': '321', 'pre_dir': None, 'street': 'BROADWAY', 'suffix': None, 'post_dir': None, 'city': 'MANHATTAN', 'state': None, 'zip': None},
        )

    def test_staten_island1(self):
        self.assertParseContains('321 BROADWAY, STATEN ISLAND',
            {'number': '321', 'pre_dir': None, 'street': 'BROADWAY', 'suffix': None, 'post_dir': None, 'city': 'STATEN ISLAND', 'state': None, 'zip': None},
        )

    def test_staten_island2(self):
        self.assertParseContains('349 TRAVIS AVENUE, STATEN ISLAND',
            {'number': '349', 'pre_dir': None, 'street': 'TRAVIS', 'suffix': 'AVE', 'post_dir': None, 'city': 'STATEN ISLAND', 'state': None, 'zip': None},
        )

    def test_queens_address_range(self):
        self.assertParseContains('25-82 MAIN ST, QUEENS',
            {'number': '25', 'pre_dir': None, 'street': 'MAIN', 'suffix': 'ST', 'post_dir': None, 'city': 'QUEENS', 'state': None, 'zip': None},
        )

    def test_ft_washington_ave(self):
        self.assertParseContains('270 FT WASHINGTON AVENUE, MANHATTAN',
            {'number': '270', 'pre_dir': None, 'street': 'FT WASHINGTON', 'suffix': 'AVE', 'post_dir': None, 'city': 'MANHATTAN', 'state': None, 'zip': None},
        )

    def test_east_broadway(self):
        self.assertParseContains('183 EAST BROADWAY, MANHATTAN',
            {'number': '183', 'pre_dir': None, 'street': 'EAST BROADWAY', 'suffix': None, 'post_dir': None, 'city': 'MANHATTAN', 'state': None, 'zip': None},
        )

    def test_one_nob_hill(self):
        self.assertParseContains('1 Nob Hill',
            {'number': '1', 'pre_dir': None, 'street': 'NOB HILL', 'suffix': None, 'post_dir': None, 'city': None, 'state': None, 'zip': None}
        )

    def test_west_irving_park(self):
        self.assertParseContains('1234 W IRVING PARK',
            {'number': '1234', 'pre_dir': 'W', 'street': 'IRVING PARK', 'suffix': None, 'post_dir': None, 'city': None, 'state': None, 'zip': None}
        )

    def test_half_address1(self):
        self.assertParseContains('123 1/2 MAIN ST',
            {'number': '123', 'pre_dir': None, 'street': 'MAIN', 'suffix': 'ST', 'post_dir': None, 'city': None, 'state': None, 'zip': None},
        )

    def test_half_address2(self):
        self.assertParseContains('123 I/2 MAIN ST',
            {'number': '123', 'pre_dir': None, 'street': 'MAIN', 'suffix': 'ST', 'post_dir': None, 'city': None, 'state': None, 'zip': None},
        )

    def test_hyphen_space_address1(self):
        self.assertParseContains('123 - 125 MAIN ST',
            {'number': '123', 'pre_dir': None, 'street': 'MAIN', 'suffix': 'ST', 'post_dir': None, 'city': None, 'state': None, 'zip': None},
        )

    def test_hyphen_space_address2(self):
        self.assertParseContains('123- 125 MAIN ST',
            {'number': '123', 'pre_dir': None, 'street': 'MAIN', 'suffix': 'ST', 'post_dir': None, 'city': None, 'state': None, 'zip': None},
        )

    def test_hyphen_space_address3(self):
        self.assertParseContains('123 -125 MAIN ST',
            {'number': '123', 'pre_dir': None, 'street': 'MAIN', 'suffix': 'ST', 'post_dir': None, 'city': None, 'state': None, 'zip': None},
        )

    def test_multiple_hyphen_space_address(self):
        self.assertParseContains('123--125 MAIN ST',
            {'number': '123', 'pre_dir': None, 'street': 'MAIN', 'suffix': 'ST', 'post_dir': None, 'city': None, 'state': None, 'zip': None},
        )

    def test_letter_in_address(self):
        self.assertParseContains('2833A W CHICAGO AVE',
            {'number': '2833', 'pre_dir': 'W', 'street': 'CHICAGO', 'suffix': 'AVE', 'post_dir': None, 'city': None, 'state': None, 'zip': None},
        )

    def test_letter_in_address_range(self):
        self.assertParseContains('2833A-2835A W CHICAGO AVE',
            {'number': '2833', 'pre_dir': 'W', 'street': 'CHICAGO', 'suffix': 'AVE', 'post_dir': None, 'city': None, 'state': None, 'zip': None},
        )

    def test_mies_van_der_rohe_wy(self):
        self.assertParseContains('830 N MIES VAN DER ROHE WY',
            {'number': '830', 'pre_dir': 'N', 'street': 'MIES VAN DER ROHE', 'suffix': 'WAY', 'post_dir': None, 'city': None, 'state': None, 'zip': None},
        )

    def test_the_bronx_address1(self):
        self.assertParseContains('823 East 147th St, The Bronx',
            {'number': '823', 'pre_dir': 'E', 'street': '147TH', 'suffix': 'ST', 'post_dir': None, 'city': 'THE BRONX', 'state': None, 'zip': None},
        )

    def test_the_bronx_address2(self):
        self.assertParseContains('1401 Grand Concourse, The Bronx',
            {'number': '1401', 'pre_dir': None, 'street': 'GRAND CONCOURSE', 'suffix': None, 'post_dir': None, 'city': 'THE BRONX', 'state': None, 'zip': None},
        )

    def test_the_bronx_address3(self):
        self.assertParseContains('1110 Bronx River Ave, The Bronx',
            {'number': '1110', 'pre_dir': None, 'street': 'BRONX RIVER', 'suffix': 'AVE', 'post_dir': None, 'city': 'THE BRONX', 'state': None, 'zip': None},
        )

if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = reverse
import unittest
from psycopg2 import Binary
from django.contrib.gis.geos import Point
from django.db import connection
from ebpub.streets.models import Block

class ReverseGeocodeError(Exception):
    pass

def reverse_geocode(point):
    """
    Looks up the nearest block to the point.
    """
    # In degrees for now because transforming to a projected space is
    # too slow for this purpose. TODO: store projected versions of the
    # locations alongside the canonical lng/lat versions.
    min_distance = 0.007
    # We use min_distance to cut down on the searchable space, because
    # the distance query we do next that actually compares distances
    # between geometries does not use the spatial index. TODO: convert
    # this to GeoDjango syntax. Should be possible but there are some
    # subtleties / performance issues with the DB API.
    cursor = connection.cursor()
    cursor.execute("""
        SELECT %(field_list)s, ST_Distance(ST_GeomFromWKB(E%(pt_wkb)s, 4326), %(geom_fieldname)s) AS "dist"
        FROM %(tablename)s
        WHERE id IN
            (SELECT id
             FROM %(tablename)s
             WHERE ST_DWithin(%(geom_fieldname)s, ST_GeomFromWKB(E%(pt_wkb)s, 4326), %(min_distance)s))
        ORDER BY "dist"
        LIMIT 1;
    """ % {'field_list': ', '.join([f.column for f in Block._meta.fields]),
           'pt_wkb': Binary(point.wkb),
           'geom_fieldname': 'location',
           'tablename': Block._meta.db_table,
           'min_distance': min_distance})
    num_fields = len(Block._meta.fields)
    try:
        block, distance = [(Block(*row[:num_fields]), row[-1]) for row in cursor.fetchall()][0]
    except IndexError:
        raise ReverseGeocodeError()
    return block, distance

########NEW FILE########
__FILENAME__ = parser
from ebpub.geocoder.parser.parsing import strip_unit
import unittest

class StripUnitTestCase(unittest.TestCase):
    def assertStripUnit(self, text, expected):
        self.assertEqual(strip_unit(text), expected)

    def test_suite01(self):
        self.assertStripUnit('123 Main St Suite 1', '123 Main St')

    def test_suite02(self):
        self.assertStripUnit('123 Main St, Suite 1', '123 Main St')

    def test_suite03(self):
        self.assertStripUnit('123 Main St, Suite 2', '123 Main St')

    def test_suite04(self):
        self.assertStripUnit('123 Main St, Suite #2', '123 Main St')

    def test_suite05(self):
        self.assertStripUnit('123 Main St, Suite 465', '123 Main St')

    def test_suite06(self):
        self.assertStripUnit('123 Main St, Suite A', '123 Main St')

    def test_suite07(self):
        self.assertStripUnit('123 Main St, Suite AB', '123 Main St')

    def test_suite08(self):
        self.assertStripUnit('123 Main St, Suite 1A', '123 Main St')

    def test_suite09(self):
        self.assertStripUnit('123 Main St, Suite 2B', '123 Main St')

    def test_suite10(self):
        self.assertStripUnit('123 Main St, Suite 1-A', '123 Main St')

    def test_suite11(self):
        self.assertStripUnit('123 Main St, Suite #1', '123 Main St')

    def test_suite12(self):
        self.assertStripUnit('123 Main St, Suite #1A', '123 Main St')

    def test_suite13(self):
        self.assertStripUnit('123 Main St, Suite #1-A', '123 Main St')

    def test_suite14(self):
        self.assertStripUnit('123 Main St, Suite #A', '123 Main St')

    def test_ste1(self):
        self.assertStripUnit('123 Main St, Ste 14', '123 Main St')

    def test_ste2(self):
        self.assertStripUnit('123 Main St, Ste. 14', '123 Main St')

    def test_unit1(self):
        self.assertStripUnit('123 Main St, Unit 1W', '123 Main St')

    def test_hash1(self):
        self.assertStripUnit('123 Main St, #1', '123 Main St')

    def test_hash_dash(self):
        self.assertStripUnit('123 Main St, Ste K-#b', '123 Main St')

    def test_apt01(self):
        self.assertStripUnit('123 Main St, Apt 1', '123 Main St')

    def test_apt02(self):
        self.assertStripUnit('123 Main St, Apt. 1', '123 Main St')

    def test_space01(self):
        self.assertStripUnit('3015 Grand Avenue, Space 310', '3015 Grand Avenue')

    def test_unit_with_colon(self):
        self.assertStripUnit('325 Arlington Ave Unit: 140', '325 Arlington Ave')

if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = tests
from ebpub.geocoder import SmartGeocoder, AmbiguousResult, InvalidBlockButValidStreet
import os.path
import unittest
import yaml

class GeocoderTestCase(unittest.TestCase):
    address_fields = ('address', 'city', 'zip')

    def load_fixtures(self):
        fixtures_filename = 'locations.yaml'
        locations = yaml.load(open(os.path.join(os.path.dirname(__file__), fixtures_filename)))
        for key, value in locations.items():
            pass

    def assertAddressMatches(self, expected, actual):
        unmatched_fields = []

        for field in self.address_fields:
            try:
                self.assertEqual(expected[field], actual[field])
            except AssertionError, e:
                unmatched_fields.append(field)

        if unmatched_fields:
            raise AssertionError('unmatched address fields: %s' % ', '.join(unmatched_fields))

    def assertNearPoint(self, point, other):
        try:
            self.assertAlmostEqual(point.x, other.x, places=3)
            self.assertAlmostEqual(point.y, other.y, places=3)
        except AssertionError, e:
            raise AssertionError('`point\' not near enough to `other\': %s', e)

class BaseGeocoderTestCase(unittest.TestCase):
    fixtures = ['wabash.yaml']

    def setUp(self):
        self.geocoder = SmartGeocoder(use_cache=False)

    def test_address_geocoder(self):
        address = self.geocoder.geocode('200 S Wabash')
        self.assertEqual(address['city'], 'Chicago')

    def test_address_geocoder_ambiguous(self):
        self.assertRaises(AmbiguousResult, self.geocoder.geocode, '200 Wabash')

    def test_address_geocoder_invalid_block(self):
        self.assertRaises(InvalidBlockButValidStreet, self.geocoder.geocode, '100000 S Wabash')

    def test_block_geocoder(self):
        address = self.geocoder.geocode('200 block of Wabash')
        self.assertEqual(address['city'], 'Chicago')

    def test_intersection_geocoder(self):
        address = self.geocoder.geocode('Wabash and Jackson')
        self.assertEqual(address['city'], 'CHICAGO')

if __name__ == '__main__':
    pass

########NEW FILE########
__FILENAME__ = allmetros
from django.conf import settings

METRO_LIST = settings.METRO_LIST
METRO_DICT = dict([(m['short_name'], m) for m in METRO_LIST])

def get_metro(short_name=None):
    if short_name is None:
        short_name = settings.SHORT_NAME
    return METRO_DICT[short_name]

########NEW FILE########
__FILENAME__ = loader
from django.contrib.gis.gdal import DataSource
from django.contrib.gis.geos import MultiPolygon
from ebpub.metros.allmetros import METRO_DICT
from ebpub.metros.models import Metro

class Usage(Exception):
    pass

def load_metro(short_name, shpfile, layer_id=0):
    """
    Creates a new Metro object, populating geometry from shapefile and
    the rest of its fields from the old settings module.
    """
    ds = DataSource(shpfile)
    lyr = ds[layer_id]
    model_fields = set([f.name for f in Metro._meta.fields])
    metro_from_settings = METRO_DICT[short_name]
    settings_fields = set(metro_from_settings.keys())
    metro = Metro()
    for f in (model_fields & settings_fields):
        setattr(metro, f, metro_from_settings[f])
    metro.name = metro_from_settings['city_name']
    metro_geom = None
    for feature in lyr:
        if metro_geom is None:
            geom = feature.geom.geos
            geom_type = geom.geom_type
            if geom_type == 'Polygon':
                # Normalize to MultiPolygon
                metro_geom = MultiPolygon([geom])
            elif geom_type == 'MultiPolygon':
                metro_geom = geom
            else:
                raise ValueError('expected Polygon or MultiPolygon, got %s' % geom_type)
    metro.location = metro_geom
    metro.save()
    return metro

def main():
    import getopt
    import sys

    (opts, args) = getopt.getopt(sys.argv[1:], 'h', ['help'])
    try:
        for opt, value in opts:
            if opt in ('-h', '--help'):
                raise Usage()
        if len(args) != 2:
            raise Usage()
        try:
            metro = load_metro(args[0], args[1])
        except ValueError, e:
            print >> sys.stderr, e
            return 1
        else:
            print 'Created %s' % metro
            return 0
    except Usage:
        print >> sys.stderr, '%s: <short_name> /path/to/shapefile' % sys.argv[0]
        return 1

if __name__ == '__main__':
    import sys
    sys.exit(main())

########NEW FILE########
__FILENAME__ = models
from django.conf import settings
from django.contrib.gis.db import models
from ebpub.utils import multidb

class MetroManager(multidb.GeoManager):
    def get_current(self):
        return self.get(short_name=settings.SHORT_NAME)

    def containing_point(self, point):
        # First pass, just check to see if it's in the bounding box --
        # this is faster for checking across all metros
        metros = self.filter(location__bbcontains=point)
        n = metros.count()
        if not n:
            raise Metro.DoesNotExist()
        else:
            # Now do the slower but more accurate lookup to see if the
            # point is completely within the actual bounds of the
            # metro. Note that we could also have hit two or more
            # metros if they have overlapping bounding boxes.
            matches = 0
            for metro in metros:
                if metro.location.contains(point):
                    matches += 1
            if matches > 1:
                # Something went wrong, it would mean the metros have
                # overlapping borders
                raise Exception('more than one metro found to contain this point')
            elif matches == 0:
                raise Metro.DoesNotExist()
            else:
                return metro

class Metro(models.Model):
    name = models.CharField(max_length=64)
    short_name = models.CharField(max_length=64, unique=True)
    metro_name = models.CharField(max_length=64)
    population = models.IntegerField(null=True, blank=True)
    area = models.IntegerField(null=True, blank=True)
    is_public = models.BooleanField(default=False)
    multiple_cities = models.BooleanField(default=False)
    state = models.CharField(max_length=2)
    state_name = models.CharField(max_length=64)
    location = models.MultiPolygonField()
    objects = MetroManager('metros')

    def __unicode__(self):
        return self.name

    class Meta:
        unique_together = ('name', 'state')

########NEW FILE########
__FILENAME__ = tests
from django.test import TestCase
from django.contrib.gis.geos import Point
from ebpub.metros.models import Metro

pt_in_chicago = Point((-87.68489561595398, 41.852929331184384)) # point in center of Chicago
pt_in_chi_bbox = Point((-87.83384627077956, 41.85365447332586)) # point just west of Chicago's border but due south of O'Hare
pt_in_lake_mi = Point((-86.99514699540548, 41.87468001919902)) # point way out in Lake Michigan

class MetroTest(TestCase):
    fixtures = ['metros']
    
    def test_point_in_metro(self):
        """
        Tests finding a metro with a point contained by its boundary
        """
        self.assertEquals(Metro.objects.containing_point(pt_in_chicago).name, 'Chicago')

    def test_point_in_bbox_not_in_metro(self):
        """
        Tests with a point in the metro's bounding box but not in its boundary
        """
        self.assertRaises(Metro.DoesNotExist, Metro.objects.containing_point, pt_in_chi_bbox)

    def test_point_not_in_metro(self):
        """
        Tests with a point not in any metro
        """
        self.assertRaises(Metro.DoesNotExist, Metro.objects.containing_point, pt_in_lake_mi)

class MetroViewsTest(TestCase):
    fixtures = ['metros']

    def test_lookup_metro_success(self):
        """
        Tests getting a successful JSON response from a lng/lat query
        """
        response = self.client.get('/metros/lookup/', {'lng': pt_in_chicago.x, 'lat': pt_in_chicago.y}) 
        self.assertContains(response, 'Chicago', status_code=200)
        self.assertEqual(response['content-type'], 'application/javascript')

    def test_lookup_metro_in_bbox_fails(self):
        """
        Tests getting a 404 from a lng/lat query not quite in the metro
        """
        response = self.client.get('/metros/lookup/', {'lng': pt_in_chi_bbox.x, 'lat': pt_in_chi_bbox.y}) 
        self.assertEqual(response.status_code, 404)

    def test_lookup_metro_fails(self):
        """
        Tests getting a 404 from a lng/lat query not in any metro
        """
        response = self.client.get('/metros/lookup/', {'lng': pt_in_lake_mi.x, 'lat': pt_in_lake_mi.y}) 
        self.assertEqual(response.status_code, 404)

########NEW FILE########
__FILENAME__ = urls
from django.conf.urls.defaults import *
from django.views.generic import list_detail
from ebpub.metros import views
from ebpub.metros.models import Metro

urlpatterns = patterns('',
    (r'^$', list_detail.object_list, {'queryset': Metro.objects.order_by('name'), 'template_object_name': 'metro'}),
    (r'^lookup/$', views.lookup_metro),
)

########NEW FILE########
__FILENAME__ = views
from django.contrib.gis.geos import Point
from django.http import HttpResponse, Http404
from django.utils import simplejson as json
from ebpub.metros.models import Metro

def lookup_metro(request):
    """
    Lookups up a metro that contains the point represented by the two
    GET parameters, `lng' and `lat'.

    Returns a JSON object representing the Metro, minus the actual
    geometry.
    """
    try:
        lng = float(request.GET['lng'])
        lat = float(request.GET['lat'])
    except (KeyError, ValueError, TypeError):
        raise Http404('Missing/invalid lng and lat query parameters')

    try:
        metro = Metro.objects.containing_point(Point(lng, lat))
    except Metro.DoesNotExist:
        raise Http404("Couldn't find any metro matching that query")

    fields = [f.name for f in metro._meta.fields]
    fields.remove('location')
    metro = dict([(f, metro.serializable_value(f)) for f in fields])

    return HttpResponse(json.dumps(metro), mimetype='application/javascript')

########NEW FILE########
__FILENAME__ = models
from django.db import models
from ebpub.db.models import Schema

class Petition(models.Model):
    # NULL schema means a city-level petition (i.e., not tied to a schema).
    schema = models.ForeignKey(Schema, blank=True, null=True)
    slug = models.CharField(max_length=64, unique=True, blank=True)
    data_name = models.CharField(max_length=64, blank=True)
    teaser = models.CharField(max_length=255, blank=True)
    petition = models.TextField()
    creation_date = models.DateField()

    def __unicode__(self):
        return self.full_data_name()

    def full_data_name(self):
        if self.schema:
            return self.schema.name
        return self.data_name

class Petitioner(models.Model):
    petition = models.ForeignKey(Petition)
    name = models.CharField(max_length=100)
    location = models.CharField(max_length=100)
    city = models.CharField(max_length=30)
    state = models.CharField(max_length=2)
    email = models.EmailField()
    notes = models.TextField()
    date_signed = models.DateTimeField()
    ip_address = models.IPAddressField()

    def __unicode__(self):
        return self.name

########NEW FILE########
__FILENAME__ = views
from django import forms
from django.http import HttpResponseRedirect
from django.shortcuts import get_object_or_404
from ebpub.metros.allmetros import get_metro
from ebpub.petitions.models import Petition, Petitioner
from ebpub.streets.utils import full_geocode
from ebpub.utils.view_utils import eb_render
import datetime

class LocationField(forms.CharField):
    def clean(self, value):
        if not value:
            raise forms.ValidationError('Enter your location.')
        try:
            result = full_geocode(value, search_places=False)
        except Exception:
            raise forms.ValidationError("We're not familiar with this location. Could you please enter another one that we'd know, like a ZIP code, perhaps?")
        if result['ambiguous'] and result['type'] != 'block':
            raise forms.ValidationError("This location is ambiguous. Please enter one of the following: %s" % ', '.join([r['address'] for r in result['result']]))
        return value

class PetitionForm(forms.Form):
    name = forms.CharField(max_length=100, widget=forms.TextInput(attrs={'size': 30}))
    location = LocationField(max_length=100, widget=forms.TextInput(attrs={'size': 30}))
    city = forms.CharField(max_length=30, widget=forms.TextInput(attrs={'size': 30}), initial=get_metro()['city_name'])
    state = forms.CharField(max_length=2, widget=forms.TextInput(attrs={'size': 2}), initial=get_metro()['state'])
    email = forms.EmailField(widget=forms.TextInput(attrs={'size': 30}))
    notes = forms.CharField(required=False, widget=forms.Textarea(attrs={'cols': 35, 'rows': 4}))

def form_view(request, slug, is_schema):
    if is_schema:
        p = get_object_or_404(Petition, schema__slug=slug)
    else:
        p = get_object_or_404(Petition, slug=slug)
    if request.method == 'POST':
        form = PetitionForm(request.POST)
        if form.is_valid():
            cd = form.cleaned_data
            ip_address = request.META.get('HTTP_X_FORWARDED_FOR', '').split(',')[0] or request.META.get('REMOTE_ADDR', '')
            Petitioner.objects.create(
                petition=p,
                name=cd['name'].strip(),
                location=cd['location'].strip(),
                city=cd['city'].strip(),
                state=cd['state'].strip(),
                email=cd['email'].strip().lower(),
                notes=cd['notes'].strip(),
                date_signed=datetime.datetime.now(),
                ip_address=ip_address,
            )
            return HttpResponseRedirect('thanks/')
    else:
        form = PetitionForm()
    return eb_render(request, 'petitions/form.html', {'form': form, 'is_schema': is_schema, 'petition': p})

def form_thanks(request, slug, is_schema):
    if is_schema:
        p = get_object_or_404(Petition.objects.select_related(), schema__slug=slug)
    else:
        p = get_object_or_404(Petition, slug=slug)
    return eb_render(request, 'petitions/thanks.html', {'is_schema': is_schema, 'petition': p})

########NEW FILE########
__FILENAME__ = models
from django.db import models
from ebpub.db.models import Schema

class HiddenSchema(models.Model):
    user_id = models.IntegerField()
    schema = models.ForeignKey(Schema)

    def _get_user(self):
        if not hasattr(self, '_user_cache'):
            from ebpub.accounts.models import User
            try:
                self._user_cache = User.objects.get(id=self.user_id)
            except User.DoesNotExist:
                self._user_cache = None
        return self._user_cache
    user = property(_get_user)

    def __unicode__(self):
        return u'<HiddenSchema %s for user %s>' % (self.user_id, self.schema.slug)

########NEW FILE########
__FILENAME__ = views
from django import http
from ebpub.db.models import Schema
from ebpub.preferences.models import HiddenSchema

def ajax_save_hidden_schema(request):
    """
    Creates a HiddenSchema for request.POST['schema'] and request.user.
    """
    if request.method != 'POST':
        raise http.Http404()
    if 'schema' not in request.POST:
        raise http.Http404('Missing schema')
    if not request.user:
        raise http.Http404('Not logged in')

    # Validate that the HiddenSchema hasn't already been created for this user,
    # to avoid duplicates.
    try:
        schema = Schema.public_objects.get(slug=request.POST['schema'])
        sp = HiddenSchema.objects.get(user_id=request.user.id, schema=schema)
    except Schema.DoesNotExist:
        return http.HttpResponse('0') # Schema doesn't exist.
    except HiddenSchema.DoesNotExist:
        pass
    else:
        return http.HttpResponse('0') # Already exists.

    HiddenSchema.objects.create(user_id=request.user.id, schema=schema)
    return http.HttpResponse('1')

def ajax_remove_hidden_schema(request):
    """
    Removes the HiddenSchema for request.POST['schema'] and request.user.
    """
    if request.method != 'POST':
        raise http.Http404()
    if 'schema' not in request.POST:
        raise http.Http404('Missing schema')
    if not request.user:
        raise http.Http404('Not logged in')

    try:
        hidden_schema = HiddenSchema.objects.filter(user_id=request.user.id, schema__slug=request.POST['schema'])
    except HiddenSchema.DoesNotExist:
        # The schema didn't exist. This is a no-op.
        return http.HttpResponse('0')
    hidden_schema.delete()
    return http.HttpResponse('1')

########NEW FILE########
__FILENAME__ = models
from django.db import models
from ebpub.db.models import Location
from ebpub.streets.models import Block

class SavedPlace(models.Model):
    user_id = models.IntegerField()
    block = models.ForeignKey(Block, blank=True, null=True)
    location = models.ForeignKey(Location, blank=True, null=True)
    nickname = models.CharField(max_length=128, blank=True)

    def __unicode__(self):
        return u'User %s: %u' % (self.user_id, self.place.pretty_name)

    def _get_place(self):
        return self.block_id and self.block or self.location
    place = property(_get_place)

    def _get_user(self):
        if not hasattr(self, '_user_cache'):
            from ebpub.accounts.models import User
            try:
                self._user_cache = User.objects.get(id=self.user_id)
            except User.DoesNotExist:
                self._user_cache = None
        return self._user_cache
    user = property(_get_user)

    def pid(self):
        if self.block_id:
            return 'b:%s.8' % self.block_id
        else:
            return 'l:%s' % self.location_id

########NEW FILE########
__FILENAME__ = views
from django import http
from django.utils import simplejson
from ebpub.db.views import parse_pid
from ebpub.savedplaces.models import SavedPlace
from ebpub.streets.models import Block

def ajax_save_place(request):
    """
    Creates a SavedPlace for request.POST['pid'] and request.user.
    """
    if request.method != 'POST':
        raise http.Http404()
    if 'pid' not in request.POST:
        raise http.Http404('Missing pid')
    if not request.user:
        raise http.Http404('Not logged in')

    place, block_radius, xy_radius = parse_pid(request.POST['pid'])
    kwargs = {'user_id': request.user.id}
    if isinstance(place, Block):
        block, location = place, None
        kwargs['block__id'] = place.id
    else:
        block, location = None, place
        kwargs['location__id'] = place.id

    # Validate that the SavedPlace hasn't already been created for this user,
    # to avoid duplicates.
    try:
        sp = SavedPlace.objects.get(**kwargs)
    except SavedPlace.DoesNotExist:
        pass
    else:
        return http.HttpResponse('0') # Already exists.

    SavedPlace.objects.create(
        user_id=request.user.id,
        block=block,
        location=location,
        nickname=request.POST.get('nickname', '').strip(),
    )
    return http.HttpResponse('1')

def ajax_remove_place(request):
    """
    Removes the SavedPlace for request.POST['pid'] and request.user.
    """
    if request.method != 'POST':
        raise http.Http404()
    if 'pid' not in request.POST:
        raise http.Http404('Missing pid')
    if not request.user:
        raise http.Http404('Not logged in')

    place, block_radius, xy_radius = parse_pid(request.POST['pid'])
    kwargs = {'user_id': request.user.id}
    if isinstance(place, Block):
        block, location = place, None
        kwargs['block__id'] = place.id
    else:
        block, location = None, place
        kwargs['location__id'] = place.id

    SavedPlace.objects.filter(**kwargs).delete()
    return http.HttpResponse('1')

def json_saved_places(request):
    """
    Returns JSON of SavedPlaces for request.user, or an empty list
    if the user isn't logged in.
    """
    if not request.user:
        result = []
    else:
        result = [{'name': sp.place.pretty_name, 'url': sp.place.url()} for sp in SavedPlace.objects.filter(user_id=request.user.id)]
    return http.HttpResponse(simplejson.dumps(result), mimetype='application/javascript')

########NEW FILE########
__FILENAME__ = settings
import os.path

########################
# CORE DJANGO SETTINGS #
########################

DATABASE_ENGINE = 'postgresql_psycopg2' # ebpub only supports postgresql_psycopg2.
DATABASE_USER = ''
DATABASE_NAME = ''
DATABASE_HOST = ''
DATABASE_PORT = ''
DEBUG = True

TEMPLATE_DIRS = (
    os.path.normpath(os.path.join(os.path.dirname(__file__), 'templates')),
)
TEMPLATE_LOADERS = (
    'django.template.loaders.filesystem.load_template_source',
)
TEMPLATE_CONTEXT_PROCESSORS = (
    'ebpub.accounts.context_processors.user',
)

INSTALLED_APPS = (
    'ebpub.accounts',
    'ebpub.alerts',
    'ebpub.db',
    'ebpub.geocoder',
    'ebpub.petitions',
    'ebpub.preferences',
    'ebpub.savedplaces',
    'ebpub.streets',
    'django.contrib.humanize',
    'django.contrib.sessions',
)

ROOT_URLCONF = 'ebpub.urls'
MIDDLEWARE_CLASSES = (
    'django.middleware.common.CommonMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'ebpub.accounts.middleware.UserMiddleware',
)

#########################
# CUSTOM EBPUB SETTINGS #
#########################

# The domain for your site.
EB_DOMAIN = 'example.com'

# This is the short name for your city, e.g. "chicago".
SHORT_NAME = ''

# Set both of these to distinct, secret strings that include two instances
# of '%s' each. Example: 'j8#%s%s' -- but don't use that, because it's not
# secret.
PASSWORD_CREATE_SALT = ''
PASSWORD_RESET_SALT = ''

# Here, we define the different databases we use, giving each one a label
# (like 'users') so we can refer to a particular database via multidb
# managers.
#
# Note that we only need to define databases that are used by multidb
# managers -- not our default database for this settings file. Any Django
# model code that doesn't use the multidb manager will use the standard
# DATABASE_NAME/DATABASE_USER/etc. settings.
#
# THE UPSHOT: If you're only using one database, the only thing you'll need
# to set here is TIME_ZONE.
DATABASES = {
    'users': {
        'DATABASE_HOST': DATABASE_HOST,
        'DATABASE_NAME': DATABASE_NAME,
        'DATABASE_OPTIONS': {},
        'DATABASE_PASSWORD': '',
        'DATABASE_PORT': DATABASE_PORT,
        'DATABASE_USER': DATABASE_USER,
        'TIME_ZONE': '', # Same format as Django's TIME_ZONE setting.
    },
    'metros': {
        'DATABASE_HOST': DATABASE_HOST,
        'DATABASE_NAME': DATABASE_NAME,
        'DATABASE_OPTIONS': {},
        'DATABASE_PASSWORD': '',
        'DATABASE_PORT': DATABASE_PORT,
        'DATABASE_USER': DATABASE_USER, 
        'TIME_ZONE': '', # Same format as Django's TIME_ZONE setting.
    },
}

# The list of all metros this installation covers. This is a tuple of
# dictionaries.
METRO_LIST = (
    # Example dictionary:
    # {
    #     # Extent of the metro, as a longitude/latitude bounding box.
    #     'extent': (-71.191153, 42.227865, -70.986487, 42.396978),
    #
    #     # Whether this should be displayed to the public.
    #     'is_public': True,
    #
    #     # Set this to True if the metro has multiple cities.
    #     'multiple_cities': False,
    #
    #     # The major city in the metro.
    #     'city_name': 'Boston',
    #
    #     # The SHORT_NAME in the settings file (also the subdomain).
    #     'short_name': 'boston',
    #
    #     # The name of the metro, as opposed to the city (e.g., "Miami-Dade" instead of "Miami").
    #     'metro_name': 'Boston',
    #
    #     # USPS abbreviation for the state.
    #     'state': 'MA',
    #
    #     # Full name of state.
    #     'state_name': 'Massachusetts',
    #
    #     # Time zone, as required by Django's TIME_ZONE setting.
    #     'time_zone': 'America/New_York',
    # },
)

EB_MEDIA_ROOT = '' # necessary for static media versioning
EB_MEDIA_URL = '' # leave at '' for development

# Overrides datetime.datetime.today(), for development.
EB_TODAY_OVERRIDE = None

# Filesystem location of shapefiles for maps, e.g., '/home/shapefiles'.
SHAPEFILE_ROOT = ''

# For the 'autoversion' template tag.
AUTOVERSION_STATIC_MEDIA = False

# Connection info for mapserver.
MAPS_POSTGIS_HOST = '127.0.0.1'
MAPS_POSTGIS_USER = ''
MAPS_POSTGIS_PASS = ''
MAPS_POSTGIS_DB = ''

# This is used as a "From:" in e-mails sent to users.
GENERIC_EMAIL_SENDER = 'example@example.com'

# Map stuff.
MAP_SCALES = [614400, 307200, 153600, 76800, 38400, 19200, 9600, 4800, 2400, 1200]
SPATIAL_REF_SYS = '900913' # Spherical Mercator
MAP_UNITS = 'm' # see ebgeo.maps.utils for allowed unit types

# Filesystem location of tilecache config (e.g., '/etc/tilecache/tilecache.cfg').
TILECACHE_CONFIG = ''

# Filesystem location of scraper log.
SCRAPER_LOGFILE_NAME = '/tmp/scraperlog'

DATA_HARVESTER_CONFIG = {}

MAIL_STORAGE_PATH = '/home/mail'

# If this cookie is set with the given value, then the site will give the user
# staff privileges (including the ability to view non-public schemas).
STAFF_COOKIE_NAME = ''
STAFF_COOKIE_VALUE = ''

########NEW FILE########
__FILENAME__ = populate_suburbs
from ebpub.geocoder.parser.parsing import normalize
from ebpub.streets.models import Suburb

def populate_suburbs(suburb_list):
    for suburb in suburb_list:
        Suburb.objects.create(name=suburb, normalized_name=normalize(suburb))

if __name__ == "__main__":
    import sys
    suburb_list = [line for line in open(sys.argv[1], 'r').read().split('\n') if line]
    populate_suburbs(suburb_list)

########NEW FILE########
__FILENAME__ = base
from django.contrib.gis.gdal import DataSource
from ebpub.streets.models import Block
from ebpub.streets.name_utils import make_pretty_name
from ebpub.utils.text import slugify

class BlockImporter(object):
    def __init__(self, shapefile, layer_id=0):
        self.layer = DataSource(shapefile)[layer_id]

    def save(self, verbose=True):
        num_created = 0
        for feature in self.layer:
            parent_id = None
            if not self.skip_feature(feature):
                for block_fields in self.gen_blocks(feature):
                    block = Block(**block_fields)
                    block.geom = feature.geom.geos
                    street_name, block_name = make_pretty_name(
                        block_fields['left_from_num'],
                        block_fields['left_to_num'],
                        block_fields['right_from_num'],
                        block_fields['right_to_num'],
                        block_fields['predir'],
                        block_fields['street'],
                        block_fields['suffix'],
                        block_fields['postdir']
                    )
                    block.pretty_name = block_name
                    block.street_pretty_name = street_name
                    block.street_slug = slugify(' '.join((block_fields['street'], block_fields['suffix'])))
                    block.save()
                    if parent_id is None:
                        parent_id = block.id
                    else:
                        block.parent_id = parent_id
                        block.save()
                    num_created += 1
                    if verbose:
                        print 'Created block %s' % block
        return num_created

    def skip_feature(self, feature):
        """
        Subclasses can override this method to determine whether to
        skip this feature, for example, because the feature is not a
        street or is missing an address number.

        It could also be used to provide geometric filtering, for
        example, a subclass could inspect the geom attribute of the
        feature to determine if it is contained by a particular
        geometry.
        """
        return True

    def gen_blocks(self, feature):
        """
        A generator that yields dictionaries (of keys that are BLOCK_FIELDS)
        """
        raise NotImplementedError('subclass must implement this method')


########NEW FILE########
__FILENAME__ = base
from django.contrib.gis.gdal import DataSource

class EsriImporter(DataSource):
    def __init__(self, shapefile, model, ...)

########NEW FILE########
__FILENAME__ = blocks
import re
import sys
from django.contrib.gis.gdal import DataSource
from ebpub.metros.models import Metro
from ebpub.streets.models import Block
from ebpub.streets.name_utils import make_pretty_name
from ebpub.utils.text import slugify

FIELD_MAP = {
    # ESRI        # Block
    'L_F_ADD'   : 'left_from_num',
    'L_T_ADD'   : 'left_to_num',
    'R_F_ADD'   : 'right_from_num',
    'R_T_ADD'   : 'right_to_num',
    'POSTAL_L'  : 'left_zip',
    'POSTAL_R'  : 'right_zip',
    'GEONAME_L' : 'left_city',
    'GEONAME_R' : 'right_city',
    'STATE_L'   : 'left_state',
    'STATE_R'   : 'right_state',
}

NAME_FIELD_MAP = {
    'NAME'      : 'street',
    'TYPE'      : 'suffix',
    'PREFIX'    : 'predir',
    'SUFFIX'    : 'postdir',
}

# FCC == feature classification code: indicates the type of road
VALID_FCC_PREFIXES = (
    'A1', # primary highway with limited access
    'A2', # primary road without limited access
    'A3', # secondary and connecting road
    'A4'  # local, neighborhood, and rural road
)

class EsriImporter(object):
    def __init__(self, shapefile, city=None, layer_id=0):
        ds = DataSource(shapefile)
        self.layer = ds[layer_id]
        self.city = city and city or Metro.objects.get_current().name
        self.fcc_pat = re.compile('^(' + '|'.join(VALID_FCC_PREFIXES) + ')\d$')

    def save(self, verbose=False):
        alt_names_suff = ('', '1', '2', '3', '4', '5')
        num_created = 0
        for i, feature in enumerate(self.layer):
            if not self.fcc_pat.search(feature.get('FCC')):
                continue
            parent_id = None
            fields = {}
            for esri_fieldname, block_fieldname in FIELD_MAP.items():
                value = feature.get(esri_fieldname)
                if isinstance(value, basestring):
                    value = value.upper()
                elif isinstance(value, int) and value == 0:
                    value = None
                fields[block_fieldname] = value
            if not ((fields['left_from_num'] and fields['left_to_num']) or
                    (fields['right_from_num'] and fields['right_to_num'])):
                continue
            # Sometimes the "from" number is greater than the "to"
            # number in the source data, so we swap them into proper
            # ordering
            for side in ('left', 'right'):
                from_key, to_key = '%s_from_num' % side, '%s_to_num' % side
                if fields[from_key] > fields[to_key]:
                    fields[from_key], fields[to_key] = fields[to_key], fields[from_key]
            if feature.geom.geom_name != 'LINESTRING':
                continue
            for suffix in alt_names_suff:
                name_fields = {}
                for esri_fieldname, block_fieldname in NAME_FIELD_MAP.items():
                    key = esri_fieldname + suffix
                    name_fields[block_fieldname] = feature.get(key).upper()
                if not name_fields['street']:
                    continue
                # Skip blocks with bare number street names and no suffix / type
                if not name_fields['suffix'] and re.search('^\d+$', name_fields['street']):
                    continue
                fields.update(name_fields)
                block = Block(**fields)
                block.geom = feature.geom.geos
                street_name, block_name = make_pretty_name(
                    fields['left_from_num'],
                    fields['left_to_num'],
                    fields['right_from_num'],
                    fields['right_to_num'],
                    fields['predir'],
                    fields['street'],
                    fields['suffix'],
                    fields['postdir']
                )
                block.pretty_name = block_name
                block.street_pretty_name = street_name
                block.street_slug = slugify(' '.join((fields['street'], fields['suffix'])))
                block.save()
                if parent_id is None:
                    parent_id = block.id
                else:
                    block.parent_id = parent_id
                    block.save()
                num_created += 1
                if verbose:
                    print >> sys.stderr, 'Created block %s' % block
        return num_created

########NEW FILE########
__FILENAME__ = zipcodes
import datetime
from django.contrib.gis.gdal import DataSource
from django.contrib.gis.geos import MultiPolygon
from ebpub.db.models import Location, LocationType
import sys

class EsriImporter(object):
    def __init__(self, shapefile, city, layer_id=0):
        ds = DataSource(shapefile)
        self.layer = ds[layer_id]
        self.city = city
        self.location_type, _ = LocationType.objects.get_or_create(
            name = 'ZIP Code',
            plural_name = 'ZIP Codes',
            scope = 'U.S.A.',
            slug = 'zipcodes',
            is_browsable = True,
            is_significant = True,
        )

    def save(self, verbose=False):
        # The ESRI ZIP Code layer breaks ZIP Codes up along county
        # boundaries, so we need to collapse them first before
        # proceeding
        zipcodes = {}
        for feature in self.layer:
            zipcode = feature.get('POSTAL')
            geom = feature.geom.geos
            if zipcode not in zipcodes:
                zipcodes[zipcode] = geom
            else:
                # If it's a MultiPolygon geom we're adding to our
                # existing geom, we need to "unroll" it into its
                # constituent polygons 
                if isinstance(geom, MultiPolygon):
                    subgeoms = list(geom)
                else:
                    subgeoms = [geom]
                existing_geom = zipcodes[zipcode]
                if not isinstance(existing_geom, MultiPolygon):
                    new_geom = MultiPolygon([existing_geom])
                    new_geom.extend(subgeoms)
                    zipcodes[zipcode] = new_geom
                else:
                    existing_geom.extend(subgeoms)

        sorted_zipcodes = sorted(zipcodes.iteritems(), key=lambda x: int(x[0]))
        now = datetime.datetime.now()
        num_created = 0
        for i, (zipcode, geom) in enumerate(sorted_zipcodes):
            if not geom.valid:
                geom = geom.buffer(0.0)
                if not geom.valid:
                    print >> sys.stderr, 'Warning: invalid geometry for %s' % zipcode
            geom.srid = 4326
            zipcode_obj, created = Location.objects.get_or_create(
                name = zipcode,
                normalized_name = zipcode,
                slug = zipcode,
                location_type = self.location_type,
                location = geom,
                centroid = geom.centroid,
                display_order = i,
                city = self.city,
                source = 'ESRI',
                area = geom.transform(3395, True).area,
                is_public = True,
                creation_date = now,
                last_mod_date = now,
            )
            if created:
                num_created += 1
            if verbose:
                print >> sys.stderr, '%s ZIP Code %s ' % (created and 'Created' or 'Already had', zipcode_obj.name)
        return num_created

########NEW FILE########
__FILENAME__ = importesri
from django.core.management.base import BaseCommand, CommandError
from ebpub.streets.blockimport.esri import importers

class Command(BaseCommand):
    help = 'Import a shapefile from the ESRI data'
    
    def handle(self, *args, **options):
        if len(args) != 3:
            raise CommandError('Usage: import_esri <importer_type> <city> </path/to/shapefile/>')
        (importer_type, city, shapefile) = args
        importer_mod = getattr(importers, importer_type, None) 
        if importer_mod is None:
            raise CommandError('Invalid importer_type %s' % importer_type)
        importer_cls = getattr(importer_mod, 'EsriImporter', None)
        if importer_cls is None:
            raise CommandError('importer module must define an EsriImporter class')
        importer = importer_cls(shapefile, city)
        if options['verbosity'] == 2:
            verbose = True
        else:
            verbose = False
        num_created = importer.save(verbose)
        if options['verbosity'] > 0:
            print 'Created %d %s(s)' % (num_created, importer_type)

########NEW FILE########
__FILENAME__ = import_blocks
#!/usr/bin/env python
import sys
import optparse
from django.contrib.gis.gdal import DataSource
from ebdata.parsing import dbf
from ebpub.streets.blockimport import BlockImporter

STATE_FIPS = {
    '02': ('AK', 'ALASKA'),
    '01': ('AL', 'ALABAMA'),
    '05': ('AR', 'ARKANSAS'),
    '60': ('AS', 'AMERICAN SAMOA'),
    '04': ('AZ', 'ARIZONA'),
    '06': ('CA', 'CALIFORNIA'),
    '08': ('CO', 'COLORADO'),
    '09': ('CT', 'CONNECTICUT'),
    '11': ('DC', 'DISTRICT OF COLUMBIA'),
    '10': ('DE', 'DELAWARE'),
    '12': ('FL', 'FLORIDA'),
    '13': ('GA', 'GEORGIA'),
    '66': ('GU', 'GUAM'),
    '15': ('HI', 'HAWAII'),
    '19': ('IA', 'IOWA'),
    '16': ('ID', 'IDAHO'),
    '17': ('IL', 'ILLINOIS'),
    '18': ('IN', 'INDIANA'),
    '20': ('KS', 'KANSAS'),
    '21': ('KY', 'KENTUCKY'),
    '22': ('LA', 'LOUISIANA'),
    '25': ('MA', 'MASSACHUSETTS'),
    '24': ('MD', 'MARYLAND'),
    '23': ('ME', 'MAINE'),
    '26': ('MI', 'MICHIGAN'),
    '27': ('MN', 'MINNESOTA'),
    '29': ('MO', 'MISSOURI'),
    '28': ('MS', 'MISSISSIPPI'),
    '30': ('MT', 'MONTANA'),
    '37': ('NC', 'NORTH CAROLINA'),
    '38': ('ND', 'NORTH DAKOTA'),
    '31': ('NE', 'NEBRASKA'),
    '33': ('NH', 'NEW HAMPSHIRE'),
    '34': ('NJ', 'NEW JERSEY'),
    '35': ('NM', 'NEW MEXICO'),
    '32': ('NV', 'NEVADA'),
    '36': ('NY', 'NEW YORK'),
    '39': ('OH', 'OHIO'),
    '40': ('OK', 'OKLAHOMA'),
    '41': ('OR', 'OREGON'),
    '42': ('PA', 'PENNSYLVANIA'),
    '72': ('PR', 'PUERTO RICO'),
    '44': ('RI', 'RHODE ISLAND'),
    '45': ('SC', 'SOUTH CAROLINA'),
    '46': ('SD', 'SOUTH DAKOTA'),
    '47': ('TN', 'TENNESSEE'),
    '48': ('TX', 'TEXAS'),
    '49': ('UT', 'UTAH'),
    '51': ('VA', 'VIRGINIA'),
    '78': ('VI', 'VIRGIN ISLANDS'),
    '50': ('VT', 'VERMONT'),
    '53': ('WA', 'WASHINGTON'),
    '55': ('WI', 'WISCONSIN'),
    '54': ('WV', 'WEST VIRGINIA'),
    '56': ('WY', 'WYOMING'),
}

# Only import features with these MTFCC codes - primary road,
# secondary road, and city street
VALID_MTFCC = set(['S1100', 'S1200', 'S1400'])

class TigerImporter(BlockImporter):
    """
    Imports blocks using TIGER/Line data from the US Census.

    Note this importer requires a lot of memory, because it loads the
    necessary .DBF files into memory for various lookups.

    Please refer to Census TIGER/Line shapefile documentation
    regarding the relationships between shapefiles and support DBF
    databases:

    http://www.census.gov/geo/www/tiger/tgrshp2008/rel_file_desc_2008.txt
    """
    def __init__(self, edges_shp, featnames_dbf, faces_dbf, place_shp, filter_city=None):
        self.layer = DataSource(edges_shp)[0]
        self.featnames_db = featnames_db = {}
        for row in self._load_rel_db(featnames_dbf, 'LINEARID').itervalues():
            if row['MTFCC'] not in VALID_MTFCC:
                continue
            tlid = row['TLID']
            featnames_db.setdefault(tlid, [])
            featnames_db[tlid].append(row)
        self.faces_db = self._load_rel_db(faces_dbf, 'TFID')
        # Load places keyed by FIPS code
        places_layer = DataSource(place_shp)[0]
        fields = places_layer.fields
        self.places = places = {}
        for feature in DataSource(place_shp)[0]:
            fips = feature.get('PLACEFP')
            values = dict(zip(fields, map(feature.get, fields)))
            places[fips] = values
        self.filter_city = filter_city and filter_city.upper() or None

    def _load_rel_db(self, dbf_file, rel_key):
        f = open(dbf_file, 'rb')
        db = {}
        try:
            for row in dbf.dict_reader(f, strip_values=True):
                db[row[rel_key]] = row
        finally:
            f.close()
        return db

    def _get_city(self, feature, side):
        fid = feature.get('TFID' + side)
        city = ''
        if fid in self.faces_db:
            face = self.faces_db[fid]
            pid = face['PLACEFP']
            if pid in self.places:
                place = self.places[pid]
                city = place['NAME']
        return city

    def _get_state(self, feature, side):
        fid = feature.get('TFID' + side)
        if fid in self.faces_db:
            face = self.faces_db[fid]
            return STATE_FIPS[face['STATEFP']][0]
        else:
            return ''

    def skip_feature(self, feature):
        if self.filter_city:
            in_city = False
            for side in ('R', 'L'):
                if self._get_city(feature, side).upper() == self.filter_city:
                    in_city = True
            if not in_city:
                return True
        return not feature.get('MTFCC') in VALID_MTFCC or not \
               ((feature.get('RFROMADD') and feature.get('RTOADD')) or \
                (feature.get('LFROMADD') and feature.get('LTOADD')))

    def gen_blocks(self, feature):
        block_fields = {}
        tlid = feature.get('TLID')
        for side in ('right', 'left'):
            for end in ('from', 'to'):
                field_key = '%s_%s_num' % (side, end)
                sl = side[0].upper() # side letter
                try:
                    block_fields[field_key] = int(feature.get('%s%sADD' % (sl, end.upper())))
                except ValueError:
                    block_fields[field_key] = None
        block_fields['right_zip'] = feature.get('ZIPR')
        block_fields['left_zip'] = feature.get('ZIPL')
        for side in ('right', 'left'):
            block_fields[side + '_city'] = self._get_city(feature, side[0].upper()).upper()
            block_fields[side + '_state'] = self._get_state(feature, side[0].upper()).upper()
        if tlid in self.featnames_db:
            for featname in self.featnames_db[tlid]:
                name_fields = {}
                name_fields['street'] = featname['NAME'].upper()
                name_fields['predir'] = featname['PREDIRABRV'].upper()
                name_fields['suffix'] = featname['SUFTYPABRV'].upper()
                name_fields['postdir'] = featname['SUFDIRABRV'].upper()
                block_fields.update(name_fields)
                yield block_fields

def main(argv=None):
    if argv is None:
        argv = sys.argv[1:]
    parser = optparse.OptionParser(usage='%prog edges.shp featnames.dbf faces.dbf place.shp')
    parser.add_option('-v', '--verbose', action='store_true', dest='verbose', default=False)
    parser.add_option('-c', '--city', dest='city', help='A city name to filter against')
    (options, args) = parser.parse_args(argv)
    if len(args) != 4:
        return parser.error('must provide 4 arguments, see usage')
    tiger = TigerImporter(*args, filter_city=options.city)
    tiger.save(options.verbose)

if __name__ == '__main__':
    sys.exit(main())

########NEW FILE########
__FILENAME__ = models
from django.contrib.localflavor.us.models import USStateField
from django.contrib.gis.db import models
from django.contrib.gis.geos import fromstr
from django.db.models import Q
from ebpub.metros.allmetros import get_metro
import operator
import re

class ImproperCity(Exception):
    pass

def proper_city(block):
    """
    Returns the "proper" city for block.

    This function is necessary because in the Block model there are
    two sides of the street, and the city on the left side could
    differ from the city on the right. This function uses knowledge
    about metros and cities to return the canonical city
    for our purposes for a block.

    Note that if ImproperCity is raised, it implies that there is a
    mismatch between the block data and our understanding about what
    should be in there. i.e., neither the left nor right side city is
    one of our metros or city within a multiple-city metro.
    """
    from ebpub.db.models import Location
    metro = get_metro()
    if metro['multiple_cities']:
        cities = set([l.name.upper() for l in Location.objects.filter(location_type__slug=metro['city_location_type']).exclude(location_type__name__startswith='Unknown')])
    else:
        cities = set([metro['city_name'].upper()])
    # Determine the block's city, which because of blocks that
    # border two different municipalities, and because of metros
    # with multiple cities like NYC and Miami-Dade, means checking
    # both sides of the block and comparing with known city names.
    block_city = None
    if block.left_city != block.right_city:
        # Note that if both left_city and right_city are valid, then we
        # return the left_city.
        if block.left_city in cities:
            block_city = block.left_city
        elif block.right_city in cities:
            block_city = block.right_city
    elif block.left_city in cities:
        block_city = block.left_city
    if block_city is None:
        raise ImproperCity("Error: Unknown city '%s' from block %s (%s)" % (block.left_city, block.id, block))
    return block_city

class BlockManager(models.GeoManager):
    def search(self, street, number=None, predir=None, suffix=None, postdir=None, city=None, state=None, zipcode=None, strict_number=False):
        """
        Searches the blocks for the given address bits. Returns a list
        of 2-tuples, (block, geocoded_pt).

        geocoded_pt will be None if number is None.

        We make these assumptions about the input:

            * Everything is already in all-uppercase
            * The predir and postdir have been standardized

        strict_number=True means that, if a number is given, it must
        be within a side of the street's from and to number range, and
        that its parity (even / odd) matches that of the number range.

        strict_number=False, the default, means that if a canonical
        block is not found for this number (i.e., one that meets the
        conditions of strict_number=True), then as long as the number
        is within a number range, we don't enforce the parity
        matching. This is friendlier to the user. For example, 3181
        would match the block 3180-3188.
        """
        filters = {'street': street.upper()}
        sided_filters = []
        if predir:
            filters['predir'] = predir.upper()
        if suffix:
            filters['suffix'] = suffix.upper()
        if postdir:
            filters['postdir'] = postdir.upper()
        if city:
            city_filter = Q(left_city=city.upper()) | Q(right_city=city.upper())
            sided_filters.append(city_filter)
        if state:
            state_filter = Q(left_state=state.upper()) | Q(right_state=state.upper())
            sided_filters.append(state_filter)
        if zipcode:
            zip_filter = Q(left_zip=zipcode) | Q(right_zip=zipcode)
            sided_filters.append(zip_filter)

        qs = self.filter(*sided_filters, **filters)

        # If a number was given, search against the address ranges in the
        # Block table.
        if number:
            number = int(re.sub(r'\D', '', number))
            block_tuples = []
            for block in qs.filter(from_num__lte=number, to_num__gte=number):
                contains, from_num, to_num = block.contains_number(number)
                if contains:
                    block_tuples.append((block, from_num, to_num))
            blocks = []
            if block_tuples:
                from django.db import connection
                cursor = connection.cursor()
                for block, from_num, to_num in block_tuples:
                    try:
                        fraction = (float(number) - from_num) / (to_num - from_num)
                    except ZeroDivisionError:
                        fraction = 0.5
                    # We rely on PostGIS line_interpolate_point() because there
                    # isn't a matching GeoDjango/Python API.
                    cursor.execute('SELECT line_interpolate_point(%s, %s)', [block.geom.wkt, fraction])
                    wkb_hex = cursor.fetchone()[0]
                    blocks.append((block, fromstr(wkb_hex)))
        else:
            blocks = list([(b, None) for b in qs])
        return blocks

class Block(models.Model):
    pretty_name = models.CharField(max_length=255)
    predir = models.CharField(max_length=2, blank=True, db_index=True)
    street = models.CharField(max_length=255, db_index=True) # Always uppercase!
    street_slug = models.SlugField()
    street_pretty_name = models.CharField(max_length=255)
    suffix = models.CharField(max_length=32, blank=True, db_index=True) # Always uppercase
    postdir = models.CharField(max_length=2, blank=True, db_index=True) # Always uppercase
    left_from_num = models.IntegerField(db_index=True, blank=True, null=True)
    left_to_num = models.IntegerField(db_index=True, blank=True, null=True)
    right_from_num = models.IntegerField(db_index=True, blank=True, null=True)
    right_to_num = models.IntegerField(db_index=True, blank=True, null=True)
    from_num = models.IntegerField(db_index=True, blank=True, null=True)
    to_num = models.IntegerField(db_index=True, blank=True, null=True)
    left_zip = models.CharField(max_length=10, db_index=True, blank=True, null=True) # Possible Plus-4
    right_zip = models.CharField(max_length=10, db_index=True, blank=True, null=True) # Possible Plus-4
    left_city = models.CharField(max_length=255, db_index=True) # Always uppercase
    right_city = models.CharField(max_length=255, db_index=True) # Always uppercase
    left_state = USStateField(db_index=True) # Always uppercase
    right_state = USStateField(db_index=True) # Always uppercase
    parent_id = models.IntegerField(db_index=True, blank=True, null=True) # This field is used for blocks that are alternate names for another block, which is pointed to by this ID
    geom = models.LineStringField()
    objects = BlockManager()

    class Meta:
        db_table = 'blocks'

    def __unicode__(self):
        return self.pretty_name

    def number(self):
        """
        Returns a formatted street number
        """
        if self.from_num == self.to_num:
            return unicode(self.from_num)
        if not self.from_num:
            return unicode(self.to_num)
        if not self.to_num:
            return unicode(self.from_num)
        return u'%s-%s' % (self.from_num, self.to_num)

    def dir_url_bit(self):
        """
        Returns the directional bit of the URL.

        For example, if the pre-directional is "N" and the post-directional is
        blank, returns "n".

        If the pre-directional is "E" and the post-directional is "SW",
        returns "e-sw".

        If the pre-directional is blank and the post-directional is "e",
        return "-e".

        If both are blank, returns the empty string.
        """
        url = []
        if self.predir:
            url.append(self.predir.lower())
        if self.postdir:
            url.extend(['-', self.postdir.lower()])
        return ''.join(url)

    def url(self):
        return '%s%s%s/' % (self.street_url(), self.number(), self.dir_url_bit())

    def street_url(self):
        if get_metro()['multiple_cities']:
            return '/streets/%s/%s/' % (self.city_object().slug, self.street_slug)
        else:
            return '/streets/%s/' % self.street_slug

    def rss_url(self):
        return '/rss%s' % self.url()

    def alert_url(self):
        return '%salerts/' % self.url()

    def city_object(self):
        return City.from_norm_name(self.city)

    def contains_number(self, number):
        """
        Returns a tuple of (boolean, from_num, to_num), where boolean is
        True if this Block contains the given address number. The from_num
        and to_num values are the ones that were used to calculate it.

        Checks both the block range and the parity (even vs. odd numbers).
        """
        parity = number % 2
        if self.left_from_num and self.right_from_num:
            left_parity = self.left_from_num % 2
            # If this block's left side has the same parity as the right side,
            # all bets are off -- just use the from_num and to_num.
            if self.right_to_num % 2 == left_parity or self.left_to_num % 2 == self.right_from_num % 2:
                from_num, to_num = self.from_num, self.to_num
            elif left_parity == parity:
                from_num, to_num = self.left_from_num, self.left_to_num
            else:
                from_num, to_num = self.right_from_num, self.right_to_num
        elif self.left_from_num:
            from_parity, to_parity = self.left_from_num % 2, self.left_to_num % 2
            from_num, to_num = self.left_from_num, self.left_to_num
            # If the parity is equal for from_num and to_num, make sure the
            # parity of the number is the same.
            if (from_parity == to_parity) and from_parity != parity:
                return False, from_num, to_num
        else:
            from_parity, to_parity = self.right_from_num % 2, self.right_to_num % 2
            from_num, to_num = self.right_from_num, self.right_to_num
            # If the parity is equal for from_num and to_num, make sure the
            # parity of the number is the same.
            if (from_parity == to_parity) and from_parity != parity:
                return False, from_num, to_num
        return (from_num <= number <= to_num), from_num, to_num

    def _get_location(self):
        return self.geom
    location = property(_get_location)

    def _get_city(self):
        if not hasattr(self, '_city_cache'):
            self._city_cache = proper_city(self)
        return self._city_cache
    city = property(_get_city)

    def _get_state(self):
        if self.left_state == self.right_state:
            return self.left_state
        else:
            return get_metro()['state_abbr']
    state = property(_get_state)

    def _get_zip(self):
        return self.left_zip
    zip = property(_get_zip)

class Street(models.Model):
    street = models.CharField(max_length=255, db_index=True) # Always uppercase
    pretty_name = models.CharField(max_length=255)
    street_slug = models.SlugField()
    suffix = models.CharField(max_length=32, blank=True, db_index=True) # Always uppercase
    city = models.CharField(max_length=255, db_index=True) # Always uppercase
    state = USStateField(db_index=True) # Always uppercase

    class Meta:
        db_table = 'streets'

    def __unicode__(self):
        return self.pretty_name

    def url(self):
        if get_metro()['multiple_cities']:
            return '/streets/%s/%s/' % (self.city_object().slug, self.street_slug)
        else:
            return '/streets/%s/' % self.street_slug

    def city_object(self):
        return City.from_norm_name(self.city)

class Misspelling(models.Model):
    incorrect = models.CharField(max_length=255, unique=True) # Always uppercase, single spaces
    correct = models.CharField(max_length=255)

    def __unicode__(self):
        return self.incorrect

class StreetMisspellingManager(models.Manager):
    def make_correction(self, street_name):
        """
        Returns the correct spelling of the given street name. If the given
        street name is already correctly spelled, then it's returned as-is.

        Note that the given street name will be converted to all caps.
        """
        street_name = street_name.upper()
        try:
            return self.get(incorrect=street_name).correct
        except self.model.DoesNotExist:
            return street_name

class StreetMisspelling(models.Model):
    incorrect = models.CharField(max_length=255, unique=True) # Always uppercase, single spaces
    correct = models.CharField(max_length=255)
    objects = StreetMisspellingManager()

    def __unicode__(self):
        return self.incorrect

# A generic place, like "Millennium Park" or "Sears Tower"
class Place(models.Model):
    pretty_name = models.CharField(max_length=255)
    normalized_name = models.CharField(max_length=255) # Always uppercase, single spaces
    address = models.CharField(max_length=255, blank=True)
    location = models.GeometryField()
    objects = models.GeoManager()

    def __unicode__(self):
        if self.address:
            return u'%s (%s)' % (self.pretty_name, self.address)
        return self.pretty_name

    def save(self):
        if not self.normalized_name:
            from ebpub.geocoder.parser.parsing import normalize
            self.normalized_name = normalize(self.pretty_name)
        super(Place, self).save()

class City(object):
    def __init__(self, name, slug, norm_name):
        self.name, self.slug, self.norm_name = name, slug, norm_name

    def from_name(cls, name):
        return cls(name, name.lower().replace(' ', '-'), name.upper())
    from_name = classmethod(from_name)

    def from_slug(cls, slug):
        return cls(slug.title().replace('-', ' '), slug, slug.upper().replace('-', ' '))
    from_slug = classmethod(from_slug)

    def from_norm_name(cls, norm_name):
        return cls(norm_name.title(), norm_name.lower().replace(' ', '-'), norm_name)
    from_norm_name = classmethod(from_norm_name)

class BlockIntersection(models.Model):
    block = models.ForeignKey(Block)
    intersecting_block = models.ForeignKey(Block, related_name="intersecting_block")
    intersection = models.ForeignKey("Intersection", blank=True, null=True)
    location = models.PointField()

    class Meta:
        unique_together = ("block", "intersecting_block")

    def __unicode__(self):
        return u'%s intersecting %s' % (self.block, self.intersecting_block)

class IntersectionManager(models.GeoManager):
    def search(self, predir_a=None, street_a=None, suffix_a=None, postdir_a=None,
                     predir_b=None, street_b=None, suffix_b=None, postdir_b=None):
        """
        Returns a queryset of intersections.
        """
        # Since intersections are symmetrical---"N. Kimball Ave. & W. Diversey
        # Ave." == "W. Diversey Ave. & N. Kimball Ave."---we use Q
        # objects for the OR reverse of the ordering of the keyword
        # arguments.
        filters = [{}, {}]
        if predir_a:
            filters[0]["predir"] = predir_a.upper()
        if street_a:
            filters[0]["street"] = street_a.upper()
        if suffix_a:
            filters[0]["suffix"] = suffix_a.upper()
        if postdir_a:
            filters[0]["postdir"] = postdir_a.upper()
        if predir_b:
            filters[1]["predir"] = predir_b.upper()
        if street_b:
            filters[1]["street"] = street_b.upper()
        if suffix_b:
            filters[1]["suffix"] = suffix_b.upper()
        if postdir_b:
            filters[1]["postdir"] = postdir_b.upper()
        q1 = reduce(operator.and_, [Q(**{k+"_a": v}) for k,v in filters[0].iteritems()] +
                                   [Q(**{k+"_b": v}) for k,v in filters[1].iteritems()])
        q2 = reduce(operator.and_, [Q(**{k+"_a": v}) for k,v in filters[1].iteritems()] +
                                   [Q(**{k+"_b": v}) for k,v in filters[0].iteritems()])
        qs = self.filter(q1 | q2)
        qs = qs.extra(select={"point": "AsText(location)"})
        return qs

class Intersection(models.Model):
    pretty_name = models.CharField(max_length=255, unique=True) # eg., "N. Kimball Ave. & W. Diversey Ave.
    slug = models.SlugField(max_length=64) # eg., "n-kimball-ave-and-w-diversey-ave"
    # Street A
    predir_a = models.CharField(max_length=2, blank=True, db_index=True) # eg., "N"
    street_a = models.CharField(max_length=255, db_index=True) # eg., "KIMBALL"
    suffix_a = models.CharField(max_length=32, blank=True, db_index=True) # eg., "AVE"
    postdir_a = models.CharField(max_length=2, blank=True, db_index=True) # eg., "NW"
    # Street B
    predir_b = models.CharField(max_length=2, blank=True, db_index=True) # eg., "W"
    street_b = models.CharField(max_length=255, db_index=True) # eg., "DIVERSEY"
    suffix_b = models.CharField(max_length=32, blank=True, db_index=True) # eg., "AVE"
    postdir_b = models.CharField(max_length=2, blank=True, db_index=True) # eg., "SE"
    zip = models.CharField(max_length=10, db_index=True) # Possible Plus-4
    city = models.CharField(max_length=255, db_index=True) # Always uppercase
    state = USStateField(db_index=True) # Always uppercase
    location = models.PointField()
    objects = IntersectionManager()

    class Meta:
        db_table = 'intersections'
        unique_together = ("predir_a", "street_a", "suffix_a", "postdir_a", "predir_b", "street_b", "suffix_b", "postdir_b")

    def __unicode__(self):
        return self.pretty_name

    def reverse_pretty_name(self):
        return u" & ".join(self.pretty_name.split(" & ")[::-1])

    def url(self):
        # Use the URL of the first block found of those which comprise
        # this intersection.
        try:
            first_block = self.blockintersection_set.all()[0].block
        except IndexError:
            return ''
        return first_block.url()

    def alert_url(self):
        return '%salerts/' % self.url()

class Suburb(models.Model):
    # This model keeps track of nearby cities that we don't care about.
    # It's essentially a blacklist.
    name = models.CharField(max_length=255)
    normalized_name = models.CharField(max_length=255, unique=True)

    def __unicode__(self):
        return self.name

########NEW FILE########
__FILENAME__ = name_utils
"""
Utility functions for munging address/block/street names.
"""

import re
from ebpub.utils.text import smart_title, slugify

def make_street_pretty_name(street, suffix):
    street_name = smart_title(street)
    if suffix:
        street_name += u' %s.' % smart_title(suffix)
    return street_name

def make_block_number(left_from_num, left_to_num, right_from_num, right_to_num):
    lo_num = min([x for x in (left_from_num, left_to_num, right_from_num, right_to_num) if x])
    hi_num = max([x for x in (left_from_num, left_to_num, right_from_num, right_to_num) if x])
    if lo_num == hi_num:
        number = unicode(lo_num)
    elif lo_num and not hi_num:
        number = unicode(lo_num)
    elif hi_num and not lo_num:
        number = unicode(hi_num)
    else:
        number = u'%s-%s' % (lo_num, hi_num)
    return number

def make_pretty_directional(directional):
    """
    Returns a formatted directional.

    e.g.:

        N -> N.
        NW -> N.W.
    """
    return "".join(u"%s." % c for c in directional)

def make_pretty_name(left_from_num, left_to_num, right_from_num, right_to_num, predir, street, suffix, postdir=None):
    """
    Returns a tuple of (street_pretty_name, block_pretty_name) for the
    given address bits.
    """
    street_name = make_street_pretty_name(street, suffix)
    num_part = make_block_number(left_from_num, left_to_num, right_from_num, right_to_num)
    predir_part = predir and make_pretty_directional(predir) or u''
    postdir_part = postdir and make_pretty_directional(postdir) or u''
    block_name = u'%s %s %s %s' % (num_part, predir_part, street_name, postdir_part)
    block_name = re.sub('\s+', ' ', block_name).strip()
    return street_name, block_name

def make_dir_street_name(block):
    """
    Returns a street name from a block with the directional included.

    If the block has a ``predir``, the directional is prepended:

        "W. Diversey Ave."

    If the block has a ``postdir``, the directional is appended:

        "18th St. N.W."
    """
    name = make_street_pretty_name(block.street, block.suffix)
    if block.predir:
        name = u"%s %s" % (make_pretty_directional(block.predir), name)
    if block.postdir:
        name = u"%s %s" % (name, make_pretty_directional(block.postdir))
    return name

def pretty_name_from_blocks(block_a, block_b):
    return u"%s & %s" % (make_dir_street_name(block_a), make_dir_street_name(block_b))

def slug_from_blocks(block_a, block_b):
    slug = u"%s-and-%s" % (slugify(make_dir_street_name(block_a)),
                           slugify(make_dir_street_name(block_b)))
    # If it's too long for the slug field, drop the directionals
    if len(slug) > 64:
        slug = u"%s-and-%s" % (slugify(make_street_pretty_name(block_a.street, block_a.suffix)),
                               slugify(make_street_pretty_name(block_b.street, block_b.suffix)))
    # If it's still too long, drop the suffixes
    if len(slug) > 64:
        slug = u"%s-and-%s" % (slugify(block_a.street),
                               slugify(block_b.street))

    return slug

########NEW FILE########
__FILENAME__ = populate_streets
#!/usr/bin/env python
"""
Populates the street, block intersection, and intersection
model tables.

How to populate intersections
=============================

First import the blocks and populate the streets from them: not
described here.

Ensure the ZIP Codes are populated in the db_location table: they
are needed for resolving 'disputes' when the two intersecting
blocks of an intersection have different ZIP Codes. This is also
not described here: there should be a ZIP Code importer script
available for each city.

This is the part that takes the longest: populate the
db_blockintersection table. This goes through each block (and
remember, there are on the order of tens of thousands of blocks in
each city) and calculates all the other blocks which intersect with
it. This is inherently slow, even with the heavy-lifting offloaded
to pgsql/postgis. (If we were particularly clever, we'd probably
write our own custom ultra-optimized C module to burn through this
operation. But we're clever enough to recognize that this is a
one-time cost so it's better not to be too cleverer.)

In this module, execute the populate_block_intersections() function.

When that completes, execute the populate_intersections()
function. This is a comparatively fast operation which just looks
up the pre-calculated intersections for each block and creates
a new object representing a particular intersection, eliminating
potential duplicates.
"""

import logging
import sys
import optparse
from django.contrib.gis.geos import fromstr
from django.db import connection, transaction
from ebpub.db.models import Location
from ebpub.metros.allmetros import get_metro
from ebpub.streets.models import Block, Street, BlockIntersection, Intersection
from ebpub.streets.name_utils import make_dir_street_name, pretty_name_from_blocks, slug_from_blocks

def intersecting_blocks(block):
    """
    Returns a list of blocks that intersect the given one.

    Note that blocks with the same street name and suffix are
    excluded -- this is a heuristic that keeps the adjacent blocks
    of the same street out.
    """
    select_list = ["b.%s" % f.name for f in block._meta.fields] + ["ST_Intersection(a.geom, b.geom)"]
    table = block._meta.db_table
    cursor = connection.cursor()
    sql = """
        SELECT %s
        FROM %s a,
             %s b
        WHERE
            a.id = %%s AND
            ST_Intersects(a.geom, b.geom) AND
            GeometryType(ST_Intersection(a.geom, b.geom)) = 'POINT' AND
            NOT (b.street = a.street AND b.suffix = a.suffix)
        ORDER BY
            b.predir, b.street, b.suffix, b.left_from_num, b.right_from_num
        """ % (", ".join(select_list), table, table)
    cursor.execute(sql, [block.id])
    intersections = []
    for row in cursor.fetchall():
        block = Block(*row[:-1])
        intersection_pt = fromstr(row[-1])
        intersections.append((block, intersection_pt))
    return intersections

def intersection_from_blocks(block_a, block_b, intersection_pt, city, state, zip):
    obj = Intersection(
        pretty_name=pretty_name_from_blocks(block_a, block_b),
        slug=slug_from_blocks(block_a, block_b),
        predir_a=block_a.predir,
        street_a=block_a.street,
        suffix_a=block_a.suffix,
        postdir_a=block_a.postdir,
        predir_b=block_b.predir,
        street_b=block_b.street,
        suffix_b=block_b.suffix,
        postdir_b=block_b.postdir,
        city=city,
        state=state,
        zip=zip,
        location=intersection_pt
    )
    return obj

@transaction.commit_on_success
def populate_streets(*args, **kwargs):
    """
    Populates the streets table from the blocks table
    """
    print 'Populating the streets table'
    cursor = connection.cursor()
    cursor.execute("TRUNCATE streets")
    cursor.execute("""
        INSERT INTO streets (street, pretty_name, street_slug, suffix, city, state)
        SELECT DISTINCT street, street_pretty_name, street_slug, suffix, left_city, left_state
        FROM blocks
        UNION SELECT DISTINCT street, street_pretty_name, street_slug, suffix, right_city, right_state
        FROM blocks
    """)
    connection._commit()

    print "Deleting extraneous cities..."
    metro = get_metro()
    cities = [l.name.upper() for l in Location.objects.filter(location_type__slug=metro['city_location_type']).exclude(location_type__name__startswith='Unknown')]
    Street.objects.exclude(city__in=cities).delete()

@transaction.commit_on_success
def populate_block_intersections(*args, **kwargs):
    for block in Block.objects.all():
        logging.info('Calculating the blocks that intersect %s' % block)
        for iblock, intersection_pt in intersecting_blocks(block):
            BlockIntersection.objects.create(
                block=block,
                intersecting_block=iblock,
                location=intersection_pt
            )

@transaction.commit_on_success
def populate_intersections(*args, **kwargs):
    # On average, there are 2.3 blocks per intersection. So for
    # example in the case of Chicago, where there are 788,496 blocks,
    # we'd expect to see approximately 340,000 intersections
    logging.info("Starting to populate intersections")
    metro = get_metro()
    zipcodes = Location.objects.filter(location_type__name__istartswith="zip").exclude(name__startswith='Unknown')
    def lookup_zipcode(pt):
        for zipcode in zipcodes:
            if zipcode.location.contains(pt):
                return zipcode
    intersections_seen = {}
    for i in Intersection.objects.all():
        intersections_seen[i.pretty_name] = i.id
        intersections_seen[i.reverse_pretty_name()] = i.id
    for bi in BlockIntersection.objects.iterator():
        street_name = make_dir_street_name(bi.block)
        i_street_name = make_dir_street_name(bi.intersecting_block)
        # This tuple enables us to skip over intersections
        # we've already seen. Since intersections are
        # symmetrical---eg., "N. Kimball Ave. & W. Diversey
        # Ave." == "W. Diversey Ave. & N. Kimball Ave."---we
        # use both orderings.
        seen_intersection = (u"%s & %s" % (street_name, i_street_name),
                             u"%s & %s" % (i_street_name, street_name))
        if seen_intersection[0] not in intersections_seen and \
           seen_intersection[1] not in intersections_seen:
            if bi.block.left_city != bi.block.right_city:
                city = metro['city_name'].upper()
            else:
                city = bi.block.left_city
            if bi.block.left_state != bi.block.right_state:
                state = metro['state'].upper()
            else:
                state = bi.block.left_state
            if (bi.block.left_zip != bi.block.right_zip or \
                bi.intersecting_block.left_zip != bi.intersecting_block.right_zip) or \
               (bi.block.left_zip != bi.intersecting_block.left_zip):
                zipcode_obj = lookup_zipcode(bi.location)
                if zipcode_obj:
                    zipcode = zipcode_obj.name
                else:
                    zipcode = bi.block.left_zip
            else:
                zipcode = bi.block.left_zip
            intersection = intersection_from_blocks(bi.block, bi.intersecting_block, bi.location, city, state, zipcode)
            intersection.save()
            logging.debug("Created intersection %s" % intersection)
            bi.intersection = intersection
            bi.save()
            intersections_seen[seen_intersection[0]] = intersection.id
            intersections_seen[seen_intersection[1]] = intersection.id
        else:
            if not bi.intersection:
                bi.intersection_id = intersections_seen[seen_intersection[0]]
                bi.save()
            logging.debug("Already seen intersection %s" % " / ".join(seen_intersection))
    logging.info("Finished populating intersections")

LOG_VERBOSITY = {0: logging.NOTSET,
                 1: logging.CRITICAL,
                 2: logging.ERROR,
                 3: logging.WARNING,
                 4: logging.INFO,
                 5: logging.DEBUG}

def main(argv=None):
    if argv is None:
        argv = sys.argv[1:]
    valid_actions = {'streets': populate_streets,
                     'block_intersections': populate_block_intersections,
                     'intersections': populate_intersections}
    parser = optparse.OptionParser('Usage: %%prog [opts] {%s}' % \
                                   '|'.join(valid_actions.keys()),
                                   description=__doc__)
    parser.add_option('-v', '--verbose', action='count', dest='verbosity',
                      default=0, help='verbosity, add more -v to be more verbose')
    opts, args = parser.parse_args(argv)
    if len(args) != 1 or args[0] not in valid_actions:
        parser.error('must supply an valid action, one of: %r' % \
                     valid_actions.keys())
    logging.basicConfig(level=LOG_VERBOSITY.get(opts.verbosity, 0),
                        format="%(asctime)-15s %(levelname)-8s %(message)s")
    # Call the action
    valid_actions[args[0]](**opts.__dict__)

if __name__ == '__main__':
    sys.exit(main())

########NEW FILE########
__FILENAME__ = update_block_pretty_names
from ebpub.streets.models import make_pretty_name
from ebpub.streets.models import Block

def update_block_pretty_names():
    for b in Block.objects.all().iterator():
        name = make_pretty_name(b.from_num, b.to_num, b.predir, b.street, b.suffix)[1]
        if name != b.pretty_name:
            print '%s -- %s' % (b.pretty_name, name)
            b.pretty_name = name
            b.save()

if __name__ == "__main__":
    update_block_pretty_names()

########NEW FILE########
__FILENAME__ = utils
from ebpub.db.models import Location
from ebpub.geocoder import SmartGeocoder, AmbiguousResult, InvalidBlockButValidStreet
from ebpub.geocoder.parser.parsing import normalize
from ebpub.streets.models import Misspelling, Place

def full_geocode(query, search_places=True):
    """
    Tries the full geocoding stack on the given query (a string):
        * Normalizes whitespace/capitalization
        * Searches the Misspelling table to corrects location misspellings
        * Searches the Location table
        * Failing that, searches the Place table (if search_places is True)
        * Failing that, uses the given geocoder to parse this as an address
        * Failing that, raises whichever error is raised by the geocoder --
          except AmbiguousResult, in which case all possible results are
          returned

    Returns a dictionary of {type, result, ambiguous}, where ambiguous is True
    or False and type can be:
        * 'location' -- in which case result is a Location object.
        * 'place' -- in which case result is a Place object. (This is only
          possible if search_places is True.)
        * 'address' -- in which case result is an Address object as returned
          by geocoder.geocode().
        * 'block' -- in which case result is a list of Block objects.

    If ambiguous is True, result will be a list of objects.
    """
    query = normalize(query)

    # First, try correcting the spelling ("LAKEVIEW" -> "LAKE VIEW").
    try:
        miss = Misspelling.objects.get(incorrect=query)
    except Misspelling.DoesNotExist:
        pass
    else:
        query = miss.correct

    # Search the Location table.
    try:
        loc = Location.objects.get(normalized_name=query)
    except Location.DoesNotExist:
        pass
    else:
        return {'type': 'location', 'result': loc, 'ambiguous': False}

    # Search the Place table, for stuff like "Sears Tower".
    if search_places:
        places = Place.objects.filter(normalized_name=query)
        if len(places) == 1:
            return {'type': 'place', 'result': places[0], 'ambiguous': False}
        elif len(places) > 1:
            return {'type': 'place', 'result': places, 'ambiguous': True}

    # Try geocoding this as an address.
    geocoder = SmartGeocoder()
    try:
        result = geocoder.geocode(query)
    except AmbiguousResult, e:
        return {'type': 'address', 'result': e.choices, 'ambiguous': True}
    except InvalidBlockButValidStreet, e:
        return {'type': 'block', 'result': e.block_list, 'ambiguous': True, 'street_name': e.street_name, 'block_number': e.block_number}
    except:
        raise
    return {'type': 'address', 'result': result, 'ambiguous': False}

########NEW FILE########
__FILENAME__ = urls
from django.conf import settings
from django.conf.urls.defaults import *
from ebpub.alerts import views as alert_views
from ebpub.db import feeds, views
from ebpub.db.constants import BLOCK_URL_REGEX
from ebpub.petitions import views as petition_views
from ebpub.utils.urlresolvers import metro_patterns

if settings.DEBUG:
    urlpatterns = patterns('',
        (r'^(?P<path>(?:images|scripts|styles|openlayers).*)$', 'django.views.static.serve', {'document_root': settings.EB_MEDIA_ROOT}),
    )
else:
    urlpatterns = patterns('')

urlpatterns += patterns('',
    (r'^$', views.homepage),
    (r'^search/$', views.search),
    (r'^news/$', views.schema_list),
    (r'^locations/$', 'django.views.generic.simple.redirect_to', {'url': '/locations/neighborhoods/'}),
    (r'^locations/([-_a-z0-9]{1,32})/$', views.location_type_detail),
    (r'^locations/([-_a-z0-9]{1,32})/([-_a-z0-9]{1,32})/$', views.place_detail, {'place_type': 'location', 'detail_page': True}),
    (r'^locations/([-_a-z0-9]{1,32})/([-_a-z0-9]{1,32})/overview/$', views.place_detail, {'place_type': 'location'}),
    (r'^locations/([-_a-z0-9]{1,32})/([-_a-z0-9]{1,32})/feeds/$', views.feed_signup, {'place_type': 'location'}),
    (r'^locations/([-_a-z0-9]{1,32})/([-_a-z0-9]{1,32})/alerts/$', alert_views.signup, {'place_type': 'location'}),
    (r'^rss/(.+)/$', feeds.feed_view),
    (r'^maps/', include('ebgeo.maps.urls')),
    (r'^accounts/', include('ebpub.accounts.urls')),
    (r'^validate-address/$', views.validate_address),
    (r'^alerts/unsubscribe/\d\d(\d{1,10})\d/$', alert_views.unsubscribe),
    (r'^petitions/([-\w]{4,32})/$', petition_views.form_view, {'is_schema': False}),
    (r'^petitions/([-\w]{4,32})/thanks/$', petition_views.form_thanks, {'is_schema': False}),
    (r'^api/wkt/$', views.ajax_wkt),
    (r'^api/map-popups/$', views.ajax_map_popups),
    (r'^api/place-recent-items/$', views.ajax_place_newsitems),
    (r'^api/place-lookup-chart/$', views.ajax_place_lookup_chart),
    (r'^api/place-date-chart/$', views.ajax_place_date_chart),
    (r'^api/map-browser/location-types/$', views.ajax_location_type_list),
    (r'^api/map-browser/location-types/(\d{1,9})/$', views.ajax_location_list),
    (r'^api/map-browser/locations/(\d{1,9})/$', views.ajax_location),
)

urlpatterns += metro_patterns(
    multi=(
        (r'^streets/$', views.city_list),
        (r'^streets/([-a-z]{3,40})/$', views.street_list),
        (r'^streets/([-a-z]{3,40})/([-a-z0-9]{1,64})/$', views.block_list),
        (r'^streets/([-a-z]{3,40})/([-a-z0-9]{1,64})/%s/$' % BLOCK_URL_REGEX, views.place_detail, {'place_type': 'block', 'detail_page': True}),
        (r'^streets/([-a-z]{3,40})/([-a-z0-9]{1,64})/%s/overview/$' % BLOCK_URL_REGEX, views.place_detail, {'place_type': 'block'}),
        (r'^streets/([-a-z]{3,40})/([-a-z0-9]{1,64})/%s/feeds/$' % BLOCK_URL_REGEX, views.feed_signup, {'place_type': 'block'}),
        (r'^streets/([-a-z]{3,40})/([-a-z0-9]{1,64})/%s/alerts/$' % BLOCK_URL_REGEX, alert_views.signup, {'place_type': 'block'}),
    ),
    single=(
        (r'^streets/()$', views.street_list),
        (r'^streets/()([-a-z0-9]{1,64})/$', views.block_list),
        (r'^streets/()([-a-z0-9]{1,64})/%s/$' % BLOCK_URL_REGEX, views.place_detail, {'place_type': 'block', 'detail_page': True}),
        (r'^streets/()([-a-z0-9]{1,64})/%s/overview/$' % BLOCK_URL_REGEX, views.place_detail, {'place_type': 'block'}),
        (r'^streets/()([-a-z0-9]{1,64})/%s/feeds/$' % BLOCK_URL_REGEX, views.feed_signup, {'place_type': 'block'}),
        (r'^streets/()([-a-z0-9]{1,64})/%s/alerts/$' % BLOCK_URL_REGEX, alert_views.signup, {'place_type': 'block'}),
    )
)

urlpatterns += patterns('',
    (r'^([-\w]{4,32})/$', views.schema_detail),
    (r'^([-\w]{4,32})/about/$', views.schema_about),
    (r'^([-\w]{4,32})/search/$', views.search),
    (r'^([-\w]{4,32})/petition/$', petition_views.form_view, {'is_schema': True}),
    (r'^([-\w]{4,32})/petition/thanks/$', petition_views.form_thanks, {'is_schema': True}),
    (r'^([-\w]{4,32})/by-date/(\d{4})/(\d\d?)/(\d\d?)/(\d{1,8})/$', views.newsitem_detail),
    (r'^([-\w]{4,32})/(?:filter/)?([^/].+/)?$', views.schema_filter),
)

########NEW FILE########
__FILENAME__ = bunch
import math

# From http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/425044
def bunch(lst, size):
    size = int(size)
    return [lst[i:i+size] for i in range(0, len(lst), size)]

def bunchlong(lst, size):
    size = float(int(size))
    return bunch(lst, int(math.ceil(len(lst) / size)))

def stride(lst, size):
    """
    >>> stride([1, 2, 3, 4, 5, 6], 2)
    [[1, 3, 5], [2, 4, 6]]
    >>> stride([1, 2, 3, 4, 5], 2)
    [[1, 3, 5], [2, 4]]
    >>> stride([1, 2, 3, 4, 5], 1)
    [[1, 2, 3, 4, 5]]
    >>> stride([1, 2, 3, 4, 5, 6], 3)
    [[1, 4], [2, 5], [3, 6]]
    >>> stride([1, 2, 3, 4, 5, 6, 7], 3)
    [[1, 4, 7], [2, 5], [3, 6]]
    """
    size = int(size)
    return [lst[i::size] for i in range(size)]

if __name__ == "__main__":
    import doctest
    doctest.testmod()

########NEW FILE########
__FILENAME__ = dates
import datetime
import time

def daterange(d1, d2):
    "Iterator that returns every date between d1 and d2, inclusive."
    current = d1
    while current <= d2:
        yield current
        current += datetime.timedelta(days=1)

def parse_date(value, format, return_datetime=False):
    """
    Equivalent to time.strptime, but it returns a datetime.date or
    datetime.datetime object instead of a struct_time object.

    Returns None if the value evaluates to False.
    """
    # See http://docs.python.org/lib/node85.html
    idx = return_datetime and 7 or 3
    func = return_datetime and datetime.datetime or datetime.date
    if value:
        return func(*time.strptime(value, format)[:idx])
    return None

def parse_time(value, format):
    """
    Equivalent to time.strptime, but it returns a datetime.time object.
    """
    return datetime.time(*time.strptime(value, format)[3:6])

########NEW FILE########
__FILENAME__ = multidb
from django.db import models
from django.db.backends.postgresql_psycopg2.base import DatabaseWrapper
from django.conf import settings
from django.contrib.gis.db.models import GeoManager as BaseGeoManager
from django.core import signals

# Global that keeps the currently open connections, keyed by connection_name.
connections = {}

# Close all the connections after every request.
def close_connections(**kwargs):
    for conn in connections.values():
        conn.close()
signals.request_finished.connect(close_connections)

# Based loosely on http://www.eflorenzano.com/blog/post/easy-multi-database-support-django/
class Manager(models.Manager):
    """
    This Manager lets you set database connections on a per-model basis.
    """
    def __init__(self, connection_name, *args, **kwargs):
        # connection_name should correspond to a key in the DATABASES setting.
        models.Manager.__init__(self, *args, **kwargs)
        self.connection_name = connection_name
        self.database_settings = settings.DATABASES[connection_name] # Let KeyError propogate.

    def get_query_set(self):
        qs = models.Manager.get_query_set(self)
        try:
            # First, check the global connection dictionary, because this
            # connection might have already been created.
            conn = connections[self.connection_name]
        except KeyError:
            conn = DatabaseWrapper(self.database_settings)
            connections[self.connection_name] = conn
        qs.query.connection = conn
        return qs

    # TODO: Override _insert() to get inserts/updates/deletions working.

class GeoManager(BaseGeoManager):
    """
    Subclass of django.contrib.gis's GeoManager that lets you set database
    connections on a per-model basis.
    """
    def __init__(self, connection_name, *args, **kwargs):
        BaseGeoManager.__init__(self, *args, **kwargs)
        self.connection_name = connection_name
        self.database_settings = settings.DATABASES[connection_name]

    def get_query_set(self):
        qs = BaseGeoManager.get_query_set(self)
        try:
            # First, check the global connection dictionary, because this
            # connection might have already been created.
            conn = connections[self.connection_name]
        except KeyError:
            conn = DatabaseWrapper(self.database_settings)
            connections[self.connection_name] = conn
        qs.query.connection = conn
        return qs

########NEW FILE########
__FILENAME__ = stats
from __future__ import division
import math

def normalize(min_val, max_val):
    """
    Maps a value to a range between 0.0 and 1.0.

    >>> n = normalize(5, 45)
    >>> n(25)
    0.5
    >>> L = [23, 34, 23, 38, 35, 17, 15, 25, 19, 10]
    >>> ['%.3f' % n(i) for i in L] # doctest: +NORMALIZE_WHITESPACE
    ['0.450', '0.725', '0.450', '0.825', '0.750', '0.300', '0.250', '0.500',
    '0.350', '0.125']
    >>> n(5)
    0.0
    >>> n(45)
    1.0
    >>> n(4)
    0.0
    >>> n(46)
    1.0
    """
    def f(value, clip=True):
        if min_val == max_val:
            return 0.0
        if clip:
            if value <= min_val:
                return 0.0
            elif value >= max_val:
                return 1.0
        return (value - min_val) * (1.0 / (max_val - min_val))

    return f

def mean(values):
    if len(values) == 0:
        return 0.0
    return sum(values) / len(values)

def sliding_window(values, N):
    """
    Generator of a slice of a list of values of length N, that starts
    at the beginning and slides along yielding ranges until it runs
    out of room.

    Example, with N == 3:
    
    ['a', 'b', 'c', 'd', 'e']
      ^    ^    ^             -> ['a', 'b', 'c']
           ^    ^    ^        -> ['b', 'c', 'd']
                ^    ^    ^   -> ['c', 'd', 'e']
    """
    i, j = 0, N-1
    len_values = len(values)
    while (j < len_values):
        yield values[i:j]
        i += 1
        j += 1
    
def moving_function(values, N, f):
    return [f(v) for v in sliding_window(values, N)]

def moving_average(values, N):
    """
    Calculates the N-moving average of a list of data points.
    
    Assumes `values' is sorted.
    """
    return moving_function(values, N, mean)

def moving_sum(values, N):
    return moving_function(values, N, sum)

def variance(values):
    """
    Calculates the variance, or the mean deviation of a list of values
    from the mean.
    """
    if len(values) == 0:
        return 0.0
    mean_ = mean(values)
    return sum(math.pow(X - mean_, 2) for X in values) / len(values)

def stddev(values):
    """
    Calculates the standard deviation of a list of values.

    Standard deviation is the square root of the variance.
    """
    return math.sqrt(variance(values))

def percent_within_stddev(values, N=1):
    """
    Calculates the percentage of values that lie within N standard
    deviations of the mean.

    The 68-95-99.7 rule (aka three sigma rule, or empirical rule),
    tells us that almost all values in a normal distribution lie
    within 3 standard deviations of the mean.
    """
    mean_ = mean(values)
    stddev_ = stddev(values)
    num_within = len([v for v in values
                      if (v - stddev_ * N) <= mean_ and (v + stddev_ * N) >= mean_])
    return num_within / len(values)

if __name__ == '__main__':
    import doctest
    doctest.testmod()

########NEW FILE########
__FILENAME__ = text
import re

slugify = lambda x: re.sub('[-\s]+', '-', re.sub('[^\w\s-]', '', x.strip())).lower()

def intcomma(orig):
    """
    Converts an integer to a string containing commas every three digits.
    For example, 3000 becomes '3,000' and 45000 becomes '45,000'.
    """
    new = re.sub("^(-?\d+)(\d{3})", '\g<1>,\g<2>', orig)
    if orig == new:
        return new
    else:
        return intcomma(new)

def clean_address(addr):
    """
    Given an address string, normalizes it to look pretty.

    >>> clean_address('123 MAIN')
    '123 Main'
    >>> clean_address('123 MAIN ST')
    '123 Main St.'
    >>> clean_address('123 MAIN ST S')
    '123 Main St. S.'
    >>> clean_address('123 AVENUE A')
    '123 Avenue A'
    >>> clean_address('2 N ST LAWRENCE PKWY')
    '2 N. St. Lawrence Pkwy.'
    >>> clean_address('123 NORTH AVENUE') # Don't abbreviate 'AVENUE'
    '123 North Avenue'
    >>> clean_address('123 N. Main St.')
    '123 N. Main St.'
    >>> clean_address('  123  N  WABASH  AVE   ')
    '123 N. Wabash Ave.'
    >>> clean_address('123 MAIN ST SW')
    '123 Main St. S.W.'
    >>> clean_address('123 MAIN ST NE')
    '123 Main St. N.E.'
    >>> clean_address('123 NEW YORK ST NE') # Don't punctuate 'NEW' (which contains 'NE')
    '123 New York St. N.E.'
    >>> clean_address('123 MAIN St Ne')
    '123 Main St. N.E.'
    >>> clean_address('123 MAIN St n.e.')
    '123 Main St. N.E.'
    """
    addr = smart_title(addr)
    addr = re.sub(r'\b(Ave|Blvd|Bvd|Cir|Ct|Dr|Ln|Pkwy|Pl|Plz|Pt|Pts|Rd|Rte|Sq|Sqs|St|Sts|Ter|Terr|Trl|Wy|N|S|E|W)(?!\.)\b', r'\1.', addr)

    # Take care of NE/NW/SE/SW.
    addr = re.sub(r'\b([NSns])\.?([EWew])\b\.?', lambda m: ('%s.%s.' % m.groups()).upper(), addr)

    addr = re.sub(r'\s\s+', ' ', addr).strip()
    return addr

def address_to_block(addr):
    """
    Given an address string, normalizes it to the 100 block level.

    >>> address_to_block('1 N. Main Street')
    '0 block of N. Main Street'
    >>> address_to_block('10 N. Main Street')
    '0 block of N. Main Street'
    >>> address_to_block('123 Main Street')
    '100 block of Main Street'
    >>> address_to_block('123 MAIN STREET')
    '100 block of MAIN STREET'
    >>> address_to_block('4523 Main Street')
    '4500 block of Main Street'
    >>> address_to_block('  123 Main Street')
    '100 block of Main Street'
    """
    return re.sub(r'^\s*(\d+) ', lambda m: '%s block of ' % re.sub('..?$', (len(m.group(1)) > 2 and '00' or '0'), m.group(1)), addr)

def smart_title(s, exceptions=None):
    r"""
    Like .title(), but smarter.

    >>> smart_title('hello THERE')
    'Hello There'
    >>> smart_title('128th street')
    '128th Street'
    >>> smart_title('"what the heck," he said. "let\'s go to the zoo."')
    '"What The Heck," He Said. "Let\'s Go To The Zoo."'
    >>> smart_title('')
    ''
    >>> smart_title('a')
    'A'
    >>> smart_title('(this is a parenthetical.)')
    '(This Is A Parenthetical.)'
    >>> smart_title('non-functional')
    'Non-Functional'
    >>> smart_title("BILL'S HOUSE OF WAX LIPS LLC", ["of", "LLC"])
    "Bill's House of Wax Lips LLC"
    >>> smart_title("The C.I.A.", ["C.I.A."])
    'The C.I.A.'
    """
    result = re.sub(r"(?<=[\s\"\(-])(\w)", lambda m: m.group(1).upper(), s.lower())
    if result:
        result = result[0].upper() + result[1:]

    # Handle the exceptions.
    if exceptions is not None:
        for e in exceptions:
            pat = re.escape(e)
            if re.search("^\w", pat):
                pat = r"\b%s" % pat
            if re.search("\w$", pat):
                pat = r"%s\b" % pat
            pat = r"(?i)%s" % pat
            result = re.sub(pat, e, result)

    return result

def smart_excerpt(text, highlighted_text):
    """
    Returns a short excerpt of the given text with `highlighted_text`
    guaranteed to be in the middle.
    """
    m = re.search('(?:\w+\W+){0,15}%s(?:\W+\w+){0,15}' % highlighted_text, text)
    if not m:
        raise ValueError('Value not found in text')
    excerpt = m.group()
    elipsis_start = not text.startswith(excerpt)
    elipsis_end = not text.endswith(excerpt)
    if elipsis_start:
        excerpt = '...' + excerpt
    if elipsis_end:
        excerpt += '...'
    return excerpt

if __name__ == "__main__":
    import doctest
    doctest.testmod()

########NEW FILE########
__FILENAME__ = urlresolvers
"""
Django URL resolvers that take into account the value of get_metro().

TODO: Currently, get_metro() is called and calculated each time through the
URL patterns, which could be inefficient. Look for a better way of doing this.
"""

from django.core.exceptions import ImproperlyConfigured
from django.core.urlresolvers import RegexURLPattern
from ebpub.metros.allmetros import get_metro

class MulticityRegexURLPattern(RegexURLPattern):
    def resolve(self, path):
        if not get_metro()['multiple_cities']:
            return None
        return RegexURLPattern.resolve(self, path)

class SinglecityRegexURLPattern(RegexURLPattern):
    def resolve(self, path):
        if get_metro()['multiple_cities']:
            return None
        return RegexURLPattern.resolve(self, path)

def metro_patterns(multi, single):
    pattern_list = []
    for t in multi:
        pattern_list.append(url(MulticityRegexURLPattern, *t))
    for t in single:
        pattern_list.append(url(SinglecityRegexURLPattern, *t))
    return pattern_list

def url(klass, regex, view, kwargs=None, name=None, prefix=''):
    if type(view) == list:
        # For include(...) processing.
        return RegexURLResolver(regex, view[0], kwargs)
    else:
        if isinstance(view, basestring):
            if not view:
                raise ImproperlyConfigured('Empty URL pattern view name not permitted (for pattern %r)' % regex)
            if prefix:
                view = prefix + '.' + view
        return klass(regex, view, kwargs, name)

########NEW FILE########
__FILENAME__ = view_utils
from django.shortcuts import render_to_response
from django.template.context import RequestContext

def eb_render(request, *args, **kwargs):
    """
    Replacement for render_to_response that uses RequestContext and sets an
    extra template variable, TEMPLATE_NAME.
    """
    kwargs['context_instance'] = RequestContext(request)
    return render_to_response(*args, **kwargs)

########NEW FILE########
__FILENAME__ = settings
import os

DEBUG = True

DATABASE_ENGINE = 'postgresql_psycopg2'
DATABASE_USER = ''
DATABASE_HOST = ''
DATABASE_NAME = 'wiki'

INSTALLED_APPS = (
    'ebwiki.wiki',
)

TEMPLATE_DIRS = (
    os.path.normpath(os.path.join(os.path.dirname(__file__), 'templates')),
)

ROOT_URLCONF = 'ebwiki.wiki.urls'

WIKI_DOC_ROOT = os.path.normpath(os.path.join(os.path.dirname(__file__), 'media'))

########NEW FILE########
__FILENAME__ = feeds
from django.contrib.syndication.feeds import Feed
from django.utils.feedgenerator import Atom1Feed
from ebwiki.wiki.models import Page

class LatestEdits(Feed):
    title = "ebwiki"
    link = "/"
    subtitle = "Latest edits made to the ebwiki."
    title_template = "wiki/feeds/latest_title.html"
    description_template = "wiki/feeds/latest_description.html"

    feed_type = Atom1Feed

    def items(self):
        return Page.objects.order_by("-change_date")[:30]

    def item_link(self, item):
        return item.version_url()

    def item_author_name(self, item):
        return item.change_user

    def item_pubdate(self, item):
        return item.change_date

########NEW FILE########
__FILENAME__ = markdown
#!/usr/bin/env python

# The following constant specifies the name used in the usage
# statement displayed for python versions lower than 2.3.  (With
# python2.3 and higher the usage statement is generated by optparse
# and uses the actual name of the executable called.)

EXECUTABLE_NAME_FOR_USAGE = "python markdown.py"

SPEED_TEST = 0

"""
====================================================================
IF YOA ARE LOOKING TO EXTEND MARKDOWN, SEE THE "FOOTNOTES" SECTION
====================================================================

ADRIAN CHANGED STUFF AROUND LINE 617 TO ENABLE WIKI-SPECIFIC LINKS
AND ADDED THE urllib IMPORT

Python-Markdown
===============

Converts Markdown to HTML.  Basic usage as a module:

    import markdown
    html = markdown.markdown(your_text_string)

Started by [Manfred Stienstra](http://www.dwerg.net/).  Continued and
maintained  by [Yuri Takhteyev](http://www.freewisdom.org).

Project website: http://www.freewisdom.org/projects/python-markdown
Contact: yuri [at] freewisdom.org

License: GPL 2 (http://www.gnu.org/copyleft/gpl.html) or BSD

Version: 1.5a (July 9, 2006)

For changelog, see end of file
"""

import re, sys, codecs
import urllib

# set debug level: 3 none, 2 critical, 1 informative, 0 all
(VERBOSE, INFO, CRITICAL, NONE) = range(4)

MESSAGE_THRESHOLD = CRITICAL

def message(level, text) :
    if level >= MESSAGE_THRESHOLD :
        print text


# --------------- CONSTANTS YOU MIGHT WANT TO MODIFY -----------------

# all tabs will be expanded to up to this many spaces
TAB_LENGTH = 4
ENABLE_ATTRIBUTES = 1
SMART_EMPHASIS = 1

# --------------- CONSTANTS YOU _SHOULD NOT_ HAVE TO CHANGE ----------

# a template for html placeholders
HTML_PLACEHOLDER_PREFIX = "qaodmasdkwaspemas"
HTML_PLACEHOLDER = HTML_PLACEHOLDER_PREFIX + "%dajkqlsmdqpakldnzsdfls"

BLOCK_LEVEL_ELEMENTS = ['p', 'div', 'blockquote', 'pre', 'table',
                        'dl', 'ol', 'ul', 'script', 'noscript',
                        'form', 'fieldset', 'iframe', 'math', 'ins',
                        'del', 'hr', 'hr/', 'style']

def is_block_level (tag) :
    return ( (tag in BLOCK_LEVEL_ELEMENTS) or
             (tag[0] == 'h' and tag[1] in "0123456789") )

"""
======================================================================
========================== NANODOM ===================================
======================================================================

The three classes below implement some of the most basic DOM
methods.  I use this instead of minidom because I need a simpler
functionality and do not want to require additional libraries.

Importantly, NanoDom does not do normalization, which is what we
want. It also adds extra white space when converting DOM to string
"""


class Document :

    def appendChild(self, child) :
        self.documentElement = child
        child.parent = self
        self.entities = {}

    def createElement(self, tag, textNode=None) :
        el = Element(tag)
        el.doc = self
        if textNode :
            el.appendChild(self.createTextNode(textNode))
        return el

    def createTextNode(self, text) :
        node = TextNode(text)
        node.doc = self
        return node

    def createEntityReference(self, entity):
        if entity not in self.entities:
            self.entities[entity] = EntityReference(entity)
        return self.entities[entity]

    def toxml (self) :
        return self.documentElement.toxml()

    def normalizeEntities(self, text) :

        pairs = [ ("&", "&amp;"),
                  ("<", "&lt;"),
                  (">", "&gt;"),
                  ("\"", "&quot;")]


        for old, new in pairs :
            text = text.replace(old, new)
        return text

    def find(self, test) :
        return self.documentElement.find(test)

    def unlink(self) :
        self.documentElement.unlink()
        self.documentElement = None


class Element :

    type = "element"

    def __init__ (self, tag) :

        self.nodeName = tag
        self.attributes = []
        self.attribute_values = {}
        self.childNodes = []

    def unlink(self) :
        for child in self.childNodes :
            if child.type == "element" :
                child.unlink()
        self.childNodes = None

    def setAttribute(self, attr, value) :
        if not attr in self.attributes :
            self.attributes.append(attr)

        self.attribute_values[attr] = value

    def insertChild(self, position, child) :
        self.childNodes.insert(position, child)
        child.parent = self

    def removeChild(self, child) :
        self.childNodes.remove(child)

    def replaceChild(self, oldChild, newChild) :
        position = self.childNodes.index(oldChild)
        self.removeChild(oldChild)
        self.insertChild(position, newChild)

    def appendChild(self, child) :
        self.childNodes.append(child)
        child.parent = self

    def handleAttributes(self) :
        pass

    def find(self, test, depth=0) :
        """ Returns a list of descendants that pass the test function """
        matched_nodes = []
        for child in self.childNodes :
            if test(child) :
                matched_nodes.append(child)
            if child.type == "element" :
                matched_nodes += child.find(test, depth+1)
        return matched_nodes

    def toxml(self):
        if ENABLE_ATTRIBUTES :
            for child in self.childNodes:
                child.handleAttributes()
        buffer = ""
        if self.nodeName in ['h1', 'h2', 'h3', 'h4'] :
            buffer += "\n"
        elif self.nodeName in ['li'] :
            buffer += "\n "
        buffer += "<" + self.nodeName
        for attr in self.attributes :
            value = self.attribute_values[attr]
            value = self.doc.normalizeEntities(value)
            buffer += ' %s="%s"' % (attr, value)
        if self.childNodes or self.nodeName in ['blockquote']:
            buffer += ">"
            for child in self.childNodes :
                buffer += child.toxml()
            if self.nodeName == 'p' :
                buffer += "\n"
            elif self.nodeName == 'li' :
                buffer += "\n "
            buffer += "</%s>" % self.nodeName
        else :
            buffer += "/>"
        if self.nodeName in ['p', 'li', 'ul', 'ol',
                             'h1', 'h2', 'h3', 'h4'] :
            buffer += "\n"

        return buffer


class TextNode :

    type = "text"
    attrRegExp = re.compile(r'\{@([^\}]*)=([^\}]*)}') # {@id=123}

    def __init__ (self, text) :
        self.value = text

    def attributeCallback(self, match) :
        self.parent.setAttribute(match.group(1), match.group(2))

    def handleAttributes(self) :
        self.value = self.attrRegExp.sub(self.attributeCallback, self.value)

    def toxml(self) :
        text = self.value
        if not text.startswith(HTML_PLACEHOLDER_PREFIX):
            if self.parent.nodeName == "p" :
                text = text.replace("\n", "\n   ")
            elif (self.parent.nodeName == "li"
                  and self.parent.childNodes[0]==self):
                text = "\n     " + text.replace("\n", "\n     ")
        text = self.doc.normalizeEntities(text)
        return text


class EntityReference:

    type = "entity_ref"

    def __init__(self, entity):
        self.entity = entity

    def handleAttributes(self):
        pass

    def toxml(self):
        return "&" + self.entity + ";"


"""
======================================================================
========================== PRE-PROCESSORS ============================
======================================================================

Preprocessors munge source text before we start doing anything too
complicated.

Each preprocessor implements a "run" method that takes a pointer to a list of lines of the document,
modifies it as necessary and returns either the same pointer or a
pointer to a new list.  Preprocessors must extend
markdown.Preprocessor.

"""


class Preprocessor :
    pass


class HeaderPreprocessor (Preprocessor):

    """
       Replaces underlined headers with hashed headers to avoid
       the nead for lookahead later.
    """

    def run (self, lines) :

        i = -1
        while i+1 < len(lines) :
            i = i+1
            if not lines[i].strip() :
                continue

            if lines[i].startswith("#") :
                lines.insert(i+1, "\n")

            if (i+1 <= len(lines)
                  and lines[i+1]
                  and lines[i+1][0] in ['-', '=']) :

                underline = lines[i+1].strip()

                if underline == "="*len(underline) :
                    lines[i] = "# " + lines[i].strip()
                    lines[i+1] = ""
                elif underline == "-"*len(underline) :
                    lines[i] = "## " + lines[i].strip()
                    lines[i+1] = ""

        #for l in lines :
        #    print l.encode('utf8')
        #sys.exit(0)

        return lines

HEADER_PREPROCESSOR = HeaderPreprocessor()

class LinePreprocessor (Preprocessor):
    """Deals with HR lines (needs to be done before processing lists)"""

    def run (self, lines) :
        for i in range(len(lines)) :
            if self._isLine(lines[i]) :
                lines[i] = "<hr />"
        return lines

    def _isLine(self, block) :
        """Determines if a block should be replaced with an <HR>"""
        if block.startswith("    ") : return 0  # a code block
        text = "".join([x for x in block if not x.isspace()])
        if len(text) <= 2 :
            return 0
        for pattern in ['isline1', 'isline2', 'isline3'] :
            m = RE.regExp[pattern].match(text)
            if (m and m.group(1)) :
                return 1
        else:
            return 0

LINE_PREPROCESSOR = LinePreprocessor()


class LineBreaksPreprocessor (Preprocessor):
    """Replaces double spaces at the end of the lines with <br/ >."""

    def run (self, lines) :
        for i in range(len(lines)) :
            if (lines[i].endswith("  ")
                and not RE.regExp['tabbed'].match(lines[i]) ):
                lines[i] += "<br />"
        return lines

LINE_BREAKS_PREPROCESSOR = LineBreaksPreprocessor()


class HtmlBlockPreprocessor (Preprocessor):
    """Removes html blocks from self.lines"""

    def _get_left_tag(self, block):
        return block[1:].replace(">", " ", 1).split()[0].lower()


    def _get_right_tag(self, left_tag, block):
        return block.rstrip()[-len(left_tag)-2:-1].lower()

    def _equal_tags(self, left_tag, right_tag):
        if left_tag in ['?', '?php', 'div'] : # handle PHP, etc.
            return True
        if ("/" + left_tag) == right_tag:
            return True
        elif left_tag == right_tag[1:] \
            and right_tag[0] != "<":
            return True
        else:
            return False

    def _is_oneliner(self, tag):
        return (tag in ['hr', 'hr/'])


    def run (self, lines) :
        new_blocks = []
        text = "\n".join(lines)
        text = text.split("\n\n")

        items = []
        left_tag = ''
        right_tag = ''
        in_tag = False # flag

        for block in text:
            if block.startswith("\n") :
                block = block[1:]

            if not in_tag:

                if block.startswith("<"):

                    left_tag = self._get_left_tag(block)
                    right_tag = self._get_right_tag(left_tag, block)

                    if not (is_block_level(left_tag) \
                        or block[1] in ["!", "?", "@", "%"]):
                        new_blocks.append(block)
                        continue

                    if self._is_oneliner(left_tag):
                        new_blocks.append(block.strip())
                        continue

                    if block[1] == "!":
                        # is a comment block
                        left_tag = "--"
                        right_tag = self._get_right_tag(left_tag, block)
                        # keep checking conditions below and maybe just append

                    if block.rstrip().endswith(">") \
                        and self._equal_tags(left_tag, right_tag):
                        new_blocks.append(
                            self.stash.store(block.strip()))
                        continue
                    elif not block[1] == "!":
                        # if is block level tag and is not complete
                        items.append(block.strip())
                        in_tag = True
                        continue

                new_blocks.append(block)

            else:
                items.append(block.strip())

                right_tag = self._get_right_tag(left_tag, block)
                if self._equal_tags(left_tag, right_tag):
                    # if find closing tag
                    in_tag = False
                    new_blocks.append(
                        self.stash.store('\n\n'.join(items)))
                    items = []

        return "\n\n".join(new_blocks).split("\n")

HTML_BLOCK_PREPROCESSOR = HtmlBlockPreprocessor()


class ReferencePreprocessor (Preprocessor):

    def run (self, lines) :

        new_text = [];
        for line in lines:
            m = RE.regExp['reference-def'].match(line)
            if m:
                id = m.group(2).strip().lower()
                t = m.group(4).strip()  # potential title
                if not t :
                    self.references[id] = (m.group(3), t)
                elif (len(t) >= 2
                      and (t[0] == t[-1] == "\""
                           or t[0] == t[-1] == "\'"
                           or (t[0] == "(" and t[-1] == ")") ) ) :
                    self.references[id] = (m.group(3), t[1:-1])
                else :
                    new_text.append(line)
            else:
                new_text.append(line)

        return new_text #+ "\n"

REFERENCE_PREPROCESSOR = ReferencePreprocessor()

"""
======================================================================
========================== INLINE PATTERNS ===========================
======================================================================

Inline patterns such as *emphasis* are handled by means of auxiliary
objects, one per pattern.  Pattern objects must be instances of classes
that extend markdown.Pattern.  Each pattern object uses a single regular
expression and needs support the following methods:

  pattern.getCompiledRegExp() - returns a regular expression

  pattern.handleMatch(m, doc) - takes a match object and returns
                                a NanoDom node (as a part of the provided
                                doc) or None

All of python markdown's built-in patterns subclass from Patter,
but you can add additional patterns that don't.

Also note that all the regular expressions used by inline must
capture the whole block.  For this reason, they all start with
'^(.*)' and end with '(.*)!'.  In case with built-in expression
Pattern takes care of adding the "^(.*)" and "(.*)!".

Finally, the order in which regular expressions are applied is very
important - e.g. if we first replace http://.../ links with <a> tags
and _then_ try to replace inline html, we would end up with a mess.
So, we apply the expressions in the following order:

       * escape and backticks have to go before everything else, so
         that we can preempt any markdown patterns by escaping them.

       * then we handle auto-links (must be done before inline html)

       * then we handle inline HTML.  At this point we will simply
         replace all inline HTML strings with a placeholder and add
         the actual HTML to a hash.

       * then inline images (must be done before links)

       * then bracketed links, first regular then reference-style

       * finally we apply strong and emphasis
"""

NOBRACKET = r'[^\]\[]*'
BRK = ( r'\[('
        + (NOBRACKET + r'(\['+NOBRACKET)*6
        + (NOBRACKET+ r'\])*'+NOBRACKET)*6
        + NOBRACKET + r')\]' )

BACKTICK_RE = r'\`([^\`]*)\`'                    # `e= m*c^2`
DOUBLE_BACKTICK_RE =  r'\`\`(.*)\`\`'            # ``e=f("`")``
ESCAPE_RE = r'\\(.)'                             # \<
EMPHASIS_RE = r'\*([^\*]*)\*'                    # *emphasis*
STRONG_RE = r'\*\*(.*)\*\*'                      # **strong**
STRONG_EM_RE = r'\*\*\*([^_]*)\*\*\*'            # ***strong***

if SMART_EMPHASIS:
    EMPHASIS_2_RE = r'(?<!\S)_(\S[^_]*)_'        # _emphasis_
else :
    EMPHASIS_2_RE = r'_([^_]*)_'                 # _emphasis_

STRONG_2_RE = r'__([^_]*)__'                     # __strong__
STRONG_EM_2_RE = r'___([^_]*)___'                # ___strong___

LINK_RE = BRK + r'\s*\(([^\)]*)\)'               # [text](url)
LINK_ANGLED_RE = BRK + r'\s*\(<([^\)]*)>\)'      # [text](<url>)
IMAGE_LINK_RE = r'\!' + BRK + r'\s*\(([^\)]*)\)' # ![alttxt](http://x.com/)
REFERENCE_RE = BRK+ r'\s*\[([^\]]*)\]'           # [Google][3]
IMAGE_REFERENCE_RE = r'\!' + BRK + '\s*\[([^\]]*)\]' # ![alt text][2]
NOT_STRONG_RE = r'( \* )'                        # stand-alone * or _
AUTOLINK_RE = r'<(http://[^>]*)>'                # <http://www.123.com>
AUTOMAIL_RE = r'<([^> \!]*@[^> ]*)>'               # <me@example.com>
#HTML_RE = r'(\<[^\>]*\>)'                        # <...>
HTML_RE = r'(\<[a-zA-Z/][^\>]*\>)'               # <...>
ENTITY_RE = r'(&[\#a-zA-Z0-9]*;)'                # &amp;

class Pattern:

    def __init__ (self, pattern) :
        self.pattern = pattern
        self.compiled_re = re.compile("^(.*)%s(.*)$" % pattern, re.DOTALL)

    def getCompiledRegExp (self) :
        return self.compiled_re

BasePattern = Pattern # for backward compatibility

class SimpleTextPattern (Pattern) :

    def handleMatch(self, m, doc) :
        return doc.createTextNode(m.group(2))

class SimpleTagPattern (Pattern):

    def __init__ (self, pattern, tag) :
        Pattern.__init__(self, pattern)
        self.tag = tag

    def handleMatch(self, m, doc) :
        el = doc.createElement(self.tag)
        el.appendChild(doc.createTextNode(m.group(2)))
        return el

class BacktickPattern (Pattern):

    def __init__ (self, pattern):
        Pattern.__init__(self, pattern)
        self.tag = "code"

    def handleMatch(self, m, doc) :
        el = doc.createElement(self.tag)
        text = m.group(2).strip()
        #text = text.replace("&", "&amp;")
        el.appendChild(doc.createTextNode(text))
        return el


class DoubleTagPattern (SimpleTagPattern) :

    def handleMatch(self, m, doc) :
        tag1, tag2 = self.tag.split(",")
        el1 = doc.createElement(tag1)
        el2 = doc.createElement(tag2)
        el1.appendChild(el2)
        el2.appendChild(doc.createTextNode(m.group(2)))
        return el1


class HtmlPattern (Pattern):

    def handleMatch (self, m, doc) :
        place_holder = self.stash.store(m.group(2))
        return doc.createTextNode(place_holder)


class LinkPattern (Pattern):

    def handleMatch(self, m, doc) :
        el = doc.createElement('a')
        el.appendChild(doc.createTextNode(m.group(2)))
        parts = m.group(9).split()
        # We should now have [], [href], or [href, title]
        if parts :
            # ADRIAN CHANGED THIS
            url = parts[0]
            if not re.search('^https?://', url):
                url = '/%s/' % url
            else:
                url = '/r/?url=%s' % urllib.quote(url)
            el.setAttribute('href', url)
        else :
            el.setAttribute('href', "")
        if len(parts) > 1 :
            # we also got a title
            title = " ".join(parts[1:]).strip()
            title = dequote(title) #.replace('"', "&quot;")
            el.setAttribute('title', title)
        return el


class ImagePattern (Pattern):

    def handleMatch(self, m, doc):
        el = doc.createElement('img')
        src_parts = m.group(9).split()
        el.setAttribute('src', src_parts[0])
        if len(src_parts) > 1 :
            el.setAttribute('title', dequote(" ".join(src_parts[1:])))
        if ENABLE_ATTRIBUTES :
            text = doc.createTextNode(m.group(2))
            el.appendChild(text)
            text.handleAttributes()
            truealt = text.value
            el.childNodes.remove(text)
        else:
            truealt = m.group(2)
        el.setAttribute('alt', truealt)
        return el

class ReferencePattern (Pattern):

    def handleMatch(self, m, doc):
        if m.group(9) :
            id = m.group(9).lower()
        else :
            # if we got something like "[Google][]"
            # we'll use "google" as the id
            id = m.group(2).lower()
        if not self.references.has_key(id) : # ignore undefined refs
            return None
        href, title = self.references[id]
        text = m.group(2)
        return self.makeTag(href, title, text, doc)

    def makeTag(self, href, title, text, doc):
        el = doc.createElement('a')
        el.setAttribute('href', href)
        if title :
            el.setAttribute('title', title)
        el.appendChild(doc.createTextNode(text))
        return el


class ImageReferencePattern (ReferencePattern):

    def makeTag(self, href, title, text, doc):
        el = doc.createElement('img')
        el.setAttribute('src', href)
        if title :
            el.setAttribute('title', title)
        el.setAttribute('alt', text)
        return el


class AutolinkPattern (Pattern):

    def handleMatch(self, m, doc):
        el = doc.createElement('a')
        el.setAttribute('href', m.group(2))
        el.appendChild(doc.createTextNode(m.group(2)))
        return el

class AutomailPattern (Pattern):

    def handleMatch(self, m, doc) :
        el = doc.createElement('a')
        email = m.group(2)
        if email.startswith("mailto:"):
            email = email[len("mailto:"):]
        for letter in email:
            entity = doc.createEntityReference("#%d" % ord(letter))
            el.appendChild(entity)
        mailto = "mailto:" + email
        mailto = "".join(['&#%d;' % ord(letter) for letter in mailto])
        el.setAttribute('href', mailto)
        return el

ESCAPE_PATTERN          = SimpleTextPattern(ESCAPE_RE)
NOT_STRONG_PATTERN      = SimpleTextPattern(NOT_STRONG_RE)

BACKTICK_PATTERN        = BacktickPattern(BACKTICK_RE)
DOUBLE_BACKTICK_PATTERN = BacktickPattern(DOUBLE_BACKTICK_RE)
STRONG_PATTERN          = SimpleTagPattern(STRONG_RE, 'strong')
STRONG_PATTERN_2        = SimpleTagPattern(STRONG_2_RE, 'strong')
EMPHASIS_PATTERN        = SimpleTagPattern(EMPHASIS_RE, 'em')
EMPHASIS_PATTERN_2      = SimpleTagPattern(EMPHASIS_2_RE, 'em')

STRONG_EM_PATTERN       = DoubleTagPattern(STRONG_EM_RE, 'strong,em')
STRONG_EM_PATTERN_2     = DoubleTagPattern(STRONG_EM_2_RE, 'strong,em')

LINK_PATTERN            = LinkPattern(LINK_RE)
LINK_ANGLED_PATTERN     = LinkPattern(LINK_ANGLED_RE)
IMAGE_LINK_PATTERN      = ImagePattern(IMAGE_LINK_RE)
IMAGE_REFERENCE_PATTERN = ImageReferencePattern(IMAGE_REFERENCE_RE)
REFERENCE_PATTERN       = ReferencePattern(REFERENCE_RE)

HTML_PATTERN            = HtmlPattern(HTML_RE)
ENTITY_PATTERN          = HtmlPattern(ENTITY_RE)

AUTOLINK_PATTERN        = AutolinkPattern(AUTOLINK_RE)
AUTOMAIL_PATTERN        = AutomailPattern(AUTOMAIL_RE)


"""
======================================================================
========================== POST-PROCESSORS ===========================
======================================================================

Markdown also allows post-processors, which are similar to
preprocessors in that they need to implement a "run" method.  Unlike
pre-processors, they take a NanoDom document as a parameter and work
with that.

Post-Processor should extend markdown.Postprocessor.

There are currently no standard post-processors, but the footnote
extension below uses one.
"""

class Postprocessor :
    pass


"""
======================================================================
========================== MISC AUXILIARY CLASSES ====================
======================================================================
"""

class HtmlStash :
    """This class is used for stashing HTML objects that we extract
        in the beginning and replace with place-holders."""

    def __init__ (self) :
        self.html_counter = 0 # for counting inline html segments
        self.rawHtmlBlocks=[]

    def store(self, html) :
        """Saves an HTML segment for later reinsertion.  Returns a
           placeholder string that needs to be inserted into the
           document.

           @param html: an html segment
           @returns : a placeholder string """
        self.rawHtmlBlocks.append(html)
        placeholder = HTML_PLACEHOLDER % self.html_counter
        self.html_counter += 1
        return placeholder


class BlockGuru :

    def _findHead(self, lines, fn, allowBlank=0) :

        """Functional magic to help determine boundaries of indented
           blocks.

           @param lines: an array of strings
           @param fn: a function that returns a substring of a string
                      if the string matches the necessary criteria
           @param allowBlank: specifies whether it's ok to have blank
                      lines between matching functions
           @returns: a list of post processes items and the unused
                      remainder of the original list"""

        items = []
        item = -1

        i = 0 # to keep track of where we are

        for line in lines :

            if not line.strip() and not allowBlank:
                return items, lines[i:]

            if not line.strip() and allowBlank:
                # If we see a blank line, this _might_ be the end
                i += 1

                # Find the next non-blank line
                for j in range(i, len(lines)) :
                    if lines[j].strip() :
                        next = lines[j]
                        break
                else :
                    # There is no more text => this is the end
                    break

                # Check if the next non-blank line is still a part of the list

                part = fn(next)

                if part :
                    items.append("")
                    continue
                else :
                    break # found end of the list

            part = fn(line)

            if part :
                items.append(part)
                i += 1
                continue
            else :
                return items, lines[i:]
        else :
            i += 1

        return items, lines[i:]


    def detabbed_fn(self, line) :
        """ An auxiliary method to be passed to _findHead """
        m = RE.regExp['tabbed'].match(line)
        if m:
            return m.group(4)
        else :
            return None


    def detectTabbed(self, lines) :

        return self._findHead(lines, self.detabbed_fn,
                              allowBlank = 1)


def print_error(string):
    """Print an error string to stderr"""
    sys.stderr.write(string +'\n')


def dequote(string) :
    """ Removes quotes from around a string """
    if ( ( string.startswith('"') and string.endswith('"'))
         or (string.startswith("'") and string.endswith("'")) ) :
        return string[1:-1]
    else :
        return string

"""
======================================================================
========================== CORE MARKDOWN =============================
======================================================================

This stuff is ugly, so if you are thinking of extending the syntax,
see first if you can do it via pre-processors, post-processors,
inline patterns or a combination of the three.
"""

class CorePatterns :
    """This class is scheduled for removal as part of a refactoring
        effort."""

    patterns = {
        'header':          r'(#*)([^#]*)(#*)', # # A title
        'reference-def' :  r'(\ ?\ ?\ ?)\[([^\]]*)\]:\s*([^ ]*)(.*)',
                           # [Google]: http://www.google.com/
        'containsline':    r'([-]*)$|^([=]*)', # -----, =====, etc.
        'ol':              r'[ ]{0,3}[\d]*\.\s+(.*)', # 1. text
        'ul':              r'[ ]{0,3}[*+-]\s+(.*)', # "* text"
        'isline1':         r'(\**)', # ***
        'isline2':         r'(\-*)', # ---
        'isline3':         r'(\_*)', # ___
        'tabbed':          r'((\t)|(    ))(.*)', # an indented line
        'quoted' :         r'> ?(.*)', # a quoted block ("> ...")
    }

    def __init__ (self) :

        self.regExp = {}
        for key in self.patterns.keys() :
            self.regExp[key] = re.compile("^%s$" % self.patterns[key],
                                          re.DOTALL)

        self.regExp['containsline'] = re.compile(r'^([-]*)$|^([=]*)$', re.M)

RE = CorePatterns()


class Markdown:
    """ Markdown formatter class for creating an html document from
        Markdown text """


    def __init__(self, source=None,
                 extensions=[],
                 extension_configs=None,
                 encoding=None,
                 safe_mode = True):
        """Creates a new Markdown instance.

           @param source: The text in Markdown format.
           @param encoding: The character encoding of <text>. """

        self.safeMode = safe_mode
        self.encoding = encoding
        self.source = source
        self.blockGuru = BlockGuru()
        self.registeredExtensions = []
        self.stripTopLevelTags = 1
        self.docType = ""

        self.preprocessors = [ HEADER_PREPROCESSOR,
                               LINE_PREPROCESSOR,
                               HTML_BLOCK_PREPROCESSOR,
                               LINE_BREAKS_PREPROCESSOR,
                               # A footnote preprocessor will
                               # get inserted here
                               REFERENCE_PREPROCESSOR ]


        self.postprocessors = [] # a footnote postprocessor will get
                                 # inserted later

        self.textPostprocessors = [] # a footnote postprocessor will get
                                     # inserted later

        self.prePatterns = []


        self.inlinePatterns = [ DOUBLE_BACKTICK_PATTERN,
                                BACKTICK_PATTERN,
                                ESCAPE_PATTERN,
                                IMAGE_LINK_PATTERN,
                                IMAGE_REFERENCE_PATTERN,
                                REFERENCE_PATTERN,
                                LINK_ANGLED_PATTERN,
                                LINK_PATTERN,
                                AUTOLINK_PATTERN,
                                AUTOMAIL_PATTERN,
                                HTML_PATTERN,
                                ENTITY_PATTERN,
                                NOT_STRONG_PATTERN,
                                STRONG_EM_PATTERN,
                                STRONG_EM_PATTERN_2,
                                STRONG_PATTERN,
                                STRONG_PATTERN_2,
                                EMPHASIS_PATTERN,
                                EMPHASIS_PATTERN_2
                                # The order of the handlers matters!!!
                                ]

        self.registerExtensions(extensions = extensions,
                                configs = extension_configs)

        self.reset()


    def registerExtensions(self, extensions, configs) :

        if not configs :
            configs = {}

        for ext in extensions :

            extension_module_name = "mdx_" + ext

            try :
                module = __import__(extension_module_name)

            except :
                message(CRITICAL,
                        "couldn't load extension %s (looking for %s module)"
                        % (ext, extension_module_name) )
            else :

                if configs.has_key(ext) :
                    configs_for_ext = configs[ext]
                else :
                    configs_for_ext = []
                extension = module.makeExtension(configs_for_ext)
                extension.extendMarkdown(self, globals())




    def registerExtension(self, extension) :
        """ This gets called by the extension """
        self.registeredExtensions.append(extension)

    def reset(self) :
        """Resets all state variables so that we can start
            with a new text."""
        self.references={}
        self.htmlStash = HtmlStash()

        HTML_BLOCK_PREPROCESSOR.stash = self.htmlStash
        REFERENCE_PREPROCESSOR.references = self.references
        HTML_PATTERN.stash = self.htmlStash
        ENTITY_PATTERN.stash = self.htmlStash
        REFERENCE_PATTERN.references = self.references
        IMAGE_REFERENCE_PATTERN.references = self.references

        for extension in self.registeredExtensions :
            extension.reset()


    def _transform(self):
        """Transforms the Markdown text into a XHTML body document

           @returns: A NanoDom Document """

        # Setup the document

        self.doc = Document()
        self.top_element = self.doc.createElement("span")
        self.top_element.appendChild(self.doc.createTextNode('\n'))
        self.top_element.setAttribute('class', 'markdown')
        self.doc.appendChild(self.top_element)

        # Fixup the source text
        text = self.source.strip()
        text = text.replace("\r\n", "\n").replace("\r", "\n")
        text += "\n\n"
        text = text.expandtabs(TAB_LENGTH)

        # Split into lines and run the preprocessors that will work with
        # self.lines

        self.lines = text.split("\n")

        # Run the pre-processors on the lines
        for prep in self.preprocessors :
            self.lines = prep.run(self.lines)

        # Create a NanoDom tree from the lines and attach it to Document


        buffer = []
        for line in self.lines :
            if line.startswith("#") :
                self._processSection(self.top_element, buffer)
                buffer = [line]
            else :
                buffer.append(line)
        self._processSection(self.top_element, buffer)

        #self._processSection(self.top_element, self.lines)

        # Not sure why I put this in but let's leave it for now.
        self.top_element.appendChild(self.doc.createTextNode('\n'))

        # Run the post-processors
        for postprocessor in self.postprocessors :
            postprocessor.run(self.doc)

        return self.doc


    def _processSection(self, parent_elem, lines,
                        inList = 0, looseList = 0) :

        """Process a section of a source document, looking for high
           level structural elements like lists, block quotes, code
           segments, html blocks, etc.  Some those then get stripped
           of their high level markup (e.g. get unindented) and the
           lower-level markup is processed recursively.

           @param parent_elem: A NanoDom element to which the content
                               will be added
           @param lines: a list of lines
           @param inList: a level
           @returns: None"""

        if not lines :
            return

        # Check if this section starts with a list, a blockquote or
        # a code block

        processFn = { 'ul' :     self._processUList,
                      'ol' :     self._processOList,
                      'quoted' : self._processQuote,
                      'tabbed' : self._processCodeBlock }

        for regexp in ['ul', 'ol', 'quoted', 'tabbed'] :
            m = RE.regExp[regexp].match(lines[0])
            if m :
                processFn[regexp](parent_elem, lines, inList)
                return

        # We are NOT looking at one of the high-level structures like
        # lists or blockquotes.  So, it's just a regular paragraph
        # (though perhaps nested inside a list or something else).  If
        # we are NOT inside a list, we just need to look for a blank
        # line to find the end of the block.  If we ARE inside a
        # list, however, we need to consider that a sublist does not
        # need to be separated by a blank line.  Rather, the following
        # markup is legal:
        #
        # * The top level list item
        #
        #     Another paragraph of the list.  This is where we are now.
        #     * Underneath we might have a sublist.
        #

        if inList :

            start, theRest = self._linesUntil(lines, (lambda line:
                             RE.regExp['ul'].match(line)
                             or RE.regExp['ol'].match(line)
                                              or not line.strip()))

            self._processSection(parent_elem, start,
                                 inList - 1, looseList = looseList)
            self._processSection(parent_elem, theRest,
                                 inList - 1, looseList = looseList)


        else : # Ok, so it's just a simple block

            paragraph, theRest = self._linesUntil(lines, lambda line:
                                                 not line.strip())

            if len(paragraph) and paragraph[0].startswith('#') :
                m = RE.regExp['header'].match(paragraph[0])
                if m :
                    level = len(m.group(1))
                    h = self.doc.createElement("h%d" % level)
                    parent_elem.appendChild(h)
                    for item in self._handleInlineWrapper2(m.group(2).strip()) :
                        h.appendChild(item)
                else :
                    message(CRITICAL, "We've got a problem header!")

            elif paragraph :

                list = self._handleInlineWrapper2("\n".join(paragraph))

                if ( parent_elem.nodeName == 'li'
                     and not (looseList or parent_elem.childNodes)):

                    #and not parent_elem.childNodes) :
                    # If this is the first paragraph inside "li", don't
                    # put <p> around it - append the paragraph bits directly
                    # onto parent_elem
                    el = parent_elem
                else :
                    # Otherwise make a "p" element
                    el = self.doc.createElement("p")
                    parent_elem.appendChild(el)

                for item in list :
                    el.appendChild(item)

            if theRest :
                theRest = theRest[1:]  # skip the first (blank) line

            self._processSection(parent_elem, theRest, inList)



    def _processUList(self, parent_elem, lines, inList) :
        self._processList(parent_elem, lines, inList,
                         listexpr='ul', tag = 'ul')

    def _processOList(self, parent_elem, lines, inList) :
        self._processList(parent_elem, lines, inList,
                         listexpr='ol', tag = 'ol')


    def _processList(self, parent_elem, lines, inList, listexpr, tag) :
        """Given a list of document lines starting with a list item,
           finds the end of the list, breaks it up, and recursively
           processes each list item and the remainder of the text file.

           @param parent_elem: A dom element to which the content will be added
           @param lines: a list of lines
           @param inList: a level
           @returns: None"""

        ul = self.doc.createElement(tag)  # ul might actually be '<ol>'
        parent_elem.appendChild(ul)

        looseList = 0

        # Make a list of list items
        items = []
        item = -1

        i = 0  # a counter to keep track of where we are

        for line in lines :

            loose = 0
            if not line.strip() :
                # If we see a blank line, this _might_ be the end of the list
                i += 1
                loose = 1

                # Find the next non-blank line
                for j in range(i, len(lines)) :
                    if lines[j].strip() :
                        next = lines[j]
                        break
                else :
                    # There is no more text => end of the list
                    break

                # Check if the next non-blank line is still a part of the list
                if ( RE.regExp['ul'].match(next) or
                     RE.regExp['ol'].match(next) or
                     RE.regExp['tabbed'].match(next) ):
                    # get rid of any white space in the line
                    items[item].append(line.strip())
                    looseList = loose or looseList
                    continue
                else :
                    break # found end of the list

            # Now we need to detect list items (at the current level)
            # while also detabing child elements if necessary

            for expr in ['ul', 'ol', 'tabbed']:

                m = RE.regExp[expr].match(line)
                if m :
                    if expr in ['ul', 'ol'] :  # We are looking at a new item
                        if m.group(1) :
                            items.append([m.group(1)])
                            item += 1
                    elif expr == 'tabbed' :  # This line needs to be detabbed
                        items[item].append(m.group(4)) #after the 'tab'

                    i += 1
                    break
            else :
                items[item].append(line)  # Just regular continuation
                i += 1 # added on 2006.02.25
        else :
            i += 1

        # Add the dom elements
        for item in items :
            li = self.doc.createElement("li")
            ul.appendChild(li)

            self._processSection(li, item, inList + 1, looseList = looseList)

        # Process the remaining part of the section

        self._processSection(parent_elem, lines[i:], inList)


    def _linesUntil(self, lines, condition) :
        """ A utility function to break a list of lines upon the
            first line that satisfied a condition.  The condition
            argument should be a predicate function.
            """

        i = -1
        for line in lines :
            i += 1
            if condition(line) : break
        else :
            i += 1
        return lines[:i], lines[i:]

    def _processQuote(self, parent_elem, lines, inList) :
        """Given a list of document lines starting with a quote finds
           the end of the quote, unindents it and recursively
           processes the body of the quote and the remainder of the
           text file.

           @param parent_elem: DOM element to which the content will be added
           @param lines: a list of lines
           @param inList: a level
           @returns: None """

        dequoted = []
        i = 0
        for line in lines :
            m = RE.regExp['quoted'].match(line)
            if m :
                dequoted.append(m.group(1))
                i += 1
            else :
                break
        else :
            i += 1

        blockquote = self.doc.createElement('blockquote')
        parent_elem.appendChild(blockquote)

        self._processSection(blockquote, dequoted, inList)
        self._processSection(parent_elem, lines[i:], inList)




    def _processCodeBlock(self, parent_elem, lines, inList) :
        """Given a list of document lines starting with a code block
           finds the end of the block, puts it into the dom verbatim
           wrapped in ("<pre><code>") and recursively processes the
           the remainder of the text file.

           @param parent_elem: DOM element to which the content will be added
           @param lines: a list of lines
           @param inList: a level
           @returns: None"""

        detabbed, theRest = self.blockGuru.detectTabbed(lines)

        pre = self.doc.createElement('pre')
        code = self.doc.createElement('code')
        parent_elem.appendChild(pre)
        pre.appendChild(code)
        text = "\n".join(detabbed).rstrip()+"\n"
        #text = text.replace("&", "&amp;")
        code.appendChild(self.doc.createTextNode(text))
        self._processSection(parent_elem, theRest, inList)


    def _handleInlineWrapper2 (self, line) :


        parts = [line]

        #if not(line):
        #    return [self.doc.createTextNode(' ')]

        for pattern in self.inlinePatterns :

            #print
            #print self.inlinePatterns.index(pattern)

            i = 0

            #print parts
            while i < len(parts) :

                x = parts[i]
                #print i
                if isinstance(x, (str, unicode)) :
                    result = self._applyPattern(x, pattern)
                    #print result
                    #print result
                    #print parts, i
                    if result :
                        i -= 1
                        parts.remove(x)
                        for y in result :
                            parts.insert(i+1,y)

                i += 1

        for i in range(len(parts)) :
            x = parts[i]
            if isinstance(x, (str, unicode)) :
                parts[i] = self.doc.createTextNode(x)

        return parts



    def _handleInlineWrapper (self, line) :

        # A wrapper around _handleInline to avoid recursion

        parts = [line]

        i = 0

        while i < len(parts) :
            x = parts[i]
            if isinstance(x, (str, unicode)) :
                parts.remove(x)
                result = self._handleInline(x)
                for y in result :
                    parts.insert(i,y)
            else :
                i += 1

        return parts

    def _handleInline(self,  line):
        """Transform a Markdown line with inline elements to an XHTML
        fragment.

        This function uses auxiliary objects called inline patterns.
        See notes on inline patterns above.

        @param item: A block of Markdown text
        @return: A list of NanoDom nodes """

        if not(line):
            return [self.doc.createTextNode(' ')]

        for pattern in self.inlinePatterns :
            list = self._applyPattern( line, pattern)
            if list: return list

        return [self.doc.createTextNode(line)]

    def _applyPattern(self, line, pattern) :
        """ Given a pattern name, this function checks if the line
        fits the pattern, creates the necessary elements, and returns
        back a list consisting of NanoDom elements and/or strings.

        @param line: the text to be processed
        @param pattern: the pattern to be checked

        @returns: the appropriate newly created NanoDom element if the
                  pattern matches, None otherwise.
        """

        # match the line to pattern's pre-compiled reg exp.
        # if no match, move on.

        m = pattern.getCompiledRegExp().match(line)
        if not m :
            return None

        # if we got a match let the pattern make us a NanoDom node
        # if it doesn't, move on
        node = pattern.handleMatch(m, self.doc)

        if node :
            # Those are in the reverse order!
            return ( m.groups()[-1], # the string to the left
                     node,           # the new node
                     m.group(1))     # the string to the right of the match

        else :
            return None

    def __str__(self, source = None):
        """Return the document in XHTML format.

        @returns: A serialized XHTML body."""
        #try :

        if source :
            self.source = source

        doc = self._transform()
        xml = doc.toxml()

        #finally:
        #    doc.unlink()

        # Let's stick in all the raw html pieces

        for i in range(self.htmlStash.html_counter) :
            html = self.htmlStash.rawHtmlBlocks[i]
            if self.safeMode :
                html = "[HTML_REMOVED]"

            xml = xml.replace("<p>%s\n</p>" % (HTML_PLACEHOLDER % i),
                              html + "\n")
            xml = xml.replace(HTML_PLACEHOLDER % i,
                              html)

        # And return everything but the top level tag

        if self.stripTopLevelTags :
            xml = xml.strip()[23:-7] + "\n"

        for pp in self.textPostprocessors :
            xml = pp.run(xml)

        return self.docType + xml


    toString = __str__


    def __unicode__(self):
        """Return the document in XHTML format as a Unicode object.
        """
        return str(self)#.decode(self.encoding)


    toUnicode = __unicode__




# ====================================================================

def markdownFromFile(input = None,
                     output = None,
                     extensions = [],
                     encoding = None,
                     message_threshold = CRITICAL,
                     safe = False) :

    global MESSAGE_THRESHOLD
    MESSAGE_THRESHOLD = message_threshold

    message(VERBOSE, "input file: %s" % input)


    if not encoding :
        encoding = "utf-8"

    input_file = codecs.open(input, mode="r", encoding="utf-8")
    text = input_file.read()
    input_file.close()

    new_text = markdown(text, extensions, encoding, safe_mode = safe)

    if output :
        output_file = codecs.open(output, "w", encoding=encoding)
        output_file.write(new_text)
        output_file.close()

    else :
        sys.stdout.write(new_text.encode(encoding))

def markdown(text,
             extensions = [],
             encoding = None,
             safe_mode = False) :

    message(VERBOSE, "in markdown.markdown(), received text:\n%s" % text)

    extension_names = []
    extension_configs = {}

    for ext in extensions :
        pos = ext.find("(")
        if pos == -1 :
            extension_names.append(ext)
        else :
            name = ext[:pos]
            extension_names.append(name)
            pairs = [x.split("=") for x in ext[pos+1:-1].split(",")]
            configs = [(x.strip(), y.strip()) for (x, y) in pairs]
            extension_configs[name] = configs
            #print configs

    md = Markdown(text, extensions=extension_names,
                  extension_configs=extension_configs,
                  safe_mode = safe_mode)

    return md.toString()


class Extension :

    def __init__(self, configs = {}) :
        self.config = configs

    def getConfig(self, key) :
        if self.config.has_key(key) :
            #print self.config[key][0]
            return self.config[key][0]
        else :
            return ""

    def getConfigInfo(self) :
        return [(key, self.config[key][1]) for key in self.config.keys()]

    def setConfig(self, key, value) :
        self.config[key][0] = value


OPTPARSE_WARNING = """
Python 2.3 or higher required for advanced command line options.
For lower versions of Python use:

      %s INPUT_FILE > OUTPUT_FILE

""" % EXECUTABLE_NAME_FOR_USAGE

def parse_options() :

    try :
        optparse = __import__("optparse")
    except :
        if len(sys.argv) == 2 :
            return {'input' : sys.argv[1],
                    'output' : None,
                    'message_threshold' : CRITICAL,
                    'safe' : False,
                    'extensions' : [],
                    'encoding' : None }

        else :
            print OPTPARSE_WARNING
            return None

    parser = optparse.OptionParser(usage="%prog INPUTFILE [options]")

    parser.add_option("-f", "--file", dest="filename",
                      help="write output to OUTPUT_FILE",
                      metavar="OUTPUT_FILE")
    parser.add_option("-e", "--encoding", dest="encoding",
                      help="encoding for input and output files",)
    parser.add_option("-q", "--quiet", default = CRITICAL,
                      action="store_const", const=NONE, dest="verbose",
                      help="suppress all messages")
    parser.add_option("-v", "--verbose",
                      action="store_const", const=INFO, dest="verbose",
                      help="print info messages")
    parser.add_option("-s", "--safe",
                      action="store_const", const=True, dest="safe",
                      help="same mode (strip user's HTML tag)")

    parser.add_option("--noisy",
                      action="store_const", const=VERBOSE, dest="verbose",
                      help="print debug messages")
    parser.add_option("-x", "--extension", action="append", dest="extensions",
                      help = "load extension EXTENSION", metavar="EXTENSION")

    (options, args) = parser.parse_args()

    if not len(args) == 1 :
        parser.print_help()
        return None
    else :
        input_file = args[0]

    if not options.extensions :
        options.extensions = []

    return {'input' : input_file,
            'output' : options.filename,
            'message_threshold' : options.verbose,
            'safe' : options.safe,
            'extensions' : options.extensions,
            'encoding' : options.encoding }

########NEW FILE########
__FILENAME__ = models
from django.db import models
import re

class PageManager(models.Manager):
    def create_with_auto_version(self, slug, headline, content, change_message, change_user, change_ip, minor_edit):
        """
        Creates and returns a Page object with the given attributes.
        Automatically sets version to the next available version number for
        the given slug, in a way that avoids race conditions.
        """
        from django.db import connection
        db_table = self.model._meta.db_table
        cursor = connection.cursor()
        cursor.execute("""
            INSERT INTO %s
                (slug, headline, content, version, change_date, change_message, change_user, change_ip, minor_edit)
            VALUES
                (%%s, %%s, %%s, (SELECT COALESCE(MAX(version), 0) + 1 FROM %s WHERE slug=%%s), NOW(), %%s, %%s, %%s, %%s)""" %\
            (db_table, db_table),
            (slug, headline, content, slug, change_message, change_user, change_ip, minor_edit))
        new_id = connection.ops.last_insert_id(cursor, db_table, 'id')
        connection._commit()
        return self.get(id=new_id)

    def select_all_latest(self):
        """
        Returns a QuerySet of the most recent version of each Page.
        """
        from django.db import connection
        db_table = self.model._meta.db_table
        cursor = connection.cursor()
        cursor.execute("SELECT DISTINCT ON (slug) ID FROM %s ORDER BY slug, version DESC" % db_table)
        return self.filter(id__in=cursor.fetchall())

    def find_orphans(self):
        """
        Returns a list of Pages which aren't linked to by any other Page.
        """
        link_re = re.compile(r'''(?x)
            (?<=\]\() # A link starts with an open paren immediately after a close square bracket
            [^)]+     # Match everything up to the close paren
            (?=\))    # Sanity check: look ahead for the close paren
        ''')
        pages = self.select_all_latest()
        orphans = dict([(p.slug, p) for p in pages])
        for page in pages:
            for slug in link_re.findall(page.content):
                if not slug.startswith("http://"):
                    try:
                        del orphans[slug]
                    except KeyError:
                        pass
        return orphans.values()

class Page(models.Model):
    slug = models.CharField(max_length=30)
    headline = models.CharField(max_length=80)
    content = models.TextField()
    version = models.PositiveIntegerField()
    change_date = models.DateTimeField()
    change_message = models.CharField(max_length=100)
    change_user = models.CharField(max_length=64)
    change_ip = models.IPAddressField()
    minor_edit = models.BooleanField()
    objects = PageManager()

    class Meta:
        unique_together = (('slug', 'version'),)

    def __unicode__(self):
        return self.slug

    def url(self):
        return '/%s/' % self.slug

    def edit_url(self):
        return '/%s/edit/' % self.slug

    def history_url(self):
        return '/%s/history/' % self.slug

    def version_url(self):
        return '/%s/history/%s/' % (self.slug, self.version)

    def diff_url(self):
        return '/%s/history/%s/diff/' % (self.slug, self.version)

########NEW FILE########
__FILENAME__ = wiki
from django import template
from ebwiki.wiki.markdown import markdown as markdown_func

register = template.Library()

def markdown(value):
    return markdown_func(value)
register.filter('markdown', markdown)

########NEW FILE########
__FILENAME__ = urls
from django.conf import settings
from django.conf.urls.defaults import *
from django.contrib.syndication.views import feed
from ebwiki.wiki.feeds import LatestEdits
import views # relative import

feeds = {
    'latest': LatestEdits
}

if settings.DEBUG:
    urlpatterns = patterns('',
        (r'^(?P<path>styles.*)$', 'django.views.static.serve', {'document_root': settings.WIKI_DOC_ROOT}),
    )
else:
    urlpatterns = patterns('')

urlpatterns += patterns('',
    (r'^$', views.view_page, {'slug': 'index'}),
    (r'^r/$', views.redirecter),
    (r'^latest-changes/$', views.latest_changes),
    (r'^orphans/$', views.list_orphans),

    (r'^(\w{1,30})/$', views.view_page),
    (r'^(\w{1,30})/edit/$', views.edit_page),
    (r'^(\w{1,30})/history/$', views.history),
    (r'^(\w{1,30})/history/(\d{1,6})/$', views.view_version),
    (r'^(\w{1,30})/history/(\d{1,6})/diff/$', views.previous_version_diff),
    (r'^feeds/(?P<url>.*)/$', feed, {'feed_dict': feeds}),
)

########NEW FILE########
__FILENAME__ = utils
import re

def wikify(text):
    text = re.sub(r'\[(https?://.*?)\s+(.*?)\]', r'<a href="\1">\2</a>', text)
    text = re.sub(r'\[(\w{1,30})\s+(.*?)\]', r'<a href="/\1/">\2</a>', text)
    text = re.sub(r'\r?\n', '<br />', text)
    text = re.sub(r'(?m)^h(\d)\. (.*?)$', r'<h\1>\2</h\1>', text)
    return text

########NEW FILE########
__FILENAME__ = views
from django import forms
from django.core.paginator import Paginator, EmptyPage
from django.http import HttpResponse, HttpResponseRedirect, Http404
from django.shortcuts import get_object_or_404, render_to_response
from ebwiki.wiki.models import Page
from difflib import unified_diff
import urllib

easy_diff = lambda x, y: '\n'.join(unified_diff(x.split('\n'), y.split('\n'), 'Old page', 'New page', lineterm=""))

class PageForm(forms.Form):
    headline = forms.CharField(max_length=80, widget=forms.TextInput(attrs={'size': 80}))
    content = forms.CharField(widget=forms.Textarea(attrs={'rows': 20, 'cols': 70}))
    change_message = forms.CharField(max_length=100, required=False, widget=forms.TextInput(attrs={'size': 100}))
    minor_edit = forms.BooleanField(widget=forms.CheckboxInput, required=False)
    version = forms.IntegerField(widget=forms.HiddenInput)

def redirecter(request):
    "Redirects to a given URL without sending the 'Referer' header."
    try:
        url = request.GET['url']
    except KeyError:
        raise Http404
    if not url.startswith('http://') and not url.startswith('https://'):
        raise Http404
    return HttpResponse('<html><head><meta http-equiv="Refresh" content="0; URL=%s"></head><body>Redirecting...</body></html>' % urllib.unquote_plus(url))

def view_page(request, slug):
    try:
        page = Page.objects.order_by('-version').filter(slug=slug)[0]
    except IndexError:
        page = Page(slug=slug) # Temporarily construct Page object so we can call edit_url()
        return HttpResponseRedirect(page.edit_url())
    return render_to_response('wiki/view_page.html', {'page': page})

def view_version(request, slug, version):
    page = get_object_or_404(Page, slug=slug, version=version)
    return render_to_response('wiki/view_version.html', {'page': page})

def edit_page(request, slug):
    try:
        latest_page = Page.objects.order_by('-version').filter(slug=slug)[0]
    except IndexError:
        latest_page = None
    if request.method == 'POST':
        form = PageForm(request.POST)
        if form.is_valid():
            cd = form.cleaned_data
            if latest_page:
                if latest_page.headline == cd['headline'] and latest_page.content == cd['content']:
                    # If no changes were made, don't touch the database.
                    return HttpResponseRedirect(latest_page.url())
                if latest_page.version != int(cd['version']):
                    diff = easy_diff(Page.objects.get(slug=slug, version=cd['version']).content, cd['content'])
                    return render_to_response('wiki/edit_conflict.html', {'diff': diff, 'latest_page': latest_page})
            new_page = Page.objects.create_with_auto_version(slug, cd['headline'], cd['content'],
                cd['change_message'], request.META.get('REMOTE_USER', 'anonymous'), request.META['REMOTE_ADDR'],
                cd['minor_edit'])
            return HttpResponseRedirect(new_page.url())
    else:
        if latest_page is not None:
            form = PageForm({'headline': latest_page.headline, 'content': latest_page.content, 'version': latest_page.version})
        else:
            form = PageForm(initial={'change_message': 'Created page', 'version': 0})
    return render_to_response('wiki/edit_page.html', {'old_page': latest_page, 'slug': slug, 'form': form})

def history(request, slug):
    page_list = Page.objects.filter(slug=slug).order_by('-version')
    if not page_list:
        raise Http404("A history doesn't exist for the given slug.")
    return render_to_response('wiki/history.html', {'page_list': page_list, 'slug': slug})

def version_diff(request, slug, version1, version2):
    if int(version1) == 0:
        old_content = ''
    else:
        old_content = get_object_or_404(Page, slug=slug, version=version1).content
    page = get_object_or_404(Page, slug=slug, version=version2)
    diff = easy_diff(old_content, page.content)
    return render_to_response('wiki/version_diff.html', {'diff': diff, 'slug': slug, 'version1': version1, 'version2': version2, 'page': page})

def previous_version_diff(request, slug, version):
    return version_diff(request, slug, int(version)-1, version)

def latest_changes(request):
    p = Paginator(Page.objects.order_by('-change_date'), 30)
    try:
        page_num = int(request.GET.get('p', '1'))
    except ValueError:
        page_num = 1
    try:
        page = p.page(page_num)
    except EmptyPage:
        raise Http404('Invalid page')
    return render_to_response('wiki/latest_changes.html', {'result_page': page})

def list_orphans(request):
    orphans = Page.objects.find_orphans()
    return render_to_response('wiki/list_orphans.html', {'orphans': orphans})

########NEW FILE########
__FILENAME__ = urls
from django.conf.urls.defaults import *
from django.conf import settings
from django.views.generic.simple import direct_to_template
from everyblock.utils.redirecter import redirecter
import views # relative import

urlpatterns = patterns('',
    (r'^$', direct_to_template, {'template': 'admin/index.html'}),
    (r'^schemas/$', views.schema_list),
    (r'^schemas/(\d{1,6})/$', views.edit_schema),
    (r'^schemas/(\d{1,6})/lookups/(\d{1,6})/$', views.edit_schema_lookups),
    (r'^schemafields/$', views.schemafield_list),
    (r'^sources/$', views.blob_seed_list),
    (r'^sources/add/$', views.add_blob_seed),
    (r'^scraper-history/$', views.scraper_history_list),
    (r'^scraper-history/([-\w]{4,32})/$', views.scraper_history_schema),
    (r'^set-staff-cookie/$', views.set_staff_cookie),
    (r'^newsitems/(\d{1,6})/$', views.newsitem_details),
    (r'^geocoder-success-rates/$', views.geocoder_success_rates),
)

########NEW FILE########
__FILENAME__ = views
# -*- coding: utf-8 -*-
from ebdata.blobs.create_seeds import create_rss_seed
from ebdata.blobs.models import Seed
from ebpub.db.models import Schema, SchemaInfo, SchemaField, NewsItem, Attribute, Lookup, DataUpdate, LocationType, Location, AggregateLocationDay
from django import forms
from django.conf import settings
from django.http import HttpResponseRedirect, Http404, HttpResponse
from django.shortcuts import get_object_or_404, render_to_response

FREQUENCY_CHOICES = ('Hourly', 'Throughout the day', 'Daily', 'Twice a week', 'Weekly', 'Twice a month', 'Monthly', 'Quarterly', 'Sporadically', 'No longer updated')
FREQUENCY_CHOICES = [(a, a) for a in FREQUENCY_CHOICES]

class SchemaInfoForm(forms.Form):
    number_in_overview = forms.IntegerField(required=True, min_value=1, max_value=100)
    short_description = forms.CharField(widget=forms.Textarea(attrs={'rows': 2, 'cols': 80}))
    short_source = forms.CharField(max_length=128, widget=forms.TextInput(attrs={'size': 80}))
    update_frequency = forms.ChoiceField(choices=FREQUENCY_CHOICES, widget=forms.RadioSelect)
    intro = forms.CharField(required=False, widget=forms.Textarea(attrs={'rows': 10, 'cols': 80}))
    summary = forms.CharField(widget=forms.Textarea(attrs={'rows': 20, 'cols': 80}))
    source = forms.CharField(required=False, widget=forms.Textarea(attrs={'rows': 20, 'cols': 80}))
    grab_bag_headline = forms.CharField(required=False, max_length=128, widget=forms.TextInput(attrs={'size': 80}))
    grab_bag = forms.CharField(required=False, widget=forms.Textarea(attrs={'rows': 20, 'cols': 80}))

class SchemaLookupsForm(forms.Form):
    def __init__(self, lookup_ids, *args, **kwargs):
        super(SchemaLookupsForm, self).__init__(*args, **kwargs)
        for look_id in lookup_ids:
            self.fields['%s-name' % look_id] = forms.CharField(widget=forms.TextInput(attrs={'size': 50}))
            self.fields['%s-name' % look_id].lookup_obj = Lookup.objects.get(id=look_id)
            self.fields['%s-description' % look_id] = forms.CharField(required=False, widget=forms.Textarea())

class BlobSeedForm(forms.Form):
    rss_url = forms.CharField(max_length=512, widget=forms.TextInput(attrs={'size': 80}))
    site_url = forms.CharField(max_length=512, widget=forms.TextInput(attrs={'size': 80}))
    rss_full_entry = forms.BooleanField(required=False)
    pretty_name = forms.CharField(max_length=128, widget=forms.TextInput(attrs={'size': 80}))
    guess_article_text = forms.BooleanField(required=False)
    strip_noise = forms.BooleanField(required=False)

# Returns the username for a given request, taking into account our proxy
# (which sets HTTP_X_REMOTE_USER).
request_username = lambda request: request.META.get('REMOTE_USER', '') or request.META.get('HTTP_X_REMOTE_USER', '')

user_is_staff = lambda username: settings.DEBUG

def schema_list(request):
    s_list = []
    for s in Schema.objects.order_by('name'):
        s_list.append({
            'schema': s,
            'lookups': s.schemafield_set.filter(is_lookup=True).order_by('pretty_name_plural'),
        })
    return render_to_response('admin/schema_list.html', {'schema_list': s_list})

def set_staff_cookie(request):
    r = HttpResponseRedirect('../')
    r.set_cookie(settings.STAFF_COOKIE_NAME, settings.STAFF_COOKIE_VALUE)
    return r

def edit_schema(request, schema_id):
    s = get_object_or_404(Schema, id=schema_id)
    try:
        si = SchemaInfo.objects.get(schema__id=s.id)
    except SchemaInfo.DoesNotExist:
        si = SchemaInfo.objects.create(schema=s, short_description='', summary='',
            source='', grab_bag_headline='', grab_bag='', short_source='',
            update_frequency='', intro='')
    if request.method == 'POST':
        form = SchemaInfoForm(request.POST)
        if form.is_valid():
            for field in ('short_description', 'summary', 'source', 'grab_bag_headline', 'grab_bag', 'short_source', 'update_frequency', 'intro'):
                setattr(si, field, form.cleaned_data[field])
            si.save()
            Schema.objects.filter(id=s.id).update(number_in_overview=form.cleaned_data['number_in_overview'])
            return HttpResponseRedirect('../')
    else:
        form = SchemaInfoForm(initial={
            'number_in_overview': s.number_in_overview,
            'short_description': si.short_description,
            'summary': si.summary,
            'source': si.source,
            'grab_bag_headline': si.grab_bag_headline,
            'grab_bag': si.grab_bag,
            'short_source': si.short_source,
            'update_frequency': si.update_frequency,
            'intro': si.intro,
        })
    return render_to_response('admin/edit_schema.html', {'schema': s, 'form': form})

def edit_schema_lookups(request, schema_id, schema_field_id):
    s = get_object_or_404(Schema, id=schema_id)
    sf = get_object_or_404(SchemaField, id=schema_field_id, schema__id=s.id, is_lookup=True)
    lookups = Lookup.objects.filter(schema_field__id=sf.id).order_by('name')
    lookup_ids = [look.id for look in lookups]
    if request.method == 'POST':
        form = SchemaLookupsForm(lookup_ids, request.POST)
        if form.is_valid():
            # Save any lookup values that changed.
            for look in lookups:
                name = request.POST.get('%s-name' % look.id)
                description = request.POST.get('%s-description' % look.id)
                if name is not None and description is not None and (name != look.name or description != look.description):
                    look.name = name
                    look.description = description
                    look.save()
            return HttpResponseRedirect('../../../')
    else:
        initial = {}
        for look in lookups:
            initial['%s-name' % look.id] = look.name
            initial['%s-description' % look.id] = look.description
        form = SchemaLookupsForm(lookup_ids, initial=initial)
    return render_to_response('admin/edit_schema_lookups.html', {'schema': s, 'schema_field': sf, 'form': list(form)})

def schemafield_list(request):
    sf_list = SchemaField.objects.select_related().order_by('db_schema.name', 'display_order')
    return render_to_response('admin/schemafield_list.html', {'schemafield_list': sf_list})

def geocoder_success_rates(request):
    from django.db import connection
    sql = """
        select s.plural_name, count(ni.location) as geocoded, count(*) as total
        from db_newsitem ni
        inner join db_schema s
        on s.id=ni.schema_id
        group by s.plural_name
        order by count(ni.location)::float / count(*)::float;
    """
    cursor = connection.cursor()
    cursor.execute(sql)
    results = cursor.fetchall()
    schema_list = [{'name': r[0], 'geocoded': r[1], 'total': r[2], 'ratio': float(r[1]) / float(r[2])} for r in results]
    return render_to_response('admin/geocoder_success_rates.html', {'schema_list': schema_list})

def blob_seed_list(request):
    s_list = Seed.objects.order_by('autodetect_locations', 'pretty_name').filter(is_rss_feed=True)
    return render_to_response('admin/blob_seed_list.html', {'seed_list': s_list})

def add_blob_seed(request):
    if request.method == 'POST':
        form = BlobSeedForm(request.POST)
        if form.is_valid():
            cd = form.cleaned_data
            create_rss_seed(cd['rss_url'], cd['site_url'], cd['rss_full_entry'], cd['pretty_name'], cd['guess_article_text'], cd['strip_noise'])
            return HttpResponseRedirect('../')
    else:
        form = BlobSeedForm()
    return render_to_response('admin/add_blob_seed.html', {'form': form})

def scraper_history_list(request):
    schema_ids = [i['schema'] for i in DataUpdate.objects.select_related().order_by('schema__plural_name').distinct().values('schema')]
    s_dict = Schema.objects.in_bulk(schema_ids)
    s_list = [s_dict[i] for i in schema_ids]
    return render_to_response('admin/scraper_history_list.html', {'schema_list': s_list})

def scraper_history_schema(request, slug):
    s = get_object_or_404(Schema, slug=slug)
    du_list = DataUpdate.objects.filter(schema__id=s.id).order_by('schema__name', '-update_start')
    return render_to_response('admin/scraper_history_schema.html', {'schema': s, 'dataupdate_list': du_list})

def newsitem_details(request, news_item_id):
    """
    Shows all of the raw values in a NewsItem for debugging.
    """
    ni = get_object_or_404(NewsItem, pk=news_item_id)
    real_names = [
        'varchar01', 'varchar02', 'varchar03', 'varchar04', 'varchar05',
        'date01', 'date02', 'date03', 'date04', 'date05',
        'time01', 'time02',
        'datetime01', 'datetime02', 'datetime03', 'datetime04',
        'bool01', 'bool02', 'bool03', 'bool04', 'bool05',
        'int01', 'int02', 'int03', 'int04', 'int05', 'int06', 'int07',
        'text01'
    ]
    schema_fields = {}
    for sf in SchemaField.objects.filter(schema=ni.schema):
        schema_fields[sf.real_name] = sf
    attributes = []
    for real_name in real_names:
        schema_field = schema_fields.get(real_name, None)
        attributes.append({
            'real_name': real_name,
            'name': schema_field and schema_field.name or None,
            'raw_value': schema_field and ni.attributes[schema_field.name] or None,
            'schema_field': schema_field,
        })
    return render_to_response('admin/news_item_detail.html', {
        'news_item': ni, 'attributes': attributes
    })

########NEW FILE########
__FILENAME__ = retrieval
"""
Screen scraper for Atlanta building permits.
http://atlantaga.govhost.com/government/planning_onlinepermits.aspx?section=City%20Services
"""

from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date
import datetime
import re

class PermitScraper(NewsItemListDetailScraper):
    schema_slugs = ('building-permits',)
    has_detail = False
    parse_list_re = re.compile(r'(?m)Permit No:\s+(?P<permit_number>.*?)\s+NPU:\s+(?P<npu>.*?)\s+Issued:\s+(?P<issue_date>\d{2}/\d{2}/\d{4})\s+Address:\s+(?P<address>.*?)\s+Scope:\s+(?P<scope>.*?)\s+Inspector:\s+(?P<inspector>.*?)\s+Cost:\s+(?P<cost>.*?)\s+Contractor:\s+(?P<contractor>.*?)\s+Owner:\s+(?P<owner>.*?)\s+')

    def __init__(self, years=None):
        super(PermitScraper, self).__init__()
        # Scrape the years given, or use the year from yesterday. This will
        # allow a final scrape of the previous year on Jan 1.
        self.years = years or [(datetime.date.today() - datetime.timedelta(days=1)).year]

    def list_pages(self):
        uri = 'http://apps.atlantaga.gov/citydir/dpcd/dpcd%%20web/buildings/%s%s.txt'
        for year in self.years:
            for letter_range in ['ag', 'hm', 'ns', 'tz']:
                for record in self.get_html(uri % (letter_range, year)).split('Premit No:'):
                    yield record

    def clean_list_record(self, record):
        record['issue_date'] = parse_date(record['issue_date'], '%m/%d/%Y')
        return record

    def existing_record(self, record):
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id)
            qs = qs.by_attribute(self.schema_fields['permit_number'], record['permit_number'])
            return qs[0]
        except IndexError:
            return None

    def save(self, old_record, list_record, detail_record):
        if old_record is not None:
            self.logger.debug('Record already exists')
            return

        scope = self.get_or_create_lookup('scope', list_record['scope'], list_record['scope'], make_text_slug=False)
        npu = self.get_or_create_lookup('npu', list_record['npu'], list_record['npu'], make_text_slug=False)

        attributes = {
            'permit_number': list_record['permit_number'],
            'scope': scope.id,
            'npu': npu.id,
            'cost': list_record['cost'],
            'owner': list_record['owner'],
            'contractor': list_record['contractor'],
        }
        self.create_newsitem(
            attributes,
            title='Building permit issued at %s' % list_record['address'],
            item_date=list_record['issue_date'],
            location_name=list_record['address'],
        )

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    PermitScraper().update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Screen scraper for Atlanta crime.
http://www.atlantapd.org/index.asp?nav=CrimeMapping
"""

from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date
from django.contrib.gis.geos import Point
from cStringIO import StringIO
import datetime
import zipfile
import sys
import csv
import os

COLUMN_NAMES = [
    'address',
    'incident_number',
    'ucr_number',
    'offense_description',
    'zon',
    'beat',
    'location',
    'report_date',
    'date_from',
    'day_from',
    'time_from',
    'date_to',
    'day_to',
    'time_to',
    'shift',
    'st_number',
    'street_name',
    'type',
    'quad',
    'apt',
    'intersection',
    'weapon',
    'disp',
    'number_vics',
    'x',
    'y',
    'neighborhood',
    'npu',
    'mi_sql_rec_num',
    'mi_sql_x',
    'mi_sql_y',
]

class CrimeScraper(NewsItemListDetailScraper):
    schema_slugs = ('crime',)
    has_detail = False

    def __init__(self, filename=None):
        super(CrimeScraper, self).__init__()
        self.filename = filename

    def list_pages(self):
        year = (datetime.date.today() - datetime.timedelta(days=1)).year
        uri = 'http://www.atlantapd.org/files/CrimeData/PI-%s.zip' % year
        if self.filename is None:
            filename = self.retriever.get_to_file(uri)
            text_filname = 'PI-%s.txt' % year
        else:
            filename = self.filename
            text_filname = self.filename.split('/')[-1][:-4] + '.txt'
        fh = open(filename, 'r')
        zf = zipfile.ZipFile(fh, 'r')
        reader = csv.DictReader(StringIO(zf.read(text_filname)), fieldnames=COLUMN_NAMES)
        yield reader
        fh.close()
        if self.filename is None:
            os.unlink(filename) # Clean up the temporary file.

    def parse_list(self, reader):
        for row in reader:
            yield row

    def clean_list_record(self, record):
        record['address'] = record['address'].strip().replace('&&', 'and')
        record['incident_number'] = record['incident_number'].strip()
        record['offense'] = record['offense_description'].strip()
        record['beat'] = record['beat'].strip()
        record['report_date'] = parse_date(record['report_date'], '%Y-%m-%d %H:%M:%S.000')
        record['x'] = record['x'].strip()
        record['y'] = record['y'].strip()
        return record

    def existing_record(self, record):
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id)
            qs = qs.by_attribute(self.schema_fields['incident_number'], record['incident_number'])
            return qs[0]
        except IndexError:
            return None

    def save(self, old_record, list_record, detail_record):
        if list_record['x'] and list_record['y']:
            crime_location = Point(float(list_record['x']), float(list_record['y']))
            crime_location = self.safe_location(list_record['address'], crime_location, 375)
        else:
            crime_location = None

        offense = self.get_or_create_lookup('offense', list_record['offense'], list_record['offense'])
        beat = self.get_or_create_lookup('beat', list_record['beat'], list_record['beat'])

        kwargs = {
            'title': offense.name,
            'item_date': list_record['report_date'],
            'location_name': list_record['address'],
            'location': crime_location
        }
        attributes = {
            'incident_number': list_record['incident_number'],
            'offense': offense.id,
            'beat': beat.id,
            'xy': '%s;%s' % (list_record['x'], list_record['y'])
        }
        if old_record is None:
            self.create_newsitem(attributes, **kwargs)
        else:
            self.update_existing(old_record, kwargs, attributes)

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    try:
        filename = sys.argv[1]
        CrimeScraper(filename=filename).update()
    except IndexError:
        CrimeScraper().update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Screen scraper for Boston building permits.
http://www.cityofboston.gov/isd/building/asofright/default.asp
"""

from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date
from ebpub.utils.text import smart_title
import re
import urllib

# These are the neighborhoods that can be searched-by at
# http://www.cityofboston.gov/isd/building/asofright/default.asp
NEIGHBORHOODS = (
    'Allston',
    'Back Bay',
    'Beacon Hill',
    'Brighton',
    'Charlestown',
    'Chinatown',
    'Dorchester',
    'Dorchester (Lower Mills)',
    'Dorchester (Meeting House Hill)',
    'Dorchester (Neponset, Cedar Grove)',
    'Dorchester (Savin Hill)',
    'East Boston',
    'Fenway',
    'Financial District',
    'Hyde Park',
    'Jamaica Plain',
    'Mattapan',
    'Mission Hill',
    'North Dorchester',
    'NorthEnd',
    'Roslindale',
    'Roxbury',
    'South Boston',
    'South End',
    'West End',
    'West Roxbury',
)

class PermitScraper(NewsItemListDetailScraper):
    schema_slugs = ('building-permits',)
    has_detail = False
    parse_list_re = re.compile(r'<tr[^>]*><td[^>]*>(?P<permit_date>\d\d?/\d\d/\d{4})\s*</td><td[^>]*>(?P<address>[^<]*)<br/>(?P<neighborhood>[^<]*)</td><td[^>]*>(?P<owner>[^<]*)</td><td[^>]*>(?P<description>[^<]*)</td></tr>', re.IGNORECASE | re.DOTALL)

    def list_pages(self):
        for name in NEIGHBORHOODS:
            url = 'http://www.cityofboston.gov/isd/building/asofright/default.asp?ispostback=true&nhood=%s' % urllib.quote_plus(name)
            yield self.get_html(url)

    def clean_list_record(self, record):
        record['permit_date'] = parse_date(record['permit_date'], '%m/%d/%Y')
        record['description'] = re.sub(r'[\r\n]+', ' ', record['description']).strip()
        record['description'] = record['description'].decode('iso-8859-1') # Avoid database-level encoding errors
        record['clean_address'] = smart_title(record['address'])
        return record

    def existing_record(self, record):
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id, item_date=record['permit_date'])
            qs = qs.by_attribute(self.schema_fields['raw_address'], record['address'])
            return qs[0]
        except IndexError:
            return None

    def save(self, old_record, list_record, detail_record):
        if old_record is not None:
            self.logger.debug('Record already exists')
            return

        attributes = {
            'raw_address': list_record['address'],
            'description': list_record['description'],
            'owner': list_record['owner'],
        }
        self.create_newsitem(
            attributes,
            title='Building permit issued at %s' % list_record['clean_address'],
            description=list_record['description'],
            item_date=list_record['permit_date'],
            location_name=list_record['clean_address'],
        )

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    PermitScraper().update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Scraper for Boston building permits.

http://www.cityofboston.gov/cityclerk/search_reply.asp
"""

from ebdata.retrieval.scrapers.list_detail import SkipRecord
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date
from ebpub.utils.text import smart_title
from urllib import urlencode
import datetime
import re

# An opt-out list of businesses to ignore for privacy reasons.
BUSINESS_NAMES_TO_IGNORE = set([
    ('THINK COOL COSMETICS', '176 WASHINGTON ST'),
])

class Scraper(NewsItemListDetailScraper):
    schema_slugs = ['business-licenses']
    has_detail = False
    parse_list_re = re.compile(r'(?s)<div class="mainColTextBlueBold">(?P<name>.*?)</div><br>\s+?<b>Date:</b>(?P<date>.*?)<br>\s+?<b>Type:</b>(?P<business_type>.*?)<br>\s+?<b>Business Address:</b>(?P<location>.*?)<br>\s+?<b>File #:</b>(?P<file_number>.*?)<br>')
    sleep = 1
    uri = 'http://www.cityofboston.gov/cityclerk/search_reply.asp'

    def __init__(self, *args, **kwargs):
        self.start_date = kwargs.pop('start_date', None)
        super(Scraper, self).__init__(*args, **kwargs)

    def find_next_page_url(self, html, current_page_number):
        pattern = r"<a href='(.*?)'>%s</a>" % (current_page_number + 1)
        print pattern
        m = re.search(pattern, html)
        if m is None:
            return None
        return "http://www.cityofboston.gov%s" % m.group(1)

    def list_pages(self):
        if not self.start_date:
            date = datetime.date.today() - datetime.timedelta(days=7)
        else:
            date = self.start_date
        while date <= datetime.date.today():
            page_number = 1
            while 1:
                params = {
                    'whichpage': str(page_number),
                    'pagesize': '10',
                    'name_fold': '',
                    'name_doc': date.strftime('%Y-%m-%d'),
                    'index1': '',
                    'index2': '',
                    'index3': '',
                    'index4': '',
                    'index6': '',
                    'tempday': date.strftime('%d'),
                    'tempmonth': date.strftime('%m'),
                    'tempyear': date.strftime('%Y'),
                }
                html = self.get_html(self.uri + '?' + urlencode(params))
                try:
                    max_pages = int(re.search(r'Page \d+ of (\d+)', html).group(1))
                except AttributeError:
                    break
                yield html
                page_number += 1
                if page_number > max_pages:
                    break
            date = date + datetime.timedelta(days=1)

    def clean_list_record(self, record):
        record['name'] = record['name'].strip()
        record['business_type'] = record['business_type'].strip()
        record['location'] = smart_title(record['location'].strip())
        record['date'] = parse_date(record['date'].strip(), '%Y-%m-%d')
        if (record['name'].upper(), record['location'].upper()) in BUSINESS_NAMES_TO_IGNORE:
            raise SkipRecord('Skipping %s' % record['name'])
        return record

    def existing_record(self, list_record):
        qs = NewsItem.objects.filter(schema__id=self.schema.id, item_date=list_record['date'])
        qs = qs.by_attribute(self.schema_fields['name'], list_record['name'])
        try:
            return qs[0]
        except IndexError:
            return None

    def save(self, old_record, list_record, detail_record):
        if old_record is not None:
            return
        if list_record['name'].upper() in ['NONE', '']:
            return
        business_type_lookup = self.get_or_create_lookup('business_type', list_record['business_type'], list_record['business_type'], make_text_slug=False)
        attributes = {
            'name': list_record['name'],
            'file_number': list_record['file_number'],
            'business_type': business_type_lookup.id
        }
        self.create_newsitem(
            attributes,
            title=list_record['name'],
            item_date=list_record['date'],
            location_name=list_record['location']
        )

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    start_date = datetime.date(2003, 1, 2)
    Scraper(start_date=start_date).update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Boston city press release scraper.

http://www.cityofboston.gov/news/
Example: http://www.cityofboston.gov/news/default.aspx?id=3910
"""

from ebdata.blobs.scrapers import IncrementalCrawler
import re

class BostonCityPressReleaseCrawler(IncrementalCrawler):
    schema = 'city-press-releases'
    seed_url = 'http://www.cityofboston.gov/news/'
    date_headline_re = re.compile(r'(?si)<span id="lblTitle">(?P<article_headline>[^>]*)</span>.*?<span id="lblDate">(?P<article_date>\d\d?/\d\d?/\d\d\d\d)</span>')
    date_format = '%m/%d/%Y'
    max_blanks = 8

    def public_url(self, id_value):
        return 'http://www.cityofboston.gov/news/default.aspx?id=%s' % id_value

    def id_for_url(self, url):
        return url.split('id=')[1]

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    BostonCityPressReleaseCrawler().update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Screen scraper for Boston restaurant inspections.

http://www.cityofboston.gov/isd/health/mfc/search.asp
"""

from django.core.serializers.json import DjangoJSONEncoder
from ebdata.retrieval.scrapers.base import ScraperBroken
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date
from ebpub.utils.text import smart_title
import re

parse_main_re = re.compile(r"<tr[^>]*><td[^>]*><a href='insphistory\.asp\?licno=(?P<restaurant_id>\d+)'>(?P<restaurant_name>[^<]*)</a></td><td[^>]*>(?P<address>[^<]*)</td><td[^>]*>(?P<neighborhood>[^<]*)</td></tr>")
detail_violations_re = re.compile(r"<tr[^>]*><td[^>]*><span[^>]*>(?P<stars>\*+)</span></td><td[^>]*><span[^>]*>(?P<status>[^<]*)</span></td><td[^>]*><span[^>]*>(?P<code>[^<]*)</span></td><td[^>]*><span[^>]*>(?P<description>[^<]*)</span></td><td[^>]*>(?P<location>.*?)</td><td[^>]*>(?P<comment>.*?)</td></tr>", re.DOTALL)
detail_url = lambda inspection_id: 'http://www.cityofboston.gov/isd/health/mfc/viewinsp.asp?inspno=%s' % inspection_id

strip_tags = lambda x: re.sub(r'(?s)</?[^>]*>', '', x).replace('&nbsp;', ' ').strip()

class RestaurantScraper(NewsItemListDetailScraper):
    schema_slugs = ('restaurant-inspections',)
    parse_list_re = re.compile(r"<a href='viewinsp\.asp\?inspno=(?P<inspection_id>\d+)'>(?P<inspection_date>[^<]*)</a></span> - <span[^>]*>(?P<result>[^<]*)</span>")
    parse_detail_re = re.compile(r"<tr><th[^>]*>[^<]*</th><th[^>]*>Status</th><th[^>]*>Code Violation</th><th[^>]*>Description</th><th[^>]*>Location</th><th[^>]*>Comment</th></tr>(?P<body>.*?)</table>", re.DOTALL)
    sleep = 5

    def __init__(self, name_start=''):
        # name_start, if given, should be a string of the first restaurant name
        # to start scraping, alphabetically. This is useful if you've run the
        # scraper and it's broken several hours into it -- you can pick up
        # around where it left off.
        NewsItemListDetailScraper.__init__(self)
        self.name_start = name_start.lower()

    def list_pages(self):
        # Submit the search form with ' ' as the neighborhood to get *every*
        # restaurant in the city.
        #
        # Note that this site is technically *three* levels deep -- there's a
        # main list of all restaurants, then a list of inspections for each
        # restaurant, then a page for each inspection. Because this is slightly
        # different than a strict list-detail site, list_pages() yields the
        # inspection pages, not the main page.
        url = 'http://www.cityofboston.gov/isd/health/mfc/search.asp'
        html = self.get_html(url, {'ispostback': 'true', 'restname': '', 'cboNhood': ' '}).decode('ISO-8859-2')
        for record in parse_main_re.finditer(html):
            record = record.groupdict()
            if self.name_start and record['restaurant_name'].lower() < self.name_start:
                self.logger.debug('Skipping %r due to name_start %r', record['restaurant_name'], self.name_start)
                continue
            url = 'http://www.cityofboston.gov/isd/health/mfc/insphistory.asp?licno=%s' % record['restaurant_id']
            yield (record, self.get_html(url))

    def parse_list(self, record_html):
        list_record, html = record_html
        for record in NewsItemListDetailScraper.parse_list(self, html):
            yield dict(list_record, **record)

    def clean_list_record(self, record):
        record['inspection_date'] = parse_date(record['inspection_date'], '%A, %B %d, %Y')
        record['address'] = smart_title(record['address'])
        record['restaurant_name'] = smart_title(record['restaurant_name'])
        record['result'] = smart_title(record['result'])
        return record

    def existing_record(self, record):
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id)
            qs = qs.by_attribute(self.schema_fields['inspection_id'], record['inspection_id'])
            return qs[0]
        except IndexError:
            return None

    def detail_required(self, list_record, old_record):
        return old_record is None

    def get_detail(self, record):
        return self.get_html(detail_url(record['inspection_id'])).decode('ISO-8859-2')

    def clean_detail_record(self, record):
        body = record.pop('body')
        violations = [m.groupdict() for m in detail_violations_re.finditer(body)]
        if not violations and not 'There are no violations for this inspection' in body:
            raise ScraperBroken('Could not find violations')
        for vio in violations:
            vio['severity'] = {1: 'Non critical', 2: 'Critical', 3: 'Critical foodborne illness'}[vio.pop('stars').count('*')]
            vio['comment'] = strip_tags(vio['comment']).strip()
            vio['location'] = strip_tags(vio['location']).strip()
        record['violation_list'] = violations
        return record

    def save(self, old_record, list_record, detail_record):
        if old_record is not None:
            return # We already have this inspection.

        result = self.get_or_create_lookup('result', list_record['result'], list_record['result'])
        violation_lookups = [self.get_or_create_lookup('violation', v['description'], v['code'], make_text_slug=False) for v in detail_record['violation_list']]
        violation_lookup_text = ','.join([str(v.id) for v in violation_lookups])

        # There's a bunch of data about every particular violation, and we
        # store it as a JSON object. Here, we create the JSON object.
        v_lookup_dict = dict([(v.code, v) for v in violation_lookups])
        v_list = [{'lookup_id': v_lookup_dict[v['code']].id, 'comment': v['comment'], 'location': v['location'], 'severity': v['severity'], 'status': v['status']} for v in detail_record['violation_list']]
        violations_json = DjangoJSONEncoder().encode(v_list)

        title = '%s inspected: %s' % (list_record['restaurant_name'], result.name)
        attributes = {
            'restaurant_id': list_record['restaurant_id'],
            'inspection_id': list_record['inspection_id'],
            'restaurant_name': list_record['restaurant_name'],
            'result': result.id,
            'violation': violation_lookup_text,
            'details': violations_json,
        }
        self.create_newsitem(
            attributes,
            title=title,
            url=detail_url(list_record['inspection_id']),
            item_date=list_record['inspection_date'],
            location_name=list_record['address'],
        )

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    RestaurantScraper().update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Screen scraper for Charlotte building permits.
http://dwexternal.co.mecklenburg.nc.us/ids/RptGrid01.aspx?rpt=Daily_Building_Permits_Issued
"""

from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date
from ebpub.utils.text import smart_title
from everyblock.cities.charlotte.utils import MecklenburgScraper
import datetime

class BuildingPermitScraper(MecklenburgScraper):
    schema_slugs = ('building-permits',)
    has_detail = False
    root_uri = 'http://dwexternal.co.mecklenburg.nc.us/ids/RptGrid01.aspx?rpt=Daily_Building_Permits_Issued'

    def search_arguments(self, start, end, viewstate):
        return {
            '__EVENTARGUMENT': '',
            '__EVENTTARGET': '',
            '__VIEWSTATE': viewstate,
            '_ctl0:QUERY': '',
            #'_ctl3:btn_Search': 'Search',
            '_ctl3:btn_Download': 'File Download',
            '_ctl4:date_IssueDate_from': start,
            '_ctl4:date_IssueDate_to': end,
            '_ctl4:txt_ContractorName_s': '',
            '_ctl4:txt_ExternalFileNum': '',
            '_ctl4:txt_PermitType': '',
            '_ctl4:txt_ProjectAddress': '',
            '_ctl4:txt_ProjectName': '',
            '_ctl4:txt_TaxJurisdiction': 'CHARLOTTE',
            '_ctl4:txt_USDCCodeNumber': '',
            'fmt': 'standard',
        }

    def clean_list_record(self, record):
        formats = ['%Y-%m-%dT00:00:00', '%m/%d/%Y']
        for format in formats:
            try:
                record['Issue_Date'] = parse_date(record['Issue_Date'], format)
                break
            except ValueError:
                continue
        # collapse some occupancy types
        mapping = {
            'A3C     * ASSEMBLY - CHURCH': 'A3     * ASSEMBLY - CHURCH',
            'F1     * FACTORY - MODERATE': 'F1     * FACTORY/INDUSTRIAL - MODERATE HAZARD',
            'F2     * FACTORY - LOW': 'F2     * FACTORY/INDUSTRIAL - LOW HAZARD',
            'R3     * RESIDENTIAL - SINGLE FAM': 'R3     * RESIDENTIAL - SINGLE FAMILY'
        }
        record['occupancy_type'] = mapping.get(record['Occupancy'], record['Occupancy'])
        return record

    def existing_record(self, record):
        if not isinstance(record['Issue_Date'], datetime.date):
            return None
        qs = NewsItem.objects.filter(schema__id=self.schema.id, item_date=record['Issue_Date'])
        qs = qs.by_attribute(self.schema_fields['permit_number'], record['ExternalFileNum'])
        try:
            return qs[0]
        except IndexError:
            return None

    def save(self, old_record, list_record, detail_record):
        if not isinstance(list_record['Issue_Date'], datetime.date):
            self.logger.debug("Did not save %s. Invalid date %s." % (list_record['ExternalFileNum'], list_record['Issue_Date']))
            return
        if old_record is not None:
            self.logger.debug('Record already exists')
            return
        permit_type = self.get_or_create_lookup('permit_type', list_record['PermitType'], list_record['PermitType'], make_text_slug=False)
        project_type = self.get_or_create_lookup('project_type', list_record['USDCCodeNumber'], list_record['USDCCodeNumber'], make_text_slug=False)
        occupancy_type = self.get_or_create_lookup('occupancy_type', list_record['occupancy_type'], list_record['occupancy_type'], make_text_slug=False)
        construction_type = self.get_or_create_lookup('construction_type', list_record['ConstructionType'], list_record['ConstructionType'], make_text_slug=False)

        title = 'Permit issued for %s' % project_type.name
        attributes = {
            'permit_number': list_record['ExternalFileNum'],
            'cost': list_record['Construction_Cost'],
            'permit_type': permit_type.id,
            'project_type': project_type.id,
            'project_number': list_record['ProjectNumber'],
            'owner': list_record['OwnerTenant'],
            'occupancy_type': occupancy_type.id,
            'occupancy_type_raw': list_record['Occupancy'],
            'number_of_stories': list_record['NumberOfStories'],
            'construction_type': construction_type.id,
            'total_fee': list_record['TotalFee'],
        }
        self.create_newsitem(
            attributes,
            title=title,
            item_date=list_record['Issue_Date'],
            location_name=smart_title(list_record['Address']),
        )

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    import sys
    if len(sys.argv) > 1:
        start_date = parse_date(sys.argv[1], '%Y-%m-%d')
    else:
        start_date = None
    BuildingPermitScraper(start_date=start_date).update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Screen scraper for Charlotte certificates of occupancy.
http://dwexternal.co.mecklenburg.nc.us/ids/RptGrid01.aspx?rpt=Certificate%20of%20Occupancy%20Report
"""

from ebdata.retrieval.scrapers.list_detail import SkipRecord
from ebpub.db.models import NewsItem, SchemaField
from ebpub.utils.dates import parse_date
from ebpub.utils.text import smart_title
from everyblock.cities.charlotte.utils import MecklenburgScraper
from dateutil.relativedelta import relativedelta
import datetime

class BuildingInspectionScraper(MecklenburgScraper):
    schema_slugs = ('building-permit-inspections',)
    has_detail = False
    root_uri = 'http://dwexternal.co.mecklenburg.nc.us/ids/RptGrid01.aspx?rpt=CFR%20Inspections'

    def get_location(self, permit_number):
        # cache the schemafield for building permit permit numbers so we don't
        # have to look it up for each record
        if not hasattr(self, '_permit_number_field'):
            self._permit_number_field = SchemaField.objects.get(
                schema__slug='building-permits', name='permit_number')
        # lookup address from the permit number
        qs = NewsItem.objects.filter(schema__slug='building-permits')
        qs = qs.by_attribute(self._permit_number_field, permit_number)
        try:
            permit = qs[0]
            return permit.location_name
        except IndexError:
            return None

    def list_pages(self):
        self.login()
        return super(BuildingInspectionScraper, self).list_pages()

    def date_pairs(self):
        # Alwys start the scraper running 4 months ago. The data is updated
        # quarterly, so this will give us a cushion if they publish it up to a
        # month after the quarter ends.
        d = datetime.date.today() - relativedelta(months=4)
        while d < datetime.date.today():
            start = d.strftime('%m/%d/%Y')
            end = (d + relativedelta(days=6)).strftime('%m/%d/%Y')
            d += relativedelta(days=7)
            yield (start, end)

    def search_arguments(self, start, end, viewstate):
        return {
            '_ctl3:btn_Download': 'File Download',
            '__EVENTARGUMENT': '',
            '__EVENTTARGET': '',
            '__VIEWSTATE': viewstate,
            '_ctl0:QUERY': '',
            #'_ctl3:btn_Search': 'Search',
            '_ctl4:CFR_CONFIRMATIONNUMBER': '',
            '_ctl4:CFR_Contractor': '',
            '_ctl4:CFR_Contractor_stem': '',
            '_ctl4:CFR_INSPECTORINFO': '',
            '_ctl4:CFR_LagTime_Days_from': '',
            '_ctl4:CFR_LagTime_Days_to': '',
            '_ctl4:CFR_PERMITTRADE': '',
            '_ctl4:CFR_PERMITTYPE': '',
            '_ctl4:CFR_REQUESTDATE_from': start,
            '_ctl4:CFR_REQUESTDATE_to': end,
            '_ctl4:CFR_RESULTDATE_from': '',
            '_ctl4:CFR_RESULTDATE_to': '',
            '_ctl4:CFR_Result': '',
            '_ctl4:CFR_USDC_Activity_Type': '',
            '_ctl4:CFR_USDCcode': '',
            'fmt': 'standard'
        }

    def clean_list_record(self, record):
        record['location'] = self.get_location(record['PERMIT_NUMBER'])
        if record['location'] is None:
            raise SkipRecord("No permit found for '%s'" % record['PERMIT_NUMBER'])
        record['InspectionDate'] = parse_date(record['InspectionDate'], '%Y-%m-%dT00:00:00')
        return record

    def existing_record(self, record):
        inspection_type = self.get_or_create_lookup('inspection_type', record['TASKPERFORMED'], record['TASKPERFORMED'], make_text_slug=False)
        # use permit number, task performed, and date
        qs = NewsItem.objects.filter(schema__id=self.schema.id, item_date=record['InspectionDate'])
        qs = qs.by_attribute(self.schema_fields['permit_number'], record['PERMIT_NUMBER'])
        qs = qs.by_attribute(self.schema_fields['inspection_type'], inspection_type.id)
        try:
            return qs[0]
        except IndexError:
            return None

    def save(self, old_record, list_record, detail_record):
        if old_record is not None:
            self.logger.debug('Record already exists')
            return
        if list_record['location'] is None:
            self.logger.debug('Skipping %s. No address found.' % list_record['PERMIT_NUMBER'])
            return
        project_type = self.get_or_create_lookup('project_type', list_record['USDC_Activity_Type'], list_record['USDC_Activity_Type'], make_text_slug=False)
        inspection_type = self.get_or_create_lookup('inspection_type', list_record['TASKPERFORMED'], list_record['TASKPERFORMED'], make_text_slug=False)
        result = self.get_or_create_lookup('result', list_record['RESULT'], list_record['RESULT'], make_text_slug=False)
        detail_lookups = []
        for i in range(1, 10):
            code = list_record['Defect%s_Code' % i].strip()
            name = list_record['DEFECT%s' % i].strip()
            if name != '' and code != '':
                lookup = self.get_or_create_lookup('details', name, code, make_text_slug=False)
                detail_lookups.append(lookup)
        if list_record['RESULT'] == '01 - Passed':
            title = "Project passed inspection at %s" % list_record['location']
        elif list_record['RESULT'] == '02 - Failed':
            title = "Project failed inspection at %s" % list_record['location']
        elif list_record['RESULT'] == '03 - Inaccessible':
            title = "Project conditionally passed inspection at %s" % list_record['location']
        elif list_record['RESULT'] == 'Not Done':
            title = "Project was not inspected at %s" % list_record['location']
        attributes = {
            'contractor_id': list_record['CONTRACTORID'],
            'contractor': list_record['CONTRACTOR'],
            'project_type': project_type.id,
            'permit_number': list_record['PERMIT_NUMBER'],
            'inspection_type': inspection_type.id,
            'result': result.id,
            'details': ','.join([str(d.id) for d in detail_lookups])
        }
        self.create_newsitem(
            attributes,
            title=title,
            item_date=list_record['InspectionDate'],
            location_name=smart_title(list_record['location']),
        )

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    import sys
    if len(sys.argv) > 1:
        start_date = parse_date(sys.argv[1], '%Y-%m-%d')
    else:
        start_date = None
    BuildingInspectionScraper(start_date=start_date).update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Screen scraper for Charlotte city council minutes and zoning items
http://www.charmeck.org/Departments/City+Clerk/Council+Related/Minutes+2007-2014.htm
"""

from ebdata.blobs.geotagging import save_locations_for_page
from ebdata.blobs.models import Page, Seed
from ebdata.parsing.pdftotext import pdf_to_text
from ebdata.retrieval.scrapers.base import BaseScraper
from dateutil.parser import parse as parse_date
from lxml.html import document_fromstring
import os
import datetime

ROOT_URI = 'http://www.charmeck.org/Departments/City+Clerk/Council+Related/Minutes+2007-2014.htm'

class CharlotteCityCouncilMeetingsScraper(BaseScraper):
    schema_name = 'city-council-minutes'

    def get_to_file(self, *args, **kwargs):
        if self.retriever._cookies:
            # Build the Cookie header manually. We get:
            #   socket.error: (54, 'Connection reset by peer')
            # if we send newline separated cookies. Semicolon separated works fine.
            cookie = self.retriever._cookies.output(attrs=[], header='', sep=';').strip()
            kwargs['send_cookies'] = False
            kwargs['headers'] = {'Cookie': cookie}
        return self.retriever.get_to_file(*args, **kwargs)

    def list_pages(self):
        html = self.get_html(ROOT_URI)
        t = document_fromstring(html)
        for link in t.xpath("//table[@id='table1']//a"):
            url = "http://www.charmeck.org%s" % link.get('href')
            title = link.text or ''
            if self.already_downloaded(url):
                continue
            pdf_path = self.get_to_file(url)
            yield {
                'title': title,
                'url': url,
                'data': self.parse_pdf(pdf_path),
                'date': parse_date(title, fuzzy=True)
            }
            os.unlink(pdf_path) # Clean up the temporary file.

    def parse_pdf(self, pdf_path):
        return pdf_to_text(pdf_path, keep_layout=True, raw=False).decode('Latin-1')

    def already_downloaded(self, url):
        try:
            blobs = Page.objects.filter(url=url)[0]
            return True
        except IndexError:
            return False

    def save(self, page):
        if not hasattr(self, 'seed'):
            self.seed = Seed.objects.get(schema__slug__exact=self.schema_name)
        p = Page.objects.create(
            seed=self.seed,
            url=page['url'],
            scraped_url=page['url'],
            html=page['data'],
            when_crawled=datetime.datetime.now(),
            is_article=True,
            is_pdf=True,
            is_printer_friendly=False,
            article_headline=self.get_headline(page),
            article_date=page['date'],
            has_addresses=None,
            when_geocoded=None,
            geocoded_by='',
            times_skipped=0,
            robot_report='',
        )
        save_locations_for_page(p)
        return p

    def update(self):
        for page in self.list_pages():
            self.save(page)

    def get_headline(self, page):
        return "City Council minutes, %s" % page['date'].strftime('%B %d, %Y')

if __name__ == '__main__':
    from ebdata.retrieval import log_debug
    CharlotteCityCouncilMeetingsScraper().update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Scraper for Mecklenberg County Board Minutes

http://www.charmeck.org/Departments/BOCC/Meetings/Meeting+Minutes/Home.htm
"""

from ebdata.blobs.geotagging import save_locations_for_page
from ebdata.blobs.models import Page, Seed
from ebdata.parsing.pdftotext import pdf_to_text
from ebdata.retrieval.scrapers.base import BaseScraper
from dateutil.parser import parse as parse_date
from lxml.html import document_fromstring
import os
import datetime

ROOT_URI = 'http://www.charmeck.org/Departments/BOCC/Meetings/Meeting+Minutes/Home.htm'

class BoardMinutesScraper(BaseScraper):
    schema_name = 'county-board-proceedings'

    def get_to_file(self, *args, **kwargs):
        if self.retriever._cookies:
            # Build the Cookie header manually. We get:
            #   socket.error: (54, 'Connection reset by peer')
            # if we send newline separated cookies. Semicolon separated works fine.
            cookie = self.retriever._cookies.output(attrs=[], header='', sep=';').strip()
            kwargs['send_cookies'] = False
            kwargs['headers'] = {'Cookie': cookie}
        return self.retriever.get_to_file(*args, **kwargs)

    def list_pages(self):
        html = self.get_html(ROOT_URI)
        t = document_fromstring(html)
        for link in t.xpath("//table[@id='Table8']//a"):
            if not link.get('href')[-4:] == '.pdf':
                continue
            url = "http://www.charmeck.org%s" % link.get('href')
            title = link.text or ''
            if self.already_downloaded(url):
                continue
            pdf_path = self.get_to_file(url)
            yield {
                'title': title,
                'url': url,
                'data': self.parse_pdf(pdf_path),
                'date': parse_date(title, fuzzy=True)
            }
            os.unlink(pdf_path) # Clean up the temporary file.

    def parse_pdf(self, pdf_path):
        return pdf_to_text(pdf_path, keep_layout=True, raw=False).decode('Latin-1')

    def already_downloaded(self, url):
        try:
            blobs = Page.objects.filter(url=url)[0]
            return True
        except IndexError:
            return False

    def save(self, page):
        if not hasattr(self, 'seed'):
            self.seed = Seed.objects.get(schema__slug__exact=self.schema_name)
        p = Page.objects.create(
            seed=self.seed,
            url=page['url'],
            scraped_url=page['url'],
            html=page['data'],
            when_crawled=datetime.datetime.now(),
            is_article=True,
            is_pdf=True,
            is_printer_friendly=False,
            article_headline=self.get_headline(page),
            article_date=page['date'],
            has_addresses=None,
            when_geocoded=None,
            geocoded_by='',
            times_skipped=0,
        )
        save_locations_for_page(p)
        return p

    def update(self):
        for page in self.list_pages():
            self.save(page)

    def get_headline(self, page):
        return "Mecklenberg County Board item, %s" % page['date'].strftime('%B %d, %Y')

if __name__ == '__main__':
    from ebdata.retrieval import log_debug
    BoardMinutesScraper().update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Charlotte library scraper
"""

from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from lxml import etree
import datetime
import re
import urllib

ITEM_TYPES = (
    ('afn', 'Fiction'),
    ('anfbn', 'Biography'),
    ('anfn', 'Nonfiction'),
    ('dvdn', 'DVD'),
)

BRANCHES = (
    ('Main library', 'ml', '310 North Tryon St.'),
    ('Beatties Ford Road Branch', 'bfr', '2412 Beatties Ford Road'),
    ('Belmont Center Branch', 'bc', '700 Parkwood Avenue'),
    ('Carmel Branch', 'ca', '6624 Walsh Boulevard'),
    ('Checkit Outlet', 'ckt', '435 South Tryon Street'),
    ('Freedom Regional', 'frl', '1230 Alleghany Street'),
    ('Hickory Grove Branch', 'hg', '7209 E. W.T. Harris Blvd.'),
    ('Imaginon: The Joe and Joan Martin Center', 'img', '300 East 7th St.'),
    ('Independence Regional', 'ib', '6015 Conference Drive'),
    ('Mountain Island', 'mti', '300 Hoyt Galvin Way'),
    ('Morrison Regional', 'mor', '7015 Morrison Boulevard'),
    ('Myers Park Branch', 'mpk', '1361 Queens Road'),
    ('Plaza Midwood Branch', 'pm', '1623 Central Avenue'),
    ('Scaleybark Branch', 'sc', '101 Scaleybark Road'),
    ('South County Regional', 'sor', '5801 Rea Road'),
    ('Steele Creek Branch', 'st', '13620 Steele Creek Road'),
    ('Sugar Creek Branch', 'sug', '4045 N. Tryon Street'),
    ('University City Regional', 'uc', '301 E. W.T. Harris Boulevard'),
    ('West Boulevard', 'wbl', '2157 West Boulevard'),
)

rows_xpath = etree.XPath('//row')

class LibraryScraper(NewsItemListDetailScraper):
    schema_slugs = ('new-library-items',)
    has_detail = False
    sleep = 3

    def list_pages(self):
        for item_type_code, item_type in ITEM_TYPES:
            for branch_name, branch_code, branch_address in BRANCHES:
                # Note that we use the hard-coded query string here instead of
                # urllib.urlencode because urllib.urlencode encodes pluses,
                # which seems to break the library site.
                url = 'http://hip.plcmc.org/ipac20/ipac.jsp?menu=search&profile=plcmc&index=.TW&term=*&oper=and&limitbox_1=CO01+%%3D+co_%s&limitbox_2=LO01+%%3D+%s&GetXML=true' % (item_type_code, branch_code)
                yield self.get_html(url), item_type, branch_name, branch_address

    def parse_list(self, bunch):
        html, item_type, branch_name, branch_address = bunch

        # The feed puts these comments (and whitespace) before the <?xml>
        # declaration, which lxml doesn't like.
        html = re.sub(r'\s*<!--searching-->\s*', '', html)

        tree = etree.fromstring(html)
        for row in rows_xpath(tree):
            yield {
                'item_type': item_type,
                'branch_name': branch_name,
                'branch_address': branch_address,
                'row': row,
            }

    def clean_list_record(self, record):
        row = record.pop('row')
        record['isbn'] = row.find('isbn') and row.find('isbn').text or None
        record['item_key'] = row.find('key').text
        record['image'] = urllib.unquote(row.find('small_ec_image_url').text)
        record['title'] = row.xpath('TITLE/data/text')[0].text.split(' / ')[0].replace('[DVD]', '').strip()
        record['url'] = 'http://hip.plcmc.org/ipac20/ipac.jsp?profile=plcmc&uri=%s' % row.xpath('TITLE/data/link/func')[0].text
        return record

    def existing_record(self, record):
        # Due to the nature of the data, there's no great way to tell whether
        # an item is "existing." We do it here by looking for any item at the
        # same library branch with the same item_key that was added in the past
        # 7 days.
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id, item_date__gt=self.start_time - datetime.timedelta(days=7))
            qs = qs.by_attribute(self.schema_fields['branch'], record['branch_name'], is_lookup=True)
            qs = qs.by_attribute(self.schema_fields['item_key'], record['item_key'])
            return qs[0]
        except IndexError:
            return None

    def save(self, old_record, list_record, detail_record):
        if old_record is not None:
            self.logger.debug('Record already exists')
            return

        branch = self.get_or_create_lookup('branch', list_record['branch_name'], list_record['branch_name'])
        item_type = self.get_or_create_lookup('item_type', list_record['item_type'], list_record['item_type'])

        attributes = {
            'image': list_record['image'],
            'isbn': list_record['isbn'],
            'item_type': item_type.id,
            'item_key': list_record['item_key'],
            'branch': branch.id,
        }
        self.create_newsitem(
            attributes,
            title=list_record['title'][:255],
            url=list_record['url'],
            item_date=self.start_time.date(),
            location_name=list_record['branch_address'],
        )

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    LibraryScraper().update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Screen scraper for Charlotte certificates of occupancy.
http://dwexternal.co.mecklenburg.nc.us/ids/RptGrid01.aspx?rpt=Certificate%20of%20Occupancy%20Report
"""

from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date
from ebpub.utils.text import smart_title
from everyblock.cities.charlotte.utils import MecklenburgScraper

class OccupancyScraper(MecklenburgScraper):
    schema_slugs = ('occupancy',)
    has_detail = False
    root_uri = 'http://dwexternal.co.mecklenburg.nc.us/ids/RptGrid01.aspx?rpt=Certificate%20of%20Occupancy%20Report'

    def list_pages(self):
        self.login()
        return super(OccupancyScraper, self).list_pages()

    def search_arguments(self, start, end, viewstate):
        return {
            '__EVENTARGUMENT': '',
            '__EVENTTARGET': '',
            '__VIEWSTATE': viewstate,
            '_ctl0:QUERY': '',
            #'_ctl3:btn_Search': 'Search',
            '_ctl3:btn_Download': 'File Download',
            '_ctl4:Municipality:0': 'on',
            '_ctl4:date_COIssueDate_from': start,
            '_ctl4:date_COIssueDate_to': end,
            '_ctl4:date_ContractCost_From': '',
            '_ctl4:date_ContractCost_To': '',
            '_ctl4:date_IssueDate_from': '',
            '_ctl4:date_IssueDate_to':  '',
            '_ctl4:txt_BuildingAddress': '',
            '_ctl4:txt_CAMA_Parcel_From': '',
            '_ctl4:txt_CAMA_Parcel_To': '',
            '_ctl4:txt_JobStatus': '',
            '_ctl4:txt_JobTypeName': 'j_BuildingPermit',
            '_ctl4:txt_PermitID_From': '',
            '_ctl4:txt_PermitID_To': '',
            '_ctl4:txt_PermitType': '',
            '_ctl4:txt_StreetName': '',
            '_ctl4:txt_USDCCode_From': '',
            '_ctl4:txt_USDCCode_To': '',
            'fmt': 'standard'
        }

    def clean_list_record(self, record):
        record['CO_Date'] = parse_date(record['CO_Date'], '%m/%d/%Y')
        mapping = {
          '328 - Other Not-CO\'able Non-Residential Buildings(well h': '328 - Other CO\'able Non-Res Bldgs(jails,post office)',
          '437 - All Other CO\'able Buildings / Structures': '437 - All Other Buildings / Structures(additions, remode'
        }
        record['project_type'] = mapping.get(record['USDC_Code'], record['USDC_Code'])
        return record

    def existing_record(self, record):
        qs = NewsItem.objects.filter(schema__id=self.schema.id, item_date=record['CO_Date'])
        qs = qs.by_attribute(self.schema_fields['permit_number'], record['Permit_Number'])
        try:
            return qs[0]
        except IndexError:
            return None
        return None

    def save(self, old_record, list_record, detail_record):
        print list_record
        if old_record is not None:
            self.logger.debug('Record already exists')
            return
        project_type = self.get_or_create_lookup('project_type', list_record['project_type'], list_record['project_type'], make_text_slug=False)
        title = 'Certificate issued for %s' % project_type.name
        attributes = {
            'parcel_id': list_record['PID__Parcel_ID_'],
            'permit_number': list_record['Permit_Number'],
            'cost': list_record['Cost'],
            'project_type': project_type.id,
            'project_type_raw': list_record['USDC_Code'],
            'num_units': list_record['NumberOfUnits'],
            'heated_sqft': list_record['Heated_Square_Feet']
        }
        self.create_newsitem(
            attributes,
            title=title,
            item_date=list_record['CO_Date'],
            location_name=smart_title(list_record['Project_Address']),
        )

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    import sys
    if len(sys.argv) > 1:
        start_date = parse_date(sys.argv[1], '%Y-%m-%d')
    else:
        start_date = None
    OccupancyScraper(start_date=start_date).update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Screen scraper for Charlotte calls for service (911 calls).
http://maps.cmpdweb.org/cfs/Default.aspx
"""

from ebdata.retrieval.scrapers.base import ScraperBroken
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date
from ebpub.utils.text import smart_title, clean_address, address_to_block
from django.template.defaultfilters import capfirst
import re

SOURCE_URL = 'http://maps.cmpdweb.org/cfs/Default.aspx'

CATEGORIES = {
    '': 'UNKNOWN',
    '10-33 HELP ME QUICK': 'POLICE ACTION',
    '10-18 URGENT ASSISTANCE NEEDED': 'POLICE ACTION',
    'ABANDONED ANIMAL': 'ANIMAL RELATED',
    'ABANDONED PROPERTY': 'PROPERTY RELATED',
    'ABANDONED VEHICLE': 'PROPERTY RELATED',
    'A/C ASSIST OTHER AGENCY': 'POLICE ACTION',
    'A/C CHECK COMPLIANCE': 'POLICE ACTION',
    'ACCIDENT-FATALITY': 'TRAFFIC RELATED',
    'ACCIDENT IN ROADWAY-PROPERTY DAMAGE': 'TRAFFIC RELATED',
    'ACCIDENT NON ROADWAY-PROPERTY DAMAGE': 'TRAFFIC RELATED',
    'ACCIDENT-PERSONAL INJURY': 'TRAFFIC RELATED',
    'AGRRESSIVE ANIMAL': 'ANIMAL RELATED',
    'ALARM-AUDIBLE': 'ALARM',
    'ALARM-AUTO': 'ALARM',
    'ALARM-COMMERCIAL': 'ALARM',
    'ALARM-COMMERCIAL-HOLD UP': 'ALARM',
    'ALARM NO PERMIT NUMBER': 'ALARM',
    'ALARM PERSONAL': 'ALARM',
    'ALARM-RESIDENTIAL': 'ALARM',
    'ALARM-RESIDENTIAL-PRIORITY': 'ALARM',
    'ABC-INTOXICATED PERSON': 'ALCOHOL RELATED',
    'ABC-VIOLATIONS-CITATIONS': 'ALCOHOL RELATED',
    'ANIMAL ATTACK': 'ANIMAL RELATED',
    'ANIMAL BARKING': 'ANIMAL RELATED',
    'ANIMAL BITE': 'ANIMAL RELATED',
    'ANIMAL CRUELTY': 'ANIMAL RELATED',
    'ANIMAL EVICTION': 'ANIMAL RELATED',
    'ANIMAL FIGHTING': 'ANIMAL RELATED',
    'ANIMAL ODOR': 'ANIMAL RELATED',
    'ANIMAL RABIES EXPOSURE': 'ANIMAL RELATED',
    'ANIMAL SAFETY CONCERN REFERRAL': 'ANIMAL RELATED',
    'ANIMAL TRANSPORT': 'ANIMAL RELATED',
    'ANIMAL TRAP': 'ANIMAL RELATED',
    'ARMED PERSON': 'WEAPON RELATED',
    'ARMED TO THE TERROR OF PUBLIC': 'WEAPON RELATED',
    'ASSAULT- PHYSICAL ONLY': 'ASSAULT',
    'ADW- WITH INJURY': 'ASSAULT',
    'ADW-NO INJURY': 'ASSAULT',
    'ASSIST FIRE DEPARTMENT': 'POLICE ACTION',
    'ASSIST MEDIC': 'POLICE ACTION',
    'ASSIST OTHER AGENCY': 'POLICE ACTION',
    'ASSIST OTHER JURISDICTION': 'POLICE ACTION',
    'ASSIST WATER DEPARTMENT': 'POLICE ACTION',
    'ATTEMPT TO LOCATE': 'POLICE ACTION',
    'BOATING WHILE IMPAIRED': 'ALCOHOL RELATED',
    'BOMB-SUSPICIOUS ITEM FOUND': 'BOMB RELATED',
    'BOMB THREAT': 'BOMB RELATED',
    'BREAK/ENTER COMMERCIAL': 'BREAK / ENTER',
    'BREAK/ENTER RESIDENTIAL-OCCUPIED': 'BREAK / ENTER',
    'BREAK/ENTER RESIDENTIAL-UNOCCUPIED': 'BREAK / ENTER',
    'BREAK/ENTER VENDING MACHINES': 'BREAK / ENTER',
    'CANINE DEFECATION': 'ANIMAL RELATED',
    'CARELESS/RECKLESS DRIVING': 'TRAFFIC RELATED',
    'CARRYING CONCEALED WEAPON': 'WEAPON RELATED',
    'CHECK THE WELFARE OF': 'POLICE ACTION',
    'CITIZEN CONTACT': 'POLICE ACTION',
    'COMMUNICATING THREATS-OTHER': 'THREATS',
    'COMMUNICATING THREATS-PERSON': 'THREATS',
    'CPTED ANALYSIS': 'POLICE ACTION',
    'CRIME SCENE NEEDED': 'POLICE ACTION',
    'CRITICAL INCIDENT': 'CRITICAL INCIDENT',
    'CRITICAL INCIDENT CIVIL UNREST': 'CRITICAL INCIDENT',
    'CRITICAL INCIDENT CODE AT AIRPORT': 'CRITICAL INCIDENT',
    'CRITICAL INCIDENT SWAT': 'CRITICAL INCIDENT',
    'DEATH INVESTIGATION': 'DEATH RELATED',
    'DEATH-NATURAL': 'DEATH RELATED',
    'DISABLED BOATER': 'LAKE INCIDENT',
    'DISCHARGING A FIREARM': 'WEAPON RELATED',
    'DISTURBANCE': 'PUBLIC DISTURBANCE',
    'DOMESTIC DISTURBANCE': 'DOMESTIC INCIDENT',
    'DOMESTIC PROPERTY RECOVERY': 'DOMESTIC INCIDENT',
    'DOMESTIC TRESPASS': 'DOMESTIC INCIDENT',
    'DV-ADW- WITH INJURY': 'DOMESTIC INCIDENT',
    'DV-ADW-NO INJURY': 'DOMESTIC INCIDENT',
    'DV-COMMUNICATING THREATS-OTHER': 'DOMESTIC INCIDENT',
    'DV-COMMUNICATING THREATS-PERSON': 'DOMESTIC INCIDENT',
    'DOMESTIC VIOLENCE-PHYSICAL ASSAULT': 'DOMESTIC INCIDENT',
    'DV-VIOLATION OF LEGAL ORDER': 'DOMESTIC INCIDENT',
    'DRAG RACING': 'TRAFFIC RELATED',
    'DWI': 'ALCOHOL RELATED',
    'DRIVING WHILE LICENSE REVOKED': 'TRAFFIC RELATED',
    'DRUG PARAPHERNALIA-FOUND/PICKUP': 'DRUG RELATED',
    'DRUG POSSESSION-SUBSTANCE/PARAPHERNALIA': 'DRUG RELATED',
    'DRUG PRESCRIPTION-FRAUD': 'DRUG RELATED',
    'ESCORT': 'SEX RELATED',
    'EXTORTION/BLACKMAIL': 'POLICE ACTION',
    'FELINE NUISANCE': 'ANIMAL RELATED',
    'FIGHT': 'FIGHT',
    'FIGHT-CROWD': 'FIGHT',
    'FIRE CASE/INV': 'POLICE ACTION',
    'FOOT PURSUIT': 'POLICE ACTION',
    'FOUND PROPERTY': 'PROPERTY RELATED',
    'FRAUD/FORGERY': 'THEFT',
    'GRAFFITI': 'PUBLIC DISTURBANCE',
    'HARASSING PHONE CALLS': 'PUBLIC DISTURBANCE',
    'HIT & RUN-FATALITY': 'HIT AND RUN',
    'HIT & RUN-IN ROADWAY-PROPERTY DAMAGE': 'HIT AND RUN',
    'HIT & RUN-NON ROADWAY-PROPERTY DAMAGE': 'HIT AND RUN',
    'HIT & RUN-PERSONAL INJURY': 'HIT AND RUN',
    'HOMELESS PEOPLE': 'PUBLIC DISTURBANCE',
    'ILLEGAL PARKING': 'TRAFFIC RELATED',
    'INDECENT EXPOSURE': 'SEX RELATED',
    'INJURED ANIMAL': 'ANIMAL RELATED',
    'INJURY TO REAL/PERSONAL PROPERTY': 'PROPERTY RELATED',
    'JUVENILE-WEAPON AT SCHOOL': 'WEAPON RELATED',
    'KIDNAPPING': 'KIDNAPPING',
    'KIDNAPPING-JUVENILE-STRANGER': 'KIDNAPPING',
    'LAKE ABANDONED BOAT': 'LAKE INCIDENT',
    'LAKE ACCIDENT PERSONAL INJURY': 'LAKE INCIDENT',
    'LAKE ACCIDENT PROPERTY DAMAGE': 'LAKE INCIDENT',
    'LAKE ASSIST OTHER JURISDICTIONS': 'LAKE INCIDENT',
    'LAKE CHECK CHANNEL MARKER': 'LAKE INCIDENT',
    'LAKE ILLEGAL WASTE OR FUEL DISCHARGE': 'LAKE INCIDENT',
    'LAKE MEDICAL ASSISTANCE': 'LAKE INCIDENT',
    'LAKE NAVIGATIONAL HAZARD': 'LAKE INCIDENT',
    'LAKE RECKLESS OPERATION': 'LAKE INCIDENT',
    'LAKE SEARCH RESCUE/RECOVERY': 'LAKE INCIDENT',
    'LAKE WAKE VIOLATION': 'LAKE INCIDENT',
    'LARCENY': 'THEFT',
    'LARCENY FROM VEHICLE': 'THEFT',
    'LARGE ANIMAL': 'ANIMAL RELATED',
    'LOITERING': 'LOITERING',
    'LOITERING-ALCOHOL RELATED': 'LOITERING',
    'LOITERING FOR MONEY': 'LOITERING',
    'LOITERING-PROSTITUTION RELATED': 'LOITERING',
    'LOITERING-SALE/PURCHASE DRUGS': 'LOITERING',
    'LOST PROPERTY': 'PROPERTY RELATED',
    'MISSING PERSON': 'MISSING PERSON(S)',
    'MISSING PERSON-RUNAWAY': 'MISSING PERSON(S)',
    'MISSING PERSON-SPEC NEEDS/CHILD': 'MISSING PERSON(S)',
    'MISSING PERSONS RECOVERY': 'MISSING PERSON(S)',
    'NOISE COMPLAINT': 'PUBLIC DISTURBANCE',
    'NOISE COMPLAINT-CROWD': 'PUBLIC DISTURBANCE',
    'NOISE COMPLAINT-FIREWORKS': 'PUBLIC DISTURBANCE',
    'NO OPERATOR LICENSE': 'TRAFFIC RECOVERY',
    'NOTIFY': 'POLICE ACTION',
    'OVERDOSE': 'DRUG RELATED',
    'OWNER SURRENDER': 'POLICE ACTION',
    'PEEPING TOM': 'PUBLIC DISTURBANCE',
    'PERSON DOWN/PUBLIC ACCIDENT': 'PUBLIC DISTURBANCE',
    'PICK UP PROPERTY OR EVIDENCE': 'POLICE ACTION',
    'PORNOGRAPHY': 'SEX RELATED',
    'PROSTITUTION': 'SEX RELATED',
    'PROSTITUTION STING OR ARREST': 'SEX RELATED',
    'PUBLIC URINATION': 'PUBLIC DISTURBANCE',
    'ROAD BLOCKAGE': 'TRAFFIC RELATED',
    'ROBBERY FROM BUSINESS': 'THEFT',
    'ROBBERY FROM BUSINESS-ARMED': 'THEFT',
    'ROBBERY FROM PERSON': 'THEFT',
    'ROBBERY FROM PERSON-ARMED': 'THEFT',
    'SEXUALLY ORIENTED BUSINESS': 'SEX RELATED',
    'SPECIAL EVENT': 'POLICE ACTION',
    'STALKING': 'PUBLIC DISTURBANCE',
    'STOLEN VEHICLE': 'THEFT',
    'STRAY ANIMAL': 'ANIMAL RELATED',
    'SUSPICIOUS-AIRCRAFT': 'SUSPICIOUS ACTIVITY',
    'SUSPICIOUS PERSON/PROWLER': 'SUSPICIOUS ACTIVITY',
    'SUSPICIOUS PROPERTY': 'SUSPICIOUS ACTIVITY',
    'SUSPICIOUS VEHICLE OCCUPIED': 'SUSPICIOUS ACTIVITY',
    'SUSPICIOUS VEHICLE UNOCCUPIED': 'SUSPICIOUS ACTIVITY',
    'TOWED VEHICLE-ADVISED EVENT': 'TRAFFIC RELATED',
    'TRAFFIC CONTROL/MALFUNCTION': 'TRAFFIC RELATED',
    'TRASH/LITTERING': 'PUBLIC DISTURBANCE',
    'TRESPASS': 'PUBLIC DISTURBANCE',
    'TRUANCY': 'PUBLIC DISTURBANCE',
    'UNAUTHORIZED USE OF VEHICLE': 'TRAFFIC RELATED',
    'VEHICLE DISABLED IN ROADWAY': 'TRAFFIC RELATED',
    'VEHICLE DISABLED NOT IN ROADWAY': 'TRAFFIC RELATED',
    'VEHICLE PURSUIT': 'TRAFFIC RELATED',
    'VEHICLE RECOVERY': 'POLICE ACTION',
    'WILD ANIMAL': 'ANIMAL RELATED',
    'WORTHLESS CHECKS': 'THEFT',
    'ZONE CHECK': 'POLICE ACTION',
}

class CallScraper(NewsItemListDetailScraper):
    schema_slugs = ('police-calls',)
    has_detail = False
    parse_list_re = re.compile(r'<tr[^>]*>\s*<td>(?P<datetime>\d\d?/\d\d?/\d{4} \d\d?:\d\d:\d\d [AP]M)</td><td>(?P<division>[^>]*)</td><td>(?P<address>[^>]*)\s*</td><td>(?P<event>[^>]*)</td><td>(?P<disposition>[^>]*)</td>\s*</tr>', re.IGNORECASE | re.DOTALL)

    def __init__(self, hours=8, *args, **kwargs):
        self.num_hours = hours
        NewsItemListDetailScraper.__init__(self, *args, **kwargs)

    def list_pages(self):
        html = self.get_html(SOURCE_URL)
        
        m = re.search(r'<input type="hidden" name="__VIEWSTATE" value="([^"]*)"', html)
        if not m:
            raise ScraperBroken('VIEWSTATE not found on %s' % self.source_url)
        viewstate = m.group(1)
        
        yield self.get_html(SOURCE_URL, {'__VIEWSTATE': viewstate, 'ddlEvtHours': self.num_hours, 'btnRefresh': 'Refresh'})

    def clean_list_record(self, record):
        # Save the raw address so we can use it to find duplicate records in
        # the future.
        address = smart_title(record['address'].strip().replace('&amp;', '&').replace('&nbsp;', ' ')).strip()
        record['raw_address'] = address
        record['address'] = address_to_block(clean_address(address))

        record['disposition'] = record['disposition'].replace('&amp;', '&').replace('&nbsp;', ' ').strip() or 'Not available'
        record['event'] = record['event'].replace('&amp;', '&').replace('&nbsp;', ' ').strip()
        item_date = parse_date(record['datetime'], '%m/%d/%Y %I:%M:%S %p', return_datetime=True)
        record['item_date'] = item_date.date()
        record['item_time'] = item_date.time()

        # Normalize this value.
        if record['disposition'] == 'CANCCOMM':
            record['disposition'] = 'CANCELLED BY COMMUNICATIONS'

        return record

    def existing_record(self, record):
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id, item_date=record['item_date'])
            qs = qs.by_attribute(self.schema_fields['raw_address'], record['address'])
            return qs[0]
        except IndexError:
            return None

    def save(self, old_record, list_record, detail_record):
        if old_record is not None:
            self.logger.debug('Record already exists')
            return

        division = self.get_or_create_lookup('division', list_record['division'], list_record['division'])
        event = self.get_or_create_lookup('event', list_record['event'], list_record['event'])
        disposition = self.get_or_create_lookup('disposition', list_record['disposition'], list_record['disposition'])
        category_name = CATEGORIES[event.code.upper().strip()]
        category = self.get_or_create_lookup('category', capfirst(category_name.lower()), category_name)

        attributes = {
            'raw_address': list_record['address'],
            'division': division.id,
            'disposition': disposition.id,
            'event': event.id,
            'event_time': list_record['item_time'],
            'category': category.id
        }
        self.create_newsitem(
            attributes,
            title=event.name,
            item_date=list_record['item_date'],
            location_name=list_record['address'],
        )

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    CallScraper().update()

########NEW FILE########
__FILENAME__ = update-buckets
from ebpub.db.models import NewsItem, SchemaField, Lookup
from everyblock.cities.charlotte.police_calls.retrieval import CATEGORIES
from django.template.defaultfilters import capfirst

if __name__ == '__main__':
    schema_slug = 'police-calls'
    schema_field = SchemaField.objects.get(schema__slug=schema_slug, name='category')
    for ni in NewsItem.objects.filter(schema__slug=schema_slug):
        event = Lookup.objects.get(pk=ni.attributes['event'])
        category_code = CATEGORIES[event.name.upper()]
        category_name = capfirst(category_code.lower())
        bucket = Lookup.objects.get_or_create_lookup(schema_field, category_name, category_code)
        old_bucket_id = ni.attributes['category']
        if old_bucket_id is None or bucket.id != old_bucket_id:
            ni.attributes['category'] = bucket.id
            print "Bucket changed"
            print old_bucket_id
            print bucket

########NEW FILE########
__FILENAME__ = retrieval
"""
Scraper for Charlotte health inspections.
"""

from django.core.serializers.json import DjangoJSONEncoder
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebdata.retrieval.scrapers.list_detail import SkipRecord
from ebdata.retrieval.retrievers import Retriever
from ebpub.db.models import NewsItem
from ebpub.utils.text import smart_title
from cStringIO import StringIO
from dateutil.parser import parse as parse_date
import datetime
import csv
import re

FOOD_SLUG = 'food-inspections'
POOL_SLUG = 'pool-inspections'

EXCLUDED_FACILITY_TYPES = ['20', '21', '22', '23', '41', '44', '03', '04']

# commented out types should be excluded
FACILITY_TYPES = {
    '01': {'schema': FOOD_SLUG, 'name': 'Restaurants'},
    '02': {'schema': FOOD_SLUG, 'name': 'Food Stands'},
    '05': {'schema': FOOD_SLUG, 'name': 'Private School Lunchrooms'},
    '11': {'schema': FOOD_SLUG, 'name': 'Public School Lunchrooms'},
    #'20': {'schema': FOOD_SLUG, 'name': 'Lodging'},
    #'21': {'schema': FOOD_SLUG, 'name': 'B&B Homes'},
    #'22': {'schema': FOOD_SLUG, 'name': 'Summer Camps'},
    #'23': {'schema': FOOD_SLUG, 'name': 'B&B Inns'},
    '30': {'schema': FOOD_SLUG, 'name': 'Meat Markets'},
    #'41': {'schema': FOOD_SLUG, 'name': 'Hospitals'},
    #'44': {'schema': FOOD_SLUG, 'name': 'School Building (Private & Public)'},
    #'03': {'schema': FOOD_SLUG, 'name': 'Mobile Food Units'},
    #'04': {'schema': FOOD_SLUG, 'name': 'Pushcarts'},

    '50': {'schema': POOL_SLUG, 'name': 'Seasonal Swimming Pools'},
    '51': {'schema': POOL_SLUG, 'name': 'Seasonal Wading Pools'},
    '52': {'schema': POOL_SLUG, 'name': 'Seasonal Spas'},
    '53': {'schema': POOL_SLUG, 'name': 'Year-Round Swimming Pools'},
    '54': {'schema': POOL_SLUG, 'name': 'Year-Round Wading Pools'},
    '55': {'schema': POOL_SLUG, 'name': 'Year-Round Spas'},
}

class Scraper(NewsItemListDetailScraper):
    schema_slugs = (FOOD_SLUG, POOL_SLUG)
    has_detail = False

    def __init__(self, *args, **kwargs):
        super(Scraper, self).__init__(*args, **kwargs)
        # The file is around 41MB (as of 8/08) and takes awhile to start
        # downloading, so wait more than 20 seconds for it.
        self.retriever = Retriever(timeout=300)

    def list_pages(self):
        # There's only one page of data, so just return it as a list with a
        # single item.
        uri = ''
        data = self.get_html(uri)
        return [data]

    def parse_list(self, page):
        # Remove null bytes
        page = re.sub(r'\0', r' ', page)
        # Remove sequences of '''''''
        page = re.sub(r"'+", "'", page)
        reader = csv.DictReader(StringIO(page), quoting=csv.QUOTE_ALL, escapechar='\\')
        # There is one row in the data for each violation, not just each
        # inspection. Violations from the same inspection will be contiguous,
        # so roll up the violations until we see a different inspection.
        current_record = None
        for row in reader:
            if row['CITY'] != 'CHARLOTTE':
                continue
            row['comments'] = []
            # Strip any leading zeros. Both 01 and 1 appear sometimes, but
            # they mean the same thing.
            item_id = row['ITEM_NUM'].lstrip('0')
            violation = {'id': item_id, 'value': row['ITEM_VALUE'], 'comment': row['COMMENT']}
            if current_record is None:
                current_record = row
                current_record['violation'] = [violation]
            elif current_record['FAC_NAME'] != row['FAC_NAME'] or current_record['DATE'] != row['DATE']:
                yield current_record
                current_record = row
                current_record['violation'] = [violation]
            else:
                current_record['violation'].append(violation)
        # The final record won't be yielded from the loop above because it has
        # no following record to trigger it, so yield it here.
        yield current_record

    def clean_list_record(self, record):
        # The facility type is determied by the 6th and 7th digits in the
        # facility ID.
        facility_type = record['FAC_ID'][5:7]
        if facility_type in EXCLUDED_FACILITY_TYPES:
            raise SkipRecord('Excluding record from facility type %s' % facility_type)
        record['DATE'] = parse_date(record['DATE']).date()
        record['FAC_NAME'] = record['FAC_NAME'].decode('Latin-1')
        record['FIN_SCORE'] = float(record['FIN_SCORE']) + float(record['RAW_SCORE'])
        record['schema_slug'] = self.get_schema_slug(record)
        record['facility_type'] = self.clean_facility_type(facility_type)
        record['result'] = self.get_result(record)
        return record

    def clean_facility_type(self, facility_type):
        if FACILITY_TYPES.has_key(facility_type):
            return FACILITY_TYPES[facility_type]['name']
        return 'Unknown'

    def get_schema_slug(self, record):
        # The facility type is determied by the 6th and 7th digits in the
        # facility ID.
        value = record['FAC_ID'][5:7]
        return FACILITY_TYPES[value]['schema']

    def get_result(self, record):
        # Set pass/fail
        if record['TYPE_ACT'] in ['Status Change', 'Visit', 'CV Visit', 'CV Follow-up']:
            return 'N/A'
        if record['schema_slug'] == FOOD_SLUG:
            if record['facility_type'] in ['Mobile Food Units', 'Pushcarts']:
                if record['CLASSIFICATION'] == 'Approved':
                    result = 'Pass'
                elif record['CLASSIFICATION'] == 'Dispproved':
                    result = 'Fail'
                else:
                    result = 'Unknown'
            else:
                if record['FIN_SCORE'] in (None, ''):
                    result = 'Unknown'
                elif record['FIN_SCORE'] >= 70.0:
                    result = 'Pass'
                else:
                    result = 'Fail'
        elif record['schema_slug'] == POOL_SLUG:
            if record['ACT_PSC'] in ['A', 'I']:
                result = 'Pass'
            elif record['ACT_PSC'] in ['E', 'W']:
                result = 'Fail'
        return result

    def existing_record(self, record):
        schema_slug = record['schema_slug']
        schema = self.schemas[schema_slug]
        if not isinstance(record['DATE'], datetime.date):
            return None
        qs = NewsItem.objects.filter(schema__id=schema.id, item_date=record['DATE'])
        qs = qs.by_attribute(self.schema_fields[schema_slug]['name'], record['FAC_NAME'])
        try:
            return qs[0]
        except IndexError:
            return None

    def save(self, old_record, list_record, detail_record):
        if list_record['TYPE_ACT'] in ('Visit', 'CV Visit', 'Status Change', 'CV Follow-Up'):
            return

        schema_slug = list_record['schema_slug']
        schema = self.schemas[schema_slug]

        result_lookup = self.get_or_create_lookup('result', list_record['result'], list_record['result'], schema=schema_slug)
        facility_type_lookup = self.get_or_create_lookup('facility_type', list_record['facility_type'], list_record['facility_type'], schema=schema_slug, make_text_slug=False)
        facility_status_lookup = self.get_or_create_lookup('facility_status', list_record['ACT_PSC'], list_record['ACT_PSC'], schema=schema_slug, make_text_slug=False)
        classification_lookup = self.get_or_create_lookup('classification', list_record['CLASSIFICATION'], list_record['CLASSIFICATION'], schema=schema_slug, make_text_slug=False)
        action_type_lookup = self.get_or_create_lookup('action_type', list_record['TYPE_ACT'], list_record['TYPE_ACT'], schema=schema_slug, make_text_slug=False)

        if schema_slug == FOOD_SLUG:
            if list_record['DATE'] >= datetime.date(2008, 7, 1):
                prefix = '2.'
            else:
                prefix = '1.'
        else:
            prefix = ''
        v_type_lookup_list = []
        v_list = []
        for v in list_record['violation']:
            v_type_lookup = self.get_or_create_lookup('violation', prefix + v['id'], prefix + v['id'], schema=schema_slug, make_text_slug=False)
            v_type_lookup_list.append(v_type_lookup)
            v_list.append({'lookup_id': v_type_lookup.id, 'value': v['value'], 'comment': v['comment']})
        violations_json = DjangoJSONEncoder().encode(v_list)

        title = list_record['FAC_NAME']
        address = ' '.join([list_record['ADDR1'], list_record['ADDR2']])
        attributes = {
            'name': list_record['FAC_NAME'],
            'facility_type': facility_type_lookup.id,
            'facility_status': facility_status_lookup.id,
            'raw_score':list_record['RAW_SCORE'],
            'final_score': list_record['FIN_SCORE'],
            'result': result_lookup.id,
            'classification': classification_lookup.id,
            'facility_id': list_record['FAC_ID'],
            'violation': ','.join([str(v.id) for v in v_type_lookup_list]),
            'violation_detail': violations_json,
            'action_type': action_type_lookup.id
        }
        values = {
            'schema': schema,
            'title': title,
            'item_date': list_record['DATE'],
            'location_name': smart_title(address),
        }
        if old_record is None:
            self.create_newsitem(attributes, **values)
        else:
            self.update_existing(old_record, values, attributes)

if __name__ == '__main__':
    from ebdata.retrieval import log_debug
    Scraper().update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Screen scraper for Charlotte police "significant events."
http://maps.cmpdweb.org/significanteventlog/
"""

from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date
import re

SOURCE_URL = 'http://maps.cmpdweb.org/cfs/Default.aspx'

class SignificantEventScraper(NewsItemListDetailScraper):
    schema_slugs = ('significant-police-events',)
    has_detail = False
    parse_list_re = re.compile(r'Incident Address</td><td[^>]*>&nbsp;</td><td[^>]*>(?P<address>[^>]*)</td><td[^>]*>Division</td><td[^>]*>(?P<division>[^>]*)</td></tr><tr[^>]*><td[^>]*>(?P<incident_date>[^>]*)</td><td[^>]*>(?P<incident_time>[^>]*)</td><td[^>]*>&nbsp;</td><td[^>]*>(?P<incident_type>[^>]*)</td><td[^>]*>(?P<complaint_number>[^>]*)</td><td[^>]*></td><td[^>]*>(?P<officer>[^>]*)</tr><tr[^>]*><td[^>]*>(?P<description>[^>]*)</td>', re.IGNORECASE | re.DOTALL)

    def list_pages(self):
        yield self.get_html('http://maps.cmpdweb.org/significanteventlog/')

    def clean_list_record(self, record):
        item_datetime = parse_date('%s_%s' % (record['incident_date'].strip(), record['incident_time'].strip()), '%m/%d/%Y_%H:%M', return_datetime=True)
        record['item_date'] = item_datetime.date()
        record['item_time'] = item_datetime.time()
        record['description'] = record['description'].strip()
        return record

    def existing_record(self, record):
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id)
            qs = qs.by_attribute(self.schema_fields['complaint_number'], record['complaint_number'])
            return qs[0]
        except IndexError:
            return None

    def save(self, old_record, list_record, detail_record):
        if old_record is not None:
            self.logger.debug('Record already exists')
            return

        division = self.get_or_create_lookup('division', list_record['division'], list_record['division'])
        incident_type = self.get_or_create_lookup('incident_type', list_record['incident_type'], list_record['incident_type'])

        attributes = {
            'division': division.id,
            'incident_type': incident_type.id,
            'complaint_number': list_record['complaint_number'],
            'description': list_record['description'],
            'officer': list_record['officer'],
            'incident_time': list_record['item_time'],
        }
        self.create_newsitem(
            attributes,
            title=incident_type.name,
            item_date=list_record['item_date'],
            location_name=list_record['address'],
        )

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    SignificantEventScraper().update()

########NEW FILE########
__FILENAME__ = utils
from ebdata.retrieval.scrapers.base import ScraperBroken
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from dateutil.relativedelta import relativedelta
import csv
import datetime
import time
import re
from cStringIO import StringIO

LOGIN_URI = 'http://dwexternal.co.mecklenburg.nc.us/ids/login.aspx?ReturnUrl=/ids/&AcceptsCookies=1'
USERNAME = ''
PASSWORD = ''

class MecklenburgScraper(NewsItemListDetailScraper):
    """
    Common functionality for scraping data from http://dwexternal.co.mecklenburg.nc.us
    """
    has_detail = False

    def __init__(self, start_date=None, *args, **kwargs):
        # if a start date isn't provided, start scraping data starting 7 days ago
        self.start_date = start_date or datetime.date.today() - relativedelta(days=7)
        super(MecklenburgScraper, self).__init__(*args, **kwargs)

    def login(self):
        html = self.get_html(LOGIN_URI)
        m = re.search(r'<input type="hidden" name="__VIEWSTATE" value="([^"]*)"', html)
        if not m:
            raise ScraperBroken('VIEWSTATE not found')
        viewstate = m.group(1)
        html = self.get_html(LOGIN_URI, {
            '__EVENTTARGET': '',
            '__EVENTARGUMENT': '',
            '__VIEWSTATE': viewstate,
            'btn_Submit': 'Login',
            'Remember_Password': 'on',
            'User_Name': USERNAME,
            'Password': PASSWORD,
            '_ctl1:QUERY': '',
            'fmt': 'standard'
        }, follow_redirects=False)

    def get_html(self, *args, **kwargs):
        if self.retriever._cookies:
            # Build the Cookie header manually. We get:
            #   socket.error: (54, 'Connection reset by peer')
            # if we send newline separated cookies. Semicolon separated works fine.
            cookie = self.retriever._cookies.output(attrs=[], header='', sep=';').strip()
            kwargs['send_cookies'] = False
            kwargs['headers'] = {'Cookie': cookie}
        return super(MecklenburgScraper, self).get_html(*args, **kwargs)

    def get_viewstate(self, uri=None):
        uri = uri or self.root_uri
        html = self.get_html(self.root_uri)
        m = re.search(r'<input type="hidden" name="__VIEWSTATE" value="([^"]*)"', html)
        if not m:
            raise ScraperBroken('VIEWSTATE not found')
        return m.group(1)

    def date_pairs(self):
        d = self.start_date
        while d < datetime.date.today():
            start = d.strftime('%m/%d/%Y')
            end = (d + relativedelta(days=6)).strftime('%m/%d/%Y')
            d += relativedelta(days=7)
            yield (start, end)

    def list_pages(self):
        viewstate = self.get_viewstate()
        for start, end in self.date_pairs():
            args = self.search_arguments(start, end, viewstate)
            yield self.get_html(self.root_uri, data=args)
            time.sleep(10) # Be nice to their servers.

    def parse_list(self, page):
        reader = csv.DictReader(StringIO(page))
        for row in reader:
            yield row


########NEW FILE########
__FILENAME__ = retrieval
"""
Scraper for Chicago bike racks.

http://www.chicagobikes.org/kml/bikeracks.php
"""

from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date
from ebpub.utils.text import clean_address
from lxml import etree
import re

class Scraper(NewsItemListDetailScraper):
    schema_slugs = ['bike-racks']
    has_detail = False
    sleep = 1
    rack_id_re = re.compile(r"http://www\.chicagobikes\.org/bikeparking/rackinfo\.php\?id=(\d+)")
    rack_count_re = re.compile(r'(\d+) bike rack')

    def list_pages(self):
        yield self.get_html('')

    def parse_list(self, kml):
        kml = kml.replace('&eacute;', '&#233;')
        kml = kml.replace('UTF-8', 'ISO-8859-2')
        tree = etree.fromstring(kml)
        ns = 'http://earth.google.com/kml/2.1'
        cdot_ns = 'http://www.chicagobikes.org/data'
        for pm in tree.findall('.//{%s}Placemark' % ns):
            description = pm.find('{%s}description' % ns).text
            rack_id = self.rack_id_re.search(description).group(1)
            m = self.rack_count_re.search(description)
            if m:
                rack_count = m.group(1)
            else:
                # Lately the data hasn't been displaying a number:
                # "plural bike racks located here". Just use None in
                # that case.
                rack_count = None
            record = {
                'address': pm.find('{%s}name' % ns).text,
                'installation_date': pm.find('{%s}TimeStamp/{%s}when' % (ns, ns)).text,
                'rack_id': rack_id,
                'rack_count': rack_count,
                'url': 'http://www.chicagobikes.org/bikeparking/rackinfo.php?id=%s' % rack_id
            }
            locname_el =  pm.find('{%s}ExtendedData/{%s}locName' % (ns, cdot_ns))
            if locname_el is not None:
                record['place_name'] = locname_el.text
            yield record

    def clean_list_record(self, list_record):
        list_record['installation_date'] = parse_date(list_record['installation_date'], '%Y-%m-%d')
        try:
            list_record['rack_count'] = int(list_record['rack_count'])
        except TypeError:
            pass
        return list_record

    def existing_record(self, list_record):
        qs = NewsItem.objects.filter(schema__id=self.schema.id)
        qs = qs.by_attribute(self.schema_fields['rack_id'], list_record['rack_id'])
        try:
            return qs[0]
        except IndexError:
            return None

    def save(self, old_record, list_record, detail_record):
        address = clean_address(list_record['address'])
        attributes = {
            'place_name': list_record.get('place_name', ''),
            'rack_id': list_record['rack_id'],
            'rack_count': list_record['rack_count']
        }
        values = {
            'title': 'Bike rack installed near %s' % list_record.get('place_name', address),
            'item_date': list_record['installation_date'],
            'location_name': address,
            'url': list_record['url']
        }
        if old_record is None:
            self.create_newsitem(attributes, **values)
        else:
            self.update_existing(old_record, values, attributes)

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    Scraper().update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Import script for Chicago building permits.

This data is imported from an Excel file, which is e-mailed to us every month.

Note: As of 2008-04-25, the Excel file is in an older Excel format that isn't
supported by our Excel-parsing library. (XLRDError: "BIFF version 2 is not
supported".) To fix this, open the file in Excel and save it; you'll be
prompted to save it to a newer version.
"""

from ebdata.parsing.excel import ExcelDictReader
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date
from ebpub.utils.text import smart_title
import re

class BuildingPermitImporter(NewsItemListDetailScraper):
    schema_slugs = ('building-permits',)
    has_detail = False

    def __init__(self, excel_file_name, *args, **kwargs):
        super(BuildingPermitImporter, self).__init__(*args, **kwargs)
        self.excel_file_name = excel_file_name

    def list_pages(self):
        reader = ExcelDictReader(self.excel_file_name, sheet_index=0, header_row_num=0, start_row_num=1)
        yield reader

    def parse_list(self, reader):
        for row in reader:
            yield row

    def clean_list_record(self, record):
        issue_datetime = parse_date(record['issdttm'], '%m/%d/%Y %H:%M:%S', return_datetime=True)
        record['issue_date'] = issue_datetime.date()
        record['issue_time'] = issue_datetime.time()
        record['clean_address'] = '%s %s. %s %s.' % (record['stno'], record['predir'],
            smart_title(record['stname']), smart_title(record['suffix']))
        record['clean_permit_type'] = smart_title(re.sub(r'^PERMIT - ', '', record['apdesc']))
        try:
            record['description'] = record['compute_0009']
        except KeyError:
            record['description'] = record['permit_description']
        return record

    def existing_record(self, record):
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id)
            qs = qs.by_attribute(self.schema_fields['application_number'], record['apno'])
            return qs[0]
        except IndexError:
            return None

    def save(self, old_record, list_record, detail_record):
        if old_record is not None:
            return
        work_type = self.get_or_create_lookup('work_type', list_record['clean_permit_type'], list_record['aptype'])
        title = 'Permit issued for %s' % work_type.name.lower()
        attributes = {
            'application_number': list_record['apno'],
            'work_type': work_type.id,
            'description': list_record['description'],
            'estimated_value': list_record['declvltn'],
            'issue_time': list_record['issue_time'],
        }
        self.create_newsitem(
            attributes,
            title=title,
            item_date=list_record['issue_date'],
            location_name=list_record['clean_address'],
        )

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    import sys
    importer = BuildingPermitImporter(sys.argv[1])
    importer.update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Screen scraper for City of Chicago Business License Holders data.
http://webapps.cityofchicago.org/lic/iris.jsp
"""

from ebdata.retrieval.scrapers.list_detail import SkipRecord
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date
from ebpub.utils.text import clean_address
import re

strip_tags = lambda x: re.sub(r'(?s)</?[^>]*>', '', x).replace('&nbsp;', ' ').strip()

# List of DBAs to skip, due to privacy reasons or whatever else.
SKIPPED_DBAS = set([
    'SHALVA', # Domestic abuse center; sensitive address
])

class BusinessLicense(NewsItemListDetailScraper):
    schema_slugs = ('business-licenses',)
    parse_list_re = re.compile(r'(?si)<tr>\s*<td width="115"[^>]*>(?P<address>.*?)</td>\s*<td[^>]*>(?P<dba>.*?)</td>\s*<td[^>]*>(?P<structure>.*?)</td>\s*<td[^>]*>(?P<ward>.*?)</td>\s*<td[^>]*>(?P<precinct>.*?)</td>\s*<td[^>]*>.*?</td>\s*<td[^>]*>.*?callLic\((?P<city_id>\d+),(?P<site_id>\d+)')

    def __init__(self, wards=None):
        super(BusinessLicense, self).__init__()
        self.wards = wards or xrange(1, 51)

    def list_pages(self):
        next_offset = re.compile(r'(?si)<a href="irisresult\.jsp\?firstRow=(\d+)&newSearch=f">Next\s+Page').search
        for ward in self.wards:
            # For the first page, submit a search.
            data = {'str_nbr': '', 'str_nbr2': '', 'str_direction': '', 'str_nm': '', 'str_type_cde': '',
                    'dba_txt': '', 'ward_cde': str(ward), 'precinct_cde': ''}
            html = self.get_html('http://webapps.cityofchicago.org/lic/irisresult.jsp?newSearch=t', data)

            # For subsequent pages, just submit the row offset. The rest of
            # the search is saved in a cookie. Get the row offset by parsing
            # each result page.
            while 1:
                yield html
                m = next_offset(html)
                if not m:
                    break # No more "Next page" link.
                offset = m.group(1)
                html = self.get_html('http://webapps.cityofchicago.org/lic/irisresult.jsp?firstRow=%s&newSearch=f' % offset)

    def clean_list_record(self, record):
        for k, v in record.items():
            v = strip_tags(v)
            record[k] = re.sub(r'(?s)\s\s+', ' ', v).strip()

        # Remove the "Suite/Apt" or "Floor" clause from the address, if it exists.
        record['address'] = record['address'].replace(' ,', ',')
        m = re.search(r'^(.*?), (?:Suite/Apt|Floor):.*$', record['address'])
        if m:
            record['address'] = m.group(1)
        record['address'] = clean_address(record['address'])

        if record['dba'] in SKIPPED_DBAS:
            raise SkipRecord('Skipping %r' % record['dba'])

        # For privacy reasons, skip individuals.
        if record['structure'].upper().strip() == 'INDIVIDUAL':
            raise SkipRecord('Skipping structure=individual')

        record['city_id'] = int(record['city_id'])
        record['site_id'] = int(record['site_id'])
        return record

    def existing_record(self, record):
        qs = NewsItem.objects.filter(schema__id=self.schema.id)
        qs = qs.by_attribute(self.schema_fields['city_id'], record['city_id'])
        qs = qs.by_attribute(self.schema_fields['site_id'], record['site_id'])
        records = {}
        for ni in qs:
            # We need the license type and both dates to uniquely identify a
            # particular license for a business.
            key = (ni.attributes['license'], ni.item_date, ni.attributes['expiration_date'])
            records[key] = ni
        return records

    def detail_required(self, list_record, old_record):
        return True # Detail is always required, unfortunately.

    def get_detail(self, record):
        return self.get_html('http://webapps.cityofchicago.org/lic/irislic.jsp?acct_nbr=%s&site_nbr=%s' % \
            (record['city_id'], record['site_id']))

    def parse_detail(self, page, list_record):
        licenses = []
        for record in re.finditer(r'(?si)<tr>\s*<td[^>]*>(?P<license>.*?)</td>\s*<td[^>]*>(?P<issue_date>.*?)</td>\s*<td[^>]*>(?P<expiration_date>.*?)</td>', page):
            data = record.groupdict()

            # For privacy reasons, skip street performers and peddlers.
            normalized_license = data['license'].upper().strip()
            if (normalized_license == 'STREET PERFORMER') or ('PEDDLER' in normalized_license):
                continue

            for k, v in data.items():
                data[k] = strip_tags(v)
            licenses.append(data)
        return {'licenses': licenses[1:]} # Skip first row (the header)

    def clean_detail_record(self, record):
        for i, license in enumerate(record['licenses']):
            license['issue_date'] = license['issue_date'] != 'null' and parse_date(license['issue_date'], '%m/%d/%Y') or None
            license['expiration_date'] = license['expiration_date'] != 'null' and parse_date(license['expiration_date'], '%m/%d/%Y') or None
            if license['issue_date'] is None:
                del record['licenses'][i] # If the issue date is empty, just drop it.
        return record

    def save(self, old_record, list_record, detail_record):
        structure = self.get_or_create_lookup('structure', list_record['structure'], list_record['structure'])
        title = list_record['dba']

        for license in detail_record['licenses']:
            license_type = self.get_or_create_lookup('license', license['license'], license['license'])
            if license_type.name.upper().strip() == 'HOME OCCUPATION':
                continue # Skip "Home Occupation"
            new_attributes = {
                'license': license_type.id,
                'expiration_date': license['expiration_date'],
                'structure': structure.id,
                'dba': list_record['dba'],
                'city_id': list_record['city_id'],
                'site_id': list_record['site_id'],
            }

            key = (license_type.id, license['issue_date'], license['expiration_date'])
            if not old_record.has_key(key) \
                    or old_record[key].item_date != license['issue_date'] \
                    or old_record[key].attributes['expiration_date'] != license['expiration_date']:
                self.create_newsitem(
                    new_attributes,
                    title=title,
                    description=u'The business was issued a license of type "%s".' % license_type.name,
                    url='http://webapps.cityofchicago.org/lic/iris.jsp',
                    item_date=license['issue_date'],
                    location_name=list_record['address'],
                )
            else:
                # This business already exists in our database, but check
                # whether any of the values have changed.
                new_values = {'title': title, 'location_name': list_record['address']}
                self.update_existing(old_record[key], new_values, new_attributes)

if __name__ == "__main__":
    s = BusinessLicense()
    s.update()

########NEW FILE########
__FILENAME__ = retrieval
from ebdata.blobs.scrapers import IncrementalCrawler
import re

class ChicagoCityPressReleaseCrawler(IncrementalCrawler):
    schema = 'city-press-releases'
    seed_url = 'http://egov.cityofchicago.org/'
    date_headline_re = re.compile(r'(?s)E-mail: <a href="mailto:[^"]*">.*?</a><br>\s*(?P<article_date>.*?)\s*</td>\s*</tr>\s*<tr>\s*<td align="center" class="bodytextbold">(?P<article_headline>.*?)</td>')
    date_format = '%A, %B %d, %Y'
    max_blanks = 60

    def public_url(self, id_value):
        return 'http://egov.cityofchicago.org/city/webportal/portalContentItemAction.do?contenTypeName=COC_EDITORIAL&topChannelName=HomePage&contentOID=%s' % id_value

    def retrieval_url(self, id_value):
        return 'http://egov.cityofchicago.org/city/webportal/jsp/content/showNewsItem.jsp?print=true&contenTypeName=1006&contentOID=%s' % id_value

    def id_for_url(self, url):
        return url.split('contentOID=')[1]

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    ChicagoCityPressReleaseCrawler().update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Screen scraper for Chicago Police CLEARmap crime site
http://gis.chicagopolice.org/
"""

from django.contrib.gis.geos import Point
from ebdata.retrieval.scrapers.base import ScraperBroken
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from ebpub.utils.text import smart_title
import datetime
import re
import time
from xml.dom import minidom

class CrimeScraper(NewsItemListDetailScraper):
    schema_slugs = ('crime',)
    has_detail = False

    def __init__(self, start_date=None, end_date=None):
        super(CrimeScraper, self).__init__()
        self.start_date, self.end_date = start_date, end_date

    def call_clearpath(self, where, offset=0, limit=2000):
        """
        Makes a request to the CPD site with the given WHERE clause (a list)
        and offset/limit. Returns a DOM object of the parsed XML.

        Note that the maximum limit is 2000. All results will always return
        at most 2000 records.
        """
        # Example valid WHERE clauses:
        #     GIS.clearMap_crime_90days.DATEOCC between {ts &apos;2007-09-01 00:00:00&apos;} AND {ts &apos;2007-09-01 23:59:59&apos;}
        #     GIS.clearMap_crime_90days.DATEOCC &gt;= {ts &apos;2007-09-01&apos;}
        # Good documentation is available here:
        #     http://edndoc.esri.com/arcims/9.2/elements/get_features.htm
        xml_request = """
            <?xml version="1.0" encoding="UTF-8" ?>
            <ARCXML VERSION="1.1">
            <REQUEST>
                <GET_FEATURES outputmode="xml" geometry="true" globalenvelope="false" envelope="false" compact="true" beginrecord="%(offset)s" featurelimit="%(limit)s">
                    <LAYER id="999" type="featureclass">
                        <DATASET name="GIS.clearMap_crime_90days" type="point" workspace="sde_ws-1"  />
                    </LAYER>
                    <SPATIALQUERY where="%(where)s" subfields="#ALL#"></SPATIALQUERY>
                </GET_FEATURES>
            </REQUEST>
            </ARCXML>""" % {'where': ' AND '.join(where), 'offset': offset, 'limit': limit}
        data = {
            'ArcXMLRequest': xml_request.strip(),
            'JavaScriptFunction': 'parent.MapFrame.processXML',
            'BgColor': '#000000',
            'FormCharset': 'ISO-8859-1',
            'RedirectURL': '',
            'HeaderFile': '',
            'FooterFile': '',
        }
        url = 'http://gis.chicagopolice.org/servlet/com.esri.esrimap.Esrimap?ServiceName=clearMap&CustomService=Query&ClientVersion=4.0&Form=True&Encode=False'
        html = self.get_html(url, data)

        # The resulting HTML has some XML embedded in it. Extract that.
        m = re.search(r"var XMLResponse='(.*?)';\s*parent\.MapFrame\.processXML", html)
        if not m:
            raise ScraperBroken('"var XMLResponse" XML not found')
        raw_xml = m.group(1)

        # Clean invalid XML --
        # Attributes that start with "#".
        raw_xml = raw_xml.replace('#', 'Z')
        # Unescaped ampersands.
        raw_xml = re.sub(r'&(?!amp;)', '&amp;', raw_xml)
        # Unescaped '<' signs (shows up in "<18" in attributes).
        raw_xml = raw_xml.replace(r'<18', '&lt;18')

        return minidom.parseString(raw_xml)

    def list_pages(self):
        # Note that this method yields XML objects, not strings, because
        # it parses the XML in order to determine the pagination needs.
        if self.start_date and self.end_date:
            where = ['GIS.clearMap_crime_90days.DATEOCC between {ts &apos;%s 00:00:00&apos;} AND {ts &apos;%s 23:59:59&apos;}' % (self.start_date, self.end_date)]
        else:
            where = []
        offset, limit = 0, 2000
        while 1:
            xml = self.call_clearpath(where, offset=offset, limit=limit)
            yield xml
            # Keep paginating and loading pages until <FEATURECOUNT hasmore="false">.
            if xml.getElementsByTagName('FEATURECOUNT')[0].getAttribute('hasmore') == 'false':
                break
            offset += limit

    def parse_list(self, xml):
        # Note that the argument is XML because list_pages() returns XML
        # objects, not strings.

        # Maps our field name to the data column name.
        xml_attributes = (
            ('beat', 'BEAT_NUM'),
            ('secondary_type_id', 'CURR_IUCR'),
            ('crime_datetime', 'DATEOCC'),
            ('secondary_type', 'DESCRIPTION'),
            ('primary_type', 'PRIMARY'),
            ('domestic', 'DOMESTIC_I'),
            ('fbi_cd', 'FBI_CD'),
            ('fbi_type', 'FBI_DESCR'),
            ('place', 'LOCATION_DESCR'),
            # ('police_id', 'OBJECTID'), # We don't save the police ID, because it appears to change randomly.
            ('case_number', 'RD'),
            ('status', 'STATUS'),
            ('addr_direction', 'STDIR'),
            ('addr_block', 'STNUM'),
            ('addr_street', 'STREET'),
            ('ward', 'WARD'),
        )

        for report in xml.getElementsByTagName('FEATURE'):
            crime = report.getElementsByTagName('FIELDS')[0]
            x_coord, y_coord = report.getElementsByTagName('COORDS')[0].childNodes[0].nodeValue.split(' ')
            yield dict(
                [('x_coord', x_coord), ('y_coord', y_coord)] + \
                [(att, crime.getAttribute('GIS.clearMap_crime_90days.%s' % col)) for att, col in xml_attributes]
            )

    def clean_list_record(self, record):
        # Convert the 'DATEOCC' field to a Python date object.
        dt = datetime.datetime(*time.gmtime(int(record.pop('crime_datetime')) / 1000)[:6])
        record['crime_date'] = dt.date()
        record['crime_time'] = dt.time()
        record['domestic'] = (record['domestic'] == 'Y')
        record['x_coord'] = float(record['x_coord'])
        record['y_coord'] = float(record['y_coord'])
        return record

    def existing_record(self, record):
        try:
            return NewsItem.objects.filter(schema__id=self.schema.id).by_attribute(self.schema_fields['case_number'], record['case_number'])[0]
        except IndexError:
            return None

    def save(self, old_record, list_record, detail_record):
        secondary_type = self.get_or_create_lookup('secondary_type', list_record['secondary_type'], list_record['secondary_type_id'])
        real_address = '%s %s %s' % (list_record['addr_block'], list_record['addr_direction'], list_record['addr_street'])
        block_address = '%s00 block %s. %s' % (list_record['addr_block'][:-2], list_record['addr_direction'], smart_title(list_record['addr_street']))

        crime_location = Point(list_record['x_coord'], list_record['y_coord'], srid=102671)
        crime_location = self.safe_location(real_address, crime_location, 375)

        new_attributes = {
            'is_outdated': False,
            'case_number': list_record['case_number'],
            'crime_time': list_record['crime_time'],
            'primary_type': self.get_or_create_lookup('primary_type', list_record['primary_type'], list_record['primary_type']).id,
            'secondary_type': secondary_type.id,
            'place': self.get_or_create_lookup('place', list_record['place'], list_record['place']).id,
            'beat': self.get_or_create_lookup('beat', list_record['beat'], list_record['beat']).id,
            'domestic': list_record['domestic'],
            'xy': '%s;%s' % (list_record['x_coord'], list_record['y_coord']),
            'real_address': real_address,
        }
        if old_record is None:
            self.create_newsitem(
                new_attributes,
                title=secondary_type.name,
                url='http://gis.chicagopolice.org/',
                item_date=list_record['crime_date'],
                location=crime_location,
                location_name=block_address,
            )
        else:
            # This crime already exists in our database, but check whether any
            # of the values have changed.
            new_values = {'title': secondary_type.name, 'item_date': list_record['crime_date'], 'location_name': block_address}
            self.update_existing(old_record, new_values, new_attributes)

def update_current():
    """
    Runs the crime updater for the latest 7 days and oldest 3 days.
    """
    today = datetime.date.today()

    # The latest date with available crimes is 8 days ago, but that might not
    # have *every* crime for that day, so we use 9 days ago.
    latest_available = today - datetime.timedelta(9)

    # The oldest date with available crimes is 97 days ago.
    oldest_available = today - datetime.timedelta(97)

    # Update the latest 7 days and the oldest 3 days.
    # The reasoning here is that crime data changes, so we can't just scrape
    # each day once and never check it again -- but we also don't want to have
    # to scrape *every* available day every day. So we compromise and scrape
    # the latest 7 days and the oldest 3 days.
    s = CrimeScraper(latest_available - datetime.timedelta(6), latest_available)
    s.update()
    s = CrimeScraper(oldest_available, oldest_available + datetime.timedelta(2))
    s.update()

if __name__ == "__main__":
    update_current()

########NEW FILE########
__FILENAME__ = retrieval
"""
Import script for Chicago film locations data.

This data is imported from a CSV file, which is generated from an Excel file
that the film office sends us.

IMPORTANT NOTE: The script doesn't check whether a filming exists in the
database yet, so ensure that you don't import the same data twice.
"""

from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.utils.dates import parse_date
from ebpub.utils.text import smart_title
from csv import DictReader

class FilmingImporter(NewsItemListDetailScraper):
    schema_slugs = ("filmings",)
    has_detail = False

    def __init__(self, csvfile, *args, **kwargs):
        super(FilmingImporter, self).__init__(*args, **kwargs)
        self.csvfile = csvfile

    def list_pages(self):
        return [DictReader(self.csvfile)]

    def parse_list(self, reader):
        for row in reader:
            yield row

    def clean_list_record(self, record):
        try:
            record['Date'] = parse_date(record['Date'], '%m/%d/%Y') # 12/31/2007
        except ValueError:
            record['Date'] = parse_date(record['Date'], '%m/%d/%y') # 12/31/07
        for key in ('Location', 'Notes', 'Title', 'Type'):
            if key in record and record[key]:
                record[key] = record[key].strip()
            else:
                record[key] = ''

        record['Location'] = smart_title(record['Location'])
        record['Title'] = smart_title(record['Title'])

        # This is temporary! The CSV files we get are inconsistent -- sometimes
        # they're only films and don't have a "Type" field.
        if record['Type'] == '':
            record['Type'] = 'Film'

        # Normalize inconsistent data.
        if record['Type'] in ('Stills', 'Still'):
            record['Type'] = 'Still photography'
        if record['Type'] in ('Fim', 'Movie'):
            record['Type'] = 'Film'

        return record

    def existing_record(self, record):
        # Assume that none of this data exists in the database yet.
        return None

    def save(self, old_record, list_record, detail_record):
        filming_type = self.get_or_create_lookup('type', list_record['Type'], list_record['Type'])
        film_title = self.get_or_create_lookup('title', list_record['Title'], list_record['Title'])
        newsitem_title = u'%s "%s" filmed' % (filming_type.name, list_record['Title'])
        attributes = {
            'title': film_title.id,
            'type': filming_type.id,
            'notes': list_record['Notes'],
        }
        self.create_newsitem(
            attributes,
            title=newsitem_title,
            item_date=list_record['Date'],
            location_name=list_record['Location'],
        )

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    import sys
    filename = sys.argv[1]
    importer = FilmingImporter(open(filename, 'rb'))
    importer.update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Importer for Chicago foreclosure data from the Woodstock Institute.

The data is in a ZIP file on a site protected with HTTP authentication.
"""

from ebdata.parsing import dbf, excel
from ebdata.retrieval.scrapers.list_detail import SkipRecord
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.geocoder.parser.parsing import strip_unit
from ebpub.utils.dates import parse_date
from ebpub.utils.text import address_to_block, clean_address
from cStringIO import StringIO
from tempfile import mkstemp
import base64
import os
import zipfile

USERNAME = ''
PASSWORD = ''

def strip_dict(d):
    for k, v in d.items():
        if isinstance(v, basestring):
            d[k] = v.strip()

class WoodstockScraper(NewsItemListDetailScraper):
    """
    Generic base class for the two Woodstock schemas.
    """
    has_detail = False

    source_url = 'http://www.woodstockinst.org/3683123610/junetooct.zip'

    def __init__(self, filename=None):
        self.woodstock_filename = filename
        NewsItemListDetailScraper.__init__(self)

    def list_pages(self):
        if self.woodstock_filename:
            yield open(self.woodstock_filename).read()
        else:
            # Our scraper infrastructure doesn't have a nice API to handle
            # usernames/passwords in basic HTTP authentication, so we have to
            # do it manually by passing in a base64 header.
            # See http://en.wikipedia.org/wiki/Basic_access_authentication
            auth_header = 'Basic %s' % base64.encodestring('%s:%s' % (USERNAME, PASSWORD)).strip()
            yield self.get_html(self.source_url, headers={'Authorization': auth_header})

    def parse_list(self, raw_zip_data):
        # The input is a ZIP file full of directories and/or files. Files can
        # be ZIP, DBF or XLS.
        zf = zipfile.ZipFile(StringIO(raw_zip_data))
        for zi in zf.filelist:
            if zi.file_size == 0:
                continue # Skip directories.
            if zi.filename.lower().endswith('.zip'):
                for data in self.parse_list(zf.read(zi.filename)):
                    yield data
            elif zi.filename.lower().endswith('.dbf'):
                try:
                    reader = dbf.dict_reader(StringIO(zf.read(zi.filename)))
                    for row in reader:
                        yield row
                except ValueError:
                    self.logger.warn('Skipping file %r: could not be parsed as DBF', zi.filename)
            elif zi.filename.lower().endswith('.xls'):
                # The Excel parser requires that the file be on the filesystem,
                # so write out a temp file.
                fd, filename = mkstemp()
                fp = os.fdopen(fd, 'wb')
                fp.write(zf.read(zi.filename))
                fp.close()

                # The workbook might have multiple worksheets, so we loop over
                # the ones we care about (by checking the worksheet's name
                # against self.excel_sheet_name).
                reader = excel.ExcelDictReader(filename, header_row_num=0, start_row_num=1)
                sheet_indexes = [sheet.number for sheet in reader.workbook.sheets() if self.excel_sheet_name == sheet.name.lower()]
                for index in sheet_indexes:
                    reader.sheet_index = index
                    for row in reader:
                        yield row
            else:
                self.logger.warn('Got unknown file type: %r', zi.filename)

    def clean_list_record(self, record):
        strip_dict(record)
        try:
            record['filing_date'] = parse_date(str(int(record['filing_dat'])), '%m%d%y')
        except ValueError:
            record['filing_date'] = None
        if record['filing_date'] is None:
            self.logger.info('Skipping invalid filing date %r', record['filing_dat'])
            raise SkipRecord
        record['address'] = clean_address(record.pop('address'))
        record['case_number'] = record.pop('case_#')
        record['document_number'] = record.pop('document_#')
        record['pin_number'] = record.pop('pin_number')
        try:
            record['year_of_mortgage'] = str(record.pop('year_of_mo').year)
        except AttributeError:
            record['year_of_mortgage'] = 'Unknown'

        # Normalize inconsistent headers
        for old, new in (('SF', 'sf'), ('SMF', 'smf'), ('Condo', 'condo')):
            try:
                record[new] = record.pop(old)
            except KeyError:
                pass

        if int(record['sf']):
            record['property_type'] = 'Single family'
        elif int(record['smf']):
            record['property_type'] = 'Multi-unit'
        elif int(record['condo']):
            record['property_type'] = 'Condo'
        else:
            record['property_type'] = 'Unknown'

        return record

    def existing_record(self, list_record):
        # Each time we run the scrape, we assume the data hasn't been seen yet.
        return None

class ForeclosureAuctionResultScraper(WoodstockScraper):
    schema_slugs = ('foreclosure-auction-results',)
    excel_sheet_name = 'ar'

    def clean_list_record(self, record):
        record = WoodstockScraper.clean_list_record(self, record)
        if record['reo_y_n']:
            record['purchaser'] = 'Lender or bank'
        else:
            record['purchaser'] = 'Third party'
        record['sale_date'] = record['sheriffs']
        try:
            record['auction_price'] = int(record['sale_price'])
        except ValueError:
            record['auction_price'] = None
        return record

    def save(self, old_record, list_record, detail_record):
        if old_record is not None:
            return

        property_type = self.get_or_create_lookup('property_type', list_record['property_type'], list_record['property_type'])
        year_of_mortgage = self.get_or_create_lookup('year_of_mortgage', list_record['year_of_mortgage'], list_record['year_of_mortgage'])
        purchaser = self.get_or_create_lookup('purchaser', list_record['purchaser'], list_record['purchaser'])
        newsitem_title = 'Property on the %s sold at foreclosure auction' % address_to_block(strip_unit(list_record['address']))
        attributes = {
            'original_principal': list_record['original_i'],
            'property_type': property_type.id,
            'year_of_mortgage': year_of_mortgage.id,
            'pin_number': list_record['pin_number'],
            'case_number': list_record['case_number'],
            'document_number': list_record['document_number'],
            'filing_date': list_record['filing_date'],
            'raw_address': list_record['address'],
            'auction_price': list_record['auction_price'],
            'purchaser': purchaser.id,
        }
        self.create_newsitem(
            attributes,
            convert_to_block=True,
            title=newsitem_title,
            item_date=list_record['sale_date'],
            location_name=strip_unit(list_record['address']),
        )

class ForeclosureScraper(WoodstockScraper):
    schema_slugs = ('foreclosures',)
    excel_sheet_name = 'cha'

    def save(self, old_record, list_record, detail_record):
        if old_record is not None:
            return

        property_type = self.get_or_create_lookup('property_type', list_record['property_type'], list_record['property_type'])
        year_of_mortgage = self.get_or_create_lookup('year_of_mortgage', list_record['year_of_mortgage'], list_record['year_of_mortgage'])
        newsitem_title = 'Foreclosure filed for property in the %s' % address_to_block(strip_unit(list_record['address']))
        attributes = {
            'original_principal': list_record['original_i'],
            'property_type': property_type.id,
            'year_of_mortgage': year_of_mortgage.id,
            'pin_number': list_record['pin_number'],
            'filing_date': list_record['filing_date'],
            'case_number': list_record['case_number'],
            'document_number': list_record['document_number'],
            'raw_address': list_record['address'],
        }
        self.create_newsitem(
            attributes,
            convert_to_block=True,
            title=newsitem_title,
            item_date=list_record['filing_date'],
            location_name=strip_unit(list_record['address']),
        )

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    import sys
    if len(sys.argv) == 2:
        args = [sys.argv[1]]
    else:
        args = []
    ForeclosureAuctionResultScraper(*args).update()
    ForeclosureScraper(*args).update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Screen scraper for City of Chicago liquor-license application data.
https://webapps.cityofchicago.org/liqppa/liqppa.jsp?liqppa=liq
"""

from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebdata.retrieval.utils import norm_dict_space
from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date
import re

SOURCE_URL = 'https://webapps.cityofchicago.org/liqppa/liqppa.jsp?liqppa=liq'

class LiquorLicenseApplication(NewsItemListDetailScraper):
    schema_slugs = ('liquor-license-applications',)
    has_detail = False
    parse_list_re = re.compile(r'<tr.*?<font size="2" face="Arial, Helvetica, sans-serif">(?P<legal_name>.*?)</font>.*?<font size="2" face="Arial, Helvetica, sans-serif">(?P<dba>.*?)</font>.*?<font size="2" face="Arial, Helvetica, sans-serif">(?P<address_raw>.*?)</font>.*?<font size="2" face="Arial, Helvetica, sans-serif">(?P<license>.*?)</font>.*?<font size="2" face="Arial, Helvetica, sans-serif">(?P<start_date>.*?)</font>.*?<font size="2" face="Arial, Helvetica, sans-serif">(?P<application_date>.*?)</font>.*?<a href="Javascript:callOwner\((?P<city_business_id>\d+),', re.IGNORECASE | re.MULTILINE | re.DOTALL)

    def list_pages(self):
        yield self.get_html(SOURCE_URL)

    def clean_list_record(self, record):
        norm_dict_space(record, 'legal_name', 'address_raw', 'license', 'dba', 'start_date', 'application_date')
        record['dba'], record['business_type'] = record['dba'].rsplit(' - ', 1)
        record['start_date'] = parse_date(record['start_date'], '%m/%d/%Y')
        record['application_date'] = parse_date(record['application_date'], '%m/%d/%Y')

        # Remove the "Suite/Apt" or "Floor" clause from the address, if it exists.
        record['address_raw'] = record['address_raw'].replace(' ,', ',')
        m = re.search(r'^(.*?), (?:Suite/Apt|Floor):.*$', record['address_raw'])
        if m:
            record['address_raw'] = m.group(1)

        return record

    def existing_record(self, record):
        try:
            return NewsItem.objects.filter(schema__id=self.schema.id, item_date=record['application_date']).by_attribute(self.schema_fields['legal_name'], record['legal_name'])[0]
        except IndexError:
            return None

    def save(self, old_record, list_record, detail_record):
        if old_record is not None:
            # Liquor license applications never change, so we don't have to
            # worry about changing applications that already exist.
            self.logger.debug('Application already exists')
            return

        license = self.get_or_create_lookup('license', list_record['license'], list_record['license'])
        business_type = self.get_or_create_lookup('business_type', list_record['business_type'], list_record['business_type'])

        attributes = {
            'city_business_id': list_record['city_business_id'],
            'legal_name': list_record['legal_name'],
            'dba': list_record['dba'],
            'business_type': business_type.id,
            'license': license.id,
            'start_date': list_record['start_date'],
        }
        self.create_newsitem(
            attributes,
            title=list_record['legal_name'],
            description=u'%s applied for a license of type "%s."' % (list_record['legal_name'], license.name),
            url=SOURCE_URL,
            pub_date=list_record['application_date'],
            item_date=list_record['application_date'],
            location_name=list_record['address_raw'],
        )

if __name__ == "__main__":
    LiquorLicenseApplication().update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Import script for Chicago property transfers.

This data is imported from an Excel file, which is e-mailed to us every two
weeks.
"""

from ebdata.parsing.excel import ExcelDictReader
from ebdata.retrieval.scrapers.list_detail import SkipRecord
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date
from ebpub.utils.text import intcomma, clean_address

class PropertyTransferImporter(NewsItemListDetailScraper):
    schema_slugs = ('property-transfers',)
    has_detail = False

    def __init__(self, excel_file_name, *args, **kwargs):
        super(PropertyTransferImporter, self).__init__(*args, **kwargs)
        self.excel_file_name = excel_file_name

    def list_pages(self):
        reader = ExcelDictReader(self.excel_file_name, sheet_index=0, header_row_num=0, start_row_num=1)
        yield reader

    def parse_list(self, reader):
        for row in reader:
            yield row

    def clean_list_record(self, record):
        if record['City'].strip().upper() != 'CHICAGO':
            raise SkipRecord
        if record['Amount'].strip().upper() == 'UNKNOWN':
            record['Amount'] = None
        else:
            record['Amount'] = record['Amount'].replace('.00', '').replace('$', '').replace(',', '')

        record['Executed'] = parse_date(record['Executed'], '%m/%d/%Y')
        record['Recorded'] = parse_date(record['Recorded'], '%m/%d/%Y')

        record['clean_address'] = clean_address(record['Address'])
        unit = record['Unit #'] not in ('', 'MANY') and record['Unit #'] or None
        record['clean_address_with_unit'] = '%s%s' % (record['clean_address'], (unit and ', unit ' + unit or ''))

        try:
            record['doc_number'] = record['Doc Number']
        except KeyError:
            record['doc_number'] = record['Doc #']

        return record

    def existing_record(self, record):
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id)
            qs = qs.by_attribute(self.schema_fields['doc_number'], record['doc_number'])
            return qs[0]
        except IndexError:
            return None

    def save(self, old_record, list_record, detail_record):
        if old_record is not None:
            return
        if list_record['Amount'] is None:
            title = '%s transferred ownership' % list_record['clean_address_with_unit']
        else:
            title = '%s sold for $%s' % (list_record['clean_address_with_unit'], intcomma(list_record['Amount']))
        attributes = {
            'doc_number': list_record['doc_number'],
            'sale_price': list_record['Amount'],
            'seller': list_record['Seller'],
            'buyer': list_record['Buyer'],
            'pin': list_record['PIN'],
            'date_executed': list_record['Executed'],
        }
        self.create_newsitem(
            attributes,
            title=title,
            item_date=list_record['Recorded'],
            location_name=list_record['clean_address'],
        )

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    import sys
    importer = PropertyTransferImporter(sys.argv[1])
    importer.update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Screen scraper for Chicago restaurant-inspection data
http://webapps.cityofchicago.org/healthinspection/inspection.jsp
"""

from django.utils.text import get_text_list
from ebdata.retrieval.scrapers.list_detail import SkipRecord
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebdata.retrieval.utils import norm_dict_space
from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date
from ebpub.utils.text import clean_address
import re

list_aka_re = re.compile(r'<br><font color=blue><i>\((.*?)\)</i></font>')
detail_violations_re = re.compile(r"<a href='inspectiondesc\.jsp\?v=.*?'>(.*?)</a></td><td.*?</td><td align='center' valign='middle'>(.*?)</td>", re.DOTALL)

detail_url = lambda list_record: 'http://webapps.cityofchicago.org/healthinspection/inspectiondate.jsp?eid=%s' % list_record['city_id']

class RestaurantScraper(NewsItemListDetailScraper):
    schema_slugs = ('restaurant-inspections',)
    parse_list_re = re.compile(r'<tr>\s*<td valign="middle"[^>]*><a href="inspectiondate\.jsp\?eid=(?P<city_id>\d+)">(?P<name>.*?)</a><br><font color=green><i>\((?P<dba>.*?)\)</i></font>(?P<aka><br><font color=blue><i>\(.*?\)</i></font>)?\s+<br>.*?Address:</font></b>(?P<address>.*?)</td>\s*<td align="center"[^>]*>(?P<last_inspection_date>.*?)\s*</td>\s*<td align="center"[^>]*>(?P<result>.*?)\s*</td>\s*<td align="center"[^>]*>(?P<license_status>.*?)\s*</td>', re.DOTALL)
    parse_detail_re = re.compile(r'<b>Last Inspection Date: </b>(?P<inspection_date>.*?)<br><b>Inspection Result: </b>(?P<inspection_result>.*?)<br>.*?(?P<violations><table.*?</table>)', re.DOTALL)

    def list_pages(self):
        # Submit a search for the address range "0-99999".
        yield self.get_html('http://webapps.cityofchicago.org/healthinspection/inspectionresultrow.jsp?REST=&STR_NBR=0&STR_NBR2=99999&STR_DIRECTION=&STR_NM=&ZIP=&submit=Search')

    def clean_list_record(self, record):
        if record['last_inspection_date'].lower() == 'not available':
            raise SkipRecord('No inspection available')
        else:
            record['last_inspection_date'] = parse_date(record['last_inspection_date'], '%m/%d/%Y')
        if record['aka']:
            record['aka'] = list_aka_re.findall(record['aka'])[0]
        else:
            record['aka'] = ''
        norm_dict_space(record, 'name', 'dba', 'address')
        record['result'] = record['result'].replace('&nbsp;', '').strip()
        record['city_id'] = int(record['city_id'])

        # Remove the trailing ZIP code from the address, if it exists.
        m = re.search(r'(.*?)\s+\d\d\d\d\d$', record['address'])
        if m:
            record['address'] = m.group(1)
        record['address'] = clean_address(record['address'])

        return record

    def existing_record(self, record):
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id, item_date=record['last_inspection_date'])
            qs = qs.by_attribute(self.schema_fields['city_id'], record['city_id'])
            return qs[0]
        except IndexError:
            return None

    def detail_required(self, list_record, old_record):
        # If we've never seen this restaurant before, then a detail page is
        # required.
        if old_record is None:
            self.logger.debug('detail_required: True, because old_record is None')
            return True

        # Otherwise, check each field against the stored version to see whether
        # anything has changed. If at least one field has changed, then check
        # the detail page.
        if old_record.item_date != list_record['last_inspection_date']:
            self.logger.debug('detail_required: True, because last_inspection_date has changed')
            return True
        for field in ('name', 'dba', 'aka'):
            if old_record.attributes.get(field, '').encode('utf8') != list_record[field]:
                self.logger.debug('detail_required: True, because %s has changed (%r, %r)', field, old_record.attributes.get(field, '').encode('utf8'), list_record[field])
                return True
        return False

    def get_detail(self, record):
        url = detail_url(record)
        return self.get_html(url)

    def clean_detail_record(self, record):
        try:
            record['inspection_date'] = parse_date(record['inspection_date'], '%m/%d/%Y')
        except KeyError:
            # Sometimes the inspection_date is missing, for whatever reason.
            # Raise a warning in this case.
            self.logger.info('Record %r has no inspection_date. Skipping.', record)
            raise SkipRecord
        record['violations'] = [(i[0], i[1] == 'Yes') for i in detail_violations_re.findall(record['violations'])]

        # Determine the notes (a textual representation of which violations,
        # if any, were corrected during the inspection).
        corrected_violations = [v[0] for v in record['violations'] if v[1]]
        if record['violations']:
            if not corrected_violations:
                if len(corrected_violations) == 1:
                    note_bit = 'violation was not'
                else:
                    note_bit = 'violations were not'
            elif len(corrected_violations) == 1:
                if len(record['violations']) == 1:
                    note_bit = 'violation was'
                else:
                    note_bit = '%s violation was' % corrected_violations[0]
            else: # Multiple corrected violations.
                note_bit = '%s violations were' % get_text_list(corrected_violations, 'and')
            notes = 'The %s corrected during the inspection.' % note_bit
        else:
            # There's no need for notes if there were no violations.
            notes = ''
        record['notes'] = notes

        return record

    def save(self, old_record, list_record, detail_record):
        if detail_record is None:
            return # No need to update the record.
        result = self.get_or_create_lookup('result', list_record['result'], list_record['result'])
        violations = [self.get_or_create_lookup('violation', v[0], v[0]) for v in detail_record['violations']]
        violations_text = ','.join([str(v.id) for v in violations])
        result_past_tense = {
            'Pass': 'passed inspection',
            'Pass With Conditions': 'passed inspection with conditions',
            'Fail': 'failed inspection',
        }[list_record['result']]
        title = u'%s %s' % (list_record['dba'], result_past_tense)

        new_attributes = {
            'name': list_record['name'],
            'dba': list_record['dba'],
            'aka': list_record['aka'],
            'result': result.id,
            'violation': violations_text,
            'city_id': list_record['city_id'],
            'notes': detail_record['notes'],
        }

        if old_record is None:
            self.create_newsitem(
                new_attributes,
                title=title,
                url=detail_url(list_record),
                item_date=detail_record['inspection_date'],
                location_name=list_record['address'],
            )
        else:
            # This already exists in our database, but check whether any
            # of the values have changed.
            new_values = {'title': title, 'item_date': detail_record['inspection_date'], 'location_name': list_record['address']}
            self.update_existing(old_record, new_values, new_attributes)

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    RestaurantScraper().update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Screen scraper for City of Chicago street closures.
http://www.cityofchicago.org/Transportation/TravelAdvisories/streetclosures.html
"""

from django.utils.dateformat import format
from django.utils.text import capfirst
from ebdata.retrieval.scrapers.base import ScraperBroken
from ebdata.retrieval.scrapers.list_detail import SkipRecord
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date
import re

SOURCE_URL = 'http://www.cityofchicago.org/Transportation/TravelAdvisories/streetclosures.html'

strip_unneeded_tags = lambda x: re.sub(r'(?si)</?(?:b|font)\b[^>]*>', '', x).replace('&nbsp;', ' ').strip()

class StreetClosure(NewsItemListDetailScraper):
    schema_slugs = ('street-closures',)
    has_detail = False

    def list_pages(self):
        yield self.get_html(SOURCE_URL)

    def parse_list(self, page):
        # First, get the date and time that the page was updated.
        update_re = re.compile(r'(?si)<td height="42" bgcolor="white"><b><font size="3" face="Arial">(?P<update_date>.*?)</font></b></td>\s*</tr>\s*</table>\s*</td>\s*<td width="73" rowspan="2" valign="top">\s*<table border="0" width="71">\s*<tr>\s*<td height="42" bgcolor="white"><b><font size="3" face="Arial">(?P<update_time>.*?)</font></b></td>')
        m = update_re.search(page)
        if not m:
            raise ScraperBroken('Update date not found')
        updated = m.groupdict()

        # Next, get the table that contains the rows we want.
        m = re.search(r'(?si)<table [^>]* width="868">(.*?)</table>', page)
        if not m:
            raise ScraperBroken('Data table not found')
        table = m.group(1)

        # Return each data row in that table *after* the first row (the headers).
        parse_list_re = re.compile(r'(?si)<tr>\s*<td[^>]*>(?P<street_name>.*?)</td>\s*<td[^>]*>(?P<street_dir>.*?)</td>\s*<td[^>]*>(?P<block_from>.*?)</td>\s*<td[^>]*>(?P<block_to>.*?)</td>\s*<td[^>]*>(?P<street_suffix>.*?)</td>\s*<td[^>]*>(?P<start_date>.*?)</td>\s*<td[^>]*>(?P<end_date>.*?)</td>\s*<td[^>]*>(?P<closure_type>.*?)</td>\s*<td[^>]*>(?P<details>.*?)</td>\s*</tr>')
        for match in parse_list_re.finditer(table):
            record = match.groupdict()
            if 'street name' in record['street_name'].lower():
                continue # Skip the header row.
            yield dict(record, **updated)

    def clean_list_record(self, record):
        for k, v in record.items(): record[k] = strip_unneeded_tags(v)
        if not record['street_name']:
            raise SkipRecord('Street name not found')
        record['start_date'] = parse_date(record['start_date'], '%m/%d/%y')
        record['end_date'] = parse_date(record['end_date'], '%m/%d/%y')
        record['date_posted'] = parse_date('%s_%s' % (record.pop('update_date'), record.pop('update_time')), '%m/%d/%Y_%I:%M %p', return_datetime=True)
        record['address'] = '%s-%s %s %s %s' % (record['block_from'], record['block_to'], record['street_dir'], record['street_name'], record['street_suffix'])
        record['details'] = re.sub(r'(?i)<br>', '\n', record['details']).replace('&nbsp;', ' ').strip()
        return record

    def existing_record(self, record):
        try:
            obj = NewsItem.objects.filter(schema__id=self.schema.id,
                item_date=record['start_date']).by_attribute(self.schema_fields['address'], record['address']).by_attribute(self.schema_fields['end_date'], record['end_date'])[0]
        except IndexError:
            return None
        # If the details have changed, treat this as a new record.
        if obj.attributes['details'] != record['details']:
            return None
        return obj

    def save(self, old_record, list_record, detail_record):
        if old_record is not None:
            # Street closures never change, so we don't have to
            # worry about changing applications that already exist.
            self.logger.debug('Closure already exists')
            return

        closure_type = self.get_or_create_lookup('closure_type', capfirst(list_record['closure_type'].lower()), list_record['closure_type'].upper())

        # Calculate a "friendly" date range to avoid lameness like "from Oct. 3 to Oct. 3".
        if list_record['start_date'] == list_record['end_date']:
            friendly_date_range = 'on %s' % format(list_record['start_date'], 'F j')
        else:
            friendly_date_range = 'from %s to %s' % (format(list_record['start_date'], 'F j'), format(list_record['end_date'], 'F j'))

        attributes = {
            'block_from': list_record['block_from'],
            'block_to': list_record['block_to'],
            'address': list_record['address'],
            'street_dir': list_record['street_dir'],
            'street_name': list_record['street_name'],
            'street_suffix': list_record['street_suffix'],
            'closure_type': closure_type.id,
            'end_date': list_record['end_date'],
            'details': list_record['details'],
            'date_posted': list_record['date_posted'],
        }
        self.create_newsitem(
            attributes,
            title=u'%s %s' % (closure_type.name, friendly_date_range),
            url=SOURCE_URL,
            pub_date=list_record['date_posted'],
            item_date=list_record['start_date'],
            location_name=list_record['address'],
        )

if __name__ == "__main__":
    s = StreetClosure()
    s.update()

########NEW FILE########
__FILENAME__ = normalize-addresses
from ebpub.db.models import NewsItem
from retrieval import StreetNormalizer # relative import

def normalize():
    normalizer = StreetNormalizer()
    for ni in NewsItem.objects.filter(schema__slug='crime-reports').iterator():
        block, direction, street = ni.attributes['street'].split(';')
        record = {
            'offensestreet': street,
            'offenseblock': block,
            'offensedirection': direction,
        }
        normalized_address = normalizer.normalize_address(record)
        if ni.location_name != normalized_address:
            ni.location_name = normalized_address
            ni.save()
            print ni.attributes['street']
            print normalized_address
    normalizer.print_stats()

if __name__ == "__main__":
    normalize()

########NEW FILE########
__FILENAME__ = retrieval
"""
Scraper for Dallas crime.
http://66.97.146.94/dpdpublic/offense.aspx
ftp://66.97.146.94/
"""

from ebdata.retrieval.scrapers.list_detail import SkipRecord
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from ebpub.streets.models import Street
from ebpub.utils.dates import parse_date
from ebpub.utils.text import smart_title, address_to_block
from cStringIO import StringIO
from decimal import Decimal
from lxml import etree
from ucrmapping import CATEGORY_MAPPING, UCR_MAPPING # relative import
import datetime
import zipfile
import ftplib
import time
import re

FTP_SERVER = '66.97.146.94'
FTP_USERNAME = ''
FTP_PASSWORD = ''

# UCR prefixes of things like attempted suidicide that we shouldn't publish.
SKIPPED_UCR_PREFIXES = [
    '38', # suicide
    '39', # attempted suicide
]

class StreetNormalizer(object):
    def __init__(self):
        # Store a mapping of condensed and normalized street names to street
        # names that are in our database. We use this to look up actual street
        # names since they come to us stripped of any internal spaces.
        self.streets = {}
        for street in Street.objects.values('street'):
            street_name = street['street']
            normalized_name = re.sub(r'\s+', '', street_name.upper())
            self.streets[normalized_name] = street_name
        self.records_seen = 0
        self.matches_found = 0

    def print_stats(self):
        pct_normalized = Decimal(self.matches_found) / Decimal(self.records_seen)
        print "Normalized %s out of %s addresses. (%s%%)" % (self.matches_found, self.records_seen, pct_normalized)

    def normalize_address(self, record):
        """
        Addresses are provided with no spaces, so try to find the suffix if
        there is one, then compare the reamining part to the streets table.
        """
        if record['offensestreet'] is None:
            raise SkipRecord('Skipping record with no street')

        street = record['offensestreet']
        matching_suffix = ''

        suffix_groups = [
            ('EXPWY', 'PKWY', 'FRWY', 'BLVD'),
            ('HWY', 'FRW', 'AVE', 'CIR', 'EXT', 'BLV', 'PKW', 'ROW', 'WAY', 'EXP'),
            ('DR', 'ST', 'RD', 'LN', 'BL', 'TR', 'WY', 'CT', 'PL', 'AV', 'CI'),
            ('P', 'R', 'F', 'D', 'S', 'L')
        ]

        match_found = False
        for group in suffix_groups:
            if match_found:
                break
            for suffix in group:
                if record['offensestreet'].endswith(suffix):
                    street_name = record['offensestreet'][:-len(suffix)]
                    # Try looking up the street name from a dictionary mapping
                    # collapsed street names to names in the streets table.
                    try:
                        street = self.streets[street_name]
                        matching_suffix = suffix
                        match_found = True
                        break
                    except KeyError:
                        # SAINT is encoded as ST in the data, but Saint in the streets table,
                        # so try that if the address starts with ST.
                        if street_name.startswith('ST'):
                            street_name = 'SAINT%s' % street_name[2:]
                            try:
                                street = self.streets[street_name]
                                matching_suffix = suffix
                                match_found = True
                                break
                            except KeyError:
                                continue

        if match_found:
            self.matches_found += 1
        self.records_seen += 1

        normalized_block = record['offenseblock'].lstrip('0')
        if normalized_block[-2:] == 'xx':
            normalized_block = normalized_block.replace('xx', '00')
        address = '%s %s %s %s' % (
            normalized_block, record['offensedirection'] or '', street, matching_suffix
        )
        address = re.sub(r'\s+', ' ', address)
        return address_to_block(address)

class BaseScraper(NewsItemListDetailScraper):
    schema_slugs = ('crime-reports',)
    has_detail = False
    def __init__(self, filename=None, get_all=False):
        super(BaseScraper, self).__init__(self)
        self.filename = filename
        self.get_all = get_all

    def retrieve_file(self, date):
        yesterday = datetime.date.today() - datetime.timedelta(days=1)
        ftp_filename = self.filename_pattern % (date.month, date.day, date.year)
        f = StringIO() # This buffer stores the retrieved file in memory.
        self.logger.debug('Connecting via FTP to %s', FTP_SERVER)
        ftp = ftplib.FTP(FTP_SERVER, FTP_USERNAME, FTP_PASSWORD)
        ftp.set_pasv(False)
        if date.year != yesterday.year or date.month != yesterday.month:
            dirname = date.strftime('/%Y/%B/')
            self.logger.debug('Changing to %s' % dirname)
            ftp.cwd(dirname)
        self.logger.debug('Retrieving file %s', ftp_filename)
        try:
            ftp.retrbinary('RETR %s' % ftp_filename, f.write)
        except ftplib.error_perm:
            self.logger.warn("Couldn't find file %s" % ftp_filename)
            return None
        ftp.quit()
        self.logger.debug('Done downloading')
        f.seek(0)
        return f

    def list_pages(self):
        if self.filename is None and self.get_all:
            date = datetime.date(2009, 1, 31)
            while 1:
                date = date + datetime.timedelta(days=1)
                if date == datetime.date.today():
                    break
                f = self.retrieve_file(date)
                if f is None:
                    continue
                zf = zipfile.ZipFile(f, 'r')
                xml_filename = zf.namelist()[0]
                yield StringIO(zf.read(xml_filename))
                zf.close()
                f.close()
        elif self.filename is None:
            date = datetime.date.today() - datetime.timedelta(days=1)
            f = self.retrieve_file(date)
            zf = zipfile.ZipFile(f, 'r')
            xml_filename = zf.namelist()[0]
            yield StringIO(zf.read(xml_filename))
            zf.close()
            f.close()
        else:
            f = open(self.filename, 'r')
            zf = zipfile.ZipFile(f, 'r')
            xml_filename = zf.namelist()[0]
            yield StringIO(zf.read(xml_filename))
            zf.close()
            f.close()

    def parse_list(self, xml_file):
        tree = etree.parse(xml_file)
        for record_element in tree.xpath('/NewDataSet/Record'):
            record = {}
            for element in record_element:
                record[element.tag] = element.text
            yield record

    def existing_record(self, record):
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id, item_date=record['offensedate'])
            return qs.by_attribute(self.schema_fields['service_number'], record['offenseservicenumber'])[0]
        except IndexError:
            return None

class OffenseScraper(BaseScraper):
    filename_pattern = 'OFFENSE_%s_%s_%s.zip'

    def __init__(self, *args, **kwargs):
        super(OffenseScraper, self).__init__(*args, **kwargs)
        self.normalizer = StreetNormalizer()

    def clean_list_record(self, record):
        # Get the first 2 digits of the UCR, or None.
        ucr1_prefix = record['offenseucr1'] and record['offenseucr1'][:2] or None
        ucr2_prefix = record['offenseucr2'] and record['offenseucr2'][:2] or None

        if ucr1_prefix in SKIPPED_UCR_PREFIXES:
            raise SkipRecord('Skipping record with UCR %s' % record['offenseucr1'])
        if ucr2_prefix in SKIPPED_UCR_PREFIXES:
            raise SkipRecord('Skipping record with UCR %s' % record['offenseucr2'])

        record['address'] = self.normalizer.normalize_address(record)

        # If we can't find the code in the mapping, just use the code itself
        # as the value.
        record['category'] = CATEGORY_MAPPING.get(ucr1_prefix, ucr1_prefix)
        record['crime_type'] = UCR_MAPPING.get(record['offenseucr1'], record['offenseucr1'])
        record['secondary_category'] = CATEGORY_MAPPING.get(ucr2_prefix, ucr2_prefix)
        record['secondary_crime_type'] = UCR_MAPPING.get(record['offenseucr2'], record['offenseucr2'])

        record['offensedate'] = parse_date(record['offensedate'], '%m/%d/%Y')
        record['offensestarttime'] = datetime.time(*time.strptime(record['offensestarttime'], '%H:%M:%S')[3:5])
        record['offensebeat'] = record['offensebeat'] or ''
        return record

    def save(self, old_record, list_record, detail_record):
        category = self.get_or_create_lookup('category', list_record['category'], list_record['category'], make_text_slug=False)
        secondary_category = self.get_or_create_lookup('secondary_category', list_record['secondary_category'], list_record['secondary_category'], make_text_slug=False)
        beat = self.get_or_create_lookup('beat', list_record['offensebeat'], list_record['offensebeat'], make_text_slug=False)
        premises = self.get_or_create_lookup('premises', list_record['offensepremises'], list_record['offensepremises'], make_text_slug=False)
        crime_type = self.get_or_create_lookup('crime_type', list_record['crime_type'], list_record['crime_type'], make_text_slug=False)
        secondary_crime_type = self.get_or_create_lookup('secondary_crime_type', list_record['secondary_crime_type'], list_record['secondary_crime_type'], make_text_slug=False)

        kwargs = {
            'title': smart_title(list_record['offensedescription']),
            'item_date': list_record['offensedate'],
            'location_name': list_record['address']
        }
        attributes = {
            'category': category.id,
            'secondary_category': secondary_category.id,
            'service_number': list_record['offenseservicenumber'],
            'offense_time': list_record['offensestarttime'],
            'description': list_record['offensedescription'],
            'beat': beat.id,
            'premises': premises.id,
            'crime_type': crime_type.id,
            'secondary_crime_type': secondary_crime_type.id,
            'method': list_record['offensemethodofoffense'],
            # street is block;direction;street
            # This will allow us to reprocess the original data when we
            # improve the address normalizer.
            'street': ';'.join((list_record['offenseblock'] or '', list_record['offensedirection'] or '', list_record['offensestreet'] or '')),
            'ucr': ';'.join((list_record['offenseucr1'] or '', list_record['offenseucr2'] or ''))
        }
        if old_record is None:
            self.create_newsitem(attributes, **kwargs)
        else:
            self.update_existing(old_record, kwargs, attributes)

class NarrativeScraper(BaseScraper):
    filename_pattern = 'OFFENSENARRATIVE_%s_%s_%s.zip'

    def clean_list_record(self, record):
        record['offensedate'] = parse_date(record['offensedate'], '%m/%d/%Y')
        if record['offensenarrative'] is not None:
            record['offensenarrative'] = record['offensenarrative'].strip()
        return record

    def save(self, old_record, list_record, detail_record):
        # We're updating existing records, so if there isn't one, skip.
        if old_record is None:
            return
        attributes = {'narrative': list_record['offensenarrative']}
        self.update_existing(old_record, {}, attributes)

def update():
    os = OffenseScraper()
    ns = NarrativeScraper()
    os.update()
    ns.update()

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    os = OffenseScraper()
    ns = NarrativeScraper()
    os.update()
    ns.update()
    os.normalizer.print_stats()

########NEW FILE########
__FILENAME__ = ucrmapping
"""
UCR to text mappings from http://66.97.146.94/dpdpublic/ucr.htm
"""

CATEGORY_MAPPING = {
    None: 'NONE',
    '01': 'MURDER',
    '02': 'RAPE',
    '03': 'ROBBERY',
    '04': 'AGG ASSAULT',
    '05': 'BURGLARY',
    '06': 'THEFT',
    '07': 'UNAUTHOR USE OF MOT VEH UUMV',
    '08': 'ASSAULT',
    '09': 'ARSON includes BOMB THREAT',
    '10': 'FORGE & COUNTERFEIT',
    '11': 'FRAUD',
    '12': 'EMBEZZLEMENT',
    '13': 'FENCE',
    '14': 'VANDAL & CRIM MISCH',
    '15': 'CPW CH TO WEAPON',
    '16': 'PROSTI COMMER VICE',
    '17': 'SEX OFF & INDEC COND',
    '18': 'NARC & DRUGS',
    '19': 'GAMBLING',
    '20': 'CHILD (OFF AGAINST CHILD)',
    '21': 'DWI',
    '22': 'LIQUOR',
    '23': 'DRUNK & DISORD',
    '24': 'DISORDERLY CONDUCT ',
    '26': 'OTHERS',
    '29': 'RUNAWAY',
    '30': 'TRAFFIC HAZARDOUS',
    '41': 'MISSING PERSON',
    '42': 'LOST',
    '43': 'FOUND',
    '44': 'AIRPLANE',
    '45': 'HOLD',
}

UCR_MAPPING = {
    None: 'NONE',
    '01001': 'MURDER - FIRE PERSON - VEHICLE FATALITY NOT DWI (intentional) - 1800-0600',
    '01002': 'MURDER - FIRE PERSON - VEHICLE FATALITY NOT DWI (intentional) - 0600-1800',
    '01003': 'MURDER - FIRE PERSON - VEHICLE FATALITY NOT DWI (intentional) - UNKNOWN',
    '01011': 'MURDER - FIRE PERSON - FIREARM - 1800-0600',
    '01012': 'MURDER - FIRE PERSON - FIREARM - 0600-1800',
    '01013': 'MURDER - FIRE PERSON - FIREARM - UNKNOWN',
    '01021': 'MURDER - FIRE PERSON - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '01022': 'MURDER - FIRE PERSON - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '01023': 'MURDER - FIRE PERSON - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '01031': 'MURDER - FIRE PERSON - BLUDGEON - 1800-0600',
    '01032': 'MURDER - FIRE PERSON - BLUDGEON - 0600-1800',
    '01033': 'MURDER - FIRE PERSON - BLUDGEON - UNKNOWN',
    '01041': 'MURDER - FIRE PERSON - STRANGULATION/HANDING, ETC - 1800-0600',
    '01042': 'MURDER - FIRE PERSON - STRANGULATION/HANDING, ETC - 0600-1800',
    '01043': 'MURDER - FIRE PERSON - STRANGULATION/HANDING, ETC - UNKNOWN',
    '01051': 'MURDER - FIRE PERSON - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '01052': 'MURDER - FIRE PERSON - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '01053': 'MURDER - FIRE PERSON - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '01061': 'MURDER - FIRE PERSON - POISON OR DRUGS - 1800-0600',
    '01062': 'MURDER - FIRE PERSON - POISON OR DRUGS - 0600-1800',
    '01063': 'MURDER - FIRE PERSON - POISON OR DRUGS - UNKNOWN',
    '01071': 'MURDER - FIRE PERSON - BURNING/SCALDING - 1800-0600',
    '01072': 'MURDER - FIRE PERSON - BURNING/SCALDING - 0600-1800',
    '01073': 'MURDER - FIRE PERSON - BURNING/SCALDING - UNKNOWN',
    '01081': 'MURDER - FIRE PERSON - INTOX MANSLAUGHTER (DWI FATAL) - 1800-0600',
    '01082': 'MURDER - FIRE PERSON - INTOX MANSLAUGHTER (DWI FATAL) - 0600-1800',
    '01083': 'MURDER - FIRE PERSON - INTOX MANSLAUGHTER (DWI FATAL) - UNKNOWN',
    '01091': 'MURDER - FIRE PERSON - NON-VEHICLE NEGLIGENT HOMICIDE - 1800-0600',
    '01092': 'MURDER - FIRE PERSON - NON-VEHICLE NEGLIGENT HOMICIDE - 0600-1800',
    '01093': 'MURDER - FIRE PERSON - NON-VEHICLE NEGLIGENT HOMICIDE - UNKNOWN',
    '01101': 'MURDER - ADULT W/M - VEHICLE FATALITY NOT DWI (intentional) - 1800-0600',
    '01102': 'MURDER - ADULT W/M - VEHICLE FATALITY NOT DWI (intentional) - 0600-1800',
    '01103': 'MURDER - ADULT W/M - VEHICLE FATALITY NOT DWI (intentional) - UNKNOWN',
    '01111': 'MURDER - ADULT W/M - FIREARM - 1800-0600',
    '01112': 'MURDER - ADULT W/M - FIREARM - 0600-1800',
    '01113': 'MURDER - ADULT W/M - FIREARM - UNKNOWN',
    '01121': 'MURDER - ADULT W/M - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '01122': 'MURDER - ADULT W/M - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '01123': 'MURDER - ADULT W/M - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '01131': 'MURDER - ADULT W/M - BLUDGEON - 1800-0600',
    '01132': 'MURDER - ADULT W/M - BLUDGEON - 0600-1800',
    '01133': 'MURDER - ADULT W/M - BLUDGEON - UNKNOWN',
    '01141': 'MURDER - ADULT W/M - STRANGULATION/HANDING, ETC - 1800-0600',
    '01142': 'MURDER - ADULT W/M - STRANGULATION/HANDING, ETC - 0600-1800',
    '01143': 'MURDER - ADULT W/M - STRANGULATION/HANDING, ETC - UNKNOWN',
    '01151': 'MURDER - ADULT W/M - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '01152': 'MURDER - ADULT W/M - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '01153': 'MURDER - ADULT W/M - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '01161': 'MURDER - ADULT W/M - POISON OR DRUGS - 1800-0600',
    '01162': 'MURDER - ADULT W/M - POISON OR DRUGS - 0600-1800',
    '01163': 'MURDER - ADULT W/M - POISON OR DRUGS - UNKNOWN',
    '01171': 'MURDER - ADULT W/M - BURNING/SCALDING - 1800-0600',
    '01172': 'MURDER - ADULT W/M - BURNING/SCALDING - 0600-1800',
    '01173': 'MURDER - ADULT W/M - BURNING/SCALDING - UNKNOWN',
    '01181': 'MURDER - ADULT W/M - INTOX MANSLAUGHTER (DWI FATAL) - 1800-0600',
    '01182': 'MURDER - ADULT W/M - INTOX MANSLAUGHTER (DWI FATAL) - 0600-1800',
    '01183': 'MURDER - ADULT W/M - INTOX MANSLAUGHTER (DWI FATAL) - UNKNOWN',
    '01191': 'MURDER - ADULT W/M - NON-VEHICLE NEGLIGENT HOMICIDE - 1800-0600',
    '01192': 'MURDER - ADULT W/M - NON-VEHICLE NEGLIGENT HOMICIDE - 0600-1800',
    '01193': 'MURDER - ADULT W/M - NON-VEHICLE NEGLIGENT HOMICIDE - UNKNOWN',
    '01201': 'MURDER - ADULT B/M - VEHICLE FATALITY NOT DWI (intentional) - 1800-0600',
    '01202': 'MURDER - ADULT B/M - VEHICLE FATALITY NOT DWI (intentional) - 0600-1800',
    '01203': 'MURDER - ADULT B/M - VEHICLE FATALITY NOT DWI (intentional) - UNKNOWN',
    '01211': 'MURDER - ADULT B/M - FIREARM - 1800-0600',
    '01212': 'MURDER - ADULT B/M - FIREARM - 0600-1800',
    '01213': 'MURDER - ADULT B/M - FIREARM - UNKNOWN',
    '01221': 'MURDER - ADULT B/M - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '01222': 'MURDER - ADULT B/M - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '01223': 'MURDER - ADULT B/M - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '01231': 'MURDER - ADULT B/M - BLUDGEON - 1800-0600',
    '01232': 'MURDER - ADULT B/M - BLUDGEON - 0600-1800',
    '01233': 'MURDER - ADULT B/M - BLUDGEON - UNKNOWN',
    '01241': 'MURDER - ADULT B/M - STRANGULATION/HANDING, ETC - 1800-0600',
    '01242': 'MURDER - ADULT B/M - STRANGULATION/HANDING, ETC - 0600-1800',
    '01243': 'MURDER - ADULT B/M - STRANGULATION/HANDING, ETC - UNKNOWN',
    '01251': 'MURDER - ADULT B/M - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '01252': 'MURDER - ADULT B/M - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '01253': 'MURDER - ADULT B/M - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '01261': 'MURDER - ADULT B/M - POISON OR DRUGS - 1800-0600',
    '01262': 'MURDER - ADULT B/M - POISON OR DRUGS - 0600-1800',
    '01263': 'MURDER - ADULT B/M - POISON OR DRUGS - UNKNOWN',
    '01271': 'MURDER - ADULT B/M - BURNING/SCALDING - 1800-0600',
    '01272': 'MURDER - ADULT B/M - BURNING/SCALDING - 0600-1800',
    '01273': 'MURDER - ADULT B/M - BURNING/SCALDING - UNKNOWN',
    '01281': 'MURDER - ADULT B/M - INTOX MANSLAUGHTER (DWI FATAL) - 1800-0600',
    '01282': 'MURDER - ADULT B/M - INTOX MANSLAUGHTER (DWI FATAL) - 0600-1800',
    '01283': 'MURDER - ADULT B/M - INTOX MANSLAUGHTER (DWI FATAL) - UNKNOWN',
    '01291': 'MURDER - ADULT B/M - NON-VEHICLE NEGLIGENT HOMICIDE - 1800-0600',
    '01292': 'MURDER - ADULT B/M - NON-VEHICLE NEGLIGENT HOMICIDE - 0600-1800',
    '01293': 'MURDER - ADULT B/M - NON-VEHICLE NEGLIGENT HOMICIDE - UNKNOWN',
    '01301': 'MURDER - ADULT W/F - VEHICLE FATALITY NOT DWI (intentional) - 1800-0600',
    '01302': 'MURDER - ADULT W/F - VEHICLE FATALITY NOT DWI (intentional) - 0600-1800',
    '01303': 'MURDER - ADULT W/F - VEHICLE FATALITY NOT DWI (intentional) - UNKNOWN',
    '01311': 'MURDER - ADULT W/F - FIREARM - 1800-0600',
    '01312': 'MURDER - ADULT W/F - FIREARM - 0600-1800',
    '01313': 'MURDER - ADULT W/F - FIREARM - UNKNOWN',
    '01321': 'MURDER - ADULT W/F - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '01322': 'MURDER - ADULT W/F - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '01323': 'MURDER - ADULT W/F - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '01331': 'MURDER - ADULT W/F - BLUDGEON - 1800-0600',
    '01332': 'MURDER - ADULT W/F - BLUDGEON - 0600-1800',
    '01333': 'MURDER - ADULT W/F - BLUDGEON - UNKNOWN',
    '01341': 'MURDER - ADULT W/F - STRANGULATION/HANDING, ETC - 1800-0600',
    '01342': 'MURDER - ADULT W/F - STRANGULATION/HANDING, ETC - 0600-1800',
    '01343': 'MURDER - ADULT W/F - STRANGULATION/HANDING, ETC - UNKNOWN',
    '01351': 'MURDER - ADULT W/F - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '01352': 'MURDER - ADULT W/F - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '01353': 'MURDER - ADULT W/F - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '01361': 'MURDER - ADULT W/F - POISON OR DRUGS - 1800-0600',
    '01362': 'MURDER - ADULT W/F - POISON OR DRUGS - 0600-1800',
    '01363': 'MURDER - ADULT W/F - POISON OR DRUGS - UNKNOWN',
    '01371': 'MURDER - ADULT W/F - BURNING/SCALDING - 1800-0600',
    '01372': 'MURDER - ADULT W/F - BURNING/SCALDING - 0600-1800',
    '01373': 'MURDER - ADULT W/F - BURNING/SCALDING - UNKNOWN',
    '01381': 'MURDER - ADULT W/F - INTOX MANSLAUGHTER (DWI FATAL) - 1800-0600',
    '01382': 'MURDER - ADULT W/F - INTOX MANSLAUGHTER (DWI FATAL) - 0600-1800',
    '01383': 'MURDER - ADULT W/F - INTOX MANSLAUGHTER (DWI FATAL) - UNKNOWN',
    '01391': 'MURDER - ADULT W/F - NON-VEHICLE NEGLIGENT HOMICIDE - 1800-0600',
    '01392': 'MURDER - ADULT W/F - NON-VEHICLE NEGLIGENT HOMICIDE - 0600-1800',
    '01393': 'MURDER - ADULT W/F - NON-VEHICLE NEGLIGENT HOMICIDE - UNKNOWN',
    '01401': 'MURDER - ADULT B/F - VEHICLE FATALITY NOT DWI (intentional) - 1800-0600',
    '01402': 'MURDER - ADULT B/F - VEHICLE FATALITY NOT DWI (intentional) - 0600-1800',
    '01403': 'MURDER - ADULT B/F - VEHICLE FATALITY NOT DWI (intentional) - UNKNOWN',
    '01411': 'MURDER - ADULT B/F - FIREARM - 1800-0600',
    '01412': 'MURDER - ADULT B/F - FIREARM - 0600-1800',
    '01413': 'MURDER - ADULT B/F - FIREARM - UNKNOWN',
    '01421': 'MURDER - ADULT B/F - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '01422': 'MURDER - ADULT B/F - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '01423': 'MURDER - ADULT B/F - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '01431': 'MURDER - ADULT B/F - BLUDGEON - 1800-0600',
    '01432': 'MURDER - ADULT B/F - BLUDGEON - 0600-1800',
    '01433': 'MURDER - ADULT B/F - BLUDGEON - UNKNOWN',
    '01441': 'MURDER - ADULT B/F - STRANGULATION/HANDING, ETC - 1800-0600',
    '01442': 'MURDER - ADULT B/F - STRANGULATION/HANDING, ETC - 0600-1800',
    '01443': 'MURDER - ADULT B/F - STRANGULATION/HANDING, ETC - UNKNOWN',
    '01451': 'MURDER - ADULT B/F - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '01452': 'MURDER - ADULT B/F - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '01453': 'MURDER - ADULT B/F - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '01461': 'MURDER - ADULT B/F - POISON OR DRUGS - 1800-0600',
    '01462': 'MURDER - ADULT B/F - POISON OR DRUGS - 0600-1800',
    '01463': 'MURDER - ADULT B/F - POISON OR DRUGS - UNKNOWN',
    '01471': 'MURDER - ADULT B/F - BURNING/SCALDING - 1800-0600',
    '01472': 'MURDER - ADULT B/F - BURNING/SCALDING - 0600-1800',
    '01473': 'MURDER - ADULT B/F - BURNING/SCALDING - UNKNOWN',
    '01481': 'MURDER - ADULT B/F - INTOX MANSLAUGHTER (DWI FATAL) - 1800-0600',
    '01482': 'MURDER - ADULT B/F - INTOX MANSLAUGHTER (DWI FATAL) - 0600-1800',
    '01483': 'MURDER - ADULT B/F - INTOX MANSLAUGHTER (DWI FATAL) - UNKNOWN',
    '01491': 'MURDER - ADULT B/F - NON-VEHICLE NEGLIGENT HOMICIDE - 1800-0600',
    '01492': 'MURDER - ADULT B/F - NON-VEHICLE NEGLIGENT HOMICIDE - 0600-1800',
    '01493': 'MURDER - ADULT B/F - NON-VEHICLE NEGLIGENT HOMICIDE - UNKNOWN',
    '01501': 'MURDER - JUV W/M - VEHICLE FATALITY NOT DWI (intentional) - 1800-0600',
    '01502': 'MURDER - JUV W/M - VEHICLE FATALITY NOT DWI (intentional) - 0600-1800',
    '01503': 'MURDER - JUV W/M - VEHICLE FATALITY NOT DWI (intentional) - UNKNOWN',
    '01511': 'MURDER - JUV W/M - FIREARM - 1800-0600',
    '01512': 'MURDER - JUV W/M - FIREARM - 0600-1800',
    '01513': 'MURDER - JUV W/M - FIREARM - UNKNOWN',
    '01521': 'MURDER - JUV W/M - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '01522': 'MURDER - JUV W/M - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '01523': 'MURDER - JUV W/M - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '01531': 'MURDER - JUV W/M - BLUDGEON - 1800-0600',
    '01532': 'MURDER - JUV W/M - BLUDGEON - 0600-1800',
    '01533': 'MURDER - JUV W/M - BLUDGEON - UNKNOWN',
    '01541': 'MURDER - JUV W/M - STRANGULATION/HANDING, ETC - 1800-0600',
    '01542': 'MURDER - JUV W/M - STRANGULATION/HANDING, ETC - 0600-1800',
    '01543': 'MURDER - JUV W/M - STRANGULATION/HANDING, ETC - UNKNOWN',
    '01551': 'MURDER - JUV W/M - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '01552': 'MURDER - JUV W/M - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '01553': 'MURDER - JUV W/M - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '01561': 'MURDER - JUV W/M - POISON OR DRUGS - 1800-0600',
    '01562': 'MURDER - JUV W/M - POISON OR DRUGS - 0600-1800',
    '01563': 'MURDER - JUV W/M - POISON OR DRUGS - UNKNOWN',
    '01571': 'MURDER - JUV W/M - BURNING/SCALDING - 1800-0600',
    '01572': 'MURDER - JUV W/M - BURNING/SCALDING - 0600-1800',
    '01573': 'MURDER - JUV W/M - BURNING/SCALDING - UNKNOWN',
    '01581': 'MURDER - JUV W/M - INTOX MANSLAUGHTER (DWI FATAL) - 1800-0600',
    '01582': 'MURDER - JUV W/M - INTOX MANSLAUGHTER (DWI FATAL) - 0600-1800',
    '01583': 'MURDER - JUV W/M - INTOX MANSLAUGHTER (DWI FATAL) - UNKNOWN',
    '01591': 'MURDER - JUV W/M - NON-VEHICLE NEGLIGENT HOMICIDE - 1800-0600',
    '01592': 'MURDER - JUV W/M - NON-VEHICLE NEGLIGENT HOMICIDE - 0600-1800',
    '01593': 'MURDER - JUV W/M - NON-VEHICLE NEGLIGENT HOMICIDE - UNKNOWN',
    '01601': 'MURDER - JUV B/M - VEHICLE FATALITY NOT DWI (intentional) - 1800-0600',
    '01602': 'MURDER - JUV B/M - VEHICLE FATALITY NOT DWI (intentional) - 0600-1800',
    '01603': 'MURDER - JUV B/M - VEHICLE FATALITY NOT DWI (intentional) - UNKNOWN',
    '01611': 'MURDER - JUV B/M - FIREARM - 1800-0600',
    '01612': 'MURDER - JUV B/M - FIREARM - 0600-1800',
    '01613': 'MURDER - JUV B/M - FIREARM - UNKNOWN',
    '01621': 'MURDER - JUV B/M - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '01622': 'MURDER - JUV B/M - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '01623': 'MURDER - JUV B/M - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '01631': 'MURDER - JUV B/M - BLUDGEON - 1800-0600',
    '01632': 'MURDER - JUV B/M - BLUDGEON - 0600-1800',
    '01633': 'MURDER - JUV B/M - BLUDGEON - UNKNOWN',
    '01641': 'MURDER - JUV B/M - STRANGULATION/HANDING, ETC - 1800-0600',
    '01642': 'MURDER - JUV B/M - STRANGULATION/HANDING, ETC - 0600-1800',
    '01643': 'MURDER - JUV B/M - STRANGULATION/HANDING, ETC - UNKNOWN',
    '01651': 'MURDER - JUV B/M - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '01652': 'MURDER - JUV B/M - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '01653': 'MURDER - JUV B/M - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '01661': 'MURDER - JUV B/M - POISON OR DRUGS - 1800-0600',
    '01662': 'MURDER - JUV B/M - POISON OR DRUGS - 0600-1800',
    '01663': 'MURDER - JUV B/M - POISON OR DRUGS - UNKNOWN',
    '01671': 'MURDER - JUV B/M - BURNING/SCALDING - 1800-0600',
    '01672': 'MURDER - JUV B/M - BURNING/SCALDING - 0600-1800',
    '01673': 'MURDER - JUV B/M - BURNING/SCALDING - UNKNOWN',
    '01681': 'MURDER - JUV B/M - INTOX MANSLAUGHTER (DWI FATAL) - 1800-0600',
    '01682': 'MURDER - JUV B/M - INTOX MANSLAUGHTER (DWI FATAL) - 0600-1800',
    '01683': 'MURDER - JUV B/M - INTOX MANSLAUGHTER (DWI FATAL) - UNKNOWN',
    '01691': 'MURDER - JUV B/M - NON-VEHICLE NEGLIGENT HOMICIDE - 1800-0600',
    '01692': 'MURDER - JUV B/M - NON-VEHICLE NEGLIGENT HOMICIDE - 0600-1800',
    '01693': 'MURDER - JUV B/M - NON-VEHICLE NEGLIGENT HOMICIDE - UNKNOWN',
    '01701': 'MURDER - JUV W/F - VEHICLE FATALITY NOT DWI (intentional) - 1800-0600',
    '01702': 'MURDER - JUV W/F - VEHICLE FATALITY NOT DWI (intentional) - 0600-1800',
    '01703': 'MURDER - JUV W/F - VEHICLE FATALITY NOT DWI (intentional) - UNKNOWN',
    '01711': 'MURDER - JUV W/F - FIREARM - 1800-0600',
    '01712': 'MURDER - JUV W/F - FIREARM - 0600-1800',
    '01713': 'MURDER - JUV W/F - FIREARM - UNKNOWN',
    '01721': 'MURDER - JUV W/F - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '01722': 'MURDER - JUV W/F - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '01723': 'MURDER - JUV W/F - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '01731': 'MURDER - JUV W/F - BLUDGEON - 1800-0600',
    '01732': 'MURDER - JUV W/F - BLUDGEON - 0600-1800',
    '01733': 'MURDER - JUV W/F - BLUDGEON - UNKNOWN',
    '01741': 'MURDER - JUV W/F - STRANGULATION/HANDING, ETC - 1800-0600',
    '01742': 'MURDER - JUV W/F - STRANGULATION/HANDING, ETC - 0600-1800',
    '01743': 'MURDER - JUV W/F - STRANGULATION/HANDING, ETC - UNKNOWN',
    '01751': 'MURDER - JUV W/F - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '01752': 'MURDER - JUV W/F - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '01753': 'MURDER - JUV W/F - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '01761': 'MURDER - JUV W/F - POISON OR DRUGS - 1800-0600',
    '01762': 'MURDER - JUV W/F - POISON OR DRUGS - 0600-1800',
    '01763': 'MURDER - JUV W/F - POISON OR DRUGS - UNKNOWN',
    '01771': 'MURDER - JUV W/F - BURNING/SCALDING - 1800-0600',
    '01772': 'MURDER - JUV W/F - BURNING/SCALDING - 0600-1800',
    '01773': 'MURDER - JUV W/F - BURNING/SCALDING - UNKNOWN',
    '01781': 'MURDER - JUV W/F - INTOX MANSLAUGHTER (DWI FATAL) - 1800-0600',
    '01782': 'MURDER - JUV W/F - INTOX MANSLAUGHTER (DWI FATAL) - 0600-1800',
    '01783': 'MURDER - JUV W/F - INTOX MANSLAUGHTER (DWI FATAL) - UNKNOWN',
    '01791': 'MURDER - JUV W/F - NON-VEHICLE NEGLIGENT HOMICIDE - 1800-0600',
    '01792': 'MURDER - JUV W/F - NON-VEHICLE NEGLIGENT HOMICIDE - 0600-1800',
    '01793': 'MURDER - JUV W/F - NON-VEHICLE NEGLIGENT HOMICIDE - UNKNOWN',
    '01801': 'MURDER - JUV B/F - VEHICLE FATALITY NOT DWI (intentional) - 1800-0600',
    '01802': 'MURDER - JUV B/F - VEHICLE FATALITY NOT DWI (intentional) - 0600-1800',
    '01803': 'MURDER - JUV B/F - VEHICLE FATALITY NOT DWI (intentional) - UNKNOWN',
    '01811': 'MURDER - JUV B/F - FIREARM - 1800-0600',
    '01812': 'MURDER - JUV B/F - FIREARM - 0600-1800',
    '01813': 'MURDER - JUV B/F - FIREARM - UNKNOWN',
    '01821': 'MURDER - JUV B/F - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '01822': 'MURDER - JUV B/F - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '01823': 'MURDER - JUV B/F - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '01831': 'MURDER - JUV B/F - BLUDGEON - 1800-0600',
    '01832': 'MURDER - JUV B/F - BLUDGEON - 0600-1800',
    '01833': 'MURDER - JUV B/F - BLUDGEON - UNKNOWN',
    '01841': 'MURDER - JUV B/F - STRANGULATION/HANDING, ETC - 1800-0600',
    '01842': 'MURDER - JUV B/F - STRANGULATION/HANDING, ETC - 0600-1800',
    '01843': 'MURDER - JUV B/F - STRANGULATION/HANDING, ETC - UNKNOWN',
    '01851': 'MURDER - JUV B/F - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '01852': 'MURDER - JUV B/F - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '01853': 'MURDER - JUV B/F - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '01861': 'MURDER - JUV B/F - POISON OR DRUGS - 1800-0600',
    '01862': 'MURDER - JUV B/F - POISON OR DRUGS - 0600-1800',
    '01863': 'MURDER - JUV B/F - POISON OR DRUGS - UNKNOWN',
    '01871': 'MURDER - JUV B/F - BURNING/SCALDING - 1800-0600',
    '01872': 'MURDER - JUV B/F - BURNING/SCALDING - 0600-1800',
    '01873': 'MURDER - JUV B/F - BURNING/SCALDING - UNKNOWN',
    '01881': 'MURDER - JUV B/F - INTOX MANSLAUGHTER (DWI FATAL) - 1800-0600',
    '01882': 'MURDER - JUV B/F - INTOX MANSLAUGHTER (DWI FATAL) - 0600-1800',
    '01883': 'MURDER - JUV B/F - INTOX MANSLAUGHTER (DWI FATAL) - UNKNOWN',
    '01891': 'MURDER - JUV B/F - NON-VEHICLE NEGLIGENT HOMICIDE - 1800-0600',
    '01892': 'MURDER - JUV B/F - NON-VEHICLE NEGLIGENT HOMICIDE - 0600-1800',
    '01893': 'MURDER - JUV B/F - NON-VEHICLE NEGLIGENT HOMICIDE - UNKNOWN',
    '01901': 'MURDER - POLICE OFFICER - VEHICLE FATALITY NOT DWI (intentional) - 1800-0600',
    '01902': 'MURDER - POLICE OFFICER - VEHICLE FATALITY NOT DWI (intentional) - 0600-1800',
    '01903': 'MURDER - POLICE OFFICER - VEHICLE FATALITY NOT DWI (intentional) - UNKNOWN',
    '01911': 'MURDER - POLICE OFFICER - FIREARM - 1800-0600',
    '01912': 'MURDER - POLICE OFFICER - FIREARM - 0600-1800',
    '01913': 'MURDER - POLICE OFFICER - FIREARM - UNKNOWN',
    '01921': 'MURDER - POLICE OFFICER - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '01922': 'MURDER - POLICE OFFICER - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '01923': 'MURDER - POLICE OFFICER - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '01931': 'MURDER - POLICE OFFICER - BLUDGEON - 1800-0600',
    '01932': 'MURDER - POLICE OFFICER - BLUDGEON - 0600-1800',
    '01933': 'MURDER - POLICE OFFICER - BLUDGEON - UNKNOWN',
    '01941': 'MURDER - POLICE OFFICER - STRANGULATION/HANDING, ETC - 1800-0600',
    '01942': 'MURDER - POLICE OFFICER - STRANGULATION/HANDING, ETC - 0600-1800',
    '01943': 'MURDER - POLICE OFFICER - STRANGULATION/HANDING, ETC - UNKNOWN',
    '01951': 'MURDER - POLICE OFFICER - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '01952': 'MURDER - POLICE OFFICER - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '01953': 'MURDER - POLICE OFFICER - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '01961': 'MURDER - POLICE OFFICER - POISON OR DRUGS - 1800-0600',
    '01962': 'MURDER - POLICE OFFICER - POISON OR DRUGS - 0600-1800',
    '01963': 'MURDER - POLICE OFFICER - POISON OR DRUGS - UNKNOWN',
    '01971': 'MURDER - POLICE OFFICER - BURNING/SCALDING - 1800-0600',
    '01972': 'MURDER - POLICE OFFICER - BURNING/SCALDING - 0600-1800',
    '01973': 'MURDER - POLICE OFFICER - BURNING/SCALDING - UNKNOWN',
    '01981': 'MURDER - POLICE OFFICER - INTOX MANSLAUGHTER (DWI FATAL) - 1800-0600',
    '01982': 'MURDER - POLICE OFFICER - INTOX MANSLAUGHTER (DWI FATAL) - 0600-1800',
    '01983': 'MURDER - POLICE OFFICER - INTOX MANSLAUGHTER (DWI FATAL) - UNKNOWN',
    '01991': 'MURDER - POLICE OFFICER - NON-VEHICLE NEGLIGENT HOMICIDE - 1800-0600',
    '01992': 'MURDER - POLICE OFFICER - NON-VEHICLE NEGLIGENT HOMICIDE - 0600-1800',
    '01993': 'MURDER - POLICE OFFICER - NON-VEHICLE NEGLIGENT HOMICIDE - UNKNOWN',
    '02011': 'RAPE - FIRE PERSON - FIREARMS - 1800-0600',
    '02012': 'RAPE - FIRE PERSON - FIREARMS - 0600-1800',
    '02013': 'RAPE - FIRE PERSON - FIREARMS - UNKNOWN',
    '02021': 'RAPE - FIRE PERSON - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '02022': 'RAPE - FIRE PERSON - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '02023': 'RAPE - FIRE PERSON - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '02031': 'RAPE - FIRE PERSON - STRANGULATION - 1800-0600',
    '02032': 'RAPE - FIRE PERSON - STRANGULATION - 0600-1800',
    '02033': 'RAPE - FIRE PERSON - STRANGULATION - UNKNOWN',
    '02041': 'RAPE - FIRE PERSON - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '02042': 'RAPE - FIRE PERSON - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '02043': 'RAPE - FIRE PERSON - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '02051': 'RAPE - FIRE PERSON - THREATS - 1800-0600',
    '02052': 'RAPE - FIRE PERSON - THREATS - 0600-1800',
    '02053': 'RAPE - FIRE PERSON - THREATS - UNKNOWN',
    '02061': 'RAPE - FIRE PERSON - ALL ATTEMPTS - 1800-0600',
    '02062': 'RAPE - FIRE PERSON - ALL ATTEMPTS - 0600-1800',
    '02063': 'RAPE - FIRE PERSON - ALL ATTEMPTS - UNKNOWN',
    '02071': 'RAPE - FIRE PERSON - OTHER WEAPON - 1800-0600',
    '02072': 'RAPE - FIRE PERSON - OTHER WEAPON - 0600-1800',
    '02073': 'RAPE - FIRE PERSON - OTHER WEAPON - UNKNOWN',
    '02111': 'RAPE - ADULT W/F - FIREARMS - 1800-0600',
    '02112': 'RAPE - ADULT W/F - FIREARMS - 0600-1800',
    '02113': 'RAPE - ADULT W/F - FIREARMS - UNKNOWN',
    '02121': 'RAPE - ADULT W/F - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '02122': 'RAPE - ADULT W/F - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '02123': 'RAPE - ADULT W/F - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '02131': 'RAPE - ADULT W/F - STRANGULATION - 1800-0600',
    '02132': 'RAPE - ADULT W/F - STRANGULATION - 0600-1800',
    '02133': 'RAPE - ADULT W/F - STRANGULATION - UNKNOWN',
    '02141': 'RAPE - ADULT W/F - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '02142': 'RAPE - ADULT W/F - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '02143': 'RAPE - ADULT W/F - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '02151': 'RAPE - ADULT W/F - THREATS - 1800-0600',
    '02152': 'RAPE - ADULT W/F - THREATS - 0600-1800',
    '02153': 'RAPE - ADULT W/F - THREATS - UNKNOWN',
    '02161': 'RAPE - ADULT W/F - ALL ATTEMPTS - 1800-0600',
    '02162': 'RAPE - ADULT W/F - ALL ATTEMPTS - 0600-1800',
    '02163': 'RAPE - ADULT W/F - ALL ATTEMPTS - UNKNOWN',
    '02171': 'RAPE - ADULT W/F - OTHER WEAPON - 1800-0600',
    '02172': 'RAPE - ADULT W/F - OTHER WEAPON - 0600-1800',
    '02173': 'RAPE - ADULT W/F - OTHER WEAPON - UNKNOWN',
    '02211': 'RAPE - JUV W/F NON-FV - FIREARMS - 1800-0600',
    '02212': 'RAPE - JUV W/F NON-FV - FIREARMS - 0600-1800',
    '02213': 'RAPE - JUV W/F NON-FV - FIREARMS - UNKNOWN',
    '02221': 'RAPE - JUV W/F NON-FV - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '02222': 'RAPE - JUV W/F NON-FV - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '02223': 'RAPE - JUV W/F NON-FV - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '02231': 'RAPE - JUV W/F NON-FV - STRANGULATION - 1800-0600',
    '02232': 'RAPE - JUV W/F NON-FV - STRANGULATION - 0600-1800',
    '02233': 'RAPE - JUV W/F NON-FV - STRANGULATION - UNKNOWN',
    '02241': 'RAPE - JUV W/F NON-FV - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '02242': 'RAPE - JUV W/F NON-FV - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '02243': 'RAPE - JUV W/F NON-FV - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '02251': 'RAPE - JUV W/F NON-FV - THREATS - 1800-0600',
    '02252': 'RAPE - JUV W/F NON-FV - THREATS - 0600-1800',
    '02253': 'RAPE - JUV W/F NON-FV - THREATS - UNKNOWN',
    '02261': 'RAPE - JUV W/F NON-FV - ALL ATTEMPTS - 1800-0600',
    '02262': 'RAPE - JUV W/F NON-FV - ALL ATTEMPTS - 0600-1800',
    '02263': 'RAPE - JUV W/F NON-FV - ALL ATTEMPTS - UNKNOWN',
    '02271': 'RAPE - JUV W/F NON-FV - OTHER WEAPON - 1800-0600',
    '02272': 'RAPE - JUV W/F NON-FV - OTHER WEAPON - 0600-1800',
    '02273': 'RAPE - JUV W/F NON-FV - OTHER WEAPON - UNKNOWN',
    '02311': 'RAPE - ADULT B/F - FIREARMS - 1800-0600',
    '02312': 'RAPE - ADULT B/F - FIREARMS - 0600-1800',
    '02313': 'RAPE - ADULT B/F - FIREARMS - UNKNOWN',
    '02321': 'RAPE - ADULT B/F - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '02322': 'RAPE - ADULT B/F - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '02323': 'RAPE - ADULT B/F - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '02331': 'RAPE - ADULT B/F - STRANGULATION - 1800-0600',
    '02332': 'RAPE - ADULT B/F - STRANGULATION - 0600-1800',
    '02333': 'RAPE - ADULT B/F - STRANGULATION - UNKNOWN',
    '02341': 'RAPE - ADULT B/F - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '02342': 'RAPE - ADULT B/F - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '02343': 'RAPE - ADULT B/F - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '02351': 'RAPE - ADULT B/F - THREATS - 1800-0600',
    '02352': 'RAPE - ADULT B/F - THREATS - 0600-1800',
    '02353': 'RAPE - ADULT B/F - THREATS - UNKNOWN',
    '02361': 'RAPE - ADULT B/F - ALL ATTEMPTS - 1800-0600',
    '02362': 'RAPE - ADULT B/F - ALL ATTEMPTS - 0600-1800',
    '02363': 'RAPE - ADULT B/F - ALL ATTEMPTS - UNKNOWN',
    '02371': 'RAPE - ADULT B/F - OTHER WEAPON - 1800-0600',
    '02372': 'RAPE - ADULT B/F - OTHER WEAPON - 0600-1800',
    '02373': 'RAPE - ADULT B/F - OTHER WEAPON - UNKNOWN',
    '02411': 'RAPE - JUV B/F NON-FV - FIREARMS - 1800-0600',
    '02412': 'RAPE - JUV B/F NON-FV - FIREARMS - 0600-1800',
    '02413': 'RAPE - JUV B/F NON-FV - FIREARMS - UNKNOWN',
    '02421': 'RAPE - JUV B/F NON-FV - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '02422': 'RAPE - JUV B/F NON-FV - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '02423': 'RAPE - JUV B/F NON-FV - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '02431': 'RAPE - JUV B/F NON-FV - STRANGULATION - 1800-0600',
    '02432': 'RAPE - JUV B/F NON-FV - STRANGULATION - 0600-1800',
    '02433': 'RAPE - JUV B/F NON-FV - STRANGULATION - UNKNOWN',
    '02441': 'RAPE - JUV B/F NON-FV - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '02442': 'RAPE - JUV B/F NON-FV - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '02443': 'RAPE - JUV B/F NON-FV - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '02451': 'RAPE - JUV B/F NON-FV - THREATS - 1800-0600',
    '02452': 'RAPE - JUV B/F NON-FV - THREATS - 0600-1800',
    '02453': 'RAPE - JUV B/F NON-FV - THREATS - UNKNOWN',
    '02461': 'RAPE - JUV B/F NON-FV - ALL ATTEMPTS - 1800-0600',
    '02462': 'RAPE - JUV B/F NON-FV - ALL ATTEMPTS - 0600-1800',
    '02463': 'RAPE - JUV B/F NON-FV - ALL ATTEMPTS - UNKNOWN',
    '02471': 'RAPE - JUV B/F NON-FV - OTHER WEAPON - 1800-0600',
    '02472': 'RAPE - JUV B/F NON-FV - OTHER WEAPON - 0600-1800',
    '02473': 'RAPE - JUV B/F NON-FV - OTHER WEAPON - UNKNOWN',
    '02511': 'RAPE - ** NOT USED ** - FIREARMS - 1800-0600',
    '02512': 'RAPE - ** NOT USED ** - FIREARMS - 0600-1800',
    '02513': 'RAPE - ** NOT USED ** - FIREARMS - UNKNOWN',
    '02521': 'RAPE - ** NOT USED ** - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '02522': 'RAPE - ** NOT USED ** - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '02523': 'RAPE - ** NOT USED ** - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '02531': 'RAPE - ** NOT USED ** - STRANGULATION - 1800-0600',
    '02532': 'RAPE - ** NOT USED ** - STRANGULATION - 0600-1800',
    '02533': 'RAPE - ** NOT USED ** - STRANGULATION - UNKNOWN',
    '02541': 'RAPE - ** NOT USED ** - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '02542': 'RAPE - ** NOT USED ** - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '02543': 'RAPE - ** NOT USED ** - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '02551': 'RAPE - ** NOT USED ** - THREATS - 1800-0600',
    '02552': 'RAPE - ** NOT USED ** - THREATS - 0600-1800',
    '02553': 'RAPE - ** NOT USED ** - THREATS - UNKNOWN',
    '02561': 'RAPE - ** NOT USED ** - ALL ATTEMPTS - 1800-0600',
    '02562': 'RAPE - ** NOT USED ** - ALL ATTEMPTS - 0600-1800',
    '02563': 'RAPE - ** NOT USED ** - ALL ATTEMPTS - UNKNOWN',
    '02571': 'RAPE - ** NOT USED ** - OTHER WEAPON - 1800-0600',
    '02572': 'RAPE - ** NOT USED ** - OTHER WEAPON - 0600-1800',
    '02573': 'RAPE - ** NOT USED ** - OTHER WEAPON - UNKNOWN',
    '02611': 'RAPE - JUV W/F FV - FIREARMS - 1800-0600',
    '02612': 'RAPE - JUV W/F FV - FIREARMS - 0600-1800',
    '02613': 'RAPE - JUV W/F FV - FIREARMS - UNKNOWN',
    '02621': 'RAPE - JUV W/F FV - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '02622': 'RAPE - JUV W/F FV - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '02623': 'RAPE - JUV W/F FV - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '02631': 'RAPE - JUV W/F FV - STRANGULATION - 1800-0600',
    '02632': 'RAPE - JUV W/F FV - STRANGULATION - 0600-1800',
    '02633': 'RAPE - JUV W/F FV - STRANGULATION - UNKNOWN',
    '02641': 'RAPE - JUV W/F FV - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '02642': 'RAPE - JUV W/F FV - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '02643': 'RAPE - JUV W/F FV - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '02651': 'RAPE - JUV W/F FV - THREATS - 1800-0600',
    '02652': 'RAPE - JUV W/F FV - THREATS - 0600-1800',
    '02653': 'RAPE - JUV W/F FV - THREATS - UNKNOWN',
    '02661': 'RAPE - JUV W/F FV - ALL ATTEMPTS - 1800-0600',
    '02662': 'RAPE - JUV W/F FV - ALL ATTEMPTS - 0600-1800',
    '02663': 'RAPE - JUV W/F FV - ALL ATTEMPTS - UNKNOWN',
    '02671': 'RAPE - JUV W/F FV - OTHER WEAPON - 1800-0600',
    '02672': 'RAPE - JUV W/F FV - OTHER WEAPON - 0600-1800',
    '02673': 'RAPE - JUV W/F FV - OTHER WEAPON - UNKNOWN',
    '02711': 'RAPE - JUV B/F FV - FIREARMS - 1800-0600',
    '02712': 'RAPE - JUV B/F FV - FIREARMS - 0600-1800',
    '02713': 'RAPE - JUV B/F FV - FIREARMS - UNKNOWN',
    '02721': 'RAPE - JUV B/F FV - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '02722': 'RAPE - JUV B/F FV - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '02723': 'RAPE - JUV B/F FV - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '02731': 'RAPE - JUV B/F FV - STRANGULATION - 1800-0600',
    '02732': 'RAPE - JUV B/F FV - STRANGULATION - 0600-1800',
    '02733': 'RAPE - JUV B/F FV - STRANGULATION - UNKNOWN',
    '02741': 'RAPE - JUV B/F FV - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '02742': 'RAPE - JUV B/F FV - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '02743': 'RAPE - JUV B/F FV - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '02751': 'RAPE - JUV B/F FV - THREATS - 1800-0600',
    '02752': 'RAPE - JUV B/F FV - THREATS - 0600-1800',
    '02753': 'RAPE - JUV B/F FV - THREATS - UNKNOWN',
    '02761': 'RAPE - JUV B/F FV - ALL ATTEMPTS - 1800-0600',
    '02762': 'RAPE - JUV B/F FV - ALL ATTEMPTS - 0600-1800',
    '02763': 'RAPE - JUV B/F FV - ALL ATTEMPTS - UNKNOWN',
    '02771': 'RAPE - JUV B/F FV - OTHER WEAPON - 1800-0600',
    '02772': 'RAPE - JUV B/F FV - OTHER WEAPON - 0600-1800',
    '02773': 'RAPE - JUV B/F FV - OTHER WEAPON - UNKNOWN',
    '02811': 'RAPE - ** NOT USED ** - FIREARMS - 1800-0600',
    '02812': 'RAPE - ** NOT USED ** - FIREARMS - 0600-1800',
    '02813': 'RAPE - ** NOT USED ** - FIREARMS - UNKNOWN',
    '02821': 'RAPE - ** NOT USED ** - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '02822': 'RAPE - ** NOT USED ** - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '02823': 'RAPE - ** NOT USED ** - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '02831': 'RAPE - ** NOT USED ** - STRANGULATION - 1800-0600',
    '02832': 'RAPE - ** NOT USED ** - STRANGULATION - 0600-1800',
    '02833': 'RAPE - ** NOT USED ** - STRANGULATION - UNKNOWN',
    '02841': 'RAPE - ** NOT USED ** - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '02842': 'RAPE - ** NOT USED ** - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '02843': 'RAPE - ** NOT USED ** - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '02851': 'RAPE - ** NOT USED ** - THREATS - 1800-0600',
    '02852': 'RAPE - ** NOT USED ** - THREATS - 0600-1800',
    '02853': 'RAPE - ** NOT USED ** - THREATS - UNKNOWN',
    '02861': 'RAPE - ** NOT USED ** - ALL ATTEMPTS - 1800-0600',
    '02862': 'RAPE - ** NOT USED ** - ALL ATTEMPTS - 0600-1800',
    '02863': 'RAPE - ** NOT USED ** - ALL ATTEMPTS - UNKNOWN',
    '02871': 'RAPE - ** NOT USED ** - OTHER WEAPON - 1800-0600',
    '02872': 'RAPE - ** NOT USED ** - OTHER WEAPON - 0600-1800',
    '02873': 'RAPE - ** NOT USED ** - OTHER WEAPON - UNKNOWN',
    '02911': 'RAPE - POLICE OFFICER - FIREARMS - 1800-0600',
    '02912': 'RAPE - POLICE OFFICER - FIREARMS - 0600-1800',
    '02913': 'RAPE - POLICE OFFICER - FIREARMS - UNKNOWN',
    '02921': 'RAPE - POLICE OFFICER - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '02922': 'RAPE - POLICE OFFICER - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '02923': 'RAPE - POLICE OFFICER - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '02931': 'RAPE - POLICE OFFICER - STRANGULATION - 1800-0600',
    '02932': 'RAPE - POLICE OFFICER - STRANGULATION - 0600-1800',
    '02933': 'RAPE - POLICE OFFICER - STRANGULATION - UNKNOWN',
    '02941': 'RAPE - POLICE OFFICER - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '02942': 'RAPE - POLICE OFFICER - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '02943': 'RAPE - POLICE OFFICER - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '02951': 'RAPE - POLICE OFFICER - THREATS - 1800-0600',
    '02952': 'RAPE - POLICE OFFICER - THREATS - 0600-1800',
    '02953': 'RAPE - POLICE OFFICER - THREATS - UNKNOWN',
    '02961': 'RAPE - POLICE OFFICER - ALL ATTEMPTS - 1800-0600',
    '02962': 'RAPE - POLICE OFFICER - ALL ATTEMPTS - 0600-1800',
    '02963': 'RAPE - POLICE OFFICER - ALL ATTEMPTS - UNKNOWN',
    '02971': 'RAPE - POLICE OFFICER - OTHER WEAPON - 1800-0600',
    '02972': 'RAPE - POLICE OFFICER - OTHER WEAPON - 0600-1800',
    '02973': 'RAPE - POLICE OFFICER - OTHER WEAPON - UNKNOWN',
    '03111': 'ROBBERY - HWY,ST,ALLEY - FIREARMS (HANDGUN ONLY) - 1800-0600',
    '03112': 'ROBBERY - HWY,ST,ALLEY - FIREARMS (HANDGUN ONLY) - 0600-1800',
    '03113': 'ROBBERY - HWY,ST,ALLEY - FIREARMS (HANDGUN ONLY) - UNKNOWN',
    '03121': 'ROBBERY - HWY,ST,ALLEY - FIREARMS (ALL EXC HANDGUNS) - 1800-0600',
    '03122': 'ROBBERY - HWY,ST,ALLEY - FIREARMS (ALL EXC HANDGUNS) - 0600-1800',
    '03123': 'ROBBERY - HWY,ST,ALLEY - FIREARMS (ALL EXC HANDGUNS) - UNKNOWN',
    '03131': 'ROBBERY - HWY,ST,ALLEY - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '03132': 'ROBBERY - HWY,ST,ALLEY - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '03133': 'ROBBERY - HWY,ST,ALLEY - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '03141': 'ROBBERY - HWY,ST,ALLEY - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '03142': 'ROBBERY - HWY,ST,ALLEY - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '03143': 'ROBBERY - HWY,ST,ALLEY - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '03151': 'ROBBERY - HWY,ST,ALLEY - EXPLOSIVE (ACTUAL OR THREATEN) - 1800-0600',
    '03152': 'ROBBERY - HWY,ST,ALLEY - EXPLOSIVE (ACTUAL OR THREATEN) - 0600-1800',
    '03153': 'ROBBERY - HWY,ST,ALLEY - EXPLOSIVE (ACTUAL OR THREATEN) - UNKNOWN',
    '03161': 'ROBBERY - HWY,ST,ALLEY - OTHER THREATS - 1800-0600',
    '03162': 'ROBBERY - HWY,ST,ALLEY - OTHER THREATS - 0600-1800',
    '03163': 'ROBBERY - HWY,ST,ALLEY - OTHER THREATS - UNKNOWN',
    '03171': 'ROBBERY - HWY,ST,ALLEY - OTHER WEAPON - 1800-0600',
    '03172': 'ROBBERY - HWY,ST,ALLEY - OTHER WEAPON - 0600-1800',
    '03173': 'ROBBERY - HWY,ST,ALLEY - OTHER WEAPON - UNKNOWN',
    '03211': 'ROBBERY - COMM HSE,X-3,4,6 - FIREARMS (HANDGUN ONLY) - 1800-0600',
    '03212': 'ROBBERY - COMM HSE,X-3,4,6 - FIREARMS (HANDGUN ONLY) - 0600-1800',
    '03213': 'ROBBERY - COMM HSE,X-3,4,6 - FIREARMS (HANDGUN ONLY) - UNKNOWN',
    '03221': 'ROBBERY - COMM HSE,X-3,4,6 - FIREARMS (ALL EXC HANDGUNS) - 1800-0600',
    '03222': 'ROBBERY - COMM HSE,X-3,4,6 - FIREARMS (ALL EXC HANDGUNS) - 0600-1800',
    '03223': 'ROBBERY - COMM HSE,X-3,4,6 - FIREARMS (ALL EXC HANDGUNS) - UNKNOWN',
    '03231': 'ROBBERY - COMM HSE,X-3,4,6 - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '03232': 'ROBBERY - COMM HSE,X-3,4,6 - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '03233': 'ROBBERY - COMM HSE,X-3,4,6 - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '03241': 'ROBBERY - COMM HSE,X-3,4,6 - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '03242': 'ROBBERY - COMM HSE,X-3,4,6 - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '03243': 'ROBBERY - COMM HSE,X-3,4,6 - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '03251': 'ROBBERY - COMM HSE,X-3,4,6 - EXPLOSIVE (ACTUAL OR THREATEN) - 1800-0600',
    '03252': 'ROBBERY - COMM HSE,X-3,4,6 - EXPLOSIVE (ACTUAL OR THREATEN) - 0600-1800',
    '03253': 'ROBBERY - COMM HSE,X-3,4,6 - EXPLOSIVE (ACTUAL OR THREATEN) - UNKNOWN',
    '03261': 'ROBBERY - COMM HSE,X-3,4,6 - OTHER THREATS - 1800-0600',
    '03262': 'ROBBERY - COMM HSE,X-3,4,6 - OTHER THREATS - 0600-1800',
    '03263': 'ROBBERY - COMM HSE,X-3,4,6 - OTHER THREATS - UNKNOWN',
    '03271': 'ROBBERY - COMM HSE,X-3,4,6 - OTHER WEAPON - 1800-0600',
    '03272': 'ROBBERY - COMM HSE,X-3,4,6 - OTHER WEAPON - 0600-1800',
    '03273': 'ROBBERY - COMM HSE,X-3,4,6 - OTHER WEAPON - UNKNOWN',
    '03311': 'ROBBERY - GAS/SERV STA - FIREARMS (HANDGUN ONLY) - 1800-0600',
    '03312': 'ROBBERY - GAS/SERV STA - FIREARMS (HANDGUN ONLY) - 0600-1800',
    '03313': 'ROBBERY - GAS/SERV STA - FIREARMS (HANDGUN ONLY) - UNKNOWN',
    '03321': 'ROBBERY - GAS/SERV STA - FIREARMS (ALL EXC HANDGUNS) - 1800-0600',
    '03322': 'ROBBERY - GAS/SERV STA - FIREARMS (ALL EXC HANDGUNS) - 0600-1800',
    '03323': 'ROBBERY - GAS/SERV STA - FIREARMS (ALL EXC HANDGUNS) - UNKNOWN',
    '03331': 'ROBBERY - GAS/SERV STA - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '03332': 'ROBBERY - GAS/SERV STA - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '03333': 'ROBBERY - GAS/SERV STA - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '03341': 'ROBBERY - GAS/SERV STA - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '03342': 'ROBBERY - GAS/SERV STA - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '03343': 'ROBBERY - GAS/SERV STA - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '03351': 'ROBBERY - GAS/SERV STA - EXPLOSIVE (ACTUAL OR THREATEN) - 1800-0600',
    '03352': 'ROBBERY - GAS/SERV STA - EXPLOSIVE (ACTUAL OR THREATEN) - 0600-1800',
    '03353': 'ROBBERY - GAS/SERV STA - EXPLOSIVE (ACTUAL OR THREATEN) - UNKNOWN',
    '03361': 'ROBBERY - GAS/SERV STA - OTHER THREATS - 1800-0600',
    '03362': 'ROBBERY - GAS/SERV STA - OTHER THREATS - 0600-1800',
    '03363': 'ROBBERY - GAS/SERV STA - OTHER THREATS - UNKNOWN',
    '03371': 'ROBBERY - GAS/SERV STA - OTHER WEAPON - 1800-0600',
    '03372': 'ROBBERY - GAS/SERV STA - OTHER WEAPON - 0600-1800',
    '03373': 'ROBBERY - GAS/SERV STA - OTHER WEAPON - UNKNOWN',
    '03411': 'ROBBERY - CHAIN STORE - FIREARMS (HANDGUN ONLY) - 1800-0600',
    '03412': 'ROBBERY - CHAIN STORE - FIREARMS (HANDGUN ONLY) - 0600-1800',
    '03413': 'ROBBERY - CHAIN STORE - FIREARMS (HANDGUN ONLY) - UNKNOWN',
    '03421': 'ROBBERY - CHAIN STORE - FIREARMS (ALL EXC HANDGUNS) - 1800-0600',
    '03422': 'ROBBERY - CHAIN STORE - FIREARMS (ALL EXC HANDGUNS) - 0600-1800',
    '03423': 'ROBBERY - CHAIN STORE - FIREARMS (ALL EXC HANDGUNS) - UNKNOWN',
    '03431': 'ROBBERY - CHAIN STORE - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '03432': 'ROBBERY - CHAIN STORE - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '03433': 'ROBBERY - CHAIN STORE - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '03441': 'ROBBERY - CHAIN STORE - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '03442': 'ROBBERY - CHAIN STORE - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '03443': 'ROBBERY - CHAIN STORE - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '03451': 'ROBBERY - CHAIN STORE - EXPLOSIVE (ACTUAL OR THREATEN) - 1800-0600',
    '03452': 'ROBBERY - CHAIN STORE - EXPLOSIVE (ACTUAL OR THREATEN) - 0600-1800',
    '03453': 'ROBBERY - CHAIN STORE - EXPLOSIVE (ACTUAL OR THREATEN) - UNKNOWN',
    '03461': 'ROBBERY - CHAIN STORE - OTHER THREATS - 1800-0600',
    '03462': 'ROBBERY - CHAIN STORE - OTHER THREATS - 0600-1800',
    '03463': 'ROBBERY - CHAIN STORE - OTHER THREATS - UNKNOWN',
    '03471': 'ROBBERY - CHAIN STORE - OTHER WEAPON - 1800-0600',
    '03472': 'ROBBERY - CHAIN STORE - OTHER WEAPON - 0600-1800',
    '03473': 'ROBBERY - CHAIN STORE - OTHER WEAPON - UNKNOWN',
    '03511': 'ROBBERY - RESID(ALL) - FIREARMS (HANDGUN ONLY) - 1800-0600',
    '03512': 'ROBBERY - RESID(ALL) - FIREARMS (HANDGUN ONLY) - 0600-1800',
    '03513': 'ROBBERY - RESID(ALL) - FIREARMS (HANDGUN ONLY) - UNKNOWN',
    '03521': 'ROBBERY - RESID(ALL) - FIREARMS (ALL EXC HANDGUNS) - 1800-0600',
    '03522': 'ROBBERY - RESID(ALL) - FIREARMS (ALL EXC HANDGUNS) - 0600-1800',
    '03523': 'ROBBERY - RESID(ALL) - FIREARMS (ALL EXC HANDGUNS) - UNKNOWN',
    '03531': 'ROBBERY - RESID(ALL) - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '03532': 'ROBBERY - RESID(ALL) - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '03533': 'ROBBERY - RESID(ALL) - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '03541': 'ROBBERY - RESID(ALL) - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '03542': 'ROBBERY - RESID(ALL) - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '03543': 'ROBBERY - RESID(ALL) - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '03551': 'ROBBERY - RESID(ALL) - EXPLOSIVE (ACTUAL OR THREATEN) - 1800-0600',
    '03552': 'ROBBERY - RESID(ALL) - EXPLOSIVE (ACTUAL OR THREATEN) - 0600-1800',
    '03553': 'ROBBERY - RESID(ALL) - EXPLOSIVE (ACTUAL OR THREATEN) - UNKNOWN',
    '03561': 'ROBBERY - RESID(ALL) - OTHER THREATS - 1800-0600',
    '03562': 'ROBBERY - RESID(ALL) - OTHER THREATS - 0600-1800',
    '03563': 'ROBBERY - RESID(ALL) - OTHER THREATS - UNKNOWN',
    '03571': 'ROBBERY - RESID(ALL) - OTHER WEAPON - 1800-0600',
    '03572': 'ROBBERY - RESID(ALL) - OTHER WEAPON - 0600-1800',
    '03573': 'ROBBERY - RESID(ALL) - OTHER WEAPON - UNKNOWN',
    '03611': 'ROBBERY - BANK - FIREARMS (HANDGUN ONLY) - 1800-0600',
    '03612': 'ROBBERY - BANK - FIREARMS (HANDGUN ONLY) - 0600-1800',
    '03613': 'ROBBERY - BANK - FIREARMS (HANDGUN ONLY) - UNKNOWN',
    '03621': 'ROBBERY - BANK - FIREARMS (ALL EXC HANDGUNS) - 1800-0600',
    '03622': 'ROBBERY - BANK - FIREARMS (ALL EXC HANDGUNS) - 0600-1800',
    '03623': 'ROBBERY - BANK - FIREARMS (ALL EXC HANDGUNS) - UNKNOWN',
    '03631': 'ROBBERY - BANK - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '03632': 'ROBBERY - BANK - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '03633': 'ROBBERY - BANK - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '03641': 'ROBBERY - BANK - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '03642': 'ROBBERY - BANK - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '03643': 'ROBBERY - BANK - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '03651': 'ROBBERY - BANK - EXPLOSIVE (ACTUAL OR THREATEN) - 1800-0600',
    '03652': 'ROBBERY - BANK - EXPLOSIVE (ACTUAL OR THREATEN) - 0600-1800',
    '03653': 'ROBBERY - BANK - EXPLOSIVE (ACTUAL OR THREATEN) - UNKNOWN',
    '03661': 'ROBBERY - BANK - OTHER THREATS - 1800-0600',
    '03662': 'ROBBERY - BANK - OTHER THREATS - 0600-1800',
    '03663': 'ROBBERY - BANK - OTHER THREATS - UNKNOWN',
    '03671': 'ROBBERY - BANK - OTHER WEAPON - 1800-0600',
    '03672': 'ROBBERY - BANK - OTHER WEAPON - 0600-1800',
    '03673': 'ROBBERY - BANK - OTHER WEAPON - UNKNOWN',
    '03711': 'ROBBERY - MISC BUSINESS - FIREARMS (HANDGUN ONLY) - 1800-0600',
    '03712': 'ROBBERY - MISC BUSINESS - FIREARMS (HANDGUN ONLY) - 0600-1800',
    '03713': 'ROBBERY - MISC BUSINESS - FIREARMS (HANDGUN ONLY) - UNKNOWN',
    '03721': 'ROBBERY - MISC BUSINESS - FIREARMS (ALL EXC HANDGUNS) - 1800-0600',
    '03722': 'ROBBERY - MISC BUSINESS - FIREARMS (ALL EXC HANDGUNS) - 0600-1800',
    '03723': 'ROBBERY - MISC BUSINESS - FIREARMS (ALL EXC HANDGUNS) - UNKNOWN',
    '03731': 'ROBBERY - MISC BUSINESS - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '03732': 'ROBBERY - MISC BUSINESS - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '03733': 'ROBBERY - MISC BUSINESS - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '03741': 'ROBBERY - MISC BUSINESS - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '03742': 'ROBBERY - MISC BUSINESS - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '03743': 'ROBBERY - MISC BUSINESS - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '03751': 'ROBBERY - MISC BUSINESS - EXPLOSIVE (ACTUAL OR THREATEN) - 1800-0600',
    '03752': 'ROBBERY - MISC BUSINESS - EXPLOSIVE (ACTUAL OR THREATEN) - 0600-1800',
    '03753': 'ROBBERY - MISC BUSINESS - EXPLOSIVE (ACTUAL OR THREATEN) - UNKNOWN',
    '03761': 'ROBBERY - MISC BUSINESS - OTHER THREATS - 1800-0600',
    '03762': 'ROBBERY - MISC BUSINESS - OTHER THREATS - 0600-1800',
    '03763': 'ROBBERY - MISC BUSINESS - OTHER THREATS - UNKNOWN',
    '03771': 'ROBBERY - MISC BUSINESS - OTHER WEAPON - 1800-0600',
    '03772': 'ROBBERY - MISC BUSINESS - OTHER WEAPON - 0600-1800',
    '03773': 'ROBBERY - MISC BUSINESS - OTHER WEAPON - UNKNOWN',
    '03811': 'ROBBERY - HGHWY-INDIV - FIREARMS (HANDGUN ONLY) - 1800-0600',
    '03812': 'ROBBERY - HGHWY-INDIV - FIREARMS (HANDGUN ONLY) - 0600-1800',
    '03813': 'ROBBERY - HGHWY-INDIV - FIREARMS (HANDGUN ONLY) - UNKNOWN',
    '03821': 'ROBBERY - HGHWY-INDIV - FIREARMS (ALL EXC HANDGUNS) - 1800-0600',
    '03822': 'ROBBERY - HGHWY-INDIV - FIREARMS (ALL EXC HANDGUNS) - 0600-1800',
    '03823': 'ROBBERY - HGHWY-INDIV - FIREARMS (ALL EXC HANDGUNS) - UNKNOWN',
    '03831': 'ROBBERY - HGHWY-INDIV - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '03832': 'ROBBERY - HGHWY-INDIV - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '03833': 'ROBBERY - HGHWY-INDIV - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '03841': 'ROBBERY - HGHWY-INDIV - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '03842': 'ROBBERY - HGHWY-INDIV - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '03843': 'ROBBERY - HGHWY-INDIV - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '03851': 'ROBBERY - HGHWY-INDIV - EXPLOSIVE (ACTUAL OR THREATEN) - 1800-0600',
    '03852': 'ROBBERY - HGHWY-INDIV - EXPLOSIVE (ACTUAL OR THREATEN) - 0600-1800',
    '03853': 'ROBBERY - HGHWY-INDIV - EXPLOSIVE (ACTUAL OR THREATEN) - UNKNOWN',
    '03861': 'ROBBERY - HGHWY-INDIV - OTHER THREATS - 1800-0600',
    '03862': 'ROBBERY - HGHWY-INDIV - OTHER THREATS - 0600-1800',
    '03863': 'ROBBERY - HGHWY-INDIV - OTHER THREATS - UNKNOWN',
    '03871': 'ROBBERY - HGHWY-INDIV - OTHER WEAPON - 1800-0600',
    '03872': 'ROBBERY - HGHWY-INDIV - OTHER WEAPON - 0600-1800',
    '03873': 'ROBBERY - HGHWY-INDIV - OTHER WEAPON - UNKNOWN',
    '03911': 'ROBBERY - MISC LOCAT-INDIV - FIREARMS (HANDGUN ONLY) - 1800-0600',
    '03912': 'ROBBERY - MISC LOCAT-INDIV - FIREARMS (HANDGUN ONLY) - 0600-1800',
    '03913': 'ROBBERY - MISC LOCAT-INDIV - FIREARMS (HANDGUN ONLY) - UNKNOWN',
    '03921': 'ROBBERY - MISC LOCAT-INDIV - FIREARMS (ALL EXC HANDGUNS) - 1800-0600',
    '03922': 'ROBBERY - MISC LOCAT-INDIV - FIREARMS (ALL EXC HANDGUNS) - 0600-1800',
    '03923': 'ROBBERY - MISC LOCAT-INDIV - FIREARMS (ALL EXC HANDGUNS) - UNKNOWN',
    '03931': 'ROBBERY - MISC LOCAT-INDIV - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '03932': 'ROBBERY - MISC LOCAT-INDIV - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '03933': 'ROBBERY - MISC LOCAT-INDIV - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '03941': 'ROBBERY - MISC LOCAT-INDIV - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '03942': 'ROBBERY - MISC LOCAT-INDIV - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '03943': 'ROBBERY - MISC LOCAT-INDIV - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '03951': 'ROBBERY - MISC LOCAT-INDIV - EXPLOSIVE (ACTUAL OR THREATEN) - 1800-0600',
    '03952': 'ROBBERY - MISC LOCAT-INDIV - EXPLOSIVE (ACTUAL OR THREATEN) - 0600-1800',
    '03953': 'ROBBERY - MISC LOCAT-INDIV - EXPLOSIVE (ACTUAL OR THREATEN) - UNKNOWN',
    '03961': 'ROBBERY - MISC LOCAT-INDIV - OTHER THREATS - 1800-0600',
    '03962': 'ROBBERY - MISC LOCAT-INDIV - OTHER THREATS - 0600-1800',
    '03963': 'ROBBERY - MISC LOCAT-INDIV - OTHER THREATS - UNKNOWN',
    '03971': 'ROBBERY - MISC LOCAT-INDIV - OTHER WEAPON - 1800-0600',
    '03972': 'ROBBERY - MISC LOCAT-INDIV - OTHER WEAPON - 0600-1800',
    '03973': 'ROBBERY - MISC LOCAT-INDIV - OTHER WEAPON - UNKNOWN',
    '03A11': 'ROBBERY - CARJACK-INDIV - FIREARMS (HANDGUN ONLY) - 1800-0600',
    '03A12': 'ROBBERY - CARJACK-INDIV - FIREARMS (HANDGUN ONLY) - 0600-1800',
    '03A13': 'ROBBERY - CARJACK-INDIV - FIREARMS (HANDGUN ONLY) - UNKNOWN',
    '03A21': 'ROBBERY - CARJACK-INDIV - FIREARMS (ALL EXC HANDGUNS) - 1800-0600',
    '03A22': 'ROBBERY - CARJACK-INDIV - FIREARMS (ALL EXC HANDGUNS) - 0600-1800',
    '03A23': 'ROBBERY - CARJACK-INDIV - FIREARMS (ALL EXC HANDGUNS) - UNKNOWN',
    '03A31': 'ROBBERY - CARJACK-INDIV - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '03A32': 'ROBBERY - CARJACK-INDIV - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '03A33': 'ROBBERY - CARJACK-INDIV - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '03A41': 'ROBBERY - CARJACK-INDIV - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '03A42': 'ROBBERY - CARJACK-INDIV - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '03A43': 'ROBBERY - CARJACK-INDIV - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '03A51': 'ROBBERY - CARJACK-INDIV - EXPLOSIVE (ACTUAL OR THREATEN) - 1800-0600',
    '03A52': 'ROBBERY - CARJACK-INDIV - EXPLOSIVE (ACTUAL OR THREATEN) - 0600-1800',
    '03A53': 'ROBBERY - CARJACK-INDIV - EXPLOSIVE (ACTUAL OR THREATEN) - UNKNOWN',
    '03A61': 'ROBBERY - CARJACK-INDIV - OTHER THREATS - 1800-0600',
    '03A62': 'ROBBERY - CARJACK-INDIV - OTHER THREATS - 0600-1800',
    '03A63': 'ROBBERY - CARJACK-INDIV - OTHER THREATS - UNKNOWN',
    '03A71': 'ROBBERY - CARJACK-INDIV - OTHER WEAPON - 1800-0600',
    '03A72': 'ROBBERY - CARJACK-INDIV - OTHER WEAPON - 0600-1800',
    '03A73': 'ROBBERY - CARJACK-INDIV - OTHER WEAPON - UNKNOWN',
    '03B11': 'ROBBERY - CARJACK-BUS - FIREARMS (HANDGUN ONLY) - 1800-0600',
    '03B12': 'ROBBERY - CARJACK-BUS - FIREARMS (HANDGUN ONLY) - 0600-1800',
    '03B13': 'ROBBERY - CARJACK-BUS - FIREARMS (HANDGUN ONLY) - UNKNOWN',
    '03B21': 'ROBBERY - CARJACK-BUS - FIREARMS (ALL EXC HANDGUNS) - 1800-0600',
    '03B22': 'ROBBERY - CARJACK-BUS - FIREARMS (ALL EXC HANDGUNS) - 0600-1800',
    '03B23': 'ROBBERY - CARJACK-BUS - FIREARMS (ALL EXC HANDGUNS) - UNKNOWN',
    '03B31': 'ROBBERY - CARJACK-BUS - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '03B32': 'ROBBERY - CARJACK-BUS - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '03B33': 'ROBBERY - CARJACK-BUS - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '03B41': 'ROBBERY - CARJACK-BUS - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '03B42': 'ROBBERY - CARJACK-BUS - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '03B43': 'ROBBERY - CARJACK-BUS - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '03B51': 'ROBBERY - CARJACK-BUS - EXPLOSIVE (ACTUAL OR THREATEN) - 1800-0600',
    '03B52': 'ROBBERY - CARJACK-BUS - EXPLOSIVE (ACTUAL OR THREATEN) - 0600-1800',
    '03B53': 'ROBBERY - CARJACK-BUS - EXPLOSIVE (ACTUAL OR THREATEN) - UNKNOWN',
    '03B61': 'ROBBERY - CARJACK-BUS - OTHER THREATS - 1800-0600',
    '03B62': 'ROBBERY - CARJACK-BUS - OTHER THREATS - 0600-1800',
    '03B63': 'ROBBERY - CARJACK-BUS - OTHER THREATS - UNKNOWN',
    '03B71': 'ROBBERY - CARJACK-BUS - OTHER WEAPON - 1800-0600',
    '03B72': 'ROBBERY - CARJACK-BUS - OTHER WEAPON - 0600-1800',
    '03B73': 'ROBBERY - CARJACK-BUS - OTHER WEAPON - UNKNOWN',
    '04011': 'AGG ASSAULT - FIRE PERSON - FIREARMS (ALL GUNS) - 1800-0600',
    '04012': 'AGG ASSAULT - FIRE PERSON - FIREARMS (ALL GUNS) - 0600-1800',
    '04013': 'AGG ASSAULT - FIRE PERSON - FIREARMS (ALL GUNS) - UNKNOWN',
    '04021': 'AGG ASSAULT - FIRE PERSON - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '04022': 'AGG ASSAULT - FIRE PERSON - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '04023': 'AGG ASSAULT - FIRE PERSON - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '04031': 'AGG ASSAULT - FIRE PERSON - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '04032': 'AGG ASSAULT - FIRE PERSON - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '04033': 'AGG ASSAULT - FIRE PERSON - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '04041': 'AGG ASSAULT - FIRE PERSON - ADMINISTERING POISON - 1800-0600',
    '04042': 'AGG ASSAULT - FIRE PERSON - ADMINISTERING POISON - 0600-1800',
    '04043': 'AGG ASSAULT - FIRE PERSON - ADMINISTERING POISON - UNKNOWN',
    '04051': 'AGG ASSAULT - FIRE PERSON - MAIMING, CASTRATION - 1800-0600',
    '04052': 'AGG ASSAULT - FIRE PERSON - MAIMING, CASTRATION - 0600-1800',
    '04053': 'AGG ASSAULT - FIRE PERSON - MAIMING, CASTRATION - UNKNOWN',
    '04061': 'AGG ASSAULT - FIRE PERSON - MOTOR VEHICLE (INTENTIONAL) - 1800-0600',
    '04062': 'AGG ASSAULT - FIRE PERSON - MOTOR VEHICLE (INTENTIONAL) - 0600-1800',
    '04063': 'AGG ASSAULT - FIRE PERSON - MOTOR VEHICLE (INTENTIONAL) - UNKNOWN',
    '04071': 'AGG ASSAULT - FIRE PERSON - OTHER DANGEROUS WEAPON - 1800-0600',
    '04072': 'AGG ASSAULT - FIRE PERSON - OTHER DANGEROUS WEAPON - 0600-1800',
    '04073': 'AGG ASSAULT - FIRE PERSON - OTHER DANGEROUS WEAPON - UNKNOWN',
    '04081': 'AGG ASSAULT - FIRE PERSON - ABUSE (ALL PARENT/GUARD ABUSE) - 1800-0600',
    '04082': 'AGG ASSAULT - FIRE PERSON - ABUSE (ALL PARENT/GUARD ABUSE) - 0600-1800',
    '04083': 'AGG ASSAULT - FIRE PERSON - ABUSE (ALL PARENT/GUARD ABUSE) - UNKNOWN',
    '04091': 'AGG ASSAULT - FIRE PERSON - AIDING SUICIDE - 1800-0600',
    '04092': 'AGG ASSAULT - FIRE PERSON - AIDING SUICIDE - 0600-1800',
    '04093': 'AGG ASSAULT - FIRE PERSON - AIDING SUICIDE - UNKNOWN',
    '04111': 'AGG ASSAULT - ADULT W/M - FIREARMS (ALL GUNS) - 1800-0600',
    '04112': 'AGG ASSAULT - ADULT W/M - FIREARMS (ALL GUNS) - 0600-1800',
    '04113': 'AGG ASSAULT - ADULT W/M - FIREARMS (ALL GUNS) - UNKNOWN',
    '04121': 'AGG ASSAULT - ADULT W/M - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '04122': 'AGG ASSAULT - ADULT W/M - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '04123': 'AGG ASSAULT - ADULT W/M - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '04131': 'AGG ASSAULT - ADULT W/M - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '04132': 'AGG ASSAULT - ADULT W/M - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '04133': 'AGG ASSAULT - ADULT W/M - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '04141': 'AGG ASSAULT - ADULT W/M - ADMINISTERING POISON - 1800-0600',
    '04142': 'AGG ASSAULT - ADULT W/M - ADMINISTERING POISON - 0600-1800',
    '04143': 'AGG ASSAULT - ADULT W/M - ADMINISTERING POISON - UNKNOWN',
    '04151': 'AGG ASSAULT - ADULT W/M - MAIMING, CASTRATION - 1800-0600',
    '04152': 'AGG ASSAULT - ADULT W/M - MAIMING, CASTRATION - 0600-1800',
    '04153': 'AGG ASSAULT - ADULT W/M - MAIMING, CASTRATION - UNKNOWN',
    '04161': 'AGG ASSAULT - ADULT W/M - MOTOR VEHICLE (INTENTIONAL) - 1800-0600',
    '04162': 'AGG ASSAULT - ADULT W/M - MOTOR VEHICLE (INTENTIONAL) - 0600-1800',
    '04163': 'AGG ASSAULT - ADULT W/M - MOTOR VEHICLE (INTENTIONAL) - UNKNOWN',
    '04171': 'AGG ASSAULT - ADULT W/M - OTHER DANGEROUS WEAPON - 1800-0600',
    '04172': 'AGG ASSAULT - ADULT W/M - OTHER DANGEROUS WEAPON - 0600-1800',
    '04173': 'AGG ASSAULT - ADULT W/M - OTHER DANGEROUS WEAPON - UNKNOWN',
    '04181': 'AGG ASSAULT - ADULT W/M - ABUSE (ALL PARENT/GUARD ABUSE) - 1800-0600',
    '04182': 'AGG ASSAULT - ADULT W/M - ABUSE (ALL PARENT/GUARD ABUSE) - 0600-1800',
    '04183': 'AGG ASSAULT - ADULT W/M - ABUSE (ALL PARENT/GUARD ABUSE) - UNKNOWN',
    '04191': 'AGG ASSAULT - ADULT W/M - AIDING SUICIDE - 1800-0600',
    '04192': 'AGG ASSAULT - ADULT W/M - AIDING SUICIDE - 0600-1800',
    '04193': 'AGG ASSAULT - ADULT W/M - AIDING SUICIDE - UNKNOWN',
    '04211': 'AGG ASSAULT - ADULT B/M - FIREARMS (ALL GUNS) - 1800-0600',
    '04212': 'AGG ASSAULT - ADULT B/M - FIREARMS (ALL GUNS) - 0600-1800',
    '04213': 'AGG ASSAULT - ADULT B/M - FIREARMS (ALL GUNS) - UNKNOWN',
    '04221': 'AGG ASSAULT - ADULT B/M - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '04222': 'AGG ASSAULT - ADULT B/M - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '04223': 'AGG ASSAULT - ADULT B/M - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '04231': 'AGG ASSAULT - ADULT B/M - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '04232': 'AGG ASSAULT - ADULT B/M - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '04233': 'AGG ASSAULT - ADULT B/M - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '04241': 'AGG ASSAULT - ADULT B/M - ADMINISTERING POISON - 1800-0600',
    '04242': 'AGG ASSAULT - ADULT B/M - ADMINISTERING POISON - 0600-1800',
    '04243': 'AGG ASSAULT - ADULT B/M - ADMINISTERING POISON - UNKNOWN',
    '04251': 'AGG ASSAULT - ADULT B/M - MAIMING, CASTRATION - 1800-0600',
    '04252': 'AGG ASSAULT - ADULT B/M - MAIMING, CASTRATION - 0600-1800',
    '04253': 'AGG ASSAULT - ADULT B/M - MAIMING, CASTRATION - UNKNOWN',
    '04261': 'AGG ASSAULT - ADULT B/M - MOTOR VEHICLE (INTENTIONAL) - 1800-0600',
    '04262': 'AGG ASSAULT - ADULT B/M - MOTOR VEHICLE (INTENTIONAL) - 0600-1800',
    '04263': 'AGG ASSAULT - ADULT B/M - MOTOR VEHICLE (INTENTIONAL) - UNKNOWN',
    '04271': 'AGG ASSAULT - ADULT B/M - OTHER DANGEROUS WEAPON - 1800-0600',
    '04272': 'AGG ASSAULT - ADULT B/M - OTHER DANGEROUS WEAPON - 0600-1800',
    '04273': 'AGG ASSAULT - ADULT B/M - OTHER DANGEROUS WEAPON - UNKNOWN',
    '04281': 'AGG ASSAULT - ADULT B/M - ABUSE (ALL PARENT/GUARD ABUSE) - 1800-0600',
    '04282': 'AGG ASSAULT - ADULT B/M - ABUSE (ALL PARENT/GUARD ABUSE) - 0600-1800',
    '04283': 'AGG ASSAULT - ADULT B/M - ABUSE (ALL PARENT/GUARD ABUSE) - UNKNOWN',
    '04291': 'AGG ASSAULT - ADULT B/M - AIDING SUICIDE - 1800-0600',
    '04292': 'AGG ASSAULT - ADULT B/M - AIDING SUICIDE - 0600-1800',
    '04293': 'AGG ASSAULT - ADULT B/M - AIDING SUICIDE - UNKNOWN',
    '04311': 'AGG ASSAULT - ADULT W/F - FIREARMS (ALL GUNS) - 1800-0600',
    '04312': 'AGG ASSAULT - ADULT W/F - FIREARMS (ALL GUNS) - 0600-1800',
    '04313': 'AGG ASSAULT - ADULT W/F - FIREARMS (ALL GUNS) - UNKNOWN',
    '04321': 'AGG ASSAULT - ADULT W/F - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '04322': 'AGG ASSAULT - ADULT W/F - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '04323': 'AGG ASSAULT - ADULT W/F - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '04331': 'AGG ASSAULT - ADULT W/F - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '04332': 'AGG ASSAULT - ADULT W/F - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '04333': 'AGG ASSAULT - ADULT W/F - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '04341': 'AGG ASSAULT - ADULT W/F - ADMINISTERING POISON - 1800-0600',
    '04342': 'AGG ASSAULT - ADULT W/F - ADMINISTERING POISON - 0600-1800',
    '04343': 'AGG ASSAULT - ADULT W/F - ADMINISTERING POISON - UNKNOWN',
    '04351': 'AGG ASSAULT - ADULT W/F - MAIMING, CASTRATION - 1800-0600',
    '04352': 'AGG ASSAULT - ADULT W/F - MAIMING, CASTRATION - 0600-1800',
    '04353': 'AGG ASSAULT - ADULT W/F - MAIMING, CASTRATION - UNKNOWN',
    '04361': 'AGG ASSAULT - ADULT W/F - MOTOR VEHICLE (INTENTIONAL) - 1800-0600',
    '04362': 'AGG ASSAULT - ADULT W/F - MOTOR VEHICLE (INTENTIONAL) - 0600-1800',
    '04363': 'AGG ASSAULT - ADULT W/F - MOTOR VEHICLE (INTENTIONAL) - UNKNOWN',
    '04371': 'AGG ASSAULT - ADULT W/F - OTHER DANGEROUS WEAPON - 1800-0600',
    '04372': 'AGG ASSAULT - ADULT W/F - OTHER DANGEROUS WEAPON - 0600-1800',
    '04373': 'AGG ASSAULT - ADULT W/F - OTHER DANGEROUS WEAPON - UNKNOWN',
    '04381': 'AGG ASSAULT - ADULT W/F - ABUSE (ALL PARENT/GUARD ABUSE) - 1800-0600',
    '04382': 'AGG ASSAULT - ADULT W/F - ABUSE (ALL PARENT/GUARD ABUSE) - 0600-1800',
    '04383': 'AGG ASSAULT - ADULT W/F - ABUSE (ALL PARENT/GUARD ABUSE) - UNKNOWN',
    '04391': 'AGG ASSAULT - ADULT W/F - AIDING SUICIDE - 1800-0600',
    '04392': 'AGG ASSAULT - ADULT W/F - AIDING SUICIDE - 0600-1800',
    '04393': 'AGG ASSAULT - ADULT W/F - AIDING SUICIDE - UNKNOWN',
    '04411': 'AGG ASSAULT - ADULT B/F - FIREARMS (ALL GUNS) - 1800-0600',
    '04412': 'AGG ASSAULT - ADULT B/F - FIREARMS (ALL GUNS) - 0600-1800',
    '04413': 'AGG ASSAULT - ADULT B/F - FIREARMS (ALL GUNS) - UNKNOWN',
    '04421': 'AGG ASSAULT - ADULT B/F - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '04422': 'AGG ASSAULT - ADULT B/F - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '04423': 'AGG ASSAULT - ADULT B/F - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '04431': 'AGG ASSAULT - ADULT B/F - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '04432': 'AGG ASSAULT - ADULT B/F - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '04433': 'AGG ASSAULT - ADULT B/F - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '04441': 'AGG ASSAULT - ADULT B/F - ADMINISTERING POISON - 1800-0600',
    '04442': 'AGG ASSAULT - ADULT B/F - ADMINISTERING POISON - 0600-1800',
    '04443': 'AGG ASSAULT - ADULT B/F - ADMINISTERING POISON - UNKNOWN',
    '04451': 'AGG ASSAULT - ADULT B/F - MAIMING, CASTRATION - 1800-0600',
    '04452': 'AGG ASSAULT - ADULT B/F - MAIMING, CASTRATION - 0600-1800',
    '04453': 'AGG ASSAULT - ADULT B/F - MAIMING, CASTRATION - UNKNOWN',
    '04461': 'AGG ASSAULT - ADULT B/F - MOTOR VEHICLE (INTENTIONAL) - 1800-0600',
    '04462': 'AGG ASSAULT - ADULT B/F - MOTOR VEHICLE (INTENTIONAL) - 0600-1800',
    '04463': 'AGG ASSAULT - ADULT B/F - MOTOR VEHICLE (INTENTIONAL) - UNKNOWN',
    '04471': 'AGG ASSAULT - ADULT B/F - OTHER DANGEROUS WEAPON - 1800-0600',
    '04472': 'AGG ASSAULT - ADULT B/F - OTHER DANGEROUS WEAPON - 0600-1800',
    '04473': 'AGG ASSAULT - ADULT B/F - OTHER DANGEROUS WEAPON - UNKNOWN',
    '04481': 'AGG ASSAULT - ADULT B/F - ABUSE (ALL PARENT/GUARD ABUSE) - 1800-0600',
    '04482': 'AGG ASSAULT - ADULT B/F - ABUSE (ALL PARENT/GUARD ABUSE) - 0600-1800',
    '04483': 'AGG ASSAULT - ADULT B/F - ABUSE (ALL PARENT/GUARD ABUSE) - UNKNOWN',
    '04491': 'AGG ASSAULT - ADULT B/F - AIDING SUICIDE - 1800-0600',
    '04492': 'AGG ASSAULT - ADULT B/F - AIDING SUICIDE - 0600-1800',
    '04493': 'AGG ASSAULT - ADULT B/F - AIDING SUICIDE - UNKNOWN',
    '04511': 'AGG ASSAULT - JUV W/M - FIREARMS (ALL GUNS) - 1800-0600',
    '04512': 'AGG ASSAULT - JUV W/M - FIREARMS (ALL GUNS) - 0600-1800',
    '04513': 'AGG ASSAULT - JUV W/M - FIREARMS (ALL GUNS) - UNKNOWN',
    '04521': 'AGG ASSAULT - JUV W/M - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '04522': 'AGG ASSAULT - JUV W/M - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '04523': 'AGG ASSAULT - JUV W/M - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '04531': 'AGG ASSAULT - JUV W/M - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '04532': 'AGG ASSAULT - JUV W/M - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '04533': 'AGG ASSAULT - JUV W/M - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '04541': 'AGG ASSAULT - JUV W/M - ADMINISTERING POISON - 1800-0600',
    '04542': 'AGG ASSAULT - JUV W/M - ADMINISTERING POISON - 0600-1800',
    '04543': 'AGG ASSAULT - JUV W/M - ADMINISTERING POISON - UNKNOWN',
    '04551': 'AGG ASSAULT - JUV W/M - MAIMING, CASTRATION - 1800-0600',
    '04552': 'AGG ASSAULT - JUV W/M - MAIMING, CASTRATION - 0600-1800',
    '04553': 'AGG ASSAULT - JUV W/M - MAIMING, CASTRATION - UNKNOWN',
    '04561': 'AGG ASSAULT - JUV W/M - MOTOR VEHICLE (INTENTIONAL) - 1800-0600',
    '04562': 'AGG ASSAULT - JUV W/M - MOTOR VEHICLE (INTENTIONAL) - 0600-1800',
    '04563': 'AGG ASSAULT - JUV W/M - MOTOR VEHICLE (INTENTIONAL) - UNKNOWN',
    '04571': 'AGG ASSAULT - JUV W/M - OTHER DANGEROUS WEAPON - 1800-0600',
    '04572': 'AGG ASSAULT - JUV W/M - OTHER DANGEROUS WEAPON - 0600-1800',
    '04573': 'AGG ASSAULT - JUV W/M - OTHER DANGEROUS WEAPON - UNKNOWN',
    '04581': 'AGG ASSAULT - JUV W/M - ABUSE (ALL PARENT/GUARD ABUSE) - 1800-0600',
    '04582': 'AGG ASSAULT - JUV W/M - ABUSE (ALL PARENT/GUARD ABUSE) - 0600-1800',
    '04583': 'AGG ASSAULT - JUV W/M - ABUSE (ALL PARENT/GUARD ABUSE) - UNKNOWN',
    '04591': 'AGG ASSAULT - JUV W/M - AIDING SUICIDE - 1800-0600',
    '04592': 'AGG ASSAULT - JUV W/M - AIDING SUICIDE - 0600-1800',
    '04593': 'AGG ASSAULT - JUV W/M - AIDING SUICIDE - UNKNOWN',
    '04611': 'AGG ASSAULT - JUV B/M - FIREARMS (ALL GUNS) - 1800-0600',
    '04612': 'AGG ASSAULT - JUV B/M - FIREARMS (ALL GUNS) - 0600-1800',
    '04613': 'AGG ASSAULT - JUV B/M - FIREARMS (ALL GUNS) - UNKNOWN',
    '04621': 'AGG ASSAULT - JUV B/M - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '04622': 'AGG ASSAULT - JUV B/M - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '04623': 'AGG ASSAULT - JUV B/M - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '04631': 'AGG ASSAULT - JUV B/M - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '04632': 'AGG ASSAULT - JUV B/M - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '04633': 'AGG ASSAULT - JUV B/M - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '04641': 'AGG ASSAULT - JUV B/M - ADMINISTERING POISON - 1800-0600',
    '04642': 'AGG ASSAULT - JUV B/M - ADMINISTERING POISON - 0600-1800',
    '04643': 'AGG ASSAULT - JUV B/M - ADMINISTERING POISON - UNKNOWN',
    '04651': 'AGG ASSAULT - JUV B/M - MAIMING, CASTRATION - 1800-0600',
    '04652': 'AGG ASSAULT - JUV B/M - MAIMING, CASTRATION - 0600-1800',
    '04653': 'AGG ASSAULT - JUV B/M - MAIMING, CASTRATION - UNKNOWN',
    '04661': 'AGG ASSAULT - JUV B/M - MOTOR VEHICLE (INTENTIONAL) - 1800-0600',
    '04662': 'AGG ASSAULT - JUV B/M - MOTOR VEHICLE (INTENTIONAL) - 0600-1800',
    '04663': 'AGG ASSAULT - JUV B/M - MOTOR VEHICLE (INTENTIONAL) - UNKNOWN',
    '04671': 'AGG ASSAULT - JUV B/M - OTHER DANGEROUS WEAPON - 1800-0600',
    '04672': 'AGG ASSAULT - JUV B/M - OTHER DANGEROUS WEAPON - 0600-1800',
    '04673': 'AGG ASSAULT - JUV B/M - OTHER DANGEROUS WEAPON - UNKNOWN',
    '04681': 'AGG ASSAULT - JUV B/M - ABUSE (ALL PARENT/GUARD ABUSE) - 1800-0600',
    '04682': 'AGG ASSAULT - JUV B/M - ABUSE (ALL PARENT/GUARD ABUSE) - 0600-1800',
    '04683': 'AGG ASSAULT - JUV B/M - ABUSE (ALL PARENT/GUARD ABUSE) - UNKNOWN',
    '04691': 'AGG ASSAULT - JUV B/M - AIDING SUICIDE - 1800-0600',
    '04692': 'AGG ASSAULT - JUV B/M - AIDING SUICIDE - 0600-1800',
    '04693': 'AGG ASSAULT - JUV B/M - AIDING SUICIDE - UNKNOWN',
    '04711': 'AGG ASSAULT - JUV W/F - FIREARMS (ALL GUNS) - 1800-0600',
    '04712': 'AGG ASSAULT - JUV W/F - FIREARMS (ALL GUNS) - 0600-1800',
    '04713': 'AGG ASSAULT - JUV W/F - FIREARMS (ALL GUNS) - UNKNOWN',
    '04721': 'AGG ASSAULT - JUV W/F - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '04722': 'AGG ASSAULT - JUV W/F - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '04723': 'AGG ASSAULT - JUV W/F - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '04731': 'AGG ASSAULT - JUV W/F - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '04732': 'AGG ASSAULT - JUV W/F - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '04733': 'AGG ASSAULT - JUV W/F - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '04741': 'AGG ASSAULT - JUV W/F - ADMINISTERING POISON - 1800-0600',
    '04742': 'AGG ASSAULT - JUV W/F - ADMINISTERING POISON - 0600-1800',
    '04743': 'AGG ASSAULT - JUV W/F - ADMINISTERING POISON - UNKNOWN',
    '04751': 'AGG ASSAULT - JUV W/F - MAIMING, CASTRATION - 1800-0600',
    '04752': 'AGG ASSAULT - JUV W/F - MAIMING, CASTRATION - 0600-1800',
    '04753': 'AGG ASSAULT - JUV W/F - MAIMING, CASTRATION - UNKNOWN',
    '04761': 'AGG ASSAULT - JUV W/F - MOTOR VEHICLE (INTENTIONAL) - 1800-0600',
    '04762': 'AGG ASSAULT - JUV W/F - MOTOR VEHICLE (INTENTIONAL) - 0600-1800',
    '04763': 'AGG ASSAULT - JUV W/F - MOTOR VEHICLE (INTENTIONAL) - UNKNOWN',
    '04771': 'AGG ASSAULT - JUV W/F - OTHER DANGEROUS WEAPON - 1800-0600',
    '04772': 'AGG ASSAULT - JUV W/F - OTHER DANGEROUS WEAPON - 0600-1800',
    '04773': 'AGG ASSAULT - JUV W/F - OTHER DANGEROUS WEAPON - UNKNOWN',
    '04781': 'AGG ASSAULT - JUV W/F - ABUSE (ALL PARENT/GUARD ABUSE) - 1800-0600',
    '04782': 'AGG ASSAULT - JUV W/F - ABUSE (ALL PARENT/GUARD ABUSE) - 0600-1800',
    '04783': 'AGG ASSAULT - JUV W/F - ABUSE (ALL PARENT/GUARD ABUSE) - UNKNOWN',
    '04791': 'AGG ASSAULT - JUV W/F - AIDING SUICIDE - 1800-0600',
    '04792': 'AGG ASSAULT - JUV W/F - AIDING SUICIDE - 0600-1800',
    '04793': 'AGG ASSAULT - JUV W/F - AIDING SUICIDE - UNKNOWN',
    '04811': 'AGG ASSAULT - JUV B/F - FIREARMS (ALL GUNS) - 1800-0600',
    '04812': 'AGG ASSAULT - JUV B/F - FIREARMS (ALL GUNS) - 0600-1800',
    '04813': 'AGG ASSAULT - JUV B/F - FIREARMS (ALL GUNS) - UNKNOWN',
    '04821': 'AGG ASSAULT - JUV B/F - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '04822': 'AGG ASSAULT - JUV B/F - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '04823': 'AGG ASSAULT - JUV B/F - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '04831': 'AGG ASSAULT - JUV B/F - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '04832': 'AGG ASSAULT - JUV B/F - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '04833': 'AGG ASSAULT - JUV B/F - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '04841': 'AGG ASSAULT - JUV B/F - ADMINISTERING POISON - 1800-0600',
    '04842': 'AGG ASSAULT - JUV B/F - ADMINISTERING POISON - 0600-1800',
    '04843': 'AGG ASSAULT - JUV B/F - ADMINISTERING POISON - UNKNOWN',
    '04851': 'AGG ASSAULT - JUV B/F - MAIMING, CASTRATION - 1800-0600',
    '04852': 'AGG ASSAULT - JUV B/F - MAIMING, CASTRATION - 0600-1800',
    '04853': 'AGG ASSAULT - JUV B/F - MAIMING, CASTRATION - UNKNOWN',
    '04861': 'AGG ASSAULT - JUV B/F - MOTOR VEHICLE (INTENTIONAL) - 1800-0600',
    '04862': 'AGG ASSAULT - JUV B/F - MOTOR VEHICLE (INTENTIONAL) - 0600-1800',
    '04863': 'AGG ASSAULT - JUV B/F - MOTOR VEHICLE (INTENTIONAL) - UNKNOWN',
    '04871': 'AGG ASSAULT - JUV B/F - OTHER DANGEROUS WEAPON - 1800-0600',
    '04872': 'AGG ASSAULT - JUV B/F - OTHER DANGEROUS WEAPON - 0600-1800',
    '04873': 'AGG ASSAULT - JUV B/F - OTHER DANGEROUS WEAPON - UNKNOWN',
    '04881': 'AGG ASSAULT - JUV B/F - ABUSE (ALL PARENT/GUARD ABUSE) - 1800-0600',
    '04882': 'AGG ASSAULT - JUV B/F - ABUSE (ALL PARENT/GUARD ABUSE) - 0600-1800',
    '04883': 'AGG ASSAULT - JUV B/F - ABUSE (ALL PARENT/GUARD ABUSE) - UNKNOWN',
    '04891': 'AGG ASSAULT - JUV B/F - AIDING SUICIDE - 1800-0600',
    '04892': 'AGG ASSAULT - JUV B/F - AIDING SUICIDE - 0600-1800',
    '04893': 'AGG ASSAULT - JUV B/F - AIDING SUICIDE - UNKNOWN',
    '04911': 'AGG ASSAULT - POLICE OFFICER - FIREARMS (ALL GUNS) - 1800-0600',
    '04912': 'AGG ASSAULT - POLICE OFFICER - FIREARMS (ALL GUNS) - 0600-1800',
    '04913': 'AGG ASSAULT - POLICE OFFICER - FIREARMS (ALL GUNS) - UNKNOWN',
    '04921': 'AGG ASSAULT - POLICE OFFICER - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '04922': 'AGG ASSAULT - POLICE OFFICER - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '04923': 'AGG ASSAULT - POLICE OFFICER - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '04931': 'AGG ASSAULT - POLICE OFFICER - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '04932': 'AGG ASSAULT - POLICE OFFICER - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '04933': 'AGG ASSAULT - POLICE OFFICER - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '04941': 'AGG ASSAULT - POLICE OFFICER - ADMINISTERING POISON - 1800-0600',
    '04942': 'AGG ASSAULT - POLICE OFFICER - ADMINISTERING POISON - 0600-1800',
    '04943': 'AGG ASSAULT - POLICE OFFICER - ADMINISTERING POISON - UNKNOWN',
    '04951': 'AGG ASSAULT - POLICE OFFICER - MAIMING, CASTRATION - 1800-0600',
    '04952': 'AGG ASSAULT - POLICE OFFICER - MAIMING, CASTRATION - 0600-1800',
    '04953': 'AGG ASSAULT - POLICE OFFICER - MAIMING, CASTRATION - UNKNOWN',
    '04961': 'AGG ASSAULT - POLICE OFFICER - MOTOR VEHICLE (INTENTIONAL) - 1800-0600',
    '04962': 'AGG ASSAULT - POLICE OFFICER - MOTOR VEHICLE (INTENTIONAL) - 0600-1800',
    '04963': 'AGG ASSAULT - POLICE OFFICER - MOTOR VEHICLE (INTENTIONAL) - UNKNOWN',
    '04971': 'AGG ASSAULT - POLICE OFFICER - OTHER DANGEROUS WEAPON - 1800-0600',
    '04972': 'AGG ASSAULT - POLICE OFFICER - OTHER DANGEROUS WEAPON - 0600-1800',
    '04973': 'AGG ASSAULT - POLICE OFFICER - OTHER DANGEROUS WEAPON - UNKNOWN',
    '04981': 'AGG ASSAULT - POLICE OFFICER - ABUSE (ALL PARENT/GUARD ABUSE) - 1800-0600',
    '04982': 'AGG ASSAULT - POLICE OFFICER - ABUSE (ALL PARENT/GUARD ABUSE) - 0600-1800',
    '04983': 'AGG ASSAULT - POLICE OFFICER - ABUSE (ALL PARENT/GUARD ABUSE) - UNKNOWN',
    '04991': 'AGG ASSAULT - POLICE OFFICER - AIDING SUICIDE - 1800-0600',
    '04992': 'AGG ASSAULT - POLICE OFFICER - AIDING SUICIDE - 0600-1800',
    '04993': 'AGG ASSAULT - POLICE OFFICER - AIDING SUICIDE - UNKNOWN',
    '04C11': 'AGG ASSAULT - JUV W/M FV - FIREARMS (ALL GUNS) - 1800-0600',
    '04C12': 'AGG ASSAULT - JUV W/M FV - FIREARMS (ALL GUNS) - 0600-1800',
    '04C13': 'AGG ASSAULT - JUV W/M FV - FIREARMS (ALL GUNS) - UNKNOWN',
    '04C21': 'AGG ASSAULT - JUV W/M FV - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '04C22': 'AGG ASSAULT - JUV W/M FV - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '04C23': 'AGG ASSAULT - JUV W/M FV - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '04C31': 'AGG ASSAULT - JUV W/M FV - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '04C32': 'AGG ASSAULT - JUV W/M FV - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '04C33': 'AGG ASSAULT - JUV W/M FV - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '04C41': 'AGG ASSAULT - JUV W/M FV - ADMINISTERING POISON - 1800-0600',
    '04C42': 'AGG ASSAULT - JUV W/M FV - ADMINISTERING POISON - 0600-1800',
    '04C43': 'AGG ASSAULT - JUV W/M FV - ADMINISTERING POISON - UNKNOWN',
    '04C51': 'AGG ASSAULT - JUV W/M FV - MAIMING, CASTRATION - 1800-0600',
    '04C52': 'AGG ASSAULT - JUV W/M FV - MAIMING, CASTRATION - 0600-1800',
    '04C53': 'AGG ASSAULT - JUV W/M FV - MAIMING, CASTRATION - UNKNOWN',
    '04C61': 'AGG ASSAULT - JUV W/M FV - MOTOR VEHICLE (INTENTIONAL) - 1800-0600',
    '04C62': 'AGG ASSAULT - JUV W/M FV - MOTOR VEHICLE (INTENTIONAL) - 0600-1800',
    '04C63': 'AGG ASSAULT - JUV W/M FV - MOTOR VEHICLE (INTENTIONAL) - UNKNOWN',
    '04C71': 'AGG ASSAULT - JUV W/M FV - OTHER DANGEROUS WEAPON - 1800-0600',
    '04C72': 'AGG ASSAULT - JUV W/M FV - OTHER DANGEROUS WEAPON - 0600-1800',
    '04C73': 'AGG ASSAULT - JUV W/M FV - OTHER DANGEROUS WEAPON - UNKNOWN',
    '04C81': 'AGG ASSAULT - JUV W/M FV - ABUSE (ALL PARENT/GUARD ABUSE) - 1800-0600',
    '04C82': 'AGG ASSAULT - JUV W/M FV - ABUSE (ALL PARENT/GUARD ABUSE) - 0600-1800',
    '04C83': 'AGG ASSAULT - JUV W/M FV - ABUSE (ALL PARENT/GUARD ABUSE) - UNKNOWN',
    '04C91': 'AGG ASSAULT - JUV W/M FV - AIDING SUICIDE - 1800-0600',
    '04C92': 'AGG ASSAULT - JUV W/M FV - AIDING SUICIDE - 0600-1800',
    '04C93': 'AGG ASSAULT - JUV W/M FV - AIDING SUICIDE - UNKNOWN',
    '04D11': 'AGG ASSAULT - JUV B/M FV - FIREARMS (ALL GUNS) - 1800-0600',
    '04D12': 'AGG ASSAULT - JUV B/M FV - FIREARMS (ALL GUNS) - 0600-1800',
    '04D13': 'AGG ASSAULT - JUV B/M FV - FIREARMS (ALL GUNS) - UNKNOWN',
    '04D21': 'AGG ASSAULT - JUV B/M FV - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '04D22': 'AGG ASSAULT - JUV B/M FV - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '04D23': 'AGG ASSAULT - JUV B/M FV - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '04D31': 'AGG ASSAULT - JUV B/M FV - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '04D32': 'AGG ASSAULT - JUV B/M FV - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '04D33': 'AGG ASSAULT - JUV B/M FV - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '04D41': 'AGG ASSAULT - JUV B/M FV - ADMINISTERING POISON - 1800-0600',
    '04D42': 'AGG ASSAULT - JUV B/M FV - ADMINISTERING POISON - 0600-1800',
    '04D43': 'AGG ASSAULT - JUV B/M FV - ADMINISTERING POISON - UNKNOWN',
    '04D51': 'AGG ASSAULT - JUV B/M FV - MAIMING, CASTRATION - 1800-0600',
    '04D52': 'AGG ASSAULT - JUV B/M FV - MAIMING, CASTRATION - 0600-1800',
    '04D53': 'AGG ASSAULT - JUV B/M FV - MAIMING, CASTRATION - UNKNOWN',
    '04D61': 'AGG ASSAULT - JUV B/M FV - MOTOR VEHICLE (INTENTIONAL) - 1800-0600',
    '04D62': 'AGG ASSAULT - JUV B/M FV - MOTOR VEHICLE (INTENTIONAL) - 0600-1800',
    '04D63': 'AGG ASSAULT - JUV B/M FV - MOTOR VEHICLE (INTENTIONAL) - UNKNOWN',
    '04D71': 'AGG ASSAULT - JUV B/M FV - OTHER DANGEROUS WEAPON - 1800-0600',
    '04D72': 'AGG ASSAULT - JUV B/M FV - OTHER DANGEROUS WEAPON - 0600-1800',
    '04D73': 'AGG ASSAULT - JUV B/M FV - OTHER DANGEROUS WEAPON - UNKNOWN',
    '04D81': 'AGG ASSAULT - JUV B/M FV - ABUSE (ALL PARENT/GUARD ABUSE) - 1800-0600',
    '04D82': 'AGG ASSAULT - JUV B/M FV - ABUSE (ALL PARENT/GUARD ABUSE) - 0600-1800',
    '04D83': 'AGG ASSAULT - JUV B/M FV - ABUSE (ALL PARENT/GUARD ABUSE) - UNKNOWN',
    '04D91': 'AGG ASSAULT - JUV B/M FV - AIDING SUICIDE - 1800-0600',
    '04D92': 'AGG ASSAULT - JUV B/M FV - AIDING SUICIDE - 0600-1800',
    '04D93': 'AGG ASSAULT - JUV B/M FV - AIDING SUICIDE - UNKNOWN',
    '04E11': 'AGG ASSAULT - JUV W/F FV - FIREARMS (ALL GUNS) - 1800-0600',
    '04E12': 'AGG ASSAULT - JUV W/F FV - FIREARMS (ALL GUNS) - 0600-1800',
    '04E13': 'AGG ASSAULT - JUV W/F FV - FIREARMS (ALL GUNS) - UNKNOWN',
    '04E21': 'AGG ASSAULT - JUV W/F FV - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '04E22': 'AGG ASSAULT - JUV W/F FV - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '04E23': 'AGG ASSAULT - JUV W/F FV - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '04E31': 'AGG ASSAULT - JUV W/F FV - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '04E32': 'AGG ASSAULT - JUV W/F FV - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '04E33': 'AGG ASSAULT - JUV W/F FV - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '04E41': 'AGG ASSAULT - JUV W/F FV - ADMINISTERING POISON - 1800-0600',
    '04E42': 'AGG ASSAULT - JUV W/F FV - ADMINISTERING POISON - 0600-1800',
    '04E43': 'AGG ASSAULT - JUV W/F FV - ADMINISTERING POISON - UNKNOWN',
    '04E51': 'AGG ASSAULT - JUV W/F FV - MAIMING, CASTRATION - 1800-0600',
    '04E52': 'AGG ASSAULT - JUV W/F FV - MAIMING, CASTRATION - 0600-1800',
    '04E53': 'AGG ASSAULT - JUV W/F FV - MAIMING, CASTRATION - UNKNOWN',
    '04E61': 'AGG ASSAULT - JUV W/F FV - MOTOR VEHICLE (INTENTIONAL) - 1800-0600',
    '04E62': 'AGG ASSAULT - JUV W/F FV - MOTOR VEHICLE (INTENTIONAL) - 0600-1800',
    '04E63': 'AGG ASSAULT - JUV W/F FV - MOTOR VEHICLE (INTENTIONAL) - UNKNOWN',
    '04E71': 'AGG ASSAULT - JUV W/F FV - OTHER DANGEROUS WEAPON - 1800-0600',
    '04E72': 'AGG ASSAULT - JUV W/F FV - OTHER DANGEROUS WEAPON - 0600-1800',
    '04E73': 'AGG ASSAULT - JUV W/F FV - OTHER DANGEROUS WEAPON - UNKNOWN',
    '04E81': 'AGG ASSAULT - JUV W/F FV - ABUSE (ALL PARENT/GUARD ABUSE) - 1800-0600',
    '04E82': 'AGG ASSAULT - JUV W/F FV - ABUSE (ALL PARENT/GUARD ABUSE) - 0600-1800',
    '04E83': 'AGG ASSAULT - JUV W/F FV - ABUSE (ALL PARENT/GUARD ABUSE) - UNKNOWN',
    '04E91': 'AGG ASSAULT - JUV W/F FV - AIDING SUICIDE - 1800-0600',
    '04E92': 'AGG ASSAULT - JUV W/F FV - AIDING SUICIDE - 0600-1800',
    '04E93': 'AGG ASSAULT - JUV W/F FV - AIDING SUICIDE - UNKNOWN',
    '04F11': 'AGG ASSAULT - JUV B/F FV - FIREARMS (ALL GUNS) - 1800-0600',
    '04F12': 'AGG ASSAULT - JUV B/F FV - FIREARMS (ALL GUNS) - 0600-1800',
    '04F13': 'AGG ASSAULT - JUV B/F FV - FIREARMS (ALL GUNS) - UNKNOWN',
    '04F21': 'AGG ASSAULT - JUV B/F FV - KNIFE/CUTTING/STAB INSTRUMENT - 1800-0600',
    '04F22': 'AGG ASSAULT - JUV B/F FV - KNIFE/CUTTING/STAB INSTRUMENT - 0600-1800',
    '04F23': 'AGG ASSAULT - JUV B/F FV - KNIFE/CUTTING/STAB INSTRUMENT - UNKNOWN',
    '04F31': 'AGG ASSAULT - JUV B/F FV - HANDS/FISTS/FEET/BODILY FORCE - 1800-0600',
    '04F32': 'AGG ASSAULT - JUV B/F FV - HANDS/FISTS/FEET/BODILY FORCE - 0600-1800',
    '04F33': 'AGG ASSAULT - JUV B/F FV - HANDS/FISTS/FEET/BODILY FORCE - UNKNOWN',
    '04F41': 'AGG ASSAULT - JUV B/F FV - ADMINISTERING POISON - 1800-0600',
    '04F42': 'AGG ASSAULT - JUV B/F FV - ADMINISTERING POISON - 0600-1800',
    '04F43': 'AGG ASSAULT - JUV B/F FV - ADMINISTERING POISON - UNKNOWN',
    '04F51': 'AGG ASSAULT - JUV B/F FV - MAIMING, CASTRATION - 1800-0600',
    '04F52': 'AGG ASSAULT - JUV B/F FV - MAIMING, CASTRATION - 0600-1800',
    '04F53': 'AGG ASSAULT - JUV B/F FV - MAIMING, CASTRATION - UNKNOWN',
    '04F61': 'AGG ASSAULT - JUV B/F FV - MOTOR VEHICLE (INTENTIONAL) - 1800-0600',
    '04F62': 'AGG ASSAULT - JUV B/F FV - MOTOR VEHICLE (INTENTIONAL) - 0600-1800',
    '04F63': 'AGG ASSAULT - JUV B/F FV - MOTOR VEHICLE (INTENTIONAL) - UNKNOWN',
    '04F71': 'AGG ASSAULT - JUV B/F FV - OTHER DANGEROUS WEAPON - 1800-0600',
    '04F72': 'AGG ASSAULT - JUV B/F FV - OTHER DANGEROUS WEAPON - 0600-1800',
    '04F73': 'AGG ASSAULT - JUV B/F FV - OTHER DANGEROUS WEAPON - UNKNOWN',
    '04F81': 'AGG ASSAULT - JUV B/F FV - ABUSE (ALL PARENT/GUARD ABUSE) - 1800-0600',
    '04F82': 'AGG ASSAULT - JUV B/F FV - ABUSE (ALL PARENT/GUARD ABUSE) - 0600-1800',
    '04F83': 'AGG ASSAULT - JUV B/F FV - ABUSE (ALL PARENT/GUARD ABUSE) - UNKNOWN',
    '04F91': 'AGG ASSAULT - JUV B/F FV - AIDING SUICIDE - 1800-0600',
    '04F92': 'AGG ASSAULT - JUV B/F FV - AIDING SUICIDE - 0600-1800',
    '04F93': 'AGG ASSAULT - JUV B/F FV - AIDING SUICIDE - UNKNOWN',
    '05111': 'BURGLARY - BUSINESS/NIGHT - KEY/COMBINATiON LOCK - FRONT DOOR',
    '05112': 'BURGLARY - BUSINESS/NIGHT - KEY/COMBINATiON LOCK - REAR DOOR',
    '05113': 'BURGLARY - BUSINESS/NIGHT - KEY/COMBINATiON LOCK - SIDE DOOR',
    '05114': 'BURGLARY - BUSINESS/NIGHT - KEY/COMBINATiON LOCK - FRT WIND',
    '05115': 'BURGLARY - BUSINESS/NIGHT - KEY/COMBINATiON LOCK - REAR WIND',
    '05116': 'BURGLARY - BUSINESS/NIGHT - KEY/COMBINATiON LOCK - SIDE WIND',
    '05117': 'BURGLARY - BUSINESS/NIGHT - KEY/COMBINATiON LOCK - ROOF,FLOOR',
    '05118': 'BURGLARY - BUSINESS/NIGHT - KEY/COMBINATiON LOCK - WALL',
    '05119': 'BURGLARY - BUSINESS/NIGHT - KEY/COMBINATiON LOCK - UNKNOWN',
    '0511A': 'BURGLARY - BUSINESS/NIGHT - KEY/COMBINATiON LOCK - RES GRG DR',
    '0511B': 'BURGLARY - BUSINESS/NIGHT - KEY/COMBINATiON LOCK - HID IN BLDG',
    '0511C': 'BURGLARY - BUSINESS/NIGHT - KEY/COMBINATiON LOCK - OVRHEAD DR',
    '05121': 'BURGLARY - BUSINESS/NIGHT - SMASHED, KICKED,STRUCK - FRONT DOOR',
    '05122': 'BURGLARY - BUSINESS/NIGHT - SMASHED, KICKED,STRUCK - REAR DOOR',
    '05123': 'BURGLARY - BUSINESS/NIGHT - SMASHED, KICKED,STRUCK - SIDE DOOR',
    '05124': 'BURGLARY - BUSINESS/NIGHT - SMASHED, KICKED,STRUCK - FRT WIND',
    '05125': 'BURGLARY - BUSINESS/NIGHT - SMASHED, KICKED,STRUCK - REAR WIND',
    '05126': 'BURGLARY - BUSINESS/NIGHT - SMASHED, KICKED,STRUCK - SIDE WIND',
    '05127': 'BURGLARY - BUSINESS/NIGHT - SMASHED, KICKED,STRUCK - ROOF,FLOOR',
    '05128': 'BURGLARY - BUSINESS/NIGHT - SMASHED, KICKED,STRUCK - WALL',
    '05129': 'BURGLARY - BUSINESS/NIGHT - SMASHED, KICKED,STRUCK - UNKNOWN',
    '0512A': 'BURGLARY - BUSINESS/NIGHT - SMASHED, KICKED,STRUCK - RES GRG DR',
    '0512B': 'BURGLARY - BUSINESS/NIGHT - SMASHED, KICKED,STRUCK - HID IN BLDG',
    '0512C': 'BURGLARY - BUSINESS/NIGHT - SMASHED, KICKED,STRUCK - OVRHEAD DR',
    '05131': 'BURGLARY - BUSINESS/NIGHT - PRY, BREAK SAW,CUT, ETC. - FRONT DOOR',
    '05132': 'BURGLARY - BUSINESS/NIGHT - PRY, BREAK SAW,CUT, ETC. - REAR DOOR',
    '05133': 'BURGLARY - BUSINESS/NIGHT - PRY, BREAK SAW,CUT, ETC. - SIDE DOOR',
    '05134': 'BURGLARY - BUSINESS/NIGHT - PRY, BREAK SAW,CUT, ETC. - FRT WIND',
    '05135': 'BURGLARY - BUSINESS/NIGHT - PRY, BREAK SAW,CUT, ETC. - REAR WIND',
    '05136': 'BURGLARY - BUSINESS/NIGHT - PRY, BREAK SAW,CUT, ETC. - SIDE WIND',
    '05137': 'BURGLARY - BUSINESS/NIGHT - PRY, BREAK SAW,CUT, ETC. - ROOF,FLOOR',
    '05138': 'BURGLARY - BUSINESS/NIGHT - PRY, BREAK SAW,CUT, ETC. - WALL',
    '05139': 'BURGLARY - BUSINESS/NIGHT - PRY, BREAK SAW,CUT, ETC. - UNKNOWN',
    '0513A': 'BURGLARY - BUSINESS/NIGHT - PRY, BREAK SAW,CUT, ETC. - RES GRG DR',
    '0513B': 'BURGLARY - BUSINESS/NIGHT - PRY, BREAK SAW,CUT, ETC. - HID IN BLDG',
    '0513C': 'BURGLARY - BUSINESS/NIGHT - PRY, BREAK SAW,CUT, ETC. - OVRHEAD DR',
    '05141': 'BURGLARY - BUSINESS/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - FRONT DOOR',
    '05142': 'BURGLARY - BUSINESS/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - REAR DOOR',
    '05143': 'BURGLARY - BUSINESS/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - SIDE DOOR',
    '05144': 'BURGLARY - BUSINESS/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - FRT WIND',
    '05145': 'BURGLARY - BUSINESS/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - REAR WIND',
    '05146': 'BURGLARY - BUSINESS/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - SIDE WIND',
    '05147': 'BURGLARY - BUSINESS/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - ROOF,FLOOR',
    '05148': 'BURGLARY - BUSINESS/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - WALL',
    '05149': 'BURGLARY - BUSINESS/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - UNKNOWN',
    '0514A': 'BURGLARY - BUSINESS/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - RES GRG DR',
    '0514B': 'BURGLARY - BUSINESS/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - HID IN BLDG',
    '0514C': 'BURGLARY - BUSINESS/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - OVRHEAD DR',
    '05151': 'BURGLARY - BUSINESS/NIGHT - NO FORCE (HIDE INSIDE) - FRONT DOOR',
    '05152': 'BURGLARY - BUSINESS/NIGHT - NO FORCE (HIDE INSIDE) - REAR DOOR',
    '05153': 'BURGLARY - BUSINESS/NIGHT - NO FORCE (HIDE INSIDE) - SIDE DOOR',
    '05154': 'BURGLARY - BUSINESS/NIGHT - NO FORCE (HIDE INSIDE) - FRT WIND',
    '05155': 'BURGLARY - BUSINESS/NIGHT - NO FORCE (HIDE INSIDE) - REAR WIND',
    '05156': 'BURGLARY - BUSINESS/NIGHT - NO FORCE (HIDE INSIDE) - SIDE WIND',
    '05157': 'BURGLARY - BUSINESS/NIGHT - NO FORCE (HIDE INSIDE) - ROOF,FLOOR',
    '05158': 'BURGLARY - BUSINESS/NIGHT - NO FORCE (HIDE INSIDE) - WALL',
    '05159': 'BURGLARY - BUSINESS/NIGHT - NO FORCE (HIDE INSIDE) - UNKNOWN',
    '0515A': 'BURGLARY - BUSINESS/NIGHT - NO FORCE (HIDE INSIDE) - RES GRG DR',
    '0515B': 'BURGLARY - BUSINESS/NIGHT - NO FORCE (HIDE INSIDE) - HID IN BLDG',
    '0515C': 'BURGLARY - BUSINESS/NIGHT - NO FORCE (HIDE INSIDE) - OVRHEAD DR',
    '05161': 'BURGLARY - BUSINESS/NIGHT - WINDOW,DOOR,ETC UNLOCKED - FRONT DOOR',
    '05162': 'BURGLARY - BUSINESS/NIGHT - WINDOW,DOOR,ETC UNLOCKED - REAR DOOR',
    '05163': 'BURGLARY - BUSINESS/NIGHT - WINDOW,DOOR,ETC UNLOCKED - SIDE DOOR',
    '05164': 'BURGLARY - BUSINESS/NIGHT - WINDOW,DOOR,ETC UNLOCKED - FRT WIND',
    '05165': 'BURGLARY - BUSINESS/NIGHT - WINDOW,DOOR,ETC UNLOCKED - REAR WIND',
    '05166': 'BURGLARY - BUSINESS/NIGHT - WINDOW,DOOR,ETC UNLOCKED - SIDE WIND',
    '05167': 'BURGLARY - BUSINESS/NIGHT - WINDOW,DOOR,ETC UNLOCKED - ROOF,FLOOR',
    '05168': 'BURGLARY - BUSINESS/NIGHT - WINDOW,DOOR,ETC UNLOCKED - WALL',
    '05169': 'BURGLARY - BUSINESS/NIGHT - WINDOW,DOOR,ETC UNLOCKED - UNKNOWN',
    '0516A': 'BURGLARY - BUSINESS/NIGHT - WINDOW,DOOR,ETC UNLOCKED - RES GRG DR',
    '0516B': 'BURGLARY - BUSINESS/NIGHT - WINDOW,DOOR,ETC UNLOCKED - HID IN BLDG',
    '0516C': 'BURGLARY - BUSINESS/NIGHT - WINDOW,DOOR,ETC UNLOCKED - OVRHEAD DR',
    '05171': 'BURGLARY - BUSINESS/NIGHT - ALL ATTEMPTS (NO FORCE) - FRONT DOOR',
    '05172': 'BURGLARY - BUSINESS/NIGHT - ALL ATTEMPTS (NO FORCE) - REAR DOOR',
    '05173': 'BURGLARY - BUSINESS/NIGHT - ALL ATTEMPTS (NO FORCE) - SIDE DOOR',
    '05174': 'BURGLARY - BUSINESS/NIGHT - ALL ATTEMPTS (NO FORCE) - FRT WIND',
    '05175': 'BURGLARY - BUSINESS/NIGHT - ALL ATTEMPTS (NO FORCE) - REAR WIND',
    '05176': 'BURGLARY - BUSINESS/NIGHT - ALL ATTEMPTS (NO FORCE) - SIDE WIND',
    '05177': 'BURGLARY - BUSINESS/NIGHT - ALL ATTEMPTS (NO FORCE) - ROOF,FLOOR',
    '05178': 'BURGLARY - BUSINESS/NIGHT - ALL ATTEMPTS (NO FORCE) - WALL',
    '05179': 'BURGLARY - BUSINESS/NIGHT - ALL ATTEMPTS (NO FORCE) - UNKNOWN',
    '0517A': 'BURGLARY - BUSINESS/NIGHT - ALL ATTEMPTS (NO FORCE) - RES GRG DR',
    '0517B': 'BURGLARY - BUSINESS/NIGHT - ALL ATTEMPTS (NO FORCE) - HID IN BLDG',
    '0517C': 'BURGLARY - BUSINESS/NIGHT - ALL ATTEMPTS (NO FORCE) - OVRHEAD DR',
    '05181': 'BURGLARY - BUSINESS/NIGHT - ALL OTHER ATTEMPTS - FRONT DOOR',
    '05182': 'BURGLARY - BUSINESS/NIGHT - ALL OTHER ATTEMPTS - REAR DOOR',
    '05183': 'BURGLARY - BUSINESS/NIGHT - ALL OTHER ATTEMPTS - SIDE DOOR',
    '05184': 'BURGLARY - BUSINESS/NIGHT - ALL OTHER ATTEMPTS - FRT WIND',
    '05185': 'BURGLARY - BUSINESS/NIGHT - ALL OTHER ATTEMPTS - REAR WIND',
    '05186': 'BURGLARY - BUSINESS/NIGHT - ALL OTHER ATTEMPTS - SIDE WIND',
    '05187': 'BURGLARY - BUSINESS/NIGHT - ALL OTHER ATTEMPTS - ROOF,FLOOR',
    '05188': 'BURGLARY - BUSINESS/NIGHT - ALL OTHER ATTEMPTS - WALL',
    '05189': 'BURGLARY - BUSINESS/NIGHT - ALL OTHER ATTEMPTS - UNKNOWN',
    '0518A': 'BURGLARY - BUSINESS/NIGHT - ALL OTHER ATTEMPTS - RES GRG DR',
    '0518B': 'BURGLARY - BUSINESS/NIGHT - ALL OTHER ATTEMPTS - HID IN BLDG',
    '0518C': 'BURGLARY - BUSINESS/NIGHT - ALL OTHER ATTEMPTS - OVRHEAD DR',
    '05191': 'BURGLARY - BUSINESS/NIGHT - OTHERS - FRONT DOOR',
    '05192': 'BURGLARY - BUSINESS/NIGHT - OTHERS - REAR DOOR',
    '05193': 'BURGLARY - BUSINESS/NIGHT - OTHERS - SIDE DOOR',
    '05194': 'BURGLARY - BUSINESS/NIGHT - OTHERS - FRT WIND',
    '05195': 'BURGLARY - BUSINESS/NIGHT - OTHERS - REAR WIND',
    '05196': 'BURGLARY - BUSINESS/NIGHT - OTHERS - SIDE WIND',
    '05197': 'BURGLARY - BUSINESS/NIGHT - OTHERS - ROOF,FLOOR',
    '05198': 'BURGLARY - BUSINESS/NIGHT - OTHERS - WALL',
    '05199': 'BURGLARY - BUSINESS/NIGHT - OTHERS - UNKNOWN',
    '0519A': 'BURGLARY - BUSINESS/NIGHT - OTHERS - RES GRG DR',
    '0519B': 'BURGLARY - BUSINESS/NIGHT - OTHERS - HID IN BLDG',
    '0519C': 'BURGLARY - BUSINESS/NIGHT - OTHERS - OVRHEAD DR',
    '05211': 'BURGLARY - BUSINESS/DAY - KEY/COMBINATiON LOCK - FRONT DOOR',
    '05212': 'BURGLARY - BUSINESS/DAY - KEY/COMBINATiON LOCK - REAR DOOR',
    '05213': 'BURGLARY - BUSINESS/DAY - KEY/COMBINATiON LOCK - SIDE DOOR',
    '05214': 'BURGLARY - BUSINESS/DAY - KEY/COMBINATiON LOCK - FRT WIND',
    '05215': 'BURGLARY - BUSINESS/DAY - KEY/COMBINATiON LOCK - REAR WIND',
    '05216': 'BURGLARY - BUSINESS/DAY - KEY/COMBINATiON LOCK - SIDE WIND',
    '05217': 'BURGLARY - BUSINESS/DAY - KEY/COMBINATiON LOCK - ROOF,FLOOR',
    '05218': 'BURGLARY - BUSINESS/DAY - KEY/COMBINATiON LOCK - WALL',
    '05219': 'BURGLARY - BUSINESS/DAY - KEY/COMBINATiON LOCK - UNKNOWN',
    '0521A': 'BURGLARY - BUSINESS/DAY - KEY/COMBINATiON LOCK - RES GRG DR',
    '0521B': 'BURGLARY - BUSINESS/DAY - KEY/COMBINATiON LOCK - HID IN BLDG',
    '0521C': 'BURGLARY - BUSINESS/DAY - KEY/COMBINATiON LOCK - OVRHEAD DR',
    '05221': 'BURGLARY - BUSINESS/DAY - SMASHED, KICKED,STRUCK - FRONT DOOR',
    '05222': 'BURGLARY - BUSINESS/DAY - SMASHED, KICKED,STRUCK - REAR DOOR',
    '05223': 'BURGLARY - BUSINESS/DAY - SMASHED, KICKED,STRUCK - SIDE DOOR',
    '05224': 'BURGLARY - BUSINESS/DAY - SMASHED, KICKED,STRUCK - FRT WIND',
    '05225': 'BURGLARY - BUSINESS/DAY - SMASHED, KICKED,STRUCK - REAR WIND',
    '05226': 'BURGLARY - BUSINESS/DAY - SMASHED, KICKED,STRUCK - SIDE WIND',
    '05227': 'BURGLARY - BUSINESS/DAY - SMASHED, KICKED,STRUCK - ROOF,FLOOR',
    '05228': 'BURGLARY - BUSINESS/DAY - SMASHED, KICKED,STRUCK - WALL',
    '05229': 'BURGLARY - BUSINESS/DAY - SMASHED, KICKED,STRUCK - UNKNOWN',
    '0522A': 'BURGLARY - BUSINESS/DAY - SMASHED, KICKED,STRUCK - RES GRG DR',
    '0522B': 'BURGLARY - BUSINESS/DAY - SMASHED, KICKED,STRUCK - HID IN BLDG',
    '0522C': 'BURGLARY - BUSINESS/DAY - SMASHED, KICKED,STRUCK - OVRHEAD DR',
    '05231': 'BURGLARY - BUSINESS/DAY - PRY, BREAK SAW,CUT, ETC. - FRONT DOOR',
    '05232': 'BURGLARY - BUSINESS/DAY - PRY, BREAK SAW,CUT, ETC. - REAR DOOR',
    '05233': 'BURGLARY - BUSINESS/DAY - PRY, BREAK SAW,CUT, ETC. - SIDE DOOR',
    '05234': 'BURGLARY - BUSINESS/DAY - PRY, BREAK SAW,CUT, ETC. - FRT WIND',
    '05235': 'BURGLARY - BUSINESS/DAY - PRY, BREAK SAW,CUT, ETC. - REAR WIND',
    '05236': 'BURGLARY - BUSINESS/DAY - PRY, BREAK SAW,CUT, ETC. - SIDE WIND',
    '05237': 'BURGLARY - BUSINESS/DAY - PRY, BREAK SAW,CUT, ETC. - ROOF,FLOOR',
    '05238': 'BURGLARY - BUSINESS/DAY - PRY, BREAK SAW,CUT, ETC. - WALL',
    '05239': 'BURGLARY - BUSINESS/DAY - PRY, BREAK SAW,CUT, ETC. - UNKNOWN',
    '0523A': 'BURGLARY - BUSINESS/DAY - PRY, BREAK SAW,CUT, ETC. - RES GRG DR',
    '0523B': 'BURGLARY - BUSINESS/DAY - PRY, BREAK SAW,CUT, ETC. - HID IN BLDG',
    '0523C': 'BURGLARY - BUSINESS/DAY - PRY, BREAK SAW,CUT, ETC. - OVRHEAD DR',
    '05241': 'BURGLARY - BUSINESS/DAY - EXPLOSION,ARSON,CUTTING TORCH - FRONT DOOR',
    '05242': 'BURGLARY - BUSINESS/DAY - EXPLOSION,ARSON,CUTTING TORCH - REAR DOOR',
    '05243': 'BURGLARY - BUSINESS/DAY - EXPLOSION,ARSON,CUTTING TORCH - SIDE DOOR',
    '05244': 'BURGLARY - BUSINESS/DAY - EXPLOSION,ARSON,CUTTING TORCH - FRT WIND',
    '05245': 'BURGLARY - BUSINESS/DAY - EXPLOSION,ARSON,CUTTING TORCH - REAR WIND',
    '05246': 'BURGLARY - BUSINESS/DAY - EXPLOSION,ARSON,CUTTING TORCH - SIDE WIND',
    '05247': 'BURGLARY - BUSINESS/DAY - EXPLOSION,ARSON,CUTTING TORCH - ROOF,FLOOR',
    '05248': 'BURGLARY - BUSINESS/DAY - EXPLOSION,ARSON,CUTTING TORCH - WALL',
    '05249': 'BURGLARY - BUSINESS/DAY - EXPLOSION,ARSON,CUTTING TORCH - UNKNOWN',
    '0524A': 'BURGLARY - BUSINESS/DAY - EXPLOSION,ARSON,CUTTING TORCH - RES GRG DR',
    '0524B': 'BURGLARY - BUSINESS/DAY - EXPLOSION,ARSON,CUTTING TORCH - HID IN BLDG',
    '0524C': 'BURGLARY - BUSINESS/DAY - EXPLOSION,ARSON,CUTTING TORCH - OVRHEAD DR',
    '05251': 'BURGLARY - BUSINESS/DAY - NO FORCE (HIDE INSIDE) - FRONT DOOR',
    '05252': 'BURGLARY - BUSINESS/DAY - NO FORCE (HIDE INSIDE) - REAR DOOR',
    '05253': 'BURGLARY - BUSINESS/DAY - NO FORCE (HIDE INSIDE) - SIDE DOOR',
    '05254': 'BURGLARY - BUSINESS/DAY - NO FORCE (HIDE INSIDE) - FRT WIND',
    '05255': 'BURGLARY - BUSINESS/DAY - NO FORCE (HIDE INSIDE) - REAR WIND',
    '05256': 'BURGLARY - BUSINESS/DAY - NO FORCE (HIDE INSIDE) - SIDE WIND',
    '05257': 'BURGLARY - BUSINESS/DAY - NO FORCE (HIDE INSIDE) - ROOF,FLOOR',
    '05258': 'BURGLARY - BUSINESS/DAY - NO FORCE (HIDE INSIDE) - WALL',
    '05259': 'BURGLARY - BUSINESS/DAY - NO FORCE (HIDE INSIDE) - UNKNOWN',
    '0525A': 'BURGLARY - BUSINESS/DAY - NO FORCE (HIDE INSIDE) - RES GRG DR',
    '0525B': 'BURGLARY - BUSINESS/DAY - NO FORCE (HIDE INSIDE) - HID IN BLDG',
    '0525C': 'BURGLARY - BUSINESS/DAY - NO FORCE (HIDE INSIDE) - OVRHEAD DR',
    '05261': 'BURGLARY - BUSINESS/DAY - WINDOW,DOOR,ETC UNLOCKED - FRONT DOOR',
    '05262': 'BURGLARY - BUSINESS/DAY - WINDOW,DOOR,ETC UNLOCKED - REAR DOOR',
    '05263': 'BURGLARY - BUSINESS/DAY - WINDOW,DOOR,ETC UNLOCKED - SIDE DOOR',
    '05264': 'BURGLARY - BUSINESS/DAY - WINDOW,DOOR,ETC UNLOCKED - FRT WIND',
    '05265': 'BURGLARY - BUSINESS/DAY - WINDOW,DOOR,ETC UNLOCKED - REAR WIND',
    '05266': 'BURGLARY - BUSINESS/DAY - WINDOW,DOOR,ETC UNLOCKED - SIDE WIND',
    '05267': 'BURGLARY - BUSINESS/DAY - WINDOW,DOOR,ETC UNLOCKED - ROOF,FLOOR',
    '05268': 'BURGLARY - BUSINESS/DAY - WINDOW,DOOR,ETC UNLOCKED - WALL',
    '05269': 'BURGLARY - BUSINESS/DAY - WINDOW,DOOR,ETC UNLOCKED - UNKNOWN',
    '0526A': 'BURGLARY - BUSINESS/DAY - WINDOW,DOOR,ETC UNLOCKED - RES GRG DR',
    '0526B': 'BURGLARY - BUSINESS/DAY - WINDOW,DOOR,ETC UNLOCKED - HID IN BLDG',
    '0526C': 'BURGLARY - BUSINESS/DAY - WINDOW,DOOR,ETC UNLOCKED - OVRHEAD DR',
    '05271': 'BURGLARY - BUSINESS/DAY - ALL ATTEMPTS (NO FORCE) - FRONT DOOR',
    '05272': 'BURGLARY - BUSINESS/DAY - ALL ATTEMPTS (NO FORCE) - REAR DOOR',
    '05273': 'BURGLARY - BUSINESS/DAY - ALL ATTEMPTS (NO FORCE) - SIDE DOOR',
    '05274': 'BURGLARY - BUSINESS/DAY - ALL ATTEMPTS (NO FORCE) - FRT WIND',
    '05275': 'BURGLARY - BUSINESS/DAY - ALL ATTEMPTS (NO FORCE) - REAR WIND',
    '05276': 'BURGLARY - BUSINESS/DAY - ALL ATTEMPTS (NO FORCE) - SIDE WIND',
    '05277': 'BURGLARY - BUSINESS/DAY - ALL ATTEMPTS (NO FORCE) - ROOF,FLOOR',
    '05278': 'BURGLARY - BUSINESS/DAY - ALL ATTEMPTS (NO FORCE) - WALL',
    '05279': 'BURGLARY - BUSINESS/DAY - ALL ATTEMPTS (NO FORCE) - UNKNOWN',
    '0527A': 'BURGLARY - BUSINESS/DAY - ALL ATTEMPTS (NO FORCE) - RES GRG DR',
    '0527B': 'BURGLARY - BUSINESS/DAY - ALL ATTEMPTS (NO FORCE) - HID IN BLDG',
    '0527C': 'BURGLARY - BUSINESS/DAY - ALL ATTEMPTS (NO FORCE) - OVRHEAD DR',
    '05281': 'BURGLARY - BUSINESS/DAY - ALL OTHER ATTEMPTS - FRONT DOOR',
    '05282': 'BURGLARY - BUSINESS/DAY - ALL OTHER ATTEMPTS - REAR DOOR',
    '05283': 'BURGLARY - BUSINESS/DAY - ALL OTHER ATTEMPTS - SIDE DOOR',
    '05284': 'BURGLARY - BUSINESS/DAY - ALL OTHER ATTEMPTS - FRT WIND',
    '05285': 'BURGLARY - BUSINESS/DAY - ALL OTHER ATTEMPTS - REAR WIND',
    '05286': 'BURGLARY - BUSINESS/DAY - ALL OTHER ATTEMPTS - SIDE WIND',
    '05287': 'BURGLARY - BUSINESS/DAY - ALL OTHER ATTEMPTS - ROOF,FLOOR',
    '05288': 'BURGLARY - BUSINESS/DAY - ALL OTHER ATTEMPTS - WALL',
    '05289': 'BURGLARY - BUSINESS/DAY - ALL OTHER ATTEMPTS - UNKNOWN',
    '0528A': 'BURGLARY - BUSINESS/DAY - ALL OTHER ATTEMPTS - RES GRG DR',
    '0528B': 'BURGLARY - BUSINESS/DAY - ALL OTHER ATTEMPTS - HID IN BLDG',
    '0528C': 'BURGLARY - BUSINESS/DAY - ALL OTHER ATTEMPTS - OVRHEAD DR',
    '05291': 'BURGLARY - BUSINESS/DAY - OTHERS - FRONT DOOR',
    '05292': 'BURGLARY - BUSINESS/DAY - OTHERS - REAR DOOR',
    '05293': 'BURGLARY - BUSINESS/DAY - OTHERS - SIDE DOOR',
    '05294': 'BURGLARY - BUSINESS/DAY - OTHERS - FRT WIND',
    '05295': 'BURGLARY - BUSINESS/DAY - OTHERS - REAR WIND',
    '05296': 'BURGLARY - BUSINESS/DAY - OTHERS - SIDE WIND',
    '05297': 'BURGLARY - BUSINESS/DAY - OTHERS - ROOF,FLOOR',
    '05298': 'BURGLARY - BUSINESS/DAY - OTHERS - WALL',
    '05299': 'BURGLARY - BUSINESS/DAY - OTHERS - UNKNOWN',
    '0529A': 'BURGLARY - BUSINESS/DAY - OTHERS - RES GRG DR',
    '0529B': 'BURGLARY - BUSINESS/DAY - OTHERS - HID IN BLDG',
    '0529C': 'BURGLARY - BUSINESS/DAY - OTHERS - OVRHEAD DR',
    '05311': 'BURGLARY - BUSINESS/UNK - KEY/COMBINATiON LOCK - FRONT DOOR',
    '05312': 'BURGLARY - BUSINESS/UNK - KEY/COMBINATiON LOCK - REAR DOOR',
    '05313': 'BURGLARY - BUSINESS/UNK - KEY/COMBINATiON LOCK - SIDE DOOR',
    '05314': 'BURGLARY - BUSINESS/UNK - KEY/COMBINATiON LOCK - FRT WIND',
    '05315': 'BURGLARY - BUSINESS/UNK - KEY/COMBINATiON LOCK - REAR WIND',
    '05316': 'BURGLARY - BUSINESS/UNK - KEY/COMBINATiON LOCK - SIDE WIND',
    '05317': 'BURGLARY - BUSINESS/UNK - KEY/COMBINATiON LOCK - ROOF,FLOOR',
    '05318': 'BURGLARY - BUSINESS/UNK - KEY/COMBINATiON LOCK - WALL',
    '05319': 'BURGLARY - BUSINESS/UNK - KEY/COMBINATiON LOCK - UNKNOWN',
    '0531A': 'BURGLARY - BUSINESS/UNK - KEY/COMBINATiON LOCK - RES GRG DR',
    '0531B': 'BURGLARY - BUSINESS/UNK - KEY/COMBINATiON LOCK - HID IN BLDG',
    '0531C': 'BURGLARY - BUSINESS/UNK - KEY/COMBINATiON LOCK - OVRHEAD DR',
    '05321': 'BURGLARY - BUSINESS/UNK - SMASHED, KICKED,STRUCK - FRONT DOOR',
    '05322': 'BURGLARY - BUSINESS/UNK - SMASHED, KICKED,STRUCK - REAR DOOR',
    '05323': 'BURGLARY - BUSINESS/UNK - SMASHED, KICKED,STRUCK - SIDE DOOR',
    '05324': 'BURGLARY - BUSINESS/UNK - SMASHED, KICKED,STRUCK - FRT WIND',
    '05325': 'BURGLARY - BUSINESS/UNK - SMASHED, KICKED,STRUCK - REAR WIND',
    '05326': 'BURGLARY - BUSINESS/UNK - SMASHED, KICKED,STRUCK - SIDE WIND',
    '05327': 'BURGLARY - BUSINESS/UNK - SMASHED, KICKED,STRUCK - ROOF,FLOOR',
    '05328': 'BURGLARY - BUSINESS/UNK - SMASHED, KICKED,STRUCK - WALL',
    '05329': 'BURGLARY - BUSINESS/UNK - SMASHED, KICKED,STRUCK - UNKNOWN',
    '0532A': 'BURGLARY - BUSINESS/UNK - SMASHED, KICKED,STRUCK - RES GRG DR',
    '0532B': 'BURGLARY - BUSINESS/UNK - SMASHED, KICKED,STRUCK - HID IN BLDG',
    '0532C': 'BURGLARY - BUSINESS/UNK - SMASHED, KICKED,STRUCK - OVRHEAD DR',
    '05331': 'BURGLARY - BUSINESS/UNK - PRY, BREAK SAW,CUT, ETC. - FRONT DOOR',
    '05332': 'BURGLARY - BUSINESS/UNK - PRY, BREAK SAW,CUT, ETC. - REAR DOOR',
    '05333': 'BURGLARY - BUSINESS/UNK - PRY, BREAK SAW,CUT, ETC. - SIDE DOOR',
    '05334': 'BURGLARY - BUSINESS/UNK - PRY, BREAK SAW,CUT, ETC. - FRT WIND',
    '05335': 'BURGLARY - BUSINESS/UNK - PRY, BREAK SAW,CUT, ETC. - REAR WIND',
    '05336': 'BURGLARY - BUSINESS/UNK - PRY, BREAK SAW,CUT, ETC. - SIDE WIND',
    '05337': 'BURGLARY - BUSINESS/UNK - PRY, BREAK SAW,CUT, ETC. - ROOF,FLOOR',
    '05338': 'BURGLARY - BUSINESS/UNK - PRY, BREAK SAW,CUT, ETC. - WALL',
    '05339': 'BURGLARY - BUSINESS/UNK - PRY, BREAK SAW,CUT, ETC. - UNKNOWN',
    '0533A': 'BURGLARY - BUSINESS/UNK - PRY, BREAK SAW,CUT, ETC. - RES GRG DR',
    '0533B': 'BURGLARY - BUSINESS/UNK - PRY, BREAK SAW,CUT, ETC. - HID IN BLDG',
    '0533C': 'BURGLARY - BUSINESS/UNK - PRY, BREAK SAW,CUT, ETC. - OVRHEAD DR',
    '05341': 'BURGLARY - BUSINESS/UNK - EXPLOSION,ARSON,CUTTING TORCH - FRONT DOOR',
    '05342': 'BURGLARY - BUSINESS/UNK - EXPLOSION,ARSON,CUTTING TORCH - REAR DOOR',
    '05343': 'BURGLARY - BUSINESS/UNK - EXPLOSION,ARSON,CUTTING TORCH - SIDE DOOR',
    '05344': 'BURGLARY - BUSINESS/UNK - EXPLOSION,ARSON,CUTTING TORCH - FRT WIND',
    '05345': 'BURGLARY - BUSINESS/UNK - EXPLOSION,ARSON,CUTTING TORCH - REAR WIND',
    '05346': 'BURGLARY - BUSINESS/UNK - EXPLOSION,ARSON,CUTTING TORCH - SIDE WIND',
    '05347': 'BURGLARY - BUSINESS/UNK - EXPLOSION,ARSON,CUTTING TORCH - ROOF,FLOOR',
    '05348': 'BURGLARY - BUSINESS/UNK - EXPLOSION,ARSON,CUTTING TORCH - WALL',
    '05349': 'BURGLARY - BUSINESS/UNK - EXPLOSION,ARSON,CUTTING TORCH - UNKNOWN',
    '0534A': 'BURGLARY - BUSINESS/UNK - EXPLOSION,ARSON,CUTTING TORCH - RES GRG DR',
    '0534B': 'BURGLARY - BUSINESS/UNK - EXPLOSION,ARSON,CUTTING TORCH - HID IN BLDG',
    '0534C': 'BURGLARY - BUSINESS/UNK - EXPLOSION,ARSON,CUTTING TORCH - OVRHEAD DR',
    '05351': 'BURGLARY - BUSINESS/UNK - NO FORCE (HIDE INSIDE) - FRONT DOOR',
    '05352': 'BURGLARY - BUSINESS/UNK - NO FORCE (HIDE INSIDE) - REAR DOOR',
    '05353': 'BURGLARY - BUSINESS/UNK - NO FORCE (HIDE INSIDE) - SIDE DOOR',
    '05354': 'BURGLARY - BUSINESS/UNK - NO FORCE (HIDE INSIDE) - FRT WIND',
    '05355': 'BURGLARY - BUSINESS/UNK - NO FORCE (HIDE INSIDE) - REAR WIND',
    '05356': 'BURGLARY - BUSINESS/UNK - NO FORCE (HIDE INSIDE) - SIDE WIND',
    '05357': 'BURGLARY - BUSINESS/UNK - NO FORCE (HIDE INSIDE) - ROOF,FLOOR',
    '05358': 'BURGLARY - BUSINESS/UNK - NO FORCE (HIDE INSIDE) - WALL',
    '05359': 'BURGLARY - BUSINESS/UNK - NO FORCE (HIDE INSIDE) - UNKNOWN',
    '0535A': 'BURGLARY - BUSINESS/UNK - NO FORCE (HIDE INSIDE) - RES GRG DR',
    '0535B': 'BURGLARY - BUSINESS/UNK - NO FORCE (HIDE INSIDE) - HID IN BLDG',
    '0535C': 'BURGLARY - BUSINESS/UNK - NO FORCE (HIDE INSIDE) - OVRHEAD DR',
    '05361': 'BURGLARY - BUSINESS/UNK - WINDOW,DOOR,ETC UNLOCKED - FRONT DOOR',
    '05362': 'BURGLARY - BUSINESS/UNK - WINDOW,DOOR,ETC UNLOCKED - REAR DOOR',
    '05363': 'BURGLARY - BUSINESS/UNK - WINDOW,DOOR,ETC UNLOCKED - SIDE DOOR',
    '05364': 'BURGLARY - BUSINESS/UNK - WINDOW,DOOR,ETC UNLOCKED - FRT WIND',
    '05365': 'BURGLARY - BUSINESS/UNK - WINDOW,DOOR,ETC UNLOCKED - REAR WIND',
    '05366': 'BURGLARY - BUSINESS/UNK - WINDOW,DOOR,ETC UNLOCKED - SIDE WIND',
    '05367': 'BURGLARY - BUSINESS/UNK - WINDOW,DOOR,ETC UNLOCKED - ROOF,FLOOR',
    '05368': 'BURGLARY - BUSINESS/UNK - WINDOW,DOOR,ETC UNLOCKED - WALL',
    '05369': 'BURGLARY - BUSINESS/UNK - WINDOW,DOOR,ETC UNLOCKED - UNKNOWN',
    '0536A': 'BURGLARY - BUSINESS/UNK - WINDOW,DOOR,ETC UNLOCKED - RES GRG DR',
    '0536B': 'BURGLARY - BUSINESS/UNK - WINDOW,DOOR,ETC UNLOCKED - HID IN BLDG',
    '0536C': 'BURGLARY - BUSINESS/UNK - WINDOW,DOOR,ETC UNLOCKED - OVRHEAD DR',
    '05371': 'BURGLARY - BUSINESS/UNK - ALL ATTEMPTS (NO FORCE) - FRONT DOOR',
    '05372': 'BURGLARY - BUSINESS/UNK - ALL ATTEMPTS (NO FORCE) - REAR DOOR',
    '05373': 'BURGLARY - BUSINESS/UNK - ALL ATTEMPTS (NO FORCE) - SIDE DOOR',
    '05374': 'BURGLARY - BUSINESS/UNK - ALL ATTEMPTS (NO FORCE) - FRT WIND',
    '05375': 'BURGLARY - BUSINESS/UNK - ALL ATTEMPTS (NO FORCE) - REAR WIND',
    '05376': 'BURGLARY - BUSINESS/UNK - ALL ATTEMPTS (NO FORCE) - SIDE WIND',
    '05377': 'BURGLARY - BUSINESS/UNK - ALL ATTEMPTS (NO FORCE) - ROOF,FLOOR',
    '05378': 'BURGLARY - BUSINESS/UNK - ALL ATTEMPTS (NO FORCE) - WALL',
    '05379': 'BURGLARY - BUSINESS/UNK - ALL ATTEMPTS (NO FORCE) - UNKNOWN',
    '0537A': 'BURGLARY - BUSINESS/UNK - ALL ATTEMPTS (NO FORCE) - RES GRG DR',
    '0537B': 'BURGLARY - BUSINESS/UNK - ALL ATTEMPTS (NO FORCE) - HID IN BLDG',
    '0537C': 'BURGLARY - BUSINESS/UNK - ALL ATTEMPTS (NO FORCE) - OVRHEAD DR',
    '05381': 'BURGLARY - BUSINESS/UNK - ALL OTHER ATTEMPTS - FRONT DOOR',
    '05382': 'BURGLARY - BUSINESS/UNK - ALL OTHER ATTEMPTS - REAR DOOR',
    '05383': 'BURGLARY - BUSINESS/UNK - ALL OTHER ATTEMPTS - SIDE DOOR',
    '05384': 'BURGLARY - BUSINESS/UNK - ALL OTHER ATTEMPTS - FRT WIND',
    '05385': 'BURGLARY - BUSINESS/UNK - ALL OTHER ATTEMPTS - REAR WIND',
    '05386': 'BURGLARY - BUSINESS/UNK - ALL OTHER ATTEMPTS - SIDE WIND',
    '05387': 'BURGLARY - BUSINESS/UNK - ALL OTHER ATTEMPTS - ROOF,FLOOR',
    '05388': 'BURGLARY - BUSINESS/UNK - ALL OTHER ATTEMPTS - WALL',
    '05389': 'BURGLARY - BUSINESS/UNK - ALL OTHER ATTEMPTS - UNKNOWN',
    '0538A': 'BURGLARY - BUSINESS/UNK - ALL OTHER ATTEMPTS - RES GRG DR',
    '0538B': 'BURGLARY - BUSINESS/UNK - ALL OTHER ATTEMPTS - HID IN BLDG',
    '0538C': 'BURGLARY - BUSINESS/UNK - ALL OTHER ATTEMPTS - OVRHEAD DR',
    '05391': 'BURGLARY - BUSINESS/UNK - OTHERS - FRONT DOOR',
    '05392': 'BURGLARY - BUSINESS/UNK - OTHERS - REAR DOOR',
    '05393': 'BURGLARY - BUSINESS/UNK - OTHERS - SIDE DOOR',
    '05394': 'BURGLARY - BUSINESS/UNK - OTHERS - FRT WIND',
    '05395': 'BURGLARY - BUSINESS/UNK - OTHERS - REAR WIND',
    '05396': 'BURGLARY - BUSINESS/UNK - OTHERS - SIDE WIND',
    '05397': 'BURGLARY - BUSINESS/UNK - OTHERS - ROOF,FLOOR',
    '05398': 'BURGLARY - BUSINESS/UNK - OTHERS - WALL',
    '05399': 'BURGLARY - BUSINESS/UNK - OTHERS - UNKNOWN',
    '0539A': 'BURGLARY - BUSINESS/UNK - OTHERS - RES GRG DR',
    '0539B': 'BURGLARY - BUSINESS/UNK - OTHERS - HID IN BLDG',
    '0539C': 'BURGLARY - BUSINESS/UNK - OTHERS - OVRHEAD DR',
    '05411': 'BURGLARY - RESIDENCE/NIGHT - KEY/COMBINATiON LOCK - FRONT DOOR',
    '05412': 'BURGLARY - RESIDENCE/NIGHT - KEY/COMBINATiON LOCK - REAR DOOR',
    '05413': 'BURGLARY - RESIDENCE/NIGHT - KEY/COMBINATiON LOCK - SIDE DOOR',
    '05414': 'BURGLARY - RESIDENCE/NIGHT - KEY/COMBINATiON LOCK - FRT WIND',
    '05415': 'BURGLARY - RESIDENCE/NIGHT - KEY/COMBINATiON LOCK - REAR WIND',
    '05416': 'BURGLARY - RESIDENCE/NIGHT - KEY/COMBINATiON LOCK - SIDE WIND',
    '05417': 'BURGLARY - RESIDENCE/NIGHT - KEY/COMBINATiON LOCK - ROOF,FLOOR',
    '05418': 'BURGLARY - RESIDENCE/NIGHT - KEY/COMBINATiON LOCK - WALL',
    '05419': 'BURGLARY - RESIDENCE/NIGHT - KEY/COMBINATiON LOCK - UNKNOWN',
    '0541A': 'BURGLARY - RESIDENCE/NIGHT - KEY/COMBINATiON LOCK - RES GRG DR',
    '0541B': 'BURGLARY - RESIDENCE/NIGHT - KEY/COMBINATiON LOCK - HID IN BLDG',
    '0541C': 'BURGLARY - RESIDENCE/NIGHT - KEY/COMBINATiON LOCK - OVRHEAD DR',
    '05421': 'BURGLARY - RESIDENCE/NIGHT - SMASHED, KICKED,STRUCK - FRONT DOOR',
    '05422': 'BURGLARY - RESIDENCE/NIGHT - SMASHED, KICKED,STRUCK - REAR DOOR',
    '05423': 'BURGLARY - RESIDENCE/NIGHT - SMASHED, KICKED,STRUCK - SIDE DOOR',
    '05424': 'BURGLARY - RESIDENCE/NIGHT - SMASHED, KICKED,STRUCK - FRT WIND',
    '05425': 'BURGLARY - RESIDENCE/NIGHT - SMASHED, KICKED,STRUCK - REAR WIND',
    '05426': 'BURGLARY - RESIDENCE/NIGHT - SMASHED, KICKED,STRUCK - SIDE WIND',
    '05427': 'BURGLARY - RESIDENCE/NIGHT - SMASHED, KICKED,STRUCK - ROOF,FLOOR',
    '05428': 'BURGLARY - RESIDENCE/NIGHT - SMASHED, KICKED,STRUCK - WALL',
    '05429': 'BURGLARY - RESIDENCE/NIGHT - SMASHED, KICKED,STRUCK - UNKNOWN',
    '0542A': 'BURGLARY - RESIDENCE/NIGHT - SMASHED, KICKED,STRUCK - RES GRG DR',
    '0542B': 'BURGLARY - RESIDENCE/NIGHT - SMASHED, KICKED,STRUCK - HID IN BLDG',
    '0542C': 'BURGLARY - RESIDENCE/NIGHT - SMASHED, KICKED,STRUCK - OVRHEAD DR',
    '05431': 'BURGLARY - RESIDENCE/NIGHT - PRY, BREAK SAW,CUT, ETC. - FRONT DOOR',
    '05432': 'BURGLARY - RESIDENCE/NIGHT - PRY, BREAK SAW,CUT, ETC. - REAR DOOR',
    '05433': 'BURGLARY - RESIDENCE/NIGHT - PRY, BREAK SAW,CUT, ETC. - SIDE DOOR',
    '05434': 'BURGLARY - RESIDENCE/NIGHT - PRY, BREAK SAW,CUT, ETC. - FRT WIND',
    '05435': 'BURGLARY - RESIDENCE/NIGHT - PRY, BREAK SAW,CUT, ETC. - REAR WIND',
    '05436': 'BURGLARY - RESIDENCE/NIGHT - PRY, BREAK SAW,CUT, ETC. - SIDE WIND',
    '05437': 'BURGLARY - RESIDENCE/NIGHT - PRY, BREAK SAW,CUT, ETC. - ROOF,FLOOR',
    '05438': 'BURGLARY - RESIDENCE/NIGHT - PRY, BREAK SAW,CUT, ETC. - WALL',
    '05439': 'BURGLARY - RESIDENCE/NIGHT - PRY, BREAK SAW,CUT, ETC. - UNKNOWN',
    '0543A': 'BURGLARY - RESIDENCE/NIGHT - PRY, BREAK SAW,CUT, ETC. - RES GRG DR',
    '0543B': 'BURGLARY - RESIDENCE/NIGHT - PRY, BREAK SAW,CUT, ETC. - HID IN BLDG',
    '0543C': 'BURGLARY - RESIDENCE/NIGHT - PRY, BREAK SAW,CUT, ETC. - OVRHEAD DR',
    '05441': 'BURGLARY - RESIDENCE/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - FRONT DOOR',
    '05442': 'BURGLARY - RESIDENCE/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - REAR DOOR',
    '05443': 'BURGLARY - RESIDENCE/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - SIDE DOOR',
    '05444': 'BURGLARY - RESIDENCE/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - FRT WIND',
    '05445': 'BURGLARY - RESIDENCE/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - REAR WIND',
    '05446': 'BURGLARY - RESIDENCE/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - SIDE WIND',
    '05447': 'BURGLARY - RESIDENCE/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - ROOF,FLOOR',
    '05448': 'BURGLARY - RESIDENCE/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - WALL',
    '05449': 'BURGLARY - RESIDENCE/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - UNKNOWN',
    '0544A': 'BURGLARY - RESIDENCE/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - RES GRG DR',
    '0544B': 'BURGLARY - RESIDENCE/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - HID IN BLDG',
    '0544C': 'BURGLARY - RESIDENCE/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - OVRHEAD DR',
    '05451': 'BURGLARY - RESIDENCE/NIGHT - NO FORCE (HIDE INSIDE) - FRONT DOOR',
    '05452': 'BURGLARY - RESIDENCE/NIGHT - NO FORCE (HIDE INSIDE) - REAR DOOR',
    '05453': 'BURGLARY - RESIDENCE/NIGHT - NO FORCE (HIDE INSIDE) - SIDE DOOR',
    '05454': 'BURGLARY - RESIDENCE/NIGHT - NO FORCE (HIDE INSIDE) - FRT WIND',
    '05455': 'BURGLARY - RESIDENCE/NIGHT - NO FORCE (HIDE INSIDE) - REAR WIND',
    '05456': 'BURGLARY - RESIDENCE/NIGHT - NO FORCE (HIDE INSIDE) - SIDE WIND',
    '05457': 'BURGLARY - RESIDENCE/NIGHT - NO FORCE (HIDE INSIDE) - ROOF,FLOOR',
    '05458': 'BURGLARY - RESIDENCE/NIGHT - NO FORCE (HIDE INSIDE) - WALL',
    '05459': 'BURGLARY - RESIDENCE/NIGHT - NO FORCE (HIDE INSIDE) - UNKNOWN',
    '0545A': 'BURGLARY - RESIDENCE/NIGHT - NO FORCE (HIDE INSIDE) - RES GRG DR',
    '0545B': 'BURGLARY - RESIDENCE/NIGHT - NO FORCE (HIDE INSIDE) - HID IN BLDG',
    '0545C': 'BURGLARY - RESIDENCE/NIGHT - NO FORCE (HIDE INSIDE) - OVRHEAD DR',
    '05461': 'BURGLARY - RESIDENCE/NIGHT - WINDOW,DOOR,ETC UNLOCKED - FRONT DOOR',
    '05462': 'BURGLARY - RESIDENCE/NIGHT - WINDOW,DOOR,ETC UNLOCKED - REAR DOOR',
    '05463': 'BURGLARY - RESIDENCE/NIGHT - WINDOW,DOOR,ETC UNLOCKED - SIDE DOOR',
    '05464': 'BURGLARY - RESIDENCE/NIGHT - WINDOW,DOOR,ETC UNLOCKED - FRT WIND',
    '05465': 'BURGLARY - RESIDENCE/NIGHT - WINDOW,DOOR,ETC UNLOCKED - REAR WIND',
    '05466': 'BURGLARY - RESIDENCE/NIGHT - WINDOW,DOOR,ETC UNLOCKED - SIDE WIND',
    '05467': 'BURGLARY - RESIDENCE/NIGHT - WINDOW,DOOR,ETC UNLOCKED - ROOF,FLOOR',
    '05468': 'BURGLARY - RESIDENCE/NIGHT - WINDOW,DOOR,ETC UNLOCKED - WALL',
    '05469': 'BURGLARY - RESIDENCE/NIGHT - WINDOW,DOOR,ETC UNLOCKED - UNKNOWN',
    '0546A': 'BURGLARY - RESIDENCE/NIGHT - WINDOW,DOOR,ETC UNLOCKED - RES GRG DR',
    '0546B': 'BURGLARY - RESIDENCE/NIGHT - WINDOW,DOOR,ETC UNLOCKED - HID IN BLDG',
    '0546C': 'BURGLARY - RESIDENCE/NIGHT - WINDOW,DOOR,ETC UNLOCKED - OVRHEAD DR',
    '05471': 'BURGLARY - RESIDENCE/NIGHT - ALL ATTEMPTS (NO FORCE) - FRONT DOOR',
    '05472': 'BURGLARY - RESIDENCE/NIGHT - ALL ATTEMPTS (NO FORCE) - REAR DOOR',
    '05473': 'BURGLARY - RESIDENCE/NIGHT - ALL ATTEMPTS (NO FORCE) - SIDE DOOR',
    '05474': 'BURGLARY - RESIDENCE/NIGHT - ALL ATTEMPTS (NO FORCE) - FRT WIND',
    '05475': 'BURGLARY - RESIDENCE/NIGHT - ALL ATTEMPTS (NO FORCE) - REAR WIND',
    '05476': 'BURGLARY - RESIDENCE/NIGHT - ALL ATTEMPTS (NO FORCE) - SIDE WIND',
    '05477': 'BURGLARY - RESIDENCE/NIGHT - ALL ATTEMPTS (NO FORCE) - ROOF,FLOOR',
    '05478': 'BURGLARY - RESIDENCE/NIGHT - ALL ATTEMPTS (NO FORCE) - WALL',
    '05479': 'BURGLARY - RESIDENCE/NIGHT - ALL ATTEMPTS (NO FORCE) - UNKNOWN',
    '0547A': 'BURGLARY - RESIDENCE/NIGHT - ALL ATTEMPTS (NO FORCE) - RES GRG DR',
    '0547B': 'BURGLARY - RESIDENCE/NIGHT - ALL ATTEMPTS (NO FORCE) - HID IN BLDG',
    '0547C': 'BURGLARY - RESIDENCE/NIGHT - ALL ATTEMPTS (NO FORCE) - OVRHEAD DR',
    '05481': 'BURGLARY - RESIDENCE/NIGHT - ALL OTHER ATTEMPTS - FRONT DOOR',
    '05482': 'BURGLARY - RESIDENCE/NIGHT - ALL OTHER ATTEMPTS - REAR DOOR',
    '05483': 'BURGLARY - RESIDENCE/NIGHT - ALL OTHER ATTEMPTS - SIDE DOOR',
    '05484': 'BURGLARY - RESIDENCE/NIGHT - ALL OTHER ATTEMPTS - FRT WIND',
    '05485': 'BURGLARY - RESIDENCE/NIGHT - ALL OTHER ATTEMPTS - REAR WIND',
    '05486': 'BURGLARY - RESIDENCE/NIGHT - ALL OTHER ATTEMPTS - SIDE WIND',
    '05487': 'BURGLARY - RESIDENCE/NIGHT - ALL OTHER ATTEMPTS - ROOF,FLOOR',
    '05488': 'BURGLARY - RESIDENCE/NIGHT - ALL OTHER ATTEMPTS - WALL',
    '05489': 'BURGLARY - RESIDENCE/NIGHT - ALL OTHER ATTEMPTS - UNKNOWN',
    '0548A': 'BURGLARY - RESIDENCE/NIGHT - ALL OTHER ATTEMPTS - RES GRG DR',
    '0548B': 'BURGLARY - RESIDENCE/NIGHT - ALL OTHER ATTEMPTS - HID IN BLDG',
    '0548C': 'BURGLARY - RESIDENCE/NIGHT - ALL OTHER ATTEMPTS - OVRHEAD DR',
    '05491': 'BURGLARY - RESIDENCE/NIGHT - OTHERS - FRONT DOOR',
    '05492': 'BURGLARY - RESIDENCE/NIGHT - OTHERS - REAR DOOR',
    '05493': 'BURGLARY - RESIDENCE/NIGHT - OTHERS - SIDE DOOR',
    '05494': 'BURGLARY - RESIDENCE/NIGHT - OTHERS - FRT WIND',
    '05495': 'BURGLARY - RESIDENCE/NIGHT - OTHERS - REAR WIND',
    '05496': 'BURGLARY - RESIDENCE/NIGHT - OTHERS - SIDE WIND',
    '05497': 'BURGLARY - RESIDENCE/NIGHT - OTHERS - ROOF,FLOOR',
    '05498': 'BURGLARY - RESIDENCE/NIGHT - OTHERS - WALL',
    '05499': 'BURGLARY - RESIDENCE/NIGHT - OTHERS - UNKNOWN',
    '0549A': 'BURGLARY - RESIDENCE/NIGHT - OTHERS - RES GRG DR',
    '0549B': 'BURGLARY - RESIDENCE/NIGHT - OTHERS - HID IN BLDG',
    '0549C': 'BURGLARY - RESIDENCE/NIGHT - OTHERS - OVRHEAD DR',
    '05511': 'BURGLARY - RESIDENCE/DAY - KEY/COMBINATiON LOCK - FRONT DOOR',
    '05512': 'BURGLARY - RESIDENCE/DAY - KEY/COMBINATiON LOCK - REAR DOOR',
    '05513': 'BURGLARY - RESIDENCE/DAY - KEY/COMBINATiON LOCK - SIDE DOOR',
    '05514': 'BURGLARY - RESIDENCE/DAY - KEY/COMBINATiON LOCK - FRT WIND',
    '05515': 'BURGLARY - RESIDENCE/DAY - KEY/COMBINATiON LOCK - REAR WIND',
    '05516': 'BURGLARY - RESIDENCE/DAY - KEY/COMBINATiON LOCK - SIDE WIND',
    '05517': 'BURGLARY - RESIDENCE/DAY - KEY/COMBINATiON LOCK - ROOF,FLOOR',
    '05518': 'BURGLARY - RESIDENCE/DAY - KEY/COMBINATiON LOCK - WALL',
    '05519': 'BURGLARY - RESIDENCE/DAY - KEY/COMBINATiON LOCK - UNKNOWN',
    '0551A': 'BURGLARY - RESIDENCE/DAY - KEY/COMBINATiON LOCK - RES GRG DR',
    '0551B': 'BURGLARY - RESIDENCE/DAY - KEY/COMBINATiON LOCK - HID IN BLDG',
    '0551C': 'BURGLARY - RESIDENCE/DAY - KEY/COMBINATiON LOCK - OVRHEAD DR',
    '05521': 'BURGLARY - RESIDENCE/DAY - SMASHED, KICKED,STRUCK - FRONT DOOR',
    '05522': 'BURGLARY - RESIDENCE/DAY - SMASHED, KICKED,STRUCK - REAR DOOR',
    '05523': 'BURGLARY - RESIDENCE/DAY - SMASHED, KICKED,STRUCK - SIDE DOOR',
    '05524': 'BURGLARY - RESIDENCE/DAY - SMASHED, KICKED,STRUCK - FRT WIND',
    '05525': 'BURGLARY - RESIDENCE/DAY - SMASHED, KICKED,STRUCK - REAR WIND',
    '05526': 'BURGLARY - RESIDENCE/DAY - SMASHED, KICKED,STRUCK - SIDE WIND',
    '05527': 'BURGLARY - RESIDENCE/DAY - SMASHED, KICKED,STRUCK - ROOF,FLOOR',
    '05528': 'BURGLARY - RESIDENCE/DAY - SMASHED, KICKED,STRUCK - WALL',
    '05529': 'BURGLARY - RESIDENCE/DAY - SMASHED, KICKED,STRUCK - UNKNOWN',
    '0552A': 'BURGLARY - RESIDENCE/DAY - SMASHED, KICKED,STRUCK - RES GRG DR',
    '0552B': 'BURGLARY - RESIDENCE/DAY - SMASHED, KICKED,STRUCK - HID IN BLDG',
    '0552C': 'BURGLARY - RESIDENCE/DAY - SMASHED, KICKED,STRUCK - OVRHEAD DR',
    '05531': 'BURGLARY - RESIDENCE/DAY - PRY, BREAK SAW,CUT, ETC. - FRONT DOOR',
    '05532': 'BURGLARY - RESIDENCE/DAY - PRY, BREAK SAW,CUT, ETC. - REAR DOOR',
    '05533': 'BURGLARY - RESIDENCE/DAY - PRY, BREAK SAW,CUT, ETC. - SIDE DOOR',
    '05534': 'BURGLARY - RESIDENCE/DAY - PRY, BREAK SAW,CUT, ETC. - FRT WIND',
    '05535': 'BURGLARY - RESIDENCE/DAY - PRY, BREAK SAW,CUT, ETC. - REAR WIND',
    '05536': 'BURGLARY - RESIDENCE/DAY - PRY, BREAK SAW,CUT, ETC. - SIDE WIND',
    '05537': 'BURGLARY - RESIDENCE/DAY - PRY, BREAK SAW,CUT, ETC. - ROOF,FLOOR',
    '05538': 'BURGLARY - RESIDENCE/DAY - PRY, BREAK SAW,CUT, ETC. - WALL',
    '05539': 'BURGLARY - RESIDENCE/DAY - PRY, BREAK SAW,CUT, ETC. - UNKNOWN',
    '0553A': 'BURGLARY - RESIDENCE/DAY - PRY, BREAK SAW,CUT, ETC. - RES GRG DR',
    '0553B': 'BURGLARY - RESIDENCE/DAY - PRY, BREAK SAW,CUT, ETC. - HID IN BLDG',
    '0553C': 'BURGLARY - RESIDENCE/DAY - PRY, BREAK SAW,CUT, ETC. - OVRHEAD DR',
    '05541': 'BURGLARY - RESIDENCE/DAY - EXPLOSION,ARSON,CUTTING TORCH - FRONT DOOR',
    '05542': 'BURGLARY - RESIDENCE/DAY - EXPLOSION,ARSON,CUTTING TORCH - REAR DOOR',
    '05543': 'BURGLARY - RESIDENCE/DAY - EXPLOSION,ARSON,CUTTING TORCH - SIDE DOOR',
    '05544': 'BURGLARY - RESIDENCE/DAY - EXPLOSION,ARSON,CUTTING TORCH - FRT WIND',
    '05545': 'BURGLARY - RESIDENCE/DAY - EXPLOSION,ARSON,CUTTING TORCH - REAR WIND',
    '05546': 'BURGLARY - RESIDENCE/DAY - EXPLOSION,ARSON,CUTTING TORCH - SIDE WIND',
    '05547': 'BURGLARY - RESIDENCE/DAY - EXPLOSION,ARSON,CUTTING TORCH - ROOF,FLOOR',
    '05548': 'BURGLARY - RESIDENCE/DAY - EXPLOSION,ARSON,CUTTING TORCH - WALL',
    '05549': 'BURGLARY - RESIDENCE/DAY - EXPLOSION,ARSON,CUTTING TORCH - UNKNOWN',
    '0554A': 'BURGLARY - RESIDENCE/DAY - EXPLOSION,ARSON,CUTTING TORCH - RES GRG DR',
    '0554B': 'BURGLARY - RESIDENCE/DAY - EXPLOSION,ARSON,CUTTING TORCH - HID IN BLDG',
    '0554C': 'BURGLARY - RESIDENCE/DAY - EXPLOSION,ARSON,CUTTING TORCH - OVRHEAD DR',
    '05551': 'BURGLARY - RESIDENCE/DAY - NO FORCE (HIDE INSIDE) - FRONT DOOR',
    '05552': 'BURGLARY - RESIDENCE/DAY - NO FORCE (HIDE INSIDE) - REAR DOOR',
    '05553': 'BURGLARY - RESIDENCE/DAY - NO FORCE (HIDE INSIDE) - SIDE DOOR',
    '05554': 'BURGLARY - RESIDENCE/DAY - NO FORCE (HIDE INSIDE) - FRT WIND',
    '05555': 'BURGLARY - RESIDENCE/DAY - NO FORCE (HIDE INSIDE) - REAR WIND',
    '05556': 'BURGLARY - RESIDENCE/DAY - NO FORCE (HIDE INSIDE) - SIDE WIND',
    '05557': 'BURGLARY - RESIDENCE/DAY - NO FORCE (HIDE INSIDE) - ROOF,FLOOR',
    '05558': 'BURGLARY - RESIDENCE/DAY - NO FORCE (HIDE INSIDE) - WALL',
    '05559': 'BURGLARY - RESIDENCE/DAY - NO FORCE (HIDE INSIDE) - UNKNOWN',
    '0555A': 'BURGLARY - RESIDENCE/DAY - NO FORCE (HIDE INSIDE) - RES GRG DR',
    '0555B': 'BURGLARY - RESIDENCE/DAY - NO FORCE (HIDE INSIDE) - HID IN BLDG',
    '0555C': 'BURGLARY - RESIDENCE/DAY - NO FORCE (HIDE INSIDE) - OVRHEAD DR',
    '05561': 'BURGLARY - RESIDENCE/DAY - WINDOW,DOOR,ETC UNLOCKED - FRONT DOOR',
    '05562': 'BURGLARY - RESIDENCE/DAY - WINDOW,DOOR,ETC UNLOCKED - REAR DOOR',
    '05563': 'BURGLARY - RESIDENCE/DAY - WINDOW,DOOR,ETC UNLOCKED - SIDE DOOR',
    '05564': 'BURGLARY - RESIDENCE/DAY - WINDOW,DOOR,ETC UNLOCKED - FRT WIND',
    '05565': 'BURGLARY - RESIDENCE/DAY - WINDOW,DOOR,ETC UNLOCKED - REAR WIND',
    '05566': 'BURGLARY - RESIDENCE/DAY - WINDOW,DOOR,ETC UNLOCKED - SIDE WIND',
    '05567': 'BURGLARY - RESIDENCE/DAY - WINDOW,DOOR,ETC UNLOCKED - ROOF,FLOOR',
    '05568': 'BURGLARY - RESIDENCE/DAY - WINDOW,DOOR,ETC UNLOCKED - WALL',
    '05569': 'BURGLARY - RESIDENCE/DAY - WINDOW,DOOR,ETC UNLOCKED - UNKNOWN',
    '0556A': 'BURGLARY - RESIDENCE/DAY - WINDOW,DOOR,ETC UNLOCKED - RES GRG DR',
    '0556B': 'BURGLARY - RESIDENCE/DAY - WINDOW,DOOR,ETC UNLOCKED - HID IN BLDG',
    '0556C': 'BURGLARY - RESIDENCE/DAY - WINDOW,DOOR,ETC UNLOCKED - OVRHEAD DR',
    '05571': 'BURGLARY - RESIDENCE/DAY - ALL ATTEMPTS (NO FORCE) - FRONT DOOR',
    '05572': 'BURGLARY - RESIDENCE/DAY - ALL ATTEMPTS (NO FORCE) - REAR DOOR',
    '05573': 'BURGLARY - RESIDENCE/DAY - ALL ATTEMPTS (NO FORCE) - SIDE DOOR',
    '05574': 'BURGLARY - RESIDENCE/DAY - ALL ATTEMPTS (NO FORCE) - FRT WIND',
    '05575': 'BURGLARY - RESIDENCE/DAY - ALL ATTEMPTS (NO FORCE) - REAR WIND',
    '05576': 'BURGLARY - RESIDENCE/DAY - ALL ATTEMPTS (NO FORCE) - SIDE WIND',
    '05577': 'BURGLARY - RESIDENCE/DAY - ALL ATTEMPTS (NO FORCE) - ROOF,FLOOR',
    '05578': 'BURGLARY - RESIDENCE/DAY - ALL ATTEMPTS (NO FORCE) - WALL',
    '05579': 'BURGLARY - RESIDENCE/DAY - ALL ATTEMPTS (NO FORCE) - UNKNOWN',
    '0557A': 'BURGLARY - RESIDENCE/DAY - ALL ATTEMPTS (NO FORCE) - RES GRG DR',
    '0557B': 'BURGLARY - RESIDENCE/DAY - ALL ATTEMPTS (NO FORCE) - HID IN BLDG',
    '0557C': 'BURGLARY - RESIDENCE/DAY - ALL ATTEMPTS (NO FORCE) - OVRHEAD DR',
    '05581': 'BURGLARY - RESIDENCE/DAY - ALL OTHER ATTEMPTS - FRONT DOOR',
    '05582': 'BURGLARY - RESIDENCE/DAY - ALL OTHER ATTEMPTS - REAR DOOR',
    '05583': 'BURGLARY - RESIDENCE/DAY - ALL OTHER ATTEMPTS - SIDE DOOR',
    '05584': 'BURGLARY - RESIDENCE/DAY - ALL OTHER ATTEMPTS - FRT WIND',
    '05585': 'BURGLARY - RESIDENCE/DAY - ALL OTHER ATTEMPTS - REAR WIND',
    '05586': 'BURGLARY - RESIDENCE/DAY - ALL OTHER ATTEMPTS - SIDE WIND',
    '05587': 'BURGLARY - RESIDENCE/DAY - ALL OTHER ATTEMPTS - ROOF,FLOOR',
    '05588': 'BURGLARY - RESIDENCE/DAY - ALL OTHER ATTEMPTS - WALL',
    '05589': 'BURGLARY - RESIDENCE/DAY - ALL OTHER ATTEMPTS - UNKNOWN',
    '0558A': 'BURGLARY - RESIDENCE/DAY - ALL OTHER ATTEMPTS - RES GRG DR',
    '0558B': 'BURGLARY - RESIDENCE/DAY - ALL OTHER ATTEMPTS - HID IN BLDG',
    '0558C': 'BURGLARY - RESIDENCE/DAY - ALL OTHER ATTEMPTS - OVRHEAD DR',
    '05591': 'BURGLARY - RESIDENCE/DAY - OTHERS - FRONT DOOR',
    '05592': 'BURGLARY - RESIDENCE/DAY - OTHERS - REAR DOOR',
    '05593': 'BURGLARY - RESIDENCE/DAY - OTHERS - SIDE DOOR',
    '05594': 'BURGLARY - RESIDENCE/DAY - OTHERS - FRT WIND',
    '05595': 'BURGLARY - RESIDENCE/DAY - OTHERS - REAR WIND',
    '05596': 'BURGLARY - RESIDENCE/DAY - OTHERS - SIDE WIND',
    '05597': 'BURGLARY - RESIDENCE/DAY - OTHERS - ROOF,FLOOR',
    '05598': 'BURGLARY - RESIDENCE/DAY - OTHERS - WALL',
    '05599': 'BURGLARY - RESIDENCE/DAY - OTHERS - UNKNOWN',
    '0559A': 'BURGLARY - RESIDENCE/DAY - OTHERS - RES GRG DR',
    '0559B': 'BURGLARY - RESIDENCE/DAY - OTHERS - HID IN BLDG',
    '0559C': 'BURGLARY - RESIDENCE/DAY - OTHERS - OVRHEAD DR',
    '05611': 'BURGLARY - RESIDENCE/UNK - KEY/COMBINATiON LOCK - FRONT DOOR',
    '05612': 'BURGLARY - RESIDENCE/UNK - KEY/COMBINATiON LOCK - REAR DOOR',
    '05613': 'BURGLARY - RESIDENCE/UNK - KEY/COMBINATiON LOCK - SIDE DOOR',
    '05614': 'BURGLARY - RESIDENCE/UNK - KEY/COMBINATiON LOCK - FRT WIND',
    '05615': 'BURGLARY - RESIDENCE/UNK - KEY/COMBINATiON LOCK - REAR WIND',
    '05616': 'BURGLARY - RESIDENCE/UNK - KEY/COMBINATiON LOCK - SIDE WIND',
    '05617': 'BURGLARY - RESIDENCE/UNK - KEY/COMBINATiON LOCK - ROOF,FLOOR',
    '05618': 'BURGLARY - RESIDENCE/UNK - KEY/COMBINATiON LOCK - WALL',
    '05619': 'BURGLARY - RESIDENCE/UNK - KEY/COMBINATiON LOCK - UNKNOWN',
    '0561A': 'BURGLARY - RESIDENCE/UNK - KEY/COMBINATiON LOCK - RES GRG DR',
    '0561B': 'BURGLARY - RESIDENCE/UNK - KEY/COMBINATiON LOCK - HID IN BLDG',
    '0561C': 'BURGLARY - RESIDENCE/UNK - KEY/COMBINATiON LOCK - OVRHEAD DR',
    '05621': 'BURGLARY - RESIDENCE/UNK - SMASHED, KICKED,STRUCK - FRONT DOOR',
    '05622': 'BURGLARY - RESIDENCE/UNK - SMASHED, KICKED,STRUCK - REAR DOOR',
    '05623': 'BURGLARY - RESIDENCE/UNK - SMASHED, KICKED,STRUCK - SIDE DOOR',
    '05624': 'BURGLARY - RESIDENCE/UNK - SMASHED, KICKED,STRUCK - FRT WIND',
    '05625': 'BURGLARY - RESIDENCE/UNK - SMASHED, KICKED,STRUCK - REAR WIND',
    '05626': 'BURGLARY - RESIDENCE/UNK - SMASHED, KICKED,STRUCK - SIDE WIND',
    '05627': 'BURGLARY - RESIDENCE/UNK - SMASHED, KICKED,STRUCK - ROOF,FLOOR',
    '05628': 'BURGLARY - RESIDENCE/UNK - SMASHED, KICKED,STRUCK - WALL',
    '05629': 'BURGLARY - RESIDENCE/UNK - SMASHED, KICKED,STRUCK - UNKNOWN',
    '0562A': 'BURGLARY - RESIDENCE/UNK - SMASHED, KICKED,STRUCK - RES GRG DR',
    '0562B': 'BURGLARY - RESIDENCE/UNK - SMASHED, KICKED,STRUCK - HID IN BLDG',
    '0562C': 'BURGLARY - RESIDENCE/UNK - SMASHED, KICKED,STRUCK - OVRHEAD DR',
    '05631': 'BURGLARY - RESIDENCE/UNK - PRY, BREAK SAW,CUT, ETC. - FRONT DOOR',
    '05632': 'BURGLARY - RESIDENCE/UNK - PRY, BREAK SAW,CUT, ETC. - REAR DOOR',
    '05633': 'BURGLARY - RESIDENCE/UNK - PRY, BREAK SAW,CUT, ETC. - SIDE DOOR',
    '05634': 'BURGLARY - RESIDENCE/UNK - PRY, BREAK SAW,CUT, ETC. - FRT WIND',
    '05635': 'BURGLARY - RESIDENCE/UNK - PRY, BREAK SAW,CUT, ETC. - REAR WIND',
    '05636': 'BURGLARY - RESIDENCE/UNK - PRY, BREAK SAW,CUT, ETC. - SIDE WIND',
    '05637': 'BURGLARY - RESIDENCE/UNK - PRY, BREAK SAW,CUT, ETC. - ROOF,FLOOR',
    '05638': 'BURGLARY - RESIDENCE/UNK - PRY, BREAK SAW,CUT, ETC. - WALL',
    '05639': 'BURGLARY - RESIDENCE/UNK - PRY, BREAK SAW,CUT, ETC. - UNKNOWN',
    '0563A': 'BURGLARY - RESIDENCE/UNK - PRY, BREAK SAW,CUT, ETC. - RES GRG DR',
    '0563B': 'BURGLARY - RESIDENCE/UNK - PRY, BREAK SAW,CUT, ETC. - HID IN BLDG',
    '0563C': 'BURGLARY - RESIDENCE/UNK - PRY, BREAK SAW,CUT, ETC. - OVRHEAD DR',
    '05641': 'BURGLARY - RESIDENCE/UNK - EXPLOSION,ARSON,CUTTING TORCH - FRONT DOOR',
    '05642': 'BURGLARY - RESIDENCE/UNK - EXPLOSION,ARSON,CUTTING TORCH - REAR DOOR',
    '05643': 'BURGLARY - RESIDENCE/UNK - EXPLOSION,ARSON,CUTTING TORCH - SIDE DOOR',
    '05644': 'BURGLARY - RESIDENCE/UNK - EXPLOSION,ARSON,CUTTING TORCH - FRT WIND',
    '05645': 'BURGLARY - RESIDENCE/UNK - EXPLOSION,ARSON,CUTTING TORCH - REAR WIND',
    '05646': 'BURGLARY - RESIDENCE/UNK - EXPLOSION,ARSON,CUTTING TORCH - SIDE WIND',
    '05647': 'BURGLARY - RESIDENCE/UNK - EXPLOSION,ARSON,CUTTING TORCH - ROOF,FLOOR',
    '05648': 'BURGLARY - RESIDENCE/UNK - EXPLOSION,ARSON,CUTTING TORCH - WALL',
    '05649': 'BURGLARY - RESIDENCE/UNK - EXPLOSION,ARSON,CUTTING TORCH - UNKNOWN',
    '0564A': 'BURGLARY - RESIDENCE/UNK - EXPLOSION,ARSON,CUTTING TORCH - RES GRG DR',
    '0564B': 'BURGLARY - RESIDENCE/UNK - EXPLOSION,ARSON,CUTTING TORCH - HID IN BLDG',
    '0564C': 'BURGLARY - RESIDENCE/UNK - EXPLOSION,ARSON,CUTTING TORCH - OVRHEAD DR',
    '05651': 'BURGLARY - RESIDENCE/UNK - NO FORCE (HIDE INSIDE) - FRONT DOOR',
    '05652': 'BURGLARY - RESIDENCE/UNK - NO FORCE (HIDE INSIDE) - REAR DOOR',
    '05653': 'BURGLARY - RESIDENCE/UNK - NO FORCE (HIDE INSIDE) - SIDE DOOR',
    '05654': 'BURGLARY - RESIDENCE/UNK - NO FORCE (HIDE INSIDE) - FRT WIND',
    '05655': 'BURGLARY - RESIDENCE/UNK - NO FORCE (HIDE INSIDE) - REAR WIND',
    '05656': 'BURGLARY - RESIDENCE/UNK - NO FORCE (HIDE INSIDE) - SIDE WIND',
    '05657': 'BURGLARY - RESIDENCE/UNK - NO FORCE (HIDE INSIDE) - ROOF,FLOOR',
    '05658': 'BURGLARY - RESIDENCE/UNK - NO FORCE (HIDE INSIDE) - WALL',
    '05659': 'BURGLARY - RESIDENCE/UNK - NO FORCE (HIDE INSIDE) - UNKNOWN',
    '0565A': 'BURGLARY - RESIDENCE/UNK - NO FORCE (HIDE INSIDE) - RES GRG DR',
    '0565B': 'BURGLARY - RESIDENCE/UNK - NO FORCE (HIDE INSIDE) - HID IN BLDG',
    '0565C': 'BURGLARY - RESIDENCE/UNK - NO FORCE (HIDE INSIDE) - OVRHEAD DR',
    '05661': 'BURGLARY - RESIDENCE/UNK - WINDOW,DOOR,ETC UNLOCKED - FRONT DOOR',
    '05662': 'BURGLARY - RESIDENCE/UNK - WINDOW,DOOR,ETC UNLOCKED - REAR DOOR',
    '05663': 'BURGLARY - RESIDENCE/UNK - WINDOW,DOOR,ETC UNLOCKED - SIDE DOOR',
    '05664': 'BURGLARY - RESIDENCE/UNK - WINDOW,DOOR,ETC UNLOCKED - FRT WIND',
    '05665': 'BURGLARY - RESIDENCE/UNK - WINDOW,DOOR,ETC UNLOCKED - REAR WIND',
    '05666': 'BURGLARY - RESIDENCE/UNK - WINDOW,DOOR,ETC UNLOCKED - SIDE WIND',
    '05667': 'BURGLARY - RESIDENCE/UNK - WINDOW,DOOR,ETC UNLOCKED - ROOF,FLOOR',
    '05668': 'BURGLARY - RESIDENCE/UNK - WINDOW,DOOR,ETC UNLOCKED - WALL',
    '05669': 'BURGLARY - RESIDENCE/UNK - WINDOW,DOOR,ETC UNLOCKED - UNKNOWN',
    '0566A': 'BURGLARY - RESIDENCE/UNK - WINDOW,DOOR,ETC UNLOCKED - RES GRG DR',
    '0566B': 'BURGLARY - RESIDENCE/UNK - WINDOW,DOOR,ETC UNLOCKED - HID IN BLDG',
    '0566C': 'BURGLARY - RESIDENCE/UNK - WINDOW,DOOR,ETC UNLOCKED - OVRHEAD DR',
    '05671': 'BURGLARY - RESIDENCE/UNK - ALL ATTEMPTS (NO FORCE) - FRONT DOOR',
    '05672': 'BURGLARY - RESIDENCE/UNK - ALL ATTEMPTS (NO FORCE) - REAR DOOR',
    '05673': 'BURGLARY - RESIDENCE/UNK - ALL ATTEMPTS (NO FORCE) - SIDE DOOR',
    '05674': 'BURGLARY - RESIDENCE/UNK - ALL ATTEMPTS (NO FORCE) - FRT WIND',
    '05675': 'BURGLARY - RESIDENCE/UNK - ALL ATTEMPTS (NO FORCE) - REAR WIND',
    '05676': 'BURGLARY - RESIDENCE/UNK - ALL ATTEMPTS (NO FORCE) - SIDE WIND',
    '05677': 'BURGLARY - RESIDENCE/UNK - ALL ATTEMPTS (NO FORCE) - ROOF,FLOOR',
    '05678': 'BURGLARY - RESIDENCE/UNK - ALL ATTEMPTS (NO FORCE) - WALL',
    '05679': 'BURGLARY - RESIDENCE/UNK - ALL ATTEMPTS (NO FORCE) - UNKNOWN',
    '0567A': 'BURGLARY - RESIDENCE/UNK - ALL ATTEMPTS (NO FORCE) - RES GRG DR',
    '0567B': 'BURGLARY - RESIDENCE/UNK - ALL ATTEMPTS (NO FORCE) - HID IN BLDG',
    '0567C': 'BURGLARY - RESIDENCE/UNK - ALL ATTEMPTS (NO FORCE) - OVRHEAD DR',
    '05681': 'BURGLARY - RESIDENCE/UNK - ALL OTHER ATTEMPTS - FRONT DOOR',
    '05682': 'BURGLARY - RESIDENCE/UNK - ALL OTHER ATTEMPTS - REAR DOOR',
    '05683': 'BURGLARY - RESIDENCE/UNK - ALL OTHER ATTEMPTS - SIDE DOOR',
    '05684': 'BURGLARY - RESIDENCE/UNK - ALL OTHER ATTEMPTS - FRT WIND',
    '05685': 'BURGLARY - RESIDENCE/UNK - ALL OTHER ATTEMPTS - REAR WIND',
    '05686': 'BURGLARY - RESIDENCE/UNK - ALL OTHER ATTEMPTS - SIDE WIND',
    '05687': 'BURGLARY - RESIDENCE/UNK - ALL OTHER ATTEMPTS - ROOF,FLOOR',
    '05688': 'BURGLARY - RESIDENCE/UNK - ALL OTHER ATTEMPTS - WALL',
    '05689': 'BURGLARY - RESIDENCE/UNK - ALL OTHER ATTEMPTS - UNKNOWN',
    '0568A': 'BURGLARY - RESIDENCE/UNK - ALL OTHER ATTEMPTS - RES GRG DR',
    '0568B': 'BURGLARY - RESIDENCE/UNK - ALL OTHER ATTEMPTS - HID IN BLDG',
    '0568C': 'BURGLARY - RESIDENCE/UNK - ALL OTHER ATTEMPTS - OVRHEAD DR',
    '05691': 'BURGLARY - RESIDENCE/UNK - OTHERS - FRONT DOOR',
    '05692': 'BURGLARY - RESIDENCE/UNK - OTHERS - REAR DOOR',
    '05693': 'BURGLARY - RESIDENCE/UNK - OTHERS - SIDE DOOR',
    '05694': 'BURGLARY - RESIDENCE/UNK - OTHERS - FRT WIND',
    '05695': 'BURGLARY - RESIDENCE/UNK - OTHERS - REAR WIND',
    '05696': 'BURGLARY - RESIDENCE/UNK - OTHERS - SIDE WIND',
    '05697': 'BURGLARY - RESIDENCE/UNK - OTHERS - ROOF,FLOOR',
    '05698': 'BURGLARY - RESIDENCE/UNK - OTHERS - WALL',
    '05699': 'BURGLARY - RESIDENCE/UNK - OTHERS - UNKNOWN',
    '0569A': 'BURGLARY - RESIDENCE/UNK - OTHERS - RES GRG DR',
    '0569B': 'BURGLARY - RESIDENCE/UNK - OTHERS - HID IN BLDG',
    '0569C': 'BURGLARY - RESIDENCE/UNK - OTHERS - OVRHEAD DR',
    '05711': 'BURGLARY - RESID SAFE/NIGHT - KEY/COMBINATiON LOCK - FRONT DOOR',
    '05712': 'BURGLARY - RESID SAFE/NIGHT - KEY/COMBINATiON LOCK - REAR DOOR',
    '05713': 'BURGLARY - RESID SAFE/NIGHT - KEY/COMBINATiON LOCK - SIDE DOOR',
    '05714': 'BURGLARY - RESID SAFE/NIGHT - KEY/COMBINATiON LOCK - FRT WIND',
    '05715': 'BURGLARY - RESID SAFE/NIGHT - KEY/COMBINATiON LOCK - REAR WIND',
    '05716': 'BURGLARY - RESID SAFE/NIGHT - KEY/COMBINATiON LOCK - SIDE WIND',
    '05717': 'BURGLARY - RESID SAFE/NIGHT - KEY/COMBINATiON LOCK - ROOF,FLOOR',
    '05718': 'BURGLARY - RESID SAFE/NIGHT - KEY/COMBINATiON LOCK - WALL',
    '05719': 'BURGLARY - RESID SAFE/NIGHT - KEY/COMBINATiON LOCK - UNKNOWN',
    '0571A': 'BURGLARY - RESID SAFE/NIGHT - KEY/COMBINATiON LOCK - RES GRG DR',
    '0571B': 'BURGLARY - RESID SAFE/NIGHT - KEY/COMBINATiON LOCK - HID IN BLDG',
    '0571C': 'BURGLARY - RESID SAFE/NIGHT - KEY/COMBINATiON LOCK - OVRHEAD DR',
    '05721': 'BURGLARY - RESID SAFE/NIGHT - SMASHED, KICKED,STRUCK - FRONT DOOR',
    '05722': 'BURGLARY - RESID SAFE/NIGHT - SMASHED, KICKED,STRUCK - REAR DOOR',
    '05723': 'BURGLARY - RESID SAFE/NIGHT - SMASHED, KICKED,STRUCK - SIDE DOOR',
    '05724': 'BURGLARY - RESID SAFE/NIGHT - SMASHED, KICKED,STRUCK - FRT WIND',
    '05725': 'BURGLARY - RESID SAFE/NIGHT - SMASHED, KICKED,STRUCK - REAR WIND',
    '05726': 'BURGLARY - RESID SAFE/NIGHT - SMASHED, KICKED,STRUCK - SIDE WIND',
    '05727': 'BURGLARY - RESID SAFE/NIGHT - SMASHED, KICKED,STRUCK - ROOF,FLOOR',
    '05728': 'BURGLARY - RESID SAFE/NIGHT - SMASHED, KICKED,STRUCK - WALL',
    '05729': 'BURGLARY - RESID SAFE/NIGHT - SMASHED, KICKED,STRUCK - UNKNOWN',
    '0572A': 'BURGLARY - RESID SAFE/NIGHT - SMASHED, KICKED,STRUCK - RES GRG DR',
    '0572B': 'BURGLARY - RESID SAFE/NIGHT - SMASHED, KICKED,STRUCK - HID IN BLDG',
    '0572C': 'BURGLARY - RESID SAFE/NIGHT - SMASHED, KICKED,STRUCK - OVRHEAD DR',
    '05731': 'BURGLARY - RESID SAFE/NIGHT - PRY, BREAK SAW,CUT, ETC. - FRONT DOOR',
    '05732': 'BURGLARY - RESID SAFE/NIGHT - PRY, BREAK SAW,CUT, ETC. - REAR DOOR',
    '05733': 'BURGLARY - RESID SAFE/NIGHT - PRY, BREAK SAW,CUT, ETC. - SIDE DOOR',
    '05734': 'BURGLARY - RESID SAFE/NIGHT - PRY, BREAK SAW,CUT, ETC. - FRT WIND',
    '05735': 'BURGLARY - RESID SAFE/NIGHT - PRY, BREAK SAW,CUT, ETC. - REAR WIND',
    '05736': 'BURGLARY - RESID SAFE/NIGHT - PRY, BREAK SAW,CUT, ETC. - SIDE WIND',
    '05737': 'BURGLARY - RESID SAFE/NIGHT - PRY, BREAK SAW,CUT, ETC. - ROOF,FLOOR',
    '05738': 'BURGLARY - RESID SAFE/NIGHT - PRY, BREAK SAW,CUT, ETC. - WALL',
    '05739': 'BURGLARY - RESID SAFE/NIGHT - PRY, BREAK SAW,CUT, ETC. - UNKNOWN',
    '0573A': 'BURGLARY - RESID SAFE/NIGHT - PRY, BREAK SAW,CUT, ETC. - RES GRG DR',
    '0573B': 'BURGLARY - RESID SAFE/NIGHT - PRY, BREAK SAW,CUT, ETC. - HID IN BLDG',
    '0573C': 'BURGLARY - RESID SAFE/NIGHT - PRY, BREAK SAW,CUT, ETC. - OVRHEAD DR',
    '05741': 'BURGLARY - RESID SAFE/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - FRONT DOOR',
    '05742': 'BURGLARY - RESID SAFE/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - REAR DOOR',
    '05743': 'BURGLARY - RESID SAFE/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - SIDE DOOR',
    '05744': 'BURGLARY - RESID SAFE/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - FRT WIND',
    '05745': 'BURGLARY - RESID SAFE/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - REAR WIND',
    '05746': 'BURGLARY - RESID SAFE/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - SIDE WIND',
    '05747': 'BURGLARY - RESID SAFE/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - ROOF,FLOOR',
    '05748': 'BURGLARY - RESID SAFE/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - WALL',
    '05749': 'BURGLARY - RESID SAFE/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - UNKNOWN',
    '0574A': 'BURGLARY - RESID SAFE/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - RES GRG DR',
    '0574B': 'BURGLARY - RESID SAFE/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - HID IN BLDG',
    '0574C': 'BURGLARY - RESID SAFE/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - OVRHEAD DR',
    '05751': 'BURGLARY - RESID SAFE/NIGHT - NO FORCE (HIDE INSIDE) - FRONT DOOR',
    '05752': 'BURGLARY - RESID SAFE/NIGHT - NO FORCE (HIDE INSIDE) - REAR DOOR',
    '05753': 'BURGLARY - RESID SAFE/NIGHT - NO FORCE (HIDE INSIDE) - SIDE DOOR',
    '05754': 'BURGLARY - RESID SAFE/NIGHT - NO FORCE (HIDE INSIDE) - FRT WIND',
    '05755': 'BURGLARY - RESID SAFE/NIGHT - NO FORCE (HIDE INSIDE) - REAR WIND',
    '05756': 'BURGLARY - RESID SAFE/NIGHT - NO FORCE (HIDE INSIDE) - SIDE WIND',
    '05757': 'BURGLARY - RESID SAFE/NIGHT - NO FORCE (HIDE INSIDE) - ROOF,FLOOR',
    '05758': 'BURGLARY - RESID SAFE/NIGHT - NO FORCE (HIDE INSIDE) - WALL',
    '05759': 'BURGLARY - RESID SAFE/NIGHT - NO FORCE (HIDE INSIDE) - UNKNOWN',
    '0575A': 'BURGLARY - RESID SAFE/NIGHT - NO FORCE (HIDE INSIDE) - RES GRG DR',
    '0575B': 'BURGLARY - RESID SAFE/NIGHT - NO FORCE (HIDE INSIDE) - HID IN BLDG',
    '0575C': 'BURGLARY - RESID SAFE/NIGHT - NO FORCE (HIDE INSIDE) - OVRHEAD DR',
    '05761': 'BURGLARY - RESID SAFE/NIGHT - WINDOW,DOOR,ETC UNLOCKED - FRONT DOOR',
    '05762': 'BURGLARY - RESID SAFE/NIGHT - WINDOW,DOOR,ETC UNLOCKED - REAR DOOR',
    '05763': 'BURGLARY - RESID SAFE/NIGHT - WINDOW,DOOR,ETC UNLOCKED - SIDE DOOR',
    '05764': 'BURGLARY - RESID SAFE/NIGHT - WINDOW,DOOR,ETC UNLOCKED - FRT WIND',
    '05765': 'BURGLARY - RESID SAFE/NIGHT - WINDOW,DOOR,ETC UNLOCKED - REAR WIND',
    '05766': 'BURGLARY - RESID SAFE/NIGHT - WINDOW,DOOR,ETC UNLOCKED - SIDE WIND',
    '05767': 'BURGLARY - RESID SAFE/NIGHT - WINDOW,DOOR,ETC UNLOCKED - ROOF,FLOOR',
    '05768': 'BURGLARY - RESID SAFE/NIGHT - WINDOW,DOOR,ETC UNLOCKED - WALL',
    '05769': 'BURGLARY - RESID SAFE/NIGHT - WINDOW,DOOR,ETC UNLOCKED - UNKNOWN',
    '0576A': 'BURGLARY - RESID SAFE/NIGHT - WINDOW,DOOR,ETC UNLOCKED - RES GRG DR',
    '0576B': 'BURGLARY - RESID SAFE/NIGHT - WINDOW,DOOR,ETC UNLOCKED - HID IN BLDG',
    '0576C': 'BURGLARY - RESID SAFE/NIGHT - WINDOW,DOOR,ETC UNLOCKED - OVRHEAD DR',
    '05771': 'BURGLARY - RESID SAFE/NIGHT - ALL ATTEMPTS (NO FORCE) - FRONT DOOR',
    '05772': 'BURGLARY - RESID SAFE/NIGHT - ALL ATTEMPTS (NO FORCE) - REAR DOOR',
    '05773': 'BURGLARY - RESID SAFE/NIGHT - ALL ATTEMPTS (NO FORCE) - SIDE DOOR',
    '05774': 'BURGLARY - RESID SAFE/NIGHT - ALL ATTEMPTS (NO FORCE) - FRT WIND',
    '05775': 'BURGLARY - RESID SAFE/NIGHT - ALL ATTEMPTS (NO FORCE) - REAR WIND',
    '05776': 'BURGLARY - RESID SAFE/NIGHT - ALL ATTEMPTS (NO FORCE) - SIDE WIND',
    '05777': 'BURGLARY - RESID SAFE/NIGHT - ALL ATTEMPTS (NO FORCE) - ROOF,FLOOR',
    '05778': 'BURGLARY - RESID SAFE/NIGHT - ALL ATTEMPTS (NO FORCE) - WALL',
    '05779': 'BURGLARY - RESID SAFE/NIGHT - ALL ATTEMPTS (NO FORCE) - UNKNOWN',
    '0577A': 'BURGLARY - RESID SAFE/NIGHT - ALL ATTEMPTS (NO FORCE) - RES GRG DR',
    '0577B': 'BURGLARY - RESID SAFE/NIGHT - ALL ATTEMPTS (NO FORCE) - HID IN BLDG',
    '0577C': 'BURGLARY - RESID SAFE/NIGHT - ALL ATTEMPTS (NO FORCE) - OVRHEAD DR',
    '05781': 'BURGLARY - RESID SAFE/NIGHT - ALL OTHER ATTEMPTS - FRONT DOOR',
    '05782': 'BURGLARY - RESID SAFE/NIGHT - ALL OTHER ATTEMPTS - REAR DOOR',
    '05783': 'BURGLARY - RESID SAFE/NIGHT - ALL OTHER ATTEMPTS - SIDE DOOR',
    '05784': 'BURGLARY - RESID SAFE/NIGHT - ALL OTHER ATTEMPTS - FRT WIND',
    '05785': 'BURGLARY - RESID SAFE/NIGHT - ALL OTHER ATTEMPTS - REAR WIND',
    '05786': 'BURGLARY - RESID SAFE/NIGHT - ALL OTHER ATTEMPTS - SIDE WIND',
    '05787': 'BURGLARY - RESID SAFE/NIGHT - ALL OTHER ATTEMPTS - ROOF,FLOOR',
    '05788': 'BURGLARY - RESID SAFE/NIGHT - ALL OTHER ATTEMPTS - WALL',
    '05789': 'BURGLARY - RESID SAFE/NIGHT - ALL OTHER ATTEMPTS - UNKNOWN',
    '0578A': 'BURGLARY - RESID SAFE/NIGHT - ALL OTHER ATTEMPTS - RES GRG DR',
    '0578B': 'BURGLARY - RESID SAFE/NIGHT - ALL OTHER ATTEMPTS - HID IN BLDG',
    '0578C': 'BURGLARY - RESID SAFE/NIGHT - ALL OTHER ATTEMPTS - OVRHEAD DR',
    '05791': 'BURGLARY - RESID SAFE/NIGHT - OTHERS - FRONT DOOR',
    '05792': 'BURGLARY - RESID SAFE/NIGHT - OTHERS - REAR DOOR',
    '05793': 'BURGLARY - RESID SAFE/NIGHT - OTHERS - SIDE DOOR',
    '05794': 'BURGLARY - RESID SAFE/NIGHT - OTHERS - FRT WIND',
    '05795': 'BURGLARY - RESID SAFE/NIGHT - OTHERS - REAR WIND',
    '05796': 'BURGLARY - RESID SAFE/NIGHT - OTHERS - SIDE WIND',
    '05797': 'BURGLARY - RESID SAFE/NIGHT - OTHERS - ROOF,FLOOR',
    '05798': 'BURGLARY - RESID SAFE/NIGHT - OTHERS - WALL',
    '05799': 'BURGLARY - RESID SAFE/NIGHT - OTHERS - UNKNOWN',
    '0579A': 'BURGLARY - RESID SAFE/NIGHT - OTHERS - RES GRG DR',
    '0579B': 'BURGLARY - RESID SAFE/NIGHT - OTHERS - HID IN BLDG',
    '0579C': 'BURGLARY - RESID SAFE/NIGHT - OTHERS - OVRHEAD DR',
    '05811': 'BURGLARY - RESID SAFE/DAY - KEY/COMBINATiON LOCK - FRONT DOOR',
    '05812': 'BURGLARY - RESID SAFE/DAY - KEY/COMBINATiON LOCK - REAR DOOR',
    '05813': 'BURGLARY - RESID SAFE/DAY - KEY/COMBINATiON LOCK - SIDE DOOR',
    '05814': 'BURGLARY - RESID SAFE/DAY - KEY/COMBINATiON LOCK - FRT WIND',
    '05815': 'BURGLARY - RESID SAFE/DAY - KEY/COMBINATiON LOCK - REAR WIND',
    '05816': 'BURGLARY - RESID SAFE/DAY - KEY/COMBINATiON LOCK - SIDE WIND',
    '05817': 'BURGLARY - RESID SAFE/DAY - KEY/COMBINATiON LOCK - ROOF,FLOOR',
    '05818': 'BURGLARY - RESID SAFE/DAY - KEY/COMBINATiON LOCK - WALL',
    '05819': 'BURGLARY - RESID SAFE/DAY - KEY/COMBINATiON LOCK - UNKNOWN',
    '0581A': 'BURGLARY - RESID SAFE/DAY - KEY/COMBINATiON LOCK - RES GRG DR',
    '0581B': 'BURGLARY - RESID SAFE/DAY - KEY/COMBINATiON LOCK - HID IN BLDG',
    '0581C': 'BURGLARY - RESID SAFE/DAY - KEY/COMBINATiON LOCK - OVRHEAD DR',
    '05821': 'BURGLARY - RESID SAFE/DAY - SMASHED, KICKED,STRUCK - FRONT DOOR',
    '05822': 'BURGLARY - RESID SAFE/DAY - SMASHED, KICKED,STRUCK - REAR DOOR',
    '05823': 'BURGLARY - RESID SAFE/DAY - SMASHED, KICKED,STRUCK - SIDE DOOR',
    '05824': 'BURGLARY - RESID SAFE/DAY - SMASHED, KICKED,STRUCK - FRT WIND',
    '05825': 'BURGLARY - RESID SAFE/DAY - SMASHED, KICKED,STRUCK - REAR WIND',
    '05826': 'BURGLARY - RESID SAFE/DAY - SMASHED, KICKED,STRUCK - SIDE WIND',
    '05827': 'BURGLARY - RESID SAFE/DAY - SMASHED, KICKED,STRUCK - ROOF,FLOOR',
    '05828': 'BURGLARY - RESID SAFE/DAY - SMASHED, KICKED,STRUCK - WALL',
    '05829': 'BURGLARY - RESID SAFE/DAY - SMASHED, KICKED,STRUCK - UNKNOWN',
    '0582A': 'BURGLARY - RESID SAFE/DAY - SMASHED, KICKED,STRUCK - RES GRG DR',
    '0582B': 'BURGLARY - RESID SAFE/DAY - SMASHED, KICKED,STRUCK - HID IN BLDG',
    '0582C': 'BURGLARY - RESID SAFE/DAY - SMASHED, KICKED,STRUCK - OVRHEAD DR',
    '05831': 'BURGLARY - RESID SAFE/DAY - PRY, BREAK SAW,CUT, ETC. - FRONT DOOR',
    '05832': 'BURGLARY - RESID SAFE/DAY - PRY, BREAK SAW,CUT, ETC. - REAR DOOR',
    '05833': 'BURGLARY - RESID SAFE/DAY - PRY, BREAK SAW,CUT, ETC. - SIDE DOOR',
    '05834': 'BURGLARY - RESID SAFE/DAY - PRY, BREAK SAW,CUT, ETC. - FRT WIND',
    '05835': 'BURGLARY - RESID SAFE/DAY - PRY, BREAK SAW,CUT, ETC. - REAR WIND',
    '05836': 'BURGLARY - RESID SAFE/DAY - PRY, BREAK SAW,CUT, ETC. - SIDE WIND',
    '05837': 'BURGLARY - RESID SAFE/DAY - PRY, BREAK SAW,CUT, ETC. - ROOF,FLOOR',
    '05838': 'BURGLARY - RESID SAFE/DAY - PRY, BREAK SAW,CUT, ETC. - WALL',
    '05839': 'BURGLARY - RESID SAFE/DAY - PRY, BREAK SAW,CUT, ETC. - UNKNOWN',
    '0583A': 'BURGLARY - RESID SAFE/DAY - PRY, BREAK SAW,CUT, ETC. - RES GRG DR',
    '0583B': 'BURGLARY - RESID SAFE/DAY - PRY, BREAK SAW,CUT, ETC. - HID IN BLDG',
    '0583C': 'BURGLARY - RESID SAFE/DAY - PRY, BREAK SAW,CUT, ETC. - OVRHEAD DR',
    '05841': 'BURGLARY - RESID SAFE/DAY - EXPLOSION,ARSON,CUTTING TORCH - FRONT DOOR',
    '05842': 'BURGLARY - RESID SAFE/DAY - EXPLOSION,ARSON,CUTTING TORCH - REAR DOOR',
    '05843': 'BURGLARY - RESID SAFE/DAY - EXPLOSION,ARSON,CUTTING TORCH - SIDE DOOR',
    '05844': 'BURGLARY - RESID SAFE/DAY - EXPLOSION,ARSON,CUTTING TORCH - FRT WIND',
    '05845': 'BURGLARY - RESID SAFE/DAY - EXPLOSION,ARSON,CUTTING TORCH - REAR WIND',
    '05846': 'BURGLARY - RESID SAFE/DAY - EXPLOSION,ARSON,CUTTING TORCH - SIDE WIND',
    '05847': 'BURGLARY - RESID SAFE/DAY - EXPLOSION,ARSON,CUTTING TORCH - ROOF,FLOOR',
    '05848': 'BURGLARY - RESID SAFE/DAY - EXPLOSION,ARSON,CUTTING TORCH - WALL',
    '05849': 'BURGLARY - RESID SAFE/DAY - EXPLOSION,ARSON,CUTTING TORCH - UNKNOWN',
    '0584A': 'BURGLARY - RESID SAFE/DAY - EXPLOSION,ARSON,CUTTING TORCH - RES GRG DR',
    '0584B': 'BURGLARY - RESID SAFE/DAY - EXPLOSION,ARSON,CUTTING TORCH - HID IN BLDG',
    '0584C': 'BURGLARY - RESID SAFE/DAY - EXPLOSION,ARSON,CUTTING TORCH - OVRHEAD DR',
    '05851': 'BURGLARY - RESID SAFE/DAY - NO FORCE (HIDE INSIDE) - FRONT DOOR',
    '05852': 'BURGLARY - RESID SAFE/DAY - NO FORCE (HIDE INSIDE) - REAR DOOR',
    '05853': 'BURGLARY - RESID SAFE/DAY - NO FORCE (HIDE INSIDE) - SIDE DOOR',
    '05854': 'BURGLARY - RESID SAFE/DAY - NO FORCE (HIDE INSIDE) - FRT WIND',
    '05855': 'BURGLARY - RESID SAFE/DAY - NO FORCE (HIDE INSIDE) - REAR WIND',
    '05856': 'BURGLARY - RESID SAFE/DAY - NO FORCE (HIDE INSIDE) - SIDE WIND',
    '05857': 'BURGLARY - RESID SAFE/DAY - NO FORCE (HIDE INSIDE) - ROOF,FLOOR',
    '05858': 'BURGLARY - RESID SAFE/DAY - NO FORCE (HIDE INSIDE) - WALL',
    '05859': 'BURGLARY - RESID SAFE/DAY - NO FORCE (HIDE INSIDE) - UNKNOWN',
    '0585A': 'BURGLARY - RESID SAFE/DAY - NO FORCE (HIDE INSIDE) - RES GRG DR',
    '0585B': 'BURGLARY - RESID SAFE/DAY - NO FORCE (HIDE INSIDE) - HID IN BLDG',
    '0585C': 'BURGLARY - RESID SAFE/DAY - NO FORCE (HIDE INSIDE) - OVRHEAD DR',
    '05861': 'BURGLARY - RESID SAFE/DAY - WINDOW,DOOR,ETC UNLOCKED - FRONT DOOR',
    '05862': 'BURGLARY - RESID SAFE/DAY - WINDOW,DOOR,ETC UNLOCKED - REAR DOOR',
    '05863': 'BURGLARY - RESID SAFE/DAY - WINDOW,DOOR,ETC UNLOCKED - SIDE DOOR',
    '05864': 'BURGLARY - RESID SAFE/DAY - WINDOW,DOOR,ETC UNLOCKED - FRT WIND',
    '05865': 'BURGLARY - RESID SAFE/DAY - WINDOW,DOOR,ETC UNLOCKED - REAR WIND',
    '05866': 'BURGLARY - RESID SAFE/DAY - WINDOW,DOOR,ETC UNLOCKED - SIDE WIND',
    '05867': 'BURGLARY - RESID SAFE/DAY - WINDOW,DOOR,ETC UNLOCKED - ROOF,FLOOR',
    '05868': 'BURGLARY - RESID SAFE/DAY - WINDOW,DOOR,ETC UNLOCKED - WALL',
    '05869': 'BURGLARY - RESID SAFE/DAY - WINDOW,DOOR,ETC UNLOCKED - UNKNOWN',
    '0586A': 'BURGLARY - RESID SAFE/DAY - WINDOW,DOOR,ETC UNLOCKED - RES GRG DR',
    '0586B': 'BURGLARY - RESID SAFE/DAY - WINDOW,DOOR,ETC UNLOCKED - HID IN BLDG',
    '0586C': 'BURGLARY - RESID SAFE/DAY - WINDOW,DOOR,ETC UNLOCKED - OVRHEAD DR',
    '05871': 'BURGLARY - RESID SAFE/DAY - ALL ATTEMPTS (NO FORCE) - FRONT DOOR',
    '05872': 'BURGLARY - RESID SAFE/DAY - ALL ATTEMPTS (NO FORCE) - REAR DOOR',
    '05873': 'BURGLARY - RESID SAFE/DAY - ALL ATTEMPTS (NO FORCE) - SIDE DOOR',
    '05874': 'BURGLARY - RESID SAFE/DAY - ALL ATTEMPTS (NO FORCE) - FRT WIND',
    '05875': 'BURGLARY - RESID SAFE/DAY - ALL ATTEMPTS (NO FORCE) - REAR WIND',
    '05876': 'BURGLARY - RESID SAFE/DAY - ALL ATTEMPTS (NO FORCE) - SIDE WIND',
    '05877': 'BURGLARY - RESID SAFE/DAY - ALL ATTEMPTS (NO FORCE) - ROOF,FLOOR',
    '05878': 'BURGLARY - RESID SAFE/DAY - ALL ATTEMPTS (NO FORCE) - WALL',
    '05879': 'BURGLARY - RESID SAFE/DAY - ALL ATTEMPTS (NO FORCE) - UNKNOWN',
    '0587A': 'BURGLARY - RESID SAFE/DAY - ALL ATTEMPTS (NO FORCE) - RES GRG DR',
    '0587B': 'BURGLARY - RESID SAFE/DAY - ALL ATTEMPTS (NO FORCE) - HID IN BLDG',
    '0587C': 'BURGLARY - RESID SAFE/DAY - ALL ATTEMPTS (NO FORCE) - OVRHEAD DR',
    '05881': 'BURGLARY - RESID SAFE/DAY - ALL OTHER ATTEMPTS - FRONT DOOR',
    '05882': 'BURGLARY - RESID SAFE/DAY - ALL OTHER ATTEMPTS - REAR DOOR',
    '05883': 'BURGLARY - RESID SAFE/DAY - ALL OTHER ATTEMPTS - SIDE DOOR',
    '05884': 'BURGLARY - RESID SAFE/DAY - ALL OTHER ATTEMPTS - FRT WIND',
    '05885': 'BURGLARY - RESID SAFE/DAY - ALL OTHER ATTEMPTS - REAR WIND',
    '05886': 'BURGLARY - RESID SAFE/DAY - ALL OTHER ATTEMPTS - SIDE WIND',
    '05887': 'BURGLARY - RESID SAFE/DAY - ALL OTHER ATTEMPTS - ROOF,FLOOR',
    '05888': 'BURGLARY - RESID SAFE/DAY - ALL OTHER ATTEMPTS - WALL',
    '05889': 'BURGLARY - RESID SAFE/DAY - ALL OTHER ATTEMPTS - UNKNOWN',
    '0588A': 'BURGLARY - RESID SAFE/DAY - ALL OTHER ATTEMPTS - RES GRG DR',
    '0588B': 'BURGLARY - RESID SAFE/DAY - ALL OTHER ATTEMPTS - HID IN BLDG',
    '0588C': 'BURGLARY - RESID SAFE/DAY - ALL OTHER ATTEMPTS - OVRHEAD DR',
    '05891': 'BURGLARY - RESID SAFE/DAY - OTHERS - FRONT DOOR',
    '05892': 'BURGLARY - RESID SAFE/DAY - OTHERS - REAR DOOR',
    '05893': 'BURGLARY - RESID SAFE/DAY - OTHERS - SIDE DOOR',
    '05894': 'BURGLARY - RESID SAFE/DAY - OTHERS - FRT WIND',
    '05895': 'BURGLARY - RESID SAFE/DAY - OTHERS - REAR WIND',
    '05896': 'BURGLARY - RESID SAFE/DAY - OTHERS - SIDE WIND',
    '05897': 'BURGLARY - RESID SAFE/DAY - OTHERS - ROOF,FLOOR',
    '05898': 'BURGLARY - RESID SAFE/DAY - OTHERS - WALL',
    '05899': 'BURGLARY - RESID SAFE/DAY - OTHERS - UNKNOWN',
    '0589A': 'BURGLARY - RESID SAFE/DAY - OTHERS - RES GRG DR',
    '0589B': 'BURGLARY - RESID SAFE/DAY - OTHERS - HID IN BLDG',
    '0589C': 'BURGLARY - RESID SAFE/DAY - OTHERS - OVRHEAD DR',
    '05911': 'BURGLARY - RESID SAFE/UNK - KEY/COMBINATiON LOCK - FRONT DOOR',
    '05912': 'BURGLARY - RESID SAFE/UNK - KEY/COMBINATiON LOCK - REAR DOOR',
    '05913': 'BURGLARY - RESID SAFE/UNK - KEY/COMBINATiON LOCK - SIDE DOOR',
    '05914': 'BURGLARY - RESID SAFE/UNK - KEY/COMBINATiON LOCK - FRT WIND',
    '05915': 'BURGLARY - RESID SAFE/UNK - KEY/COMBINATiON LOCK - REAR WIND',
    '05916': 'BURGLARY - RESID SAFE/UNK - KEY/COMBINATiON LOCK - SIDE WIND',
    '05917': 'BURGLARY - RESID SAFE/UNK - KEY/COMBINATiON LOCK - ROOF,FLOOR',
    '05918': 'BURGLARY - RESID SAFE/UNK - KEY/COMBINATiON LOCK - WALL',
    '05919': 'BURGLARY - RESID SAFE/UNK - KEY/COMBINATiON LOCK - UNKNOWN',
    '0591A': 'BURGLARY - RESID SAFE/UNK - KEY/COMBINATiON LOCK - RES GRG DR',
    '0591B': 'BURGLARY - RESID SAFE/UNK - KEY/COMBINATiON LOCK - HID IN BLDG',
    '0591C': 'BURGLARY - RESID SAFE/UNK - KEY/COMBINATiON LOCK - OVRHEAD DR',
    '05921': 'BURGLARY - RESID SAFE/UNK - SMASHED, KICKED,STRUCK - FRONT DOOR',
    '05922': 'BURGLARY - RESID SAFE/UNK - SMASHED, KICKED,STRUCK - REAR DOOR',
    '05923': 'BURGLARY - RESID SAFE/UNK - SMASHED, KICKED,STRUCK - SIDE DOOR',
    '05924': 'BURGLARY - RESID SAFE/UNK - SMASHED, KICKED,STRUCK - FRT WIND',
    '05925': 'BURGLARY - RESID SAFE/UNK - SMASHED, KICKED,STRUCK - REAR WIND',
    '05926': 'BURGLARY - RESID SAFE/UNK - SMASHED, KICKED,STRUCK - SIDE WIND',
    '05927': 'BURGLARY - RESID SAFE/UNK - SMASHED, KICKED,STRUCK - ROOF,FLOOR',
    '05928': 'BURGLARY - RESID SAFE/UNK - SMASHED, KICKED,STRUCK - WALL',
    '05929': 'BURGLARY - RESID SAFE/UNK - SMASHED, KICKED,STRUCK - UNKNOWN',
    '0592A': 'BURGLARY - RESID SAFE/UNK - SMASHED, KICKED,STRUCK - RES GRG DR',
    '0592B': 'BURGLARY - RESID SAFE/UNK - SMASHED, KICKED,STRUCK - HID IN BLDG',
    '0592C': 'BURGLARY - RESID SAFE/UNK - SMASHED, KICKED,STRUCK - OVRHEAD DR',
    '05931': 'BURGLARY - RESID SAFE/UNK - PRY, BREAK SAW,CUT, ETC. - FRONT DOOR',
    '05932': 'BURGLARY - RESID SAFE/UNK - PRY, BREAK SAW,CUT, ETC. - REAR DOOR',
    '05933': 'BURGLARY - RESID SAFE/UNK - PRY, BREAK SAW,CUT, ETC. - SIDE DOOR',
    '05934': 'BURGLARY - RESID SAFE/UNK - PRY, BREAK SAW,CUT, ETC. - FRT WIND',
    '05935': 'BURGLARY - RESID SAFE/UNK - PRY, BREAK SAW,CUT, ETC. - REAR WIND',
    '05936': 'BURGLARY - RESID SAFE/UNK - PRY, BREAK SAW,CUT, ETC. - SIDE WIND',
    '05937': 'BURGLARY - RESID SAFE/UNK - PRY, BREAK SAW,CUT, ETC. - ROOF,FLOOR',
    '05938': 'BURGLARY - RESID SAFE/UNK - PRY, BREAK SAW,CUT, ETC. - WALL',
    '05939': 'BURGLARY - RESID SAFE/UNK - PRY, BREAK SAW,CUT, ETC. - UNKNOWN',
    '0593A': 'BURGLARY - RESID SAFE/UNK - PRY, BREAK SAW,CUT, ETC. - RES GRG DR',
    '0593B': 'BURGLARY - RESID SAFE/UNK - PRY, BREAK SAW,CUT, ETC. - HID IN BLDG',
    '0593C': 'BURGLARY - RESID SAFE/UNK - PRY, BREAK SAW,CUT, ETC. - OVRHEAD DR',
    '05941': 'BURGLARY - RESID SAFE/UNK - EXPLOSION,ARSON,CUTTING TORCH - FRONT DOOR',
    '05942': 'BURGLARY - RESID SAFE/UNK - EXPLOSION,ARSON,CUTTING TORCH - REAR DOOR',
    '05943': 'BURGLARY - RESID SAFE/UNK - EXPLOSION,ARSON,CUTTING TORCH - SIDE DOOR',
    '05944': 'BURGLARY - RESID SAFE/UNK - EXPLOSION,ARSON,CUTTING TORCH - FRT WIND',
    '05945': 'BURGLARY - RESID SAFE/UNK - EXPLOSION,ARSON,CUTTING TORCH - REAR WIND',
    '05946': 'BURGLARY - RESID SAFE/UNK - EXPLOSION,ARSON,CUTTING TORCH - SIDE WIND',
    '05947': 'BURGLARY - RESID SAFE/UNK - EXPLOSION,ARSON,CUTTING TORCH - ROOF,FLOOR',
    '05948': 'BURGLARY - RESID SAFE/UNK - EXPLOSION,ARSON,CUTTING TORCH - WALL',
    '05949': 'BURGLARY - RESID SAFE/UNK - EXPLOSION,ARSON,CUTTING TORCH - UNKNOWN',
    '0594A': 'BURGLARY - RESID SAFE/UNK - EXPLOSION,ARSON,CUTTING TORCH - RES GRG DR',
    '0594B': 'BURGLARY - RESID SAFE/UNK - EXPLOSION,ARSON,CUTTING TORCH - HID IN BLDG',
    '0594C': 'BURGLARY - RESID SAFE/UNK - EXPLOSION,ARSON,CUTTING TORCH - OVRHEAD DR',
    '05951': 'BURGLARY - RESID SAFE/UNK - NO FORCE (HIDE INSIDE) - FRONT DOOR',
    '05952': 'BURGLARY - RESID SAFE/UNK - NO FORCE (HIDE INSIDE) - REAR DOOR',
    '05953': 'BURGLARY - RESID SAFE/UNK - NO FORCE (HIDE INSIDE) - SIDE DOOR',
    '05954': 'BURGLARY - RESID SAFE/UNK - NO FORCE (HIDE INSIDE) - FRT WIND',
    '05955': 'BURGLARY - RESID SAFE/UNK - NO FORCE (HIDE INSIDE) - REAR WIND',
    '05956': 'BURGLARY - RESID SAFE/UNK - NO FORCE (HIDE INSIDE) - SIDE WIND',
    '05957': 'BURGLARY - RESID SAFE/UNK - NO FORCE (HIDE INSIDE) - ROOF,FLOOR',
    '05958': 'BURGLARY - RESID SAFE/UNK - NO FORCE (HIDE INSIDE) - WALL',
    '05959': 'BURGLARY - RESID SAFE/UNK - NO FORCE (HIDE INSIDE) - UNKNOWN',
    '0595A': 'BURGLARY - RESID SAFE/UNK - NO FORCE (HIDE INSIDE) - RES GRG DR',
    '0595B': 'BURGLARY - RESID SAFE/UNK - NO FORCE (HIDE INSIDE) - HID IN BLDG',
    '0595C': 'BURGLARY - RESID SAFE/UNK - NO FORCE (HIDE INSIDE) - OVRHEAD DR',
    '05961': 'BURGLARY - RESID SAFE/UNK - WINDOW,DOOR,ETC UNLOCKED - FRONT DOOR',
    '05962': 'BURGLARY - RESID SAFE/UNK - WINDOW,DOOR,ETC UNLOCKED - REAR DOOR',
    '05963': 'BURGLARY - RESID SAFE/UNK - WINDOW,DOOR,ETC UNLOCKED - SIDE DOOR',
    '05964': 'BURGLARY - RESID SAFE/UNK - WINDOW,DOOR,ETC UNLOCKED - FRT WIND',
    '05965': 'BURGLARY - RESID SAFE/UNK - WINDOW,DOOR,ETC UNLOCKED - REAR WIND',
    '05966': 'BURGLARY - RESID SAFE/UNK - WINDOW,DOOR,ETC UNLOCKED - SIDE WIND',
    '05967': 'BURGLARY - RESID SAFE/UNK - WINDOW,DOOR,ETC UNLOCKED - ROOF,FLOOR',
    '05968': 'BURGLARY - RESID SAFE/UNK - WINDOW,DOOR,ETC UNLOCKED - WALL',
    '05969': 'BURGLARY - RESID SAFE/UNK - WINDOW,DOOR,ETC UNLOCKED - UNKNOWN',
    '0596A': 'BURGLARY - RESID SAFE/UNK - WINDOW,DOOR,ETC UNLOCKED - RES GRG DR',
    '0596B': 'BURGLARY - RESID SAFE/UNK - WINDOW,DOOR,ETC UNLOCKED - HID IN BLDG',
    '0596C': 'BURGLARY - RESID SAFE/UNK - WINDOW,DOOR,ETC UNLOCKED - OVRHEAD DR',
    '05971': 'BURGLARY - RESID SAFE/UNK - ALL ATTEMPTS (NO FORCE) - FRONT DOOR',
    '05972': 'BURGLARY - RESID SAFE/UNK - ALL ATTEMPTS (NO FORCE) - REAR DOOR',
    '05973': 'BURGLARY - RESID SAFE/UNK - ALL ATTEMPTS (NO FORCE) - SIDE DOOR',
    '05974': 'BURGLARY - RESID SAFE/UNK - ALL ATTEMPTS (NO FORCE) - FRT WIND',
    '05975': 'BURGLARY - RESID SAFE/UNK - ALL ATTEMPTS (NO FORCE) - REAR WIND',
    '05976': 'BURGLARY - RESID SAFE/UNK - ALL ATTEMPTS (NO FORCE) - SIDE WIND',
    '05977': 'BURGLARY - RESID SAFE/UNK - ALL ATTEMPTS (NO FORCE) - ROOF,FLOOR',
    '05978': 'BURGLARY - RESID SAFE/UNK - ALL ATTEMPTS (NO FORCE) - WALL',
    '05979': 'BURGLARY - RESID SAFE/UNK - ALL ATTEMPTS (NO FORCE) - UNKNOWN',
    '0597A': 'BURGLARY - RESID SAFE/UNK - ALL ATTEMPTS (NO FORCE) - RES GRG DR',
    '0597B': 'BURGLARY - RESID SAFE/UNK - ALL ATTEMPTS (NO FORCE) - HID IN BLDG',
    '0597C': 'BURGLARY - RESID SAFE/UNK - ALL ATTEMPTS (NO FORCE) - OVRHEAD DR',
    '05981': 'BURGLARY - RESID SAFE/UNK - ALL OTHER ATTEMPTS - FRONT DOOR',
    '05982': 'BURGLARY - RESID SAFE/UNK - ALL OTHER ATTEMPTS - REAR DOOR',
    '05983': 'BURGLARY - RESID SAFE/UNK - ALL OTHER ATTEMPTS - SIDE DOOR',
    '05984': 'BURGLARY - RESID SAFE/UNK - ALL OTHER ATTEMPTS - FRT WIND',
    '05985': 'BURGLARY - RESID SAFE/UNK - ALL OTHER ATTEMPTS - REAR WIND',
    '05986': 'BURGLARY - RESID SAFE/UNK - ALL OTHER ATTEMPTS - SIDE WIND',
    '05987': 'BURGLARY - RESID SAFE/UNK - ALL OTHER ATTEMPTS - ROOF,FLOOR',
    '05988': 'BURGLARY - RESID SAFE/UNK - ALL OTHER ATTEMPTS - WALL',
    '05989': 'BURGLARY - RESID SAFE/UNK - ALL OTHER ATTEMPTS - UNKNOWN',
    '0598A': 'BURGLARY - RESID SAFE/UNK - ALL OTHER ATTEMPTS - RES GRG DR',
    '0598B': 'BURGLARY - RESID SAFE/UNK - ALL OTHER ATTEMPTS - HID IN BLDG',
    '0598C': 'BURGLARY - RESID SAFE/UNK - ALL OTHER ATTEMPTS - OVRHEAD DR',
    '05991': 'BURGLARY - RESID SAFE/UNK - OTHERS - FRONT DOOR',
    '05992': 'BURGLARY - RESID SAFE/UNK - OTHERS - REAR DOOR',
    '05993': 'BURGLARY - RESID SAFE/UNK - OTHERS - SIDE DOOR',
    '05994': 'BURGLARY - RESID SAFE/UNK - OTHERS - FRT WIND',
    '05995': 'BURGLARY - RESID SAFE/UNK - OTHERS - REAR WIND',
    '05996': 'BURGLARY - RESID SAFE/UNK - OTHERS - SIDE WIND',
    '05997': 'BURGLARY - RESID SAFE/UNK - OTHERS - ROOF,FLOOR',
    '05998': 'BURGLARY - RESID SAFE/UNK - OTHERS - WALL',
    '05999': 'BURGLARY - RESID SAFE/UNK - OTHERS - UNKNOWN',
    '0599A': 'BURGLARY - RESID SAFE/UNK - OTHERS - RES GRG DR',
    '0599B': 'BURGLARY - RESID SAFE/UNK - OTHERS - HID IN BLDG',
    '0599C': 'BURGLARY - RESID SAFE/UNK - OTHERS - OVRHEAD DR',
    '05A11': 'BURGLARY - BUS SAFE/NIGHT - KEY/COMBINATiON LOCK - FRONT DOOR',
    '05A12': 'BURGLARY - BUS SAFE/NIGHT - KEY/COMBINATiON LOCK - REAR DOOR',
    '05A13': 'BURGLARY - BUS SAFE/NIGHT - KEY/COMBINATiON LOCK - SIDE DOOR',
    '05A14': 'BURGLARY - BUS SAFE/NIGHT - KEY/COMBINATiON LOCK - FRT WIND',
    '05A15': 'BURGLARY - BUS SAFE/NIGHT - KEY/COMBINATiON LOCK - REAR WIND',
    '05A16': 'BURGLARY - BUS SAFE/NIGHT - KEY/COMBINATiON LOCK - SIDE WIND',
    '05A17': 'BURGLARY - BUS SAFE/NIGHT - KEY/COMBINATiON LOCK - ROOF,FLOOR',
    '05A18': 'BURGLARY - BUS SAFE/NIGHT - KEY/COMBINATiON LOCK - WALL',
    '05A19': 'BURGLARY - BUS SAFE/NIGHT - KEY/COMBINATiON LOCK - UNKNOWN',
    '05A1A': 'BURGLARY - BUS SAFE/NIGHT - KEY/COMBINATiON LOCK - RES GRG DR',
    '05A1B': 'BURGLARY - BUS SAFE/NIGHT - KEY/COMBINATiON LOCK - HID IN BLDG',
    '05A1C': 'BURGLARY - BUS SAFE/NIGHT - KEY/COMBINATiON LOCK - OVRHEAD DR',
    '05A21': 'BURGLARY - BUS SAFE/NIGHT - SMASHED, KICKED,STRUCK - FRONT DOOR',
    '05A22': 'BURGLARY - BUS SAFE/NIGHT - SMASHED, KICKED,STRUCK - REAR DOOR',
    '05A23': 'BURGLARY - BUS SAFE/NIGHT - SMASHED, KICKED,STRUCK - SIDE DOOR',
    '05A24': 'BURGLARY - BUS SAFE/NIGHT - SMASHED, KICKED,STRUCK - FRT WIND',
    '05A25': 'BURGLARY - BUS SAFE/NIGHT - SMASHED, KICKED,STRUCK - REAR WIND',
    '05A26': 'BURGLARY - BUS SAFE/NIGHT - SMASHED, KICKED,STRUCK - SIDE WIND',
    '05A27': 'BURGLARY - BUS SAFE/NIGHT - SMASHED, KICKED,STRUCK - ROOF,FLOOR',
    '05A28': 'BURGLARY - BUS SAFE/NIGHT - SMASHED, KICKED,STRUCK - WALL',
    '05A29': 'BURGLARY - BUS SAFE/NIGHT - SMASHED, KICKED,STRUCK - UNKNOWN',
    '05A2A': 'BURGLARY - BUS SAFE/NIGHT - SMASHED, KICKED,STRUCK - RES GRG DR',
    '05A2B': 'BURGLARY - BUS SAFE/NIGHT - SMASHED, KICKED,STRUCK - HID IN BLDG',
    '05A2C': 'BURGLARY - BUS SAFE/NIGHT - SMASHED, KICKED,STRUCK - OVRHEAD DR',
    '05A31': 'BURGLARY - BUS SAFE/NIGHT - PRY, BREAK SAW,CUT, ETC. - FRONT DOOR',
    '05A32': 'BURGLARY - BUS SAFE/NIGHT - PRY, BREAK SAW,CUT, ETC. - REAR DOOR',
    '05A33': 'BURGLARY - BUS SAFE/NIGHT - PRY, BREAK SAW,CUT, ETC. - SIDE DOOR',
    '05A34': 'BURGLARY - BUS SAFE/NIGHT - PRY, BREAK SAW,CUT, ETC. - FRT WIND',
    '05A35': 'BURGLARY - BUS SAFE/NIGHT - PRY, BREAK SAW,CUT, ETC. - REAR WIND',
    '05A36': 'BURGLARY - BUS SAFE/NIGHT - PRY, BREAK SAW,CUT, ETC. - SIDE WIND',
    '05A37': 'BURGLARY - BUS SAFE/NIGHT - PRY, BREAK SAW,CUT, ETC. - ROOF,FLOOR',
    '05A38': 'BURGLARY - BUS SAFE/NIGHT - PRY, BREAK SAW,CUT, ETC. - WALL',
    '05A39': 'BURGLARY - BUS SAFE/NIGHT - PRY, BREAK SAW,CUT, ETC. - UNKNOWN',
    '05A3A': 'BURGLARY - BUS SAFE/NIGHT - PRY, BREAK SAW,CUT, ETC. - RES GRG DR',
    '05A3B': 'BURGLARY - BUS SAFE/NIGHT - PRY, BREAK SAW,CUT, ETC. - HID IN BLDG',
    '05A3C': 'BURGLARY - BUS SAFE/NIGHT - PRY, BREAK SAW,CUT, ETC. - OVRHEAD DR',
    '05A41': 'BURGLARY - BUS SAFE/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - FRONT DOOR',
    '05A42': 'BURGLARY - BUS SAFE/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - REAR DOOR',
    '05A43': 'BURGLARY - BUS SAFE/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - SIDE DOOR',
    '05A44': 'BURGLARY - BUS SAFE/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - FRT WIND',
    '05A45': 'BURGLARY - BUS SAFE/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - REAR WIND',
    '05A46': 'BURGLARY - BUS SAFE/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - SIDE WIND',
    '05A47': 'BURGLARY - BUS SAFE/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - ROOF,FLOOR',
    '05A48': 'BURGLARY - BUS SAFE/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - WALL',
    '05A49': 'BURGLARY - BUS SAFE/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - UNKNOWN',
    '05A4A': 'BURGLARY - BUS SAFE/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - RES GRG DR',
    '05A4B': 'BURGLARY - BUS SAFE/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - HID IN BLDG',
    '05A4C': 'BURGLARY - BUS SAFE/NIGHT - EXPLOSION,ARSON,CUTTING TORCH - OVRHEAD DR',
    '05A51': 'BURGLARY - BUS SAFE/NIGHT - NO FORCE (HIDE INSIDE) - FRONT DOOR',
    '05A52': 'BURGLARY - BUS SAFE/NIGHT - NO FORCE (HIDE INSIDE) - REAR DOOR',
    '05A53': 'BURGLARY - BUS SAFE/NIGHT - NO FORCE (HIDE INSIDE) - SIDE DOOR',
    '05A54': 'BURGLARY - BUS SAFE/NIGHT - NO FORCE (HIDE INSIDE) - FRT WIND',
    '05A55': 'BURGLARY - BUS SAFE/NIGHT - NO FORCE (HIDE INSIDE) - REAR WIND',
    '05A56': 'BURGLARY - BUS SAFE/NIGHT - NO FORCE (HIDE INSIDE) - SIDE WIND',
    '05A57': 'BURGLARY - BUS SAFE/NIGHT - NO FORCE (HIDE INSIDE) - ROOF,FLOOR',
    '05A58': 'BURGLARY - BUS SAFE/NIGHT - NO FORCE (HIDE INSIDE) - WALL',
    '05A59': 'BURGLARY - BUS SAFE/NIGHT - NO FORCE (HIDE INSIDE) - UNKNOWN',
    '05A5A': 'BURGLARY - BUS SAFE/NIGHT - NO FORCE (HIDE INSIDE) - RES GRG DR',
    '05A5B': 'BURGLARY - BUS SAFE/NIGHT - NO FORCE (HIDE INSIDE) - HID IN BLDG',
    '05A5C': 'BURGLARY - BUS SAFE/NIGHT - NO FORCE (HIDE INSIDE) - OVRHEAD DR',
    '05A61': 'BURGLARY - BUS SAFE/NIGHT - WINDOW,DOOR,ETC UNLOCKED - FRONT DOOR',
    '05A62': 'BURGLARY - BUS SAFE/NIGHT - WINDOW,DOOR,ETC UNLOCKED - REAR DOOR',
    '05A63': 'BURGLARY - BUS SAFE/NIGHT - WINDOW,DOOR,ETC UNLOCKED - SIDE DOOR',
    '05A64': 'BURGLARY - BUS SAFE/NIGHT - WINDOW,DOOR,ETC UNLOCKED - FRT WIND',
    '05A65': 'BURGLARY - BUS SAFE/NIGHT - WINDOW,DOOR,ETC UNLOCKED - REAR WIND',
    '05A66': 'BURGLARY - BUS SAFE/NIGHT - WINDOW,DOOR,ETC UNLOCKED - SIDE WIND',
    '05A67': 'BURGLARY - BUS SAFE/NIGHT - WINDOW,DOOR,ETC UNLOCKED - ROOF,FLOOR',
    '05A68': 'BURGLARY - BUS SAFE/NIGHT - WINDOW,DOOR,ETC UNLOCKED - WALL',
    '05A69': 'BURGLARY - BUS SAFE/NIGHT - WINDOW,DOOR,ETC UNLOCKED - UNKNOWN',
    '05A6A': 'BURGLARY - BUS SAFE/NIGHT - WINDOW,DOOR,ETC UNLOCKED - RES GRG DR',
    '05A6B': 'BURGLARY - BUS SAFE/NIGHT - WINDOW,DOOR,ETC UNLOCKED - HID IN BLDG',
    '05A6C': 'BURGLARY - BUS SAFE/NIGHT - WINDOW,DOOR,ETC UNLOCKED - OVRHEAD DR',
    '05A71': 'BURGLARY - BUS SAFE/NIGHT - ALL ATTEMPTS (NO FORCE) - FRONT DOOR',
    '05A72': 'BURGLARY - BUS SAFE/NIGHT - ALL ATTEMPTS (NO FORCE) - REAR DOOR',
    '05A73': 'BURGLARY - BUS SAFE/NIGHT - ALL ATTEMPTS (NO FORCE) - SIDE DOOR',
    '05A74': 'BURGLARY - BUS SAFE/NIGHT - ALL ATTEMPTS (NO FORCE) - FRT WIND',
    '05A75': 'BURGLARY - BUS SAFE/NIGHT - ALL ATTEMPTS (NO FORCE) - REAR WIND',
    '05A76': 'BURGLARY - BUS SAFE/NIGHT - ALL ATTEMPTS (NO FORCE) - SIDE WIND',
    '05A77': 'BURGLARY - BUS SAFE/NIGHT - ALL ATTEMPTS (NO FORCE) - ROOF,FLOOR',
    '05A78': 'BURGLARY - BUS SAFE/NIGHT - ALL ATTEMPTS (NO FORCE) - WALL',
    '05A79': 'BURGLARY - BUS SAFE/NIGHT - ALL ATTEMPTS (NO FORCE) - UNKNOWN',
    '05A7A': 'BURGLARY - BUS SAFE/NIGHT - ALL ATTEMPTS (NO FORCE) - RES GRG DR',
    '05A7B': 'BURGLARY - BUS SAFE/NIGHT - ALL ATTEMPTS (NO FORCE) - HID IN BLDG',
    '05A7C': 'BURGLARY - BUS SAFE/NIGHT - ALL ATTEMPTS (NO FORCE) - OVRHEAD DR',
    '05A81': 'BURGLARY - BUS SAFE/NIGHT - ALL OTHER ATTEMPTS - FRONT DOOR',
    '05A82': 'BURGLARY - BUS SAFE/NIGHT - ALL OTHER ATTEMPTS - REAR DOOR',
    '05A83': 'BURGLARY - BUS SAFE/NIGHT - ALL OTHER ATTEMPTS - SIDE DOOR',
    '05A84': 'BURGLARY - BUS SAFE/NIGHT - ALL OTHER ATTEMPTS - FRT WIND',
    '05A85': 'BURGLARY - BUS SAFE/NIGHT - ALL OTHER ATTEMPTS - REAR WIND',
    '05A86': 'BURGLARY - BUS SAFE/NIGHT - ALL OTHER ATTEMPTS - SIDE WIND',
    '05A87': 'BURGLARY - BUS SAFE/NIGHT - ALL OTHER ATTEMPTS - ROOF,FLOOR',
    '05A88': 'BURGLARY - BUS SAFE/NIGHT - ALL OTHER ATTEMPTS - WALL',
    '05A89': 'BURGLARY - BUS SAFE/NIGHT - ALL OTHER ATTEMPTS - UNKNOWN',
    '05A8A': 'BURGLARY - BUS SAFE/NIGHT - ALL OTHER ATTEMPTS - RES GRG DR',
    '05A8B': 'BURGLARY - BUS SAFE/NIGHT - ALL OTHER ATTEMPTS - HID IN BLDG',
    '05A8C': 'BURGLARY - BUS SAFE/NIGHT - ALL OTHER ATTEMPTS - OVRHEAD DR',
    '05A91': 'BURGLARY - BUS SAFE/NIGHT - OTHERS - FRONT DOOR',
    '05A92': 'BURGLARY - BUS SAFE/NIGHT - OTHERS - REAR DOOR',
    '05A93': 'BURGLARY - BUS SAFE/NIGHT - OTHERS - SIDE DOOR',
    '05A94': 'BURGLARY - BUS SAFE/NIGHT - OTHERS - FRT WIND',
    '05A95': 'BURGLARY - BUS SAFE/NIGHT - OTHERS - REAR WIND',
    '05A96': 'BURGLARY - BUS SAFE/NIGHT - OTHERS - SIDE WIND',
    '05A97': 'BURGLARY - BUS SAFE/NIGHT - OTHERS - ROOF,FLOOR',
    '05A98': 'BURGLARY - BUS SAFE/NIGHT - OTHERS - WALL',
    '05A99': 'BURGLARY - BUS SAFE/NIGHT - OTHERS - UNKNOWN',
    '05A9A': 'BURGLARY - BUS SAFE/NIGHT - OTHERS - RES GRG DR',
    '05A9B': 'BURGLARY - BUS SAFE/NIGHT - OTHERS - HID IN BLDG',
    '05A9C': 'BURGLARY - BUS SAFE/NIGHT - OTHERS - OVRHEAD DR',
    '05B11': 'BURGLARY - BUS SAFE/DAY - KEY/COMBINATiON LOCK - FRONT DOOR',
    '05B12': 'BURGLARY - BUS SAFE/DAY - KEY/COMBINATiON LOCK - REAR DOOR',
    '05B13': 'BURGLARY - BUS SAFE/DAY - KEY/COMBINATiON LOCK - SIDE DOOR',
    '05B14': 'BURGLARY - BUS SAFE/DAY - KEY/COMBINATiON LOCK - FRT WIND',
    '05B15': 'BURGLARY - BUS SAFE/DAY - KEY/COMBINATiON LOCK - REAR WIND',
    '05B16': 'BURGLARY - BUS SAFE/DAY - KEY/COMBINATiON LOCK - SIDE WIND',
    '05B17': 'BURGLARY - BUS SAFE/DAY - KEY/COMBINATiON LOCK - ROOF,FLOOR',
    '05B18': 'BURGLARY - BUS SAFE/DAY - KEY/COMBINATiON LOCK - WALL',
    '05B19': 'BURGLARY - BUS SAFE/DAY - KEY/COMBINATiON LOCK - UNKNOWN',
    '05B1A': 'BURGLARY - BUS SAFE/DAY - KEY/COMBINATiON LOCK - RES GRG DR',
    '05B1B': 'BURGLARY - BUS SAFE/DAY - KEY/COMBINATiON LOCK - HID IN BLDG',
    '05B1C': 'BURGLARY - BUS SAFE/DAY - KEY/COMBINATiON LOCK - OVRHEAD DR',
    '05B21': 'BURGLARY - BUS SAFE/DAY - SMASHED, KICKED,STRUCK - FRONT DOOR',
    '05B22': 'BURGLARY - BUS SAFE/DAY - SMASHED, KICKED,STRUCK - REAR DOOR',
    '05B23': 'BURGLARY - BUS SAFE/DAY - SMASHED, KICKED,STRUCK - SIDE DOOR',
    '05B24': 'BURGLARY - BUS SAFE/DAY - SMASHED, KICKED,STRUCK - FRT WIND',
    '05B25': 'BURGLARY - BUS SAFE/DAY - SMASHED, KICKED,STRUCK - REAR WIND',
    '05B26': 'BURGLARY - BUS SAFE/DAY - SMASHED, KICKED,STRUCK - SIDE WIND',
    '05B27': 'BURGLARY - BUS SAFE/DAY - SMASHED, KICKED,STRUCK - ROOF,FLOOR',
    '05B28': 'BURGLARY - BUS SAFE/DAY - SMASHED, KICKED,STRUCK - WALL',
    '05B29': 'BURGLARY - BUS SAFE/DAY - SMASHED, KICKED,STRUCK - UNKNOWN',
    '05B2A': 'BURGLARY - BUS SAFE/DAY - SMASHED, KICKED,STRUCK - RES GRG DR',
    '05B2B': 'BURGLARY - BUS SAFE/DAY - SMASHED, KICKED,STRUCK - HID IN BLDG',
    '05B2C': 'BURGLARY - BUS SAFE/DAY - SMASHED, KICKED,STRUCK - OVRHEAD DR',
    '05B31': 'BURGLARY - BUS SAFE/DAY - PRY, BREAK SAW,CUT, ETC. - FRONT DOOR',
    '05B32': 'BURGLARY - BUS SAFE/DAY - PRY, BREAK SAW,CUT, ETC. - REAR DOOR',
    '05B33': 'BURGLARY - BUS SAFE/DAY - PRY, BREAK SAW,CUT, ETC. - SIDE DOOR',
    '05B34': 'BURGLARY - BUS SAFE/DAY - PRY, BREAK SAW,CUT, ETC. - FRT WIND',
    '05B35': 'BURGLARY - BUS SAFE/DAY - PRY, BREAK SAW,CUT, ETC. - REAR WIND',
    '05B36': 'BURGLARY - BUS SAFE/DAY - PRY, BREAK SAW,CUT, ETC. - SIDE WIND',
    '05B37': 'BURGLARY - BUS SAFE/DAY - PRY, BREAK SAW,CUT, ETC. - ROOF,FLOOR',
    '05B38': 'BURGLARY - BUS SAFE/DAY - PRY, BREAK SAW,CUT, ETC. - WALL',
    '05B39': 'BURGLARY - BUS SAFE/DAY - PRY, BREAK SAW,CUT, ETC. - UNKNOWN',
    '05B3A': 'BURGLARY - BUS SAFE/DAY - PRY, BREAK SAW,CUT, ETC. - RES GRG DR',
    '05B3B': 'BURGLARY - BUS SAFE/DAY - PRY, BREAK SAW,CUT, ETC. - HID IN BLDG',
    '05B3C': 'BURGLARY - BUS SAFE/DAY - PRY, BREAK SAW,CUT, ETC. - OVRHEAD DR',
    '05B41': 'BURGLARY - BUS SAFE/DAY - EXPLOSION,ARSON,CUTTING TORCH - FRONT DOOR',
    '05B42': 'BURGLARY - BUS SAFE/DAY - EXPLOSION,ARSON,CUTTING TORCH - REAR DOOR',
    '05B43': 'BURGLARY - BUS SAFE/DAY - EXPLOSION,ARSON,CUTTING TORCH - SIDE DOOR',
    '05B44': 'BURGLARY - BUS SAFE/DAY - EXPLOSION,ARSON,CUTTING TORCH - FRT WIND',
    '05B45': 'BURGLARY - BUS SAFE/DAY - EXPLOSION,ARSON,CUTTING TORCH - REAR WIND',
    '05B46': 'BURGLARY - BUS SAFE/DAY - EXPLOSION,ARSON,CUTTING TORCH - SIDE WIND',
    '05B47': 'BURGLARY - BUS SAFE/DAY - EXPLOSION,ARSON,CUTTING TORCH - ROOF,FLOOR',
    '05B48': 'BURGLARY - BUS SAFE/DAY - EXPLOSION,ARSON,CUTTING TORCH - WALL',
    '05B49': 'BURGLARY - BUS SAFE/DAY - EXPLOSION,ARSON,CUTTING TORCH - UNKNOWN',
    '05B4A': 'BURGLARY - BUS SAFE/DAY - EXPLOSION,ARSON,CUTTING TORCH - RES GRG DR',
    '05B4B': 'BURGLARY - BUS SAFE/DAY - EXPLOSION,ARSON,CUTTING TORCH - HID IN BLDG',
    '05B4C': 'BURGLARY - BUS SAFE/DAY - EXPLOSION,ARSON,CUTTING TORCH - OVRHEAD DR',
    '05B51': 'BURGLARY - BUS SAFE/DAY - NO FORCE (HIDE INSIDE) - FRONT DOOR',
    '05B52': 'BURGLARY - BUS SAFE/DAY - NO FORCE (HIDE INSIDE) - REAR DOOR',
    '05B53': 'BURGLARY - BUS SAFE/DAY - NO FORCE (HIDE INSIDE) - SIDE DOOR',
    '05B54': 'BURGLARY - BUS SAFE/DAY - NO FORCE (HIDE INSIDE) - FRT WIND',
    '05B55': 'BURGLARY - BUS SAFE/DAY - NO FORCE (HIDE INSIDE) - REAR WIND',
    '05B56': 'BURGLARY - BUS SAFE/DAY - NO FORCE (HIDE INSIDE) - SIDE WIND',
    '05B57': 'BURGLARY - BUS SAFE/DAY - NO FORCE (HIDE INSIDE) - ROOF,FLOOR',
    '05B58': 'BURGLARY - BUS SAFE/DAY - NO FORCE (HIDE INSIDE) - WALL',
    '05B59': 'BURGLARY - BUS SAFE/DAY - NO FORCE (HIDE INSIDE) - UNKNOWN',
    '05B5A': 'BURGLARY - BUS SAFE/DAY - NO FORCE (HIDE INSIDE) - RES GRG DR',
    '05B5B': 'BURGLARY - BUS SAFE/DAY - NO FORCE (HIDE INSIDE) - HID IN BLDG',
    '05B5C': 'BURGLARY - BUS SAFE/DAY - NO FORCE (HIDE INSIDE) - OVRHEAD DR',
    '05B61': 'BURGLARY - BUS SAFE/DAY - WINDOW,DOOR,ETC UNLOCKED - FRONT DOOR',
    '05B62': 'BURGLARY - BUS SAFE/DAY - WINDOW,DOOR,ETC UNLOCKED - REAR DOOR',
    '05B63': 'BURGLARY - BUS SAFE/DAY - WINDOW,DOOR,ETC UNLOCKED - SIDE DOOR',
    '05B64': 'BURGLARY - BUS SAFE/DAY - WINDOW,DOOR,ETC UNLOCKED - FRT WIND',
    '05B65': 'BURGLARY - BUS SAFE/DAY - WINDOW,DOOR,ETC UNLOCKED - REAR WIND',
    '05B66': 'BURGLARY - BUS SAFE/DAY - WINDOW,DOOR,ETC UNLOCKED - SIDE WIND',
    '05B67': 'BURGLARY - BUS SAFE/DAY - WINDOW,DOOR,ETC UNLOCKED - ROOF,FLOOR',
    '05B68': 'BURGLARY - BUS SAFE/DAY - WINDOW,DOOR,ETC UNLOCKED - WALL',
    '05B69': 'BURGLARY - BUS SAFE/DAY - WINDOW,DOOR,ETC UNLOCKED - UNKNOWN',
    '05B6A': 'BURGLARY - BUS SAFE/DAY - WINDOW,DOOR,ETC UNLOCKED - RES GRG DR',
    '05B6B': 'BURGLARY - BUS SAFE/DAY - WINDOW,DOOR,ETC UNLOCKED - HID IN BLDG',
    '05B6C': 'BURGLARY - BUS SAFE/DAY - WINDOW,DOOR,ETC UNLOCKED - OVRHEAD DR',
    '05B71': 'BURGLARY - BUS SAFE/DAY - ALL ATTEMPTS (NO FORCE) - FRONT DOOR',
    '05B72': 'BURGLARY - BUS SAFE/DAY - ALL ATTEMPTS (NO FORCE) - REAR DOOR',
    '05B73': 'BURGLARY - BUS SAFE/DAY - ALL ATTEMPTS (NO FORCE) - SIDE DOOR',
    '05B74': 'BURGLARY - BUS SAFE/DAY - ALL ATTEMPTS (NO FORCE) - FRT WIND',
    '05B75': 'BURGLARY - BUS SAFE/DAY - ALL ATTEMPTS (NO FORCE) - REAR WIND',
    '05B76': 'BURGLARY - BUS SAFE/DAY - ALL ATTEMPTS (NO FORCE) - SIDE WIND',
    '05B77': 'BURGLARY - BUS SAFE/DAY - ALL ATTEMPTS (NO FORCE) - ROOF,FLOOR',
    '05B78': 'BURGLARY - BUS SAFE/DAY - ALL ATTEMPTS (NO FORCE) - WALL',
    '05B79': 'BURGLARY - BUS SAFE/DAY - ALL ATTEMPTS (NO FORCE) - UNKNOWN',
    '05B7A': 'BURGLARY - BUS SAFE/DAY - ALL ATTEMPTS (NO FORCE) - RES GRG DR',
    '05B7B': 'BURGLARY - BUS SAFE/DAY - ALL ATTEMPTS (NO FORCE) - HID IN BLDG',
    '05B7C': 'BURGLARY - BUS SAFE/DAY - ALL ATTEMPTS (NO FORCE) - OVRHEAD DR',
    '05B81': 'BURGLARY - BUS SAFE/DAY - ALL OTHER ATTEMPTS - FRONT DOOR',
    '05B82': 'BURGLARY - BUS SAFE/DAY - ALL OTHER ATTEMPTS - REAR DOOR',
    '05B83': 'BURGLARY - BUS SAFE/DAY - ALL OTHER ATTEMPTS - SIDE DOOR',
    '05B84': 'BURGLARY - BUS SAFE/DAY - ALL OTHER ATTEMPTS - FRT WIND',
    '05B85': 'BURGLARY - BUS SAFE/DAY - ALL OTHER ATTEMPTS - REAR WIND',
    '05B86': 'BURGLARY - BUS SAFE/DAY - ALL OTHER ATTEMPTS - SIDE WIND',
    '05B87': 'BURGLARY - BUS SAFE/DAY - ALL OTHER ATTEMPTS - ROOF,FLOOR',
    '05B88': 'BURGLARY - BUS SAFE/DAY - ALL OTHER ATTEMPTS - WALL',
    '05B89': 'BURGLARY - BUS SAFE/DAY - ALL OTHER ATTEMPTS - UNKNOWN',
    '05B8A': 'BURGLARY - BUS SAFE/DAY - ALL OTHER ATTEMPTS - RES GRG DR',
    '05B8B': 'BURGLARY - BUS SAFE/DAY - ALL OTHER ATTEMPTS - HID IN BLDG',
    '05B8C': 'BURGLARY - BUS SAFE/DAY - ALL OTHER ATTEMPTS - OVRHEAD DR',
    '05B91': 'BURGLARY - BUS SAFE/DAY - OTHERS - FRONT DOOR',
    '05B92': 'BURGLARY - BUS SAFE/DAY - OTHERS - REAR DOOR',
    '05B93': 'BURGLARY - BUS SAFE/DAY - OTHERS - SIDE DOOR',
    '05B94': 'BURGLARY - BUS SAFE/DAY - OTHERS - FRT WIND',
    '05B95': 'BURGLARY - BUS SAFE/DAY - OTHERS - REAR WIND',
    '05B96': 'BURGLARY - BUS SAFE/DAY - OTHERS - SIDE WIND',
    '05B97': 'BURGLARY - BUS SAFE/DAY - OTHERS - ROOF,FLOOR',
    '05B98': 'BURGLARY - BUS SAFE/DAY - OTHERS - WALL',
    '05B99': 'BURGLARY - BUS SAFE/DAY - OTHERS - UNKNOWN',
    '05B9A': 'BURGLARY - BUS SAFE/DAY - OTHERS - RES GRG DR',
    '05B9B': 'BURGLARY - BUS SAFE/DAY - OTHERS - HID IN BLDG',
    '05B9C': 'BURGLARY - BUS SAFE/DAY - OTHERS - OVRHEAD DR',
    '05C11': 'BURGLARY - BUS SAFE/UNK - KEY/COMBINATiON LOCK - FRONT DOOR',
    '05C12': 'BURGLARY - BUS SAFE/UNK - KEY/COMBINATiON LOCK - REAR DOOR',
    '05C13': 'BURGLARY - BUS SAFE/UNK - KEY/COMBINATiON LOCK - SIDE DOOR',
    '05C14': 'BURGLARY - BUS SAFE/UNK - KEY/COMBINATiON LOCK - FRT WIND',
    '05C15': 'BURGLARY - BUS SAFE/UNK - KEY/COMBINATiON LOCK - REAR WIND',
    '05C16': 'BURGLARY - BUS SAFE/UNK - KEY/COMBINATiON LOCK - SIDE WIND',
    '05C17': 'BURGLARY - BUS SAFE/UNK - KEY/COMBINATiON LOCK - ROOF,FLOOR',
    '05C18': 'BURGLARY - BUS SAFE/UNK - KEY/COMBINATiON LOCK - WALL',
    '05C19': 'BURGLARY - BUS SAFE/UNK - KEY/COMBINATiON LOCK - UNKNOWN',
    '05C1A': 'BURGLARY - BUS SAFE/UNK - KEY/COMBINATiON LOCK - RES GRG DR',
    '05C1B': 'BURGLARY - BUS SAFE/UNK - KEY/COMBINATiON LOCK - HID IN BLDG',
    '05C1C': 'BURGLARY - BUS SAFE/UNK - KEY/COMBINATiON LOCK - OVRHEAD DR',
    '05C21': 'BURGLARY - BUS SAFE/UNK - SMASHED, KICKED,STRUCK - FRONT DOOR',
    '05C22': 'BURGLARY - BUS SAFE/UNK - SMASHED, KICKED,STRUCK - REAR DOOR',
    '05C23': 'BURGLARY - BUS SAFE/UNK - SMASHED, KICKED,STRUCK - SIDE DOOR',
    '05C24': 'BURGLARY - BUS SAFE/UNK - SMASHED, KICKED,STRUCK - FRT WIND',
    '05C25': 'BURGLARY - BUS SAFE/UNK - SMASHED, KICKED,STRUCK - REAR WIND',
    '05C26': 'BURGLARY - BUS SAFE/UNK - SMASHED, KICKED,STRUCK - SIDE WIND',
    '05C27': 'BURGLARY - BUS SAFE/UNK - SMASHED, KICKED,STRUCK - ROOF,FLOOR',
    '05C28': 'BURGLARY - BUS SAFE/UNK - SMASHED, KICKED,STRUCK - WALL',
    '05C29': 'BURGLARY - BUS SAFE/UNK - SMASHED, KICKED,STRUCK - UNKNOWN',
    '05C2A': 'BURGLARY - BUS SAFE/UNK - SMASHED, KICKED,STRUCK - RES GRG DR',
    '05C2B': 'BURGLARY - BUS SAFE/UNK - SMASHED, KICKED,STRUCK - HID IN BLDG',
    '05C2C': 'BURGLARY - BUS SAFE/UNK - SMASHED, KICKED,STRUCK - OVRHEAD DR',
    '05C31': 'BURGLARY - BUS SAFE/UNK - PRY, BREAK SAW,CUT, ETC. - FRONT DOOR',
    '05C32': 'BURGLARY - BUS SAFE/UNK - PRY, BREAK SAW,CUT, ETC. - REAR DOOR',
    '05C33': 'BURGLARY - BUS SAFE/UNK - PRY, BREAK SAW,CUT, ETC. - SIDE DOOR',
    '05C34': 'BURGLARY - BUS SAFE/UNK - PRY, BREAK SAW,CUT, ETC. - FRT WIND',
    '05C35': 'BURGLARY - BUS SAFE/UNK - PRY, BREAK SAW,CUT, ETC. - REAR WIND',
    '05C36': 'BURGLARY - BUS SAFE/UNK - PRY, BREAK SAW,CUT, ETC. - SIDE WIND',
    '05C37': 'BURGLARY - BUS SAFE/UNK - PRY, BREAK SAW,CUT, ETC. - ROOF,FLOOR',
    '05C38': 'BURGLARY - BUS SAFE/UNK - PRY, BREAK SAW,CUT, ETC. - WALL',
    '05C39': 'BURGLARY - BUS SAFE/UNK - PRY, BREAK SAW,CUT, ETC. - UNKNOWN',
    '05C3A': 'BURGLARY - BUS SAFE/UNK - PRY, BREAK SAW,CUT, ETC. - RES GRG DR',
    '05C3B': 'BURGLARY - BUS SAFE/UNK - PRY, BREAK SAW,CUT, ETC. - HID IN BLDG',
    '05C3C': 'BURGLARY - BUS SAFE/UNK - PRY, BREAK SAW,CUT, ETC. - OVRHEAD DR',
    '05C41': 'BURGLARY - BUS SAFE/UNK - EXPLOSION,ARSON,CUTTING TORCH - FRONT DOOR',
    '05C42': 'BURGLARY - BUS SAFE/UNK - EXPLOSION,ARSON,CUTTING TORCH - REAR DOOR',
    '05C43': 'BURGLARY - BUS SAFE/UNK - EXPLOSION,ARSON,CUTTING TORCH - SIDE DOOR',
    '05C44': 'BURGLARY - BUS SAFE/UNK - EXPLOSION,ARSON,CUTTING TORCH - FRT WIND',
    '05C45': 'BURGLARY - BUS SAFE/UNK - EXPLOSION,ARSON,CUTTING TORCH - REAR WIND',
    '05C46': 'BURGLARY - BUS SAFE/UNK - EXPLOSION,ARSON,CUTTING TORCH - SIDE WIND',
    '05C47': 'BURGLARY - BUS SAFE/UNK - EXPLOSION,ARSON,CUTTING TORCH - ROOF,FLOOR',
    '05C48': 'BURGLARY - BUS SAFE/UNK - EXPLOSION,ARSON,CUTTING TORCH - WALL',
    '05C49': 'BURGLARY - BUS SAFE/UNK - EXPLOSION,ARSON,CUTTING TORCH - UNKNOWN',
    '05C4A': 'BURGLARY - BUS SAFE/UNK - EXPLOSION,ARSON,CUTTING TORCH - RES GRG DR',
    '05C4B': 'BURGLARY - BUS SAFE/UNK - EXPLOSION,ARSON,CUTTING TORCH - HID IN BLDG',
    '05C4C': 'BURGLARY - BUS SAFE/UNK - EXPLOSION,ARSON,CUTTING TORCH - OVRHEAD DR',
    '05C51': 'BURGLARY - BUS SAFE/UNK - NO FORCE (HIDE INSIDE) - FRONT DOOR',
    '05C52': 'BURGLARY - BUS SAFE/UNK - NO FORCE (HIDE INSIDE) - REAR DOOR',
    '05C53': 'BURGLARY - BUS SAFE/UNK - NO FORCE (HIDE INSIDE) - SIDE DOOR',
    '05C54': 'BURGLARY - BUS SAFE/UNK - NO FORCE (HIDE INSIDE) - FRT WIND',
    '05C55': 'BURGLARY - BUS SAFE/UNK - NO FORCE (HIDE INSIDE) - REAR WIND',
    '05C56': 'BURGLARY - BUS SAFE/UNK - NO FORCE (HIDE INSIDE) - SIDE WIND',
    '05C57': 'BURGLARY - BUS SAFE/UNK - NO FORCE (HIDE INSIDE) - ROOF,FLOOR',
    '05C58': 'BURGLARY - BUS SAFE/UNK - NO FORCE (HIDE INSIDE) - WALL',
    '05C59': 'BURGLARY - BUS SAFE/UNK - NO FORCE (HIDE INSIDE) - UNKNOWN',
    '05C5A': 'BURGLARY - BUS SAFE/UNK - NO FORCE (HIDE INSIDE) - RES GRG DR',
    '05C5B': 'BURGLARY - BUS SAFE/UNK - NO FORCE (HIDE INSIDE) - HID IN BLDG',
    '05C5C': 'BURGLARY - BUS SAFE/UNK - NO FORCE (HIDE INSIDE) - OVRHEAD DR',
    '05C61': 'BURGLARY - BUS SAFE/UNK - WINDOW,DOOR,ETC UNLOCKED - FRONT DOOR',
    '05C62': 'BURGLARY - BUS SAFE/UNK - WINDOW,DOOR,ETC UNLOCKED - REAR DOOR',
    '05C63': 'BURGLARY - BUS SAFE/UNK - WINDOW,DOOR,ETC UNLOCKED - SIDE DOOR',
    '05C64': 'BURGLARY - BUS SAFE/UNK - WINDOW,DOOR,ETC UNLOCKED - FRT WIND',
    '05C65': 'BURGLARY - BUS SAFE/UNK - WINDOW,DOOR,ETC UNLOCKED - REAR WIND',
    '05C66': 'BURGLARY - BUS SAFE/UNK - WINDOW,DOOR,ETC UNLOCKED - SIDE WIND',
    '05C67': 'BURGLARY - BUS SAFE/UNK - WINDOW,DOOR,ETC UNLOCKED - ROOF,FLOOR',
    '05C68': 'BURGLARY - BUS SAFE/UNK - WINDOW,DOOR,ETC UNLOCKED - WALL',
    '05C69': 'BURGLARY - BUS SAFE/UNK - WINDOW,DOOR,ETC UNLOCKED - UNKNOWN',
    '05C6A': 'BURGLARY - BUS SAFE/UNK - WINDOW,DOOR,ETC UNLOCKED - RES GRG DR',
    '05C6B': 'BURGLARY - BUS SAFE/UNK - WINDOW,DOOR,ETC UNLOCKED - HID IN BLDG',
    '05C6C': 'BURGLARY - BUS SAFE/UNK - WINDOW,DOOR,ETC UNLOCKED - OVRHEAD DR',
    '05C71': 'BURGLARY - BUS SAFE/UNK - ALL ATTEMPTS (NO FORCE) - FRONT DOOR',
    '05C72': 'BURGLARY - BUS SAFE/UNK - ALL ATTEMPTS (NO FORCE) - REAR DOOR',
    '05C73': 'BURGLARY - BUS SAFE/UNK - ALL ATTEMPTS (NO FORCE) - SIDE DOOR',
    '05C74': 'BURGLARY - BUS SAFE/UNK - ALL ATTEMPTS (NO FORCE) - FRT WIND',
    '05C75': 'BURGLARY - BUS SAFE/UNK - ALL ATTEMPTS (NO FORCE) - REAR WIND',
    '05C76': 'BURGLARY - BUS SAFE/UNK - ALL ATTEMPTS (NO FORCE) - SIDE WIND',
    '05C77': 'BURGLARY - BUS SAFE/UNK - ALL ATTEMPTS (NO FORCE) - ROOF,FLOOR',
    '05C78': 'BURGLARY - BUS SAFE/UNK - ALL ATTEMPTS (NO FORCE) - WALL',
    '05C79': 'BURGLARY - BUS SAFE/UNK - ALL ATTEMPTS (NO FORCE) - UNKNOWN',
    '05C7A': 'BURGLARY - BUS SAFE/UNK - ALL ATTEMPTS (NO FORCE) - RES GRG DR',
    '05C7B': 'BURGLARY - BUS SAFE/UNK - ALL ATTEMPTS (NO FORCE) - HID IN BLDG',
    '05C7C': 'BURGLARY - BUS SAFE/UNK - ALL ATTEMPTS (NO FORCE) - OVRHEAD DR',
    '05C81': 'BURGLARY - BUS SAFE/UNK - ALL OTHER ATTEMPTS - FRONT DOOR',
    '05C82': 'BURGLARY - BUS SAFE/UNK - ALL OTHER ATTEMPTS - REAR DOOR',
    '05C83': 'BURGLARY - BUS SAFE/UNK - ALL OTHER ATTEMPTS - SIDE DOOR',
    '05C84': 'BURGLARY - BUS SAFE/UNK - ALL OTHER ATTEMPTS - FRT WIND',
    '05C85': 'BURGLARY - BUS SAFE/UNK - ALL OTHER ATTEMPTS - REAR WIND',
    '05C86': 'BURGLARY - BUS SAFE/UNK - ALL OTHER ATTEMPTS - SIDE WIND',
    '05C87': 'BURGLARY - BUS SAFE/UNK - ALL OTHER ATTEMPTS - ROOF,FLOOR',
    '05C88': 'BURGLARY - BUS SAFE/UNK - ALL OTHER ATTEMPTS - WALL',
    '05C89': 'BURGLARY - BUS SAFE/UNK - ALL OTHER ATTEMPTS - UNKNOWN',
    '05C8A': 'BURGLARY - BUS SAFE/UNK - ALL OTHER ATTEMPTS - RES GRG DR',
    '05C8B': 'BURGLARY - BUS SAFE/UNK - ALL OTHER ATTEMPTS - HID IN BLDG',
    '05C8C': 'BURGLARY - BUS SAFE/UNK - ALL OTHER ATTEMPTS - OVRHEAD DR',
    '05C91': 'BURGLARY - BUS SAFE/UNK - OTHERS - FRONT DOOR',
    '05C92': 'BURGLARY - BUS SAFE/UNK - OTHERS - REAR DOOR',
    '05C93': 'BURGLARY - BUS SAFE/UNK - OTHERS - SIDE DOOR',
    '05C94': 'BURGLARY - BUS SAFE/UNK - OTHERS - FRT WIND',
    '05C95': 'BURGLARY - BUS SAFE/UNK - OTHERS - REAR WIND',
    '05C96': 'BURGLARY - BUS SAFE/UNK - OTHERS - SIDE WIND',
    '05C97': 'BURGLARY - BUS SAFE/UNK - OTHERS - ROOF,FLOOR',
    '05C98': 'BURGLARY - BUS SAFE/UNK - OTHERS - WALL',
    '05C99': 'BURGLARY - BUS SAFE/UNK - OTHERS - UNKNOWN',
    '05C9A': 'BURGLARY - BUS SAFE/UNK - OTHERS - RES GRG DR',
    '05C9B': 'BURGLARY - BUS SAFE/UNK - OTHERS - HID IN BLDG',
    '05C9C': 'BURGLARY - BUS SAFE/UNK - OTHERS - OVRHEAD DR',
    '06101': 'THEFT - FROM PERSON - OTHERS - $200+',
    '06102': 'THEFT - FROM PERSON - OTHERS - $50 - $199.99',
    '06103': 'THEFT - FROM PERSON - OTHERS - $20 - $49.99',
    '06104': 'THEFT - FROM PERSON - OTHERS - $5 - $19.99',
    '06105': 'THEFT - FROM PERSON - OTHERS - UNDER $5',
    '06111': 'THEFT - FROM PERSON - PICKED POCKET - $200+',
    '06112': 'THEFT - FROM PERSON - PICKED POCKET - $50 - $199.99',
    '06113': 'THEFT - FROM PERSON - PICKED POCKET - $20 - $49.99',
    '06114': 'THEFT - FROM PERSON - PICKED POCKET - $5 - $19.99',
    '06115': 'THEFT - FROM PERSON - PICKED POCKET - UNDER $5',
    '06121': 'THEFT - FROM PERSON - PURSE SNATCH - $200+',
    '06122': 'THEFT - FROM PERSON - PURSE SNATCH - $50 - $199.99',
    '06123': 'THEFT - FROM PERSON - PURSE SNATCH - $20 - $49.99',
    '06124': 'THEFT - FROM PERSON - PURSE SNATCH - $5 - $19.99',
    '06125': 'THEFT - FROM PERSON - PURSE SNATCH - UNDER $5',
    '06131': 'THEFT - FROM PERSON - SHOPLIFT - $200+',
    '06132': 'THEFT - FROM PERSON - SHOPLIFT - $50 - $199.99',
    '06133': 'THEFT - FROM PERSON - SHOPLIFT - $20 - $49.99',
    '06134': 'THEFT - FROM PERSON - SHOPLIFT - $5 - $19.99',
    '06135': 'THEFT - FROM PERSON - SHOPLIFT - UNDER $5',
    '06141': 'THEFT - FROM PERSON - BMV/UNATTACH PROP INSIDE VEH - $200+',
    '06142': 'THEFT - FROM PERSON - BMV/UNATTACH PROP INSIDE VEH - $50 - $199.99',
    '06143': 'THEFT - FROM PERSON - BMV/UNATTACH PROP INSIDE VEH - $20 - $49.99',
    '06144': 'THEFT - FROM PERSON - BMV/UNATTACH PROP INSIDE VEH - $5 - $19.99',
    '06145': 'THEFT - FROM PERSON - BMV/UNATTACH PROP INSIDE VEH - UNDER $5',
    '06151': 'THEFT - FROM PERSON - ATTACH AUTO PARTS/ACCESSORY - $200+',
    '06152': 'THEFT - FROM PERSON - ATTACH AUTO PARTS/ACCESSORY - $50 - $199.99',
    '06153': 'THEFT - FROM PERSON - ATTACH AUTO PARTS/ACCESSORY - $20 - $49.99',
    '06154': 'THEFT - FROM PERSON - ATTACH AUTO PARTS/ACCESSORY - $5 - $19.99',
    '06155': 'THEFT - FROM PERSON - ATTACH AUTO PARTS/ACCESSORY - UNDER $5',
    '06161': 'THEFT - FROM PERSON - FROM BLDGS, EXCEPT 3 & 7 - $200+',
    '06162': 'THEFT - FROM PERSON - FROM BLDGS, EXCEPT 3 & 7 - $50 - $199.99',
    '06163': 'THEFT - FROM PERSON - FROM BLDGS, EXCEPT 3 & 7 - $20 - $49.99',
    '06164': 'THEFT - FROM PERSON - FROM BLDGS, EXCEPT 3 & 7 - $5 - $19.99',
    '06165': 'THEFT - FROM PERSON - FROM BLDGS, EXCEPT 3 & 7 - UNDER $5',
    '06171': 'THEFT - FROM PERSON - BURG COIN-OP MACHINE (BCOM) - $200+',
    '06172': 'THEFT - FROM PERSON - BURG COIN-OP MACHINE (BCOM) - $50 - $199.99',
    '06173': 'THEFT - FROM PERSON - BURG COIN-OP MACHINE (BCOM) - $20 - $49.99',
    '06174': 'THEFT - FROM PERSON - BURG COIN-OP MACHINE (BCOM) - $5 - $19.99',
    '06175': 'THEFT - FROM PERSON - BURG COIN-OP MACHINE (BCOM) - UNDER $5',
    '06181': 'THEFT - FROM PERSON - CARGO THEFTS - $200+',
    '06182': 'THEFT - FROM PERSON - CARGO THEFTS - $50 - $199.99',
    '06183': 'THEFT - FROM PERSON - CARGO THEFTS - $20 - $49.99',
    '06184': 'THEFT - FROM PERSON - CARGO THEFTS - $5 - $19.99',
    '06185': 'THEFT - FROM PERSON - CARGO THEFTS - UNDER $5',
    '06191': 'THEFT - FROM PERSON - BICYCLES (2 WHEEL/NO MOTOR) - $200+',
    '06192': 'THEFT - FROM PERSON - BICYCLES (2 WHEEL/NO MOTOR) - $50 - $199.99',
    '06193': 'THEFT - FROM PERSON - BICYCLES (2 WHEEL/NO MOTOR) - $20 - $49.99',
    '06194': 'THEFT - FROM PERSON - BICYCLES (2 WHEEL/NO MOTOR) - $5 - $19.99',
    '06195': 'THEFT - FROM PERSON - BICYCLES (2 WHEEL/NO MOTOR) - UNDER $5',
    '061A1': 'THEFT - FROM PERSON - ATTEMPTS - $200+',
    '061A2': 'THEFT - FROM PERSON - ATTEMPTS - $50 - $199.99',
    '061A3': 'THEFT - FROM PERSON - ATTEMPTS - $20 - $49.99',
    '061A4': 'THEFT - FROM PERSON - ATTEMPTS - $5 - $19.99',
    '061A5': 'THEFT - FROM PERSON - ATTEMPTS - UNDER $5',
    '061B1': 'THEFT - FROM PERSON - FARM EQUIPMENT - $200+',
    '061B2': 'THEFT - FROM PERSON - FARM EQUIPMENT - $50 - $199.99',
    '061B3': 'THEFT - FROM PERSON - FARM EQUIPMENT - $20 - $49.99',
    '061B4': 'THEFT - FROM PERSON - FARM EQUIPMENT - $5 - $19.99',
    '061B5': 'THEFT - FROM PERSON - FARM EQUIPMENT - UNDER $5',
    '061C1': 'THEFT - FROM PERSON - BULLDOZER - $200+',
    '061C2': 'THEFT - FROM PERSON - BULLDOZER - $50 - $199.99',
    '061C3': 'THEFT - FROM PERSON - BULLDOZER - $20 - $49.99',
    '061C4': 'THEFT - FROM PERSON - BULLDOZER - $5 - $19.99',
    '061C5': 'THEFT - FROM PERSON - BULLDOZER - UNDER $5',
    '061D1': 'THEFT - FROM PERSON - AIRPLANES - $200+',
    '061D2': 'THEFT - FROM PERSON - AIRPLANES - $50 - $199.99',
    '061D3': 'THEFT - FROM PERSON - AIRPLANES - $20 - $49.99',
    '061D4': 'THEFT - FROM PERSON - AIRPLANES - $5 - $19.99',
    '061D5': 'THEFT - FROM PERSON - AIRPLANES - UNDER $5',
    '061E1': 'THEFT - FROM PERSON - CONSTRUCTION EQUIPMENT - $200+',
    '061E2': 'THEFT - FROM PERSON - CONSTRUCTION EQUIPMENT - $50 - $199.99',
    '061E3': 'THEFT - FROM PERSON - CONSTRUCTION EQUIPMENT - $20 - $49.99',
    '061E4': 'THEFT - FROM PERSON - CONSTRUCTION EQUIPMENT - $5 - $19.99',
    '061E5': 'THEFT - FROM PERSON - CONSTRUCTION EQUIPMENT - UNDER $5',
    '061F1': 'THEFT - FROM PERSON - THEFT OF SERVICE - $200+',
    '061F2': 'THEFT - FROM PERSON - THEFT OF SERVICE - $50 - $199.99',
    '061F3': 'THEFT - FROM PERSON - THEFT OF SERVICE - $20 - $49.99',
    '061F4': 'THEFT - FROM PERSON - THEFT OF SERVICE - $5 - $19.99',
    '061F5': 'THEFT - FROM PERSON - THEFT OF SERVICE - UNDER $5',
    '061X1': 'THEFT - FROM PERSON - THEFT OF TRADE SECRETS - $200+',
    '061X2': 'THEFT - FROM PERSON - THEFT OF TRADE SECRETS - $50 - $199.99',
    '061X3': 'THEFT - FROM PERSON - THEFT OF TRADE SECRETS - $20 - $49.99',
    '061X4': 'THEFT - FROM PERSON - THEFT OF TRADE SECRETS - $5 - $19.99',
    '061X5': 'THEFT - FROM PERSON - THEFT OF TRADE SECRETS - UNDER $5',
    '06201': 'THEFT - BY CONVERSION - OTHERS - $200+',
    '06202': 'THEFT - BY CONVERSION - OTHERS - $50 - $199.99',
    '06203': 'THEFT - BY CONVERSION - OTHERS - $20 - $49.99',
    '06204': 'THEFT - BY CONVERSION - OTHERS - $5 - $19.99',
    '06205': 'THEFT - BY CONVERSION - OTHERS - UNDER $5',
    '06211': 'THEFT - BY CONVERSION - PICKED POCKET - $200+',
    '06212': 'THEFT - BY CONVERSION - PICKED POCKET - $50 - $199.99',
    '06213': 'THEFT - BY CONVERSION - PICKED POCKET - $20 - $49.99',
    '06214': 'THEFT - BY CONVERSION - PICKED POCKET - $5 - $19.99',
    '06215': 'THEFT - BY CONVERSION - PICKED POCKET - UNDER $5',
    '06221': 'THEFT - BY CONVERSION - PURSE SNATCH - $200+',
    '06222': 'THEFT - BY CONVERSION - PURSE SNATCH - $50 - $199.99',
    '06223': 'THEFT - BY CONVERSION - PURSE SNATCH - $20 - $49.99',
    '06224': 'THEFT - BY CONVERSION - PURSE SNATCH - $5 - $19.99',
    '06225': 'THEFT - BY CONVERSION - PURSE SNATCH - UNDER $5',
    '06231': 'THEFT - BY CONVERSION - SHOPLIFT - $200+',
    '06232': 'THEFT - BY CONVERSION - SHOPLIFT - $50 - $199.99',
    '06233': 'THEFT - BY CONVERSION - SHOPLIFT - $20 - $49.99',
    '06234': 'THEFT - BY CONVERSION - SHOPLIFT - $5 - $19.99',
    '06235': 'THEFT - BY CONVERSION - SHOPLIFT - UNDER $5',
    '06241': 'THEFT - BY CONVERSION - BMV/UNATTACH PROP INSIDE VEH - $200+',
    '06242': 'THEFT - BY CONVERSION - BMV/UNATTACH PROP INSIDE VEH - $50 - $199.99',
    '06243': 'THEFT - BY CONVERSION - BMV/UNATTACH PROP INSIDE VEH - $20 - $49.99',
    '06244': 'THEFT - BY CONVERSION - BMV/UNATTACH PROP INSIDE VEH - $5 - $19.99',
    '06245': 'THEFT - BY CONVERSION - BMV/UNATTACH PROP INSIDE VEH - UNDER $5',
    '06251': 'THEFT - BY CONVERSION - ATTACH AUTO PARTS/ACCESSORY - $200+',
    '06252': 'THEFT - BY CONVERSION - ATTACH AUTO PARTS/ACCESSORY - $50 - $199.99',
    '06253': 'THEFT - BY CONVERSION - ATTACH AUTO PARTS/ACCESSORY - $20 - $49.99',
    '06254': 'THEFT - BY CONVERSION - ATTACH AUTO PARTS/ACCESSORY - $5 - $19.99',
    '06255': 'THEFT - BY CONVERSION - ATTACH AUTO PARTS/ACCESSORY - UNDER $5',
    '06261': 'THEFT - BY CONVERSION - FROM BLDGS, EXCEPT 3 & 7 - $200+',
    '06262': 'THEFT - BY CONVERSION - FROM BLDGS, EXCEPT 3 & 7 - $50 - $199.99',
    '06263': 'THEFT - BY CONVERSION - FROM BLDGS, EXCEPT 3 & 7 - $20 - $49.99',
    '06264': 'THEFT - BY CONVERSION - FROM BLDGS, EXCEPT 3 & 7 - $5 - $19.99',
    '06265': 'THEFT - BY CONVERSION - FROM BLDGS, EXCEPT 3 & 7 - UNDER $5',
    '06271': 'THEFT - BY CONVERSION - BURG COIN-OP MACHINE (BCOM) - $200+',
    '06272': 'THEFT - BY CONVERSION - BURG COIN-OP MACHINE (BCOM) - $50 - $199.99',
    '06273': 'THEFT - BY CONVERSION - BURG COIN-OP MACHINE (BCOM) - $20 - $49.99',
    '06274': 'THEFT - BY CONVERSION - BURG COIN-OP MACHINE (BCOM) - $5 - $19.99',
    '06275': 'THEFT - BY CONVERSION - BURG COIN-OP MACHINE (BCOM) - UNDER $5',
    '06281': 'THEFT - BY CONVERSION - CARGO THEFTS - $200+',
    '06282': 'THEFT - BY CONVERSION - CARGO THEFTS - $50 - $199.99',
    '06283': 'THEFT - BY CONVERSION - CARGO THEFTS - $20 - $49.99',
    '06284': 'THEFT - BY CONVERSION - CARGO THEFTS - $5 - $19.99',
    '06285': 'THEFT - BY CONVERSION - CARGO THEFTS - UNDER $5',
    '06291': 'THEFT - BY CONVERSION - BICYCLES (2 WHEEL/NO MOTOR) - $200+',
    '06292': 'THEFT - BY CONVERSION - BICYCLES (2 WHEEL/NO MOTOR) - $50 - $199.99',
    '06293': 'THEFT - BY CONVERSION - BICYCLES (2 WHEEL/NO MOTOR) - $20 - $49.99',
    '06294': 'THEFT - BY CONVERSION - BICYCLES (2 WHEEL/NO MOTOR) - $5 - $19.99',
    '06295': 'THEFT - BY CONVERSION - BICYCLES (2 WHEEL/NO MOTOR) - UNDER $5',
    '062A1': 'THEFT - BY CONVERSION - ATTEMPTS - $200+',
    '062A2': 'THEFT - BY CONVERSION - ATTEMPTS - $50 - $199.99',
    '062A3': 'THEFT - BY CONVERSION - ATTEMPTS - $20 - $49.99',
    '062A4': 'THEFT - BY CONVERSION - ATTEMPTS - $5 - $19.99',
    '062A5': 'THEFT - BY CONVERSION - ATTEMPTS - UNDER $5',
    '062B1': 'THEFT - BY CONVERSION - FARM EQUIPMENT - $200+',
    '062B2': 'THEFT - BY CONVERSION - FARM EQUIPMENT - $50 - $199.99',
    '062B3': 'THEFT - BY CONVERSION - FARM EQUIPMENT - $20 - $49.99',
    '062B4': 'THEFT - BY CONVERSION - FARM EQUIPMENT - $5 - $19.99',
    '062B5': 'THEFT - BY CONVERSION - FARM EQUIPMENT - UNDER $5',
    '062C1': 'THEFT - BY CONVERSION - BULLDOZER - $200+',
    '062C2': 'THEFT - BY CONVERSION - BULLDOZER - $50 - $199.99',
    '062C3': 'THEFT - BY CONVERSION - BULLDOZER - $20 - $49.99',
    '062C4': 'THEFT - BY CONVERSION - BULLDOZER - $5 - $19.99',
    '062C5': 'THEFT - BY CONVERSION - BULLDOZER - UNDER $5',
    '062D1': 'THEFT - BY CONVERSION - AIRPLANES - $200+',
    '062D2': 'THEFT - BY CONVERSION - AIRPLANES - $50 - $199.99',
    '062D3': 'THEFT - BY CONVERSION - AIRPLANES - $20 - $49.99',
    '062D4': 'THEFT - BY CONVERSION - AIRPLANES - $5 - $19.99',
    '062D5': 'THEFT - BY CONVERSION - AIRPLANES - UNDER $5',
    '062E1': 'THEFT - BY CONVERSION - CONSTRUCTION EQUIPMENT - $200+',
    '062E2': 'THEFT - BY CONVERSION - CONSTRUCTION EQUIPMENT - $50 - $199.99',
    '062E3': 'THEFT - BY CONVERSION - CONSTRUCTION EQUIPMENT - $20 - $49.99',
    '062E4': 'THEFT - BY CONVERSION - CONSTRUCTION EQUIPMENT - $5 - $19.99',
    '062E5': 'THEFT - BY CONVERSION - CONSTRUCTION EQUIPMENT - UNDER $5',
    '062F1': 'THEFT - BY CONVERSION - THEFT OF SERVICE - $200+',
    '062F2': 'THEFT - BY CONVERSION - THEFT OF SERVICE - $50 - $199.99',
    '062F3': 'THEFT - BY CONVERSION - THEFT OF SERVICE - $20 - $49.99',
    '062F4': 'THEFT - BY CONVERSION - THEFT OF SERVICE - $5 - $19.99',
    '062F5': 'THEFT - BY CONVERSION - THEFT OF SERVICE - UNDER $5',
    '062X1': 'THEFT - BY CONVERSION - THEFT OF TRADE SECRETS - $200+',
    '062X2': 'THEFT - BY CONVERSION - THEFT OF TRADE SECRETS - $50 - $199.99',
    '062X3': 'THEFT - BY CONVERSION - THEFT OF TRADE SECRETS - $20 - $49.99',
    '062X4': 'THEFT - BY CONVERSION - THEFT OF TRADE SECRETS - $5 - $19.99',
    '062X5': 'THEFT - BY CONVERSION - THEFT OF TRADE SECRETS - UNDER $5',
    '06301': 'THEFT - MAIL THEFT - OTHERS - $200+',
    '06302': 'THEFT - MAIL THEFT - OTHERS - $50 - $199.99',
    '06303': 'THEFT - MAIL THEFT - OTHERS - $20 - $49.99',
    '06304': 'THEFT - MAIL THEFT - OTHERS - $5 - $19.99',
    '06305': 'THEFT - MAIL THEFT - OTHERS - UNDER $5',
    '06311': 'THEFT - MAIL THEFT - PICKED POCKET - $200+',
    '06312': 'THEFT - MAIL THEFT - PICKED POCKET - $50 - $199.99',
    '06313': 'THEFT - MAIL THEFT - PICKED POCKET - $20 - $49.99',
    '06314': 'THEFT - MAIL THEFT - PICKED POCKET - $5 - $19.99',
    '06315': 'THEFT - MAIL THEFT - PICKED POCKET - UNDER $5',
    '06321': 'THEFT - MAIL THEFT - PURSE SNATCH - $200+',
    '06322': 'THEFT - MAIL THEFT - PURSE SNATCH - $50 - $199.99',
    '06323': 'THEFT - MAIL THEFT - PURSE SNATCH - $20 - $49.99',
    '06324': 'THEFT - MAIL THEFT - PURSE SNATCH - $5 - $19.99',
    '06325': 'THEFT - MAIL THEFT - PURSE SNATCH - UNDER $5',
    '06331': 'THEFT - MAIL THEFT - SHOPLIFT - $200+',
    '06332': 'THEFT - MAIL THEFT - SHOPLIFT - $50 - $199.99',
    '06333': 'THEFT - MAIL THEFT - SHOPLIFT - $20 - $49.99',
    '06334': 'THEFT - MAIL THEFT - SHOPLIFT - $5 - $19.99',
    '06335': 'THEFT - MAIL THEFT - SHOPLIFT - UNDER $5',
    '06341': 'THEFT - MAIL THEFT - BMV/UNATTACH PROP INSIDE VEH - $200+',
    '06342': 'THEFT - MAIL THEFT - BMV/UNATTACH PROP INSIDE VEH - $50 - $199.99',
    '06343': 'THEFT - MAIL THEFT - BMV/UNATTACH PROP INSIDE VEH - $20 - $49.99',
    '06344': 'THEFT - MAIL THEFT - BMV/UNATTACH PROP INSIDE VEH - $5 - $19.99',
    '06345': 'THEFT - MAIL THEFT - BMV/UNATTACH PROP INSIDE VEH - UNDER $5',
    '06351': 'THEFT - MAIL THEFT - ATTACH AUTO PARTS/ACCESSORY - $200+',
    '06352': 'THEFT - MAIL THEFT - ATTACH AUTO PARTS/ACCESSORY - $50 - $199.99',
    '06353': 'THEFT - MAIL THEFT - ATTACH AUTO PARTS/ACCESSORY - $20 - $49.99',
    '06354': 'THEFT - MAIL THEFT - ATTACH AUTO PARTS/ACCESSORY - $5 - $19.99',
    '06355': 'THEFT - MAIL THEFT - ATTACH AUTO PARTS/ACCESSORY - UNDER $5',
    '06361': 'THEFT - MAIL THEFT - FROM BLDGS, EXCEPT 3 & 7 - $200+',
    '06362': 'THEFT - MAIL THEFT - FROM BLDGS, EXCEPT 3 & 7 - $50 - $199.99',
    '06363': 'THEFT - MAIL THEFT - FROM BLDGS, EXCEPT 3 & 7 - $20 - $49.99',
    '06364': 'THEFT - MAIL THEFT - FROM BLDGS, EXCEPT 3 & 7 - $5 - $19.99',
    '06365': 'THEFT - MAIL THEFT - FROM BLDGS, EXCEPT 3 & 7 - UNDER $5',
    '06371': 'THEFT - MAIL THEFT - BURG COIN-OP MACHINE (BCOM) - $200+',
    '06372': 'THEFT - MAIL THEFT - BURG COIN-OP MACHINE (BCOM) - $50 - $199.99',
    '06373': 'THEFT - MAIL THEFT - BURG COIN-OP MACHINE (BCOM) - $20 - $49.99',
    '06374': 'THEFT - MAIL THEFT - BURG COIN-OP MACHINE (BCOM) - $5 - $19.99',
    '06375': 'THEFT - MAIL THEFT - BURG COIN-OP MACHINE (BCOM) - UNDER $5',
    '06381': 'THEFT - MAIL THEFT - CARGO THEFTS - $200+',
    '06382': 'THEFT - MAIL THEFT - CARGO THEFTS - $50 - $199.99',
    '06383': 'THEFT - MAIL THEFT - CARGO THEFTS - $20 - $49.99',
    '06384': 'THEFT - MAIL THEFT - CARGO THEFTS - $5 - $19.99',
    '06385': 'THEFT - MAIL THEFT - CARGO THEFTS - UNDER $5',
    '06391': 'THEFT - MAIL THEFT - BICYCLES (2 WHEEL/NO MOTOR) - $200+',
    '06392': 'THEFT - MAIL THEFT - BICYCLES (2 WHEEL/NO MOTOR) - $50 - $199.99',
    '06393': 'THEFT - MAIL THEFT - BICYCLES (2 WHEEL/NO MOTOR) - $20 - $49.99',
    '06394': 'THEFT - MAIL THEFT - BICYCLES (2 WHEEL/NO MOTOR) - $5 - $19.99',
    '06395': 'THEFT - MAIL THEFT - BICYCLES (2 WHEEL/NO MOTOR) - UNDER $5',
    '063A1': 'THEFT - MAIL THEFT - ATTEMPTS - $200+',
    '063A2': 'THEFT - MAIL THEFT - ATTEMPTS - $50 - $199.99',
    '063A3': 'THEFT - MAIL THEFT - ATTEMPTS - $20 - $49.99',
    '063A4': 'THEFT - MAIL THEFT - ATTEMPTS - $5 - $19.99',
    '063A5': 'THEFT - MAIL THEFT - ATTEMPTS - UNDER $5',
    '063B1': 'THEFT - MAIL THEFT - FARM EQUIPMENT - $200+',
    '063B2': 'THEFT - MAIL THEFT - FARM EQUIPMENT - $50 - $199.99',
    '063B3': 'THEFT - MAIL THEFT - FARM EQUIPMENT - $20 - $49.99',
    '063B4': 'THEFT - MAIL THEFT - FARM EQUIPMENT - $5 - $19.99',
    '063B5': 'THEFT - MAIL THEFT - FARM EQUIPMENT - UNDER $5',
    '063C1': 'THEFT - MAIL THEFT - BULLDOZER - $200+',
    '063C2': 'THEFT - MAIL THEFT - BULLDOZER - $50 - $199.99',
    '063C3': 'THEFT - MAIL THEFT - BULLDOZER - $20 - $49.99',
    '063C4': 'THEFT - MAIL THEFT - BULLDOZER - $5 - $19.99',
    '063C5': 'THEFT - MAIL THEFT - BULLDOZER - UNDER $5',
    '063D1': 'THEFT - MAIL THEFT - AIRPLANES - $200+',
    '063D2': 'THEFT - MAIL THEFT - AIRPLANES - $50 - $199.99',
    '063D3': 'THEFT - MAIL THEFT - AIRPLANES - $20 - $49.99',
    '063D4': 'THEFT - MAIL THEFT - AIRPLANES - $5 - $19.99',
    '063D5': 'THEFT - MAIL THEFT - AIRPLANES - UNDER $5',
    '063E1': 'THEFT - MAIL THEFT - CONSTRUCTION EQUIPMENT - $200+',
    '063E2': 'THEFT - MAIL THEFT - CONSTRUCTION EQUIPMENT - $50 - $199.99',
    '063E3': 'THEFT - MAIL THEFT - CONSTRUCTION EQUIPMENT - $20 - $49.99',
    '063E4': 'THEFT - MAIL THEFT - CONSTRUCTION EQUIPMENT - $5 - $19.99',
    '063E5': 'THEFT - MAIL THEFT - CONSTRUCTION EQUIPMENT - UNDER $5',
    '063F1': 'THEFT - MAIL THEFT - THEFT OF SERVICE - $200+',
    '063F2': 'THEFT - MAIL THEFT - THEFT OF SERVICE - $50 - $199.99',
    '063F3': 'THEFT - MAIL THEFT - THEFT OF SERVICE - $20 - $49.99',
    '063F4': 'THEFT - MAIL THEFT - THEFT OF SERVICE - $5 - $19.99',
    '063F5': 'THEFT - MAIL THEFT - THEFT OF SERVICE - UNDER $5',
    '063X1': 'THEFT - MAIL THEFT - THEFT OF TRADE SECRETS - $200+',
    '063X2': 'THEFT - MAIL THEFT - THEFT OF TRADE SECRETS - $50 - $199.99',
    '063X3': 'THEFT - MAIL THEFT - THEFT OF TRADE SECRETS - $20 - $49.99',
    '063X4': 'THEFT - MAIL THEFT - THEFT OF TRADE SECRETS - $5 - $19.99',
    '063X5': 'THEFT - MAIL THEFT - THEFT OF TRADE SECRETS - UNDER $5',
    '06401': 'THEFT - FR INTERST SHIP - OTHERS - $200+',
    '06402': 'THEFT - FR INTERST SHIP - OTHERS - $50 - $199.99',
    '06403': 'THEFT - FR INTERST SHIP - OTHERS - $20 - $49.99',
    '06404': 'THEFT - FR INTERST SHIP - OTHERS - $5 - $19.99',
    '06405': 'THEFT - FR INTERST SHIP - OTHERS - UNDER $5',
    '06411': 'THEFT - FR INTERST SHIP - PICKED POCKET - $200+',
    '06412': 'THEFT - FR INTERST SHIP - PICKED POCKET - $50 - $199.99',
    '06413': 'THEFT - FR INTERST SHIP - PICKED POCKET - $20 - $49.99',
    '06414': 'THEFT - FR INTERST SHIP - PICKED POCKET - $5 - $19.99',
    '06415': 'THEFT - FR INTERST SHIP - PICKED POCKET - UNDER $5',
    '06421': 'THEFT - FR INTERST SHIP - PURSE SNATCH - $200+',
    '06422': 'THEFT - FR INTERST SHIP - PURSE SNATCH - $50 - $199.99',
    '06423': 'THEFT - FR INTERST SHIP - PURSE SNATCH - $20 - $49.99',
    '06424': 'THEFT - FR INTERST SHIP - PURSE SNATCH - $5 - $19.99',
    '06425': 'THEFT - FR INTERST SHIP - PURSE SNATCH - UNDER $5',
    '06431': 'THEFT - FR INTERST SHIP - SHOPLIFT - $200+',
    '06432': 'THEFT - FR INTERST SHIP - SHOPLIFT - $50 - $199.99',
    '06433': 'THEFT - FR INTERST SHIP - SHOPLIFT - $20 - $49.99',
    '06434': 'THEFT - FR INTERST SHIP - SHOPLIFT - $5 - $19.99',
    '06435': 'THEFT - FR INTERST SHIP - SHOPLIFT - UNDER $5',
    '06441': 'THEFT - FR INTERST SHIP - BMV/UNATTACH PROP INSIDE VEH - $200+',
    '06442': 'THEFT - FR INTERST SHIP - BMV/UNATTACH PROP INSIDE VEH - $50 - $199.99',
    '06443': 'THEFT - FR INTERST SHIP - BMV/UNATTACH PROP INSIDE VEH - $20 - $49.99',
    '06444': 'THEFT - FR INTERST SHIP - BMV/UNATTACH PROP INSIDE VEH - $5 - $19.99',
    '06445': 'THEFT - FR INTERST SHIP - BMV/UNATTACH PROP INSIDE VEH - UNDER $5',
    '06451': 'THEFT - FR INTERST SHIP - ATTACH AUTO PARTS/ACCESSORY - $200+',
    '06452': 'THEFT - FR INTERST SHIP - ATTACH AUTO PARTS/ACCESSORY - $50 - $199.99',
    '06453': 'THEFT - FR INTERST SHIP - ATTACH AUTO PARTS/ACCESSORY - $20 - $49.99',
    '06454': 'THEFT - FR INTERST SHIP - ATTACH AUTO PARTS/ACCESSORY - $5 - $19.99',
    '06455': 'THEFT - FR INTERST SHIP - ATTACH AUTO PARTS/ACCESSORY - UNDER $5',
    '06461': 'THEFT - FR INTERST SHIP - FROM BLDGS, EXCEPT 3 & 7 - $200+',
    '06462': 'THEFT - FR INTERST SHIP - FROM BLDGS, EXCEPT 3 & 7 - $50 - $199.99',
    '06463': 'THEFT - FR INTERST SHIP - FROM BLDGS, EXCEPT 3 & 7 - $20 - $49.99',
    '06464': 'THEFT - FR INTERST SHIP - FROM BLDGS, EXCEPT 3 & 7 - $5 - $19.99',
    '06465': 'THEFT - FR INTERST SHIP - FROM BLDGS, EXCEPT 3 & 7 - UNDER $5',
    '06471': 'THEFT - FR INTERST SHIP - BURG COIN-OP MACHINE (BCOM) - $200+',
    '06472': 'THEFT - FR INTERST SHIP - BURG COIN-OP MACHINE (BCOM) - $50 - $199.99',
    '06473': 'THEFT - FR INTERST SHIP - BURG COIN-OP MACHINE (BCOM) - $20 - $49.99',
    '06474': 'THEFT - FR INTERST SHIP - BURG COIN-OP MACHINE (BCOM) - $5 - $19.99',
    '06475': 'THEFT - FR INTERST SHIP - BURG COIN-OP MACHINE (BCOM) - UNDER $5',
    '06481': 'THEFT - FR INTERST SHIP - CARGO THEFTS - $200+',
    '06482': 'THEFT - FR INTERST SHIP - CARGO THEFTS - $50 - $199.99',
    '06483': 'THEFT - FR INTERST SHIP - CARGO THEFTS - $20 - $49.99',
    '06484': 'THEFT - FR INTERST SHIP - CARGO THEFTS - $5 - $19.99',
    '06485': 'THEFT - FR INTERST SHIP - CARGO THEFTS - UNDER $5',
    '06491': 'THEFT - FR INTERST SHIP - BICYCLES (2 WHEEL/NO MOTOR) - $200+',
    '06492': 'THEFT - FR INTERST SHIP - BICYCLES (2 WHEEL/NO MOTOR) - $50 - $199.99',
    '06493': 'THEFT - FR INTERST SHIP - BICYCLES (2 WHEEL/NO MOTOR) - $20 - $49.99',
    '06494': 'THEFT - FR INTERST SHIP - BICYCLES (2 WHEEL/NO MOTOR) - $5 - $19.99',
    '06495': 'THEFT - FR INTERST SHIP - BICYCLES (2 WHEEL/NO MOTOR) - UNDER $5',
    '064A1': 'THEFT - FR INTERST SHIP - ATTEMPTS - $200+',
    '064A2': 'THEFT - FR INTERST SHIP - ATTEMPTS - $50 - $199.99',
    '064A3': 'THEFT - FR INTERST SHIP - ATTEMPTS - $20 - $49.99',
    '064A4': 'THEFT - FR INTERST SHIP - ATTEMPTS - $5 - $19.99',
    '064A5': 'THEFT - FR INTERST SHIP - ATTEMPTS - UNDER $5',
    '064B1': 'THEFT - FR INTERST SHIP - FARM EQUIPMENT - $200+',
    '064B2': 'THEFT - FR INTERST SHIP - FARM EQUIPMENT - $50 - $199.99',
    '064B3': 'THEFT - FR INTERST SHIP - FARM EQUIPMENT - $20 - $49.99',
    '064B4': 'THEFT - FR INTERST SHIP - FARM EQUIPMENT - $5 - $19.99',
    '064B5': 'THEFT - FR INTERST SHIP - FARM EQUIPMENT - UNDER $5',
    '064C1': 'THEFT - FR INTERST SHIP - BULLDOZER - $200+',
    '064C2': 'THEFT - FR INTERST SHIP - BULLDOZER - $50 - $199.99',
    '064C3': 'THEFT - FR INTERST SHIP - BULLDOZER - $20 - $49.99',
    '064C4': 'THEFT - FR INTERST SHIP - BULLDOZER - $5 - $19.99',
    '064C5': 'THEFT - FR INTERST SHIP - BULLDOZER - UNDER $5',
    '064D1': 'THEFT - FR INTERST SHIP - AIRPLANES - $200+',
    '064D2': 'THEFT - FR INTERST SHIP - AIRPLANES - $50 - $199.99',
    '064D3': 'THEFT - FR INTERST SHIP - AIRPLANES - $20 - $49.99',
    '064D4': 'THEFT - FR INTERST SHIP - AIRPLANES - $5 - $19.99',
    '064D5': 'THEFT - FR INTERST SHIP - AIRPLANES - UNDER $5',
    '064E1': 'THEFT - FR INTERST SHIP - CONSTRUCTION EQUIPMENT - $200+',
    '064E2': 'THEFT - FR INTERST SHIP - CONSTRUCTION EQUIPMENT - $50 - $199.99',
    '064E3': 'THEFT - FR INTERST SHIP - CONSTRUCTION EQUIPMENT - $20 - $49.99',
    '064E4': 'THEFT - FR INTERST SHIP - CONSTRUCTION EQUIPMENT - $5 - $19.99',
    '064E5': 'THEFT - FR INTERST SHIP - CONSTRUCTION EQUIPMENT - UNDER $5',
    '064F1': 'THEFT - FR INTERST SHIP - THEFT OF SERVICE - $200+',
    '064F2': 'THEFT - FR INTERST SHIP - THEFT OF SERVICE - $50 - $199.99',
    '064F3': 'THEFT - FR INTERST SHIP - THEFT OF SERVICE - $20 - $49.99',
    '064F4': 'THEFT - FR INTERST SHIP - THEFT OF SERVICE - $5 - $19.99',
    '064F5': 'THEFT - FR INTERST SHIP - THEFT OF SERVICE - UNDER $5',
    '064X1': 'THEFT - FR INTERST SHIP - THEFT OF TRADE SECRETS - $200+',
    '064X2': 'THEFT - FR INTERST SHIP - THEFT OF TRADE SECRETS - $50 - $199.99',
    '064X3': 'THEFT - FR INTERST SHIP - THEFT OF TRADE SECRETS - $20 - $49.99',
    '064X4': 'THEFT - FR INTERST SHIP - THEFT OF TRADE SECRETS - $5 - $19.99',
    '064X5': 'THEFT - FR INTERST SHIP - THEFT OF TRADE SECRETS - UNDER $5',
    '06501': 'THEFT - GOV\'T PROPERTY - OTHERS - $200+',
    '06502': 'THEFT - GOV\'T PROPERTY - OTHERS - $50 - $199.99',
    '06503': 'THEFT - GOV\'T PROPERTY - OTHERS - $20 - $49.99',
    '06504': 'THEFT - GOV\'T PROPERTY - OTHERS - $5 - $19.99',
    '06505': 'THEFT - GOV\'T PROPERTY - OTHERS - UNDER $5',
    '06511': 'THEFT - GOV\'T PROPERTY - PICKED POCKET - $200+',
    '06512': 'THEFT - GOV\'T PROPERTY - PICKED POCKET - $50 - $199.99',
    '06513': 'THEFT - GOV\'T PROPERTY - PICKED POCKET - $20 - $49.99',
    '06514': 'THEFT - GOV\'T PROPERTY - PICKED POCKET - $5 - $19.99',
    '06515': 'THEFT - GOV\'T PROPERTY - PICKED POCKET - UNDER $5',
    '06521': 'THEFT - GOV\'T PROPERTY - PURSE SNATCH - $200+',
    '06522': 'THEFT - GOV\'T PROPERTY - PURSE SNATCH - $50 - $199.99',
    '06523': 'THEFT - GOV\'T PROPERTY - PURSE SNATCH - $20 - $49.99',
    '06524': 'THEFT - GOV\'T PROPERTY - PURSE SNATCH - $5 - $19.99',
    '06525': 'THEFT - GOV\'T PROPERTY - PURSE SNATCH - UNDER $5',
    '06531': 'THEFT - GOV\'T PROPERTY - SHOPLIFT - $200+',
    '06532': 'THEFT - GOV\'T PROPERTY - SHOPLIFT - $50 - $199.99',
    '06533': 'THEFT - GOV\'T PROPERTY - SHOPLIFT - $20 - $49.99',
    '06534': 'THEFT - GOV\'T PROPERTY - SHOPLIFT - $5 - $19.99',
    '06535': 'THEFT - GOV\'T PROPERTY - SHOPLIFT - UNDER $5',
    '06541': 'THEFT - GOV\'T PROPERTY - BMV/UNATTACH PROP INSIDE VEH - $200+',
    '06542': 'THEFT - GOV\'T PROPERTY - BMV/UNATTACH PROP INSIDE VEH - $50 - $199.99',
    '06543': 'THEFT - GOV\'T PROPERTY - BMV/UNATTACH PROP INSIDE VEH - $20 - $49.99',
    '06544': 'THEFT - GOV\'T PROPERTY - BMV/UNATTACH PROP INSIDE VEH - $5 - $19.99',
    '06545': 'THEFT - GOV\'T PROPERTY - BMV/UNATTACH PROP INSIDE VEH - UNDER $5',
    '06551': 'THEFT - GOV\'T PROPERTY - ATTACH AUTO PARTS/ACCESSORY - $200+',
    '06552': 'THEFT - GOV\'T PROPERTY - ATTACH AUTO PARTS/ACCESSORY - $50 - $199.99',
    '06553': 'THEFT - GOV\'T PROPERTY - ATTACH AUTO PARTS/ACCESSORY - $20 - $49.99',
    '06554': 'THEFT - GOV\'T PROPERTY - ATTACH AUTO PARTS/ACCESSORY - $5 - $19.99',
    '06555': 'THEFT - GOV\'T PROPERTY - ATTACH AUTO PARTS/ACCESSORY - UNDER $5',
    '06561': 'THEFT - GOV\'T PROPERTY - FROM BLDGS, EXCEPT 3 & 7 - $200+',
    '06562': 'THEFT - GOV\'T PROPERTY - FROM BLDGS, EXCEPT 3 & 7 - $50 - $199.99',
    '06563': 'THEFT - GOV\'T PROPERTY - FROM BLDGS, EXCEPT 3 & 7 - $20 - $49.99',
    '06564': 'THEFT - GOV\'T PROPERTY - FROM BLDGS, EXCEPT 3 & 7 - $5 - $19.99',
    '06565': 'THEFT - GOV\'T PROPERTY - FROM BLDGS, EXCEPT 3 & 7 - UNDER $5',
    '06571': 'THEFT - GOV\'T PROPERTY - BURG COIN-OP MACHINE (BCOM) - $200+',
    '06572': 'THEFT - GOV\'T PROPERTY - BURG COIN-OP MACHINE (BCOM) - $50 - $199.99',
    '06573': 'THEFT - GOV\'T PROPERTY - BURG COIN-OP MACHINE (BCOM) - $20 - $49.99',
    '06574': 'THEFT - GOV\'T PROPERTY - BURG COIN-OP MACHINE (BCOM) - $5 - $19.99',
    '06575': 'THEFT - GOV\'T PROPERTY - BURG COIN-OP MACHINE (BCOM) - UNDER $5',
    '06581': 'THEFT - GOV\'T PROPERTY - CARGO THEFTS - $200+',
    '06582': 'THEFT - GOV\'T PROPERTY - CARGO THEFTS - $50 - $199.99',
    '06583': 'THEFT - GOV\'T PROPERTY - CARGO THEFTS - $20 - $49.99',
    '06584': 'THEFT - GOV\'T PROPERTY - CARGO THEFTS - $5 - $19.99',
    '06585': 'THEFT - GOV\'T PROPERTY - CARGO THEFTS - UNDER $5',
    '06591': 'THEFT - GOV\'T PROPERTY - BICYCLES (2 WHEEL/NO MOTOR) - $200+',
    '06592': 'THEFT - GOV\'T PROPERTY - BICYCLES (2 WHEEL/NO MOTOR) - $50 - $199.99',
    '06593': 'THEFT - GOV\'T PROPERTY - BICYCLES (2 WHEEL/NO MOTOR) - $20 - $49.99',
    '06594': 'THEFT - GOV\'T PROPERTY - BICYCLES (2 WHEEL/NO MOTOR) - $5 - $19.99',
    '06595': 'THEFT - GOV\'T PROPERTY - BICYCLES (2 WHEEL/NO MOTOR) - UNDER $5',
    '065A1': 'THEFT - GOV\'T PROPERTY - ATTEMPTS - $200+',
    '065A2': 'THEFT - GOV\'T PROPERTY - ATTEMPTS - $50 - $199.99',
    '065A3': 'THEFT - GOV\'T PROPERTY - ATTEMPTS - $20 - $49.99',
    '065A4': 'THEFT - GOV\'T PROPERTY - ATTEMPTS - $5 - $19.99',
    '065A5': 'THEFT - GOV\'T PROPERTY - ATTEMPTS - UNDER $5',
    '065B1': 'THEFT - GOV\'T PROPERTY - FARM EQUIPMENT - $200+',
    '065B2': 'THEFT - GOV\'T PROPERTY - FARM EQUIPMENT - $50 - $199.99',
    '065B3': 'THEFT - GOV\'T PROPERTY - FARM EQUIPMENT - $20 - $49.99',
    '065B4': 'THEFT - GOV\'T PROPERTY - FARM EQUIPMENT - $5 - $19.99',
    '065B5': 'THEFT - GOV\'T PROPERTY - FARM EQUIPMENT - UNDER $5',
    '065C1': 'THEFT - GOV\'T PROPERTY - BULLDOZER - $200+',
    '065C2': 'THEFT - GOV\'T PROPERTY - BULLDOZER - $50 - $199.99',
    '065C3': 'THEFT - GOV\'T PROPERTY - BULLDOZER - $20 - $49.99',
    '065C4': 'THEFT - GOV\'T PROPERTY - BULLDOZER - $5 - $19.99',
    '065C5': 'THEFT - GOV\'T PROPERTY - BULLDOZER - UNDER $5',
    '065D1': 'THEFT - GOV\'T PROPERTY - AIRPLANES - $200+',
    '065D2': 'THEFT - GOV\'T PROPERTY - AIRPLANES - $50 - $199.99',
    '065D3': 'THEFT - GOV\'T PROPERTY - AIRPLANES - $20 - $49.99',
    '065D4': 'THEFT - GOV\'T PROPERTY - AIRPLANES - $5 - $19.99',
    '065D5': 'THEFT - GOV\'T PROPERTY - AIRPLANES - UNDER $5',
    '065E1': 'THEFT - GOV\'T PROPERTY - CONSTRUCTION EQUIPMENT - $200+',
    '065E2': 'THEFT - GOV\'T PROPERTY - CONSTRUCTION EQUIPMENT - $50 - $199.99',
    '065E3': 'THEFT - GOV\'T PROPERTY - CONSTRUCTION EQUIPMENT - $20 - $49.99',
    '065E4': 'THEFT - GOV\'T PROPERTY - CONSTRUCTION EQUIPMENT - $5 - $19.99',
    '065E5': 'THEFT - GOV\'T PROPERTY - CONSTRUCTION EQUIPMENT - UNDER $5',
    '065F1': 'THEFT - GOV\'T PROPERTY - THEFT OF SERVICE - $200+',
    '065F2': 'THEFT - GOV\'T PROPERTY - THEFT OF SERVICE - $50 - $199.99',
    '065F3': 'THEFT - GOV\'T PROPERTY - THEFT OF SERVICE - $20 - $49.99',
    '065F4': 'THEFT - GOV\'T PROPERTY - THEFT OF SERVICE - $5 - $19.99',
    '065F5': 'THEFT - GOV\'T PROPERTY - THEFT OF SERVICE - UNDER $5',
    '065X1': 'THEFT - GOV\'T PROPERTY - THEFT OF TRADE SECRETS - $200+',
    '065X2': 'THEFT - GOV\'T PROPERTY - THEFT OF TRADE SECRETS - $50 - $199.99',
    '065X3': 'THEFT - GOV\'T PROPERTY - THEFT OF TRADE SECRETS - $20 - $49.99',
    '065X4': 'THEFT - GOV\'T PROPERTY - THEFT OF TRADE SECRETS - $5 - $19.99',
    '065X5': 'THEFT - GOV\'T PROPERTY - THEFT OF TRADE SECRETS - UNDER $5',
    '06601': 'THEFT - HORSES - OTHERS - $200+',
    '06602': 'THEFT - HORSES - OTHERS - $50 - $199.99',
    '06603': 'THEFT - HORSES - OTHERS - $20 - $49.99',
    '06604': 'THEFT - HORSES - OTHERS - $5 - $19.99',
    '06605': 'THEFT - HORSES - OTHERS - UNDER $5',
    '06611': 'THEFT - HORSES - PICKED POCKET - $200+',
    '06612': 'THEFT - HORSES - PICKED POCKET - $50 - $199.99',
    '06613': 'THEFT - HORSES - PICKED POCKET - $20 - $49.99',
    '06614': 'THEFT - HORSES - PICKED POCKET - $5 - $19.99',
    '06615': 'THEFT - HORSES - PICKED POCKET - UNDER $5',
    '06621': 'THEFT - HORSES - PURSE SNATCH - $200+',
    '06622': 'THEFT - HORSES - PURSE SNATCH - $50 - $199.99',
    '06623': 'THEFT - HORSES - PURSE SNATCH - $20 - $49.99',
    '06624': 'THEFT - HORSES - PURSE SNATCH - $5 - $19.99',
    '06625': 'THEFT - HORSES - PURSE SNATCH - UNDER $5',
    '06631': 'THEFT - HORSES - SHOPLIFT - $200+',
    '06632': 'THEFT - HORSES - SHOPLIFT - $50 - $199.99',
    '06633': 'THEFT - HORSES - SHOPLIFT - $20 - $49.99',
    '06634': 'THEFT - HORSES - SHOPLIFT - $5 - $19.99',
    '06635': 'THEFT - HORSES - SHOPLIFT - UNDER $5',
    '06641': 'THEFT - HORSES - BMV/UNATTACH PROP INSIDE VEH - $200+',
    '06642': 'THEFT - HORSES - BMV/UNATTACH PROP INSIDE VEH - $50 - $199.99',
    '06643': 'THEFT - HORSES - BMV/UNATTACH PROP INSIDE VEH - $20 - $49.99',
    '06644': 'THEFT - HORSES - BMV/UNATTACH PROP INSIDE VEH - $5 - $19.99',
    '06645': 'THEFT - HORSES - BMV/UNATTACH PROP INSIDE VEH - UNDER $5',
    '06651': 'THEFT - HORSES - ATTACH AUTO PARTS/ACCESSORY - $200+',
    '06652': 'THEFT - HORSES - ATTACH AUTO PARTS/ACCESSORY - $50 - $199.99',
    '06653': 'THEFT - HORSES - ATTACH AUTO PARTS/ACCESSORY - $20 - $49.99',
    '06654': 'THEFT - HORSES - ATTACH AUTO PARTS/ACCESSORY - $5 - $19.99',
    '06655': 'THEFT - HORSES - ATTACH AUTO PARTS/ACCESSORY - UNDER $5',
    '06661': 'THEFT - HORSES - FROM BLDGS, EXCEPT 3 & 7 - $200+',
    '06662': 'THEFT - HORSES - FROM BLDGS, EXCEPT 3 & 7 - $50 - $199.99',
    '06663': 'THEFT - HORSES - FROM BLDGS, EXCEPT 3 & 7 - $20 - $49.99',
    '06664': 'THEFT - HORSES - FROM BLDGS, EXCEPT 3 & 7 - $5 - $19.99',
    '06665': 'THEFT - HORSES - FROM BLDGS, EXCEPT 3 & 7 - UNDER $5',
    '06671': 'THEFT - HORSES - BURG COIN-OP MACHINE (BCOM) - $200+',
    '06672': 'THEFT - HORSES - BURG COIN-OP MACHINE (BCOM) - $50 - $199.99',
    '06673': 'THEFT - HORSES - BURG COIN-OP MACHINE (BCOM) - $20 - $49.99',
    '06674': 'THEFT - HORSES - BURG COIN-OP MACHINE (BCOM) - $5 - $19.99',
    '06675': 'THEFT - HORSES - BURG COIN-OP MACHINE (BCOM) - UNDER $5',
    '06681': 'THEFT - HORSES - CARGO THEFTS - $200+',
    '06682': 'THEFT - HORSES - CARGO THEFTS - $50 - $199.99',
    '06683': 'THEFT - HORSES - CARGO THEFTS - $20 - $49.99',
    '06684': 'THEFT - HORSES - CARGO THEFTS - $5 - $19.99',
    '06685': 'THEFT - HORSES - CARGO THEFTS - UNDER $5',
    '06691': 'THEFT - HORSES - BICYCLES (2 WHEEL/NO MOTOR) - $200+',
    '06692': 'THEFT - HORSES - BICYCLES (2 WHEEL/NO MOTOR) - $50 - $199.99',
    '06693': 'THEFT - HORSES - BICYCLES (2 WHEEL/NO MOTOR) - $20 - $49.99',
    '06694': 'THEFT - HORSES - BICYCLES (2 WHEEL/NO MOTOR) - $5 - $19.99',
    '06695': 'THEFT - HORSES - BICYCLES (2 WHEEL/NO MOTOR) - UNDER $5',
    '066A1': 'THEFT - HORSES - ATTEMPTS - $200+',
    '066A2': 'THEFT - HORSES - ATTEMPTS - $50 - $199.99',
    '066A3': 'THEFT - HORSES - ATTEMPTS - $20 - $49.99',
    '066A4': 'THEFT - HORSES - ATTEMPTS - $5 - $19.99',
    '066A5': 'THEFT - HORSES - ATTEMPTS - UNDER $5',
    '066B1': 'THEFT - HORSES - FARM EQUIPMENT - $200+',
    '066B2': 'THEFT - HORSES - FARM EQUIPMENT - $50 - $199.99',
    '066B3': 'THEFT - HORSES - FARM EQUIPMENT - $20 - $49.99',
    '066B4': 'THEFT - HORSES - FARM EQUIPMENT - $5 - $19.99',
    '066B5': 'THEFT - HORSES - FARM EQUIPMENT - UNDER $5',
    '066C1': 'THEFT - HORSES - BULLDOZER - $200+',
    '066C2': 'THEFT - HORSES - BULLDOZER - $50 - $199.99',
    '066C3': 'THEFT - HORSES - BULLDOZER - $20 - $49.99',
    '066C4': 'THEFT - HORSES - BULLDOZER - $5 - $19.99',
    '066C5': 'THEFT - HORSES - BULLDOZER - UNDER $5',
    '066D1': 'THEFT - HORSES - AIRPLANES - $200+',
    '066D2': 'THEFT - HORSES - AIRPLANES - $50 - $199.99',
    '066D3': 'THEFT - HORSES - AIRPLANES - $20 - $49.99',
    '066D4': 'THEFT - HORSES - AIRPLANES - $5 - $19.99',
    '066D5': 'THEFT - HORSES - AIRPLANES - UNDER $5',
    '066E1': 'THEFT - HORSES - CONSTRUCTION EQUIPMENT - $200+',
    '066E2': 'THEFT - HORSES - CONSTRUCTION EQUIPMENT - $50 - $199.99',
    '066E3': 'THEFT - HORSES - CONSTRUCTION EQUIPMENT - $20 - $49.99',
    '066E4': 'THEFT - HORSES - CONSTRUCTION EQUIPMENT - $5 - $19.99',
    '066E5': 'THEFT - HORSES - CONSTRUCTION EQUIPMENT - UNDER $5',
    '066F1': 'THEFT - HORSES - THEFT OF SERVICE - $200+',
    '066F2': 'THEFT - HORSES - THEFT OF SERVICE - $50 - $199.99',
    '066F3': 'THEFT - HORSES - THEFT OF SERVICE - $20 - $49.99',
    '066F4': 'THEFT - HORSES - THEFT OF SERVICE - $5 - $19.99',
    '066F5': 'THEFT - HORSES - THEFT OF SERVICE - UNDER $5',
    '066X1': 'THEFT - HORSES - THEFT OF TRADE SECRETS - $200+',
    '066X2': 'THEFT - HORSES - THEFT OF TRADE SECRETS - $50 - $199.99',
    '066X3': 'THEFT - HORSES - THEFT OF TRADE SECRETS - $20 - $49.99',
    '066X4': 'THEFT - HORSES - THEFT OF TRADE SECRETS - $5 - $19.99',
    '066X5': 'THEFT - HORSES - THEFT OF TRADE SECRETS - UNDER $5',
    '06701': 'THEFT - EDIBLE ANIMALS - OTHERS - $200+',
    '06702': 'THEFT - EDIBLE ANIMALS - OTHERS - $50 - $199.99',
    '06703': 'THEFT - EDIBLE ANIMALS - OTHERS - $20 - $49.99',
    '06704': 'THEFT - EDIBLE ANIMALS - OTHERS - $5 - $19.99',
    '06705': 'THEFT - EDIBLE ANIMALS - OTHERS - UNDER $5',
    '06711': 'THEFT - EDIBLE ANIMALS - PICKED POCKET - $200+',
    '06712': 'THEFT - EDIBLE ANIMALS - PICKED POCKET - $50 - $199.99',
    '06713': 'THEFT - EDIBLE ANIMALS - PICKED POCKET - $20 - $49.99',
    '06714': 'THEFT - EDIBLE ANIMALS - PICKED POCKET - $5 - $19.99',
    '06715': 'THEFT - EDIBLE ANIMALS - PICKED POCKET - UNDER $5',
    '06721': 'THEFT - EDIBLE ANIMALS - PURSE SNATCH - $200+',
    '06722': 'THEFT - EDIBLE ANIMALS - PURSE SNATCH - $50 - $199.99',
    '06723': 'THEFT - EDIBLE ANIMALS - PURSE SNATCH - $20 - $49.99',
    '06724': 'THEFT - EDIBLE ANIMALS - PURSE SNATCH - $5 - $19.99',
    '06725': 'THEFT - EDIBLE ANIMALS - PURSE SNATCH - UNDER $5',
    '06731': 'THEFT - EDIBLE ANIMALS - SHOPLIFT - $200+',
    '06732': 'THEFT - EDIBLE ANIMALS - SHOPLIFT - $50 - $199.99',
    '06733': 'THEFT - EDIBLE ANIMALS - SHOPLIFT - $20 - $49.99',
    '06734': 'THEFT - EDIBLE ANIMALS - SHOPLIFT - $5 - $19.99',
    '06735': 'THEFT - EDIBLE ANIMALS - SHOPLIFT - UNDER $5',
    '06741': 'THEFT - EDIBLE ANIMALS - BMV/UNATTACH PROP INSIDE VEH - $200+',
    '06742': 'THEFT - EDIBLE ANIMALS - BMV/UNATTACH PROP INSIDE VEH - $50 - $199.99',
    '06743': 'THEFT - EDIBLE ANIMALS - BMV/UNATTACH PROP INSIDE VEH - $20 - $49.99',
    '06744': 'THEFT - EDIBLE ANIMALS - BMV/UNATTACH PROP INSIDE VEH - $5 - $19.99',
    '06745': 'THEFT - EDIBLE ANIMALS - BMV/UNATTACH PROP INSIDE VEH - UNDER $5',
    '06751': 'THEFT - EDIBLE ANIMALS - ATTACH AUTO PARTS/ACCESSORY - $200+',
    '06752': 'THEFT - EDIBLE ANIMALS - ATTACH AUTO PARTS/ACCESSORY - $50 - $199.99',
    '06753': 'THEFT - EDIBLE ANIMALS - ATTACH AUTO PARTS/ACCESSORY - $20 - $49.99',
    '06754': 'THEFT - EDIBLE ANIMALS - ATTACH AUTO PARTS/ACCESSORY - $5 - $19.99',
    '06755': 'THEFT - EDIBLE ANIMALS - ATTACH AUTO PARTS/ACCESSORY - UNDER $5',
    '06761': 'THEFT - EDIBLE ANIMALS - FROM BLDGS, EXCEPT 3 & 7 - $200+',
    '06762': 'THEFT - EDIBLE ANIMALS - FROM BLDGS, EXCEPT 3 & 7 - $50 - $199.99',
    '06763': 'THEFT - EDIBLE ANIMALS - FROM BLDGS, EXCEPT 3 & 7 - $20 - $49.99',
    '06764': 'THEFT - EDIBLE ANIMALS - FROM BLDGS, EXCEPT 3 & 7 - $5 - $19.99',
    '06765': 'THEFT - EDIBLE ANIMALS - FROM BLDGS, EXCEPT 3 & 7 - UNDER $5',
    '06771': 'THEFT - EDIBLE ANIMALS - BURG COIN-OP MACHINE (BCOM) - $200+',
    '06772': 'THEFT - EDIBLE ANIMALS - BURG COIN-OP MACHINE (BCOM) - $50 - $199.99',
    '06773': 'THEFT - EDIBLE ANIMALS - BURG COIN-OP MACHINE (BCOM) - $20 - $49.99',
    '06774': 'THEFT - EDIBLE ANIMALS - BURG COIN-OP MACHINE (BCOM) - $5 - $19.99',
    '06775': 'THEFT - EDIBLE ANIMALS - BURG COIN-OP MACHINE (BCOM) - UNDER $5',
    '06781': 'THEFT - EDIBLE ANIMALS - CARGO THEFTS - $200+',
    '06782': 'THEFT - EDIBLE ANIMALS - CARGO THEFTS - $50 - $199.99',
    '06783': 'THEFT - EDIBLE ANIMALS - CARGO THEFTS - $20 - $49.99',
    '06784': 'THEFT - EDIBLE ANIMALS - CARGO THEFTS - $5 - $19.99',
    '06785': 'THEFT - EDIBLE ANIMALS - CARGO THEFTS - UNDER $5',
    '06791': 'THEFT - EDIBLE ANIMALS - BICYCLES (2 WHEEL/NO MOTOR) - $200+',
    '06792': 'THEFT - EDIBLE ANIMALS - BICYCLES (2 WHEEL/NO MOTOR) - $50 - $199.99',
    '06793': 'THEFT - EDIBLE ANIMALS - BICYCLES (2 WHEEL/NO MOTOR) - $20 - $49.99',
    '06794': 'THEFT - EDIBLE ANIMALS - BICYCLES (2 WHEEL/NO MOTOR) - $5 - $19.99',
    '06795': 'THEFT - EDIBLE ANIMALS - BICYCLES (2 WHEEL/NO MOTOR) - UNDER $5',
    '067A1': 'THEFT - EDIBLE ANIMALS - ATTEMPTS - $200+',
    '067A2': 'THEFT - EDIBLE ANIMALS - ATTEMPTS - $50 - $199.99',
    '067A3': 'THEFT - EDIBLE ANIMALS - ATTEMPTS - $20 - $49.99',
    '067A4': 'THEFT - EDIBLE ANIMALS - ATTEMPTS - $5 - $19.99',
    '067A5': 'THEFT - EDIBLE ANIMALS - ATTEMPTS - UNDER $5',
    '067B1': 'THEFT - EDIBLE ANIMALS - FARM EQUIPMENT - $200+',
    '067B2': 'THEFT - EDIBLE ANIMALS - FARM EQUIPMENT - $50 - $199.99',
    '067B3': 'THEFT - EDIBLE ANIMALS - FARM EQUIPMENT - $20 - $49.99',
    '067B4': 'THEFT - EDIBLE ANIMALS - FARM EQUIPMENT - $5 - $19.99',
    '067B5': 'THEFT - EDIBLE ANIMALS - FARM EQUIPMENT - UNDER $5',
    '067C1': 'THEFT - EDIBLE ANIMALS - BULLDOZER - $200+',
    '067C2': 'THEFT - EDIBLE ANIMALS - BULLDOZER - $50 - $199.99',
    '067C3': 'THEFT - EDIBLE ANIMALS - BULLDOZER - $20 - $49.99',
    '067C4': 'THEFT - EDIBLE ANIMALS - BULLDOZER - $5 - $19.99',
    '067C5': 'THEFT - EDIBLE ANIMALS - BULLDOZER - UNDER $5',
    '067D1': 'THEFT - EDIBLE ANIMALS - AIRPLANES - $200+',
    '067D2': 'THEFT - EDIBLE ANIMALS - AIRPLANES - $50 - $199.99',
    '067D3': 'THEFT - EDIBLE ANIMALS - AIRPLANES - $20 - $49.99',
    '067D4': 'THEFT - EDIBLE ANIMALS - AIRPLANES - $5 - $19.99',
    '067D5': 'THEFT - EDIBLE ANIMALS - AIRPLANES - UNDER $5',
    '067E1': 'THEFT - EDIBLE ANIMALS - CONSTRUCTION EQUIPMENT - $200+',
    '067E2': 'THEFT - EDIBLE ANIMALS - CONSTRUCTION EQUIPMENT - $50 - $199.99',
    '067E3': 'THEFT - EDIBLE ANIMALS - CONSTRUCTION EQUIPMENT - $20 - $49.99',
    '067E4': 'THEFT - EDIBLE ANIMALS - CONSTRUCTION EQUIPMENT - $5 - $19.99',
    '067E5': 'THEFT - EDIBLE ANIMALS - CONSTRUCTION EQUIPMENT - UNDER $5',
    '067F1': 'THEFT - EDIBLE ANIMALS - THEFT OF SERVICE - $200+',
    '067F2': 'THEFT - EDIBLE ANIMALS - THEFT OF SERVICE - $50 - $199.99',
    '067F3': 'THEFT - EDIBLE ANIMALS - THEFT OF SERVICE - $20 - $49.99',
    '067F4': 'THEFT - EDIBLE ANIMALS - THEFT OF SERVICE - $5 - $19.99',
    '067F5': 'THEFT - EDIBLE ANIMALS - THEFT OF SERVICE - UNDER $5',
    '067X1': 'THEFT - EDIBLE ANIMALS - THEFT OF TRADE SECRETS - $200+',
    '067X2': 'THEFT - EDIBLE ANIMALS - THEFT OF TRADE SECRETS - $50 - $199.99',
    '067X3': 'THEFT - EDIBLE ANIMALS - THEFT OF TRADE SECRETS - $20 - $49.99',
    '067X4': 'THEFT - EDIBLE ANIMALS - THEFT OF TRADE SECRETS - $5 - $19.99',
    '067X5': 'THEFT - EDIBLE ANIMALS - THEFT OF TRADE SECRETS - UNDER $5',
    '06801': 'THEFT - OTH ANIMAL(PET) - OTHERS - $200+',
    '06802': 'THEFT - OTH ANIMAL(PET) - OTHERS - $50 - $199.99',
    '06803': 'THEFT - OTH ANIMAL(PET) - OTHERS - $20 - $49.99',
    '06804': 'THEFT - OTH ANIMAL(PET) - OTHERS - $5 - $19.99',
    '06805': 'THEFT - OTH ANIMAL(PET) - OTHERS - UNDER $5',
    '06811': 'THEFT - OTH ANIMAL(PET) - PICKED POCKET - $200+',
    '06812': 'THEFT - OTH ANIMAL(PET) - PICKED POCKET - $50 - $199.99',
    '06813': 'THEFT - OTH ANIMAL(PET) - PICKED POCKET - $20 - $49.99',
    '06814': 'THEFT - OTH ANIMAL(PET) - PICKED POCKET - $5 - $19.99',
    '06815': 'THEFT - OTH ANIMAL(PET) - PICKED POCKET - UNDER $5',
    '06821': 'THEFT - OTH ANIMAL(PET) - PURSE SNATCH - $200+',
    '06822': 'THEFT - OTH ANIMAL(PET) - PURSE SNATCH - $50 - $199.99',
    '06823': 'THEFT - OTH ANIMAL(PET) - PURSE SNATCH - $20 - $49.99',
    '06824': 'THEFT - OTH ANIMAL(PET) - PURSE SNATCH - $5 - $19.99',
    '06825': 'THEFT - OTH ANIMAL(PET) - PURSE SNATCH - UNDER $5',
    '06831': 'THEFT - OTH ANIMAL(PET) - SHOPLIFT - $200+',
    '06832': 'THEFT - OTH ANIMAL(PET) - SHOPLIFT - $50 - $199.99',
    '06833': 'THEFT - OTH ANIMAL(PET) - SHOPLIFT - $20 - $49.99',
    '06834': 'THEFT - OTH ANIMAL(PET) - SHOPLIFT - $5 - $19.99',
    '06835': 'THEFT - OTH ANIMAL(PET) - SHOPLIFT - UNDER $5',
    '06841': 'THEFT - OTH ANIMAL(PET) - BMV/UNATTACH PROP INSIDE VEH - $200+',
    '06842': 'THEFT - OTH ANIMAL(PET) - BMV/UNATTACH PROP INSIDE VEH - $50 - $199.99',
    '06843': 'THEFT - OTH ANIMAL(PET) - BMV/UNATTACH PROP INSIDE VEH - $20 - $49.99',
    '06844': 'THEFT - OTH ANIMAL(PET) - BMV/UNATTACH PROP INSIDE VEH - $5 - $19.99',
    '06845': 'THEFT - OTH ANIMAL(PET) - BMV/UNATTACH PROP INSIDE VEH - UNDER $5',
    '06851': 'THEFT - OTH ANIMAL(PET) - ATTACH AUTO PARTS/ACCESSORY - $200+',
    '06852': 'THEFT - OTH ANIMAL(PET) - ATTACH AUTO PARTS/ACCESSORY - $50 - $199.99',
    '06853': 'THEFT - OTH ANIMAL(PET) - ATTACH AUTO PARTS/ACCESSORY - $20 - $49.99',
    '06854': 'THEFT - OTH ANIMAL(PET) - ATTACH AUTO PARTS/ACCESSORY - $5 - $19.99',
    '06855': 'THEFT - OTH ANIMAL(PET) - ATTACH AUTO PARTS/ACCESSORY - UNDER $5',
    '06861': 'THEFT - OTH ANIMAL(PET) - FROM BLDGS, EXCEPT 3 & 7 - $200+',
    '06862': 'THEFT - OTH ANIMAL(PET) - FROM BLDGS, EXCEPT 3 & 7 - $50 - $199.99',
    '06863': 'THEFT - OTH ANIMAL(PET) - FROM BLDGS, EXCEPT 3 & 7 - $20 - $49.99',
    '06864': 'THEFT - OTH ANIMAL(PET) - FROM BLDGS, EXCEPT 3 & 7 - $5 - $19.99',
    '06865': 'THEFT - OTH ANIMAL(PET) - FROM BLDGS, EXCEPT 3 & 7 - UNDER $5',
    '06871': 'THEFT - OTH ANIMAL(PET) - BURG COIN-OP MACHINE (BCOM) - $200+',
    '06872': 'THEFT - OTH ANIMAL(PET) - BURG COIN-OP MACHINE (BCOM) - $50 - $199.99',
    '06873': 'THEFT - OTH ANIMAL(PET) - BURG COIN-OP MACHINE (BCOM) - $20 - $49.99',
    '06874': 'THEFT - OTH ANIMAL(PET) - BURG COIN-OP MACHINE (BCOM) - $5 - $19.99',
    '06875': 'THEFT - OTH ANIMAL(PET) - BURG COIN-OP MACHINE (BCOM) - UNDER $5',
    '06881': 'THEFT - OTH ANIMAL(PET) - CARGO THEFTS - $200+',
    '06882': 'THEFT - OTH ANIMAL(PET) - CARGO THEFTS - $50 - $199.99',
    '06883': 'THEFT - OTH ANIMAL(PET) - CARGO THEFTS - $20 - $49.99',
    '06884': 'THEFT - OTH ANIMAL(PET) - CARGO THEFTS - $5 - $19.99',
    '06885': 'THEFT - OTH ANIMAL(PET) - CARGO THEFTS - UNDER $5',
    '06891': 'THEFT - OTH ANIMAL(PET) - BICYCLES (2 WHEEL/NO MOTOR) - $200+',
    '06892': 'THEFT - OTH ANIMAL(PET) - BICYCLES (2 WHEEL/NO MOTOR) - $50 - $199.99',
    '06893': 'THEFT - OTH ANIMAL(PET) - BICYCLES (2 WHEEL/NO MOTOR) - $20 - $49.99',
    '06894': 'THEFT - OTH ANIMAL(PET) - BICYCLES (2 WHEEL/NO MOTOR) - $5 - $19.99',
    '06895': 'THEFT - OTH ANIMAL(PET) - BICYCLES (2 WHEEL/NO MOTOR) - UNDER $5',
    '068A1': 'THEFT - OTH ANIMAL(PET) - ATTEMPTS - $200+',
    '068A2': 'THEFT - OTH ANIMAL(PET) - ATTEMPTS - $50 - $199.99',
    '068A3': 'THEFT - OTH ANIMAL(PET) - ATTEMPTS - $20 - $49.99',
    '068A4': 'THEFT - OTH ANIMAL(PET) - ATTEMPTS - $5 - $19.99',
    '068A5': 'THEFT - OTH ANIMAL(PET) - ATTEMPTS - UNDER $5',
    '068B1': 'THEFT - OTH ANIMAL(PET) - FARM EQUIPMENT - $200+',
    '068B2': 'THEFT - OTH ANIMAL(PET) - FARM EQUIPMENT - $50 - $199.99',
    '068B3': 'THEFT - OTH ANIMAL(PET) - FARM EQUIPMENT - $20 - $49.99',
    '068B4': 'THEFT - OTH ANIMAL(PET) - FARM EQUIPMENT - $5 - $19.99',
    '068B5': 'THEFT - OTH ANIMAL(PET) - FARM EQUIPMENT - UNDER $5',
    '068C1': 'THEFT - OTH ANIMAL(PET) - BULLDOZER - $200+',
    '068C2': 'THEFT - OTH ANIMAL(PET) - BULLDOZER - $50 - $199.99',
    '068C3': 'THEFT - OTH ANIMAL(PET) - BULLDOZER - $20 - $49.99',
    '068C4': 'THEFT - OTH ANIMAL(PET) - BULLDOZER - $5 - $19.99',
    '068C5': 'THEFT - OTH ANIMAL(PET) - BULLDOZER - UNDER $5',
    '068D1': 'THEFT - OTH ANIMAL(PET) - AIRPLANES - $200+',
    '068D2': 'THEFT - OTH ANIMAL(PET) - AIRPLANES - $50 - $199.99',
    '068D3': 'THEFT - OTH ANIMAL(PET) - AIRPLANES - $20 - $49.99',
    '068D4': 'THEFT - OTH ANIMAL(PET) - AIRPLANES - $5 - $19.99',
    '068D5': 'THEFT - OTH ANIMAL(PET) - AIRPLANES - UNDER $5',
    '068E1': 'THEFT - OTH ANIMAL(PET) - CONSTRUCTION EQUIPMENT - $200+',
    '068E2': 'THEFT - OTH ANIMAL(PET) - CONSTRUCTION EQUIPMENT - $50 - $199.99',
    '068E3': 'THEFT - OTH ANIMAL(PET) - CONSTRUCTION EQUIPMENT - $20 - $49.99',
    '068E4': 'THEFT - OTH ANIMAL(PET) - CONSTRUCTION EQUIPMENT - $5 - $19.99',
    '068E5': 'THEFT - OTH ANIMAL(PET) - CONSTRUCTION EQUIPMENT - UNDER $5',
    '068F1': 'THEFT - OTH ANIMAL(PET) - THEFT OF SERVICE - $200+',
    '068F2': 'THEFT - OTH ANIMAL(PET) - THEFT OF SERVICE - $50 - $199.99',
    '068F3': 'THEFT - OTH ANIMAL(PET) - THEFT OF SERVICE - $20 - $49.99',
    '068F4': 'THEFT - OTH ANIMAL(PET) - THEFT OF SERVICE - $5 - $19.99',
    '068F5': 'THEFT - OTH ANIMAL(PET) - THEFT OF SERVICE - UNDER $5',
    '068X1': 'THEFT - OTH ANIMAL(PET) - THEFT OF TRADE SECRETS - $200+',
    '068X2': 'THEFT - OTH ANIMAL(PET) - THEFT OF TRADE SECRETS - $50 - $199.99',
    '068X3': 'THEFT - OTH ANIMAL(PET) - THEFT OF TRADE SECRETS - $20 - $49.99',
    '068X4': 'THEFT - OTH ANIMAL(PET) - THEFT OF TRADE SECRETS - $5 - $19.99',
    '068X5': 'THEFT - OTH ANIMAL(PET) - THEFT OF TRADE SECRETS - UNDER $5',
    '06901': 'THEFT - OTH THEFTS - OTHERS - $200+',
    '06902': 'THEFT - OTH THEFTS - OTHERS - $50 - $199.99',
    '06903': 'THEFT - OTH THEFTS - OTHERS - $20 - $49.99',
    '06904': 'THEFT - OTH THEFTS - OTHERS - $5 - $19.99',
    '06905': 'THEFT - OTH THEFTS - OTHERS - UNDER $5',
    '06911': 'THEFT - OTH THEFTS - PICKED POCKET - $200+',
    '06912': 'THEFT - OTH THEFTS - PICKED POCKET - $50 - $199.99',
    '06913': 'THEFT - OTH THEFTS - PICKED POCKET - $20 - $49.99',
    '06914': 'THEFT - OTH THEFTS - PICKED POCKET - $5 - $19.99',
    '06915': 'THEFT - OTH THEFTS - PICKED POCKET - UNDER $5',
    '06921': 'THEFT - OTH THEFTS - PURSE SNATCH - $200+',
    '06922': 'THEFT - OTH THEFTS - PURSE SNATCH - $50 - $199.99',
    '06923': 'THEFT - OTH THEFTS - PURSE SNATCH - $20 - $49.99',
    '06924': 'THEFT - OTH THEFTS - PURSE SNATCH - $5 - $19.99',
    '06925': 'THEFT - OTH THEFTS - PURSE SNATCH - UNDER $5',
    '06931': 'THEFT - OTH THEFTS - SHOPLIFT - $200+',
    '06932': 'THEFT - OTH THEFTS - SHOPLIFT - $50 - $199.99',
    '06933': 'THEFT - OTH THEFTS - SHOPLIFT - $20 - $49.99',
    '06934': 'THEFT - OTH THEFTS - SHOPLIFT - $5 - $19.99',
    '06935': 'THEFT - OTH THEFTS - SHOPLIFT - UNDER $5',
    '06941': 'THEFT - OTH THEFTS - BMV/UNATTACH PROP INSIDE VEH - $200+',
    '06942': 'THEFT - OTH THEFTS - BMV/UNATTACH PROP INSIDE VEH - $50 - $199.99',
    '06943': 'THEFT - OTH THEFTS - BMV/UNATTACH PROP INSIDE VEH - $20 - $49.99',
    '06944': 'THEFT - OTH THEFTS - BMV/UNATTACH PROP INSIDE VEH - $5 - $19.99',
    '06945': 'THEFT - OTH THEFTS - BMV/UNATTACH PROP INSIDE VEH - UNDER $5',
    '06951': 'THEFT - OTH THEFTS - ATTACH AUTO PARTS/ACCESSORY - $200+',
    '06952': 'THEFT - OTH THEFTS - ATTACH AUTO PARTS/ACCESSORY - $50 - $199.99',
    '06953': 'THEFT - OTH THEFTS - ATTACH AUTO PARTS/ACCESSORY - $20 - $49.99',
    '06954': 'THEFT - OTH THEFTS - ATTACH AUTO PARTS/ACCESSORY - $5 - $19.99',
    '06955': 'THEFT - OTH THEFTS - ATTACH AUTO PARTS/ACCESSORY - UNDER $5',
    '06961': 'THEFT - OTH THEFTS - FROM BLDGS, EXCEPT 3 & 7 - $200+',
    '06962': 'THEFT - OTH THEFTS - FROM BLDGS, EXCEPT 3 & 7 - $50 - $199.99',
    '06963': 'THEFT - OTH THEFTS - FROM BLDGS, EXCEPT 3 & 7 - $20 - $49.99',
    '06964': 'THEFT - OTH THEFTS - FROM BLDGS, EXCEPT 3 & 7 - $5 - $19.99',
    '06965': 'THEFT - OTH THEFTS - FROM BLDGS, EXCEPT 3 & 7 - UNDER $5',
    '06971': 'THEFT - OTH THEFTS - BURG COIN-OP MACHINE (BCOM) - $200+',
    '06972': 'THEFT - OTH THEFTS - BURG COIN-OP MACHINE (BCOM) - $50 - $199.99',
    '06973': 'THEFT - OTH THEFTS - BURG COIN-OP MACHINE (BCOM) - $20 - $49.99',
    '06974': 'THEFT - OTH THEFTS - BURG COIN-OP MACHINE (BCOM) - $5 - $19.99',
    '06975': 'THEFT - OTH THEFTS - BURG COIN-OP MACHINE (BCOM) - UNDER $5',
    '06981': 'THEFT - OTH THEFTS - CARGO THEFTS - $200+',
    '06982': 'THEFT - OTH THEFTS - CARGO THEFTS - $50 - $199.99',
    '06983': 'THEFT - OTH THEFTS - CARGO THEFTS - $20 - $49.99',
    '06984': 'THEFT - OTH THEFTS - CARGO THEFTS - $5 - $19.99',
    '06985': 'THEFT - OTH THEFTS - CARGO THEFTS - UNDER $5',
    '06991': 'THEFT - OTH THEFTS - BICYCLES (2 WHEEL/NO MOTOR) - $200+',
    '06992': 'THEFT - OTH THEFTS - BICYCLES (2 WHEEL/NO MOTOR) - $50 - $199.99',
    '06993': 'THEFT - OTH THEFTS - BICYCLES (2 WHEEL/NO MOTOR) - $20 - $49.99',
    '06994': 'THEFT - OTH THEFTS - BICYCLES (2 WHEEL/NO MOTOR) - $5 - $19.99',
    '06995': 'THEFT - OTH THEFTS - BICYCLES (2 WHEEL/NO MOTOR) - UNDER $5',
    '069A1': 'THEFT - OTH THEFTS - ATTEMPTS - $200+',
    '069A2': 'THEFT - OTH THEFTS - ATTEMPTS - $50 - $199.99',
    '069A3': 'THEFT - OTH THEFTS - ATTEMPTS - $20 - $49.99',
    '069A4': 'THEFT - OTH THEFTS - ATTEMPTS - $5 - $19.99',
    '069A5': 'THEFT - OTH THEFTS - ATTEMPTS - UNDER $5',
    '069B1': 'THEFT - OTH THEFTS - FARM EQUIPMENT - $200+',
    '069B2': 'THEFT - OTH THEFTS - FARM EQUIPMENT - $50 - $199.99',
    '069B3': 'THEFT - OTH THEFTS - FARM EQUIPMENT - $20 - $49.99',
    '069B4': 'THEFT - OTH THEFTS - FARM EQUIPMENT - $5 - $19.99',
    '069B5': 'THEFT - OTH THEFTS - FARM EQUIPMENT - UNDER $5',
    '069C1': 'THEFT - OTH THEFTS - BULLDOZER - $200+',
    '069C2': 'THEFT - OTH THEFTS - BULLDOZER - $50 - $199.99',
    '069C3': 'THEFT - OTH THEFTS - BULLDOZER - $20 - $49.99',
    '069C4': 'THEFT - OTH THEFTS - BULLDOZER - $5 - $19.99',
    '069C5': 'THEFT - OTH THEFTS - BULLDOZER - UNDER $5',
    '069D1': 'THEFT - OTH THEFTS - AIRPLANES - $200+',
    '069D2': 'THEFT - OTH THEFTS - AIRPLANES - $50 - $199.99',
    '069D3': 'THEFT - OTH THEFTS - AIRPLANES - $20 - $49.99',
    '069D4': 'THEFT - OTH THEFTS - AIRPLANES - $5 - $19.99',
    '069D5': 'THEFT - OTH THEFTS - AIRPLANES - UNDER $5',
    '069E1': 'THEFT - OTH THEFTS - CONSTRUCTION EQUIPMENT - $200+',
    '069E2': 'THEFT - OTH THEFTS - CONSTRUCTION EQUIPMENT - $50 - $199.99',
    '069E3': 'THEFT - OTH THEFTS - CONSTRUCTION EQUIPMENT - $20 - $49.99',
    '069E4': 'THEFT - OTH THEFTS - CONSTRUCTION EQUIPMENT - $5 - $19.99',
    '069E5': 'THEFT - OTH THEFTS - CONSTRUCTION EQUIPMENT - UNDER $5',
    '069F1': 'THEFT - OTH THEFTS - THEFT OF SERVICE - $200+',
    '069F2': 'THEFT - OTH THEFTS - THEFT OF SERVICE - $50 - $199.99',
    '069F3': 'THEFT - OTH THEFTS - THEFT OF SERVICE - $20 - $49.99',
    '069F4': 'THEFT - OTH THEFTS - THEFT OF SERVICE - $5 - $19.99',
    '069F5': 'THEFT - OTH THEFTS - THEFT OF SERVICE - UNDER $5',
    '069X1': 'THEFT - OTH THEFTS - THEFT OF TRADE SECRETS - $200+',
    '069X2': 'THEFT - OTH THEFTS - THEFT OF TRADE SECRETS - $50 - $199.99',
    '069X3': 'THEFT - OTH THEFTS - THEFT OF TRADE SECRETS - $20 - $49.99',
    '069X4': 'THEFT - OTH THEFTS - THEFT OF TRADE SECRETS - $5 - $19.99',
    '069X5': 'THEFT - OTH THEFTS - THEFT OF TRADE SECRETS - UNDER $5',
    '07111': 'UNAUTHOR USE OF MOT VEH UUMV - PASS CARS - CHEVROLET - 1800-0600',
    '07112': 'UNAUTHOR USE OF MOT VEH UUMV - PASS CARS - CHEVROLET - 0600-1800',
    '07113': 'UNAUTHOR USE OF MOT VEH UUMV - PASS CARS - CHEVROLET - UNKNOWN',
    '07121': 'UNAUTHOR USE OF MOT VEH UUMV - PASS CARS - FORD - 1800-0600',
    '07122': 'UNAUTHOR USE OF MOT VEH UUMV - PASS CARS - FORD - 0600-1800',
    '07123': 'UNAUTHOR USE OF MOT VEH UUMV - PASS CARS - FORD - UNKNOWN',
    '07131': 'UNAUTHOR USE OF MOT VEH UUMV - PASS CARS - PONTIAC - 1800-0600',
    '07132': 'UNAUTHOR USE OF MOT VEH UUMV - PASS CARS - PONTIAC - 0600-1800',
    '07133': 'UNAUTHOR USE OF MOT VEH UUMV - PASS CARS - PONTIAC - UNKNOWN',
    '07141': 'UNAUTHOR USE OF MOT VEH UUMV - PASS CARS - OLDSMOBILE - 1800-0600',
    '07142': 'UNAUTHOR USE OF MOT VEH UUMV - PASS CARS - OLDSMOBILE - 0600-1800',
    '07143': 'UNAUTHOR USE OF MOT VEH UUMV - PASS CARS - OLDSMOBILE - UNKNOWN',
    '07151': 'UNAUTHOR USE OF MOT VEH UUMV - PASS CARS - PLYMOUTH - 1800-0600',
    '07152': 'UNAUTHOR USE OF MOT VEH UUMV - PASS CARS - PLYMOUTH - 0600-1800',
    '07153': 'UNAUTHOR USE OF MOT VEH UUMV - PASS CARS - PLYMOUTH - UNKNOWN',
    '07161': 'UNAUTHOR USE OF MOT VEH UUMV - PASS CARS - DODGE - 1800-0600',
    '07162': 'UNAUTHOR USE OF MOT VEH UUMV - PASS CARS - DODGE - 0600-1800',
    '07163': 'UNAUTHOR USE OF MOT VEH UUMV - PASS CARS - DODGE - UNKNOWN',
    '07171': 'UNAUTHOR USE OF MOT VEH UUMV - PASS CARS - ALL FOREIGN CARS - 1800-0600',
    '07172': 'UNAUTHOR USE OF MOT VEH UUMV - PASS CARS - ALL FOREIGN CARS - 0600-1800',
    '07173': 'UNAUTHOR USE OF MOT VEH UUMV - PASS CARS - ALL FOREIGN CARS - UNKNOWN',
    '07181': 'UNAUTHOR USE OF MOT VEH UUMV - PASS CARS - MOTORCYCLE - 1800-0600',
    '07182': 'UNAUTHOR USE OF MOT VEH UUMV - PASS CARS - MOTORCYCLE - 0600-1800',
    '07183': 'UNAUTHOR USE OF MOT VEH UUMV - PASS CARS - MOTORCYCLE - UNKNOWN',
    '07191': 'UNAUTHOR USE OF MOT VEH UUMV - PASS CARS - ALL OTHERS - 1800-0600',
    '07192': 'UNAUTHOR USE OF MOT VEH UUMV - PASS CARS - ALL OTHERS - 0600-1800',
    '07193': 'UNAUTHOR USE OF MOT VEH UUMV - PASS CARS - ALL OTHERS - UNKNOWN',
    '07211': 'UNAUTHOR USE OF MOT VEH UUMV - PICKUP TRUCKS - CHEVROLET - 1800-0600',
    '07212': 'UNAUTHOR USE OF MOT VEH UUMV - PICKUP TRUCKS - CHEVROLET - 0600-1800',
    '07213': 'UNAUTHOR USE OF MOT VEH UUMV - PICKUP TRUCKS - CHEVROLET - UNKNOWN',
    '07221': 'UNAUTHOR USE OF MOT VEH UUMV - PICKUP TRUCKS - FORD - 1800-0600',
    '07222': 'UNAUTHOR USE OF MOT VEH UUMV - PICKUP TRUCKS - FORD - 0600-1800',
    '07223': 'UNAUTHOR USE OF MOT VEH UUMV - PICKUP TRUCKS - FORD - UNKNOWN',
    '07231': 'UNAUTHOR USE OF MOT VEH UUMV - PICKUP TRUCKS - PONTIAC - 1800-0600',
    '07232': 'UNAUTHOR USE OF MOT VEH UUMV - PICKUP TRUCKS - PONTIAC - 0600-1800',
    '07233': 'UNAUTHOR USE OF MOT VEH UUMV - PICKUP TRUCKS - PONTIAC - UNKNOWN',
    '07241': 'UNAUTHOR USE OF MOT VEH UUMV - PICKUP TRUCKS - OLDSMOBILE - 1800-0600',
    '07242': 'UNAUTHOR USE OF MOT VEH UUMV - PICKUP TRUCKS - OLDSMOBILE - 0600-1800',
    '07243': 'UNAUTHOR USE OF MOT VEH UUMV - PICKUP TRUCKS - OLDSMOBILE - UNKNOWN',
    '07251': 'UNAUTHOR USE OF MOT VEH UUMV - PICKUP TRUCKS - PLYMOUTH - 1800-0600',
    '07252': 'UNAUTHOR USE OF MOT VEH UUMV - PICKUP TRUCKS - PLYMOUTH - 0600-1800',
    '07253': 'UNAUTHOR USE OF MOT VEH UUMV - PICKUP TRUCKS - PLYMOUTH - UNKNOWN',
    '07261': 'UNAUTHOR USE OF MOT VEH UUMV - PICKUP TRUCKS - DODGE - 1800-0600',
    '07262': 'UNAUTHOR USE OF MOT VEH UUMV - PICKUP TRUCKS - DODGE - 0600-1800',
    '07263': 'UNAUTHOR USE OF MOT VEH UUMV - PICKUP TRUCKS - DODGE - UNKNOWN',
    '07271': 'UNAUTHOR USE OF MOT VEH UUMV - PICKUP TRUCKS - ALL FOREIGN CARS - 1800-0600',
    '07272': 'UNAUTHOR USE OF MOT VEH UUMV - PICKUP TRUCKS - ALL FOREIGN CARS - 0600-1800',
    '07273': 'UNAUTHOR USE OF MOT VEH UUMV - PICKUP TRUCKS - ALL FOREIGN CARS - UNKNOWN',
    '07281': 'UNAUTHOR USE OF MOT VEH UUMV - PICKUP TRUCKS - MOTORCYCLE - 1800-0600',
    '07282': 'UNAUTHOR USE OF MOT VEH UUMV - PICKUP TRUCKS - MOTORCYCLE - 0600-1800',
    '07283': 'UNAUTHOR USE OF MOT VEH UUMV - PICKUP TRUCKS - MOTORCYCLE - UNKNOWN',
    '07291': 'UNAUTHOR USE OF MOT VEH UUMV - PICKUP TRUCKS - ALL OTHERS - 1800-0600',
    '07292': 'UNAUTHOR USE OF MOT VEH UUMV - PICKUP TRUCKS - ALL OTHERS - 0600-1800',
    '07293': 'UNAUTHOR USE OF MOT VEH UUMV - PICKUP TRUCKS - ALL OTHERS - UNKNOWN',
    '07311': 'UNAUTHOR USE OF MOT VEH UUMV - TRACT TRLR RIGS - CHEVROLET - 1800-0600',
    '07312': 'UNAUTHOR USE OF MOT VEH UUMV - TRACT TRLR RIGS - CHEVROLET - 0600-1800',
    '07313': 'UNAUTHOR USE OF MOT VEH UUMV - TRACT TRLR RIGS - CHEVROLET - UNKNOWN',
    '07321': 'UNAUTHOR USE OF MOT VEH UUMV - TRACT TRLR RIGS - FORD - 1800-0600',
    '07322': 'UNAUTHOR USE OF MOT VEH UUMV - TRACT TRLR RIGS - FORD - 0600-1800',
    '07323': 'UNAUTHOR USE OF MOT VEH UUMV - TRACT TRLR RIGS - FORD - UNKNOWN',
    '07331': 'UNAUTHOR USE OF MOT VEH UUMV - TRACT TRLR RIGS - PONTIAC - 1800-0600',
    '07332': 'UNAUTHOR USE OF MOT VEH UUMV - TRACT TRLR RIGS - PONTIAC - 0600-1800',
    '07333': 'UNAUTHOR USE OF MOT VEH UUMV - TRACT TRLR RIGS - PONTIAC - UNKNOWN',
    '07341': 'UNAUTHOR USE OF MOT VEH UUMV - TRACT TRLR RIGS - OLDSMOBILE - 1800-0600',
    '07342': 'UNAUTHOR USE OF MOT VEH UUMV - TRACT TRLR RIGS - OLDSMOBILE - 0600-1800',
    '07343': 'UNAUTHOR USE OF MOT VEH UUMV - TRACT TRLR RIGS - OLDSMOBILE - UNKNOWN',
    '07351': 'UNAUTHOR USE OF MOT VEH UUMV - TRACT TRLR RIGS - PLYMOUTH - 1800-0600',
    '07352': 'UNAUTHOR USE OF MOT VEH UUMV - TRACT TRLR RIGS - PLYMOUTH - 0600-1800',
    '07353': 'UNAUTHOR USE OF MOT VEH UUMV - TRACT TRLR RIGS - PLYMOUTH - UNKNOWN',
    '07361': 'UNAUTHOR USE OF MOT VEH UUMV - TRACT TRLR RIGS - DODGE - 1800-0600',
    '07362': 'UNAUTHOR USE OF MOT VEH UUMV - TRACT TRLR RIGS - DODGE - 0600-1800',
    '07363': 'UNAUTHOR USE OF MOT VEH UUMV - TRACT TRLR RIGS - DODGE - UNKNOWN',
    '07371': 'UNAUTHOR USE OF MOT VEH UUMV - TRACT TRLR RIGS - ALL FOREIGN CARS - 1800-0600',
    '07372': 'UNAUTHOR USE OF MOT VEH UUMV - TRACT TRLR RIGS - ALL FOREIGN CARS - 0600-1800',
    '07373': 'UNAUTHOR USE OF MOT VEH UUMV - TRACT TRLR RIGS - ALL FOREIGN CARS - UNKNOWN',
    '07381': 'UNAUTHOR USE OF MOT VEH UUMV - TRACT TRLR RIGS - MOTORCYCLE - 1800-0600',
    '07382': 'UNAUTHOR USE OF MOT VEH UUMV - TRACT TRLR RIGS - MOTORCYCLE - 0600-1800',
    '07383': 'UNAUTHOR USE OF MOT VEH UUMV - TRACT TRLR RIGS - MOTORCYCLE - UNKNOWN',
    '07391': 'UNAUTHOR USE OF MOT VEH UUMV - TRACT TRLR RIGS - ALL OTHERS - 1800-0600',
    '07392': 'UNAUTHOR USE OF MOT VEH UUMV - TRACT TRLR RIGS - ALL OTHERS - 0600-1800',
    '07393': 'UNAUTHOR USE OF MOT VEH UUMV - TRACT TRLR RIGS - ALL OTHERS - UNKNOWN',
    '07411': 'UNAUTHOR USE OF MOT VEH UUMV - FLTBD/BBTAIL TRK - CHEVROLET - 1800-0600',
    '07412': 'UNAUTHOR USE OF MOT VEH UUMV - FLTBD/BBTAIL TRK - CHEVROLET - 0600-1800',
    '07413': 'UNAUTHOR USE OF MOT VEH UUMV - FLTBD/BBTAIL TRK - CHEVROLET - UNKNOWN',
    '07421': 'UNAUTHOR USE OF MOT VEH UUMV - FLTBD/BBTAIL TRK - FORD - 1800-0600',
    '07422': 'UNAUTHOR USE OF MOT VEH UUMV - FLTBD/BBTAIL TRK - FORD - 0600-1800',
    '07423': 'UNAUTHOR USE OF MOT VEH UUMV - FLTBD/BBTAIL TRK - FORD - UNKNOWN',
    '07431': 'UNAUTHOR USE OF MOT VEH UUMV - FLTBD/BBTAIL TRK - PONTIAC - 1800-0600',
    '07432': 'UNAUTHOR USE OF MOT VEH UUMV - FLTBD/BBTAIL TRK - PONTIAC - 0600-1800',
    '07433': 'UNAUTHOR USE OF MOT VEH UUMV - FLTBD/BBTAIL TRK - PONTIAC - UNKNOWN',
    '07441': 'UNAUTHOR USE OF MOT VEH UUMV - FLTBD/BBTAIL TRK - OLDSMOBILE - 1800-0600',
    '07442': 'UNAUTHOR USE OF MOT VEH UUMV - FLTBD/BBTAIL TRK - OLDSMOBILE - 0600-1800',
    '07443': 'UNAUTHOR USE OF MOT VEH UUMV - FLTBD/BBTAIL TRK - OLDSMOBILE - UNKNOWN',
    '07451': 'UNAUTHOR USE OF MOT VEH UUMV - FLTBD/BBTAIL TRK - PLYMOUTH - 1800-0600',
    '07452': 'UNAUTHOR USE OF MOT VEH UUMV - FLTBD/BBTAIL TRK - PLYMOUTH - 0600-1800',
    '07453': 'UNAUTHOR USE OF MOT VEH UUMV - FLTBD/BBTAIL TRK - PLYMOUTH - UNKNOWN',
    '07461': 'UNAUTHOR USE OF MOT VEH UUMV - FLTBD/BBTAIL TRK - DODGE - 1800-0600',
    '07462': 'UNAUTHOR USE OF MOT VEH UUMV - FLTBD/BBTAIL TRK - DODGE - 0600-1800',
    '07463': 'UNAUTHOR USE OF MOT VEH UUMV - FLTBD/BBTAIL TRK - DODGE - UNKNOWN',
    '07471': 'UNAUTHOR USE OF MOT VEH UUMV - FLTBD/BBTAIL TRK - ALL FOREIGN CARS - 1800-0600',
    '07472': 'UNAUTHOR USE OF MOT VEH UUMV - FLTBD/BBTAIL TRK - ALL FOREIGN CARS - 0600-1800',
    '07473': 'UNAUTHOR USE OF MOT VEH UUMV - FLTBD/BBTAIL TRK - ALL FOREIGN CARS - UNKNOWN',
    '07481': 'UNAUTHOR USE OF MOT VEH UUMV - FLTBD/BBTAIL TRK - MOTORCYCLE - 1800-0600',
    '07482': 'UNAUTHOR USE OF MOT VEH UUMV - FLTBD/BBTAIL TRK - MOTORCYCLE - 0600-1800',
    '07483': 'UNAUTHOR USE OF MOT VEH UUMV - FLTBD/BBTAIL TRK - MOTORCYCLE - UNKNOWN',
    '07491': 'UNAUTHOR USE OF MOT VEH UUMV - FLTBD/BBTAIL TRK - ALL OTHERS - 1800-0600',
    '07492': 'UNAUTHOR USE OF MOT VEH UUMV - FLTBD/BBTAIL TRK - ALL OTHERS - 0600-1800',
    '07493': 'UNAUTHOR USE OF MOT VEH UUMV - FLTBD/BBTAIL TRK - ALL OTHERS - UNKNOWN',
    '07511': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - CHEVROLET - 1800-0600',
    '07512': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - CHEVROLET - 0600-1800',
    '07513': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - CHEVROLET - UNKNOWN',
    '07521': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - FORD - 1800-0600',
    '07522': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - FORD - 0600-1800',
    '07523': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - FORD - UNKNOWN',
    '07531': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - PONTIAC - 1800-0600',
    '07532': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - PONTIAC - 0600-1800',
    '07533': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - PONTIAC - UNKNOWN',
    '07541': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - OLDSMOBILE - 1800-0600',
    '07542': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - OLDSMOBILE - 0600-1800',
    '07543': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - OLDSMOBILE - UNKNOWN',
    '07551': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - PLYMOUTH - 1800-0600',
    '07552': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - PLYMOUTH - 0600-1800',
    '07553': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - PLYMOUTH - UNKNOWN',
    '07561': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - DODGE - 1800-0600',
    '07562': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - DODGE - 0600-1800',
    '07563': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - DODGE - UNKNOWN',
    '07571': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - ALL FOREIGN CARS - 1800-0600',
    '07572': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - ALL FOREIGN CARS - 0600-1800',
    '07573': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - ALL FOREIGN CARS - UNKNOWN',
    '07581': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - MOTORCYCLE - 1800-0600',
    '07582': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - MOTORCYCLE - 0600-1800',
    '07583': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - MOTORCYCLE - UNKNOWN',
    '07591': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - ALL OTHERS - 1800-0600',
    '07592': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - ALL OTHERS - 0600-1800',
    '07593': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - ALL OTHERS - UNKNOWN',
    '07611': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - CHEVROLET - 1800-0600',
    '07612': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - CHEVROLET - 0600-1800',
    '07613': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - CHEVROLET - UNKNOWN',
    '07621': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - FORD - 1800-0600',
    '07622': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - FORD - 0600-1800',
    '07623': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - FORD - UNKNOWN',
    '07631': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - PONTIAC - 1800-0600',
    '07632': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - PONTIAC - 0600-1800',
    '07633': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - PONTIAC - UNKNOWN',
    '07641': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - OLDSMOBILE - 1800-0600',
    '07642': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - OLDSMOBILE - 0600-1800',
    '07643': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - OLDSMOBILE - UNKNOWN',
    '07651': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - PLYMOUTH - 1800-0600',
    '07652': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - PLYMOUTH - 0600-1800',
    '07653': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - PLYMOUTH - UNKNOWN',
    '07661': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - DODGE - 1800-0600',
    '07662': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - DODGE - 0600-1800',
    '07663': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - DODGE - UNKNOWN',
    '07671': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - ALL FOREIGN CARS - 1800-0600',
    '07672': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - ALL FOREIGN CARS - 0600-1800',
    '07673': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - ALL FOREIGN CARS - UNKNOWN',
    '07681': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - MOTORCYCLE - 1800-0600',
    '07682': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - MOTORCYCLE - 0600-1800',
    '07683': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - MOTORCYCLE - UNKNOWN',
    '07691': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - ALL OTHERS - 1800-0600',
    '07692': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - ALL OTHERS - 0600-1800',
    '07693': 'UNAUTHOR USE OF MOT VEH UUMV - **NOT USED** - ALL OTHERS - UNKNOWN',
    '07711': 'UNAUTHOR USE OF MOT VEH UUMV - MOT HOME(RV\'S) - CHEVROLET - 1800-0600',
    '07712': 'UNAUTHOR USE OF MOT VEH UUMV - MOT HOME(RV\'S) - CHEVROLET - 0600-1800',
    '07713': 'UNAUTHOR USE OF MOT VEH UUMV - MOT HOME(RV\'S) - CHEVROLET - UNKNOWN',
    '07721': 'UNAUTHOR USE OF MOT VEH UUMV - MOT HOME(RV\'S) - FORD - 1800-0600',
    '07722': 'UNAUTHOR USE OF MOT VEH UUMV - MOT HOME(RV\'S) - FORD - 0600-1800',
    '07723': 'UNAUTHOR USE OF MOT VEH UUMV - MOT HOME(RV\'S) - FORD - UNKNOWN',
    '07731': 'UNAUTHOR USE OF MOT VEH UUMV - MOT HOME(RV\'S) - PONTIAC - 1800-0600',
    '07732': 'UNAUTHOR USE OF MOT VEH UUMV - MOT HOME(RV\'S) - PONTIAC - 0600-1800',
    '07733': 'UNAUTHOR USE OF MOT VEH UUMV - MOT HOME(RV\'S) - PONTIAC - UNKNOWN',
    '07741': 'UNAUTHOR USE OF MOT VEH UUMV - MOT HOME(RV\'S) - OLDSMOBILE - 1800-0600',
    '07742': 'UNAUTHOR USE OF MOT VEH UUMV - MOT HOME(RV\'S) - OLDSMOBILE - 0600-1800',
    '07743': 'UNAUTHOR USE OF MOT VEH UUMV - MOT HOME(RV\'S) - OLDSMOBILE - UNKNOWN',
    '07751': 'UNAUTHOR USE OF MOT VEH UUMV - MOT HOME(RV\'S) - PLYMOUTH - 1800-0600',
    '07752': 'UNAUTHOR USE OF MOT VEH UUMV - MOT HOME(RV\'S) - PLYMOUTH - 0600-1800',
    '07753': 'UNAUTHOR USE OF MOT VEH UUMV - MOT HOME(RV\'S) - PLYMOUTH - UNKNOWN',
    '07761': 'UNAUTHOR USE OF MOT VEH UUMV - MOT HOME(RV\'S) - DODGE - 1800-0600',
    '07762': 'UNAUTHOR USE OF MOT VEH UUMV - MOT HOME(RV\'S) - DODGE - 0600-1800',
    '07763': 'UNAUTHOR USE OF MOT VEH UUMV - MOT HOME(RV\'S) - DODGE - UNKNOWN',
    '07771': 'UNAUTHOR USE OF MOT VEH UUMV - MOT HOME(RV\'S) - ALL FOREIGN CARS - 1800-0600',
    '07772': 'UNAUTHOR USE OF MOT VEH UUMV - MOT HOME(RV\'S) - ALL FOREIGN CARS - 0600-1800',
    '07773': 'UNAUTHOR USE OF MOT VEH UUMV - MOT HOME(RV\'S) - ALL FOREIGN CARS - UNKNOWN',
    '07781': 'UNAUTHOR USE OF MOT VEH UUMV - MOT HOME(RV\'S) - MOTORCYCLE - 1800-0600',
    '07782': 'UNAUTHOR USE OF MOT VEH UUMV - MOT HOME(RV\'S) - MOTORCYCLE - 0600-1800',
    '07783': 'UNAUTHOR USE OF MOT VEH UUMV - MOT HOME(RV\'S) - MOTORCYCLE - UNKNOWN',
    '07791': 'UNAUTHOR USE OF MOT VEH UUMV - MOT HOME(RV\'S) - ALL OTHERS - 1800-0600',
    '07792': 'UNAUTHOR USE OF MOT VEH UUMV - MOT HOME(RV\'S) - ALL OTHERS - 0600-1800',
    '07793': 'UNAUTHOR USE OF MOT VEH UUMV - MOT HOME(RV\'S) - ALL OTHERS - UNKNOWN',
    '07811': 'UNAUTHOR USE OF MOT VEH UUMV - M/CYC /MINIBIKE - CHEVROLET - 1800-0600',
    '07812': 'UNAUTHOR USE OF MOT VEH UUMV - M/CYC /MINIBIKE - CHEVROLET - 0600-1800',
    '07813': 'UNAUTHOR USE OF MOT VEH UUMV - M/CYC /MINIBIKE - CHEVROLET - UNKNOWN',
    '07821': 'UNAUTHOR USE OF MOT VEH UUMV - M/CYC /MINIBIKE - FORD - 1800-0600',
    '07822': 'UNAUTHOR USE OF MOT VEH UUMV - M/CYC /MINIBIKE - FORD - 0600-1800',
    '07823': 'UNAUTHOR USE OF MOT VEH UUMV - M/CYC /MINIBIKE - FORD - UNKNOWN',
    '07831': 'UNAUTHOR USE OF MOT VEH UUMV - M/CYC /MINIBIKE - PONTIAC - 1800-0600',
    '07832': 'UNAUTHOR USE OF MOT VEH UUMV - M/CYC /MINIBIKE - PONTIAC - 0600-1800',
    '07833': 'UNAUTHOR USE OF MOT VEH UUMV - M/CYC /MINIBIKE - PONTIAC - UNKNOWN',
    '07841': 'UNAUTHOR USE OF MOT VEH UUMV - M/CYC /MINIBIKE - OLDSMOBILE - 1800-0600',
    '07842': 'UNAUTHOR USE OF MOT VEH UUMV - M/CYC /MINIBIKE - OLDSMOBILE - 0600-1800',
    '07843': 'UNAUTHOR USE OF MOT VEH UUMV - M/CYC /MINIBIKE - OLDSMOBILE - UNKNOWN',
    '07851': 'UNAUTHOR USE OF MOT VEH UUMV - M/CYC /MINIBIKE - PLYMOUTH - 1800-0600',
    '07852': 'UNAUTHOR USE OF MOT VEH UUMV - M/CYC /MINIBIKE - PLYMOUTH - 0600-1800',
    '07853': 'UNAUTHOR USE OF MOT VEH UUMV - M/CYC /MINIBIKE - PLYMOUTH - UNKNOWN',
    '07861': 'UNAUTHOR USE OF MOT VEH UUMV - M/CYC /MINIBIKE - DODGE - 1800-0600',
    '07862': 'UNAUTHOR USE OF MOT VEH UUMV - M/CYC /MINIBIKE - DODGE - 0600-1800',
    '07863': 'UNAUTHOR USE OF MOT VEH UUMV - M/CYC /MINIBIKE - DODGE - UNKNOWN',
    '07871': 'UNAUTHOR USE OF MOT VEH UUMV - M/CYC /MINIBIKE - ALL FOREIGN CARS - 1800-0600',
    '07872': 'UNAUTHOR USE OF MOT VEH UUMV - M/CYC /MINIBIKE - ALL FOREIGN CARS - 0600-1800',
    '07873': 'UNAUTHOR USE OF MOT VEH UUMV - M/CYC /MINIBIKE - ALL FOREIGN CARS - UNKNOWN',
    '07881': 'UNAUTHOR USE OF MOT VEH UUMV - M/CYC /MINIBIKE - MOTORCYCLE - 1800-0600',
    '07882': 'UNAUTHOR USE OF MOT VEH UUMV - M/CYC /MINIBIKE - MOTORCYCLE - 0600-1800',
    '07883': 'UNAUTHOR USE OF MOT VEH UUMV - M/CYC /MINIBIKE - MOTORCYCLE - UNKNOWN',
    '07891': 'UNAUTHOR USE OF MOT VEH UUMV - M/CYC /MINIBIKE - ALL OTHERS - 1800-0600',
    '07892': 'UNAUTHOR USE OF MOT VEH UUMV - M/CYC /MINIBIKE - ALL OTHERS - 0600-1800',
    '07893': 'UNAUTHOR USE OF MOT VEH UUMV - M/CYC /MINIBIKE - ALL OTHERS - UNKNOWN',
    '07911': 'UNAUTHOR USE OF MOT VEH UUMV - BUS(PASS VANS) - CHEVROLET - 1800-0600',
    '07912': 'UNAUTHOR USE OF MOT VEH UUMV - BUS(PASS VANS) - CHEVROLET - 0600-1800',
    '07913': 'UNAUTHOR USE OF MOT VEH UUMV - BUS(PASS VANS) - CHEVROLET - UNKNOWN',
    '07921': 'UNAUTHOR USE OF MOT VEH UUMV - BUS(PASS VANS) - FORD - 1800-0600',
    '07922': 'UNAUTHOR USE OF MOT VEH UUMV - BUS(PASS VANS) - FORD - 0600-1800',
    '07923': 'UNAUTHOR USE OF MOT VEH UUMV - BUS(PASS VANS) - FORD - UNKNOWN',
    '07931': 'UNAUTHOR USE OF MOT VEH UUMV - BUS(PASS VANS) - PONTIAC - 1800-0600',
    '07932': 'UNAUTHOR USE OF MOT VEH UUMV - BUS(PASS VANS) - PONTIAC - 0600-1800',
    '07933': 'UNAUTHOR USE OF MOT VEH UUMV - BUS(PASS VANS) - PONTIAC - UNKNOWN',
    '07941': 'UNAUTHOR USE OF MOT VEH UUMV - BUS(PASS VANS) - OLDSMOBILE - 1800-0600',
    '07942': 'UNAUTHOR USE OF MOT VEH UUMV - BUS(PASS VANS) - OLDSMOBILE - 0600-1800',
    '07943': 'UNAUTHOR USE OF MOT VEH UUMV - BUS(PASS VANS) - OLDSMOBILE - UNKNOWN',
    '07951': 'UNAUTHOR USE OF MOT VEH UUMV - BUS(PASS VANS) - PLYMOUTH - 1800-0600',
    '07952': 'UNAUTHOR USE OF MOT VEH UUMV - BUS(PASS VANS) - PLYMOUTH - 0600-1800',
    '07953': 'UNAUTHOR USE OF MOT VEH UUMV - BUS(PASS VANS) - PLYMOUTH - UNKNOWN',
    '07961': 'UNAUTHOR USE OF MOT VEH UUMV - BUS(PASS VANS) - DODGE - 1800-0600',
    '07962': 'UNAUTHOR USE OF MOT VEH UUMV - BUS(PASS VANS) - DODGE - 0600-1800',
    '07963': 'UNAUTHOR USE OF MOT VEH UUMV - BUS(PASS VANS) - DODGE - UNKNOWN',
    '07971': 'UNAUTHOR USE OF MOT VEH UUMV - BUS(PASS VANS) - ALL FOREIGN CARS - 1800-0600',
    '07972': 'UNAUTHOR USE OF MOT VEH UUMV - BUS(PASS VANS) - ALL FOREIGN CARS - 0600-1800',
    '07973': 'UNAUTHOR USE OF MOT VEH UUMV - BUS(PASS VANS) - ALL FOREIGN CARS - UNKNOWN',
    '07981': 'UNAUTHOR USE OF MOT VEH UUMV - BUS(PASS VANS) - MOTORCYCLE - 1800-0600',
    '07982': 'UNAUTHOR USE OF MOT VEH UUMV - BUS(PASS VANS) - MOTORCYCLE - 0600-1800',
    '07983': 'UNAUTHOR USE OF MOT VEH UUMV - BUS(PASS VANS) - MOTORCYCLE - UNKNOWN',
    '07991': 'UNAUTHOR USE OF MOT VEH UUMV - BUS(PASS VANS) - ALL OTHERS - 1800-0600',
    '07992': 'UNAUTHOR USE OF MOT VEH UUMV - BUS(PASS VANS) - ALL OTHERS - 0600-1800',
    '07993': 'UNAUTHOR USE OF MOT VEH UUMV - BUS(PASS VANS) - ALL OTHERS - UNKNOWN',
    '07A11': 'UNAUTHOR USE OF MOT VEH UUMV - ATTEMPTS - CHEVROLET - 1800-0600',
    '07A12': 'UNAUTHOR USE OF MOT VEH UUMV - ATTEMPTS - CHEVROLET - 0600-1800',
    '07A13': 'UNAUTHOR USE OF MOT VEH UUMV - ATTEMPTS - CHEVROLET - UNKNOWN',
    '07A21': 'UNAUTHOR USE OF MOT VEH UUMV - ATTEMPTS - FORD - 1800-0600',
    '07A22': 'UNAUTHOR USE OF MOT VEH UUMV - ATTEMPTS - FORD - 0600-1800',
    '07A23': 'UNAUTHOR USE OF MOT VEH UUMV - ATTEMPTS - FORD - UNKNOWN',
    '07A31': 'UNAUTHOR USE OF MOT VEH UUMV - ATTEMPTS - PONTIAC - 1800-0600',
    '07A32': 'UNAUTHOR USE OF MOT VEH UUMV - ATTEMPTS - PONTIAC - 0600-1800',
    '07A33': 'UNAUTHOR USE OF MOT VEH UUMV - ATTEMPTS - PONTIAC - UNKNOWN',
    '07A41': 'UNAUTHOR USE OF MOT VEH UUMV - ATTEMPTS - OLDSMOBILE - 1800-0600',
    '07A42': 'UNAUTHOR USE OF MOT VEH UUMV - ATTEMPTS - OLDSMOBILE - 0600-1800',
    '07A43': 'UNAUTHOR USE OF MOT VEH UUMV - ATTEMPTS - OLDSMOBILE - UNKNOWN',
    '07A51': 'UNAUTHOR USE OF MOT VEH UUMV - ATTEMPTS - PLYMOUTH - 1800-0600',
    '07A52': 'UNAUTHOR USE OF MOT VEH UUMV - ATTEMPTS - PLYMOUTH - 0600-1800',
    '07A53': 'UNAUTHOR USE OF MOT VEH UUMV - ATTEMPTS - PLYMOUTH - UNKNOWN',
    '07A61': 'UNAUTHOR USE OF MOT VEH UUMV - ATTEMPTS - DODGE - 1800-0600',
    '07A62': 'UNAUTHOR USE OF MOT VEH UUMV - ATTEMPTS - DODGE - 0600-1800',
    '07A63': 'UNAUTHOR USE OF MOT VEH UUMV - ATTEMPTS - DODGE - UNKNOWN',
    '07A71': 'UNAUTHOR USE OF MOT VEH UUMV - ATTEMPTS - ALL FOREIGN CARS - 1800-0600',
    '07A72': 'UNAUTHOR USE OF MOT VEH UUMV - ATTEMPTS - ALL FOREIGN CARS - 0600-1800',
    '07A73': 'UNAUTHOR USE OF MOT VEH UUMV - ATTEMPTS - ALL FOREIGN CARS - UNKNOWN',
    '07A81': 'UNAUTHOR USE OF MOT VEH UUMV - ATTEMPTS - MOTORCYCLE - 1800-0600',
    '07A82': 'UNAUTHOR USE OF MOT VEH UUMV - ATTEMPTS - MOTORCYCLE - 0600-1800',
    '07A83': 'UNAUTHOR USE OF MOT VEH UUMV - ATTEMPTS - MOTORCYCLE - UNKNOWN',
    '07A91': 'UNAUTHOR USE OF MOT VEH UUMV - ATTEMPTS - ALL OTHERS - 1800-0600',
    '07A92': 'UNAUTHOR USE OF MOT VEH UUMV - ATTEMPTS - ALL OTHERS - 0600-1800',
    '07A93': 'UNAUTHOR USE OF MOT VEH UUMV - ATTEMPTS - ALL OTHERS - UNKNOWN',
    '07B11': 'UNAUTHOR USE OF MOT VEH UUMV - DUNEBUG,GOCRT - CHEVROLET - 1800-0600',
    '07B12': 'UNAUTHOR USE OF MOT VEH UUMV - DUNEBUG,GOCRT - CHEVROLET - 0600-1800',
    '07B13': 'UNAUTHOR USE OF MOT VEH UUMV - DUNEBUG,GOCRT - CHEVROLET - UNKNOWN',
    '07B21': 'UNAUTHOR USE OF MOT VEH UUMV - DUNEBUG,GOCRT - FORD - 1800-0600',
    '07B22': 'UNAUTHOR USE OF MOT VEH UUMV - DUNEBUG,GOCRT - FORD - 0600-1800',
    '07B23': 'UNAUTHOR USE OF MOT VEH UUMV - DUNEBUG,GOCRT - FORD - UNKNOWN',
    '07B31': 'UNAUTHOR USE OF MOT VEH UUMV - DUNEBUG,GOCRT - PONTIAC - 1800-0600',
    '07B32': 'UNAUTHOR USE OF MOT VEH UUMV - DUNEBUG,GOCRT - PONTIAC - 0600-1800',
    '07B33': 'UNAUTHOR USE OF MOT VEH UUMV - DUNEBUG,GOCRT - PONTIAC - UNKNOWN',
    '07B41': 'UNAUTHOR USE OF MOT VEH UUMV - DUNEBUG,GOCRT - OLDSMOBILE - 1800-0600',
    '07B42': 'UNAUTHOR USE OF MOT VEH UUMV - DUNEBUG,GOCRT - OLDSMOBILE - 0600-1800',
    '07B43': 'UNAUTHOR USE OF MOT VEH UUMV - DUNEBUG,GOCRT - OLDSMOBILE - UNKNOWN',
    '07B51': 'UNAUTHOR USE OF MOT VEH UUMV - DUNEBUG,GOCRT - PLYMOUTH - 1800-0600',
    '07B52': 'UNAUTHOR USE OF MOT VEH UUMV - DUNEBUG,GOCRT - PLYMOUTH - 0600-1800',
    '07B53': 'UNAUTHOR USE OF MOT VEH UUMV - DUNEBUG,GOCRT - PLYMOUTH - UNKNOWN',
    '07B61': 'UNAUTHOR USE OF MOT VEH UUMV - DUNEBUG,GOCRT - DODGE - 1800-0600',
    '07B62': 'UNAUTHOR USE OF MOT VEH UUMV - DUNEBUG,GOCRT - DODGE - 0600-1800',
    '07B63': 'UNAUTHOR USE OF MOT VEH UUMV - DUNEBUG,GOCRT - DODGE - UNKNOWN',
    '07B71': 'UNAUTHOR USE OF MOT VEH UUMV - DUNEBUG,GOCRT - ALL FOREIGN CARS - 1800-0600',
    '07B72': 'UNAUTHOR USE OF MOT VEH UUMV - DUNEBUG,GOCRT - ALL FOREIGN CARS - 0600-1800',
    '07B73': 'UNAUTHOR USE OF MOT VEH UUMV - DUNEBUG,GOCRT - ALL FOREIGN CARS - UNKNOWN',
    '07B81': 'UNAUTHOR USE OF MOT VEH UUMV - DUNEBUG,GOCRT - MOTORCYCLE - 1800-0600',
    '07B82': 'UNAUTHOR USE OF MOT VEH UUMV - DUNEBUG,GOCRT - MOTORCYCLE - 0600-1800',
    '07B83': 'UNAUTHOR USE OF MOT VEH UUMV - DUNEBUG,GOCRT - MOTORCYCLE - UNKNOWN',
    '07B91': 'UNAUTHOR USE OF MOT VEH UUMV - DUNEBUG,GOCRT - ALL OTHERS - 1800-0600',
    '07B92': 'UNAUTHOR USE OF MOT VEH UUMV - DUNEBUG,GOCRT - ALL OTHERS - 0600-1800',
    '07B93': 'UNAUTHOR USE OF MOT VEH UUMV - DUNEBUG,GOCRT - ALL OTHERS - UNKNOWN',
    '08001': 'ASSAULT - FIRE PERSON - AIDING SUICIDE (NO SBI OR DEATH) - 1800-0600',
    '08002': 'ASSAULT - FIRE PERSON - AIDING SUICIDE (NO SBI OR DEATH) - 0600-1800',
    '08003': 'ASSAULT - FIRE PERSON - AIDING SUICIDE (NO SBI OR DEATH) - UNKNOWN',
    '08011': 'ASSAULT - FIRE PERSON - OFFENSE CONTACT - 1800-0600',
    '08012': 'ASSAULT - FIRE PERSON - OFFENSE CONTACT - 0600-1800',
    '08013': 'ASSAULT - FIRE PERSON - OFFENSE CONTACT - UNKNOWN',
    '08021': 'ASSAULT - FIRE PERSON - CAUSE PAIN, INJURY (M/A) - 1800-0600',
    '08022': 'ASSAULT - FIRE PERSON - CAUSE PAIN, INJURY (M/A) - 0600-1800',
    '08023': 'ASSAULT - FIRE PERSON - CAUSE PAIN, INJURY (M/A) - UNKNOWN',
    '08031': 'ASSAULT - FIRE PERSON - TAMP W/CONSUM PROD (NON-AGG) - 1800-0600',
    '08032': 'ASSAULT - FIRE PERSON - TAMP W/CONSUM PROD (NON-AGG) - 0600-1800',
    '08033': 'ASSAULT - FIRE PERSON - TAMP W/CONSUM PROD (NON-AGG) - UNKNOWN',
    '08041': 'ASSAULT - FIRE PERSON - EXHIBITION OF FIREARM - 1800-0600',
    '08042': 'ASSAULT - FIRE PERSON - EXHIBITION OF FIREARM - 0600-1800',
    '08043': 'ASSAULT - FIRE PERSON - EXHIBITION OF FIREARM - UNKNOWN',
    '08051': 'ASSAULT - FIRE PERSON - DEADLY CONDUCT - 1800-0600',
    '08052': 'ASSAULT - FIRE PERSON - DEADLY CONDUCT - 0600-1800',
    '08053': 'ASSAULT - FIRE PERSON - DEADLY CONDUCT - UNKNOWN',
    '08061': 'ASSAULT - FIRE PERSON - WITH MOTOR VEHICLE - 1800-0600',
    '08062': 'ASSAULT - FIRE PERSON - WITH MOTOR VEHICLE - 0600-1800',
    '08063': 'ASSAULT - FIRE PERSON - WITH MOTOR VEHICLE - UNKNOWN',
    '08071': 'ASSAULT - FIRE PERSON - **NOT USED** - 1800-0600',
    '08072': 'ASSAULT - FIRE PERSON - **NOT USED** - 0600-1800',
    '08073': 'ASSAULT - FIRE PERSON - **NOT USED** - UNKNOWN',
    '08081': 'ASSAULT - FIRE PERSON - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - 1800-0600',
    '08082': 'ASSAULT - FIRE PERSON - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - 0600-1800',
    '08083': 'ASSAULT - FIRE PERSON - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - UNKNOWN',
    '08091': 'ASSAULT - FIRE PERSON - VERBAL THREAT/ABUSE LANGUAGE - 1800-0600',
    '08092': 'ASSAULT - FIRE PERSON - VERBAL THREAT/ABUSE LANGUAGE - 0600-1800',
    '08093': 'ASSAULT - FIRE PERSON - VERBAL THREAT/ABUSE LANGUAGE - UNKNOWN',
    '08101': 'ASSAULT - ADULT W/M - AIDING SUICIDE (NO SBI OR DEATH) - 1800-0600',
    '08102': 'ASSAULT - ADULT W/M - AIDING SUICIDE (NO SBI OR DEATH) - 0600-1800',
    '08103': 'ASSAULT - ADULT W/M - AIDING SUICIDE (NO SBI OR DEATH) - UNKNOWN',
    '08111': 'ASSAULT - ADULT W/M - OFFENSE CONTACT - 1800-0600',
    '08112': 'ASSAULT - ADULT W/M - OFFENSE CONTACT - 0600-1800',
    '08113': 'ASSAULT - ADULT W/M - OFFENSE CONTACT - UNKNOWN',
    '08121': 'ASSAULT - ADULT W/M - CAUSE PAIN, INJURY (M/A) - 1800-0600',
    '08122': 'ASSAULT - ADULT W/M - CAUSE PAIN, INJURY (M/A) - 0600-1800',
    '08123': 'ASSAULT - ADULT W/M - CAUSE PAIN, INJURY (M/A) - UNKNOWN',
    '08131': 'ASSAULT - ADULT W/M - TAMP W/CONSUM PROD (NON-AGG) - 1800-0600',
    '08132': 'ASSAULT - ADULT W/M - TAMP W/CONSUM PROD (NON-AGG) - 0600-1800',
    '08133': 'ASSAULT - ADULT W/M - TAMP W/CONSUM PROD (NON-AGG) - UNKNOWN',
    '08141': 'ASSAULT - ADULT W/M - EXHIBITION OF FIREARM - 1800-0600',
    '08142': 'ASSAULT - ADULT W/M - EXHIBITION OF FIREARM - 0600-1800',
    '08143': 'ASSAULT - ADULT W/M - EXHIBITION OF FIREARM - UNKNOWN',
    '08151': 'ASSAULT - ADULT W/M - DEADLY CONDUCT - 1800-0600',
    '08152': 'ASSAULT - ADULT W/M - DEADLY CONDUCT - 0600-1800',
    '08153': 'ASSAULT - ADULT W/M - DEADLY CONDUCT - UNKNOWN',
    '08161': 'ASSAULT - ADULT W/M - WITH MOTOR VEHICLE - 1800-0600',
    '08162': 'ASSAULT - ADULT W/M - WITH MOTOR VEHICLE - 0600-1800',
    '08163': 'ASSAULT - ADULT W/M - WITH MOTOR VEHICLE - UNKNOWN',
    '08171': 'ASSAULT - ADULT W/M - **NOT USED** - 1800-0600',
    '08172': 'ASSAULT - ADULT W/M - **NOT USED** - 0600-1800',
    '08173': 'ASSAULT - ADULT W/M - **NOT USED** - UNKNOWN',
    '08181': 'ASSAULT - ADULT W/M - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - 1800-0600',
    '08182': 'ASSAULT - ADULT W/M - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - 0600-1800',
    '08183': 'ASSAULT - ADULT W/M - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - UNKNOWN',
    '08191': 'ASSAULT - ADULT W/M - VERBAL THREAT/ABUSE LANGUAGE - 1800-0600',
    '08192': 'ASSAULT - ADULT W/M - VERBAL THREAT/ABUSE LANGUAGE - 0600-1800',
    '08193': 'ASSAULT - ADULT W/M - VERBAL THREAT/ABUSE LANGUAGE - UNKNOWN',
    '08201': 'ASSAULT - ADULT B/M - AIDING SUICIDE (NO SBI OR DEATH) - 1800-0600',
    '08202': 'ASSAULT - ADULT B/M - AIDING SUICIDE (NO SBI OR DEATH) - 0600-1800',
    '08203': 'ASSAULT - ADULT B/M - AIDING SUICIDE (NO SBI OR DEATH) - UNKNOWN',
    '08211': 'ASSAULT - ADULT B/M - OFFENSE CONTACT - 1800-0600',
    '08212': 'ASSAULT - ADULT B/M - OFFENSE CONTACT - 0600-1800',
    '08213': 'ASSAULT - ADULT B/M - OFFENSE CONTACT - UNKNOWN',
    '08221': 'ASSAULT - ADULT B/M - CAUSE PAIN, INJURY (M/A) - 1800-0600',
    '08222': 'ASSAULT - ADULT B/M - CAUSE PAIN, INJURY (M/A) - 0600-1800',
    '08223': 'ASSAULT - ADULT B/M - CAUSE PAIN, INJURY (M/A) - UNKNOWN',
    '08231': 'ASSAULT - ADULT B/M - TAMP W/CONSUM PROD (NON-AGG) - 1800-0600',
    '08232': 'ASSAULT - ADULT B/M - TAMP W/CONSUM PROD (NON-AGG) - 0600-1800',
    '08233': 'ASSAULT - ADULT B/M - TAMP W/CONSUM PROD (NON-AGG) - UNKNOWN',
    '08241': 'ASSAULT - ADULT B/M - EXHIBITION OF FIREARM - 1800-0600',
    '08242': 'ASSAULT - ADULT B/M - EXHIBITION OF FIREARM - 0600-1800',
    '08243': 'ASSAULT - ADULT B/M - EXHIBITION OF FIREARM - UNKNOWN',
    '08251': 'ASSAULT - ADULT B/M - DEADLY CONDUCT - 1800-0600',
    '08252': 'ASSAULT - ADULT B/M - DEADLY CONDUCT - 0600-1800',
    '08253': 'ASSAULT - ADULT B/M - DEADLY CONDUCT - UNKNOWN',
    '08261': 'ASSAULT - ADULT B/M - WITH MOTOR VEHICLE - 1800-0600',
    '08262': 'ASSAULT - ADULT B/M - WITH MOTOR VEHICLE - 0600-1800',
    '08263': 'ASSAULT - ADULT B/M - WITH MOTOR VEHICLE - UNKNOWN',
    '08271': 'ASSAULT - ADULT B/M - **NOT USED** - 1800-0600',
    '08272': 'ASSAULT - ADULT B/M - **NOT USED** - 0600-1800',
    '08273': 'ASSAULT - ADULT B/M - **NOT USED** - UNKNOWN',
    '08281': 'ASSAULT - ADULT B/M - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - 1800-0600',
    '08282': 'ASSAULT - ADULT B/M - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - 0600-1800',
    '08283': 'ASSAULT - ADULT B/M - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - UNKNOWN',
    '08291': 'ASSAULT - ADULT B/M - VERBAL THREAT/ABUSE LANGUAGE - 1800-0600',
    '08292': 'ASSAULT - ADULT B/M - VERBAL THREAT/ABUSE LANGUAGE - 0600-1800',
    '08293': 'ASSAULT - ADULT B/M - VERBAL THREAT/ABUSE LANGUAGE - UNKNOWN',
    '08301': 'ASSAULT - ADULT W/F - AIDING SUICIDE (NO SBI OR DEATH) - 1800-0600',
    '08302': 'ASSAULT - ADULT W/F - AIDING SUICIDE (NO SBI OR DEATH) - 0600-1800',
    '08303': 'ASSAULT - ADULT W/F - AIDING SUICIDE (NO SBI OR DEATH) - UNKNOWN',
    '08311': 'ASSAULT - ADULT W/F - OFFENSE CONTACT - 1800-0600',
    '08312': 'ASSAULT - ADULT W/F - OFFENSE CONTACT - 0600-1800',
    '08313': 'ASSAULT - ADULT W/F - OFFENSE CONTACT - UNKNOWN',
    '08321': 'ASSAULT - ADULT W/F - CAUSE PAIN, INJURY (M/A) - 1800-0600',
    '08322': 'ASSAULT - ADULT W/F - CAUSE PAIN, INJURY (M/A) - 0600-1800',
    '08323': 'ASSAULT - ADULT W/F - CAUSE PAIN, INJURY (M/A) - UNKNOWN',
    '08331': 'ASSAULT - ADULT W/F - TAMP W/CONSUM PROD (NON-AGG) - 1800-0600',
    '08332': 'ASSAULT - ADULT W/F - TAMP W/CONSUM PROD (NON-AGG) - 0600-1800',
    '08333': 'ASSAULT - ADULT W/F - TAMP W/CONSUM PROD (NON-AGG) - UNKNOWN',
    '08341': 'ASSAULT - ADULT W/F - EXHIBITION OF FIREARM - 1800-0600',
    '08342': 'ASSAULT - ADULT W/F - EXHIBITION OF FIREARM - 0600-1800',
    '08343': 'ASSAULT - ADULT W/F - EXHIBITION OF FIREARM - UNKNOWN',
    '08351': 'ASSAULT - ADULT W/F - DEADLY CONDUCT - 1800-0600',
    '08352': 'ASSAULT - ADULT W/F - DEADLY CONDUCT - 0600-1800',
    '08353': 'ASSAULT - ADULT W/F - DEADLY CONDUCT - UNKNOWN',
    '08361': 'ASSAULT - ADULT W/F - WITH MOTOR VEHICLE - 1800-0600',
    '08362': 'ASSAULT - ADULT W/F - WITH MOTOR VEHICLE - 0600-1800',
    '08363': 'ASSAULT - ADULT W/F - WITH MOTOR VEHICLE - UNKNOWN',
    '08371': 'ASSAULT - ADULT W/F - **NOT USED** - 1800-0600',
    '08372': 'ASSAULT - ADULT W/F - **NOT USED** - 0600-1800',
    '08373': 'ASSAULT - ADULT W/F - **NOT USED** - UNKNOWN',
    '08381': 'ASSAULT - ADULT W/F - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - 1800-0600',
    '08382': 'ASSAULT - ADULT W/F - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - 0600-1800',
    '08383': 'ASSAULT - ADULT W/F - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - UNKNOWN',
    '08391': 'ASSAULT - ADULT W/F - VERBAL THREAT/ABUSE LANGUAGE - 1800-0600',
    '08392': 'ASSAULT - ADULT W/F - VERBAL THREAT/ABUSE LANGUAGE - 0600-1800',
    '08393': 'ASSAULT - ADULT W/F - VERBAL THREAT/ABUSE LANGUAGE - UNKNOWN',
    '08401': 'ASSAULT - ADULT B/F - AIDING SUICIDE (NO SBI OR DEATH) - 1800-0600',
    '08402': 'ASSAULT - ADULT B/F - AIDING SUICIDE (NO SBI OR DEATH) - 0600-1800',
    '08403': 'ASSAULT - ADULT B/F - AIDING SUICIDE (NO SBI OR DEATH) - UNKNOWN',
    '08411': 'ASSAULT - ADULT B/F - OFFENSE CONTACT - 1800-0600',
    '08412': 'ASSAULT - ADULT B/F - OFFENSE CONTACT - 0600-1800',
    '08413': 'ASSAULT - ADULT B/F - OFFENSE CONTACT - UNKNOWN',
    '08421': 'ASSAULT - ADULT B/F - CAUSE PAIN, INJURY (M/A) - 1800-0600',
    '08422': 'ASSAULT - ADULT B/F - CAUSE PAIN, INJURY (M/A) - 0600-1800',
    '08423': 'ASSAULT - ADULT B/F - CAUSE PAIN, INJURY (M/A) - UNKNOWN',
    '08431': 'ASSAULT - ADULT B/F - TAMP W/CONSUM PROD (NON-AGG) - 1800-0600',
    '08432': 'ASSAULT - ADULT B/F - TAMP W/CONSUM PROD (NON-AGG) - 0600-1800',
    '08433': 'ASSAULT - ADULT B/F - TAMP W/CONSUM PROD (NON-AGG) - UNKNOWN',
    '08441': 'ASSAULT - ADULT B/F - EXHIBITION OF FIREARM - 1800-0600',
    '08442': 'ASSAULT - ADULT B/F - EXHIBITION OF FIREARM - 0600-1800',
    '08443': 'ASSAULT - ADULT B/F - EXHIBITION OF FIREARM - UNKNOWN',
    '08451': 'ASSAULT - ADULT B/F - DEADLY CONDUCT - 1800-0600',
    '08452': 'ASSAULT - ADULT B/F - DEADLY CONDUCT - 0600-1800',
    '08453': 'ASSAULT - ADULT B/F - DEADLY CONDUCT - UNKNOWN',
    '08461': 'ASSAULT - ADULT B/F - WITH MOTOR VEHICLE - 1800-0600',
    '08462': 'ASSAULT - ADULT B/F - WITH MOTOR VEHICLE - 0600-1800',
    '08463': 'ASSAULT - ADULT B/F - WITH MOTOR VEHICLE - UNKNOWN',
    '08471': 'ASSAULT - ADULT B/F - **NOT USED** - 1800-0600',
    '08472': 'ASSAULT - ADULT B/F - **NOT USED** - 0600-1800',
    '08473': 'ASSAULT - ADULT B/F - **NOT USED** - UNKNOWN',
    '08481': 'ASSAULT - ADULT B/F - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - 1800-0600',
    '08482': 'ASSAULT - ADULT B/F - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - 0600-1800',
    '08483': 'ASSAULT - ADULT B/F - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - UNKNOWN',
    '08491': 'ASSAULT - ADULT B/F - VERBAL THREAT/ABUSE LANGUAGE - 1800-0600',
    '08492': 'ASSAULT - ADULT B/F - VERBAL THREAT/ABUSE LANGUAGE - 0600-1800',
    '08493': 'ASSAULT - ADULT B/F - VERBAL THREAT/ABUSE LANGUAGE - UNKNOWN',
    '08501': 'ASSAULT - JUV W/M - AIDING SUICIDE (NO SBI OR DEATH) - 1800-0600',
    '08502': 'ASSAULT - JUV W/M - AIDING SUICIDE (NO SBI OR DEATH) - 0600-1800',
    '08503': 'ASSAULT - JUV W/M - AIDING SUICIDE (NO SBI OR DEATH) - UNKNOWN',
    '08511': 'ASSAULT - JUV W/M - OFFENSE CONTACT - 1800-0600',
    '08512': 'ASSAULT - JUV W/M - OFFENSE CONTACT - 0600-1800',
    '08513': 'ASSAULT - JUV W/M - OFFENSE CONTACT - UNKNOWN',
    '08521': 'ASSAULT - JUV W/M - CAUSE PAIN, INJURY (M/A) - 1800-0600',
    '08522': 'ASSAULT - JUV W/M - CAUSE PAIN, INJURY (M/A) - 0600-1800',
    '08523': 'ASSAULT - JUV W/M - CAUSE PAIN, INJURY (M/A) - UNKNOWN',
    '08531': 'ASSAULT - JUV W/M - TAMP W/CONSUM PROD (NON-AGG) - 1800-0600',
    '08532': 'ASSAULT - JUV W/M - TAMP W/CONSUM PROD (NON-AGG) - 0600-1800',
    '08533': 'ASSAULT - JUV W/M - TAMP W/CONSUM PROD (NON-AGG) - UNKNOWN',
    '08541': 'ASSAULT - JUV W/M - EXHIBITION OF FIREARM - 1800-0600',
    '08542': 'ASSAULT - JUV W/M - EXHIBITION OF FIREARM - 0600-1800',
    '08543': 'ASSAULT - JUV W/M - EXHIBITION OF FIREARM - UNKNOWN',
    '08551': 'ASSAULT - JUV W/M - DEADLY CONDUCT - 1800-0600',
    '08552': 'ASSAULT - JUV W/M - DEADLY CONDUCT - 0600-1800',
    '08553': 'ASSAULT - JUV W/M - DEADLY CONDUCT - UNKNOWN',
    '08561': 'ASSAULT - JUV W/M - WITH MOTOR VEHICLE - 1800-0600',
    '08562': 'ASSAULT - JUV W/M - WITH MOTOR VEHICLE - 0600-1800',
    '08563': 'ASSAULT - JUV W/M - WITH MOTOR VEHICLE - UNKNOWN',
    '08571': 'ASSAULT - JUV W/M - **NOT USED** - 1800-0600',
    '08572': 'ASSAULT - JUV W/M - **NOT USED** - 0600-1800',
    '08573': 'ASSAULT - JUV W/M - **NOT USED** - UNKNOWN',
    '08581': 'ASSAULT - JUV W/M - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - 1800-0600',
    '08582': 'ASSAULT - JUV W/M - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - 0600-1800',
    '08583': 'ASSAULT - JUV W/M - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - UNKNOWN',
    '08591': 'ASSAULT - JUV W/M - VERBAL THREAT/ABUSE LANGUAGE - 1800-0600',
    '08592': 'ASSAULT - JUV W/M - VERBAL THREAT/ABUSE LANGUAGE - 0600-1800',
    '08593': 'ASSAULT - JUV W/M - VERBAL THREAT/ABUSE LANGUAGE - UNKNOWN',
    '08601': 'ASSAULT - JUV B/M - AIDING SUICIDE (NO SBI OR DEATH) - 1800-0600',
    '08602': 'ASSAULT - JUV B/M - AIDING SUICIDE (NO SBI OR DEATH) - 0600-1800',
    '08603': 'ASSAULT - JUV B/M - AIDING SUICIDE (NO SBI OR DEATH) - UNKNOWN',
    '08611': 'ASSAULT - JUV B/M - OFFENSE CONTACT - 1800-0600',
    '08612': 'ASSAULT - JUV B/M - OFFENSE CONTACT - 0600-1800',
    '08613': 'ASSAULT - JUV B/M - OFFENSE CONTACT - UNKNOWN',
    '08621': 'ASSAULT - JUV B/M - CAUSE PAIN, INJURY (M/A) - 1800-0600',
    '08622': 'ASSAULT - JUV B/M - CAUSE PAIN, INJURY (M/A) - 0600-1800',
    '08623': 'ASSAULT - JUV B/M - CAUSE PAIN, INJURY (M/A) - UNKNOWN',
    '08631': 'ASSAULT - JUV B/M - TAMP W/CONSUM PROD (NON-AGG) - 1800-0600',
    '08632': 'ASSAULT - JUV B/M - TAMP W/CONSUM PROD (NON-AGG) - 0600-1800',
    '08633': 'ASSAULT - JUV B/M - TAMP W/CONSUM PROD (NON-AGG) - UNKNOWN',
    '08641': 'ASSAULT - JUV B/M - EXHIBITION OF FIREARM - 1800-0600',
    '08642': 'ASSAULT - JUV B/M - EXHIBITION OF FIREARM - 0600-1800',
    '08643': 'ASSAULT - JUV B/M - EXHIBITION OF FIREARM - UNKNOWN',
    '08651': 'ASSAULT - JUV B/M - DEADLY CONDUCT - 1800-0600',
    '08652': 'ASSAULT - JUV B/M - DEADLY CONDUCT - 0600-1800',
    '08653': 'ASSAULT - JUV B/M - DEADLY CONDUCT - UNKNOWN',
    '08661': 'ASSAULT - JUV B/M - WITH MOTOR VEHICLE - 1800-0600',
    '08662': 'ASSAULT - JUV B/M - WITH MOTOR VEHICLE - 0600-1800',
    '08663': 'ASSAULT - JUV B/M - WITH MOTOR VEHICLE - UNKNOWN',
    '08671': 'ASSAULT - JUV B/M - **NOT USED** - 1800-0600',
    '08672': 'ASSAULT - JUV B/M - **NOT USED** - 0600-1800',
    '08673': 'ASSAULT - JUV B/M - **NOT USED** - UNKNOWN',
    '08681': 'ASSAULT - JUV B/M - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - 1800-0600',
    '08682': 'ASSAULT - JUV B/M - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - 0600-1800',
    '08683': 'ASSAULT - JUV B/M - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - UNKNOWN',
    '08691': 'ASSAULT - JUV B/M - VERBAL THREAT/ABUSE LANGUAGE - 1800-0600',
    '08692': 'ASSAULT - JUV B/M - VERBAL THREAT/ABUSE LANGUAGE - 0600-1800',
    '08693': 'ASSAULT - JUV B/M - VERBAL THREAT/ABUSE LANGUAGE - UNKNOWN',
    '08701': 'ASSAULT - JUV W/F - AIDING SUICIDE (NO SBI OR DEATH) - 1800-0600',
    '08702': 'ASSAULT - JUV W/F - AIDING SUICIDE (NO SBI OR DEATH) - 0600-1800',
    '08703': 'ASSAULT - JUV W/F - AIDING SUICIDE (NO SBI OR DEATH) - UNKNOWN',
    '08711': 'ASSAULT - JUV W/F - OFFENSE CONTACT - 1800-0600',
    '08712': 'ASSAULT - JUV W/F - OFFENSE CONTACT - 0600-1800',
    '08713': 'ASSAULT - JUV W/F - OFFENSE CONTACT - UNKNOWN',
    '08721': 'ASSAULT - JUV W/F - CAUSE PAIN, INJURY (M/A) - 1800-0600',
    '08722': 'ASSAULT - JUV W/F - CAUSE PAIN, INJURY (M/A) - 0600-1800',
    '08723': 'ASSAULT - JUV W/F - CAUSE PAIN, INJURY (M/A) - UNKNOWN',
    '08731': 'ASSAULT - JUV W/F - TAMP W/CONSUM PROD (NON-AGG) - 1800-0600',
    '08732': 'ASSAULT - JUV W/F - TAMP W/CONSUM PROD (NON-AGG) - 0600-1800',
    '08733': 'ASSAULT - JUV W/F - TAMP W/CONSUM PROD (NON-AGG) - UNKNOWN',
    '08741': 'ASSAULT - JUV W/F - EXHIBITION OF FIREARM - 1800-0600',
    '08742': 'ASSAULT - JUV W/F - EXHIBITION OF FIREARM - 0600-1800',
    '08743': 'ASSAULT - JUV W/F - EXHIBITION OF FIREARM - UNKNOWN',
    '08751': 'ASSAULT - JUV W/F - DEADLY CONDUCT - 1800-0600',
    '08752': 'ASSAULT - JUV W/F - DEADLY CONDUCT - 0600-1800',
    '08753': 'ASSAULT - JUV W/F - DEADLY CONDUCT - UNKNOWN',
    '08761': 'ASSAULT - JUV W/F - WITH MOTOR VEHICLE - 1800-0600',
    '08762': 'ASSAULT - JUV W/F - WITH MOTOR VEHICLE - 0600-1800',
    '08763': 'ASSAULT - JUV W/F - WITH MOTOR VEHICLE - UNKNOWN',
    '08771': 'ASSAULT - JUV W/F - **NOT USED** - 1800-0600',
    '08772': 'ASSAULT - JUV W/F - **NOT USED** - 0600-1800',
    '08773': 'ASSAULT - JUV W/F - **NOT USED** - UNKNOWN',
    '08781': 'ASSAULT - JUV W/F - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - 1800-0600',
    '08782': 'ASSAULT - JUV W/F - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - 0600-1800',
    '08783': 'ASSAULT - JUV W/F - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - UNKNOWN',
    '08791': 'ASSAULT - JUV W/F - VERBAL THREAT/ABUSE LANGUAGE - 1800-0600',
    '08792': 'ASSAULT - JUV W/F - VERBAL THREAT/ABUSE LANGUAGE - 0600-1800',
    '08793': 'ASSAULT - JUV W/F - VERBAL THREAT/ABUSE LANGUAGE - UNKNOWN',
    '08801': 'ASSAULT - JUV B/F - AIDING SUICIDE (NO SBI OR DEATH) - 1800-0600',
    '08802': 'ASSAULT - JUV B/F - AIDING SUICIDE (NO SBI OR DEATH) - 0600-1800',
    '08803': 'ASSAULT - JUV B/F - AIDING SUICIDE (NO SBI OR DEATH) - UNKNOWN',
    '08811': 'ASSAULT - JUV B/F - OFFENSE CONTACT - 1800-0600',
    '08812': 'ASSAULT - JUV B/F - OFFENSE CONTACT - 0600-1800',
    '08813': 'ASSAULT - JUV B/F - OFFENSE CONTACT - UNKNOWN',
    '08821': 'ASSAULT - JUV B/F - CAUSE PAIN, INJURY (M/A) - 1800-0600',
    '08822': 'ASSAULT - JUV B/F - CAUSE PAIN, INJURY (M/A) - 0600-1800',
    '08823': 'ASSAULT - JUV B/F - CAUSE PAIN, INJURY (M/A) - UNKNOWN',
    '08831': 'ASSAULT - JUV B/F - TAMP W/CONSUM PROD (NON-AGG) - 1800-0600',
    '08832': 'ASSAULT - JUV B/F - TAMP W/CONSUM PROD (NON-AGG) - 0600-1800',
    '08833': 'ASSAULT - JUV B/F - TAMP W/CONSUM PROD (NON-AGG) - UNKNOWN',
    '08841': 'ASSAULT - JUV B/F - EXHIBITION OF FIREARM - 1800-0600',
    '08842': 'ASSAULT - JUV B/F - EXHIBITION OF FIREARM - 0600-1800',
    '08843': 'ASSAULT - JUV B/F - EXHIBITION OF FIREARM - UNKNOWN',
    '08851': 'ASSAULT - JUV B/F - DEADLY CONDUCT - 1800-0600',
    '08852': 'ASSAULT - JUV B/F - DEADLY CONDUCT - 0600-1800',
    '08853': 'ASSAULT - JUV B/F - DEADLY CONDUCT - UNKNOWN',
    '08861': 'ASSAULT - JUV B/F - WITH MOTOR VEHICLE - 1800-0600',
    '08862': 'ASSAULT - JUV B/F - WITH MOTOR VEHICLE - 0600-1800',
    '08863': 'ASSAULT - JUV B/F - WITH MOTOR VEHICLE - UNKNOWN',
    '08871': 'ASSAULT - JUV B/F - **NOT USED** - 1800-0600',
    '08872': 'ASSAULT - JUV B/F - **NOT USED** - 0600-1800',
    '08873': 'ASSAULT - JUV B/F - **NOT USED** - UNKNOWN',
    '08881': 'ASSAULT - JUV B/F - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - 1800-0600',
    '08882': 'ASSAULT - JUV B/F - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - 0600-1800',
    '08883': 'ASSAULT - JUV B/F - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - UNKNOWN',
    '08891': 'ASSAULT - JUV B/F - VERBAL THREAT/ABUSE LANGUAGE - 1800-0600',
    '08892': 'ASSAULT - JUV B/F - VERBAL THREAT/ABUSE LANGUAGE - 0600-1800',
    '08893': 'ASSAULT - JUV B/F - VERBAL THREAT/ABUSE LANGUAGE - UNKNOWN',
    '08901': 'ASSAULT - POLICE OFF - AIDING SUICIDE (NO SBI OR DEATH) - 1800-0600',
    '08902': 'ASSAULT - POLICE OFF - AIDING SUICIDE (NO SBI OR DEATH) - 0600-1800',
    '08903': 'ASSAULT - POLICE OFF - AIDING SUICIDE (NO SBI OR DEATH) - UNKNOWN',
    '08911': 'ASSAULT - POLICE OFF - OFFENSE CONTACT - 1800-0600',
    '08912': 'ASSAULT - POLICE OFF - OFFENSE CONTACT - 0600-1800',
    '08913': 'ASSAULT - POLICE OFF - OFFENSE CONTACT - UNKNOWN',
    '08921': 'ASSAULT - POLICE OFF - CAUSE PAIN, INJURY (M/A) - 1800-0600',
    '08922': 'ASSAULT - POLICE OFF - CAUSE PAIN, INJURY (M/A) - 0600-1800',
    '08923': 'ASSAULT - POLICE OFF - CAUSE PAIN, INJURY (M/A) - UNKNOWN',
    '08931': 'ASSAULT - POLICE OFF - TAMP W/CONSUM PROD (NON-AGG) - 1800-0600',
    '08932': 'ASSAULT - POLICE OFF - TAMP W/CONSUM PROD (NON-AGG) - 0600-1800',
    '08933': 'ASSAULT - POLICE OFF - TAMP W/CONSUM PROD (NON-AGG) - UNKNOWN',
    '08941': 'ASSAULT - POLICE OFF - EXHIBITION OF FIREARM - 1800-0600',
    '08942': 'ASSAULT - POLICE OFF - EXHIBITION OF FIREARM - 0600-1800',
    '08943': 'ASSAULT - POLICE OFF - EXHIBITION OF FIREARM - UNKNOWN',
    '08951': 'ASSAULT - POLICE OFF - DEADLY CONDUCT - 1800-0600',
    '08952': 'ASSAULT - POLICE OFF - DEADLY CONDUCT - 0600-1800',
    '08953': 'ASSAULT - POLICE OFF - DEADLY CONDUCT - UNKNOWN',
    '08961': 'ASSAULT - POLICE OFF - WITH MOTOR VEHICLE - 1800-0600',
    '08962': 'ASSAULT - POLICE OFF - WITH MOTOR VEHICLE - 0600-1800',
    '08963': 'ASSAULT - POLICE OFF - WITH MOTOR VEHICLE - UNKNOWN',
    '08971': 'ASSAULT - POLICE OFF - **NOT USED** - 1800-0600',
    '08972': 'ASSAULT - POLICE OFF - **NOT USED** - 0600-1800',
    '08973': 'ASSAULT - POLICE OFF - **NOT USED** - UNKNOWN',
    '08981': 'ASSAULT - POLICE OFF - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - 1800-0600',
    '08982': 'ASSAULT - POLICE OFF - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - 0600-1800',
    '08983': 'ASSAULT - POLICE OFF - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - UNKNOWN',
    '08991': 'ASSAULT - POLICE OFF - VERBAL THREAT/ABUSE LANGUAGE - 1800-0600',
    '08992': 'ASSAULT - POLICE OFF - VERBAL THREAT/ABUSE LANGUAGE - 0600-1800',
    '08993': 'ASSAULT - POLICE OFF - VERBAL THREAT/ABUSE LANGUAGE - UNKNOWN',
    '08A01': 'ASSAULT - INTOXIC ASLT - AIDING SUICIDE (NO SBI OR DEATH) - 1800-0600',
    '08A02': 'ASSAULT - INTOXIC ASLT - AIDING SUICIDE (NO SBI OR DEATH) - 0600-1800',
    '08A03': 'ASSAULT - INTOXIC ASLT - AIDING SUICIDE (NO SBI OR DEATH) - UNKNOWN',
    '08A11': 'ASSAULT - INTOXIC ASLT - OFFENSE CONTACT - 1800-0600',
    '08A12': 'ASSAULT - INTOXIC ASLT - OFFENSE CONTACT - 0600-1800',
    '08A13': 'ASSAULT - INTOXIC ASLT - OFFENSE CONTACT - UNKNOWN',
    '08A21': 'ASSAULT - INTOXIC ASLT - CAUSE PAIN, INJURY (M/A) - 1800-0600',
    '08A22': 'ASSAULT - INTOXIC ASLT - CAUSE PAIN, INJURY (M/A) - 0600-1800',
    '08A23': 'ASSAULT - INTOXIC ASLT - CAUSE PAIN, INJURY (M/A) - UNKNOWN',
    '08A31': 'ASSAULT - INTOXIC ASLT - TAMP W/CONSUM PROD (NON-AGG) - 1800-0600',
    '08A32': 'ASSAULT - INTOXIC ASLT - TAMP W/CONSUM PROD (NON-AGG) - 0600-1800',
    '08A33': 'ASSAULT - INTOXIC ASLT - TAMP W/CONSUM PROD (NON-AGG) - UNKNOWN',
    '08A41': 'ASSAULT - INTOXIC ASLT - EXHIBITION OF FIREARM - 1800-0600',
    '08A42': 'ASSAULT - INTOXIC ASLT - EXHIBITION OF FIREARM - 0600-1800',
    '08A43': 'ASSAULT - INTOXIC ASLT - EXHIBITION OF FIREARM - UNKNOWN',
    '08A51': 'ASSAULT - INTOXIC ASLT - DEADLY CONDUCT - 1800-0600',
    '08A52': 'ASSAULT - INTOXIC ASLT - DEADLY CONDUCT - 0600-1800',
    '08A53': 'ASSAULT - INTOXIC ASLT - DEADLY CONDUCT - UNKNOWN',
    '08A61': 'ASSAULT - INTOXIC ASLT - WITH MOTOR VEHICLE - 1800-0600',
    '08A62': 'ASSAULT - INTOXIC ASLT - WITH MOTOR VEHICLE - 0600-1800',
    '08A63': 'ASSAULT - INTOXIC ASLT - WITH MOTOR VEHICLE - UNKNOWN',
    '08A71': 'ASSAULT - INTOXIC ASLT - **NOT USED** - 1800-0600',
    '08A72': 'ASSAULT - INTOXIC ASLT - **NOT USED** - 0600-1800',
    '08A73': 'ASSAULT - INTOXIC ASLT - **NOT USED** - UNKNOWN',
    '08A81': 'ASSAULT - INTOXIC ASLT - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - 1800-0600',
    '08A82': 'ASSAULT - INTOXIC ASLT - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - 0600-1800',
    '08A83': 'ASSAULT - INTOXIC ASLT - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - UNKNOWN',
    '08A91': 'ASSAULT - INTOXIC ASLT - VERBAL THREAT/ABUSE LANGUAGE - 1800-0600',
    '08A92': 'ASSAULT - INTOXIC ASLT - VERBAL THREAT/ABUSE LANGUAGE - 0600-1800',
    '08A93': 'ASSAULT - INTOXIC ASLT - VERBAL THREAT/ABUSE LANGUAGE - UNKNOWN',
    '08B01': 'ASSAULT - JUV W/M FV - AIDING SUICIDE (NO SBI OR DEATH) - 1800-0600',
    '08B02': 'ASSAULT - JUV W/M FV - AIDING SUICIDE (NO SBI OR DEATH) - 0600-1800',
    '08B03': 'ASSAULT - JUV W/M FV - AIDING SUICIDE (NO SBI OR DEATH) - UNKNOWN',
    '08B11': 'ASSAULT - JUV W/M FV - OFFENSE CONTACT - 1800-0600',
    '08B12': 'ASSAULT - JUV W/M FV - OFFENSE CONTACT - 0600-1800',
    '08B13': 'ASSAULT - JUV W/M FV - OFFENSE CONTACT - UNKNOWN',
    '08B21': 'ASSAULT - JUV W/M FV - CAUSE PAIN, INJURY (M/A) - 1800-0600',
    '08B22': 'ASSAULT - JUV W/M FV - CAUSE PAIN, INJURY (M/A) - 0600-1800',
    '08B23': 'ASSAULT - JUV W/M FV - CAUSE PAIN, INJURY (M/A) - UNKNOWN',
    '08B31': 'ASSAULT - JUV W/M FV - TAMP W/CONSUM PROD (NON-AGG) - 1800-0600',
    '08B32': 'ASSAULT - JUV W/M FV - TAMP W/CONSUM PROD (NON-AGG) - 0600-1800',
    '08B33': 'ASSAULT - JUV W/M FV - TAMP W/CONSUM PROD (NON-AGG) - UNKNOWN',
    '08B41': 'ASSAULT - JUV W/M FV - EXHIBITION OF FIREARM - 1800-0600',
    '08B42': 'ASSAULT - JUV W/M FV - EXHIBITION OF FIREARM - 0600-1800',
    '08B43': 'ASSAULT - JUV W/M FV - EXHIBITION OF FIREARM - UNKNOWN',
    '08B51': 'ASSAULT - JUV W/M FV - DEADLY CONDUCT - 1800-0600',
    '08B52': 'ASSAULT - JUV W/M FV - DEADLY CONDUCT - 0600-1800',
    '08B53': 'ASSAULT - JUV W/M FV - DEADLY CONDUCT - UNKNOWN',
    '08B61': 'ASSAULT - JUV W/M FV - WITH MOTOR VEHICLE - 1800-0600',
    '08B62': 'ASSAULT - JUV W/M FV - WITH MOTOR VEHICLE - 0600-1800',
    '08B63': 'ASSAULT - JUV W/M FV - WITH MOTOR VEHICLE - UNKNOWN',
    '08B71': 'ASSAULT - JUV W/M FV - **NOT USED** - 1800-0600',
    '08B72': 'ASSAULT - JUV W/M FV - **NOT USED** - 0600-1800',
    '08B73': 'ASSAULT - JUV W/M FV - **NOT USED** - UNKNOWN',
    '08B81': 'ASSAULT - JUV W/M FV - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - 1800-0600',
    '08B82': 'ASSAULT - JUV W/M FV - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - 0600-1800',
    '08B83': 'ASSAULT - JUV W/M FV - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - UNKNOWN',
    '08B91': 'ASSAULT - JUV W/M FV - VERBAL THREAT/ABUSE LANGUAGE - 1800-0600',
    '08B92': 'ASSAULT - JUV W/M FV - VERBAL THREAT/ABUSE LANGUAGE - 0600-1800',
    '08B93': 'ASSAULT - JUV W/M FV - VERBAL THREAT/ABUSE LANGUAGE - UNKNOWN',
    '08C01': 'ASSAULT - JUV B/M FV - AIDING SUICIDE (NO SBI OR DEATH) - 1800-0600',
    '08C02': 'ASSAULT - JUV B/M FV - AIDING SUICIDE (NO SBI OR DEATH) - 0600-1800',
    '08C03': 'ASSAULT - JUV B/M FV - AIDING SUICIDE (NO SBI OR DEATH) - UNKNOWN',
    '08C11': 'ASSAULT - JUV B/M FV - OFFENSE CONTACT - 1800-0600',
    '08C12': 'ASSAULT - JUV B/M FV - OFFENSE CONTACT - 0600-1800',
    '08C13': 'ASSAULT - JUV B/M FV - OFFENSE CONTACT - UNKNOWN',
    '08C21': 'ASSAULT - JUV B/M FV - CAUSE PAIN, INJURY (M/A) - 1800-0600',
    '08C22': 'ASSAULT - JUV B/M FV - CAUSE PAIN, INJURY (M/A) - 0600-1800',
    '08C23': 'ASSAULT - JUV B/M FV - CAUSE PAIN, INJURY (M/A) - UNKNOWN',
    '08C31': 'ASSAULT - JUV B/M FV - TAMP W/CONSUM PROD (NON-AGG) - 1800-0600',
    '08C32': 'ASSAULT - JUV B/M FV - TAMP W/CONSUM PROD (NON-AGG) - 0600-1800',
    '08C33': 'ASSAULT - JUV B/M FV - TAMP W/CONSUM PROD (NON-AGG) - UNKNOWN',
    '08C41': 'ASSAULT - JUV B/M FV - EXHIBITION OF FIREARM - 1800-0600',
    '08C42': 'ASSAULT - JUV B/M FV - EXHIBITION OF FIREARM - 0600-1800',
    '08C43': 'ASSAULT - JUV B/M FV - EXHIBITION OF FIREARM - UNKNOWN',
    '08C51': 'ASSAULT - JUV B/M FV - DEADLY CONDUCT - 1800-0600',
    '08C52': 'ASSAULT - JUV B/M FV - DEADLY CONDUCT - 0600-1800',
    '08C53': 'ASSAULT - JUV B/M FV - DEADLY CONDUCT - UNKNOWN',
    '08C61': 'ASSAULT - JUV B/M FV - WITH MOTOR VEHICLE - 1800-0600',
    '08C62': 'ASSAULT - JUV B/M FV - WITH MOTOR VEHICLE - 0600-1800',
    '08C63': 'ASSAULT - JUV B/M FV - WITH MOTOR VEHICLE - UNKNOWN',
    '08C71': 'ASSAULT - JUV B/M FV - **NOT USED** - 1800-0600',
    '08C72': 'ASSAULT - JUV B/M FV - **NOT USED** - 0600-1800',
    '08C73': 'ASSAULT - JUV B/M FV - **NOT USED** - UNKNOWN',
    '08C81': 'ASSAULT - JUV B/M FV - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - 1800-0600',
    '08C82': 'ASSAULT - JUV B/M FV - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - 0600-1800',
    '08C83': 'ASSAULT - JUV B/M FV - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - UNKNOWN',
    '08C91': 'ASSAULT - JUV B/M FV - VERBAL THREAT/ABUSE LANGUAGE - 1800-0600',
    '08C92': 'ASSAULT - JUV B/M FV - VERBAL THREAT/ABUSE LANGUAGE - 0600-1800',
    '08C93': 'ASSAULT - JUV B/M FV - VERBAL THREAT/ABUSE LANGUAGE - UNKNOWN',
    '08D01': 'ASSAULT - JUV W/F FV - AIDING SUICIDE (NO SBI OR DEATH) - 1800-0600',
    '08D02': 'ASSAULT - JUV W/F FV - AIDING SUICIDE (NO SBI OR DEATH) - 0600-1800',
    '08D03': 'ASSAULT - JUV W/F FV - AIDING SUICIDE (NO SBI OR DEATH) - UNKNOWN',
    '08D11': 'ASSAULT - JUV W/F FV - OFFENSE CONTACT - 1800-0600',
    '08D12': 'ASSAULT - JUV W/F FV - OFFENSE CONTACT - 0600-1800',
    '08D13': 'ASSAULT - JUV W/F FV - OFFENSE CONTACT - UNKNOWN',
    '08D21': 'ASSAULT - JUV W/F FV - CAUSE PAIN, INJURY (M/A) - 1800-0600',
    '08D22': 'ASSAULT - JUV W/F FV - CAUSE PAIN, INJURY (M/A) - 0600-1800',
    '08D23': 'ASSAULT - JUV W/F FV - CAUSE PAIN, INJURY (M/A) - UNKNOWN',
    '08D31': 'ASSAULT - JUV W/F FV - TAMP W/CONSUM PROD (NON-AGG) - 1800-0600',
    '08D32': 'ASSAULT - JUV W/F FV - TAMP W/CONSUM PROD (NON-AGG) - 0600-1800',
    '08D33': 'ASSAULT - JUV W/F FV - TAMP W/CONSUM PROD (NON-AGG) - UNKNOWN',
    '08D41': 'ASSAULT - JUV W/F FV - EXHIBITION OF FIREARM - 1800-0600',
    '08D42': 'ASSAULT - JUV W/F FV - EXHIBITION OF FIREARM - 0600-1800',
    '08D43': 'ASSAULT - JUV W/F FV - EXHIBITION OF FIREARM - UNKNOWN',
    '08D51': 'ASSAULT - JUV W/F FV - DEADLY CONDUCT - 1800-0600',
    '08D52': 'ASSAULT - JUV W/F FV - DEADLY CONDUCT - 0600-1800',
    '08D53': 'ASSAULT - JUV W/F FV - DEADLY CONDUCT - UNKNOWN',
    '08D61': 'ASSAULT - JUV W/F FV - WITH MOTOR VEHICLE - 1800-0600',
    '08D62': 'ASSAULT - JUV W/F FV - WITH MOTOR VEHICLE - 0600-1800',
    '08D63': 'ASSAULT - JUV W/F FV - WITH MOTOR VEHICLE - UNKNOWN',
    '08D71': 'ASSAULT - JUV W/F FV - **NOT USED** - 1800-0600',
    '08D72': 'ASSAULT - JUV W/F FV - **NOT USED** - 0600-1800',
    '08D73': 'ASSAULT - JUV W/F FV - **NOT USED** - UNKNOWN',
    '08D81': 'ASSAULT - JUV W/F FV - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - 1800-0600',
    '08D82': 'ASSAULT - JUV W/F FV - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - 0600-1800',
    '08D83': 'ASSAULT - JUV W/F FV - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - UNKNOWN',
    '08D91': 'ASSAULT - JUV W/F FV - VERBAL THREAT/ABUSE LANGUAGE - 1800-0600',
    '08D92': 'ASSAULT - JUV W/F FV - VERBAL THREAT/ABUSE LANGUAGE - 0600-1800',
    '08D93': 'ASSAULT - JUV W/F FV - VERBAL THREAT/ABUSE LANGUAGE - UNKNOWN',
    '08E01': 'ASSAULT - JUV B/F FV - AIDING SUICIDE (NO SBI OR DEATH) - 1800-0600',
    '08E02': 'ASSAULT - JUV B/F FV - AIDING SUICIDE (NO SBI OR DEATH) - 0600-1800',
    '08E03': 'ASSAULT - JUV B/F FV - AIDING SUICIDE (NO SBI OR DEATH) - UNKNOWN',
    '08E11': 'ASSAULT - JUV B/F FV - OFFENSE CONTACT - 1800-0600',
    '08E12': 'ASSAULT - JUV B/F FV - OFFENSE CONTACT - 0600-1800',
    '08E13': 'ASSAULT - JUV B/F FV - OFFENSE CONTACT - UNKNOWN',
    '08E21': 'ASSAULT - JUV B/F FV - CAUSE PAIN, INJURY (M/A) - 1800-0600',
    '08E22': 'ASSAULT - JUV B/F FV - CAUSE PAIN, INJURY (M/A) - 0600-1800',
    '08E23': 'ASSAULT - JUV B/F FV - CAUSE PAIN, INJURY (M/A) - UNKNOWN',
    '08E31': 'ASSAULT - JUV B/F FV - TAMP W/CONSUM PROD (NON-AGG) - 1800-0600',
    '08E32': 'ASSAULT - JUV B/F FV - TAMP W/CONSUM PROD (NON-AGG) - 0600-1800',
    '08E33': 'ASSAULT - JUV B/F FV - TAMP W/CONSUM PROD (NON-AGG) - UNKNOWN',
    '08E41': 'ASSAULT - JUV B/F FV - EXHIBITION OF FIREARM - 1800-0600',
    '08E42': 'ASSAULT - JUV B/F FV - EXHIBITION OF FIREARM - 0600-1800',
    '08E43': 'ASSAULT - JUV B/F FV - EXHIBITION OF FIREARM - UNKNOWN',
    '08E51': 'ASSAULT - JUV B/F FV - DEADLY CONDUCT - 1800-0600',
    '08E52': 'ASSAULT - JUV B/F FV - DEADLY CONDUCT - 0600-1800',
    '08E53': 'ASSAULT - JUV B/F FV - DEADLY CONDUCT - UNKNOWN',
    '08E61': 'ASSAULT - JUV B/F FV - WITH MOTOR VEHICLE - 1800-0600',
    '08E62': 'ASSAULT - JUV B/F FV - WITH MOTOR VEHICLE - 0600-1800',
    '08E63': 'ASSAULT - JUV B/F FV - WITH MOTOR VEHICLE - UNKNOWN',
    '08E71': 'ASSAULT - JUV B/F FV - **NOT USED** - 1800-0600',
    '08E72': 'ASSAULT - JUV B/F FV - **NOT USED** - 0600-1800',
    '08E73': 'ASSAULT - JUV B/F FV - **NOT USED** - UNKNOWN',
    '08E81': 'ASSAULT - JUV B/F FV - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - 1800-0600',
    '08E82': 'ASSAULT - JUV B/F FV - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - 0600-1800',
    '08E83': 'ASSAULT - JUV B/F FV - RESIST ARREST (RESIST SEARCH/TRANS IS 26480) - UNKNOWN',
    '08E91': 'ASSAULT - JUV B/F FV - VERBAL THREAT/ABUSE LANGUAGE - 1800-0600',
    '08E92': 'ASSAULT - JUV B/F FV - VERBAL THREAT/ABUSE LANGUAGE - 0600-1800',
    '08E93': 'ASSAULT - JUV B/F FV - VERBAL THREAT/ABUSE LANGUAGE - UNKNOWN',
    '09111': 'ARSON includes BOMB THREAT - RESIDENCE - EXPLOSIVES - REVENGE',
    '09112': 'ARSON includes BOMB THREAT - RESIDENCE - EXPLOSIVES - PROFIT',
    '09113': 'ARSON includes BOMB THREAT - RESIDENCE - EXPLOSIVES - CONC CRIME',
    '09114': 'ARSON includes BOMB THREAT - RESIDENCE - EXPLOSIVES - RIOT:INSUR',
    '09115': 'ARSON includes BOMB THREAT - RESIDENCE - EXPLOSIVES - SUBVERSION',
    '09116': 'ARSON includes BOMB THREAT - RESIDENCE - EXPLOSIVES - OTHER',
    '09117': 'ARSON includes BOMB THREAT - RESIDENCE - EXPLOSIVES - UNKNOWN',
    '09121': 'ARSON includes BOMB THREAT - RESIDENCE - FIREBOMB - REVENGE',
    '09122': 'ARSON includes BOMB THREAT - RESIDENCE - FIREBOMB - PROFIT',
    '09123': 'ARSON includes BOMB THREAT - RESIDENCE - FIREBOMB - CONC CRIME',
    '09124': 'ARSON includes BOMB THREAT - RESIDENCE - FIREBOMB - RIOT:INSUR',
    '09125': 'ARSON includes BOMB THREAT - RESIDENCE - FIREBOMB - SUBVERSION',
    '09126': 'ARSON includes BOMB THREAT - RESIDENCE - FIREBOMB - OTHER',
    '09127': 'ARSON includes BOMB THREAT - RESIDENCE - FIREBOMB - UNKNOWN',
    '09131': 'ARSON includes BOMB THREAT - RESIDENCE - COMBUSTIBLE MATERIALS - REVENGE',
    '09132': 'ARSON includes BOMB THREAT - RESIDENCE - COMBUSTIBLE MATERIALS - PROFIT',
    '09133': 'ARSON includes BOMB THREAT - RESIDENCE - COMBUSTIBLE MATERIALS - CONC CRIME',
    '09134': 'ARSON includes BOMB THREAT - RESIDENCE - COMBUSTIBLE MATERIALS - RIOT:INSUR',
    '09135': 'ARSON includes BOMB THREAT - RESIDENCE - COMBUSTIBLE MATERIALS - SUBVERSION',
    '09136': 'ARSON includes BOMB THREAT - RESIDENCE - COMBUSTIBLE MATERIALS - OTHER',
    '09137': 'ARSON includes BOMB THREAT - RESIDENCE - COMBUSTIBLE MATERIALS - UNKNOWN',
    '09141': 'ARSON includes BOMB THREAT - RESIDENCE - OTHER - REVENGE',
    '09142': 'ARSON includes BOMB THREAT - RESIDENCE - OTHER - PROFIT',
    '09143': 'ARSON includes BOMB THREAT - RESIDENCE - OTHER - CONC CRIME',
    '09144': 'ARSON includes BOMB THREAT - RESIDENCE - OTHER - RIOT:INSUR',
    '09145': 'ARSON includes BOMB THREAT - RESIDENCE - OTHER - SUBVERSION',
    '09146': 'ARSON includes BOMB THREAT - RESIDENCE - OTHER - OTHER',
    '09147': 'ARSON includes BOMB THREAT - RESIDENCE - OTHER - UNKNOWN',
    '09151': 'ARSON includes BOMB THREAT - RESIDENCE - UNKNOWN - REVENGE',
    '09152': 'ARSON includes BOMB THREAT - RESIDENCE - UNKNOWN - PROFIT',
    '09153': 'ARSON includes BOMB THREAT - RESIDENCE - UNKNOWN - CONC CRIME',
    '09154': 'ARSON includes BOMB THREAT - RESIDENCE - UNKNOWN - RIOT:INSUR',
    '09155': 'ARSON includes BOMB THREAT - RESIDENCE - UNKNOWN - SUBVERSION',
    '09156': 'ARSON includes BOMB THREAT - RESIDENCE - UNKNOWN - OTHER',
    '09157': 'ARSON includes BOMB THREAT - RESIDENCE - UNKNOWN - UNKNOWN',
    '09161': 'ARSON includes BOMB THREAT - RESIDENCE - BOMB THREAT (incl HOAX BOMB) - REVENGE',
    '09162': 'ARSON includes BOMB THREAT - RESIDENCE - BOMB THREAT (incl HOAX BOMB) - PROFIT',
    '09163': 'ARSON includes BOMB THREAT - RESIDENCE - BOMB THREAT (incl HOAX BOMB) - CONC CRIME',
    '09164': 'ARSON includes BOMB THREAT - RESIDENCE - BOMB THREAT (incl HOAX BOMB) - RIOT:INSUR',
    '09165': 'ARSON includes BOMB THREAT - RESIDENCE - BOMB THREAT (incl HOAX BOMB) - SUBVERSION',
    '09166': 'ARSON includes BOMB THREAT - RESIDENCE - BOMB THREAT (incl HOAX BOMB) - OTHER',
    '09167': 'ARSON includes BOMB THREAT - RESIDENCE - BOMB THREAT (incl HOAX BOMB) - UNKNOWN',
    '09211': 'ARSON includes BOMB THREAT - VEHICLE - EXPLOSIVES - REVENGE',
    '09212': 'ARSON includes BOMB THREAT - VEHICLE - EXPLOSIVES - PROFIT',
    '09213': 'ARSON includes BOMB THREAT - VEHICLE - EXPLOSIVES - CONC CRIME',
    '09214': 'ARSON includes BOMB THREAT - VEHICLE - EXPLOSIVES - RIOT:INSUR',
    '09215': 'ARSON includes BOMB THREAT - VEHICLE - EXPLOSIVES - SUBVERSION',
    '09216': 'ARSON includes BOMB THREAT - VEHICLE - EXPLOSIVES - OTHER',
    '09217': 'ARSON includes BOMB THREAT - VEHICLE - EXPLOSIVES - UNKNOWN',
    '09221': 'ARSON includes BOMB THREAT - VEHICLE - FIREBOMB - REVENGE',
    '09222': 'ARSON includes BOMB THREAT - VEHICLE - FIREBOMB - PROFIT',
    '09223': 'ARSON includes BOMB THREAT - VEHICLE - FIREBOMB - CONC CRIME',
    '09224': 'ARSON includes BOMB THREAT - VEHICLE - FIREBOMB - RIOT:INSUR',
    '09225': 'ARSON includes BOMB THREAT - VEHICLE - FIREBOMB - SUBVERSION',
    '09226': 'ARSON includes BOMB THREAT - VEHICLE - FIREBOMB - OTHER',
    '09227': 'ARSON includes BOMB THREAT - VEHICLE - FIREBOMB - UNKNOWN',
    '09231': 'ARSON includes BOMB THREAT - VEHICLE - COMBUSTIBLE MATERIALS - REVENGE',
    '09232': 'ARSON includes BOMB THREAT - VEHICLE - COMBUSTIBLE MATERIALS - PROFIT',
    '09233': 'ARSON includes BOMB THREAT - VEHICLE - COMBUSTIBLE MATERIALS - CONC CRIME',
    '09234': 'ARSON includes BOMB THREAT - VEHICLE - COMBUSTIBLE MATERIALS - RIOT:INSUR',
    '09235': 'ARSON includes BOMB THREAT - VEHICLE - COMBUSTIBLE MATERIALS - SUBVERSION',
    '09236': 'ARSON includes BOMB THREAT - VEHICLE - COMBUSTIBLE MATERIALS - OTHER',
    '09237': 'ARSON includes BOMB THREAT - VEHICLE - COMBUSTIBLE MATERIALS - UNKNOWN',
    '09241': 'ARSON includes BOMB THREAT - VEHICLE - OTHER - REVENGE',
    '09242': 'ARSON includes BOMB THREAT - VEHICLE - OTHER - PROFIT',
    '09243': 'ARSON includes BOMB THREAT - VEHICLE - OTHER - CONC CRIME',
    '09244': 'ARSON includes BOMB THREAT - VEHICLE - OTHER - RIOT:INSUR',
    '09245': 'ARSON includes BOMB THREAT - VEHICLE - OTHER - SUBVERSION',
    '09246': 'ARSON includes BOMB THREAT - VEHICLE - OTHER - OTHER',
    '09247': 'ARSON includes BOMB THREAT - VEHICLE - OTHER - UNKNOWN',
    '09251': 'ARSON includes BOMB THREAT - VEHICLE - UNKNOWN - REVENGE',
    '09252': 'ARSON includes BOMB THREAT - VEHICLE - UNKNOWN - PROFIT',
    '09253': 'ARSON includes BOMB THREAT - VEHICLE - UNKNOWN - CONC CRIME',
    '09254': 'ARSON includes BOMB THREAT - VEHICLE - UNKNOWN - RIOT:INSUR',
    '09255': 'ARSON includes BOMB THREAT - VEHICLE - UNKNOWN - SUBVERSION',
    '09256': 'ARSON includes BOMB THREAT - VEHICLE - UNKNOWN - OTHER',
    '09257': 'ARSON includes BOMB THREAT - VEHICLE - UNKNOWN - UNKNOWN',
    '09261': 'ARSON includes BOMB THREAT - VEHICLE - BOMB THREAT (incl HOAX BOMB) - REVENGE',
    '09262': 'ARSON includes BOMB THREAT - VEHICLE - BOMB THREAT (incl HOAX BOMB) - PROFIT',
    '09263': 'ARSON includes BOMB THREAT - VEHICLE - BOMB THREAT (incl HOAX BOMB) - CONC CRIME',
    '09264': 'ARSON includes BOMB THREAT - VEHICLE - BOMB THREAT (incl HOAX BOMB) - RIOT:INSUR',
    '09265': 'ARSON includes BOMB THREAT - VEHICLE - BOMB THREAT (incl HOAX BOMB) - SUBVERSION',
    '09266': 'ARSON includes BOMB THREAT - VEHICLE - BOMB THREAT (incl HOAX BOMB) - OTHER',
    '09267': 'ARSON includes BOMB THREAT - VEHICLE - BOMB THREAT (incl HOAX BOMB) - UNKNOWN',
    '09311': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - EXPLOSIVES - REVENGE',
    '09312': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - EXPLOSIVES - PROFIT',
    '09313': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - EXPLOSIVES - CONC CRIME',
    '09314': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - EXPLOSIVES - RIOT:INSUR',
    '09315': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - EXPLOSIVES - SUBVERSION',
    '09316': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - EXPLOSIVES - OTHER',
    '09317': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - EXPLOSIVES - UNKNOWN',
    '09321': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - FIREBOMB - REVENGE',
    '09322': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - FIREBOMB - PROFIT',
    '09323': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - FIREBOMB - CONC CRIME',
    '09324': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - FIREBOMB - RIOT:INSUR',
    '09325': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - FIREBOMB - SUBVERSION',
    '09326': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - FIREBOMB - OTHER',
    '09327': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - FIREBOMB - UNKNOWN',
    '09331': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - COMBUSTIBLE MATERIALS - REVENGE',
    '09332': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - COMBUSTIBLE MATERIALS - PROFIT',
    '09333': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - COMBUSTIBLE MATERIALS - CONC CRIME',
    '09334': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - COMBUSTIBLE MATERIALS - RIOT:INSUR',
    '09335': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - COMBUSTIBLE MATERIALS - SUBVERSION',
    '09336': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - COMBUSTIBLE MATERIALS - OTHER',
    '09337': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - COMBUSTIBLE MATERIALS - UNKNOWN',
    '09341': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - OTHER - REVENGE',
    '09342': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - OTHER - PROFIT',
    '09343': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - OTHER - CONC CRIME',
    '09344': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - OTHER - RIOT:INSUR',
    '09345': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - OTHER - SUBVERSION',
    '09346': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - OTHER - OTHER',
    '09347': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - OTHER - UNKNOWN',
    '09351': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - UNKNOWN - REVENGE',
    '09352': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - UNKNOWN - PROFIT',
    '09353': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - UNKNOWN - CONC CRIME',
    '09354': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - UNKNOWN - RIOT:INSUR',
    '09355': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - UNKNOWN - SUBVERSION',
    '09356': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - UNKNOWN - OTHER',
    '09357': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - UNKNOWN - UNKNOWN',
    '09361': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - BOMB THREAT (incl HOAX BOMB) - REVENGE',
    '09362': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - BOMB THREAT (incl HOAX BOMB) - PROFIT',
    '09363': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - BOMB THREAT (incl HOAX BOMB) - CONC CRIME',
    '09364': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - BOMB THREAT (incl HOAX BOMB) - RIOT:INSUR',
    '09365': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - BOMB THREAT (incl HOAX BOMB) - SUBVERSION',
    '09366': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - BOMB THREAT (incl HOAX BOMB) - OTHER',
    '09367': 'ARSON includes BOMB THREAT - ANY OBJECT NOT 1-2,4-7 - BOMB THREAT (incl HOAX BOMB) - UNKNOWN',
    '09411': 'ARSON includes BOMB THREAT - AIRCRAFT - EXPLOSIVES - REVENGE',
    '09412': 'ARSON includes BOMB THREAT - AIRCRAFT - EXPLOSIVES - PROFIT',
    '09413': 'ARSON includes BOMB THREAT - AIRCRAFT - EXPLOSIVES - CONC CRIME',
    '09414': 'ARSON includes BOMB THREAT - AIRCRAFT - EXPLOSIVES - RIOT:INSUR',
    '09415': 'ARSON includes BOMB THREAT - AIRCRAFT - EXPLOSIVES - SUBVERSION',
    '09416': 'ARSON includes BOMB THREAT - AIRCRAFT - EXPLOSIVES - OTHER',
    '09417': 'ARSON includes BOMB THREAT - AIRCRAFT - EXPLOSIVES - UNKNOWN',
    '09421': 'ARSON includes BOMB THREAT - AIRCRAFT - FIREBOMB - REVENGE',
    '09422': 'ARSON includes BOMB THREAT - AIRCRAFT - FIREBOMB - PROFIT',
    '09423': 'ARSON includes BOMB THREAT - AIRCRAFT - FIREBOMB - CONC CRIME',
    '09424': 'ARSON includes BOMB THREAT - AIRCRAFT - FIREBOMB - RIOT:INSUR',
    '09425': 'ARSON includes BOMB THREAT - AIRCRAFT - FIREBOMB - SUBVERSION',
    '09426': 'ARSON includes BOMB THREAT - AIRCRAFT - FIREBOMB - OTHER',
    '09427': 'ARSON includes BOMB THREAT - AIRCRAFT - FIREBOMB - UNKNOWN',
    '09431': 'ARSON includes BOMB THREAT - AIRCRAFT - COMBUSTIBLE MATERIALS - REVENGE',
    '09432': 'ARSON includes BOMB THREAT - AIRCRAFT - COMBUSTIBLE MATERIALS - PROFIT',
    '09433': 'ARSON includes BOMB THREAT - AIRCRAFT - COMBUSTIBLE MATERIALS - CONC CRIME',
    '09434': 'ARSON includes BOMB THREAT - AIRCRAFT - COMBUSTIBLE MATERIALS - RIOT:INSUR',
    '09435': 'ARSON includes BOMB THREAT - AIRCRAFT - COMBUSTIBLE MATERIALS - SUBVERSION',
    '09436': 'ARSON includes BOMB THREAT - AIRCRAFT - COMBUSTIBLE MATERIALS - OTHER',
    '09437': 'ARSON includes BOMB THREAT - AIRCRAFT - COMBUSTIBLE MATERIALS - UNKNOWN',
    '09441': 'ARSON includes BOMB THREAT - AIRCRAFT - OTHER - REVENGE',
    '09442': 'ARSON includes BOMB THREAT - AIRCRAFT - OTHER - PROFIT',
    '09443': 'ARSON includes BOMB THREAT - AIRCRAFT - OTHER - CONC CRIME',
    '09444': 'ARSON includes BOMB THREAT - AIRCRAFT - OTHER - RIOT:INSUR',
    '09445': 'ARSON includes BOMB THREAT - AIRCRAFT - OTHER - SUBVERSION',
    '09446': 'ARSON includes BOMB THREAT - AIRCRAFT - OTHER - OTHER',
    '09447': 'ARSON includes BOMB THREAT - AIRCRAFT - OTHER - UNKNOWN',
    '09451': 'ARSON includes BOMB THREAT - AIRCRAFT - UNKNOWN - REVENGE',
    '09452': 'ARSON includes BOMB THREAT - AIRCRAFT - UNKNOWN - PROFIT',
    '09453': 'ARSON includes BOMB THREAT - AIRCRAFT - UNKNOWN - CONC CRIME',
    '09454': 'ARSON includes BOMB THREAT - AIRCRAFT - UNKNOWN - RIOT:INSUR',
    '09455': 'ARSON includes BOMB THREAT - AIRCRAFT - UNKNOWN - SUBVERSION',
    '09456': 'ARSON includes BOMB THREAT - AIRCRAFT - UNKNOWN - OTHER',
    '09457': 'ARSON includes BOMB THREAT - AIRCRAFT - UNKNOWN - UNKNOWN',
    '09461': 'ARSON includes BOMB THREAT - AIRCRAFT - BOMB THREAT (incl HOAX BOMB) - REVENGE',
    '09462': 'ARSON includes BOMB THREAT - AIRCRAFT - BOMB THREAT (incl HOAX BOMB) - PROFIT',
    '09463': 'ARSON includes BOMB THREAT - AIRCRAFT - BOMB THREAT (incl HOAX BOMB) - CONC CRIME',
    '09464': 'ARSON includes BOMB THREAT - AIRCRAFT - BOMB THREAT (incl HOAX BOMB) - RIOT:INSUR',
    '09465': 'ARSON includes BOMB THREAT - AIRCRAFT - BOMB THREAT (incl HOAX BOMB) - SUBVERSION',
    '09466': 'ARSON includes BOMB THREAT - AIRCRAFT - BOMB THREAT (incl HOAX BOMB) - OTHER',
    '09467': 'ARSON includes BOMB THREAT - AIRCRAFT - BOMB THREAT (incl HOAX BOMB) - UNKNOWN',
    '09511': 'ARSON includes BOMB THREAT - SCHOOL - EXPLOSIVES - REVENGE',
    '09512': 'ARSON includes BOMB THREAT - SCHOOL - EXPLOSIVES - PROFIT',
    '09513': 'ARSON includes BOMB THREAT - SCHOOL - EXPLOSIVES - CONC CRIME',
    '09514': 'ARSON includes BOMB THREAT - SCHOOL - EXPLOSIVES - RIOT:INSUR',
    '09515': 'ARSON includes BOMB THREAT - SCHOOL - EXPLOSIVES - SUBVERSION',
    '09516': 'ARSON includes BOMB THREAT - SCHOOL - EXPLOSIVES - OTHER',
    '09517': 'ARSON includes BOMB THREAT - SCHOOL - EXPLOSIVES - UNKNOWN',
    '09521': 'ARSON includes BOMB THREAT - SCHOOL - FIREBOMB - REVENGE',
    '09522': 'ARSON includes BOMB THREAT - SCHOOL - FIREBOMB - PROFIT',
    '09523': 'ARSON includes BOMB THREAT - SCHOOL - FIREBOMB - CONC CRIME',
    '09524': 'ARSON includes BOMB THREAT - SCHOOL - FIREBOMB - RIOT:INSUR',
    '09525': 'ARSON includes BOMB THREAT - SCHOOL - FIREBOMB - SUBVERSION',
    '09526': 'ARSON includes BOMB THREAT - SCHOOL - FIREBOMB - OTHER',
    '09527': 'ARSON includes BOMB THREAT - SCHOOL - FIREBOMB - UNKNOWN',
    '09531': 'ARSON includes BOMB THREAT - SCHOOL - COMBUSTIBLE MATERIALS - REVENGE',
    '09532': 'ARSON includes BOMB THREAT - SCHOOL - COMBUSTIBLE MATERIALS - PROFIT',
    '09533': 'ARSON includes BOMB THREAT - SCHOOL - COMBUSTIBLE MATERIALS - CONC CRIME',
    '09534': 'ARSON includes BOMB THREAT - SCHOOL - COMBUSTIBLE MATERIALS - RIOT:INSUR',
    '09535': 'ARSON includes BOMB THREAT - SCHOOL - COMBUSTIBLE MATERIALS - SUBVERSION',
    '09536': 'ARSON includes BOMB THREAT - SCHOOL - COMBUSTIBLE MATERIALS - OTHER',
    '09537': 'ARSON includes BOMB THREAT - SCHOOL - COMBUSTIBLE MATERIALS - UNKNOWN',
    '09541': 'ARSON includes BOMB THREAT - SCHOOL - OTHER - REVENGE',
    '09542': 'ARSON includes BOMB THREAT - SCHOOL - OTHER - PROFIT',
    '09543': 'ARSON includes BOMB THREAT - SCHOOL - OTHER - CONC CRIME',
    '09544': 'ARSON includes BOMB THREAT - SCHOOL - OTHER - RIOT:INSUR',
    '09545': 'ARSON includes BOMB THREAT - SCHOOL - OTHER - SUBVERSION',
    '09546': 'ARSON includes BOMB THREAT - SCHOOL - OTHER - OTHER',
    '09547': 'ARSON includes BOMB THREAT - SCHOOL - OTHER - UNKNOWN',
    '09551': 'ARSON includes BOMB THREAT - SCHOOL - UNKNOWN - REVENGE',
    '09552': 'ARSON includes BOMB THREAT - SCHOOL - UNKNOWN - PROFIT',
    '09553': 'ARSON includes BOMB THREAT - SCHOOL - UNKNOWN - CONC CRIME',
    '09554': 'ARSON includes BOMB THREAT - SCHOOL - UNKNOWN - RIOT:INSUR',
    '09555': 'ARSON includes BOMB THREAT - SCHOOL - UNKNOWN - SUBVERSION',
    '09556': 'ARSON includes BOMB THREAT - SCHOOL - UNKNOWN - OTHER',
    '09557': 'ARSON includes BOMB THREAT - SCHOOL - UNKNOWN - UNKNOWN',
    '09561': 'ARSON includes BOMB THREAT - SCHOOL - BOMB THREAT (incl HOAX BOMB) - REVENGE',
    '09562': 'ARSON includes BOMB THREAT - SCHOOL - BOMB THREAT (incl HOAX BOMB) - PROFIT',
    '09563': 'ARSON includes BOMB THREAT - SCHOOL - BOMB THREAT (incl HOAX BOMB) - CONC CRIME',
    '09564': 'ARSON includes BOMB THREAT - SCHOOL - BOMB THREAT (incl HOAX BOMB) - RIOT:INSUR',
    '09565': 'ARSON includes BOMB THREAT - SCHOOL - BOMB THREAT (incl HOAX BOMB) - SUBVERSION',
    '09566': 'ARSON includes BOMB THREAT - SCHOOL - BOMB THREAT (incl HOAX BOMB) - OTHER',
    '09567': 'ARSON includes BOMB THREAT - SCHOOL - BOMB THREAT (incl HOAX BOMB) - UNKNOWN',
    '09611': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - EXPLOSIVES - REVENGE',
    '09612': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - EXPLOSIVES - PROFIT',
    '09613': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - EXPLOSIVES - CONC CRIME',
    '09614': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - EXPLOSIVES - RIOT:INSUR',
    '09615': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - EXPLOSIVES - SUBVERSION',
    '09616': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - EXPLOSIVES - OTHER',
    '09617': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - EXPLOSIVES - UNKNOWN',
    '09621': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - FIREBOMB - REVENGE',
    '09622': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - FIREBOMB - PROFIT',
    '09623': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - FIREBOMB - CONC CRIME',
    '09624': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - FIREBOMB - RIOT:INSUR',
    '09625': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - FIREBOMB - SUBVERSION',
    '09626': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - FIREBOMB - OTHER',
    '09627': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - FIREBOMB - UNKNOWN',
    '09631': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - COMBUSTIBLE MATERIALS - REVENGE',
    '09632': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - COMBUSTIBLE MATERIALS - PROFIT',
    '09633': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - COMBUSTIBLE MATERIALS - CONC CRIME',
    '09634': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - COMBUSTIBLE MATERIALS - RIOT:INSUR',
    '09635': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - COMBUSTIBLE MATERIALS - SUBVERSION',
    '09636': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - COMBUSTIBLE MATERIALS - OTHER',
    '09637': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - COMBUSTIBLE MATERIALS - UNKNOWN',
    '09641': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - OTHER - REVENGE',
    '09642': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - OTHER - PROFIT',
    '09643': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - OTHER - CONC CRIME',
    '09644': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - OTHER - RIOT:INSUR',
    '09645': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - OTHER - SUBVERSION',
    '09646': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - OTHER - OTHER',
    '09647': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - OTHER - UNKNOWN',
    '09651': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - UNKNOWN - REVENGE',
    '09652': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - UNKNOWN - PROFIT',
    '09653': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - UNKNOWN - CONC CRIME',
    '09654': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - UNKNOWN - RIOT:INSUR',
    '09655': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - UNKNOWN - SUBVERSION',
    '09656': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - UNKNOWN - OTHER',
    '09657': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - UNKNOWN - UNKNOWN',
    '09661': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - BOMB THREAT (incl HOAX BOMB) - REVENGE',
    '09662': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - BOMB THREAT (incl HOAX BOMB) - PROFIT',
    '09663': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - BOMB THREAT (incl HOAX BOMB) - CONC CRIME',
    '09664': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - BOMB THREAT (incl HOAX BOMB) - RIOT:INSUR',
    '09665': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - BOMB THREAT (incl HOAX BOMB) - SUBVERSION',
    '09666': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - BOMB THREAT (incl HOAX BOMB) - OTHER',
    '09667': 'ARSON includes BOMB THREAT - PUBLIC BUILDING - BOMB THREAT (incl HOAX BOMB) - UNKNOWN',
    '09711': 'ARSON includes BOMB THREAT - OTHER BUILDING - EXPLOSIVES - REVENGE',
    '09712': 'ARSON includes BOMB THREAT - OTHER BUILDING - EXPLOSIVES - PROFIT',
    '09713': 'ARSON includes BOMB THREAT - OTHER BUILDING - EXPLOSIVES - CONC CRIME',
    '09714': 'ARSON includes BOMB THREAT - OTHER BUILDING - EXPLOSIVES - RIOT:INSUR',
    '09715': 'ARSON includes BOMB THREAT - OTHER BUILDING - EXPLOSIVES - SUBVERSION',
    '09716': 'ARSON includes BOMB THREAT - OTHER BUILDING - EXPLOSIVES - OTHER',
    '09717': 'ARSON includes BOMB THREAT - OTHER BUILDING - EXPLOSIVES - UNKNOWN',
    '09721': 'ARSON includes BOMB THREAT - OTHER BUILDING - FIREBOMB - REVENGE',
    '09722': 'ARSON includes BOMB THREAT - OTHER BUILDING - FIREBOMB - PROFIT',
    '09723': 'ARSON includes BOMB THREAT - OTHER BUILDING - FIREBOMB - CONC CRIME',
    '09724': 'ARSON includes BOMB THREAT - OTHER BUILDING - FIREBOMB - RIOT:INSUR',
    '09725': 'ARSON includes BOMB THREAT - OTHER BUILDING - FIREBOMB - SUBVERSION',
    '09726': 'ARSON includes BOMB THREAT - OTHER BUILDING - FIREBOMB - OTHER',
    '09727': 'ARSON includes BOMB THREAT - OTHER BUILDING - FIREBOMB - UNKNOWN',
    '09731': 'ARSON includes BOMB THREAT - OTHER BUILDING - COMBUSTIBLE MATERIALS - REVENGE',
    '09732': 'ARSON includes BOMB THREAT - OTHER BUILDING - COMBUSTIBLE MATERIALS - PROFIT',
    '09733': 'ARSON includes BOMB THREAT - OTHER BUILDING - COMBUSTIBLE MATERIALS - CONC CRIME',
    '09734': 'ARSON includes BOMB THREAT - OTHER BUILDING - COMBUSTIBLE MATERIALS - RIOT:INSUR',
    '09735': 'ARSON includes BOMB THREAT - OTHER BUILDING - COMBUSTIBLE MATERIALS - SUBVERSION',
    '09736': 'ARSON includes BOMB THREAT - OTHER BUILDING - COMBUSTIBLE MATERIALS - OTHER',
    '09737': 'ARSON includes BOMB THREAT - OTHER BUILDING - COMBUSTIBLE MATERIALS - UNKNOWN',
    '09741': 'ARSON includes BOMB THREAT - OTHER BUILDING - OTHER - REVENGE',
    '09742': 'ARSON includes BOMB THREAT - OTHER BUILDING - OTHER - PROFIT',
    '09743': 'ARSON includes BOMB THREAT - OTHER BUILDING - OTHER - CONC CRIME',
    '09744': 'ARSON includes BOMB THREAT - OTHER BUILDING - OTHER - RIOT:INSUR',
    '09745': 'ARSON includes BOMB THREAT - OTHER BUILDING - OTHER - SUBVERSION',
    '09746': 'ARSON includes BOMB THREAT - OTHER BUILDING - OTHER - OTHER',
    '09747': 'ARSON includes BOMB THREAT - OTHER BUILDING - OTHER - UNKNOWN',
    '09751': 'ARSON includes BOMB THREAT - OTHER BUILDING - UNKNOWN - REVENGE',
    '09752': 'ARSON includes BOMB THREAT - OTHER BUILDING - UNKNOWN - PROFIT',
    '09753': 'ARSON includes BOMB THREAT - OTHER BUILDING - UNKNOWN - CONC CRIME',
    '09754': 'ARSON includes BOMB THREAT - OTHER BUILDING - UNKNOWN - RIOT:INSUR',
    '09755': 'ARSON includes BOMB THREAT - OTHER BUILDING - UNKNOWN - SUBVERSION',
    '09756': 'ARSON includes BOMB THREAT - OTHER BUILDING - UNKNOWN - OTHER',
    '09757': 'ARSON includes BOMB THREAT - OTHER BUILDING - UNKNOWN - UNKNOWN',
    '09761': 'ARSON includes BOMB THREAT - OTHER BUILDING - BOMB THREAT (incl HOAX BOMB) - REVENGE',
    '09762': 'ARSON includes BOMB THREAT - OTHER BUILDING - BOMB THREAT (incl HOAX BOMB) - PROFIT',
    '09763': 'ARSON includes BOMB THREAT - OTHER BUILDING - BOMB THREAT (incl HOAX BOMB) - CONC CRIME',
    '09764': 'ARSON includes BOMB THREAT - OTHER BUILDING - BOMB THREAT (incl HOAX BOMB) - RIOT:INSUR',
    '09765': 'ARSON includes BOMB THREAT - OTHER BUILDING - BOMB THREAT (incl HOAX BOMB) - SUBVERSION',
    '09766': 'ARSON includes BOMB THREAT - OTHER BUILDING - BOMB THREAT (incl HOAX BOMB) - OTHER',
    '09767': 'ARSON includes BOMB THREAT - OTHER BUILDING - BOMB THREAT (incl HOAX BOMB) - UNKNOWN',
    '10011': 'FORGE & COUNTERFEIT - A/F PUBL REC O$50',
    '10012': 'FORGE & COUNTERFEIT - A/F PUBL REC U$50',
    '10021': 'FORGE & COUNTERFEIT - P A/F/C BILLS,NOTES O$50',
    '10022': 'FORGE & COUNTERFEIT - P A/F/C BILLS,NOTES U$50',
    '10023': 'FORGE & COUNTERFEIT - A/F LOTTERY TICKETS',
    '10031': 'FORGE & COUNTERFEIT - F WILLS/DEEDS O$50',
    '10032': 'FORGE & COUNTERFEIT - F WILLS/DEEDS U$50',
    '10041': 'FORGE & COUNTERFEIT - C MONEY O$50',
    '10042': 'FORGE & COUNTERFEIT - C MONEY U$50',
    '10051': 'FORGE & COUNTERFEIT - P F/C INSTRUMENT O$50',
    '10052': 'FORGE & COUNTERFEIT - P F/C INSTRUMENT U$50',
    '10061': 'FORGE & COUNTERFEIT - ERASURES O $50',
    '10062': 'FORGE & COUNTERFEIT - ERASURES U $50',
    '10071': 'FORGE & COUNTERFEIT - SIGN OTHER/FICT NAME O$50',
    '10072': 'FORGE & COUNTERFEIT - SIGN OTHER/FICT NAME U$50',
    '10081': 'FORGE & COUNTERFEIT - PO/M APPARATUS O$50',
    '10082': 'FORGE & COUNTERFEIT - PO/M APPARATUS U$50',
    '10091': 'FORGE & COUNTERFEIT - U F LABELS O$50',
    '10092': 'FORGE & COUNTERFEIT - U F LABELS U$50',
    '10101': 'FORGE & COUNTERFEIT - S TM GOODS A/F/C O$50',
    '10102': 'FORGE & COUNTERFEIT - S TM GOODS A/F/C U$50',
    '10111': 'FORGE & COUNTERFEIT - WORTHLESS CHECK O$50',
    '10112': 'FORGE & COUNTERFEIT - WORTHLESS CHECK U$50',
    '10121': 'FORGE & COUNTERFEIT - ATT FORGERY O$50',
    '10122': 'FORGE & COUNTERFEIT - ATT FORGERY U$50',
    '11011': 'FRAUD - 3 CARD MONTE FRAUD - 3 CARD MONTE',
    '11012': 'FRAUD - FOUND ENVELOPE FRAUD - FOUND ENVELOPE',
    '11013': 'FRAUD - HOME REP/PEST CONT FRAUD - HOME REP/PEST CONT',
    '11014': 'FRAUD - MISC/OTHERS FRAUD - MISC/OTHERS',
    '11020': 'FRAUD - LABELS/ACCOUNTS FRAUD - LABELS/ACCOUNTS',
    '11030': 'FRAUD - DOCUMENTS/CLAIMS FRAUD - DOCUMENTS/CLAIMS',
    '11031': 'FRAUD - PASS ALTER LOTT TICK FRAUD - PASS ALTER LOTT TICK',
    '11040': 'FRAUD - CHEAT/SWINDLE FRAUD - CHEAT/SWINDLE',
    '11051': 'FRAUD - USE CRED CARD O$50 FRAUD - USE CRED CARD O$50',
    '11052': 'FRAUD - USE CRED CARD U$50 FRAUD - USE CRED CARD U$50',
    '11060': 'FRAUD - INSURANCE FRAUDS FRAUD - INSURANCE FRAUDS',
    '11070': 'FRAUD - FALSE WTS/MEASURE FRAUD - FALSE WTS/MEASURE',
    '11071': 'FRAUD - LOTT MACH TAMPER FRAUD - LOTT MACH TAMPER',
    '11080': 'FRAUD - FALSE ADVERTISING FRAUD - FALSE ADVERTISING',
    '11090': 'FRAUD - TICKET SCALPING FRAUD - TICKET SCALPING',
    '11101': 'FRAUD - CREDIT/TRNS MV/CC REC O$50 FRAUD - CREDIT/TRNS MV/CC REC O$50',
    '11102': 'FRAUD - CREDIT/TRNS MV/CC REC U$50 FRAUD - CREDIT/TRNS MV/CC REC U$50',
    '11110': 'FRAUD - ATTEMPT TO DO ABOVE FRAUD - ATTEMPT TO DO ABOVE',
    '12011': 'EMBEZZLEMENT - MONEY $200+ EMBEZZLEMENT - MONEY $200+',
    '12012': 'EMBEZZLEMENT - MONEY $50 > $199.99 EMBEZZLEMENT - MONEY $50 > $199.99',
    '12013': 'EMBEZZLEMENT - MONEY $20 > $49.99 EMBEZZLEMENT - MONEY $20 > $49.99',
    '12014': 'EMBEZZLEMENT - MONEY $5 > $19.99 EMBEZZLEMENT - MONEY $5 > $19.99',
    '12015': 'EMBEZZLEMENT - MONEY UNDER $5 EMBEZZLEMENT - MONEY UNDER $5',
    '12021': 'EMBEZZLEMENT - PROPERTY $200+ EMBEZZLEMENT - PROPERTY $200+',
    '12022': 'EMBEZZLEMENT - PROPERTY $50 > $199.99 EMBEZZLEMENT - PROPERTY $50 > $199.99',
    '12023': 'EMBEZZLEMENT - PROPERTY $20 > $49.99 EMBEZZLEMENT - PROPERTY $20 > $49.99',
    '12024': 'EMBEZZLEMENT - PROPERTY $5 > $19.99 EMBEZZLEMENT - PROPERTY $5 > $19.99',
    '12025': 'EMBEZZLEMENT - PROPERTY UNDER $5 EMBEZZLEMENT - PROPERTY UNDER $5',
    '13010': 'FENCE - BUYING FENCE - BUYING',
    '13020': 'FENCE - RECEIVING FENCE -RECEIVING',
    '13030': 'FENCE - POSSESSING FENCE - POSSESSING',
    '13040': 'FENCE - ATTEMPT AT ABOVE FENCE - ATTEMPT AT ABOVE',
    '14010': 'VANDAL & CRIM MISCH - DESTROY ANIMAL FENCE',
    '14020': 'VANDAL & CRIM MISCH - BRANDING',
    '14030': 'VANDAL & CRIM MISCH - RECKLESS DMG/DEST NON-VEHICLE',
    '14031': 'VANDAL & CRIM MISCH - RECKLESS DMG/DEST WEAPON USED IS VEHICLE',
    '14050': 'VANDAL & CRIM MISCH - INTERFERE W/RR PROP',
    '14081': 'VANDAL & CRIM MISCH - CRIM MISCH $500>$1499.99',
    '14082': 'VANDAL & CRIM MISCH - CRIM MISCH $20>$499.99',
    '14083': 'VANDAL & CRIM MISCH - CRIM MISCH U$20',
    '14091': 'VANDAL & CRIM MISCH - CRIM MISCH O$20,000',
    '14092': 'VANDAL & CRIM MISCH - CRIM MISCH $1500>$19,999.99',
    '14100': 'VANDAL & CRIM MISCH - CRUELTY TO ANIMALS',
    '14120': 'VANDAL & CRIM MISCH - ATTEMPTS OF ABOVE',
    '15010': 'CPW CH TO WEAPON - UCW - NOT HANDGUN',
    '15020': 'CPW CH TO WEAPON - UNLAW TRANSFER OF FIREARM',
    '15030': 'CPW CH TO WEAPON - UCW HANDGUN ONLY',
    '15040': 'CPW CH TO WEAPON - UNLAW POSS FIREARM FELON',
    '15050': 'CPW CH TO WEAPON - UNLAW POSS/MFG/TRF/SALE',
    '15060': 'CPW CH TO WEAPON - VIOL CONCEAL WEAP LAW',
    '16010': 'PROSTI COMMER VICE - PROSTITUTION',
    '16020': 'PROSTI COMMER VICE - PROMOTE PROSTITUTION',
    '16030': 'PROSTI COMMER VICE - COMPELL-PROSTITUTION',
    '17020': 'SEX OFF & INDEC COND - PROH SEX COND ADULT INCEST',
    '17030': 'SEX OFF & INDEC COND - INDECENT EXPOSURE',
    '17031': 'SEX OFF & INDEC COND - INDECENT EXPOSE ADULT FV',
    '17040': 'SEX OFF & INDEC COND - INDEC W/CHILD(EXPOSE ONLY)',
    '17041': 'SEX OFF & INDEC COND - INDECENCY W/CHILD EXPOS FV',
    '17050': 'SEX OFF & INDEC COND - PUBLIC LEWDNESS',
    '17060': 'SEX OFF & INDEC COND - HOMOSEXUAL CONDUCT',
    '17061': 'SEX OFF & INDEC COND - HOMOSEXUAL CONDUCT FV',
    '17080': 'SEX OFF & INDEC COND - SEXUAL ASSAULT',
    '17081': 'SEX OFF & INDEC COND - SEXUAL ASSAULT FV',
    '17090': 'SEX OFF & INDEC COND - AGG SEX ASSAULT',
    '17091': 'SEX OFF & INDEC COND - AGG SEX ASSAULT FV',
    '17100': 'SEX OFF & INDEC COND - RAPE CHILD NO FORCE PENETR',
    '17140': 'SEX OFF & INDEC COND - INDEC W/CHILD CONTACT FONDLING ONLY',
    '17141': 'SEX OFF & INDEC COND - INDEC W/CHILD CONTACT FV',
    '17190': 'SEX OFF & INDEC COND - AGG SEX ASSAULT JUV',
    '18111': 'NARC & DRUGS - NARCOTICS - HEROIN - POSSESSION',
    '18112': 'NARC & DRUGS - NARCOTICS - HEROIN - SALE',
    '18113': 'NARC & DRUGS - NARCOTICS - HEROIN - MAKE',
    '18114': 'NARC & DRUGS - NARCOTICS - HEROIN - DEALER POSS',
    '18121': 'NARC & DRUGS - NARCOTICS - COCAINE - POSSESSION',
    '18122': 'NARC & DRUGS - NARCOTICS - COCAINE - SALE',
    '18123': 'NARC & DRUGS - NARCOTICS - COCAINE - MAKE',
    '18124': 'NARC & DRUGS - NARCOTICS - COCAINE - DEALER POSS',
    '18131': 'NARC & DRUGS - NARCOTICS - OTHER NARCOTICS - POSSESSION',
    '18132': 'NARC & DRUGS - NARCOTICS - OTHER NARCOTICS - SALE',
    '18133': 'NARC & DRUGS - NARCOTICS - OTHER NARCOTICS - MAKE',
    '18134': 'NARC & DRUGS - NARCOTICS - OTHER NARCOTICS - DEALER POSS',
    '18141': 'NARC & DRUGS - NARCOTICS - OPIATES - POSSESSION',
    '18142': 'NARC & DRUGS - NARCOTICS - OPIATES - SALE',
    '18143': 'NARC & DRUGS - NARCOTICS - OPIATES - MAKE',
    '18144': 'NARC & DRUGS - NARCOTICS - OPIATES - DEALER POSS',
    '18151': 'NARC & DRUGS - NARCOTICS - PARAPHERNALIA NARCOTICS - POSSESSION',
    '18152': 'NARC & DRUGS - NARCOTICS - PARAPHERNALIA NARCOTICS - SALE',
    '18153': 'NARC & DRUGS - NARCOTICS - PARAPHERNALIA NARCOTICS - MAKE',
    '18154': 'NARC & DRUGS - NARCOTICS - PARAPHERNALIA NARCOTICS - DEALER POSS',
    '18211': 'NARC & DRUGS - SYNTHETICS - METHADONE - POSSESSION',
    '18212': 'NARC & DRUGS - SYNTHETICS - METHADONE - SALE',
    '18213': 'NARC & DRUGS - SYNTHETICS - METHADONE - MAKE',
    '18214': 'NARC & DRUGS - SYNTHETICS - METHADONE - DEALER POSS',
    '18221': 'NARC & DRUGS - SYNTHETICS - OTHER SYNTHETICS - POSSESSION',
    '18222': 'NARC & DRUGS - SYNTHETICS - OTHER SYNTHETICS - SALE',
    '18223': 'NARC & DRUGS - SYNTHETICS - OTHER SYNTHETICS - MAKE',
    '18224': 'NARC & DRUGS - SYNTHETICS - OTHER SYNTHETICS - DEALER POSS',
    '18231': 'NARC & DRUGS - SYNTHETICS - STEROIDS - POSSESSION',
    '18232': 'NARC & DRUGS - SYNTHETICS - STEROIDS - SALE',
    '18233': 'NARC & DRUGS - SYNTHETICS - STEROIDS - MAKE',
    '18234': 'NARC & DRUGS - SYNTHETICS - STEROIDS - DEALER POSS',
    '18311': 'NARC & DRUGS - DANGEROUS NON-NARC - BARBITURATES - POSSESSION',
    '18312': 'NARC & DRUGS - DANGEROUS NON-NARC - BARBITURATES - SALE',
    '18313': 'NARC & DRUGS - DANGEROUS NON-NARC - BARBITURATES - MAKE',
    '18314': 'NARC & DRUGS - DANGEROUS NON-NARC - BARBITURATES - DEALER POSS',
    '18321': 'NARC & DRUGS - DANGEROUS NON-NARC - AMPHETAMINE - POSSESSION',
    '18322': 'NARC & DRUGS - DANGEROUS NON-NARC - AMPHETAMINE - SALE',
    '18323': 'NARC & DRUGS - DANGEROUS NON-NARC - AMPHETAMINE - MAKE',
    '18324': 'NARC & DRUGS - DANGEROUS NON-NARC - AMPHETAMINE - DEALER POSS',
    '18331': 'NARC & DRUGS - DANGEROUS NON-NARC - METHAMPHETAMINE - POSSESSION',
    '18332': 'NARC & DRUGS - DANGEROUS NON-NARC - METHAMPHETAMINE - SALE',
    '18333': 'NARC & DRUGS - DANGEROUS NON-NARC - METHAMPHETAMINE - MAKE',
    '18334': 'NARC & DRUGS - DANGEROUS NON-NARC - METHAMPHETAMINE - DEALER POSS',
    '18341': 'NARC & DRUGS - DANGEROUS NON-NARC - OTHER DANG DRUG NON-NARC - POSSESSION',
    '18342': 'NARC & DRUGS - DANGEROUS NON-NARC - OTHER DANG DRUG NON-NARC - SALE',
    '18343': 'NARC & DRUGS - DANGEROUS NON-NARC - OTHER DANG DRUG NON-NARC - MAKE',
    '18344': 'NARC & DRUGS - DANGEROUS NON-NARC - OTHER DANG DRUG NON-NARC - DEALER POSS',
    '18351': 'NARC & DRUGS - DANGEROUS NON-NARC - LSD - POSSESSION',
    '18352': 'NARC & DRUGS - DANGEROUS NON-NARC - LSD - SALE',
    '18353': 'NARC & DRUGS - DANGEROUS NON-NARC - LSD - MAKE',
    '18354': 'NARC & DRUGS - DANGEROUS NON-NARC - LSD - DEALER POSS',
    '18361': 'NARC & DRUGS - DANGEROUS NON-NARC - OTHER HALLUCINOGENS - POSSESSION',
    '18362': 'NARC & DRUGS - DANGEROUS NON-NARC - OTHER HALLUCINOGENS - SALE',
    '18363': 'NARC & DRUGS - DANGEROUS NON-NARC - OTHER HALLUCINOGENS - MAKE',
    '18364': 'NARC & DRUGS - DANGEROUS NON-NARC - OTHER HALLUCINOGENS - DEALER POSS',
    '18371': 'NARC & DRUGS - DANGEROUS NON-NARC - TRANQUILIZERS - POSSESSION',
    '18372': 'NARC & DRUGS - DANGEROUS NON-NARC - TRANQUILIZERS - SALE',
    '18373': 'NARC & DRUGS - DANGEROUS NON-NARC - TRANQUILIZERS - MAKE',
    '18374': 'NARC & DRUGS - DANGEROUS NON-NARC - TRANQUILIZERS - DEALER POSS',
    '18381': 'NARC & DRUGS - DANGEROUS NON-NARC - PARAPHERNALIS DANG DRUG - POSSESSION',
    '18382': 'NARC & DRUGS - DANGEROUS NON-NARC - PARAPHERNALIS DANG DRUG - SALE',
    '18383': 'NARC & DRUGS - DANGEROUS NON-NARC - PARAPHERNALIS DANG DRUG - MAKE',
    '18384': 'NARC & DRUGS - DANGEROUS NON-NARC - PARAPHERNALIS DANG DRUG - DEALER POSS',
    '18411': 'NARC & DRUGS - MARIJUANA - MARIJUANA - POSSESSION',
    '18412': 'NARC & DRUGS - MARIJUANA - MARIJUANA - SALE',
    '18413': 'NARC & DRUGS - MARIJUANA - MARIJUANA - MAKE',
    '18414': 'NARC & DRUGS - MARIJUANA - MARIJUANA - DEALER POSS',
    '18415': 'NARC & DRUGS - MARIJUANA - MARIJUANA - GROWING',
    '18421': 'NARC & DRUGS - MARIJUANA - HASHISH - POSSESSION',
    '18422': 'NARC & DRUGS - MARIJUANA - HASHISH - SALE',
    '18423': 'NARC & DRUGS - MARIJUANA - HASHISH - MAKE',
    '18424': 'NARC & DRUGS - MARIJUANA - HASHISH - DEALER POSS',
    '18425': 'NARC & DRUGS - MARIJUANA - HASHISH - GROWING',
    '18431': 'NARC & DRUGS - MARIJUANA - MARIJUANA PARAPHERNALIS - POSSESSION',
    '18432': 'NARC & DRUGS - MARIJUANA - MARIJUANA PARAPHERNALIS - SALE',
    '18433': 'NARC & DRUGS - MARIJUANA - MARIJUANA PARAPHERNALIS - MAKE',
    '18434': 'NARC & DRUGS - MARIJUANA - MARIJUANA PARAPHERNALIS - DEALER POSS',
    '18435': 'NARC & DRUGS - MARIJUANA - MARIJUANA PARAPHERNALIS - GROWING',
    '18711': 'NARC & DRUGS - PRESCRIPTION FORGERY - NARCOTICS PRESCRIPTION - POSSESSION',
    '18712': 'NARC & DRUGS - PRESCRIPTION FORGERY - NARCOTICS PRESCRIPTION - SALE',
    '18713': 'NARC & DRUGS - PRESCRIPTION FORGERY - NARCOTICS PRESCRIPTION - MAKE',
    '18714': 'NARC & DRUGS - PRESCRIPTION FORGERY - NARCOTICS PRESCRIPTION - DEALER POSS',
    '18721': 'NARC & DRUGS - PRESCRIPTION FORGERY - DANGEROUS DRUG PRESCRIPTION - POSSESSION',
    '18722': 'NARC & DRUGS - PRESCRIPTION FORGERY - DANGEROUS DRUG PRESCRIPTION - SALE',
    '18723': 'NARC & DRUGS - PRESCRIPTION FORGERY - DANGEROUS DRUG PRESCRIPTION - MAKE',
    '18724': 'NARC & DRUGS - PRESCRIPTION FORGERY - DANGEROUS DRUG PRESCRIPTION - DEALER POSS',
    '19020': 'GAMBLING - GAMBLING >>> ARREST ONLY',
    '19030': 'GAMBLING - GAMBLING PROMOTION',
    '19040': 'GAMBLING - KEEP GAMBLING PLACE',
    '19050': 'GAMBLING - COMMON GAMBLING INFORMA',
    '19060': 'GAMBLING - POSSESS GAMBLING DEVICE >>>ARREST ONLY',
    '19070': 'GAMBLING - POSSESS GAMBLING PARAPH >>ARREST ONLY',
    '20010': 'CHILD (OFF AGAINST CHILD) - CHILD PORNOGRAPHY',
    '20020': 'CHILD (OFF AGAINST CHILD) - ENTICE/AGREE TO ABDUCT CHILD',
    '20030': 'CHILD (OFF AGAINST CHILD) - INTERFERE W/CHILD CUSTODY',
    '20040': 'CHILD (OFF AGAINST CHILD) - PROTECTIVE CUSTODY',
    '20051': 'CHILD (OFF AGAINST CHILD) - SALE OF CHILD',
    '20052': 'CHILD (OFF AGAINST CHILD) - PURCHASE OF CHILD',
    '20060': 'CHILD (OFF AGAINST CHILD) - EMPLOY HARMFUL TO MINOR',
    '20070': 'CHILD (OFF AGAINST CHILD) - PROTECTIVE ORDERS',
    '20080': 'CHILD (OFF AGAINST CHILD) - ENDANGER/ABANDON CHILD',
    '21010': 'DWI - OPER MOT VEH-ALCOHOL',
    '21011': 'DWI - OPER MOT VEH-DRUGS',
    '21020': 'DWI - OPER OTHER VEH-ALCOHOL',
    '21021': 'DWI - OPER OTHER VEH-DRUGS',
    '22010': 'LIQUOR - MFG/SALE/POSSESS',
    '22020': 'LIQUOR - OPEN BAR',
    '22030': 'LIQUOR - BOOTLEGGING',
    '22040': 'LIQUOR - OPERATE STILL',
    '22050': 'LIQUOR - LIQUOR TO MINORS',
    '22060': 'LIQUOR - ILLEGAL TRANSPORTATION ALC',
    '22070': 'LIQUOR - DRINK ON PUBL TRANSPORTATION',
    '22080': 'LIQUOR - ATTEMPT ALL ABOVE',
    '22090': 'LIQUOR - DRINKING ON PREMISES',
    '22100': 'LIQUOR - SELL WITHOUT LICENSE',
    '22110': 'LIQUOR - SELL ON ELECTION DAY',
    '22120': 'LIQUOR - MINOR POSSESSION',
    '23010': 'DRUNK & DISORD - PUBLIC INTOXICATION',
    '23020': 'DRUNK & DISORD - DRUNK &DISORDERLY IN A CAR',
    '24020': 'DISORDERLY CONDUCT - AFFRAY',
    '24030': 'DISORDERLY CONDUCT - DISRUPT MEET/PROCESSION',
    '24040': 'DISORDERLY CONDUCT - HINDER PROCEEDINGS',
    '24050': 'DISORDERLY CONDUCT - DISTURBING PUBL PEACE',
    '24100': 'DISORDERLY CONDUCT - RELEASING ODORS',
    '24110': 'DISORDERLY CONDUCT - REFUSE ASSIST PEACE OFFICER',
    '24120': 'DISORDERLY CONDUCT - SLEEP IN PUBL',
    '24140': 'DISORDERLY CONDUCT - HINDER APPREH/PROSECUTE',
    '24210': 'DISORDERLY CONDUCT - SKATE BOARD ORD',
    '24230': 'DISORDERLY CONDUCT - HARASSING PHONE CALL',
    '24240': 'DISORDERLY CONDUCT - THREATENING PHONE CALL',
    '24250': 'DISORDERLY CONDUCT - HARASSMENT STALKING',
    '24260': 'DISORDERLY CONDUCT - PEEPING TOM',
    '24270': 'DISORDERLY CONDUCT - THREATENING LETTER',
    '24280': 'DISORDERLY CONDUCT - OBSCENE PHONE CALLS',
    '26020': 'OTHERS - BIGAMY',
    '26040': 'OTHERS - BRIBERY OF VOTER',
    '26050': 'OTHERS - BOND FORFEITURE',
    '26060': 'OTHERS - BRIBE PUBLIC SERVANT',
    '26070': 'OTHERS - BUILDING ORD',
    '26080': 'OTHERS - ZONING',
    '26090': 'OTHERS - ENGAGE ORGAN CRIMINAL ACT',
    '26100': 'OTHERS - BAIL JUMP/FTA',
    '26110': 'OTHERS - CURFEW LAWS',
    '26140': 'OTHERS - FAIL TO STOP/RENDER AID',
    '26150': 'OTHERS - SIDEWALK DISPLAYS',
    '26160': 'OTHERS - ACCEPT BRIBE-PUBL SERV',
    '26170': 'OTHERS - ELECTRICAL ORD',
    '26180': 'OTHERS - FAIL TO ID AS WITNESS',
    '26190': 'OTHERS - EXTORTION-BLACKMAIL',
    '26200': 'OTHERS - FIREWORKS ORD',
    '26210': 'OTHERS - FALSE REPORT TO POLICE',
    '26220': 'OTHERS - FUGITIVES',
    '26230': 'OTHERS - 9-1-1 ABUSE',
    '26260': 'OTHERS - FIRE ORD',
    '26270': 'OTHERS - FALSE ALARM/REPORT',
    '26280': 'OTHERS - IMPROPER INFLUENCE',
    '26290': 'OTHERS - TAMPER WITH WITNESS',
    '26330': 'OTHERS - KIDNAPPING',
    '26331': 'OTHERS - FALSE IMPRISONMENT',
    '26340': 'OTHERS - LUNACY',
    '26350': 'OTHERS - OFFER GIFT PUBL SERV',
    '26360': 'OTHERS - COMMERCIAL BRIBERY',
    '26370': 'OTHERS - CRIMINAL SOLICITATION',
    '26390': 'OTHERS - OFFICIAL MISCONDUCT',
    '26400': 'OTHERS - PERJURY-FALSE SWEARING',
    '26410': 'OTHERS - PEDDLERS LICENSE',
    '26420': 'OTHERS - EVADING ARREST',
    '26430': 'OTHERS - UNLAW USE CRIM INTRUMENT',
    '26440': 'OTHERS - POSSESS/SELL OBSCENE LIT',
    '26441': 'OTHERS - DISPLAY/DISTR OBSCENE MAT',
    '26450': 'OTHERS - BARRATRY',
    '26460': 'OTHERS - ESCAPE',
    '26461': 'OTHERS - PERMIT ESCAPE',
    '26462': 'OTHERS - PROVIDE IMPLEMENT ESCAPE',
    '26471': 'OTHERS - MISAPPROPRIATE O$10,,000',
    '26472': 'OTHERS - MISAPPROPRIATE $200>$10,000',
    '26473': 'OTHERS - MISAPPROPRIATE U$200',
    '26480': 'OTHERS - RESISTING SEARCH/TRANSP',
    '26490': 'OTHERS - RIOT',
    '26500': 'OTHERS - REFUSE/FAIL TO OBEY OFFIC',
    '26510': 'OTHERS - SMOKING ORD',
    '26530': 'OTHERS - CRIMINAL TRESPASS',
    '26531': 'OTHERS - INTERFERE PUB DUTY-ANIMAL',
    '26540': 'OTHERS - VIOLATE TAXI ORD',
    '26550': 'OTHERS - IMPERSONATE PUBL SERV',
    '26560': 'OTHERS - TRUANCY',
    '26570': 'OTHERS - POSSESS COMP OF EXPLOSIVE',
    '26580': 'OTHERS - COERCE PUBL SERV',
    '26590': 'OTHERS - TAKE WEAPON-PEACE OFF',
    '26610': 'OTHERS - INTERFERE W/PUBLIC DUTIES',
    '26620': 'OTHERS - VIOLATION WRECKER ORD',
    '26630': 'OTHERS - ABUSE OF OFFICIAL CAPACITY',
    '26640': 'OTHERS - UNLAWFUL COMMUNICATION',
    '26660': 'OTHERS - DANCE HALL ORD',
    '26670': 'OTHERS - VIOLATE AMBULANCE ORD',
    '26680': 'OTHERS - VIOLATE ANTI-NOISE ORD',
    '26750': 'OTHERS - JUNK VEH ORD',
    '26760': 'OTHERS - TERRORISTIC THREATS',
    '26770': 'OTHERS - GLUE SNIFFING',
    '26790': 'OTHERS - DRIVE-IN',
    '26800': 'OTHERS - DOG LEASH LAW',
    '26810': 'OTHERS - CAPIAS PROFINE',
    '26820': 'OTHERS - ALIAS TICKETS',
    '26830': 'OTHERS - RETALIATIONS',
    '26840': 'OTHERS - HITCH-HIKING',
    '26860': 'OTHERS - PHYSICAL EVID TAMPER/FABRIC',
    '26890': 'OTHERS - VIOL U.S. IMMIGRATION LAW',
    '26930': 'OTHERS - ABUSE OF CORPSE',
    '26940': 'OTHERS - OBSTRUCT PUBL PASSAGE',
    '26960': 'OTHERS - HARBORING A RUNAWAY',
    '26970': 'OTHERS - VIOL ALARM ORD',
    '29010': 'RUNAWAY - JUV MALE 10-16 INCLUSIVE',
    '29020': 'RUNAWAY - JUV FEMALE 10-16 INCLUSIVE',
    '30011': 'TRAFFIC HAZARDOUS - SPEEDING OVER LIMIT',
    '30012': 'TRAFFIC HAZARDOUS - SPEEDING FOR CONDITIONS',
    '30013': 'TRAFFIC HAZARDOUS - MINIMUM SPEED VIOLATION',
    '30020': 'TRAFFIC HAZARDOUS - STOP SIGN',
    '30030': 'TRAFFIC HAZARDOUS - RED LIGHT',
    '30040': 'TRAFFIC HAZARDOUS - FLASHING RED LIGHT',
    '30050': 'TRAFFIC HAZARDOUS - VIOL TRAFFIC CONT DEVICE',
    '30060': 'TRAFFIC HAZARDOUS - FAILURE TO OBEY OFFICER',
    '30070': 'TRAFFIC HAZARDOUS - FAILURE TO SIGNAL',
    '30081': 'TRAFFIC HAZARDOUS - FAIL TO YIELD/MOT VEH',
    '30082': 'TRAFFIC HAZARDOUS - FAIL TO YIELD/PEDESTRIAN',
    '30083': 'TRAFFIC HAZARDOUS - FAIL TO YIELD TO EMERG VEH',
    '30091': 'TRAFFIC HAZARDOUS - IMPROP TURN/WRONG LANE',
    '30092': 'TRAFFIC HAZARDOUS - TURN PROHIBITED',
    '30093': 'TRAFFIC HAZARDOUS - IMPROPER TURN/OTHER',
    '30100': 'TRAFFIC HAZARDOUS - IMPROPER PASSING',
    '30110': 'TRAFFIC HAZARDOUS - ILLEGAL PASSING',
    '30120': 'TRAFFIC HAZARDOUS - IMPROPER LANE CHANGE',
    '30130': 'TRAFFIC HAZARDOUS - ILLEGAL BACKING',
    '30140': 'TRAFFIC HAZARDOUS - DR WRONG SIDE OF STREET',
    '30150': 'TRAFFIC HAZARDOUS - DR WRONG WAY/ONE WAY ST',
    '30161': 'TRAFFIC HAZARDOUS - NEG COLL PROP W/MOT VEH',
    '30162': 'TRAFFIC HAZARDOUS - NEG COLL W/FIXED OBJECT',
    '30163': 'TRAFFIC HAZARDOUS - NEG COLL OTHER',
    '30171': 'TRAFFIC HAZARDOUS - NEG COLL INJ W/MOT VEH',
    '30172': 'TRAFFIC HAZARDOUS - NEG COLL INJ W/FIXED OBJECT',
    '30173': 'TRAFFIC HAZARDOUS - NEG COLL W/BICYCLIST',
    '30174': 'TRAFFIC HAZARDOUS - NEG COLL W/PEDESTRIAN',
    '30175': 'TRAFFIC HAZARDOUS - NEG COLL OTHER',
    '30180': 'TRAFFIC HAZARDOUS - NEG COLL W/JUV',
    '30190': 'TRAFFIC HAZARDOUS - FOLLOW TOO CLOSE',
    '30200': 'TRAFFIC HAZARDOUS - DEFECTIVE LIGHTS',
    '30210': 'TRAFFIC HAZARDOUS - FAIL TO DIM LIGHTS',
    '30220': 'TRAFFIC HAZARDOUS - DRIVERS LIC-VIOL RESTRICT',
    '30230': 'TRAFFIC HAZARDOUS - MOTORCYCLIST-NO OPS',
    '30240': 'TRAFFIC HAZARDOUS - NO MOTORCYCLIST HELMET',
    '30251': 'TRAFFIC HAZARDOUS - JAYWALK',
    '30252': 'TRAFFIC HAZARDOUS - PED-WALK ON RED LIGHT',
    '30254': 'TRAFFIC HAZARDOUS - PED-OTHER VIOLATION',
    '30260': 'TRAFFIC HAZARDOUS - CLINGING TO MOTOR VEH',
    '30270': 'TRAFFIC HAZARDOUS - UNLAWFUL RIDING MOT VEH',
    '30280': 'TRAFFIC HAZARDOUS - RACING',
    '30300': 'TRAFFIC HAZARDOUS - CROSSING FIRE HOSE',
    '30310': 'TRAFFIC HAZARDOUS - OTHER VIOLATION',
    '41011': 'MISSING PERSON - ADULT W/M',
    '41012': 'MISSING PERSON - ADULT B/M',
    '41013': 'MISSING PERSON - ADULT W/F',
    '41014': 'MISSING PERSON - ADULT B/F',
    '41015': 'MISSING PERSON - JUV W/M U10 CRITICAL MISSING',
    '41016': 'MISSING PERSON - JUV B/M U10 CRITICAL MISSING',
    '41017': 'MISSING PERSON - JUV W/F U10 CRITICAL MISSING',
    '41018': 'MISSING PERSON - JUV B/F U10 CRITICAL MISSING',
    '41021': 'MISSING PERSON - WANT TO LOC ADULT W/M',
    '41022': 'MISSING PERSON - WANT TO LOC ADULT B/M',
    '41023': 'MISSING PERSON - WANT TO LOC ADULT W/F',
    '41024': 'MISSING PERSON - WANT TO LOC ADULT B/F',
    '42010': 'LOST - ANIMAL',
    '42011': 'LOST - WANT TO LOCATE ANIMAL',
    '42020': 'LOST - PROPERTY',
    '42021': 'LOST - WANT TO LOCATE PROPERTY',
    '43010': 'FOUND - ANIMAL',
    '43020': 'FOUND - PROPERTY',
    '43021': 'FOUND - FIREARM',
    '43030': 'FOUND - OT STOLEN,LOCALLY REC VEH',
    '43040': 'FOUND - OT STOLEN,LOCALLY REC PROP',
    '44000': 'AIRPLANE - AIRPLANE CRASH - NO INJURIES',
    '45300': 'HOLD - FEDERAL',
    '45301': 'HOLD - FUGITIVE WARRANT',
    '45302': 'HOLD - SHERIFF',
    '45303': 'HOLD - MILITARY',
    '45304': 'HOLD - IN-TRANSIT',
    '45305': 'HOLD - REGIONAL CITIES',
    '45306': 'HOLD - OTHERS',
    '45307': 'HOLD - IMMIGRATION',
    '45308': 'HOLD - PROBATION VIOLATION',
    '45309': 'HOLD - PAROLE VIOLATION',
}
########NEW FILE########
__FILENAME__ = retrieval
"""
Screen scraper for Dallas restaurant inspections.
http://www2.dallascityhall.com/FoodInspection/SearchScores.cfm
"""

from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem, Location
from ebpub.utils.dates import parse_date
from ebpub.utils.text import smart_title
from django.utils import simplejson as json
import md5
import re

class RestaurantScraper(NewsItemListDetailScraper):
    schema_slugs = ('restaurant-inspections',)
    has_detail = False
    parse_list_re = re.compile(r'(?m)<tr[^>]*>\s+<td[^>]*>(?P<name>.*?)</td>\s+<!--<td[^>]*><a href="[^"]*">.*?</a></td>-->\s+<td[^>]*><a href="[^"]*">(?P<address>.*?)</a></td>\s+<!--\s+<td[^>]*>.*?</td>\s+-->\s+<!--\s+<td[^>]*>.*?</td>\s+-->\s+<!--\s+<td[^>]*>.*?</td>\s+-->\s+<!--\s+<td[^>]*>.*?</td>\s+-->\s+<td[^>]*>(?P<suite>.*?)</td>\s+<td[^>]*>(?P<zipcode>.*?)</td>\s+<td[^>]*>(?P<mapsco>.*?)</td>\s+<td[^>]*>(?P<inspection_date>.*?)</td>\s+<td[^>]*>(?P<score>\d+?)</td>\s+<td[^>]*>(?P<inspection_type>.*?)</td>\s+</tr>')
    next_re = re.compile(r'<a href="(.*?)">Next</a>')

    def list_pages(self):
        uri = 'http://www2.dallascityhall.com/FoodInspection/SearchScoresAction.cfm'
        zip_codes = [l.name for l in Location.objects.filter(location_type__slug='zipcodes')]
        for zipcode in zip_codes:
            params = {
                'NAME': '',
                'STNO': '',
                'STNAME': '',
                'ZIP': str(zipcode),
                'Submit': 'Search+Scores'
            }
            html = self.get_html(uri, params)
            yield html
            while 1:
                m = self.next_re.search(html)
                if m is None:
                    break
                next_uri = 'http://www2.dallascityhall.com/FoodInspection/' + m.group(1)
                html = self.get_html(next_uri)
                yield html

    def clean_list_record(self, record):
        record['inspection_date'] = parse_date(record['inspection_date'], '%m/%d/%Y')
        record['address'] = re.sub(r'\s+', ' ', record['address'])

        score = int(record['score'])
        if score <= 59:
            record['result'] = 'UNACCEPTABLE'
        elif score <= 69 and score >= 60:
            record['result'] = 'FAILING'
        elif score <= 79 and score >= 70:
            record['result'] = 'PASSING'
        elif score <= 89 and score >= 80:
            record['result'] = 'GOOD'
        elif score <= 100 and score >= 90:
            record['result'] = 'VERY_GOOD'
        return record

    def existing_record(self, record):
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id, item_date=record['inspection_date'])
            qs = qs.filter(location_name=record['address'])
            qs = qs.by_attribute(self.schema_fields['name'], record['name'])
            return qs[0]
        except IndexError:
            return None

    def save(self, old_record, list_record, detail_record):
        inspection_type = self.get_or_create_lookup('inspection_type', list_record['inspection_type'], list_record['inspection_type'])
        result = self.get_or_create_lookup('result', list_record['result'], list_record['result'])

        # Make up a unique id so we can show other inspections at this
        # facility on the detail page.
        eb_facility_id = md5.new('%s:%s' % (list_record['name'], list_record['address'])).hexdigest()
        json_data = {
            'zipcode': list_record['zipcode'],
            'suite': list_record['suite'],
            'mapsco': list_record['mapsco']
        }
        kwargs = {
            'title': smart_title(list_record['name'].decode('utf-8')),
            'item_date': list_record['inspection_date'],
            'location_name': list_record['address']
        }
        attributes = {
            'name': list_record['name'].decode('utf-8'),
            'inspection_type': inspection_type.id,
            'result': result.id,
            'score': list_record['score'],
            'eb_facility_id': eb_facility_id,
            'json': json.dumps(json_data)
        }
        if old_record is None:
            self.create_newsitem(attributes, **kwargs)
        else:
            self.update_existing(old_record, kwargs, attributes)

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    RestaurantScraper().update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Import script for DC construction projects.
http://data.octo.dc.gov/Main_DataCatalog_Go.aspx?category=0&view=All
Metadata: http://data.octo.dc.gov/Metadata.aspx?id=13
"""

from django.contrib.gis.geos import Point
from ebdata.retrieval.scrapers.list_detail import SkipRecord
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date
from cStringIO import StringIO
from lxml import etree
import datetime
import re
import zipfile

class ConstructionProjectScraper(NewsItemListDetailScraper):
    schema_slugs = ('road-construction',)
    has_detail = False

    project_xml_sources = (('ccp', 'ccp', 'CCProject'), ('cmpltcp', 'cmpltcp', 'CompletedCProject'))

    def list_pages(self):
        # Download the latest ZIP file, extract it and yield the XML file
        # within. Note that there are two files to check -- the current open
        # projects and the closed projects. We check the closed projects
        # because a project might be removed from the open projects feed.
        for url, file_name, xml_name in self.project_xml_sources:
            z = self.get_html('http://data.octo.dc.gov/feeds/%s/%s_current_plain.zip' % (url, file_name))
            zf = zipfile.ZipFile(StringIO(z))
            yield zf.read('%s_current_plain.xml' % file_name), xml_name

    def parse_list(self, bunch):
        text, xml_name = bunch
        xml = etree.fromstring(text)
        strip_ns = re.compile(r'^\{.*?\}').sub
        for el in xml.findall('{http://dc.gov/dcstat/types/1.0/}%s' % xml_name):
            yield dict([(strip_ns('', child.tag), child.text) for child in el])
    
    def clean_list_record(self, record):
        if record['lat'] == '0' or record['long'] == '0':
            raise SkipRecord('Got 0 for lat/long')
        record['lat'] = float(record['lat'])
        record['long'] = float(record['long'])
        record['location_name'] = '%s %s, from %s to %s' % (record['street'], record['quadrant'], record['fromintersection'], record['tointersection'])
        # record['order_date'] = parse_date(record['serviceorderdate'].split('T')[0], '%Y-%m-%d')
        # record['add_date'] = parse_date(record['adddate'].split('T')[0], '%Y-%m-%d')
        if record['estimatedcompletiondate']:
            record['estimated_completion_date'] = parse_date(record['estimatedcompletiondate'].split('T')[0], '%Y-%m-%d')
        else:
            record['estimated_completion_date'] = None
        if record['actualcompletiondate']:
            record['actual_completion_date'] = parse_date(record['actualcompletiondate'].split('T')[0], '%Y-%m-%d')
        else:
            record['actual_completion_date'] = None
        if record['estimatedstartdate']:
            record['estimated_start_date'] = parse_date(record['estimatedstartdate'].split('T')[0], '%Y-%m-%d')
        else:
            record['estimated_start_date'] = None
        record['project_name'] = record['projectname'] or ''
        record['road_type'] = record['functionalclass'] or 'N/A'
        return record

    def existing_record(self, record):
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id)
            qs = qs.by_attribute(self.schema_fields['project_id'], record['projectid'])
            return qs[0]
        except IndexError:
            return None

    def save(self, old_record, list_record, detail_record):
        work_description = self.get_or_create_lookup('work_description', list_record['workdescription'], list_record['workdescription'])
        road_type = self.get_or_create_lookup('road_type', list_record['road_type'], list_record['road_type'])
        status = self.get_or_create_lookup('status', list_record['status'], list_record['status'])
        attributes = {
            'project_id': list_record['projectid'],
            'project_name': list_record['project_name'],
            'remarks': list_record['remarks'],
            'miles': list_record['miles'],
            'num_blocks': list_record['noofblocks'],
            'percent_completed': list_record['percentcompleted'],
            'work_description': work_description.id,
            'road_type': road_type.id,
            'status': status.id,
            'estimated_completion_date': list_record['estimated_completion_date'],
            'actual_completion_date': list_record['actual_completion_date'],
            'estimated_start_date': list_record['estimated_start_date'],
        }
        title = '%s on %s' % (work_description.name, list_record['street'])
        if old_record is None:
            self.create_newsitem(
                attributes,
                title=title,
                item_date=datetime.date.today(),
                location=Point(list_record['long'], list_record['lat']),
                location_name=list_record['location_name'],
            )
        else:
            # TODO: Include location Point object in new_values?
            new_values = {'title': title, 'location_name': list_record['location_name']}
            self.update_existing(old_record, new_values, attributes)

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    ConstructionProjectScraper().update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Import script for DC crime data.
http://data.octo.dc.gov/Main_DataCatalog_Go.aspx?category=6&view=All
Metadata: http://data.octo.dc.gov/Metadata.aspx?id=3
"""

from django.contrib.gis.geos import Point
from ebdata.retrieval.scrapers.list_detail import SkipRecord
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date
from cStringIO import StringIO
from lxml import etree
import re
import zipfile

class CrimeScraper(NewsItemListDetailScraper):
    schema_slugs = ('crime',)
    has_detail = False

    def list_pages(self):
        # Download the latest ZIP file, extract it and yield the XML file within.
        z = self.get_html('http://data.octo.dc.gov/feeds/crime_incidents/crime_incidents_current_plain.zip')
        zf = zipfile.ZipFile(StringIO(z))
        yield zf.read('crime_incidents_current_plain.xml')

    def parse_list(self, text):
        xml = etree.fromstring(text)
        strip_ns = re.compile(r'^\{.*?\}').sub
        for el in xml.findall('{http://dc.gov/dcstat/types/1.0/}ReportedCrime'):
            yield dict([(strip_ns('', child.tag), child.text) for child in el])

    def clean_list_record(self, record):
        if record['lat'] == '0' or record['long'] == '0':
            raise SkipRecord('Got 0 for lat/long')
        record['lat'] = float(record['lat'])
        record['long'] = float(record['long'])
        record['report_date'] = parse_date(record['reportdatetime'].split('T')[0], '%Y-%m-%d')
        record['address'] = record['blocksiteaddress'].replace(' B/O ', ' block of ')
        if record['narrative'].upper() == 'NO NARRATIVE IS AVAILABLE.':
            record['narrative'] == 'Not available'
        return record

    def existing_record(self, record):
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id)
            qs = qs.by_attribute(self.schema_fields['nid'], record['nid'])
            return qs[0]
        except IndexError:
            return None

    def save(self, old_record, list_record, detail_record):
        # The method is relative to offense, so create a multi-value key for it.
        method_code = '%s -- %s' % (list_record['offense'], list_record['method'])

        offense = self.get_or_create_lookup('offense', list_record['offense'], list_record['offense'])
        method = self.get_or_create_lookup('method', method_code, method_code)
        shift = self.get_or_create_lookup('shift', list_record['shift'], list_record['shift'])
        attributes = {
            'nid': list_record['nid'],
            'crime_control_number': list_record['ccn'],
            'narrative': list_record['narrative'],
            'offense': offense.id,
            'method': method.id,
            'shift': shift.id,
        }
        title = method.name
        if old_record is None:
            self.create_newsitem(
                attributes,
                title=title,
                item_date=list_record['report_date'],
                location=Point(list_record['long'], list_record['lat']),
                location_name=list_record['address'],
            )
        else:
            # TODO: Include location Point object in new_values?
            new_values = {'title': title, 'item_date': list_record['report_date'], 'location_name': list_record['address']}
            self.update_existing(old_record, new_values, attributes)

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    CrimeScraper().update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Site-specific scrapers for DC news sources that don't have RSS feeds.
"""

from ebdata.blobs.scrapers import IncrementalCrawler
import re

class WashingtonCityPaperCrawler(IncrementalCrawler):
    schema = 'news-articles'
    seed_url = 'http://www.washingtoncitypaper.com/'
    date_headline_re = re.compile(r'(?s)<h1 class="article-headline">(?P<article_headline>.*?)</h1>.*?<span class="article-date">Posted: (?P<article_date>\w+ \d\d?, \d\d\d\d)</span>')
    date_format = '%B %d, %Y'
    max_blanks = 7

    def public_url(self, id_value):
        return 'http://www.washingtoncitypaper.com/display.php?id=%s' % id_value

    def retrieval_url(self, id_value):
        return 'http://www.washingtoncitypaper.com/printerpage.php?id=%s' % id_value

    def id_for_url(self, url):
        return url.split('id=')[1]

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    WashingtonCityPaperCrawler().update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Import script for DC service requests.
http://data.octo.dc.gov/Main_DataCatalog_Go.aspx?category=0&view=All
Metadata: http://data.octo.dc.gov/Metadata.aspx?id=4
"""

from django.contrib.gis.geos import Point
from ebdata.retrieval.scrapers.list_detail import SkipRecord
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date
from cStringIO import StringIO
from lxml import etree
import re
import zipfile

class ServiceRequestScraper(NewsItemListDetailScraper):
    schema_slugs = ('service-requests',)
    has_detail = False

    def list_pages(self):
        # Download the latest ZIP file, extract it and yield the XML file within.
        z = self.get_html('http://data.octo.dc.gov/feeds/src/src_current_plain.zip')
        zf = zipfile.ZipFile(StringIO(z))
        yield zf.read('src_current_plain.xml')

    def parse_list(self, text):
        xml = etree.fromstring(text)
        strip_ns = re.compile(r'^\{.*?\}').sub
        for el in xml.findall('{http://dc.gov/dcstat/types/1.0/}ServiceRequest'):
            yield dict([(strip_ns('', child.tag), child.text) for child in el])
    
    def clean_list_record(self, record):
        if record['lat'] == '0' or record['long'] == '0':
            raise SkipRecord('Got 0 for lat/long')
        if record['lat'] is None or record['long'] is None:
            raise SkipRecord('Got no value for lat/long')
        if (record['lat'] == '' or record['long'] == 0) and record['siteaddress'] == '':
            raise SkipRecord('No value found for lat/long or address')
        record['lat'] = float(record['lat'])
        record['long'] = float(record['long'])
        if record['servicetypecode'] in ('METERS',):
            raise SkipRecord('Skipping parking meter data')
        record['order_date'] = parse_date(record['serviceorderdate'].split('T')[0], '%Y-%m-%d')
        record['add_date'] = parse_date(record['adddate'].split('T')[0], '%Y-%m-%d')
        if record['resolutiondate']:
            record['resolution_date'] = parse_date(record['resolutiondate'].split('T')[0], '%Y-%m-%d')
        else:
            record['resolution_date'] = None
        if record['inspectiondate']:
            record['inspection_date'] = parse_date(record['inspectiondate'].split('T')[0], '%Y-%m-%d')
        else:
            record['inspection_date'] = None
        if record['serviceduedate']:
            record['service_due_date'] = parse_date(record['serviceduedate'].split('T')[0], '%Y-%m-%d')
        else:
            record['service_due_date'] = None
        record['service_notes'] = record['servicenotes'] or ''
        record['inspection_complete'] = (record['inspectionflag'] == 'Y')
        return record

    def existing_record(self, record):
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id)
            qs = qs.by_attribute(self.schema_fields['request_id'], record['servicerequestid'])
            return qs[0]
        except IndexError:
            return None
    
    def save(self, old_record, list_record, detail_record):
        agency = self.get_or_create_lookup('agency', list_record['agencyabbreviation'], list_record['agencyabbreviation'])
        request_type_code = self.get_or_create_lookup('request_type_code', list_record['servicetypecodedescription'], list_record['servicetypecode'])
        request_type = self.get_or_create_lookup('request_type', list_record['servicecodedescription'], list_record['servicecode'])
        resolution = self.get_or_create_lookup('resolution', list_record['resolution'], list_record['resolution'])
        status = self.get_or_create_lookup('status', list_record['serviceorderstatus'], list_record['serviceorderstatus'])
        priority = self.get_or_create_lookup('priority', list_record['servicepriority'], list_record['servicepriority'])
        attributes = {
            'request_id': list_record['servicerequestid'],
            'dcstat_location_id': list_record['dcstatlocationkey'],
            'dcstat_address_id': list_record['dcstataddresskey'],
            'mar_id': list_record['maraddressrepositoryid'],
            'agency': agency.id,
            'request_type_code': request_type_code.id,
            'request_type': request_type.id,
            'resolution': resolution.id,
            'status': status.id,
            'priority': priority.id,
            'add_date': list_record['add_date'],
            'resolution_date': list_record['resolution_date'],
            'inspection_date': list_record['inspection_date'],
            'service_due_date': list_record['service_due_date'],
            'service_call_count': list_record['servicecallcount'],
            'notes': list_record['service_notes'],
            'inspection_complete': list_record['inspection_complete'],
        }
        title = request_type.name
        if old_record is None:
            kwargs = {}
	    if list_record['long'] is not None and list_record['lat'] is not None:
	        kwargs['location'] = Point(list_record['long'], list_record['lat'])
            self.create_newsitem(
                attributes,
                title=title,
                item_date=list_record['order_date'],
                location_name=list_record.get('siteaddress') or '(long, lat)',
		**kwargs
            )
        else:
            # TODO: Include location Point object in new_values?
            new_values = {'title': title, 'item_date': list_record['order_date']}
            if list_record['siteaddress'] is not None:
                new_values['location_name'] = list_record['siteaddress']
            self.update_existing(old_record, new_values, attributes)

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    ServiceRequestScraper().update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Screen scraper for Houston crime.
http://www.houstontx.gov/police/cs/stats2.htm
"""

from ebdata.parsing.excel import ExcelDictReader
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from dateutil.relativedelta import relativedelta
import datetime
import time
import sys
import os

class CrimeScraper(NewsItemListDetailScraper):
    schema_slugs = ('crime',)
    has_detail = False

    def __init__(self, excel_url=None):
        super(CrimeScraper, self).__init__()
        if excel_url is None:
            last_month = datetime.date.today() - relativedelta(months=1)
            filename = last_month.strftime('%b%y').lower()
            excel_url = 'http://www.houstontx.gov/police/cs/xls/%s.xls' % filename
        self.excel_url = excel_url

    def list_pages(self):
        file_path = self.retriever.get_to_file(self.excel_url)
        reader = ExcelDictReader(file_path, sheet_index=0, header_row_num=0, start_row_num=1)
        yield reader
        os.unlink(file_path) # Clean up the temporary file.

    def parse_list(self, reader):
        for row in reader:
            yield row

    def clean_list_record(self, record):
        if record['Street Name'] == 'OUTSIDE':
            record['address'] = 'Outside'
        else:
            record['address'] = "%s %s block of %s %s" % (int(record['Block']), record['Suffix'], record['Street Name'], record['Type'])
        record['offense_time'] = datetime.datetime(*time.strptime(record['Offense Time'], '%H%M')[:5]).time()
        return record

    def existing_record(self, record):
        offense = self.get_or_create_lookup('offense', record['Offense'], record['Offense'])
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id, item_date=record['Offense Date'])
            qs = qs.by_attribute(self.schema_fields['offense_time'], record['offense_time'])
            qs = qs.by_attribute(self.schema_fields['offense'], offense.id)
            return qs[0]
        except IndexError:
            return None
        return None

    def save(self, old_record, list_record, detail_record):
        if old_record is not None:
            self.logger.debug('Record already exists')
            return

        offense = self.get_or_create_lookup('offense', list_record['Offense'], list_record['Offense'])
        beat = self.get_or_create_lookup('beat', list_record['Beat'], list_record['Beat'])
        premise_type = self.get_or_create_lookup('premise_type', list_record['Premise'], list_record['Premise'])

        attributes = {
            'offense': offense.id,
            'offense_time': list_record['offense_time'],
            'beat': beat.id,
            'premise_type': premise_type.id,
        }
        self.create_newsitem(
            attributes,
            title=offense.name,
            item_date=list_record['Offense Date'],
            location_name=list_record['address'],
        )

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    try:
        excel_url = sys.argv[1]
        CrimeScraper(excel_url=excel_url).update()
    except IndexError:
        CrimeScraper().update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Screen scraper for Houston restaurant-inspection data
http://houston.tx.gegov.com/media/search.cfm
"""

from django.core.serializers.json import DjangoJSONEncoder
from ebdata.retrieval.scrapers.base import ScraperBroken
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date
import datetime
import re

FACILITY_TYPES = (
    ('042', 'Assisted Living Centers'),
    ('120', 'Bakery - Retail'),
    ('121', 'Bakery - Wholesale'),
    ('062', 'Bar - Full Service'),
    ('060', 'Bar - Restricted'),
    ('061', 'Bar - Single Service'),
    ('080', 'Catering Establishment'),
    ('130', 'Commercial Food Processors - Non PHF Only'),
    ('131', 'Commercial Food Processors - PHF'),
    ('081', 'Commissary - Mobile Food Unit'),
    ('101', 'Convenience Grocery - Open foods'),
    ('100', 'Convenience Grocery - Packaged'),
    ('052', 'Day Care Center - Adults'),
    ('050', 'Day Care Center - Open Foods'),
    ('051', 'Day Care Center - Packaged Foods Only'),
    ('181', 'Food Pantry - Open Foods'),
    ('180', 'Food Pantry - Packaged'),
    # ('044', 'Halfway Houses'),
    ('040', 'Hospitals'),
    ('072', 'Mobile - Conventional, Restricted, Motorized'),
    ('073', 'Mobile - Conventional, Restricted, Non-motorized'),
    ('071', 'Mobile - Conventional, Unrestricted , Non-motorized'),
    ('070', 'Mobile - Conventional, Unrestricted, Motorized'),
    ('142', 'Mobile - Fixed Location, Restricted'),
    ('143', 'Mobile - Fixed Location, Unrestricted'),
    ('074', 'Mobile - Ice Cream Only Motorized'),
    ('075', 'Mobile - Ice Cream Only Non-motorized'),
    ('140', 'Mobile - Park, Restricted'),
    ('141', 'Mobile - Park, Unrestricted'),
    ('041', 'Nursing Homes'),
    ('043', 'Other Care Facilities'),
    ('112', 'Produce Certified Farmer\'s Market'),
    ('110', 'Produce Establishment'),
    ('111', 'Produce Peddler'),
    ('001', 'Restaurant - Full Service'),
    ('002', 'Restaurant - Single Service'),
    ('091', 'Retail Food Market - Multi Service'),
    ('092', 'Retail Food Market - Seafood Only'),
    ('090', 'Retail Food Market with Meat Market'),
    ('183', 'Salvage Store - Open Foods'),
    ('182', 'Salvage Store - Packaged'),
    ('036', 'School Athletic Concessions'),
    ('035', 'School Cafeteria - College/University'),
    ('030', 'School Cafeteria - Elementary'),
    ('032', 'School Cafeteria - High School'),
    ('031', 'School Cafeteria - Intermediate / Middle / Jr. High'),
    ('033', 'School Cafeteria - Private'),
    ('034', 'School Cafeteria - PTO'),
    ('163', 'Temporary Food Establishment - Combined Booths'),
    ('162', 'Temporary Food Establishment - Community Based Organization'),
    ('161', 'Temporary Food Establishment - Packaged Foods Only'),
    ('160', 'Temporary Food Establishments - Open Foods'),
    # ('151', 'Warehouses +A85'),
    # ('150', 'Warehouses - No Potentially Hazardous Foods'),
)

detail_url = lambda list_record: 'http://houston.tx.gegov.com/media/search.cfm?q=d&f=%s&i=%s' % (list_record['business_id'], list_record['inspection_id'])
detail_violations_re = re.compile(r'(?si)<tr[^>]*>\s*<td[^>]*>(?P<number>\d+)</td>\s*<td[^>]*><a href="#" onmouseover="ddrivetip\(\'(?P<description>.*?)\',\'[^\']*\', \d+\)"[^>]*>Houston Ordinance Violation: (?P<code>.*?)</a></td>\s*<td[^>]*>(?P<result>.*?)</td>\s*</tr>')

class RestaurantScraper(NewsItemListDetailScraper):
    schema_slugs = ('food-inspections',)
    parse_list_re = re.compile(r'(?si)<tr bgcolor="[^"]+">\s*<td[^>]*>\s*<a href="[^"]+&f=(?P<business_id>[^&]+)[^"]*&i=(?P<inspection_id>[^&"]+)[^"]*">(?P<business_name>.*?)</a>\s*<br>\s*(?P<address>[^<]*?)\s*</td>\s*<td[^>]*>\s*(?P<site>[^<]*?)\s*</td>\s*<td[^>]*>\s*(?P<inspection_date>\d\d?/\d\d?/\d\d\d\d)\s*</td>\s*<td[^>]*>\s*(?P<status>[^<]*?)\s*</td>')
    parse_detail_re = re.compile(r'(?si)<td[^>]*>Insp\. Date</td>\s*<td[^>]*>Insp\. Site</td>\s*<td[^>]*>Status</td>\s*<td[^>]*>Activity</td>\s*</tr>\s*</table>\s*<table[^>]*>\s*<tr[^>]*>\s*<td[^>]*>(?P<inspection_date>\d\d?/\d\d?/\d\d\d\d)</td>\s*<td[^>]*>(?P<site>[^<]*)</td>\s*<td[^>]*>(?P<status>[^<]*)</td>\s*\s*<td[^>]*>(?P<activity>[^<]*)</td>\s*</tr>.*?<table[^>]*>(?P<violations_html>.*?)</table>')
    sleep = 2

    def __init__(self, start_date=None, end_date=None):
        NewsItemListDetailScraper.__init__(self, use_cache=False)
        if start_date is None:
            today = datetime.date.today()
            start_date, end_date = today - datetime.timedelta(days=5), today
        self.start_date, self.end_date = start_date, end_date

    def list_pages(self):
        # Submit a search for each facility type.
        for code, facility_type in FACILITY_TYPES:
            data = {
                'q': 's',
                'e': '',
                'k': '',
                'r': '',
                'tp': code,
                'sd': self.start_date.strftime('%m/%d/%Y'),
                'ed': self.end_date.strftime('%m/%d/%Y'),
                'z': 'ALL',
                'm': 'LIKE',
                'maxrows': '500',
            }
            html = self.get_html('http://houston.tx.gegov.com/media/search.cfm', data, send_cookies=False)
            # Convert to Unicode to handle non-ASCII characters.
            html = html.decode('iso-8859-1')
            yield (facility_type, html)

    def parse_list(self, page):
        facility_type, html = page
        for record in NewsItemListDetailScraper.parse_list(self, html):
            yield dict(record, facility_type=facility_type)

    def clean_list_record(self, record):
        record['inspection_date'] = parse_date(record['inspection_date'], '%m/%d/%Y')
        record['detail_url'] = detail_url(record)
        return record

    def existing_record(self, record):
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id, item_date=record['inspection_date'])
            qs = qs.by_attribute(self.schema_fields['business_id'], record['business_id'])
            qs = qs.by_attribute(self.schema_fields['inspection_id'], record['inspection_id'])
            return qs[0]
        except IndexError:
            return None

    def detail_required(self, list_record, old_record):
        return old_record is None

    def get_detail(self, record):
        # Convert to Unicode to handle non-ASCII characters.
        return self.get_html(record['detail_url'], send_cookies=False).decode('iso-8859-1')

    def clean_detail_record(self, record):
        violations = []
        last_number = 0
        for vio_bits in detail_violations_re.finditer(record.pop('violations_html')):
            vio = vio_bits.groupdict()

            # This is a sanity check of the scraper regex to make sure we get
            # every violation. The source data gives a sequential integer
            # number to each violation, so we just make sure the numbers are
            # indeed sequential. If they're not, then our regex is too strict.
            number = int(vio.pop('number'))
            if number - last_number != 1:
                raise ScraperBroken('Did not detect violation #%s at %s' % (number - 1, record['detail_url']))
            last_number = number

            # Unescape the JavaScript string escaping.
            vio['description'] = vio['description'].replace(r"\'", "'")

            vio['result'] = re.sub(r'\s*<br>\s*', ', ', vio['result'])

            # Skip violations with an empty code. This happens if there are no
            # violations (in this case, the site displays a single, empty
            # violation).
            if vio['code']:
                # We can't just use the violation code to determine uniqueness
                # of the violation type, because sometimes there are violations
                # with the same codes but different descriptions. Here, we use
                # a combination of the code and description as the primary key
                # for the Lookup object. (This will be used by
                # get_or_create_lookup() later.)
                code_for_db = '%s %s' % (vio['code'], vio['description'])
                vio['code_for_db'] = code_for_db[:255] # Fit database column limit.
                violations.append(vio)
        record['violation_list'] = violations
        return record

    def save(self, old_record, list_record, detail_record):
        if detail_record is None:
            return # No need to update the record.

        # This bit of logic lives here because it requires access to both
        # detail_record and list_record.
        if detail_record['activity'] in ('Door Notice (008)', 'Establishment Out of Business (009)'):
            list_record['status'] = 'Unavailable'

        status = self.get_or_create_lookup('status', list_record['status'], list_record['status'], make_text_slug=False)
        activity = self.get_or_create_lookup('activity', detail_record['activity'], detail_record['activity'], make_text_slug=False)
        facility_type = self.get_or_create_lookup('facility_type', list_record['facility_type'], list_record['facility_type'], make_text_slug=False)
        violation_lookups = [self.get_or_create_lookup('violation', v['code'], v['code_for_db'], v['description'], make_text_slug=False) for v in detail_record['violation_list']]
        violation_lookup_text = ','.join([str(v.id) for v in violation_lookups])

        # There's a bunch of data about every particular violation, and we
        # store it as a JSON object. Here, we create the JSON object.
        v_lookup_dict = dict([(v.code, v) for v in violation_lookups])
        v_list = [{'lookup_id': v_lookup_dict[v['code_for_db']].id, 'result': v['result']} for v in detail_record['violation_list']]
        violations_json = DjangoJSONEncoder().encode(v_list)

        title = list_record['business_name']
        attributes = {
            'status': status.id,
            'activity': activity.id,
            'facility_type': facility_type.id,
            'site': detail_record['site'],
            'business_id': list_record['business_id'],
            'inspection_id': list_record['inspection_id'],
            'business_name': list_record['business_name'],
            'violation': violation_lookup_text,
            'details': violations_json,
        }
        if old_record is None:
            self.create_newsitem(
                attributes,
                title=title,
                url=list_record['detail_url'],
                item_date=list_record['inspection_date'],
                location_name=list_record['address'],
            )
        else:
            new_values = {'title': title, 'item_date': list_record['inspection_date'], 'location_name': list_record['address']}
            self.update_existing(old_record, new_values, attributes)

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    start_date = datetime.date(2009, 5, 1)
    end_date = datetime.date(2009, 5, 1)
    s = RestaurantScraper(start_date=start_date, end_date=end_date).update()

########NEW FILE########
__FILENAME__ = fix_geoms
from ebdata.retrieval.utils import locations_are_close
from ebpub.db.models import NewsItem
from ebpub.geocoder import SmartGeocoder, ParsingError, GeocodingException
from django.contrib.gis.geos import Point

geocoder = SmartGeocoder()
THRESHOLD = 375

def fix_crime_geom():
    qs = NewsItem.objects.filter(schema__slug='crime', location__isnull=False)
    count = qs.count()
    for i, ni in enumerate(qs.iterator()):
        print '# => Checking %s of %s' % (i, count)
        x, y = [float(n) for n in ni.attributes['xy'].split(';')]
        pt = Point((x, y))
        pt.srid = 4326
        location_name = ni.location_name.replace('XX', '01')
        try:
            result = geocoder.geocode(location_name)
        except (GeocodingException, ParsingError):
            print '     Could not geocode'
            NewsItem.objects.filter(id=ni.id).update(location=None)
        else:
            is_close, distance = locations_are_close(ni.location, pt, THRESHOLD)
            if not is_close:
                print '     Too far: %s' % distance
                NewsItem.objects.filter(id=ni.id).update(location=None)
            else:
                print '     Fine'

if __name__ == "__main__":
    fix_crime_geom()

########NEW FILE########
__FILENAME__ = retrieval
"""
Scraper for crime in Los Angeles

http://www.lapdonline.org/crimemap/
http://www.lapdonline.org/crimemap/crime_search.php?&startDate=07/13/2008&interval=3&radius=1&lon=-118.322109&lat=34.07141&c[1]=1&c[2]=1&c[3]=1&c[4]=1&c[5]=1&c[6]=1&c[7]=1&c[8]=1
"""

from django.contrib.gis.geos import Point
from ebdata.retrieval.scrapers.base import ScraperBroken
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date, parse_time
import datetime
import re
import time
from urllib import urlencode

# This list of long/lat tuples was generated by ebgeo.maps.tess.cover_city
# with a one-mile (1.609 km) radius.
SEARCH_POINTS = (
    (-118.28013040793886, 33.710913934398796),
    (-118.31768272329963, 33.728947212353383),
    (-118.29264784639244, 33.728947212353383),
    (-118.26761296948527, 33.728947212353383),
    (-118.2425780925781, 33.728947212353383),
    (-118.30516528484603, 33.746976701674861),
    (-118.28013040793886, 33.746976701674861),
    (-118.25509553103169, 33.746976701674861),
    (-118.29264784639244, 33.765002401374289),
    (-118.26761296948527, 33.765002401374289),
    (-118.30516528484603, 33.783024310464434),
    (-118.28013040793886, 33.783024310464434),
    (-118.25509553103169, 33.783024310464434),
    (-118.23006065412451, 33.783024310464434),
    (-118.30516528484603, 33.819056752876492),
    (-118.30516528484603, 33.855074021047329),
    (-118.43033966938185, 33.927063003346973),
    (-118.28013040793886, 33.927063003346973),
    (-118.25509553103169, 33.927063003346973),
    (-118.44285710783544, 33.94505075280977),
    (-118.41782223092827, 33.94505075280977),
    (-118.39278735402112, 33.94505075280977),
    (-118.26761296948527, 33.94505075280977),
    (-118.2425780925781, 33.94505075280977),
    (-118.45537454628904, 33.963034701884808),
    (-118.43033966938185, 33.963034701884808),
    (-118.40530479247471, 33.963034701884808),
    (-118.38026991556754, 33.963034701884808),
    (-118.30516528484603, 33.963034701884808),
    (-118.28013040793886, 33.963034701884808),
    (-118.46789198474262, 33.981014849603611),
    (-118.41782223092827, 33.981014849603611),
    (-118.31768272329963, 33.981014849603611),
    (-118.29264784639244, 33.981014849603611),
    (-118.26761296948527, 33.981014849603611),
    (-118.45537454628904, 33.998991194999419),
    (-118.40530479247471, 33.998991194999419),
    (-118.33020016175321, 33.998991194999419),
    (-118.30516528484603, 33.998991194999419),
    (-118.28013040793886, 33.998991194999419),
    (-118.25509553103169, 33.998991194999419),
    (-118.44285710783544, 34.016963737107147),
    (-118.41782223092827, 34.016963737107147),
    (-118.36775247711394, 34.016963737107147),
    (-118.3427176002068, 34.016963737107147),
    (-118.31768272329963, 34.016963737107147),
    (-118.29264784639244, 34.016963737107147),
    (-118.26761296948527, 34.016963737107147),
    (-118.2425780925781, 34.016963737107147),
    (-118.21754321567094, 34.016963737107147),
    (-118.19250833876377, 34.016963737107147),
    (-118.53047917701053, 34.034932474963448),
    (-118.45537454628904, 34.034932474963448),
    (-118.43033966938185, 34.034932474963448),
    (-118.40530479247471, 34.034932474963448),
    (-118.38026991556754, 34.034932474963448),
    (-118.35523503866035, 34.034932474963448),
    (-118.33020016175321, 34.034932474963448),
    (-118.30516528484603, 34.034932474963448),
    (-118.28013040793886, 34.034932474963448),
    (-118.25509553103169, 34.034932474963448),
    (-118.23006065412451, 34.034932474963448),
    (-118.20502577721736, 34.034932474963448),
    (-118.56803149237129, 34.05289740760665),
    (-118.54299661546412, 34.05289740760665),
    (-118.51796173855696, 34.05289740760665),
    (-118.49292686164979, 34.05289740760665),
    (-118.46789198474262, 34.05289740760665),
    (-118.44285710783544, 34.05289740760665),
    (-118.41782223092827, 34.05289740760665),
    (-118.39278735402112, 34.05289740760665),
    (-118.36775247711394, 34.05289740760665),
    (-118.3427176002068, 34.05289740760665),
    (-118.31768272329963, 34.05289740760665),
    (-118.29264784639244, 34.05289740760665),
    (-118.26761296948527, 34.05289740760665),
    (-118.2425780925781, 34.05289740760665),
    (-118.21754321567094, 34.05289740760665),
    (-118.5555140539177, 34.070858534076763),
    (-118.53047917701053, 34.070858534076763),
    (-118.50544430010338, 34.070858534076763),
    (-118.48040942319621, 34.070858534076763),
    (-118.45537454628904, 34.070858534076763),
    (-118.43033966938185, 34.070858534076763),
    (-118.38026991556754, 34.070858534076763),
    (-118.35523503866035, 34.070858534076763),
    (-118.33020016175321, 34.070858534076763),
    (-118.30516528484603, 34.070858534076763),
    (-118.28013040793886, 34.070858534076763),
    (-118.25509553103169, 34.070858534076763),
    (-118.23006065412451, 34.070858534076763),
    (-118.20502577721736, 34.070858534076763),
    (-118.17999090031019, 34.070858534076763),
    (-118.56803149237129, 34.088815853415539),
    (-118.54299661546412, 34.088815853415539),
    (-118.51796173855696, 34.088815853415539),
    (-118.49292686164979, 34.088815853415539),
    (-118.46789198474262, 34.088815853415539),
    (-118.44285710783544, 34.088815853415539),
    (-118.3427176002068, 34.088815853415539),
    (-118.31768272329963, 34.088815853415539),
    (-118.29264784639244, 34.088815853415539),
    (-118.26761296948527, 34.088815853415539),
    (-118.2425780925781, 34.088815853415539),
    (-118.21754321567094, 34.088815853415539),
    (-118.19250833876377, 34.088815853415539),
    (-118.1674734618566, 34.088815853415539),
    (-118.5555140539177, 34.106769364666398),
    (-118.53047917701053, 34.106769364666398),
    (-118.50544430010338, 34.106769364666398),
    (-118.48040942319621, 34.106769364666398),
    (-118.45537454628904, 34.106769364666398),
    (-118.43033966938185, 34.106769364666398),
    (-118.40530479247471, 34.106769364666398),
    (-118.38026991556754, 34.106769364666398),
    (-118.35523503866035, 34.106769364666398),
    (-118.33020016175321, 34.106769364666398),
    (-118.30516528484603, 34.106769364666398),
    (-118.28013040793886, 34.106769364666398),
    (-118.25509553103169, 34.106769364666398),
    (-118.23006065412451, 34.106769364666398),
    (-118.20502577721736, 34.106769364666398),
    (-118.17999090031019, 34.106769364666398),
    (-118.56803149237129, 34.124719066874498),
    (-118.54299661546412, 34.124719066874498),
    (-118.51796173855696, 34.124719066874498),
    (-118.49292686164979, 34.124719066874498),
    (-118.46789198474262, 34.124719066874498),
    (-118.44285710783544, 34.124719066874498),
    (-118.41782223092827, 34.124719066874498),
    (-118.39278735402112, 34.124719066874498),
    (-118.36775247711394, 34.124719066874498),
    (-118.3427176002068, 34.124719066874498),
    (-118.31768272329963, 34.124719066874498),
    (-118.29264784639244, 34.124719066874498),
    (-118.26761296948527, 34.124719066874498),
    (-118.2425780925781, 34.124719066874498),
    (-118.21754321567094, 34.124719066874498),
    (-118.19250833876377, 34.124719066874498),
    (-118.58054893082488, 34.14266495908663),
    (-118.5555140539177, 34.14266495908663),
    (-118.53047917701053, 34.14266495908663),
    (-118.50544430010338, 34.14266495908663),
    (-118.48040942319621, 34.14266495908663),
    (-118.45537454628904, 34.14266495908663),
    (-118.43033966938185, 34.14266495908663),
    (-118.40530479247471, 34.14266495908663),
    (-118.38026991556754, 34.14266495908663),
    (-118.33020016175321, 34.14266495908663),
    (-118.30516528484603, 34.14266495908663),
    (-118.28013040793886, 34.14266495908663),
    (-118.20502577721736, 34.14266495908663),
    (-118.61810124618565, 34.160607040351344),
    (-118.59306636927846, 34.160607040351344),
    (-118.56803149237129, 34.160607040351344),
    (-118.54299661546412, 34.160607040351344),
    (-118.51796173855696, 34.160607040351344),
    (-118.49292686164979, 34.160607040351344),
    (-118.46789198474262, 34.160607040351344),
    (-118.44285710783544, 34.160607040351344),
    (-118.41782223092827, 34.160607040351344),
    (-118.39278735402112, 34.160607040351344),
    (-118.36775247711394, 34.160607040351344),
    (-118.65565356154637, 34.178545309718842),
    (-118.63061868463923, 34.178545309718842),
    (-118.60558380773205, 34.178545309718842),
    (-118.58054893082488, 34.178545309718842),
    (-118.5555140539177, 34.178545309718842),
    (-118.53047917701053, 34.178545309718842),
    (-118.50544430010338, 34.178545309718842),
    (-118.48040942319621, 34.178545309718842),
    (-118.45537454628904, 34.178545309718842),
    (-118.43033966938185, 34.178545309718842),
    (-118.40530479247471, 34.178545309718842),
    (-118.38026991556754, 34.178545309718842),
    (-118.64313612309282, 34.196479766241048),
    (-118.61810124618565, 34.196479766241048),
    (-118.59306636927846, 34.196479766241048),
    (-118.56803149237129, 34.196479766241048),
    (-118.54299661546412, 34.196479766241048),
    (-118.51796173855696, 34.196479766241048),
    (-118.49292686164979, 34.196479766241048),
    (-118.46789198474262, 34.196479766241048),
    (-118.44285710783544, 34.196479766241048),
    (-118.41782223092827, 34.196479766241048),
    (-118.39278735402112, 34.196479766241048),
    (-118.63061868463923, 34.214410408971595),
    (-118.60558380773205, 34.214410408971595),
    (-118.58054893082488, 34.214410408971595),
    (-118.5555140539177, 34.214410408971595),
    (-118.53047917701053, 34.214410408971595),
    (-118.50544430010338, 34.214410408971595),
    (-118.48040942319621, 34.214410408971595),
    (-118.45537454628904, 34.214410408971595),
    (-118.43033966938185, 34.214410408971595),
    (-118.40530479247471, 34.214410408971595),
    (-118.38026991556754, 34.214410408971595),
    (-118.35523503866035, 34.214410408971595),
    (-118.64313612309282, 34.23233723696579),
    (-118.61810124618565, 34.23233723696579),
    (-118.59306636927846, 34.23233723696579),
    (-118.56803149237129, 34.23233723696579),
    (-118.54299661546412, 34.23233723696579),
    (-118.51796173855696, 34.23233723696579),
    (-118.49292686164979, 34.23233723696579),
    (-118.46789198474262, 34.23233723696579),
    (-118.44285710783544, 34.23233723696579),
    (-118.41782223092827, 34.23233723696579),
    (-118.39278735402112, 34.23233723696579),
    (-118.36775247711394, 34.23233723696579),
    (-118.3427176002068, 34.23233723696579),
    (-118.31768272329963, 34.23233723696579),
    (-118.29264784639244, 34.23233723696579),
    (-118.26761296948527, 34.23233723696579),
    (-118.63061868463923, 34.250260249280622),
    (-118.60558380773205, 34.250260249280622),
    (-118.58054893082488, 34.250260249280622),
    (-118.5555140539177, 34.250260249280622),
    (-118.53047917701053, 34.250260249280622),
    (-118.50544430010338, 34.250260249280622),
    (-118.48040942319621, 34.250260249280622),
    (-118.45537454628904, 34.250260249280622),
    (-118.43033966938185, 34.250260249280622),
    (-118.40530479247471, 34.250260249280622),
    (-118.38026991556754, 34.250260249280622),
    (-118.35523503866035, 34.250260249280622),
    (-118.33020016175321, 34.250260249280622),
    (-118.30516528484603, 34.250260249280622),
    (-118.28013040793886, 34.250260249280622),
    (-118.61810124618565, 34.268179444974798),
    (-118.59306636927846, 34.268179444974798),
    (-118.56803149237129, 34.268179444974798),
    (-118.54299661546412, 34.268179444974798),
    (-118.51796173855696, 34.268179444974798),
    (-118.49292686164979, 34.268179444974798),
    (-118.46789198474262, 34.268179444974798),
    (-118.44285710783544, 34.268179444974798),
    (-118.41782223092827, 34.268179444974798),
    (-118.39278735402112, 34.268179444974798),
    (-118.36775247711394, 34.268179444974798),
    (-118.3427176002068, 34.268179444974798),
    (-118.31768272329963, 34.268179444974798),
    (-118.29264784639244, 34.268179444974798),
    (-118.26761296948527, 34.268179444974798),
    (-118.2425780925781, 34.268179444974798),
    (-118.58054893082488, 34.286094823108719),
    (-118.5555140539177, 34.286094823108719),
    (-118.53047917701053, 34.286094823108719),
    (-118.50544430010338, 34.286094823108719),
    (-118.48040942319621, 34.286094823108719),
    (-118.45537454628904, 34.286094823108719),
    (-118.30516528484603, 34.286094823108719),
    (-118.54299661546412, 34.304006382744468),
    (-118.51796173855696, 34.304006382744468),
    (-118.49292686164979, 34.304006382744468),
    (-118.46789198474262, 34.304006382744468),
    (-118.44285710783544, 34.304006382744468),
    (-118.41782223092827, 34.304006382744468),
    (-118.50544430010338, 34.32191412294582),
    (-118.48040942319621, 34.32191412294582),
    (-118.45537454628904, 34.32191412294582),
    (-118.43033966938185, 34.32191412294582),
    (-118.40530479247471, 34.32191412294582)
)

class CrimeScraper(NewsItemListDetailScraper):
    schema_slugs = ('crime',)
    has_detail = False
    parse_list_re = re.compile(r"(?si)new searchPoint \('\d+', '\#(?P<case_number>\d+)', '(?P<color>[^']*)', '\d+', '(?P<lon>-\d+\.\d+)', '(?P<lat>\d+\.\d+)', '(?P<location_name>[^']*)', '[^']*', '[^']*', '(?P<crime_date>\d\d-\d\d-\d\d\d\d) (?P<crime_time>\d\d?:\d\d:\d\d [AP]M)', '(?P<division>[^']*)'\);")
    sleep = 8

    def __init__(self, start_date=None, end_date=None):
        NewsItemListDetailScraper.__init__(self)
        today = datetime.date.today()
        self.start_date = start_date or (today - datetime.timedelta(days=31))
        self.end_date = end_date or today

    def get_html(self, *args, **kwargs):
        MAX_TRIES = 4
        tries = 0
        while tries <= MAX_TRIES:
            html = NewsItemListDetailScraper.get_html(self, *args, **kwargs)
            if 'Unable to connect to PostgreSQL server' in html:
                self.logger.debug('Got "Unable to connect to PostgreSQL" error')
                time.sleep(3)
                continue
            return html
        raise ScraperBroken('Got PostgreSQL error %s times' % MAX_TRIES)

    def list_pages(self):
        INTERVAL = 3
        date = self.end_date - datetime.timedelta(days=INTERVAL)
        while date >= self.start_date - datetime.timedelta(days=INTERVAL):
            for lng, lat in SEARCH_POINTS:
                params = {
                    'startDate': date.strftime('%m/%d/%Y'),
                    'interval': INTERVAL,
                    'radius': 1,
                    'lon': lng,
                    'lat': lat,
                    'c[1]': '1',
                    'c[2]': '1',
                    'c[3]': '1',
                    'c[4]': '1',
                    'c[5]': '1',
                    'c[6]': '1',
                    'c[7]': '1',
                    'c[8]': '1',
                }
                yield self.get_html('http://www.lapdonline.org/crimemap/crime_search.php?%s' % urlencode(params))
            date -= datetime.timedelta(days=INTERVAL)

    def parse_list(self, page):
        records = list(NewsItemListDetailScraper.parse_list(self, page))
        self.logger.debug('Got %s records', len(records))
        if len(records) >= 99:
            raise ScraperBroken('Got %s records. Consider changing date interval' % len(records))
        return records

    def clean_list_record(self, record):
        record['crime_date'] = parse_date(record['crime_date'], '%m-%d-%Y')
        record['crime_time'] = parse_time(record['crime_time'], '%I:%M:%S %p')
        record['lon'] = float(record['lon'])
        record['lat'] = float(record['lat'])
        return record

    def existing_record(self, record):
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id, item_date=record['crime_date'])
            return qs.by_attribute(self.schema_fields['case_number'], record['case_number'])[0]
        except IndexError:
            return None

    def save(self, old_record, list_record, detail_record):
        crime_type = self.get_or_create_lookup('crime_type', list_record['color'], list_record['color'])
        division = self.get_or_create_lookup('division', list_record['division'], list_record['division'])
        attributes = {
            'case_number': list_record['case_number'],
            'crime_time': list_record['crime_time'],
            'crime_type': crime_type.id,
            'division': division.id,
            'xy': '%s;%s' % (list_record['lon'], list_record['lat'])
        }
        title = u'#%s: %s' % (list_record['case_number'], crime_type.name)

        crime_location = Point(list_record['lon'], list_record['lat'], srid=4326)
        crime_location = self.safe_location(list_record['location_name'].replace('XX', '01'), crime_location, 375)

        if old_record is None:
            ni = self.create_newsitem(
                attributes,
                title=title,
                item_date=list_record['crime_date'],
                location=crime_location,
                location_name=list_record['location_name'],
            )
        else:
            # Convert the date to a datetime, because it's stored as a datetime
            # in the database, and the comparison to the existing record will
            # fail if we compare a date to a datetime.
            item_date = datetime.datetime(list_record['crime_date'].year, list_record['crime_date'].month, list_record['crime_date'].day)
            new_values = {'title': title, 'item_date': item_date, 'location_name': list_record['location_name']}
            self.update_existing(old_record, new_values, attributes)
            ni = old_record

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    CrimeScraper().update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Scraper for fire alerts in Los Angeles

http://groups.google.com/group/LAFD_ALERT/
RSS: http://groups.google.com/group/LAFD_ALERT/feed/rss_v2_0_msgs.xml?num=50
"""

from ebdata.retrieval.scrapers.base import ScraperBroken
from ebdata.retrieval.scrapers.list_detail import RssListDetailScraper, SkipRecord
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
import datetime
import re

class AlertScraper(NewsItemListDetailScraper, RssListDetailScraper):
    schema_slugs = ('fire-alerts',)
    has_detail = False
    sleep = 4

    def list_pages(self):
        yield self.get_html('http://groups.google.com/group/LAFD_ALERT/feed/rss_v2_0_msgs.xml?num=50')

    def clean_list_record(self, rss_record):
        record = {
            'pub_date': datetime.date(*rss_record.pop('updated_parsed')[:3]),
            'summary': rss_record['summary'].strip(),
        }
        if re.search(r'^(?i)\*UPDATE:', record['summary']):
            m = re.search(r'^\*UPDATE:\s*(?P<location_name>[^\*]*)\*\s*(?P<description>.*)\s*-\s*(?P<reporter>.*?)\#\#\#$', record['summary'])
            if not m:
                self.logger.warn('Could not parse update %r' % record['summary'])
                raise SkipRecord('Could not parse update %r' % record['summary'])
            record.update(m.groupdict())
            record.update({
                'is_update': True,
                'incident_type': '',
                'fire_station': '',
                'radio_channels': '',
                'incident_time': '',
            })
        else: # Not an update
            m = re.search(r'^\*(?P<incident_type>[^\*]*)\*\s*(?P<location_name>[^;]*);\s*MAP (?:\d+[- ]\w\d)?;\s*FS (?P<fire_station>\d+); (?P<description>.*?); Ch:(?P<radio_channels>[\d, ]+)\s*@(?P<incident_time>\d\d?:\d\d [AP]M)?\s*-(?P<reporter>.*?)\#\#\#$', record['summary'])
            if not m:
                raise SkipRecord('Could not parse %r' % record['summary'])
            record.update(m.groupdict())
            record['incident_type'] = record['incident_type'].upper() # Normalize
            record['radio_channels'] = ','.join(record['radio_channels'].split(','))
            record['is_update'] = False
        record['description'] = record['description'].replace('&nbsp;', ' ').replace('&quot;', '"').replace('&amp;', '&').strip()
        record['location_name'] = record['location_name'].strip()

        # Get the incident ID and message ID from the Google Groups URL.
        # We'll use these as unique identifiers.
        m = re.search(r'browse_thread/thread/(?P<incident_id>[^/]*)/(?P<message_id>[^\?]*)\?', rss_record['link'])
        if not m:
            raise ScraperBroken('Got weird URL: %r', rss_record['link'])
        record.update(m.groupdict())
        record['link'] = rss_record['link']

        # I can't figure out why this record is causing errors, so for now
        # we'll just skip it.
        if record['message_id'] == '0faabeab3aad8492':
            raise SkipRecord()

        return record

    def existing_record(self, record):
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id)
            qs = qs.by_attribute(self.schema_fields['message_id'], record['message_id'])
            return qs[0]
        except IndexError:
            return None

    def save(self, old_record, list_record, detail_record):
        if old_record is not None:
            return
        incident_type = self.get_or_create_lookup('incident_type', list_record['incident_type'], list_record['incident_type'], make_text_slug=False)
        reporter = self.get_or_create_lookup('reporter', list_record['reporter'], list_record['reporter'])
        fire_station = self.get_or_create_lookup('fire_station', list_record['fire_station'], list_record['fire_station'])
        attributes = {
            'incident_type': incident_type.id,
            'description': list_record['summary'],
            'reporter': reporter.id,
            'fire_station': fire_station.id,
            'incident_time': list_record['incident_time'],
            'incident_id': list_record['incident_id'],
            'message_id': list_record['message_id'],
        }
        if list_record['is_update']:
            title = 'Update' # TODO: Better title that takes into account the incident type.
        else:
            title = incident_type.name
        self.create_newsitem(
            attributes,
            title=title,
            description=list_record['description'],
            url=list_record['link'],
            item_date=list_record['pub_date'],
            location_name=list_record['location_name'],
        )

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    AlertScraper().update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Scraper for Los Angeles pool inspections

http://publichealth.lacounty.gov/phcommon/public/eh/pool/
"""

from ebdata.retrieval.scrapers.new_newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from ebpub.streets.models import Suburb
from ebpub.utils.dates import parse_date
import re

FACILITY_TYPES = (
    'APARTMENT HOUSE',
    'CONDOMINIUM/TOWNHOUSE',
    'COUNTRY CLUB',
    'FRESH WATER SWIM AREA, PUBLIC',
    'HEALTH CLUB',
    'HOTEL',
    'INSPECTION OF INACTIVE POOL',
    'MEDICAL FACILITY',
    'MOBILE HOME PARK',
    'MOTEL/TRAVEL COURT',
    'MUNICIPAL POOL',
    'ORGANIZATION',
    'OTHER',
    'POOL AT FOUR UNIT DWELLING',
    'PRIVATE HOMES COMMUNITY',
    'PRIVATE SCHOOL',
    'PUBLIC SCHOOL',
    'RESORT/CAMP',
    'SWIM SCHOOL',
    'WATER THEME PARK'
)

class Scraper(NewsItemListDetailScraper):
    schema_slugs = ['pool-inspections']

    parse_list_re = re.compile(r'(?s)<tr>\s*<td[^>]*>(?P<name>[\w\s\']*?)<br></td>\s*<td[^>]*>\s*(?P<location>.*?)<br>\s*</td>\s*<td[^>]*>(?P<city>.*?)<br></td>\s*<td[^>]*>(?P<zip>\d*?)<br></td>\s*<td[^>]*>(?P<date>\d{2}/\d{2}/\d{4})<br></td>\s*<td[^>]*>(?P<facility_type>.*?)<br></td>\s*<td[^>]*>(?P<pool_type>.*?)</td>\s*<td[^>]*>\s*<form[^>]*>\s*<input[^>]*>\s*<input type="hidden" name="dsiteid" value="(?P<dsiteid>\d+)">')
    parse_detail_re = re.compile(r'(?s)<tr>\s*<td[^>]*>\s*Inspection Date:</td>\s*<td[^>]*>\s*(?P<date>\d{2}/\d{2}/\d{4})</td>\s*(?P<html>.*?)<tr>\s*<td colspan="3"><hr class="contLine"></td>\s*</tr>')
    parse_violations_re = re.compile(r'<tr>\s*<td[^>]*>\s*(?:Violations:|\s*)</td>\s*<td[^>]*>\s*(?P<code>\d+)\s*</td>\s*<td[^>]*>\s*<a[^>]*>\s*(?P<violation>.*?)</a>')

    sleep = 0
    list_uri = 'http://publichealth.lacounty.gov/phcommon/public/eh/pool/poolsearchaction.cfm'
    detail_uri = 'http://publichealth.lacounty.gov/phcommon/public/eh/pool/pooldetail.cfm'

    def __init__(self, *args, **kwargs):
        self.get_archive = kwargs.pop('get_archive', False)
        self.suburbs = set([d['normalized_name'].upper() for d in Suburb.objects.values('normalized_name')])
        super(Scraper, self).__init__(*args, **kwargs)

    def list_pages(self):
        for facility_type in FACILITY_TYPES:
            row = 0
            while 1:
                params = {
                    'B1': 'Submit',
                    'address': '',
                    'city': '',
                    'dba': '',
                    'sort': 'inspdt',
                    'type': facility_type,
                    'zipcode': '',
                    'start': row + 1,
                    'row': row
                }
                page = self.get_page(self.list_uri, params)
                total = re.search(r'<span[^>]*>(\d+) record\(s\) match your search criteria.</span>', page.html).group(1)
                yield page
                row += 100
                if row > int(total) or (not self.get_archive):
                    break

    def clean_list_record(self, record):
        record['date'] = parse_date(record['date'], '%m/%d/%Y')
        return record

    def existing_record(self, list_record):
        # Each list record can result in multiple detail records. The save
        # method should handle existing records instead.
        return None

    def detail_required(self, list_record, old_record):
        return old_record is None

    def get_detail(self, list_record):
        params = {
            'address': '',
            'alphalist': '',
            'checkbox': 'no',
            'city': '',
            'dba': '',
            'dsiteid': list_record['dsiteid'],
            'pooltypdsc': '',
            'sort': 'inspdt',
            'start': '1',
            'type': '',
            'zipcode': '',
        }
        return self.get_page(self.detail_uri, params)

    def parse_detail(self, page, list_record):
        detail_record = {'inspections': []}
        for m in self.parse_detail_re.finditer(page):
            inspection = {
                'date': m.groupdict()['date'],
                'violations': []
            }
            for vm in self.parse_violations_re.finditer(m.groupdict()['html']):
                inspection['violations'].append(vm.groupdict())
            detail_record['inspections'].append(inspection)
        return detail_record

    def clean_detail_record(self, record):
        for inspection in record['inspections']:
            inspection['date'] = parse_date(inspection['date'], '%m/%d/%Y')
        return record

    def save(self, old_record, list_record, detail_record, list_page, detail_page):
        facility_type_lookup = self.get_or_create_lookup('facility_type', list_record['facility_type'], list_record['facility_type'], make_text_slug=False)
        pool_type_lookup = self.get_or_create_lookup('pool_type', list_record['pool_type'], list_record['pool_type'], make_text_slug=False)
        city_lookup = self.get_or_create_lookup('city', list_record['city'], list_record['city'], make_text_slug=False)

        # Store the existing inspection dates in a set so we can quickly
        # determine whether or not we need to save the current one.
        qs = NewsItem.objects.filter(schema__id=self.schema.id)
        qs = qs.by_attribute(self.schema_fields['dsiteid'], list_record['dsiteid'])
        existing_inspection_dates = set([d['item_date'] for d in qs.values('item_date')])

        for inspection in detail_record['inspections']:
            if inspection['date'] in existing_inspection_dates:
                continue

            violation_lookups = []
            for v in inspection['violations']:
                lookup = self.get_or_create_lookup('violations', v['violation'], v['code'], make_text_slug=False)
                violation_lookups.append(lookup)

            violation_count = len(violation_lookups)
            name = list_record['name'] or list_record['location']
            if violation_count == 0:
                violation_string = 'No violations'
            elif violation_count == 1:
                violation_string = '1 violation'
            elif violation_count > 1:
                violation_string = '%s violations' % violation_count
            title = "Pool at %s inspected: %s" % (name, violation_string)
            attributes = {
                'name': name,
                'facility_type': facility_type_lookup.id,
                'pool_type': pool_type_lookup.id,
                'violations': ','.join([str(l.id) for l in violation_lookups]),
                'dsiteid': list_record['dsiteid'],
                'city': city_lookup.id
            }
            values = {
                'list_page': list_page,
                'detail_page': detail_page,
                'title': title,
                'item_date': inspection['date'],
                'location_name': '%s, %s' % (list_record['location'], city_lookup.name),
            }
            # Don't try to geocode items in the suburbs table
            if city_lookup.name.upper() in self.suburbs:
                values['location'] = None
            self.create_newsitem(attributes, **values)

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    Scraper(get_archive=True).update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Scraper for pool closures in Los Angeles

http://publichealth.lacounty.gov/phcommon/public/eh/pool/plclosure.cfm
"""

from ebdata.retrieval.scrapers.new_newsitem_list_detail import NewsItemListDetailScraper
from ebdata.retrieval.models import ScrapedPage
from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date
import datetime
import re

FACILITY_TYPES = (
    'APARTMENT HOUSE',
    'CONDOMINIUM/TOWNHOUSE',
    'HEALTH CLUB',
    'HOTEL',
    'MOBILE HOME PARK',
    'MOTEL/TRAVEL COURT',
    'MUNICIPAL POOL',
    'PRIVATE HOMES COMMUNITY',
    'RESORT/CAMP'
)

class Scraper(NewsItemListDetailScraper):
    schema_slugs = ['pool-closures']
    parse_list_re = re.compile(r'(?s)<tr>\s*<td[^>]*>\s*<br>\s*<hr[^>]*>\s*<span class="contTitle2">(?P<name>.*?)</span>\s*,\s*(?P<location>.*?), (?P<city>.*?), CA, \d+ <br>(?:.*?)</td>\s*</tr>\s*<tr>\s*<td[^>]*>\s*<ul[^>]*>\s*(?P<html>.*?)\s*</td>')
    date_closed_re = re.compile(r'<li><span[^>]*>Date Closed:</span>\s*(\w+ \d{2}, \d{4})')
    date_reopened_re = re.compile(r'<li><span[^>]*>Date Reopened:</span>\s*(\w+ \d{2}, \d{4})')
    reasons_html_re = re.compile(r'(?s)<li><span[^>]*>Reason for Closure:</span><br>\s*(.*?)(?:<li>|$)')
    reasons_re = re.compile(r'&nbsp;([^;]*?)<br>')
    sleep = 1
    list_uri = 'http://publichealth.lacounty.gov/phcommon/public/eh/pool/plclosure.cfm'

    def list_pages(self):
        for facility_type in FACILITY_TYPES:
            params = {
                'TYPE': facility_type,
                'addrcity': '',
                'address': '',
                'addrzip': '',
                'dba': '',
                'selsort': 'cl_date',
            }
            yield self.get_page(self.list_uri, params)

    def clean_list_record(self, record):
        # Deviousness follows: The city name is repeated twice in the data, but
        # there is no reliable delimiter between the address and the first
        # occurence of the city name, so use the second occurance to strip the
        # city name out of the address.
        record['location'] = record['location'].replace(record['city'], '').strip()
        record['location'] = re.sub(r'\s+', ' ', record['location'])
        return record

    def existing_record(self, list_record):
        # We check for exisitng records in the save method because the list
        # record doesn't contain a date.
        return None

    def detail_required(self, list_record, old_record):
        return old_record is None

    def get_detail(self, list_record):
        return ScrapedPage(html=list_record['html'], when_crawled=datetime.datetime.now())

    def parse_detail(self, page, list_record):
        date_closed_match = self.date_closed_re.search(page)
        date_reopened_match = self.date_reopened_re.search(page)
        reasons_match = self.reasons_html_re.search(page)
        if reasons_match is not None:
            reasons_html = self.reasons_html_re.search(page).group(1)
            reasons = [m.group(1) for m in self.reasons_re.finditer(reasons_html)]
        else:
            reasons = None

        detail_record = {
            'date_closed': date_closed_match and date_closed_match.group(1) or None,
            'date_reopened': date_reopened_match and date_reopened_match.group(1) or None,
            'reasons': reasons
        }
        return detail_record

    def clean_detail_record(self, record):
        record['date_closed'] = parse_date(record['date_closed'], '%B %d, %Y')
        if record['date_reopened'] is not None:
            record['date_reopened'] = parse_date(record['date_reopened'], '%B %d, %Y')
        return record

    def save(self, old_record, list_record, detail_record, list_page, detail_page):
        if detail_record['date_closed'] is None:
            return
        qs = NewsItem.objects.filter(schema__id=self.schema.id, item_date=detail_record['date_closed'])
        qs = qs.by_attribute(self.schema_fields['name'], list_record['name'])
        try:
            old_record = qs[0]
        except IndexError:
            old_record = None

        city_lookup = self.get_or_create_lookup('city', list_record['city'], list_record['city'], make_text_slug=False)
        reason_lookups = []
        for r in detail_record['reasons']:
            lookup = self.get_or_create_lookup('reasons', r, r, make_text_slug=False)
            reason_lookups.append(lookup)
        name = list_record['name'] or list_record['location']
        title = "Pool at %s closed" % name
        attributes = {
            'name': name,
            'date_reopened': detail_record['date_reopened'],
            'reasons': ','.join([str(l.id) for l in reason_lookups]),
            'city': city_lookup.id
        }
        values = {
            'title': title,
            'item_date': detail_record['date_closed'],
            'location_name': '%s, %s' % (list_record['location'], city_lookup.name)
        }
        if old_record is None:
            self.create_newsitem(attributes, list_page=list_page, **values)
        else:
            self.update_existing(old_record, values, attributes, list_page=list_page)

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    Scraper().update()

########NEW FILE########
__FILENAME__ = repair
"""
Sets locaion=None on newsitems from cities in the suburbs table.
"""

from ebpub.db.models import NewsItem, Lookup
from ebpub.streets.models import Suburb
from everyblock.utils import queryset
import pprint

SCHEMA = 'food-inspections'
SUBURBS = [s[0] for s in Suburb.objects.values_list('normalized_name')]
CITIES = {}
for city_id, city_name in Lookup.objects.filter(schema_field__name='city', schema_field__schema__slug=SCHEMA).values_list('id', 'name'):
    CITIES[city_id] = city_name

def deloacte_suburbs():
    stats = {}
    for start, end, total, qs in queryset.batch(NewsItem.objects.filter(schema__slug=SCHEMA)):
        print "processing %s to %s of %s" % (start, end, total)
        for ni in qs:
            city = CITIES[ni.attributes['city']]
            if ni.location is not None and city in SUBURBS:
                ni.location = None
                stats[city] = stats.get(city, 0) + 1
                ni.save()
    print pprint.pprint(stats)

if __name__ == '__main__':
    deloacte_suburbs()

########NEW FILE########
__FILENAME__ = retrieval
"""
Scraper for Los Angeles restaurant inspections

http://www.lapublichealth.org/rating/
"""

from ebdata.retrieval.scrapers.new_newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from ebpub.streets.models import Suburb
from ebpub.utils.dates import parse_date
import datetime
import re

FACILITY_TYPES = (
    'Caterer',
    'Food Warehouse',
    'Restaurant',
    'Retail Food Market',
    'Retail Food Processor',
    'Wholesale Food Market',
)

class Scraper(NewsItemListDetailScraper):
    schema_slugs = ['food-inspections']
    parse_list_re = re.compile(r'(?s)<tr>\s*<td[^>]*>(?P<name>[\w\s\']*?)<br></td>\s*<td[^>]*>\s*(?P<location>.*?)<br>\s*</td>\s*<td[^>]*>(?P<city>.*?)<br></td>\s*<td[^>]*>(?P<zip>\d*?)<br></td>\s*<td[^>]*>(?P<date>\d{2}/\d{2}/\d{4})<br></td>\s*<td[^>]*>(?P<score>.*?)<br></td>\s*<td[^>]*>(?P<facility_type>.*?)<br></td>\s*<td[^>]*>\s*<form[^>]*>.*?<input type="hidden" name="dsiteid" value="(?P<dsiteid>.*?)">.*?</td>')
    parse_detail_re = re.compile(r'<tr>\s*<td[^>]*>\s*(?:Violations:|\s*)</td>\s*<td[^>]*>\s*<span[^>]*>\s*(?P<code>\d+)</span>\s*</td>\s*<td[^>]*>\s*<a[^>]*>\s*<span[^>]*>\s*(?P<violation>.*?)</span></a>')
    sleep = 1
    list_uri = 'http://www.lapublichealth.org/phcommon/public/eh/rating/ratesearchaction.cfm'
    detail_uri = 'http://www.lapublichealth.org/phcommon/public/eh/rating/ratedetail.cfm'

    def __init__(self, *args, **kwargs):
        self.get_archive = kwargs.pop('get_archive', False)
        self.perform_updates = kwargs.pop('perform_updates', False) # should we update existing records?
        self.suburbs = set([d['normalized_name'].upper() for d in Suburb.objects.values('normalized_name')])
        super(Scraper, self).__init__(*args, **kwargs)

    def list_pages(self):
        for facility_type in FACILITY_TYPES:
            row = 0
            while 1:
                params = {
                    'B1': 'Submit',
                    'address': '',
                    'city': '',
                    'dba': '',
                    'score': '',
                    'sort': 'inspdt',
                    'type': facility_type,
                    'zipcode': '',
                    'start': row + 1,
                    'row': row
                }
                page = self.get_page(self.list_uri, params)
                total = re.search(r'<b>(\d+) record\(s\) match your search criteria.<br></b>', page.html).group(1)
                yield page
                row += 100
                if row > int(total) or (not self.get_archive):
                    break

    def clean_list_record(self, record):
        record['date'] = parse_date(record['date'], '%m/%d/%Y')
        if record['score'] == 'NA':
            record['score'] = 'N/A'
        return record

    def existing_record(self, list_record):
        qs = NewsItem.objects.filter(schema__id=self.schema.id, item_date=list_record['date'])
        qs = qs.by_attribute(self.schema_fields['name'], list_record['name'])
        try:
            return qs[0]
        except IndexError:
            return None

    def detail_required(self, list_record, old_record):
        return self.perform_updates or old_record is None

    def get_detail(self, list_record):
        params = {
            'address': '',
            'alphalist': '',
            'checkbox': 'no',
            'city': list_record['city'],
            'dba': '',
            'dsiteid': list_record['dsiteid'],
            'score': '',
            'sort': 'inspdt',
            'start': '1',
            'type': '',
            'zipcode': '',
        }
        return self.get_page(self.detail_uri, params)

    def parse_detail(self, page, list_record):
        detail_record = {'violations': []}
        for m in self.parse_detail_re.finditer(page):
            detail_record['violations'].append(m.groupdict())
        return detail_record

    def clean_detail_record(self, record):
        return record

    def save(self, old_record, list_record, detail_record, list_page, detail_page):
        if list_record['date'] < datetime.date(2007, 7, 1):
            return
        if detail_record is None:
	    return
        facility_type_lookup = self.get_or_create_lookup('facility_type', list_record['facility_type'], list_record['facility_type'], make_text_slug=False)
        city_lookup = self.get_or_create_lookup('city', list_record['city'], list_record['city'], make_text_slug=False)

        violation_lookups = []
        for v in detail_record['violations']:
            lookup = self.get_or_create_lookup('violations', v['violation'], v['code'], make_text_slug=False)
            violation_lookups.append(lookup)
        attributes = {
            'name': list_record['name'],
            'facility_type': facility_type_lookup.id,
            'score': list_record['score'],
            'violations': ','.join([str(l.id) for l in violation_lookups]),
            'dsiteid': list_record['dsiteid'],
            'city': city_lookup.id
        }
        location_name = u'%s, %s' % (list_record['location'].decode('latin-1'), city_lookup.name)
        values = {
            'title': list_record['name'],
            'item_date': list_record['date'],
            'location_name': location_name,
        }
        # Don't try to geocode items in the suburbs table
        if city_lookup.name.upper() in self.suburbs:
            values['location'] = None
        if old_record is None:
            self.create_newsitem(attributes, list_page=list_page, detail_page=detail_page, **values)
        elif self.perform_updates:
            self.update_existing(old_record, values, attributes, list_page=list_page, detail_page=detail_page)

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    Scraper(get_archive=True, perform_updates=True).update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Scraper for restaurant closures in Los Angeles

http://www.lapublichealth.org/phcommon/public/eh/closure/restall1.cfm
"""

from ebdata.retrieval.scrapers.new_newsitem_list_detail import NewsItemListDetailScraper
from ebdata.retrieval.models import ScrapedPage
from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date
import datetime
import re

FACILITY_TYPES = (
    'Caterer',
    'Food Warehouse',
    'Restaurant',
    'Retail Food Market',
    'Retail Food Processor',
    'Wholesale Food Market',
)

class Scraper(NewsItemListDetailScraper):
    schema_slugs = ['food-closures']
    parse_list_re = re.compile(r'(?s)<tr>\s*<td[^>]*><br>\s*<hr[^>]*>\s*<b>(?P<name>.*?)</b>,\s*(?P<location>.*?), (?P<city>.*?), (?P<also_city>.*?), CA, \d+ <br></td>\s*</tr>\s*<tr>\s*<td[^>]*>\s*<ul[^>]*>\s*(?P<html>.*?)\s*</td>')
    date_closed_re = re.compile(r'<li><b><span[^>]*> Date Closed:</span></b>\s*(\w+ \d{2}, \d{4})')
    date_reopened_re = re.compile(r'<li><b>Date Reopened:</b>\s*(\w+ \d{2}, \d{4})')
    reasons_html_re = re.compile(r'(?s)<li><b>Reason for Closure:</b><br>\s*(.*?)(?:<li>|$)')
    reasons_re = re.compile(r'&nbsp;([^;]*?)<br>')
    sleep = 1
    list_uri = 'http://www.lapublichealth.org/phcommon/public/eh/closure/restall1.cfm'

    def list_pages(self):
        for facility_type in FACILITY_TYPES:
            params = {
                'TYPE': facility_type,
                'addrcity': '',
                'address': '',
                'addrzip': '',
                'dba': '',
                'selsort': 'cl_date',
            }
            yield self.get_page(self.list_uri, params)

    def clean_list_record(self, record):
        return record

    def existing_record(self, list_record):
        # We check for exisitng records in the save method because the list
        # record doesn't contain a date.
        return None

    def detail_required(self, list_record, old_record):
        return old_record is None

    def get_detail(self, list_record):
        return ScrapedPage(html=list_record['html'], when_crawled=datetime.datetime.now())

    def parse_detail(self, page, list_record):
        date_reopened_match = self.date_reopened_re.search(page)
        reasons_html = self.reasons_html_re.search(page).group(1)
        reasons = [m.group(1) for m in self.reasons_re.finditer(reasons_html)]

        detail_record = {
            'date_closed': self.date_closed_re.search(page).group(1),
            'date_reopened': date_reopened_match and date_reopened_match.group(1) or None,
            'reasons': reasons
        }
        return detail_record

    def clean_detail_record(self, record):
        record['date_closed'] = parse_date(record['date_closed'], '%B %d, %Y')
        if record['date_reopened'] is not None:
            record['date_reopened'] = parse_date(record['date_reopened'], '%B %d, %Y')
        return record

    def save(self, old_record, list_record, detail_record, list_page, detail_page):
        qs = NewsItem.objects.filter(schema__id=self.schema.id, item_date=detail_record['date_closed'])
        qs = qs.by_attribute(self.schema_fields['name'], list_record['name'])
        try:
            old_record = qs[0]
        except IndexError:
            old_record = None

        city_lookup = self.get_or_create_lookup('city', list_record['city'], list_record['city'], make_text_slug=False)
        reason_lookups = []
        for r in detail_record['reasons']:
            lookup = self.get_or_create_lookup('reasons', r, r, make_text_slug=False)
            reason_lookups.append(lookup)
        attributes = {
            'name': list_record['name'],
            'date_reopened': detail_record['date_reopened'],
            'reasons': ','.join([str(l.id) for l in reason_lookups]),
            'city': city_lookup.id
        }
        values = {
            'title': list_record['name'],
            'item_date': detail_record['date_closed'],
            'location_name': '%s, %s' % (list_record['location'], city_lookup.name)
        }
        if old_record is None:
            self.create_newsitem(attributes, list_page=list_page, **values)
        else:
            self.update_existing(old_record, values, attributes, list_page=list_page)

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    Scraper().update()

########NEW FILE########
__FILENAME__ = retrieval
from ebdata.blobs.scrapers import PageAreaCrawler
import re
import urlparse

class MiamiPressReleaseScraper(PageAreaCrawler):
    schema = 'city-press-releases'
    seed_url = 'http://www.miamigov.com/cms/comm/1724.asp'
    date_headline_re = re.compile(r'(?si)For Immediate Release<br>(?:[a-z]+, )?(?P<article_date>[a-z]+ \d\d?, \d\d\d\d)</p>.*?<FONT size=5>(?P<article_headline>.*?)</FONT>')
    date_format = '%B %d, %Y'

    def get_links(self, html):
        m = re.search(r'(?si)<div id="pageHeader">\s*News&nbsp;\s*</div>(.*?)<hr id="footerHR" />', html)
        area = m.group(1)
        return [urlparse.urljoin(self.seed_url, link) for link in re.findall('<a href="([^"]*)"', area)]

class CoralGablesPressReleaseScraper(PageAreaCrawler):
    schema = 'city-press-releases'
    seed_url = 'http://www.citybeautiful.net/CGWeb/newslist.aspx?newsid=ALL'
    date_headline_re = re.compile(r"(?si)<font class ='bodycopy'>(?P<article_date>\d\d?/\d\d?/\d\d\d\d): </font><font class='headline'>(?P<article_headline>.*?)</font>")
    date_format = '%m/%d/%Y'

    def get_links(self, html):
        return [urlparse.urljoin(self.seed_url, link) for link in re.findall("<a href='([^']*)'>Read full story</a>", html)]

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    MiamiPressReleaseScraper().update()
    CoralGablesPressReleaseScraper().update()

########NEW FILE########
__FILENAME__ = import_miami_blocks
#!/usr/bin/env python
import sys
import logging
from ebpub.streets.models import make_pretty_name, Block
from ebpub.utils.text import slugify
from django.contrib.gis.gdal import DataSource

logger = logging.getLogger('eb.cities.miami.import_blocks')

def import_blocks(blocks_layer):
    sides = ('R', 'L')
    for i, feature in enumerate(blocks_layer):
        for side in sides:
            from_num = feature['%s_ADD_FROM' % side]
            to_num = feature['%s_ADD_TO' % side]
            zip = feature['%s_ZIP' % side]
            street_name = feature['ST_NAME']
            if from_num and to_num and zip and street_name:
                if from_num > to_num:
                    from_num, to_num = to_num, from_num
                street_pretty_name, block_pretty_name = make_pretty_name(
                    from_num, to_num, feature['PRE_DIR'], street_name,
                    feature['ST_TYPE'], feature['SUF_DIR'])
                block, created = Block.objects.get_or_create(
                    pretty_name=block_pretty_name,
                    predir=(feature['PRE_DIR'] or ''),
                    street=street_name,
                    street_slug=slugify('%s %s' % (street_name, (feature['ST_TYPE'] or ''))),
                    street_pretty_name=street_pretty_name,
                    suffix=(feature['ST_TYPE'] or ''),
                    postdir=(feature['SUF_DIR'] or ''),
                    from_num=from_num,
                    to_num=to_num,
                    zip=zip,
                    city='FAKE', # we don't know it yetc
                    state='FL',
                    location='SRID=4326;%s' % str(feature.geometry)
                )
                logger.debug('%s block %r' % (created and 'Created' or 'Already had', block))
                if i % 100 == 0:
                    logger.info('Created %s blocks' % i)

class Usage(Exception):
    def __init__(self, msg):
        self.msg = msg

def main(argv=None):
    if argv is None:
        argv = sys.argv[1:]
    try:
        if len(argv) != 1:
            raise Usage('must provide path to streets shapefile')
        else:
            path = argv[0]
        logging.basicConfig(level=logging.DEBUG,
                            format='%(asctime)-15s %(levelname)-8s %(message)s')
        ds = DataSource(path)
        layer = ds[0]
        import_blocks(layer)
    except Usage, e:
        print >> sys.stderr, 'Usage: %s /path/to/shapefile' % sys.argv[0]
        print >> sys.stderr, e.msg
        return 2

if __name__ == '__main__':
    sys.exit(main())

########NEW FILE########
__FILENAME__ = retrieval
"""
Screen scraper for NYC Dept. of Buildings building permits.

The data comes in monthly Excel reports from the "Job Weekly Statistical Reports"
section of this page:
http://www.nyc.gov/html/dob/html/guides/weekly.shtml
"""

from ebdata.parsing.excel import ExcelDictReader
from ebdata.retrieval.scrapers.list_detail import SkipRecord
from ebdata.retrieval.retrievers import PageNotFoundError
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from ebpub.utils.text import smart_title
import datetime
import os

JOB_STATUS_HEADLINE_VERBS = {
    'A': 'pre-filed',
    'B': 'processed',
    'C': 'processed',
    'D': 'processed',
    'E': 'processed',
    'F': 'assigned for DOB exam',
    'G': 'changed plans',
    'H': 'initiated for exam',
    'J': 'disapproved by DOB',
    'K': 'partially approved',
    'L': 'waiting for change fee assessment',
    'M': 'approved for changed plans',
    'P': 'approved by DOB exam',
    'Q': 'approved for a partial permit',
    'R': 'approved',
    'U': 'completed',
    'X': 'signed off',
}

JOB_TYPE_HEADLINE_NOUNS = {
    'A1': 'alteration to property',
    'A2': 'alteration to property',
    'A3': 'alteration to property',
    'NB': 'creation of a new building',
    'PA': 'place of assembly',
    'DM': 'demolition',
    'SC': 'subdivision of property into condo units',
    'SI': 'subdivision of property into new/different lot sizes',
}

class PermitScraper(NewsItemListDetailScraper):
    schema_slugs = ('building-permits',)
    has_detail = False

    def __init__(self, week_end_dates=None):
        """
        week_end_dates is a list of datetime objects representing the week to
        download. If it's not provided, this will use the last three weeks.
        """
        super(PermitScraper, self).__init__(use_cache=False)
        if week_end_dates is None:
            # Use the last three Saturdays.
            end_date = datetime.date.today()
            while 1:
                end_date -= datetime.timedelta(days=1)
                if end_date.weekday() == 5:
                    break
            week_end_dates = [end_date - datetime.timedelta(days=7), end_date]
        self.week_end_dates = week_end_dates

    def list_pages(self):
        for week_end_date in self.week_end_dates:
            # They've used a different URL scheme over time.
            if week_end_date <= datetime.date(2005, 5, 13):
                url = 'http://www.nyc.gov/html/dob/downloads/download/foil/job%s.xls' % week_end_date.strftime('%m%d%y')
            else:
                url = 'http://www.nyc.gov/html/dob/downloads/excel/job%s.xls' % week_end_date.strftime('%m%d%y')

            try:
                workbook_path = self.retriever.get_to_file(url)
                yield ExcelDictReader(workbook_path, sheet_index=0, header_row_num=2, start_row_num=3)
                os.unlink(workbook_path) # Clean up the temporary file.
            except PageNotFoundError:
                self.logger.warn("Could not find %s" % url)

    def parse_list(self, reader):
        for row in reader:
            yield row

    def clean_list_record(self, record):
        record['address'] = '%s %s, %s' % (record['House #'], smart_title(record['Street Name']), smart_title(record['Borough']))
        record['is_landmark'] = record['Landmarked'] == 'Y'
        record['is_adult_establishment'] = record['Adult Estab'] == 'Y'
        record['is_city_owned'] = record['City Owned'] == 'Y'
        if not isinstance(record['Latest Action Date'], (datetime.date, datetime.datetime)):
            raise SkipRecord('Got last action date %s' % record['Latest Action Date'])
        return record

    def existing_record(self, record):
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id)
            qs = qs.by_attribute(self.schema_fields['job_number'], record['Job #'])
            qs = qs.by_attribute(self.schema_fields['doc_number'], record['Doc #'])
            return qs[0]
        except IndexError:
            return None

    def save(self, old_record, list_record, detail_record):
        if old_record is not None:
            return

        try:
            title = 'Permit application %s for %s' % \
                (JOB_STATUS_HEADLINE_VERBS[list_record['Job Status']],
                JOB_TYPE_HEADLINE_NOUNS[list_record['Job Type']])
        except KeyError:
            self.logger.warning('Got unknown values Job Status=%s, Job Type=%s', list_record['Job Status'], list_record['Job Type'])
            return

        job_type = self.get_or_create_lookup('job_type', list_record['Job Type'], list_record['Job Type'], make_text_slug=False)
        job_status = self.get_or_create_lookup('job_status', list_record['Job Status'], list_record['Job Status'], make_text_slug=False)
        building_type = self.get_or_create_lookup('building_type', list_record['Building Type'], list_record['Building Type'], make_text_slug=False)
        existing_occupancy = self.get_or_create_lookup('existing_occupancy', list_record['Existing Occupancy'], list_record['Existing Occupancy'], make_text_slug=False)
        proposed_occupancy = self.get_or_create_lookup('proposed_occupancy', list_record['Proposed Occupancy'], list_record['Proposed Occupancy'], make_text_slug=False)

        attributes = {
            'job_number': list_record['Job #'],
            'doc_number': list_record['Doc #'],
            'bin_number': list_record['Bin #'],
            'job_type': job_type.id,
            'job_status': job_status.id,
            'building_type': building_type.id,
            'is_landmark': list_record['is_landmark'],
            'is_adult_establishment': list_record['is_adult_establishment'],
            'is_city_owned': list_record['is_city_owned'],
            'estimated_cost': list_record['Initial Cost'] or None,
            'existing_occupancy': existing_occupancy.id,
            'proposed_occupancy': proposed_occupancy.id,
            'job_description': list_record['Job Description'],
        }
        self.create_newsitem(
            attributes,
            title=title,
            pub_date=list_record['Latest Action Date'],
            item_date=list_record['Latest Action Date'],
            location_name=list_record['address'],
        )

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    PermitScraper().update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Screen scraper for NYC crime aggregate data
http://www.nyc.gov/html/nypd/html/crime_prevention/crime_statistics.shtml
"""

from ebdata.parsing.pdftotext import pdfstring_to_text
from ebdata.retrieval.scrapers.base import ScraperBroken
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem, Location
from ebpub.utils.dates import parse_date
import re

PRECINCTS = (
    1, 5, 6, 7, 9, 10, 13, 14, 17, 18, 19, 20, 22, 23, 24, 25, 26, 28, 30, 32,
    33, 34, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 52, 60, 61, 62, 63, 66,
    67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 81, 83, 84, 88, 90, 94,
    100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114,
    115, 120, 122, 123
)

pdf_re = re.compile(r'Report Covering the Week (?:of )?(?P<start_date>\d\d?/\d\d?/\d\d\d\d) Through (?P<end_date>\d\d?/\d\d?/\d\d\d\d).*?Murder Rape Robbery Fel\. Assault Burglary Gr. Larceny G\.L\.A\.\s*(?P<murder>\d+) (?P<rape>\d+) (?P<robbery>\d+) (?P<felony_assault>\d+) (?P<burglary>\d+) (?P<grand_larceny>\d+) (?P<grand_larceny_auto>\d+)', re.DOTALL)

class CrimeScraper(NewsItemListDetailScraper):
    schema_slugs = ("crime",)
    has_detail = False

    def list_pages(self):
        for precinct in PRECINCTS:
            url = 'http://www.nyc.gov/html/nypd/downloads/pdf/crime_statistics/cs%03dpct.pdf' % precinct
            yield precinct, self.get_html(url)

    def parse_list(self, page):
        precinct, raw_pdf = page
        pdf_text = pdfstring_to_text(raw_pdf, keep_layout=False)
        m = pdf_re.search(pdf_text)
        if not m:
            raise ScraperBroken("Didn't find data in PDF for precinct %s" % precinct)
        else:
            yield dict(m.groupdict(), precinct=precinct)

    def clean_list_record(self, record):
        record['start_date'] = parse_date(record['start_date'], '%m/%d/%Y')
        record['end_date'] = parse_date(record['end_date'], '%m/%d/%Y')
        crime_types = ('murder', 'rape', 'robbery', 'felony_assault', 'burglary', 'grand_larceny', 'grand_larceny_auto')
        for key in crime_types:
            record[key] = int(record[key])
        record['total'] = sum([record[key] for key in crime_types])
        return record

    def existing_record(self, record):
        record['precinct_obj'] = self.get_or_create_lookup('precinct', 'Precinct %s' % record['precinct'], record['precinct'])
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id, item_date=record['start_date'])
            qs = qs.by_attribute(self.schema_fields['precinct'], record['precinct_obj'].id)
            return qs[0]
        except IndexError:
            return None

    def save(self, old_record, list_record, detail_record):
        if old_record is not None:
            # This data never changes, so we don't have to
            # worry about changing data that already exists.
            self.logger.debug('Data already exists')
            return

        num_crimes = list_record['total'] == 0 and 'No' or list_record['total']
        newsitem_title = '%s crime%s reported in Precinct %s' % (num_crimes, list_record['total'] != 1 and 's' or '', list_record['precinct'])
        location_object = Location.objects.get(location_type__slug='police-precincts', slug=list_record['precinct'])

        attributes = {
            'end_date': list_record['end_date'],
            'precinct': list_record['precinct_obj'].id,
            'murder': list_record['murder'],
            'rape': list_record['rape'],
            'robbery': list_record['robbery'],
            'felony_assault': list_record['felony_assault'],
            'burglary': list_record['burglary'],
            'grand_larceny': list_record['grand_larceny'],
            'grand_larceny_auto': list_record['grand_larceny_auto'],
        }
        self.create_newsitem(
            attributes,
            title=newsitem_title,
            item_date=list_record['start_date'],
            location=location_object.location,
            location_name='Precinct %s' % list_record['precinct'],
            location_object=location_object,
        )

class ArchivedCrimeScraper(CrimeScraper):
    """
    A scraper that gets its data from archived PDF files off the local
    filesystem. This is useful for loading backdated data.
    """
    def __init__(self, dir_name):
        super(ArchivedCrimeScraper, self).__init__()
        self.dir_name = dir_name

    def list_pages(self):
        import os
        for precinct in PRECINCTS:
            filename = os.path.join(self.dir_name, 'cs%03dpct.pdf' % precinct)
            yield precinct, open(filename).read()

if __name__ == "__main__":
    s = CrimeScraper()
    s.update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Screen scraper for NYC graffiti location data
https://a002-oom01.nyc.gov/graffiti/

More information is here:
http://www.nyc.gov/html/cau/html/anti_graffiti/main.shtml
"""

from ebdata.retrieval.scrapers.base import ScraperBroken
from ebdata.retrieval.scrapers.list_detail import SkipRecord
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date
import re

class GraffitiScraperBase(NewsItemListDetailScraper):
    has_detail = False

    def list_pages(self):
        html = self.get_html(self.source_url)

        m = re.search(r'<input type="hidden" name="__VIEWSTATE" id="__VIEWSTATE" value="([^"]*)"', html)
        if not m:
            raise ScraperBroken('VIEWSTATE not found on %s' % self.source_url)
        viewstate = m.group(1)

        m = re.search(r'<input type="hidden" name="__EVENTVALIDATION" id="__EVENTVALIDATION" value="([^"]*)"', html)
        if not m:
            raise ScraperBroken('EVENTVALIDATION not found on %s' % self.source_url)
        eventvalidation = m.group(1)

        yield self.get_html(self.source_url, {'__VIEWSTATE': viewstate, '__EVENTVALIDATION': eventvalidation, 'cmdFind': 'Find'})

    def parse_list(self, page):
        page = page.replace('&nbsp;', ' ')
        for record in self.parse_list_re.finditer(page):
            yield record.groupdict()

    def clean_list_record(self, record):
        record['waiver_date'] = parse_date(record['waiver_date'], '%m/%d/%y')
        record['address'] = ('%s %s %s' % (record.pop('street_number', '').strip(), record.pop('street_name', '').strip(), record.pop('street_suffix', '').strip())).strip()
        try:
            record['borough'] = {
                'BK': 'Brooklyn',
                'BX': 'The Bronx',
                'MN': 'Manhattan',
                'QS': 'Queens',
                'SI': 'Staten Island',
            }[record['borough']]
        except KeyError:
            raise SkipRecord('Invalid borough')
        return record

class PendingGraffitiScraper(GraffitiScraperBase):
    schema_slugs = ('graffiti-pending-cleanup',)
    parse_list_re = re.compile(r'(?si)<tr[^>]*>\s*<td[^>]*>(?P<street_number>[^<]*)</td><td[^>]*>(?P<street_name>[^<]*)</td><td[^>]*>(?P<street_suffix>[^<]*)</td><td[^>]*>(?P<borough>[^<]*)</td><td[^>]*>(?P<zipcode>[^<]*)</td><td[^>]*>[^<]*</td><td[^>]*>[^<]*</td><td[^>]*>[^<]*</td><td[^>]*>(?P<waiver_date>[^<]*)</td><td[^>]*>Waiver Received</td>\s*</tr>')
    source_url = 'https://a002-oom03.nyc.gov/graffiti/Pending.aspx'

    def existing_record(self, record):
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id, item_date=record['waiver_date'])
            qs = qs.by_attribute(self.schema_fields['address'], record['address'])
            qs = qs.by_attribute(self.schema_fields['borough'], record['borough'])
            return qs[0]
        except IndexError:
            return None

    def save(self, old_record, list_record, detail_record):
        if old_record is not None:
            # Graffiti data never changes, so we don't have to
            # worry about changing data that already exists.
            self.logger.debug('Data already exists')
            return

        attributes = {
            'address': list_record['address'],
            'borough': list_record['borough'],
        }
        self.create_newsitem(
            attributes,
            title='Graffiti reported at %s, %s' % (list_record['address'], list_record['borough']),
            url=self.source_url,
            item_date=list_record['waiver_date'],
            location_name='%s, %s' % (list_record['address'], list_record['borough']),
        )

class CompletedGraffitiScraper(GraffitiScraperBase):
    schema_slugs = ('graffiti-cleaned',)
    parse_list_re = re.compile(r'(?si)<tr[^>]*>\s*<td[^>]*>(?P<street_number>[^<]*)</td><td[^>]*>(?P<street_name>[^<]*)</td><td[^>]*>(?P<street_suffix>[^<]*)</td><td[^>]*>(?P<borough>[^<]*)</td><td[^>]*>(?P<zipcode>[^<]*)</td><td[^>]*>[^<]*</td><td[^>]*>[^<]*</td><td[^>]*>[^<]*</td><td[^>]*>(?P<waiver_date>\d\d/\d\d/\d\d)</td><td[^>]*>(?P<completed_on>\d\d/\d\d/\d\d)</td><td[^>]*>(?P<status>[^<]*)</td>\s*</tr>')
    source_url = 'https://a002-oom03.nyc.gov/graffiti/Completed.aspx'

    def clean_list_record(self, record):
        record = GraffitiScraperBase.clean_list_record(self, record)
        record['completed_on'] = parse_date(record['completed_on'], '%m/%d/%y')
        return record

    def existing_record(self, record):
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id, item_date=record['completed_on'])
            qs = qs.by_attribute(self.schema_fields['address'], record['address'])
            qs = qs.by_attribute(self.schema_fields['borough'], record['borough'])
            qs = qs.by_attribute(self.schema_fields['waiver_date'], record['waiver_date'])
            return qs[0]
        except IndexError:
            return None

    def save(self, old_record, list_record, detail_record):
        status = self.get_or_create_lookup('status', list_record['status'], list_record['status'], make_text_slug=False)
        attributes = {
            'address': list_record['address'],
            'borough': list_record['borough'],
            'waiver_date': list_record['waiver_date'],
            'status': status.id,
        }
        values = {
            'title': 'Graffiti cleaned up at %s, %s' % (list_record['address'], list_record['borough']),
            'url': self.source_url,
            'item_date': list_record['completed_on'],
            'location_name': '%s, %s' % (list_record['address'], list_record['borough']),
        }

        if old_record is None:
            self.create_newsitem(attributes, **values)
        else:
            self.update_existing(old_record, values, attributes)

def update_newest():
    s = PendingGraffitiScraper()
    s.update()
    s = CompletedGraffitiScraper()
    s.update()

if __name__ == "__main__":
    update_newest()

########NEW FILE########
__FILENAME__ = retrieval
"""
Screen scraper for NYC Landmarks Preservation Commission data, as found on
citylaw.org.

To replicate what this scraper does, go here:
   http://www.nyls.edu/centers/harlan_scholar_centers/center_for_new_york_city_law/cityadmin_library/

Check the "LANDMARKS" checkbox, then enter "cofa" in the search keywords box
and click "Search." Each resulting PDF is converted to text and parsed with a
regex.
"""

from ebdata.parsing.pdftotext import pdfstring_to_text
from ebdata.retrieval.scrapers.list_detail import StopScraping, SkipRecord
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date
import re
import urlparse

issued_re = re.compile('(?s)^ISSUED TO:.*?\n\n')

class LandmarkScraper(NewsItemListDetailScraper):
    schema_slugs = ('landmark-building-permits',)
    parse_list_re = re.compile(r'<a href="http://archive\.citylaw\.org/lpc/permit/(?P<pdf_year>\d{4})/(?P<pdf_id>\d+).pdf" target="_blank">(?P<link_text>.*?)</a>', re.DOTALL)
    parse_detail_re = re.compile(r'CERTIFICATE OF APPROPRIATENESS\n\s+ISSUE DATE:\s{5,}EXPIRATION DATE:\s{5,}DOCKET #:\s{5,}COFA #:\n+\s+(?P<issue_date>\d\d?/\d\d?/\d\d\d\d)\s{5,}(?P<expiration_date>\d\d?/\d\d?/\d\d\d\d)?\s{5,}(?P<docket>[-\d]+)\s{5,}(?P<cofa>COFA [-\d]+)\n+\s+ADDRESS\s{5,}BOROUGH:\s{5,}BLOCK/LOT:\n+\s+(?P<address>.*?)\n+\s+(?P<district_or_landmark>HISTORIC DISTRICT|INDIVIDUAL LANDMARK|INTERIOR LANDMARK)\n\s+(?P<historic_district>.*?)\s{5,}(?P<borough>.*?)\s{5,}\d+/\d+\n(?P<second_line_of_district>[^\n]*?)\n\n\s*(?P<text>.+)\s*$', re.DOTALL)

    def list_pages(self):
        # This relies on a subsequent part of the scraper to raise StopScraping.
        start = 0
        while 1:
            params = '?sort=date%%3AD%%3AS%%3Ad1&num=10&q=cofa&site=cl_lpc&start=%s' % start
            page_url = urlparse.urljoin('http://www.nyls.edu/centers/harlan_scholar_centers/center_for_new_york_city_law/cityadmin_library/', params)
            yield self.get_html(page_url)
            start += 10

    def clean_list_record(self, record):
        record['pdf_url'] = 'http://archive.citylaw.org/lpc/permit/%s/%s.pdf' % (record['pdf_year'], record['pdf_id'])
        return record

    def existing_record(self, record):
        # We can't determine whether there's an existing record until we see
        # list_record, so this decision is deferred until clean_detail_record().
        return None

    def detail_required(self, list_record, old_record):
        return True

    def get_detail(self, record):
        # Save the URL so that we can refer to it from parse_detail().
        self.__current_url = record['pdf_url']
        return self.get_html(record['pdf_url'])

    def parse_detail(self, page, list_record):
        text = pdfstring_to_text(page)
        m = self.parse_detail_re.search(text)
        if m:
            self.logger.debug('Got a match for parse_detail_re')
            return m.groupdict()
        else:
            self.logger.warning("Regex failed on %s", self.__current_url)
            raise SkipRecord

    def clean_detail_record(self, record):
        record['expiration_date'] = parse_date(record['expiration_date'], '%m/%d/%Y')
        record['issue_date'] = parse_date(record['issue_date'], '%m/%d/%Y')

        # The PDF text is in the ISO-8859-1 encoding. Convert it here so that
        # we don't get an encoding error when we save it to the database.
        record['text'] = record['text'].decode('iso-8859-1')
        record['text'] = record['text'].replace('Display This Permit While Work Is In Progress', '')
        record['text'] = record['text'].strip()

        # Remove the "ISSUED TO" section, as it contains the name of the person
        # who owns the property, and we have a policy of not displaying names.
        # Note that we include a sanity check that the "ISSUED TO" section
        # doesn't contain more than 9 newlines, as that would signify a broken
        # regular expression.
        m = issued_re.search(record['text'])
        if m and m.group(0).count('\n') < 10:
            record['text'] = issued_re.sub('', record['text'])

        if record['second_line_of_district'].strip():
            record['historic_district'] += record['second_line_of_district']

        if record['district_or_landmark'] == 'HISTORIC DISTRICT':
            record['landmark'] = 'N/A'
        else:
            record['landmark'] = record['historic_district']
            record['historic_district'] = 'N/A'

        # Check for a duplicate record. Because the scraper works in
        # reverse-chronological order, we can safely raise StopScraping if we
        # reach a duplicate.
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id)
            qs = qs.by_attribute(self.schema_fields['docket'], record['docket'])
            qs = qs.by_attribute(self.schema_fields['cofa'], record['cofa'])
            old_record = qs[0]
        except IndexError:
            pass
        else:
            raise StopScraping('Found a duplicate record %s' % old_record.id)

        return record

    def save(self, old_record, list_record, detail_record):
        historic_district = self.get_or_create_lookup('historic_district', detail_record['historic_district'], detail_record['historic_district'])
        address = '%s, %s' % (detail_record['address'], detail_record['borough'])

        if detail_record['historic_district'] == 'N/A':
            title = 'Landmark permit issued for %s' % address
        else:
            title = 'Landmark permit issued for %s in %s' % (address, historic_district.name)

        attributes = {
            'cofa': detail_record['cofa'],
            'docket': detail_record['docket'],
            'historic_district': historic_district.id,
            'landmark_name': detail_record['landmark'],
            'expiration_date': detail_record['expiration_date'],
            'text': detail_record['text'],
        }
        self.create_newsitem(
            attributes,
            title=title,
            url=list_record['pdf_url'],
            item_date=detail_record['issue_date'],
            location_name=address,
        )

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    s = LandmarkScraper()
    s.update()

########NEW FILE########
__FILENAME__ = import_codes
from ebpub.db.models import Lookup, SchemaField, Schema
from ebpub.utils.text import smart_title
import re
import os.path

codes = re.compile(r"^([A-Z0-9]{3})\s+([0-9A-Z*]{1,2})\s+(.+)$").findall
schema = Schema.objects.get(slug="liquor-licenses")
schema_field = SchemaField.objects.get(schema=schema, name="license_class")
try:
    f = open(os.path.join(os.path.dirname(__file__), "codes.txt"))
    for line in f:
        class_code, license_type, name = codes(line[:-1])[0]
        name = " / ".join([smart_title(s, ["O.P."]).strip() for s in name.split("/")])
        lookup, created = Lookup.objects.get_or_create(
            schema_field=schema_field,
            name=name,
            code=class_code+"-"+license_type
        )
        if created:
            print "Created %r" % lookup
finally:
    f.close()

########NEW FILE########
__FILENAME__ = retrieval
"""
Scraper for NYC liquor licenses.

http://www.trans.abc.state.ny.us/JSP/query/PublicQueryAdvanceSearchPage.jsp
"""

from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem, Location
from ebpub.streets.models import Misspelling
from ebpub.utils.text import smart_title
from dateutil.parser import parse
from lxml.html import document_fromstring
import datetime
import re

smart_business_title = lambda s: smart_title(s, ["LLC", "BBQ"])

nyc_counties = {
    "NEW": "New York",
    "KING": "Kings",
    "BRON": "Bronx",
    "RICH": "Richmond",
    "QUEE": "Queens"
}

class LiquorLicenseScraper(NewsItemListDetailScraper):
    schema_slugs = ('liquor-licenses',)
    uri = 'http://www.trans.abc.state.ny.us/servlet/ApplicationServlet'
    sleep = 0

    def __init__(self, *args, **kwargs):
        self.start_date = kwargs.pop('start_date', None)
        self.first_page = True # False once we've retreived the first list page
        self.failed_cities = {}
        super(LiquorLicenseScraper, self).__init__(*args, **kwargs)

    def list_pages(self):
        if self.start_date is None:
            self.start_date = datetime.datetime.now() - datetime.timedelta(days=7)

        for county in nyc_counties.keys():
            if self.first_page:
                html = self.get_html(self.uri, {
                    'category': 'NonePremise',
                    'city': '',
                    'county': county,
                    'dateType': 'rd', # Date received
                    'endDay': '1',
                    'endMonth': '1',
                    'endYear': '',
                    'licenseStatus': 'al',
                    'pageName': 'com.ibm.nysla.data.publicquery.PublicQueryAdvanceSearchPage',
                    'startDay': str(self.start_date.day),
                    'startMonth': str(self.start_date.month),
                    'startYear': str(self.start_date.year),
                    'validated': 'true',
                    'zipCode': '',
                })

                m = re.search(r'Displaying records \d+ - (\d+)', html)
                last_result_num = m.group(1)
                m = re.search(r'Found (\d+) matches', html)
                total = m.group(1)

                self.first_page = False
                yield html

            while 1:
                # next_list_page will return None when we reach the last page
                page = self.next_list_page()
                if page is None:
                    self.first_page = True
                    break
                yield page

    def next_list_page(self):
        html = self.get_html(self.uri, {
            'NextButton': '+Next+',
            'pageName': 'com.ibm.nysla.data.publicquery.PublicQueryAdvanceSearchPageResults',
            'validated': 'true'
        })
        m = re.search(r'Displaying records \d+ - (\d+)', html)
        last_result_num = m.group(1)
        m = re.search(r'Found (\d+) matches', html)
        total = m.group(1)
        print "%s/%s" % (last_result_num, total)
        if last_result_num == total:
            return None
        return html

    def parse_list(self, page):
        t = document_fromstring(page)
        for tr in t.xpath("//tr/td[@class='displayvalue']/parent::*"):
            record = {
                'name': tr[0][0].text_content().strip(),
                'url': tr[0][0].get('href'),
                'address': re.sub(r'[\r|\n|\t]+', ' ', tr[1].text_content()).strip(),
                'license_class': tr[2].text_content(),
                'license_type': tr[3].text_content(),
                'expiration_date': tr[4].text_content(),
                'status': tr[5].text_content()
            }
            record['serial_number'] = re.search(r'serialNumber=(\d+)&', record['url']).group(1)
            yield record

    def get_detail(self, list_record):
        return self.get_html('http://www.trans.abc.state.ny.us' + list_record['url'])

    def detail_required(self, list_record, old_record):
        return True

    def parse_detail(self, page, list_record):
        t = document_fromstring(page)
        record = {}
        for tr in t.xpath("//td[@class='displayvalue']/parent::*"):
            key = tr[1].text_content() or ''
            value = tr[2].text_content() or ''
            record[key.strip()] = value.strip()

        # If there's no filing date, this detail page is related to another
        # license. Go get the dates from that page.
        if not record.has_key('Filing Date:'):
            a = t.xpath("//div[@class='instructions']//a")[0]
            page = self.get_html('http://www.trans.abc.state.ny.us' + a.get('href'))
            t = document_fromstring(page)
            parent_record = {}
            for tr in t.xpath("//td[@class='displayvalue']/parent::*"):
                key = tr[1].text_content() or ''
                value = tr[2].text_content() or ''
                parent_record[key.strip()] = value.strip()
            dates = {
                'Filing Date:': parent_record['Filing Date:'],
                'Effective Date:': parent_record['Effective Date:'],
                'Expiration Date:': parent_record['Expiration Date:'],
            }
            record.update(dates)
        return record

    def existing_record(self, record):
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id)
            return qs.by_attribute(self.schema_fields['serial_number'], record['serial_number'])[0]
        except IndexError:
            return None

    def clean_list_record(self, record):
        return record

    def clean_detail_record(self, record):
        if not record.has_key('Filing Date:'):
            return None
        # dateutil.parser.parse will return the current date when given an empty
        # string, so we test for that here and set the cleaned dates to None
        # for empty strings rather than using parse.
        record['effective_date'] = record['Effective Date:'] and parse(record['Effective Date:'], fuzzy=True).date() or None
        record['expiration_date'] = record['Expiration Date:'] and parse(record['Expiration Date:'], fuzzy=True).date() or None
        record['filing_date'] = record['Filing Date:'] and parse(record['Filing Date:'], fuzzy=True).date() or None
        record['address'] = record['Address:'].split(' AKA ')[0]
	if record['address'] == '':
	    # No address. Bail out on cleaning.
	    return record
        record['address2'] = record['']
        record['premises_name'] = smart_business_title(record['Premises Name:'])
        record['city'] = re.match(r'([\w|\s]+), N', record['address2']).groups(1)[0].strip()
        # Normalize things like "Broadway At 65th Street, Manhattan" and
        # "351 353 West 14th Street, Manhattan"
        record['address'] = re.sub(r'(\d+)\s(\d+)', r'\1', record['address']).replace(' AT ', ' and ')

        record['city'] = record['city'].upper().strip()
        try:
            record['city'] = Misspelling.objects.get(incorrect=record['city']).correct
        except Misspelling.DoesNotExist:
            pass
        if record['city'] == 'NEW YORK':
            record['city'] = 'MANHATTAN'
        return record

    def save(self, old_record, list_record, detail_record):
        # Throw away records with no filing date. We need to draw a line
        # somewhere. That line is here.
        if detail_record['filing_date'] is None:
            return
        if detail_record['filing_date'] < datetime.date(2008, 1, 1):
            return
        if detail_record['address'] == '':
	    return
        # If we can't find the city in the locations table, skip this record.
        try:
            loc = Location.objects.select_related().get(normalized_name=detail_record['city'])
            # If we have a neighborhood, the city should be the borough.
            if loc.location_type.slug == 'neighborhoods':
                detail_record['city'] = loc.city
        except Location.DoesNotExist:
            self.failed_cities[detail_record['city']] = self.failed_cities.get(detail_record['city'], 0) + 1
            return

        status = self.get_or_create_lookup('status', detail_record['License Status:'], detail_record['License Status:'], make_text_slug=False)
        license_type = self.get_or_create_lookup('license_type', smart_title(detail_record['License Type:']), detail_record['License Type:'], make_text_slug=False)

        premises_name = detail_record['premises_name']
        pretty_date = detail_record['filing_date'].strftime('%B %d, %Y')
        title = 'Application for %s' % premises_name
        item_date = detail_record['filing_date']
        location_name = smart_title("%s, %s" % (detail_record['address'], detail_record['city']))
        attributes = {
            'serial_number': list_record['serial_number'],
            'effective_date': detail_record['effective_date'],
            'expiration_date': detail_record['expiration_date'],
            'premises_name': premises_name,
            'status': status.id,
            'license_type': license_type.id,
        }
        if old_record is None:
            self.create_newsitem(
                attributes,
                title=title,
                item_date=item_date,
                location_name=location_name,
                url='http://www.trans.abc.state.ny.us' + list_record['url']
            )
        else:
            # This license already exists in our database, but it may have
            # changed status, so save any new values.
            new_values = {
                'title': title,
                'item_date': item_date,
                'location_name': location_name,
            }
            self.update_existing(old_record, new_values, attributes)

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    scraper = LiquorLicenseScraper()
    scraper.update()
    print scraper.failed_cities

########NEW FILE########
__FILENAME__ = retrieval
"""
Site-specific scrapers for NYC news sources that don't have RSS feeds.
"""

from ebdata.blobs.scrapers import IncrementalCrawler
import re

class BrooklynEagleCrawler(IncrementalCrawler):
    schema = 'news-articles'
    seed_url = 'http://www.brooklyneagle.com/'
    date_headline_re = re.compile(r'<b><span class="f24">(?P<article_headline>.*?)</span></b><div align="justify" class="f11">.*?, published online <span[^>]*>(?P<article_date>\d\d?-\d\d?-\d\d\d\d)</span></div>')
    date_format = '%m-%d-%Y'
    max_blanks = 7

    def public_url(self, id_value):
        # Note that the category_id doesn't matter.
        return 'http://www.brooklyneagle.com/categories/category.php?id=%s' % id_value

    def id_for_url(self, url):
        return url.split('id=')[1]

class Ny1Crawler(IncrementalCrawler):
    schema = 'news-articles'
    seed_url = 'http://www.ny1.com/'
    date_headline_re = re.compile(r'<span id="ArPrint_lblArHeadline" class="blackheadline1">(?P<article_headline>.*?)</span><br />\s*<span id="ArPrint_lblArPostDate" class="black11">(?:<strong>Updated&nbsp;</strong>)?(?P<article_date>\d\d?/\d\d?/\d\d\d\d)')
    date_format = '%m/%d/%Y'
    max_blanks = 7

    def public_url(self, id_value):
        return 'http://www.ny1.com/Default.aspx?ArID=%s' % id_value

    def retrieval_url(self, id_value):
        return 'http://www.ny1.com/printarticle.aspx?ArID=%s' % id_value

    def id_for_url(self, url):
        return url.split('ArID=')[1]

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    BrooklynEagleCrawler().update()
    Ny1Crawler().update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Screen scraper for NYC property sales.

The data comes in monthly Excel reports from the "Rolling Sales" section of
this page:
http://www.nyc.gov/html/dof/html/property/property_val_sales.shtml
"""

from django.utils.text import capfirst
from ebdata.parsing.excel import ExcelDictReader
from ebdata.retrieval.scrapers.base import ScraperBroken
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from ebpub.utils.text import intcomma, smart_title
import os
import re
import urlparse

class SalesScraper(NewsItemListDetailScraper):
    schema_slugs = ('property-sales',)
    has_detail = False

    def list_pages(self):
        # Get the HTML page, which includes links to Excel files (one for each
        # borough). We do this instead of hard-coding the Excel file names in
        # the scraper because the Excel file names tend to change each month.
        url = 'http://www.nyc.gov/html/dof/html/property/property_val_sales.shtml'
        html = self.get_html(url)
        excel_links = re.findall(r'href="([^"]+\.xls)"', html)
        if len(excel_links) != 12:
            raise ScraperBroken('Got a strange number of Excel links: %s' % len(excel_links))

        # The first five links are the "New York City Sales Data" links,
        # which is what we want.
        for excel_link in excel_links[:5]:
            excel_url = urlparse.urljoin(url, excel_link)
            workbook_path = self.retriever.get_to_file(excel_url)
            reader = ExcelDictReader(workbook_path, sheet_index=0, header_row_num=4, start_row_num=5)
            yield reader
            os.unlink(workbook_path) # Clean up the temporary file.

    def parse_list(self, reader):
        for row in reader:
            yield row

    def clean_list_record(self, record):
        # Strip extra internal whitespace.
        record['BUILDING CLASS CATEGORY'] = re.sub(r'\s+', ' ', record['BUILDING CLASS CATEGORY'])
        record['category_name'] = capfirst(record['BUILDING CLASS CATEGORY'][3:].lower())
        try:
            record['sale_price'] = str(int(record['SALE PRICE']))
        except ValueError:
            record['sale_price'] = 'N/A'

        try:
            year_built = str(int(record['YEAR BUILT']))
        except ValueError:
            year_built = 'N/A'
        if year_built == '0':
            year_built = 'N/A'
        record['year_built'] = year_built

        try:
            address, unit = record['ADDRESS'].split(', ')
        except ValueError:
            address, unit = record['ADDRESS'], ''
        address = smart_title(address)
        record['clean_address'] = address
        record['clean_address_with_unit'] = '%s%s' % (address, (unit and ', ' + unit or ''))

        record['borough'] = {
            1: 'Manhattan',
            2: 'Bronx',
            3: 'Brooklyn',
            4: 'Queens',
            5: 'Staten Island'
        }[record['BOROUGH']]

        return record

    def existing_record(self, list_record):
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id, item_date=list_record['SALE DATE'])
            qs = qs.by_attribute(self.schema_fields['raw_address'], list_record['ADDRESS'])
            return qs[0]
        except IndexError:
            return None

    def save(self, old_record, list_record, detail_record):
        if old_record is not None:
            # Records never change, so we don't have to
            # worry about changing ones that already exist.
            self.logger.debug('Permit record already exists: %s' % old_record)
            return

        category = self.get_or_create_lookup('category', list_record['category_name'], list_record['BUILDING CLASS CATEGORY'])
        year_built = self.get_or_create_lookup('year_built', list_record['year_built'], list_record['year_built'])
        building_class = self.get_or_create_lookup('building_class', list_record['BUILDING CLASS AT TIME OF SALE'], list_record['BUILDING CLASS AT TIME OF SALE'])

        if list_record['sale_price'] == '0':
            title = '%s transferred ownership with no cash consideration' % list_record['clean_address_with_unit']
        else:
            title = '%s sold for $%s' % (list_record['clean_address_with_unit'], intcomma(list_record['sale_price']))

        attributes = {
            'clean_address': list_record['clean_address_with_unit'],
            'raw_address': list_record['ADDRESS'], # Save this for use in future scrapes.
            'sale_price': list_record['sale_price'],
            'category': category.id,
            'year_built': year_built.id,
            'building_class': building_class.id,
            'gross_square_feet': list_record['GROSS SQUARE FEET'],
            'total_units': list_record['TOTAL UNITS'],
        }
        self.create_newsitem(
            attributes,
            title=title,
            pub_date=list_record['SALE DATE'],
            item_date=list_record['SALE DATE'],
            location_name='%s, %s' % (list_record['clean_address'], list_record['borough']),
        )

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    SalesScraper().update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Screen scraper for NYC restaurant inspections.

Introductory page:
http://www.nyc.gov/html/doh/html/rii/index.shtml

Direct link to search page:
http://167.153.150.32/RI/web/index.do;?method=alphaSearch
"""

from ebdata.retrieval.scrapers.base import ScraperBroken
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebdata.retrieval import UnicodeRetriever
from ebdata.retrieval.utils import convert_entities
from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date
import md5
import re
from urllib import urlencode

violation_list_re = re.compile(r'<tr><td[^>]*>.*?</td><td[^>]*>(.*?)</td></tr>')
strip_tags = lambda x: re.sub(r'<[^>]*>', ' ', x)
detail_url = lambda restaurant_id, inspection_date: 'http://167.153.150.32/RI/web/detail.do?method=detail&restaurantId=%s&inspectionDate=%s' % (restaurant_id, inspection_date.strftime('%Y%m%d'))

DEFAULT_LETTERS = ['0-9'] + list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')

class RestaurantScraper(NewsItemListDetailScraper):
    schema_slugs = ('restaurant-inspections',)
    parse_list_re = re.compile(r'class="restaurantinfo"><b><a href="detail\.do\?method=detail&restaurantId=(?P<restaurant_id>\d+)&inspectionDate=(?P<inspection_date>\d+)"')
    parse_detail_re = re.compile(r'<tr><td valign="top" width="99%"><b>(?P<restaurant_name>[^>]+)</b><br />(?P<address>[^>]+)<br />(?P<phone>[^>]*)<br /><br /><span style="letter-spacing:-1px;">Violation points: <b>\s*(?P<violation_points>.+?)</b></span></td><td valign="top" width="1%" align="right">.*?</td></tr><tr><td colspan="2"><br />Inspection Date: <b>(?P<inspection_date>[^>]*)</b><br /><br /><b>[^\n]*\n\s*<table border="0" cellspacing="0" cellpadding="3" width="90%" align="left">\s*(?P<violations>.*?)\s*</table>', re.DOTALL)
    sleep = 1

    def __init__(self, letters=None, *args, **kwargs):
        super(RestaurantScraper, self).__init__(*args, **kwargs)
        self.letters = letters or DEFAULT_LETTERS
        self.retriever = UnicodeRetriever()

    def list_pages(self):
        url = 'http://167.153.150.32/RI/web/index.do?'
        params = {
            'method': 'alphaSearch',
            'selection': '',
            'searchValue': '',
            'requestedSortOrder': '1', # 1=alphabetically, 2=by violation points
            'boroughSelect': '',
            'state': 'prompt',
        }
        for letter in self.letters:
            for page in xrange(1, 3000): # We'll never reach 3000, so use it as an upper bound.
                html = self.get_html(url + urlencode(dict(params, alphaValue=letter, pageNum=page)))
                if 'class="restaurantinfo"' not in html:
                    break
                yield html

    def clean_list_record(self, record):
        record['inspection_date'] = parse_date(record['inspection_date'], '%Y%m%d')
        return record

    def existing_record(self, record):
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id, item_date=record['inspection_date'])
            qs = qs.by_attribute(self.schema_fields['restaurant_id'], record['restaurant_id'])
            return qs[0]
        except IndexError:
            return None

    def detail_required(self, list_record, old_record):
        return True

    def get_detail(self, record):
        url = detail_url(record['restaurant_id'], record['inspection_date'])
        return self.get_html(url)

    def clean_detail_record(self, record):
        if record == {}:
            # Parsing the detail page failed.
            return None
        if record['violation_points'].startswith('Not Available'):
            record['violation_points'] = 'N/A'
            record['followup_inspection'] = False
        else:
            if not record['violation_points'].isdigit():
                raise ScraperBroken('Got odd violation points value %r' % record['violation_points'])
            record['followup_inspection'] = int(record['violation_points']) > 27

        # Parse the violations from the HTML chunk. When we're done,
        # record['violation_list'] will be a (possibly empty) list of strings.
        vio_chunk = record.pop('violations')
        if vio_chunk == '':
            record['violation_list'] = []
        else:
            vios = violation_list_re.findall(vio_chunk)
            if not vios:
                raise ScraperBroken("Violation data not found for restaurant %s", record['restaurant_name'])
            record['violation_list'] = [strip_tags(convert_entities(v.strip())) for v in vios]

        # Remove the ZIP code from the address, as it complicates geocoding.
        record['address'] = re.sub(r'\s*\d{5}\s*$', '', record['address'])
        # Strip extra internal whitespace.
        record['address'] = re.sub(r'\s+', ' ', record['address'])

        return record

    def save(self, old_record, list_record, detail_record):
        # Parsing the detail page failed.
        if detail_record is None:
            return

        # Violation text can be long -- so long that it breaks our 255-character
        # limit on Lookup.code. Thus, use an MD5 hash of the violation as its
        # code.
        md5hash = lambda x: md5.new(x).hexdigest()
        violations = [self.get_or_create_lookup('violation', v, md5hash(v.encode('utf8')), make_text_slug=False) for v in detail_record['violation_list']]
        violations_text = ','.join([str(v.id) for v in violations])
        violation_points = self.get_or_create_lookup('violation_points', detail_record['violation_points'], detail_record['violation_points'])

        title = '%s inspected' % detail_record['restaurant_name']
        if detail_record['violation_points'] == '0':
            title += ': No violation points'
        elif detail_record['violation_points'] != 'N/A':
            title += ': %s violation point%s' % \
                (detail_record['violation_points'], detail_record['violation_points'] != '1' and 's' or '')

        num_violations = len(detail_record['violation_list'])
        values = {
            'title': title,
            'description': '%s violation%s cited.' % (num_violations, num_violations != '1' and 's' or ''),
            'url': detail_url(list_record['restaurant_id'], list_record['inspection_date']),
            'item_date': list_record['inspection_date'],
            'location_name': detail_record['address'],
        }
        attributes = {
            'restaurant_id': list_record['restaurant_id'],
            'restaurant_name': detail_record['restaurant_name'],
            'phone': detail_record['phone'],
            'violation_points': violation_points.id,
            'violation': violations_text,
            'followup_inspection': detail_record['followup_inspection'],
        }
        if old_record is None:
            self.create_newsitem(attributes, **values)
        else:
            self.update_existing(old_record, values, attributes)

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    s = RestaurantScraper()
    s.update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Screen scraper for NYC SCOUT data.

http://gis.nyc.gov/moo/scout/

To check status of a particular service request by service request ID:
http://www.nyc.gov/portal/site/threeoneone/

There's no way to get a permalink for a particular service request, because
they have a CAPTCHA. But at least this URL prepopulates the search field with
the given service request ID ("XXX"):
http://www.nyc.gov/portal/site/threeoneone/template.PAGE/menuitem.dfb4f4b32cf05387fd8a9010acd2f9a0/?servicerequestnumber=XXX
"""

from django.contrib.gis.geos import Point
from django.utils import simplejson
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date
from lxml import etree
import re

def parse_json(value):
    # This is custom for the JSON returned by the SCOUT site, which has some
    # funky stuff.
    value = re.sub(r'^\{\}\&\&', '', value)  # Trim leading "{}&&"
    value = re.sub(r'^\/\*', '', value)      # Trim leading "/*"
    value = re.sub(r'\*\/$', '', value)      # Trim trailing "*/"
    return simplejson.loads(value)

class ScoutScraper(NewsItemListDetailScraper):
    schema_slugs = ('scout',)
    has_detail = False
    sleep = 3

    def list_pages(self):
        # In my tests, I found that it's possible to get *every* point in NYC
        # with this two step process:
        #     1. Search for a Community Board.
        #     2. Zoom out one level.
        # Originally, this script searched each Community Board, but due to the
        # way the SCOUT site works, that meant that there was much duplication
        # in the data -- when you do a search for a CB, the site returns all
        # points within a pretty broad bounding box around that CB (i.e., not
        # just the points within the CB itself).

        # First, do a search for Community Board 405 (Queens #5). This is
        # located centrally in the city, just to be safe.
        data = {
            'clientQuery': '',
            'mapData': {
                "applicationName":"SCOUT",
                "cacheName":"basic",
                "clientDataStore":{
                    "declaredClass":"CDS",
                    "thematicPolygon":True,
                    "themeAlias":"C0",
                    "themeCountAlias":"C1",
                    "themeFeatureTypeName":"COMMUNITY_BOARD"
                },
                "declaredClass":"MD",
                "groupedClientData":{
                    "declaredClass":"PGD",
                    "envelope":{
                        "declaredClass":"ME",
                        "height":152878.5,
                        "maxX":1067317.2,
                        "maxY":272931.8,
                        "minX":913090.8,
                        "minY":120053.3,
                        "width":154226.4
                    },
                    "polygonGroupedClientData":True,
                    "themeFeatureTypeName":"COMMUNITY_BOARD"
                },
                "legend":{
                    "declaredClass":"LEG",
                    "description":"Days Since Last SCOUT Inspection by Community Board",
                    "markup":"""<?xml version="1.0" encoding="UTF-8"?><xhtml:div xmlns:svg="http://www.w3.org/2000/svg" xmlns:xhtml="http://www.w3.org/1999/xhtml" xmlns:xlink="http://www.w3.org/1999/xlink"><svg:svg height="57px" version="1.1" width="186px"><svg:rect fill="#C9BFAB" fill-opacity="1.0" height="14" id="bucket0bg" title="1 to 10 days" width="14" x="5.0" y="5.0"/><svg:rect fill="#F03B20" fill-opacity="0.8" height="14" id="bucket0rect" title="1 to 10 days" width="14" x="5.0" y="5.0"/><svg:rect fill="#C9BFAB" fill-opacity="1.0" height="14" id="bucket1bg" title="11 to 20 days" width="14" x="5.0" y="23.0"/><svg:rect fill="#FEB24C" fill-opacity="0.8" height="14" id="bucket1rect" title="11 to 20 days" width="14" x="5.0" y="23.0"/><svg:rect fill="#C9BFAB" fill-opacity="1.0" height="14" id="bucket2bg" title="21 or more days" width="14" x="5.0" y="41.0"/><svg:rect fill="#FFEDA0" fill-opacity="0.8" height="14" id="bucket2rect" title="21 or more days" width="14" x="5.0" y="41.0"/></svg:svg><xhtml:div class="polyLeg"><xhtml:div id="polyLegBucket0" title="1 to 10 days">1 to 10 days</xhtml:div><xhtml:div id="polyLegBucket1" title="11 to 20 days">11 to 20 days</xhtml:div><xhtml:div id="polyLegBucket2" title="21 or more days">21 or more days</xhtml:div></xhtml:div></xhtml:div>"""
                },
                "markupType":"svg",
                "searches":[
                    {
                        "request":{
                            "declaredClass":"ARE",
                            "featureName":"405",
                            "featureTypeName":"COMMUNITY_BOARD",
                            "searchType":"AreaSearch"
                        },
                        "searchType":"AreaSearch",
                        "declaredClass":"SEA",
                        "visible":True,
                        "id":"",
                        "title":""
                    }
                ],
                "tileCacheDescription":{
                    "declaredClass":"MID",
                    "height":2560,
                    "mapEnvelope":{
                        "declaredClass":"ME",
                        "height":777777.8,
                        "maxX":1322222.2,
                        "maxY":595555.6,
                        "minX":700000,
                        "minY":-182222.2,
                        "width":622222.2
                    },
                    "width":2048
                },
                "viewportDescription":{
                    "declaredClass":"ID",
                    "height":678,
                    "mapEnvelope":{
                        "declaredClass":"ME",
                        "height":205989.6,
                        "maxX":1104136.3,
                        "maxY":299487.3,
                        "minX":876271.7,
                        "minY":93497.7,
                        "width":227864.6
                    },
                    "offset":{
                        "declaredClass":"IP",
                        "x":-580,
                        "y":-974
                    },
                    "width":750
                },
                "visibleCompoundFeatureTypeNames":[],
                "zoomLevel":1,
                "applicationChanged":True,
                "cumulativeMapDrag":{
                    "declaredClass":"IP",
                    "y":0,
                    "x":0
                }
            },
            'themeName': 'COMMUNITY_BOARD',
        }
        data = dict([(k, simplejson.dumps(v)) for k, v in data.items()])
        data.update({'methodName': 'find'}) # This doesn't get JSON-escaped.

        # We don't actually do anything with the result. We just needed the
        # server to set a cookie.
        html = self.get_html('http://gis.nyc.gov/doitt/webmap/ClientQueryMapper', data)

        # Next, zoom the map out one level. As a result, the map will display
        # *every* SCOUT point in the city.
        data = {
            'clientQuery': '',
            'mapData': {
                "applicationName":"SCOUT",
                "cacheName":"basic",
                "declaredClass":"MD",
                "groupedClientData":{
                    "declaredClass":"POI",
                    "envelope":{
                        "declaredClass":"ME",
                        "height":148573,
                        "maxX":1067096,
                        "maxY":270929,
                        "minX":913357,
                        "minY":122356,
                        "width":153739
                    }
                },
                "legend":{
                    "declaredClass":"LEG",
                    "description":"Service Request Count by Location",
                    "markup":"""<?xml version="1.0" encoding="UTF-8"?><xhtml:div xmlns:svg="http://www.w3.org/2000/svg" xmlns:xhtml="http://www.w3.org/1999/xhtml" xmlns:xlink="http://www.w3.org/1999/xlink"><svg:svg height="120px" version="1.1" width="227px"><svg:circle cx="22.0" cy="10.0" fill="#FF9900" fill-opacity="1.0" id="bucket0" r="5" stroke="#000000" stroke-width="1" title="1 Request"/><svg:circle cx="22.0" cy="30.0" fill="#FF9900" fill-opacity="1.0" id="bucket1" r="10" stroke="#000000" stroke-width="1" title="2 to 3 Requests"/><svg:circle cx="22.0" cy="60.0" fill="#FF9900" fill-opacity="1.0" id="bucket2" r="15" stroke="#000000" stroke-width="1" title="4 Requests"/><svg:circle cx="22.0" cy="100.0" fill="#FF9900" fill-opacity="1.0" id="bucket3" r="20" stroke="#000000" stroke-width="1" title="5 Requests"/></svg:svg><xhtml:div class="ptLeg"><xhtml:div id="ptLegBucket0" title="1 Request">1 Request</xhtml:div><xhtml:div id="ptLegBucket1" title="2 to 3 Requests">2 to 3 Requests</xhtml:div><xhtml:div id="ptLegBucket2" title="4 Requests">4 Requests</xhtml:div><xhtml:div id="ptLegBucket3" title="5 Requests">5 Requests</xhtml:div></xhtml:div></xhtml:div>"""
                },
                "markupType":"svg",
                "searches":[{
                    "declaredClass":"SEA",
                    "id":"searched_COMMUNITY_BOARD.405",
                    "mapPoints":[[
                        {"declaredClass":"MP","x":1020138.6,"y":193489.6},
                        {"declaredClass":"MP","x":1020380.4,"y":193638.6},
                        {"declaredClass":"MP","x":1020499.7,"y":193766.7},
                        {"declaredClass":"MP","x":1020611.1,"y":193976.4},
                        {"declaredClass":"MP","x":1020752.8,"y":194634.2},
                        {"declaredClass":"MP","x":1020891.5,"y":194894.2},
                        {"declaredClass":"MP","x":1021126.3,"y":195114.5},
                        {"declaredClass":"MP","x":1021435.3,"y":195266.6},
                        {"declaredClass":"MP","x":1021969.7,"y":195365},
                        {"declaredClass":"MP","x":1022400.2,"y":195335.8},
                        {"declaredClass":"MP","x":1022932.7,"y":195221.9},
                        {"declaredClass":"MP","x":1023080.5,"y":195222.2},
                        {"declaredClass":"MP","x":1023301.2,"y":195265.5},
                        {"declaredClass":"MP","x":1023567,"y":195397.5},
                        {"declaredClass":"MP","x":1023364.6,"y":194817.6},
                        {"declaredClass":"MP","x":1023830.1,"y":194779},
                        {"declaredClass":"MP","x":1023853.9,"y":194947.3},
                        {"declaredClass":"MP","x":1024372.5,"y":195063.6},
                        {"declaredClass":"MP","x":1024373,"y":195283.9},
                        {"declaredClass":"MP","x":1024363,"y":195503.9},
                        {"declaredClass":"MP","x":1024287.4,"y":195696.6},
                        {"declaredClass":"MP","x":1023344.9,"y":196583.5},
                        {"declaredClass":"MP","x":1023253.1,"y":196811.6},
                        {"declaredClass":"MP","x":1023131,"y":197920.6},
                        {"declaredClass":"MP","x":1023164.4,"y":198133.2},
                        {"declaredClass":"MP","x":1023124.4,"y":198574.8},
                        {"declaredClass":"MP","x":1023185.6,"y":199216.4},
                        {"declaredClass":"MP","x":1023166,"y":199548.9},
                        {"declaredClass":"MP","x":1023050.1,"y":199776.4},
                        {"declaredClass":"MP","x":1022222.2,"y":200786.3},
                        {"declaredClass":"MP","x":1021605.5,"y":201612},
                        {"declaredClass":"MP","x":1020479.3,"y":203234.9},
                        {"declaredClass":"MP","x":1020106.7,"y":204189.9},
                        {"declaredClass":"MP","x":1019884.6,"y":204987.2},
                        {"declaredClass":"MP","x":1018857.7,"y":205510.4},
                        {"declaredClass":"MP","x":1018092.5,"y":205155.1},
                        {"declaredClass":"MP","x":1017866,"y":204937},
                        {"declaredClass":"MP","x":1017666.8,"y":204832.5},
                        {"declaredClass":"MP","x":1016544.3,"y":204742},
                        {"declaredClass":"MP","x":1015968.8,"y":204629.6},
                        {"declaredClass":"MP","x":1015765.4,"y":205474.7},
                        {"declaredClass":"MP","x":1015372.1,"y":206818.4},
                        {"declaredClass":"MP","x":1015144.5,"y":206905.5},
                        {"declaredClass":"MP","x":1014983.6,"y":206927.6},
                        {"declaredClass":"MP","x":1013963.5,"y":206891.2},
                        {"declaredClass":"MP","x":1013707.1,"y":206949.7},
                        {"declaredClass":"MP","x":1013370.2,"y":207076.9},
                        {"declaredClass":"MP","x":1012432.4,"y":207143.8},
                        {"declaredClass":"MP","x":1012177,"y":207113.4},
                        {"declaredClass":"MP","x":1011921.1,"y":206998.3},
                        {"declaredClass":"MP","x":1011789.4,"y":206876.7},
                        {"declaredClass":"MP","x":1010530.2,"y":204869.1},
                        {"declaredClass":"MP","x":1010047.4,"y":204367.3},
                        {"declaredClass":"MP","x":1008483.7,"y":202784.1},
                        {"declaredClass":"MP","x":1008446,"y":202800.4},
                        {"declaredClass":"MP","x":1007469.6,"y":202399.7},
                        {"declaredClass":"MP","x":1007380.1,"y":202333.4},
                        {"declaredClass":"MP","x":1006406.3,"y":202601.2},
                        {"declaredClass":"MP","x":1006280.6,"y":202687.2},
                        {"declaredClass":"MP","x":1006382.3,"y":203060.5},
                        {"declaredClass":"MP","x":1005805,"y":203433.2},
                        {"declaredClass":"MP","x":1004612.1,"y":203081.3},
                        {"declaredClass":"MP","x":1004944,"y":202197.3},
                        {"declaredClass":"MP","x":1005267.1,"y":201608.1},
                        {"declaredClass":"MP","x":1005240.4,"y":201067.4},
                        {"declaredClass":"MP","x":1005583.3,"y":200504.6},
                        {"declaredClass":"MP","x":1005218.3,"y":199986.9},
                        {"declaredClass":"MP","x":1005307.8,"y":199417.9},
                        {"declaredClass":"MP","x":1005809.1,"y":198997.9},
                        {"declaredClass":"MP","x":1005960.9,"y":198648.3},
                        {"declaredClass":"MP","x":1006000.2,"y":198338.2},
                        {"declaredClass":"MP","x":1006222.4,"y":198151.3},
                        {"declaredClass":"MP","x":1005904.8,"y":197738},
                        {"declaredClass":"MP","x":1008702.9,"y":195568.9},
                        {"declaredClass":"MP","x":1008399.2,"y":195177.6},
                        {"declaredClass":"MP","x":1009016.7,"y":194698.8},
                        {"declaredClass":"MP","x":1008703.9,"y":194294.8},
                        {"declaredClass":"MP","x":1010798.5,"y":192753.1},
                        {"declaredClass":"MP","x":1010373.3,"y":192179.5},
                        {"declaredClass":"MP","x":1011406.6,"y":191405.4},
                        {"declaredClass":"MP","x":1011508.6,"y":191282},
                        {"declaredClass":"MP","x":1011637.5,"y":191201.3},
                        {"declaredClass":"MP","x":1011455,"y":191069.6},
                        {"declaredClass":"MP","x":1011865.2,"y":190015.8},
                        {"declaredClass":"MP","x":1011661,"y":189904.1},
                        {"declaredClass":"MP","x":1012956.9,"y":187926.9},
                        {"declaredClass":"MP","x":1013388.4,"y":188598.9},
                        {"declaredClass":"MP","x":1013639.1,"y":188908.6},
                        {"declaredClass":"MP","x":1014058.7,"y":188284.5},
                        {"declaredClass":"MP","x":1014297.8,"y":188510.5},
                        {"declaredClass":"MP","x":1014735.2,"y":188754.5},
                        {"declaredClass":"MP","x":1014861.1,"y":188581},
                        {"declaredClass":"MP","x":1015289.2,"y":188967},
                        {"declaredClass":"MP","x":1015415.9,"y":189225.3},
                        {"declaredClass":"MP","x":1015959.2,"y":189541.9},
                        {"declaredClass":"MP","x":1016482.3,"y":189904.8},
                        {"declaredClass":"MP","x":1017665,"y":191102.6},
                        {"declaredClass":"MP","x":1019184.7,"y":192213.9},
                        {"declaredClass":"MP","x":1020599.4,"y":192565.5},
                        {"declaredClass":"MP","x":1020138.6,"y":193489.6}
                    ]],
                    "request":{
                        "declaredClass":"ARE",
                        "featureName":"405",
                        "featureTypeName":"COMMUNITY_BOARD"
                    },
                    "searchType":"AreaSearch",
                    "title":"Community Board: 5 QUEENS",
                    "visible":True
                }],
                "tileCacheDescription":{
                    "declaredClass":"MID",
                    "height":2560,
                    "mapEnvelope":{
                        "declaredClass":"ME",
                        "height":142222.2,
                        "maxX":1069777.8,
                        "maxY":269333.3,
                        "minX":956000,
                        "minY":127111.1,
                        "width":113777.8
                    },
                    "width":2048
                },
                "viewportDescription":{
                    "declaredClass":"ID",
                    "height":678,
                    "mapEnvelope":{
                        "declaredClass":"ME",
                        "height":37666.7,
                        "maxX":1035325.9,
                        "maxY":216368.7,
                        "minX":993659.2,
                        "minY":178702,
                        "width":41666.7
                    },
                    "offset":{
                        "declaredClass":"IP",
                        "x":-678,
                        "y":-953
                    },
                    "width":750
                },
                "visibleCompoundFeatureTypeNames":[],
                "zoomLevel":4,
                "cumulativeMapDrag":{
                    "declaredClass":"IP",
                    "y":0,
                    "x":0
                }
            },
            'themeName': 'COMMUNITY_BOARD',
        }
        data = dict([(k, simplejson.dumps(v)) for k, v in data.items()])
        data.update({'methodName': 'zoomToLevel', 'zoomLevel': '3'}) # This doesn't get JSON-escaped.
        html = self.get_html('http://gis.nyc.gov/doitt/webmap/ClientQueryMapper', data)

        # The result is a JSON string whose 'markup' value is XML that contains
        # a <circle> for every circle on the map. A circle has one or more
        # actual items on it, and the only way to get the detail record is by
        # doing another query for each circle.
        data = parse_json(html)
        xml_string = data['markup'].replace('<\\/', '</').encode('utf8')
        xml = etree.fromstring(xml_string)
        circle_list = etree.ETXPath('//{http://www.w3.org/2000/svg}circle')(xml)
        num_circles = len(circle_list)
        self.logger.debug('Found %s circles', num_circles)

        for i, circle in enumerate(circle_list):
            x, y = circle.attrib['id'].split('@')
            seconds_remaining = self.sleep * (num_circles - i - 1)
            self.logger.debug('Getting circle %s of %s. Time remaining: %s minutes', i, num_circles, (seconds_remaining // 60))
            data = {
                'applicationName': 'SCOUT',
                'caller': '_idClientData',
                'callerWindow': '',
                'clientQuery': '',
                'cumulativeMapDrag': {
                    "preamble": None,
                    "declaredClass": "gov.nyc.doitt.gis.geometry.domain.ImagePoint",
                    "y": 0,
                    "x": 0,
                },
                'featureId': '"%s"' % circle.attrib['id'],
                'featureTypeName': 'locations',
                'mapData': {
                    u'applicationChanged': False,
                    u'applicationName': u'SCOUT',
                    u'cacheName': u'basic',
                    u'clientDataStore': {
                        u'declaredClass': u'gov.nyc.doitt.gis.webmap.domain.ClientDataStore',
                        u'identifier': u'C0',
                        u'thematicPolygon': False,
                        u'xCoordAlias': u'C1',
                        u'yCoordAlias': u'C2'
                    },
                    u'cumulativeMapDrag': {
                        u'declaredClass': u'gov.nyc.doitt.gis.geometry.domain.ImagePoint',
                        u'preamble': None,
                        u'x': 0,
                        u'y': 0
                    },
                    u'declaredClass': u'gov.nyc.doitt.gis.service.webmap.domain.MapData',
                    u'filterField': u'',
                    u'filterValue': u'',
                    u'groupedClientData': {
                        u'declaredClass': u'gov.nyc.doitt.gis.service.webmap.domain.PointGroupedClientData',
                        u'envelope': {
                            u'declaredClass': u'gov.nyc.doitt.gis.geometry.domain.MapEnvelope',
                            u'height': 146449,
                            u'maxX': 1066580,
                            u'maxY': 270798,
                            u'minX': 915309,
                            u'minY': 124349,
                            u'width': 151271
                        }
                    },
                    u'legend': {
                        u'declaredClass': u'gov.nyc.doitt.gis.service.webmap.domain.Legend',
                        u'description': u'Service Request Count by Location',
                        u'markup': u'<?xml version="1.0" encoding="UTF-8"?><xhtml:div xmlns:xhtml="http://www.w3.org/1999/xhtml"><svg:svg xmlns:svg="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" height="120px" version="1.1" width="172px"><svg:circle cx="22.0" cy="10.0" fill="#FF9900" fill-opacity="1.0" id="bucket0" r="5" stroke="#000000" stroke-width="1" title="1 Request" xmlns:svg="http://www.w3.org/2000/svg"/><svg:circle cx="22.0" cy="30.0" fill="#FF9900" fill-opacity="1.0" id="bucket1" r="10" stroke="#000000" stroke-width="1" title="2 Requests" xmlns:svg="http://www.w3.org/2000/svg"/><svg:circle cx="22.0" cy="60.0" fill="#FF9900" fill-opacity="1.0" id="bucket2" r="15" stroke="#000000" stroke-width="1" title="3 Requests" xmlns:svg="http://www.w3.org/2000/svg"/><svg:circle cx="22.0" cy="100.0" fill="#FF9900" fill-opacity="1.0" id="bucket3" r="20" stroke="#000000" stroke-width="1" title="4 Requests" xmlns:svg="http://www.w3.org/2000/svg"/></svg:svg><xhtml:div class="ptLeg" xmlns:xhtml="http://www.w3.org/1999/xhtml"><xhtml:div id="ptLegBucket0" title="1 Request">1 Request</xhtml:div><xhtml:div id="ptLegBucket1" title="2 Requests">2 Requests</xhtml:div><xhtml:div id="ptLegBucket2" title="3 Requests">3 Requests</xhtml:div><xhtml:div id="ptLegBucket3" title="4 Requests">4 Requests</xhtml:div></xhtml:div></xhtml:div>'
                    },
                    u'markup': None,
                    u'markupType': u'svg',
                    u'pointThemeDisabled': False,
                    u'responseStatus': {
                        u'code': 0,
                        u'declaredClass': u'gov.nyc.doitt.gis.service.webmap.domain.ResponseStatus'
                    },
                    u'searches': [
                        {
                            u'declaredClass': u'gov.nyc.doitt.gis.service.webmap.domain.Search',
                            u'found': False,
                            u'id': u'searched_COMMUNITY_BOARD.405',
                            u'mapPoints': [],
                            u'request': {
                                u'declaredClass': u'gov.nyc.doitt.gis.service.webmap.domain.AreaRequest',
                                u'featureName': u'405',
                                u'featureTypeName': u'COMMUNITY_BOARD'
                            },
                            u'searchType': u'AreaSearch',
                            u'title': u'Community Board: 5 QUEENS',
                            u'visible': True
                        }
                    ],
                    u'tableOfContents': {
                        u'declaredClass': u'gov.nyc.doitt.gis.webmap.domain.TableOfContents',
                        u'featureTypeGroups': [],
                        u'name': u'SCOUT',
                        u'title': u'Scout Table of Contents'
                    },
                    u'viewportDescription': {
                        u'declaredClass': u'gov.nyc.doitt.gis.service.webmap.domain.ImageDescription',
                        u'height': 678,
                        u'mapEnvelope': {
                            u'declaredClass': u'gov.nyc.doitt.gis.geometry.domain.MapEnvelope',
                            u'height': 75333.333333333256,
                            u'maxX': 1056159.1666666665,
                            u'maxY': 235202.16666666663,
                            u'minX': 972825.83333333337,
                            u'minY': 159868.83333333337,
                            u'width': 83333.333333333139
                        },
                        u'offset': {
                            u'declaredClass': u'gov.nyc.doitt.gis.geometry.domain.ImagePoint',
                            u'x': -919,
                            u'y': -819
                        },
                        u'width': 750
                    },
                    u'zoomLevel': 3
                },
                'point': {
                    "x": 311,
                    "y": 435,
                    "preamble": None,
                    "declaredClass": "gov.nyc.doitt.gis.geometry.domain.ImagePoint"
                },
                'themeName': '"COMMUNITY_BOARD"',
            }
            data.update({'methodName': 'identify', 'x': x.split('.')[0], 'y': y.split('.')[0]}) # This doesn't get JSON-escaped.
            yield self.get_html('http://gis.nyc.gov/doitt/webmap/Identify', data)

    def parse_list(self, html):
        detail_data = parse_json(html)
        aliases = (
            ('C0', 'identifier'),
            ('C1', 'x_coord'),
            ('C2', 'y_coord'),
            ('C3', 'complaint_type'),
            ('C4', 'created_time'),
            ('C5', 'created_date'),
            ('C6', 'status'),
            ('C7', 'community_board'),
            ('C8', 'agency_name'),
            ('C9', 'agency_abbr'),
            ('C10', 'description'),
            ('C11', 'sr_id'),
        )
        for item in detail_data['data']['items']:
            for k, v in aliases:
                item[v] = item.pop(k)
            yield item

    def clean_list_record(self, record):
        created_datetime = parse_date(record['created_time'], '%m/%d/%Y %H:%M', return_datetime=True)
        record['created_date'] = created_datetime.date()
        record['created_time'] = created_datetime.time()

        record['x_coord'] = float(record['x_coord'])
        record['y_coord'] = float(record['y_coord'])

        return record

    def existing_record(self, record):
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id)
            qs = qs.by_attribute(self.schema_fields['sr_id'], record['sr_id'])
            return qs[0]
        except IndexError:
            return None

    def save(self, old_record, list_record, detail_record):
        agency = self.get_or_create_lookup('agency', list_record['agency_name'], list_record['agency_name'])
        complaint_type = self.get_or_create_lookup('complaint_type', list_record['complaint_type'], list_record['complaint_type'])
        description = self.get_or_create_lookup('description', list_record['description'], list_record['description'])
        status = self.get_or_create_lookup('status', list_record['status'], list_record['status'])
        title = '%s: %s' % (complaint_type.name, description.name)
        location_name = 'See map for location'
        new_attributes = {
            'agency': agency.id,
            'complaint_type': complaint_type.id,
            'description': description.id,
            'status': status.id,
            'created_time': list_record['created_time'],
            'sr_id': list_record['sr_id'],
            'x_coord': list_record['x_coord'],
            'y_coord': list_record['y_coord'],
        }
        if old_record is None:
            self.create_newsitem(
                new_attributes,
                title=title,
                item_date=list_record['created_date'],
                location=Point(list_record['x_coord'], list_record['y_coord'], srid=2263),
                location_name=location_name,
            )
        else:
            new_values = {'title': title, 'item_date': list_record['created_date'], 'location_name': location_name}
            self.update_existing(old_record, new_values, new_attributes)

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    ScoutScraper().update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Screen scraper for NYC Dept. of Buildings sign permits.

The data comes in weekly Excel reports from the "Sign Monthly Statistical
Reports" section of this page:
http://www.nyc.gov/html/dob/html/guides/weekly.shtml
"""

from ebdata.parsing.excel import ExcelDictReader
from ebdata.retrieval.scrapers.list_detail import SkipRecord
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from ebpub.utils.text import smart_title
import datetime
import os

JOB_STATUS_HEADLINE_VERBS = {
    'A': 'pre-filed',
    'B': 'processed',
    'C': 'processed',
    'D': 'processed',
    'E': 'processed',
    'F': 'assigned for DOB exam',
    'G': 'changed plans',
    'H': 'initiated for exam',
    'J': 'disapproved by DOB',
    'K': 'partially approved',
    'L': 'waiting for change fee assessment',
    'M': 'approved for changed plans',
    'P': 'approved by DOB exam',
    'Q': 'approved for a partial permit',
    'R': 'approved',
    'U': 'completed',
    'X': 'signed off',
}

JOB_STATUS_NAMES = {
    'A': 'Pre-filed',
    'B': 'Processed without payment',
    'C': 'Processed with payment only',
    'D': 'Processed',
    'E': 'Processed without a request for an exam by the Department of Buildings, and the applicant will professionally certify the plans',
    'F': 'Assigned for exam by the DOB',
    'G': 'Changed after the DOB approved plans and requires a change fee',
    'H': 'Initiated for exam',
    'J': 'Disapproved after a DOB exam',
    'K': 'Partially approved',
    'L': 'Waiting for change fee assessment after change of plans',
    'M': 'Approved for change of plans after change fee accepted',
    'P': 'Approved by DOB exam',
    'Q': 'Approved for a partial permit',
    'R': 'Approved',
    'U': 'Completed',
    'X': 'Signed off',
}

class SignScraper(NewsItemListDetailScraper):
    schema_slugs = ('sign-permits',)
    has_detail = False

    def __init__(self, week_end_dates=None):
        """
        week_end_dates is a list of datetime objects representing the week to
        download. If it's not provided, this will use the last three weeks.
        """
        super(SignScraper, self).__init__(use_cache=False)
        if week_end_dates is None:
            # Use the last three Saturdays.
            end_date = datetime.date.today()
            while 1:
                end_date -= datetime.timedelta(days=1)
                if end_date.weekday() == 5:
                    break
            week_end_dates = [end_date - datetime.timedelta(days=7), end_date]
        self.week_end_dates = week_end_dates

    def list_pages(self):
        for week_end_date in self.week_end_dates:
            # They've used a different URL scheme over time.
            if week_end_date <= datetime.date(2005, 5, 13):
                url = 'http://www.nyc.gov/html/dob/downloads/download/foil/sg%s.xls' % week_end_date.strftime('%m%d%y')
            else:
                url = 'http://www.nyc.gov/html/dob/downloads/excel/sg%s.xls' % week_end_date.strftime('%m%d%y')

            workbook_path = self.retriever.get_to_file(url)
            yield ExcelDictReader(workbook_path, sheet_index=0, header_row_num=2, start_row_num=3,
                use_last_header_if_duplicate=False)
            os.unlink(workbook_path) # Clean up the temporary file.

    def parse_list(self, reader):
        for row in reader:
            yield row

    def clean_list_record(self, record):
        record['address'] = '%s %s, %s' % (record['House #'], smart_title(record['Street Name']), smart_title(record['Borough']))
        record['is_landmark'] = record['Landmark'] == 'Y'
        record['is_adult_establishment'] = record['Adult Estab'] == 'Y'
        record['is_city_owned'] = record['City Owned'] == 'Y'
        record['illumination_type'] = record.get('Sign Illumination Type', 'Not available') or 'Not illuminated'
        if not isinstance(record['Latest Action Date'], datetime.datetime):
            self.logger.info('Skipping job #%s, with latest action date %s', record.get('Job #'), record['Latest Action Date'])
            raise SkipRecord()

        try:
            record['sign_text'] = record['Text on Sign'].strip()
            if len(record['sign_text']) > 255:
                # Some records are malformed and have a bad and long value
                # for sign text.
                self.logger.info('Skipping job #%s, with Text on Sign %s', record.get('Job #'), record['Text on Sign'])
                raise SkipRecord()
        except AttributeError:
            try:
                record['sign_text'] = str(int(record['Text on Sign']))
            except TypeError:
                self.logger.info('Skipping job #%s, with Text on Sign %s', record.get('Job #'), record['Text on Sign'])
                raise SkipRecord()

        try:
            record['sign_for'] = record['Sign Advertising']
        except KeyError:
            record['sign_for'] = record['Usage']

        try:
            record['is_near_highway'] = record['Sign Near Highway'] == 'Y'
        except KeyError:
            # Older spreadsheets don't have a 'Sign Near Highway' column,
            # and there's nothing we can do about it. They have a column called
            # 'Adjacent to Arterial Highway', but that's not necessarily the
            # same thing.
            record['is_near_highway'] = None

        try:
            record['is_changeable_copy'] = record['Sign Changeable Copy'] == 'Y'
        except KeyError:
            # Older spreadsheets don't have a 'Sign Changeable Copy' column,
            # but we can deduce the value: if there's text, then it's not
            # changeable. Otherwise, it's NULL.
            if record['sign_text']:
                record['is_changeable_copy'] = False
            else:
                record['is_changeable_copy'] = None

        return record

    def existing_record(self, record):
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id)
            qs = qs.by_attribute(self.schema_fields['job_number'], record['Job #'])
            return qs[0]
        except IndexError:
            return None

    def save(self, old_record, list_record, detail_record):
        if old_record is not None:
            return

        job_status = self.get_or_create_lookup('job_status', JOB_STATUS_NAMES[list_record['Job Status']], list_record['Job Status'])
        illumination_type = self.get_or_create_lookup('illumination_type', list_record['illumination_type'], list_record['illumination_type'])
        sign_location = self.get_or_create_lookup('sign_location', smart_title(list_record['Sign Type']), list_record['Sign Type'])
        sign_for = self.get_or_create_lookup('sign_for', smart_title(list_record['sign_for']), list_record['sign_for'])
        title = 'Permit application %s for an %s %s sign' % \
            (JOB_STATUS_HEADLINE_VERBS[list_record['Job Status']],
            (list_record['Sign Illumination'] == 'Y' and 'illuminated' or 'unilluminated'),
            (list_record['sign_for'] == 'BUSINESS' and 'business' or 'advertising'))
        attributes = {
            'bin': list_record['Bin #'],
            'job_number': list_record['Job #'],
            'job_status': job_status.id,
            'is_landmark': list_record['is_landmark'],
            'is_adult_establishment': list_record['is_adult_establishment'],
            'is_city_owned': list_record['is_city_owned'],
            'is_changeable_copy': list_record['is_changeable_copy'],
            'estimated_cost': list_record['Initial Cost'],
            'illumination_type': illumination_type.id,
            'sign_location': sign_location.id,
            'size': list_record['Sign SQ Footage'],
            'sign_for': sign_for.id,
            'sign_text': list_record['sign_text'],
            'job_description': list_record['Job Description 1'],
        }
        self.create_newsitem(
            attributes,
            title=title,
            pub_date=list_record['Latest Action Date'],
            item_date=list_record['Latest Action Date'],
            location_name=list_record['address'],
        )

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    SignScraper().update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Scrapers for city council items

http://legislation.phila.gov/calendar/
"""

from ebdata.blobs.models import Seed, Page
from ebdata.parsing.pdftotext import pdf_to_text
from ebdata.retrieval.scrapers.base import BaseScraper, ScraperBroken
from ebpub.utils.dates import parse_date
from lxml.html import document_fromstring
import datetime
import re
import os

ROOT_URI = 'http://legislation.phila.gov/calendar/search.aspx'

class CityCouncilCalendarScraper(BaseScraper):
    def get_viewstate(self, uri):
        html = self.get_html(uri)
        m = re.search(r'<input type="hidden" name="__VIEWSTATE" value="([^"]*)"', html)
        if not m:
            raise ScraperBroken('VIEWSTATE not found')
        return m.group(1)

    def search_args(self):
        viewstate = self.get_viewstate(ROOT_URI)
        return {
            '__EVENTARGUMENT': '',
            '__EVENTTARGET': 'cboBody',
            '__VIEWSTATE': viewstate,
            'cboBody': self.cboBody,
            'cboYear': str(datetime.date.today().year),
            'txtSearchTerm': ''
        }

    def get_pages(self):
        html = self.retriever.get_html(ROOT_URI, self.search_args())
        t = document_fromstring(html)
        for a in t.xpath("//table[@id='grdCalendar']//a"):
            url = a.get('href')
            title = a.text or ''
            if url is None or self.already_downloaded(url):
                continue
            pdf_path = self.retriever.get_to_file(url)
            m = re.search(r'(\d{2}-\d{2}-\d{2})', url)
            date = parse_date(m.group(1), '%y-%m-%d')
            yield {
                'url': url,
                'data': self.parse_pdf(pdf_path),
                'title': title,
                'date': date
            }
            os.unlink(pdf_path) # Clean up the temporary file.

    def parse_pdf(self, pdf_path):
        return pdf_to_text(pdf_path, keep_layout=True, raw=False).decode('Latin-1')

    def already_downloaded(self, url):
        try:
            pages = Page.objects.filter(url=url)[0]
            return True
        except IndexError:
            return False

    def save(self, page):
        if not hasattr(self, 'seed'):
            self.seed = Seed.objects.get(schema__slug__exact=self.schema_name)
        return Page.objects.create(
            seed=self.seed,
            url=page['url'],
            scraped_url=page['url'],
            html=page['data'],
            when_crawled=datetime.datetime.now(),
            is_article=True,
            is_pdf=True,
            is_printer_friendly=False,
            article_headline=self.get_headline(page),
            article_date=page['date'],
            has_addresses=None,
            when_geocoded=None,
            geocoded_by='',
            times_skipped=0,
            robot_report='',
        )

    def update(self):
        for page in self.get_pages():
            self.save(page)

class StreetsAndServicesScraper(CityCouncilCalendarScraper):
    cboBody = '41' # an identifier for streets and services
    schema_name = 'streets-and-services'

    def get_headline(self, page):
        return "City Council Streets and Services committee meeting agenda item, %s" % page['date'].strftime('%B %d, %Y')

class PublicPropertyAndPublicWorksScraper(CityCouncilCalendarScraper):
    cboBody= '42' # an identifier for public propert and public works
    schema_name = 'public-works'

    def get_headline(self, page):
        return "City Council Public Works committee meeting agenda item, %s" % page['date'].strftime('%B %d, %Y')

class ZoningScraper(CityCouncilCalendarScraper):
    cboBody= '53'
    schema_name = 'rules'

    def get_headline(self, page):
        return "City Council committee on Rules meeting agenda item, %s" % page['date'].strftime('%B %d, %Y')

def update():
    StreetsAndServicesScraper().update()
    PublicPropertyAndPublicWorksScraper().update()
    ZoningScraper().update()

if __name__ == '__main__':
    from ebdata.retrieval import log_debug
    update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Philly city press release scraper.

http://ework.phila.gov/philagov/news/address.asp
"""

from ebdata.blobs.scrapers import IncrementalCrawler
import re

class PhillyCityPressReleaseCrawler(IncrementalCrawler):
    schema = 'city-press-releases'
    seed_url = 'http://ework.phila.gov/'
    date_headline_re = re.compile(r'(?si)<td valign="top" width="150" class="reldate">(?P<article_date>\d\d?/\d\d?/\d\d\d\d)</td></tr><tr><td valign="top" class="title">(?P<article_headline>.*?)</td>')
    date_format = '%m/%d/%Y'
    max_blanks = 3

    def public_url(self, id_value):
        return 'http://ework.phila.gov/philagov/news/prelease.asp?id=%s' % id_value

    def id_for_url(self, url):
        return url.split('id=')[1]

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    PhillyCityPressReleaseCrawler().update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Screen scraper for Philadelphia crime data.
http://citymaps.phila.gov/CrimeMap/
"""

from django.contrib.gis.gdal import OGRGeometry
from django.contrib.gis.gdal.srs import SpatialReference
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date
from ebpub.utils.text import smart_title
from cStringIO import StringIO
import csv
import datetime
import time

# The SRS of the crime data, Pennsylvania State Plane South, isn't one
# that is part of the set of SRSes that come with PostGIS, so we
# define it here
SRS_WKT = """\
PROJCS["NAD_1983_StatePlane_Pennsylvania_South_FIPS_3702_Feet",
    GEOGCS["GCS_North_American_1983",
        DATUM["North_American_Datum_1983",
            SPHEROID["GRS_1980",6378137,298.257222101]],
        PRIMEM["Greenwich",0],
        UNIT["Degree",0.017453292519943295]],
    PROJECTION["Lambert_Conformal_Conic_2SP"],
    PARAMETER["False_Easting",1968500],
    PARAMETER["False_Northing",0],
    PARAMETER["Central_Meridian",-77.75],
    PARAMETER["Standard_Parallel_1",39.93333333333333],
    PARAMETER["Standard_Parallel_2",40.96666666666667],
    PARAMETER["Latitude_Of_Origin",39.33333333333334],
    UNIT["Foot_US",0.30480060960121924],
    AUTHORITY["EPSG","102729"]]"""
srs = SpatialReference(SRS_WKT)

class CrimeScraper(NewsItemListDetailScraper):
    schema_slugs = ('crime',)
    has_detail = False

    def __init__(self, start_date=None, end_date=None):
        super(CrimeScraper, self).__init__()
        self.start_date, self.end_date = start_date, end_date

    def list_pages(self):
        # Yields CSV documents
        for crime_type in (10, 11, 12, 13, 20, 30, 31, 32, 40, 41, 42, 50, 51, 52, 53, 54, 60, 61, 62, 63, 64, 65, 66, 67, 70, 71, 72):
            url = 'http://citymaps.phila.gov/CrimeMap/ExportCSV.aspx?crimetype=%s&from=%s&to=%s&bounds=2468450%%2C158500%%2C2941550%%2C348500' % \
                (crime_type, self.start_date.strftime('%m/%d/%Y'), self.end_date.strftime('%m/%d/%Y'))
            yield self.get_html(url)
            time.sleep(10) # Be nice to their servers.

    def parse_list(self, text):
        reader = csv.DictReader(StringIO(text))
        for row in reader:
            yield row

    def clean_list_record(self, record):
        dispatch_datetime = parse_date(record['DISPATCH_DATE_TIME'], '%m/%d/%Y %I:%M:%S %p', return_datetime=True)
        record['dispatch_date'] = dispatch_datetime.date()
        record['dispatch_time'] = dispatch_datetime.time()
        record['LOCATION'] = smart_title(record['LOCATION'])
        # Convert '531 - Burglary: Day; No Force: Prvt. Residence' to 'Burglary'.
        record['primary_type'] = record['UCR_TEXT'].split(':')[0].split(' - ')[1].title()

        # Clean up an inconsistency.
        if record['primary_type'] == 'Auto Theft':
            record['primary_type'] = 'Vehicle Theft'

        record['X_COORD'] = float(record['X_COORD'])
        record['Y_COORD'] = float(record['Y_COORD'])
        return record

    def existing_record(self, record):
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id)
            qs = qs.by_attribute(self.schema_fields['dc_key'], record['DC_KEY'])
            return qs[0]
        except IndexError:
            return None

    def get_geom(self, list_record):
        ogrgeom = OGRGeometry('POINT(%s %s)' % (list_record['X_COORD'], list_record['Y_COORD']), srs)
        ogrgeom.transform(4326)
        return ogrgeom.geos

    def save(self, old_record, list_record, detail_record):
        premise = self.get_or_create_lookup('premise', list_record['PREMISE_TEXT'], list_record['PREMISE_TEXT'])
        sector = self.get_or_create_lookup('sector', list_record['SECTOR'], list_record['SECTOR'])
        primary_type = self.get_or_create_lookup('primary_type', list_record['primary_type'], list_record['primary_type'], make_text_slug=True)
        secondary_type = self.get_or_create_lookup('secondary_type', list_record['UCR_TEXT'], list_record['UCR_TEXT'], make_text_slug=False)
        attributes = {
            'dc_key': list_record['DC_KEY'],
            'premise': premise.id,
            'sector': sector.id,
            'primary_type': primary_type.id,
            'secondary_type': secondary_type.id,
            'xy': '%s;%s' % (list_record['X_COORD'], list_record['Y_COORD']),
            'dispatch_time': list_record['dispatch_time'],
        }
        if old_record is None:
            self.create_newsitem(
                attributes,
                title=secondary_type.name,
                item_date=list_record['dispatch_date'],
                location=self.get_geom(list_record),
                location_name=list_record['LOCATION'],
            )
        else:
            new_values = {'title': secondary_type.name, 'item_date': list_record['dispatch_date'], 'location_name': list_record['LOCATION']}
            self.update_existing(old_record, new_values, attributes)

def update_latest():
    # Do three 14-day chunks, instead of a single 50-day chunk, because the
    # police site has a limit of 500 records per result. We don't want to risk
    # not getting all of the data for a given date range.
    today = datetime.date.today()
    CrimeScraper(today - datetime.timedelta(days=14), today).update()
    CrimeScraper(today - datetime.timedelta(days=30), today - datetime.timedelta(days=15)).update()
    CrimeScraper(today - datetime.timedelta(days=46), today - datetime.timedelta(days=31)).update()

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    update_latest()

########NEW FILE########
__FILENAME__ = retrieval
"""
Scraper for Philly historical images.

http://www.phillyhistory.org/PhotoArchive/
"""

from django.contrib.gis.geos import Point
from django.utils import simplejson
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from urllib import urlencode, unquote, urlopen
from Cookie import SimpleCookie
import datetime
import re

philly_proj = "+proj=lcc +lat_1=39.93333333333333 +lat_2=40.96666666666667 +lat_0=39.33333333333334 +lon_0=-77.75 +x_0=600000.0000000001 +y_0=0 +ellps=GRS80 +datum=NAD83 +to_meter=0.3048006096012192 +no_defs"

# httplib2, urllib2, and httplib won't work because of a bug in httplib. Or
# rather, because of a bug in IIS that httplib doesn't handle.
# http://bugs.python.org/issue2645

def repair_json(json):
    # For some reason some colons, semicolons, periods, and ampersands are
    # escped with a backslash. Fix that.
    json = json.replace(r'\:', r':')
    json = json.replace(r'\;', r';')
    json = json.replace(r'\.', r'.')
    json = json.replace(r'\&', r'&amp;')
    return json

class HistoricalImagesScraper(NewsItemListDetailScraper):
    list_uri = 'http://www.phillyhistory.org/PhotoArchive/Thumbnails.ashx'
    detail_uri = 'http://www.phillyhistory.org/PhotoArchive/Details.ashx'
    sleep = 1
    schema_slugs = ('historical-images',)

    def __init__(self, *args, **kwargs):
        self.limit = kwargs.pop('limit', 2000)
        super(HistoricalImagesScraper, self).__init__(*args, **kwargs)

    def get_json(self, uri, data):
        if not hasattr(self, 'cookies'):
            self.cookies = SimpleCookie()
        json = urlopen(uri, urlencode(data)).read()
        json = repair_json(json)
        if json == '':
            return None
        return simplejson.loads(json)

    def list_pages(self):
        batch_size = 10
        if self.limit:
            image_limit = self.limit
        else:
            # Start out with something huge. This will be set to the actual
            # number during the first iteration of the loop.
            image_limit = 10000000
        start = 0
        while start + batch_size < image_limit:
            data = self.get_json(self.list_uri, {
                'start': start,
                'limit': batch_size,
                'urlqs': 'type=area&updateDays=30&sortOrder=UpdatedDateDesc&minx=2648000&miny=187000&maxx=2762000&maxy=320000',
                'request': 'Images',
            })
            if data is None:
                continue
            if self.limit is None:
                image_limit = data['totalImages']
            print "Retrieved %s - %s of %s" % (start, start + batch_size, image_limit)
            start += batch_size
            yield data

    def parse_list(self, page):
        return page['images']

    def detail_required(self, list_record, old_record):
        return True

    def get_detail(self, list_record):
        return self.get_json(self.detail_uri, {'assetId': list_record['assetId']})

    def parse_detail(self, page, list_record):
        return page['assets'][0]

    def clean_list_record(self, record):
        # MediaStream.ashx?mediaId=
        record['mediaId'] = str(record['url'][25:])
        record['assetId'] = str(record['assetId'])
        return record

    def clean_detail_record(self, record):
        record['assetId'] = str(record['assetId'])
        record['address'] = unquote(record['address'])
        m = re.search('(\d{4})', record.get('date', ''))
        if m is not None:
            # Turn 1926 into 1920s
            record['decade'] = m.group(1)[:3] + "0s"
        else:
            record['decade'] = 'Unknown'
        return record

    def existing_record(self, record):
        qs = NewsItem.objects.filter(schema__id=self.schema.id)
        qs = qs.by_attribute(self.schema_fields['asset_id'], record['assetId'])
        try:
            return qs[0]
        except IndexError:
            return None

    def save(self, old_record, list_record, detail_record):
        # Skip images that we already have in the database.
        if old_record is not None:
            return
        decade = self.get_or_create_lookup('decade', detail_record['decade'], detail_record['decade'])
        attrs = {
            'media_id': list_record['mediaId'],
            'asset_id': detail_record['assetId'],
            'decade': decade.id,
        }
        x, y = list_record['loc'].split('?')

        self.create_newsitem(
            attrs,
            title=detail_record['title'].strip(),
            item_date=datetime.date.today(),
            location=Point(float(x), float(y), srid=philly_proj),
            location_name=detail_record['address'],
        )

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    HistoricalImagesScraper().update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Screen scraper for Philadelphia restaurant inspections.

http://www.phila.gov/health/units/ehs/Restaurant_Inspectio.html

The data is in PDFs, with a PDF for each section of Philly (7 total).
"""

from django.core.serializers.json import DjangoJSONEncoder
from ebdata.parsing.pdftotext import pdf_to_text
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date
import md5
import re

class RestaurantScraper(NewsItemListDetailScraper):
    schema_slugs = ('restaurant-inspections',)
    has_detail = False

    def __init__(self, pdf_filename):
        super(RestaurantScraper, self).__init__()
        self.pdf_filename = pdf_filename

    def list_pages(self):
        yield pdf_to_text(self.pdf_filename, keep_layout=True, raw=False).split('\n')

    def parse_list(self, text):
        inspection_date = violations = address = city = restaurant_name = restaurant_type = None
        last_line_was_restaurant = False
        in_page_header = True
        for line in text:
            if line.startswith('\x0c'): # New page
                if address is not None and inspection_date is not None:
                    yield {
                        'restaurant_name': restaurant_name,
                        'address': address,
                        'city': city,
                        'restaurant_type': restaurant_type,
                        'violation_list': violations,
                        'inspection_date': inspection_date,
                    }

                inspection_date = violations = address = city = restaurant_name = restaurant_type = None
                last_line_was_restaurant = False
                in_page_header = True

            line = line.strip().decode('iso-8859-1')

            # Skip cruft, like page numbers, etc.
            if re.search(r'^Environmental Health Services\s{10,}\d\d?/\d\d?/\d\d\d\d$', line) \
                    or re.search(r'^Food Establishment Inspections,', line) \
                    or re.search('^Page \d+ of \d+$', line) \
                    or line == 'City of Philadelphia' \
                    or line == 'Environmental Health Services' \
                    or line == 'FOOD PROTECTIONS VIOLATIONS' \
                    or line == '':
                continue

            m = re.search(r'(.*?)\s{5,}((?:Retail Food|Institution|Vendor|Wholesale|Water Supply):.*)$', line)
            if m:
                in_page_header = False
                if address is not None:
                    yield {
                        'restaurant_name': restaurant_name,
                        'address': address,
                        'city': city,
                        'restaurant_type': restaurant_type,
                        'violation_list': violations,
                        'inspection_date': inspection_date,
                    }
                restaurant_name = m.group(1)
                restaurant_type = m.group(2)
                inspection_date = violations = address = city = None
                last_line_was_restaurant = True
                continue
            # Ugly special cases because these only have a single space between name and type.
            elif line in ('VALLE OLIMPICO RESTAURANT & SUPER BAKERY Retail Food: Restaurant, Eat-in', 'MCMILLAN CHRISTIAN FAMILY CHILD DAY CARE Institution: Child, Family Day Care Homes'):
                m = re.search(r'^(.*?) ((?:Retail Food|Institution):.*)$', line)
                restaurant_name = m.group(1)
                restaurant_type = m.group(2)
                inspection_date = violations = address = city = None
                last_line_was_restaurant = True
                continue
            elif in_page_header:
                continue

            if last_line_was_restaurant:
                m = re.search(r'(.*?)\s{10,}(.*)$', line)
                if not m:
                    print line
                    raise ValueError
                address = m.group(1)
                city = m.group(2)
                last_line_was_restaurant = False
                continue

            if line == 'Inspection Date':
                violations = []
                continue

            if re.search(r'^\d\d?/\d\d?/\d\d$', line):
                if address is not None and inspection_date is not None:
                    yield {
                        'restaurant_name': restaurant_name,
                        'address': address,
                        'city': city,
                        'restaurant_type': restaurant_type,
                        'violation_list': violations,
                        'inspection_date': inspection_date,
                    }
                    violations = []
                inspection_date = line
                continue

            # The remaining text must be the violation. It's either a new
            # violation, or a continuation of the violation text from the
            # previous line.
            if re.search(r'^(?:PM-\d+\.\d+|\d+-\d|\d+\.\d+|PLEASE NOTE:|Proper plans,|This inspection|Person in Control|Note: )', line):
                try:
                    violations.append(line)
                except AttributeError:
                    print repr(line)
                    raise
            elif line.lower() == 'no critical violations':
                pass
            else:
                try:
                    violations[-1] += ' ' + line
                except IndexError:
                    print repr(line)
                    raise
        if address is not None and inspection_date is not None:
            yield {
                'restaurant_name': restaurant_name,
                'address': address,
                'city': city,
                'restaurant_type': restaurant_type,
                'violation_list': violations,
                'inspection_date': inspection_date,
            }

    def clean_list_record(self, record):
        record['inspection_date'] = parse_date(record['inspection_date'], '%m/%d/%y')
        # The PDFs don't include any sort of unique ID for a restaurant, so we
        # create our own by hashing the restaurant name and address.
        record['restaurant_hash'] = md5.new('%s.%s' % (record['restaurant_name'].upper().encode('utf8'), record['address'].upper().encode('utf8'))).hexdigest()
        record['raw_address'] = record['address'].upper()
        record['raw_city'] = record['city'].upper()

        clean_violations = []
        notes = []
        for vio in record['violation_list']:
            # Split the violation into code and comment. The tricky part here
            # is that this text could be split over multiple lines, and
            # sometimes the text that begins a line *looks* like a code but
            # really isn't. The prime example is "215.685.7498" -- the phone
            # number for the health department -- which we special-case in the
            # following regex.
            m = re.search(r'^(?!215[-\.]685[-\.]74|856[-\.]912[-\.]4193)(\d[-\d\.]+(?::?\s*?\([A-Z]\))?)\s+(.*)$', vio)
            if m:
                vio_code, vio_comment = m.groups()
                vio_code = re.sub(r'\s\s+', ' ', vio_code) # Collapse multiple spaces into one.

                # Violation comments have stuff like 'foo-------bar', so clean that up.
                vio_comment = re.sub(r'\s*--+\s*', ' -- ', vio_comment)

                clean_violations.append((vio_code, vio_comment))
            else:
                notes.append(vio)
        record['violation_list'] = clean_violations
        record['notes'] = ' '.join(notes)
        return record

    def existing_record(self, record):
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id, item_date=record['inspection_date'])
            qs = qs.by_attribute(self.schema_fields['restaurant_hash'], record['restaurant_hash'])
            return qs[0]
        except IndexError:
            return None

    def save(self, old_record, list_record, detail_record):
        if old_record is not None:
            return # We already have this inspection.

        restaurant_type = self.get_or_create_lookup('restaurant_type', list_record['restaurant_type'], list_record['restaurant_type'])
        violation_lookups = [self.get_or_create_lookup('violation', v[0], v[0], make_text_slug=False) for v in list_record['violation_list']]
        violation_lookup_text = ','.join([str(v.id) for v in violation_lookups])

        # There's a bunch of data about every particular violation, and we
        # store it as a JSON object. Here, we create the JSON object.
        v_lookup_dict = dict([(v.code, v) for v in violation_lookups])
        v_list = [{'lookup_id': v_lookup_dict[code].id, 'comment': comment} for code, comment in list_record['violation_list']]
        details_json = DjangoJSONEncoder().encode({'notes': list_record['notes'], 'violations': v_list})

        title = u'%s inspected: ' % list_record['restaurant_name']
        if not list_record['violation_list']:
            title += u'No critical violations'
        else:
            num = len(list_record['violation_list'])
            title += u'%s critical violation%s' % (num, num != 1 and 's' or '')

        attributes = {
            'raw_address': list_record['raw_address'],
            'raw_city': list_record['raw_city'],
            'restaurant_hash': list_record['restaurant_hash'],
            'details': details_json,
            'restaurant_name': list_record['restaurant_name'],
            'restaurant_type': restaurant_type.id,
            'violation': violation_lookup_text,
        }
        self.create_newsitem(
            attributes,
            title=title,
            item_date=list_record['inspection_date'],
            location_name=list_record['address'],
        )

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    for name in ('w', 'cc', 'lne', 'ne', 'n', 'nw', 's'):
        RestaurantScraper('ehs_inspections_-_%s_jan09.pdf' % name).display_data()

########NEW FILE########
__FILENAME__ = retrieval
"""
Scraper for San Jose permit data

https://www.sjpermits.org/permits/permits/general/reportdata.asp
"""

from ebdata.retrieval.scrapers.list_detail import SkipRecord
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date
from cStringIO import StringIO
import csv

class Scraper(NewsItemListDetailScraper):
    schema_slugs = ['building-permit-actions']
    has_detail = False
    sleep = 1
    uri = 'https://www.sjpermits.com/sanjoseftp/permitdataMonths/PDIssue_latest.TXT'

    def list_pages(self):
        yield self.get_html(self.uri)

    def parse_list(self, page):
        fh = StringIO(page)
        reader = csv.DictReader(fh, delimiter='\t')
        for record in reader:
            yield record

    def clean_permit_type(self, pa):
        permit_types_mapping = {
            'B-4. Complete': 'B-Complete',
            'E-4. Complete': 'E-Complete',
            'P-4. Complete': 'P-Complete',
            'M-4. Complete': 'M-Complete',
            'M-1. *n/a':     'M-Complete',
            'P-1. *n/a':     'P-Complete',
            'E-1. *n/a':     'E-Complete',
        }
        pa = pa.strip()
        if permit_types_mapping.has_key(pa):
            return permit_types_mapping[pa]
        return pa

    def clean_list_record(self, record):
        if record['WORKDESC'] == '4635':
            raise SkipRecord('WORKDESC is 4635')
        if record['JOBLOCATION'] is None:
            raise SkipRecord('Record has no location')
        if record['PERMITAPPROVALS'] is None:
            record['permit_types'] = []
        else:
            record['permit_types'] = [self.clean_permit_type(pa) for pa in record['PERMITAPPROVALS'].split(',')]

        # Addresses and extra data generally seem to be separated by 2 or more spaces
        record['location'] = record['JOBLOCATION'].split('  ')[0]
        for format in ['%d-%b-%y', '%m/%d/%Y']:
            try:
                issue_date = parse_date(record['ISSUEDATE'], format)
                break
            except ValueError:
                continue
        record['issue_date'] = issue_date
        record['WORKDESC'] = record['WORKDESC'].strip()
        record['SUBDESC'] = record['SUBDESC'].strip()
        return record

    def existing_record(self, list_record):
        record = None
        qs = NewsItem.objects.filter(schema__id=self.schema.id, item_date=list_record['issue_date'])
        qs = qs.by_attribute(self.schema_fields['apn'], list_record['APN'])
        qs = qs.by_attribute(self.schema_fields['work_type'], list_record['WORKDESC'].upper(), is_lookup=True)
        try:
            return qs[0]
        except IndexError:
            return None

    def detail_required(self, list_record, old_record):
        return True

    def get_detail(self, list_record):
        page = ''
        return page

    def parse_detail(self, page, list_record):
        detail_record = {}
        return detail_record

    def clean_detail_record(self, record):
        return record

    def save(self, old_record, list_record, detail_record):
        if old_record is not None:
            return
        work_type_lookup = self.get_or_create_lookup('work_type', list_record['WORKDESC'], list_record['WORKDESC'].upper(), make_text_slug=False)
        sub_type_lookup = self.get_or_create_lookup('sub_type', list_record['SUBDESC'], list_record['SUBDESC'].upper(), make_text_slug=False)

        permit_type_lookups = []
        for permit_type in list_record['permit_types']:
            permit_type_lookup = self.get_or_create_lookup('permit_types', permit_type, permit_type, make_text_slug=False)
            permit_type_lookups.append(permit_type_lookup)

        attributes = {
            'apn': list_record['APN'],
            'work_type': work_type_lookup.id,
            'sub_type': sub_type_lookup.id,
            'permit_types': ','.join([str(l.id) for l in permit_type_lookups])
        }
        self.create_newsitem(
            attributes,
            title=work_type_lookup.name,
            item_date=list_record['issue_date'],
            location_name=list_record['location']
        )

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    Scraper().update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Scraper for San Jose crime.

http://public.coronasolutions.com/?page=agency_home&agency=25
"""

from ebdata.retrieval.retrievers import PageNotFoundError
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date
import datetime
import re

class PoliceCallsScraper(NewsItemListDetailScraper):
    schema_slugs = ['police-calls']
    has_detail = True
    parse_list_re = re.compile(r'(?s)<h2>(?P<crime_type>.*?)</h2>\s*<table[^>]*>(?P<content>.*?)</table>')
    parse_detail_re = re.compile(r'(?s)<td>(?P<event_number>.*?)</td>\s*<td>(?P<entry_date>.*?)</td>\s*<td>(?P<location>.*?)</td>\s*<td>(?P<disposition>.*?)</td>')
    sleep = 1
    max_bbb_number = 421
    url_template = 'http://public.coronasolutions.com/25/reports/zones/%s/Zone_EventListing.html'

    def list_pages(self):
        missing_pages = 0
        for bbb in range(1, self.max_bbb_number):
            try:
                yield self.get_html(self.url_template % bbb)
            except PageNotFoundError:
                missing_pages += 1
                print "Missing BBB: %s" % bbb
                continue

    def existing_record(self, list_record):
        # We don't know the event_number until we parse the detail record, so
        # we'll check for existing records in the save method.
        return None

    def detail_required(self, list_record, old_record):
        return True

    def get_detail(self, list_record):
        return list_record['content']

    def clean_detail_record(self, record):
        record['location'] = re.sub(r'\s+', ' ', record['location'])
        entry_datetime = parse_date(record['entry_date'], '%Y-%m-%d %H:%M:%S', return_datetime=True)
        record['entry_time'] = entry_datetime.time()
        record['entry_date'] = entry_datetime.date()
        return record

    def save(self, old_record, list_record, detail_record):
        # Check for existing records here since we didn't have enough
        # information to check for them in self.existing_record()
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id)
            old_record = qs.by_attribute(self.schema_fields['event_number'], detail_record['event_number'])[0]
            return
        except IndexError:
            old_record = None
        # Don't import records older than Oct. 21st, 2008. The system *says* that
        # it keeps 90 days worth of data, but the older stuff seems too sparse
        # for that to be true.
        if detail_record['entry_date'] < datetime.date(2008, 10, 21):
            return
        crime_type = self.get_or_create_lookup('crime_type', list_record['crime_type'], list_record['crime_type'], make_text_slug=False)
        disposition = self.get_or_create_lookup('disposition', detail_record['disposition'], detail_record['disposition'], make_text_slug=False)

        attributes = {
            'crime_type': crime_type.id,
            'disposition': disposition.id,
            'event_number': detail_record['event_number'],
            'event_time': detail_record['entry_time'],
        }
        self.create_newsitem(
            attributes,
            title=crime_type.name,
            item_date=detail_record['entry_date'],
            location_name=detail_record['location']
        )

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    PoliceCallsScraper().update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Scraper for San Jose Restaurant Inspections

http://www.decadeonline.com/results.phtml?agency=scc&offset=0&city=san+jose&sort=FACILITY_NAME
"""

from ebdata.retrieval.scrapers.list_detail import SkipRecord
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date
from ebpub.utils.text import smart_title, clean_address
import re

# These inspection types reflect records that aren't actually inspections, so
# we ignore them.
INSPECTION_TYPES_TO_IGNORE = set([
    'BILINGUAL ASSISTANCE',
    'CONSULTATION/MEETING',
    'FOOD SAFETY CLASS',
    'IN-SERVICE TRAINING-TRAINEE',
    'IN-SERVICE TRAINING-TRAINER',
    'PLAN/MAP CHECK',
])

class Scraper(NewsItemListDetailScraper):
    schema_slugs = ['restaurant-inspections']
    parse_list_re = re.compile(r'(?s)<tr class="bodytext"[^>]*?>\s*<td[^>]*?>\s*?<a[^>]*?facid=(?P<facid>.*?)\">(?P<name>.*?)</a>\s*?</td>\s*?<td[^>]*?>(?P<location>.*?)</td>')
    parse_detail_re = re.compile(r'(?s)<td[^>]*>\s*(?P<inspection_date>\d{2}/\d{2}/\d{4})\s*</td>.*?<td[^>]*>\s*<A[^>]*>(?P<inspection_type>.*?)</A><A[^>]*>(?:.*?)</A>\s*</td>\s*</tr>(?P<html>.*?)(?:<tr bgcolor="silver" class="bodytext">|<A[^>]*>Search Again</A>)')
    parse_violations_re = re.compile(r'<strong>(.*?)</strong>')
    sleep = 1
    list_uri = 'http://www.decadeonline.com/results.phtml?agency=scc&offset=%s&city=san+jose&sort=FACILITY_NAME'
    detail_uri = 'http://www.decadeonline.com/fac.phtml?agency=scc&facid=%s'

    def list_pages(self):
        next_page_re = re.compile(r'<a[^>]*>Next Page</a>')
        offset = 0
        while 1:
            html = self.get_html(self.list_uri % offset)
            yield html
            offset += 50
            if next_page_re.search(html) == None:
                break

    def clean_list_record(self, record):
        record['location'] = re.sub(r'\s+', ' ', record['location']).replace(', SAN JOSE ', '').strip()
        return record

    def existing_record(self, list_record):
        # There is no way to determine an existing record from list_record
        return None

    def detail_required(self, list_record, old_record):
        return True

    def get_detail(self, list_record):
        html = self.get_html(self.detail_uri % list_record['facid'])
        return html

    def parse_detail(self, page, list_record):
        detail_record = []
        for m in self.parse_detail_re.finditer(page):
            record = m.groupdict()
            if record['inspection_type'] in INSPECTION_TYPES_TO_IGNORE:
                raise SkipRecord('Got ignorable inspection type %r' % record['inspection_type'])
            html = record.pop('html')
            record['violations'] = []
            for vm in self.parse_violations_re.finditer(html):
                record['violations'].append(vm.group(1))
            detail_record.append(record)
        return detail_record

    def clean_detail_record(self, record):
        for inspection in record:
            inspection['inspection_date'] = parse_date(inspection['inspection_date'], '%m/%d/%Y')
        return record

    def save(self, old_record, list_record, detail_record):
        for record in detail_record:
            # Since parse_detail emits more than one record, we check for existing
            # records here rather than in self.existing_record()
            try:
                qs = NewsItem.objects.filter(schema__id=self.schema.id, item_date=record['inspection_date'])
                obj = qs.by_attribute(self.schema_fields['facility_id'], list_record['facid'])[0]
            except IndexError:
                pass
            else:
                return None

            inspection_type_lookup = self.get_or_create_lookup('inspection_type', record['inspection_type'], record['inspection_type'], make_text_slug=False)
            violations_lookups = []
            for violation in record['violations']:
                lookup = self.get_or_create_lookup('violations', violation, violation, make_text_slug=False)
                violations_lookups.append(lookup)
            attributes = {
                'name': list_record['name'],
                'inspection_type': inspection_type_lookup.id,
                'violations': ','.join([str(l.id) for l in violations_lookups]),
                'facility_id': list_record['facid'],
            }
            self.create_newsitem(
                attributes,
                title=smart_title(list_record['name']),
                url=self.detail_uri % list_record['facid'],
                item_date=record['inspection_date'],
                location_name=clean_address(list_record['location'])
            )

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    Scraper().update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Scraper for Seattle building permits

http://web1.seattle.gov/dpd/dailyissuance/Default.aspx
"""

from django.utils.text import capfirst
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
import datetime
import re

PERMIT_CATEGORIES = (
    ('chkContruction', 'Construction'),
    ('chkLandUse', 'Land Use'),
    ('chkOtc', 'Over the Counter'),
)

class Scraper(NewsItemListDetailScraper):
    schema_slugs = ['building-permits']
    has_detail = False
    parse_list_re = re.compile(r'<tr class="ReportRow[^>]*>\s*<td[^>]*>(?P<permit_number>.*?)</td><td[^>]*>(?P<permit_type>.*?)</td><td[^>]*>(?P<location>.*?)</td><td[^>]*>(?P<description>.*?)</td>')
    sleep = 1
    uri = 'http://web1.seattle.gov/dpd/dailyissuance/Default.aspx'

    def __init__(self, *args, **kwargs):
        self.start_date = kwargs.pop('start_date', None)
        super(Scraper, self).__init__(*args, **kwargs)

    def list_pages(self):
        viewstate = re.search(r'<input type="hidden" name="__VIEWSTATE" value="(.*?)" />', self.get_html(self.uri)).group(1)
        if self.start_date:
            date = self.start_date
        else:
            date = datetime.date.today() - datetime.timedelta(days=7)
        while date <= datetime.date.today():
            for key, name in PERMIT_CATEGORIES:
                params = {
                    '__EVENTARGUMENT': '',
                    '__EVENTTARGET': 'btnSearch',
                    '__VIEWSTATE': viewstate,
                    'dgIssuedPermits_LCS': '0',
                    'dgIssuedPermits_LSD': 'Ascending',
                    'txtIssuanceDate': date.strftime('%m/%d/%Y'),
                    'txtIssuanceDate_OriginalText': date.strftime('%m/%d/%Y'),
                }
                params[key] = 'on'
                yield (name, date, self.get_html(self.uri, params))
            date += datetime.timedelta(days=1)

    def parse_list(self, page):
        category, date, html = page
        records = super(Scraper, self).parse_list(html)
        for record in records:
            record['date'] = date
            record['category'] = category
            yield record

    def clean_list_record(self, record):
        record['location'] = re.sub(r'\s+', ' ', record['location'])
        return record

    def existing_record(self, list_record):
        record = None
        qs = NewsItem.objects.filter(schema__id=self.schema.id)
        qs = qs.by_attribute(self.schema_fields['permit_number'], list_record['permit_number'])
        try:
            return qs[0]
        except IndexError:
            return None

    def save(self, old_record, list_record, detail_record):
        if old_record is not None:
            return
        category_lookup = self.get_or_create_lookup('category', list_record['category'], list_record['category'], make_text_slug=False)
        permit_type_lookup = self.get_or_create_lookup('permit_type', capfirst(list_record['permit_type'].lower()), list_record['permit_type'], make_text_slug=False)
        attributes = {
            'permit_number': list_record['permit_number'],
            'category': category_lookup.id,
            'permit_type': permit_type_lookup.id,
            'description': list_record['description']
        }
        self.create_newsitem(
            attributes,
            title=category_lookup.name,
            item_date=list_record['date'],
            url="http://web1.seattle.gov/DPD/permitstatus/Project.aspx?id=%s" % list_record['permit_number'],
            location_name=list_record['location']
        )

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    Scraper(start_date=datetime.date(2008, 1, 1)).update()


########NEW FILE########
__FILENAME__ = retrieval
"""
Seattle city press release scraper.

http://www.seattle.gov/news/
Example: http://www.seattle.gov/news/detail.asp?ID=8622
"""

from ebdata.blobs.scrapers import IncrementalCrawler
import re

class SeattleCityPressReleaseCrawler(IncrementalCrawler):
    schema = 'city-press-releases'
    seed_url = 'http://www.seattle.gov/news/'
    date_headline_re = re.compile(r'(?si)<b>SUBJECT:</b>&nbsp;&nbsp; (?P<article_headline>.*?)\s*</tr>.*?<b>FOR IMMEDIATE RELEASE:&nbsp;&nbsp;&nbsp;</b><br>\s*(?P<article_date>\d\d?/\d\d?/\d\d\d\d)')
    date_format = '%m/%d/%Y'
    max_blanks = 8

    def public_url(self, id_value):
        return 'http://www.seattle.gov/news/detail.asp?ID=%s' % id_value

    def id_for_url(self, url):
        return url.split('ID=')[1]

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    SeattleCityPressReleaseCrawler().update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Screen scraper for Seattle real-time fire 911 calls.

http://www2.seattle.gov/fire/realTime911/getRecsForDatePub.asp?action=Today&incDate=&rad1=des
"""

from ebdata.retrieval import UnicodeRetriever
from ebdata.retrieval.scrapers.list_detail import SkipRecord
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from urllib import urlencode
from time import strptime
import datetime
import re

class SeattleFireDispatchScraper(NewsItemListDetailScraper):
    uri_template = 'http://www2.seattle.gov/fire/realTime911/getRecsForDatePub.asp?%s'
    schema_slugs = ['fire-dispatch']
    has_detail = False
    sleep = 1
    parse_list_re = re.compile(r"(?s)<tr id=row_\d+[^>]*>\s*<td[^>]*>(?P<date>.*?)</td>\s*<td[^>]*>(?P<incident_number>.*?)</td>\s*<td[^>]*>(?P<level>.*?)</td>\s*<td[^>]*>(?P<units>.*?)</td>\s*<td[^>]*>(?P<location>.*?)</td>\s*<td[^>]*>(?P<type>.*?)</td>\s*</tr>")

    def __init__(self, *args, **kwargs):
        self.get_archive = kwargs.pop('get_archive', False)
        super(SeattleFireDispatchScraper, self).__init__(*args, **kwargs)
        self.retriever = UnicodeRetriever()

    def list_pages(self):
        if self.get_archive:
            date = datetime.date(2003, 11, 7)
        else:
            date = datetime.date.today() - datetime.timedelta(days=7)
        while 1:
            if date == datetime.date.today():
                break
            params = urlencode({'incDate': date.strftime('%m/%d/%y'), 'rad1': 'des'})
            yield self.get_html(self.uri_template % params)
            date += datetime.timedelta(days=1)

    def clean_list_record(self, record):
        try:
            dt = strptime(record['date'].strip(), '%m/%d/%Y %I:%M:%S %p')
            record['incident_date'] = datetime.date(*dt[:3])
            record['incident_time'] = datetime.time(*dt[3:6])
        except ValueError:
            dt = strptime(record['date'].strip(), '%m/%d/%Y')
            record['incident_date'] = datetime.date(*dt[:3])
            record['incident_time'] = None
        record['units'] = record['units'].split()
        if 'mutual aid' in record['type'].lower():
            raise SkipRecord('Skipping mutual aid: %r' % record['type'])
        return record

    def existing_record(self, record):
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id)
            return qs.by_attribute(self.schema_fields['incident_number'], record['incident_number'])[0]
        except IndexError:
            return None

    def save(self, old_record, list_record, detail_record):
        if old_record is not None:
            return
        incident_type = self.get_or_create_lookup('incident_type', list_record['type'], list_record['type'], make_text_slug=False)
        unit_lookups = []
        for unit in list_record['units']:
            unit_lookup = self.get_or_create_lookup('units', unit, unit, make_text_slug=False)
            unit_lookups.append(unit_lookup)
            
        attributes = {
            'incident_date': list_record['incident_date'],
            'incident_time': list_record['incident_time'],
            'incident_number': list_record['incident_number'],
            'incident_type': incident_type.id,
            'units': ','.join([str(u.id) for u in unit_lookups])
        }
        self.create_newsitem(
            attributes,
            title=incident_type.name,
            item_date=list_record['incident_date'],
            location_name=list_record['location']
        )

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    SeattleFireDispatchScraper(get_archive=True).update()

########NEW FILE########
__FILENAME__ = retrieval
# -*- coding: utf-8 -*-
"""
Scraper for Seattle land use bulletins

http://web1.seattle.gov/dpd/luib/Default.aspx
"""

from django.utils.text import capfirst
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date
from ebpub.utils.text import smart_title
import re

class Scraper(NewsItemListDetailScraper):
    schema_slugs = ['land-use-bulletins']
    parse_list_re = re.compile(r'<td><a href="Notice.aspx\?BID=(?P<bid>\d+)&amp;NID=(?P<nid>\d+)"><nobr>(?P<project_number>.*?)</nobr></a></td><td[^>]*>(?P<location>.*?)</td><td[^>]*><a[^>]*>(?P<bulletin_type>.*?)</a></td>')
    parse_detail_regexes = {
        'description': re.compile(r'(?s)<tr id="trProjectDescription">\s*<td[^>]*>(.*?)</td>'),
        'zone': re.compile(r'(?s)<span id=\"lblZoning\"[^>]*>(.*?)</span>'),
        'application_date': re.compile(r'(?s)<span id=\"lblApplicationDate\"[^>]*>(.*?)</span>'),
        'complete_date': re.compile(r'(?s)<span id=\"lblCompleteDate\"[^>]*>(.*?)</span>'),
    }
    sleep = 1
    list_uri = 'http://web1.seattle.gov/dpd/luib/Default.aspx'
    detail_uri = 'http://web1.seattle.gov/dpd/luib/Notice.aspx?BID=%s&NID=%s'

    def __init__(self, *args, **kwargs):
        self.get_archive = kwargs.pop('get_archive', None)
        super(Scraper, self).__init__(*args, **kwargs)

    def list_pages(self):
        html = self.get_html(self.list_uri)
        viewstate = re.search(r'<input type="hidden" name="__VIEWSTATE" value="(.*?)" />', html).group(1)
        options = []
        for m in re.compile(r'\{"Text":"(?P<date>\d{2}/\d{2}/\d{4})","Value":"(?P<value>\d+)",.*?"ClientID":"RadComboBox1_c(?P<index>\d+)"\}').finditer(html):
            options.append(m.groupdict())
        # If we aren't scraping the entire archive, just grab pages for the
        # last couple of days.
        if not self.get_archive:
            options = options[:2]
        for option in options:
            params = {
                '__EVENTTARGET': 'RadComboBox1',
                '__EVENTARGUMENT': 'TextChange',
                '__VIEWSTATE': viewstate,
                'RadComboBox1_Input': option['date'],
                'RadComboBox1_text': option['date'],
                'RadComboBox1_value': option['value'],
                'RadComboBox1_index': option['index'],
                'RadComboBox1_clientWidth': '',
                'RadComboBox1_clientHeight': '',
                'RadGrid1PostDataValue': '',
                'RadAJAXControlID': 'RadAjaxManager2',
                'httprequest': 'true',
            }
            yield (option['date'], self.get_html(self.list_uri, params))

    def parse_list(self, page):
        date, html = page
        records = super(Scraper, self).parse_list(html)
        for record in records:
            record['bulletin_date'] = date
            yield record

    def clean_zone(self, zone):
        zone = zone.strip().upper()
        zone = re.sub(r'[\.|\'|]', '', zone)
        misspellings = (
            ('AIPORT', 'AIRPORT'),
            ('AIPRT', 'AIRPORT'),
            ('AIRPRT', 'AIRPORT'),
            ('ARTERL', 'ARTERIAL'),
            ('COMMCERCIAL', 'COMMERCIAL'),
            ('CMRCL', 'COMMERCIAL'),
            ('COMMERCL', 'COMMERCIAL'),
            ('DOWNTWN', 'DOWNTOWN'),
            ('GENRL', 'GENERAL'),
            ('HIGHT', 'HEIGHT'),
            ('PARAKING', 'PARKING'),
            ('ARTERIAL WITH 100 FT', 'ARTERIAL WITHIN 100 FT'),
            ('C1 65', 'C1-65'),
            ('CORE2', 'CORE 2'),
            ('GENERAL1', 'GENERAL 1'),
            ('GENERAL2', 'GENERAL 2'),
            ('NC 2-40', 'NC2-40'),
            ('NC2 40', 'NC2-40'),
            ('ZONNING', 'ZONING'),
        )
        # Normalize DIST vs DISTRICT
        zone = re.sub(r'\bDIST\b', 'DISTRICT', zone)
        # Normalize FT vs FEET
        zone = re.sub(r'\bFEET\b', 'FT', zone)
        zone = re.sub(r'COMMERCIAL(\d+)', r'COMMERCIAL \1', zone)
        zone = re.sub(r'COMMERCIAL-', r'COMMERCIAL ', zone)
        zone = re.sub(r'GENERAL(\d+)', r'GENERAL \1', zone)
        zone = re.sub(r'LOWRISE-(\d+)', r'LOWRISE \1', zone)
        zone = zone.replace('100FT', '100 FT')

        zone = re.sub(r'ARTERIAL WITHIN 100$', r'ARTERIAL WITHIN 100 FT', zone)
        zone = re.sub(r'STEEP SLOPE \(>=40$', r'STEEP SLOPE (>=40%)', zone)
        zone = re.sub(r'SCENIC VIEW WITHIN 500$', r'SCENIC VIEW WITHIN 500 FT', zone)
        zone = re.sub(r'URBAN VIL$', r'URBAN VILLAGE', zone)

        for incorrect, correct in misspellings:
            zone = zone.replace(incorrect, correct)
        return zone

    def clean_list_record(self, record):
        record['bulletin_date'] = parse_date(record['bulletin_date'], '%m/%d/%Y')
        return record

    def existing_record(self, list_record):
        qs = NewsItem.objects.filter(schema__id=self.schema.id)
        qs = qs.by_attribute(self.schema_fields['bid'], list_record['bid'])
        qs = qs.by_attribute(self.schema_fields['nid'], list_record['nid'])
        try:
            return qs[0]
        except IndexError:
            return None

    def detail_required(self, list_record, old_record):
        return old_record is None

    def get_detail(self, list_record):
        html = self.get_html(self.detail_uri % (list_record['bid'], list_record['nid']))
        return html

    def parse_detail(self, page, list_record):
        record = {}
        for key, detail_re in self.parse_detail_regexes.items():
            m = detail_re.search(page)
            if m is None:
                record[key] = None
            else:
                record[key] = m.group(1)
        return record

    def clean_detail_record(self, record):
        zones = record['zone'] and record['zone'].split(',') or []
        record['zone'] = [self.clean_zone(z) for z in zones if z != '&nbsp;']
        for key in ['application_date', 'complete_date']:
            if record.has_key(key):
                record[key] = parse_date(record[key], '%m/%d/%Y')
            else:
                record[key] = None
        return record

    def save(self, old_record, list_record, detail_record):
        if old_record is not None:
            return
        if list_record['location'] == '&nbsp;':
            return
        bulletin_type_lookup = self.get_or_create_lookup('bulletin_type', capfirst(list_record['bulletin_type'].lower()), list_record['bulletin_type'].upper(), make_text_slug=False)
        zone_lookups = []
        for z in detail_record['zone']:
            zone_lookup = self.get_or_create_lookup('zone', capfirst(z.lower()), z, make_text_slug=False)
            zone_lookups.append(zone_lookup)
        attributes = {
            'project_number': list_record['project_number'],
            'description': detail_record.get('description', None),
            'bulletin_type': bulletin_type_lookup.id,
            'application_date': detail_record['application_date'],
            'complete_date': detail_record['complete_date'],
            'zone': ','.join([str(z.id) for z in zone_lookups]),
            'bid': list_record['bid'],
            'nid': list_record['nid'],
        }
        self.create_newsitem(
            attributes,
            title=bulletin_type_lookup.name,
            url='http://web1.seattle.gov/dpd/luib/Notice.aspx?BID=%s&NID=%s' % (list_record['bid'], list_record['nid']),
            item_date=list_record['bulletin_date'],
            location_name=smart_title(list_record['location'])
        )

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    Scraper(get_archive=True).update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Scraper for Seattle restaurant inspections

http://www.decadeonline.com/main.phtml?agency=skc
"""

from django.core.serializers.json import DjangoJSONEncoder
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date
from ebpub.utils.text import smart_title
import re

class Scraper(NewsItemListDetailScraper):
    schema_slugs = ['restaurant-inspections']
    parse_list_re = re.compile(r'(?s)<tr>\s*<td[^>]*?>\s*?<a[^>]*?facid=(?P<facid>.*?)\"><font[^>]*?>\s*(?P<name>.*?)\s*</font>\s*</a>\s*?</td>\s*?<td[^>]*?>\s*<font[^>]*?>\s*(?P<location>.*?)\s*</font>\s*</td>')
    sleep = 1
    list_uri = 'http://www.decadeonline.com/results.phtml?agency=skc&offset=%s&city=seattle&sort=FACILITY_NAME'
    detail_uri = 'http://www.decadeonline.com/fac.phtml?agency=skc&facid=%s'

    # Finds the table that holds all of the inspections and violations.
    table_re = re.compile(r'(?s)(?P<content><table cellspacing="0" border="0" width="560">.*?</table>)')
    # Finds all table rows so we can process them one at a time.
    tr_re = re.compile(r'(?s)<tr[^>]*>(.*?)</tr>')
    # Finds an inspection in a table row.
    inpection_re = re.compile(r'(?s)<font[^>]*>(?P<inspection_type>.+?)</font>.*<font[^>]*>(?P<inspection_date>\d{2}/\d{2}/\d{4})</font>.*<font[^>]*>(?P<points>.+?)</font>')
    # Finds a violation in a table row.
    violation_re = re.compile(r'(?s)<font.+?color=\s*"#(?P<color>[\w\d]{6})"[^>]*>-(?P<violation>.*?)</font>')

    def list_pages(self):
        offset = 0
        html = self.get_html(self.list_uri % offset)
        self.total_records = int(re.search(r'(?s)Results\s+\d+\s+-\s+\d+\s+of\s+(\d+)', html).group(1))
        yield html

        while 1:
            offset += 50
            if offset >= self.total_records:
                break
            yield self.get_html(self.list_uri % offset)

    def clean_list_record(self, record):
        record['location'] = re.sub(r'\s+', ' ', record['location'])
        record['location'] = re.sub(r'(?i),\s*SEATTLE', '', record['location']).strip()
        return record

    def existing_record(self, list_record):
        # Since parse_detail will emit more than one record, check for existing
        # records in the save method.
        return None

    def detail_required(self, list_record, old_record):
        return True

    def get_detail(self, list_record):
        html = self.get_html(self.detail_uri % list_record['facid'])
        return html

    def parse_detail(self, page, list_record):
        inspections = []
        table_match = self.table_re.search(page)
        if table_match is None:
            return []
        for m in self.tr_re.finditer(table_match.group(1)):
            html = m.group(1)
            inspection_match = self.inpection_re.search(html)
            # First try to find an inspection and if that fails, look for a violation
            # and add it to the current inspection.
            if inspection_match is not None:
                inspection = inspection_match.groupdict()
                inspection['violations'] = []
                inspections.append(inspection)
            else:
                violation_match = self.violation_re.search(html)
                if violation_match is None:
                    continue
                inspection['violations'].append(violation_match.groupdict())
        return inspections

    def clean_detail_record(self, records):
        for record in records:
            record['inspection_date'] = parse_date(record['inspection_date'], '%m/%d/%Y')
            for v in record['violations']:
                if v['color'] == 'ff0000':
                    v['severity'] = 'critical'
                else:
                    v['severity'] = 'normal'
                v['violation'] = v['violation'].decode('latin-1')
        return records

    def save(self, old_record, list_record, detail_records):
        for record in detail_records:
            # Since parse_detail emits more than one record, we check for existing
            # records here rather than in self.existing_record()
            try:
                qs = NewsItem.objects.filter(schema__id=self.schema.id, item_date=record['inspection_date'])
                obj = qs.by_attribute(self.schema_fields['facility_id'], list_record['facid'])[0]
            except IndexError:
                pass
            else:
                return None
            if record['inspection_type'] == 'Consultation/Education - Field':
                continue
            inspection_type_lookup = self.get_or_create_lookup('inspection_type', record['inspection_type'], record['inspection_type'], make_text_slug=False)
            violations_lookups = []
            for v in record['violations']:
                vl = self.get_or_create_lookup('violations', v['violation'], v['violation'], make_text_slug=False)
                violations_lookups.append(vl)
            attributes = {
                'name': list_record['name'],
                'inspection_type': inspection_type_lookup.id,
                'points': record['points'],
                'violations': ','.join([str(l.id) for l in violations_lookups]),
                'violations_json': DjangoJSONEncoder().encode(record['violations']),
                'facility_id': list_record['facid'],
            }
            self.create_newsitem(
                attributes,
                title=list_record['name'],
                url='http://www.decadeonline.com/fac.phtml?agency=skc&forceresults=1&facid=%s' % list_record['facid'],
                item_date=record['inspection_date'],
                location_name=smart_title(list_record['location'])
            )

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    Scraper().update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Screen scraper for San Francisco building permits data.
It's the "Building reports filed and issued" link here:
http://www.sfgov.org/site/dbi_page.asp?id=30605
"""

from ebdata.parsing.excel import ExcelDictReader
from ebdata.retrieval.scrapers.base import ScraperBroken
from ebdata.retrieval.scrapers.list_detail import SkipRecord
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from urlparse import urljoin
import os
import re

class PermitScraper(NewsItemListDetailScraper):
    schema_slugs = ('building-permits',)
    has_detail = False

    def __init__(self, month):
        """
        month is a datetime object representing the month to download.
        """
        super(PermitScraper, self).__init__(use_cache=False)
        self.month = month
        self._excel_url_cache = None

    def get_excel_url(self):
        """
        Returns the full URL for the Excel file for self.month.

        This value is cached the first time the function is called.
        """
        if self._excel_url_cache is None:
            # Download the index page and search all of the ".xls" links for
            # the given month/year.
            index_url = 'http://www.sfgov.org/site/dbi_page.asp?id=30608'
            html = self.get_html(index_url)
            excel_links = re.findall(r'<a href="(.*?\.xls)">(.*?)</a>', html)
            month_name, year = self.month.strftime('%B,%Y').split(',')
            this_month_links = [link[0] for link in excel_links if (month_name in link[0] or month_name in link[1]) and (year in link[0] or year in link[1])]
            if len(this_month_links) != 1:
                raise ScraperBroken('Found %s links for %s %s on %s' % (len(this_month_links), month_name, year, index_url))
            self._excel_url_cache = urljoin('http://www.sfgov.org/', this_month_links[0])
        return self._excel_url_cache

    def list_pages(self):
        workbook_path = self.retriever.get_to_file(self.get_excel_url())
        yield ExcelDictReader(workbook_path, sheet_index=0, header_row_num=0, start_row_num=1)
        os.unlink(workbook_path) # Clean up the temporary file.

    def parse_list(self, reader):
        for row in reader:
            yield row

    def clean_list_record(self, record):
        # Normalize inconsistent header names.
        norm_headers = (
            ('FORM_NUMBER', ('FORM_#', 'FORM #')),
            ('APPLICATION #', ('APPLICATION_NUMBER', 'APPLICATION NUMBER', 'APPLICATION NO.', 'APPLICATION NO')),
            ('EXISTING USE', ('EXISTING_USE', 'EXISTINGUSE')),
            ('EXISTING UNITS', ('EXISTING_UNITS', 'EXISTINGUNITS')),
            ('PROPOSED USE', ('PROPOSED_USE', 'PROPOSEDUSE')),
            ('PROPOSED UNITS', ('PROPOSED_UNITS', 'PROPOSEDUNITS')),
        )
        for good_header, bad_headers in norm_headers:
            if good_header not in record:
                for bad_header in bad_headers:
                    if bad_header in record:
                        record[good_header] = record[bad_header]
                        break

        if not str(record['STATUS_DATE']).strip():
            raise SkipRecord
        if not isinstance(record['APPLICATION #'], basestring):
            record['APPLICATION #'] = str(int(record['APPLICATION #']))

        # Drop the '#', if it's in there.
        record['APPLICATION #'] = record['APPLICATION #'].replace('#', '')

        try:
            block = int(record['STREET_NUMBER']) # '12.0' -> '12'
        except ValueError:
            block = record['STREET_NUMBER'] # '12A'
        record['address'] = '%s %s %s' % (block, record['AVS_STREET_NAME'], record['AVS_STREET_SFX'])
        record['existing_units'] = record['EXISTING UNITS'] and int(record['EXISTING UNITS']) or ''
        record['proposed_units'] = record['PROPOSED UNITS'] and int(record['PROPOSED UNITS']) or ''

        # This lookup comes from http://www.sfgov.org/site/uploadedfiles/dbi/reports/ReportLegend.xls
        record['project_type'] = {
            1: 'New construction: Non-wood',
            2: 'New construction: Wood',
            3: 'Additions, alterations or repairs: Major',
            4: 'Erect sign: Erector',
            5: 'Grading, excavation, fill or quarry',
            6: 'Demolition',
            7: 'Erect sign: Wall or painted',
            8: 'Additions, alterations or repairs: Non-major / over the counter',
        }[int(record['FORM_NUMBER'])]
        record['STATUS'] = record['STATUS'].title()
        return record

    def existing_record(self, list_record):
        list_record['status_object'] = self.get_or_create_lookup('status', list_record['STATUS'], list_record['STATUS'])
        lookup = {
            'schema__id': self.schema.id,
            'attribute__%s' % str(self.schema_field_mapping['application_number']): list_record['APPLICATION #'],
            'attribute__%s' % str(self.schema_field_mapping['status']): list_record['status_object'].id,
        }
        try:
            return NewsItem.objects.get(**lookup)
        except NewsItem.DoesNotExist:
            return None

    def save(self, old_record, list_record, detail_record):
        if old_record is not None:
            # Records never change, so we don't have to
            # worry about changing ones that already exist.
            self.logger.info('Permit record already exists: %s' % old_record)
            return

        status = list_record['status_object']
        status = self.get_or_create_lookup('status', list_record['STATUS'], list_record['STATUS'])
        project_type = self.get_or_create_lookup('project_type', list_record['project_type'], list_record['project_type'])
        existing_use = self.get_or_create_lookup('existing_use', list_record['EXISTING USE'], list_record['EXISTING USE'])
        proposed_use = self.get_or_create_lookup('proposed_use', list_record['PROPOSED USE'], list_record['PROPOSED USE'])
        title = calculate_title(list_record['APPLICATION #'], status, project_type, existing_use, proposed_use)

        attributes = {
            'application_number': list_record['APPLICATION #'],
            'project_type': project_type.id,
            'file_date': list_record['FILE_DATE'],
            'status': status.id,
            'expiration_date': list_record['EXPIRATION_DATE'] or None,
            'existing_use': existing_use.id,
            'existing_units': list_record['existing_units'],
            'proposed_use': proposed_use.id,
            'proposed_units': list_record['proposed_units'],
            'description': list_record['DESCRIPTION'],
        }
        self.create_newsitem(
            attributes,
            title=title,
            url=self.get_excel_url(),
            item_date=list_record['STATUS_DATE'],
            location_name=list_record['address'],
        )

def calculate_title(application_number, status, project_type, existing_use, proposed_use):
    title = '%s %s: %s' % (application_number, status.name.lower(), project_type.name)
    if project_type.code == 'New construction' and proposed_use.code:
        title += ' of %s' % proposed_use.name
    elif project_type.code == 'Demolition' and existing_use.code:
        title += ' of %s' % existing_use.name
    elif project_type.code == 'Additions, alterations or repairs' and existing_use.code:
        title += ' to %s' % existing_use.name
    return title

def refresh_titles():
    """
    Refreshes the titles of all building-permit NewsItems.

    This is useful if any of the Lookups' values have changed.
    """
    from ebpub.db.models import Lookup
    for ni in NewsItem.objects.filter(schema__slug='building-permits'):
        atts = ni.attributes
        title = calculate_title(atts['application_number'],
            Lookup.objects.get(id=atts['status']),
            Lookup.objects.get(id=atts['project_type']),
            Lookup.objects.get(id=atts['existing_use']),
            Lookup.objects.get(id=atts['proposed_use'])
        )
        if ni.title != title:
            print "%s\n%s\n" % (ni.title, title)
            ni.title = title
            ni.save()

########NEW FILE########
__FILENAME__ = models
from django.contrib.gis.db import models
from django.contrib.gis.geos import GEOSGeometry

class SfStreetManager(models.GeoManager):
    def get_intersection(self, street_a, street_b):
        """
        Returns the geometry of the intersection of the two streets, or None if it
        can't be determined.
        """
        from django.db import connection
        cursor = connection.cursor()
        table = self.model._meta.db_table
        cursor.execute("""
            SELECT ST_Intersection(a.location, b.location)
            FROM %s a, %s b
            WHERE a.name = %%s
                AND b.name = %%s
                AND ST_Intersects(a.location, b.location)
        """ % (table, table), (street_a, street_b))
        row = cursor.fetchone()
        if row:
            return GEOSGeometry(row[0])
        return None

    def get_intersection_by_cnn(self, cnn):
        """
        Returns the WKB of an intersection by CNN, or None if it doesn't exist.
        """
        try:
            return self.get(cnn=cnn).location
        except self.model.DoesNotExist:
            return None

class SfStreet(models.Model):
    name = models.CharField(max_length=36, db_index=True)
    cnn = models.CharField(max_length=8, db_index=True) # Centerline Network Number
    layer = models.CharField(max_length=16)
    location = models.LineStringField()
    objects = SfStreetManager()

    def __unicode__(self):
        return self.cnn

########NEW FILE########
__FILENAME__ = retrieval
"""
Importer for SF crime.
http://www.sfgov.org/site/uploadedfiles/police/ftpfiles/CADdataZIP.tar.gz
"""

from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date, parse_time
from ebpub.utils.text import clean_address
from cStringIO import StringIO
from lxml import etree
import re
import tarfile

item_xpath = etree.XPath('//CRIME_ID')

# This maps the SFPD's categories to tuples of (broad_category, detail_category).
CUSTOM_CATEGORIES = {
    'ABANDONED VEH': ('Police action/assistance', 'Police action/assistance'),
    'AGGR ASSAULT/ADW': ('Violent crime', 'Assault'),
    'AGGR ASSAULT/ADW - DV': ('Domestic violence', 'Assault'),
    'AIDED CASE': ('Police action/assistance', 'Police action/assistance'),
    'ALARM': ('Nonviolent crime', 'Alarm'),
    'AMBULANCE': ('Police action/assistance', 'Police action/assistance'),
    'ARREST MADE': ('Police action/assistance', 'Police action/assistance'),
    'ASSAULT/BATTERY': ('Violent crime', 'Assault'),
    'ASSAULT/BATTERY - DV': ('Domestic violence', 'Assault'),
    'ATTEMPT SUICIDE': ('Disorder', 'Attempted suicide'),
    'AUTO BOOST/STRIP': ('Nonviolent crime', 'Theft'),
    'BOMB THREAT': ('Violent crime', 'Disorder'),
    'BONFIRE': ('Disorder', 'Disorder'),
    'BRGLR ALRM AUDIB': ('Nonviolent crime', 'Alarm'),
    'BRGLR ALRM SILNT': ('Nonviolent crime', 'Alarm'),
    'BRGLR ALRM VEHIC': ('Nonviolent crime', 'Alarm'),
    'BROKEN WINDOW': ('Nonviolent crime', 'Vandalism'),
    'BURGLARY': ('Nonviolent crime', 'Theft'),
    'CANCEL COMPLAINT': ('Other', 'Other'),
    'CHECK WELL BEING': ('Police action/assistance', 'Police action/assistance'),
    'CHECK WELL BEING - DV': ('Domestic violence', 'Police action/assistance'),
    'CIT REQ INTER -DV': ('Domestic violence', 'Police action/assistance'),
    'CITIZEN ARREST': ('Police action/assistance', 'Police action/assistance'),
    'CITIZEN STANDBY': ('Police action/assistance', 'Police action/assistance'),
    'CITIZEN STANDBY - DV': ('Domestic violence', 'Police action/assistance'),
    'CITY LOT CHECK': ('Police action/assistance', 'Police action/assistance'),
    'COMPLAINT UNKN': ('Other', 'Other'),
    'CORONER': ('Police action/assistance', 'Police action/assistance'),
    'DEMONSTRATION': ('Disorder', 'Disorder'),
    'DRIVEWAY - TOW': ('Police action/assistance', 'Police action/assistance'),
    'DRUNK DRIVER': ('Nonviolent crime', 'Nonviolent crime'),
    'EXPLOSION': ('Disorder', 'Disorder'),
    'FIGHT W/WEAPON': ('Violent crime', 'Fight'),
    'FIGHT W/WEAPONS DV': ('Domestic violence', 'Fight'),
    'FIGHT-NO WEAPONS': ('Violent crime', 'Fight'),
    'FIGHT-NO WEAPONS - DV': ('Domestic violence', 'Fight'),
    'FIRE': ('Disorder', 'Disorder'),
    'FRAUD': ('Nonviolent crime', 'Nonviolent crime'),
    'GRAFFITI': ('Nonviolent crime', 'Vandalism'),
    'GRAND THEFT': ('Nonviolent crime', 'Theft'),
    'HAZMAT INCIDENT': ('Police action/assistance', 'Police action/assistance'),
    'HOMICIDE': ('Violent crime', 'Homicide'),
    'IN SVC/ON FOOT': ('Police action/assistance', 'Police action/assistance'),
    'IND EXPOSURE': ('Nonviolent crime', 'Nonviolent crime'),
    'INTOX PERSON': ('Disorder', 'Disorder'),
    'JUV BEYOND CONT': ('Police action/assistance', 'Police action/assistance'),
    'JUVENILE DIST': ('Disorder', 'Disorder'),
    'KIDNAPPING': ('Violent crime', 'Kidnapping'),
    'MEET W/CITIZEN': ('Police action/assistance', 'Police action/assistance'),
    'MEET W/OFFICER': ('Police action/assistance', 'Police action/assistance'),
    'MENT DISTURBED': ('Disorder', 'Disorder'),
    'MISSING JUV': ('Police action/assistance', 'Police action/assistance'),
    'MISSING PERSON': ('Police action/assistance', 'Police action/assistance'),
    'MUNI ALARM': ('Nonviolent crime', 'Alarm'),
    'MUNI INSP PROG': ('Police action/assistance', 'Police action/assistance'),
    'NOISE NUISANCE': ('Disorder', 'Threats'),
    'PANIC ALARM': ('Nonviolent crime', 'Alarm'),
    'PARKING': ('Nonviolent crime', 'Nonviolent crime'),
    'PASSING CALL': ('Police action/assistance', 'Police action/assistance'),
    'PER.BREAKING IN': ('Nonviolent crime', 'Person breaking in'),
    'PER.BREAKING IN.DV': ('Nonviolent crime', 'Person breaking in'),
    'PERS RING DOOR': ('Disorder', 'Disorder'),
    'PERSON DOWN': ('Disorder', 'Disorder'),
    'PERSON DUMPING': ('Nonviolent crime', 'Nonviolent crime'),
    'PERSON SCREAMING': ('Disorder', 'Disorder'),
    'PERSON W/KNIFE': ('Violent crime', 'Person with knife'),
    'PERSON W/KNIFE - DV': ('Domestic violence', 'Person with knife'),
    'PERSON WITH GUN': ('Violent crime', 'Gun-related'),
    'PETTY THEFT': ('Nonviolent crime', 'Theft'),
    'PRIS. TRANSPORT': ('Police action/assistance', 'Police action/assistance'),
    'PROWLER': ('Nonviolent crime', 'Nonviolent crime'),
    'PSYCH EVAL': ('Police action/assistance', 'Police action/assistance'),
    'PURSESNATCH': ('Nonviolent crime', 'Theft'),
    'Police label': ('Umbrella', 'Detail bucket'),
    'RECVR STOLEN VEH': ('Police action/assistance', 'Theft'),
    'RESISTING ARREST': ('Disorder', 'Disorder'),
    'RESPOND BACKUP': ('Police action/assistance', 'Police action/assistance'),
    'ROADBLOCK': ('Disorder', 'Disorder'),
    'ROBBERY': ('Violent crime', 'Theft'),
    'ROLL INTOX PERS': ('Nonviolent crime', 'Theft'),
    'SENILE PERSON': ('Disorder', 'Disorder'),
    'SERVICE REQUEST': ('Police action/assistance', 'Police action/assistance'),
    'SEX CRIME.CHILD': ('Violent crime', 'Sexual assault'),
    'SEXUAL ASSAULT': ('Violent crime', 'Sexual assault'),
    'SEXUAL ASSAULT - DV': ('Domestic violence', 'Sexual assault'),
    'SHOOTING': ('Violent crime', 'Gun-related'),
    'SHOTS FIRED': ('Violent crime', 'Gun-related'),
    'SHOTSPOTTER': ('Violent crime', 'Gun-related'),
    'SILNT HLDUP ALARM': ('Nonviolent crime', 'Alarm'),
    'SOLICITING PROS': ('Nonviolent crime', 'Nonviolent crime'),
    'STABBING/CUTTING': ('Violent crime', 'Stabbing/cutting'),
    'STABBING/CUTTING - DV': ('Domestic violence', 'Stabbing/cutting'),
    'STOLEN PROPERTY': ('Nonviolent crime', 'Theft'),
    'STOLEN VEHICLE': ('Nonviolent crime', 'Theft'),
    'STRONGARM ROB.': ('Violent crime', 'Strong-arm robbery'),
    'SUSP HOMELESS': ('Disorder', 'Disorder'),
    'SUSP PERSON': ('Disorder', 'Disorder'),
    'SUSP PERSON-VEH': ('Disorder', 'Disorder'),
    'SUSPIC. MAILING': ('Nonviolent crime', 'Nonviolent crime'),
    'THREATS': ('Violent crime', 'Threats'),
    'THREATS - DV': ('Domestic violence', 'Threats'),
    'TOW TRUCK': ('Police action/assistance', 'Police action/assistance'),
    'TRAFFIC STOP': ('Police action/assistance', 'Police action/assistance'),
    'TRESPASSER': ('Nonviolent crime', 'Nonviolent crime'),
    'VANDALISM': ('Nonviolent crime', 'Vandalism'),
    'VANDALISM - DV': ('Domestic violence', 'Vandalism'),
    'VEH ACC.INJ HR': ('Traffic incident', 'Vehicle accident'),
    'VEH ACCIDENT, HR': ('Traffic incident', 'Vehicle accident'),
    'VEH ACCIDENT, INJ': ('Traffic incident', 'Vehicle accident'),
    'VEH ACCIDENT, NI': ('Traffic incident', 'Vehicle accident'),
    'WANTED SUSP/VEH': ('Police action/assistance', 'Police action/assistance'),
    'YGC/JAIL ESCAPE': ('Police action/assistance', 'Police action/assistance')
}

class CrimeScraper(NewsItemListDetailScraper):
    schema_slugs = ('police-calls',)
    has_detail = False

    def list_pages(self):
        raw_file = self.get_html('http://www.sfgov.org/site/uploadedfiles/police/ftpfiles/CADdataZIP.tar.gz')
        tf = tarfile.open('', 'r:gz', StringIO(raw_file))
        for filename in tf.getnames():
            inner_file = tf.extractfile(filename)
            yield inner_file.read()

    def parse_list(self, text):
        if not text.startswith('<Root>'):
            # Older versions of the files are in text format, which isn't quite
            # valid XML, so we clean those up here.
            text = re.sub(r'\s*\d+ rows? selected\.\s*$', '', text)
            text = '<Root>%s</Root>' % text
        xml = etree.fromstring(text)
        for el in item_xpath(xml):
            data = dict([(t.tag, t.text) for t in el])
            data['crime_id'] = el.attrib['CRIME_ID']
            yield data

    def clean_list_record(self, record):
        record['CALL_DATE'] = parse_date(record['CALL_DATE'], '%Y-%m-%d')
        record['OFFENSE_DATE'] = parse_date(record['OFFENSE_DATE'], '%Y-%m-%d')
        record['REPORT_DATE'] = parse_date(record['REPORT_DATE'], '%Y-%m-%d')
        record['COMMON_LOCATION'] = record['COMMON_LOCATION'].strip()
        record['ADDRESS_NBR'] = record['ADDRESS_NBR'].strip()
        record['ADDRESS'] = record['ADDRESS'].strip()

        # The 'NARRATIVE' field includes time and disposition data. Parse that out.
        m = re.search(r'^Time: (?P<TIME>\d\d?:\d\d)<br>.*?<br>Disposition: (?P<DISPOSITION>.*)$', record.pop('NARRATIVE'))
        record.update(m.groupdict())

        record['TIME'] = parse_time(record['TIME'], '%H:%M')

        # Set location_name. The logic is different depending on the ADDRESS_TYPE.
        address_type = record['ADDRESS_TYPE']
        if address_type == 'PREMISE ADDRESS':
            record['location_name'] = '%s block of %s' % (record['ADDRESS_NBR'], clean_address(record['ADDRESS']))
        elif address_type == 'INTERSECTION':
            if '/' in record['ADDRESS']:
                streets = record['ADDRESS'].split('/')
                record['location_name'] = '%s and %s' % (clean_address(streets[0]), clean_address(streets[1]))
            else:
                record['location_name'] = clean_address(record['ADDRESS'])
        elif address_type == 'GEO-OVERRIDE':
            record['location_name'] = clean_address(record['ADDRESS'])
        elif address_type == 'COMMON LOCATION':
            if record['ADDRESS_NBR'] and record['ADDRESS']:
                record['location_name'] = '%s %s' % (record['ADDRESS_NBR'], clean_address(record['ADDRESS']))
            elif record['ADDRESS'] and record['COMMON_LOCATION']:
                record['location_name'] = '%s (%s)' % (clean_address(record['ADDRESS']), clean_address(record['COMMON_LOCATION']))
            elif record['COMMON_LOCATION']:
                record['location_name'] = clean_address(record['COMMON_LOCATION'])
            elif record['ADDRESS']:
                record['location_name'] = clean_address(record['ADDRESS'])
            else:
                record['location_name'] = 'Unknown'
        else:
            record['location_name'] = 'Unknown'

        try:
            d = CUSTOM_CATEGORIES[record['ORIG_CRIMETYPE_NAME']]
        except KeyError:
            d = ('Unknown', 'Unknown')
        record['broad_category'], record['detail_category'] = d

        return record

    def existing_record(self, record):
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id)
            qs = qs.by_attribute(self.schema_fields['crime_id'], record['crime_id'])
            return qs[0]
        except IndexError:
            return None

    def save(self, old_record, list_record, detail_record):
        result = self.get_or_create_lookup('result', list_record['DISPOSITION'], list_record['DISPOSITION'], make_text_slug=False)
        incident_type = self.get_or_create_lookup('incident_type', list_record['ORIG_CRIMETYPE_NAME'], list_record['ORIG_CRIMETYPE_NAME'], make_text_slug=False)
        broad_category = self.get_or_create_lookup('broad_category', list_record['broad_category'], list_record['broad_category'], make_text_slug=False)
        detail_category = self.get_or_create_lookup('detail_category', list_record['detail_category'], list_record['detail_category'], make_text_slug=False)
        address_type = self.get_or_create_lookup('address_type', list_record['ADDRESS_TYPE'], list_record['ADDRESS_TYPE'])
        values = {
            'title': incident_type.name,
            'item_date': list_record['OFFENSE_DATE'],
            'location_name': list_record['location_name'],
        }
        attributes = {
            'address_type': address_type.id,
            'crime_time': list_record['TIME'],
            'incident_type': incident_type.id,
            'broad_category': broad_category.id,
            'detail_category': detail_category.id,
            'result': result.id,
            'common_location': list_record['COMMON_LOCATION'],
            'call_date': list_record['CALL_DATE'],
            'report_date': list_record['REPORT_DATE'],
            'crime_id': list_record['crime_id'],
        }
        if old_record is None:
            self.create_newsitem(attributes, **values)
        else:
            self.update_existing(old_record, values, attributes)

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    CrimeScraper().update()

########NEW FILE########
__FILENAME__ = repair
"""
Change the ad-hoc encoding of Excavation permit details to json.
"""

from ebpub.db.models import NewsItem
from ebpub.utils.text import smart_title
from everyblock.cities.sf.publicway.retrieval import ExcavationPermitScraper
from everyblock.utils import queryset
from django.utils import simplejson
from pprint import pprint
import re

intersection_re = re.compile(r'\s?Intersection of (.*?) and (.*)')
segment_re = re.compile(r'(.*?) from (.*?) to (.*)')

VARIOUS = 'Various streets (see map)'

def repair_details(details):
    """
    Convert old-style details into a more structured format.
    """
    m = segment_re.match(details)
    if m is None:
        m = intersection_re.match(details)
        if m is None:
            raise Exception("No match was found for %s" % details)
        else:
            cross_street_1, cross_street_2 = m.groups()
            return {
                'cross_street_1': cross_street_1.upper().strip(),
                'cross_street_2': cross_street_2.upper().strip()
            }
    else:
        street_name, cross_street_1, cross_street_2 = m.groups()
        if cross_street_2 == '':
            # THE FOLLOWING RETURN VALUE IS NOT A TYPO. THERE IS NO cross_street_2, 
            # SO THIS REALLY IS AN ITERSECTION.
            return {
                'cross_street_1': street_name.upper().strip(),
                'cross_street_2': cross_street_1.upper().strip()
            }
        else:
            return {
                'street_name': street_name.upper().strip(),
                'cross_street_1': cross_street_1.upper().strip(),
                'cross_street_2': cross_street_2.upper().strip()
            }

def verbose_detail(detail):
    clean_cross_1 = smart_title(detail['cross_street_1'])
    clean_cross_2 = smart_title(detail['cross_street_2'])
    if detail.has_key('street_name'):
        clean_street_name = smart_title(detail['street_name'])
        return '%s from %s to %s' % (clean_street_name, clean_cross_1, clean_cross_2)
    else:
        return 'Intersection of %s and %s' % (clean_cross_1, clean_cross_2)

def convert_to_json():
    news_items = NewsItem.objects.filter(schema__slug='excavation-permits').order_by('id')
    for start, end, total, qs in queryset.batch(news_items):
        print "processing %s to %s of %s" % (start + 1, end, total)
        for ni in qs:
            #print ni.attributes['location_details']
            cnn_list, details = ni.attributes['location_details'].split('___')
            details = [repair_details(d) for d in details.split(';')]
            location_details = {
                'cnn_list': cnn_list.split(','),
                'details': details,
            }
            #pprint(location_details)
            ni.attributes['location_details'] = simplejson.dumps(location_details)


            streets = set()
            for detail in details:
                if detail.has_key('street_name'):
                    streets.add(detail['street_name'])
                else:
                    streets.add(detail['cross_street_1'])

            location_name = ', '.join([smart_title(street) for street in streets])
            if len(location_name) > 150:
                location_name = VARIOUS

            description = '; '.join([verbose_detail(loc) for loc in location_details['details']])


            ni.location_name = location_name
            ni.description = description
            ni.save()
            
            #cnn_list = cnn_list.split(',')
            #location_details = location_details.split(';')
            #
            #cnn_count = len(cnn_list)
            #cnn_count_distinct = len(set(cnn_list))
            #location_count = len(location_details)
            #location_count_distinct = len(set(location_details))
            #
            #if cnn_count != cnn_count_distinct:
            #    print "CNN MISMATCH"
            #    print cnn_list
            #    print set(cnn_list)
            #if location_count != location_count_distinct:
            #    print "LOCATION MISMATCH"
            #    print location_details
            #    print set(location_details)
            #if cnn_count != location_count:
            #    print "CNN/LOCATION MISMATCH"
            #    print cnn_list
            #    print location_details
            #if cnn_count_distinct != location_count_distinct:
            #    print "CNN/LOCATION MISMATCH"
            #    print set(cnn_list)
            #    print set(location_details)
            #if cnn_count != cnn_count_distinct != location_count != location_count_distinct:
            #    print ni.attributes['location_details']
            #    print '----------------------------------------------------------'


class Scraper(ExcavationPermitScraper):
    def save(self, old_record, list_record, detail_record):
        locations = simplejson.loads(old_record.attributes['location_details'])
        locations['old_locations'] = [tuple(loc) for loc in locations['old_locations']]
        locations['new_locations'] = [tuple(loc) for loc in locations['new_locations']]

        #pprint(location_list)

        detail = self.clean_detail(list_record)
        old_location = (list_record['cnn'], detail)

        try:
            # remove the old location and add the new one
            i = locations['old_locations'].index(old_location)
            del locations['old_locations'][i]
            locations['new_locations'].append((
                list_record['cnn'],
                detail,
                list_record['streetname'],
                list_record['Cross Street 1'],
                list_record['Cross Street 2']
            ))
            pprint(locations)
        except ValueError:
            pass

        old_record.attributes['location_details'] = simplejson.dumps(locations)
        old_record.save()


if __name__ == '__main__':
    convert_to_json()
    #Scraper().update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Screen scraper for SF public way permits.

There are three retrievers here, corresponding to three schemas.

The data is on an FTP site and is updated daily.
"""

from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebgeo.utils.geodjango import make_geomcoll, line_merge
from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date
from ebpub.utils.text import smart_title
from everyblock.cities.sf.models import SfStreet
from everyblock.utils import queryset
from django.utils.text import capfirst
from django.utils import simplejson
import csv
import datetime
import ftplib
import re
from cStringIO import StringIO

FTP_SERVER = 'bsm.sfdpw.org'
FTP_USERNAME = ''
FTP_PASSWORD = ''

remove_leading_zero = lambda x: re.sub(r'^0+', '', x)

VARIOUS = 'Various streets (see map)'

class BasePublicwayScraper(NewsItemListDetailScraper):
    has_detail = False

    def list_pages(self):
        f = StringIO() # This buffer stores the retrieved file in memory.
        self.logger.debug('Connecting via FTP to %s', FTP_SERVER)
        ftp = ftplib.FTP(FTP_SERVER, FTP_USERNAME, FTP_PASSWORD)
        ftp.set_pasv(False) # Turn off passive mode, which is on by default.
        ftp.cwd('/activepermits')
        self.logger.debug('Retrieving file %s', self.ftp_filename)
        ftp.retrbinary('RETR %s' % self.ftp_filename, f.write)
        self.logger.debug('Done downloading')
        f.seek(0)
        ftp.quit() # Note that we quit the connection before scraping starts -- otherwise we get a timeout!
        yield f
        f.close()

    def parse_list(self, csv_file):
        data = csv_file.read()
        csv_file.close()

        # Sometimes the data spans multiple lines without escaping the
        # newline character. In that case, remove the newlines. Note that this
        # regex assumes that EVERY value in the CSV is surrounded by double
        # quotes, which means this can't be used on any CSV.
        data = re.sub(r'([^"\s\\] *)(\r?\n)', r'\1 ', data)

        # The csv data sometimes includes unscaped double quotes, but is formatted
        # as a comma separated and double quoted file. This set of regexes
        # converts it into a tab delimited file with no quoting. This should
        # work in almost every case unless fields actually contain a comma
        # surrounded by double quotes.
        data = re.sub(r'","', r'\t', data)
        data = re.sub(r'"\r\n"', r'\n' , data)
        data = re.sub(r'^"', r'' , data)
        data = re.sub(r'"(?:\r\n)?$', r'' , data)

        csv_file = StringIO(data)

        reader = csv.DictReader(csv_file, self.csv_fieldnames, quoting=csv.QUOTE_NONE, delimiter='\t')
        for row in reader:
            yield row

    def existing_record(self, list_record):
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id)
            return qs.by_attribute(self.schema_fields['permit_number'], list_record['permit_number'])[0]
        except IndexError:
            return None

class ExcavationPermitScraper(BasePublicwayScraper):
    schema_slugs = ('excavation-permits',)
    ftp_filename = 'excavation.txt'
    # The CSV file contains the fieldnames on the first row:
    #     ("permit_number", "streetname", "Cross Street 1", "Cross Street 2",
    #      "Utility / Contractor", "Permit Reason", "Utility Type",
    #      "Effective Date", "Expiration Date", "Status", "cnn")
    csv_fieldnames = None

    def clean_list_record(self, record):
        record['clean_street_name'] = smart_title(remove_leading_zero(record['streetname']))
        record['clean_cross_1'] = smart_title(remove_leading_zero(record['Cross Street 1'].replace(' \ ', ' / ')))
        record['clean_cross_2'] = smart_title(remove_leading_zero(record['Cross Street 2'].replace(' \ ', ' / ')))
        record['Permit Reason'] = capfirst(record['Permit Reason'].lower()).replace('Cut off service', 'Cut-off service')
        record['Effective Date'] = parse_date(record['Effective Date'], '%Y-%m-%d %H:%M:%S')
        record['Expiration Date'] = parse_date(record['Expiration Date'], '%Y-%m-%d %H:%M:%S')
        return record

    def verbose_detail(self, detail):
        clean_cross_1 = smart_title(detail['cross_street_1'])
        clean_cross_2 = smart_title(detail['cross_street_2'])
        if detail.has_key('street_name'):
            clean_street_name = smart_title(detail['street_name'])
            return '%s from %s to %s' % (clean_street_name, clean_cross_1, clean_cross_2)
        else:
            return 'Intersection of %s and %s' % (clean_cross_1, clean_cross_2)

    def save(self, old_record, list_record, detail_record):
        contractor = self.get_or_create_lookup('utility_contractor', list_record['Utility / Contractor'], list_record['Utility / Contractor'])
        permit_reason = self.get_or_create_lookup('reason', list_record['Permit Reason'], list_record['Permit Reason'])
        item_date = list_record['Effective Date']

        if list_record['Cross Street 2'].lower() in ('intersection', ''):
            detail = {
                'cnn': list_record['cnn'],
                'cross_street_1': list_record['streetname'].upper(),
                'cross_street_2': list_record['clean_cross_1'].upper()
            }
        else:
            detail = {
                'cnn': list_record['cnn'],
                'street_name': list_record['streetname'].upper(),
                'cross_street_1': list_record['clean_cross_1'].upper(),
                'cross_street_2': list_record['clean_cross_2'].upper()
            }

        if old_record is None:
            location_details = {'cnn_list': [], 'details': [detail]}
        else:
            location_details = simplejson.loads(old_record.attributes['location_details'])
            cnn_list = set(location_details['cnn_list'])
            details = location_details['details']

            # pop any old cnn's from cnn_list
            if list_record['cnn'] in cnn_list:
                cnn_list.remove(list_record['cnn'])
            # replace any old detail with the new details
            for i, d in enumerate(details):
                if detail['cross_street_1'] == d['cross_street_1'] and \
                   detail['cross_street_2'] == d['cross_street_2'] and \
                   detail.get('street_name', True) == d.get('street_name', True):
                    del details[i]
                    details.insert(i, detail)
            location_details['cnn_list'] = list(cnn_list)

        streets = set()
        for detail in location_details['details']:
            if detail.has_key('street_name'):
                streets.add(detail['street_name'])
            else:
                streets.add(detail['cross_street_1'])

        location_name = ', '.join([smart_title(street) for street in streets])
        if len(location_name) > 150:
            location_name = VARIOUS

        description = '; '.join([self.verbose_detail(loc) for loc in location_details['details']])

        attributes = {
            'location_details': simplejson.dumps(location_details),
            'permit_number': list_record['permit_number'],
            'reason': permit_reason.id,
            'expiration_date': list_record['Expiration Date'],
            'utility_contractor': contractor.id,
        }
        if old_record is None:
            self.create_newsitem(
                attributes,
                title='%s received permit to %s' % (contractor.name, permit_reason.name.lower()),
                item_date=item_date,
                location=None, # we'll compute this at the end of self.update()
                location_name=location_name,
            )
        else:
            new_values = {'description': description, 'location_name': location_name}
            self.update_existing(old_record, new_values, attributes)

    def set_location(self, ni):
        """
        Calculates and sets the location for this NewsItem from the cnn_list.
        """
        geom_set = []

        location_details = simplejson.loads(ni.attributes['location_details'])

        for cnn in location_details['cnn_list']:
            try:
                geom = SfStreet.objects.get(cnn=cnn).location
            except SfStreet.DoesNotExist:
                pass
            else:
                geom_set.append(geom)

        for detail in location_details['details']:
            # Try using the cnn first, but for some reason the sf streets db
            # doesn't have *every* cnn.
            if detail.has_key('cnn'):
                try:
                    geom = SfStreet.objects.get(cnn=detail['cnn']).location
                except SfStreet.DoesNotExist:
                    pass
                else:
                    geom_set.append(geom)

            # If the cnn wasn't found, and this is an itersection, try getting
            # the location by cross streets.
            else:
                geom = SfStreet.objects.get_intersection(
                    detail['cross_street_1'],
                    detail['cross_street_2']
                )
                if geom:
                    geom_set.append(geom)

        geom = line_merge(make_geomcoll(geom_set))
        if not geom.empty:
            ni.location = geom
        else:
            self.logger.debug('got an empty geometry from list of geoms: %r' % geom_set)

    def update(self):
        super(ExcavationPermitScraper, self).update()
        for start, end, total, qs in queryset.batch(NewsItem.objects.filter(schema=self.schema)):
            self.logger.debug("Updating location for %s - %s of %s" % (start, end, total))
            for ni in qs:
                self.set_location(ni)
                ni.save()

class StreetSpacePermitScraper(BasePublicwayScraper):
    schema_slugs = ('street-space-permits',)
    ftp_filename = 'StreetSpace.txt'
    csv_fieldnames = ('permit_number', 'Address', 'Cross Street 1', 'Cross Street 2', 'Description', 'From Date', 'To Date', 'Linear Feet', 'cnn')

    def clean_list_record(self, record):
        record['From Date'] = parse_date(record['From Date'], '%Y-%m-%d %H:%M:%S')
        record['To Date'] = parse_date(record['To Date'], '%Y-%m-%d %H:%M:%S')
        if record['From Date'] is None and isinstance(record['To Date'], datetime.date):
            record['From Date'] = record['To Date']
        elif record['To Date'] is None and isinstance(record['From Date'], datetime.date):
            record['To Date'] = record['From Date']
        record['Description'] = record['Description'].strip()
        return record

    def save(self, old_record, list_record, detail_record):
        if old_record is not None:
            return # Records don't change, so skip.
        location = SfStreet.objects.get_intersection_by_cnn(list_record['cnn'])
        attributes = {
            'permit_number': list_record['permit_number'],
            'description': list_record['Description'],
            'to_date': list_record['To Date'],
        }
        self.create_newsitem(
            attributes,
            title='Street space permit at %s' % list_record['Address'],
            item_date=list_record['From Date'],
            location=location,
            location_name=list_record['Address'],
        )

STREET_USE_PERMIT_TYPES = {
    'TempOccup': 'Temporary Occupancy',
    'OverwideDr': 'Overwide Driveway',
    'TankRemove': 'Underground Tank Removal',
    'PipeBarr': 'Sidewalk Pipe Barrier',
    'Boring': 'Boring/Monitoring Well',
    'Vault': 'Sidewalk Vault Encroachment',
    'SideSewer': 'Excavation - Side Sewer',
    'ExcStreet': 'General Street Excavation',
    'SpecSide': 'Special Sidewalk Surface',
    'StrtImprov': 'Street Improvement Excavation',
    'MinorEnc': 'Minor Sidewalk Encroachment',
    'minorenc': 'Minor Sidewalk Encroachment',
}

class StreetUsePermitScraper(BasePublicwayScraper):
    schema_slugs = ('street-use-permits',)
    ftp_filename = 'streetuse.txt'
    csv_fieldnames = ('permit_number', 'Location', 'Cross Street 1', 'Cross Street 2', 'Permit Type', 'Agent', 'Permit Purpose', 'Approved Date', 'Status', 'cnn')

    def clean_list_record(self, record):
        record['Approved Date'] = parse_date(record['Approved Date'], '%Y-%m-%d %H:%M:%S')
        record['clean_location'] = smart_title(remove_leading_zero(record['Location'])).strip()
        record['clean_cross_1'] = smart_title(remove_leading_zero(record['Cross Street 1'].replace(' \ ', ' / ')))
        record['clean_cross_2'] = smart_title(remove_leading_zero(record['Cross Street 2'].replace(' \ ', ' / ')))
        record['Agent'] = record['Agent'] and record['Agent'].strip() or 'Unknown agent'
        record['Permit Type'] = record['Permit Type'] or 'N/A'
        try:
            record['Permit Type'] = STREET_USE_PERMIT_TYPES[record['Permit Type']]
        except KeyError:
            pass
        return record

    def save(self, old_record, list_record, detail_record):
        if list_record['Approved Date'] is None:
            return
        agent = self.get_or_create_lookup('agent', list_record['Agent'], list_record['Agent'], make_text_slug=False)
        permit_type = self.get_or_create_lookup('permit_type', list_record['Permit Type'], list_record['Permit Type'], make_text_slug=False)
        item_date = list_record['Approved Date']

        if old_record is None:
            cnn_list = set([list_record['cnn']])
            location_name_list = set([list_record['clean_location']])
        else:
            json = simplejson.loads(old_record.attributes['json'])
            cnn_list = set(json['cnn_list'])
            location_name_list = set(json['locations'])
            if list_record['clean_location'] and list_record['clean_location'] not in location_name_list:
                location_name_list.add(list_record['clean_location'])
            if list_record['cnn'] not in cnn_list:
                cnn_list.add(list_record['cnn'])
        location_name = ', '.join(list(location_name_list))
        if len(location_name) > 150:
            location_name = VARIOUS

        json = simplejson.dumps({
            'cnn_list': [str(cnn) for cnn in list(cnn_list)],
            'purpose': list_record['Permit Purpose'].decode('latin-1'),
            'locations': list(location_name_list)
        })
        attributes = {
            'permit_number': list_record['permit_number'],
            'permit_type': permit_type.id,
            'agent': agent.id,
            'json': json
        }
        if old_record is None:
            self.create_newsitem(
                attributes,
                title='%s received permit for %s' % (agent.name, permit_type.name.lower()),
                item_date=item_date,
                location=None, # we'll compute this at the end of self.update()
                location_name=location_name,
            )
        else:
            new_values = {'location_name': location_name}
            self.update_existing(old_record, new_values, attributes)

    def set_location(self, ni):
        """
        Calculates and sets the location for this NewsItem from the cnn_list.
        """
        geom_set = []
        for cnn in simplejson.loads(ni.attributes['json'])['cnn_list']:
            try:
                geom = SfStreet.objects.get(cnn=cnn).location
            except SfStreet.DoesNotExist:
                pass
            else:
                geom_set.append(geom)

        geom = line_merge(make_geomcoll(geom_set))
        if not geom.empty:
            ni.location = geom
        else:
            self.logger.debug('got an empty geometry from list of geoms: %r' % geom_set)

    def update(self):
        super(StreetUsePermitScraper, self).update()
        for start, end, total, qs in queryset.batch(NewsItem.objects.filter(schema=self.schema)):
            self.logger.debug("Updating location for %s - %s of %s" % (start, end, total))
            for ni in qs:
                self.set_location(ni)
                ni.save()

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    ExcavationPermitScraper().update()
    StreetSpacePermitScraper().update()
    StreetUsePermitScraper().update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Screen scraper for SF restaurant data
https://dph-tumble1.sfdph.org/
Username and password are in the code below.

This data is provided via Microsoft Access files behind a password-protected
file-administration interface, so the two interesting tasks with this scraper
are to screen-scrape that interface and parse the Access file.
"""

from ebdata.parsing.mdb import TableReader
from ebdata.retrieval.scrapers.base import ScraperBroken
from ebdata.retrieval.scrapers.list_detail import SkipRecord
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date
import re
import urlparse

USERNAME = ''
PASSWORD = ''
ENCODING = 'iso-8859-1' # Encoding of the data in the MDB file.
FILE_MANAGER_URL = 'https://dph-tumble1.sfdph.org/'

class RestaurantScraper(NewsItemListDetailScraper):
    schema_slugs = ('restaurant-inspections',)
    has_detail = False

    def __init__(self, mdb_filename=None):
        # If mdb_filename is given, it should be the name of an MDB file on the
        # local filesystem to import. Otherwise, this will try to find the
        # latest one available online.
        NewsItemListDetailScraper.__init__(self)
        self._local_mdb_filename = mdb_filename
        self._mdb_filename = None
        self._locations_cache = self._inspection_type_cache = self._violations_cache = self._violation_type_cache = None

    def mdb_filename(self):
        """
        Lazily loads the MDB filename so it's only done once per scraper
        instance.
        """
        if self._mdb_filename is None:
            if self._local_mdb_filename is None:
                self._mdb_filename = self.get_access_db()
            else:
                self._mdb_filename = self._local_mdb_filename
        return self._mdb_filename

    def locations(self):
        """
        Lazily loads *all* locations into memory and returns a dictionary
        keyed by location ID.
        """
        if self._locations_cache is None:
            self._locations_cache = dict([(row['LocationID'], row) for row in self.mdb_table('tblLocations')])
            if not self._locations_cache:
                raise ScraperBroken('tblLocations was either empty or nonexistent')
        return self._locations_cache

    def inspection_types(self):
        """
        Lazily loads *all* inspection types into memory and returns a dictionary
        keyed by inspection type ID.
        """
        if self._inspection_type_cache is None:
            self._inspection_type_cache = dict([(row['InspectionTypeID'], row) for row in self.mdb_table('tblInspectionTypes')])
            if not self._inspection_type_cache:
                raise ScraperBroken('tblInspectionTypes was either empty or nonexistent')
        return self._inspection_type_cache

    def violations(self):
        """
        Lazily loads *all* violations into memory and returns a dictionary
        keyed by inspection ID.
        """
        if self._violations_cache is None:
            vs = {}
            for row in self.mdb_table('tblViolations'):
                vs.setdefault(row['InspectionID'], []).append(row)
            self._violations_cache = vs
            if not self._violations_cache:
                raise ScraperBroken('tblViolations was either empty or nonexistent')
        return self._violations_cache

    def violation_types(self):
        """
        Lazily loads *all* violation types into memory and returns a dictionary
        keyed by violation type ID.
        """
        if self._violation_type_cache is None:
            self._violation_type_cache = dict([(row['ViolationTypeID'], row) for row in self.mdb_table('tblViolationTypes')])
            if not self._violation_type_cache:
                raise ScraperBroken('tblViolationTypes was either empty or nonexistent')
        return self._violation_type_cache

    def mdb_table(self, table_name):
        "Returns a TableReader instance for the given table name."
        return TableReader(self.mdb_filename(), table_name)

    def get_access_db(self, file_date=None):
        """
        Downloads the requested Microsoft Access file, saves it to a temporary
        file and returns the local file name.

        If file_date is None, then this will download the latest Access file.
        Otherwise, it will download the file with the given date, raising
        ScraperBroken if a file isn't available for that date.
        """
        # First, log into the file manager and get the list of all available
        # Microsoft Access (MDB) files.
        params = {'user': USERNAME, 'password': PASSWORD, 'start-url': '/', 'switch': 'Log In'}
        html = self.get_html(FILE_MANAGER_URL, params)
        mdb_files = re.findall(r'PrintFileURL\("(.*?\.mdb)"', html)
        if not mdb_files:
            raise ScraperBroken('Found no MDB files')
        mdb_files.sort()

        if file_date:
            requested_file = 'SFFOOD%s.mdb' % file_date.strftime('%m%d%Y')
            if requested_file not in mdb_files:
                raise ScraperBroken('%r not found. Choices are: %r' % (requested_file, mdb_files))
        else:
            # Assume the last filename in alphabetical order is the latest one.
            requested_file = mdb_files[-1]

        # Finally, download the file and return the local filename.
        mdb_url = urlparse.urljoin(FILE_MANAGER_URL, requested_file)
        filename = self.retriever.get_to_file(mdb_url)
        self.logger.debug('%s saved to %s', mdb_url, filename)
        return filename

    def list_pages(self):
        for row in self.mdb_table('tblInspections'):
            yield row

    def parse_list(self, row):
        yield row # It's already a dictionary, so just yield it.

    def clean_list_record(self, record):
        record['DateOfInspection'] = parse_date(record['DateOfInspection'], '%Y-%m-%d')
        if not record['DateOfInspection']:
            raise SkipRecord('Inspection date not given')
        record['Total Time'] = record['Total Time'].strip() or None
        record['EmployeeIDofInspector'] = record['EmployeeIDofInspector'].strip() or None

        # Calculate the score range.
        record['Score'] = record['Score'].strip() or None
        if record['Score']:
            try:
                record['Score'] = int(record['Score'])
            except ValueError:
                raise SkipRecord('Got sketchy non-integer score %r' % record['Score'])
            if record['Score'] >= 91:
                record['score_range'] = '91-100'
            elif record['Score'] >= 81:
                record['score_range'] = '81-90'
            elif record['Score'] >= 71:
                record['score_range'] = '71-80'
            elif record['Score'] >= 61:
                record['score_range'] = '61-70'
            elif record['Score'] >= 51:
                record['score_range'] = '51-60'
            elif record['Score'] >= 41:
                record['score_range'] = '41-50'
            elif record['Score'] >= 31:
                record['score_range'] = '31-40'
            elif record['Score'] >= 21:
                record['score_range'] = '21-30'
            elif record['Score'] >= 11:
                record['score_range'] = '11-20'
            else:
                record['score_range'] = '0-10'
        else:
            record['score_range'] = 'N/A'

        # Get the location data from the locations table.
        try:
            loc = self.locations()[record['LocationIDInspected']]
        except KeyError:
            raise SkipRecord('Location not found')
        # Explicitly convert to a Unicode object using the utf8 codec, because
        # this might contain funky characters.
        record['restaurant_dba'] = loc['DBA'].decode('utf8').strip()
        record['restaurant_type'] = loc['Type Description'].strip()
        record['address'] = loc['StreetAddress'].decode('utf8').strip()
        record['LocationIDInspected'] = int(record['LocationIDInspected'])

        # Get the inspection type data from the inspection types table.
        try:
            inspection_type = self.inspection_types()[record['InspectionTypeID']]
        except KeyError:
            raise SkipRecord('Inspection type not found')
        record['inspection_type'] = inspection_type['InspectionType']

        # Get the violation data from the violations table.
        try:
            vios = self.violations()[record['InspectionID']]
        except KeyError:
            record['violations'] = []
        else:
            vio_types = self.violation_types()
            record['violations'] = [dict(vio, type=vio_types[vio['ViolationTypeID']]) for vio in vios]

        return record

    def existing_record(self, record):
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id)
            qs = qs.by_attribute(self.schema_fields['inspection_id'], record['InspectionID'])
            return qs[0]
        except IndexError:
            return None

    def save(self, old_record, list_record, detail_record):
        restaurant_type = self.get_or_create_lookup('restaurant_type', list_record['restaurant_type'], list_record['restaurant_type'], make_text_slug=False)
        inspection_type = self.get_or_create_lookup('inspection_type', list_record['inspection_type'], list_record['inspection_type'])
        score_range = self.get_or_create_lookup('score_range', list_record['score_range'], list_record['score_range'])
        violations = [self.get_or_create_lookup('violation', v['type']['ViolationType'], v['type']['ViolationType']) for v in list_record['violations']]
        violations_text = ','.join([str(v.id) for v in violations])
        item_date = list_record['DateOfInspection']
        title = u'%s inspection at %s' % (inspection_type.name, list_record['restaurant_dba'])

        # Create the notes by appending the violation-specific notes to the
        # inspection notes.
        notes = list_record['InspectionNotes'].decode('utf8')
        for vio in list_record['violations']:
            if vio['ViolationNotes'].strip():
                notes += u'\n\nNote on "%s" violation: %s' % (vio['type']['ViolationType'].decode('utf8').strip(), vio['ViolationNotes'].decode('utf8').strip())
        notes = notes.strip()

        new_attributes = {
            'inspection_id': list_record['InspectionID'],
            'total_time': list_record['Total Time'],
            'inspector': list_record['EmployeeIDofInspector'],
            'location': list_record['LocationIDInspected'],
            'restaurant_dba': list_record['restaurant_dba'],
            'restaurant_type': restaurant_type.id,
            'inspection_type': inspection_type.id,
            'violation': violations_text,
            'score': list_record['Score'],
            'score_range': score_range.id,
            'notes': notes,
        }

        if old_record is None:
            self.create_newsitem(
                new_attributes,
                title=title,
                item_date=item_date,
                location_name=list_record['address'],
            )
        else:
            # This already exists in our database, but check whether any
            # of the values have changed.
            new_values = {'title': title, 'item_date': item_date, 'location_name': list_record['address']}
            self.update_existing(old_record, new_values, new_attributes)

class CsvRestaurantScraper(RestaurantScraper):
    """
    A RestaurantScraper that gets its data from a directory of CSV files
    instead of an MDB file.

    This is useful for development in an environment that doesn't have
    mdbtools.

    CsvRestaurantScraper.__init__() takes a directory name. This directory
    must contain a CSV file for every table in the MDB file you're trying to
    emulate, named 'TABLENAME.txt'.
    """
    def __init__(self, mdb_directory):
        RestaurantScraper.__init__(self)
        self.mdb_directory = mdb_directory

    def mdb_table(self, table_name):
        "Returns a TableReader instance for the given table name."
        import csv
        import os.path
        csv_name = os.path.join(self.mdb_directory, table_name + '.txt')
        return csv.DictReader(open(csv_name))

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    RestaurantScraper().update()

########NEW FILE########
__FILENAME__ = harvester
import re

def regex_tagger(regex, field_name, index=None, classes=[]):
    def tag_data(data):
        classes.extend(['locationdetected', 'field-%s' % index])
        pre = '<a href="#" class="%s" field="%s-input">' % (' '.join(classes), field_name)
        post = '</a>'
        return re.sub(regex, lambda m: pre + m.group(0) + post, data)
    return tag_data

def lookup_tagger(field, index=None, classes=[]):
    from ebdata.nlp.places import phrase_tagger
    def tag_data(data):
        phrases = [p['name'] for p in field.lookup_set.values('name')]
        classes.extend(['locationdetected', 'field-%s' % index])
        pre = '<a href="#" class="%s" field="%s-input">' % (' '.join(classes), field.name)
        post = '</a>'
        tag_phrases = phrase_tagger(phrases, pre, post)
        return tag_phrases(data)
    return tag_data

class SFZoningAgendasExtractor(object):
    fields = (
        {'name': 'description', 'required': True},
        {'name': 'agenda_item', 'required': True, 'persist': True},
        {'name': 'case_number', 'required': True},
        {'name': 'action_requested', 'required': True},
        {'name': 'preliminary_recommendation', 'required': False},
    )

    def get_title(self, location, attrs):
        return '%s at %s' % (attrs['action_requested'].name, location)

    def get_description(self, location, attrs):
        return ''

    def extract_data(self, blob):
        from lxml.etree import tostring
        from lxml.html import document_fromstring
        from ebdata.textmining.treeutils import preprocess

        tree = document_fromstring(blob.html).xpath("//div[@id='contents']")[0]
        tree = preprocess(tree,
            drop_tags=('a', 'area', 'b', 'center', 'font', 'form', 'img', 'input', 'map', 'small', 'span', 'sub', 'sup', 'topic', 'u'),
            drop_trees=('applet', 'button', 'embed', 'iframe', 'object', 'select', 'textarea'),
            drop_attrs=('background', 'border', 'cellpadding', 'cellspacing', 'class', 'clear', 'id', 'rel', 'style', 'target'))
        html = tostring(tree, method='html')
        # Remove non breaking spaces (&nbsp; and &#160;) so tagging regexes
        # can be less complicated
        return html.replace('&nbsp;', ' ').replace('&#160;', ' ')

    def tag_data(self, data):
        data = self.tag_addresses(data)
        for f in self.fields:
            tagger = getattr(self, 'tag_%s' % f['name'], None)
            if tagger is None:
                continue
            data = tagger(data)
        return data

    def tag_addresses(self, data):
        regex = r'(?i)(\d+[\b|\s+\-\s+][\d\w\s]+\b)(STREET|AVENUE|BOULEVARD|DRIVE)'
        tag_data = regex_tagger(regex, 'location', 1, classes=['location'])
        return tag_data(data)

    def get_date(self, blob):
        """
        Return the date to be used for a NewsItem.
        """
        from ebpub.utils.dates import parse_date
        # Dates are generally in a format like January 15, 2008, and sometimes
        # followed by other text. Pull out the date part so we can parse it.
        date_match = re.search(r'(\w+\s*\d{1,2}\s*,\s*\d{4})', blob.title)
        if date_match is None:
            return ''
        else:
            date_string = date_match.group(1)
            return parse_date(date_string, '%B %d, %Y')

    def tag_agenda_item(self, data):
        from ebpub.db.models import SchemaField
        field = SchemaField.objects.get(schema__slug='zoning-minutes', name='agenda_item')
        tag_phrases = lookup_tagger(field, 0)
        return tag_phrases(data)

    def tag_case_number(self, data):
        regex = r'(\d{4}\.[\w\d]{4,})'
        tag_data = regex_tagger(regex, 'case_number', 1)
        return tag_data(data)

    def tag_action_requested(self, data):
        from ebpub.db.models import SchemaField
        field = SchemaField.objects.get(schema__slug='zoning-minutes', name='action_requested')
        tag_phrases = lookup_tagger(field, 0)
        return tag_phrases(data)

    def tag_preliminary_recommendation(self, data):
        regex = r'(?i)(Preliminary Recommendation:\s*)([\w\s]+)(\n*<)' # still not working all the time
        pre = '<a href="#" class="locationdetected" field="preliminary_recommendation-input">'
        post = '</a>'
        def _temp2(match):
            return match.group(1) + pre + match.group(2) + post + match.group(3)
        return re.sub(regex, _temp2, data)

class SFZoningMinutesExtractor(SFZoningAgendasExtractor):
    fields = (
        {'name': 'description', 'required': True},
        {'name': 'agenda_item', 'required': True, 'persist': True},
        {'name': 'case_number', 'required': True},
        {'name': 'action_requested', 'required': True},
        {'name': 'preliminary_recommendation', 'required': False},
        {'name': 'action', 'required': False},
    )

    def tag_action(self, data):
        regex = r'(?i)(ACTION:\s*)([\w\s]+)(\n*<)'
        pre = '<a href="#" class="locationdetected" field="action-input">'
        post = '</a>'
        def _temp2(match):
            return match.group(1) + pre + match.group(2) + post + match.group(3)
        return re.sub(regex, _temp2, data)

########NEW FILE########
__FILENAME__ = new_retrieval
"""
Screen scraper for SF zoning changes.
http://sfgov.org/site/planning_meeting.asp?id=15840

"""

import time
from datetime import datetime
from lxml.html import document_fromstring
from ebdata.blobs.models import Blob
from ebdata.retrieval import UnicodeRetriever
from ebpub.db.models import Schema

class ZoningUpdater(object):
    def __init__(self):
        self.url = 'http://sfgov.org/site/planning_meeting.asp?id=15840'
        self.retriever = UnicodeRetriever()
        self.delay = 2

    def update(self):
        for year in self.get_years(self.url):
            self.update_year(year['url'])

    def get_years(self, url):
        html = self.retriever.get_html(url)
        t = document_fromstring(html)
        for a in t.xpath("//table[@id='Table4']//a"):
            year_url = 'http://sfgov.org/site/planning_meeting.asp%s' % a.get('href')[:-8]
            yield {'url': year_url, 'year': a.text}

    def update_year(self, url):
        minutes_schema = Schema.objects.get(slug='zoning-minutes')
        agendas_schema = Schema.objects.get(slug='zoning-agenda')
        for page in self.get_minutes(url):
            self.save_page(page, minutes_schema)
        for page in self.get_agendas(url):
            self.save_page(page, agendas_schema)

    def get_minutes(self, url):
        return self._helper(url, 'Minutes')

    def get_agendas(self, url):
        return self._helper(url, 'Agendas')

    def _helper(self, url, item_type):
        html = self.retriever.get_html(url)
        t = document_fromstring(html)
        for a in t.xpath("//a[@name='%s']/parent::td/parent::tr/following-sibling::*[4]//a" % item_type):
            if '(cancellation notice)' in a.text.lower():
                continue
            url = 'http://sfgov.org/site/%s' % a.get('href')
            yield {'title': a.text, 'url': url}

    def save_page(self, page, schema):
        url = page['url']
        # If we've already retrieved the page, there's no need to retrieve
        # it again.
        try:
            Blob.objects.filter(url=url)[0]
        except IndexError:
            pass
        else:
            #self.logger.debug('URL %s has already been retrieved', url)
            return

        # Fetch the html for the page and save it
        html = self.retriever.get_html(url + '&mode=text')
        b = Blob(
            schema=schema,
            title=page['title'],
            url=url,
            html=html,
            is_pdf=False,
            when_crawled=datetime.now(),
            has_addresses=None,
            when_geocoded=None,
            geocoded_by=''
        ).save()

        time.sleep(self.delay)

def update():
    s = ZoningUpdater()
    s.update()

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Screen scraper for SF zoning changes.
http://sfgov.org/site/planning_meeting.asp?id=15840

Input is a manually created text file in this format:

    http://sfgov.org/site/planning_page.asp?id=73133
    2008-01-10
    CONSIDERATION OF ITEMS PROPOSED FOR CONTINUANCE
    2007.0718C
    507 Columbus Avenue
    507 Columbus Avenue - west side between Union and Green Streets, Lot 005 in Assessors Block 0117 - Request for Conditional Use Authorization to establish a retail wine store and a bar (dba Vino Divino) of approximately 807 square feet within the vacant, existing ground-floor commercial space.  No physical expansion of the existing building is proposed.  The bar portion of the proposal is intended to be a wine bar which will sell beer and wine for consumption on- site with the retail wine store portion of the business selling beer and wine for consumption off-site. This site is within the North Beach Neighborhood Commercial District, and a 40-X Height and Bulk District.
    Request for Conditional Use Authorization
    Proposed for Continuance to January 17, 2008
"""

from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.utils.dates import parse_date

class ZoningScraper(NewsItemListDetailScraper):
    schema_slugs = ('zoning-agenda',)
    has_detail = False

    def __init__(self, text, *args, **kwargs):
        super(ZoningScraper, self).__init__(*args, **kwargs)
        self.text = text.decode('utf8')

    def list_pages(self):
        yield self.text

    def parse_list(self, text):
        for chunk in text.strip().split('\n\n'):
            yield chunk.split('\n')

    def existing_record(self, record):
        # Assume this is always being input manually.
        return None

    def save(self, old_record, list_record, detail_record):
        agenda_item = self.get_or_create_lookup('agenda_item', list_record[2], list_record[2], make_text_slug=False)
        action_requested = self.get_or_create_lookup('action_requested', list_record[6], list_record[6], make_text_slug=False)
        title = '%s at %s' % (action_requested.name, list_record[4])
        attributes = {
            'case_number': list_record[3],
            'agenda_item': agenda_item.id,
            'description': list_record[5],
            'action_requested': action_requested.id,
            'preliminary_recommendation': list_record[7],
        }
        self.create_newsitem(
            attributes,
            title=title,
            url=list_record[0],
            item_date=parse_date(list_record[1], '%Y-%m-%d'),
            location_name=list_record[4],
        )

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    import sys
    text = open(sys.argv[1]).read()
    s = ZoningScraper(text)
    s.update()

########NEW FILE########
__FILENAME__ = settings
import os

DEBUG = True

SHORT_NAME = 'chicago'

DATABASE_ENGINE = 'postgresql_psycopg2'
DATABASE_USER = ''
DATABASE_HOST = ''
DATABASE_NAME = SHORT_NAME

INSTALLED_APPS = (
    'django.contrib.humanize',
    'ebpub.db',
    'everyblock.staticmedia',
)

TEMPLATE_DIRS = (
    os.path.normpath(os.path.join(os.path.dirname(__file__), 'templates')),
)

ROOT_URLCONF = 'everyblock.urls'

# See ebpub.settings for how to configure METRO_LIST
METRO_LIST = (
)

EB_MEDIA_ROOT = os.path.normpath(os.path.join(os.path.dirname(__file__), 'media'))
EB_MEDIA_URL = ''

AUTOVERSION_STATIC_MEDIA = False

########NEW FILE########
__FILENAME__ = remove-duplicates
from ebpub.db.models import Schema, NewsItem, Lookup

schema = Schema.objects.get(slug='liquor-licenses')
schema_fields = {}
for f in schema.schemafield_set.all():
    schema_fields[f.name] = f

# Run over the newsitems in reverse id order, if any duplicated were found for
# the current newsitem, delete the newsitem. This way, we will look at each
# newsitem once, and nothing will be deleted before we get to it in the loop.

for ni in NewsItem.objects.filter(schema=schema).order_by('-id').iterator():
    qs = NewsItem.objects.filter(schema=schema, item_date=ni.item_date).exclude(id=ni.id)
    qs = qs.by_attribute(schema_fields['page_id'], ni.attributes['page_id'])
    qs = qs.by_attribute(schema_fields['type'], ni.attributes['type'])

    record_type = Lookup.objects.get(pk=ni.attributes['record_type'])

    if record_type.code == 'STATUS_CHANGE':
        qs = qs.by_attribute(schema_fields['old_status'], ni.attributes['old_status'])
        qs = qs.by_attribute(schema_fields['new_status'], ni.attributes['new_status'])
    else:
        qs = qs.by_attribute(schema_fields['action'], ni.attributes['action'])

    duplicate_count = qs.count()
    if duplicate_count > 0:
        ni.delete()
        print "Deleting %s because %s duplicates were found." % (ni, duplicate_count)
########NEW FILE########
__FILENAME__ = retrieval
"""
Screen scraper for California liquor-license data.
http://www.abc.ca.gov/datport/SubscrMenu.asp
"""

from ebdata.retrieval.scrapers.base import ScraperBroken
from ebdata.retrieval.scrapers.list_detail import SkipRecord
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date
import re

GEOCODES = {
    'la': 1993,
    'sf': 3800,
    'sanjose': 4313,
}

license_types_re = re.compile(r'(?si)<b>\d\) License Type:</b> (?P<license_type>.*?)</td></tr><tr><td><b>\s*License Type Status:</b>  (?P<license_type_status>.*?)</td></tr><tr><td><b>\s*Status Date: </b> (?P<status_date>.*?) <b>\s*Term: (?P<term>.*?)</td></tr><tr><td><b>\s*Original Issue Date: </b> (?P<original_issue_date>.*?) <b>\s*Expiration Date: </b> (?P<expiration_date>.*?) </tr><tr><td><b>\s*Master: </b> (?P<master>.*?)\s*<b>\s*Duplicate: </b> (?P<duplicate>.*?) <b>\s*Fee Code: </b> (?P<fee_code>.*?)</td></tr>')

detail_url = lambda page_id: 'http://www.abc.ca.gov/datport/LQSdata.asp?ID=%s' % page_id

class LiquorLicenseScraper(NewsItemListDetailScraper):
    """
    A base class that encapsulates how to scrape California liquor licenses.

    Do not instantiate this class directly; use one of the subclasses.
    """
    schema_slugs = ('liquor-licenses',)
    has_detail = True
    parse_detail_re = re.compile(r'(?si)License Number:  </b>.*? <b>\s*Status:  </b>(?P<status>.*?)</td></tr><tr><td><b>Primary Owner:  </b>(?P<primary_owner>.*?)</td></tr><td><b>ABC Office of Application:  </b>(?P<office_of_application>.*?)</td></tr></tr><tr><td bgcolor=#260066 class=header><font color=white> <b>Business Name </font></b></td></tr>(?P<business_name>.*?)<tr><td bgcolor=#260066 class=header><font color=white> <b>Business Address </font></b></td><td bgcolor=#260066 class=header></td></tr><tr><td><b>Address: </b>(?P<address>.*?)<b>Census Tract: </b>(?P<census_tract>.*?)</td></tr><tr><td><b>City: </b>(?P<city>.*?)     <b>County: </b>(?P<county>.*?)</td></tr><tr><td><b>State: </b>(?P<state>.*?)     <b>Zip Code: </b>(?P<zipcode>.*?)</td></tr><tr><td bgcolor=#260066 class=header><font color=white> <b>Licensee Information </font></b></td></tr><tr><td><b>Licensee: </b>.*?</td></tr><tr><td bgcolor=#260066 class=header><font color=white> <B>License Types </font></b></td></tr><tr><td>(?P<license_types>.*?)<tr><td bgcolor=#260066 class=header><font color=white> <b>Current Disciplinary Action </font>')

    def parse_list(self, page):
        page = page.replace('&nbsp;', ' ')

        # First, get the report date by looking for "Report as of XXXX".
        m = re.search(r'(?i)report as of (\w+ \d\d?, \d\d\d\d)</U>', page)
        if not m:
            raise ScraperBroken('Could not find "Report as of" in page')
        report_date = parse_date(m.group(1), '%B %d, %Y')

        # Determine the headers by looking at the <th> tags, and clean them up
        # to match our style for keys in the list_record dictionary (lower
        # case, underscores instead of spaces).
        headers = [h.lower() for h in re.findall('(?i)<th[^>]*>(?:<a[^>]+>)?\s*(.*?)\s*(?:</a>)?</th>', page)]
        headers = [h.replace('<br>', ' ') for h in headers]
        headers = [re.sub(r'[^a-z]', ' ', h) for h in headers]
        headers = [re.sub(r'\s+', '_', h.strip()) for h in headers]

        # Dynamically construct a regex based on the number of headers.
        # Note that this assumes that at most *one* of the headers has an
        # empty name; if more than one header has an empty name, this regex
        # will have multiple named groups with the same name, which will cause
        # an error.
        pattern = '(?si)<tr valign=top class=report_column>%s</tr>'% '\s*'.join(['\s*<td[^>]*>\s*(?:<center>)?\s*(?P<%s>.*?)\s*(?:</center>)?\s*</td[^>]*>\s*' % (h or 'number') for h in headers])
        for record in re.finditer(pattern, page):
            yield dict(record.groupdict(), report_date=report_date)

    def clean_list_record(self, record):
        try:
            license_number = record.pop('license_num')
        except KeyError:
            license_number = record.pop('license_number')
        m = re.search(r'(?i)<a href=.*?LQSdata\.asp\?ID=(\d+)>\s*(\d+)\s*</a>', license_number)
        if not m:
            raise ScraperBroken('License number link not found in %r' % license_number)
        record['place_id'], record['license_number'] = m.groups()
        return record

    def get_detail(self, record):
        url = detail_url(record['place_id'])
        return self.get_html(url)

    def parse_detail(self, page, list_record):
        # They use a ton of &nbsp;s for some reason, so convert them to spaces
        # to make the parse_detail_re regex more readable.
        page = page.replace('&nbsp;', ' ')
        return NewsItemListDetailScraper.parse_detail(self, page, list_record)

    def clean_detail_record(self, record):
        if 'No Active DBA found' in record['business_name']:
            record['business_name'] = ''
        else:
            m = re.search(r'(?si)<tr><td><b>Doing Business As: </b>(.*?)</td></tr>', record['business_name'])
            if not m:
                raise ScraperBroken('Got unknown business_name value %r' % record['business_name'])
            record['business_name'] = m.group(1)
        record['address'] = record['address'].strip()

        # There can be multiple license types, so this requires further parsing
        # to create a list.
        license_types = []
        for m in license_types_re.finditer(record['license_types']):
            d = m.groupdict()
            d['status_date'] = parse_date(d['status_date'], '%d-%b-%Y')
            if not d['status_date']:
                # Skip license types that don't have a status date, because
                # a NewsItem is required to have an item_date, and we don't
                # care about licenses that don't have a change date.
                continue
            d['original_issue_date'] = parse_date(d['original_issue_date'], '%d-%b-%Y')
            d['expiration_date'] = parse_date(d['expiration_date'], '%d-%b-%Y')
            d['term'] = d['term'].replace('</B>', '').strip()
            license_types.append(d)
        record['license_types'] = license_types

        return record

    def existing_record(self, record):
        # We don't have enough information from the list_record to determine
        # whether this record exists.
        return None

    def detail_required(self, list_record, old_record):
        # Always download the detail page.
        return True

    def save(self, old_record, list_record, detail_record):
        # Each status change only applies to a single license type (e.g.
        # "Winegrower"). The list page says which license type we're interested
        # in, but only the detail page has the description, so we have to use
        # one to look up the other.
        try:
            license = [t for t in detail_record['license_types'] if t['license_type'][:2] == list_record['type']][0]
        except IndexError:
            raise ScraperBroken('License type %r not found on detail page' % list_record['type'])

        license_type = self.get_or_create_lookup('type', license['license_type'][5:], list_record['type'])
        status = self.get_or_create_lookup('status', license['license_type_status'], license['license_type_status'])
        if not list_record.has_key('action'):
            list_record['action'] = '' # Status changes do not have actions
        action = self.get_or_create_lookup('action', list_record['action'], list_record['action'])

        if self.record_type.code == 'STATUS_CHANGE':
            old_status = self.get_or_create_lookup('old_status', list_record['status_from'], list_record['status_from'])
            new_status = self.get_or_create_lookup('new_status', list_record['status_to'], list_record['status_to'])
        else:
            # New licesnses and new application have no old status.
            old_status = self.get_or_create_lookup('old_status', 'None', 'NONE')
            new_status = self.get_or_create_lookup('new_status', list_record['status'], list_record['status'])

        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id, item_date=list_record['report_date'])
            qs = qs.by_attribute(self.schema_fields['page_id'], list_record['place_id'])
            qs = qs.by_attribute(self.schema_fields['type'], license_type.id)

            if self.record_type.code == 'STATUS_CHANGE':
                qs = qs.by_attribute(self.schema_fields['old_status'], old_status.id)
                qs = qs.by_attribute(self.schema_fields['new_status'], new_status.id)
            else:
                qs = qs.by_attribute(self.schema_fields['action'], action.id)

            old_record = qs[0]
        except IndexError:
            pass
        else:
            return # No need to save again, if this record already exists.

        title = '%s for %s' % (self.record_type.name, detail_record['business_name'] or detail_record['primary_owner'])

        attributes = {
            'page_id': list_record['place_id'],
            'address': detail_record['address'],
            'business_name': detail_record['business_name'],
            'original_issue_date': license['original_issue_date'],
            'expiration_date': license['expiration_date'],
            'type': license_type.id,
            'status': status.id,
            'license_number': list_record['license_number'],
            'primary_owner': detail_record['primary_owner'],
            'action': action.id,
            'record_type': self.record_type.id,
            'old_status': old_status.id,
            'new_status': new_status.id,
        }
        self.create_newsitem(
            attributes,
            title=title,
            url=detail_url(list_record['place_id']),
            item_date=license['status_date'],
            location_name=detail_record['address'],
        )

class NewIssuedLicenseScraper(LiquorLicenseScraper):
    def __init__(self, geo_code):
        # geo_code is a numeric code describing the part of California we're
        # interested in. For all the choices, see page three of this PDF:
        # http://www.abc.ca.gov/datport/ABC_Data_Layout.PDF
        LiquorLicenseScraper.__init__(self)
        self.geo_code = str(geo_code)
        self.record_type = self.get_or_create_lookup('record_type', 'New license issued', 'ISSUED')

    def list_pages(self):
        yield self.get_html('http://www.abc.ca.gov/datport/SubscrOption.asp', {'SUBCRIT': 'p_DlyIssApp'})

    def clean_list_record(self, record):
        if record['geo_code'] != self.geo_code:
            raise SkipRecord
        record = LiquorLicenseScraper.clean_list_record(self, record)
        record['type'], record['dup'] = record.pop('type_dup').split('/')
        record['expir_date'] = parse_date(record['expir_date'], '%m/%d/%Y')
        return record

class NewApplicationScraper(LiquorLicenseScraper):
    def __init__(self, geo_code):
        LiquorLicenseScraper.__init__(self)
        self.geo_code = str(geo_code)
        self.record_type = self.get_or_create_lookup('record_type', 'New application', 'APPLICATION')

    def list_pages(self):
        yield self.get_html('http://www.abc.ca.gov/datport/SubscrOption.asp', {'SUBCRIT': 'p_DlyNuApp'})

    def clean_list_record(self, record):
        if record['geo_code'] != self.geo_code:
            raise SkipRecord
        record = LiquorLicenseScraper.clean_list_record(self, record)
        record['type'], record['dup'] = record.pop('type_dup').split('/')
        return record

class StatusChangeScraper(LiquorLicenseScraper):
    def __init__(self, geo_code):
        LiquorLicenseScraper.__init__(self)
        self.geo_code = str(geo_code)
        self.record_type = self.get_or_create_lookup('record_type', 'Status change', 'STATUS_CHANGE')

    def list_pages(self):
        yield self.get_html('http://www.abc.ca.gov/datport/SubscrOption.asp', {'SUBCRIT': 'p_DlyStat'})

    def clean_list_record(self, record):
        if record['geo_code'] != self.geo_code:
            raise SkipRecord
        record = LiquorLicenseScraper.clean_list_record(self, record)
        record['type'], record['dup'] = record.pop('type_dup').split('/')
        record['status_from'], record['status_to'] = record.pop('status_changed_from_to').split(' / ')
        record['transfer_info_from'], record['transfer_info_to'] = record.pop('transfer_info_from_to').split('/')
        record['orig_iss_date'] = parse_date(record['orig_iss_date'], '%m/%d/%Y')
        record['expir_date'] = parse_date(record['expir_date'], '%m/%d/%Y')
        return record

def update_newest(geo_code):
    # San Francisco is geo_code 3800. San Jose is 4313. LA is 1933.
    # You can get the geo_code for a city by looking here:
    # http://www.abc.ca.gov/datport/SubDlyNuRep.asp
    s = NewIssuedLicenseScraper(geo_code)
    s.update()
    s = NewApplicationScraper(geo_code)
    s.update()
    s = StatusChangeScraper(geo_code)
    s.update()

if __name__ == '__main__':
    import sys
    from ebdata.retrieval import log_debug
    try:
        geocode = GEOCODES[sys.argv[1]]
    except (KeyError, IndexError):
        print "Usage: retrieval.py %s" % '|'.join(GEOCODES.keys())
        sys.exit(0)
    update_newest(int(geocode))

########NEW FILE########
__FILENAME__ = retrieval
"""
Scraper for Florida residential real estate approvals.
http://www.myflorida.com/dbpr/sto/file_download/lsc_download.shtml
"""

from ebdata.retrieval.scrapers.list_detail import SkipRecord
from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date
from ebpub.utils.text import clean_address
from everyblock.states.florida.myflorida import MyFloridaScraper

class BaseScraper(MyFloridaScraper):
    has_detail = False
    date_format = '%m/%d/%Y'

    def __init__(self, county_names):
        super(BaseScraper, self).__init__()
        self.county_names = set(county_names)

    def clean_list_record(self, record):
        if record['county'].upper() not in self.county_names:
            raise SkipRecord('Skipping county %s' % record['county'])
        record['approval_date'] = parse_date(record['approval_date'], self.date_format)
        if record['approval_date'] is None:
            raise SkipRecord('Record has no date.')
        return record

    def existing_record(self, record):
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id)
            qs = qs.by_attribute(self.schema_fields['project_number'], record['project_number'])
            qs = qs.by_attribute(self.schema_fields['file_number'], record['file_number'])
            return qs[0]
        except IndexError:
            return None

    def save(self, old_record, list_record, detail_record):
        values = {
            'title': self.get_title(list_record),
            'item_date': list_record['approval_date'],
            'location_name': clean_address('%s, %s' % (list_record['street'].strip(), list_record['city'])),
        }
        attributes = self.get_attributes(list_record)
        if old_record is None:
            self.create_newsitem(attributes, **values)
        else:
            self.update_existing(old_record, values, attributes)

class ConvertedCondominiumsScraper(BaseScraper):
    schema_slugs = ('condo-conversions',)
    name_var = 'coop_name'
    florida_ftp_filename = 'condo_conv.exe'
    florida_csv_fieldnames = ('project_number', 'file_number', 'coop_name', 'county',
        'street', 'city', 'state', 'zip', 'units', 'approval_date',
        'primary_status', 'secondary_status', 'manager_number', 'manager_name',
        'manager_route', 'manager_street', 'manager_city', 'manager_state',
        'manager_zip')

    def list_pages(self):
        f = open('/home/jkocherhans/miami-condos/condo_conv.exe', 'rb')
        yield f
        f.close()

    def get_title(self, record):
        return "%s" % record['coop_name']

    def clean_list_record(self, record):
        if record['primary_status'] == 'Acknowledged':
            raise SkipRecord('Status is "Acknowledged"')
        return super(ConvertedCondominiumsScraper, self).clean_list_record(record)

    def get_attributes(self, record):
        primary_status = self.get_or_create_lookup('primary_status', record['primary_status'],  record['primary_status'], make_text_slug=False)
        secondary_status = self.get_or_create_lookup('secondary_status', record['secondary_status'],  record['secondary_status'], make_text_slug=False)
        return {
            'name': record['coop_name'],
            'project_number': record['project_number'],
            'file_number': record['file_number'],
            'units': record['units'],
            'primary_status': primary_status.id,
            'secondary_status': secondary_status.id,
        }

class IntendedConversionsScraper(BaseScraper):
    schema_slugs = ('condo-conversion-notices',)
    name_var = 'noic_name'
    date_format = '%m/%d/%y'
    florida_ftp_filename = 'noic.exe'
    florida_csv_fieldnames = ('file_number', 'noic_name', 'county', 'street',
        'city', 'state', 'zip', 'approval_date', 'status', 'developer_name',
        'developer_route', 'developer_street', 'developer_city',
        'developer_state', 'developer_zip')

    def list_pages(self):
        f = open('/home/jkocherhans/miami-condos/noic.exe', 'rb')
        yield f
        f.close()

    def existing_record(self, record):
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id)
            qs = qs.by_attribute(self.schema_fields['file_number'], record['file_number'])
            return qs[0]
        except IndexError:
            return None

    def get_title(self, record):
        return "%s" % record['noic_name']

    def get_attributes(self, record):
        status = self.get_or_create_lookup('status', record['status'],  record['status'], make_text_slug=False)
        return {
            'name': record['noic_name'],
            'file_number': record['file_number'],
            'status': status.id,
        }

if __name__ == '__main__':
    from ebdata.retrieval import log_debug
    ConvertedCondominiumsScraper(('DADE',)).update()
    IntendedConversionsScraper(('DADE',)).update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Scraper for Florida liquor licenses.
http://www.myflorida.com/dbpr/sto/file_download/file-download-ABandT.shtml
"""

from ebdata.retrieval.scrapers.list_detail import SkipRecord
from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date
from ebpub.utils.text import smart_title, clean_address
from everyblock.states.florida.myflorida import MyFloridaScraper
import datetime

class Scraper(MyFloridaScraper):
    schema_slugs = ('liquor-licenses',)
    has_detail = False

    florida_ftp_filename = 'daily.exe'
    florida_csv_filename = 'daily.txt'
    florida_csv_fieldnames = ('profession', 'county', 'license_number',
        'series_rank', 'class_modifier', 'dba', 'licensee', 'address',
        'address2', 'address3', 'city', 'state', 'zip_code', 'activity_date',
        'activity_code', 'activity_description')

    def __init__(self, *args, **kwargs):
        self.counties = kwargs.pop('counties', [])
        super(Scraper, self).__init__(*args, **kwargs)

    def list_pages(self):
        date = datetime.date(2008, 11, 6)
        while 1:
            if date >= datetime.date.today():
                break
            f = open('/home/jkocherhans/miami-liquor/%s.exe' % date.strftime('%Y-%m-%d'), 'rb')
            date = date + datetime.timedelta(days=1)
            yield f
            f.close()

    def clean_list_record(self, record):
        if record['county'].upper().strip() not in self.counties:
            raise SkipRecord('Record not in %s.' % self.counties)
        record['activity_date'] = parse_date(record['activity_date'], '%m/%d/%Y')
        record['dba'] = smart_title(record['dba'])
        record['address'] = clean_address(record['address'])
        return record

    def existing_record(self, record):
       try:
           qs = NewsItem.objects.filter(schema__id=self.schema.id, item_date=record['activity_date'])
           qs = qs.by_attribute(self.schema_fields['activity_code'], record['activity_code'])
           qs = qs.by_attribute(self.schema_fields['license_number'], record['license_number'])
           return qs[0]
       except IndexError:
           return None

    def save(self, old_record, list_record, detail_record):
        normalized_activity = "(%s) %s" % (list_record['activity_code'], list_record['activity_description'].upper().strip())
        activity = self.get_or_create_lookup('activity', normalized_activity, normalized_activity, make_text_slug=False)
        license_type = self.get_or_create_lookup('license_type', list_record['series_rank'], list_record['series_rank'], make_text_slug=False)
        attributes = {
            'license_number': list_record['license_number'],
            'licensee': list_record['licensee'],
            'dba': list_record['dba'],
            'activity': activity.id,
            'activity_code': list_record['activity_code'],
            'license_type': license_type.id,
        }
        values = {
            'title': '%s for %s' % (activity.name, list_record['dba']),
            'item_date': list_record['activity_date'],
            'location_name': list_record['address']
        }
        if old_record is None:
            self.create_newsitem(attributes, **values)
        else:
            self.update_existing(old_record, values, attributes)

class MiamiDadeScraper(Scraper):
    def __init__(self, *args, **kwargs):
        kwargs['counties'] = ('DADE',)
        super(MiamiDadeScraper, self).__init__(*args, **kwargs)

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    s = MiamiDadeScraper()
    s.update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Scraper for Florida lodging inspections.
http://www.myflorida.com/dbpr/sto/file_download/hr_inspection.shtml
"""

from ebdata.retrieval.scrapers.list_detail import SkipRecord
from everyblock.states.florida.restaurants.retrieval import Scraper as RestaurantScraper

# The data is in the same format as restaurant inspections, so we can
# merely subclass the restaurant scraper.

class Scraper(RestaurantScraper):
    schema_slugs = ('lodging-inspections',)

    def __init__(self, city_names, district):
        super(Scraper, self).__init__(city_names, district)
        self.florida_ftp_filename = '%sldinspi.exe' % district

    def clean_list_record(self, record):
        record = super(Scraper, self).clean_list_record(record)

        # Special-case: a bad record identified on 2009-02-11.
        if str(record['inspection_visit_id']) == '3145783' and str(record['license_id']) == '2164597':
            raise SkipRecord('Invalid record')

        return record

class MiamiScraper(Scraper):
    def __init__(self):
        city_names = ['MIAMI', 'AVENTURA', 'BAL HARBOUR', 'BAY HARBOR ISLANDS',
            'BISCAYNE PARK', 'CORAL GABLES', 'CUTLER BAY', 'DORAL', 'EL PORTAL',
            'FLORIDA CITY', 'GOLDEN BEACH', 'HIALEAH', 'HIALEAH GARDENS',
            'HOMESTEAD', 'INDIAN CREEK VILLAGE', 'ISLANDIA', 'KEY BISCAYNE',
            'MEDLEY', 'MIAMI BEACH', 'MIAMI GARDENS', 'MIAMI LAKES',
            'MIAMI SHORES', 'MIAMI SPRINGS', 'NORTH BAY VILLAGE', 'NORTH MIAMI',
            'NORTH MIAMI BEACH', 'OPA-LOCKA', 'PALMETTO BAY', 'PINECREST',
            'SOUTH MIAMI', 'SUNNY ISLES BEACH', 'SURFSIDE', 'SWEETWATER',
            'VIRGINIA GARDENS', 'WEST MIAMI']
        super(MiamiScraper, self).__init__(city_names, 1)

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    MiamiScraper().update()

########NEW FILE########
__FILENAME__ = myflorida
"""
Base class for scrapers that get stuff via FTP from http://www.myflorida.com/
"""

from ebdata.parsing.unicodecsv import UnicodeDictReader
from ebdata.retrieval.scrapers.base import ScraperBroken
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from cStringIO import StringIO
import ftplib
import zipfile

class MyFloridaScraper(NewsItemListDetailScraper):
    # Subclasses must set these attributes before list_pages() is called.
    florida_ftp_filename = None
    florida_csv_fieldnames = None
    florida_csv_filename = None

    def list_pages(self):
        self.logger.debug('Connecting via FTP')
        ftp = ftplib.FTP('dbprftp.state.fl.us')
        ftp.login() # This is a necessary step; otherwise we get "530 Please login with USER and PASS".
        ftp.set_pasv(False) # Turn off passive mode, which is on by default.
        ftp.cwd('/pub/llweb')
        self.logger.debug('Retrieving file %s' % self.florida_ftp_filename)
        f = StringIO() # This buffer stores the retrieved file in memory.
        ftp.retrbinary('RETR %s' % self.florida_ftp_filename, f.write)
        self.logger.debug('Done downloading')
        f.seek(0)
        ftp.quit() # Note that we quit the connection before scraping starts -- otherwise we get a timeout!
        yield f
        f.close()

    def parse_list(self, file_obj):
        # Unzip the file. Although it has an .exe extension, we can treat it
        # just like a ZIP file.
        zf = zipfile.ZipFile(file_obj)
        if self.florida_csv_filename is None:
            csv_names = [n for n in zf.namelist() if n.lower().endswith('.csv')]
            if len(csv_names) != 1:
                raise ScraperBroken('Found %s CSV file(s) in the zip' % len(csv_names))
            csv_filename = csv_names[0]
        else:
            csv_filename = self.florida_csv_filename
        csv_text = zf.read(csv_filename)
        # The data is in iso-8859-1 encoding, so we use UnicodeDictReader so
        # that it gets converted properly to Unicode objects.
        reader = UnicodeDictReader(StringIO(csv_text), self.florida_csv_fieldnames, encoding='iso8859-1')
        for row in reader:
            yield row

########NEW FILE########
__FILENAME__ = retrieval
"""
Scraper for Florida restaurant plan review applications.
http://www.myflorida.com/dbpr/sto/file_download/hr_food_service_files.shtml
"""

from ebdata.retrieval.scrapers.list_detail import SkipRecord
from ebpub.db.models import NewsItem
from ebpub.geocoder.parser.parsing import strip_unit
from ebpub.utils.dates import parse_date
from ebpub.utils.text import clean_address
from everyblock.states.florida.myflorida import MyFloridaScraper

class Scraper(MyFloridaScraper):
    schema_slugs = ('plan-reviews',)
    has_detail = False

    florida_ftp_filename = 'HR_plan_review.exe'
    florida_csv_fieldnames = ('district', 'county', 'business_name',
        'address', 'city', 'zip', 'phone', 'email', 'status',
        'application_date', 'review_date', 'application_type', 'facility_type',
        'file_number', 'application_number', 'license_number', 'transaction',
        'variance', 'mailing_name', 'mailing_address', 'mailing_city',
        'mailing_state', 'mailing_zip', 'mailing_country', 'contact_phone',
        'contact_email', 'alternate_phone', 'alternate_email')

    def __init__(self, city_names):
        # city_names should be a list of uppercase strings like 'MIAMI'.
        super(Scraper, self).__init__()
        self.city_names = set(city_names)

    def clean_list_record(self, record):
        record['application_date'] = parse_date(record['application_date'], '%m/%d/%Y')

        try:
            record['review_date'] = parse_date(record['review_date'], '%m/%d/%Y')
        except ValueError: # sometimes it's 'n/a'
            record['review_date'] = None

        record['address'] = strip_unit(clean_address(record['address']))
        if record['city'] not in self.city_names:
            raise SkipRecord('Skipping city %s' % record['city'])
        record['city'] = record['city'].title()
        return record

    def existing_record(self, record):
        # To determine previous records, we use the application number and
        # review date. There can be multiple records for a single application
        # number.
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id)
            qs = qs.by_attribute(self.schema_fields['application_number'], record['application_number'])
            qs = qs.by_attribute(self.schema_fields['review_date'], record['review_date'])
            return qs[0]
        except IndexError:
            return None

    def save(self, old_record, list_record, detail_record):
        application_type = self.get_or_create_lookup('application_type', list_record['application_type'], list_record['application_type'])
        facility_type = self.get_or_create_lookup('facility_type', list_record['facility_type'], list_record['facility_type'])
        status = self.get_or_create_lookup('status', list_record['status'], list_record['status'])

        title = u'%s applied for plan review' % list_record['business_name']
        values = {
            'title': title,
            'item_date': list_record['application_date'],
            'location_name': '%s, %s' % (list_record['address'], list_record['city']),
        }
        attributes = {
            'file_number': list_record['file_number'],
            'application_number': list_record['application_number'],
            'application_type': application_type.id,
            'business_name': list_record['business_name'],
            'facility_type': facility_type.id,
            'license_number': list_record['license_number'],
            'review_date': list_record['review_date'],
            'status': status.id,
        }
        if old_record is None:
            self.create_newsitem(attributes, **values)
        else:
            self.update_existing(old_record, values, attributes)

class MiamiScraper(Scraper):
    def __init__(self):
        city_names = ['MIAMI', 'AVENTURA', 'BAL HARBOUR', 'BAY HARBOR ISLANDS',
            'BISCAYNE PARK', 'CORAL GABLES', 'CUTLER BAY', 'DORAL', 'EL PORTAL',
            'FLORIDA CITY', 'GOLDEN BEACH', 'HIALEAH', 'HIALEAH GARDENS',
            'HOMESTEAD', 'INDIAN CREEK VILLAGE', 'ISLANDIA', 'KEY BISCAYNE',
            'MEDLEY', 'MIAMI BEACH', 'MIAMI GARDENS', 'MIAMI LAKES',
            'MIAMI SHORES', 'MIAMI SPRINGS', 'NORTH BAY VILLAGE', 'NORTH MIAMI',
            'NORTH MIAMI BEACH', 'OPA-LOCKA', 'PALMETTO BAY', 'PINECREST',
            'SOUTH MIAMI', 'SUNNY ISLES BEACH', 'SURFSIDE', 'SWEETWATER',
            'VIRGINIA GARDENS', 'WEST MIAMI']
        super(MiamiScraper, self).__init__(city_names)

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    MiamiScraper().update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Scraper for Florida restaurant inspections.
http://www.myfloridalicense.com/dbpr/sto/file_download/hr_food_service_inspection.shtml
"""

from django.core.serializers.json import DjangoJSONEncoder
from ebpub.db.models import NewsItem
from ebpub.utils.dates import parse_date
from ebpub.utils.text import clean_address
from everyblock.states.florida.myflorida import MyFloridaScraper

class Scraper(MyFloridaScraper):
    schema_slugs = ('restaurant-inspections',)
    has_detail = False

    florida_csv_fieldnames = ('district', 'county_number', 'county_name',
        'license_type_code', 'license_number', 'business_name', 'address',
        'city', 'zip', 'inspection_number', 'visit_number', 'inspection_class',
        'inspection_type', 'disposition', 'inspection_date',
        'critical_violations', 'noncritical_violations', 'total_violations',
        'pda_status', 'vio1', 'vio2', 'vio3', 'vio4', 'vio5', 'vio6', 'vio7',
        'vio8', 'vio9', 'vio10', 'vio11', 'vio12', 'vio13', 'vio14', 'vio15',
        'vio16', 'vio17', 'vio18', 'vio19', 'vio20', 'vio21', 'vio22', 'vio23',
        'vio24', 'vio25', 'vio26', 'vio27', 'vio28', 'vio29', 'vio30', 'vio31',
        'vio32', 'vio33', 'vio34', 'vio35', 'vio36', 'vio37', 'vio38', 'vio39',
        'vio40', 'vio41', 'vio42', 'vio43', 'vio44', 'vio45', 'vio46', 'vio47',
        'vio48', 'vio49', 'vio50', 'vio51', 'vio52', 'vio53', 'vio54', 'vio55',
        'vio56', 'vio57', 'vio58', 'license_id', 'inspection_visit_id')

    def __init__(self, city_names, district):
        # city_names should be a list of uppercase strings like 'MIAMI'.
        # district should be an integers
        super(Scraper, self).__init__()
        self.city_names = set(city_names)
        self.florida_ftp_filename = '%sfdinspi.exe' % district

    def clean_list_record(self, record):
        # Collapse the violations into a single value, rather than 58 values,
        # most of which are zero.
        num_violations = []
        for i in range(1, 59):
            val = int(record.pop('vio%s' % i))
            if val:
                num_violations.append((str(i), val))
        record['violations'] = num_violations

        record['inspection_date'] = parse_date(record['inspection_date'], '%m/%d/%Y')
        record['address'] = clean_address(record['address'])

        if record['city'] not in self.city_names:
            raise SkipRecord('Skipping city %s' % record['city'])

        record['city'] = record['city'].title()
        record['visit_number'] = int(record['visit_number'])
        record['critical_violations'] = int(record['critical_violations'])
        record['noncritical_violations'] = int(record['noncritical_violations'])
        record['total_violations'] = int(record['total_violations'])
        record['inspection_number'] = int(record['inspection_number'])

        return record

    def existing_record(self, record):
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id)
            qs = qs.by_attribute(self.schema_fields['inspection_visit_id'], record['inspection_visit_id'])
            return qs[0]
        except IndexError:
            return None

    def save(self, old_record, list_record, detail_record):
        license_type = self.get_or_create_lookup('license_type', list_record['license_type_code'], list_record['license_type_code'], make_text_slug=False)
        disposition = self.get_or_create_lookup('disposition', list_record['disposition'], list_record['disposition'], make_text_slug=False)
        violation_lookups = [self.get_or_create_lookup('violation', v[0], v[0], make_text_slug=False) for v in list_record['violations']]
        violation_lookup_text = ','.join([str(v.id) for v in violation_lookups])

        v_lookup_dict = dict([(v.code, v.id) for v in violation_lookups])
        v_list = [{'lookup_id': v_lookup_dict[code], 'number': number} for code, number in list_record['violations']]
        details_json = DjangoJSONEncoder().encode(v_list)

        title = u'%s inspected: %s violation%s' % (list_record['business_name'], list_record['total_violations'], list_record['total_violations'] != 1 and 's' or '')

        values = {
            'title': title,
            'item_date': list_record['inspection_date'],
            'location_name': '%s, %s' % (list_record['address'], list_record['city']),
        }
        attributes = {
            'inspection_visit_id': list_record['inspection_visit_id'],
            'license_id': list_record['license_id'],
            'license_number': list_record['license_number'],
            'business_name': list_record['business_name'],
            'inspection_number': list_record['inspection_number'],
            'license_type': license_type.id,
            'critical_violations': list_record['critical_violations'],
            'noncritical_violations': list_record['noncritical_violations'],
            'total_violations': list_record['total_violations'],
            'visit_number': list_record['visit_number'],
            'disposition': disposition.id,
            'violation': violation_lookup_text,
            'violation_details': details_json,
        }
        if old_record is None:
            self.create_newsitem(attributes, **values)
        else:
            self.update_existing(old_record, values, attributes)

class MiamiScraper(Scraper):
    def __init__(self):
        city_names = ['MIAMI', 'AVENTURA', 'BAL HARBOUR', 'BAY HARBOR ISLANDS',
            'BISCAYNE PARK', 'CORAL GABLES', 'CUTLER BAY', 'DORAL', 'EL PORTAL',
            'FLORIDA CITY', 'GOLDEN BEACH', 'HIALEAH', 'HIALEAH GARDENS',
            'HOMESTEAD', 'INDIAN CREEK VILLAGE', 'ISLANDIA', 'KEY BISCAYNE',
            'MEDLEY', 'MIAMI BEACH', 'MIAMI GARDENS', 'MIAMI LAKES',
            'MIAMI SHORES', 'MIAMI SPRINGS', 'NORTH BAY VILLAGE', 'NORTH MIAMI',
            'NORTH MIAMI BEACH', 'OPA-LOCKA', 'PALMETTO BAY', 'PINECREST',
            'SOUTH MIAMI', 'SUNNY ISLES BEACH', 'SURFSIDE', 'SWEETWATER',
            'VIRGINIA GARDENS', 'WEST MIAMI']
        super(MiamiScraper, self).__init__(city_names, 1)

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    s = MiamiScraper()
    s.update()

########NEW FILE########
__FILENAME__ = retrieval
"""
Screen scraper for Washington state liquor-license data.
http://www.liq.wa.gov/Media_Releases/MediaReleasesCountyMap.asp

Our scraper works by submitting a form for a particular city, but
note that the entire state's licenses are in one place here:
http://www.liq.wa.gov/Media_Releases/EntireStateWeb.asp
"""

from django.core.serializers.json import DjangoJSONEncoder
from ebdata.retrieval.scrapers.base import ScraperBroken
from ebdata.retrieval.scrapers.list_detail import SkipRecord
from ebdata.retrieval.scrapers.newsitem_list_detail import NewsItemListDetailScraper
from ebpub.db.models import NewsItem
from ebpub.geocoder.parser.parsing import strip_unit
from ebpub.utils.dates import parse_date
from ebpub.utils.text import clean_address
from lxml.html import document_fromstring
from lxml import etree
import re

def rows(table):
    # Yields lists of strings, with a string representing a <td>/<th> and
    # a list representing a <tr>.
    for tr in etree.XPath('tr')(table):
        yield [col.text_content() for col in etree.XPath('td | th')(tr)]

def clean_washington_address(add, city):
    add = add.replace(u'\xa0', ' ')
    add = re.sub(r',\s+%s,\s+WA.*$' % city, '', add)
    add = clean_address(add)
    return add, strip_unit(add).strip()

class LiquorLicenseScraper(NewsItemListDetailScraper):
    schema_slugs = ('liquor-licenses',)
    has_detail = False

    def __init__(self, county, city):
        super(LiquorLicenseScraper, self).__init__(use_cache=False)
        self.license_county, self.license_city = county, city

    def list_pages(self):
        # Note that we scrape the HTML instead of the Excel file because the
        # Excel file cannot be read properly by our Excel library:
        #    xlrd.biffh.XLRDError: Expected BOF record; found 0x0a0d
        url = 'http://www.liq.wa.gov/Media_Releases/MediaReleasesReport3Excel.asp'
        data = {
            'hiddenCounty': self.license_county,
            'cboCity': self.license_city,
            'radFormat': 'on',
            'txtFormat': 'Web',
        }
        yield self.get_html(url, data)

    def parse_list(self, html):
        xml = document_fromstring(html)

        # The record headers (e.g., "Notification Date", "Application Type")
        # have trailing colons, so this regex removes those. It also removes
        # leading backslash characters, which show up very rarely.
        unwanted_characters = re.compile(r'^\\?\s*|:$')

        for table in etree.XPath('//table')(xml)[1:]:
            full_record = {}
            category = None
            for i, row in enumerate(rows(table)):
                row = [val.strip() for val in row]
                if i == 0: # The first row has the category, e.g. "SEATTLE NEW LIQUOR LICENSE APPLICATIONS".
                    category = row[0]
                    full_record['category'] = category
                elif len(row) < 2 or ''.join(row) == '':
                    # New data block.
                    if len(full_record) > 1:
                        yield full_record
                    full_record = {'category': category}
                else:
                    full_record[unwanted_characters.sub('', row[0])] = row[1]
            if len(full_record) > 1:
                yield full_record

    def clean_list_record(self, record):
        record['category'] = record['category'].replace(u'\xa0', ' ').replace(self.license_city + ' ', '')

        try:
            add = record.pop('Business Location')
        except KeyError:
            add = record.pop('Current Business Location')
        record['address'], record['clean_address'] = clean_washington_address(add, self.license_city)
        if 'New Business Location' in record:
            record['new_address'] = clean_washington_address(record.pop('New Business Location'), self.license_city)[0]
        else:
            record['new_address'] = ''

        if 'Discontinued Date' in record:
            record['item_date'] = parse_date(record.pop('Discontinued Date'), '%m/%d/%Y')
        elif 'Approved Date' in record:
            record['item_date'] = parse_date(record.pop('Approved Date'), '%m/%d/%Y')
        elif 'Notification Date' in record:
            record['item_date'] = parse_date(record.pop('Notification Date'), '%m/%d/%Y')
        else:
            raise ScraperBroken("Didn't find a date in %r" % record)

        if 'Business Name' in record:
            record['business_name'] = record.pop('Business Name')
        elif 'Current Business Name' in record:
            record['business_name'] = record.pop('Current Business Name')
        else:
            record['business_name'] = ''

        if 'Applicant(s)' in record:
            record['applicant'] = record.pop('Applicant(s)')
        elif 'Current Applicant(s)' in record:
            record['applicant'] = record.pop('Current Applicant(s)')
        else:
            record['applicant'] = ''

        record['new_business_name'] = record.pop('New Business Name', '')
        record['new_applicant'] = record.pop('New Applicant(s)', '')

        license_types = record['Liquor License Type'].split('; ')
        license_types = [re.sub(r'^\d+,\s+', '', lt) for lt in license_types]
        record['license_types'] = [re.sub('^DIRECT SHIPMENT RECEIVER-(?:IN/OUT WA|IN WA ONLY)$', 'DIRECT SHIPMENT RECEIVER', lt) for lt in license_types]

        try:
            record['title'] = {
                ('DISCONTINUED LIQUOR LICENSES', 'DISCONTINUED'): u'Liquor license discontinued for %s',
                ('NEW LIQUOR LICENSE APPLICATIONS', 'ASSUMPTION'): u'%s applied to assume license',
                ('NEW LIQUOR LICENSE APPLICATIONS', 'NEW APPLICATION'): u'%s applied for new liquor license',
                ('NEW LIQUOR LICENSE APPLICATIONS', 'ADDED/CHANGE OF CLASS/IN LIEU'): u'%s applied for additional liquor license class',
                ('NEW LIQUOR LICENSE APPLICATIONS', 'ADDED/CHANGE OF TRADENAME'): u'%s applied for trade name change',
                ('NEW LIQUOR LICENSE APPLICATIONS', 'CHANGE OF CORPORATE NAME'): u'%s applied for corporate name change',
                ('NEW LIQUOR LICENSE APPLICATIONS', 'CHANGE OF CORPORATE OFFICER'): u'%s applied to add or remove a corporate officer',
                ('NEW LIQUOR LICENSE APPLICATIONS', 'CHANGE OF LOCATION'): u'%s applied for change of location',
                ('NEW LIQUOR LICENSE APPLICATIONS', 'CHANGE OF LLC MEMBER'): u'%s applied to add or remove an LLC member',
                ('NEW LIQUOR LICENSE APPLICATIONS', 'IN LIEU'): u'%s applied to change liquor license class',
                ('RECENTLY APPROVED LIQUOR LICENSES', 'ADDED FEES'): u'%s approved for addition of fees',
                ('RECENTLY APPROVED LIQUOR LICENSES', 'ASSUMPTION'): u'%s approved to assume license',
                ('RECENTLY APPROVED LIQUOR LICENSES', 'NEW APPLICATION'): u'%s approved for new liquor license',
                ('RECENTLY APPROVED LIQUOR LICENSES', 'ADDED/CHANGE OF CLASS/IN LIEU'): u'%s approved for additional liquor license class',
                ('RECENTLY APPROVED LIQUOR LICENSES', 'ADDED/CHANGE OF TRADENAME'): u'%s approved for trade name change',
                ('RECENTLY APPROVED LIQUOR LICENSES', 'CHANGE OF CORPORATE NAME'): u'%s approved for corporate name change',
                ('RECENTLY APPROVED LIQUOR LICENSES', 'CHANGE OF CORPORATE OFFICER'): u'%s approved to add or remove a corporate officer',
                ('RECENTLY APPROVED LIQUOR LICENSES', 'CHANGE OF LOCATION'): u'%s approved for change of location',
                ('RECENTLY APPROVED LIQUOR LICENSES', 'CHANGE OF LLC MEMBER'): u'%s approved to add or remove an LLC member',
                ('RECENTLY APPROVED LIQUOR LICENSES', 'IN LIEU'): u'%s approved to change liquor license class',
            }[(record['category'], record['Application Type'])]
        except KeyError:
            self.logger.warn('Got unsupported combo %r and %r', record['category'], record['Application Type'])
            raise SkipRecord
        record['title'] = record['title'] % record['business_name']

        return record

    def existing_record(self, record):
        try:
            qs = NewsItem.objects.filter(schema__id=self.schema.id, item_date=record['item_date'])
            qs = qs.by_attribute(self.schema_fields['license_number'], record['License Number'])
            qs = qs.by_attribute(self.schema_fields['application_type'], record['Application Type'], is_lookup=True)
            return qs[0]
        except IndexError:
            return None

    def save(self, old_record, list_record, detail_record):
        if old_record is not None:
            return

        application_type = self.get_or_create_lookup('application_type', list_record['Application Type'], list_record['Application Type'])
        license_types = [self.get_or_create_lookup('license_type', lt, lt, make_text_slug=False) for lt in list_record['license_types']]
        category = self.get_or_create_lookup('category', list_record['category'], list_record['category'], make_text_slug=False)

        json_data = {
            'business_name': list_record['business_name'],
            'new_business_name': list_record['new_business_name'],
            'new_address': list_record['new_address'],
        }

        attributes = {
            'applicant': list_record['applicant'],
            'new_applicant': list_record['new_applicant'],
            'license_number': list_record['License Number'],
            'category': category.id,
            'application_type': application_type.id,
            'license_type': ','.join([str(lt.id) for lt in license_types]),
            'details': DjangoJSONEncoder().encode(json_data),
        }
        self.create_newsitem(
            attributes,
            title=list_record['title'],
            item_date=list_record['item_date'],
            location_name=list_record['clean_address'],
        )

if __name__ == "__main__":
    from ebdata.retrieval import log_debug
    LiquorLicenseScraper('King', 'SEATTLE').update()

########NEW FILE########
__FILENAME__ = staticmedia
import os.path
from django import template
from django.conf import settings
from everyblock.utils.genstaticversions import lookup

register = template.Library()

def has_minified_version(resource):
    if resource.startswith('/'):
        resource = resource[1:]
    return os.path.exists(minified_path(os.path.join(settings.EB_MEDIA_ROOT, resource)))

def minified_path(path):
    """
    Returns a path to a minified resource.

    Example:

    >>> minified_path('/scripts/maps.js')
    '/scripts/maps.min.js'
    """
    return '%s.min%s' % os.path.splitext(path)

def is_versioned(path):
    return lookup(path)

def versioned_path(path, vhash):
    return '/v%s%s' % (vhash, path)

def production_path(path):
    """
    Returns a path to a resource in a production environment, which
    may include a minified alternative and a version hash.
    """
    orig_path = path
    vhash = is_versioned(path)
    if vhash:
        path = versioned_path(path, vhash)
    if has_minified_version(orig_path):
        path = minified_path(path)
    return path

class AutoVersionNode(template.Node):
    def __init__(self, resource):
        self.resource = resource

    def render(self, context):
        path = self.resource
        if settings.AUTOVERSION_STATIC_MEDIA:
            path = production_path(path)
        return '%s%s' % (settings.EB_MEDIA_URL, path)

@register.tag(name='autoversion')
def do_autoversion(parser, token):
    try:
        tag_name, resource = token.split_contents()
    except ValueError:
        raise template.TemplateSyntaxError, '%r tag requires a single argument' % token.contents.split()[0]
    if not (resource[0] == resource[-1] and resource[0] in ('"', "'")):
        raise template.TemplateSyntaxError, "%r tag's argument should be in quotes" % tag_name
    return AutoVersionNode(resource[1:-1])

########NEW FILE########
__FILENAME__ = urls
from django.conf import settings
from django.conf.urls.defaults import *

if settings.DEBUG:
    urlpatterns = patterns('',
        (r'^(?P<path>(?:images|scripts|styles|openlayers).*)$', 'django.views.static.serve', {'document_root': settings.EB_MEDIA_ROOT}),
    )
else:
    urlpatterns = patterns('')

urlpatterns += patterns('',
    (r'^admin/', include('everyblock.admin.urls')),
)

########NEW FILE########
__FILENAME__ = cache_population
from django.conf import settings
from django.core.cache import cache
from django.test.client import ClientHandler
from ebpub.db.models import LocationType, Schema

CACHED_URLS = ('/', '/robots.txt', '/news/', '/streets/')

def iter_urls():
    for url in CACHED_URLS:
        yield url
    for s in Schema.public_objects.all():
        schema_url = s.url()
        yield schema_url
        yield schema_url + 'about/'
    # Cache location type detail pages, but not the root of
    # custom-drawn ones, because that will be unique to the user
    for lt in LocationType.objects.exclude(slug='custom'):
        yield lt.url()

def populate_cache():
    # Here, we make requests directly via a ClientHandler rather than over
    # the live Internet, because we want to cache the whole HttpResponse
    # object, not just the text of the response. In order to do this, we have
    # to set up an environment and pass that to ClientHandler for each request.
    environ = {
        # Set __raw__ so that the templates are only half rendered.
        # (See everyblock.utils.middleware.CachedTemplateMiddleware for more.)
        'QUERY_STRING':      '__raw__=1',
        'REMOTE_ADDR':       '127.0.0.1',
        'REQUEST_METHOD':    'GET',
        'SCRIPT_NAME':       '',
        'SERVER_NAME':       'cacheserver',
        'SERVER_PORT':       '80',
        'SERVER_PROTOCOL':   'HTTP/1.1',
        'wsgi.version':      (1,0),
        'wsgi.url_scheme':   'http',
        'wsgi.multiprocess': True,
        'wsgi.multithread':  False,
        'wsgi.run_once':     False,
    }
    handler = ClientHandler()
    for url in iter_urls():
        cache_key = '%s%s' % (settings.SHORT_NAME, url)
        print cache_key
        response = handler(dict(environ, PATH_INFO=url))
        cache.set(cache_key, response, 60 * 60 * 24) # Cache for 24 hours.

if __name__ == "__main__":
    populate_cache()

########NEW FILE########
__FILENAME__ = genstaticversions
#!/usr/bin/env python

"""
Calculates a version hash for each of static media files and stuffs
them in a cache.

There are two main ways to interact with this module. First is by
calling it, with one argument, from the commandline -- the argument
being the path to the root of the static media files. This will
populate the cache with the file version hashes.

The second way is by importing the module and calling the `lookup'
function with a single argument, that being the path of a single
static media file (path relative to the static media root, the same as
that used in the call to the commandline).
"""

import os
import sys
import md5
from django.conf import settings
from django.core.cache import cache

SALT = '1234567890'
CACHE_PREFIX = '' # Set this to your domain
EXTENSIONS = ['js', 'css', 'png', 'gif']
CACHE_TIMEOUT = 60 * 60 * 24

def media_root_join(path):
    if path.startswith('/'):
        path = path[1:]
    return os.path.join(settings.EB_MEDIA_ROOT, path)

def get_version(path, version_func=os.path.getmtime):
    """
    Returns the version for a single static media file.
    """
    try:
        return version_func(media_root_join(path))
    except OSError:
        return ''

def get_filelist():
    media_root = os.path.normpath(settings.EB_MEDIA_ROOT)
    filelist = []
    for root, dirs, files in os.walk(media_root):
        for name in files:
            (froot, ext) = os.path.splitext(name)
            if ext[1:].lower() in EXTENSIONS:
                filelist.append(os.path.join(root.replace(media_root, ''), name))
    return filelist
    
def get_versions(version_func=os.path.getmtime):
    """
    Returns a dictionary mapping filenames to their version numbers.

    Filenames are relative to the root of the static media directory.

    The kwarg `version_func' is a function that takes one argument,
    the file path, and returns a version / revision number.
    """
    return dict([(f, version_func(media_root_join(f)))
                 for f in get_filelist()])

def cache_key(filename):
    return '%s%s' % (CACHE_PREFIX, filename)

def version_hash(rev_num):
    return md5.new(str(rev_num) + SALT).hexdigest()

def cache_versions(fileversions):
    """
    Populates the cache with filenames and corresponding version
    hashes.
    
    `fileversions' is a dictionary mapping filename to version number,
    same as output of `get_versions()'.
    """
    for filename, rev_num in fileversions.iteritems():
        cache.set(cache_key(filename), version_hash(rev_num), CACHE_TIMEOUT)

def clear_cache():
    for filename in get_filelist():
        cache.delete(cache_key(filename))

def lookup(path):
    """
    Returns a version hash for a file. The `path' should be a path
    to the resource relative to the static media root, i.e., how it
    would be used in a reference in the HTML:

    >>> lookup('/scripts/maps.js')
    u'63fb5fa1cbdebb778b7396a522b9191a'
    """
    key = cache_key(path)
    vhash = cache.get(key, '')
    if not vhash:
        rev_num = get_version(path)
        if not rev_num:
            return ''
        vhash = version_hash(rev_num)
        cache.set(key, vhash, CACHE_TIMEOUT)
    return vhash

def main(argv=None):
    cache_versions(get_versions())

if __name__ == "__main__":
    sys.exit(main())

########NEW FILE########
__FILENAME__ = middleware
import re
import urllib
from django.http import Http404
from django.conf import settings
from django.core.cache import cache
from django.template import Template
from django.template.context import RequestContext
from django.db import connection
from ebpub.metros.allmetros import METRO_DICT

class CachedTemplateMiddleware(object):
    # Re-renders any text/html responses through the template system again,
    # to catch any template code that's been bubbled up via the {% raw %}
    # template tag.
    #
    # Also checks the cache for cached requests. Note that cached requests
    # use keys like "chicago/locations/neighborhoods/", and the
    # value in the cache is assumed to be an HttpResponse object.
    def process_view(self, request, view_func, view_args, view_kwargs):
        response = None

        # If '__raw__' is provided in the query string, then we don't hit
        # the cache. This is because the populate_cache() function in
        # everyblock.utils needs a hook for making a request that bypasses
        # the cache. If it didn't have this hook, then it would just read
        # the existing page from the cache and put it back in the cache!
        if request.method == 'GET' and '__raw__' not in request.GET:
            # e.g., "chicago/locations/neighborhoods/"
            cache_key = '%s%s' % (settings.SHORT_NAME, request.path)
            # URL-encode the cache_key, because it's built from
            # the request.path, which may contain control characters
            # (like spaces) that the memcached backend doesn't allow.
            cache_key = urllib.quote(cache_key)
            response = cache.get(cache_key, None)

        if response is None:
            response = view_func(request, *view_args, **view_kwargs)

        # For simplicity, there's a backdoor to get the raw template for any
        # page -- '__raw__' in the query string.
        if '__raw__' not in request.GET and response['content-type'].startswith('text/html'):
            t = Template('{% load raw %}{% raw silent %}' + response.content + '{% endraw %}')
            response.content = t.render(RequestContext(request))

        return response

class DynamicCitySettingsMiddleware(object):
    """
    Sets certain Django settings dynamically based on the city short
    name as derived from the EB_CITY_SLUG environment variable.
    """
    def process_request(self, request):
        # We look to see if the web server has set the EB_CITY_SLUG
        # environment variable, and use that if so, since it will be
        # slightly faster than doing it ourselves in Python. But we
        # fall back to that if it's not.
        short_name = request.environ.get('EB_CITY_SLUG')
        if short_name is None:
            raise Http404(u'unknown city') 
        if short_name not in METRO_DICT:
            raise Http404(u'unknown city')

        # Reset the database connection
        conn = connection
        conn.connection = None
        conn.settings_dict['DATABASE_NAME'] = short_name
        settings.DATABASE_NAME = short_name

        settings.TEMPLATE_DIRS = (
            '/home/eb/templates/base',
            '/home/eb/templates/cities/' + short_name,
        )
        try:
            settings.TIME_ZONE = METRO_DICT[short_name]['time_zone']
        except KeyError:
            raise Http404(u'time zone not set')
        settings.SHORT_NAME = short_name

        return None

########NEW FILE########
__FILENAME__ = multicity
from django.utils.safestring import SafeUnicode, SafeString
from ebpub.metros.allmetros import METRO_LIST
import psycopg2
import psycopg2.extensions

# This is taken from django/db/backends/postgresql_psycopg2/base.py
psycopg2.extensions.register_type(psycopg2.extensions.UNICODE)
psycopg2.extensions.register_adapter(SafeString, psycopg2.extensions.QuotedString)
psycopg2.extensions.register_adapter(SafeUnicode, psycopg2.extensions.QuotedString)

USERNAME = 'username'
HOST = '192.168.1.100'

def run_query(sql, params):
    """
    Runs the given SQL query against every city database and returns a
    dictionary mapping the short_name to the result.
    """
    result = {}
    for metro in METRO_LIST:
        # The connection parameters are hard-coded rather than using the
        # settings files because Django doesn't yet work with multiple
        # settings files.
        connection = psycopg2.connect('user=%s dbname=%s host=%s' % (USERNAME, metro['short_name'], HOST))
        connection.set_client_encoding('UTF8')
        cursor = connection.cursor()
        cursor.execute(sql, params)
        result[metro['short_name']] = cursor.fetchall()
        connection.close()
    return result

########NEW FILE########
__FILENAME__ = queryset
def batch(qs, size=1000):
    total = qs.count()
    for start in range(0, total, size):
        end = min(start + size, total)
        yield (start, end, total, qs[start:end])

########NEW FILE########
__FILENAME__ = redirecter
from django.http import Http404, HttpResponse
import urllib

def redirecter(request):
    "Redirects to a given URL without sending the 'Referer' header."
    try:
        url = request.GET['url']
    except KeyError:
        raise Http404
    if not url.startswith('http://') and not url.startswith('https://'):
        raise Http404
    return HttpResponse('<html><head><meta http-equiv="Refresh" content="0; URL=%s"></head><body>Loading...</body></html>' % urllib.unquote_plus(url))

########NEW FILE########

__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# rdflib documentation build configuration file, created by
# sphinx-quickstart on Fri May 15 15:03:54 2009.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os, re

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#sys.path.append(os.path.abspath('.'))

# -- General configuration -----------------------------------------------------

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
# extensions = ['sphinx.ext.autodoc', 'sphinx.ext.todo', 'sphinx.ext.doctest']
extensions = ['sphinx.ext.autodoc', #'sphinx.ext.autosummary',
              'sphinx.ext.doctest', 'sphinx.ext.intersphinx',
              'sphinx.ext.todo', 'sphinx.ext.coverage',
              'sphinx.ext.ifconfig', 'sphinx.ext.viewcode']

autodoc_default_flags = [ "special-members" ]

autosummary_generate = True

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

#epydoc_mapping = {
#   '/_static/api/': [r'rdflib\.'],
#   }


# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
source_encoding = 'utf-8'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'rdflib'
copyright = u'2009 - 2013, RDFLib Team'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.

# Find version. We have to do this because we can't import it in Python 3 until
# its been automatically converted in the setup process.
def find_version(filename):
    _version_re = re.compile(r'__version__ = "(.*)"')
    for line in open(filename):
        version_match = _version_re.match(line)
        if version_match:
            return version_match.group(1)

# The full version, including alpha/beta/rc tags.
release = find_version('../rdflib/__init__.py')
# The short X.Y version.
version = re.sub("[0-9]+\\.[0-9]\\..*", "\1", release)

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of documents that shouldn't be included in the build.
#unused_docs = []

# List of directories, relative to source directory, that shouldn't be searched
# for source files.
exclude_trees = ['_build', 'draft']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  Major themes that come with
# Sphinx are currently 'default' and 'sphinxdoc'.
html_theme = 'armstrong'


# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
html_theme_path = ["_themes", ]

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None
html_logo = '_static/logo.svg'

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_use_modindex = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# If nonempty, this is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = ''

# Output file base name for HTML help builder.
htmlhelp_basename = 'rdflibdoc'


# -- Options for LaTeX output --------------------------------------------------

# The paper size ('letter' or 'a4').
#latex_paper_size = 'letter'

# The font size ('10pt', '11pt' or '12pt').
#latex_font_size = '10pt'

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'rdflib.tex', u'rdflib Documentation',
   u'RDFLib Team', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# Additional stuff for the LaTeX preamble.
#latex_preamble = ''

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_use_modindex = True


# Example configuration for intersphinx: refer to the Python standard library.
intersphinx_mapping = {
    'python': ('http://docs.python.org/2.7', None),
}

########NEW FILE########
__FILENAME__ = plugintable
"""
Crappy utility for generating Sphinx tables 
for rdflib plugins
"""

import sys

from rdflib.plugin import _plugins

cls = sys.argv[1]

p = {}

for (name, kind), plugin in _plugins.items(): 
    if "/" in name: continue # skip duplicate entries for mimetypes
    if cls == kind.__name__: 
        p[name]="%s.%s"%(plugin.module_path, plugin.class_name)

l1=max(len(x) for x in p)
l2=max(10+len(x) for x in p.values())

def hr():
    print "="*l1,"="*l2

hr()
print "%-*s"%(l1,"Name"), "%-*s"%(l2, "Class")
hr()

for n in sorted(p): 
    print "%-*s"%(l1,n), ":class:`~%s`"%p[n]
hr()
print 


########NEW FILE########
__FILENAME__ = conjunctive_graphs
"""

An RDFLib ConjunctiveGraph is an (unamed) aggregation of all the named graphs
within a Store. The :meth:`~rdflib.graph.ConjunctiveGraph.get_context`
method can be used to get a particular named graph, or triples can be
added to the default graph

This example shows how to create some named graphs and work with the
conjunction of all the graphs.

"""

from rdflib import Namespace, Literal, URIRef
from rdflib.graph import Graph, ConjunctiveGraph
from rdflib.plugins.memory import IOMemory

if __name__=='__main__':


    ns = Namespace("http://love.com#")

    mary = URIRef("http://love.com/lovers/mary#")
    john = URIRef("http://love.com/lovers/john#")

    cmary=URIRef("http://love.com/lovers/mary#")
    cjohn=URIRef("http://love.com/lovers/john#")

    store = IOMemory()

    g = ConjunctiveGraph(store=store)
    g.bind("love",ns)

    gmary = Graph(store=store, identifier=cmary)

    gmary.add((mary, ns['hasName'], Literal("Mary")))
    gmary.add((mary, ns['loves'], john))

    gjohn = Graph(store=store, identifier=cjohn)
    gjohn.add((john, ns['hasName'], Literal("John")))

    #enumerate contexts
    for c in g.contexts():
        print("-- %s " % c)

    #separate graphs
    print(gjohn.serialize(format='n3'))
    print("===================")
    print(gmary.serialize(format='n3'))
    print("===================")

    #full graph
    print(g.serialize(format='n3'))

    # query the conjunction of all graphs

    print 'Mary loves:'
    for x in g[mary : ns.loves/ns.hasName]:
        print x

########NEW FILE########
__FILENAME__ = custom_datatype
"""

RDFLib can map between data-typed literals and python objects. 

Mapping for integers, floats, dateTimes, etc. are already added, but
you can also add your own.

This example shows how :meth:`rdflib.term.bind` lets you register new
mappings between literal datatypes and python objects

"""


from rdflib import Graph, Literal, Namespace, XSD
from rdflib.term import bind

if __name__=='__main__':

    # complex numbers are not registered by default
    # no custom constructor/serializer needed since 
    # complex('(2+3j)') works fine
    bind(XSD.complexNumber, complex) 

    ns=Namespace("urn:my:namespace:")

    c=complex(2,3)

    l=Literal(c)

    g=Graph()
    g.add((ns.mysubject, ns.myprop, l))

    n3=g.serialize(format='n3')

    # round-trip through n3

    g2=Graph()
    g2.parse(data=n3, format='n3')

    l2=list(g2)[0][2]

    print l2 

    print l2.value == c # back to a python complex object



########NEW FILE########
__FILENAME__ = custom_eval
"""

This example shows how a custom evaluation function can be added to
handle certain SPARQL Algebra elements

A custom function is added that adds ``rdfs:subClassOf`` "inference" when
asking for ``rdf:type`` triples.

Here the custom eval function is added manually, normally you would use
setuptools and entry_points to do it:
i.e. in your setup.py::

    entry_points = {
        'rdf.plugins.sparqleval': [
            'myfunc =     mypackage:MyFunction',
            ],
    }

"""

import rdflib

from rdflib.plugins.sparql.evaluate import evalBGP
from rdflib.namespace import FOAF

inferredSubClass = \
    rdflib.RDFS.subClassOf * '*'  # any number of rdfs.subClassOf


def customEval(ctx, part):
    """
    Rewrite triple patterns to get super-classes
    """

    if part.name == 'BGP':

        # rewrite triples
        triples = []
        for t in part.triples:
            if t[1] == rdflib.RDF.type:
                bnode = rdflib.BNode()
                triples.append((t[0], t[1], bnode))
                triples.append((bnode, inferredSubClass, t[2]))
            else:
                triples.append(t)

        # delegate to normal evalBGP
        return evalBGP(ctx, triples)

    raise NotImplementedError()

if __name__=='__main__':

    # add function directly, normally we would use setuptools and entry_points
    rdflib.plugins.sparql.CUSTOM_EVALS['exampleEval'] = customEval

    g = rdflib.Graph()
    g.load("foaf.rdf")

    # Add the subClassStmt so that we can query for it!
    g.add((FOAF.Person,
           rdflib.RDFS.subClassOf,
           FOAF.Agent))

    # Find all FOAF Agents
    for x in g.query(
            'PREFIX foaf: <%s> SELECT * WHERE { ?s a foaf:Agent . }' % FOAF):
        print x

########NEW FILE########
__FILENAME__ = film
#!/usr/bin/env python
""" 

film.py: a simple tool to manage your movies review
Simon Rozet, http://atonie.org/

@@ :
- manage directors and writers
- manage actors
- handle non IMDB uri
- markdown support in comment

Requires download and import of Python imdb library from 
http://imdbpy.sourceforge.net/ - (warning: installation
will trigger automatic installation of several other packages)

--
Usage:
    film.py whoami "John Doe <john@doe.org>"
        Initialize the store and set your name and email.
    film.py whoami
        Tell you who you are
    film.py http://www.imdb.com/title/tt0105236/
        Review the movie "Reservoir Dogs"
"""
import datetime, os, sys, re, time

try: 
    import imdb
except ImportError: 
    imdb = None

from rdflib import BNode, ConjunctiveGraph, URIRef, Literal, Namespace, RDF
from rdflib.namespace import FOAF, DC


storefn = os.path.expanduser('~/movies.n3')
#storefn = '/home/simon/codes/film.dev/movies.n3'
storeuri = 'file://'+storefn
title = 'Movies viewed by %s'

r_who = re.compile('^(.*?) <([a-z0-9_-]+(\.[a-z0-9_-]+)*@[a-z0-9_-]+(\.[a-z0-9_-]+)+)>$')

IMDB = Namespace('http://www.csd.abdn.ac.uk/~ggrimnes/dev/imdb/IMDB#')
REV = Namespace('http://purl.org/stuff/rev#')

class Store:
    def __init__(self):
        self.graph = ConjunctiveGraph()
        if os.path.exists(storefn):
            self.graph.load(storeuri, format='n3')
        self.graph.bind('dc', DC)
        self.graph.bind('foaf', FOAF)
        self.graph.bind('imdb', IMDB)
        self.graph.bind('rev', 'http://purl.org/stuff/rev#')
    
    def save(self):
        self.graph.serialize(storeuri, format='n3')
    
    def who(self, who=None):
        if who is not None:
            name, email = (r_who.match(who).group(1), r_who.match(who).group(2))
            self.graph.add((URIRef(storeuri), DC['title'], Literal(title % name)))
            self.graph.add((URIRef(storeuri+'#author'), RDF.type, FOAF['Person']))
            self.graph.add((URIRef(storeuri+'#author'), FOAF['name'], Literal(name)))
            self.graph.add((URIRef(storeuri+'#author'), FOAF['mbox'], Literal(email)))
            self.save()
        else:
            return self.graph.objects(URIRef(storeuri+'#author'), FOAF['name'])
        
    def new_movie(self, movie):
        movieuri = URIRef('http://www.imdb.com/title/tt%s/' % movie.movieID)
        self.graph.add((movieuri, RDF.type, IMDB['Movie']))
        self.graph.add((movieuri, DC['title'], Literal(movie['title'])))
        self.graph.add((movieuri, IMDB['year'], Literal(int(movie['year']))))
        self.save()
    
    def new_review(self, movie, date, rating, comment=None):
        review = BNode() # @@ humanize the identifier (something like #rev-$date)
        movieuri = URIRef('http://www.imdb.com/title/tt%s/' % movie.movieID)
        self.graph.add((movieuri, REV['hasReview'], URIRef('%s#%s' % (storeuri, review))))
        self.graph.add((review, RDF.type, REV['Review']))
        self.graph.add((review, DC['date'], Literal(date)))
        self.graph.add((review, REV['maxRating'], Literal(5)))
        self.graph.add((review, REV['minRating'], Literal(0)))
        self.graph.add((review, REV['reviewer'], URIRef(storeuri+'#author')))
        self.graph.add((review, REV['rating'], Literal(rating)))
        if comment is not None:
            self.graph.add((review, REV['text'], Literal(comment)))
        self.save()

    def movie_is_in(self, uri):
        return (URIRef(uri), RDF.type, IMDB['Movie']) in self.graph
        
def help():
    print(__doc__.split('--')[1])

def main(argv=None):
    if not argv:
        argv = sys.argv
    s = Store()
    if argv[1] in ('help', '--help', 'h', '-h'):
        help()
    elif argv[1] == 'whoami':
        if os.path.exists(storefn):
            print(list(s.who())[0])
        else:
            s.who(argv[2])
    elif argv[1].startswith('http://www.imdb.com/title/tt'):
        if s.movie_is_in(argv[1]):
            raise
        else:
            i = imdb.IMDb()
            movie = i.get_movie(argv[1][len('http://www.imdb.com/title/tt'):-1])
            print('%s (%s)' % (movie['title'].encode('utf-8'), movie['year']))
            for director in movie['director']:
                print('directed by: %s' % director['name'].encode('utf-8'))
            for writer in movie['writer']:
                print('written by: %s' % writer['name'].encode('utf-8'))
            s.new_movie(movie)
            rating = None
            while not rating or (rating > 5 or rating <= 0):
                try:
                    rating = int(raw_input('Rating (on five): '))
                except ValueError:
                    rating = None
            date = None
            while not date:
                try:
                    i = raw_input('Review date (YYYY-MM-DD): ')
                    date = datetime.datetime(*time.strptime(i, '%Y-%m-%d')[:6])
                except:
                    date = None
            comment = raw_input('Comment: ')
            s.new_review(movie, date, rating, comment)
    else:
        help()

if __name__ == '__main__':
    if not imdb: 
        raise Exception('This example requires the IMDB library! Install with "pip install imdbpy"')
    main()

########NEW FILE########
__FILENAME__ = foafpaths
"""

SPARQL 1.1 defines path operators for combining/repeating predicates
in triple-patterns.

We overload some python operators on URIRefs to allow creating path
operators directly in python. 

============ =========================================
Operator     Path
============ =========================================
``p1 / p2``  Path sequence
``p1 | p2``  Path alternative
``p1 * '*'`` chain of 0 or more p's
``p1 * '+'`` chain of 1 or more p's
``p1 * '?'`` 0 or 1 p 
``~p1``      p1 inverted, i.e. (s p1 o) <=> (o ~p1 s)
``-p1``      NOT p1, i.e. any property but p1
============ =========================================


these can then be used in property position for ``s,p,o`` triple queries
for any graph method.

See the docs for :mod:`rdflib.paths` for the details.

This example shows how to get the name of friends with a single query.

"""

from rdflib import URIRef, Graph
from rdflib.namespace import FOAF

if __name__=='__main__':

    g = Graph()
    g.load("foaf.rdf")

    tim = URIRef("http://www.w3.org/People/Berners-Lee/card#i")

    print "Timbl knows:"

    for o in g.objects(tim, FOAF.knows / FOAF.name):
        print o

########NEW FILE########
__FILENAME__ = prepared_query

"""

SPARQL Queries be prepared (i.e parsed and translated to SPARQL algebra)
by the :meth:`rdflib.plugins.sparql.prepareQuery` method.

When executing, variables can be bound with the 
``initBindings`` keyword parameter


"""

import rdflib
from rdflib.plugins.sparql import prepareQuery
from rdflib.namespace import FOAF

if __name__=='__main__':

    q = prepareQuery(
        'SELECT ?s WHERE { ?person foaf:knows ?s .}', 
        initNs = { "foaf": FOAF })

    g = rdflib.Graph()
    g.load("foaf.rdf")

    tim = rdflib.URIRef("http://www.w3.org/People/Berners-Lee/card#i")

    for row in g.query(q, initBindings={'person': tim}):
        print row

########NEW FILE########
__FILENAME__ = rdfa_example
"""
 
A simple example showing how to process RDFa from the web

"""

from rdflib import Graph

if __name__ == '__main__':
    g = Graph()

    g.parse('http://www.worldcat.org/title/library-of-babel/oclc/44089369', format='rdfa')

    print "Books found:"

    for row in g.query("""SELECT ?title ?author WHERE { 
       [ a schema:Book ; 
         schema:author [ rdfs:label ?author ] ;
         schema:name ?title ]
       FILTER (LANG(?title) = 'en') } """):

        print "%s by %s"%(row.title, row.author)

########NEW FILE########
__FILENAME__ = resource
"""

RDFLib has a :class:`~rdflib.resource.Resource` class, for a resource-centric API.

A resource acts like a URIRef with an associated graph, and allows
quickly adding or querying for triples where this resource is the
subject.


"""

from rdflib import Graph, RDF, RDFS, Literal
from rdflib.namespace import FOAF

if __name__=='__main__':

    g = Graph()

    bob = g.resource('urn:bob')

    bob.set(RDF.type, FOAF.Person) # .set replaces all other values
    bob.set(FOAF.name, Literal("Bob"))


    bill = g.resource('urn:bill')

    bill.add(RDF.type, FOAF.Person) # add adds to existing values
    bill.add(RDF.type, FOAF.Agent)
    bill.set(RDFS.label, Literal("Bill"))

    bill.add(FOAF.knows, bob)

    # Resources returned when querying are 'auto-boxed' as resources:

    print "Bill's friend: ", bill.value(FOAF.knows).value(FOAF.name)

    # slicing ([] syntax) can also be used: 

    print "Bill knows: ",
    for friend in bill[FOAF.knows]: 
        print friend[FOAF.name].next(), " "

    # or even quicker with paths:
    print "Bill knows: ",
    for friend in bill[FOAF.knows/FOAF.name]:
        print friend

    # setting single properties is also possible:
    bill[RDFS.label]=Literal("William")

    print g.serialize(format='n3')

########NEW FILE########
__FILENAME__ = simple_example

from rdflib import Graph, Literal, BNode, RDF
from rdflib.namespace import FOAF, DC

if __name__=='__main__':

    store = Graph()

    # Bind a few prefix, namespace pairs for pretty output
    store.bind("dc", DC)
    store.bind("foaf", FOAF)

    # Create an identifier to use as the subject for Donna.
    donna = BNode()

    # Add triples using store's add method.
    store.add((donna, RDF.type, FOAF.Person))
    store.add((donna, FOAF.nick, Literal("donna", lang="foo")))
    store.add((donna, FOAF.name, Literal("Donna Fales")))

    # Iterate over triples in store and print them out.
    print "--- printing raw triples ---"
    for s, p, o in store:
        print s, p, o

    # For each foaf:Person in the store print out its mbox property.
    print "--- printing mboxes ---"
    for person in store.subjects(RDF.type, FOAF["Person"]):
        for mbox in store.objects(person, FOAF["mbox"]):
            print mbox

    # Serialize the store as RDF/XML to the file donna_foaf.rdf.
    store.serialize("donna_foaf.rdf", format="pretty-xml", max_depth=3)

    # Let's show off the serializers

    print "RDF Serializations:"

    # Serialize as XML
    print "--- start: rdf-xml ---"
    print store.serialize(format="pretty-xml")
    print "--- end: rdf-xml ---\n"

    # Serialize as Turtle
    print "--- start: turtle ---"
    print store.serialize(format="turtle")
    print "--- end: turtle ---\n"

    # Serialize as NTriples
    print "--- start: ntriples ---"
    print store.serialize(format="nt")
    print "--- end: ntriples ---\n"
 

########NEW FILE########
__FILENAME__ = sleepycat_example
"""

A simple example showing how to use a Sleepycat store to do on-disk
persistence.

"""

from rdflib import ConjunctiveGraph, Namespace, Literal
from rdflib.store import NO_STORE, VALID_STORE

from tempfile import mktemp

if __name__ == '__main__':
    path = mktemp()

    # Open previously created store, or create it if it doesn't exist yet
    graph = ConjunctiveGraph('Sleepycat')
    
    rt = graph.open(path, create=False)

    if rt == NO_STORE:
        # There is no underlying Sleepycat infrastructure, create it
        graph.open(path, create=True)
    else:
        assert rt == VALID_STORE, 'The underlying store is corrupt'

    print 'Triples in graph before add: ', len(graph)

    # Now we'll add some triples to the graph & commit the changes
    rdflib = Namespace('http://rdflib.net/test/')
    graph.bind('test', 'http://rdflib.net/test/')

    graph.add((rdflib['pic:1'], rdflib.name, Literal('Jane & Bob')))
    graph.add((rdflib['pic:2'], rdflib.name, Literal('Squirrel in Tree')))

    print 'Triples in graph after add: ', len(graph)

    # display the graph in RDF/XML
    print graph.serialize(format='n3')

    # close when done, otherwise sleepycat will leak lock entries. 
    graph.close()
    
    graph = None
    
    # reopen the graph
    
    graph = ConjunctiveGraph('Sleepycat')
    
    graph.open(path, create = False) 
    
    print 'Triples still in graph: ', len(graph)
    
    graph.close()

    # Clean up the temp folder to remove the Sleepycat database files...
    import os
    for f in os.listdir(path): 
        os.unlink(path+'/'+f)
    os.rmdir(path)

########NEW FILE########
__FILENAME__ = slice
"""

RDFLib Graphs (and Resources) can be "sliced" with [] syntax

This is a short-hand for iterating over triples

Combined with SPARQL paths (see ``foafpaths.py``) - quite complex queries
can be realised.

See :meth:`rdflib.graph.Graph.__getitem__` for details

"""

from rdflib import Graph, RDF
from rdflib.namespace import FOAF

if __name__=='__main__':

    graph = Graph()

    graph.load("foaf.rdf")

    for person in graph[: RDF.type : FOAF.Person]:

        friends = list(graph[person:FOAF.knows * '+'/FOAF.name])
        if friends: 
            print "%s's circle of friends:"%graph.value(person, FOAF.name)
            for name in friends:
                print name


########NEW FILE########
__FILENAME__ = smushing
"""
A FOAF smushing example.

Filter a graph by normalizing all ``foaf:Persons`` into URIs based on
their ``mbox_sha1sum``.

Suppose I got two `FOAF <http://xmlns.com/foaf/0.1>`_ documents each
talking about the same person (according to ``mbox_sha1sum``) but they
each used a :class:`rdflib.term.BNode` for the subject. For this demo
I've combined those two documents into one file:

This filters a graph by changing every subject with a
``foaf:mbox_sha1sum`` into a new subject whose URI is based on the
``sha1sum``. This new graph might be easier to do some operations on.

An advantage of this approach over other methods for collapsing BNodes
is that I can incrementally process new FOAF documents as they come in
without having to access my ever-growing archive. Even if another
``65b983bb397fb71849da910996741752ace8369b`` document comes in next
year, I would still give it the same stable subject URI that merges
with my existing data.

"""

from rdflib import Graph, Namespace
from rdflib.namespace import FOAF

STABLE = Namespace("http://example.com/person/mbox_sha1sum/")

if __name__=='__main__':
    g = Graph()
    g.parse("smushingdemo.n3", format="n3")

    newURI = {} # old subject : stable uri
    for s,p,o in g.triples((None, FOAF['mbox_sha1sum'], None)):
        newURI[s] = STABLE[o]


    out = Graph()
    out.bind('foaf', FOAF)

    for s,p,o in g:
        s = newURI.get(s, s)
        o = newURI.get(o, o) # might be linked to another person
        out.add((s,p,o))

    print out.serialize(format="n3")

########NEW FILE########
__FILENAME__ = sparqlstore_example
"""

A simple example showing how to use the SPARQLStore

"""

from rdflib import Graph, URIRef, Namespace

if __name__ == '__main__': 

    dbo = Namespace('http://dbpedia.org/ontology/')

    graph = Graph('SPARQLStore') 

    graph.open("http://dbpedia.org/sparql")

    pop = graph.value(
        URIRef("http://dbpedia.org/resource/Berlin"), 
        dbo.populationTotal)

    print "According to DBPedia Berlin has a population of", pop
              

########NEW FILE########
__FILENAME__ = sparql_query_example

"""

SPARQL Query using :meth:`rdflib.graph.Graph.query`

The method returns a :class:`~rdflib.query.Result`, iterating over
this yields :class:`~rdflib.query.ResultRow` objects 

The variable bindings can be access as attributes of the row objects
For variable names that are not valid python identifiers, dict access
(i.e. with ``row[var] / __getitem__``) is also possible.

:attr:`~rdflib.query.ResultRow.vars` contains the variables

"""

import rdflib

if __name__=='__main__':

    g = rdflib.Graph()
    g.load("foaf.rdf")

    # the QueryProcessor knows the FOAF prefix from the graph
    # which in turn knows it from reading the RDF/XML file
    for row in g.query(
            'select ?s where { [] foaf:knows ?s .}'):
        print row.s 
        # or row["s"]
        # or row[rdflib.Variable("s")]





########NEW FILE########
__FILENAME__ = sparql_update_example

"""

SPARQL Update statements can be applied with :meth:`rdflib.graph.Graph.update`

"""

import rdflib

if __name__=='__main__':

    g = rdflib.Graph()
    g.load("foaf.rdf")

    g.update('''
    PREFIX foaf: <http://xmlns.com/foaf/0.1/>
    PREFIX dbpedia: <http://dbpedia.org/resource/>
    INSERT
        { ?s a dbpedia:Human . }
    WHERE
        { ?s a foaf:Person . }
    ''')

    for x in g.subjects(
            rdflib.RDF.type, rdflib.URIRef('http://dbpedia.org/resource/Human')):
        print x

########NEW FILE########
__FILENAME__ = swap_primer
"""

This is a simple primer using some of the 
example stuff in the Primer on N3:

http://www.w3.org/2000/10/swap/Primer

"""

# Load up RDFLib

from rdflib import ConjunctiveGraph, Namespace, Literal
from rdflib.namespace import OWL, DC

if __name__=='__main__':

    # Firstly, it doesn't have to be so complex.
    # Here we create a "Graph" of our work.
    # Think of it as a blank piece of graph paper!

    primer = ConjunctiveGraph()
    myNS = Namespace('#')

    primer.add((myNS.pat, myNS.knows, myNS.jo))
    # or:
    primer.add((myNS['pat'], myNS['age'], Literal(24)))


    # Now, with just that, lets see how the system
    # recorded *way* too many details about what
    # you just asserted as fact.
    #

    from pprint import pprint
    pprint(list(primer))


    # just think .whatever((s, p, o))
    # here we report on what we know

    pprint(list(primer.subjects()))
    pprint(list(primer.predicates()))
    pprint(list(primer.objects()))

    # and other things that make sense

    # what do we know about pat?
    pprint(list(primer.predicate_objects(myNS.pat)))

    # who is what age?
    pprint(list(primer.subject_objects(myNS.age)))



    # Okay, so lets now work with a bigger
    # dataset from the example, and start
    # with a fresh new graph.


    primer = ConjunctiveGraph()


    # Lets start with a verbatim string straight from the primer text:

    mySource = """


    @prefix : <http://www.w3.org/2000/10/swap/Primer#>.
    @prefix rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
    @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
    @prefix owl:  <http://www.w3.org/2002/07/owl#> .
    @prefix dc:  <http://purl.org/dc/elements/1.1/> .
    @prefix foo: <http://www.w3.org/2000/10/swap/Primer#>.
    @prefix swap: <http://www.w3.org/2000/10/swap/>.

    <> dc:title
      "Primer - Getting into the Semantic Web and RDF using N3".

    <#pat> <#knows> <#jo> .
    <#pat> <#age> 24 .
    <#al> is <#child> of <#pat> .

    <#pat> <#child>  <#al>, <#chaz>, <#mo> ;
           <#age>    24 ;
           <#eyecolor> "blue" .


    :Person a rdfs:Class.

    :Pat a :Person.

    :Woman a rdfs:Class; rdfs:subClassOf :Person .

    :sister a rdf:Property.

    :sister rdfs:domain :Person; 
            rdfs:range :Woman.

    :Woman = foo:FemaleAdult .
    :Title a rdf:Property; = dc:title .



    """ # --- End of primer code

    # To make this go easier to spit back out... 
    # technically, we already created a namespace
    # with the object init (and it added some namespaces as well)
    # By default, your main namespace is the URI of your 
    # current working directory, so lets make that simpler:

    myNS = Namespace('http://www.w3.org/2000/10/swap/Primer#')
    primer.bind('', myNS)
    primer.bind('owl', OWL)
    primer.bind('dc', DC)
    primer.bind('swap', 'http://www.w3.org/2000/10/swap/')

    # Lets load it up!

    primer.parse(data=mySource, format='n3')


    # Now you can query, either directly straight into a list:

    [(x, y, z) for x, y, z in primer]

    # or spit it back out (mostly) the way we created it:

    print primer.serialize(format='n3')

    # for more insight into things already done, lets see the namespaces

    list(primer.namespaces())

    # lets ask something about the data

    list(primer.objects(myNS.pat, myNS.child))



########NEW FILE########
__FILENAME__ = transitive
"""

An example illustrating how to use the
:meth:`~rdflib.graph.Graph.transitive_subjects` and
:meth:`~rdflib.graph.Graph.transitive_objects` graph methods

Formal definition
^^^^^^^^^^^^^^^^^^

The :meth:`~rdflib.graph.Graph.transitive_objects` method finds all
nodes such that there is a path from subject to one of those nodes
using only the predicate property in the triples. The
:meth:`~rdflib.graph.Graph.transitive_subjects` method is similar; it
finds all nodes such that there is a path from the node to the object
using only the predicate property.

Informal description, with an example
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In brief, :meth:`~rdflib.graph.Graph.transitive_objects` walks forward
in a graph using a particular property, and
:meth:`~rdflib.graph.Graph.transitive_subjects` walks backward. A good
example uses a property ``ex:parent``, the semantics of which are
biological parentage. The
:meth:`~rdflib.graph.Graph.transitive_objects` method would get all
the ancestors of a particular person (all nodes such that there is a
parent path between the person and the object). The
:meth:`~rdflib.graph.Graph.transitive_subjects` method would get all
the descendants of a particular person (all nodes such that there is a
parent path between the node and the person). So, say that your URI is
``ex:person``.

This example would get all of your (known) ancestors, and then get all
the (known) descendants of your maternal grandmother.

.. warning:: The :meth:`transitive_objects` method has the start node
    as the *first* argument, but the :meth:`transitive_subjects`
    method has the start node as the *second* argument.

User-defined transitive closures
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The method :meth:`~rdflib.graph.Graph.transitiveClosure` returns
transtive closures of user-defined functions.

"""

if __name__=='__main__':
    from rdflib import ConjunctiveGraph, URIRef

    person = URIRef('ex:person')
    dad = URIRef('ex:d')
    mom = URIRef('ex:m')
    momOfDad = URIRef('ex:gm0')
    momOfMom = URIRef('ex:gm1')
    dadOfDad = URIRef('ex:gf0')
    dadOfMom = URIRef('ex:gf1')

    parent = URIRef('ex:parent')

    g = ConjunctiveGraph()
    g.add((person, parent, dad))
    g.add((person, parent, mom))
    g.add((dad, parent, momOfDad))
    g.add((dad, parent, dadOfDad))
    g.add((mom, parent, momOfMom))
    g.add((mom, parent, dadOfMom))

    print "Parents, forward from `ex:person`:"
    for i in g.transitive_objects(person, parent):
        print i

    print "Parents, *backward* from `ex:gm1`:"
    for i in g.transitive_subjects(parent, momOfMom):
        print i
      

########NEW FILE########
__FILENAME__ = collection
from rdflib.namespace import RDF
from rdflib.term import BNode
from rdflib.term import Literal
from rdflib.graph import Graph
from rdflib.py3compat import format_doctest_out

__all__ = ['Collection']


class Collection(object):
    __doc__ = format_doctest_out("""
    See 3.3.5 Emulating container types:
    http://docs.python.org/ref/sequence-types.html#l2h-232

    >>> from rdflib.graph import Graph
    >>> from pprint import pprint
    >>> listName = BNode()
    >>> g = Graph('IOMemory')
    >>> listItem1 = BNode()
    >>> listItem2 = BNode()
    >>> g.add((listName, RDF.first, Literal(1)))
    >>> g.add((listName, RDF.rest, listItem1))
    >>> g.add((listItem1, RDF.first, Literal(2)))
    >>> g.add((listItem1, RDF.rest, listItem2))
    >>> g.add((listItem2, RDF.rest, RDF.nil))
    >>> g.add((listItem2, RDF.first, Literal(3)))
    >>> c = Collection(g,listName)
    >>> pprint([term.n3() for term in c])
    [%(u)s'"1"^^<http://www.w3.org/2001/XMLSchema#integer>',
     %(u)s'"2"^^<http://www.w3.org/2001/XMLSchema#integer>',
     %(u)s'"3"^^<http://www.w3.org/2001/XMLSchema#integer>']

    >>> Literal(1) in c
    True
    >>> len(c)
    3
    >>> c._get_container(1) == listItem1
    True
    >>> c.index(Literal(2)) == 1
    True
    """)

    def __init__(self, graph, uri, seq=[]):
        self.graph = graph
        self.uri = uri or BNode()
        for item in seq:
            self.append(item)

    def n3(self):
        """
        >>> from rdflib.graph import Graph
        >>> listName = BNode()
        >>> g = Graph('IOMemory')
        >>> listItem1 = BNode()
        >>> listItem2 = BNode()
        >>> g.add((listName, RDF.first, Literal(1)))
        >>> g.add((listName, RDF.rest, listItem1))
        >>> g.add((listItem1, RDF.first, Literal(2)))
        >>> g.add((listItem1, RDF.rest, listItem2))
        >>> g.add((listItem2, RDF.rest, RDF.nil))
        >>> g.add((listItem2, RDF.first, Literal(3)))
        >>> c = Collection(g, listName)
        >>> print(c.n3()) #doctest: +NORMALIZE_WHITESPACE
        ( "1"^^<http://www.w3.org/2001/XMLSchema#integer>
          "2"^^<http://www.w3.org/2001/XMLSchema#integer>
          "3"^^<http://www.w3.org/2001/XMLSchema#integer> )
        """
        return "( %s )" % (' '.join([i.n3() for i in self]))

    def _get_container(self, index):
        """Gets the first, rest holding node at index."""
        assert isinstance(index, int)
        graph = self.graph
        container = self.uri
        i = 0
        while i < index:
            i += 1
            container = graph.value(container, RDF.rest)
            if container is None:
                break
        return container

    def __len__(self):
        """length of items in collection."""
        count = 0
        links = set()
        for item in self.graph.items(self.uri):
            assert item not in links, \
                "There is a loop in the RDF list! " + \
                "(%s has been processed before)" % item
            links.add(item)
            count += 1
        return count

    def index(self, item):
        """
        Returns the 0-based numerical index of the item in the list
        """
        listName = self.uri
        index = 0
        while True:
            if (listName, RDF.first, item) in self.graph:
                return index
            else:
                newLink = list(self.graph.objects(listName, RDF.rest))
                index += 1
                if newLink == [RDF.nil]:
                    raise ValueError("%s is not in %s" % (item, self.uri))
                elif not newLink:
                    raise Exception("Malformed RDF Collection: %s" % self.uri)
                else:
                    assert len(newLink) == 1, \
                        "Malformed RDF Collection: %s" % self.uri
                    listName = newLink[0]

    def __getitem__(self, key):
        """TODO"""
        c = self._get_container(key)
        if c:
            v = self.graph.value(c, RDF.first)
            if v:
                return v
            else:
                raise KeyError(key)
        else:
            raise IndexError(key)

    def __setitem__(self, key, value):
        """TODO"""
        c = self._get_container(key)
        if c:
            self.graph.add((c, RDF.first, value))
        else:
            raise IndexError(key)

    def __delitem__(self, key):
        """
        >>> from rdflib.namespace import RDF, RDFS
        >>> from rdflib import Graph
        >>> from pprint import pformat
        >>> g = Graph()
        >>> a = BNode('foo')
        >>> b = BNode('bar')
        >>> c = BNode('baz')
        >>> g.add((a, RDF.first, RDF.type))
        >>> g.add((a, RDF.rest, b))
        >>> g.add((b, RDF.first, RDFS.label))
        >>> g.add((b, RDF.rest, c))
        >>> g.add((c, RDF.first, RDFS.comment))
        >>> g.add((c, RDF.rest, RDF.nil))
        >>> len(g)
        6
        >>> def listAncestry(node, graph):
        ...   for i in graph.subjects(RDF.rest, node):
        ...     yield i
        >>> [str(node.n3())
        ...   for node in g.transitiveClosure(listAncestry, RDF.nil)]
        ['_:baz', '_:bar', '_:foo']
        >>> lst = Collection(g, a)
        >>> len(lst)
        3
        >>> b == lst._get_container(1)
        True
        >>> c == lst._get_container(2)
        True
        >>> del lst[1]
        >>> len(lst)
        2
        >>> len(g)
        4

        """
        self[key]  # to raise any potential key exceptions
        graph = self.graph
        current = self._get_container(key)
        assert current
        if len(self) == 1 and key > 0:
            pass
        elif key == len(self) - 1:
            # the tail
            priorLink = self._get_container(key - 1)
            self.graph.set((priorLink, RDF.rest, RDF.nil))
            graph.remove((current, None, None))
        else:
            next = self._get_container(key + 1)
            prior = self._get_container(key - 1)
            assert next and prior
            graph.remove((current, None, None))
            graph.set((prior, RDF.rest, next))

    def __iter__(self):
        """Iterator over items in Collections"""
        return self.graph.items(self.uri)

    def append(self, item):
        """
        >>> from rdflib.graph import Graph
        >>> listName = BNode()
        >>> g = Graph()
        >>> c = Collection(g,listName,[Literal(1),Literal(2)])
        >>> links = [
        ...     list(g.subjects(object=i, predicate=RDF.first))[0] for i in c]
        >>> len([i for i in links if (i,RDF.rest, RDF.nil) in g])
        1

        """
        container = self.uri
        graph = self.graph
        # iterate to the end of the linked list
        rest = graph.value(container, RDF.rest)
        while rest:
            if rest == RDF.nil:
                # the end, append to the end of the linked list
                node = BNode()
                graph.set((container, RDF.rest, node))
                container = node
                break
            else:
                # move down one link
                if container != self.uri:
                    rest = graph.value(rest, RDF.rest)
                if not rest == RDF.nil:
                    container = rest
        graph.add((container, RDF.first, item))
        graph.add((container, RDF.rest, RDF.nil))

    def clear(self):
        container = self.uri
        graph = self.graph
        while container:
            rest = graph.value(container, RDF.rest)
            graph.remove((container, RDF.first, None))
            graph.remove((container, RDF.rest, None))
            container = rest


def test():
    import doctest
    doctest.testmod()

if __name__ == "__main__":
    test()

    g = Graph()

    c = Collection(g, BNode())

    assert len(c) == 0

    c = Collection(
        g, BNode(), [Literal("1"), Literal("2"), Literal("3"), Literal("4")])

    assert len(c) == 4

    assert c[1] == Literal("2"), c[1]

    del c[1]

    assert list(c) == [Literal("1"), Literal("3"), Literal("4")], list(c)

    try:
        del c[500]
    except IndexError, i:
        pass

    c.append(Literal("5"))

    print(list(c))

    for i in c:
        print(i)

    del c[3]

    c.clear()

    assert len(c) == 0

########NEW FILE########
__FILENAME__ = compare
# -*- coding: utf-8 -*-
"""
A collection of utilities for canonicalizing and inspecting graphs.

Among other things, they solve of the problem of deterministic bnode
comparisons.

Warning: the time to canonicalize bnodes may increase exponentially on larger
graphs. Use with care!

Example of comparing two graphs::

    >>> g1 = Graph().parse(format='n3', data='''
    ...     @prefix : <http://example.org/ns#> .
    ...     <http://example.org> :rel
    ...         <http://example.org/same>,
    ...         [ :label "Same" ],
    ...         <http://example.org/a>,
    ...         [ :label "A" ] .
    ... ''')
    >>> g2 = Graph().parse(format='n3', data='''
    ...     @prefix : <http://example.org/ns#> .
    ...     <http://example.org> :rel
    ...         <http://example.org/same>,
    ...         [ :label "Same" ],
    ...         <http://example.org/b>,
    ...         [ :label "B" ] .
    ... ''')
    >>>
    >>> iso1 = to_isomorphic(g1)
    >>> iso2 = to_isomorphic(g2)

These are not isomorphic::

    >>> iso1 == iso2
    False

Diff the two graphs::

    >>> in_both, in_first, in_second = graph_diff(iso1, iso2)

Present in both::

    >>> def dump_nt_sorted(g):
    ...     for l in sorted(g.serialize(format='nt').splitlines()):
    ...         if l: print(l.decode('ascii'))

    >>> dump_nt_sorted(in_both) #doctest: +SKIP
    <http://example.org>
        <http://example.org/ns#rel> <http://example.org/same> .
    <http://example.org>
        <http://example.org/ns#rel> _:cbcaabaaba17fecbc304a64f8edee4335e .
    _:cbcaabaaba17fecbc304a64f8edee4335e
        <http://example.org/ns#label> "Same" .

Only in first::

    >>> dump_nt_sorted(in_first) #doctest: +SKIP
    <http://example.org>
        <http://example.org/ns#rel> <http://example.org/a> .
    <http://example.org>
        <http://example.org/ns#rel> _:cb124e4c6da0579f810c0ffe4eff485bd9 .
    _:cb124e4c6da0579f810c0ffe4eff485bd9
        <http://example.org/ns#label> "A" .

Only in second::

    >>> dump_nt_sorted(in_second) #doctest: +SKIP
    <http://example.org>
        <http://example.org/ns#rel> <http://example.org/b> .
    <http://example.org>
        <http://example.org/ns#rel> _:cb558f30e21ddfc05ca53108348338ade8 .
    _:cb558f30e21ddfc05ca53108348338ade8
        <http://example.org/ns#label> "B" .
"""


# TODO:
# - Doesn't handle quads.
# - Add warning and/or safety mechanism before working on large graphs?
# - use this in existing Graph.isomorphic?

__all__ = ['IsomorphicGraph', 'to_isomorphic', 'isomorphic',
           'to_canonical_graph', 'graph_diff', 'similar']

from rdflib.graph import Graph, ConjunctiveGraph, ReadOnlyGraphAggregate
from rdflib.term import BNode
try:
    import hashlib
    md = hashlib.md5
except ImportError:
    # for Python << 2.5
    import md5
    md = md5.new


class IsomorphicGraph(ConjunctiveGraph):
    """
    Ported from
    <http://www.w3.org/2001/sw/DataAccess/proto-tests/tools/rdfdiff.py>
    (Sean B Palmer's RDF Graph Isomorphism Tester).
    """

    def __init__(self, **kwargs):
        super(IsomorphicGraph, self).__init__(**kwargs)

    def __eq__(self, other):
        """Graph isomorphism testing."""
        if not isinstance(other, IsomorphicGraph):
            return False
        elif len(self) != len(other):
            return False
        elif list(self) == list(other):
            return True  # TODO: really generally cheaper?
        return self.internal_hash() == other.internal_hash()

    def __ne__(self, other):
        """Negative graph isomorphism testing."""
        return not self.__eq__(other)

    def internal_hash(self):
        """
        This is defined instead of __hash__ to avoid a circular recursion
        scenario with the Memory store for rdflib which requires a hash lookup
        in order to return a generator of triples.
        """
        return _TripleCanonicalizer(self).to_hash()


class _TripleCanonicalizer(object):

    def __init__(self, graph, hashfunc=hash):
        self.graph = graph
        self.hashfunc = hashfunc

    def to_hash(self):
        return self.hashfunc(tuple(sorted(
            map(self.hashfunc, self.canonical_triples()))))

    def canonical_triples(self):
        for triple in self.graph:
            yield tuple(self._canonicalize_bnodes(triple))

    def _canonicalize_bnodes(self, triple):
        for term in triple:
            if isinstance(term, BNode):
                yield BNode(value="cb%s" % self._canonicalize(term))
            else:
                yield term

    def _canonicalize(self, term, done=False):
        return self.hashfunc(tuple(sorted(self._vhashtriples(term, done),
                                          key=_hetero_tuple_key)))

    def _vhashtriples(self, term, done):
        for triple in self.graph:
            if term in triple:
                yield tuple(self._vhashtriple(triple, term, done))

    def _vhashtriple(self, triple, target_term, done):
        for i, term in enumerate(triple):
            if not isinstance(term, BNode):
                yield term
            elif done or (term == target_term):
                yield i
            else:
                yield self._canonicalize(term, done=True)


def _hetero_tuple_key(x):
    "Sort like Python 2 - by name of type, then by value. Expects tuples."
    return tuple((type(a).__name__, a) for a in x)


def to_isomorphic(graph):
    if isinstance(graph, IsomorphicGraph):
        return graph
    return IsomorphicGraph(store=graph.store)


def isomorphic(graph1, graph2):
    """
    Compare graph for equality. Uses an algorithm to compute unique hashes
    which takes bnodes into account.

    Examples::

        >>> g1 = Graph().parse(format='n3', data='''
        ...     @prefix : <http://example.org/ns#> .
        ...     <http://example.org> :rel <http://example.org/a> .
        ...     <http://example.org> :rel <http://example.org/b> .
        ...     <http://example.org> :rel [ :label "A bnode." ] .
        ... ''')
        >>> g2 = Graph().parse(format='n3', data='''
        ...     @prefix ns: <http://example.org/ns#> .
        ...     <http://example.org> ns:rel [ ns:label "A bnode." ] .
        ...     <http://example.org> ns:rel <http://example.org/b>,
        ...             <http://example.org/a> .
        ... ''')
        >>> isomorphic(g1, g2)
        True

        >>> g3 = Graph().parse(format='n3', data='''
        ...     @prefix : <http://example.org/ns#> .
        ...     <http://example.org> :rel <http://example.org/a> .
        ...     <http://example.org> :rel <http://example.org/b> .
        ...     <http://example.org> :rel <http://example.org/c> .
        ... ''')
        >>> isomorphic(g1, g3)
        False
    """
    return _TripleCanonicalizer(graph1).to_hash() == \
        _TripleCanonicalizer(graph2).to_hash()


def to_canonical_graph(g1):
    """
    Creates a canonical, read-only graph where all bnode id:s are based on
    deterministical MD5 checksums, correlated with the graph contents.
    """
    graph = Graph()
    graph += _TripleCanonicalizer(g1, _md5_hash).canonical_triples()
    return ReadOnlyGraphAggregate([graph])


def graph_diff(g1, g2):
    """
    Returns three sets of triples: "in both", "in first" and "in second".
    """
    # bnodes have deterministic values in canonical graphs:
    cg1 = to_canonical_graph(g1)
    cg2 = to_canonical_graph(g2)
    in_both = cg1 * cg2
    in_first = cg1 - cg2
    in_second = cg2 - cg1
    return (in_both, in_first, in_second)


def _md5_hash(t):
    h = md()
    for i in t:
        if isinstance(i, tuple):
            h.update(_md5_hash(i).encode('ascii'))
        else:
            h.update(unicode(i).encode("utf8"))
    return h.hexdigest()


_MOCK_BNODE = BNode()


def similar(g1, g2):
    """
    Checks if the two graphs are "similar", by comparing sorted triples where
    all bnodes have been replaced by a singular mock bnode (the
    ``_MOCK_BNODE``).

    This is a much cheaper, but less reliable, alternative to the comparison
    algorithm in ``isomorphic``.
    """
    return all(t1 == t2 for (t1, t2) in _squashed_graphs_triples(g1, g2))


def _squashed_graphs_triples(g1, g2):
    for (t1, t2) in zip(sorted(_squash_graph(g1)), sorted(_squash_graph(g2))):
        yield t1, t2


def _squash_graph(graph):
    return (_squash_bnodes(triple) for triple in graph)


def _squash_bnodes(triple):
    return tuple((isinstance(t, BNode) and _MOCK_BNODE) or t for t in triple)

########NEW FILE########
__FILENAME__ = compat
#
# code to simplify supporting older python versions
#


import sys

from decimal import Decimal

if sys.version_info[:2] < (2, 7):

    # Pre-2.7 decimal and float did not compare correctly

    def numeric_greater(a, b):
        if isinstance(a, Decimal) and isinstance(b, float):
            return float(a) > b
        elif isinstance(a, float) and isinstance(b, Decimal):
            return a > float(b)
        else:
            return a > b

else:

    def numeric_greater(a, b):
        return a > b

########NEW FILE########
__FILENAME__ = events
__doc__ = """
Dirt Simple Events

A Dispatcher (or a subclass of Dispatcher) stores event handlers that
are 'fired' simple event objects when interesting things happen.

Create a dispatcher:

  >>> d = Dispatcher()

Now create a handler for the event and subscribe it to the dispatcher
to handle Event events.  A handler is a simple function or method that
accepts the event as an argument:

  >>> def handler1(event): print(repr(event))
  >>> d.subscribe(Event, handler1)

Now dispatch a new event into the dispatcher, and see handler1 get
fired:

  >>> d.dispatch(Event(foo='bar', data='yours', used_by='the event handlers'))
  <rdflib.events.Event ['data', 'foo', 'used_by']>
"""

__all__ = ['Event', 'Dispatcher']


class Event(object):
    """
    An event is a container for attributes.  The source of an event
    creates this object, or a subclass, gives it any kind of data that
    the events handlers need to handle the event, and then calls
    notify(event).

    The target of an event registers a function to handle the event it
    is interested with subscribe().  When a sources calls
    notify(event), each subscriber to that event will be called in no
    particular order.
    """

    def __init__(self, **kw):
        self.__dict__.update(kw)

    def __repr__(self):
        attrs = self.__dict__.keys()
        attrs.sort()
        return '<rdflib.events.Event %s>' % ([a for a in attrs],)


class Dispatcher(object):
    """
    An object that can dispatch events to a privately managed group of
    subscribers.
    """

    _dispatch_map = None

    def set_map(self, amap):
        self._dispatch_map = amap

    def get_map(self):
        return self._dispatch_map

    def subscribe(self, event_type, handler):
        """ Subscribe the given handler to an event_type.  Handlers
        are called in the order they are subscribed.
        """
        if self._dispatch_map is None:
            self.set_map({})
        lst = self._dispatch_map.get(event_type, None)
        if lst is None:
            lst = [handler]
        else:
            lst.append(handler)
        self._dispatch_map[event_type] = lst

    def dispatch(self, event):
        """ Dispatch the given event to the subscribed handlers for
        the event's type"""
        if self._dispatch_map is not None:
            lst = self._dispatch_map.get(type(event), None)
            if lst is None:
                raise ValueError("unknown event type: %s" % type(event))
            for l in lst:
                l(event)


def test():
    import doctest
    doctest.testmod()

if __name__ == '__main__':
    test()

########NEW FILE########
__FILENAME__ = exceptions
"""
TODO:
"""

__all__ = ['Error', 'TypeCheckError', 'SubjectTypeError',
           'PredicateTypeError', 'ObjectTypeError', 'ContextTypeError',
           'ParserError']


class Error(Exception):
    """Base class for rdflib exceptions."""
    def __init__(self, msg=None):
        Exception.__init__(self, msg)
        self.msg = msg


class TypeCheckError(Error):
    """Parts of assertions are subject to type checks."""

    def __init__(self, node):
        Error.__init__(self, node)
        self.type = type(node)
        self.node = node


class SubjectTypeError(TypeCheckError):
    """Subject of an assertion must be an instance of URIRef."""
    def __init__(self, node):
        TypeCheckError.__init__(self, node)
        self.msg = "Subject must be instance of URIRef or BNode: %s(%s)" \
            % (self.node, self.type)


class PredicateTypeError(TypeCheckError):
    """Predicate of an assertion must be an instance of URIRef."""
    def __init__(self, node):
        TypeCheckError.__init__(self, node)
        self.msg = "Predicate must be a URIRef instance: %s(%s)" \
            % (self.node, self.type)


class ObjectTypeError(TypeCheckError):
    """Object of an assertion must be an instance of URIRef, Literal,
    or BNode."""
    def __init__(self, node):
        TypeCheckError.__init__(self, node)
        self.msg = "\
Object must be instance of URIRef, Literal, or BNode: %s(%s)" % \
            (self.node, self.type)


class ContextTypeError(TypeCheckError):
    """Context of an assertion must be an instance of URIRef."""
    def __init__(self, node):
        TypeCheckError.__init__(self, node)
        self.msg = "Context must be instance of URIRef or BNode: %s(%s)" \
            % (self.node, self.type)


class ParserError(Error):
    """RDF Parser error."""
    def __init__(self, msg):
        Error.__init__(self, msg)
        self.msg = msg

    def __str__(self):
        return self.msg


class UniquenessError(Error):
    """A uniqueness assumption was made in the context, and that is not true"""
    def __init__(self, values):
        Error.__init__(self, "\
Uniqueness assumption is not fulfilled. Multiple values are: %s" % values)

########NEW FILE########
__FILENAME__ = cmdlineutils
import sys
import time
import getopt
import rdflib
import codecs

from rdflib.util import guess_format


def _help():
    sys.stderr.write("""
program.py [-f <format>] [-o <output>] [files...]
Read RDF files given on STDOUT - does something to the resulting graph
If no files are given, read from stdin
-o specifies file for output, if not given stdout is used
-f specifies parser to use, if not given it is guessed from extension

""")


def main(target, _help=_help, options="", stdin=True):
    """
    A main function for tools that read RDF from files given on commandline
    or from STDIN (if stdin parameter is true)
    """

    args, files = getopt.getopt(sys.argv[1:], "hf:o:" + options)
    dargs = dict(args)

    if "-h" in dargs:
        _help()
        sys.exit(-1)

    g = rdflib.Graph()

    if "-f" in dargs:
        f = dargs["-f"]
    else:
        f = None

    if "-o" in dargs:
        sys.stderr.write("Output to %s\n" % dargs["-o"])
        out = codecs.open(dargs["-o"], "w", "utf-8")
    else:
        out = sys.stdout

    start = time.time()
    if len(files) == 0 and stdin:
        sys.stderr.write("Reading from stdin as %s..." % f)
        g.load(sys.stdin, format=f)
        sys.stderr.write("[done]\n")
    else:
        size = 0
        for x in files:
            if f is None:
                f = guess_format(x)
            start1 = time.time()
            sys.stderr.write("Loading %s as %s... " % (x, f))
            g.load(x, format=f)
            sys.stderr.write("done.\t(%d triples\t%.2f seconds)\n" %
                             (len(g) - size, time.time() - start1))
            size = len(g)

    sys.stderr.write("Loaded a total of %d triples in %.2f seconds.\n" %
                     (len(g), time.time() - start))

    target(g, out, args)

########NEW FILE########
__FILENAME__ = describer
#!/usr/bin/env python
# -*- coding: utf-8 -*-
from __future__ import with_statement
from rdflib import py3compat
__doc__ = py3compat.format_doctest_out("""
A Describer is a stateful utility for creating RDF statements in a
semi-declarative manner. It has methods for creating literal values, rel and
rev resource relations (somewhat resembling RDFa).

The `rel` and ``rev`` methods return a context manager which sets the current
about to the referenced resource for the context scope (for use with the
``with`` statement).

Full example in the ``to_rdf`` method below::

    >>> import datetime
    >>> from rdflib.graph import Graph
    >>> from rdflib.namespace import Namespace, RDFS, FOAF
    >>>
    >>> ORG_URI = "http://example.org/"
    >>>
    >>> CV = Namespace("http://purl.org/captsolo/resume-rdf/0.2/cv#")
    >>>
    >>> class Person(object):
    ...     def __init__(self):
    ...         self.first_name = %(u)s"Some"
    ...         self.last_name = %(u)s"Body"
    ...         self.username = "some1"
    ...         self.presentation = %(u)s"Just a Python & RDF hacker."
    ...         self.image = "/images/persons/" + self.username + ".jpg"
    ...         self.site = "http://example.net/"
    ...         self.start_date = datetime.date(2009, 9, 4)
    ...     def get_full_name(self):
    ...         return %(u)s" ".join([self.first_name, self.last_name])
    ...     def get_absolute_url(self):
    ...         return "/persons/" + self.username
    ...     def get_thumbnail_url(self):
    ...         return self.image.replace('.jpg', '-thumb.jpg')
    ...
    ...     def to_rdf(self):
    ...         graph = Graph()
    ...         graph.bind('foaf', FOAF)
    ...         graph.bind('cv', CV)
    ...         lang = 'en'
    ...         d = Describer(graph, base=ORG_URI)
    ...         d.about(self.get_absolute_url()+'#person')
    ...         d.rdftype(FOAF.Person)
    ...         d.value(FOAF.name, self.get_full_name())
    ...         d.value(FOAF.firstName, self.first_name)
    ...         d.value(FOAF.surname, self.last_name)
    ...         d.rel(FOAF.homepage, self.site)
    ...         d.value(RDFS.comment, self.presentation, lang=lang)
    ...         with d.rel(FOAF.depiction, self.image):
    ...             d.rdftype(FOAF.Image)
    ...             d.rel(FOAF.thumbnail, self.get_thumbnail_url())
    ...         with d.rev(CV.aboutPerson):
    ...             d.rdftype(CV.CV)
    ...             with d.rel(CV.hasWorkHistory):
    ...                 d.value(CV.startDate, self.start_date)
    ...                 d.rel(CV.employedIn, ORG_URI+"#company")
    ...         return graph
    ...
    >>> person_graph = Person().to_rdf()
    >>> expected = Graph().parse(data='''<?xml version="1.0" encoding="utf-8"?>
    ... <rdf:RDF
    ...   xmlns:foaf="http://xmlns.com/foaf/0.1/"
    ...   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
    ...   xmlns:cv="http://purl.org/captsolo/resume-rdf/0.2/cv#"
    ...   xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#">
    ...   <foaf:Person rdf:about="http://example.org/persons/some1#person">
    ...     <foaf:name>Some Body</foaf:name>
    ...     <foaf:firstName>Some</foaf:firstName>
    ...     <foaf:surname>Body</foaf:surname>
    ...     <foaf:depiction>
    ...       <foaf:Image
    ...         rdf:about=
    ...             "http://example.org/images/persons/some1.jpg">
    ...         <foaf:thumbnail
    ...         rdf:resource=
    ...             "http://example.org/images/persons/some1-thumb.jpg"/>
    ...       </foaf:Image>
    ...     </foaf:depiction>
    ...     <rdfs:comment xml:lang="en">
    ...             Just a Python &amp; RDF hacker.
    ...     </rdfs:comment>
    ...     <foaf:homepage rdf:resource="http://example.net/"/>
    ...   </foaf:Person>
    ...   <cv:CV>
    ...     <cv:aboutPerson
    ...         rdf:resource="http://example.org/persons/some1#person">
    ...     </cv:aboutPerson>
    ...     <cv:hasWorkHistory>
    ...       <rdf:Description>
    ...         <cv:startDate
    ...             rdf:datatype="http://www.w3.org/2001/XMLSchema#date"
    ...             >2009-09-04</cv:startDate>
    ...         <cv:employedIn rdf:resource="http://example.org/#company"/>
    ...       </rdf:Description>
    ...     </cv:hasWorkHistory>
    ...   </cv:CV>
    ... </rdf:RDF>
    ... ''')
    >>>
    >>> from rdflib.compare import isomorphic
    >>> isomorphic(person_graph, expected)  #doctest: +SKIP
    True
""")

from contextlib import contextmanager
from rdflib.graph import Graph
from rdflib.namespace import RDF
from rdflib.term import BNode
from rdflib.term import Identifier
from rdflib.term import Literal
from rdflib.term import URIRef
from rdflib.py3compat import format_doctest_out


class Describer(object):

    def __init__(self, graph=None, about=None, base=None):
        if graph is None:
            graph = Graph()
        self.graph = graph
        self.base = base
        self._subjects = []
        self.about(about or None)

    @format_doctest_out
    def about(self, subject, **kws):
        """
        Sets the current subject. Will convert the given object into an
        ``URIRef`` if it's not an ``Identifier``.

        Usage::

            >>> d = Describer()
            >>> d._current() #doctest: +ELLIPSIS
            rdflib.term.BNode(...)
            >>> d.about("http://example.org/")
            >>> d._current()
            rdflib.term.URIRef(%(u)s'http://example.org/')

        """
        kws.setdefault('base', self.base)
        subject = cast_identifier(subject, **kws)
        if self._subjects:
            self._subjects[-1] = subject
        else:
            self._subjects.append(subject)

    @format_doctest_out
    def value(self, p, v, **kws):
        """
        Set a literal value for the given property. Will cast the value to an
        ``Literal`` if a plain literal is given.

        Usage::

            >>> from rdflib import URIRef
            >>> from rdflib.namespace import RDF, RDFS
            >>> d = Describer(about="http://example.org/")
            >>> d.value(RDFS.label, "Example")
            >>> d.graph.value(URIRef('http://example.org/'), RDFS.label)
            rdflib.term.Literal(%(u)s'Example')

        """
        v = cast_value(v, **kws)
        self.graph.add((self._current(), p, v))

    @format_doctest_out
    def rel(self, p, o=None, **kws):
        """Set an object for the given property. Will convert the given object
        into an ``URIRef`` if it's not an ``Identifier``. If none is given, a
        new ``BNode`` is used.

        Returns a context manager for use in a ``with`` block, within which the
        given object is used as current subject.

        Usage::

            >>> from rdflib import URIRef
            >>> from rdflib.namespace import RDF, RDFS
            >>> d = Describer(about="/", base="http://example.org/")
            >>> _ctxt = d.rel(RDFS.seeAlso, "/about")
            >>> d.graph.value(URIRef('http://example.org/'), RDFS.seeAlso)
            rdflib.term.URIRef(%(u)s'http://example.org/about')

            >>> with d.rel(RDFS.seeAlso, "/more"):
            ...     d.value(RDFS.label, "More")
            >>> (URIRef('http://example.org/'), RDFS.seeAlso,
            ...         URIRef('http://example.org/more')) in d.graph
            True
            >>> d.graph.value(URIRef('http://example.org/more'), RDFS.label)
            rdflib.term.Literal(%(u)s'More')

        """

        kws.setdefault('base', self.base)
        p = cast_identifier(p)
        o = cast_identifier(o, **kws)
        self.graph.add((self._current(), p, o))
        return self._subject_stack(o)

    @format_doctest_out
    def rev(self, p, s=None, **kws):
        """
        Same as ``rel``, but uses current subject as *object* of the relation.
        The given resource is still used as subject in the returned context
        manager.

        Usage::

            >>> from rdflib import URIRef
            >>> from rdflib.namespace import RDF, RDFS
            >>> d = Describer(about="http://example.org/")
            >>> with d.rev(RDFS.seeAlso, "http://example.net/"):
            ...     d.value(RDFS.label, "Net")
            >>> (URIRef('http://example.net/'), RDFS.seeAlso,
            ...         URIRef('http://example.org/')) in d.graph
            True
            >>> d.graph.value(URIRef('http://example.net/'), RDFS.label)
            rdflib.term.Literal(%(u)s'Net')

        """
        kws.setdefault('base', self.base)
        p = cast_identifier(p)
        s = cast_identifier(s, **kws)
        self.graph.add((s, p, self._current()))
        return self._subject_stack(s)

    def rdftype(self, t):
        """
        Shorthand for setting rdf:type of the current subject.

        Usage::

            >>> from rdflib import URIRef
            >>> from rdflib.namespace import RDF, RDFS
            >>> d = Describer(about="http://example.org/")
            >>> d.rdftype(RDFS.Resource)
            >>> (URIRef('http://example.org/'),
            ...     RDF.type, RDFS.Resource) in d.graph
            True

        """
        self.graph.add((self._current(), RDF.type, t))

    def _current(self):
        return self._subjects[-1]

    @contextmanager
    def _subject_stack(self, subject):
        self._subjects.append(subject)
        yield None
        self._subjects.pop()


def cast_value(v, **kws):
    if not isinstance(v, Literal):
        v = Literal(v, **kws)
    return v


def cast_identifier(ref, **kws):
    ref = ref or BNode()
    if not isinstance(ref, Identifier):
        ref = URIRef(ref, **kws)
    return ref

########NEW FILE########
__FILENAME__ = infixowl
#!/usr/bin/env python
# -*- coding: utf-8 -*-
from rdflib import py3compat

__doc__ = py3compat.format_doctest_out("""
RDFLib Python binding for OWL Abstract Syntax

see: http://www.w3.org/TR/owl-semantics/syntax.html
     http://owl-workshop.man.ac.uk/acceptedLong/submission_9.pdf

3.2.3 Axioms for complete classes without using owl:equivalentClass

Named class description of type 2 (with owl:oneOf) or type 4-6
(with owl:intersectionOf, owl:unionOf or owl:complementOf


Uses Manchester Syntax for __repr__

>>> exNs = Namespace('http://example.com/')
>>> namespace_manager = NamespaceManager(Graph())
>>> namespace_manager.bind('ex', exNs, override=False)
>>> namespace_manager.bind('owl', OWL_NS, override=False)
>>> g = Graph()
>>> g.namespace_manager = namespace_manager

Now we have an empty graph, we can construct OWL classes in it
using the Python classes defined in this module

>>> a = Class(exNs.Opera, graph=g)

Now we can assert rdfs:subClassOf and owl:equivalentClass relationships
(in the underlying graph) with other classes using the 'subClassOf'
and 'equivalentClass' descriptors which can be set to a list
of objects for the corresponding predicates.

>>> a.subClassOf = [exNs.MusicalWork]

We can then access the rdfs:subClassOf relationships

>>> print(list(a.subClassOf))
[Class: ex:MusicalWork ]

This can also be used against already populated graphs:

>>> owlGraph = Graph().parse(OWL_NS) #doctest: +SKIP
>>> namespace_manager.bind('owl', OWL_NS, override=False) #doctest: +SKIP
>>> owlGraph.namespace_manager = namespace_manager #doctest: +SKIP
>>> list(Class(OWL_NS.Class, graph=owlGraph).subClassOf) #doctest: +SKIP
[Class: rdfs:Class ]

Operators are also available. For instance we can add ex:Opera to the extension
of the ex:CreativeWork class via the '+=' operator

>>> a #doctest: +SKIP
Class: ex:Opera SubClassOf: ex:MusicalWork
>>> b = Class(exNs.CreativeWork, graph=g)
>>> b += a
>>> print(sorted(a.subClassOf, key=lambda c:c.identifier)) #doctest: +SKIP
[Class: ex:CreativeWork , Class: ex:MusicalWork ]

And we can then remove it from the extension as well

>>> b -= a
>>> a #doctest: +SKIP
Class: ex:Opera SubClassOf: ex:MusicalWork

Boolean class constructions can also  be created with Python operators.
For example, The | operator can be used to construct a class consisting of a
owl:unionOf the operands:

>>> c =  a | b | Class(exNs.Work, graph=g)
>>> c #doctest: +SKIP
( ex:Opera OR ex:CreativeWork OR ex:Work )

Boolean class expressions can also be operated as lists (using python list
operators)

>>> del c[c.index(Class(exNs.Work, graph=g))]
>>> c #doctest: +SKIP
( ex:Opera OR ex:CreativeWork )

The '&' operator can be used to construct class intersection:

>>> woman = Class(exNs.Female, graph=g) & Class(exNs.Human, graph=g)
>>> woman.identifier = exNs.Woman
>>> woman #doctest: +SKIP
( ex:Female AND ex:Human )
>>> len(woman)
2

Enumerated classes can also be manipulated

>>> contList = [Class(exNs.Africa, graph=g), Class(exNs.NorthAmerica, graph=g)]
>>> EnumeratedClass(members=contList, graph=g) #doctest: +SKIP
{ ex:Africa ex:NorthAmerica }

owl:Restrictions can also be instantiated:

>>> Restriction(exNs.hasParent, graph=g, allValuesFrom=exNs.Human) #doctest: +SKIP
( ex:hasParent ONLY ex:Human )

Restrictions can also be created using Manchester OWL syntax in 'colloquial'
Python
>>> exNs.hasParent | some | Class(exNs.Physician, graph=g) #doctest: +SKIP
( ex:hasParent SOME ex:Physician )

>>> Property(exNs.hasParent,graph=g) | max | Literal(1) #doctest: +SKIP
( ex:hasParent MAX 1 )

>>> print(g.serialize(format='pretty-xml')) #doctest: +SKIP

""")

import itertools

from rdflib import (
    BNode,
    Literal,
    Namespace,
    RDF,
    RDFS,
    URIRef,
    Variable
)
from rdflib.graph import Graph
from rdflib.collection import Collection
from rdflib.namespace import XSD as _XSD_NS
from rdflib.namespace import NamespaceManager
from rdflib.term import Identifier
from rdflib.util import first

import logging


def _debug(*args, **kw):
    # import logging
    logging.basicConfig(level=logging.ERROR, format="%(message)s")
    logger = logging.getLogger(__name__)
    logger.debug(*args, **kw)


"""
From: http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/384122

Python has the wonderful "in" operator and it would be nice to have additional
infix operator like this. This recipe shows how (almost) arbitrary infix
operators can be defined.

"""

__all__ = [
    'OWL_NS',
    'nsBinds',
    'ACE_NS',
    'CLASS_RELATIONS',
    'some',
    'only',
    'max',
    'min',
    'exactly',
    'value',
    'PropertyAbstractSyntax',
    'AllClasses',
    'AllDifferent',
    'AllProperties',
    'AnnotatableTerms',
    'BooleanClass',
    'Callable',
    'CastClass',
    'Class',
    'ClassNamespaceFactory',
    'classOrIdentifier',
    'classOrTerm',
    'CommonNSBindings',
    'ComponentTerms',
    'DeepClassClear',
    'EnumeratedClass',
    'generateQName',
    'GetIdentifiedClasses',
    'Individual',
    'MalformedClass',
    'manchesterSyntax',
    'Ontology',
    'OWLRDFListProxy',
    'Property',
    'propertyOrIdentifier',
    'Restriction',
    'termDeletionDecorator',
]

# definition of an Infix operator class
# this recipe also works in jython
# calling sequence for the infix is either:
#  x |op| y
# or:
# x <<op>> y


class Infix:
    def __init__(self, function):
        self.function = function

    def __ror__(self, other):
        return Infix(lambda x, self=self, other=other: self.function(other, x))

    def __or__(self, other):
        return self.function(other)

    def __rlshift__(self, other):
        return Infix(lambda x, self=self, other=other: self.function(other, x))

    def __rshift__(self, other):
        return self.function(other)

    def __call__(self, value1, value2):
        return self.function(value1, value2)

OWL_NS = Namespace("http://www.w3.org/2002/07/owl#")

nsBinds = {
    'skos': 'http://www.w3.org/2004/02/skos/core#',
    'rdf': RDF,
    'rdfs': RDFS,
    'owl': OWL_NS,
    'list': URIRef('http://www.w3.org/2000/10/swap/list#'),
    'dc': "http://purl.org/dc/elements/1.1/",
}


def generateQName(graph, uri):
    prefix, uri, localName = graph.compute_qname(classOrIdentifier(uri))
    return u':'.join([prefix, localName])


def classOrTerm(thing):
    if isinstance(thing, Class):
        return thing.identifier
    else:
        assert isinstance(thing, (URIRef, BNode, Literal))
        return thing


def classOrIdentifier(thing):
    if isinstance(thing, (Property, Class)):
        return thing.identifier
    else:
        assert isinstance(thing, (URIRef, BNode)), \
            "Expecting a Class, Property, URIRef, or BNode.. not a %s" % thing
        return thing


def propertyOrIdentifier(thing):
    if isinstance(thing, Property):
        return thing.identifier
    else:
        assert isinstance(thing, URIRef)
        return thing


def manchesterSyntax(thing, store, boolean=None, transientList=False):
    """
    Core serialization
    """
    assert thing is not None
    if boolean:
        if transientList:
            liveChildren = iter(thing)
            children = [manchesterSyntax(child, store) for child in thing]
        else:
            liveChildren = iter(Collection(store, thing))
            children = [manchesterSyntax(
                child, store) for child in Collection(store, thing)]
        if boolean == OWL_NS.intersectionOf:
            childList = []
            named = []
            for child in liveChildren:
                if isinstance(child, URIRef):
                    named.append(child)
                else:
                    childList.append(child)
            if named:
                def castToQName(x):
                    prefix, uri, localName = store.compute_qname(x)
                    return ':'.join([prefix, localName])

                if len(named) > 1:
                    prefix = u'( ' + u' AND '.join(map(
                        castToQName, named)) + u' )'
                else:
                    prefix = manchesterSyntax(named[0], store)
                if childList:
                    return str(prefix) + u' THAT ' + u' AND '.join(
                        [str(manchesterSyntax(x, store)) for x in childList])
                else:
                    return prefix
            else:
                return u'( ' + u' AND '.join(
                    [str(c) for c in children]) + u' )'
        elif boolean == OWL_NS.unionOf:
            return u'( ' + u' OR '.join([str(c) for c in children]) + ' )'
        elif boolean == OWL_NS.oneOf:
            return u'{ ' + u' '.join([str(c) for c in children]) + ' }'
        else:
            assert boolean == OWL_NS.complementOf
    elif OWL_NS.Restriction in store.objects(
            subject=thing, predicate=RDF.type):
        prop = list(
            store.objects(subject=thing, predicate=OWL_NS.onProperty))[0]
        prefix, uri, localName = store.compute_qname(prop)
        propString = u':'.join([prefix, localName])
        label = first(store.objects(subject=prop, predicate=RDFS.label))
        if label:
            propString = "'%s'" % label
        for onlyClass in store.objects(
                subject=thing, predicate=OWL_NS.allValuesFrom):
            return u'( %s ONLY %s )' % (
                propString, manchesterSyntax(onlyClass, store))
        for val in store.objects(subject=thing, predicate=OWL_NS.hasValue):
            return u'( %s VALUE %s )' % (
                propString,
                manchesterSyntax(val.encode('utf-8', 'ignore'), store))
        for someClass in store.objects(
                subject=thing, predicate=OWL_NS.someValuesFrom):
            return u'( %s SOME %s )' % (
                propString, manchesterSyntax(someClass, store))
        cardLookup = {OWL_NS.maxCardinality: 'MAX',
                      OWL_NS.minCardinality: 'MIN',
                      OWL_NS.cardinality: 'EQUALS'}
        for s, p, o in store.triples_choices(
                (thing, list(cardLookup.keys()), None)):
            return u'( %s %s %s )' % (
                propString, cardLookup[p], o.encode('utf-8', 'ignore'))
    compl = list(store.objects(subject=thing, predicate=OWL_NS.complementOf))
    if compl:
        return '( NOT %s )' % (manchesterSyntax(compl[0], store))
    else:
        prolog = '\n'.join(
            ["PREFIX %s: <%s>" % (k, nsBinds[k]) for k in nsBinds])
        qstr = \
            prolog + \
            "\nSELECT ?p ?bool WHERE {?class a owl:Class; ?p ?bool ." + \
            "?bool rdf:first ?foo }"
        initb = {Variable("?class"): thing}
        for boolProp, col in \
                store.query(qstr, processor="sparql", initBindings=initb):
            if not isinstance(thing, URIRef):
                return manchesterSyntax(col, store, boolean=boolProp)
        try:
            prefix, uri, localName = store.compute_qname(thing)
            qname = u':'.join([prefix, localName])
        except Exception:
            if isinstance(thing, BNode):
                return thing.n3()
            return u"<" + thing + ">"
            _debug(list(store.objects(subject=thing, predicate=RDF.type)))
            raise
            return '[]'  # +thing._id.encode('utf-8')+'</em>'
        label = first(Class(thing, graph=store).label)
        if label:
            return label.encode('utf-8', 'ignore')
        else:
            return qname.encode('utf-8', 'ignore')


def GetIdentifiedClasses(graph):
    for c in graph.subjects(predicate=RDF.type, object=OWL_NS.Class):
        if isinstance(c, URIRef):
            yield Class(c)


def termDeletionDecorator(prop):
    def someFunc(func):
        func.property = prop
        return func
    return someFunc


class TermDeletionHelper:
    def __init__(self, prop):
        self.prop = prop

    def __call__(self, f):
        def _remover(inst):
            inst.graph.remove((inst.identifier, self.prop, None))
        return _remover


class Individual(object):
    """
    A typed individual
    """
    factoryGraph = Graph()

    def serialize(self, graph):
        for fact in self.factoryGraph.triples((self.identifier, None, None)):
            graph.add(fact)

    def __init__(self, identifier=None, graph=None):
        self.__identifier = identifier is not None and identifier or BNode()
        if graph is None:
            self.graph = self.factoryGraph
        else:
            self.graph = graph
        self.qname = None
        if not isinstance(self.identifier, BNode):
            try:
                prefix, uri, localName = self.graph.compute_qname(
                    self.identifier)
                self.qname = u':'.join([prefix, localName])
            except:
                pass

    def clearInDegree(self):
        self.graph.remove((None, None, self.identifier))

    def clearOutDegree(self):
        self.graph.remove((self.identifier, None, None))

    def delete(self):
        self.clearInDegree()
        self.clearOutDegree()

    def replace(self, other):
        for s, p, o in self.graph.triples((None, None, self.identifier)):
            self.graph.add((s, p, classOrIdentifier(other)))
        self.delete()

    def _get_type(self):
        for _t in self.graph.objects(
                subject=self.identifier, predicate=RDF.type):
            yield _t

    def _set_type(self, kind):
        if not kind:
            return
        if isinstance(kind, (Individual, Identifier)):
            self.graph.add(
                (self.identifier, RDF.type, classOrIdentifier(kind)))
        else:
            for c in kind:
                assert isinstance(c, (Individual, Identifier))
                self.graph.add(
                    (self.identifier, RDF.type, classOrIdentifier(c)))

    @TermDeletionHelper(RDF.type)
    def _delete_type(self):
        """
        >>> g = Graph()
        >>> b=Individual(OWL_NS.Restriction,g)
        >>> b.type = RDF.Resource
        >>> len(list(b.type))
        1
        >>> del b.type
        >>> len(list(b.type))
        0
        """
        pass

    type = property(_get_type, _set_type, _delete_type)

    def _get_identifier(self):
        return self.__identifier

    def _set_identifier(self, i):
        assert i
        if i != self.__identifier:
            oldStmtsOut = [(p, o) for s, p, o in self.graph.triples(
                (self.__identifier, None, None))]
            oldStmtsIn = [(s, p) for s, p, o in self.graph.triples(
                (None, None, self.__identifier))]
            for p1, o1 in oldStmtsOut:
                self.graph.remove((self.__identifier, p1, o1))
            for s1, p1 in oldStmtsIn:
                self.graph.remove((s1, p1, self.__identifier))
            self.__identifier = i
            self.graph.addN(
                [(i, p1, o1, self.graph) for p1, o1 in oldStmtsOut])
            self.graph.addN([(s1, p1, i, self.graph) for s1, p1 in oldStmtsIn])
        if not isinstance(i, BNode):
            try:
                prefix, uri, localName = self.graph.compute_qname(i)
                self.qname = u':'.join([prefix, localName])
            except:
                pass

    identifier = property(_get_identifier, _set_identifier)

    def _get_sameAs(self):
        for _t in self.graph.objects(
                subject=self.identifier, predicate=OWL_NS.sameAs):
            yield _t

    def _set_sameAs(self, term):
        # if not kind:
        #     return
        if isinstance(term, (Individual, Identifier)):
            self.graph.add(
                (self.identifier, OWL_NS.sameAs, classOrIdentifier(term)))
        else:
            for c in term:
                assert isinstance(c, (Individual, Identifier))
                self.graph.add(
                    (self.identifier, OWL_NS.sameAs, classOrIdentifier(c)))

    @TermDeletionHelper(OWL_NS.sameAs)
    def _delete_sameAs(self):
        pass

    sameAs = property(_get_sameAs, _set_sameAs, _delete_sameAs)


ACE_NS = Namespace('http://attempto.ifi.uzh.ch/ace_lexicon#')


class AnnotatableTerms(Individual):
    """
    Terms in an OWL ontology with rdfs:label and rdfs:comment
    """
    def __init__(self,
                 identifier,
                 graph=None,
                 nameAnnotation=None,
                 nameIsLabel=False):
        super(AnnotatableTerms, self).__init__(identifier, graph)
        if nameAnnotation:
            self.setupACEAnnotations()
            self.PN_sgProp.extent = [(self.identifier,
                                      self.handleAnnotation(nameAnnotation))]
            if nameIsLabel:
                self.label = [nameAnnotation]

    def handleAnnotation(self, val):
        return val if isinstance(val, Literal) else Literal(val)

    def setupACEAnnotations(self):
        self.graph.bind('ace', ACE_NS, override=False)

        # PN_sg singular form of a proper name ()
        self.PN_sgProp = Property(ACE_NS.PN_sg,
                                  baseType=OWL_NS.AnnotationProperty,
                                  graph=self.graph)

        # CN_sg singular form of a common noun
        self.CN_sgProp = Property(ACE_NS.CN_sg,
                                  baseType=OWL_NS.AnnotationProperty,
                                  graph=self.graph)

        # CN_pl plural form of a common noun
        self.CN_plProp = Property(ACE_NS.CN_pl,
                                  baseType=OWL_NS.AnnotationProperty,
                                  graph=self.graph)

        # singular form of a transitive verb
        self.TV_sgProp = Property(ACE_NS.TV_sg,
                                  baseType=OWL_NS.AnnotationProperty,
                                  graph=self.graph)

        # plural form of a transitive verb
        self.TV_plProp = Property(ACE_NS.TV_pl,
                                  baseType=OWL_NS.AnnotationProperty,
                                  graph=self.graph)

        # past participle form a transitive verb
        self.TV_vbgProp = Property(ACE_NS.TV_vbg,
                                   baseType=OWL_NS.AnnotationProperty,
                                   graph=self.graph)

    def _get_comment(self):
        for comment in self.graph.objects(
                subject=self.identifier, predicate=RDFS.comment):
            yield comment

    def _set_comment(self, comment):
        if not comment:
            return
        if isinstance(comment, Identifier):
            self.graph.add((self.identifier, RDFS.comment, comment))
        else:
            for c in comment:
                self.graph.add((self.identifier, RDFS.comment, c))

    @TermDeletionHelper(RDFS.comment)
    def _del_comment(self):
        pass

    comment = property(_get_comment, _set_comment, _del_comment)

    def _get_seeAlso(self):
        for sA in self.graph.objects(
                subject=self.identifier, predicate=RDFS.seeAlso):
            yield sA

    def _set_seeAlso(self, seeAlsos):
        if not seeAlsos:
            return
        for s in seeAlsos:
            self.graph.add((self.identifier, RDFS.seeAlso, s))

    @TermDeletionHelper(RDFS.seeAlso)
    def _del_seeAlso(self):
        pass
    seeAlso = property(_get_seeAlso, _set_seeAlso, _del_seeAlso)

    def _get_label(self):
        for label in self.graph.objects(
                subject=self.identifier, predicate=RDFS.label):
            yield label

    def _set_label(self, label):
        if not label:
            return
        if isinstance(label, Identifier):
            self.graph.add((self.identifier, RDFS.label, label))
        else:
            for l in label:
                self.graph.add((self.identifier, RDFS.label, l))

    @TermDeletionHelper(RDFS.label)
    def _delete_label(self):
        """
        >>> g=Graph()
        >>> b=Individual(OWL_NS.Restriction,g)
        >>> b.label = Literal('boo')
        >>> len(list(b.label))
        1
        >>> del b.label
        >>> len(list(b.label))
        0
        """
        pass

    label = property(_get_label, _set_label, _delete_label)


class Ontology(AnnotatableTerms):
    """ The owl ontology metadata"""
    def __init__(self,
                 identifier=None, imports=None, comment=None, graph=None):
        super(Ontology, self).__init__(identifier, graph)
        self.imports = imports and imports or []
        self.comment = comment and comment or []
        if (self.identifier, RDF.type, OWL_NS.Ontology) not in self.graph:
            self.graph.add((self.identifier, RDF.type, OWL_NS.Ontology))

    def setVersion(self, version):
        self.graph.set((self.identifier, OWL_NS.versionInfo, version))

    def _get_imports(self):
        for owl in self.graph.objects(
                subject=self.identifier, predicate=OWL_NS['imports']):
            yield owl

    def _set_imports(self, other):
        if not other:
            return
        for o in other:
            self.graph.add((self.identifier, OWL_NS['imports'], o))

    @TermDeletionHelper(OWL_NS['imports'])
    def _del_imports(self):
        pass

    imports = property(_get_imports, _set_imports, _del_imports)


def AllClasses(graph):
    prevClasses = set()
    for c in graph.subjects(predicate=RDF.type, object=OWL_NS.Class):
        if c not in prevClasses:
            prevClasses.add(c)
            yield Class(c)


def AllProperties(graph):
    prevProps = set()
    for s, p, o in graph.triples_choices(
        (None, RDF.type, [OWL_NS.SymmetricProperty,
                          OWL_NS.FunctionalProperty,
                          OWL_NS.InverseFunctionalProperty,
                          OWL_NS.TransitiveProperty,
                          OWL_NS.DatatypeProperty,
                          OWL_NS.ObjectProperty,
                          OWL_NS.AnnotationProperty])):
        if o in [OWL_NS.SymmetricProperty,
                 OWL_NS.InverseFunctionalProperty,
                 OWL_NS.TransitiveProperty,
                 OWL_NS.ObjectProperty]:
            bType = OWL_NS.ObjectProperty
        else:
            bType = OWL_NS.DatatypeProperty
        if s not in prevProps:
            prevProps.add(s)
            yield Property(s,
                           graph=graph,
                           baseType=bType)


class ClassNamespaceFactory(Namespace):
    def term(self, name):
        return Class(URIRef(self + name))

    def __getitem__(self, key, default=None):
        return self.term(key)

    def __getattr__(self, name):
        if name.startswith("__"):  # ignore any special Python names!
            raise AttributeError
        else:
            return self.term(name)

CLASS_RELATIONS = set(
    OWL_NS.resourceProperties
).difference([OWL_NS.onProperty,
              OWL_NS.allValuesFrom,
              OWL_NS.hasValue,
              OWL_NS.someValuesFrom,
              OWL_NS.inverseOf,
              OWL_NS.imports,
              OWL_NS.versionInfo,
              OWL_NS.backwardCompatibleWith,
              OWL_NS.incompatibleWith,
              OWL_NS.unionOf,
              OWL_NS.intersectionOf,
              OWL_NS.oneOf])


def ComponentTerms(cls):
    """
    Takes a Class instance and returns a generator over the classes that
    are involved in its definition, ignoring unamed classes
    """
    if OWL_NS.Restriction in cls.type:
        try:
            cls = CastClass(cls, Individual.factoryGraph)
            for s, p, innerClsId in cls.factoryGraph.triples_choices(
                (cls.identifier,
                 [OWL_NS.allValuesFrom,
                  OWL_NS.someValuesFrom],
                 None)):
                innerCls = Class(innerClsId, skipOWLClassMembership=True)
                if isinstance(innerClsId, BNode):
                    for _c in ComponentTerms(innerCls):
                        yield _c
                else:
                    yield innerCls
        except:
            pass
    else:
        cls = CastClass(cls, Individual.factoryGraph)
        if isinstance(cls, BooleanClass):
            for _cls in cls:
                _cls = Class(_cls, skipOWLClassMembership=True)
                if isinstance(_cls.identifier, BNode):
                    for _c in ComponentTerms(_cls):
                        yield _c
                else:
                    yield _cls
        else:
            for innerCls in cls.subClassOf:
                if isinstance(innerCls.identifier, BNode):
                    for _c in ComponentTerms(innerCls):
                        yield _c
                else:
                    yield innerCls
            for s, p, o in cls.factoryGraph.triples_choices(
                (classOrIdentifier(cls),
                 CLASS_RELATIONS,
                 None)
            ):
                if isinstance(o, BNode):
                    for _c in ComponentTerms(
                            CastClass(o, Individual.factoryGraph)):
                        yield _c
                else:
                    yield innerCls


def DeepClassClear(classToPrune):
    """
    Recursively clear the given class, continuing
    where any related class is an anonymous class

    >>> EX = Namespace('http://example.com/')
    >>> namespace_manager = NamespaceManager(Graph())
    >>> namespace_manager.bind('ex', EX, override=False)
    >>> namespace_manager.bind('owl', OWL_NS, override=False)
    >>> g = Graph()
    >>> g.namespace_manager = namespace_manager
    >>> Individual.factoryGraph = g
    >>> classB = Class(EX.B)
    >>> classC = Class(EX.C)
    >>> classD = Class(EX.D)
    >>> classE = Class(EX.E)
    >>> classF = Class(EX.F)
    >>> anonClass = EX.someProp | some | classD #doctest: +SKIP
    >>> classF += anonClass #doctest: +SKIP
    >>> list(anonClass.subClassOf) #doctest: +SKIP
    [Class: ex:F ]
    >>> classA = classE | classF | anonClass #doctest: +SKIP
    >>> classB += classA #doctest: +SKIP
    >>> classA.equivalentClass = [Class()] #doctest: +SKIP
    >>> classB.subClassOf = [EX.someProp | some | classC] #doctest: +SKIP
    >>> classA #doctest: +SKIP
    ( ex:E OR ex:F OR ( ex:someProp SOME ex:D ) )
    >>> DeepClassClear(classA) #doctest: +SKIP
    >>> classA #doctest: +SKIP
    (  )
    >>> list(anonClass.subClassOf) #doctest: +SKIP
    []
    >>> classB #doctest: +SKIP
    Class: ex:B SubClassOf: ( ex:someProp SOME ex:C )

    >>> otherClass = classD | anonClass #doctest: +SKIP
    >>> otherClass #doctest: +SKIP
    ( ex:D OR ( ex:someProp SOME ex:D ) )
    >>> DeepClassClear(otherClass) #doctest: +SKIP
    >>> otherClass #doctest: +SKIP
    (  )
    >>> otherClass.delete() #doctest: +SKIP
    >>> list(g.triples((otherClass.identifier, None, None))) #doctest: +SKIP
    []
    """
    def deepClearIfBNode(_class):
        if isinstance(classOrIdentifier(_class), BNode):
            DeepClassClear(_class)
    classToPrune = CastClass(classToPrune, Individual.factoryGraph)
    for c in classToPrune.subClassOf:
        deepClearIfBNode(c)
    classToPrune.graph.remove((classToPrune.identifier, RDFS.subClassOf, None))
    for c in classToPrune.equivalentClass:
        deepClearIfBNode(c)
    classToPrune.graph.remove(
        (classToPrune.identifier, OWL_NS.equivalentClass, None))
    inverseClass = classToPrune.complementOf
    if inverseClass:
        classToPrune.graph.remove(
            (classToPrune.identifier, OWL_NS.complementOf, None))
        deepClearIfBNode(inverseClass)
    if isinstance(classToPrune, BooleanClass):
        for c in classToPrune:
            deepClearIfBNode(c)
        classToPrune.clear()
        classToPrune.graph.remove((classToPrune.identifier,
                                   classToPrune._operator,
                                   None))


class MalformedClass(Exception):
    def __init__(self, msg):
        self.msg = msg

    def __repr__(self):
        return self.msg


def CastClass(c, graph=None):
    graph = graph is None and c.factoryGraph or graph
    for kind in graph.objects(subject=classOrIdentifier(c),
                              predicate=RDF.type):
        if kind == OWL_NS.Restriction:
            kwArgs = {'identifier': classOrIdentifier(c),
                      'graph': graph}
            for s, p, o in graph.triples((classOrIdentifier(c),
                                          None,
                                          None)):
                if p != RDF.type:
                    if p == OWL_NS.onProperty:
                        kwArgs['onProperty'] = o
                    else:
                        if p not in Restriction.restrictionKinds:
                            continue
                        kwArgs[str(p.split(OWL_NS)[-1])] = o
            if not set([str(i.split(OWL_NS)[-1])
                        for i in Restriction.restrictionKinds]
                       ).intersection(kwArgs):
                raise MalformedClass("Malformed owl:Restriction")
            return Restriction(**kwArgs)
        else:
            for s, p, o in graph.triples_choices((classOrIdentifier(c),
                                                  [OWL_NS.intersectionOf,
                                                 OWL_NS.unionOf,
                                                 OWL_NS.oneOf],
                                                  None)):
                if p == OWL_NS.oneOf:
                    return EnumeratedClass(classOrIdentifier(c), graph=graph)
                else:
                    return BooleanClass(
                        classOrIdentifier(c), operator=p, graph=graph)
            # assert (classOrIdentifier(c),RDF.type,OWL_NS.Class) in graph
            return Class(
                classOrIdentifier(c), graph=graph, skipOWLClassMembership=True)


class Class(AnnotatableTerms):
    """
    'General form' for classes:

    The Manchester Syntax (supported in Protege) is used as the basis for the
    form of this class

    See: http://owl-workshop.man.ac.uk/acceptedLong/submission_9.pdf:

    [Annotation]
    Class: classID {Annotation
                        ( (SubClassOf: ClassExpression)
                        | (EquivalentTo ClassExpression)
                        | (DisjointWith ClassExpression)) }

    Appropriate excerpts from OWL Reference:

    ".. Subclass axioms provide us with partial definitions: they represent
     necessary but not sufficient conditions for establishing class
     membership of an individual."

    ".. A class axiom may contain (multiple) owl:equivalentClass statements"

    "..A class axiom may also contain (multiple) owl:disjointWith statements.."

    "..An owl:complementOf property links a class to precisely one class
      description."

    """
    def _serialize(self, graph):
        for cl in self.subClassOf:
            CastClass(cl, self.graph).serialize(graph)
        for cl in self.equivalentClass:
            CastClass(cl, self.graph).serialize(graph)
        for cl in self.disjointWith:
            CastClass(cl, self.graph).serialize(graph)
        if self.complementOf:
            CastClass(self.complementOf, self.graph).serialize(graph)

    def serialize(self, graph):
        for fact in self.graph.triples((self.identifier, None, None)):
            graph.add(fact)
        self._serialize(graph)

    def setupNounAnnotations(self, nounAnnotations):
        if isinstance(nounAnnotations, tuple):
            CN_sgProp, CN_plProp = nounAnnotations
        else:
            CN_sgProp = nounAnnotations
            CN_plProp = nounAnnotations

        if CN_sgProp:
            self.CN_sgProp.extent = [(self.identifier,
                                      self.handleAnnotation(CN_sgProp))]
        if CN_plProp:
            self.CN_plProp.extent = [(self.identifier,
                                      self.handleAnnotation(CN_plProp))]

    def __init__(self, identifier=None, subClassOf=None, equivalentClass=None,
                 disjointWith=None, complementOf=None, graph=None,
                 skipOWLClassMembership=False, comment=None,
                 nounAnnotations=None,
                 nameAnnotation=None,
                 nameIsLabel=False):
        super(Class, self).__init__(identifier, graph,
                                    nameAnnotation, nameIsLabel)

        if nounAnnotations:
            self.setupNounAnnotations(nounAnnotations)
        if not skipOWLClassMembership \
                and (self.identifier, RDF.type, OWL_NS.Class) \
                not in self.graph and \
                (self.identifier, RDF.type, OWL_NS.Restriction) \
                not in self.graph:
            self.graph.add((self.identifier, RDF.type, OWL_NS.Class))

        self.subClassOf = subClassOf and subClassOf or []
        self.equivalentClass = equivalentClass and equivalentClass or []
        self.disjointWith = disjointWith and disjointWith or []
        if complementOf:
            self.complementOf = complementOf
        self.comment = comment and comment or []

    def _get_extent(self, graph=None):
        for member in (
            graph is None and self.graph or graph).subjects(
                predicate=RDF.type, object=self.identifier):
            yield member

    def _set_extent(self, other):
        if not other:
            return
        for m in other:
            self.graph.add((classOrIdentifier(m), RDF.type, self.identifier))

    @TermDeletionHelper(RDF.type)
    def _del_type(self):
        pass

    extent = property(_get_extent, _set_extent, _del_type)

    def _get_annotation(self, term=RDFS.label):
        for annotation in self.graph.objects(subject=self, predicate=term):
            yield annotation

    annotation = property(_get_annotation, lambda x: x)

    def _get_extentQuery(self):
        return (Variable('CLASS'), RDF.type, self.identifier)

    def _set_extentQuery(self, other):
        pass

    extentQuery = property(_get_extentQuery, _set_extentQuery)

    def __hash__(self):
        """
        >>> b=Class(OWL_NS.Restriction)
        >>> c=Class(OWL_NS.Restriction)
        >>> len(set([b,c]))
        1
        """
        return hash(self.identifier)

    def __eq__(self, other):
        assert isinstance(other, Class), repr(other)
        return self.identifier == other.identifier

    def __iadd__(self, other):
        assert isinstance(other, Class)
        other.subClassOf = [self]
        return self

    def __isub__(self, other):
        assert isinstance(other, Class)
        self.graph.remove(
            (classOrIdentifier(other), RDFS.subClassOf, self.identifier))
        return self

    def __invert__(self):
        """
        Shorthand for Manchester syntax's not operator
        """
        return Class(complementOf=self)

    def __or__(self, other):
        """
        Construct an anonymous class description consisting of the union of
        this class and 'other' and return it
        """
        return BooleanClass(
            operator=OWL_NS.unionOf, members=[self, other], graph=self.graph)

    def __and__(self, other):
        """
        Construct an anonymous class description consisting of the
        intersection of this class and 'other' and return it

        >>> exNs = Namespace('http://example.com/')
        >>> namespace_manager = NamespaceManager(Graph())
        >>> namespace_manager.bind('ex', exNs, override=False)
        >>> namespace_manager.bind('owl', OWL_NS, override=False)
        >>> g = Graph()
        >>> g.namespace_manager = namespace_manager

        Chaining 3 intersections

        >>> female = Class(exNs.Female, graph=g)
        >>> human = Class(exNs.Human, graph=g)
        >>> youngPerson = Class(exNs.YoungPerson, graph=g)
        >>> youngWoman = female & human & youngPerson
        >>> youngWoman #doctest: +SKIP
        ex:YoungPerson THAT ( ex:Female AND ex:Human )
        >>> isinstance(youngWoman, BooleanClass)
        True
        >>> isinstance(youngWoman.identifier, BNode)
        True
        """
        return BooleanClass(
            operator=OWL_NS.intersectionOf,
            members=[self, other], graph=self.graph)

    def _get_subClassOf(self):
        for anc in self.graph.objects(
                subject=self.identifier, predicate=RDFS.subClassOf):
            yield Class(anc,
                        graph=self.graph,
                        skipOWLClassMembership=True)

    def _set_subClassOf(self, other):
        if not other:
            return
        for sc in other:
            self.graph.add(
                (self.identifier, RDFS.subClassOf, classOrIdentifier(sc)))

    @TermDeletionHelper(RDFS.subClassOf)
    def _del_subClassOf(self):
        pass

    subClassOf = property(_get_subClassOf, _set_subClassOf, _del_subClassOf)

    def _get_equivalentClass(self):
        for ec in self.graph.objects(
                subject=self.identifier, predicate=OWL_NS.equivalentClass):
            yield Class(ec, graph=self.graph)

    def _set_equivalentClass(self, other):
        if not other:
            return
        for sc in other:
            self.graph.add((self.identifier,
                           OWL_NS.equivalentClass, classOrIdentifier(sc)))

    @TermDeletionHelper(OWL_NS.equivalentClass)
    def _del_equivalentClass(self):
        pass

    equivalentClass = property(
        _get_equivalentClass, _set_equivalentClass, _del_equivalentClass)

    def _get_disjointWith(self):
        for dc in self.graph.objects(
                subject=self.identifier, predicate=OWL_NS.disjointWith):
            yield Class(dc, graph=self.graph)

    def _set_disjointWith(self, other):
        if not other:
            return
        for c in other:
            self.graph.add(
                (self.identifier, OWL_NS.disjointWith, classOrIdentifier(c)))

    @TermDeletionHelper(OWL_NS.disjointWith)
    def _del_disjointWith(self):
        pass

    disjointWith = property(
        _get_disjointWith, _set_disjointWith, _del_disjointWith)

    def _get_complementOf(self):
        comp = list(self.graph.objects(
            subject=self.identifier, predicate=OWL_NS.complementOf))
        if not comp:
            return None
        elif len(comp) == 1:
            return Class(comp[0], graph=self.graph)
        else:
            raise Exception(len(comp))

    def _set_complementOf(self, other):
        if not other:
            return
        self.graph.add(
            (self.identifier, OWL_NS.complementOf, classOrIdentifier(other)))

    @TermDeletionHelper(OWL_NS.complementOf)
    def _del_complementOf(self):
        pass

    complementOf = property(
        _get_complementOf, _set_complementOf, _del_complementOf)

    def _get_parents(self):
        """
        computed attributes that returns a generator over taxonomic 'parents'
        by disjunction, conjunction, and subsumption

        >>> from rdflib.util import first
        >>> exNs = Namespace('http://example.com/')
        >>> namespace_manager = NamespaceManager(Graph())
        >>> namespace_manager.bind('ex', exNs, override=False)
        >>> namespace_manager.bind('owl', OWL_NS, override=False)
        >>> g = Graph()
        >>> g.namespace_manager = namespace_manager
        >>> Individual.factoryGraph = g
        >>> brother = Class(exNs.Brother)
        >>> sister = Class(exNs.Sister)
        >>> sibling = brother | sister
        >>> sibling.identifier = exNs.Sibling
        >>> sibling #doctest: +SKIP
        ( ex:Brother OR ex:Sister )
        >>> first(brother.parents) #doctest: +SKIP
        Class: ex:Sibling EquivalentTo: ( ex:Brother OR ex:Sister )
        >>> parent = Class(exNs.Parent)
        >>> male = Class(exNs.Male)
        >>> father = parent & male
        >>> father.identifier = exNs.Father
        >>> list(father.parents) #doctest: +SKIP
        [Class: ex:Parent , Class: ex:Male ]

        """
        for parent in itertools.chain(self.subClassOf,
                                      self.equivalentClass):
            yield parent

        link = first(self.factoryGraph.subjects(RDF.first, self.identifier))
        if link:
            listSiblings = list(self.factoryGraph.transitive_subjects(RDF.rest,
                                                                      link))
            if listSiblings:
                collectionHead = listSiblings[-1]
            else:
                collectionHead = link
            for disjCls in self.factoryGraph.subjects(
                    OWL_NS.unionOf, collectionHead):
                if isinstance(disjCls, URIRef):
                    yield Class(disjCls, skipOWLClassMembership=True)
        for rdfList in self.factoryGraph.objects(
                self.identifier, OWL_NS.intersectionOf):
            for member in OWLRDFListProxy([rdfList], graph=self.factoryGraph):
                if isinstance(member, URIRef):
                    yield Class(member, skipOWLClassMembership=True)

    parents = property(_get_parents)

    def isPrimitive(self):
        if (self.identifier, RDF.type, OWL_NS.Restriction) in self.graph:
            return False
        # sc = list(self.subClassOf)
        ec = list(self.equivalentClass)
        for boolClass, p, rdfList in self.graph.triples_choices(
            (self.identifier,
             [OWL_NS.intersectionOf,
              OWL_NS.unionOf],
             None)):
            ec.append(manchesterSyntax(rdfList, self.graph, boolean=p))
        for e in ec:
            return False
        if self.complementOf:
            return False
        return True

    def subSumpteeIds(self):
        for s in self.graph.subjects(
                predicate=RDFS.subClassOf, object=self.identifier):
            yield s

    # def __iter__(self):
    #     for s in self.graph.subjects(
    #        predicate=RDFS.subClassOf,object=self.identifier):
    #         yield Class(s,skipOWLClassMembership=True)

    def __repr__(self, full=False, normalization=True):
        """
        Returns the Manchester Syntax equivalent for this class
        """
        exprs = []
        sc = list(self.subClassOf)
        ec = list(self.equivalentClass)
        for boolClass, p, rdfList in self.graph.triples_choices(
            (self.identifier,
             [OWL_NS.intersectionOf,
              OWL_NS.unionOf],
             None)):
            ec.append(manchesterSyntax(rdfList, self.graph, boolean=p))
        dc = list(self.disjointWith)
        c = self.complementOf
        if c:
            dc.append(c)
        klassKind = ''
        label = list(self.graph.objects(self.identifier, RDFS.label))
        label = label and '(' + label[0] + ')' or ''
        if sc:
            if full:
                scJoin = '\n                '
            else:
                scJoin = ', '
            necStatements = [
                isinstance(s, Class) and isinstance(self.identifier, BNode) and
                repr(CastClass(s, self.graph)) or
                # repr(BooleanClass(classOrIdentifier(s),
                #                  operator=None,
                #                  graph=self.graph)) or
                manchesterSyntax(classOrIdentifier(s), self.graph) for s in sc]
            if necStatements:
                klassKind = "Primitive Type %s" % label
            exprs.append("SubClassOf: %s" % scJoin.join(
                [str(n) for n in necStatements]))
            if full:
                exprs[-1] = "\n    " + exprs[-1]
        if ec:
            nec_SuffStatements = [
                isinstance(s, str) and s or
                manchesterSyntax(classOrIdentifier(s), self.graph) for s in ec]
            if nec_SuffStatements:
                klassKind = "A Defined Class %s" % label
            exprs.append("EquivalentTo: %s" % ', '.join(nec_SuffStatements))
            if full:
                exprs[-1] = "\n    " + exprs[-1]
        if dc:
            exprs.append("DisjointWith %s\n" % '\n                 '.join(
                [manchesterSyntax(classOrIdentifier(s), self.graph)
                    for s in dc]))
            if full:
                exprs[-1] = "\n    " + exprs[-1]
        descr = list(self.graph.objects(self.identifier, RDFS.comment))
        if full and normalization:
            klassDescr = klassKind and '\n    ## %s ##' % klassKind +\
                (descr and "\n    %s" % descr[0] or '') + \
                ' . '.join(exprs) or ' . '.join(exprs)
        else:
            klassDescr = full and (descr and "\n    %s" %
                                   descr[0] or '') or '' + ' . '.join(exprs)
        return (isinstance(self.identifier, BNode)
                and "Some Class "
                or "Class: %s " % self.qname) + klassDescr


class OWLRDFListProxy(object):
    def __init__(self, rdfList, members=None, graph=None):
        if graph:
            self.graph = graph
        members = members and members or []
        if rdfList:
            self._rdfList = Collection(self.graph, rdfList[0])
            for member in members:
                if member not in self._rdfList:
                    self._rdfList.append(classOrIdentifier(member))
        else:
            self._rdfList = Collection(self.graph, BNode(),
                                       [classOrIdentifier(m) for m in members])
            self.graph.add(
                (self.identifier, self._operator, self._rdfList.uri))

    def __eq__(self, other):
        """
        Equivalence of boolean class constructors is determined by
        equivalence of its members
        """
        assert isinstance(other, Class), repr(other) + repr(type(other))
        if isinstance(other, BooleanClass):
            length = len(self)
            if length != len(other):
                return False
            else:
                for idx in range(length):
                    if self[idx] != other[idx]:
                        return False
                    return True
        else:
            return self.identifier == other.identifier

    # Redirect python list accessors to the underlying Collection instance
    def __len__(self):
        return len(self._rdfList)

    def index(self, item):
        return self._rdfList.index(classOrIdentifier(item))

    def __getitem__(self, key):
        return self._rdfList[key]

    def __setitem__(self, key, value):
        self._rdfList[key] = classOrIdentifier(value)

    def __delitem__(self, key):
        del self._rdfList[key]

    def clear(self):
        self._rdfList.clear()

    def __iter__(self):
        for item in self._rdfList:
            yield item

    def __contains__(self, item):
        for i in self._rdfList:
            if i == classOrIdentifier(item):
                return 1
        return 0

    def append(self, item):
        self._rdfList.append(item)

    def __iadd__(self, other):
        self._rdfList.append(classOrIdentifier(other))
        return self


class EnumeratedClass(OWLRDFListProxy, Class):
    py3compat.format_doctest_out("""
    Class for owl:oneOf forms:

    OWL Abstract Syntax is used

    axiom ::= 'EnumeratedClass('
        classID ['Deprecated'] { annotation } { individualID } ')'


    >>> exNs = Namespace('http://example.com/')
    >>> namespace_manager = NamespaceManager(Graph())
    >>> namespace_manager.bind('ex', exNs, override=False)
    >>> namespace_manager.bind('owl', OWL_NS, override=False)
    >>> g = Graph()
    >>> g.namespace_manager = namespace_manager
    >>> Individual.factoryGraph = g
    >>> ogbujiBros = EnumeratedClass(exNs.ogbujicBros,
    ...                              members=[exNs.chime,
    ...                                       exNs.uche,
    ...                                       exNs.ejike])
    >>> ogbujiBros #doctest: +SKIP
    { ex:chime ex:uche ex:ejike }
    >>> col = Collection(g, first(
    ...    g.objects(predicate=OWL_NS.oneOf, subject=ogbujiBros.identifier)))
    >>> [g.qname(item) for item in col]
    [%(u)s'ex:chime', %(u)s'ex:uche', %(u)s'ex:ejike']
    >>> print(g.serialize(format='n3')) #doctest: +SKIP
    @prefix ex: <http://example.com/> .
    @prefix owl: <http://www.w3.org/2002/07/owl#> .
    @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
    <BLANKLINE>
    ex:ogbujicBros a owl:Class;
        owl:oneOf ( ex:chime ex:uche ex:ejike ) .
    <BLANKLINE>
    <BLANKLINE>
    """)
    _operator = OWL_NS.oneOf

    def isPrimitive(self):
        return False

    def __init__(self, identifier=None, members=None, graph=None):
        Class.__init__(self, identifier, graph=graph)
        members = members and members or []
        rdfList = list(self.graph.objects(
            predicate=OWL_NS.oneOf, subject=self.identifier))
        OWLRDFListProxy.__init__(self, rdfList, members)

    def __repr__(self):
        """
        Returns the Manchester Syntax equivalent for this class
        """
        return manchesterSyntax(
            self._rdfList.uri, self.graph, boolean=self._operator)

    def serialize(self, graph):
        clonedList = Collection(graph, BNode())
        for cl in self._rdfList:
            clonedList.append(cl)
            CastClass(cl, self.graph).serialize(graph)

        graph.add((self.identifier, self._operator, clonedList.uri))
        for s, p, o in self.graph.triples((self.identifier, None, None)):
            if p != self._operator:
                graph.add((s, p, o))
        self._serialize(graph)

BooleanPredicates = [OWL_NS.intersectionOf, OWL_NS.unionOf]


class BooleanClassExtentHelper:
    """
    >>> testGraph = Graph()
    >>> Individual.factoryGraph = testGraph
    >>> EX = Namespace("http://example.com/")
    >>> namespace_manager = NamespaceManager(Graph())
    >>> namespace_manager.bind('ex', EX, override=False)
    >>> testGraph.namespace_manager = namespace_manager
    >>> fire = Class(EX.Fire)
    >>> water = Class(EX.Water)
    >>> testClass = BooleanClass(members=[fire, water])
    >>> testClass2 = BooleanClass(
    ...     operator=OWL_NS.unionOf, members=[fire, water])
    >>> for c in BooleanClass.getIntersections():
    ...     print(c) #doctest: +SKIP
    ( ex:Fire AND ex:Water )
    >>> for c in BooleanClass.getUnions():
    ...     print(c) #doctest: +SKIP
    ( ex:Fire OR ex:Water )
    """
    def __init__(self, operator):
        self.operator = operator

    def __call__(self, f):
        def _getExtent():
            for c in Individual.factoryGraph.subjects(self.operator):
                yield BooleanClass(c, operator=self.operator)
        return _getExtent


class Callable():
    def __init__(self, anycallable):
        self.__call__ = anycallable


class BooleanClass(OWLRDFListProxy, Class):
    """
    See: http://www.w3.org/TR/owl-ref/#Boolean

    owl:complementOf is an attribute of Class, however

    """
    @BooleanClassExtentHelper(OWL_NS.intersectionOf)
    @Callable
    def getIntersections():
        pass
    getIntersections = Callable(getIntersections)

    @BooleanClassExtentHelper(OWL_NS.unionOf)
    @Callable
    def getUnions():
        pass
    getUnions = Callable(getUnions)

    def __init__(self, identifier=None, operator=OWL_NS.intersectionOf,
                 members=None, graph=None):
        if operator is None:
            props = []
            for s, p, o in graph.triples_choices((identifier,
                                                  [OWL_NS.intersectionOf,
                                                 OWL_NS.unionOf],
                                                 None)):
                props.append(p)
                operator = p
            assert len(props) == 1, repr(props)
        Class.__init__(self, identifier, graph=graph)
        assert operator in [OWL_NS.intersectionOf,
                            OWL_NS.unionOf], str(operator)
        self._operator = operator
        rdfList = list(
            self.graph.objects(predicate=operator, subject=self.identifier))
        assert not members or not rdfList, \
            "This is a previous boolean class description!" + \
            repr(Collection(self.graph, rdfList[0]).n3())
        OWLRDFListProxy.__init__(self, rdfList, members)

    def copy(self):
        """
        Create a copy of this class
        """
        copyOfClass = BooleanClass(
            operator=self._operator, members=list(self), graph=self.graph)
        return copyOfClass

    def serialize(self, graph):
        clonedList = Collection(graph, BNode())
        for cl in self._rdfList:
            clonedList.append(cl)
            CastClass(cl, self.graph).serialize(graph)

        graph.add((self.identifier, self._operator, clonedList.uri))

        for s, p, o in self.graph.triples((self.identifier, None, None)):
            if p != self._operator:
                graph.add((s, p, o))
        self._serialize(graph)

    def isPrimitive(self):
        return False

    def changeOperator(self, newOperator):
        """
        Converts a unionOf / intersectionOf class expression into one
        that instead uses the given operator


        >>> testGraph = Graph()
        >>> Individual.factoryGraph = testGraph
        >>> EX = Namespace("http://example.com/")
        >>> namespace_manager = NamespaceManager(Graph())
        >>> namespace_manager.bind('ex', EX, override=False)
        >>> testGraph.namespace_manager = namespace_manager
        >>> fire = Class(EX.Fire)
        >>> water = Class(EX.Water)
        >>> testClass = BooleanClass(members=[fire,water])
        >>> testClass #doctest: +SKIP
        ( ex:Fire AND ex:Water )
        >>> testClass.changeOperator(OWL_NS.unionOf)
        >>> testClass #doctest: +SKIP
        ( ex:Fire OR ex:Water )
        >>> try: testClass.changeOperator(OWL_NS.unionOf)
        ... except Exception%s: print(e)
        The new operator is already being used!

        """ % 'as e' if py3compat.PY3 else ', e'
        assert newOperator != self._operator, \
            "The new operator is already being used!"
        self.graph.remove((self.identifier, self._operator, self._rdfList.uri))
        self.graph.add((self.identifier, newOperator, self._rdfList.uri))
        self._operator = newOperator

    def __repr__(self):
        """
        Returns the Manchester Syntax equivalent for this class
        """
        return manchesterSyntax(
            self._rdfList.uri, self.graph, boolean=self._operator)

    def __or__(self, other):
        """
        Adds other to the list and returns self
        """
        assert self._operator == OWL_NS.unionOf
        self._rdfList.append(classOrIdentifier(other))
        return self


def AllDifferent(members):
    """
    DisjointClasses(' description description { description } ')'

    """
    pass


class Restriction(Class):
    """
    restriction ::= 'restriction('
                            datavaluedPropertyID dataRestrictionComponent
                                 { dataRestrictionComponent } ')'
                  | 'restriction(' individualvaluedPropertyID
                      individualRestrictionComponent
                      { individualRestrictionComponent } ')'
    """

    restrictionKinds = [OWL_NS.allValuesFrom,
                        OWL_NS.someValuesFrom,
                        OWL_NS.hasValue,
                        OWL_NS.maxCardinality,
                        OWL_NS.minCardinality]

    def __init__(self,
                 onProperty,
                 graph=Graph(),
                 allValuesFrom=None,
                 someValuesFrom=None,
                 value=None,
                 cardinality=None,
                 maxCardinality=None,
                 minCardinality=None,
                 identifier=None):
        super(Restriction, self).__init__(identifier,
                                          graph=graph,
                                          skipOWLClassMembership=True)
        if (self.identifier,
            OWL_NS.onProperty,
                propertyOrIdentifier(onProperty)) not in graph:
            graph.add((self.identifier, OWL_NS.onProperty,
                      propertyOrIdentifier(onProperty)))
        self.onProperty = onProperty
        restrTypes = [
            (allValuesFrom, OWL_NS.allValuesFrom),
            (someValuesFrom, OWL_NS.someValuesFrom),
            (value, OWL_NS.hasValue),
            (cardinality, OWL_NS.cardinality),
            (maxCardinality, OWL_NS.maxCardinality),
            (minCardinality, OWL_NS.minCardinality)]
        validRestrProps = [(i, oTerm) for (i, oTerm) in restrTypes if i]
        assert len(validRestrProps)
        restrictionRange, restrictionType = validRestrProps.pop()
        self.restrictionType = restrictionType
        if isinstance(restrictionRange, Identifier):
            self.restrictionRange = restrictionRange
        elif isinstance(restrictionRange, Class):
            self.restrictionRange = classOrIdentifier(restrictionRange)
        else:
            self.restrictionRange = first(self.graph.objects(self.identifier,
                                                             restrictionType))
        if (self.identifier,
            restrictionType,
                self.restrictionRange) not in self.graph:
            self.graph.add(
                (self.identifier, restrictionType, self.restrictionRange))
        assert self.restrictionRange is not None, Class(self.identifier)
        if (self.identifier, RDF.type, OWL_NS.Restriction) not in self.graph:
            self.graph.add((self.identifier, RDF.type, OWL_NS.Restriction))
            self.graph.remove((self.identifier, RDF.type, OWL_NS.Class))

    @py3compat.format_doctest_out
    def serialize(self, graph):
        """
        >>> g1 = Graph()
        >>> g2 = Graph()
        >>> EX = Namespace("http://example.com/")
        >>> namespace_manager = NamespaceManager(g1)
        >>> namespace_manager.bind('ex', EX, override=False)
        >>> namespace_manager = NamespaceManager(g2)
        >>> namespace_manager.bind('ex', EX, override=False)
        >>> Individual.factoryGraph = g1
        >>> prop = Property(EX.someProp, baseType=OWL_NS.DatatypeProperty)
        >>> restr1 = (Property(
        ...    EX.someProp,
        ...    baseType=OWL_NS.DatatypeProperty)) | some | (Class(EX.Foo))
        >>> restr1 #doctest: +SKIP
        ( ex:someProp SOME ex:Foo )
        >>> restr1.serialize(g2)
        >>> Individual.factoryGraph = g2
        >>> list(Property(
        ...     EX.someProp,baseType=None).type
        ... ) #doctest: +NORMALIZE_WHITESPACE +SKIP
        [rdflib.term.URIRef(
            %(u)s'http://www.w3.org/2002/07/owl#DatatypeProperty')]
        """
        Property(
            self.onProperty, graph=self.graph, baseType=None).serialize(graph)
        for s, p, o in self.graph.triples((self.identifier, None, None)):
            graph.add((s, p, o))
            if p in [OWL_NS.allValuesFrom, OWL_NS.someValuesFrom]:
                CastClass(o, self.graph).serialize(graph)

    def isPrimitive(self):
        return False

    def __hash__(self):
        return hash((self.onProperty, self.restrictionRange))

    def __eq__(self, other):
        """
        Equivalence of restrictions is determined by equivalence of the
        property in question and the restriction 'range'
        """
        assert isinstance(other, Class), repr(other) + repr(type(other))
        if isinstance(other, Restriction):
            return other.onProperty == self.onProperty and \
                other.restrictionRange == self.restrictionRange
        else:
            return False

    def _get_onProperty(self):
        return list(self.graph.objects(
            subject=self.identifier, predicate=OWL_NS.onProperty))[0]

    def _set_onProperty(self, prop):
        triple = (
            self.identifier, OWL_NS.onProperty, propertyOrIdentifier(prop))
        if not prop:
            return
        elif triple in self.graph:
            return
        else:
            self.graph.set(triple)

    @TermDeletionHelper(OWL_NS.onProperty)
    def _del_onProperty(self):
        pass

    onProperty = property(_get_onProperty, _set_onProperty, _del_onProperty)

    def _get_allValuesFrom(self):
        for i in self.graph.objects(
                subject=self.identifier, predicate=OWL_NS.allValuesFrom):
            return Class(i, graph=self.graph)
        return None

    def _set_allValuesFrom(self, other):
        triple = (
            self.identifier, OWL_NS.allValuesFrom, classOrIdentifier(other))
        if not other:
            return
        elif triple in self.graph:
            return
        else:
            self.graph.set(triple)

    @TermDeletionHelper(OWL_NS.allValuesFrom)
    def _del_allValuesFrom(self):
        pass

    allValuesFrom = property(
        _get_allValuesFrom, _set_allValuesFrom, _del_allValuesFrom)

    def _get_someValuesFrom(self):
        for i in self.graph.objects(
                subject=self.identifier, predicate=OWL_NS.someValuesFrom):
            return Class(i, graph=self.graph)
        return None

    def _set_someValuesFrom(self, other):
        triple = (
            self.identifier, OWL_NS.someValuesFrom, classOrIdentifier(other))
        if not other:
            return
        elif triple in self.graph:
            return
        else:
            self.graph.set(triple)

    @TermDeletionHelper(OWL_NS.someValuesFrom)
    def _del_someValuesFrom(self):
        pass

    someValuesFrom = property(
        _get_someValuesFrom, _set_someValuesFrom, _del_someValuesFrom)

    def _get_hasValue(self):
        for i in self.graph.objects(
                subject=self.identifier, predicate=OWL_NS.hasValue):
            return Class(i, graph=self.graph)
        return None

    def _set_hasValue(self, other):
        triple = (self.identifier, OWL_NS.hasValue, classOrIdentifier(other))
        if not other:
            return
        elif triple in self.graph:
            return
        else:
            self.graph.set(triple)

    @TermDeletionHelper(OWL_NS.hasValue)
    def _del_hasValue(self):
        pass

    hasValue = property(_get_hasValue, _set_hasValue, _del_hasValue)

    def _get_cardinality(self):
        for i in self.graph.objects(
                subject=self.identifier, predicate=OWL_NS.cardinality):
            return Class(i, graph=self.graph)
        return None

    def _set_cardinality(self, other):
        triple = (
            self.identifier, OWL_NS.cardinality, classOrIdentifier(other))
        if not other:
            return
        elif triple in self.graph:
            return
        else:
            self.graph.set(triple)

    @TermDeletionHelper(OWL_NS.cardinality)
    def _del_cardinality(self):
        pass

    cardinality = property(
        _get_cardinality, _set_cardinality, _del_cardinality)

    def _get_maxCardinality(self):
        for i in self.graph.objects(
                subject=self.identifier, predicate=OWL_NS.maxCardinality):
            return Class(i, graph=self.graph)
        return None

    def _set_maxCardinality(self, other):
        triple = (
            self.identifier, OWL_NS.maxCardinality, classOrIdentifier(other))
        if not other:
            return
        elif triple in self.graph:
            return
        else:
            self.graph.set(triple)

    @TermDeletionHelper(OWL_NS.maxCardinality)
    def _del_maxCardinality(self):
        pass

    maxCardinality = property(
        _get_maxCardinality, _set_maxCardinality, _del_maxCardinality)

    def _get_minCardinality(self):
        for i in self.graph.objects(
                subject=self.identifier, predicate=OWL_NS.minCardinality):
            return Class(i, graph=self.graph)
        return None

    def _set_minCardinality(self, other):
        triple = (
            self.identifier, OWL_NS.minCardinality, classOrIdentifier(other))
        if not other:
            return
        elif triple in self.graph:
            return
        else:
            self.graph.set(triple)

    @TermDeletionHelper(OWL_NS.minCardinality)
    def _del_minCardinality(self):
        pass

    minCardinality = property(
        _get_minCardinality, _set_minCardinality, _del_minCardinality)

    def restrictionKind(self):
        for p in self.graph.triple_choices((self.identifier,
                                            self.restrictionKinds,
                                            None)):
            return p.split(OWL_NS)[-1]
        raise

    def __repr__(self):
        """
        Returns the Manchester Syntax equivalent for this restriction
        """
        return manchesterSyntax(self.identifier, self.graph)

### Infix Operators ###

some = Infix(lambda prop, _class: Restriction(prop, graph=_class.graph,
                                              someValuesFrom=_class))
only = Infix(lambda prop, _class: Restriction(prop, graph=_class.graph,
                                              allValuesFrom=_class))
max = Infix(lambda prop, _class: Restriction(prop, graph=prop.graph,
                                             maxCardinality=_class))
min = Infix(lambda prop, _class: Restriction(prop, graph=prop.graph,
                                             minCardinality=_class))
exactly = Infix(lambda prop, _class: Restriction(prop, graph=prop.graph,
                                                 cardinality=_class))
value = Infix(
    lambda prop, _class: Restriction(prop, graph=prop.graph, value=_class))

PropertyAbstractSyntax =\
    """
%s( %s { %s }
%s
{ 'super(' datavaluedPropertyID ')'} ['Functional']
{ domain( %s ) } { range( %s ) } )"""


class Property(AnnotatableTerms):
    """
    axiom ::= 'DatatypeProperty(' datavaluedPropertyID ['Deprecated']
                { annotation }
                { 'super(' datavaluedPropertyID ')'} ['Functional']
                { 'domain(' description ')' } { 'range(' dataRange ')' } ')'
            | 'ObjectProperty(' individualvaluedPropertyID ['Deprecated']
                { annotation }
                { 'super(' individualvaluedPropertyID ')' }
                [ 'inverseOf(' individualvaluedPropertyID ')' ] [ 'Symmetric' ]
                [ 'Functional' | 'InverseFunctional' |
                  'Functional' 'InverseFunctional' |
                  'Transitive' ]
                { 'domain(' description ')' } { 'range(' description ')' } ')
    """

    def setupVerbAnnotations(self, verbAnnotations):
        if isinstance(verbAnnotations, tuple):
            TV_sgProp, TV_plProp, TV_vbg = verbAnnotations
        else:
            TV_sgProp = verbAnnotations
            TV_plProp = verbAnnotations
            TV_vbg = verbAnnotations
        if TV_sgProp:
            self.TV_sgProp.extent = [(self.identifier,
                                      self.handleAnnotation(TV_sgProp))]
        if TV_plProp:
            self.TV_plProp.extent = [(self.identifier,
                                      self.handleAnnotation(TV_plProp))]
        if TV_vbg:
            self.TV_vbgProp.extent = [(self.identifier,
                                       self.handleAnnotation(TV_vbg))]

    def __init__(
        self, identifier=None, graph=None, baseType=OWL_NS.ObjectProperty,
        subPropertyOf=None, domain=None, range=None, inverseOf=None,
        otherType=None, equivalentProperty=None,
        comment=None,
        verbAnnotations=None,
        nameAnnotation=None,
            nameIsLabel=False):
        super(Property, self).__init__(identifier, graph,
                                       nameAnnotation, nameIsLabel)
        if verbAnnotations:
            self.setupVerbAnnotations(verbAnnotations)

        assert not isinstance(self.identifier, BNode)
        if baseType is None:
            # None give, determine via introspection
            self._baseType = first(
                Individual(self.identifier, graph=self.graph).type)
        else:
            if (self.identifier, RDF.type, baseType) not in self.graph:
                self.graph.add((self.identifier, RDF.type, baseType))
            self._baseType = baseType
        self.subPropertyOf = subPropertyOf
        self.inverseOf = inverseOf
        self.domain = domain
        self.range = range
        self.comment = comment and comment or []

    def serialize(self, graph):
        for fact in self.graph.triples((self.identifier, None, None)):
            graph.add(fact)
        for p in itertools.chain(self.subPropertyOf,
                                 self.inverseOf):
            p.serialize(graph)
        for c in itertools.chain(self.domain,
                                 self.range):
            CastClass(c, self.graph).serialize(graph)

    def _get_extent(self, graph=None):
        for triple in (graph is None and self.graph or graph).triples(
                (None, self.identifier, None)):
            yield triple

    def _set_extent(self, other):
        if not other:
            return
        for subj, obj in other:
            self.graph.add((subj, self.identifier, obj))

    extent = property(_get_extent, _set_extent)

    def __repr__(self):
        rt = []
        if OWL_NS.ObjectProperty in self.type:
            rt.append('ObjectProperty( %s annotation(%s)'
                      % (self.qname, first(self.comment)
                          and first(self.comment) or ''))
            if first(self.inverseOf):
                twoLinkInverse = first(first(self.inverseOf).inverseOf)
                if twoLinkInverse \
                        and twoLinkInverse.identifier == self.identifier:
                    inverseRepr = first(self.inverseOf).qname
                else:
                    inverseRepr = repr(first(self.inverseOf))
                rt.append("  inverseOf( %s )%s" % (
                    inverseRepr,
                    OWL_NS.SymmetricProperty in self.type
                    and ' Symmetric'
                    or ''))
            for s, p, roleType in self.graph.triples_choices(
                (self.identifier,
                 RDF.type,
                 [OWL_NS.FunctionalProperty,
                  OWL_NS.InverseFunctionalProperty,
                  OWL_NS.TransitiveProperty])):
                rt.append(str(roleType.split(OWL_NS)[-1]))
        else:
            rt.append('DatatypeProperty( %s %s'
                      % (self.qname, first(self.comment)
                         and first(self.comment)
                         or ''))
            for s, p, roleType in self.graph.triples((
                    self.identifier, RDF.type, OWL_NS.FunctionalProperty)):
                rt.append('   Functional')

        def canonicalName(term, g):
            normalizedName = classOrIdentifier(term)
            if isinstance(normalizedName, BNode):
                return term
            elif normalizedName.startswith(_XSD_NS):
                return str(term)
            elif first(g.triples_choices((
                                         normalizedName,
                                         [OWL_NS.unionOf,
                       OWL_NS.intersectionOf], None))):
                return repr(term)
            else:
                return str(term.qname)
        rt.append(' '.join(["   super( %s )" % canonicalName(
            superP, self.graph)
            for superP in self.subPropertyOf]))
        rt.append(' '.join(["   domain( %s )" % canonicalName(
            domain, self.graph)
            for domain in self.domain]))
        rt.append(' '.join(["   range( %s )" % canonicalName(
            range, self.graph)
            for range in self.range]))
        rt = '\n'.join([expr for expr in rt if expr])
        rt += '\n)'
        return str(rt).encode('utf-8')

    def _get_subPropertyOf(self):
        for anc in self.graph.objects(
                subject=self.identifier, predicate=RDFS.subPropertyOf):
            yield Property(anc, graph=self.graph, baseType=None)

    def _set_subPropertyOf(self, other):
        if not other:
            return
        for sP in other:
            self.graph.add(
                (self.identifier, RDFS.subPropertyOf, classOrIdentifier(sP)))

    @TermDeletionHelper(RDFS.subPropertyOf)
    def _del_subPropertyOf(self):
        pass

    subPropertyOf = property(
        _get_subPropertyOf, _set_subPropertyOf, _del_subPropertyOf)

    def _get_inverseOf(self):
        for anc in self.graph.objects(
                subject=self.identifier, predicate=OWL_NS.inverseOf):
            yield Property(anc, graph=self.graph, baseType=None)

    def _set_inverseOf(self, other):
        if not other:
            return
        self.graph.add(
            (self.identifier, OWL_NS.inverseOf, classOrIdentifier(other)))

    @TermDeletionHelper(OWL_NS.inverseOf)
    def _del_inverseOf(self):
        pass

    inverseOf = property(_get_inverseOf, _set_inverseOf, _del_inverseOf)

    def _get_domain(self):
        for dom in self.graph.objects(
                subject=self.identifier, predicate=RDFS.domain):
            yield Class(dom, graph=self.graph)

    def _set_domain(self, other):
        if not other:
            return
        if isinstance(other, (Individual, Identifier)):
            self.graph.add(
                (self.identifier, RDFS.domain, classOrIdentifier(other)))
        else:
            for dom in other:
                self.graph.add(
                    (self.identifier, RDFS.domain, classOrIdentifier(dom)))

    @TermDeletionHelper(RDFS.domain)
    def _del_domain(self):
        pass

    domain = property(_get_domain, _set_domain, _del_domain)

    def _get_range(self):
        for ran in self.graph.objects(
                subject=self.identifier, predicate=RDFS.range):
            yield Class(ran, graph=self.graph)

    def _set_range(self, ranges):
        if not ranges:
            return
        if isinstance(ranges, (Individual, Identifier)):
            self.graph.add(
                (self.identifier, RDFS.range, classOrIdentifier(ranges)))
        else:
            for range in ranges:
                self.graph.add(
                    (self.identifier, RDFS.range, classOrIdentifier(range)))

    @TermDeletionHelper(RDFS.range)
    def _del_range(self):
        pass

    range = property(_get_range, _set_range, _del_range)

    def replace(self, other):
        # extension = []
        for s, p, o in self.extent:
            self.graph.add((s, propertyOrIdentifier(other), o))
        self.graph.remove((None, self.identifier, None))


def CommonNSBindings(graph, additionalNS={}):
    """
    Takes a graph and binds the common namespaces (rdf,rdfs, & owl)
    """
    namespace_manager = NamespaceManager(graph)
    namespace_manager.bind('rdfs', RDFS)
    namespace_manager.bind('rdf', RDF)
    namespace_manager.bind('owl', OWL_NS)
    for prefix, uri in list(additionalNS.items()):
        namespace_manager.bind(prefix, uri, override=False)
    graph.namespace_manager = namespace_manager


def test():
    import doctest
    doctest.testmod()

if __name__ == '__main__':
    test()

########NEW FILE########
__FILENAME__ = graph
from rdflib.term import Literal  # required for doctests
assert Literal # avoid warning
from rdflib.namespace import Namespace  # required for doctests
assert Namespace # avoid warning
from rdflib.py3compat import format_doctest_out

__doc__ = format_doctest_out("""\

RDFLib defines the following kinds of Graphs:

* :class:`~rdflib.graph.Graph`
* :class:`~rdflib.graph.QuotedGraph`
* :class:`~rdflib.graph.ConjunctiveGraph`
* :class:`~rdflib.graph.Dataset`

Graph
-----

An RDF graph is a set of RDF triples. Graphs support the python ``in``
operator, as well as iteration and some operations like union,
difference and intersection.

see :class:`~rdflib.graph.Graph`

Conjunctive Graph
-----------------

A Conjunctive Graph is the most relevant collection of graphs that are
considered to be the boundary for closed world assumptions.  This
boundary is equivalent to that of the store instance (which is itself
uniquely identified and distinct from other instances of
:class:`Store` that signify other Conjunctive Graphs).  It is
equivalent to all the named graphs within it and associated with a
``_default_`` graph which is automatically assigned a :class:`BNode`
for an identifier - if one isn't given.

see :class:`~rdflib.graph.ConjunctiveGraph`

Quoted graph
------------

The notion of an RDF graph [14] is extended to include the concept of
a formula node. A formula node may occur wherever any other kind of
node can appear. Associated with a formula node is an RDF graph that
is completely disjoint from all other graphs; i.e. has no nodes in
common with any other graph. (It may contain the same labels as other
RDF graphs; because this is, by definition, a separate graph,
considerations of tidiness do not apply between the graph at a formula
node and any other graph.)

This is intended to map the idea of "{ N3-expression }" that is used
by N3 into an RDF graph upon which RDF semantics is defined.

see :class:`~rdflib.graph.QuotedGraph`

Dataset
-------

The RDF 1.1 Dataset, a small extension to the Conjunctive Graph. The
primary term is "graphs in the datasets" and not "contexts with quads"
so there is a separate method to set/retrieve a graph in a dataset and
to operate with dataset graphs. As a consequence of this approach,
dataset graphs cannot be identified with blank nodes, a name is always
required (RDFLib will automatically add a name if one is not provided
at creation time). This implementation includes a convenience method
to directly add a single quad to a dataset graph.

see :class:`~rdflib.graph.Dataset`

Working with graphs
===================

Instantiating Graphs with default store (IOMemory) and default identifier
(a BNode):

    >>> g = Graph()
    >>> g.store.__class__
    <class 'rdflib.plugins.memory.IOMemory'>
    >>> g.identifier.__class__
    <class 'rdflib.term.BNode'>

Instantiating Graphs with a IOMemory store and an identifier -
<http://rdflib.net>:

    >>> g = Graph('IOMemory', URIRef("http://rdflib.net"))
    >>> g.identifier
    rdflib.term.URIRef(%(u)s'http://rdflib.net')
    >>> str(g) # doctest: +NORMALIZE_WHITESPACE
    "<http://rdflib.net> a rdfg:Graph;rdflib:storage
     [a rdflib:Store;rdfs:label 'IOMemory']."

Creating a ConjunctiveGraph - The top level container for all named Graphs
in a 'database':

    >>> g = ConjunctiveGraph()
    >>> str(g.default_context)
    "[a rdfg:Graph;rdflib:storage [a rdflib:Store;rdfs:label 'IOMemory']]."

Adding / removing reified triples to Graph and iterating over it directly or
via triple pattern:

    >>> g = Graph()
    >>> statementId = BNode()
    >>> print(len(g))
    0
    >>> g.add((statementId, RDF.type, RDF.Statement))
    >>> g.add((statementId, RDF.subject,
    ...     URIRef(%(u)s'http://rdflib.net/store/ConjunctiveGraph')))
    >>> g.add((statementId, RDF.predicate, RDFS.label))
    >>> g.add((statementId, RDF.object, Literal("Conjunctive Graph")))
    >>> print(len(g))
    4
    >>> for s, p, o in g:
    ...     print(type(s))
    ...
    <class 'rdflib.term.BNode'>
    <class 'rdflib.term.BNode'>
    <class 'rdflib.term.BNode'>
    <class 'rdflib.term.BNode'>

    >>> for s, p, o in g.triples((None, RDF.object, None)):
    ...     print(o)
    ...
    Conjunctive Graph
    >>> g.remove((statementId, RDF.type, RDF.Statement))
    >>> print(len(g))
    3

``None`` terms in calls to :meth:`~rdflib.graph.Graph.triples` can be
thought of as "open variables".

Graph support set-theoretic operators, you can add/subtract graphs, as
well as intersection (with multiplication operator g1*g2) and xor (g1
^ g2).

Note that BNode IDs are kept when doing set-theoretic operations, this
may or may not be what you want. Two named graphs within the same
application probably want share BNode IDs, two graphs with data from
different sources probably not.  If your BNode IDs are all generated
by RDFLib they are UUIDs and unique.

    >>> g1 = Graph()
    >>> g2 = Graph()
    >>> u = URIRef(%(u)s'http://example.com/foo')
    >>> g1.add([u, RDFS.label, Literal('foo')])
    >>> g1.add([u, RDFS.label, Literal('bar')])
    >>> g2.add([u, RDFS.label, Literal('foo')])
    >>> g2.add([u, RDFS.label, Literal('bing')])
    >>> len(g1 + g2)  # adds bing as label
    3
    >>> len(g1 - g2)  # removes foo
    1
    >>> len(g1 * g2)  # only foo
    1
    >>> g1 += g2  # now g1 contains everything


Graph Aggregation - ConjunctiveGraphs and ReadOnlyGraphAggregate within
the same store:

    >>> store = plugin.get('IOMemory', Store)()
    >>> g1 = Graph(store)
    >>> g2 = Graph(store)
    >>> g3 = Graph(store)
    >>> stmt1 = BNode()
    >>> stmt2 = BNode()
    >>> stmt3 = BNode()
    >>> g1.add((stmt1, RDF.type, RDF.Statement))
    >>> g1.add((stmt1, RDF.subject,
    ...     URIRef(%(u)s'http://rdflib.net/store/ConjunctiveGraph')))
    >>> g1.add((stmt1, RDF.predicate, RDFS.label))
    >>> g1.add((stmt1, RDF.object, Literal("Conjunctive Graph")))
    >>> g2.add((stmt2, RDF.type, RDF.Statement))
    >>> g2.add((stmt2, RDF.subject,
    ...     URIRef(%(u)s'http://rdflib.net/store/ConjunctiveGraph')))
    >>> g2.add((stmt2, RDF.predicate, RDF.type))
    >>> g2.add((stmt2, RDF.object, RDFS.Class))
    >>> g3.add((stmt3, RDF.type, RDF.Statement))
    >>> g3.add((stmt3, RDF.subject,
    ...     URIRef(%(u)s'http://rdflib.net/store/ConjunctiveGraph')))
    >>> g3.add((stmt3, RDF.predicate, RDFS.comment))
    >>> g3.add((stmt3, RDF.object, Literal(
    ...     "The top-level aggregate graph - The sum " +
    ...     "of all named graphs within a Store")))
    >>> len(list(ConjunctiveGraph(store).subjects(RDF.type, RDF.Statement)))
    3
    >>> len(list(ReadOnlyGraphAggregate([g1,g2]).subjects(
    ...     RDF.type, RDF.Statement)))
    2

ConjunctiveGraphs have a :meth:`~rdflib.graph.ConjunctiveGraph.quads` method
which returns quads instead of triples, where the fourth item is the Graph
(or subclass thereof) instance in which the triple was asserted:

    >>> uniqueGraphNames = set(
    ...     [graph.identifier for s, p, o, graph in ConjunctiveGraph(store
    ...     ).quads((None, RDF.predicate, None))])
    >>> len(uniqueGraphNames)
    3
    >>> unionGraph = ReadOnlyGraphAggregate([g1, g2])
    >>> uniqueGraphNames = set(
    ...     [graph.identifier for s, p, o, graph in unionGraph.quads(
    ...     (None, RDF.predicate, None))])
    >>> len(uniqueGraphNames)
    2

Parsing N3 from a string

    >>> g2 = Graph()
    >>> src = '''
    ... @prefix rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
    ... @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
    ... [ a rdf:Statement ;
    ...   rdf:subject <http://rdflib.net/store#ConjunctiveGraph>;
    ...   rdf:predicate rdfs:label;
    ...   rdf:object "Conjunctive Graph" ] .
    ... '''
    >>> g2 = g2.parse(data=src, format='n3')
    >>> print(len(g2))
    4

Using Namespace class:

    >>> RDFLib = Namespace('http://rdflib.net/')
    >>> RDFLib.ConjunctiveGraph
    rdflib.term.URIRef(%(u)s'http://rdflib.net/ConjunctiveGraph')
    >>> RDFLib['Graph']
    rdflib.term.URIRef(%(u)s'http://rdflib.net/Graph')

""")

import logging
_logger = logging.getLogger(__name__)

# import md5
import random
import warnings

from hashlib import md5

try:
    from io import BytesIO
    assert BytesIO
except ImportError:
    try:
        from cStringIO import StringIO as BytesIO
        assert BytesIO
    except ImportError:
        from StringIO import StringIO as BytesIO
        assert BytesIO

from rdflib.namespace import RDF, RDFS, SKOS

from rdflib import plugin, exceptions, query

from rdflib.term import Node, URIRef, Genid
from rdflib.term import BNode

import rdflib.term

from rdflib.paths import Path

from rdflib.store import Store
from rdflib.serializer import Serializer
from rdflib.parser import Parser
from rdflib.parser import create_input_source
from rdflib.namespace import NamespaceManager
from rdflib.resource import Resource
from rdflib import py3compat
b = py3compat.b

import os
import shutil
import tempfile
from urlparse import urlparse

__all__ = [
    'Graph', 'ConjunctiveGraph', 'QuotedGraph', 'Seq',
    'ModificationException', 'Dataset',
    'UnSupportedAggregateOperation', 'ReadOnlyGraphAggregate']


class Graph(Node):
    """An RDF Graph

    The constructor accepts one argument, the 'store'
    that will be used to store the graph data (see the 'store'
    package for stores currently shipped with rdflib).

    Stores can be context-aware or unaware.  Unaware stores take up
    (some) less space but cannot support features that require
    context, such as true merging/demerging of sub-graphs and
    provenance.

    The Graph constructor can take an identifier which identifies the Graph
    by name.  If none is given, the graph is assigned a BNode for its
    identifier.
    For more on named graphs, see: http://www.w3.org/2004/03/trix/

    """

    def __init__(self, store='default', identifier=None,
                 namespace_manager=None):
        super(Graph, self).__init__()
        self.__identifier = identifier or BNode()

        if not isinstance(self.__identifier, Node):
            self.__identifier = URIRef(self.__identifier)

        if not isinstance(store, Store):
            # TODO: error handling
            self.__store = store = plugin.get(store, Store)()
        else:
            self.__store = store
        self.__namespace_manager = namespace_manager
        self.context_aware = False
        self.formula_aware = False
        self.default_union = False

    def __get_store(self):
        return self.__store
    store = property(__get_store)  # read-only attr

    def __get_identifier(self):
        return self.__identifier
    identifier = property(__get_identifier)  # read-only attr

    def _get_namespace_manager(self):
        if self.__namespace_manager is None:
            self.__namespace_manager = NamespaceManager(self)
        return self.__namespace_manager

    def _set_namespace_manager(self, nm):
        self.__namespace_manager = nm

    namespace_manager = property(_get_namespace_manager,
                                 _set_namespace_manager,
                                 doc="this graph's namespace-manager")

    def __repr__(self):
        return "<Graph identifier=%s (%s)>" % (self.identifier, type(self))

    def __str__(self):
        if isinstance(self.identifier, URIRef):
            return ("%s a rdfg:Graph;rdflib:storage " +
                    "[a rdflib:Store;rdfs:label '%s'].") % (
                        self.identifier.n3(),
                        self.store.__class__.__name__)
        else:
            return ("[a rdfg:Graph;rdflib:storage " +
                    "[a rdflib:Store;rdfs:label '%s']].") % (
                        self.store.__class__.__name__)

    def toPython(self):
        return self

    def destroy(self, configuration):
        """Destroy the store identified by `configuration` if supported"""
        self.__store.destroy(configuration)

    # Transactional interfaces (optional)
    def commit(self):
        """Commits active transactions"""
        self.__store.commit()

    def rollback(self):
        """Rollback active transactions"""
        self.__store.rollback()

    def open(self, configuration, create=False):
        """Open the graph store

        Might be necessary for stores that require opening a connection to a
        database or acquiring some resource.
        """
        return self.__store.open(configuration, create)

    def close(self, commit_pending_transaction=False):
        """Close the graph store

        Might be necessary for stores that require closing a connection to a
        database or releasing some resource.
        """
        self.__store.close(
            commit_pending_transaction=commit_pending_transaction)

    def add(self, (s, p, o)):
        """Add a triple with self as context"""
        assert isinstance(s, Node), \
            "Subject %s must be an rdflib term" % (s,)
        assert isinstance(p, Node), \
            "Predicate %s must be an rdflib term" % (p,)
        assert isinstance(o, Node), \
            "Object %s must be an rdflib term" % (o,)
        self.__store.add((s, p, o), self, quoted=False)

    def addN(self, quads):
        """Add a sequence of triple with context"""

        self.__store.addN((s, p, o, c) for s, p, o, c in quads
                          if isinstance(c, Graph)
                          and c.identifier is self.identifier
                          and _assertnode(s,p,o)
                          )

    def remove(self, (s, p, o)):
        """Remove a triple from the graph

        If the triple does not provide a context attribute, removes the triple
        from all contexts.
        """
        self.__store.remove((s, p, o), context=self)

    def triples(self, (s, p, o)):
        """Generator over the triple store

        Returns triples that match the given triple pattern. If triple pattern
        does not provide a context, all contexts will be searched.
        """
        if isinstance(p, Path):
            for _s, _o in p.eval(self, s, o):
                yield (_s, p, _o)
        else:
            for (s, p, o), cg in self.__store.triples((s, p, o), context=self):
                yield (s, p, o)

    @py3compat.format_doctest_out
    def __getitem__(self, item):
        """
        A graph can be "sliced" as a shortcut for the triples method
        The python slice syntax is (ab)used for specifying triples.
        A generator over matches is returned,
        the returned tuples include only the parts not given

        >>> import rdflib
        >>> g = rdflib.Graph()
        >>> g.add((rdflib.URIRef('urn:bob'), rdflib.RDFS.label, rdflib.Literal('Bob')))

        >>> list(g[rdflib.URIRef('urn:bob')]) # all triples about bob
        [(rdflib.term.URIRef(%(u)s'http://www.w3.org/2000/01/rdf-schema#label'), rdflib.term.Literal(%(u)s'Bob'))]

        >>> list(g[:rdflib.RDFS.label]) # all label triples
        [(rdflib.term.URIRef(%(u)s'urn:bob'), rdflib.term.Literal(%(u)s'Bob'))]

        >>> list(g[::rdflib.Literal('Bob')]) # all triples with bob as object
        [(rdflib.term.URIRef(%(u)s'urn:bob'), rdflib.term.URIRef(%(u)s'http://www.w3.org/2000/01/rdf-schema#label'))]

        Combined with SPARQL paths, more complex queries can be
        written concisely:

        Name of all Bobs friends:

        g[bob : FOAF.knows/FOAF.name ]

        Some label for Bob:

        g[bob : DC.title|FOAF.name|RDFS.label]

        All friends and friends of friends of Bob

        g[bob : FOAF.knows * '+']

        etc.

        .. versionadded:: 4.0

        """

        if isinstance(item, slice):

            s,p,o=item.start,item.stop,item.step
            if s is None and p is None and o is None:
                return self.triples((s,p,o))
            elif s is None and p is None:
                return self.subject_predicates(o)
            elif s is None and o is None:
                return self.subject_objects(p)
            elif p is None and o is None:
                return self.predicate_objects(s)
            elif s is None:
                return self.subjects(p,o)
            elif p is None:
                return self.predicates(s,o)
            elif o is None:
                return self.objects(s,p)
            else:
                # all given
                return (s,p,o) in self

        elif isinstance(item, (Path,Node)):

            return self.predicate_objects(item)

        else:
            raise TypeError("You can only index a graph by a single rdflib term or path, or a slice of rdflib terms.")

    def __len__(self):
        """Returns the number of triples in the graph

        If context is specified then the number of triples in the context is
        returned instead.
        """
        return self.__store.__len__(context=self)

    def __iter__(self):
        """Iterates over all triples in the store"""
        return self.triples((None, None, None))

    def __contains__(self, triple):
        """Support for 'triple in graph' syntax"""
        for triple in self.triples(triple):
            return True
        return False

    def __hash__(self):
        return hash(self.identifier)

    def md5_term_hash(self):
        d = md5(str(self.identifier))
        d.update("G")
        return d.hexdigest()

    def __cmp__(self, other):
        if other is None:
            return -1
        elif isinstance(other, Graph):
            return cmp(self.identifier, other.identifier)
        else:
            # Note if None is considered equivalent to owl:Nothing
            # Then perhaps a graph with length 0 should be considered
            # equivalent to None (if compared to it)?
            return 1

    def __eq__(self, other):
        return isinstance(other, Graph) \
            and self.identifier == other.identifier

    def __lt__(self, other):
        return (other is None) \
            or (isinstance(other, Graph)
                and self.identifier < other.identifier)

    def __le__(self, other):
        return self < other or self == other

    def __gt__(self, other):
        return (isinstance(other, Graph)
                and self.identifier > other.identifier) \
            or (other is not None)

    def __ge__(self, other):
        return self > other or self == other

    def __iadd__(self, other):
        """Add all triples in Graph other to Graph.
           BNode IDs are not changed."""
        self.addN((s, p, o, self) for s, p, o in other)
        return self

    def __isub__(self, other):
        """Subtract all triples in Graph other from Graph.
           BNode IDs are not changed."""
        for triple in other:
            self.remove(triple)
        return self

    def __add__(self, other):
        """Set-theoretic union
           BNode IDs are not changed."""
        retval = Graph()
        for (prefix, uri) in set(
                list(self.namespaces()) + list(other.namespaces())):
            retval.bind(prefix, uri)
        for x in self:
            retval.add(x)
        for y in other:
            retval.add(y)
        return retval

    def __mul__(self, other):
        """Set-theoretic intersection.
           BNode IDs are not changed."""
        retval = Graph()
        for x in other:
            if x in self:
                retval.add(x)
        return retval

    def __sub__(self, other):
        """Set-theoretic difference.
           BNode IDs are not changed."""
        retval = Graph()
        for x in self:
            if not x in other:
                retval.add(x)
        return retval

    def __xor__(self, other):
        """Set-theoretic XOR.
           BNode IDs are not changed."""
        return (self - other) + (other - self)

    __or__ = __add__
    __and__ = __mul__

    # Conv. methods

    def set(self, triple):
        """Convenience method to update the value of object

        Remove any existing triples for subject and predicate before adding
        (subject, predicate, object).
        """
        (subject, predicate, object) = triple
        self.remove((subject, predicate, None))
        self.add((subject, predicate, object))

    def subjects(self, predicate=None, object=None):
        """A generator of subjects with the given predicate and object"""
        for s, p, o in self.triples((None, predicate, object)):
            yield s

    def predicates(self, subject=None, object=None):
        """A generator of predicates with the given subject and object"""
        for s, p, o in self.triples((subject, None, object)):
            yield p

    def objects(self, subject=None, predicate=None):
        """A generator of objects with the given subject and predicate"""
        for s, p, o in self.triples((subject, predicate, None)):
            yield o

    def subject_predicates(self, object=None):
        """A generator of (subject, predicate) tuples for the given object"""
        for s, p, o in self.triples((None, None, object)):
            yield s, p

    def subject_objects(self, predicate=None):
        """A generator of (subject, object) tuples for the given predicate"""
        for s, p, o in self.triples((None, predicate, None)):
            yield s, o

    def predicate_objects(self, subject=None):
        """A generator of (predicate, object) tuples for the given subject"""
        for s, p, o in self.triples((subject, None, None)):
            yield p, o

    def triples_choices(self, (subject, predicate, object_), context=None):
        for (s, p, o), cg in self.store.triples_choices(
                (subject, predicate, object_), context=self):
            yield (s, p, o)

    def value(self, subject=None, predicate=RDF.value, object=None,
              default=None, any=True):
        """Get a value for a pair of two criteria

        Exactly one of subject, predicate, object must be None. Useful if one
        knows that there may only be one value.

        It is one of those situations that occur a lot, hence this
        'macro' like utility

        Parameters:
        subject, predicate, object  -- exactly one must be None
        default -- value to be returned if no values found
        any -- if True, return any value in the case there is more than one,
        else, raise UniquenessError
        """
        retval = default

        if (subject is None and predicate is None) or \
                (subject is None and object is None) or \
                (predicate is None and object is None):
            return None

        if object is None:
            values = self.objects(subject, predicate)
        if subject is None:
            values = self.subjects(predicate, object)
        if predicate is None:
            values = self.predicates(subject, object)

        try:
            retval = values.next()
        except StopIteration:
            retval = default
        else:
            if any is False:
                try:
                    values.next()
                    msg = ("While trying to find a value for (%s, %s, %s) the"
                           " following multiple values where found:\n" %
                           (subject, predicate, object))
                    triples = self.store.triples(
                        (subject, predicate, object), None)
                    for (s, p, o), contexts in triples:
                        msg += "(%s, %s, %s)\n (contexts: %s)\n" % (
                            s, p, o, list(contexts))
                    raise exceptions.UniquenessError(msg)
                except StopIteration:
                    pass
        return retval

    def label(self, subject, default=''):
        """Query for the RDFS.label of the subject

        Return default if no label exists or any label if multiple exist.
        """
        if subject is None:
            return default
        return self.value(subject, RDFS.label, default=default, any=True)

    @py3compat.format_doctest_out
    def preferredLabel(self, subject, lang=None, default=None,
                       labelProperties=(SKOS.prefLabel, RDFS.label)):
        """
        Find the preferred label for subject.

        By default prefers skos:prefLabels over rdfs:labels. In case at least
        one prefLabel is found returns those, else returns labels. In case a
        language string (e.g., 'en', 'de' or even '' for no lang-tagged
        literals) is given, only such labels will be considered.

        Return a list of (labelProp, label) pairs, where labelProp is either
        skos:prefLabel or rdfs:label.

        >>> from rdflib import ConjunctiveGraph, URIRef, RDFS, Literal
        >>> from rdflib.namespace import SKOS
        >>> from pprint import pprint
        >>> g = ConjunctiveGraph()
        >>> u = URIRef(%(u)s'http://example.com/foo')
        >>> g.add([u, RDFS.label, Literal('foo')])
        >>> g.add([u, RDFS.label, Literal('bar')])
        >>> pprint(sorted(g.preferredLabel(u)))
        [(rdflib.term.URIRef(%(u)s'http://www.w3.org/2000/01/rdf-schema#label'),
          rdflib.term.Literal(%(u)s'bar')),
         (rdflib.term.URIRef(%(u)s'http://www.w3.org/2000/01/rdf-schema#label'),
          rdflib.term.Literal(%(u)s'foo'))]
        >>> g.add([u, SKOS.prefLabel, Literal('bla')])
        >>> pprint(g.preferredLabel(u))
        [(rdflib.term.URIRef(%(u)s'http://www.w3.org/2004/02/skos/core#prefLabel'),
          rdflib.term.Literal(%(u)s'bla'))]
        >>> g.add([u, SKOS.prefLabel, Literal('blubb', lang='en')])
        >>> sorted(g.preferredLabel(u)) #doctest: +NORMALIZE_WHITESPACE
        [(rdflib.term.URIRef(%(u)s'http://www.w3.org/2004/02/skos/core#prefLabel'),
          rdflib.term.Literal(%(u)s'bla')),
          (rdflib.term.URIRef(%(u)s'http://www.w3.org/2004/02/skos/core#prefLabel'),
          rdflib.term.Literal(%(u)s'blubb', lang='en'))]
        >>> g.preferredLabel(u, lang='') #doctest: +NORMALIZE_WHITESPACE
        [(rdflib.term.URIRef(%(u)s'http://www.w3.org/2004/02/skos/core#prefLabel'),
          rdflib.term.Literal(%(u)s'bla'))]
        >>> pprint(g.preferredLabel(u, lang='en'))
        [(rdflib.term.URIRef(%(u)s'http://www.w3.org/2004/02/skos/core#prefLabel'),
          rdflib.term.Literal(%(u)s'blubb', lang='en'))]
        """

        if default is None:
            default = []

        # setup the language filtering
        if lang is not None:
            if lang == '':  # we only want not language-tagged literals
                langfilter = lambda l: l.language is None
            else:
                langfilter = lambda l: l.language == lang
        else:  # we don't care about language tags
            langfilter = lambda l: True

        for labelProp in labelProperties:
            labels = filter(langfilter, self.objects(subject, labelProp))
            if len(labels) == 0:
                continue
            else:
                return [(labelProp, l) for l in labels]
        return default

    def comment(self, subject, default=''):
        """Query for the RDFS.comment of the subject

        Return default if no comment exists
        """
        if subject is None:
            return default
        return self.value(subject, RDFS.comment, default=default, any=True)

    def items(self, list):
        """Generator over all items in the resource specified by list

        list is an RDF collection.
        """
        chain = set([list])
        while list:
            item = self.value(list, RDF.first)
            if item is not None:
                yield item
            list = self.value(list, RDF.rest)
            if list in chain:
                raise ValueError("List contains a recursive rdf:rest reference")
            chain.add(list)

    def transitiveClosure(self, func, arg, seen=None):
        """
        Generates transitive closure of a user-defined
        function against the graph

        >>> from rdflib.collection import Collection
        >>> g=Graph()
        >>> a=BNode('foo')
        >>> b=BNode('bar')
        >>> c=BNode('baz')
        >>> g.add((a,RDF.first,RDF.type))
        >>> g.add((a,RDF.rest,b))
        >>> g.add((b,RDF.first,RDFS.label))
        >>> g.add((b,RDF.rest,c))
        >>> g.add((c,RDF.first,RDFS.comment))
        >>> g.add((c,RDF.rest,RDF.nil))
        >>> def topList(node,g):
        ...    for s in g.subjects(RDF.rest,node):
        ...       yield s
        >>> def reverseList(node,g):
        ...    for f in g.objects(node,RDF.first):
        ...       print(f)
        ...    for s in g.subjects(RDF.rest,node):
        ...       yield s

        >>> [rt for rt in g.transitiveClosure(
        ...     topList,RDF.nil)] # doctest: +NORMALIZE_WHITESPACE
        [rdflib.term.BNode('baz'),
         rdflib.term.BNode('bar'),
         rdflib.term.BNode('foo')]

        >>> [rt for rt in g.transitiveClosure(
        ...     reverseList,RDF.nil)] # doctest: +NORMALIZE_WHITESPACE
        http://www.w3.org/2000/01/rdf-schema#comment
        http://www.w3.org/2000/01/rdf-schema#label
        http://www.w3.org/1999/02/22-rdf-syntax-ns#type
        [rdflib.term.BNode('baz'),
         rdflib.term.BNode('bar'),
         rdflib.term.BNode('foo')]

        """
        if seen is None:
            seen = {}
        elif arg in seen:
            return
        seen[arg] = 1
        for rt in func(arg, self):
            yield rt
            for rt_2 in self.transitiveClosure(func, rt, seen):
                yield rt_2

    def transitive_objects(self, subject, property, remember=None):
        """Transitively generate objects for the ``property`` relationship

        Generated objects belong to the depth first transitive closure of the
        ``property`` relationship starting at ``subject``.
        """
        if remember is None:
            remember = {}
        if subject in remember:
            return
        remember[subject] = 1
        yield subject
        for object in self.objects(subject, property):
            for o in self.transitive_objects(object, property, remember):
                yield o

    def transitive_subjects(self, predicate, object, remember=None):
        """Transitively generate objects for the ``property`` relationship

        Generated objects belong to the depth first transitive closure of the
        ``property`` relationship starting at ``subject``.
        """
        if remember is None:
            remember = {}
        if object in remember:
            return
        remember[object] = 1
        yield object
        for subject in self.subjects(predicate, object):
            for s in self.transitive_subjects(predicate, subject, remember):
                yield s

    def seq(self, subject):
        """Check if subject is an rdf:Seq

        If yes, it returns a Seq class instance, None otherwise.
        """
        if (subject, RDF.type, RDF.Seq) in self:
            return Seq(self, subject)
        else:
            return None

    def qname(self, uri):
        return self.namespace_manager.qname(uri)

    def compute_qname(self, uri, generate=True):
        return self.namespace_manager.compute_qname(uri, generate)

    def bind(self, prefix, namespace, override=True):
        """Bind prefix to namespace

        If override is True will bind namespace to given prefix even
        if namespace was already bound to a different prefix.

        for example:  graph.bind('foaf', 'http://xmlns.com/foaf/0.1/')

        """
        return self.namespace_manager.bind(
            prefix, namespace, override=override)

    def namespaces(self):
        """Generator over all the prefix, namespace tuples"""
        for prefix, namespace in self.namespace_manager.namespaces():
            yield prefix, namespace

    def absolutize(self, uri, defrag=1):
        """Turn uri into an absolute URI if it's not one already"""
        return self.namespace_manager.absolutize(uri, defrag)

    def serialize(self, destination=None, format="xml",
                  base=None, encoding=None, **args):
        """Serialize the Graph to destination

        If destination is None serialize method returns the serialization as a
        string. Format defaults to xml (AKA rdf/xml).

        Format support can be extended with plugins,
        but 'xml', 'n3', 'turtle', 'nt', 'pretty-xml', trix' are built in.
        """
        serializer = plugin.get(format, Serializer)(self)
        if destination is None:
            stream = BytesIO()
            serializer.serialize(stream, base=base, encoding=encoding, **args)
            return stream.getvalue()
        if hasattr(destination, "write"):
            stream = destination
            serializer.serialize(stream, base=base, encoding=encoding, **args)
        else:
            location = destination
            scheme, netloc, path, params, _query, fragment = urlparse(location)
            if netloc != "":
                print("WARNING: not saving as location" +
                      "is not a local file reference")
                return
            fd, name = tempfile.mkstemp()
            stream = os.fdopen(fd, "wb")
            serializer.serialize(stream, base=base, encoding=encoding, **args)
            stream.close()
            if hasattr(shutil, "move"):
                shutil.move(name, path)
            else:
                shutil.copy(name, path)
                os.remove(name)

    def parse(self, source=None, publicID=None, format=None,
              location=None, file=None, data=None, **args):
        """
        Parse source adding the resulting triples to the Graph.

        The source is specified using one of source, location, file or
        data.

        :Parameters:

          - `source`: An InputSource, file-like object, or string. In the case
            of a string the string is the location of the source.
          - `location`: A string indicating the relative or absolute URL of the
            source. Graph's absolutize method is used if a relative location
            is specified.
          - `file`: A file-like object.
          - `data`: A string containing the data to be parsed.
          - `format`: Used if format can not be determined from source.
            Defaults to rdf/xml. Format support can be extended with plugins,
            but 'xml', 'n3', 'nt', 'trix', 'rdfa' are built in.
          - `publicID`: the logical URI to use as the document base. If None
            specified the document location is used (at least in the case where
            there is a document location).

        :Returns:

          - self, the graph instance.

        Examples:

        >>> my_data = '''
        ... <rdf:RDF
        ...   xmlns:rdf='http://www.w3.org/1999/02/22-rdf-syntax-ns#'
        ...   xmlns:rdfs='http://www.w3.org/2000/01/rdf-schema#'
        ... >
        ...   <rdf:Description>
        ...     <rdfs:label>Example</rdfs:label>
        ...     <rdfs:comment>This is really just an example.</rdfs:comment>
        ...   </rdf:Description>
        ... </rdf:RDF>
        ... '''
        >>> import tempfile
        >>> fd, file_name = tempfile.mkstemp()
        >>> f = os.fdopen(fd, 'w')
        >>> dummy = f.write(my_data)  # Returns num bytes written on py3
        >>> f.close()

        >>> g = Graph()
        >>> result = g.parse(data=my_data, format="application/rdf+xml")
        >>> len(g)
        2

        >>> g = Graph()
        >>> result = g.parse(location=file_name, format="application/rdf+xml")
        >>> len(g)
        2

        >>> g = Graph()
        >>> result = g.parse(file=open(file_name, "r"),
        ...     format="application/rdf+xml")
        >>> len(g)
        2

        >>> os.remove(file_name)

        """

        source = create_input_source(source=source, publicID=publicID,
                                     location=location, file=file,
                                     data=data, format=format)
        if format is None:
            format = source.content_type
        if format is None:
            # raise Exception("Could not determine format for %r. You can" + \
            # "expicitly specify one with the format argument." % source)
            format = "application/rdf+xml"
        parser = plugin.get(format, Parser)()
        parser.parse(source, self, **args)
        return self

    def load(self, source, publicID=None, format="xml"):
        self.parse(source, publicID, format)

    def query(self, query_object, processor='sparql',
              result='sparql', initNs=None, initBindings=None,
              use_store_provided=True, **kwargs):
        """
        Query this graph.

        A type of 'prepared queries' can be realised by providing
        initial variable bindings with initBindings

        Initial namespaces are used to resolve prefixes used in the query,
        if none are given, the namespaces from the graph's namespace manager
        are used.

        :returntype: rdflib.query.QueryResult

        """

        initBindings = initBindings or {}
        initNs = initNs or dict(self.namespaces())

        if hasattr(self.store, "query") and use_store_provided:
            try:
                return self.store.query(
                    query_object, initNs, initBindings,
                    self.default_union
                    and '__UNION__'
                    or self.identifier,
                    **kwargs)
            except NotImplementedError:
                pass  # store has no own implementation

        if not isinstance(result, query.Result):
            result = plugin.get(result, query.Result)
        if not isinstance(processor, query.Processor):
            processor = plugin.get(processor, query.Processor)(self)

        return result(processor.query(
            query_object, initBindings, initNs, **kwargs))

    def update(self, update_object, processor='sparql',
              initNs={}, initBindings={},
              use_store_provided=True, **kwargs):
        """
        """
        if hasattr(self.store, "update") and use_store_provided:
            try:
                return self.store.update(
                    update_object, initNs, initBindings,
                    self.default_union
                    and '__UNION__'
                    or self.identifier,
                    **kwargs)
            except NotImplementedError:
                pass  # store has no own implementation

        if not isinstance(processor, query.UpdateProcessor):
            processor = plugin.get(processor, query.UpdateProcessor)(self)

        return processor.update(update_object, initBindings, initNs, **kwargs)


    def n3(self):
        """return an n3 identifier for the Graph"""
        return "[%s]" % self.identifier.n3()

    def __reduce__(self):
        return (Graph, (self.store, self.identifier,))

    def isomorphic(self, other):
        """
        does a very basic check if these graphs are the same
        If no BNodes are involved, this is accurate.

        See rdflib.compare for a correct implementation of isomorphism checks
        """
        # TODO: this is only an approximation.
        if len(self) != len(other):
            return False
        for s, p, o in self:
            if not isinstance(s, BNode) and not isinstance(o, BNode):
                if not (s, p, o) in other:
                    return False
        for s, p, o in other:
            if not isinstance(s, BNode) and not isinstance(o, BNode):
                if not (s, p, o) in self:
                    return False
        # TODO: very well could be a false positive at this point yet.
        return True

    def connected(self):
        """Check if the Graph is connected

        The Graph is considered undirectional.

        Performs a search on the Graph, starting from a random node. Then
        iteratively goes depth-first through the triplets where the node is
        subject and object. Return True if all nodes have been visited and
        False if it cannot continue and there are still unvisited nodes left.
        """
        all_nodes = list(self.all_nodes())
        discovered = []

        # take a random one, could also always take the first one, doesn't
        # really matter.
        if not all_nodes:
            return False

        visiting = [all_nodes[random.randrange(len(all_nodes))]]
        while visiting:
            x = visiting.pop()
            if x not in discovered:
                discovered.append(x)
            for new_x in self.objects(subject=x):
                if new_x not in discovered and new_x not in visiting:
                    visiting.append(new_x)
            for new_x in self.subjects(object=x):
                if new_x not in discovered and new_x not in visiting:
                    visiting.append(new_x)

        # optimisation by only considering length, since no new objects can
        # be introduced anywhere.
        if len(all_nodes) == len(discovered):
            return True
        else:
            return False

    def all_nodes(self):
        obj = set(self.objects())
        allNodes = obj.union(set(self.subjects()))
        return allNodes

    def resource(self, identifier):
        """Create a new ``Resource`` instance.

        Parameters:

        - ``identifier``: a URIRef or BNode instance.

        Example::

            >>> graph = Graph()
            >>> uri = URIRef("http://example.org/resource")
            >>> resource = graph.resource(uri)
            >>> assert isinstance(resource, Resource)
            >>> assert resource.identifier is uri
            >>> assert resource.graph is graph

        """
        if not isinstance(identifier, Node):
            identifier = URIRef(identifier)
        return Resource(self, identifier)

    def _process_skolem_tuples(self, target, func):
        for t in self.triples((None, None, None)):
            target.add(func(t))

    def skolemize(self, new_graph=None, bnode=None):
        def do_skolemize(bnode, t):
            (s, p, o) = t
            if s == bnode:
                s = s.skolemize()
            if o == bnode:
                o = o.skolemize()
            return (s, p, o)

        def do_skolemize2(t):
            (s, p, o) = t
            if isinstance(s, BNode):
                s = s.skolemize()
            if isinstance(o, BNode):
                o = o.skolemize()
            return (s, p, o)

        retval = Graph() if new_graph is None else new_graph

        if bnode is None:
            self._process_skolem_tuples(retval, do_skolemize2)
        elif isinstance(bnode, BNode):
            self._process_skolem_tuples(
                retval, lambda t: do_skolemize(bnode, t))

        return retval

    def de_skolemize(self, new_graph=None, uriref=None):
        def do_de_skolemize(uriref, t):
            (s, p, o) = t
            if s == uriref:
                s = s.de_skolemize()
            if o == uriref:
                o = o.de_skolemize()
            return (s, p, o)

        def do_de_skolemize2(t):
            (s, p, o) = t
            if isinstance(s, Genid):
                s = s.de_skolemize()
            if isinstance(o, Genid):
                o = o.de_skolemize()
            return (s, p, o)

        retval = Graph() if new_graph is None else new_graph

        if uriref is None:
            self._process_skolem_tuples(retval, do_de_skolemize2)
        elif isinstance(uriref, Genid):
            self._process_skolem_tuples(
                retval, lambda t: do_de_skolemize(uriref, t))

        return retval

class ConjunctiveGraph(Graph):

    """
    A ConjunctiveGraph is an (unamed) aggregation of all the named
    graphs in a store.

    It has a ``default`` graph, whose name is associated with the
    graph throughout its life. :meth:`__init__` can take an identifier
    to use as the name of this default graph or it will assign a
    BNode.

    All methods that add triples work against this default graph.

    All queries are carried out against the union of all graphs.

    """

    def __init__(self, store='default', identifier=None):
        super(ConjunctiveGraph, self).__init__(store, identifier=identifier)
        assert self.store.context_aware, ("ConjunctiveGraph must be backed by"
                                          " a context aware store.")
        self.context_aware = True
        self.default_union = True # Conjunctive!
        self.default_context = Graph(store=self.store,
                                     identifier=identifier or BNode())

    def __str__(self):
        pattern = ("[a rdflib:ConjunctiveGraph;rdflib:storage "
                   "[a rdflib:Store;rdfs:label '%s']]")
        return pattern % self.store.__class__.__name__

    def _spoc(self, triple_or_quad, default=False):
        """
        helper method for having methods that support
        either triples or quads
        """
        if triple_or_quad is None:
            return (None, None, None, self.default_context if default else None)
        if len(triple_or_quad) == 3:
            c = self.default_context if default else None
            (s, p, o) = triple_or_quad
        elif len(triple_or_quad) == 4:
            (s, p, o, c) = triple_or_quad
            c = self._graph(c)
        return s,p,o,c


    def __contains__(self, triple_or_quad):
        """Support for 'triple/quad in graph' syntax"""
        s,p,o,c = self._spoc(triple_or_quad)
        for t in self.triples((s,p,o), context=c):
            return True
        return False


    def add(self, triple_or_quad):

        """
        Add a triple or quad to the store.

        if a triple is given it is added to the default context
        """

        s,p,o,c = self._spoc(triple_or_quad, default=True)

        _assertnode(s,p,o)

        self.store.add((s, p, o), context=c, quoted=False)

    def _graph(self, c):
        if c is None: return None
        if not isinstance(c, Graph):
            return self.get_context(c)
        else:
            return c


    def addN(self, quads):
        """Add a sequence of triples with context"""

        self.store.addN(
            (s, p, o, self._graph(c)) for s, p, o, c in quads if
            _assertnode(s, p, o)
            )

    def remove(self, triple_or_quad):
        """
        Removes a triple or quads

        if a triple is given it is removed from all contexts

        a quad is removed from the given context only

        """
        s,p,o,c = self._spoc(triple_or_quad)

        self.store.remove((s, p, o), context=c)

    def triples(self, triple_or_quad, context=None):
        """
        Iterate over all the triples in the entire conjunctive graph

        For legacy reasons, this can take the context to query either
        as a fourth element of the quad, or as the explicit context
        keyword paramater. The kw param takes precedence.
        """

        s,p,o,c = self._spoc(triple_or_quad)
        context = self._graph(context or c)

        if self.default_union:
            if context==self.default_context:
                context = None
        else:
            if context is None:
                context = self.default_context

        if isinstance(p, Path):
            if context is None:
                context = self

            for s, o in p.eval(context, s, o):
                yield (s, p, o)
        else:
            for (s, p, o), cg in self.store.triples((s, p, o), context=context):
                yield s, p, o

    def quads(self, triple_or_quad=None):
        """Iterate over all the quads in the entire conjunctive graph"""

        s,p,o,c = self._spoc(triple_or_quad)

        for (s, p, o), cg in self.store.triples((s, p, o), context=c):
            for ctx in cg:
                yield s, p, o, ctx

    def triples_choices(self, (s, p, o), context=None):
        """Iterate over all the triples in the entire conjunctive graph"""

        if context is None:
            if not self.default_union:
                context=self.default_context
        else:
            context = self._graph(context)

        for (s1, p1, o1), cg in self.store.triples_choices((s, p, o),
                                                           context=context):
            yield (s1, p1, o1)

    def __len__(self):
        """Number of triples in the entire conjunctive graph"""
        return self.store.__len__()

    def contexts(self, triple=None):
        """Iterate over all contexts in the graph

        If triple is specified, iterate over all contexts the triple is in.
        """
        for context in self.store.contexts(triple):
            if isinstance(context, Graph):
                # TODO: One of these should never happen and probably
                # should raise an exception rather than smoothing over
                # the weirdness - see #225
                yield context
            else:
                yield self.get_context(context)

    def get_context(self, identifier, quoted=False):
        """Return a context graph for the given identifier

        identifier must be a URIRef or BNode.
        """
        return Graph(store=self.store, identifier=identifier,
                     namespace_manager=self)

    def remove_context(self, context):
        """Removes the given context from the graph"""
        self.store.remove((None, None, None), context)

    def context_id(self, uri, context_id=None):
        """URI#context"""
        uri = uri.split("#", 1)[0]
        if context_id is None:
            context_id = "#context"
        return URIRef(context_id, base=uri)

    def parse(self, source=None, publicID=None, format="xml",
              location=None, file=None, data=None, **args):
        """
        Parse source adding the resulting triples to its own context
        (sub graph of this graph).

        See :meth:`rdflib.graph.Graph.parse` for documentation on arguments.

        :Returns:

        The graph into which the source was parsed. In the case of n3
        it returns the root context.
        """

        source = create_input_source(
            source=source, publicID=publicID, location=location,
            file=file, data=data, format=format)

        g_id = publicID and publicID or source.getPublicId()
        if not isinstance(g_id, Node):
            g_id = URIRef(g_id)

        context = Graph(store=self.store, identifier=g_id)
        context.remove((None, None, None)) # hmm ?
        context.parse(source, publicID=publicID, format=format,
                      location=location, file=file, data=data, **args)
        return context

    def __reduce__(self):
        return (ConjunctiveGraph, (self.store, self.identifier))



DATASET_DEFAULT_GRAPH_ID = URIRef('urn:x-rdflib:default')

class Dataset(ConjunctiveGraph):
    __doc__ = format_doctest_out("""
    RDF 1.1 Dataset. Small extension to the Conjunctive Graph:
    - the primary term is graphs in the datasets and not contexts with quads,
    so there is a separate method to set/retrieve a graph in a dataset and
    operate with graphs
    - graphs cannot be identified with blank nodes
    - added a method to directly add a single quad

    Examples of usage:

    >>> # Create a new Dataset
    >>> ds = Dataset()
    >>> # simple triples goes to default graph
    >>> ds.add((URIRef('http://example.org/a'),
    ...    URIRef('http://www.example.org/b'),
    ...    Literal('foo')))
    >>>
    >>> # Create a graph in the dataset, if the graph name has already been
    >>> # used, the corresponding graph will be returned
    >>> # (ie, the Dataset keeps track of the constituent graphs)
    >>> g = ds.graph(URIRef('http://www.example.com/gr'))
    >>>
    >>> # add triples to the new graph as usual
    >>> g.add(
    ...     (URIRef('http://example.org/x'),
    ...     URIRef('http://example.org/y'),
    ...     Literal('bar')) )
    >>> # alternatively: add a quad to the dataset -> goes to the graph
    >>> ds.add(
    ...     (URIRef('http://example.org/x'),
    ...     URIRef('http://example.org/z'),
    ...     Literal('foo-bar'),g) )
    >>>
    >>> # querying triples return them all regardless of the graph
    >>> for t in ds.triples((None,None,None)):  # doctest: +SKIP
    ...     print(t)  # doctest: +NORMALIZE_WHITESPACE
    (rdflib.term.URIRef(%(u)s'http://example.org/a'),
     rdflib.term.URIRef(%(u)s'http://www.example.org/b'),
     rdflib.term.Literal(%(u)s'foo'))
    (rdflib.term.URIRef(%(u)s'http://example.org/x'),
     rdflib.term.URIRef(%(u)s'http://example.org/z'),
     rdflib.term.Literal(%(u)s'foo-bar'))
    (rdflib.term.URIRef(%(u)s'http://example.org/x'),
     rdflib.term.URIRef(%(u)s'http://example.org/y'),
     rdflib.term.Literal(%(u)s'bar'))
    >>>
    >>> # querying quads return quads; the fourth argument can be unrestricted
    >>> # or restricted to a graph
    >>> for q in ds.quads((None, None, None, None)):  # doctest: +SKIP
    ...     print(q)  # doctest: +NORMALIZE_WHITESPACE
    (rdflib.term.URIRef(%(u)s'http://example.org/a'),
     rdflib.term.URIRef(%(u)s'http://www.example.org/b'),
     rdflib.term.Literal(%(u)s'foo'),
     None)
    (rdflib.term.URIRef(%(u)s'http://example.org/x'),
     rdflib.term.URIRef(%(u)s'http://example.org/y'),
     rdflib.term.Literal(%(u)s'bar'),
     rdflib.term.URIRef(%(u)s'http://www.example.com/gr'))
    (rdflib.term.URIRef(%(u)s'http://example.org/x'),
     rdflib.term.URIRef(%(u)s'http://example.org/z'),
     rdflib.term.Literal(%(u)s'foo-bar'),
     rdflib.term.URIRef(%(u)s'http://www.example.com/gr'))
    >>>
    >>> for q in ds.quads((None,None,None,g)):  # doctest: +SKIP
    ...     print(q)  # doctest: +NORMALIZE_WHITESPACE
    (rdflib.term.URIRef(%(u)s'http://example.org/x'),
     rdflib.term.URIRef(%(u)s'http://example.org/y'),
     rdflib.term.Literal(%(u)s'bar'),
     rdflib.term.URIRef(%(u)s'http://www.example.com/gr'))
    (rdflib.term.URIRef(%(u)s'http://example.org/x'),
     rdflib.term.URIRef(%(u)s'http://example.org/z'),
     rdflib.term.Literal(%(u)s'foo-bar'),
     rdflib.term.URIRef(%(u)s'http://www.example.com/gr'))
    >>> # Note that in the call above -
    >>> # ds.quads((None,None,None,'http://www.example.com/gr'))
    >>> # would have been accepted, too
    >>>
    >>> # graph names in the dataset can be queried:
    >>> for c in ds.graphs():  # doctest: +SKIP
    ...     print(c)  # doctest:
    DEFAULT
    http://www.example.com/gr
    >>> # A graph can be created without specifying a name; a skolemized genid
    >>> # is created on the fly
    >>> h = ds.graph()
    >>> for c in ds.graphs():  # doctest: +SKIP
    ...     print(c)  # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
    DEFAULT
    http://rdlib.net/.well-known/genid/rdflib/N...
    http://www.example.com/gr
    >>> # Note that the Dataset.graphs() call returns names of empty graphs,
    >>> # too. This can be restricted:
    >>> for c in ds.graphs(empty=False):  # doctest: +SKIP
    ...     print(c)  # doctest: +NORMALIZE_WHITESPACE
    DEFAULT
    http://www.example.com/gr
    >>>
    >>> # a graph can also be removed from a dataset via ds.remove_graph(g)

    .. versionadded:: 4.0
    """)

    def __init__(self, store='default', default_union=False):
        super(Dataset, self).__init__(store=store, identifier=None)

        if not self.store.graph_aware:
            raise Exception("DataSet must be backed by a graph-aware store!")
        self.default_context = Graph(store=self.store, identifier=DATASET_DEFAULT_GRAPH_ID)

        self.default_union = default_union


    def __str__(self):
        pattern = ("[a rdflib:Dataset;rdflib:storage "
                   "[a rdflib:Store;rdfs:label '%s']]")
        return pattern % self.store.__class__.__name__

    def graph(self, identifier=None):
        if identifier is None:
            from rdflib.term import rdflib_skolem_genid
            self.bind(
                "genid", "http://rdflib.net" + rdflib_skolem_genid,
                override=False)
            identifier = BNode().skolemize()

        g = self._graph(identifier)

        self.store.add_graph(g)
        return g

    def parse(self, source=None, publicID=None, format="xml",
              location=None, file=None, data=None, **args):
        c = ConjunctiveGraph.parse(self, source, publicID, format, location, file, data, **args)
        self.graph(c)
        return c

    def add_graph(self, g):
        """alias of graph for consistency"""
        return self.graph(g)

    def remove_graph(self, g):
        if not isinstance(g, Graph):
            g = self.get_context(g)

        self.store.remove_graph(g)
        if g is None or g == self.default_context:
            # default graph cannot be removed
            # only triples deleted, so add it back in
            self.store.add_graph(self.default_context)

    def contexts(self, triple=None):
        default = False
        for c in super(Dataset, self).contexts(triple):
            default|=c.identifier == DATASET_DEFAULT_GRAPH_ID
            yield c
        if not default: yield self.graph(DATASET_DEFAULT_GRAPH_ID)

    def quads(self, quad):
        for s, p, o, c in super(Dataset, self).quads(quad):
            if c.identifier==self.default_context:
                yield (s, p, o, None)
            else:
                yield (s, p, o, c.identifier)


class QuotedGraph(Graph):
    """
    Quoted Graphs are intended to implement Notation 3 formulae. They are
    associated with a required identifier that the N3 parser *must* provide
    in order to maintain consistent formulae identification for scenarios
    such as implication and other such processing.
    """
    def __init__(self, store, identifier):
        super(QuotedGraph, self).__init__(store, identifier)

    def add(self, (s, p, o)):
        """Add a triple with self as context"""
        assert isinstance(s, Node), \
            "Subject %s must be an rdflib term" % (s,)
        assert isinstance(p, Node), \
            "Predicate %s must be an rdflib term" % (p,)
        assert isinstance(o, Node), \
            "Object %s must be an rdflib term" % (o,)

        self.store.add((s, p, o), self, quoted=True)

    def addN(self, quads):
        """Add a sequence of triple with context"""

        self.store.addN(
            (s, p, o, c) for s, p, o, c in quads
            if isinstance(c, QuotedGraph)
            and c.identifier is self.identifier
            and _assertnode(s, p, o)
            )

    def n3(self):
        """Return an n3 identifier for the Graph"""
        return "{%s}" % self.identifier.n3()

    def __str__(self):
        identifier = self.identifier.n3()
        label = self.store.__class__.__name__
        pattern = ("{this rdflib.identifier %s;rdflib:storage "
                   "[a rdflib:Store;rdfs:label '%s']}")
        return pattern % (identifier, label)

    def __reduce__(self):
        return (QuotedGraph, (self.store, self.identifier))


# Make sure QuotedGraph is ordered correctly
# wrt to other Terms.
# this must be done here, as the QuotedGraph cannot be
# circularily imported in term.py
rdflib.term._ORDERING[QuotedGraph]=11


class Seq(object):
    """Wrapper around an RDF Seq resource

    It implements a container type in Python with the order of the items
    returned corresponding to the Seq content. It is based on the natural
    ordering of the predicate names _1, _2, _3, etc, which is the
    'implementation' of a sequence in RDF terms.
    """

    def __init__(self, graph, subject):
        """Parameters:

        - graph:
            the graph containing the Seq

        - subject:
            the subject of a Seq. Note that the init does not
            check whether this is a Seq, this is done in whoever
            creates this instance!
        """

        _list = self._list = list()
        LI_INDEX = URIRef(str(RDF) + "_")
        for (p, o) in graph.predicate_objects(subject):
            if p.startswith(LI_INDEX):  # != RDF.Seq: #
                i = int(p.replace(LI_INDEX, ''))
                _list.append((i, o))

        # here is the trick: the predicates are _1, _2, _3, etc. Ie,
        # by sorting the keys (by integer) we have what we want!
        _list.sort()

    def toPython(self):
        return self

    def __iter__(self):
        """Generator over the items in the Seq"""
        for _, item in self._list:
            yield item

    def __len__(self):
        """Length of the Seq"""
        return len(self._list)

    def __getitem__(self, index):
        """Item given by index from the Seq"""
        index, item = self._list.__getitem__(index)
        return item


class ModificationException(Exception):

    def __init__(self):
        pass

    def __str__(self):
        return ("Modifications and transactional operations not allowed on "
                "ReadOnlyGraphAggregate instances")


class UnSupportedAggregateOperation(Exception):

    def __init__(self):
        pass

    def __str__(self):
        return ("This operation is not supported by ReadOnlyGraphAggregate "
                "instances")


class ReadOnlyGraphAggregate(ConjunctiveGraph):
    """Utility class for treating a set of graphs as a single graph

    Only read operations are supported (hence the name). Essentially a
    ConjunctiveGraph over an explicit subset of the entire store.
    """

    def __init__(self, graphs, store='default'):
        if store is not None:
            super(ReadOnlyGraphAggregate, self).__init__(store)
            Graph.__init__(self, store)
            self.__namespace_manager = None

        assert isinstance(graphs, list) \
            and graphs \
            and [g for g in graphs if isinstance(g, Graph)], \
            "graphs argument must be a list of Graphs!!"
        self.graphs = graphs

    def __repr__(self):
        return "<ReadOnlyGraphAggregate: %s graphs>" % len(self.graphs)

    def destroy(self, configuration):
        raise ModificationException()

    # Transactional interfaces (optional)
    def commit(self):
        raise ModificationException()

    def rollback(self):
        raise ModificationException()

    def open(self, configuration, create=False):
        # TODO: is there a use case for this method?
        for graph in self.graphs:
            graph.open(self, configuration, create)

    def close(self):
        for graph in self.graphs:
            graph.close()

    def add(self, (s, p, o)):
        raise ModificationException()

    def addN(self, quads):
        raise ModificationException()

    def remove(self, (s, p, o)):
        raise ModificationException()

    def triples(self, (s, p, o)):
        for graph in self.graphs:
            if isinstance(p, Path):
                for s, o in p.eval(self, s, o):
                    yield s, p, o
            else:
                for s1, p1, o1 in graph.triples((s, p, o)):
                    yield (s1, p1, o1)

    def __contains__(self, triple_or_quad):
        context = None
        if len(triple_or_quad) == 4:
            context = triple_or_quad[3]
        for graph in self.graphs:
            if context is None or graph.identifier == context.identifier:
                if triple_or_quad[:3] in graph:
                    return True
        return False

    def quads(self, (s, p, o)):
        """Iterate over all the quads in the entire aggregate graph"""
        for graph in self.graphs:
            for s1, p1, o1 in graph.triples((s, p, o)):
                yield (s1, p1, o1, graph)

    def __len__(self):
        return sum(len(g) for g in self.graphs)

    def __hash__(self):
        raise UnSupportedAggregateOperation()

    def __cmp__(self, other):
        if other is None:
            return -1
        elif isinstance(other, Graph):
            return -1
        elif isinstance(other, ReadOnlyGraphAggregate):
            return cmp(self.graphs, other.graphs)
        else:
            return -1

    def __iadd__(self, other):
        raise ModificationException()

    def __isub__(self, other):
        raise ModificationException()

    # Conv. methods

    def triples_choices(self, (subject, predicate, object_), context=None):
        for graph in self.graphs:
            choices = graph.triples_choices((subject, predicate, object_))
            for (s, p, o) in choices:
                yield (s, p, o)

    def qname(self, uri):
        if hasattr(self, 'namespace_manager') and self.namespace_manager:
            return self.namespace_manager.qname(uri)
        raise UnSupportedAggregateOperation()

    def compute_qname(self, uri, generate=True):
        if hasattr(self, 'namespace_manager') and self.namespace_manager:
            return self.namespace_manager.compute_qname(uri, generate)
        raise UnSupportedAggregateOperation()

    def bind(self, prefix, namespace, override=True):
        raise UnSupportedAggregateOperation()

    def namespaces(self):
        if hasattr(self, 'namespace_manager'):
            for prefix, namespace in self.namespace_manager.namespaces():
                yield prefix, namespace
        else:
            for graph in self.graphs:
                for prefix, namespace in graph.namespaces():
                    yield prefix, namespace

    def absolutize(self, uri, defrag=1):
        raise UnSupportedAggregateOperation()

    def parse(self, source, publicID=None, format="xml", **args):
        raise ModificationException()

    def n3(self):
        raise UnSupportedAggregateOperation()

    def __reduce__(self):
        raise UnSupportedAggregateOperation()

def _assertnode(*terms):
    for t in terms:
        assert isinstance(t, Node), \
            'Term %s must be an rdflib term' % (t,)
    return True


def test():
    import doctest
    doctest.testmod()

if __name__ == '__main__':
    test()

########NEW FILE########
__FILENAME__ = namespace
from rdflib.py3compat import format_doctest_out

__doc__ = format_doctest_out("""
===================
Namespace Utilities
===================

RDFLib provides mechanisms for managing Namespaces.

In particular, there is a :class:`~rdflib.namespace.Namespace` class
that takes as its argument the base URI of the namespace.

.. code-block:: pycon

    >>> from rdflib.namespace import Namespace
    >>> owl = Namespace('http://www.w3.org/2002/07/owl#')

Fully qualified URIs in the namespace can be constructed either by attribute
or by dictionary access on Namespace instances:

.. code-block:: pycon

    >>> owl.seeAlso
    rdflib.term.URIRef(%(u)s'http://www.w3.org/2002/07/owl#seeAlso')
    >>> owl['seeAlso']
    rdflib.term.URIRef(%(u)s'http://www.w3.org/2002/07/owl#seeAlso')


Automatic handling of unknown predicates
-----------------------------------------

As a programming convenience, a namespace binding is automatically
created when :class:`rdflib.term.URIRef` predicates are added to the graph.

Importable namespaces
-----------------------

The following namespaces are available by directly importing from rdflib:

* RDF
* RDFS
* OWL
* XSD
* FOAF
* SKOS
* DOAP
* DC
* DCTERMS
* VOID

.. code-block:: pycon

    >>> from rdflib import OWL
    >>> OWL.seeAlso
    rdflib.term.URIRef(%(u)s'http://www.w3.org/2002/07/owl#seeAlso')

""")

import logging

_logger = logging.getLogger(__name__)

import os

from urlparse import urljoin, urldefrag
from urllib import pathname2url

from rdflib.term import URIRef, Variable, _XSD_PFX, _is_valid_uri

__all__ = [
    'is_ncname', 'split_uri', 'Namespace',
    'ClosedNamespace', 'NamespaceManager',
    'XMLNS', 'RDF', 'RDFS', 'XSD', 'OWL',
    'SKOS', 'DOAP', 'FOAF', 'DC', 'DCTERMS', 'VOID']


class Namespace(unicode):

    __doc__ = format_doctest_out("""
    Utility class for quickly generating URIRefs with a common prefix

    >>> from rdflib import Namespace
    >>> n = Namespace("http://example.org/")
    >>> n.Person # as attribute
    rdflib.term.URIRef(%(u)s'http://example.org/Person')
    >>> n['first-name'] # as item - for things that are not valid python identifiers
    rdflib.term.URIRef(%(u)s'http://example.org/first-name')

    """)


    def __new__(cls, value):
        try:
            rt = unicode.__new__(cls, value)
        except UnicodeDecodeError:
            rt = unicode.__new__(cls, value, 'utf-8')
        return rt


    @property
    def title(self):
        return URIRef(self + 'title')

    def term(self, name):
        # need to handle slices explicitly because of __getitem__ override
        return URIRef(self + (name if isinstance(name, basestring) else ''))

    def __getitem__(self, key, default=None):
        return self.term(key)

    def __getattr__(self, name):
        if name.startswith("__"):  # ignore any special Python names!
            raise AttributeError
        else:
            return self.term(name)

    def __repr__(self):
        return "Namespace(%s)"%unicode.__repr__(self)


class URIPattern(unicode):

    __doc__ = format_doctest_out("""
    Utility class for creating URIs according to some pattern
    This supports either new style formatting with .format
    or old-style with %% operator

    >>> u=URIPattern("http://example.org/%%s/%%d/resource")
    >>> u%%('books', 12345)
    rdflib.term.URIRef(%(u)s'http://example.org/books/12345/resource')

    """)

    def __new__(cls, value):
        try:
            rt = unicode.__new__(cls, value)
        except UnicodeDecodeError:
            rt = unicode.__new__(cls, value, 'utf-8')
        return rt

    def __mod__(self, *args, **kwargs):
        return URIRef(unicode(self).__mod__(*args, **kwargs))

    def format(self, *args, **kwargs):
        return URIRef(unicode.format(self, *args, **kwargs))

    def __repr__(self):
        return "URIPattern(%r)"%unicode.__repr__(self)



class ClosedNamespace(object):
    """
    A namespace with a closed list of members

    Trying to create terms not listen is an error
    """

    def __init__(self, uri, terms):
        self.uri = uri
        self.__uris = {}
        for t in terms:
            self.__uris[t] = URIRef(self.uri + t)

    def term(self, name):
        uri = self.__uris.get(name)
        if uri is None:
            raise Exception(
                "term '%s' not in namespace '%s'" % (name, self.uri))
        else:
            return uri

    def __getitem__(self, key, default=None):
        return self.term(key)

    def __getattr__(self, name):
        if name.startswith("__"):  # ignore any special Python names!
            raise AttributeError
        else:
            return self.term(name)

    def __str__(self):
        return str(self.uri)

    def __repr__(self):
        return """rdf.namespace.ClosedNamespace('%s')""" % str(self.uri)


class _RDFNamespace(ClosedNamespace):
    """
    Closed namespace for RDF terms
    """
    def __init__(self):
        super(_RDFNamespace, self).__init__(
            URIRef("http://www.w3.org/1999/02/22-rdf-syntax-ns#"),
            terms=[
                # Syntax Names
                "RDF", "Description", "ID", "about", "parseType",
                "resource", "li", "nodeID", "datatype",

                # RDF Classes
                "Seq", "Bag", "Alt", "Statement", "Property",
                "List", "PlainLiteral",

                # RDF Properties
                "subject", "predicate", "object", "type",
                "value", "first", "rest",
                # and _n where n is a non-negative integer

                # RDF Resources
                "nil",

                # Added in RDF 1.1
                "XMLLiteral", "HTML", "langString"]
        )

    def term(self, name):
        try:
            i = int(name)
            return URIRef("%s_%s" % (self.uri, i))
        except ValueError:
            return super(_RDFNamespace, self).term(name)

RDF = _RDFNamespace()

RDFS = ClosedNamespace(
    uri=URIRef("http://www.w3.org/2000/01/rdf-schema#"),
    terms=[
        "Resource", "Class", "subClassOf", "subPropertyOf", "comment", "label",
        "domain", "range", "seeAlso", "isDefinedBy", "Literal", "Container",
        "ContainerMembershipProperty", "member", "Datatype"]
)

OWL = Namespace('http://www.w3.org/2002/07/owl#')

XSD = Namespace(_XSD_PFX)

SKOS = Namespace('http://www.w3.org/2004/02/skos/core#')
DOAP = Namespace('http://usefulinc.com/ns/doap#')
FOAF = Namespace('http://xmlns.com/foaf/0.1/')
DC = Namespace('http://purl.org/dc/elements/1.1/')
DCTERMS = Namespace('http://purl.org/dc/terms/')
VOID = Namespace('http://rdfs.org/ns/void#')



class NamespaceManager(object):
    """

    Class for managing prefix => namespace mappings

    Sample usage from FuXi ...

    .. code-block:: python

        ruleStore = N3RuleStore(additionalBuiltins=additionalBuiltins)
        nsMgr = NamespaceManager(Graph(ruleStore))
        ruleGraph = Graph(ruleStore,namespace_manager=nsMgr)


    and ...

    .. code-block:: pycon

        >>> import rdflib
        >>> from rdflib import Graph
        >>> from rdflib.namespace import Namespace, NamespaceManager
        >>> exNs = Namespace('http://example.com/')
        >>> namespace_manager = NamespaceManager(Graph())
        >>> namespace_manager.bind('ex', exNs, override=False)
        >>> g = Graph()
        >>> g.namespace_manager = namespace_manager
        >>> all_ns = [n for n in g.namespace_manager.namespaces()]
        >>> assert ('ex', rdflib.term.URIRef('http://example.com/')) in all_ns
        >>>

    """
    def __init__(self, graph):
        self.graph = graph
        self.__cache = {}
        self.__log = None
        self.bind("xml", u"http://www.w3.org/XML/1998/namespace")
        self.bind("rdf", RDF)
        self.bind("rdfs", RDFS)
        self.bind("xsd", XSD)

    def reset(self):
        self.__cache = {}

    def __get_store(self):
        return self.graph.store
    store = property(__get_store)

    def qname(self, uri):
        prefix, namespace, name = self.compute_qname(uri)
        if prefix == "":
            return name
        else:
            return ":".join((prefix, name))

    def normalizeUri(self, rdfTerm):
        """
        Takes an RDF Term and 'normalizes' it into a QName (using the
        registered prefix) or (unlike compute_qname) the Notation 3
        form for URIs: <...URI...>
        """
        try:
            namespace, name = split_uri(rdfTerm)
            namespace = URIRef(unicode(namespace))
        except:
            if isinstance(rdfTerm, Variable):
                return "?%s" % rdfTerm
            else:
                return "<%s>" % rdfTerm
        prefix = self.store.prefix(namespace)
        if prefix is None and isinstance(rdfTerm, Variable):
            return "?%s" % rdfTerm
        elif prefix is None:
            return "<%s>" % rdfTerm
        else:
            qNameParts = self.compute_qname(rdfTerm)
            return ':'.join([qNameParts[0], qNameParts[-1]])

    def compute_qname(self, uri, generate=True):

        if not _is_valid_uri(uri):
            raise Exception('"%s" does not look like a valid URI, I cannot serialize this. Perhaps you wanted to urlencode it?'%uri)


        if not uri in self.__cache:
            namespace, name = split_uri(uri)
            namespace = URIRef(namespace)
            prefix = self.store.prefix(namespace)
            if prefix is None:
                if not generate:
                    raise Exception(
                        "No known prefix for %s and generate=False")
                num = 1
                while 1:
                    prefix = "ns%s" % num
                    if not self.store.namespace(prefix):
                        break
                    num += 1
                self.bind(prefix, namespace)
            self.__cache[uri] = (prefix, namespace, name)
        return self.__cache[uri]

    def bind(self, prefix, namespace, override=True, replace=False):

        """bind a given namespace to the prefix

        if override, rebind, even if the given namespace is already
        bound to another prefix.

        if replace, replace any existing prefix with the new namespace

        """

        namespace = URIRef(unicode(namespace))
        # When documenting explain that override only applies in what cases
        if prefix is None:
            prefix = ''
        bound_namespace = self.store.namespace(prefix)
        # Check if the bound_namespace contains a URI
        # and if so convert it into a URIRef for comparison
        # This is to prevent duplicate namespaces with the
        # same URI
        if bound_namespace:
            bound_namespace = URIRef(bound_namespace)
        if bound_namespace and bound_namespace != namespace:

            if replace:
                self.store.bind(prefix, namespace)
                return

            # prefix already in use for different namespace
            #
            # append number to end of prefix until we find one
            # that's not in use.
            if not prefix:
                prefix = "default"
            num = 1
            while 1:
                new_prefix = "%s%s" % (prefix, num)
                tnamespace = self.store.namespace(new_prefix)
                if tnamespace and namespace == URIRef(tnamespace):
                    # the prefix is already bound to the correct
                    # namespace
                    return
                if not self.store.namespace(new_prefix):
                    break
                num += 1
            self.store.bind(new_prefix, namespace)
        else:
            bound_prefix = self.store.prefix(namespace)
            if bound_prefix is None:
                self.store.bind(prefix, namespace)
            elif bound_prefix == prefix:
                pass  # already bound
            else:
                if override or bound_prefix.startswith("_"):  # or a generated
                                                              # prefix
                    self.store.bind(prefix, namespace)

    def namespaces(self):
        for prefix, namespace in self.store.namespaces():
            namespace = URIRef(namespace)
            yield prefix, namespace

    def absolutize(self, uri, defrag=1):
        base = urljoin("file:", pathname2url(os.getcwd()))
        result = urljoin("%s/" % base, uri, allow_fragments=not defrag)
        if defrag:
            result = urldefrag(result)[0]
        if not defrag:
            if uri and uri[-1] == "#" and result[-1] != "#":
                result = "%s#" % result
        return URIRef(result)

# From: http://www.w3.org/TR/REC-xml#NT-CombiningChar
#
# * Name start characters must have one of the categories Ll, Lu, Lo,
#   Lt, Nl.
#
# * Name characters other than Name-start characters must have one of
#   the categories Mc, Me, Mn, Lm, or Nd.
#
# * Characters in the compatibility area (i.e. with character code
#   greater than #xF900 and less than #xFFFE) are not allowed in XML
#   names.
#
# * Characters which have a font or compatibility decomposition
#   (i.e. those with a "compatibility formatting tag" in field 5 of the
#   database -- marked by field 5 beginning with a "<") are not allowed.
#
# * The following characters are treated as name-start characters rather
#   than name characters, because the property file classifies them as
#   Alphabetic: [#x02BB-#x02C1], #x0559, #x06E5, #x06E6.
#
# * Characters #x20DD-#x20E0 are excluded (in accordance with Unicode
#   2.0, section 5.14).
#
# * Character #x00B7 is classified as an extender, because the property
#   list so identifies it.
#
# * Character #x0387 is added as a name character, because #x00B7 is its
#   canonical equivalent.
#
# * Characters ':' and '_' are allowed as name-start characters.
#
# * Characters '-' and '.' are allowed as name characters.

from unicodedata import category

NAME_START_CATEGORIES = ["Ll", "Lu", "Lo", "Lt", "Nl"]
NAME_CATEGORIES = NAME_START_CATEGORIES + ["Mc", "Me", "Mn", "Lm", "Nd"]
ALLOWED_NAME_CHARS = [u"\u00B7", u"\u0387", u"-", u".", u"_"]

# http://www.w3.org/TR/REC-xml-names/#NT-NCName
#  [4] NCName ::= (Letter | '_') (NCNameChar)* /* An XML Name, minus
#      the ":" */
#  [5] NCNameChar ::= Letter | Digit | '.' | '-' | '_' | CombiningChar
#      | Extender


def is_ncname(name):
    first = name[0]
    if first == "_" or category(first) in NAME_START_CATEGORIES:
        for i in xrange(1, len(name)):
            c = name[i]
            if not category(c) in NAME_CATEGORIES:
                if c in ALLOWED_NAME_CHARS:
                    continue
                return 0
            # if in compatibility area
            # if decomposition(c)!='':
            #    return 0

        return 1
    else:
        return 0

XMLNS = "http://www.w3.org/XML/1998/namespace"


def split_uri(uri):
    if uri.startswith(XMLNS):
        return (XMLNS, uri.split(XMLNS)[1])
    length = len(uri)
    for i in xrange(0, length):
        c = uri[-i - 1]
        if not category(c) in NAME_CATEGORIES:
            if c in ALLOWED_NAME_CHARS:
                continue
            for j in xrange(-1 - i, length):
                if category(uri[j]) in NAME_START_CATEGORIES or uri[j] == "_":
                    ns = uri[:j]
                    if not ns:
                        break
                    ln = uri[j:]
                    return (ns, ln)
            break
    raise Exception("Can't split '%s'" % uri)

########NEW FILE########
__FILENAME__ = parser
"""
Parser plugin interface.

This module defines the parser plugin interface and contains other
related parser support code.

The module is mainly useful for those wanting to write a parser that
can plugin to rdflib. If you are wanting to invoke a parser you likely
want to do so through the Graph class parse method.

"""

import os
import sys
from urllib import pathname2url, url2pathname
from urllib2 import urlopen, Request
from urlparse import urljoin
from rdflib.py3compat import PY3
if PY3:
    from io import BytesIO
    assert BytesIO
else:
    from StringIO import StringIO as BytesIO
from xml.sax import xmlreader

from rdflib import __version__
from rdflib.term import URIRef
from rdflib.namespace import Namespace

__all__ = [
    'Parser', 'InputSource', 'StringInputSource',
    'URLInputSource', 'FileInputSource']


class Parser(object):

    def __init__(self):
        pass

    def parse(self, source, sink):
        pass


class InputSource(xmlreader.InputSource, object):
    """
    TODO:
    """

    def __init__(self, system_id=None):
        xmlreader.InputSource.__init__(self, system_id=system_id)
        self.content_type = None


class StringInputSource(InputSource):
    """
    TODO:
    """

    def __init__(self, value, system_id=None):
        super(StringInputSource, self).__init__(system_id)
        stream = BytesIO(value)
        self.setByteStream(stream)
        # TODO:
        #   encoding = value.encoding
        #   self.setEncoding(encoding)


headers = {
    'User-agent':
    'rdflib-%s (http://rdflib.net/; eikeon@eikeon.com)' % __version__
}


class URLInputSource(InputSource):
    """
    TODO:
    """

    def __init__(self, system_id=None, format=None):
        super(URLInputSource, self).__init__(system_id)
        self.url = system_id

        # copy headers to change
        myheaders = dict(headers)
        if format == 'application/rdf+xml':
            myheaders['Accept'] = 'application/rdf+xml, */*;q=0.1'
        elif format == 'n3':
            myheaders['Accept'] = 'text/n3, */*;q=0.1'
        elif format == 'nt':
            myheaders['Accept'] = 'text/plain, */*;q=0.1'
        else:
            myheaders['Accept'] = (
                'application/rdf+xml,text/rdf+n3;q=0.9,' +
                'application/xhtml+xml;q=0.5, */*;q=0.1')

        req = Request(system_id, None, myheaders)
        file = urlopen(req)
        # Fix for issue 130 https://github.com/RDFLib/rdflib/issues/130
        self.url = file.geturl()    # in case redirections took place
        self.setPublicId(self.url)
        self.content_type = file.info().get('content-type')
        self.content_type = self.content_type.split(";", 1)[0]
        self.setByteStream(file)
        # TODO: self.setEncoding(encoding)

    def __repr__(self):
        return self.url


class FileInputSource(InputSource):

    def __init__(self, file):
        base = urljoin("file:", pathname2url(os.getcwd()))
        system_id = URIRef(urljoin("file:", pathname2url(file.name)), base=base)
        super(FileInputSource, self).__init__(system_id)
        self.file = file
        self.setByteStream(file)
        # TODO: self.setEncoding(encoding)

    def __repr__(self):
        return repr(self.file)


def create_input_source(source=None, publicID=None,
                        location=None, file=None, data=None, format=None):
    """
    Return an appropriate InputSource instance for the given
    parameters.
    """

    # TODO: test that exactly one of source, location, file, and data
    # is not None.

    input_source = None

    if source is not None:
        if isinstance(source, InputSource):
            input_source = source
        else:
            if isinstance(source, basestring):
                location = source
            elif hasattr(source, "read") and not isinstance(source, Namespace):
                f = source
                input_source = InputSource()
                input_source.setByteStream(f)
                if f is sys.stdin:
                    input_source.setSystemId("file:///dev/stdin")
                elif hasattr(f, "name"):
                    input_source.setSystemId(f.name)
            else:
                raise Exception("Unexpected type '%s' for source '%s'" %
                                (type(source), source))

    absolute_location = None  # Further to fix for issue 130

    if location is not None:
        # Fix for Windows problem https://github.com/RDFLib/rdflib/issues/145
        if os.path.exists(location):
            location = pathname2url(location)
        base = urljoin("file:", "%s/" % pathname2url(os.getcwd()))
        absolute_location = URIRef(location, base=base).defrag()
        if absolute_location.startswith("file:///"):
            filename = url2pathname(absolute_location.replace("file:///", "/"))
            file = open(filename, "rb")
        else:
            input_source = URLInputSource(absolute_location, format)
        # publicID = publicID or absolute_location  # Further to fix
                                                    # for issue 130

    if file is not None:
        input_source = FileInputSource(file)

    if data is not None:
        if isinstance(data, unicode):
            data = data.encode('utf-8')
        input_source = StringInputSource(data)

    if input_source is None:
        raise Exception("could not create InputSource")
    else:
        if publicID is not None:  # Further to fix for issue 130
            input_source.setPublicId(publicID)
        # Further to fix for issue 130
        elif input_source.getPublicId() is None:
            input_source.setPublicId(absolute_location or "")
        return input_source

########NEW FILE########
__FILENAME__ = paths
from rdflib.py3compat import PY3, format_doctest_out

__doc__ = format_doctest_out("""

This module implements the SPARQL 1.1 Property path operators, as
defined in:

http://www.w3.org/TR/sparql11-query/#propertypaths

In SPARQL the syntax is as follows:

+--------------------+-------------------------------------------------+
|Syntax              | Matches                                         |
+====================+=================================================+
|iri                 | An IRI. A path of length one.                   |
+--------------------+-------------------------------------------------+
|^elt                | Inverse path (object to subject).               |
+--------------------+-------------------------------------------------+
|elt1 / elt2         | A sequence path of elt1 followed by elt2.       |
+--------------------+-------------------------------------------------+
|elt1 | elt2         | A alternative path of elt1 or elt2              |
|                    | (all possibilities are tried).                  |
+--------------------+-------------------------------------------------+
|elt*                | A path that connects the subject and object     |
|                    | of the path by zero or more matches of elt.     |
+--------------------+-------------------------------------------------+
|elt+                | A path that connects the subject and object     |
|                    | of the path by one or more matches of elt.      |
+--------------------+-------------------------------------------------+
|elt?                | A path that connects the subject and object     |
|                    | of the path by zero or one matches of elt.      |
+--------------------+-------------------------------------------------+
|!iri or             | Negated property set. An IRI which is not one of|
|!(iri\ :sub:`1`\ |  | iri\ :sub:`1`...iri\ :sub:`n`.                  |
|... |iri\ :sub:`n`) | !iri is short for !(iri).                       |
+--------------------+-------------------------------------------------+
|!^iri or            | Negated property set where the excluded matches |
|!(^iri\ :sub:`1`\ | | are based on reversed path. That is, not one of |
|... |^iri\ :sub:`n`)| iri\ :sub:`1`...iri\ :sub:`n` as reverse paths. |
|                    | !^iri is short for !(^iri).                     |
+--------------------+-------------------------------------------------+
|!(iri\ :sub:`1`\ |  | A combination of forward and reverse            |
|...|iri\ :sub:`j`\ || properties in a negated property set.           |
|^iri\ :sub:`j+1`\ | |                                                 |
|... |^iri\ :sub:`n`)|                                                 |
+--------------------+-------------------------------------------------+
|(elt)               | A group path elt, brackets control precedence.  |
+--------------------+-------------------------------------------------+

This module is used internally be the SPARQL engine, but they property paths
can also be used to query RDFLib Graphs directly.

Where possible the SPARQL syntax is mapped to python operators, and property
path objects can be constructed from existing URIRefs.

>>> from rdflib import Graph, Namespace

>>> foaf=Namespace('http://xmlns.com/foaf/0.1/')

>>> ~foaf.knows
Path(~http://xmlns.com/foaf/0.1/knows)

>>> foaf.knows/foaf.name
Path(http://xmlns.com/foaf/0.1/knows / http://xmlns.com/foaf/0.1/name)

>>> foaf.name|foaf.firstName
Path(http://xmlns.com/foaf/0.1/name | http://xmlns.com/foaf/0.1/firstName)

Modifiers (?, *, +) are done using * (the multiplication operator) and
the strings '*', '?', '+', also defined as constants in this file.

>>> foaf.knows*OneOrMore
Path(http://xmlns.com/foaf/0.1/knows+)

The path objects can also be used with the normal graph methods.

First some example data:

>>> g=Graph()

>>> g=g.parse(data='''
... @prefix : <ex:> .
...
... :a :p1 :c ; :p2 :f .
... :c :p2 :e ; :p3 :g .
... :g :p3 :h ; :p2 :j .
... :h :p3 :a ; :p2 :g .
...
... :q :px :q .
...
... ''', format='n3') # doctest: +ELLIPSIS

>>> e=Namespace('ex:')

Graph contains:
>>> (e.a, e.p1/e.p2, e.e) in g
True

Graph generator functions, triples, subjects, objects, etc. :

>>> list(g.objects(e.c, (e.p3*OneOrMore)/e.p2)) # doctest: +NORMALIZE_WHITESPACE
[rdflib.term.URIRef(%(u)s'ex:j'), rdflib.term.URIRef(%(u)s'ex:g'),
    rdflib.term.URIRef(%(u)s'ex:f')]

A more complete set of tests:

>>> list(evalPath(g, (None, e.p1/e.p2, None)))==[(e.a, e.e)]
True
>>> list(evalPath(g, (e.a, e.p1|e.p2, None)))==[(e.a,e.c), (e.a,e.f)]
True
>>> list(evalPath(g, (e.c, ~e.p1, None))) == [ (e.c, e.a) ]
True
>>> list(evalPath(g, (e.a, e.p1*ZeroOrOne, None))) == [(e.a, e.a), (e.a, e.c)]
True
>>> list(evalPath(g, (e.c, e.p3*OneOrMore, None))) == [
...     (e.c, e.g), (e.c, e.h), (e.c, e.a)]
True
>>> list(evalPath(g, (e.c, e.p3*ZeroOrMore, None))) == [(e.c, e.c),
...     (e.c, e.g), (e.c, e.h), (e.c, e.a)]
True
>>> list(evalPath(g, (e.a, -e.p1, None))) == [(e.a, e.f)]
True
>>> list(evalPath(g, (e.a, -(e.p1|e.p2), None))) == []
True
>>> list(evalPath(g, (e.g, -~e.p2, None))) == [(e.g, e.j)]
True
>>> list(evalPath(g, (e.e, ~(e.p1/e.p2), None))) == [(e.e, e.a)]
True
>>> list(evalPath(g, (e.a, e.p1/e.p3/e.p3, None))) == [(e.a, e.h)]
True

>>> list(evalPath(g, (e.q, e.px*OneOrMore, None)))
[(rdflib.term.URIRef(%(u)s'ex:q'), rdflib.term.URIRef(%(u)s'ex:q'))]

>>> list(evalPath(g, (None, e.p1|e.p2, e.c)))
[(rdflib.term.URIRef(%(u)s'ex:a'), rdflib.term.URIRef(%(u)s'ex:c'))]

>>> list(evalPath(g, (None, ~e.p1, e.a))) == [ (e.c, e.a) ]
True
>>> list(evalPath(g, (None, e.p1*ZeroOrOne, e.c))) # doctest: +NORMALIZE_WHITESPACE
[(rdflib.term.URIRef(%(u)s'ex:c'), rdflib.term.URIRef(%(u)s'ex:c')),
 (rdflib.term.URIRef(%(u)s'ex:a'), rdflib.term.URIRef(%(u)s'ex:c'))]

>>> list(evalPath(g, (None, e.p3*OneOrMore, e.a))) # doctest: +NORMALIZE_WHITESPACE
[(rdflib.term.URIRef(%(u)s'ex:h'), rdflib.term.URIRef(%(u)s'ex:a')),
 (rdflib.term.URIRef(%(u)s'ex:g'), rdflib.term.URIRef(%(u)s'ex:a')),
 (rdflib.term.URIRef(%(u)s'ex:c'), rdflib.term.URIRef(%(u)s'ex:a'))]

>>> list(evalPath(g, (None, e.p3*ZeroOrMore, e.a))) # doctest: +NORMALIZE_WHITESPACE
[(rdflib.term.URIRef(%(u)s'ex:a'), rdflib.term.URIRef(%(u)s'ex:a')),
 (rdflib.term.URIRef(%(u)s'ex:h'), rdflib.term.URIRef(%(u)s'ex:a')),
 (rdflib.term.URIRef(%(u)s'ex:g'), rdflib.term.URIRef(%(u)s'ex:a')),
 (rdflib.term.URIRef(%(u)s'ex:c'), rdflib.term.URIRef(%(u)s'ex:a'))]

>>> list(evalPath(g, (None, -e.p1, e.f))) == [(e.a, e.f)]
True
>>> list(evalPath(g, (None, -(e.p1|e.p2), e.c))) == []
True
>>> list(evalPath(g, (None, -~e.p2, e.j))) == [(e.g, e.j)]
True
>>> list(evalPath(g, (None, ~(e.p1/e.p2), e.a))) == [(e.e, e.a)]
True
>>> list(evalPath(g, (None, e.p1/e.p3/e.p3, e.h))) == [(e.a, e.h)]
True

>>> list(evalPath(g, (e.q, e.px*OneOrMore, None)))
[(rdflib.term.URIRef(%(u)s'ex:q'), rdflib.term.URIRef(%(u)s'ex:q'))]

>>> list(evalPath(g, (e.c, (e.p2|e.p3)*ZeroOrMore, e.j)))
[(rdflib.term.URIRef(%(u)s'ex:c'), rdflib.term.URIRef(%(u)s'ex:j'))]

No vars specified:

>>> sorted(list(evalPath(g, (None, e.p3*OneOrMore, None)))) #doctest: +NORMALIZE_WHITESPACE
[(rdflib.term.URIRef(%(u)s'ex:c'), rdflib.term.URIRef(%(u)s'ex:a')),
 (rdflib.term.URIRef(%(u)s'ex:c'), rdflib.term.URIRef(%(u)s'ex:g')),
 (rdflib.term.URIRef(%(u)s'ex:c'), rdflib.term.URIRef(%(u)s'ex:h')),
 (rdflib.term.URIRef(%(u)s'ex:g'), rdflib.term.URIRef(%(u)s'ex:a')),
 (rdflib.term.URIRef(%(u)s'ex:g'), rdflib.term.URIRef(%(u)s'ex:h')),
 (rdflib.term.URIRef(%(u)s'ex:h'), rdflib.term.URIRef(%(u)s'ex:a'))]

.. versionadded:: 4.0

""")


from rdflib.term import URIRef


# property paths

ZeroOrMore = '*'
OneOrMore = '+'
ZeroOrOne = '?'


class Path:
    def eval(self, graph, subj=None, obj=None):
        raise NotImplementedError()


class InvPath(Path):

    def __init__(self, arg):
        self.arg = arg

    def eval(self, graph, subj=None, obj=None):
        for s, o in evalPath(graph, (obj, self.arg, subj)):
            yield o, s

    def __repr__(self):
        return "Path(~%s)" % (self.arg,)


class SequencePath(Path):
    def __init__(self, *args):
        self.args = []
        for a in args:
            if isinstance(a, SequencePath):
                self.args += a.args
            else:
                self.args.append(a)

    def eval(self, graph, subj=None, obj=None):
        def _eval_seq(paths, subj, obj):
            if paths[1:]:
                for s, o in evalPath(graph, (subj, paths[0], None)):
                    for r in _eval_seq(paths[1:], o, obj):
                        yield s, r[1]

            else:
                for s, o in evalPath(graph, (subj, paths[0], obj)):
                    yield s, o

        def _eval_seq_bw(paths, subj, obj):
            if paths[:-1]:
                for s, o in evalPath(graph, (None, paths[-1], obj)):
                    for r in _eval_seq(paths[:-1], subj, s):
                        yield r[0], o

            else:
                for s, o in evalPath(graph, (subj, paths[0], obj)):
                    yield s, o

        if subj:
            return _eval_seq(self.args, subj, obj)
        elif obj:
            return _eval_seq_bw(self.args, subj, obj)
        else:  # no vars bound, we can start anywhere
            return _eval_seq(self.args, subj, obj)

    def __repr__(self):
        return "Path(%s)" % " / ".join(str(x) for x in self.args)


class AlternativePath(Path):
    def __init__(self, *args):
        self.args = []
        for a in args:
            if isinstance(a, AlternativePath):
                self.args += a.args
            else:
                self.args.append(a)

    def eval(self, graph, subj=None, obj=None):
        for x in self.args:
            for y in evalPath(graph, (subj, x, obj)):
                yield y

    def __repr__(self):
        return "Path(%s)" % " | ".join(str(x) for x in self.args)


class MulPath(Path):
    def __init__(self, path, mod):
        self.path = path
        self.mod = mod

        if mod == ZeroOrOne:
            self.zero = True
            self.more = False
        elif mod == ZeroOrMore:
            self.zero = True
            self.more = True
        elif mod == OneOrMore:
            self.zero = False
            self.more = True
        else:
            raise Exception('Unknown modifier %s' % mod)

    def eval(self, graph, subj=None, obj=None, first=True):
        if self.zero and first:
            if subj and obj:
                if subj == obj:
                    yield (subj, obj)
            elif subj:
                yield (subj, subj)
            elif obj:
                yield (obj, obj)

        def _fwd(subj=None, obj=None, seen=None):
            seen.add(subj)

            for s, o in evalPath(graph, (subj, self.path, None)):
                if not obj or o == obj:
                    yield s, o
                if self.more:
                    if o in seen:
                        continue
                    for s2, o2 in _fwd(o, obj, seen):
                        yield s, o2

        def _bwd(subj=None, obj=None, seen=None):
            seen.add(obj)

            for s, o in evalPath(graph, (None, self.path, obj)):
                if not subj or subj == s:
                    yield s, o
                if self.more:
                    if s in seen:
                        continue

                    for s2, o2 in _bwd(None, s, seen):
                        yield s2, o

        def _fwdbwd():
            if self.zero:
                seen1 = set()
                # According to the spec, ALL nodes are possible solutions
                # (even literals)
                # we cannot do this without going through ALL triples
                # unless we keep an index of all terms somehow
                # but lets just hope this query doesnt happen very often...
                for s, o in graph.subject_objects(None):
                    if s not in seen1:
                        seen1.add(s)
                        yield s, s
                    if o not in seen1:
                        seen1.add(o)
                        yield o, o

            for s, o in evalPath(graph, (None, self.path, None)):
                if not self.more:
                    yield s, o
                else:
                    seen = set()
                    f = list(_fwd(s, None, seen))  # cache or recompute?
                    for s3, o3 in _bwd(None, o, seen):
                        for s2, o2 in f:
                            yield s3, o2  # ?

        done = set()  # the spec does by defn. not allow duplicates
        if subj:
            for x in _fwd(subj, obj, set()):
                if x not in done:
                    done.add(x)
                    yield x
        elif obj:
            for x in _bwd(subj, obj, set()):
                if x not in done:
                    done.add(x)
                    yield x
        else:
            for x in _fwdbwd():
                if x not in done:
                    done.add(x)
                    yield x

    def __repr__(self):
        return "Path(%s%s)" % (self.path, self.mod)


class NegatedPath(Path):
    def __init__(self, arg):
        if isinstance(arg, (URIRef, InvPath)):
            self.args = [arg]
        elif isinstance(arg, AlternativePath):
            self.args = arg.args
        else:
            raise Exception(
                'Can only negate URIRefs, InvPaths or ' +
                'AlternativePaths, not: %s' % (arg,))

    def eval(self, graph, subj=None, obj=None):
        for s, p, o in graph.triples((subj, None, obj)):
            for a in self.args:
                if isinstance(a, URIRef):
                    if p == a:
                        break
                elif isinstance(a, InvPath):
                    if (o, a.arg, s) in graph:
                        break
                else:
                    raise Exception('Invalid path in NegatedPath: %s' % a)
            else:
                yield s, o

    def __repr__(self):
        return "Path(! %s)" % ",".join(str(x) for x in self.args)


class PathList(list):
    pass


def path_alternative(self, other):
    """
    alternative path
    """
    if not isinstance(other, (URIRef, Path)):
        raise Exception('Only URIRefs or Paths can be in paths!')
    return AlternativePath(self, other)


def path_sequence(self, other):
    """
    sequence path
    """
    if not isinstance(other, (URIRef, Path)):
        raise Exception('Only URIRefs or Paths can be in paths!')
    return SequencePath(self, other)


def evalPath(graph, t):
    return ((s, o) for s, p, o in graph.triples(t))

def mul_path(p, mul):
    """
    cardinality path
    """
    return MulPath(p, mul)


def inv_path(p):
    """
    inverse path
    """
    return InvPath(p)


def neg_path(p):
    """
    negated path
    """
    return NegatedPath(p)



if __name__ == '__main__':

    import doctest
    doctest.testmod()
else:
    # monkey patch
    # (these cannot be directly in terms.py
    #  as it would introduce circular imports)

    URIRef.__or__ = path_alternative
    URIRef.__mul__ = mul_path
    URIRef.__invert__ = inv_path
    URIRef.__neg__ = neg_path
    URIRef.__truediv__ = path_sequence
    if not PY3:
        URIRef.__div__ = path_sequence

    Path.__invert__ = inv_path
    Path.__neg__ = neg_path
    Path.__mul__ = mul_path
    Path.__or__ = path_alternative
    Path.__truediv__ = path_sequence
    if not PY3:
        Path.__div__ = path_sequence

########NEW FILE########
__FILENAME__ = plugin
"""
Plugin support for rdf.

There are a number of plugin points for rdf: parser, serializer,
store, query processor, and query result. Plugins can be registered
either through setuptools entry_points or by calling
rdf.plugin.register directly.

If you have a package that uses a setuptools based setup.py you can add the
following to your setup::

    entry_points = {
        'rdf.plugins.parser': [
            'nt =     rdf.plugins.parsers.nt:NTParser',
            ],
        'rdf.plugins.serializer': [
            'nt =     rdf.plugins.serializers.NTSerializer:NTSerializer',
            ],
        }

See the `setuptools dynamic discovery of services and plugins`__ for more
information.

.. __: http://peak.telecommunity.com/DevCenter/setuptools#dynamic-discovery-of-services-and-plugins

"""

from rdflib.store import Store
from rdflib.parser import Parser
from rdflib.serializer import Serializer
from rdflib.query import ResultParser, ResultSerializer, \
    Processor, Result, UpdateProcessor
from rdflib.exceptions import Error

__all__ = [
    'register', 'get', 'plugins', 'PluginException', 'Plugin', 'PKGPlugin']

entry_points = {'rdf.plugins.store': Store,
                'rdf.plugins.serializer': Serializer,
                'rdf.plugins.parser': Parser,
                'rdf.plugins.resultparser': ResultParser,
                'rdf.plugins.resultserializer': ResultSerializer,
                'rdf.plugins.queryprocessor': Processor,
                'rdf.plugins.queryresult': Result,
                'rdf.plugins.updateprocessor': UpdateProcessor
                }

_plugins = {}


class PluginException(Error):
    pass


class Plugin(object):

    def __init__(self, name, kind, module_path, class_name):
        self.name = name
        self.kind = kind
        self.module_path = module_path
        self.class_name = class_name
        self._class = None

    def getClass(self):
        if self._class is None:
            module = __import__(self.module_path, globals(), locals(), [""])
            self._class = getattr(module, self.class_name)
        return self._class


class PKGPlugin(Plugin):

    def __init__(self, name, kind, ep):
        self.name = name
        self.kind = kind
        self.ep = ep
        self._class = None

    def getClass(self):
        if self._class is None:
            self._class = self.ep.load()
        return self._class


def register(name, kind, module_path, class_name):
    """
    Register the plugin for (name, kind). The module_path and
    class_name should be the path to a plugin class.
    """
    p = Plugin(name, kind, module_path, class_name)
    _plugins[(name, kind)] = p


def get(name, kind):
    """
    Return the class for the specified (name, kind). Raises a
    PluginException if unable to do so.
    """
    try:
        p = _plugins[(name, kind)]
    except KeyError:
        raise PluginException(
            "No plugin registered for (%s, %s)" % (name, kind))
    return p.getClass()


try:
    from pkg_resources import iter_entry_points
except ImportError:
    pass  # TODO: log a message
else:
    # add the plugins specified via pkg_resources' EntryPoints.
    for entry_point, kind in entry_points.iteritems():
        for ep in iter_entry_points(entry_point):
            _plugins[(ep.name, kind)] = PKGPlugin(ep.name, kind, ep)


def plugins(name=None, kind=None):
    """
    A generator of the plugins.

    Pass in name and kind to filter... else leave None to match all.
    """
    for p in _plugins.values():
        if (name is None or name == p.name) and (
                kind is None or kind == p.kind):
            yield p

register(
    'default', Store,
    'rdflib.plugins.memory', 'IOMemory')
register(
    'IOMemory', Store,
    'rdflib.plugins.memory', 'IOMemory')
register(
    'Auditable', Store,
    'rdflib.plugins.stores.auditable', 'AuditableStore')
register(
    'Concurrent', Store,
    'rdflib.plugins.stores.concurrent', 'ConcurrentStore')
register(
    'Sleepycat', Store,
    'rdflib.plugins.sleepycat', 'Sleepycat')
register(
    'SPARQLStore', Store,
    'rdflib.plugins.stores.sparqlstore', 'SPARQLStore')
register(
    'SPARQLUpdateStore', Store,
    'rdflib.plugins.stores.sparqlstore', 'SPARQLUpdateStore')

register(
    'application/rdf+xml', Serializer,
    'rdflib.plugins.serializers.rdfxml', 'XMLSerializer')
register(
    'xml', Serializer,
    'rdflib.plugins.serializers.rdfxml', 'XMLSerializer')
register(
    'text/n3', Serializer,
    'rdflib.plugins.serializers.n3', 'N3Serializer')
register(
    'n3', Serializer,
    'rdflib.plugins.serializers.n3', 'N3Serializer')
register(
    'text/turtle', Serializer,
    'rdflib.plugins.serializers.turtle', 'TurtleSerializer')
register(
    'turtle', Serializer,
    'rdflib.plugins.serializers.turtle', 'TurtleSerializer')
register(
    'trig', Serializer,
    'rdflib.plugins.serializers.trig', 'TrigSerializer')
register(
    'application/n-triples', Serializer,
    'rdflib.plugins.serializers.nt', 'NTSerializer')
register(
    'nt', Serializer,
    'rdflib.plugins.serializers.nt', 'NTSerializer')
register(
    'pretty-xml', Serializer,
    'rdflib.plugins.serializers.rdfxml', 'PrettyXMLSerializer')
register(
    'trix', Serializer,
    'rdflib.plugins.serializers.trix', 'TriXSerializer')
register(
    'application/trix', Serializer,
    'rdflib.plugins.serializers.trix', 'TriXSerializer')
register(
    "application/n-quads", Serializer,
    'rdflib.plugins.serializers.nquads', 'NQuadsSerializer')
register(
    "nquads", Serializer,
    'rdflib.plugins.serializers.nquads', 'NQuadsSerializer')

register(
    'application/rdf+xml', Parser,
    'rdflib.plugins.parsers.rdfxml', 'RDFXMLParser')
register(
    'xml', Parser,
    'rdflib.plugins.parsers.rdfxml', 'RDFXMLParser')
register(
    'text/n3', Parser,
    'rdflib.plugins.parsers.notation3', 'N3Parser')
register(
    'n3', Parser,
    'rdflib.plugins.parsers.notation3', 'N3Parser')
register(
    'text/turtle', Parser,
    'rdflib.plugins.parsers.notation3', 'TurtleParser')
register(
    'turtle', Parser,
    'rdflib.plugins.parsers.notation3', 'TurtleParser')
register(
    'application/n-triples', Parser,
    'rdflib.plugins.parsers.nt', 'NTParser')
register(
    'nt', Parser,
    'rdflib.plugins.parsers.nt', 'NTParser')
register(
    'application/n-quads', Parser,
    'rdflib.plugins.parsers.nquads', 'NQuadsParser')
register(
    'nquads', Parser,
    'rdflib.plugins.parsers.nquads', 'NQuadsParser')
register(
    'application/trix', Parser,
    'rdflib.plugins.parsers.trix', 'TriXParser')
register(
    'trix', Parser,
    'rdflib.plugins.parsers.trix', 'TriXParser')
register(
    'trig', Parser,
    'rdflib.plugins.parsers.trig', 'TrigParser')

# The basic parsers: RDFa (by default, 1.1),
# microdata, and embedded turtle (a.k.a. hturtle)
register(
    'hturtle', Parser,
    'rdflib.plugins.parsers.hturtle', 'HTurtleParser')
register(
    'rdfa', Parser,
    'rdflib.plugins.parsers.structureddata', 'RDFaParser')
register(
    'mdata', Parser,
    'rdflib.plugins.parsers.structureddata', 'MicrodataParser')
register(
    'microdata', Parser,
    'rdflib.plugins.parsers.structureddata', 'MicrodataParser')
# A convenience to use the RDFa 1.0 syntax (although the parse method can
# be invoked with an rdfa_version keyword, too)
register(
    'rdfa1.0', Parser,
    'rdflib.plugins.parsers.structureddata', 'RDFa10Parser')
# Just for the completeness, if the user uses this
register(
    'rdfa1.1', Parser,
    'rdflib.plugins.parsers.structureddata', 'RDFaParser')
# An HTML file may contain both microdata, rdfa, or turtle. If the user
# wants them all, the parser below simply invokes all:
register(
    'html', Parser,
    'rdflib.plugins.parsers.structureddata', 'StructuredDataParser')
# Some media types are also bound to RDFa
register(
    'application/svg+xml', Parser,
    'rdflib.plugins.parsers.structureddata', 'RDFaParser')
register(
    'application/xhtml+xml', Parser,
    'rdflib.plugins.parsers.structureddata', 'RDFaParser')
# 'text/html' media type should be equivalent to html:
register(
    'text/html', Parser,
    'rdflib.plugins.parsers.structureddata', 'StructuredDataParser')


register(
    'sparql', Result,
    'rdflib.plugins.sparql.processor', 'SPARQLResult')
register(
    'sparql', Processor,
    'rdflib.plugins.sparql.processor', 'SPARQLProcessor')

register(
    'sparql', UpdateProcessor,
    'rdflib.plugins.sparql.processor', 'SPARQLUpdateProcessor')


register(
    'xml', ResultSerializer,
    'rdflib.plugins.sparql.results.xmlresults', 'XMLResultSerializer')
register(
    'txt', ResultSerializer,
    'rdflib.plugins.sparql.results.txtresults', 'TXTResultSerializer')
register(
    'json', ResultSerializer,
    'rdflib.plugins.sparql.results.jsonresults', 'JSONResultSerializer')
register(
    'csv', ResultSerializer,
    'rdflib.plugins.sparql.results.csvresults', 'CSVResultSerializer')

register(
    'xml', ResultParser,
    'rdflib.plugins.sparql.results.xmlresults', 'XMLResultParser')
register(
    'json', ResultParser,
    'rdflib.plugins.sparql.results.jsonresults', 'JSONResultParser')
register(
    'csv', ResultParser,
    'rdflib.plugins.sparql.results.csvresults', 'CSVResultParser')
register(
    'tsv', ResultParser,
    'rdflib.plugins.sparql.results.tsvresults', 'TSVResultParser')

########NEW FILE########
__FILENAME__ = memory
from rdflib.term import BNode
from rdflib.store import Store, NO_STORE, VALID_STORE

__all__ = ['Memory', 'IOMemory']

ANY = Any = None


class Memory(Store):
    """\
    An in memory implementation of a triple store.

    This triple store uses nested dictionaries to store triples. Each
    triple is stored in two such indices as follows spo[s][p][o] = 1 and
    pos[p][o][s] = 1.

    Authors: Michel Pelletier, Daniel Krech, Stefan Niederhauser
    """
    def __init__(self, configuration=None, identifier=None):
        super(Memory, self).__init__(configuration)
        self.identifier = identifier

        # indexed by [subject][predicate][object]
        self.__spo = {}

        # indexed by [predicate][object][subject]
        self.__pos = {}

        # indexed by [predicate][object][subject]
        self.__osp = {}

        self.__namespace = {}
        self.__prefix = {}

    def add(self, (subject, predicate, object), context, quoted=False):
        """\
        Add a triple to the store of triples.
        """
        # add dictionary entries for spo[s][p][p] = 1 and pos[p][o][s]
        # = 1, creating the nested dictionaries where they do not yet
        # exits.
        spo = self.__spo
        try:
            po = spo[subject]
        except:
            po = spo[subject] = {}
        try:
            o = po[predicate]
        except:
            o = po[predicate] = {}
        o[object] = 1

        pos = self.__pos
        try:
            os = pos[predicate]
        except:
            os = pos[predicate] = {}
        try:
            s = os[object]
        except:
            s = os[object] = {}
        s[subject] = 1

        osp = self.__osp
        try:
            sp = osp[object]
        except:
            sp = osp[object] = {}
        try:
            p = sp[subject]
        except:
            p = sp[subject] = {}
        p[predicate] = 1

    def remove(self, (subject, predicate, object), context=None):
        for (subject, predicate, object), c in self.triples(
                (subject, predicate, object)):
            del self.__spo[subject][predicate][object]
            del self.__pos[predicate][object][subject]
            del self.__osp[object][subject][predicate]

    def triples(self, (subject, predicate, object), context=None):
        """A generator over all the triples matching """
        if subject != ANY:  # subject is given
            spo = self.__spo
            if subject in spo:
                subjectDictionary = spo[subject]
                if predicate != ANY:  # subject+predicate is given
                    if predicate in subjectDictionary:
                        if object != ANY:  # subject+predicate+object is given
                            if object in subjectDictionary[predicate]:
                                yield (subject, predicate, object), \
                                    self.__contexts()
                            else:  # given object not found
                                pass
                        else:  # subject+predicate is given, object unbound
                            for o in subjectDictionary[predicate].keys():
                                yield (subject, predicate, o), \
                                    self.__contexts()
                    else:  # given predicate not found
                        pass
                else:  # subject given, predicate unbound
                    for p in subjectDictionary.keys():
                        if object != ANY:  # object is given
                            if object in subjectDictionary[p]:
                                yield (subject, p, object), self.__contexts()
                            else:  # given object not found
                                pass
                        else:  # object unbound
                            for o in subjectDictionary[p].keys():
                                yield (subject, p, o), self.__contexts()
            else:  # given subject not found
                pass
        elif predicate != ANY:  # predicate is given, subject unbound
            pos = self.__pos
            if predicate in pos:
                predicateDictionary = pos[predicate]
                if object != ANY:  # predicate+object is given, subject unbound
                    if object in predicateDictionary:
                        for s in predicateDictionary[object].keys():
                            yield (s, predicate, object), self.__contexts()
                    else:  # given object not found
                        pass
                else:  # predicate is given, object+subject unbound
                    for o in predicateDictionary.keys():
                        for s in predicateDictionary[o].keys():
                            yield (s, predicate, o), self.__contexts()
        elif object != ANY:  # object is given, subject+predicate unbound
            osp = self.__osp
            if object in osp:
                objectDictionary = osp[object]
                for s in objectDictionary.keys():
                    for p in objectDictionary[s].keys():
                        yield (s, p, object), self.__contexts()
        else:  # subject+predicate+object unbound
            spo = self.__spo
            for s in spo.keys():
                subjectDictionary = spo[s]
                for p in subjectDictionary.keys():
                    for o in subjectDictionary[p].keys():
                        yield (s, p, o), self.__contexts()

    def __len__(self, context=None):
        #@@ optimize
        i = 0
        for triple in self.triples((None, None, None)):
            i += 1
        return i

    def bind(self, prefix, namespace):
        self.__prefix[namespace] = prefix
        self.__namespace[prefix] = namespace

    def namespace(self, prefix):
        return self.__namespace.get(prefix, None)

    def prefix(self, namespace):
        return self.__prefix.get(namespace, None)

    def namespaces(self):
        for prefix, namespace in self.__namespace.iteritems():
            yield prefix, namespace

    def __contexts(self):
        return (c for c in [])  # TODO: best way to return empty generator


class IOMemory(Store):
    """\
    An integer-key-optimized context-aware in-memory store.

    Uses three dict indices (for subjects, objects and predicates) holding
    sets of triples. Context information is tracked in a separate dict, with
    the triple as key and a dict of {context: quoted} items as value. The
    context information is used to filter triple query results.

    Memory usage is low due to several optimizations. RDF nodes are not
    stored directly in the indices; instead, the indices hold integer keys
    and the actual nodes are only stored once in int-to-object and
    object-to-int mapping dictionaries. A default context is determined
    based on the first triple that is added to the store, and no context
    information is actually stored for subsequent other triples with the
    same context information.

    Most operations should be quite fast, but a triples() query with two
    bound parts requires a set intersection operation, which may be slow in
    some cases. When multiple contexts are used in the same store, filtering
    based on context has to be done after each query, which may also be
    slow.

    """
    context_aware = True
    formula_aware = True
    graph_aware = True

    # The following variable name conventions are used in this class:
    #
    # subject, predicate, object             unencoded triple parts
    # triple = (subject, predicate, object)  unencoded triple
    # context:                               unencoded context
    #
    # sid, pid, oid                          integer-encoded triple parts
    # enctriple = (sid, pid, oid)            integer-encoded triple
    # cid                                    integer-encoded context

    def __init__(self, configuration=None, identifier=None):
        super(IOMemory, self).__init__()
        self.__namespace = {}
        self.__prefix = {}

        # Mappings for encoding RDF nodes using integer keys, to save memory
        # in the indexes Note that None is always mapped to itself, to make
        # it easy to test for it in either encoded or unencoded form.
        self.__int2obj = {None: None}  # maps integer keys to objects
        self.__obj2int = {None: None}  # maps objects to integer keys

        # Indexes for each triple part, and a list of contexts for each triple
        self.__subjectIndex = {}    # key: sid    val: set(enctriples)
        self.__predicateIndex = {}  # key: pid    val: set(enctriples)
        self.__objectIndex = {}     # key: oid    val: set(enctriples)
        self.__tripleContexts = {
        }  # key: enctriple    val: {cid1: quoted, cid2: quoted ...}
        self.__contextTriples = {None: set()}  # key: cid    val: set(enctriples)

        # all contexts used in store (unencoded)
        self.__all_contexts = set()
        # default context information for triples
        self.__defaultContexts = None

    def bind(self, prefix, namespace):
        self.__prefix[namespace] = prefix
        self.__namespace[prefix] = namespace

    def namespace(self, prefix):
        return self.__namespace.get(prefix, None)

    def prefix(self, namespace):
        return self.__prefix.get(namespace, None)

    def namespaces(self):
        for prefix, namespace in self.__namespace.iteritems():
            yield prefix, namespace

    def add(self, triple, context, quoted=False):
        Store.add(self, triple, context, quoted)

        if context is not None:
            self.__all_contexts.add(context)

        enctriple = self.__encodeTriple(triple)
        sid, pid, oid = enctriple

        self.__addTripleContext(enctriple, context, quoted)

        if sid in self.__subjectIndex:
            self.__subjectIndex[sid].add(enctriple)
        else:
            self.__subjectIndex[sid] = set([enctriple])

        if pid in self.__predicateIndex:
            self.__predicateIndex[pid].add(enctriple)
        else:
            self.__predicateIndex[pid] = set([enctriple])

        if oid in self.__objectIndex:
            self.__objectIndex[oid].add(enctriple)
        else:
            self.__objectIndex[oid] = set([enctriple])

    def remove(self, triplepat, context=None):
        req_cid = self.__obj2id(context)
        for triple, contexts in self.triples(triplepat, context):
            enctriple = self.__encodeTriple(triple)
            for cid in self.__getTripleContexts(enctriple):
                if context is not None and req_cid != cid:
                    continue
                self.__removeTripleContext(enctriple, cid)
            ctxs = self.__getTripleContexts(enctriple, skipQuoted=True)
            if None in ctxs and (context is None or len(ctxs) == 1):
                self.__removeTripleContext(enctriple, None)
            if len(self.__getTripleContexts(enctriple)) == 0:
                # triple has been removed from all contexts
                sid, pid, oid = enctriple
                self.__subjectIndex[sid].remove(enctriple)
                self.__predicateIndex[pid].remove(enctriple)
                self.__objectIndex[oid].remove(enctriple)

                del self.__tripleContexts[enctriple]

        if not req_cid is None and \
                req_cid in self.__contextTriples and \
                len(self.__contextTriples[req_cid]) == 0:
            # all triples are removed out of this context
            # and it's not the default context so delete it
            del self.__contextTriples[req_cid]

        if triplepat == (None, None, None) and \
                context in self.__all_contexts and \
                not self.graph_aware:
            # remove the whole context
            self.__all_contexts.remove(context)

    def triples(self, triplein, context=None):
        if context is not None:
            if context == self:  # hmm...does this really ever happen?
                context = None

        cid = self.__obj2id(context)
        enctriple = self.__encodeTriple(triplein)
        sid, pid, oid = enctriple

        # all triples case (no triple parts given as pattern)
        if sid is None and pid is None and oid is None:
            return self.__all_triples(cid)

        # optimize "triple in graph" case (all parts given)
        if sid is not None and pid is not None and oid is not None:
            if sid in self.__subjectIndex and \
               enctriple in self.__subjectIndex[sid] and \
               self.__tripleHasContext(enctriple, cid):
                return ((triplein, self.__contexts(enctriple)) for i in [0])
            else:
                return self.__emptygen()

        # remaining cases: one or two out of three given
        sets = []
        if sid is not None:
            if sid in self.__subjectIndex:
                sets.append(self.__subjectIndex[sid])
            else:
                return self.__emptygen()
        if pid is not None:
            if pid in self.__predicateIndex:
                sets.append(self.__predicateIndex[pid])
            else:
                return self.__emptygen()
        if oid is not None:
            if oid in self.__objectIndex:
                sets.append(self.__objectIndex[oid])
            else:
                return self.__emptygen()

        # to get the result, do an intersection of the sets (if necessary)
        if len(sets) > 1:
            enctriples = sets[0].intersection(*sets[1:])
        else:
            enctriples = sets[0].copy()

        return ((self.__decodeTriple(enctriple), self.__contexts(enctriple))
                for enctriple in enctriples
                if self.__tripleHasContext(enctriple, cid))

    def contexts(self, triple=None):
        if triple is None or triple is (None,None,None):
            return (context for context in self.__all_contexts)

        enctriple = self.__encodeTriple(triple)
        sid, pid, oid = enctriple
        if sid in self.__subjectIndex and enctriple in self.__subjectIndex[sid]:
            return self.__contexts(enctriple)
        else:
            return self.__emptygen()

    def __len__(self, context=None):
        cid = self.__obj2id(context)
        if cid not in self.__contextTriples:
            return 0
        return len(self.__contextTriples[cid])

    def add_graph(self, graph):
        if not self.graph_aware:
            Store.add_graph(self, graph)
        else:
            self.__all_contexts.add(graph)

    def remove_graph(self, graph):
        if not self.graph_aware:
            Store.remove_graph(self, graph)
        else:
            self.remove((None,None,None), graph)
            try:
                self.__all_contexts.remove(graph)
            except KeyError:
                pass # we didn't know this graph, no problem



    # internal utility methods below

    def __addTripleContext(self, enctriple, context, quoted):
        """add the given context to the set of contexts for the triple"""
        cid = self.__obj2id(context)

        sid, pid, oid = enctriple
        if sid in self.__subjectIndex and enctriple in self.__subjectIndex[sid]:
            # we know the triple exists somewhere in the store
            if enctriple not in self.__tripleContexts:
                # triple exists with default ctx info
                # start with a copy of the default ctx info
                self.__tripleContexts[
                    enctriple] = self.__defaultContexts.copy()

            self.__tripleContexts[enctriple][cid] = quoted
            if not quoted:
                self.__tripleContexts[enctriple][None] = quoted
        else:
            # the triple didn't exist before in the store
            if quoted:  # this context only
                self.__tripleContexts[enctriple] = {cid: quoted}
            else:  # default context as well
                self.__tripleContexts[enctriple] = {cid: quoted, None: quoted}

        # if the triple is not quoted add it to the default context
        if not quoted:
            self.__contextTriples[None].add(enctriple)

        # always add the triple to given context, making sure it's initialized
        if cid not in self.__contextTriples:
            self.__contextTriples[cid] = set()
        self.__contextTriples[cid].add(enctriple)

        # if this is the first ever triple in the store, set default ctx info
        if self.__defaultContexts is None:
            self.__defaultContexts = self.__tripleContexts[enctriple]

        # if the context info is the same as default, no need to store it
        if self.__tripleContexts[enctriple] == self.__defaultContexts:
            del self.__tripleContexts[enctriple]

    def __getTripleContexts(self, enctriple, skipQuoted=False):
        """return a list of (encoded) contexts for the triple, skipping
           quoted contexts if skipQuoted==True"""

        ctxs = self.__tripleContexts.get(enctriple, self.__defaultContexts)

        if not skipQuoted:
            return ctxs.keys()

        return [cid for cid, quoted in ctxs.iteritems() if not quoted]

    def __tripleHasContext(self, enctriple, cid):
        """return True iff the triple exists in the given context"""
        ctxs = self.__tripleContexts.get(enctriple, self.__defaultContexts)
        return (cid in ctxs)

    def __removeTripleContext(self, enctriple, cid):
        """remove the context from the triple"""
        ctxs = self.__tripleContexts.get(
            enctriple, self.__defaultContexts).copy()
        del ctxs[cid]
        if ctxs == self.__defaultContexts:
            del self.__tripleContexts[enctriple]
        else:
            self.__tripleContexts[enctriple] = ctxs
        self.__contextTriples[cid].remove(enctriple)

    def __obj2id(self, obj):
        """encode object, storing it in the encoding map if necessary,
           and return the integer key"""
        if obj not in self.__obj2int:
            id = randid()
            while id in self.__int2obj:
                id = randid()
            self.__obj2int[obj] = id
            self.__int2obj[id] = obj
            return id
        return self.__obj2int[obj]

    def __encodeTriple(self, triple):
        """encode a whole triple, returning the encoded triple"""
        return tuple(map(self.__obj2id, triple))

    def __decodeTriple(self, enctriple):
        """decode a whole encoded triple, returning the original
        triple"""
        return tuple(map(self.__int2obj.get, enctriple))

    def __all_triples(self, cid):
        """return a generator which yields all the triples (unencoded)
           of the given context"""
        if cid not in self.__contextTriples:
            return
        for enctriple in self.__contextTriples[cid].copy():
            yield self.__decodeTriple(enctriple), self.__contexts(enctriple)

    def __contexts(self, enctriple):
        """return a generator for all the non-quoted contexts
           (unencoded) the encoded triple appears in"""
        return (self.__int2obj.get(cid) for cid in self.__getTripleContexts(enctriple, skipQuoted=True) if cid is not None)

    def __emptygen(self):
        """return an empty generator"""
        if False:
            yield


import random


def randid(randint=random.randint, choice=random.choice, signs=(-1, 1)):
    return choice(signs) * randint(1, 2000000000)

del random

########NEW FILE########
__FILENAME__ = hturtle
# -*- coding: utf-8 -*-
"""
Extraction parser RDF embedded verbatim into HTML or XML files. This is based
on:

* The specification on embedding turtle into html:
    http://www.w3.org/TR/turtle/#in-html

For SVG (and currently SVG only) the method also extracts an embedded RDF/XML
data, per SVG specification

License: W3C Software License,
http://www.w3.org/Consortium/Legal/copyright-software
Author: Ivan Herman
Copyright: W3C
"""

from rdflib.parser import Parser
from .pyRdfa import pyRdfa, Options
from .pyRdfa.state import ExecutionContext
from .pyRdfa.embeddedRDF import handle_embeddedRDF
from .structureddata import _get_orig_source, _check_error

try:
    import html5lib
    assert html5lib
    html5lib = True
except ImportError:
    import warnings
    warnings.warn(
        'html5lib not found! RDFa and Microdata parsers ' +
        'will not be available.')
    html5lib = False


class HTurtle(pyRdfa):
    """
    Bastardizing the RDFa 1.1 parser to do a hturtle extractions
    """
    def __init__(self, options=None, base="", media_type=""):
        pyRdfa.__init__(self, options=options, base=base,
                        media_type=media_type, rdfa_version="1.1")

    def graph_from_DOM(self, dom, graph, pgraph=None):
        """
        Stealing the parsing function from the original class, to do
        turtle extraction only
        """

        def copyGraph(tog, fromg):
            for t in fromg:
                tog.add(t)
            for k, ns in fromg.namespaces():
                tog.bind(k, ns)

        def _process_one_node(node, graph, state):
            if handle_embeddedRDF(node, graph, state):
                # we got an RDF content that has been extracted into Graph;
                # the recursion should stop
                return
            else:
                # recurse through all the child elements of the current node
                for n in node.childNodes:
                    if n.nodeType == node.ELEMENT_NODE:
                        _process_one_node(n, graph, state)

        topElement = dom.documentElement
        state = ExecutionContext(topElement, graph, base=self.base,
                                 options=self.options, rdfa_version="1.1")
        _process_one_node(topElement, graph, state)
        if pgraph is not None:
            copyGraph(pgraph, self.options.processor_graph.graph)

# This is the parser interface as it would look when called from the rest of
# RDFLib


class HTurtleParser(Parser):
    def parse(self, source, graph, pgraph=None, media_type=""):
        """
        @param source: one of the input sources that the RDFLib package defined
        @type source: InputSource class instance
        @param graph: target graph for the triples; output graph, in RDFa spec.
        parlance
        @type graph: RDFLib Graph
        @keyword media_type: explicit setting of the preferred media type
        (a.k.a. content type) of the the RDFa source. None means the content
        type of the HTTP result is used, or a guess is made based on the
        suffix of a file
        @type media_type: string
        """
        if html5lib is False:
            raise ImportError(
                'html5lib is not installed, cannot ' +
                'use RDFa and Microdata parsers.')

        (baseURI, orig_source) = _get_orig_source(source)
        self._process(
            graph, pgraph, baseURI, orig_source, media_type=media_type)

    def _process(self, graph, baseURI, orig_source, media_type=""):
        self.options = Options(output_processor_graph=None,
                               embedded_rdf=True,
                               vocab_expansion=False,
                               vocab_cache=False)

        if media_type is None:
            media_type = ""
        processor = HTurtle(
            self.options, base=baseURI, media_type=media_type)
        processor.graph_from_source(
            orig_source, graph=graph, pgraph=None, rdfOutput=False)
        # get possible error triples to raise exceptions
        _check_error(graph)

########NEW FILE########
__FILENAME__ = notation3
#!/usr/bin/env python
u"""
notation3.py - Standalone Notation3 Parser
Derived from CWM, the Closed World Machine

Authors of the original suite:

* Dan Connolly <@@>
* Tim Berners-Lee <@@>
* Yosi Scharf <@@>
* Joseph M. Reagle Jr. <reagle@w3.org>
* Rich Salz <rsalz@zolera.com>

http://www.w3.org/2000/10/swap/notation3.py

Copyright 2000-2007, World Wide Web Consortium.
Copyright 2001, MIT.
Copyright 2001, Zolera Systems Inc.

License: W3C Software License
http://www.w3.org/Consortium/Legal/copyright-software

Modified by Sean B. Palmer
Copyright 2007, Sean B. Palmer.

Modified to work with rdflib by Gunnar Aastrand Grimnes
Copyright 2010, Gunnar A. Grimnes

"""

# Python standard libraries
import types
import sys
import os
import re
import codecs

from decimal import Decimal

from uuid import uuid4

from rdflib.term import URIRef, BNode, Literal, Variable, _XSD_PFX, _unique_id
from rdflib.graph import QuotedGraph, ConjunctiveGraph, Graph
from rdflib import py3compat
b = py3compat.b

__all__ = ['BadSyntax', 'N3Parser', 'TurtleParser',
           "splitFragP", "join", "base",
           "runNamespace", "uniqueURI", "hexify"]

from rdflib.parser import Parser


def splitFragP(uriref, punct=0):
    """split a URI reference before the fragment

    Punctuation is kept.

    e.g.

    >>> splitFragP("abc#def")
    ('abc', '#def')

    >>> splitFragP("abcdef")
    ('abcdef', '')

    """

    i = uriref.rfind("#")
    if i >= 0:
        return uriref[:i], uriref[i:]
    else:
        return uriref, ''


@py3compat.format_doctest_out
def join(here, there):
    """join an absolute URI and URI reference
    (non-ascii characters are supported/doctested;
    haven't checked the details of the IRI spec though)

    ``here`` is assumed to be absolute.
    ``there`` is URI reference.

    >>> join('http://example/x/y/z', '../abc')
    'http://example/x/abc'

    Raise ValueError if there uses relative path
    syntax but here has no hierarchical path.

    >>> join('mid:foo@example', '../foo') # doctest: +NORMALIZE_WHITESPACE
    Traceback (most recent call last):
        raise ValueError(here)
    ValueError: Base <mid:foo@example> has no slash
    after colon - with relative '../foo'.

    >>> join('http://example/x/y/z', '')
    'http://example/x/y/z'

    >>> join('mid:foo@example', '#foo')
    'mid:foo@example#foo'

    We grok IRIs

    >>> len(%(u)s'Andr\\xe9')
    5

    >>> join('http://example.org/', %(u)s'#Andr\\xe9')
    %(u)s'http://example.org/#Andr\\xe9'
    """

    assert(here.find("#") < 0), \
        "Base may not contain hash: '%s'" % here  # why must caller splitFrag?

    slashl = there.find('/')
    colonl = there.find(':')

     # join(base, 'foo:/') -- absolute
    if colonl >= 0 and (slashl < 0 or colonl < slashl):
        return there

    bcolonl = here.find(':')
    assert(bcolonl >= 0), \
        "Base uri '%s' is not absolute" % here  # else it's not absolute

    path, frag = splitFragP(there)
    if not path:
        return here + frag

     # join('mid:foo@example', '../foo') bzzt
    if here[bcolonl + 1:bcolonl + 2] != '/':
        raise ValueError(
            ("Base <%s> has no slash after "
             "colon - with relative '%s'.") % (here, there))

    if here[bcolonl + 1:bcolonl + 3] == '//':
        bpath = here.find('/', bcolonl + 3)
    else:
        bpath = bcolonl + 1

     # join('http://xyz', 'foo')
    if bpath < 0:
        bpath = len(here)
        here = here + '/'

     # join('http://xyz/', '//abc') => 'http://abc'
    if there[:2] == '//':
        return here[:bcolonl + 1] + there

     # join('http://xyz/', '/abc') => 'http://xyz/abc'
    if there[:1] == '/':
        return here[:bpath] + there

    slashr = here.rfind('/')

    while 1:
        if path[:2] == './':
            path = path[2:]
        if path == '.':
            path = ''
        elif path[:3] == '../' or path == '..':
            path = path[3:]
            i = here.rfind('/', bpath, slashr)
            if i >= 0:
                here = here[:i + 1]
                slashr = i
        else:
            break

    return here[:slashr + 1] + path + frag


def base():
    """The base URI for this process - the Web equiv of cwd

    Relative or abolute unix-standard filenames parsed relative to
    this yeild the URI of the file.
    If we had a reliable way of getting a computer name,
    we should put it in the hostname just to prevent ambiguity

    """
     # return "file://" + hostname + os.getcwd() + "/"
    return "file://" + _fixslash(os.getcwd()) + "/"


def _fixslash(s):
    """ Fix windowslike filename to unixlike - (#ifdef WINDOWS)"""
    s = s.replace("\\", "/")
    if s[0] != "/" and s[1] == ":":
        s = s[2:]   # @@@ Hack when drive letter present
    return s


CONTEXT = 0
PRED = 1
SUBJ = 2
OBJ = 3

PARTS = PRED, SUBJ, OBJ
ALL4 = CONTEXT, PRED, SUBJ, OBJ

SYMBOL = 0
FORMULA = 1
LITERAL = 2
LITERAL_DT = 21
LITERAL_LANG = 22
ANONYMOUS = 3
XMLLITERAL = 25

Logic_NS = "http://www.w3.org/2000/10/swap/log#"
NODE_MERGE_URI = Logic_NS + "is"   # Pseudo-property indicating node merging
forSomeSym = Logic_NS + "forSome"
forAllSym = Logic_NS + "forAll"

RDF_type_URI = "http://www.w3.org/1999/02/22-rdf-syntax-ns#type"
RDF_NS_URI = "http://www.w3.org/1999/02/22-rdf-syntax-ns#"
OWL_NS = "http://www.w3.org/2002/07/owl#"
DAML_sameAs_URI = OWL_NS + "sameAs"
parsesTo_URI = Logic_NS + "parsesTo"
RDF_spec = "http://www.w3.org/TR/REC-rdf-syntax/"

List_NS = RDF_NS_URI      # From 20030808
_Old_Logic_NS = "http://www.w3.org/2000/10/swap/log.n3#"

N3_first = (SYMBOL, List_NS + "first")
N3_rest = (SYMBOL, List_NS + "rest")
N3_li = (SYMBOL, List_NS + "li")
N3_nil = (SYMBOL, List_NS + "nil")
N3_List = (SYMBOL, List_NS + "List")
N3_Empty = (SYMBOL, List_NS + "Empty")


runNamespaceValue = None


def runNamespace():
    "Return a URI suitable as a namespace for run-local objects"
     # @@@ include hostname (privacy?) (hash it?)
    global runNamespaceValue
    if runNamespaceValue is None:
        runNamespaceValue = join(base(), _unique_id()) + '#'
    return runNamespaceValue

nextu = 0


def uniqueURI():
    "A unique URI"
    global nextu
    nextu += 1
    # return runNamespace() + "u_" + `nextu`
    return runNamespace() + "u_" + str(nextu)


tracking = False
chatty_flag = 50

# from why import BecauseOfData, becauseSubexpression


def BecauseOfData(*args, **kargs):
     # print args, kargs
    pass


def becauseSubexpression(*args, **kargs):
     # print args, kargs
    pass

N3_forSome_URI = forSomeSym
N3_forAll_URI = forAllSym

# Magic resources we know about

ADDED_HASH = "#"   # Stop where we use this in case we want to remove it!
# This is the hash on namespace URIs

RDF_type = (SYMBOL, RDF_type_URI)
DAML_sameAs = (SYMBOL, DAML_sameAs_URI)

LOG_implies_URI = "http://www.w3.org/2000/10/swap/log#implies"

BOOLEAN_DATATYPE = _XSD_PFX + "boolean"
DECIMAL_DATATYPE = _XSD_PFX + "decimal"
DOUBLE_DATATYPE = _XSD_PFX + "double"
FLOAT_DATATYPE = _XSD_PFX + "float"
INTEGER_DATATYPE = _XSD_PFX + "integer"

option_noregen = 0    # If set, do not regenerate genids on output

# @@ I18n - the notname chars need extending for well known unicode non-text
# characters. The XML spec switched to assuming unknown things were name
# characaters.
# _namechars = string.lowercase + string.uppercase + string.digits + '_-'
_notQNameChars = \
    "\t\r\n !\"#$&'()*,+/;<=>?@[\\]^`{|}~"  # else valid qname :-/
_notKeywordsChars = _notQNameChars + "."
_notNameChars = _notQNameChars + ":"   # Assume anything else valid name :-/
_rdfns = 'http://www.w3.org/1999/02/22-rdf-syntax-ns#'

hexChars = 'ABCDEFabcdef0123456789'
escapeChars = "(_~.-!$&'()*+,;=/?#@%)" # valid for \ escapes in localnames

def unicodeExpand(m):
    try:
        return unichr(int(m.group(1), 16))
    except:
        raise Exception("Invalid unicode code point: " + m.group(1))

unicodeEscape4 = re.compile(
    r'\\u([0-9a-f]{4})', flags=re.I)
unicodeEscape8 = re.compile(
    r'\\U([0-9a-f]{8})', flags=re.I)



N3CommentCharacter = "#"      # For unix script  # ! compatabilty

########################################## Parse string to sink
#
# Regular expressions:
eol = re.compile(
    r'[ \t]*(#[^\n]*)?\r?\n')       # end  of line, poss. w/comment
eof = re.compile(
    r'[ \t]*(#[^\n]*)?$')           # end  of file, poss. w/comment
ws = re.compile(r'[ \t]*')                       # Whitespace not including NL
signed_integer = re.compile(r'[-+]?[0-9]+')      # integer
integer_syntax = re.compile(r'[-+]?[0-9]+')
decimal_syntax = re.compile(r'[-+]?[0-9]*\.[0-9]+')
exponent_syntax = re.compile(r'[-+]?(?:[0-9]+\.[0-9]*(?:e|E)[-+]?[0-9]+|'+
                             r'\.[0-9](?:e|E)[-+]?[0-9]+|'+
                             r'[0-9]+(?:e|E)[-+]?[0-9]+)')
digitstring = re.compile(r'[0-9]+')              # Unsigned integer
interesting = re.compile(r"""[\\\r\n\"\']""")
langcode = re.compile(r'[a-zA-Z0-9]+(-[a-zA-Z0-9]+)*')


class SinkParser:
    def __init__(self, store, openFormula=None, thisDoc="", baseURI=None,
                 genPrefix="", why=None, turtle=False):
        """ note: namespace names should *not* end in  # ;
        the  # will get added during qname processing """

        self._bindings = {}
        if thisDoc != "":
            assert ':' in thisDoc, "Document URI not absolute: <%s>" % thisDoc
            self._bindings[""] = thisDoc + "#"   # default

        self._store = store
        if genPrefix:
            store.setGenPrefix(genPrefix)  # pass it on

        self._thisDoc = thisDoc
        self.lines = 0               # for error handling
        self.startOfLine = 0         # For calculating character number
        self._genPrefix = genPrefix
        self.keywords = ['a', 'this', 'bind', 'has', 'is', 'of',
                         'true', 'false']
        self.keywordsSet = 0     # Then only can others be considerd qnames
        self._anonymousNodes = {}
            # Dict of anon nodes already declared ln: Term
        self._variables = {}
        self._parentVariables = {}
        self._reason = why       # Why the parser was asked to parse this

        self.turtle = turtle # raise exception when encountering N3 extensions
        # Turtle allows single or double quotes around strings, whereas N3
        # only allows double quotes.
        self.string_delimiters = ('"', "'") if turtle else ('"',)

        self._reason2 = None     # Why these triples
         # was: diag.tracking
        if tracking:
            self._reason2 = BecauseOfData(
                store.newSymbol(thisDoc), because=self._reason)

        if baseURI:
            self._baseURI = baseURI
        else:
            if thisDoc:
                self._baseURI = thisDoc
            else:
                self._baseURI = None

        assert not self._baseURI or ':' in self._baseURI

        if not self._genPrefix:
            if self._thisDoc:
                self._genPrefix = self._thisDoc + "#_g"
            else:
                self._genPrefix = uniqueURI()

        if openFormula is None:
            if self._thisDoc:
                self._formula = store.newFormula(thisDoc + "#_formula")
            else:
                self._formula = store.newFormula()
        else:
            self._formula = openFormula

        self._context = self._formula
        self._parentContext = None

    def here(self, i):
        """String generated from position in file

        This is for repeatability when refering people to bnodes in a document.
        This has diagnostic uses less formally, as it should point one to which
        bnode the arbitrary identifier actually is. It gives the
        line and character number of the '[' charcacter or path character
        which introduced the blank node. The first blank node is boringly
        _L1C1. It used to be used only for tracking, but for tests in general
        it makes the canonical ordering of bnodes repeatable."""

        return "%s_L%iC%i" % (self._genPrefix, self.lines,
                              i - self.startOfLine + 1)

    def formula(self):
        return self._formula

    def loadStream(self, stream):
        return self.loadBuf(stream.read())    # Not ideal

    def loadBuf(self, buf):
        """Parses a buffer and returns its top level formula"""
        self.startDoc()

        self.feed(buf)
        return self.endDoc()     # self._formula

    def feed(self, octets):
        """Feed an octet stream tothe parser

        if BadSyntax is raised, the string
        passed in the exception object is the
        remainder after any statements have been parsed.
        So if there is more data to feed to the
        parser, it should be straightforward to recover."""

        if not isinstance(octets, unicode):
            s = octets.decode('utf-8')
             # NB already decoded, so \ufeff
            if len(s) > 0 and s[0] == codecs.BOM_UTF8.decode('utf-8'):
                s = s[1:]
        else:
            s = octets

        i = 0
        while i >= 0:
            j = self.skipSpace(s, i)
            if j < 0:
                return

            i = self.directiveOrStatement(s, j)
            if i < 0:
                #print("# next char: %s" % s[j])
                self.BadSyntax(s, j,
                    "expected directive or statement")

    def directiveOrStatement(self, argstr, h):

        i = self.skipSpace(argstr, h)
        if i < 0:
            return i    # EOF

        if self.turtle:
            j = self.sparqlDirective(argstr, i)
            if j >= 0:
                return j

        j = self.directive(argstr, i)
        if j >= 0:
            return self.checkDot(argstr, j)

        j = self.statement(argstr, i)
        if j >= 0:
            return self.checkDot(argstr, j)

        return j

     # @@I18N
     # _namechars = string.lowercase + string.uppercase + string.digits + '_-'

    def tok(self, tok, argstr, i, colon=False):
        """Check for keyword.  Space must have been stripped on entry and
        we must not be at end of file.

        if colon, then keyword followed by colon is ok
        (@prefix:<blah> is ok, rdf:type shortcut a must be followed by ws)
        """

        assert tok[0] not in _notNameChars  # not for punctuation
        if argstr[i:i + 1] == "@":
            i = i + 1
        else:
            if tok not in self.keywords:
                return -1  # No, this has neither keywords declaration nor "@"

        if (argstr[i:i + len(tok)] == tok
            and ( argstr[i + len(tok)] in _notKeywordsChars)
            or (colon and argstr[i+len(tok)] == ':')):
            i = i + len(tok)
            return i
        else:
            return -1

    def sparqlTok(self, tok, argstr, i):
        """Check for SPARQL keyword.  Space must have been stripped on entry
        and we must not be at end of file.
        Case insensitive and not preceeded by @
        """

        assert tok[0] not in _notNameChars  # not for punctuation

        if (argstr[i:i + len(tok)].lower() == tok.lower()
                and (argstr[i + len(tok)] in _notQNameChars)):
            i = i + len(tok)
            return i
        else:
            return -1


    def directive(self, argstr, i):
        j = self.skipSpace(argstr, i)
        if j < 0:
            return j  # eof
        res = []

        j = self.tok('bind', argstr, i)         # implied "#". Obsolete.
        if j > 0:
            self.BadSyntax(argstr, i,
                "keyword bind is obsolete: use @prefix")

        j = self.tok('keywords', argstr, i)
        if j > 0:
            if self.turtle:
                self.BadSyntax(argstr, i, "Found 'keywords' when in Turtle mode.")

            i = self.commaSeparatedList(argstr, j, res, self.bareWord)
            if i < 0:
                self.BadSyntax(argstr, i,
                    "'@keywords' needs comma separated list of words")
            self.setKeywords(res[:])
            return i

        j = self.tok('forAll', argstr, i)
        if j > 0:
            if self.turtle:
                self.BadSyntax(argstr, i, "Found 'forAll' when in Turtle mode.")

            i = self.commaSeparatedList(argstr, j, res, self.uri_ref2)
            if i < 0:
                self.BadSyntax(argstr, i,
                    "Bad variable list after @forAll")
            for x in res:
                 # self._context.declareUniversal(x)
                if x not in self._variables or x in self._parentVariables:
                    self._variables[x] = self._context.newUniversal(x)
            return i

        j = self.tok('forSome', argstr, i)
        if j > 0:
            if self.turtle:
                self.BadSyntax(argstr, i, "Found 'forSome' when in Turtle mode.")

            i = self. commaSeparatedList(argstr, j, res, self.uri_ref2)
            if i < 0:
                self.BadSyntax(argstr, i,
                    "Bad variable list after @forSome")
            for x in res:
                self._context.declareExistential(x)
            return i

        j = self.tok('prefix', argstr, i, colon=True)    # no implied "#"
        if j >= 0:
            t = []
            i = self.qname(argstr, j, t)
            if i < 0:
                self.BadSyntax(argstr, j,
                    "expected qname after @prefix")
            j = self.uri_ref2(argstr, i, t)
            if j < 0:
                self.BadSyntax(argstr, i,
                    "expected <uriref> after @prefix _qname_")
            ns = self.uriOf(t[1])

            if self._baseURI:
                ns = join(self._baseURI, ns)
            elif ":" not in ns:
                self.BadSyntax(argstr, j,
                    "With no base URI, cannot use " +
                    "relative URI in @prefix <" + ns + ">")
            assert ':' in ns  # must be absolute
            self._bindings[t[0][0]] = ns
            self.bind(t[0][0], hexify(ns))
            return j

        j = self.tok('base', argstr, i)       # Added 2007/7/7
        if j >= 0:
            t = []
            i = self.uri_ref2(argstr, j, t)
            if i < 0:
                self.BadSyntax(argstr, j,
                    "expected <uri> after @base ")
            ns = self.uriOf(t[0])

            if self._baseURI:
                ns = join(self._baseURI, ns)
            else:
                self.BadSyntax(argstr, j,
                    "With no previous base URI, cannot use " +
                    "relative URI in @base  <" + ns + ">")
            assert ':' in ns  # must be absolute
            self._baseURI = ns
            return i

        return -1   # Not a directive, could be something else.

    def sparqlDirective(self, argstr, i):

        """
        turtle and trig support BASE/PREFIX without @ and without
        terminating .
        """

        j = self.skipSpace(argstr, i)
        if j < 0:
            return j  # eof

        j = self.sparqlTok('PREFIX', argstr, i)
        if j >= 0:
            t = []
            i = self.qname(argstr, j, t)
            if i < 0:
                self.BadSyntax(argstr, j,
                    "expected qname after @prefix")
            j = self.uri_ref2(argstr, i, t)
            if j < 0:
                self.BadSyntax(argstr, i,
                    "expected <uriref> after @prefix _qname_")
            ns = self.uriOf(t[1])

            if self._baseURI:
                ns = join(self._baseURI, ns)
            elif ":" not in ns:
                self.BadSyntax(argstr, j,
                    "With no base URI, cannot use " +
                    "relative URI in @prefix <" + ns + ">")
            assert ':' in ns  # must be absolute
            self._bindings[t[0][0]] = ns
            self.bind(t[0][0], hexify(ns))
            return j

        j = self.sparqlTok('BASE', argstr, i)
        if j >= 0:
            t = []
            i = self.uri_ref2(argstr, j, t)
            if i < 0:
                self.BadSyntax(argstr, j,
                    "expected <uri> after @base ")
            ns = self.uriOf(t[0])

            if self._baseURI:
                ns = join(self._baseURI, ns)
            else:
                self.BadSyntax(argstr, j,
                    "With no previous base URI, cannot use " +
                    "relative URI in @base  <" + ns + ">")
            assert ':' in ns  # must be absolute
            self._baseURI = ns
            return i

        return -1   # Not a directive, could be something else.


    def bind(self, qn, uri):
        assert isinstance(
            uri, types.StringType), "Any unicode must be %x-encoded already"
        if qn == "":
            self._store.setDefaultNamespace(uri)
        else:
            self._store.bind(qn, uri)

    def setKeywords(self, k):
        "Takes a list of strings"
        if k is None:
            self.keywordsSet = 0
        else:
            self.keywords = k
            self.keywordsSet = 1

    def startDoc(self):
         # was: self._store.startDoc()
        self._store.startDoc(self._formula)

    def endDoc(self):
        """Signal end of document and stop parsing. returns formula"""
        self._store.endDoc(self._formula)   # don't canonicalize yet
        return self._formula

    def makeStatement(self, quadruple):
         # $$$$$$$$$$$$$$$$$$$$$
         # print "# Parser output: ", `quadruple`
        self._store.makeStatement(quadruple, why=self._reason2)

    def statement(self, argstr, i):
        r = []
        i = self.object(
            argstr, i, r)   # Allow literal for subject - extends RDF
        if i < 0:
            return i

        j = self.property_list(argstr, i, r[0])

        if j < 0:
            self.BadSyntax(
                argstr, i, "expected propertylist")
        return j

    def subject(self, argstr, i, res):
        return self.item(argstr, i, res)

    def verb(self, argstr, i, res):
        """ has _prop_
        is _prop_ of
        a
        =
        _prop_
        >- prop ->
        <- prop -<
        _operator_"""

        j = self.skipSpace(argstr, i)
        if j < 0:
            return j  # eof

        r = []

        j = self.tok('has', argstr, i)
        if j >= 0:
            if self.turtle:
                self.BadSyntax(argstr, i, "Found 'has' keyword in Turtle mode")

            i = self.prop(argstr, j, r)
            if i < 0:
                self.BadSyntax(argstr, j,
                    "expected property after 'has'")
            res.append(('->', r[0]))
            return i

        j = self.tok('is', argstr, i)
        if j >= 0:
            if self.turtle:
                self.BadSyntax(argstr, i, "Found 'is' keyword in Turtle mode")

            i = self.prop(argstr, j, r)
            if i < 0:
                self.BadSyntax(argstr, j,
                    "expected <property> after 'is'")
            j = self.skipSpace(argstr, i)
            if j < 0:
                self.BadSyntax(argstr, i,
                    "End of file found, expected property after 'is'")
            i = j
            j = self.tok('of', argstr, i)
            if j < 0:
                self.BadSyntax(argstr, i,
                    "expected 'of' after 'is' <prop>")
            res.append(('<-', r[0]))
            return j

        j = self.tok('a', argstr, i)
        if j >= 0:
            res.append(('->', RDF_type))
            return j

        if argstr[i:i + 2] == "<=":
            if self.turtle:
                self.BadSyntax(argstr, i,
                    "Found '<=' in Turtle mode. ")

            res.append(('<-', self._store.newSymbol(Logic_NS + "implies")))
            return i + 2

        if argstr[i:i + 1] == "=":
            if self.turtle:
                self.BadSyntax(argstr, i, "Found '=' in Turtle mode")
            if argstr[i + 1:i + 2] == ">":
                res.append(('->', self._store.newSymbol(Logic_NS + "implies")))
                return i + 2
            res.append(('->', DAML_sameAs))
            return i + 1

        if argstr[i:i + 2] == ":=":
            if self.turtle:
                self.BadSyntax(argstr, i, "Found ':=' in Turtle mode")

             # patch file relates two formulae, uses this    @@ really?
            res.append(('->', Logic_NS + "becomes"))
            return i + 2

        j = self.prop(argstr, i, r)
        if j >= 0:
            res.append(('->', r[0]))
            return j

        if argstr[i:i + 2] == ">-" or argstr[i:i + 2] == "<-":
            self.BadSyntax(argstr, j,
                ">- ... -> syntax is obsolete.")

        return -1

    def prop(self, argstr, i, res):
        return self.item(argstr, i, res)

    def item(self, argstr, i, res):
        return self.path(argstr, i, res)

    def blankNode(self, uri=None):
        return self._store.newBlankNode(self._context, uri, why=self._reason2)

    def path(self, argstr, i, res):
        """Parse the path production.
        """
        j = self.nodeOrLiteral(argstr, i, res)
        if j < 0:
            return j   # nope

        while argstr[j:j + 1] in "!^":   # no spaces, must follow exactly (?)
            ch = argstr[j:j + 1]
            subj = res.pop()
            obj = self.blankNode(uri=self.here(j))
            j = self.node(argstr, j + 1, res)
            if j < 0:
                self.BadSyntax(argstr, j,
                    "EOF found in middle of path syntax")
            pred = res.pop()
            if ch == "^":  # Reverse traverse
                self.makeStatement((self._context, pred, obj, subj))
            else:
                self.makeStatement((self._context, pred, subj, obj))
            res.append(obj)
        return j

    def anonymousNode(self, ln):
        """Remember or generate a term for one of these _: anonymous nodes"""
        term = self._anonymousNodes.get(ln, None)
        if term is not None:
            return term
        term = self._store.newBlankNode(self._context, why=self._reason2)
        self._anonymousNodes[ln] = term
        return term

    def node(self, argstr, i, res, subjectAlready=None):
        """Parse the <node> production.
        Space is now skipped once at the beginning
        instead of in multipe calls to self.skipSpace().
        """
        subj = subjectAlready

        j = self.skipSpace(argstr, i)
        if j < 0:
            return j  # eof
        i = j
        ch = argstr[i:i + 1]   # Quick 1-character checks first:

        if ch == "[":
            bnodeID = self.here(i)
            j = self.skipSpace(argstr, i + 1)
            if j < 0:
                self.BadSyntax(argstr, i,
                    "EOF after '['")
            # Hack for "is" binding name to anon node
            if argstr[j:j + 1] == "=":
                if self.turtle:
                    self.BadSyntax(argstr, j, "Found '[=' or '[ =' when in turtle mode.")
                i = j + 1
                objs = []
                j = self.objectList(argstr, i, objs)
                if j >= 0:
                    subj = objs[0]
                    if len(objs) > 1:
                        for obj in objs:
                            self.makeStatement((self._context,
                                                DAML_sameAs, subj, obj))
                    j = self.skipSpace(argstr, j)
                    if j < 0:
                        self.BadSyntax(argstr, i,
                            "EOF when objectList expected after [ = ")
                    if argstr[j:j + 1] == ";":
                        j = j + 1
                else:
                    self.BadSyntax(argstr, i,
                        "objectList expected after [= ")

            if subj is None:
                subj = self.blankNode(uri=bnodeID)

            i = self.property_list(argstr, j, subj)
            if i < 0:
                self.BadSyntax(argstr, j,
                    "property_list expected")

            j = self.skipSpace(argstr, i)
            if j < 0:
                self.BadSyntax(argstr, i,
                    "EOF when ']' expected after [ <propertyList>")
            if argstr[j:j + 1] != "]":
                self.BadSyntax(argstr, j,
                    "']' expected")
            res.append(subj)
            return j + 1

        if not self.turtle and ch == "{":
            # if self.turtle:
            #     self.BadSyntax(argstr, i,
            #                     "found '{' while in Turtle mode, Formulas not supported!")
            ch2 = argstr[i + 1:i + 2]
            if ch2 == '$':
                # a set
                i += 1
                j = i + 1
                List = []
                first_run = True
                while 1:
                    i = self.skipSpace(argstr, j)
                    if i < 0:
                        self.BadSyntax(argstr, i,
                            "needed '$}', found end.")
                    if argstr[i:i + 2] == '$}':
                        j = i + 2
                        break

                    if not first_run:
                        if argstr[i:i + 1] == ',':
                            i += 1
                        else:
                            self.BadSyntax(
                                argstr, i, "expected: ','")
                    else:
                        first_run = False

                    item = []
                    j = self.item(
                        argstr, i, item)  # @@@@@ should be path, was object
                    if j < 0:
                        self.BadSyntax(argstr, i,
                            "expected item in set or '$}'")
                    List.append(self._store.intern(item[0]))
                res.append(self._store.newSet(List, self._context))
                return j
            else:
                # parse a formula
                j = i + 1
                oldParentContext = self._parentContext
                self._parentContext = self._context
                parentAnonymousNodes = self._anonymousNodes
                grandParentVariables = self._parentVariables
                self._parentVariables = self._variables
                self._anonymousNodes = {}
                self._variables = self._variables.copy()
                reason2 = self._reason2
                self._reason2 = becauseSubexpression
                if subj is None:
                    subj = self._store.newFormula()
                self._context = subj

                while 1:
                    i = self.skipSpace(argstr, j)
                    if i < 0:
                        self.BadSyntax(
                            argstr, i, "needed '}', found end.")

                    if argstr[i:i + 1] == "}":
                        j = i + 1
                        break

                    j = self.directiveOrStatement(argstr, i)
                    if j < 0:
                        self.BadSyntax(
                            argstr, i, "expected statement or '}'")

                self._anonymousNodes = parentAnonymousNodes
                self._variables = self._parentVariables
                self._parentVariables = grandParentVariables
                self._context = self._parentContext
                self._reason2 = reason2
                self._parentContext = oldParentContext
                res.append(subj.close())    # No use until closed
                return j

        if ch == "(":
            thing_type = self._store.newList
            ch2 = argstr[i + 1:i + 2]
            if ch2 == '$':
                thing_type = self._store.newSet
                i += 1
            j = i + 1

            List = []
            while 1:
                i = self.skipSpace(argstr, j)
                if i < 0:
                    self.BadSyntax(
                        argstr, i, "needed ')', found end.")
                if argstr[i:i + 1] == ')':
                    j = i + 1
                    break

                item = []
                j = self.item(
                    argstr, i, item)  # @@@@@ should be path, was object
                if j < 0:
                    self.BadSyntax(argstr, i,
                        "expected item in list or ')'")
                List.append(self._store.intern(item[0]))
            res.append(thing_type(List, self._context))
            return j

        j = self.tok('this', argstr, i)    # This context
        if j >= 0:
            self.BadSyntax(argstr, i,
                "Keyword 'this' was ancient N3. Now use " +
                "@forSome and @forAll keywords.")

         # booleans
        j = self.tok('true', argstr, i)
        if j >= 0:
            res.append(True)
            return j
        j = self.tok('false', argstr, i)
        if j >= 0:
            res.append(False)
            return j

        if subj is None:  # If this can be a named node, then check for a name.
            j = self.uri_ref2(argstr, i, res)
            if j >= 0:
                return j

        return -1

    def property_list(self, argstr, i, subj):
        """Parse property list
        Leaves the terminating punctuation in the buffer
        """
        while 1:
            while 1: # skip repeat ;
                j = self.skipSpace(argstr, i)
                if j < 0:
                    self.BadSyntax(argstr, i,
                                   "EOF found when expected verb in property list")
                if argstr[j]!=';': break
                i = j+1

            if argstr[j:j + 2] == ":-":
                if self.turtle:
                    self.BadSyntax(argstr, j, "Found in ':-' in Turtle mode")
                i = j + 2
                res = []
                j = self.node(argstr, i, res, subj)
                if j < 0:
                    self.BadSyntax(argstr, i,
                        "bad {} or () or [] node after :- ")
                i = j
                continue
            i = j
            v = []
            j = self.verb(argstr, i, v)
            if j <= 0:
                return i  # void but valid

            objs = []
            i = self.objectList(argstr, j, objs)
            if i < 0:
                self.BadSyntax(argstr, j,
                    "objectList expected")
            for obj in objs:
                dira, sym = v[0]
                if dira == '->':
                    self.makeStatement((self._context, sym, subj, obj))
                else:
                    self.makeStatement((self._context, sym, obj, subj))

            j = self.skipSpace(argstr, i)
            if j < 0:
                self.BadSyntax(argstr, j,
                    "EOF found in list of objects")
            if argstr[i:i + 1] != ";":
                return i
            i = i + 1  # skip semicolon and continue

    def commaSeparatedList(self, argstr, j, res, what):
        """return value: -1 bad syntax; >1 new position in argstr
        res has things found appended
        """
        i = self.skipSpace(argstr, j)
        if i < 0:
            self.BadSyntax(argstr, i,
                "EOF found expecting comma sep list")
        if argstr[i] == ".":
            return j   # empty list is OK
        i = what(argstr, i, res)
        if i < 0:
            return -1

        while 1:
            j = self.skipSpace(argstr, i)
            if j < 0:
                return j  # eof
            ch = argstr[j:j + 1]
            if ch != ",":
                if ch != ".":
                    return -1
                return j     # Found  but not swallowed "."
            i = what(argstr, j + 1, res)
            if i < 0:
                self.BadSyntax(argstr, i,
                    "bad list content")

    def objectList(self, argstr, i, res):
        i = self.object(argstr, i, res)
        if i < 0:
            return -1
        while 1:
            j = self.skipSpace(argstr, i)
            if j < 0:
                self.BadSyntax(argstr, j,
                    "EOF found after object")
            if argstr[j:j + 1] != ",":
                return j     # Found something else!
            i = self.object(argstr, j + 1, res)
            if i < 0:
                return i

    def checkDot(self, argstr, i):
        j = self.skipSpace(argstr, i)
        if j < 0:
            return j  # eof
        if argstr[j:j + 1] == ".":
            return j + 1   # skip
        if argstr[j:j + 1] == "}":
            return j      # don't skip it
        if argstr[j:j + 1] == "]":
            return j
        self.BadSyntax(argstr, j,
            "expected '.' or '}' or ']' at end of statement")

    def uri_ref2(self, argstr, i, res):
        """Generate uri from n3 representation.

        Note that the RDF convention of directly concatenating
        NS and local name is now used though I prefer inserting a '#'
        to make the namesapces look more like what XML folks expect.
        """
        qn = []
        j = self.qname(argstr, i, qn)
        if j >= 0:
            pfx, ln = qn[0]
            if pfx is None:
                assert 0, "not used?"
                ns = self._baseURI + ADDED_HASH
            else:
                try:
                    ns = self._bindings[pfx]
                except KeyError:
                    if pfx == "_":  # Magic prefix 2001/05/30, can be changed
                        res.append(self.anonymousNode(ln))
                        return j
                    if not self.turtle and pfx == "":
                        ns = join(self._baseURI or "", "#")
                    else:
                        self.BadSyntax(argstr, i,
                                       "Prefix \"%s:\" not bound" % (pfx))
            symb = self._store.newSymbol(ns + ln)
            if symb in self._variables:
                res.append(self._variables[symb])
            else:
                res.append(symb)  # @@@ "#" CONVENTION
            return j

        i = self.skipSpace(argstr, i)
        if i < 0:
            return -1

        if argstr[i] == "?":
            v = []
            j = self.variable(argstr, i, v)
            if j > 0:  # Forget varibles as a class, only in context.
                res.append(v[0])
                return j
            return -1

        elif argstr[i] == "<":
            i = i + 1
            st = i
            while i < len(argstr):
                if argstr[i] == ">":
                    uref = argstr[st:i]  # the join should dealt with "":

                    # expand unicode escapes
                    uref = unicodeEscape8.sub(unicodeExpand, uref)
                    uref = unicodeEscape4.sub(unicodeExpand, uref)

                    if self._baseURI:
                        uref = join(self._baseURI, uref)  # was: uripath.join
                    else:
                        assert ":" in uref, \
                            "With no base URI, cannot deal with relative URIs"
                    if argstr[i - 1:i] == "#" and not uref[-1:] == "#":
                        uref = uref + \
                            "#"  # She meant it! Weirdness in urlparse?
                    symb = self._store.newSymbol(uref)
                    if symb in self._variables:
                        res.append(self._variables[symb])
                    else:
                        res.append(symb)
                    return i + 1
                i = i + 1
            self.BadSyntax(argstr, j,
                "unterminated URI reference")

        elif self.keywordsSet:
            v = []
            j = self.bareWord(argstr, i, v)
            if j < 0:
                return -1       # Forget varibles as a class, only in context.
            if v[0] in self.keywords:
                self.BadSyntax(argstr, i,
                    'Keyword "%s" not allowed here.' % v[0])
            res.append(self._store.newSymbol(self._bindings[""] + v[0]))
            return j
        else:
            return -1

    def skipSpace(self, argstr, i):
        """Skip white space, newlines and comments.
        return -1 if EOF, else position of first non-ws character"""
        while 1:
            m = eol.match(argstr, i)
            if m is None:
                break
            self.lines = self.lines + 1
            i = m.end()    # Point to first character unmatched
            self.startOfLine = i
        m = ws.match(argstr, i)
        if m is not None:
            i = m.end()
        m = eof.match(argstr, i)
        if m is not None:
            return -1
        return i

    def variable(self, argstr, i, res):
        """     ?abc -> variable(:abc)
        """

        j = self.skipSpace(argstr, i)
        if j < 0:
            return -1

        if argstr[j:j + 1] != "?":
            return -1
        j = j + 1
        i = j
        if argstr[j] in "0123456789-":
            self.BadSyntax(argstr, j,
                "Varible name can't start with '%s'" % argstr[j])
        while i < len(argstr) and argstr[i] not in _notKeywordsChars:
            i = i + 1
        if self._parentContext is None:
            varURI = self._store.newSymbol(self._baseURI + "#" + argstr[j:i])
            if varURI not in self._variables:
                self._variables[varURI] = self._context.newUniversal(
                    varURI, why=self._reason2)
            res.append(self._variables[varURI])
            return i
             # @@ was:
             # self.BadSyntax(argstr, j,
             #     "Can't use ?xxx syntax for variable in outermost level: %s"
             #     % argstr[j-1:i])
        varURI = self._store.newSymbol(self._baseURI + "#" + argstr[j:i])
        if varURI not in self._parentVariables:
            self._parentVariables[varURI] = self._parentContext.newUniversal(
                varURI, why=self._reason2)
        res.append(self._parentVariables[varURI])
        return i

    def bareWord(self, argstr, i, res):
        """     abc -> :abc
        """
        j = self.skipSpace(argstr, i)
        if j < 0:
            return -1

        if argstr[j] in "0123456789-" or argstr[j] in _notKeywordsChars:
            return -1
        i = j
        while i < len(argstr) and argstr[i] not in _notKeywordsChars:
            i = i + 1
        res.append(argstr[j:i])
        return i

    def qname(self, argstr, i, res):
        """
        xyz:def -> ('xyz', 'def')
        If not in keywords and keywordsSet: def -> ('', 'def')
        :def -> ('', 'def')
        """

        i = self.skipSpace(argstr, i)
        if i < 0:
            return -1

        c = argstr[i]
        if c in "0123456789-+.":
            return -1
        if c not in _notNameChars:
            ln = c
            i = i + 1
            while i < len(argstr):
                c = argstr[i]
                if c not in _notNameChars:
                    ln = ln + c
                    i = i + 1
                else:
                    break

            if argstr[i - 1] == ".":  # qname cannot end with "."
                ln = ln[:-1]
                if not ln: return -1
                i -= 1

        else:  # First character is non-alpha
            ln = ''    # Was:  None - TBL (why? useful?)

        if i < len(argstr) and argstr[i] == ':':
            pfx = ln
            # bnodes names have different rules
            if pfx == '_':
                allowedChars = _notNameChars
            else:
                allowedChars = _notQNameChars

            i = i + 1
            lastslash = False
            # start = i # TODO first char .
            ln = ''
            while i < len(argstr):
                c = argstr[i]
                if not lastslash and c == '\\':
                    lastslash = True
                    i += 1

                elif lastslash or c not in allowedChars:

                    if lastslash:
                        if c not in escapeChars:
                            raise BadSyntax(self._thisDoc, self.line, argstr, i,
                                            "illegal escape "+c)
                    elif c=='%':
                        if argstr[i+1] not in hexChars or argstr[i+2] not in hexChars:
                            raise BadSyntax(self._thisDoc, self.line, argstr, i,
                                            "illegal hex escape "+c)

                    ln = ln + c
                    i = i + 1
                    lastslash = False
                else:
                    break

            if lastslash:
                raise BadSyntax(
                    self._thisDoc, self.line, argstr, i,
                    "qname cannot end with \\")


            if argstr[i-1]=='.':
                # localname cannot end in .
                ln = ln[:-1]
                if not ln: return -1
                i -= 1

            res.append((pfx, ln))
            return i

        else:   # delimiter was not ":"
            if ln and self.keywordsSet and ln not in self.keywords:
                res.append(('', ln))
                return i
            return -1

    def object(self, argstr, i, res):
        j = self.subject(argstr, i, res)
        if j >= 0:
            return j
        else:
            j = self.skipSpace(argstr, i)
            if j < 0:
                return -1
            else:
                i = j

            if argstr[i] in self.string_delimiters:
                if argstr[i:i + 3] == argstr[i] * 3:
                    delim = argstr[i] * 3
                else:
                    delim = argstr[i]
                i = i + len(delim)

                j, s = self.strconst(argstr, i, delim)

                res.append(self._store.newLiteral(s))
                return j
            else:
                return -1

    def nodeOrLiteral(self, argstr, i, res):
        j = self.node(argstr, i, res)
        startline = self.lines  # Remember where for error messages
        if j >= 0:
            return j
        else:
            j = self.skipSpace(argstr, i)
            if j < 0:
                return -1
            else:
                i = j

            ch = argstr[i]
            if ch in "-+0987654321.":
                m = exponent_syntax.match(argstr, i)
                if m:
                    j = m.end()
                    res.append(float(argstr[i:j]))
                    return j

                m = decimal_syntax.match(argstr, i)
                if m:
                    j = m.end()
                    res.append(Decimal(argstr[i:j]))
                    return j

                m = integer_syntax.match(argstr, i)
                if m:
                    j = m.end()
                    res.append(long(argstr[i:j]))
                    return j

                # return -1  ## or fall through?

            if argstr[i] in self.string_delimiters:
                if argstr[i:i + 3] == argstr[i] * 3:
                    delim = argstr[i] * 3
                else:
                    delim = argstr[i]
                i = i + len(delim)

                dt = None
                j, s = self.strconst(argstr, i, delim)
                lang = None
                if argstr[j:j + 1] == "@":   # Language?
                    m = langcode.match(argstr, j + 1)
                    if m is None:
                        raise BadSyntax(
                            self._thisDoc, startline, argstr, i,
                            "Bad language code syntax on string " +
                            "literal, after @")
                    i = m.end()
                    lang = argstr[j + 1:i]
                    j = i
                if argstr[j:j + 2] == "^^":
                    res2 = []
                    j = self.uri_ref2(argstr, j + 2, res2)  # Read datatype URI
                    dt = res2[0]
                res.append(self._store.newLiteral(s, dt, lang))
                return j
            else:
                return -1

    def uriOf(self, sym):
        if isinstance(sym, types.TupleType):
            return sym[1]  # old system for --pipe
         # return sym.uriref()  # cwm api
        return sym

    def strconst(self, argstr, i, delim):
        """parse an N3 string constant delimited by delim.
        return index, val
        """
        delim1 = delim[0]
        delim2, delim3, delim4, delim5 = delim1 * 2, delim1 * 3, delim1 * 4, delim1 * 5

        j = i
        ustr = u""    # Empty unicode string
        startline = self.lines  # Remember where for error messages
        while j < len(argstr):
            if argstr[j] == delim1:
                if delim == delim1:  # done when delim is " or '
                    i = j + 1
                    return i, ustr
                if delim == delim3:  # done when delim is """ or ''' and, respectively ...
                    if argstr[j:j + 5] == delim5:  # ... we have "" or '' before
                        i = j + 5
                        ustr = ustr + delim2
                        return i, ustr
                    if argstr[j:j + 4] == delim4:  # ... we have " or ' before
                        i = j + 4
                        ustr = ustr + delim1
                        return i, ustr
                    if argstr[j:j + 3] == delim3:  # current " or ' is part of delim
                        i = j + 3
                        return i, ustr

                     # we are inside of the string and current char is " or '
                    j = j + 1
                    ustr = ustr + delim1
                    continue

            m = interesting.search(argstr, j)   # was argstr[j:].
             # Note for pos param to work, MUST be compiled  ... re bug?
            assert m, "Quote expected in string at ^ in %s^%s" % (
                argstr[j - 20:j], argstr[j:j + 20])  # at least need a quote

            i = m.start()
            try:
                ustr = ustr + argstr[j:i]
            except UnicodeError:
                err = ""
                for c in argstr[j:i]:
                    err = err + (" %02x" % ord(c))
                streason = sys.exc_info()[1].__str__()
                raise BadSyntax(
                    self._thisDoc, startline, argstr, j,
                    "Unicode error appending characters" +
                    " %s to string, because\n\t%s"
                    % (err, streason))

             # print "@@@ i = ",i, " j=",j, "m.end=", m.end()

            ch = argstr[i]
            if ch == delim1:
                j = i
                continue
            elif ch in ('"', "'") and ch != delim1:
                ustr = ustr + ch
                j = i + 1
                continue
            elif ch in "\r\n":
                if delim == delim1:
                    raise BadSyntax(
                        self._thisDoc, startline, argstr, i,
                        "newline found in string literal")
                self.lines = self.lines + 1
                ustr = ustr + ch
                j = i + 1
                self.startOfLine = j

            elif ch == "\\":
                j = i + 1
                ch = argstr[j:j + 1]   # Will be empty if string ends
                if not ch:
                    raise BadSyntax(
                        self._thisDoc, startline, argstr, i,
                        "unterminated string literal (2)")
                k = 'abfrtvn\\"'.find(ch)
                if k >= 0:
                    uch = '\a\b\f\r\t\v\n\\"'[k]
                    ustr = ustr + uch
                    j = j + 1
                elif ch == "u":
                    j, ch = self.uEscape(argstr, j + 1, startline)
                    ustr = ustr + ch
                elif ch == "U":
                    j, ch = self.UEscape(argstr, j + 1, startline)
                    ustr = ustr + ch
                else:
                    self.BadSyntax(argstr, i,
                        "bad escape")

        self.BadSyntax(argstr, i,
                        "unterminated string literal")

    def _unicodeEscape(self, argstr, i, startline, reg, n):
        if len(argstr)<i+n:
            raise BadSyntax(
                    self._thisDoc, startline, argstr, i,
                    "unterminated string literal(3)")
        try:
            return i+n, reg.sub(unicodeExpand, '\\u'+argstr[i:i+n])
        except:
            raise BadSyntax(
                self._thisDoc, startline, argstr, i,
                "bad string literal hex escape: "+argstr[i:i+n])

    def uEscape(self, argstr, i, startline):
        return self._unicodeEscape(argstr, i, startline, unicodeEscape4, 4)

    def UEscape(self, argstr, i, startline):
        return self._unicodeEscape(argstr, i, startline, unicodeEscape8, 8 )

    def BadSyntax(self, argstr, i, msg):
        raise BadSyntax(self._thisDoc, self.lines, argstr, i, msg)

# If we are going to do operators then they should generate
# [  is  operator:plus  of (  \1  \2 ) ]


class BadSyntax(SyntaxError):
    def __init__(self, uri, lines, argstr, i, why):
        self._str = argstr.encode(
            'utf-8')  # Better go back to strings for errors
        self._i = i
        self._why = why
        self.lines = lines
        self._uri = uri

    def __str__(self):
        argstr = self._str
        i = self._i
        st = 0
        if i > 60:
            pre = "..."
            st = i - 60
        else:
            pre = ""
        if len(argstr) - i > 60:
            post = "..."
        else:
            post = ""

        return 'at line %i of <%s>:\nBad syntax (%s) at ^ in:\n"%s%s^%s%s"' \
               % (self.lines + 1, self._uri, self._why, pre,
                  argstr[st:i], argstr[i:i + 60], post)

    @property
    def message(self):
        return str(self)



###############################################################################
class Formula(object):
    number = 0

    def __init__(self, parent):
        self.uuid = uuid4().hex
        self.counter = 0
        Formula.number += 1
        self.number = Formula.number
        self.existentials = {}
        self.universals = {}

        self.quotedgraph = QuotedGraph(
            store=parent.store, identifier=self.id())

    def __str__(self):
        return '_:Formula%s' % self.number

    def id(self):
        return BNode('_:Formula%s' % self.number)

    def newBlankNode(self, uri=None, why=None):
        if uri is None:
            self.counter += 1
            bn = BNode('f%sb%s' % (self.uuid, self.counter))
        else:
            bn = BNode(uri.split('#').pop().replace('_', 'b'))
        return bn

    def newUniversal(self, uri, why=None):
        return Variable(uri.split('#').pop())

    def declareExistential(self, x):
        self.existentials[x] = self.newBlankNode()

    def close(self):

        return self.quotedgraph


r_hibyte = re.compile(r'([\x80-\xff])')


class RDFSink(object):
    def __init__(self, graph):
        self.rootFormula = None
        self.counter = 0
        self.graph = graph

    def newFormula(self):
        assert self.graph.store.formula_aware
        f = Formula(self.graph)
        return f

    def newGraph(self, identifier):
        return Graph(self.graph.store, identifier)

    def newSymbol(self, *args):
        return URIRef(args[0])

    def newBlankNode(self, arg=None, uri=None, why=None):
        if isinstance(arg, Formula):
            return arg.newBlankNode(uri)
        elif isinstance(arg, Graph) or arg is None:
            self.counter += 1
            bn = BNode('n' + str(self.counter))
        else:
            bn = BNode(str(arg[0]).split('#').pop().replace('_', 'b'))
        return bn

    def newLiteral(self, s, dt, lang):
        if dt:
            return Literal(s, datatype=dt)
        else:
            return Literal(s, lang=lang)

    def newList(self, n, f):
        if not n:
            return self.newSymbol(
                'http://www.w3.org/1999/02/22-rdf-syntax-ns#nil'
            )

        a = self.newBlankNode(f)
        first = self.newSymbol(
            'http://www.w3.org/1999/02/22-rdf-syntax-ns#first'
        )
        rest = self.newSymbol(
            'http://www.w3.org/1999/02/22-rdf-syntax-ns#rest')
        self.makeStatement((f, first, a, n[0]))
        self.makeStatement((f, rest, a, self.newList(n[1:], f)))
        return a

    def newSet(self, *args):
        return set(args)

    def setDefaultNamespace(self, *args):
        return ':'.join(repr(n) for n in args)

    def makeStatement(self, quadruple, why=None):
        f, p, s, o = quadruple

        if hasattr(p, 'formula'):
            raise Exception("Formula used as predicate")

        s = self.normalise(f, s)
        p = self.normalise(f, p)
        o = self.normalise(f, o)

        if f == self.rootFormula:
             # print s, p, o, '.'
            self.graph.add((s, p, o))
        elif isinstance(f, Formula):
            f.quotedgraph.add((s, p, o))
        else:
            f.add((s,p,o))

         # return str(quadruple)

    def normalise(self, f, n):
        if isinstance(n, tuple):
            return URIRef(unicode(n[1]))

        if isinstance(n, bool):
            s = Literal(str(n).lower(), datatype=BOOLEAN_DATATYPE)
            return s

        if isinstance(n, int) or isinstance(n, long):
            s = Literal(unicode(n), datatype=INTEGER_DATATYPE)
            return s

        if isinstance(n, Decimal):
            value = str(n)
            if value == '-0':
                value = '0'
            s = Literal(value, datatype=DECIMAL_DATATYPE)
            return s

        if isinstance(n, float):
            s = Literal(str(n), datatype=DOUBLE_DATATYPE)
            return s

        if isinstance(f, Formula):
            if n in f.existentials:
                return f.existentials[n]

         # if isinstance(n, Var):
         #    if f.universals.has_key(n):
         #       return f.universals[n]
         #    f.universals[n] = f.newBlankNode()
         #    return f.universals[n]

        return n

    def intern(self, something):
        return something

    def bind(self, pfx, uri):
        pass  # print pfx, ':', uri

    def startDoc(self, formula):
        self.rootFormula = formula

    def endDoc(self, formula):
        pass


###################################################
#
#  Utilities
#


@py3compat.format_doctest_out
def hexify(ustr):
    """Use URL encoding to return an ASCII string
    corresponding to the given UTF8 string

    >>> hexify("http://example/a b")
    %(b)s'http://example/a%%20b'

    """
     # s1=ustr.encode('utf-8')
    s = ""
    for ch in ustr:   # .encode('utf-8'):
        if ord(ch) > 126 or ord(ch) < 33:
            ch = "%%%02X" % ord(ch)
        else:
            ch = "%c" % ord(ch)
        s = s + ch
    return b(s)


class TurtleParser(Parser):

    """
    An RDFLib parser for Turtle

    See http://www.w3.org/TR/turtle/
    """

    def __init__(self):
        pass

    def parse(self, source, graph, encoding="utf-8", turtle=True):

        if encoding not in [None, "utf-8"]:
            raise Exception(
                ("N3/Turtle files are always utf-8 encoded, ",
                 "I was passed: %s") % encoding)

        sink = RDFSink(graph)

        baseURI = graph.absolutize(
            source.getPublicId() or source.getSystemId() or "")
        p = SinkParser(sink, baseURI=baseURI, turtle=turtle)

        p.loadStream(source.getByteStream())

        for prefix, namespace in p._bindings.items():
            graph.bind(prefix, namespace)


class N3Parser(TurtleParser):

    """
    An RDFLib parser for Notation3

    See http://www.w3.org/DesignIssues/Notation3.html

    """

    def __init__(self):
        pass

    def parse(self, source, graph, encoding="utf-8"):
         # we're currently being handed a Graph, not a ConjunctiveGraph
        assert graph.store.context_aware  # is this implied by formula_aware
        assert graph.store.formula_aware

        conj_graph = ConjunctiveGraph(store=graph.store)
        conj_graph.default_context = graph  # TODO: CG __init__ should have a
                                            # default_context arg
         # TODO: update N3Processor so that it can use conj_graph as the sink
        conj_graph.namespace_manager = graph.namespace_manager

        TurtleParser.parse(self, source, conj_graph, encoding, turtle=False)


def _test():
    import doctest
    doctest.testmod()


# if __name__ == '__main__':
#    _test()

def main():
    g = ConjunctiveGraph()

    sink = RDFSink(g)
    base_uri = 'file://' + os.path.join(os.getcwd(), sys.argv[1])

    p = SinkParser(sink, baseURI=base_uri)
    p._bindings[''] = p._baseURI + '#'
    p.startDoc()

    f = open(sys.argv[1], 'rb')
    rdbytes = f.read()
    f.close()

    p.feed(rdbytes)
    p.endDoc()
    for t in g.quads((None, None, None)):

        print t

if __name__ == '__main__':
    main()

# ends

########NEW FILE########
__FILENAME__ = nquads
"""
This is a rdflib plugin for parsing NQuad files into Conjunctive
graphs that can be used and queried. The store that backs the graph
*must* be able to handle contexts.

>>> from rdflib import ConjunctiveGraph, URIRef, Namespace
>>> g = ConjunctiveGraph()
>>> data = open("test/nquads.rdflib/example.nquads", "rb")
>>> g.parse(data, format="nquads") # doctest:+ELLIPSIS
<Graph identifier=... (<class 'rdflib.graph.Graph'>)>
>>> assert len(g.store) == 449
>>> # There should be 16 separate contexts
>>> assert len([x for x in g.store.contexts()]) == 16
>>> # is the name of entity E10009 "Arco Publications"?
>>> #   (in graph http://bibliographica.org/entity/E10009)
>>> # Looking for:
>>> # <http://bibliographica.org/entity/E10009>
>>> #   <http://xmlns.com/foaf/0.1/name>
>>> #   "Arco Publications"
>>> #   <http://bibliographica.org/entity/E10009>
>>> s = URIRef("http://bibliographica.org/entity/E10009")
>>> FOAF = Namespace("http://xmlns.com/foaf/0.1/")
>>> assert(g.value(s, FOAF.name).eq("Arco Publications"))
"""

from codecs import getreader

from rdflib.py3compat import b

from rdflib import ConjunctiveGraph

# Build up from the NTriples parser:
from rdflib.plugins.parsers.ntriples import NTriplesParser
from rdflib.plugins.parsers.ntriples import ParseError
from rdflib.plugins.parsers.ntriples import r_tail
from rdflib.plugins.parsers.ntriples import r_wspace
from rdflib.plugins.parsers.ntriples import r_wspaces

__all__ = ['NQuadsParser']


class NQuadsParser(NTriplesParser):

    def parse(self, inputsource, sink, **kwargs):
        """Parse f as an N-Triples file."""
        assert sink.store.context_aware, ("NQuadsParser must be given"
                                          " a context aware store.")
        self.sink = ConjunctiveGraph(store=sink.store)

        source = inputsource.getByteStream()

        if not hasattr(source, 'read'):
            raise ParseError("Item to parse must be a file-like object.")

        source = getreader('utf-8')(source)

        self.file = source
        self.buffer = ''
        while True:
            self.line = __line = self.readline()
            if self.line is None:
                break
            try:
                self.parseline()
            except ParseError, msg:
                raise ParseError("Invalid line (%s):\n%r" % (msg, __line))

        return self.sink

    def parseline(self):
        self.eat(r_wspace)
        if (not self.line) or self.line.startswith(('#')):
            return  # The line is empty or a comment

        subject = self.subject()
        self.eat(r_wspace)

        predicate = self.predicate()
        self.eat(r_wspace)

        obj = self.object()
        self.eat(r_wspace)

        context = self.uriref() or self.nodeid()
        self.eat(r_tail)

        if self.line:
            raise ParseError("Trailing garbage")
        # Must have a context aware store - add on a normal Graph
        # discards anything where the ctx != graph.identifier
        self.sink.get_context(context).add((subject, predicate, obj))

########NEW FILE########
__FILENAME__ = nt
from rdflib.parser import Parser
from rdflib.plugins.parsers.ntriples import NTriplesParser

__all__ = ['NTSink', 'NTParser']


class NTSink(object):
    def __init__(self, graph):
        self.graph = graph

    def triple(self, s, p, o):
        self.graph.add((s, p, o))


class NTParser(Parser):
    """parser for the ntriples format, often stored with the .nt extension

    See http://www.w3.org/TR/rdf-testcases/#ntriples"""

    def __init__(self):
        super(NTParser, self).__init__()

    def parse(self, source, sink, baseURI=None):
        f = source.getByteStream()  # TODO getCharacterStream?
        parser = NTriplesParser(NTSink(sink))
        parser.parse(f)
        f.close()

########NEW FILE########
__FILENAME__ = ntriples
#!/usr/bin/env python
__doc__ = """
N-Triples Parser
License: GPL 2, W3C, BSD, or MIT
Author: Sean B. Palmer, inamidst.com
"""

import re
from rdflib.term import URIRef as URI
from rdflib.term import BNode as bNode
from rdflib.term import Literal

from rdflib.py3compat import cast_bytes, decodeUnicodeEscape, ascii

__all__ = ['unquote', 'uriquote', 'Sink', 'NTriplesParser']

uriref = r'<([^:]+:[^\s"<>]+)>'
literal = r'"([^"\\]*(?:\\.[^"\\]*)*)"'
litinfo = r'(?:@([a-z]+(?:-[a-zA-Z0-9]+)*)|\^\^' + uriref + r')?'

r_line = re.compile(r'([^\r\n]*)(?:\r\n|\r|\n)')
r_wspace = re.compile(r'[ \t]*')
r_wspaces = re.compile(r'[ \t]+')
r_tail = re.compile(r'[ \t]*\.[ \t]*(#.*)?')
r_uriref = re.compile(uriref)
r_nodeid = re.compile(r'_:([A-Za-z0-9]*)')
r_literal = re.compile(literal + litinfo)

bufsiz = 2048
validate = False


class Node(unicode):
    pass


class ParseError(Exception):
    pass


class Sink(object):
    def __init__(self):
        self.length = 0

    def triple(self, s, p, o):
        self.length += 1
        print (s, p, o)

quot = {'t': u'\t', 'n': u'\n', 'r': u'\r', '"': u'"', '\\':
        u'\\'}
r_safe = re.compile(r'([\x20\x21\x23-\x5B\x5D-\x7E]+)')
r_quot = re.compile(r'\\(t|n|r|"|\\)')
r_uniquot = re.compile(r'\\u([0-9A-F]{4})|\\U([0-9A-F]{8})')


def unquote(s):
    """Unquote an N-Triples string."""
    if not validate:

        if isinstance(s, unicode): # nquads
            s = decodeUnicodeEscape(s)
        else:
            s = s.decode('unicode-escape')

        return s
    else:
        result = []
        while s:
            m = r_safe.match(s)
            if m:
                s = s[m.end():]
                result.append(m.group(1))
                continue

            m = r_quot.match(s)
            if m:
                s = s[2:]
                result.append(quot[m.group(1)])
                continue

            m = r_uniquot.match(s)
            if m:
                s = s[m.end():]
                u, U = m.groups()
                codepoint = int(u or U, 16)
                if codepoint > 0x10FFFF:
                    raise ParseError("Disallowed codepoint: %08X" % codepoint)
                result.append(unichr(codepoint))
            elif s.startswith('\\'):
                raise ParseError("Illegal escape at: %s..." % s[:10])
            else:
                raise ParseError("Illegal literal character: %r" % s[0])
        return u''.join(result)

r_hibyte = re.compile(ur'([\x80-\xFF])')


def uriquote(uri):
    if not validate:
        return uri
    else:
        return r_hibyte.sub(
            lambda m: '%%%02X' % ord(m.group(1)), uri)


class NTriplesParser(object):
    """An N-Triples Parser.

    Usage::

          p = NTriplesParser(sink=MySink())
          sink = p.parse(f) # file; use parsestring for a string
    """

    _bnode_ids = {}

    def __init__(self, sink=None):
        if sink is not None:
            self.sink = sink
        else:
            self.sink = Sink()

    def parse(self, f):
        """Parse f as an N-Triples file."""
        if not hasattr(f, 'read'):
            raise ParseError("Item to parse must be a file-like object.")

        f = ascii(f)

        self.file = f
        self.buffer = ''
        while True:
            self.line = self.readline()
            if self.line is None:
                break
            try:
                self.parseline()
            except ParseError:
                raise ParseError("Invalid line: %r" % self.line)
        return self.sink

    def parsestring(self, s):
        """Parse s as an N-Triples string."""
        if not isinstance(s, basestring):
            raise ParseError("Item to parse must be a string instance.")
        try:
            from io import BytesIO
            assert BytesIO
        except ImportError:
            from cStringIO import StringIO as BytesIO
            assert BytesIO
        f = BytesIO()
        f.write(cast_bytes(s))
        f.seek(0)
        self.parse(f)

    def readline(self):
        """Read an N-Triples line from buffered input."""
        # N-Triples lines end in either CRLF, CR, or LF
        # Therefore, we can't just use f.readline()
        if not self.buffer:
            buffer = self.file.read(bufsiz)
            if not buffer:
                return None
            self.buffer = buffer

        while True:
            m = r_line.match(self.buffer)
            if m:  # the more likely prospect
                self.buffer = self.buffer[m.end():]
                return m.group(1)
            else:
                buffer = self.file.read(bufsiz)
                if not buffer and not self.buffer.isspace():
                    # Last line does not need to be terminated with a newline
                    buffer += "\n"
                elif not buffer:
                    return None
                self.buffer += buffer

    def parseline(self):
        self.eat(r_wspace)
        if (not self.line) or self.line.startswith('#'):
            return  # The line is empty or a comment

        subject = self.subject()
        self.eat(r_wspaces)

        predicate = self.predicate()
        self.eat(r_wspaces)

        object = self.object()
        self.eat(r_tail)

        if self.line:
            raise ParseError("Trailing garbage")
        self.sink.triple(subject, predicate, object)

    def peek(self, token):
        return self.line.startswith(token)

    def eat(self, pattern):
        m = pattern.match(self.line)
        if not m:  # @@ Why can't we get the original pattern?
            # print(dir(pattern))
            # print repr(self.line), type(self.line)
            raise ParseError("Failed to eat %s at %s" % (pattern, self.line))
        self.line = self.line[m.end():]
        return m

    def subject(self):
        # @@ Consider using dictionary cases
        subj = self.uriref() or self.nodeid()
        if not subj:
            raise ParseError("Subject must be uriref or nodeID")
        return subj

    def predicate(self):
        pred = self.uriref()
        if not pred:
            raise ParseError("Predicate must be uriref")
        return pred

    def object(self):
        objt = self.uriref() or self.nodeid() or self.literal()
        if objt is False:
            raise ParseError("Unrecognised object type")
        return objt

    def uriref(self):
        if self.peek('<'):
            uri = self.eat(r_uriref).group(1)
            uri = unquote(uri)
            uri = uriquote(uri)
            return URI(uri)
        return False

    def nodeid(self):
        if self.peek('_'):
            # Fix for https://github.com/RDFLib/rdflib/issues/204
            bnode_id = self.eat(r_nodeid).group(1)
            new_id = self._bnode_ids.get(bnode_id, None)
            if new_id is not None:
                # Re-map to id specfic to this doc
                return bNode(new_id)
            else:
                # Replace with freshly-generated document-specific BNode id
                bnode = bNode()
                # Store the mapping
                self._bnode_ids[bnode_id] = bnode
                return bnode
        return False

    def literal(self):
        if self.peek('"'):
            lit, lang, dtype = self.eat(r_literal).groups()
            if lang:
                lang = lang
            else:
                lang = None
            if dtype:
                dtype = dtype
            else:
                dtype = None
            if lang and dtype:
                raise ParseError("Can't have both a language and a datatype")
            lit = unquote(lit)
            return Literal(lit, lang, dtype)
        return False

# # Obsolete, unused
# def parseURI(uri):
#     import urllib
#     parser = NTriplesParser()
#     u = urllib.urlopen(uri)
#     sink = parser.parse(u)
#     u.close()
#     # for triple in sink:
#     #     print triple
#     print 'Length of input:', sink.length

########NEW FILE########
__FILENAME__ = microdata
# -*- coding: utf-8 -*-
"""

The core of the Microdata->RDF conversion, a more or less verbatim implementation of the
U{W3C IG Note<http://www.w3.org/TR/microdata-rdf/>}. Because the implementation was also used to check
the note itself, it tries to be fairly close to the text.


@organization: U{World Wide Web Consortium<http://www.w3.org>}
@author: U{Ivan Herman<a href="http://www.w3.org/People/Ivan/">}
@license: This software is available for use under the
U{W3C SOFTWARE NOTICE AND LICENSE<href="http://www.w3.org/Consortium/Legal/2002/copyright-software-20021231">}
"""

"""
$Id: microdata.py,v 1.4 2012/09/05 16:40:43 ivan Exp $
$Date: 2012/09/05 16:40:43 $

Added a reaction on the RDFaStopParsing exception: if raised while setting up the local execution context, parsing
is stopped (on the whole subtree)
"""

import sys
if sys.version_info[0] >= 3 :
	from urllib.parse import urlsplit, urlunsplit
else :	
	from urlparse import urlsplit, urlunsplit

from types import *

import rdflib
from rdflib	import URIRef
from rdflib	import Literal
from rdflib	import BNode
from rdflib	import Namespace
if rdflib.__version__ >= "3.0.0" :
	from rdflib	import Graph
	from rdflib	import RDF  as ns_rdf
	from rdflib	import RDFS as ns_rdfs
else :
	from rdflib.Graph	import Graph
	from rdflib.RDFS	import RDFSNS as ns_rdfs
	from rdflib.RDF		import RDFNS  as ns_rdf
	
ns_owl = Namespace("http://www.w3.org/2002/07/owl#")

from .registry import registry, vocab_names
from .utils	   import generate_RDF_collection, get_Literal, get_time_type
from .utils	   import get_lang_from_hierarchy, is_absolute_URI, generate_URI, fragment_escape

MD_VOCAB   = "http://www.w3.org/ns/md#"
RDFA_VOCAB = URIRef("http://www.w3.org/ns/rdfa#usesVocabulary")

from . import debug

# Existing predicate schemes
class PropertySchemes :
	vocabulary = "vocabulary"
	contextual = "contextual"
	
class ValueMethod :
	unordered = "unordered"
	list      = "list"

# ----------------------------------------------------------------------------

class Evaluation_Context :
	"""
	Evaluation context structure. See Section 4.1 of the U{W3C IG Note<http://www.w3.org/TR/microdata-rdf/>}for the details.
	
	@ivar current_type : an absolute URL for the current type, used when an item does not contain an item type
	@ivar memory: mapping from items to RDF subjects
	@type memory: dictionary
	@ivar current_name: an absolute URL for the in-scope name, used for generating URIs for properties of items without an item type
	@ivar current_vocabulary: an absolute URL for the current vocabulary, from the registry
	"""
	def __init__( self ) :
		self.current_type       = None
		self.memory             = {}
		self.current_name       = None
		self.current_vocabulary = None
		
	def get_memory( self, item ) :
		"""
		Get the memory content (ie, RDF subject) for 'item', or None if not stored yet
		@param item: an 'item', in microdata terminology
		@type item: DOM Element Node
		@return: None, or an RDF Subject (URIRef or BNode)
		"""
		if item in self.memory :
			return self.memory[item]
		else :
			return None
		
	def set_memory( self, item, subject ) :
		"""
		Set the memory content, ie, the subject, for 'item'.
		@param item: an 'item', in microdata terminology
		@type item: DOM Element Node
		@param subject: RDF Subject
		@type subject: URIRef or Blank Node
		"""
		self.memory[item] = subject
		
	def new_copy(self, itype) :
		"""
		During the generation algorithm a new copy of the current context has to be done with a new current type.
		
		At the moment, the content of memory is copied, ie, a fresh dictionary is created and the content copied over.
		Not clear whether that is necessary, though, maybe a simple reference is enough...
		@param itype : an absolute URL for the current type
		@return: a new evaluation context instance
		"""
		retval = Evaluation_Context()
		for k in self.memory :
			retval.memory[k] = self.memory[k]

		retval.current_type       = itype		
		retval.current_name       = self.current_name
		retval.current_vocabulary = self.current_vocabulary
		return retval
	
	def __str__(self) :
		retval = "Evaluation context:\n"
		retval += "  current type:       %s\n" % self.current_type
		retval += "  current name:       %s\n" % self.current_name
		retval += "  current vocabulary: %s\n" % self.current_vocabulary
		retval += "  memory:             %s\n" % self.memory
		retval += "----\n"
		return retval
		
class Microdata :
	"""
	This class encapsulates methods that are defined by the U{microdata spec<http://dev.w3.org/html5/md/Overview.html>},
	as opposed to the RDF conversion note.
	
	@ivar document: top of the DOM tree, as returned by the HTML5 parser
	@ivar base: the base URI of the Dom tree, either set from the outside or via a @base element
	"""
	def __init__( self, document, base = None) :
		"""
		@param document: top of the DOM tree, as returned by the HTML5 parser
		@param base: the base URI of the Dom tree, either set from the outside or via a @base element
		"""
		self.document = document
		
		#-----------------------------------------------------------------
		# set the document base, will be used to generate top level URIs
		self.base = None
		# handle the base element case for HTML
		for set_base in document.getElementsByTagName("base") :
			if set_base.hasAttribute("href") :
				# Yep, there is a local setting for base
				self.base = set_base.getAttribute("href")
				return
		# If got here, ie, if no local setting for base occurs, the input argument has it
		self.base = base	

	def get_top_level_items( self ) :
		"""
		A top level item is and element that has the @itemscope set, but no @itemtype. They have to
		be collected in pre-order and depth-first fashion.
		
		@return: list of items (ie, DOM Nodes)
		"""
		def collect_items( node ) :
			items = []
			for child in node.childNodes :
				if child.nodeType == node.ELEMENT_NODE :
					items += collect_items( child )
					
			if node.hasAttribute("itemscope") and not node.hasAttribute("itemprop") :
				# This is also a top level item
				items.append(node)
			
			return items
				
		return collect_items( self.document )
		
	def get_item_properties( self, item ) :
		"""
		Collect the item's properties, ie, all DOM descendent nodes with @itemprop until the subtree hits another
		@itemscope. @itemrefs are also added at this point.
		
		@param item: current item
		@type item: DOM Node
		@return: array of items, ie, DOM Nodes
		"""
		# go down the tree until another itemprop is hit, take care of the itemrefs, too; see the microdata doc
		# probably the ugliest stuff
		# returns a series of element nodes.
		# Is it worth filtering the ones with itemprop at that level???
		results = []
		memory  = [ item ]		
		pending = [ child for child in item.childNodes if child.nodeType == item.ELEMENT_NODE ]
		
		if item.hasAttribute("itemref") :
			for id in item.getAttribute("itemref").strip().split() :
				obj = self.getElementById(id)
				if obj != None : pending.append(obj)
		
		while len(pending) > 0 :
			current = pending.pop(0)
			if current in memory :
				# in general this raises an error; the same item cannot be there twice. In this case this is
				# simply ignored
				continue
			else :
				# this for the check above
				memory.append(current)
			
			# @itemscope is the barrier...
			if not current.hasAttribute("itemscope") :
				pending = [ child for child in current.childNodes if child.nodeType == child.ELEMENT_NODE ] + pending

			if current.hasAttribute("itemprop") and current.getAttribute("itemprop").strip() != "" :
				results.append(current)
				
		return results
	
	def getElementById(self, id) :
		"""This is a method defined for DOM 2 HTML, but the HTML5 parser does not seem to define it. Oh well...
		@param id: value of an @id attribute to look for
		@return: array of nodes whose @id attribute matches C{id} (formally, there should be only one...)
		"""
		def collect_ids( node ) :
			ids = []
			for child in node.childNodes :
				if child.nodeType == node.ELEMENT_NODE :
					ids += collect_ids( child )
					
			if node.hasAttribute("id") and node.getAttribute("id") == id :
				# This is also a top level item
				ids.append(node)
			
			return ids
		
		ids = collect_ids(self.document)
		if len(ids) > 0 :
			return ids[0]
		else :
			return None
				
class MicrodataConversion(Microdata) :
	"""
	Top level class encapsulating the conversion algorithms as described in the W3C note.
	
	@ivar graph: an RDF graph; an RDFLib Graph
	@type graph: RDFLib Graph
	@ivar document: top of the DOM tree, as returned by the HTML5 parser
	@ivar ns_md: the Namespace for the microdata vocabulary
	@ivar base: the base of the Dom tree, either set from the outside or via a @base element
	"""
	def __init__( self, document, graph, base = None, vocab_expansion = False, vocab_cache = True  ) :
		"""
		@param graph: an RDF graph; an RDFLib Graph
		@type graph: RDFLib Graph
		@param document: top of the DOM tree, as returned by the HTML5 parser
		@keyword base: the base of the Dom tree, either set from the outside or via a @base element
		@keyword vocab_expansion: whether vocab expansion should be performed or not
		@type vocab_expansion: Boolean
		@keyword vocab_cache: if vocabulary expansion is done, then perform caching of the vocabulary data
		@type vocab_cache: Boolean
		"""
		Microdata.__init__(self, document, base)
		self.vocab_expansion   = vocab_expansion
		self.vocab_cache       = vocab_cache
		self.graph             = graph
		self.ns_md             = Namespace( MD_VOCAB )
		self.graph.bind( "md",MD_VOCAB )
		self.vocabularies_used = False

		# Get the vocabularies defined in the registry bound to proper names, if any...

		def _use_rdfa_context () :
			try :
				from ..pyRdfa.initialcontext import initial_context
			except :
				from pyRdfa.initialcontext import initial_context
			retval = {}
			vocabs = initial_context["http://www.w3.org/2011/rdfa-context/rdfa-1.1"].ns
			for prefix in list(vocabs.keys()) :
				uri = vocabs[prefix]				
				if uri not in vocab_names and uri not in registry : retval[uri] = prefix
			return retval
				
		for vocab in registry :
			if vocab in vocab_names :
				self.graph.bind( vocab_names[vocab],vocab )
			else :
				hvocab = vocab + '#'
				if hvocab in vocab_names :
					self.graph.bind( vocab_names[hvocab],hvocab )
					
		# Add the prefixes defined in the RDFa initial context to improve the outlook of the output
		# I put this into a try: except: in case the pyRdfa package is not available...
		try :
			try :
				from ..pyRdfa.initialcontext import initial_context
			except :
				from pyRdfa.initialcontext import initial_context
			vocabs = initial_context["http://www.w3.org/2011/rdfa-context/rdfa-1.1"].ns
			for prefix in list(vocabs.keys()) :
				uri = vocabs[prefix]
				if uri not in registry :
					# if it is in the registry, then it may have needed some special microdata massage...
					self.graph.bind( prefix,uri )
		except :
			pass
		
	def convert( self ) :
		"""
		Top level entry to convert and generate all the triples. It finds the top level items,
		and generates triples for each of them; additionally, it generates a top level entry point
		to the items from base in the form of an RDF list.
		"""
		item_list = []
		for top_level_item in self.get_top_level_items() :
			item_list.append( self.generate_triples(top_level_item, Evaluation_Context()) )
		list = generate_RDF_collection( self.graph, item_list )
		self.graph.add( (URIRef(self.base),self.ns_md["item"],list) )
		
		# If the vocab expansion is also switched on, this is the time to do it.

		# This is the version with my current proposal: the basic expansion is always there;
		# the follow-your-nose inclusion of vocabulary is optional
		if self.vocabularies_used :
			try :
				try :
					from ..pyRdfa.rdfs.process import MiniOWL, process_rdfa_sem
					from ..pyRdfa.options      import Options
				except :
					from pyRdfa.rdfs.process import MiniOWL, process_rdfa_sem
					from pyRdfa.options      import Options
				# if we did not get here, the pyRdfa package could not be
				# imported. Too bad, but life should go on in the except branch...
				if self.vocab_expansion :
					# This is the full deal
					options = Options(vocab_expansion = self.vocab_expansion, vocab_cache = self.vocab_cache)
					process_rdfa_sem(self.graph, options)
				else :
					MiniOWL(self.graph).closure()
			except :
				pass

	def generate_triples( self, item, context ) :
		"""
		Generate the triples for a specific item. See the W3C Note for the details.
		
		@param item: the DOM Node for the specific item
		@type item: DOM Node
		@param context: an instance of an evaluation context
		@type context: L{Evaluation_Context}
		@return: a URIRef or a BNode for the (RDF) subject
		"""
		# Step 1,2: if the subject has to be set, store it in memory
		subject = context.get_memory( item )
		if subject == None :
			# nop, there is no subject set. If there is a valid @itemid, that carries it
			if item.hasAttribute("itemid") and is_absolute_URI( item.getAttribute("itemid") ):
				subject = URIRef( item.getAttribute("itemid").strip() )
			else :
				subject = BNode()
			context.set_memory( item, subject )
			
		# Step 3: set the type triples if any
		types = []
		if item.hasAttribute("itemtype") :
			types = item.getAttribute("itemtype").strip().split()
			for t in types :
				if is_absolute_URI( t ) :
					self.graph.add( (subject, ns_rdf["type"], URIRef(t)) )
		
		# Step 4, 5 and 6 to set the typing variable
		if len(types) == 0 :
			itype = None
		else :
			if is_absolute_URI(types[0]) :
				itype = types[0]
				context.current_name = None
			elif context.current_type != None :
				itype = context.current_type
			else :
				itype = None

		# Step 7, 8, 9: Check the registry for possible keys and set the vocab
		vocab = None
		if itype != None :
			for key in list(registry.keys()) :
				if itype.startswith(key) :
					# There is a predefined vocabulary for this type...
					vocab = key
					# Step 7: Issue an rdfa usesVocabulary triple
					self.graph.add( (URIRef(self.base), RDFA_VOCAB, URIRef(vocab)))
					self.vocabularies_used = True
					break
			# The registry has not set the vocabulary; has to be extracted from the type
			if vocab == None :
				parsed = urlsplit(itype)
				if parsed.fragment != "" :
					vocab = urlunsplit( (parsed.scheme,parsed.netloc,parsed.path,parsed.query,"") ) + '#'					
				elif parsed.path == "" and parsed.query == "" :
					vocab = itype
					if vocab[-1] != '/' : vocab += '/'
				else :
					vocab = itype.rsplit('/',1)[0] + '/'
		
		# Step 9: update vocab in the context
		if vocab != None :
			context.current_vocabulary = vocab
		elif item.hasAttribute("itemtype") :
			context.current_vocabulary = None

		# Step 10: set up a property list; this will be used to generate triples later.
		# each entry in the dictionary is an array of RDF objects
		property_list = {}
		
		# Step 11: Get the item properties and run a cycle on those
		for prop in self.get_item_properties(item) :
			for name in prop.getAttribute("itemprop").strip().split() :
				# 11.1.1. set a new context
				new_context = context.new_copy(itype)
				# 11.1.2, generate the URI for the property name, that will be the predicate
				# Also update the context
				new_context.current_name = predicate = self.generate_predicate_URI( name,new_context )
				# 11.1.3, generate the property value. The extra flag signals that the value is a new item
				# Note that 10.1.4 step is done in the method itself, ie, a recursion may occur there
				# if a new item is hit (in which case the return value is a RDF resource chaining to a subject)
				value  = self.get_property_value( prop, new_context )
				# 11.1.5, store all the values
				if predicate in property_list :
					property_list[predicate].append(value)
				else :
					property_list[predicate] = [ value ]
						
		# step 12: generate the triples
		for property in list(property_list.keys()) :
			self.generate_property_values( subject, URIRef(property), property_list[property], context )
			
		# Step 13: return the subject to the caller
		return subject
		
	def generate_predicate_URI( self, name, context ) :
		"""
		Generate a full URI for a predicate, using the type, the vocabulary, etc.
		
		For details of this entry, see Section 4.4
		@param name: name of the property, ie, what appears in @itemprop
		@param context: an instance of an evaluation context
		@type context: L{Evaluation_Context}
		"""
		if debug: print( "name: %s, %s" % (name,context) )
		
		# Step 1: absolute URI-s are fine, take them as they are
		if is_absolute_URI(name) : return name
		
		# Step 2: if type is none, that this is just used as a fragment
		# if not context.current_type  :
		if context.current_type == None and context.current_vocabulary == None  :
			if self.base[-1] == '#' :
				b = self.base[:-1]
			else :
				b = self.base
			return b + '#' + fragment_escape(name)

		#if context.current_type == None :
		#	return generate_URI( self.base, name )
		
		# Step 3: set the scheme
		try :
			if context.current_vocabulary in registry and "propertyURI" in registry[context.current_vocabulary] :
				scheme = registry[context.current_vocabulary]["propertyURI"]
			else :
				scheme = PropertySchemes.vocabulary
		except :
			# This is when the structure of the registry is broken
			scheme = PropertySchemes.vocabulary
			
		name = fragment_escape( name )
		if scheme == PropertySchemes.contextual :
			# Step 5.1
			s = context.current_name
			# s = context.current_type
			if s != None and s.startswith("http://www.w3.org/ns/md?type=") :
				# Step 5.2
				expandedURI = s + '.' + name
			else :
				# Step 5.3
				expandedURI =  "http://www.w3.org/ns/md?type=" + fragment_escape(context.current_type) + "&prop=" + name
		else :
			# Step 4
			if context.current_vocabulary[-1] == '#' or context.current_vocabulary[-1] == '/' :
				expandedURI =  context.current_vocabulary + name
			else :
				expandedURI =  context.current_vocabulary + '#' + name

		# see if there are subproperty/equivalentproperty relations
		try :
			vocab_mapping = registry[context.current_vocabulary]["properties"][name]
			# if we got that far, we may have some mappings

			expandedURIRef = URIRef(expandedURI)
			try :
				subpr = vocab_mapping["subPropertyOf"]
				if subpr != None :
					if isinstance(subpr,list) :
						for p in subpr :
							self.graph.add( (expandedURIRef, ns_rdfs["subPropertyOf"], URIRef(p)) )
					else :
						self.graph.add( (expandedURIRef, ns_rdfs["subPropertyOf"], URIRef(subpr)) )
			except :
				# Ok, no sub property
				pass
			try :
				subpr = vocab_mapping["equivalentProperty"]
				if subpr != None :
					if isinstance(subpr,list) :
						for p in subpr :
							self.graph.add( (expandedURIRef, ns_owl["equivalentProperty"], URIRef(p)) )
					else :
						self.graph.add( (expandedURIRef, ns_owl["equivalentProperty"], URIRef(subpr)) )
			except :
				# Ok, no sub property
				pass
		except :
			# no harm done, no extra vocabulary term
			pass


		return expandedURI
		
	def get_property_value(self, node, context) :
		"""
		Generate an RDF object, ie, the value of a property. Note that if this element contains
		an @itemscope, then a recursive call to L{MicrodataConversion.generate_triples} is done and the
		return value of that method (ie, the subject for the corresponding item) is return as an
		object.
		
		Otherwise, either URIRefs are created for <a>, <img>, etc, elements, or a Literal; the latter
		gets a time-related type for the <time> element.
		
		@param node: the DOM Node for which the property values should be generated
		@type node: DOM Node
		@param context: an instance of an evaluation context
		@type context: L{Evaluation_Context}
		@return: an RDF resource (URIRef, BNode, or Literal)
		"""
		URI_attrs = {
			"audio"		: "src",
			"embed"		: "src",
			"iframe"	: "src",
			"img"		: "src",
			"source"	: "src",
			"track"		: "src",
			"video"		: "src",
			"data"		: "src",
			"a"			: "href",
			"area"		: "href",
			"link"		: "href", 
			"object"	: "data" 
		}
		lang = get_lang_from_hierarchy( self.document, node )

		if node.hasAttribute("itemscope") :
			# THIS IS A RECURSION ENTRY POINT!
			return self.generate_triples( node, context )
			
		elif node.tagName in URI_attrs and node.hasAttribute(URI_attrs[node.tagName]) :
			return URIRef( generate_URI( self.base, node.getAttribute(URI_attrs[node.tagName]).strip() ) )
			
		elif node.tagName == "meta" and node.hasAttribute("content") :
			if lang :
				return Literal( node.getAttribute("content"), lang = lang )
			else :
				return Literal( node.getAttribute("content") )

		elif node.tagName == "time" and node.hasAttribute("datetime") :
			litval = node.getAttribute("datetime")
			dtype  = get_time_type(litval)
			if dtype :
				return Literal( litval, datatype = dtype )
			else :
				return Literal( litval )

		else :
			if lang :
				return Literal( get_Literal(node), lang = lang )
			else :
				return Literal( get_Literal(node) )
		
	def generate_property_values( self, subject, predicate, objects, context) :
		"""
		Generate the property values for for a specific subject and predicate. The context should specify whether
		the objects should be added in an RDF list or each triples individually.
		
		@param subject: RDF subject
		@type subject: RDFLib Node (URIRef or blank node)
		@param predicate: RDF predicate
		@type predicate: RDFLib URIRef
		@param objects: RDF objects
		@type objects: list of RDFLib nodes (URIRefs, Blank Nodes, or literals)
		@param context: evaluation context
		@type context: L{Evaluation_Context}
		"""
		# generate triples with a list, or a bunch of triples, depending on the context
		# The biggest complication is to find the method...
		method = ValueMethod.unordered
		superproperties = None
		
		# This is necessary because predicate is a URIRef, and I am not sure the comparisons would work well
		# to be tested, in fact...
		pred_key = "%s" % predicate
		for key in registry :
			if predicate.startswith(key) :
				# This the part of the registry corresponding to the predicate's vocabulary
				registry_object = registry[key]
				try :
					if "multipleValues" in registry_object : method = registry_object["multipleValues"]
					# The generic definition can be overwritten for a specific property. The simplest is to rely on a 'try'
					# with the right structure...
					try :
						method = registry_object["properties"][pred_key[len(key):]]["multipleValues"]
					except :
						pass
				except :
					pass
		
		if method == ValueMethod.unordered :
			for object in objects :
				self.graph.add( (subject, predicate, object) )
		else :
			self.graph.add( (subject,predicate,generate_RDF_collection( self.graph, objects )) )
		
						
				
					
		


########NEW FILE########
__FILENAME__ = registry
# -*- coding: utf-8 -*-
"""

Hardcoded version of the current microdata->RDF registry. There is also a local registry to include some test cases.
Finally, there is a local dictionary for prefix mapping for the registry items; these are the preferred prefixes
for those vocabularies, and are used to make the output nicer.

@organization: U{World Wide Web Consortium<http://www.w3.org>}
@author: U{Ivan Herman<a href="http://www.w3.org/People/Ivan/">}
@license: This software is available for use under the
U{W3C SOFTWARE NOTICE AND LICENSE<href="http://www.w3.org/Consortium/Legal/2002/copyright-software-20021231">}
"""

"""
$Id: registry.py,v 1.5 2012/09/05 16:40:43 ivan Exp $
$Date: 2012/09/05 16:40:43 $
"""

import sys
(py_v_major, py_v_minor, py_v_micro, py_v_final, py_v_serial) = sys.version_info

# To be added soon:
      # "Class"              : {"subPropertyOf" : "http://www.w3.org/2000/01/rdf-schema#Class"},
      # "Property"           : {"subPropertyOf" : "http://www.w3.org/1999/02/22-rdf-syntax-ns#Property"}

_registry = """
{
  "http://schema.org/": {
    "propertyURI":    "vocabulary",
    "multipleValues": "unordered",
    "properties": {
      "additionalType": {"subPropertyOf": "http://www.w3.org/1999/02/22-rdf-syntax-ns#type"},
      "blogPosts": {"multipleValues": "list"},
      "breadcrumb": {"multipleValues": "list"},
      "byArtist": {"multipleValues": "list"},
      "creator": {"multipleValues": "list"},
      "episode": {"multipleValues": "list"},
      "episodes": {"multipleValues": "list"},
      "event": {"multipleValues": "list"},
      "events": {"multipleValues": "list"},
      "founder": {"multipleValues": "list"},
      "founders": {"multipleValues": "list"},
      "itemListElement": {"multipleValues": "list"},
      "musicGroupMember": {"multipleValues": "list"},
      "performerIn": {"multipleValues": "list"},
      "actor": {"multipleValues": "list"},
      "actors": {"multipleValues": "list"},
      "performer": {"multipleValues": "list"},
      "performers": {"multipleValues": "list"},
      "producer": {"multipleValues": "list"},
      "recipeInstructions": {"multipleValues": "list"},
      "season": {"multipleValues": "list"},
      "seasons": {"multipleValues": "list"},
      "subEvent": {"multipleValues": "list"},
      "subEvents": {"multipleValues": "list"},
      "track": {"multipleValues": "list"},
      "tracks": {"multipleValues": "list"}
    }
  },
  "http://microformats.org/profile/hcard": {
    "propertyURI":    "vocabulary",
    "multipleValues": "unordered"
  },
  "http://microformats.org/profile/hcalendar#": {
    "propertyURI":    "vocabulary",
    "multipleValues": "unordered",
    "properties": {
      "categories": {"multipleValues": "list"}
    }
  }
}
"""

vocab_names = {
  "http://schema.org/"                         : "schema",
  "http://xmlns.com/foaf/0.1/"                 : "foaf",
  "http://microformats.org/profile/hcard#"     : "hcard",
  "http://microformats.org/profile/hcalendar#" : "hcalendar"
}

# This is the local version, added mainly for testing
_myRegistry = """
{
  "http://vocabulary.list/": {
    "propertyURI":    "vocabulary",
    "multipleValues": "list",
    "properties": {
      "list": {"multipleValues": "list"},
      "typed": {"datatype": "http://typed"}
    }
  },
  "http://vocabulary.unordered/": {
    "propertyURI":    "vocabulary",
    "multipleValues": "unordered",
    "properties": {
      "list": {"multipleValues": "list"},
      "typed": {"datatype": "http://typed"}
    }
  },
  "http://contextual.unordered/": {
    "propertyURI":    "contextual",
    "multipleValues": "unordered",
    "properties": {
      "list": {"multipleValues": "list"},
      "typed": {"datatype": "http://typed"}
    }
  },
  "http://contextual.list/": {
    "propertyURI":    "contextual",
    "multipleValues": "list",
    "properties": {
      "list": {"multipleValues": "list"},
      "typed": {"datatype": "http://typed"}
    }
  },
  "http://n.whatwg.org/work": {
    "propertyURI"    : "contextual",
    "multipleValues" : "list"
  } 
}
"""


registry   = []
myRegistry = []
if py_v_major >= 3 or (py_v_major == 2 and py_v_minor >= 6) :
  import json
  registry   = json.loads(_registry)
  myRegistry = json.loads(_myRegistry)
else :
  import simplejson
  registry   = simplejson.loads(_registry)
  myRegistry = simplejson.loads(_myRegistry)

for (k,v) in list(myRegistry.items()) : registry[k] = v

########NEW FILE########
__FILENAME__ = utils
# -*- coding: utf-8 -*-
"""
Various utilities for pyMicrodata

@organization: U{World Wide Web Consortium<http://www.w3.org>}
@author: U{Ivan Herman<a href="http://www.w3.org/People/Ivan/">}
@license: This software is available for use under the
U{W3C SOFTWARE NOTICE AND LICENSE<href="http://www.w3.org/Consortium/Legal/2002/copyright-software-20021231">}
"""

"""
$Id: utils.py,v 1.7 2012/09/01 15:17:28 ivan Exp $
$Date: 2012/09/01 15:17:28 $
"""
import os, os.path, sys
(py_v_major, py_v_minor, py_v_micro, py_v_final, py_v_serial) = sys.version_info

if py_v_major >= 3 :
	from urllib.request import Request, urlopen
	from urllib.parse   import urljoin, quote, urlparse
	from http.server    import BaseHTTPRequestHandler
	from urllib.error   import HTTPError as urllib_HTTPError
else :
	from urllib2        import Request, urlopen
	from urllib2        import HTTPError as urllib_HTTPError
	from urlparse       import urljoin, urlparse
	from urllib         import quote
	from BaseHTTPServer import BaseHTTPRequestHandler

import re
from datetime import datetime

from rdflib	import BNode
import rdflib
if rdflib.__version__ >= "3.0.0" :
	from rdflib	import RDF as ns_rdf
else :
	from rdflib.RDF	import RDFNS  as ns_rdf

#################################################################################
def is_absolute_URI( uri ) :
	return urlparse(uri)[0] != ""

#################################################################################

def fragment_escape( name ) :
	return quote(name, '/~:-.')
		
#################################################################################

def generate_URI(base, v) :
	"""
	Generate an (absolute) URI; if val is a fragment, then using it with base,
	otherwise just return the value
	@param base: Absolute URI for base
	@param v: relative or absolute URI
	"""
	if is_absolute_URI( v ) :
		return v
	else :		
		# UGLY!!! There is a bug for a corner case in python version <= 2.5.X
		if len(v) > 0 and v[0] == '?' and (py_v_major < 3 and py_v_minor <= 5) :
			return base+val
		####
		
		# Trust the python library...
		# Well, not quite:-) there is what is, in my view, a bug in the urljoin; in some cases it
		# swallows the '#' or '?' character at the end. This is clearly a problem with
		# Semantic Web URI-s
		v = fragment_escape(v.strip())
		joined = urljoin(base, v)
		try :
			if v[-1] != joined[-1] and (v[-1] == "#" or v[-1] == "?") :
				return joined + v[-1]
			else :
				return joined
		except :
			return joined		

#################################################################################
def generate_RDF_collection( graph, vals ) :
	"""
	Generate an RDF List from vals, returns the head of the list
	@param graph: RDF graph
	@type graph: RDFLib Graph
	@param vals: array of RDF Resources
	@return: head of the List (an RDF Resource)
	"""
	# generate an RDF List, returns the head
	# list has all the elements in RDF format already
	heads = [ BNode() for r in vals ] + [ ns_rdf["nil"] ]
	for i in range(0, len(vals)) :
		graph.add( (heads[i], ns_rdf["first"], vals[i]) )
		graph.add( (heads[i], ns_rdf["rest"],  heads[i+1]) )
	return heads[0]

#################################################################################
def get_Literal(Pnode):
	"""
	Get (recursively) the full text from a DOM Node.

	@param Pnode: DOM Node
	@return: string
	"""
	rc = ""
	for node in Pnode.childNodes:
		if node.nodeType == node.TEXT_NODE:
			rc = rc + node.data
		elif node.nodeType == node.ELEMENT_NODE :
			rc = rc + get_Literal(node)
			
	# This presupposes that all spaces and such should be stripped. I am not sure it is true in the spec,
	# but this is what the examples show
	# return re.sub(r'(\r| |\n|\t)+'," ",rc).strip()
	
	# at present, the agreement seems to say that white spaces are maintained:
	return rc

#################################################################################
def get_lang(node) :
	# we may have lang and xml:lang
	retval  = None
	if node.hasAttribute("lang") :
		retval = node.getAttribute("lang")
	if retval and node.hasAttribute("xml:lang") :
		xmllang = node.getAttribute("xml:lang").lower()
		if not( xmllang != None and xmllang == retval.lower() ) :
			# This is an error, in which case retval must be invalidated...
			retval = None
	return retval

def get_lang_from_hierarchy(document, node) :
	lang = get_lang(node)
	if lang == None :
		parent = node.parentNode
		if parent != None and parent != document :
			return get_lang_from_hierarchy(document, parent)
		else :
			return get_lang(document)
	else :
		return lang
	
#################################################################################
datetime_type 	= "http://www.w3.org/2001/XMLSchema#dateTime"
time_type 	 	= "http://www.w3.org/2001/XMLSchema#time"
date_type 	 	= "http://www.w3.org/2001/XMLSchema#date"
date_gYear		= "http://www.w3.org/2001/XMLSchema#gYear"
date_gYearMonth	= "http://www.w3.org/2001/XMLSchema#gYearMonth"
date_gMonthDay	= "http://www.w3.org/2001/XMLSchema#gMonthDay"
duration_type	= "http://www.w3.org/2001/XMLSchema#duration"

_formats = {
	date_gMonthDay	  : [ "%m-%d" ],
	date_gYearMonth	  : [ "%Y-%m"],
	date_gYear     	  : [ "%Y" ],
	date_type      	  : [ "%Y-%m-%d", "%Y-%m-%dZ" ],
	time_type      	  : [ "%H:%M",
					      "%H:%M:%S",
					      "%H:%M:%SZ",						
					      "%H:%M:%S.%f" ],
	datetime_type  	  : [ "%Y-%m-%dT%H:%M",
					      "%Y-%m-%dT%H:%M:%S",
					      "%Y-%m-%dT%H:%M:%S.%f",
					      "%Y-%m-%dT%H:%MZ",
					      "%Y-%m-%dT%H:%M:%SZ",
					      "%Y-%m-%dT%H:%M:%S.%fZ" ],
	duration_type     : [ "P%dD",
						  "P%YY%mM%dD",
						  "P%YY%mM",
						  "P%YY%dD",
						  "P%YY",
						  "P%mM",
						  "P%mM%dD",
						 ],
}

_dur_times = [ "%HH%MM%SS", "%HH", "%MM", "%SS", "%HH%MM", "%HH%SS", "%MM%SS" ]

def get_time_type(string) :
	"""
	Check whether the string abides to one of the accepted time related datatypes, and returns that one if yes
	@param string: the attribute value to be checked
	@return : a datatype URI or None
	"""
	for key in _formats :
		for format in _formats[key] :
			try :
				# try to check if the syntax is fine
				d = datetime.strptime(string, format)
				# bingo!
				return key
			except ValueError :
				pass

	# Now come the special cases:-(
	# Check first for the duration stuff, that is the nastiest.
	if len(string) > 2 and string[0] == 'P' or (string [0] == '-' and string[1] == 'P') :
		# this is meant to be a duration type
		# first of all, get rid of the leading '-' and check again
		if string[0] == '-' :
			for format in _formats[duration_type] :
				try :
					# try to check if the syntax is fine
					d = datetime.strptime(string, format)
					# bingo!
					return duration_type
				except ValueError :
					pass
		# Let us see if the value contains a separate time portion, and cut that one
		durs = string.split('T')
		if len(durs) == 2 :
			# yep, so we should check again
			dur = durs[0]
			tm  = durs[1]
			# Check the duration part
			td = False
			for format in _formats[duration_type] :
				try :
					# try to check if the syntax is fine
					d = datetime.strptime(dur, format)
					# bingo!
					td = True
					break
				except ValueError :
					pass
			if td == True :
				# Getting there...
				for format in _dur_times :
					try :
						# try to check if the syntax is fine
						d = datetime.strptime(tm, format)
						# bingo!
						return duration_type
					except ValueError :
						pass
			# something went wrong...
			return None			
		else :
			# Well, no more tricks, this is a plain type
			return None

	# If we got here, we should check the time zone
	# there is a discrepancy betwen the python and the HTML5/XSD lexical string,
	# which means that this has to handled separately for the date and the timezone portion
	try :
		# The time-zone-less portion of the string
		str = string[0:-6]
		# The time-zone portion
		tz = string[-5:]
		try :
			t = datetime.strptime(tz,"%H:%M")
		except ValueError :
			# Bummer, this is not a correct time
			return None
		# The time-zone is fine, the datetime portion has to be checked
		for format in _formats[datetime_type] :
			try :
				# try to check if it is fine
				d = datetime.strptime(str, format)
				# Bingo!
				return datetime_type
			except ValueError :
				pass
	except :
		pass
	return None


#########################################################################################################
# Handling URIs
class URIOpener :
	"""A wrapper around the urllib2 method to open a resource. Beyond accessing the data itself, the class
	sets the content location.
	The class also adds an accept header to the outgoing request, namely
	text/html and application/xhtml+xml (unless set explicitly by the caller).
	
	@ivar data: the real data, ie, a file-like object
	@ivar headers: the return headers as sent back by the server
	@ivar location: the real location of the data (ie, after possible redirection and content negotiation)
	"""
	CONTENT_LOCATION	= 'Content-Location'
	def __init__(self, name) :
		"""
		@param name: URL to be opened
		@keyword additional_headers: additional HTTP request headers to be added to the call
		"""		
		try :
			# Note the removal of the fragment ID. This is necessary, per the HTTP spec
			req = Request(url=name.split('#')[0])

			req.add_header('Accept', 'text/html, application/xhtml+xml')
				
			self.data		= urlopen(req)
			self.headers	= self.data.info()

			if URIOpener.CONTENT_LOCATION in self.headers :
				self.location = urlparse.urljoin(self.data.geturl(),self.headers[URIOpener.CONTENT_LOCATION])
			else :
				self.location = name
				
		except urllib_HTTPError :
			e = sys.exc_info()[1]
			from pyMicrodata import HTTPError
			msg = BaseHTTPRequestHandler.responses[e.code]
			raise HTTPError('%s' % msg[1], e.code)
		except Exception :
			e = sys.exc_info()[1]
			from pyMicrodata import MicrodataError
			raise MicrodataError('%s' % e)


########NEW FILE########
__FILENAME__ = embeddedRDF
# -*- coding: utf-8 -*-
"""
Extracting possible embedded RDF/XML content from the file and parse it separately into the Graph. This is used, for example
by U{SVG 1.2 Tiny<http://www.w3.org/TR/SVGMobile12/>}.

@author: U{Ivan Herman<a href="http://www.w3.org/People/Ivan/">}
@license: This software is available for use under the
U{W3C SOFTWARE NOTICE AND LICENSE<href="http://www.w3.org/Consortium/Legal/2002/copyright-software-20021231">}
@contact: Ivan Herman, ivan@w3.org
@version: $Id: embeddedRDF.py,v 1.15 2012/11/16 17:51:53 ivan Exp $
"""

# Python 3 foolproof way...
try :
	from io import StringIO
except :
	from StringIO import StringIO

from .host  import HostLanguage, accept_embedded_rdf_xml, accept_embedded_turtle
from .utils import return_XML
import re, sys

def handle_embeddedRDF(node, graph, state) :
	"""
	Handles embedded RDF. There are two possibilities:
	
	 - the file is one of the XML dialects that allows for an embedded RDF/XML portion. See the L{host.accept_embedded_rdf_xml} for those (a typical example is SVG). 
	 - the file is HTML and there is a turtle portion in the C{<script>} element with type text/turtle. 
	
	@param node: a DOM node for the top level element
	@param graph: target rdf graph
	@type graph: RDFLib's Graph object instance
	@param state: the inherited state (namespaces, lang, etc)
	@type state: L{state.ExecutionContext}
	@return: whether an RDF/XML or turtle content has been detected or not. If TRUE, the RDFa processing should not occur on the node and its descendents. 
	@rtype: Boolean
	"""
	#def _get_prefixes_in_turtle() :
	#	retval = ""
	#	for key in state.term_or_curie.ns :
	#		retval += "@prefix %s: <%s> .\n" % (key, state.term_or_curie.ns[key])
	#	retval += '\n'
	#	return retval
	
	# This feature is optional!
	def _get_literal(Pnode):
		"""
		Get the full text
		@param Pnode: DOM Node
		@return: string
		"""
		rc = ""
		for node in Pnode.childNodes:
			if node.nodeType in [node.TEXT_NODE, node.CDATA_SECTION_NODE] :
				rc = rc + node.data
		# Sigh... the HTML5 parser does not recognize the CDATA escapes, ie, it just passes on the <![CDATA[ and ]]> strings:-(
		return rc.replace("<![CDATA[","").replace("]]>","")

	if state.options.embedded_rdf  :
		# Embedded turtle, per the latest Turtle draft
		if state.options.host_language in accept_embedded_turtle and node.nodeName.lower() == "script" :
			if node.hasAttribute("type") and node.getAttribute("type") == "text/turtle" :
				#prefixes = _get_prefixes_in_turtle()
				#content  = _get_literal(node)
				#rdf = StringIO(prefixes + content)
				content  = _get_literal(node)
				rdf = StringIO(content)
				try :
					graph.parse(rdf, format="n3", publicID = state.base)
					state.options.add_info("The output graph includes triples coming from an embedded Turtle script")
				except :
					(type,value,traceback) = sys.exc_info()
					state.options.add_error("Embedded Turtle content could not be parsed (problems with %s?); ignored" % value)
			return True
		elif state.options.host_language in accept_embedded_rdf_xml and node.localName == "RDF" and node.namespaceURI == "http://www.w3.org/1999/02/22-rdf-syntax-ns#" :
			rdf = StringIO(return_XML(state, node))
			try :
				graph.parse(rdf)
				state.options.add_info("The output graph includes triples coming from an embedded RDF/XML subtree")
			except :
				(type,value,traceback) = sys.exc_info()
				state.options.add_error("Embedded RDF/XML content could not parsed (problems with %s?); ignored" % value)
			return True
		else :
			return False
	else :
		return False


########NEW FILE########
__FILENAME__ = httpheader
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
""" Utility functions to work with HTTP headers.

 This module provides some utility functions useful for parsing
 and dealing with some of the HTTP 1.1 protocol headers which
 are not adequately covered by the standard Python libraries.

 Requires Python 2.2 or later.

 The functionality includes the correct interpretation of the various
 Accept-* style headers, content negotiation, byte range requests,
 HTTP-style date/times, and more.

 There are a few classes defined by this module:

   * class content_type   -- media types such as 'text/plain'
   * class language_tag   -- language tags such as 'en-US'
   * class range_set      -- a collection of (byte) range specifiers
   * class range_spec     -- a single (byte) range specifier

 The primary functions in this module may be categorized as follows:

   * Content negotiation functions...
     * acceptable_content_type()
     * acceptable_language()
     * acceptable_charset()
     * acceptable_encoding()

   * Mid-level header parsing functions...
     * parse_accept_header()
     * parse_accept_language_header()
     * parse_range_header()
 
   * Date and time...
     * http_datetime()
     * parse_http_datetime()

   * Utility functions...
     * quote_string()
     * remove_comments()
     * canonical_charset()

   * Low level string parsing functions...
     * parse_comma_list()
     * parse_comment()
     * parse_qvalue_accept_list()
     * parse_media_type()
     * parse_number()
     * parse_parameter_list()
     * parse_quoted_string()
     * parse_range_set()
     * parse_range_spec()
     * parse_token()
     * parse_token_or_quoted_string()

 And there are some specialized exception classes:

   * RangeUnsatisfiableError
   * RangeUnmergableError
   * ParseError

 See also:

   * RFC 2616, "Hypertext Transfer Protocol -- HTTP/1.1", June 1999.
             <http://www.ietf.org/rfc/rfc2616.txt>
             Errata at <http://purl.org/NET/http-errata>
   * RFC 2046, "(MIME) Part Two: Media Types", November 1996.
             <http://www.ietf.org/rfc/rfc2046.txt>
   * RFC 3066, "Tags for the Identification of Languages", January 2001.
             <http://www.ietf.org/rfc/rfc3066.txt>
             
             
  Note: I have made a small modification on the regexp for internet date, 
  to make it more liberal (ie, accept a time zone string of the form +0000)
  Ivan Herman <http://www.ivan-herman.net>, March 2011.
  
  Have added statements to make it (hopefully) Python 3 compatible.
  Ivan Herman <http://www.ivan-herman.net>, August 2012.
"""

__author__ = "Deron Meranda <http://deron.meranda.us/>"
__date__ = "2012-08-31"
__version__ = "1.02"
__credits__ = """Copyright (c) 2005 Deron E. Meranda <http://deron.meranda.us/>
Licensed under GNU LGPL 2.1 or later.  See <http://www.fsf.org/>.

This library is free software; you can redistribute it and/or
modify it under the terms of the GNU Lesser General Public
License as published by the Free Software Foundation; either
version 2.1 of the License, or (at your option) any later version.

This library is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
Lesser General Public License for more details.

You should have received a copy of the GNU Lesser General Public
License along with this library; if not, write to the Free Software
Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA
"""

# Character classes from RFC 2616 section 2.2
SEPARATORS = '()<>@,;:\\"/[]?={} \t'
LWS = ' \t\n\r'  # linear white space
CRLF = '\r\n'
DIGIT = '0123456789'
HEX = '0123456789ABCDEFabcdef'

import sys
PY3 = (sys.version_info[0] >= 3)

# Try to get a set/frozenset implementation if possible
try:
    type(frozenset)
except NameError:
    try:
        # The demset.py module is available at http://deron.meranda.us/
        from demset import set, frozenset
        __emulating_set = True  # So we can clean up global namespace later
    except ImportError:
        pass

try:
    # Turn character classes into set types (for Python 2.4 or greater)
    SEPARATORS = frozenset([c for c in SEPARATORS])
    LWS = frozenset([c for c in LWS])
    CRLF = frozenset([c for c in CRLF])
    DIGIT = frozenset([c for c in DIGIT])
    HEX = frozenset([c for c in HEX])
    del c
except NameError:
    # Python 2.3 or earlier, leave as simple strings
    pass


def _is_string( obj ):
    """Returns True if the object is a string or unicode type."""
    if PY3 :
        return isinstance(obj,str)
    else :
        return isinstance(obj,str) or isinstance(obj,unicode)


def http_datetime( dt=None ):
    """Formats a datetime as an HTTP 1.1 Date/Time string.

    Takes a standard Python datetime object and returns a string
    formatted according to the HTTP 1.1 date/time format.

    If no datetime is provided (or None) then the current
    time is used.
    
    ABOUT TIMEZONES: If the passed in datetime object is naive it is
    assumed to be in UTC already.  But if it has a tzinfo component,
    the returned timestamp string will have been converted to UTC
    automatically.  So if you use timezone-aware datetimes, you need
    not worry about conversion to UTC.

    """
    if not dt:
        import datetime
        dt = datetime.datetime.utcnow()
    else:
        try:
            dt = dt - dt.utcoffset()
        except:
            pass  # no timezone offset, just assume already in UTC

    s = dt.strftime('%a, %d %b %Y %H:%M:%S GMT')
    return s


def parse_http_datetime( datestring, utc_tzinfo=None, strict=False ):
    """Returns a datetime object from an HTTP 1.1 Date/Time string.

    Note that HTTP dates are always in UTC, so the returned datetime
    object will also be in UTC.

    You can optionally pass in a tzinfo object which should represent
    the UTC timezone, and the returned datetime will then be
    timezone-aware (allowing you to more easly translate it into
    different timzeones later).

    If you set 'strict' to True, then only the RFC 1123 format
    is recognized.  Otherwise the backwards-compatible RFC 1036
    and Unix asctime(3) formats are also recognized.
    
    Please note that the day-of-the-week is not validated.
    Also two-digit years, although not HTTP 1.1 compliant, are
    treated according to recommended Y2K rules.

    """
    import re, datetime
    m = re.match(r'(?P<DOW>[a-z]+), (?P<D>\d+) (?P<MON>[a-z]+) (?P<Y>\d+) (?P<H>\d+):(?P<M>\d+):(?P<S>\d+(\.\d+)?) (?P<TZ>[a-zA-Z0-9_+]+)$',
                 datestring, re.IGNORECASE)
    if not m and not strict:
        m = re.match(r'(?P<DOW>[a-z]+) (?P<MON>[a-z]+) (?P<D>\d+) (?P<H>\d+):(?P<M>\d+):(?P<S>\d+) (?P<Y>\d+)$',
                     datestring, re.IGNORECASE)
        if not m:
            m = re.match(r'(?P<DOW>[a-z]+), (?P<D>\d+)-(?P<MON>[a-z]+)-(?P<Y>\d+) (?P<H>\d+):(?P<M>\d+):(?P<S>\d+(\.\d+)?) (?P<TZ>\w+)$',
                         datestring, re.IGNORECASE)
    if not m:
        raise ValueError('HTTP date is not correctly formatted')

    try:
        tz = m.group('TZ').upper()
    except:
        tz = 'GMT'
    if tz not in ('GMT','UTC','0000','00:00'):
        raise ValueError('HTTP date is not in GMT timezone')

    monname = m.group('MON').upper()
    mdict = {'JAN':1, 'FEB':2, 'MAR':3, 'APR':4, 'MAY':5, 'JUN':6,
             'JUL':7, 'AUG':8, 'SEP':9, 'OCT':10, 'NOV':11, 'DEC':12}
    month = mdict.get(monname)
    if not month:
        raise ValueError('HTTP date has an unrecognizable month')
    y = int(m.group('Y'))
    if y < 100:
        century = datetime.datetime.utcnow().year / 100
        if y < 50:
            y = century * 100 + y
        else:
            y = (century - 1) * 100 + y
    d = int(m.group('D'))
    hour = int(m.group('H'))
    minute = int(m.group('M'))
    try:
        second = int(m.group('S'))
    except:
        second = float(m.group('S'))
    dt = datetime.datetime( y, month, d, hour, minute, second, tzinfo=utc_tzinfo )
    return dt


class RangeUnsatisfiableError(ValueError):
    """Exception class when a byte range lies outside the file size boundaries."""
    def __init__(self, reason=None):
        if not reason:
            reason = 'Range is unsatisfiable'
        ValueError.__init__(self, reason)


class RangeUnmergableError(ValueError):
    """Exception class when byte ranges are noncontiguous and can not be merged together."""
    def __init__(self, reason=None):
        if not reason:
            reason = 'Ranges can not be merged together'
        ValueError.__init__(self, reason)


class ParseError(ValueError):
    """Exception class representing a string parsing error."""
    def __init__(self, args, input_string, at_position):
        ValueError.__init__(self, args)
        self.input_string = input_string
        self.at_position = at_position
    def __str__(self):
        if self.at_position >= len(self.input_string):
            return '%s\n\tOccured at end of string' % self.args[0]
        else:
            return '%s\n\tOccured near %s' % (self.args[0], repr(self.input_string[self.at_position:self.at_position+16]))


def is_token(s):
    """Determines if the string is a valid token."""
    for c in s:
        if ord(c) < 32 or ord(c) > 128 or c in SEPARATORS:
            return False
    return True


def parse_comma_list(s, start=0, element_parser=None, min_count=0, max_count=0):
    """Parses a comma-separated list with optional whitespace.

    Takes an optional callback function `element_parser`, which
    is assumed to be able to parse an individual element.  It
    will be passed the string and a `start` argument, and
    is expected to return a tuple (parsed_result, chars_consumed).

    If no element_parser is given, then either single tokens or
    quoted strings will be parsed.

    If min_count > 0, then at least that many non-empty elements
    must be in the list, or an error is raised.

    If max_count > 0, then no more than that many non-empty elements
    may be in the list, or an error is raised.

    """
    if min_count > 0 and start == len(s):
        raise ParseError('Comma-separated list must contain some elements',s,start)
    elif start >= len(s):
        raise ParseError('Starting position is beyond the end of the string',s,start)

    if not element_parser:
        element_parser = parse_token_or_quoted_string
    results = []
    pos = start
    while pos < len(s):
        e = element_parser( s, pos )
        if not e or e[1] == 0:
            break # end of data?
        else:
            results.append( e[0] )
            pos += e[1]
        while pos < len(s) and s[pos] in LWS:
            pos += 1
        if pos < len(s) and s[pos] != ',':
            break
        while pos < len(s) and s[pos] == ',':
            # skip comma and any "empty" elements
            pos += 1  # skip comma
            while pos < len(s) and s[pos] in LWS:
                pos += 1
    if len(results) < min_count:
        raise ParseError('Comma-separated list does not have enough elements',s,pos)
    elif max_count and len(results) > max_count:
        raise ParseError('Comma-separated list has too many elements',s,pos)
    return (results, pos-start)


def parse_token(s, start=0):
    """Parses a token.

    A token is a string defined by RFC 2616 section 2.2 as:
       token = 1*<any CHAR except CTLs or separators>

    Returns a tuple (token, chars_consumed), or ('',0) if no token
    starts at the given string position.  On a syntax error, a
    ParseError exception will be raised.

    """
    return parse_token_or_quoted_string(s, start, allow_quoted=False, allow_token=True)


def quote_string(s, always_quote=True):
    """Produces a quoted string according to HTTP 1.1 rules.

    If always_quote is False and if the string is also a valid token,
    then this function may return a string without quotes.

    """
    need_quotes = False
    q = ''
    for c in s:
        if ord(c) < 32 or ord(c) > 127 or c in SEPARATORS:
            q += '\\' + c
            need_quotes = True
        else:
            q += c
    if need_quotes or always_quote:
        return '"' + q + '"'
    else:
        return q


def parse_quoted_string(s, start=0):
    """Parses a quoted string.

    Returns a tuple (string, chars_consumed).  The quote marks will
    have been removed and all \-escapes will have been replaced with
    the characters they represent.

    """
    return parse_token_or_quoted_string(s, start, allow_quoted=True, allow_token=False)


def parse_token_or_quoted_string(s, start=0, allow_quoted=True, allow_token=True):
    """Parses a token or a quoted-string.

    's' is the string to parse, while start is the position within the
    string where parsing should begin.  It will returns a tuple
    (token, chars_consumed), with all \-escapes and quotation already
    processed.

    Syntax is according to BNF rules in RFC 2161 section 2.2,
    specifically the 'token' and 'quoted-string' declarations.
    Syntax errors in the input string will result in ParseError
    being raised.

    If allow_quoted is False, then only tokens will be parsed instead
    of either a token or quoted-string.

    If allow_token is False, then only quoted-strings will be parsed
    instead of either a token or quoted-string.
    """
    if not allow_quoted and not allow_token:
        raise ValueError('Parsing can not continue with options provided')

    if start >= len(s):
        raise ParseError('Starting position is beyond the end of the string',s,start)
    has_quote = (s[start] == '"')
    if has_quote and not allow_quoted:
        raise ParseError('A quoted string was not expected', s, start)
    if not has_quote and not allow_token:
        raise ParseError('Expected a quotation mark', s, start)

    s2 = ''
    pos = start
    if has_quote:
        pos += 1
    while pos < len(s):
        c = s[pos]
        if c == '\\' and has_quote:
            # Note this is NOT C-style escaping; the character after the \ is
            # taken literally.
            pos += 1
            if pos == len(s):
                raise ParseError("End of string while expecting a character after '\\'",s,pos)
            s2 += s[pos]
            pos += 1
        elif c == '"' and has_quote:
            break
        elif not has_quote and (c in SEPARATORS or ord(c)<32 or ord(c)>127):
            break
        else:
            s2 += c
            pos += 1
    if has_quote:
        # Make sure we have a closing quote mark
        if pos >= len(s) or s[pos] != '"':
            raise ParseError('Quoted string is missing closing quote mark',s,pos)
        else:
            pos += 1
    return s2, (pos - start)


def remove_comments(s, collapse_spaces=True):
    """Removes any ()-style comments from a string.

    In HTTP, ()-comments can nest, and this function will correctly
    deal with that.

    If 'collapse_spaces' is True, then if there is any whitespace
    surrounding the comment, it will be replaced with a single space
    character.  Whitespace also collapses across multiple comment
    sequences, so that "a (b) (c) d" becomes just "a d".

    Otherwise, if 'collapse_spaces' is False then all whitespace which
    is outside any comments is left intact as-is.

    """
    if '(' not in s:
        return s  # simple case
    A = []
    dostrip = False
    added_comment_space = False
    pos = 0
    if collapse_spaces:
        # eat any leading spaces before a comment
        i = s.find('(')
        if i >= 0:
            while pos < i and s[pos] in LWS:
                pos += 1
            if pos != i:
                pos = 0
            else:
                dostrip = True
                added_comment_space = True  # lie
    while pos < len(s):
        if s[pos] == '(':
            cmt, k = parse_comment( s, pos )
            pos += k
            if collapse_spaces:
                dostrip = True
                if not added_comment_space:
                    if len(A) > 0 and A[-1] and A[-1][-1] in LWS:
                        # previous part ended with whitespace
                        A[-1] = A[-1].rstrip()
                        A.append(' ')  # comment becomes one space
                        added_comment_space = True
        else:
            i = s.find( '(', pos )
            if i == -1:
                if dostrip:
                    text = s[pos:].lstrip()
                    if s[pos] in LWS and not added_comment_space:
                        A.append(' ')
                        added_comment_space = True
                else:
                    text = s[pos:]
                if text:
                    A.append(text)
                    dostrip = False
                    added_comment_space = False
                break # end of string
            else:
                if dostrip:
                    text = s[pos:i].lstrip()
                    if s[pos] in LWS and not added_comment_space:
                        A.append(' ')
                        added_comment_space = True
                else:
                    text = s[pos:i]
                if text:
                    A.append(text)
                    dostrip = False
                    added_comment_space = False
                pos = i
    if dostrip and len(A) > 0 and A[-1] and A[-1][-1] in LWS:
        A[-1] = A[-1].rstrip()
    return ''.join(A)


def _test_comments():
    """A self-test on comment processing.  Returns number of test failures."""
    def _testrm( a, b, collapse ):
        b2 = remove_comments( a, collapse )
        if b != b2:
            print( 'Comment test failed:' )
            print( '   remove_comments( %s, collapse_spaces=%s ) -> %s' % (repr(a), repr(collapse), repr(b2)) )
            print( '   expected %s' % repr(b) )
            return 1
        return 0
    failures = 0
    failures += _testrm( r'', '', False )
    failures += _testrm( r'(hello)', '', False)
    failures += _testrm( r'abc (hello) def', 'abc  def', False)
    failures += _testrm( r'abc (he(xyz)llo) def', 'abc  def', False)
    failures += _testrm( r'abc (he\(xyz)llo) def', 'abc llo) def', False)
    failures += _testrm( r'abc(hello)def', 'abcdef', True)
    failures += _testrm( r'abc (hello) def', 'abc def', True)
    failures += _testrm( r'abc   (hello)def', 'abc def', True)
    failures += _testrm( r'abc(hello)  def', 'abc def', True)
    failures += _testrm( r'abc(hello) (world)def', 'abc def', True)
    failures += _testrm( r'abc(hello)(world)def', 'abcdef', True)
    failures += _testrm( r'  (hello) (world) def', 'def', True)
    failures += _testrm( r'abc  (hello) (world) ', 'abc', True)
    return failures

def parse_comment(s, start=0):
    """Parses a ()-style comment from a header value.

    Returns tuple (comment, chars_consumed), where the comment will
    have had the outer-most parentheses and white space stripped.  Any
    nested comments will still have their parentheses and whitespace
    left intact.

    All \-escaped quoted pairs will have been replaced with the actual
    characters they represent, even within the inner nested comments.

    You should note that only a few HTTP headers, such as User-Agent
    or Via, allow ()-style comments within the header value.

    A comment is defined by RFC 2616 section 2.2 as:
    
       comment = "(" *( ctext | quoted-pair | comment ) ")"
       ctext   = <any TEXT excluding "(" and ")">
    """
    if start >= len(s):
        raise ParseError('Starting position is beyond the end of the string',s,start)
    if s[start] != '(':
        raise ParseError('Comment must begin with opening parenthesis',s,start)

    s2 = ''
    nestlevel = 1
    pos = start + 1
    while pos < len(s) and s[pos] in LWS:
        pos += 1

    while pos < len(s):
        c = s[pos]
        if c == '\\':
            # Note this is not C-style escaping; the character after the \ is
            # taken literally.
            pos += 1
            if pos == len(s):
                raise ParseError("End of string while expecting a character after '\\'",s,pos)
            s2 += s[pos]
            pos += 1
        elif c == '(':
            nestlevel += 1
            s2 += c
            pos += 1
        elif c == ')':
            nestlevel -= 1
            pos += 1
            if nestlevel >= 1:
                s2 += c
            else:
                break
        else:
            s2 += c
            pos += 1
    if nestlevel > 0:
        raise ParseError('End of string reached before comment was closed',s,pos)
    # Now rstrip s2 of all LWS chars.
    while len(s2) and s2[-1] in LWS:
        s2 = s2[:-1]
    return s2, (pos - start)
    

class range_spec(object):
    """A single contiguous (byte) range.

    A range_spec defines a range (of bytes) by specifying two offsets,
    the 'first' and 'last', which are inclusive in the range.  Offsets
    are zero-based (the first byte is offset 0).  The range can not be
    empty or negative (has to satisfy first <= last).

    The range can be unbounded on either end, represented here by the
    None value, with these semantics:

       * A 'last' of None always indicates the last possible byte
        (although that offset may not be known).

       * A 'first' of None indicates this is a suffix range, where
         the last value is actually interpreted to be the number
         of bytes at the end of the file (regardless of file size).

    Note that it is not valid for both first and last to be None.

    """

    __slots__ = ['first','last']

    def __init__(self, first=0, last=None):
        self.set( first, last )

    def set(self, first, last):
        """Sets the value of this range given the first and last offsets.
        """
        if first is not None and last is not None and first > last:
            raise ValueError("Byte range does not satisfy first <= last.")
        elif first is None and last is None:
            raise ValueError("Byte range can not omit both first and last offsets.")
        self.first = first
        self.last = last

    def __repr__(self):
        return '%s.%s(%s,%s)' % (self.__class__.__module__, self.__class__.__name__,
                                 self.first, self.last)

    def __str__(self):
        """Returns a string form of the range as would appear in a Range: header."""
        if self.first is None and self.last is None:
            return ''
        s = ''
        if self.first is not None:
            s += '%d' % self.first
        s += '-'
        if self.last is not None:
            s += '%d' % self.last
        return s

    def __eq__(self, other):
        """Compare ranges for equality.

        Note that if non-specific ranges are involved (such as 34- and -5),
        they could compare as not equal even though they may represent
        the same set of bytes in some contexts.
        """
        return self.first == other.first and self.last == other.last

    def __ne__(self, other):
        """Compare ranges for inequality.

        Note that if non-specific ranges are involved (such as 34- and -5),
        they could compare as not equal even though they may represent
        the same set of bytes in some contexts.
        """
        return not self.__eq__(other)

    def __lt__(self, other):
        """< operator is not defined"""
        raise NotImplementedError('Ranges can not be relationally compared')
    def __le__(self, other):
        """<= operator is not defined"""
        raise NotImplementedError('Ranges can not be ralationally compared')
    def __gt__(self, other):
        """> operator is not defined"""
        raise NotImplementedError('Ranges can not be relationally compared')
    def __ge__(self, other):
        """>= operator is not defined"""
        raise NotImplementedError('Ranges can not be relationally compared')
    
    def copy(self):
        """Makes a copy of this range object."""
        return self.__class__( self.first, self.last )

    def is_suffix(self):
        """Returns True if this is a suffix range.

        A suffix range is one that specifies the last N bytes of a
        file regardless of file size.

        """
        return self.first == None

    def is_fixed(self):
        """Returns True if this range is absolute and a fixed size.

        This occurs only if neither first or last is None.  Converse
        is the is_unbounded() method.

        """
        return first is not None and last is not None

    def is_unbounded(self):
        """Returns True if the number of bytes in the range is unspecified.

        This can only occur if either the 'first' or the 'last' member
        is None.  Converse is the is_fixed() method.

        """
        return self.first is None or self.last is None

    def is_whole_file(self):
        """Returns True if this range includes all possible bytes.

        This can only occur if the 'last' member is None and the first
        member is 0.

        """
        return self.first == 0 and self.last is None

    def __contains__(self, offset):
        """Does this byte range contain the given byte offset?

        If the offset < 0, then it is taken as an offset from the end
        of the file, where -1 is the last byte.  This type of offset
        will only work with suffix ranges.

        """
        if offset < 0:
            if self.first is not None:
                return False
            else:
                return self.last >= -offset
        elif self.first is None:
            return False
        elif self.last is None:
            return True
        else:
            return self.first <= offset <= self.last

    def fix_to_size(self, size):
        """Changes a length-relative range to an absolute range based upon given file size.

        Ranges that are already absolute are left as is.

        Note that zero-length files are handled as special cases,
        since the only way possible to specify a zero-length range is
        with the suffix range "-0".  Thus unless this range is a suffix
        range, it can not satisfy a zero-length file.

        If the resulting range (partly) lies outside the file size then an
        error is raised.
        """

        if size == 0:
            if self.first is None:
                self.last = 0
                return
            else:
                raise RangeUnsatisfiableError("Range can satisfy a zero-length file.")

        if self.first is None:
            # A suffix range
            self.first = size - self.last
            if self.first < 0:
                self.first = 0
            self.last = size - 1
        else:
            if self.first > size - 1:
                raise RangeUnsatisfiableError('Range begins beyond the file size.')
            else:
                if self.last is None:
                    # An unbounded range
                    self.last = size - 1
        return

    def merge_with(self, other):
        """Tries to merge the given range into this one.

        The size of this range may be enlarged as a result.

        An error is raised if the two ranges do not overlap or are not
        contiguous with each other.
        """
        if self.is_whole_file() or self == other:
            return
        elif other.is_whole_file():
            self.first, self.last = 0, None
            return

        a1, z1 = self.first, self.last
        a2, z2 = other.first, other.last

        if self.is_suffix():
            if z1 == 0: # self is zero-length, so merge becomes a copy
                self.first, self.last = a2, z2
                return
            elif other.is_suffix():
                self.last = max(z1, z2)
            else:
                raise RangeUnmergableError()
        elif other.is_suffix():
            if z2 == 0: # other is zero-length, so nothing to merge
                return
            else:
                raise RangeUnmergableError()

        assert a1 is not None and a2 is not None

        if a2 < a1:
            # swap ranges so a1 <= a2
            a1, z1, a2, z2 = a2, z2, a1, z1

        assert a1 <= a2

        if z1 is None:
            if z2 is not None and z2 + 1 < a1:
                raise RangeUnmergableError()
            else:
                self.first = min(a1, a2)
                self.last = None
        elif z2 is None:
            if z1 + 1 < a2:
                raise RangeUnmergableError()
            else:
                self.first = min(a1, a2)
                self.last = None
        else:
            if a2 > z1 + 1:
                raise RangeUnmergableError()
            else:
                self.first = a1
                self.last = max(z1, z2)
        return


class range_set(object):
    """A collection of range_specs, with units (e.g., bytes).
    """
    __slots__ = ['units', 'range_specs']

    def __init__(self):
        self.units = 'bytes'
        self.range_specs = []  # a list of range_spec objects

    def __str__(self):
        return self.units + '=' + ', '.join([str(s) for s in self.range_specs])

    def __repr__(self):
        return '%s.%s(%s)' % (self.__class__.__module__,
                              self.__class__.__name__,
                              repr(self.__str__()) )

    def from_str(self, s, valid_units=('bytes','none')):
        """Sets this range set based upon a string, such as the Range: header.

        You can also use the parse_range_set() function for more control.

        If a parsing error occurs, the pre-exising value of this range
        set is left unchanged.

        """
        r, k = parse_range_set( s, valid_units=valid_units )
        if k < len(s):
            raise ParseError("Extra unparsable characters in range set specifier",s,k)
        self.units = r.units
        self.range_specs = r.range_specs

    def is_single_range(self):
        """Does this range specifier consist of only a single range set?"""
        return len(self.range_specs) == 1

    def is_contiguous(self):
        """Can the collection of range_specs be coalesced into a single contiguous range?"""
        if len(self.range_specs) <= 1:
            return True
        merged = self.range_specs[0].copy()
        for s in self.range_specs[1:]:
            try:
                merged.merge_with(s)
            except:
                return False
        return True

    def fix_to_size(self, size):
        """Changes all length-relative range_specs to absolute range_specs based upon given file size.
        If none of the range_specs in this set can be satisfied, then the
        entire set is considered unsatifiable and an error is raised.
        Otherwise any unsatisfiable range_specs will simply be removed
        from this set.

        """
        for i in range(len(self.range_specs)):
            try:
                self.range_specs[i].fix_to_size( size )
            except RangeUnsatisfiableError:
                self.range_specs[i] = None
        self.range_specs = [s for s in self.range_specs if s is not None]
        if len(self.range_specs) == 0:
            raise RangeUnsatisfiableError('No ranges can be satisfied')

    def coalesce(self):
        """Collapses all consecutive range_specs which together define a contiguous range.

        Note though that this method will not re-sort the range_specs, so a
        potentially contiguous range may not be collapsed if they are
        not sorted.  For example the ranges:
            10-20, 30-40, 20-30
        will not be collapsed to just 10-40.  However if the ranges are
        sorted first as with:
            10-20, 20-30, 30-40
        then they will collapse to 10-40.
        """
        if len(self.range_specs) <= 1:
            return
        for i in range(len(self.range_specs) - 1):
            a = self.range_specs[i]
            b = self.range_specs[i+1]
            if a is not None:
                try:
                    a.merge_with( b )
                    self.range_specs[i+1] = None # to be deleted later
                except RangeUnmergableError:
                    pass
        self.range_specs = [r for r in self.range_specs if r is not None]


def parse_number( s, start=0 ):
    """Parses a positive decimal integer number from the string.

    A tuple is returned (number, chars_consumed).  If the
    string is not a valid decimal number, then (None,0) is returned.
    """
    if start >= len(s):
        raise ParseError('Starting position is beyond the end of the string',s,start)
    if s[start] not in DIGIT:
        return (None,0)  # not a number
    pos = start
    n = 0
    while pos < len(s):
        c = s[pos]
        if c in DIGIT:
            n *= 10
            n += ord(c) - ord('0')
            pos += 1
        else:
            break
    return n, pos-start


def parse_range_spec( s, start=0 ):
    """Parses a (byte) range_spec.

    Returns a tuple (range_spec, chars_consumed).
    """
    if start >= len(s):
        raise ParseError('Starting position is beyond the end of the string',s,start)
    if s[start] not in DIGIT and s[start] != '-':
        raise ParseError("Invalid range, expected a digit or '-'",s,start)
    first, last = None, None
    pos = start
    first, k = parse_number( s, pos )
    pos += k
    if s[pos] == '-':
        pos += 1
        if pos < len(s):
            last, k = parse_number( s, pos )
            pos += k
    else:
        raise ParseError("Byte range must include a '-'",s,pos)
    if first is None and last is None:
        raise ParseError('Byte range can not omit both first and last indices.',s,start)
    R = range_spec( first, last )
    return R, pos-start


def parse_range_header( header_value, valid_units=('bytes','none') ):
    """Parses the value of an HTTP Range: header.

    The value of the header as a string should be passed in; without
    the header name itself.

    Returns a range_set object.
    """
    ranges, k = parse_range_set( header_value, valid_units=valid_units )
    if k < len(header_value):
        raise ParseError('Range header has unexpected or unparsable characters',
                         header_value, k)
    return ranges


def parse_range_set( s, start=0, valid_units=('bytes','none') ):
    """Parses a (byte) range set specifier.

    Returns a tuple (range_set, chars_consumed).
    """
    if start >= len(s):
        raise ParseError('Starting position is beyond the end of the string',s,start)
    pos = start
    units, k = parse_token( s, pos )
    pos += k
    if valid_units and units not in valid_units:
        raise ParseError('Unsupported units type in range specifier',s,start)
    while pos < len(s) and s[pos] in LWS:
        pos += 1
    if pos < len(s) and s[pos] == '=':
        pos += 1
    else:
        raise ParseError("Invalid range specifier, expected '='",s,pos)
    while pos < len(s) and s[pos] in LWS:
        pos += 1
    range_specs, k = parse_comma_list( s, pos, parse_range_spec, min_count=1 )
    pos += k
    # Make sure no trash is at the end of the string
    while pos < len(s) and s[pos] in LWS:
        pos += 1
    if pos < len(s):
        raise ParseError('Unparsable characters in range set specifier',s,pos)

    ranges = range_set()
    ranges.units = units
    ranges.range_specs = range_specs
    return ranges, pos-start


def _split_at_qfactor( s ):
    """Splits a string at the quality factor (;q=) parameter.

    Returns the left and right substrings as a two-member tuple.

    """
    # It may be faster, but incorrect, to use s.split(';q=',1), since
    # HTTP allows any amount of linear white space (LWS) to appear
    # between the parts, so it could also be "; q = ".

    # We do this parsing 'manually' for speed rather than using a
    # regex, which would be r';[ \t\r\n]*q[ \t\r\n]*=[ \t\r\n]*'

    pos = 0
    while 0 <= pos < len(s):
        pos = s.find(';', pos)
        if pos < 0:
            break # no more parameters
        startpos = pos
        pos = pos + 1
        while pos < len(s) and s[pos] in LWS:
            pos = pos + 1
        if pos < len(s) and s[pos] == 'q':
            pos = pos + 1
            while pos < len(s) and s[pos] in LWS:
                pos = pos + 1
            if pos < len(s) and s[pos] == '=':
                pos = pos + 1
                while pos < len(s) and s[pos] in LWS:
                    pos = pos + 1
                return ( s[:startpos], s[pos:] )
    return (s, '')


def parse_qvalue_accept_list( s, start=0, item_parser=parse_token ):
    """Parses any of the Accept-* style headers with quality factors.

    This is a low-level function.  It returns a list of tuples, each like:
       (item, item_parms, qvalue, accept_parms)

    You can pass in a function which parses each of the item strings, or
    accept the default where the items must be simple tokens.  Note that
    your parser should not consume any paramters (past the special "q"
    paramter anyway).

    The item_parms and accept_parms are each lists of (name,value) tuples.

    The qvalue is the quality factor, a number from 0 to 1 inclusive.

    """
    itemlist = []
    pos = start
    if pos >= len(s):
        raise ParseError('Starting position is beyond the end of the string',s,pos)
    item = None
    while pos < len(s):
        item, k = item_parser(s, pos)
        pos += k
        while pos < len(s) and s[pos] in LWS:
            pos += 1
        if pos >= len(s) or s[pos] in ',;':
            itemparms, qvalue, acptparms = [], None, []
            if pos < len(s) and s[pos] == ';':
                pos += 1
                while pos < len(s) and s[pos] in LWS:
                    pos += 1
                parmlist, k = parse_parameter_list(s, pos)
                for p, v in parmlist:
                    if p == 'q' and qvalue is None:
                        try:
                            qvalue = float(v)
                        except ValueError:
                            raise ParseError('qvalue must be a floating point number',s,pos)
                        if qvalue < 0 or qvalue > 1:
                            raise ParseError('qvalue must be between 0 and 1, inclusive',s,pos)
                    elif qvalue is None:
                        itemparms.append( (p,v) )
                    else:
                        acptparms.append( (p,v) )
                pos += k
            if item:
                # Add the item to the list
                if qvalue is None:
                    qvalue = 1
                itemlist.append( (item, itemparms, qvalue, acptparms) )
                item = None
            # skip commas
            while pos < len(s) and s[pos] == ',':
                pos += 1
                while pos < len(s) and s[pos] in LWS:
                    pos += 1
        else:
            break
    return itemlist, pos - start


def parse_accept_header( header_value ):
    """Parses the Accept: header.

    The value of the header as a string should be passed in; without
    the header name itself.
    
    This will parse the value of any of the HTTP headers "Accept",
    "Accept-Charset", "Accept-Encoding", or "Accept-Language".  These
    headers are similarly formatted, in that they are a list of items
    with associated quality factors.  The quality factor, or qvalue,
    is a number in the range [0.0..1.0] which indicates the relative
    preference of each item.

    This function returns a list of those items, sorted by preference
    (from most-prefered to least-prefered).  Each item in the returned
    list is actually a tuple consisting of:

       ( item_name, item_parms, qvalue, accept_parms )

    As an example, the following string,
        text/plain; charset="utf-8"; q=.5; columns=80
    would be parsed into this resulting tuple,
        ( 'text/plain', [('charset','utf-8')], 0.5, [('columns','80')] )

    The value of the returned item_name depends upon which header is
    being parsed, but for example it may be a MIME content or media
    type (without parameters), a language tag, or so on.  Any optional
    parameters (delimited by semicolons) occuring before the "q="
    attribute will be in the item_parms list as (attribute,value)
    tuples in the same order as they appear in the header.  Any quoted
    values will have been unquoted and unescaped.

    The qvalue is a floating point number in the inclusive range 0.0
    to 1.0, and roughly indicates the preference for this item.
    Values outside this range will be capped to the closest extreme.

         (!) Note that a qvalue of 0 indicates that the item is
         explicitly NOT acceptable to the user agent, and should be
         handled differently by the caller.

    The accept_parms, like the item_parms, is a list of any attributes
    occuring after the "q=" attribute, and will be in the list as
    (attribute,value) tuples in the same order as they occur.
    Usually accept_parms will be an empty list, as the HTTP spec
    allows these extra parameters in the syntax but does not
    currently define any possible values.

    All empty items will be removed from the list.  However, duplicate
    or conflicting values are not detected or handled in any way by
    this function.
    """
    def parse_mt_only(s, start):
        mt, k = parse_media_type(s, start, with_parameters=False)
        ct = content_type()
        ct.major = mt[0]
        ct.minor = mt[1]
        return ct, k

    alist, k = parse_qvalue_accept_list( header_value, item_parser=parse_mt_only )
    if k < len(header_value):
        raise ParseError('Accept header is invalid',header_value,k)

    ctlist = []
    for ct, ctparms, q, acptparms  in alist:
        if ctparms:
            ct.set_parameters( dict(ctparms) )
        ctlist.append( (ct, q, acptparms) )
    return ctlist


def parse_media_type(media_type, start=0, with_parameters=True):
    """Parses a media type (MIME type) designator into it's parts.

    Given a media type string, returns a nested tuple of it's parts.

        ((major,minor,parmlist), chars_consumed)

    where parmlist is a list of tuples of (parm_name, parm_value).
    Quoted-values are appropriately unquoted and unescaped.
    
    If 'with_parameters' is False, then parsing will stop immediately
    after the minor media type; and will not proceed to parse any
    of the semicolon-separated paramters.

    Examples:
        image/png -> (('image','png',[]), 9)
        text/plain; charset="utf-16be"
                  -> (('text','plain',[('charset,'utf-16be')]), 30)

    """

    s = media_type
    pos = start
    ctmaj, k = parse_token(s, pos)
    if k == 0:
        raise ParseError('Media type must be of the form "major/minor".', s, pos)
    pos += k
    if pos >= len(s) or s[pos] != '/':
        raise ParseError('Media type must be of the form "major/minor".', s, pos)
    pos += 1
    ctmin, k = parse_token(s, pos)
    if k == 0:
        raise ParseError('Media type must be of the form "major/minor".', s, pos)
    pos += k
    if with_parameters:
        parmlist, k = parse_parameter_list(s, pos)
        pos += k
    else:
        parmlist = []
    return ((ctmaj, ctmin, parmlist), pos - start)


def parse_parameter_list(s, start=0):
    """Parses a semicolon-separated 'parameter=value' list.

    Returns a tuple (parmlist, chars_consumed), where parmlist
    is a list of tuples (parm_name, parm_value).

    The parameter values will be unquoted and unescaped as needed.

    Empty parameters (as in ";;") are skipped, as is insignificant
    white space.  The list returned is kept in the same order as the
    parameters appear in the string.

    """
    pos = start
    parmlist = []
    while pos < len(s):
        while pos < len(s) and s[pos] in LWS:
            pos += 1 # skip whitespace
        if pos < len(s) and s[pos] == ';':
            pos += 1
            while pos < len(s) and s[pos] in LWS:
                pos += 1 # skip whitespace
        if pos >= len(s):
            break
        parmname, k = parse_token(s, pos)
        if parmname:
            pos += k
            while pos < len(s) and s[pos] in LWS:
                pos += 1 # skip whitespace
            if not (pos < len(s) and s[pos] == '='):
                raise ParseError('Expected an "=" after parameter name', s, pos)
            pos += 1
            while pos < len(s) and s[pos] in LWS:
                pos += 1 # skip whitespace
            parmval, k = parse_token_or_quoted_string( s, pos )
            pos += k
            parmlist.append( (parmname, parmval) )
        else:
            break
    return parmlist, pos - start


class content_type(object):
    """This class represents a media type (aka a MIME content type), including parameters.

    You initialize these by passing in a content-type declaration
    string, such as "text/plain; charset=ascii", to the constructor or
    to the set() method.  If you provide no string value, the object
    returned will represent the wildcard */* content type.

    Normally you will get the value back by using str(), or optionally
    you can access the components via the 'major', 'minor', 'media_type',
    or 'parmdict' members.

    """
    def __init__(self, content_type_string=None, with_parameters=True):
        """Create a new content_type object.

        See the set() method for a description of the arguments.
        """
        if content_type_string:
            self.set( content_type_string, with_parameters=with_parameters )
        else:
            self.set( '*/*' )

    def set_parameters(self, parameter_list_or_dict):
        """Sets the optional paramters based upon the parameter list.

        The paramter list should be a semicolon-separated name=value string.
        Any paramters which already exist on this object will be deleted,
        unless they appear in the given paramter_list.

        """
        if hasattr(parameter_list_or_dict, 'has_key'):
            # already a dictionary
            pl = parameter_list_or_dict
        else:
            pl, k = parse_parameter_list(parameter_list)
            if k < len(parameter_list):
                raise ParseError('Invalid parameter list',paramter_list,k)
        self.parmdict = dict(pl)

    def set(self, content_type_string, with_parameters=True):
        """Parses the content type string and sets this object to it's value.

        For a more complete description of the arguments, see the
        documentation for the parse_media_type() function in this module.
        """
        mt, k = parse_media_type( content_type_string, with_parameters=with_parameters )
        if k < len(content_type_string):
            raise ParseError('Not a valid content type',content_type_string, k)
        major, minor, pdict = mt
        self._set_major( major )
        self._set_minor( minor )
        self.parmdict = dict(pdict)
        
    def _get_major(self):
        return self._major
    def _set_major(self, s):
        s = s.lower()  # case-insentive
        if not is_token(s):
            raise ValueError('Major media type contains an invalid character')
        self._major = s

    def _get_minor(self):
        return self._minor
    def _set_minor(self, s):
        s = s.lower()  # case-insentive
        if not is_token(s):
            raise ValueError('Minor media type contains an invalid character')
        self._minor = s

    major = property(_get_major,_set_major,doc="Major media classification")
    minor = property(_get_minor,_set_minor,doc="Minor media sub-classification")

    def __str__(self):
        """String value."""
        s = '%s/%s' % (self.major, self.minor)
        if self.parmdict:
            extra = '; '.join([ '%s=%s' % (a[0],quote_string(a[1],False)) for a in self.parmdict.items()])
            s += '; ' + extra
        return s

    def __unicode__(self):
        """Unicode string value."""
        # In Python 3 this is probably unnecessary in general, this is just to avoid possible syntax issues. I.H.
        if PY3 :
            return str(self.__str__())
        else :
            return unicode(self.__str__())

    def __repr__(self):
        """Python representation of this object."""
        s = '%s(%s)' % (self.__class__.__name__, repr(self.__str__()))
        return s


    def __hash__(self):
        """Hash this object; the hash is dependent only upon the value."""
        return hash(str(self))

    def __getstate__(self):
        """Pickler"""
        return str(self)

    def __setstate__(self, state):
        """Unpickler"""
        self.set(state)

    def __len__(self):
        """Logical length of this media type.
        For example:
           len('*/*')  -> 0
           len('image/*') -> 1
           len('image/png') -> 2
           len('text/plain; charset=utf-8')  -> 3
           len('text/plain; charset=utf-8; filename=xyz.txt') -> 4

        """
        if self.major == '*':
            return 0
        elif self.minor == '*':
            return 1
        else:
            return 2 + len(self.parmdict)

    def __eq__(self, other):
        """Equality test.

        Note that this is an exact match, including any parameters if any.
        """
        return self.major == other.major and \
                   self.minor == other.minor and \
                   self.parmdict == other.parmdict

    def __ne__(self, other):
        """Inequality test."""
        return not self.__eq__(other)
            
    def _get_media_type(self):
        """Returns the media 'type/subtype' string, without parameters."""
        return '%s/%s' % (self.major, self.minor)

    media_type = property(_get_media_type, doc="Returns the just the media type 'type/subtype' without any paramters (read-only).")

    def is_wildcard(self):
        """Returns True if this is a 'something/*' media type.
        """
        return self.minor == '*'

    def is_universal_wildcard(self):
        """Returns True if this is the unspecified '*/*' media type.
        """
        return self.major == '*' and self.minor == '*'

    def is_composite(self):
        """Is this media type composed of multiple parts.
        """
        return self.major == 'multipart' or self.major == 'message'

    def is_xml(self):
        """Returns True if this media type is XML-based.

        Note this does not consider text/html to be XML, but
        application/xhtml+xml is.
        """
        return self.minor == 'xml' or self.minor.endswith('+xml')

# Some common media types
content_formdata = content_type('multipart/form-data')
content_urlencoded = content_type('application/x-www-form-urlencoded')
content_byteranges = content_type('multipart/byteranges') # RFC 2616 sect 14.16
content_opaque = content_type('application/octet-stream')
content_html = content_type('text/html')
content_xhtml = content_type('application/xhtml+xml')


def acceptable_content_type( accept_header, content_types, ignore_wildcard=True ):
    """Determines if the given content type is acceptable to the user agent.

    The accept_header should be the value present in the HTTP
    "Accept:" header.  In mod_python this is typically obtained from
    the req.http_headers_in table; in WSGI it is environ["Accept"];
    other web frameworks may provide other methods of obtaining it.

    Optionally the accept_header parameter can be pre-parsed, as
    returned from the parse_accept_header() function in this module.

    The content_types argument should either be a single MIME media
    type string, or a sequence of them.  It represents the set of
    content types that the caller (server) is willing to send.
    Generally, the server content_types should not contain any
    wildcarded values.

    This function determines which content type which is the most
    preferred and is acceptable to both the user agent and the server.
    If one is negotiated it will return a four-valued tuple like:

        (server_content_type, ua_content_range, qvalue, accept_parms)

    The first tuple value is one of the server's content_types, while
    the remaining tuple values descript which of the client's
    acceptable content_types was matched.  In most cases accept_parms
    will be an empty list (see description of parse_accept_header()
    for more details).

    If no content type could be negotiated, then this function will
    return None (and the caller should typically cause an HTTP 406 Not
    Acceptable as a response).

    Note that the wildcarded content type "*/*" sent by the client
    will be ignored, since it is often incorrectly sent by web
    browsers that don't really mean it.  To override this, call with
    ignore_wildcard=False.  Partial wildcards such as "image/*" will
    always be processed, but be at a lower priority than a complete
    matching type.

    See also: RFC 2616 section 14.1, and
    <http://www.iana.org/assignments/media-types/>

    """
    if _is_string(accept_header):
        accept_list = parse_accept_header(accept_header)
    else:
        accept_list = accept_header

    if _is_string(content_types):
        content_types = [content_types]

    server_ctlist = [content_type(ct) for ct in content_types]
    del ct

    #print 'AC', repr(accept_list)
    #print 'SV', repr(server_ctlist)

    best = None   # (content_type, qvalue, accept_parms, matchlen)

    for server_ct in server_ctlist:
        best_for_this = None
        for client_ct, qvalue, aargs in accept_list:
            if ignore_wildcard and client_ct.is_universal_wildcard():
                continue  # */* being ignored

            matchlen = 0 # how specifically this one matches (0 is a non-match)
            if client_ct.is_universal_wildcard():
                matchlen = 1   # */* is a 1
            elif client_ct.major == server_ct.major:
                if client_ct.minor == '*':  # something/* is a 2
                    matchlen = 2
                elif client_ct.minor == server_ct.minor: # something/something is a 3
                    matchlen = 3
                    # must make sure all the parms match too
                    for pname, pval in client_ct.parmdict.items():
                        sval = server_ct.parmdict.get(pname)
                        if pname == 'charset':
                            # special case for charset to match aliases
                            pval = canonical_charset(pval)
                            sval = canonical_charset(sval)
                        if sval == pval:
                            matchlen = matchlen + 1
                        else:
                            matchlen = 0
                            break
                else:
                    matchlen = 0

            #print 'S',server_ct,'  C',client_ct,'  M',matchlen,'Q',qvalue
            if matchlen > 0:
                if not best_for_this \
                       or matchlen > best_for_this[-1] \
                       or (matchlen == best_for_this[-1] and qvalue > best_for_this[2]):
                    # This match is better
                    best_for_this = (server_ct, client_ct, qvalue, aargs, matchlen)
                    #print 'BEST2 NOW', repr(best_for_this)
        if not best or \
               (best_for_this and best_for_this[2] > best[2]):
            best = best_for_this
            #print 'BEST NOW', repr(best)
    if not best or best[1] <= 0:
        return None
    return best[:-1]


# Aliases of common charsets, see <http://www.iana.org/assignments/character-sets>.
character_set_aliases = {
    'ASCII': 'US-ASCII',
    'ISO646-US': 'US-ASCII',
    'IBM367': 'US-ASCII',
    'CP367': 'US-ASCII',
    'CSASCII': 'US-ASCII',
    'ANSI_X3.4-1968': 'US-ASCII',
    'ISO_646.IRV:1991': 'US-ASCII',

    'UTF7': 'UTF-7',

    'UTF8': 'UTF-8',

    'UTF16': 'UTF-16',
    'UTF16LE': 'UTF-16LE',
    'UTF16BE': 'UTF-16BE',

    'UTF32': 'UTF-32',
    'UTF32LE': 'UTF-32LE',
    'UTF32BE': 'UTF-32BE',

    'UCS2': 'ISO-10646-UCS-2',
    'UCS_2': 'ISO-10646-UCS-2',
    'UCS-2': 'ISO-10646-UCS-2',
    'CSUNICODE': 'ISO-10646-UCS-2',

    'UCS4': 'ISO-10646-UCS-4',
    'UCS_4': 'ISO-10646-UCS-4',
    'UCS-4': 'ISO-10646-UCS-4',
    'CSUCS4': 'ISO-10646-UCS-4',

    'ISO_8859-1': 'ISO-8859-1',
    'LATIN1': 'ISO-8859-1',
    'CP819': 'ISO-8859-1',
    'IBM819': 'ISO-8859-1',

    'ISO_8859-2': 'ISO-8859-2',
    'LATIN2': 'ISO-8859-2',

    'ISO_8859-3': 'ISO-8859-3',
    'LATIN3': 'ISO-8859-3',

    'ISO_8859-4': 'ISO-8859-4',
    'LATIN4': 'ISO-8859-4',

    'ISO_8859-5': 'ISO-8859-5',
    'CYRILLIC': 'ISO-8859-5',

    'ISO_8859-6': 'ISO-8859-6',
    'ARABIC': 'ISO-8859-6',
    'ECMA-114': 'ISO-8859-6',

    'ISO_8859-6-E': 'ISO-8859-6-E',
    'ISO_8859-6-I': 'ISO-8859-6-I',

    'ISO_8859-7': 'ISO-8859-7',
    'GREEK': 'ISO-8859-7',
    'GREEK8': 'ISO-8859-7',
    'ECMA-118': 'ISO-8859-7',

    'ISO_8859-8': 'ISO-8859-8',
    'HEBREW': 'ISO-8859-8',

    'ISO_8859-8-E': 'ISO-8859-8-E',
    'ISO_8859-8-I': 'ISO-8859-8-I',

    'ISO_8859-9': 'ISO-8859-9',
    'LATIN5': 'ISO-8859-9',

    'ISO_8859-10': 'ISO-8859-10',
    'LATIN6': 'ISO-8859-10',

    'ISO_8859-13': 'ISO-8859-13',

    'ISO_8859-14': 'ISO-8859-14',
    'LATIN8': 'ISO-8859-14',

    'ISO_8859-15': 'ISO-8859-15',
    'LATIN9': 'ISO-8859-15',

    'ISO_8859-16': 'ISO-8859-16',
    'LATIN10': 'ISO-8859-16',
    }

def canonical_charset( charset ):
    """Returns the canonical or preferred name of a charset.

    Additional character sets can be recognized by this function by
    altering the character_set_aliases dictionary in this module.
    Charsets which are not recognized are simply converted to
    upper-case (as charset names are always case-insensitive).
    
    See <http://www.iana.org/assignments/character-sets>.

    """
    # It would be nice to use Python's codecs modules for this, but
    # there is no fixed public interface to it's alias mappings.
    if not charset:
        return charset
    uc = charset.upper()
    uccon = character_set_aliases.get( uc, uc )
    return uccon


def acceptable_charset( accept_charset_header, charsets, ignore_wildcard=True, default='ISO-8859-1' ):
    """
    Determines if the given charset is acceptable to the user agent.

    The accept_charset_header should be the value present in the HTTP
    "Accept-Charset:" header.  In mod_python this is typically
    obtained from the req.http_headers table; in WSGI it is
    environ["Accept-Charset"]; other web frameworks may provide other
    methods of obtaining it.

    Optionally the accept_charset_header parameter can instead be the
    list returned from the parse_accept_header() function in this
    module.

    The charsets argument should either be a charset identifier string,
    or a sequence of them.

    This function returns the charset identifier string which is the
    most prefered and is acceptable to both the user agent and the
    caller.  It will return the default value if no charset is negotiable.
    
    Note that the wildcarded charset "*" will be ignored.  To override
    this, call with ignore_wildcard=False.

    See also: RFC 2616 section 14.2, and
    <http://www.iana.org/assignments/character-sets>

    """
    if default:
        default = _canonical_charset(default)

    if _is_string(accept_charset_header):
        accept_list = parse_accept_header(accept_charset_header)
    else:
        accept_list = accept_charset_header

    if _is_string(charsets):
        charsets = [_canonical_charset(charsets)]
    else:
        charsets = [_canonical_charset(c) for c in charsets]

    # Note per RFC that 'ISO-8859-1' is special, and is implictly in the
    # accept list with q=1; unless it is already in the list, or '*' is in the list.

    best = None
    for c, qvalue, junk in accept_list:
        if c == '*':
            default = None
            if ignore_wildcard:
                continue
            if not best or qvalue > best[1]:
                best = (c, qvalue)
        else:
            c = _canonical_charset(c)
            for test_c in charsets:
                if c == default:
                    default = None
                if c == test_c and (not best or best[0]=='*' or qvalue > best[1]):
                    best = (c, qvalue)
    if default and default in [test_c.upper() for test_c in charsets]:
        best = (default, 1)
    if best[0] == '*':
        best = (charsets[0], best[1])
    return best



class language_tag(object):
    """This class represents an RFC 3066 language tag.

    Initialize objects of this class with a single string representing
    the language tag, such as "en-US".
        
    Case is insensitive. Wildcarded subtags are ignored or stripped as
    they have no significance, so that "en-*" is the same as "en".
    However the universal wildcard "*" language tag is kept as-is.

    Note that although relational operators such as < are defined,
    they only form a partial order based upon specialization.

    Thus for example,
         "en" <= "en-US"
    but,
         not "en" <= "de", and
         not "de" <= "en".

    """

    def __init__(self, tagname):
        """Initialize objects of this class with a single string representing
        the language tag, such as "en-US".  Case is insensitive.

        """

        self.parts = tagname.lower().split('-')
        while len(self.parts) > 1 and self.parts[-1] == '*':
            del self.parts[-1]

    def __len__(self):
        """Number of subtags in this tag."""
        if len(self.parts) == 1 and self.parts[0] == '*':
            return 0
        return len(self.parts)

    def __str__(self):
        """The standard string form of this language tag."""
        a = []
        if len(self.parts) >= 1:
            a.append(self.parts[0])
        if len(self.parts) >= 2:
            if len(self.parts[1]) == 2:
                a.append( self.parts[1].upper() )
            else:
                a.append( self.parts[1] )
        a.extend( self.parts[2:] )
        return '-'.join(a)

    def __unicode__(self):
        """The unicode string form of this language tag."""
        # Probably unnecessary in Python 3
        if PY3 :
            return str(self.__str__())
        else :
            return unicode(self.__str__())

    def __repr__(self):
        """The python representation of this language tag."""
        s = '%s("%s")' % (self.__class__.__name__, self.__str__())
        return s

    def superior(self):
        """Returns another instance of language_tag which is the superior.

        Thus en-US gives en, and en gives *.

        """
        if len(self) <= 1:
            return self.__class__('*')
        return self.__class__( '-'.join(self.parts[:-1]) )

    def all_superiors(self, include_wildcard=False):
        """Returns a list of this language and all it's superiors.

        If include_wildcard is False, then "*" will not be among the
        output list, unless this language is itself "*".

        """
        langlist = [ self ]
        l = self
        while not l.is_universal_wildcard():
            l = l.superior()
            if l.is_universal_wildcard() and not include_wildcard:
                continue
            langlist.append(l)
        return langlist
                
    def is_universal_wildcard(self):
        """Returns True if this language tag represents all possible
        languages, by using the reserved tag of "*".

        """
        return len(self.parts) == 1 and self.parts[0] == '*'

    def dialect_of(self, other, ignore_wildcard=True):
        """Is this language a dialect (or subset/specialization) of another.

        This method returns True if this language is the same as or a
        specialization (dialect) of the other language_tag.

        If ignore_wildcard is False, then all languages will be
        considered to be a dialect of the special language tag of "*".

        """
        if not ignore_wildcard and self.is_universal_wildcard():
            return True
        for i in range( min(len(self), len(other)) ):
            if self.parts[i] != other.parts[i]:
                return False
        if len(self) >= len(other):
            return True
        return False

    def __eq__(self, other):
        """== operator. Are the two languages the same?"""

        return self.parts == other.parts

    def __neq__(self, other):
        """!= operator. Are the two languages different?"""

        return not self.__eq__(other)

    def __lt__(self, other):
        """< operator. Returns True if the other language is a more
        specialized dialect of this one."""

        return other.dialect_of(self) and self != other

    def __le__(self, other):
        """<= operator. Returns True if the other language is the same
        as or a more specialized dialect of this one."""
        return other.dialect_of(self)

    def __gt__(self, other):
        """> operator.  Returns True if this language is a more
        specialized dialect of the other one."""

        return self.dialect_of(other) and self != other

    def __ge__(self, other):
        """>= operator.  Returns True if this language is the same as
        or a more specialized dialect of the other one."""

        return self.dialect_of(other)


def parse_accept_language_header( header_value ):
    """Parses the Accept-Language header.

    Returns a list of tuples, each like:

        (language_tag, qvalue, accept_parameters)

    """
    alist, k = parse_qvalue_accept_list( header_value)
    if k < len(header_value):
        raise ParseError('Accept-Language header is invalid',header_value,k)

    langlist = []
    for token, langparms, q, acptparms in alist:
        if langparms:
            raise ParseError('Language tag may not have any parameters',header_value,0)
        lang = language_tag( token )
        langlist.append( (lang, q, acptparms) )

    return langlist


def acceptable_language( accept_header, server_languages, ignore_wildcard=True, assume_superiors=True ):
    """Determines if the given language is acceptable to the user agent.

    The accept_header should be the value present in the HTTP
    "Accept-Language:" header.  In mod_python this is typically
    obtained from the req.http_headers_in table; in WSGI it is
    environ["Accept-Language"]; other web frameworks may provide other
    methods of obtaining it.

    Optionally the accept_header parameter can be pre-parsed, as
    returned by the parse_accept_language_header() function defined in
    this module.

    The server_languages argument should either be a single language
    string, a language_tag object, or a sequence of them.  It
    represents the set of languages that the server is willing to
    send to the user agent.

    Note that the wildcarded language tag "*" will be ignored.  To
    override this, call with ignore_wildcard=False, and even then
    it will be the lowest-priority choice regardless of it's
    quality factor (as per HTTP spec).

    If the assume_superiors is True then it the languages that the
    browser accepts will automatically include all superior languages.
    Any superior languages which must be added are done so with one
    half the qvalue of the language which is present.  For example, if
    the accept string is "en-US", then it will be treated as if it
    were "en-US, en;q=0.5".  Note that although the HTTP 1.1 spec says
    that browsers are supposed to encourage users to configure all
    acceptable languages, sometimes they don't, thus the ability
    for this function to assume this.  But setting assume_superiors
    to False will insure strict adherence to the HTTP 1.1 spec; which
    means that if the browser accepts "en-US", then it will not
    be acceptable to send just "en" to it.

    This function returns the language which is the most prefered and
    is acceptable to both the user agent and the caller.  It will
    return None if no language is negotiable, otherwise the return
    value is always an instance of language_tag.

    See also: RFC 3066 <http://www.ietf.org/rfc/rfc3066.txt>, and
    ISO 639, links at <http://en.wikipedia.org/wiki/ISO_639>, and
    <http://www.iana.org/assignments/language-tags>.
    
    """
    # Note special instructions from RFC 2616 sect. 14.1:
    #   "The language quality factor assigned to a language-tag by the
    #   Accept-Language field is the quality value of the longest
    #   language- range in the field that matches the language-tag."

    if _is_string(accept_header):
        accept_list = parse_accept_language_header(accept_header)
    else:
        accept_list = accept_header

    # Possibly add in any "missing" languages that the browser may
    # have forgotten to include in the list. Insure list is sorted so
    # more general languages come before more specific ones.

    accept_list.sort()
    all_tags = [a[0] for a in accept_list]
    if assume_superiors:
        to_add = []
        for langtag, qvalue, aargs in accept_list:
            if len(langtag) >= 2:
                for suptag in langtag.all_superiors( include_wildcard=False ):
                    if suptag not in all_tags:
                        # Add in superior at half the qvalue
                        to_add.append( (suptag, qvalue / 2, '') )
                        all_tags.append( suptag )
        accept_list.extend( to_add )

    # Convert server_languages to a list of language_tags
    if _is_string(server_languages):
        server_languages = [language_tag(server_languages)]
    elif isinstance(server_languages, language_tag):
        server_languages = [server_languages]
    else:
        server_languages = [language_tag(lang) for lang in server_languages]

    # Select the best one
    best = None  # tuple (langtag, qvalue, matchlen)
    
    for langtag, qvalue, aargs in accept_list:
        # aargs is ignored for Accept-Language
        if qvalue <= 0:
            continue # UA doesn't accept this language

        if ignore_wildcard and langtag.is_universal_wildcard():
            continue  # "*" being ignored

        for svrlang in server_languages:
            # The best match is determined first by the quality factor,
            # and then by the most specific match.

            matchlen = -1 # how specifically this one matches (0 is a non-match)
            if svrlang.dialect_of( langtag, ignore_wildcard=ignore_wildcard ):
                matchlen = len(langtag)
                if not best \
                       or matchlen > best[2] \
                       or (matchlen == best[2] and qvalue > best[1]):
                    # This match is better
                    best = (langtag, qvalue, matchlen)
    if not best:
        return None
    return best[0]


# Clean up global namespace
try:
    if __emulating_set:
        del set
        del frozenset
except NameError:
    pass

# end of file

########NEW FILE########
__FILENAME__ = atom
# -*- coding: utf-8 -*-
"""
Simple transfomer for Atom: the C{@typeof=""} is added to the C{<entry>} element (unless something is already there).

@summary: Add a top "about" to <head> and <body>
@requires: U{RDFLib package<http://rdflib.net>}
@organization: U{World Wide Web Consortium<http://www.w3.org>}
@author: U{Ivan Herman<a href="http://www.w3.org/People/Ivan/">}
@license: This software is available for use under the
U{W3C SOFTWARE NOTICE AND LICENSE<href="http://www.w3.org/Consortium/Legal/2002/copyright-software-20021231">}
@contact: Ivan Herman, ivan@w3.org
"""

"""
$Id: atom.py,v 1.3 2013-01-07 13:03:16 ivan Exp $
$Date: 2013-01-07 13:03:16 $
"""

def atom_add_entry_type(node, state) :
	"""
	@param node: the current node that could be modified
	@param state: current state
	@type state: L{Execution context<pyRdfa.state.ExecutionContext>}
	"""
	def res_set(node) :	
		return True in [ node.hasAttribute(a) for a in ["resource", "about", "href", "src"] ]
	
	if node.tagName == "entry" and not res_set(node) and node.hasAttribute("typeof") == False :
		node.setAttribute("typeof","")

########NEW FILE########
__FILENAME__ = html5
# -*- coding: utf-8 -*-
"""
Simple transfomer for HTML5: add a @src for any @data, add a @content for the @value attribute of the <data> element, and interpret the <time> element.

@summary: Add a top "about" to <head> and <body>
@requires: U{RDFLib package<http://rdflib.net>}
@organization: U{World Wide Web Consortium<http://www.w3.org>}
@author: U{Ivan Herman<a href="http://www.w3.org/People/Ivan/">}
@license: This software is available for use under the
U{W3C SOFTWARE NOTICE AND LICENSE<href="http://www.w3.org/Consortium/Legal/2002/copyright-software-20021231">}
@contact: Ivan Herman, ivan@w3.org
"""

"""
$Id: html5.py,v 1.13 2013-02-01 10:53:48 ivan Exp $
$Date: 2013-02-01 10:53:48 $
"""
try :
	from functools import reduce
except :
	# Not important. This import is necessary in Python 3 only and the newer versions of Python 2.X it is there
	# for a forward compatibility with Python 3
	pass

# The handling of datatime is a little bit more complex... better put this in a separate function for a better management
from datetime import datetime
import re
datetime_type   = "http://www.w3.org/2001/XMLSchema#dateTime"
time_type 	    = "http://www.w3.org/2001/XMLSchema#time"
date_type       = "http://www.w3.org/2001/XMLSchema#date"
date_gYear      = "http://www.w3.org/2001/XMLSchema#gYear"
date_gYearMonth = "http://www.w3.org/2001/XMLSchema#gYearMonth"
date_gMonthDay  = "http://www.w3.org/2001/XMLSchema#gMonthDay"
duration_type   = "http://www.w3.org/2001/XMLSchema#duration"
plain           = "plain"

handled_time_types = [ datetime_type, time_type, date_type, date_gYear, date_gYearMonth, date_gMonthDay, duration_type ]

_formats = {
	date_gMonthDay	  : [ "%m-%d" ],
	date_gYearMonth	  : [ "%Y-%m"],
	date_gYear     	  : [ "%Y" ],
	date_type      	  : [ "%Y-%m-%d", "%Y-%m-%dZ" ],
	time_type      	  : [ "%H:%M",
					      "%H:%M:%S",
					      "%H:%M:%SZ",						
					      "%H:%M:%S.%f" ],
	datetime_type  	  : [ "%Y-%m-%dT%H:%M",
					      "%Y-%m-%dT%H:%M:%S",
					      "%Y-%m-%dT%H:%M:%S.%f",
					      "%Y-%m-%dT%H:%MZ",
					      "%Y-%m-%dT%H:%M:%SZ",
					      "%Y-%m-%dT%H:%M:%S.%fZ" ],
	duration_type     : [ "P%dD",
						  "P%YY%mM%dD",
						  "P%YY%mM",
						  "P%YY%dD",
						  "P%YY",
						  "P%mM",
						  "P%mM%dD",
						 ],
}

_dur_times = [ "%HH%MM%SS", "%HH", "%MM", "%SS", "%HH%MM", "%HH%SS", "%MM%SS" ]

def _format_test(string) :
	"""
	Tests the string format to see whether it fits one of the time datatypes
	@param string: attribute value to test
	@return: a URI for the xsd datatype or the string 'plain'
	"""
	# Try to get the easy cases:
	for key in _formats :
		for format in _formats[key] :
			try :
				# try to check if the syntax is fine
				d = datetime.strptime(string, format)
				# bingo!
				return key
			except ValueError :
				pass
			
	# Now come the special cases:-(
	# Check first for the duration stuff, that is the nastiest.
	if len(string) > 2 and string[0] == 'P' or (string [0] == '-' and string[1] == 'P') :
		# this is meant to be a duration type
		# first of all, get rid of the leading '-' and check again
		if string[0] == '-' :
			for format in _formats[duration_type] :
				try :
					# try to check if the syntax is fine
					d = datetime.strptime(string, format)
					# bingo!
					return duration_type
				except ValueError :
					pass
		# Let us see if the value contains a separate time portion, and cut that one
		durs = string.split('T')
		if len(durs) == 2 :
			# yep, so we should check again
			dur = durs[0]
			tm  = durs[1]
			# Check the duration part
			td = False
			for format in _formats[duration_type] :
				try :
					# try to check if the syntax is fine
					d = datetime.strptime(dur, format)
					# bingo!
					td = True
					break
				except ValueError :
					pass
			if td == True :
				# Getting there...
				for format in _dur_times :
					try :
						# try to check if the syntax is fine
						d = datetime.strptime(tm, format)
						# bingo!
						return duration_type
					except ValueError :
						pass
			# something went wrong...
			return plain			
		else :
			# Well, no more tricks, this is a plain type
			return plain
	
	
	# If we got here, we should check the time zone
	# there is a discrepancy betwen the python and the HTML5/XSD lexical string,
	# which means that this has to handled separately for the date and the timezone portion
	try :
		# The time-zone-less portion of the string
		str = string[0:-6]
		# The time-zone portion
		tz = string[-5:]
		try :
			t = datetime.strptime(tz,"%H:%M")
		except ValueError :
			# Bummer, this is not a correct time
			return plain
		# The time-zone is fine, the datetime portion has to be checked		
		for format in _formats[datetime_type] :
			try :
				# try to check if it is fine
				d = datetime.strptime(str, format)
				# Bingo!
				return datetime_type
			except ValueError :
				pass
	except :
		pass
	return plain

def html5_extra_attributes(node, state) :
	"""
	@param node: the current node that could be modified
	@param state: current state
	@type state: L{Execution context<pyRdfa.state.ExecutionContext>}
	"""
	def _get_literal(Pnode):
		"""
		Get (recursively) the full text from a DOM Node.
	
		@param Pnode: DOM Node
		@return: string
		"""
		rc = ""
		for node in Pnode.childNodes:
			if node.nodeType == node.TEXT_NODE:
				rc = rc + node.data
			elif node.nodeType == node.ELEMENT_NODE :
				rc = rc + self._get_literal(node)
		if state.options.space_preserve :
			return rc
		else :
			return re.sub(r'(\r| |\n|\t)+'," ",rc).strip()
		#return re.sub(r'(\r| |\n|\t)+',"",rc).strip()
	# end _getLiteral

	def _set_time(value) :
		if not node.hasAttribute("datatype") :			
			# Check the datatype:
			dt = _format_test(value)
			if dt != plain :
				node.setAttribute("datatype",dt)
		# Finally, set the value itself
		node.setAttribute("content",value)
	# end _set_time

	if not node.hasAttribute("content") :
		# @content has top priority over the others...
		if node.hasAttribute("datetime") :
			_set_time( node.getAttribute("datetime") )
		elif node.hasAttribute("dateTime") :
			_set_time( node.getAttribute("dateTime") )
		elif node.tagName == "time" :
			# Note that a possible @datetime value has already been taken care of
			_set_time( _get_literal(node) )
		
def remove_rel(node, state):
	"""
	If @property and @rel/@rev are on the same element, then only CURIE and URI can appear as a rel/rev value.
	
	@param node: the current node that could be modified
	@param state: current state
	@type state: L{Execution context<pyRdfa.state.ExecutionContext>}
	"""
	from ..termorcurie import termname
	def _massage_node(node,attr) :
		"""The real work for remove_rel is done here, parametrized with @rel and @rev"""
		if node.hasAttribute("property") and node.hasAttribute(attr) :
			vals = node.getAttribute(attr).strip().split()
			if len(vals) != 0 :
				final_vals = [ v for v in vals if not termname.match(v) ]
				if len(final_vals) == 0 :
					node.removeAttribute(attr)
				else :
					node.setAttribute(attr, reduce(lambda x,y: x+' '+y,final_vals))
	
	_massage_node(node, "rev")
	_massage_node(node, "rel")
	
	
	
	
	
	
	
	
	
	
	
	
	

########NEW FILE########
__FILENAME__ = initialcontext
# -*- coding: utf-8 -*-
"""
Built-in version of the initial contexts for RDFa Core, and RDFa + HTML

@summary: Management of vocabularies, terms, and their mapping to URI-s.
@requires: U{RDFLib package<http://rdflib.net>}
@organization: U{World Wide Web Consortium<http://www.w3.org>}
@author: U{Ivan Herman<a href="http://www.w3.org/People/Ivan/">}
@license: This software is available for use under the
U{W3C SOFTWARE NOTICE AND LICENSE<href="http://www.w3.org/Consortium/Legal/2002/copyright-software-20021231">}

@var initial_context: dictionary for all the initial context data, keyed through the context URI-s
"""

class Wrapper :
	pass
	
initial_context = {
	"http://www.w3.org/2011/rdfa-context/rdfa-1.1" 		 : Wrapper(),
	"http://www.w3.org/2011/rdfa-context/xhtml-rdfa-1.1" : Wrapper(),
}

initial_context["http://www.w3.org/2011/rdfa-context/rdfa-1.1"].ns = {
	'cat'		: 'http://www.w3.org/ns/dcat#',
    'qb'		: 'http://purl.org/linked-data/cube#',
    'org'		: 'http://www.w3.org/ns/org#',
	'owl'		: 'http://www.w3.org/2002/07/owl#',
	'gr'		: 'http://purl.org/goodrelations/v1#',
	'ctag'		: 'http://commontag.org/ns#',
	'cc'		: 'http://creativecommons.org/ns#',
	'grddl'		: 'http://www.w3.org/2003/g/data-view#',
	'rif'		: 'http://www.w3.org/2007/rif#',
	'sioc'		: 'http://rdfs.org/sioc/ns#',
	'skos'		: 'http://www.w3.org/2004/02/skos/core#',
	'xml'		: 'http://www.w3.org/XML/1998/namespace',
	'rr'		: 'http://www.w3.org/ns/r2rml#',
	'rdfs'		: 'http://www.w3.org/2000/01/rdf-schema#',
	'rev'		: 'http://purl.org/stuff/rev#',
	'rdfa'		: 'http://www.w3.org/ns/rdfa#',
	'dc'		: 'http://purl.org/dc/terms/',
	'dcterms'	: 'http://purl.org/dc/terms/',
	'dc11'	    : 'http://purl.org/dc/elements/1.1/',	
	'foaf'		: 'http://xmlns.com/foaf/0.1/',
	'void'		: 'http://rdfs.org/ns/void#',
	'ical'		: 'http://www.w3.org/2002/12/cal/icaltzd#',
	'vcard'		: 'http://www.w3.org/2006/vcard/ns#',
	'wdrs'		: 'http://www.w3.org/2007/05/powder-s#',
	'og'		: 'http://ogp.me/ns#',
	'wdr'		: 'http://www.w3.org/2007/05/powder#',
	'rdf'		: 'http://www.w3.org/1999/02/22-rdf-syntax-ns#',
	'xhv'		: 'http://www.w3.org/1999/xhtml/vocab#',
	'xsd'		: 'http://www.w3.org/2001/XMLSchema#',
	'v'			: 'http://rdf.data-vocabulary.org/#',
	'skosxl'	: 'http://www.w3.org/2008/05/skos-xl#',
	'schema'	: 'http://schema.org/',
	'ma'		: 'http://www.w3.org/ns/ma-ont#',
	'sd'        : 'http://www.w3.org/ns/sparql-service-description#',
	'prov'      : 'http://www.w3.org/ns/prov#',
}

initial_context["http://www.w3.org/2011/rdfa-context/rdfa-1.1"].terms = {
	'describedby'	: 'http://www.w3.org/2007/05/powder-s#describedby',
	'role'			: 'http://www.w3.org/1999/xhtml/vocab#role',
	'license'		: 'http://www.w3.org/1999/xhtml/vocab#license',
}

initial_context["http://www.w3.org/2011/rdfa-context/rdfa-1.1"].vocabulary = ""

initial_context["http://www.w3.org/2011/rdfa-context/xhtml-rdfa-1.1"].ns = {
}

initial_context["http://www.w3.org/2011/rdfa-context/xhtml-rdfa-1.1"].vocabulary = ""

initial_context["http://www.w3.org/2011/rdfa-context/xhtml-rdfa-1.1"].terms = {
	'alternate'				: 'http://www.w3.org/1999/xhtml/vocab#alternate',
	'appendix'				: 'http://www.w3.org/1999/xhtml/vocab#appendix',
	'cite'					: 'http://www.w3.org/1999/xhtml/vocab#cite',
	'bookmark'				: 'http://www.w3.org/1999/xhtml/vocab#bookmark',
	'chapter'				: 'http://www.w3.org/1999/xhtml/vocab#chapter',
	'contents'				: 'http://www.w3.org/1999/xhtml/vocab#contents',
	'copyright'				: 'http://www.w3.org/1999/xhtml/vocab#copyright',
	'glossary'				: 'http://www.w3.org/1999/xhtml/vocab#glossary',
	'help'					: 'http://www.w3.org/1999/xhtml/vocab#help',
	'icon'					: 'http://www.w3.org/1999/xhtml/vocab#icon',
	'index'					: 'http://www.w3.org/1999/xhtml/vocab#index',
	'meta'					: 'http://www.w3.org/1999/xhtml/vocab#meta',
	'next'					: 'http://www.w3.org/1999/xhtml/vocab#next',
	'license'				: 'http://www.w3.org/1999/xhtml/vocab#license',
	'p3pv1'					: 'http://www.w3.org/1999/xhtml/vocab#p3pv1',
	'prev'					: 'http://www.w3.org/1999/xhtml/vocab#prev',
	'previous'				: 'http://www.w3.org/1999/xhtml/vocab#previous',
	'role'					: 'http://www.w3.org/1999/xhtml/vocab#role',
	'section'				: 'http://www.w3.org/1999/xhtml/vocab#section',
	'stylesheet'			: 'http://www.w3.org/1999/xhtml/vocab#stylesheet',
	'subsection'			: 'http://www.w3.org/1999/xhtml/vocab#subsection',
	'start'					: 'http://www.w3.org/1999/xhtml/vocab#start',
	'up'					: 'http://www.w3.org/1999/xhtml/vocab#up',
	'last'					: 'http://www.w3.org/1999/xhtml/vocab#last',
	'first'					: 'http://www.w3.org/1999/xhtml/vocab#first',
	'top'					: 'http://www.w3.org/1999/xhtml/vocab#top',
}


########NEW FILE########
__FILENAME__ = options
# -*- coding: utf-8 -*-
"""
L{Options} class: collect the possible options that govern the parsing possibilities. The module also includes the L{ProcessorGraph} class that handles the processor graph, per RDFa 1.1 (i.e., the graph containing errors and warnings). 

@summary: RDFa parser (distiller)
@requires: U{RDFLib<http://rdflib.net>}
@organization: U{World Wide Web Consortium<http://www.w3.org>}
@author: U{Ivan Herman<a href="http://www.w3.org/People/Ivan/">}
@license: This software is available for use under the
U{W3C SOFTWARE NOTICE AND LICENSE<href="http://www.w3.org/Consortium/Legal/2002/copyright-software-20021231">}
"""

"""
$Id: options.py,v 1.20 2013-10-16 11:48:54 ivan Exp $ $Date: 2013-10-16 11:48:54 $
"""

import sys, datetime

import rdflib
from rdflib	import URIRef
from rdflib	import Literal
from rdflib	import BNode
from rdflib	import Namespace
if rdflib.__version__ >= "3.0.0" :
	from rdflib	import Graph
	from rdflib	import RDF  as ns_rdf
	from rdflib	import RDFS as ns_rdfs
else :
	from rdflib.Graph	import Graph
	from rdflib.RDFS	import RDFSNS as ns_rdfs
	from rdflib.RDF		import RDFNS  as ns_rdf

from .host 	import HostLanguage, MediaTypes, content_to_host_language, predefined_1_0_rel, require_embedded_rdf
from .		import ns_xsd, ns_distill, ns_rdfa
from . 		import RDFA_Error, RDFA_Warning, RDFA_Info
from .transform.lite import lite_prune

ns_dc = Namespace("http://purl.org/dc/terms/")
ns_ht = Namespace("http://www.w3.org/2006/http#")

class ProcessorGraph :
	"""Wrapper around the 'processor graph', ie, the (RDF) Graph containing the warnings,
	error messages, and informational messages.
	"""
	def __init__(self) :
		self.graph = Graph()
		
	def add_triples(self, msg, top_class, info_class, context, node) :
		"""
		Add an error structure to the processor graph: a bnode with a number of predicates. The structure
		follows U{the processor graph vocabulary<http://www.w3.org/2010/02/rdfa/wiki/Processor_Graph_Vocabulary>} as described
		on the RDFa WG Wiki page.
		
		@param msg: the core error message, added as an object to a dc:description
		@param top_class: Error, Warning, or Info; an explicit rdf:type added to the bnode
		@type top_class: URIRef
		@param info_class: An additional error class, added as an rdf:type to the bnode in case it is not None
		@type info_class: URIRef
		@param context: An additional information added, if not None, as an object with rdfa:context as a predicate
		@type context: either an URIRef or a URI String (an URIRef will be created in the second case)
		@param node: The node's element name that contains the error
		@type node: string
		@return: the bnode that serves as a subject for the errors. The caller may add additional information
		@rtype: BNode
		"""
		# Lazy binding of relevant prefixes
		self.graph.bind("dcterms", ns_dc)
		self.graph.bind("pyrdfa",  ns_distill)
		self.graph.bind("rdf",     ns_rdf)
		self.graph.bind("rdfa",    ns_rdfa)
		self.graph.bind("ht",      ns_ht)
		self.graph.bind("xsd",     ns_xsd)
		# Python 3 foolproof way
		try :
			is_context_string = isinstance(context, basestring)
		except :
			is_context_string = isinstance(context, str)
		
		bnode = BNode()
		
		if node != None:
			try :
				full_msg = "[In element '%s'] %s" % (node.nodeName, msg)
			except :
				full_msg = "[In element '%s'] %s" % (node, msg)
		else :
			full_msg = msg
		
		self.graph.add((bnode, ns_rdf["type"], top_class))
		if info_class :
			self.graph.add((bnode, ns_rdf["type"], info_class))
		self.graph.add((bnode, ns_dc["description"], Literal(full_msg)))
		self.graph.add((bnode, ns_dc["date"], Literal(datetime.datetime.utcnow().isoformat(),datatype=ns_xsd["dateTime"])))
		if context and (isinstance(context,URIRef) or is_context_string):
			htbnode = BNode()
			self.graph.add( (bnode,   ns_rdfa["context"],htbnode) )
			self.graph.add( (htbnode, ns_rdf["type"], ns_ht["Request"]) )
			self.graph.add( (htbnode, ns_ht["requestURI"], Literal("%s" % context)) )
		return bnode
	
	def add_http_context(self, subj, http_code) :
		"""
		Add an additional HTTP context to a message with subject in C{subj}, using the U{<http://www.w3.org/2006/http#>}
		vocabulary. Typically used to extend an error structure, as created by L{add_triples}.
		
		@param subj: an RDFLib resource, typically a blank node
		@param http_code: HTTP status code
		"""
		bnode = BNode()
		self.graph.add((subj, ns_rdfa["context"], bnode))
		self.graph.add((bnode, ns_rdf["type"], ns_ht["Response"]))
		self.graph.add((bnode, ns_ht["responseCode"], URIRef("http://www.w3.org/2006/http#%s" % http_code)))

class Options :
	"""Settable options. An instance of this class is stored in
	the L{execution context<ExecutionContext>} of the parser.

	@ivar space_preserve: whether plain literals should preserve spaces at output or not
	@type space_preserve: Boolean
	
	@ivar output_default_graph: whether the 'default' graph should be returned to the user
	@type output_default_graph: Boolean
	
	@ivar output_processor_graph: whether the 'processor' graph should be returned to the user
	@type output_processor_graph: Boolean
	
	@ivar processor_graph: the 'processor' Graph
	@type processor_graph: L{ProcessorGraph}
	
	@ivar transformers: extra transformers
	@type transformers: list
	
	@ivar vocab_cache_report: whether the details of vocabulary file caching process should be reported as information (mainly for debug)
	@type vocab_cache_report: Boolean
	
	@ivar refresh_vocab_cache: whether the caching checks of vocabs should be by-passed, ie, if caches should be re-generated regardless of the stored date (important for vocab development)
	@type refresh_vocab_cache: Boolean
	
	@ivar embedded_rdf: whether embedded RDF (ie, turtle in an HTML script element or an RDF/XML content in SVG) should be extracted and added to the final graph. This is a non-standard option...
	@type embedded_rdf: Boolean
	
	@ivar vocab_expansion: whether the @vocab elements should be expanded and a mini-RDFS processing should be done on the merged graph
	@type vocab_expansion: Boolean
	
	@ivar vocab_cache: whether the system should use the vocabulary caching mechanism when expanding via the mini-RDFS, or should just fetch the graphs every time
	@type vocab_cache: Boolean
	
	@ivar host_language: the host language for the RDFa attributes. Default is HostLanguage.xhtml, but it can be HostLanguage.rdfa_core and HostLanguage.html5, or others...
	@type host_language: integer (logically: an enumeration)
	
	@ivar content_type: the content type of the host file. Default is None
	@type content_type: string (logically: an enumeration)
	
	@ivar add_informational_messages: whether informational messages should also be added to the processor graph, or only errors and warnings

	@ivar experimental_features: whether experimental features should be activated; that is a developer's option...
	@ivar check_lite: whether RDFa Lite should be checked, to generate warnings.
	"""
	def __init__(self, output_default_graph       = True,
					   output_processor_graph     = False,
					   space_preserve             = True,
					   transformers               = [],
					   embedded_rdf               = True,
					   vocab_expansion            = False,
					   vocab_cache                = True,
					   vocab_cache_report         = False,
					   refresh_vocab_cache        = False,
					   add_informational_messages = False,
					   check_lite                 = False,
					   experimental_features      = False
					   ) :
		self.space_preserve 		    = space_preserve
		self.transformers   		    = transformers
		self.processor_graph  		    = ProcessorGraph() 
		self.output_default_graph	    = output_default_graph
		self.output_processor_graph	    = output_processor_graph
		self.host_language 			    = HostLanguage.rdfa_core
		self.vocab_cache_report		    = vocab_cache_report
		self.refresh_vocab_cache	    = refresh_vocab_cache
		self.embedded_rdf			    = embedded_rdf
		self.vocab_expansion		    = vocab_expansion
		self.vocab_cache			    = vocab_cache
		self.add_informational_messages = add_informational_messages
		self.check_lite                 = check_lite
		if check_lite :
			self.transformers.append(lite_prune)
		self.experimental_features      = experimental_features
			
	def set_host_language(self, content_type) :
		"""
		Set the host language for processing, based on the recognized types. If this is not a recognized content type,
		it falls back to RDFa core (i.e., XML)
		@param content_type: content type
		@type content_type: string
		"""
		if content_type in content_to_host_language :
			self.host_language = content_to_host_language[content_type]
		else :
			self.host_language = HostLanguage.rdfa_core
			
		if self.host_language in require_embedded_rdf :
			self.embedded_rdf = True
		
	def __str__(self) :
		retval = """Current options:
		preserve space                         : %s
		output processor graph                 : %s
		output default graph                   : %s
		host language                          : %s
		accept embedded RDF                    : %s
		perfom semantic postprocessing         : %s
		cache vocabulary graphs                : %s
		"""
		return retval % (self.space_preserve, self.output_processor_graph, self.output_default_graph, self.host_language, self.embedded_rdf, self.rdfa_sem, self.vocab_cache)
		
	def reset_processor_graph(self):
		"""Empty the processor graph. This is necessary if the same options is reused
		for several RDFa sources, and new error messages should be generated.
		"""
		self.processor_graph.graph.remove((None,None,None))

	def add_warning(self, txt, warning_type=None, context=None, node=None, buggy_value=None) :
		"""Add a warning to the processor graph.
		@param txt: the warning text. 
		@keyword warning_type: Warning Class
		@type warning_type: URIRef
		@keyword context: possible context to be added to the processor graph
		@type context: URIRef or String
		@keyword buggy_value: a special case when a 'term' is not recognized; no warning is generated for that case if the value is part of the 'usual' XHTML terms, because almost all RDFa file contains some of those and that would pollute the output
		@type buggy_value: String
		"""
		if warning_type == ns_rdfa["UnresolvedTerm"] and buggy_value in predefined_1_0_rel :
			return
		return self.processor_graph.add_triples(txt, RDFA_Warning, warning_type, context, node)

	def add_info(self, txt, info_type=None, context=None, node=None, buggy_value=None) :
		"""Add an informational comment to the processor graph.
		@param txt: the information text. 
		@keyword info_type: Info Class
		@type info_type: URIRef
		@keyword context: possible context to be added to the processor graph
		@type context: URIRef or String
		@keyword buggy_value: a special case when a 'term' is not recognized; no information is generated for that case if the value is part of the 'usual' XHTML terms, because almost all RDFa file contains some of those and that would pollute the output
		@type buggy_value: String
		"""
		if self.add_informational_messages :
			return self.processor_graph.add_triples(txt, RDFA_Info, info_type, context, node)
		else :
			return

	def add_error(self, txt, err_type=None, context=None, node=None, buggy_value=None) :
		"""Add an error  to the processor graph.
		@param txt: the information text. 
		@keyword err_type: Error Class
		@type err_type: URIRef
		@keyword context: possible context to be added to the processor graph
		@type context: URIRef or String
		@keyword buggy_value: a special case when a 'term' is not recognized; no error is generated for that case if the value is part of the 'usual' XHTML terms, because almost all RDFa file contains some of those and that would pollute the output
		@type buggy_value: String
		"""
		return self.processor_graph.add_triples(txt, RDFA_Error, err_type, context, node)


########NEW FILE########
__FILENAME__ = parse
# -*- coding: utf-8 -*-
"""
The core parsing function of RDFa. Some details are
put into other modules to make it clearer to update/modify (e.g., generation of C{@property} values, or managing the current state).

Note that the entry point (L{parse_one_node}) bifurcates into an RDFa 1.0 and RDFa 1.1 version, ie,
to L{_parse_1_0} and L{_parse_1_1}. Some of the parsing details (management of C{@property}, list facilities, changed behavior on C{@typeof})) have changed
between versions and forcing the two into one function would be counter productive.

@summary: RDFa core parser processing step
@organization: U{World Wide Web Consortium<http://www.w3.org>}
@author: U{Ivan Herman<a href="http://www.w3.org/People/Ivan/">}
@license: This software is available for use under the
U{W3C SOFTWARE NOTICE AND LICENSE<href="http://www.w3.org/Consortium/Legal/2002/copyright-software-20021231">}
"""

"""
$Id: parse.py,v 1.19 2013-01-07 12:46:43 ivan Exp $
$Date: 2013-01-07 12:46:43 $
"""

import sys

from .state   		import ExecutionContext
from .property 		import ProcessProperty
from .embeddedRDF	import handle_embeddedRDF
from .host			import HostLanguage, host_dom_transforms

import rdflib
from rdflib	import URIRef
from rdflib	import Literal
from rdflib	import BNode
from rdflib	import Namespace
if rdflib.__version__ >= "3.0.0" :
	from rdflib	import Graph
	from rdflib	import RDF  as ns_rdf
	from rdflib	import RDFS as ns_rdfs
else :
	from rdflib.Graph	import Graph
	from rdflib.RDFS	import RDFSNS as ns_rdfs
	from rdflib.RDF		import RDFNS  as ns_rdf

from .      import IncorrectBlankNodeUsage, err_no_blank_node
from .utils import has_one_of_attributes

#######################################################################
def parse_one_node(node, graph, parent_object, incoming_state, parent_incomplete_triples) :
	"""The (recursive) step of handling a single node. 
	
	This entry just switches between the RDFa 1.0 and RDFa 1.1 versions for parsing. This method is only invoked once,
	actually, from the top level; the recursion then happens in the L{_parse_1_0} and L{_parse_1_1} methods for
	RDFa 1.0 and RDFa 1.1, respectively.

	@param node: the DOM node to handle
	@param graph: the RDF graph
	@type graph: RDFLib's Graph object instance
	@param parent_object: the parent's object, as an RDFLib URIRef
	@param incoming_state: the inherited state (namespaces, lang, etc.)
	@type incoming_state: L{state.ExecutionContext}
	@param parent_incomplete_triples: list of hanging triples (the missing resource set to None) to be handled (or not)
	by the current node.
	@return: whether the caller has to complete it's parent's incomplete triples
	@rtype: Boolean
	"""
	# Branch according to versions.
	if incoming_state.rdfa_version >= "1.1" :
		_parse_1_1(node, graph, parent_object, incoming_state, parent_incomplete_triples)
	else :
		_parse_1_0(node, graph, parent_object, incoming_state, parent_incomplete_triples)

#######################################################################
def _parse_1_1(node, graph, parent_object, incoming_state, parent_incomplete_triples) :
	"""The (recursive) step of handling a single node. See the
	U{RDFa 1.1 Core document<http://www.w3.org/TR/rdfa-core/>} for further details.
	
	This is the RDFa 1.1 version.

	@param node: the DOM node to handle
	@param graph: the RDF graph
	@type graph: RDFLib's Graph object instance
	@param parent_object: the parent's object, as an RDFLib URIRef
	@param incoming_state: the inherited state (namespaces, lang, etc.)
	@type incoming_state: L{state.ExecutionContext}
	@param parent_incomplete_triples: list of hanging triples (the missing resource set to None) to be handled (or not)
	by the current node.
	@return: whether the caller has to complete it's parent's incomplete triples
	@rtype: Boolean
	"""
	def header_check(p_obj) :
		"""Special disposition for the HTML <head> and <body> elements..."""
		if state.options.host_language in [ HostLanguage.xhtml, HostLanguage.html5, HostLanguage.xhtml5 ] :
			if node.nodeName == "head" or node.nodeName == "body" :
				if not has_one_of_attributes(node, "about", "resource", "src", "href") :
					return p_obj
		else :
			return None

	def lite_check() :
		if state.options.check_lite and state.options.host_language in [ HostLanguage.html5, HostLanguage.xhtml5, HostLanguage.xhtml ] :
			if node.tagName == "link" and node.hasAttribute("rel") and state.term_or_curie.CURIE_to_URI(node.getAttribute("rel")) != None :
				state.options.add_warning("In RDFa Lite, attribute @rel in <link> is only used in non-RDFa way (consider using @property)", node=node)

	# Update the state. This means, for example, the possible local settings of
	# namespaces and lang
	state = None
	state = ExecutionContext(node, graph, inherited_state=incoming_state)

	#---------------------------------------------------------------------------------
	# Extra warning check on RDFa Lite
	lite_check()
	
	#---------------------------------------------------------------------------------
	# Handling the role attribute is pretty much orthogonal to everything else...
	handle_role_attribute(node, graph, state)

	#---------------------------------------------------------------------------------
	# Handle the special case for embedded RDF, eg, in SVG1.2. 
	# This may add some triples to the target graph that does not originate from RDFa parsing
	# If the function return TRUE, that means that an rdf:RDF has been found. No
	# RDFa parsing should be done on that subtree, so we simply return...
	if state.options.embedded_rdf and node.nodeType == node.ELEMENT_NODE and handle_embeddedRDF(node, graph, state) : 
		return	

	#---------------------------------------------------------------------------------
	# calling the host language specific massaging of the DOM
	if state.options.host_language in host_dom_transforms and node.nodeType == node.ELEMENT_NODE :
		for func in host_dom_transforms[state.options.host_language] : func(node, state)

	#---------------------------------------------------------------------------------
	# First, let us check whether there is anything to do at all. Ie,
	# whether there is any relevant RDFa specific attribute on the element
	#
	if not has_one_of_attributes(node, "href", "resource", "about", "property", "rel", "rev", "typeof", "src", "vocab", "prefix") :
		# nop, there is nothing to do here, just go down the tree and return...
		for n in node.childNodes :
			if n.nodeType == node.ELEMENT_NODE : parse_one_node(n, graph, parent_object, state, parent_incomplete_triples)
		return

	#-----------------------------------------------------------------
	# The goal is to establish the subject and object for local processing
	# The behaviour is slightly different depending on the presense or not
	# of the @rel/@rev attributes
	current_subject = None
	current_object  = None
	typed_resource	= None
	
	if has_one_of_attributes(node, "rel", "rev")  :
		# in this case there is the notion of 'left' and 'right' of @rel/@rev
		# in establishing the new Subject and the objectResource
		current_subject = header_check(parent_object)

		# set first the subject
		if node.hasAttribute("about") :
			current_subject = state.getURI("about")
			if node.hasAttribute("typeof") : typed_resource = current_subject
			
		# get_URI may return None in case of an illegal CURIE, so
		# we have to be careful here, not use only an 'else'
		if current_subject == None :
			current_subject = parent_object
		else :
			state.reset_list_mapping(origin = current_subject)
		
		# set the object resource
		current_object = state.getResource("resource", "href", "src")
			
		if node.hasAttribute("typeof") and not node.hasAttribute("about") :
			if current_object == None :
				current_object = BNode()
			typed_resource = current_object
		
		if not node.hasAttribute("inlist") and current_object != None :
			# In this case the newly defined object is, in fact, the head of the list
			# just reset the whole thing.
			state.reset_list_mapping(origin = current_object)

	elif  node.hasAttribute("property") and not has_one_of_attributes(node, "content", "datatype") :
		current_subject = header_check(parent_object)

		# this is the case when the property may take hold of @src and friends...
		if node.hasAttribute("about") :
			current_subject = state.getURI("about")
			if node.hasAttribute("typeof") : typed_resource = current_subject

		# getURI may return None in case of an illegal CURIE, so
		# we have to be careful here, not use only an 'else'
		if current_subject == None :
			current_subject = parent_object
		else :
			state.reset_list_mapping(origin = current_subject)

		if typed_resource == None and node.hasAttribute("typeof") :
			typed_resource = state.getResource("resource", "href", "src")
			if typed_resource == None :
				typed_resource = BNode()
			current_object = typed_resource
		else :
			current_object = current_subject
			
	else :
		current_subject = header_check(parent_object)

		# in this case all the various 'resource' setting attributes
		# behave identically, though they also have their own priority
		if current_subject == None :
			current_subject = state.getResource("about", "resource", "href", "src")
			
		# get_URI_ref may return None in case of an illegal CURIE, so
		# we have to be careful here, not use only an 'else'
		if current_subject == None :
			if node.hasAttribute("typeof") :
				current_subject = BNode()
				state.reset_list_mapping(origin = current_subject)
			else :
				current_subject = parent_object
		else :
			state.reset_list_mapping(origin = current_subject)

		# in this case no non-literal triples will be generated, so the
		# only role of the current_object Resource is to be transferred to
		# the children node
		current_object = current_subject
		if node.hasAttribute("typeof") : typed_resource = current_subject
		
	# ---------------------------------------------------------------------
	## The possible typeof indicates a number of type statements on the typed resource
	for defined_type in state.getURI("typeof") :
		if typed_resource :
			graph.add((typed_resource, ns_rdf["type"], defined_type))

	# ---------------------------------------------------------------------
	# In case of @rel/@rev, either triples or incomplete triples are generated
	# the (possible) incomplete triples are collected, to be forwarded to the children
	incomplete_triples  = []
	for prop in state.getURI("rel") :
		if not isinstance(prop,BNode) :
			if node.hasAttribute("inlist") :
				if current_object != None :
					# Add the content to the list. Note that if the same list
					# was initialized, at some point, by a None, it will be
					# overwritten by this real content
					state.add_to_list_mapping(prop, current_object)
				else :
					# Add a dummy entry to the list... Note that
					# if that list was initialized already with a real content
					# this call will have no effect
					state.add_to_list_mapping(prop, None)
					
					# Add a placeholder into the hanging rels
					incomplete_triples.append( (None, prop, None) )
			else :
				theTriple = (current_subject, prop, current_object)
				if current_object != None :
					graph.add(theTriple)
				else :
					incomplete_triples.append(theTriple)
		else :
			state.options.add_warning(err_no_blank_node % "rel", warning_type=IncorrectBlankNodeUsage, node=node.nodeName)

	for prop in state.getURI("rev") :
		if not isinstance(prop,BNode) :
			theTriple = (current_object,prop,current_subject)
			if current_object != None :
				graph.add(theTriple)
			else :
				incomplete_triples.append(theTriple)
		else :
			state.options.add_warning(err_no_blank_node % "rev", warning_type=IncorrectBlankNodeUsage, node=node.nodeName)

	# ----------------------------------------------------------------------
	# Generation of the @property values, including literals. The newSubject is the subject
	# A particularity of property is that it stops the parsing down the DOM tree if an XML Literal is generated,
	# because everything down there is part of the generated literal. 
	if node.hasAttribute("property") :
		ProcessProperty(node, graph, current_subject, state, typed_resource).generate_1_1()

	# ----------------------------------------------------------------------
	# Setting the current object to a bnode is setting up a possible resource
	# for the incomplete triples downwards
	if current_object == None :
		object_to_children = BNode()
	else :
		object_to_children = current_object

	#-----------------------------------------------------------------------
	# Here is the recursion step for all the children
	for n in node.childNodes :
		if n.nodeType == node.ELEMENT_NODE : 
			_parse_1_1(n, graph, object_to_children, state, incomplete_triples)

	# ---------------------------------------------------------------------
	# At this point, the parent's incomplete triples may be completed
	for (s,p,o) in parent_incomplete_triples :
		if s == None and o == None :
			# This is an encoded version of a hanging rel for a collection:
			incoming_state.add_to_list_mapping( p, current_subject )
		else :
			if s == None : s = current_subject
			if o == None : o = current_subject
			graph.add((s,p,o))

	# Generate the lists, if any and if this is the level where a new list was originally created	
	if state.new_list and not state.list_empty() :
		for prop in state.get_list_props() :
			vals  = state.get_list_value(prop)
			if vals == None :
				# This was an empty list, in fact, ie, the list has been initiated by a <xxx rel="prop" inlist>
				# but no list content has ever been added
				graph.add( (state.get_list_origin(), prop, ns_rdf["nil"]) )
			else :
				heads = [ BNode() for r in vals ] + [ ns_rdf["nil"] ]
				for i in range(0, len(vals)) :
					graph.add( (heads[i], ns_rdf["first"], vals[i]) )
					graph.add( (heads[i], ns_rdf["rest"],  heads[i+1]) )
				# Anchor the list
				graph.add( (state.get_list_origin(), prop, heads[0]) )

	# -------------------------------------------------------------------
	# This should be it...
	# -------------------------------------------------------------------
	return


##################################################################################################################
def _parse_1_0(node, graph, parent_object, incoming_state, parent_incomplete_triples) :
	"""The (recursive) step of handling a single node. See the
	U{RDFa 1.0 syntax document<http://www.w3.org/TR/rdfa-syntax>} for further details.
	
	This is the RDFa 1.0 version.

	@param node: the DOM node to handle
	@param graph: the RDF graph
	@type graph: RDFLib's Graph object instance
	@param parent_object: the parent's object, as an RDFLib URIRef
	@param incoming_state: the inherited state (namespaces, lang, etc.)
	@type incoming_state: L{state.ExecutionContext}
	@param parent_incomplete_triples: list of hanging triples (the missing resource set to None) to be handled (or not)
	by the current node.
	@return: whether the caller has to complete it's parent's incomplete triples
	@rtype: Boolean
	"""

	# Update the state. This means, for example, the possible local settings of
	# namespaces and lang
	state = None
	state = ExecutionContext(node, graph, inherited_state=incoming_state)

	#---------------------------------------------------------------------------------
	# Handling the role attribute is pretty much orthogonal to everything else...
	handle_role_attribute(node, graph, state)

	#---------------------------------------------------------------------------------
	# Handle the special case for embedded RDF, eg, in SVG1.2. 
	# This may add some triples to the target graph that does not originate from RDFa parsing
	# If the function return TRUE, that means that an rdf:RDF has been found. No
	# RDFa parsing should be done on that subtree, so we simply return...
	if state.options.embedded_rdf and node.nodeType == node.ELEMENT_NODE and handle_embeddedRDF(node, graph, state) : 
		return	

	#---------------------------------------------------------------------------------
	# calling the host language specific massaging of the DOM
	if state.options.host_language in host_dom_transforms and node.nodeType == node.ELEMENT_NODE :
		for func in host_dom_transforms[state.options.host_language] : func(node, state)

	#---------------------------------------------------------------------------------
	# First, let us check whether there is anything to do at all. Ie,
	# whether there is any relevant RDFa specific attribute on the element
	#
	if not has_one_of_attributes(node, "href", "resource", "about", "property", "rel", "rev", "typeof", "src") :
		# nop, there is nothing to do here, just go down the tree and return...
		for n in node.childNodes :
			if n.nodeType == node.ELEMENT_NODE : parse_one_node(n, graph, parent_object, state, parent_incomplete_triples)
		return

	#-----------------------------------------------------------------
	# The goal is to establish the subject and object for local processing
	# The behaviour is slightly different depending on the presense or not
	# of the @rel/@rev attributes
	current_subject = None
	current_object  = None
	prop_object		= None

	if has_one_of_attributes(node, "rel", "rev")  :
		# in this case there is the notion of 'left' and 'right' of @rel/@rev
		# in establishing the new Subject and the objectResource
		current_subject = state.getResource("about","src")
			
		# get_URI may return None in case of an illegal CURIE, so
		# we have to be careful here, not use only an 'else'
		if current_subject == None :
			if node.hasAttribute("typeof") :
				current_subject = BNode()
			else :
				current_subject = parent_object
		else :
			state.reset_list_mapping(origin = current_subject)
		
		# set the object resource
		current_object = state.getResource("resource", "href")
		
	else :
		# in this case all the various 'resource' setting attributes
		# behave identically, though they also have their own priority
		current_subject = state.getResource("about", "src", "resource", "href")
		
		# get_URI_ref may return None in case of an illegal CURIE, so
		# we have to be careful here, not use only an 'else'
		if current_subject == None :
			if node.hasAttribute("typeof") :
				current_subject = BNode()
			else :
				current_subject = parent_object
			current_subject = parent_object
		else :
			state.reset_list_mapping(origin = current_subject)

		# in this case no non-literal triples will be generated, so the
		# only role of the current_object Resource is to be transferred to
		# the children node
		current_object = current_subject
		
	# ---------------------------------------------------------------------
	## The possible typeof indicates a number of type statements on the new Subject
	for defined_type in state.getURI("typeof") :
		graph.add((current_subject, ns_rdf["type"], defined_type))

	# ---------------------------------------------------------------------
	# In case of @rel/@rev, either triples or incomplete triples are generated
	# the (possible) incomplete triples are collected, to be forwarded to the children
	incomplete_triples  = []
	for prop in state.getURI("rel") :
		if not isinstance(prop,BNode) :
			theTriple = (current_subject, prop, current_object)
			if current_object != None :
				graph.add(theTriple)
			else :
				incomplete_triples.append(theTriple)
		else :
			state.options.add_warning(err_no_blank_node % "rel", warning_type=IncorrectBlankNodeUsage, node=node.nodeName)

	for prop in state.getURI("rev") :
		if not isinstance(prop,BNode) :
			theTriple = (current_object,prop,current_subject)
			if current_object != None :
				graph.add(theTriple)
			else :
				incomplete_triples.append(theTriple)
		else :
			state.options.add_warning(err_no_blank_node % "rev", warning_type=IncorrectBlankNodeUsage, node=node.nodeName)

	# ----------------------------------------------------------------------
	# Generation of the literal values. The newSubject is the subject
	# A particularity of property is that it stops the parsing down the DOM tree if an XML Literal is generated,
	# because everything down there is part of the generated literal. 
	if node.hasAttribute("property") :
		ProcessProperty(node, graph, current_subject, state).generate_1_0()

	# ----------------------------------------------------------------------
	# Setting the current object to a bnode is setting up a possible resource
	# for the incomplete triples downwards
	if current_object == None :
		object_to_children = BNode()
	else :
		object_to_children = current_object

	#-----------------------------------------------------------------------
	# Here is the recursion step for all the children
	for n in node.childNodes :
		if n.nodeType == node.ELEMENT_NODE : 
			_parse_1_0(n, graph, object_to_children, state, incomplete_triples)

	# ---------------------------------------------------------------------
	# At this point, the parent's incomplete triples may be completed
	for (s,p,o) in parent_incomplete_triples :
		if s == None and o == None :
			# This is an encoded version of a hanging rel for a collection:
			incoming_state.add_to_list_mapping( p, current_subject )
		else :
			if s == None : s = current_subject
			if o == None : o = current_subject
			graph.add((s,p,o))

	# -------------------------------------------------------------------
	# This should be it...
	# -------------------------------------------------------------------
	return


#######################################################################
# Handle the role attribute
def handle_role_attribute(node, graph, state) :
	"""
	Handling the role attribute, according to http://www.w3.org/TR/role-attribute/#using-role-in-conjunction-with-rdfa
	@param node: the DOM node to handle
	@param graph: the RDF graph
	@type graph: RDFLib's Graph object instance
	@param state: the inherited state (namespaces, lang, etc.)
	@type state: L{state.ExecutionContext}
	"""
	if node.hasAttribute("role") :
		if node.hasAttribute("id") :
			id = node.getAttribute("id").strip()
			subject = URIRef(state.base + '#' + id)
		else :
			subject = BNode()
		predicate = URIRef('http://www.w3.org/1999/xhtml/vocab#role')
		for object in state.getURI("role") :
			graph.add((subject, predicate, object))
	
	
	
	
	
	
	
	
	
	


########NEW FILE########
__FILENAME__ = property
# -*- coding: utf-8 -*-
"""
Implementation of the C{@property} value handling.

RDFa 1.0 and RDFa 1.1 are fairly different. RDFa 1.0 generates only literals, see
U{RDFa Task Force's wiki page<http://www.w3.org/2006/07/SWD/wiki/RDFa/LiteralObject>} for the details.
On the other hand, RDFa 1.1, beyond literals, can also generate URI references. Hence the duplicate method in the L{ProcessProperty} class, one for RDFa 1.0 and the other for RDFa 1.1.

@summary: RDFa Literal generation
@requires: U{RDFLib package<http://rdflib.net>}
@organization: U{World Wide Web Consortium<http://www.w3.org>}
@author: U{Ivan Herman<a href="http://www.w3.org/People/Ivan/">}
@license: This software is available for use under the
U{W3C SOFTWARE NOTICE AND LICENSE<href="http://www.w3.org/Consortium/Legal/2002/copyright-software-20021231">}
"""

"""
$Id: property.py,v 1.11 2012/06/12 11:47:11 ivan Exp $
$Date: 2012/06/12 11:47:11 $
"""

import re, sys

import rdflib
from rdflib	import BNode
from rdflib	import Literal, URIRef, Namespace
if rdflib.__version__ >= "3.0.0" :
	from rdflib	     import RDF as ns_rdf
	from rdflib.term import XSDToPython
else :
	from rdflib.RDF	    import RDFNS as ns_rdf
	from rdflib.Literal import XSDToPython

from .	         import IncorrectBlankNodeUsage, IncorrectLiteral, err_no_blank_node, ns_xsd 
from .utils      import has_one_of_attributes, return_XML
from .host.html5 import handled_time_types

XMLLiteral  = ns_rdf["XMLLiteral"]
HTMLLiteral = URIRef("http://www.w3.org/1999/02/22-rdf-syntax-ns#HTML") 

class ProcessProperty :
	"""Generate the value for C{@property} taking into account datatype, etc.
	Note: this class is created only if the C{@property} is indeed present, no need to check. 
	
	@ivar node: DOM element node
	@ivar graph: the (RDF) graph to add the properies to
	@ivar subject: the RDFLib URIRef serving as a subject for the generated triples
	@ivar state: the current state to be used for the CURIE-s
	@type state: L{state.ExecutionContext}
	@ivar typed_resource: Typically the bnode generated by a @typeof
	"""
	def __init__(self, node, graph, subject, state, typed_resource = None) :
		"""
		@param node: DOM element node
		@param graph: the (RDF) graph to add the properies to
		@param subject: the RDFLib URIRef serving as a subject for the generated triples
		@param state: the current state to be used for the CURIE-s
		@param state: L{state.ExecutionContext}
		@param typed_resource: Typically the bnode generated by a @typeof; in RDFa 1.1, that becomes the object for C{@property}
		"""
		self.node           = node
		self.graph          = graph
		self.subject        = subject
		self.state          = state
		self.typed_resource = typed_resource
		
	def generate(self) :
		"""
		Common entry point for the RDFa 1.0 and RDFa 1.1 versions; bifurcates based on the RDFa version, as retrieved from the state object.
		"""
		if self.state.rdfa_version >= "1.1" :
			self.generate_1_1()
		else :
			self.generate_1_0()
	
	def generate_1_1(self) :
		"""Generate the property object, 1.1 version"""
				
		#########################################################################		
		# See if the target is _not_ a literal
		irirefs      = ("resource", "href", "src")
		noiri        = ("content", "datatype", "rel", "rev")
		notypediri   = ("content", "datatype", "rel", "rev", "about", "about_pruned")
		if has_one_of_attributes(self.node, irirefs) and not has_one_of_attributes(self.node, noiri) :
			# @href/@resource/@src takes the lead here...
			object = self.state.getResource(irirefs)
		elif self.node.hasAttribute("typeof") and not has_one_of_attributes(self.node, notypediri) and self.typed_resource != None :
				# a @typeof creates a special branch in case the typed resource was set during parsing
				object = self.typed_resource
		else :
			# We have to generate a literal
			
			# Get, if exists, the value of @datatype
			datatype = ''
			dtset    = False
			if self.node.hasAttribute("datatype") :
				dtset = True
				dt = self.node.getAttribute("datatype")
				if dt != "" :
					datatype = self.state.getURI("datatype")
		
			# Supress lange is set in case some elements explicitly want to supress the effect of language
			# There were discussions, for example, that the <time> element should do so. Although,
			# after all, this was reversed, the functionality is kept in the code in case another
			# element might need it...
			if self.state.lang != None and self.state.supress_lang == False :
				lang = self.state.lang
			else :
				lang = ''
	
			# The simple case: separate @content attribute
			if self.node.hasAttribute("content") :
				val = self.node.getAttribute("content")
				# Handling the automatic uri conversion case
				if dtset == False :
					object = Literal(val, lang=lang)
				else :
					object = self._create_Literal(val, datatype=datatype, lang=lang)
				# The value of datatype has been set, and the keyword paramaters take care of the rest
			else :
				# see if there *is* a datatype (even if it is empty!)
				if dtset :
					if datatype == XMLLiteral :
						litval = self._get_XML_literal(self.node)
						object = Literal(litval,datatype=XMLLiteral)
					elif datatype == HTMLLiteral :
						# I am not sure why this hack is necessary, but otherwise an encoding error occurs
						# In Python3 all this should become moot, due to the unicode everywhere approach...
						if sys.version_info[0] >= 3 :
							object = Literal(self._get_HTML_literal(self.node), datatype=HTMLLiteral)
						else :
							litval = self._get_HTML_literal(self.node)
							o = Literal(litval, datatype=XMLLiteral)	
							object = Literal(o, datatype=HTMLLiteral)					
					else :
						object = self._create_Literal(self._get_literal(self.node), datatype=datatype, lang=lang)
				else :
					object = self._create_Literal(self._get_literal(self.node), lang=lang)
	
		if object != None :
			for prop in self.state.getURI("property") :
				if not isinstance(prop, BNode) :
					if self.node.hasAttribute("inlist") :
						self.state.add_to_list_mapping(prop, object)
					else :			
						self.graph.add( (self.subject, prop, object) )
				else :
					self.state.options.add_warning(err_no_blank_node % "property", warning_type=IncorrectBlankNodeUsage, node=self.node.nodeName)
	
		# return

	def generate_1_0(self) :
		"""Generate the property object, 1.0 version"""
				
		#########################################################################		
		# We have to generate a literal indeed.
		# Get, if exists, the value of @datatype
		datatype = ''
		dtset    = False
		if self.node.hasAttribute("datatype") :
			dtset = True
			dt = self.node.getAttribute("datatype")
			if dt != "" :
				datatype = self.state.getURI("datatype")
	
		if self.state.lang != None :
			lang = self.state.lang
		else :
			lang = ''

		# The simple case: separate @content attribute
		if self.node.hasAttribute("content") :
			val = self.node.getAttribute("content")
			# Handling the automatic uri conversion case
			if dtset == False :
				object = Literal(val, lang=lang)
			else :
				object = self._create_Literal(val, datatype=datatype, lang=lang)
			# The value of datatype has been set, and the keyword paramaters take care of the rest
		else :
			# see if there *is* a datatype (even if it is empty!)
			if dtset :
				# yep. The Literal content is the pure text part of the current element:
				# We have to check whether the specified datatype is, in fact, an
				# explicit XML Literal
				if datatype == XMLLiteral :
					litval = self._get_XML_literal(self.node)
					object = Literal(litval,datatype=XMLLiteral)
				elif datatype == HTMLLiteral :
					# I am not sure why this hack is necessary, but otherwise an encoding error occurs
					# In Python3 all this should become moot, due to the unicode everywhere approach...
					if sys.version_info[0] >= 3 :
						object = Literal(self._get_HTML_literal(self.node), datatype=HTMLLiteral)
					else :
						litval = self._get_HTML_literal(self.node)
						o = Literal(litval, datatype=XMLLiteral)	
						object = Literal(o, datatype=HTMLLiteral)					
				else :
					object = self._create_Literal(self._get_literal(self.node), datatype=datatype, lang=lang)
			else :
				# no controlling @datatype. We have to see if there is markup in the contained
				# element
				if True in [ n.nodeType == self.node.ELEMENT_NODE for n in self.node.childNodes ] :
					# yep, and XML Literal should be generated
					object = self._create_Literal(self._get_XML_literal(self.node), datatype=XMLLiteral)
				else :
					# At this point, there might be entities in the string that are returned as real characters by the dom
					# implementation. That should be turned back
					object = self._create_Literal(self._get_literal(self.node), lang=lang)
	
		for prop in self.state.getURI("property") :
			if not isinstance(prop,BNode) :
				self.graph.add( (self.subject,prop,object) )
			else :
				self.state.options.add_warning(err_no_blank_node % "property", warning_type=IncorrectBlankNodeUsage, node=self.node.nodeName)
	
		# return
	
	######################################################################################################################################
	
	
	def _putBackEntities(self, str) :
		"""Put 'back' entities for the '&','<', and '>' characters, to produce a proper XML string.
		Used by the XML Literal extraction.
		@param str: string to be converted
		@return: string with entities
		@rtype: string
		"""
		return str.replace('&','&amp;').replace('<','&lt;').replace('>','&gt;')
		
	def _get_literal(self, Pnode):
		"""
		Get (recursively) the full text from a DOM Node.
	
		@param Pnode: DOM Node
		@return: string
		"""
		rc = ""
		for node in Pnode.childNodes:
			if node.nodeType == node.TEXT_NODE:
				rc = rc + node.data
			elif node.nodeType == node.ELEMENT_NODE :
				rc = rc + self._get_literal(node)
	
		# The decision of the group in February 2008 is not to normalize the result by default.
		# This is reflected in the default value of the option		
		
		if self.state.options.space_preserve :
			return rc
		else :
			return re.sub(r'(\r| |\n|\t)+'," ",rc).strip()
	# end getLiteral
	
	def _get_XML_literal(self, Pnode) :
		"""
		Get (recursively) the XML Literal content of a DOM Node. 
	
		@param Pnode: DOM Node
		@return: string
		"""	
		rc = ""		
		for node in Pnode.childNodes:
			if node.nodeType == node.TEXT_NODE:
				rc = rc + self._putBackEntities(node.data)
			elif node.nodeType == node.ELEMENT_NODE :
				rc = rc + return_XML(self.state, node, base = False)
		return rc
	# end getXMLLiteral

	def _get_HTML_literal(self, Pnode) :
		"""
		Get (recursively) the XML Literal content of a DOM Node. 
	
		@param Pnode: DOM Node
		@return: string
		"""	
		rc = ""		
		for node in Pnode.childNodes:
			if node.nodeType == node.TEXT_NODE:
				rc = rc + self._putBackEntities(node.data)
			elif node.nodeType == node.ELEMENT_NODE :
				rc = rc + return_XML(self.state, node, base = False, xmlns = False )
		return rc
	# end getXMLLiteral
	
	def _create_Literal(self, val, datatype = '', lang = '') :
		"""
		Create a literal, taking into account the datatype and language.
		@return: Literal
		"""
		if datatype == None or datatype == '' :
			return Literal(val, lang=lang)
		#elif datatype == ns_xsd["string"] :
		#	return Literal(val)
		else :
			# This is a bit convoluted... the default setup of rdflib does not gracefully react if the
			# datatype cannot properly be converted to Python. I have to copy and reuse some of the
			# rdflib code to get this working...
			# To make things worse: rdlib 3.1.0 does not handle the various xsd date types properly, ie,
			# the conversion function below will generate errors. Ie, the check should be skipped for those
			if ("%s" % datatype) in handled_time_types and rdflib.__version__ < "3.2.0" :
				convFunc = False
			else :
				convFunc = XSDToPython.get(datatype, None)
			if convFunc :
				try :
					pv = convFunc(val)
					# If we got there the literal value and its datatype match
				except :
					self.state.options.add_warning("Incompatible value (%s) and datatype (%s) in Literal definition." % (val, datatype), warning_type=IncorrectLiteral, node=self.node.nodeName)
			return Literal(val, datatype=datatype)

########NEW FILE########
__FILENAME__ = cache
# -*- coding: utf-8 -*-
"""
Managing Vocab Caching.

@summary: RDFa parser (distiller)
@requires: U{RDFLib<http://rdflib.net>}
@organization: U{World Wide Web Consortium<http://www.w3.org>}
@author: U{Ivan Herman<a href="http://www.w3.org/People/Ivan/">}
@license: This software is available for use under the
U{W3C SOFTWARE NOTICE AND LICENSE<href="http://www.w3.org/Consortium/Legal/2002/copyright-software-20021231">}
"""
import os, sys, datetime, re

PY3 = (sys.version_info[0] >= 3)

import rdflib
from rdflib	import URIRef
from rdflib	import Literal
from rdflib	import BNode
from rdflib	import Namespace
if rdflib.__version__ >= "3.0.0" :
	from rdflib	import RDF  as ns_rdf
	from rdflib	import RDFS as ns_rdfs
	from rdflib	import Graph
else :
	from rdflib.RDFS	import RDFSNS as ns_rdfs
	from rdflib.RDF		import RDFNS  as ns_rdf
	from rdflib.Graph 	import Graph

from ..			import HTTPError, RDFaError
from ..host 	import MediaTypes, HostLanguage
from ..utils	import create_file_name, URIOpener, quote_URI
from ..options	import Options
from ..			import ns_rdfa

from . import err_outdated_cache
from . import err_unreachable_vocab
from . import err_unparsable_Turtle_vocab
from . import err_unparsable_xml_vocab
from . import err_unparsable_ntriples_vocab
from . import err_unparsable_rdfa_vocab
from . import err_unrecognised_vocab_type

from . import VocabCachingInfo

# Regular expression object for a general XML application media type
xml_application_media_type = re.compile("application/[a-zA-Z0-9]+\+xml")

from ..utils import URIOpener

#===========================================================================================
if PY3 :
	import pickle
else :
	import cPickle as pickle

# Protocol to be used for pickle files. 0 is good for debug, it stores the data in ASCII; 1 is better for deployment,
# it stores data in binary format. Care should be taken for consistency; when changing from 0 to 1 or back, all
# cached data should be removed/regenerated, otherwise mess may occur...
_Pickle_Protocol = 1

# If I could rely on python 2.5 or 2.6 (or higher) I could use the with...as... idiom for what is below, it
# is indeed nicer. But I cannot...
def _load(fname) :
	"""
	Load a cached file and return the resulting object
	@param fname: file name
	"""
	try :
		f = open(fname)
		return pickle.load(f)
	finally :
		f.close()
	
def _dump(obj, fname) :
	"""
	Dump an object into cached file
	@param obj: Python object to store
	@param fname: file name
	"""
	try :
		f = open(fname, "w")
		pickle.dump(obj, f, _Pickle_Protocol)
		f.flush()
	finally :
		f.close()

#===========================================================================================
class CachedVocabIndex :
	"""
	Class to manage the cache index. Takes care of finding the vocab directory, and manages the index
	to the individual vocab data.
	
	The vocab directory is set to a platform specific area, unless an environment variable
	sets it explicitly. The environment variable is "PyRdfaCacheDir"
	
	Every time the index is changed, the index is put back (via pickle) to the directory.
	
	@ivar app_data_dir: directory for the vocabulary cache directory
	@ivar index_fname: the full path of the index file on the disc
	@ivar indeces: the in-memory version of the index (a directory mapping URI-s to tuples)
	@ivar options: the error handler (option) object to send warnings to
	@type options: L{options.Options}
	@ivar report: whether details on the caching should be reported
	@type report: Boolean
	@cvar vocabs: File name used for the index in the cache directory
	@cvar preference_path: Cache directories for the three major platforms (ie, mac, windows, unix)
	@type preference_path: directory, keyed by "mac", "win", and "unix"
	@cvar architectures: Various 'architectures' as returned by the python call, and their mapping on one of the major platforms. If an architecture is missing, it is considered to be "unix"
	@type architectures: directory, mapping architectures to "mac", "win", or "unix"
	"""
	# File Name used for the index in the cache directory
	vocabs = "cache_index"
	# Cache directories for the three major platforms...
	preference_path = {
		"mac"	: "Library/Application Support/pyRdfa-cache",
		"win"	: "pyRdfa-cache",
		"unix"	: ".pyRdfa-cache"
	}
	# various architectures as returned by the python call, and their mapping on platorm. If an architecture is not here, it is considered as unix
	architectures = {
		"darwin"	: "mac",
		"nt" 		: "win",
		"win32"		: "win",
		"cygwin"	: "win"
	}
	def __init__(self, options = None) :
		"""
		@param options: the error handler (option) object to send warnings to
		@type options: L{options.Options}
		"""
		self.options = options
		self.report  = (options != None) and options.vocab_cache_report
		
		# This is where the cache files should be
		self.app_data_dir	= self._give_preference_path()
		self.index_fname	= os.path.join(self.app_data_dir, self.vocabs)
		self.indeces 		= {}
		
		# Check whether that directory exists.
		if not os.path.isdir(self.app_data_dir) :
			try :
				os.mkdir(self.app_data_dir)
			except Exception :
				(type,value,traceback) = sys.exc_info()
				if self.report: options.add_info("Could not create the vocab cache area %s" % value, VocabCachingInfo)
				return
		else :
			# check whether it is at least readable
			if not os.access(self.app_data_dir, os.R_OK) :
				if self.report: options.add_info("Vocab cache directory is not readable", VocabCachingInfo)
				return
			if not os.access(self.app_data_dir, os.W_OK) :
				if self.report: options.add_info("Vocab cache directory is not writeable, but readable", VocabCachingInfo)
				return

		if os.path.exists(self.index_fname) :
			if os.access(self.index_fname, os.R_OK) :
				self.indeces = _load(self.index_fname)
			else :
				if self.report: options.add_info("Vocab cache index not readable", VocabCachingInfo)				
		else :
			# This is the very initial phase, creation
			# of a a new index
			if os.access(self.app_data_dir, os.W_OK) :
				# This is then put into a pickle file to put the stake in the ground...
				try :
					_dump(self.indeces, self.index_fname)
				except Exception :
					(type,value,traceback) = sys.exc_info()
					if self.report: options.add_info("Could not create the vocabulary index %s" % value, VocabCachingInfo)
			else :
				if self.report: options.add_info("Vocabulary cache directory is not writeable", VocabCachingInfo)				
				self.cache_writeable	= False	
				
	def add_ref(self, uri, vocab_reference) :
		"""
		Add a new entry to the index, possibly removing the previous one.
		
		@param uri: the URI that serves as a key in the index directory
		@param vocab_reference: tuple consisting of file name, modification date, and expiration date
		"""
		# Store the index right away
		self.indeces[uri] = vocab_reference		
		try :
			_dump(self.indeces, self.index_fname)
		except Exception :
			(type,value,traceback) = sys.exc_info()
			if self.report: self.options.add_info("Could not store the cache index %s" % value, VocabCachingInfo)
			
	def get_ref(self, uri) :
		"""
		Get an index entry, if available, None otherwise.
		The return value is a tuple: file name, modification date, and expiration date
		
		@param uri: the URI that serves as a key in the index directory		
		"""
		if uri in self.indeces :
			return tuple(self.indeces[uri])
		else :
			return None

	def _give_preference_path(self) :
		"""
		Find the vocab cache directory.
		"""
		from pyRdfa	import CACHE_DIR_VAR
		if CACHE_DIR_VAR in os.environ :
			return os.environ[CACHE_DIR_VAR]
		else :
			# find the preference path on the architecture
			platform = sys.platform
			if platform in self.architectures :
				system = self.architectures[platform]
			else :
				system = "unix"
	
			if system == "win" :
				# there is a user variable set for that purpose
				app_data = os.path.expandvars("%APPDATA%")
				return os.path.join(app_data,self.preference_path[system])
			else :
				return os.path.join(os.path.expanduser('~'),self.preference_path[system])

#===========================================================================================
class CachedVocab(CachedVocabIndex) :
	"""
	Cache for a specific vocab. The content of the cache is the graph. These are also the data that are stored
	on the disc (in pickled form)
	
	@ivar graph: the RDF graph
	@ivar URI: vocabulary URI
	@ivar filename: file name (not the complete path) of the cached version
	@ivar creation_date: creation date of the cache
	@type creation_date: datetime
	@ivar expiration_date: expiration date of the cache
	@type expiration_date: datetime
	@cvar runtime_cache : a run time cache for already 'seen' vocabulary files. Apart from (marginally) speeding up processing, this also prevents recursion
	@type runtime_cache : dictionary
	"""
	def __init__(self, URI, options = None) :
		"""
		@param URI: real URI for the vocabulary file
		@param options: the error handler (option) object to send warnings to
		@type options: L{options.Options}
		"""
		# First see if this particular vocab has been handled before. If yes, it is extracted and everything
		# else can be forgotten. 
		self.uri													= URI
		(self.filename, self.creation_date, self.expiration_date)	= ("",None,None)
		self.graph													= Graph()

		try :
			CachedVocabIndex.__init__(self, options)
			vocab_reference 	= self.get_ref(URI)
			self.caching 		= True
		except Exception :
			# what this means is that the caching becomes impossible through some system error...
			(type,value,traceback) = sys.exc_info()
			if self.report: options.add_info("Could not access the vocabulary cache area %s" % value, VocabCachingInfo, URI)
			vocab_reference		= None
			self.caching		= False

		if vocab_reference == None :
			# This has never been cached before
			if self.report: options.add_info("No cache exists for %s, generating one" % URI, VocabCachingInfo)
			
			# Store all the cache data unless caching proves to be impossible
			if self._get_vocab_data(newCache = True) and self.caching :
				self.filename = create_file_name(self.uri)
				self._store_caches()
				if self.report:
					options.add_info("Generated a cache for %s, with an expiration date of %s" % (URI,self.expiration_date), VocabCachingInfo, URI)
		else :
			(self.filename, self.creation_date, self.expiration_date) = vocab_reference
			if self.report: options.add_info("Found a cache for %s, expiring on %s" % (URI,self.expiration_date), VocabCachingInfo)
			# Check if the expiration date is still away
			if options.refresh_vocab_cache == False and datetime.datetime.utcnow() <= self.expiration_date :
				# We are fine, we can just extract the data from the cache and we're done
				if self.report: options.add_info("Cache for %s is still valid; extracting the data" % URI, VocabCachingInfo)
				fname = os.path.join(self.app_data_dir, self.filename)
				try :
					self.graph = _load(fname)
				except Exception :
					# what this means is that the caching becomes impossible VocabCachingInfo
					(type,value,traceback) = sys.exc_info()
					sys.excepthook(type,value,traceback)
					if self.report: options.add_info("Could not access the vocab cache %s (%s)" % (value,fname), VocabCachingInfo, URI)
			else :
				if self.report :
					if options.refresh_vocab_cache == True :
						options.add_info("Time check is bypassed; refreshing the cache for %s" % URI, VocabCachingInfo)
					else :
						options.add_info("Cache timeout; refreshing the cache for %s" % URI, VocabCachingInfo)
				# we have to refresh the graph
				if self._get_vocab_data(newCache = False) == False :
					# bugger; the cache could not be refreshed, using the current one, and setting the cache artificially
					# to be valid for the coming hour, hoping that the access issues will be resolved by then...
					if self.report:
						options.add_info("Could not refresh vocabulary cache for %s, using the old cache, extended its expiration time by an hour (network problems?)" % URI, VocabCachingInfo, URI)
					fname = os.path.join(self.app_data_dir, self.filename)
					try :
						self.graph = _load(fname)
						self.expiration_date = datetime.datetime.utcnow() + datetime.timedelta(hours=1)
					except Exception :
						# what this means is that the caching becomes impossible VocabCachingInfo
						(type,value,traceback) = sys.exc_info()
						sys.excepthook(type,value,traceback)
						if self.report: options.add_info("Could not access the vocabulary cache %s (%s)" % (value,fname), VocabCachingInfo, URI)
				self.creation_date = datetime.datetime.utcnow()
				if self.report:
					options.add_info("Generated a new cache for %s, with an expiration date of %s" % (URI,self.expiration_date), VocabCachingInfo, URI)
					
				self._store_caches()

	def _get_vocab_data(self, newCache = True) :
		"""Just a macro like function to get the data to be cached"""		
		from pyRdfa.rdfs.process import return_graph
		(self.graph, self.expiration_date) = return_graph(self.uri, self.options, newCache)
		return self.graph != None

	def _store_caches(self) :
		"""Called if the creation date, etc, have been refreshed or new, and
		all content must be put into a cache file
		"""
		# Store the cached version of the vocabulary file
		fname = os.path.join(self.app_data_dir, self.filename)
		try :
			_dump(self.graph, fname)
		except Exception :
			(type,value,traceback) = sys.exc_info()
			if self.report : self.options.add_info("Could not write cache file %s (%s)", (fname,value), VocabCachingInfo, self.uri)
		# Update the index
		self.add_ref(self.uri,(self.filename, self.creation_date, self.expiration_date))
		
#########################################################################################################################################

def offline_cache_generation(args) :
	"""Generate a cache for the vocabulary in args.
	
	@param args: array of vocabulary URIs.
	"""
	class LocalOption :
		def __init__(self) :
			self.vocab_cache_report = True

		def pr(self, wae, txt, warning_type, context) :
			print( "====" )
			if warning_type != None : print( warning_type )
			print( wae + ": " + txt )
			if context != None: print( context )
			print( "====" )
			
		def add_warning(self, txt, warning_type=None, context=None) :
			"""Add a warning to the processor graph.
			@param txt: the warning text. 
			@keyword warning_type: Warning Class
			@type warning_type: URIRef
			@keyword context: possible context to be added to the processor graph
			@type context: URIRef or String
			"""
			self.pr("Warning",txt,warning_type,context)
	
		def add_info(self, txt, info_type=None, context=None) :
			"""Add an informational comment to the processor graph.
			@param txt: the information text. 
			@keyword info_type: Info Class
			@type info_type: URIRef
			@keyword context: possible context to be added to the processor graph
			@type context: URIRef or String
			"""
			self.pr("Info",txt,info_type,context)
	
		def add_error(self, txt, err_type=None, context=None) :
			"""Add an error  to the processor graph.
			@param txt: the information text. 
			@keyword err_type: Error Class
			@type err_type: URIRef
			@keyword context: possible context to be added to the processor graph
			@type context: URIRef or String
			"""
			self.pr("Error",txt,err_type,context)
			
	for uri in args :
		# This should write the cache
		print( ">>>>> Writing Cache <<<<<" )
		writ = CachedVocab(uri,options = LocalOption(),report = True)
		# Now read it back and print the content for tracing
		print( ">>>>> Reading Cache <<<<<" )
		rd = CachedVocab(uri,options = LocalOption(),report = True)
		print( "URI: " + uri )
		print( "default vocab: " + rd.vocabulary )
		print( "terms: %s prefixes: %s" % (rd.terms,rd.ns) )

	
########NEW FILE########
__FILENAME__ = process
# -*- coding: utf-8 -*-
"""
@organization: U{World Wide Web Consortium<http://www.w3.org>}
@author: U{Ivan Herman<a href="http://www.w3.org/People/Ivan/">}
@license: This software is available for use under the
U{W3C SOFTWARE NOTICE AND LICENSE<href="http://www.w3.org/Consortium/Legal/2002/copyright-software-20021231">}

"""

"""
$Id: process.py,v 1.7 2012-03-23 14:06:38 ivan Exp $ $Date: 2012-03-23 14:06:38 $

"""

import sys
import os

import rdflib
from rdflib	import URIRef
from rdflib	import Literal
from rdflib	import BNode
from rdflib	import Namespace
if rdflib.__version__ >= "3.0.0" :
	from rdflib	import RDF  as ns_rdf
	from rdflib	import RDFS as ns_rdfs
	from rdflib	import Graph
else :
	from rdflib.RDFS	import RDFSNS as ns_rdfs
	from rdflib.RDF		import RDFNS  as ns_rdf
	from rdflib.Graph import Graph
	
ns_owl = Namespace("http://www.w3.org/2002/07/owl#")
	
from ..host import MediaTypes
	
from ..utils	import URIOpener

from . import err_outdated_cache
from . import err_unreachable_vocab
from . import err_unparsable_Turtle_vocab
from . import err_unparsable_xml_vocab
from . import err_unparsable_ntriples_vocab
from . import err_unparsable_rdfa_vocab
from . import err_unrecognised_vocab_type

from .. import VocabReferenceError

from .cache import CachedVocab
from .. import HTTPError, RDFaError

#############################################################################################################

def return_graph(uri, options, newCache = False) :
	"""Parse a file, and return an RDFLib Graph. The URI's content type is checked and either one of
	RDFLib's parsers is invoked (for the Turtle, RDF/XML, and N Triple cases) or a separate RDFa processing is invoked
	on the RDFa content.
			
	The Accept header of the HTTP request gives a preference to Turtle, followed by RDF/XML and then HTML (RDFa), in case content negotiation is used.
	
	This function is used to retreive the vocabulary file and turn it into an RDFLib graph.
	
	@param uri: URI for the graph
	@param options: used as a place where warnings can be sent
	@param newCache: in case this is used with caching, whether a new cache is generated; that modifies the warning text
	@return: A tuple consisting of an RDFLib Graph instance and an expiration date); None if the dereferencing or the parsing was unsuccessful
	"""
	def return_to_cache(msg) :
		if newCache :
			options.add_warning(err_unreachable_vocab % uri, warning_type=VocabReferenceError)
		else :
			options.add_warning(err_outdated_cache % uri, warning_type=VocabReferenceError)
	
	retval 			= None
	expiration_date = None
	content			= None
	
	try :
		content = URIOpener(uri,
							{'Accept' : 'text/html;q=0.8, application/xhtml+xml;q=0.8, text/turtle;q=1.0, application/rdf+xml;q=0.9'})
	except HTTPError :
		(type,value,traceback) = sys.exc_info()
		return_to_cache(value)
		return (None,None)
	except RDFaError :
		(type,value,traceback) = sys.exc_info()
		return_to_cache(value)
		return (None,None)
	except Exception :
		(type,value,traceback) = sys.exc_info()
		return_to_cache(value)
		return (None,None)
	
	# Store the expiration date of the newly accessed data
	expiration_date = content.expiration_date
					
	if content.content_type == MediaTypes.turtle :
		try :
			retval = Graph()
			retval.parse(content.data, format="n3")
		except :
			(type,value,traceback) = sys.exc_info()
			options.add_warning(err_unparsable_Turtle_vocab % (uri,value))
	elif content.content_type == MediaTypes.rdfxml :
		try :
			retval = Graph()
			retval.parse(content.data)
		except :
			(type,value,traceback) = sys.exc_info()
			options.add_warning(err_unparsable_Turtle_vocab % (uri,value))
	elif content.content_type == MediaTypes.nt :
		try :
			retval = Graph()
			retval.parse(content.data, format="nt")
		except :
			(type,value,traceback) = sys.exc_info()
			options.add_warning(err_unparsable_ntriples_vocab % (uri,value))
	elif content.content_type in [MediaTypes.xhtml, MediaTypes.html, MediaTypes.xml] or xml_application_media_type.match(content.content_type) != None :
		try :
			from pyRdfa import pyRdfa
			from pyRdfa.options	import Options
			options = Options()
			retval = pyRdfa(options).graph_from_source(content.data)
		except :
			(type,value,traceback) = sys.exc_info()
			options.add_warning(err_unparsable_rdfa_vocab % (uri,value))
	else :
		options.add_warning(err_unrecognised_vocab_type % (uri, content.content_type))
		
	return (retval, expiration_date)
	
############################################################################################
type 				= ns_rdf["type"]
Property 			= ns_rdf["Property"]
Class 				= ns_rdfs["Class"]
subClassOf			= ns_rdfs["subClassOf"]
subPropertyOf		= ns_rdfs["subPropertyOf"]
equivalentProperty	= ns_owl["equivalentProperty"]
equivalentClass 	= ns_owl["equivalentClass"]

class MiniOWL :
	"""
	Class implementing the simple OWL RL Reasoning required by RDFa in managing vocabulary files. This is done via
	a forward chaining process (in the L{closure} method) using a few simple rules as defined by the RDF and the OWL Semantics
	specifications.
	
	@ivar graph: the graph that has to be expanded
	@ivar added_triples: each cycle collects the triples that are to be added to the graph eventually.
	@type added_triples: a set, to ensure the unicity of triples being added
	"""
	def __init__(self, graph, schema_semantics = False) :
		self.graph         		= graph
		self.added_triples 		= None
		self.schema_semantics 	= schema_semantics

	def closure(self) :
		"""
		   Generate the closure the graph. This is the real 'core'.

		   The processing rules store new triples via the L{separate method<store_triple>} which stores
		   them in the L{added_triples<added_triples>} array. If that array is emtpy at the end of a cycle,
		   it means that the whole process can be stopped.
		"""

		# Go cyclically through all rules until no change happens
		new_cycle = True
		cycle_num = 0
		while new_cycle :
			# yes, there was a change, let us go again
			cycle_num += 1

			# go through all rules, and collect the replies (to see whether any change has been done)
			# the new triples to be added are collected separately not to interfere with
			# the current graph yet
			self.added_triples = set()

			# Execute all the rules; these might fill up the added triples array
			for t in self.graph : self.rules(t)

			# Add the tuples to the graph (if necessary, that is). If any new triple has been generated, a new cycle
			# will be necessary...
			new_cycle = len(self.added_triples) > 0

			for t in self.added_triples : self.graph.add(t)

	def store_triple(self, t) :
		"""
		In contrast to its name, this does not yet add anything to the graph itself, it just stores the tuple in an
		L{internal set<added_triples>}. (It is important for this to be a set: some of the rules in the various closures may
		generate the same tuples several times.) Before adding the tuple to the set, the method checks whether
		the tuple is in the final graph already (if yes, it is not added to the set).

		The set itself is emptied at the start of every processing cycle; the triples are then effectively added to the
		graph at the end of such a cycle. If the set is
		actually empty at that point, this means that the cycle has not added any new triple, and the full processing can stop.

		@param t: the triple to be added to the graph, unless it is already there
		@type t: a 3-element tuple of (s,p,o)
		"""
		(s,p,o) = t
		if t not in self.graph :
			self.added_triples.add(t)

	def rules(self, t) :
		"""
			Go through the OWL-RL entailement rules prp-spo1, prp-eqp1, prp-eqp2, cax-sco, cax-eqc1, and cax-eqc2 by extending the graph.
			@param t: a triple (in the form of a tuple)
		"""
		s,p,o = t
		if self.schema_semantics :
			# extra resonings on the vocabulary only to reduce the overall load by reducing the expected number of chaining cycles
			if p == subPropertyOf :
				for Z,Y,xxx in self.graph.triples((o, subPropertyOf, None)) :
					self.store_triple((s,subPropertyOf,xxx))  
			elif p == equivalentProperty :
				for Z,Y,xxx in self.graph.triples((o, equivalentProperty, None)) :
					self.store_triple((s,equivalentProperty,xxx))  
				for xxx,Y,Z in self.graph.triples((None, equivalentProperty, s)) :
					self.store_triple((xxx,equivalentProperty,o))  
			elif p == subClassOf :
				for Z,Y,xxx in self.graph.triples((o, subClassOf, None)) :
					self.store_triple((s,subClassOf,xxx))
			elif p == equivalentClass :
				for Z,Y,xxx in self.graph.triples((o, equivalentClass, None)) :
					self.store_triple((s,equivalentClass,xxx))  
				for xxx,Y,Z in self.graph.triples((None, equivalentClass, s)) :
					self.store_triple((xxx,equivalentClass,o))  
		else :
			if p == subPropertyOf :
				# prp-spo1
				for zzz,Z,www in self.graph.triples((None, s, None)) :
					self.store_triple((zzz, o, www))
			elif p == equivalentProperty :
				# prp-eqp1
				for zzz,Z,www in self.graph.triples((None, s, None)) :
					self.store_triple((zzz, o, www))
				# prp-eqp2
				for zzz,Z,www in self.graph.triples((None, o, None)) :
					self.store_triple((zzz, s, www))					
			elif p == subClassOf :
				# cax-sco
				for vvv,Y,Z in self.graph.triples((None, type, s)) :
					self.store_triple((vvv, type, o))
			elif p == equivalentClass :
				# cax-eqc1
				for vvv,Y,Z in self.graph.triples((None, type, s)) :
					self.store_triple((vvv, type, o))
				# cax-eqc2
				for vvv,Y,Z in self.graph.triples((None, type, o)) :
					self.store_triple((vvv, type, s))

########################################################################################################

def process_rdfa_sem(graph, options) :
	"""
	Expand the graph through the minimal RDFS and OWL rules defined for RDFa.
	
	The expansion is done in several steps:
	 1. the vocabularies are retrieved from the incoming graph (there are RDFa triples generated for that)
	 2. all vocabularies are merged into a separate vocabulary graph
	 3. the RDFS/OWL expansion is done on the vocabulary graph, to take care of all the subproperty, subclass, etc, chains
	 4. the (expanded) vocabulary graph content is added to the incoming graph
	 5. the incoming graph is expanded
	 6. the triples appearing in the vocabulary graph are removed from the incoming graph, to avoid unnecessary extra triples from the data
	 
	@param graph: an RDFLib Graph instance, to be expanded
	@param options: options as defined for the RDFa run; used to generate warnings
	@type options: L{pyRdfa.Options}
	"""
	# 1. collect the vocab URI-s
	vocabs = set()
	from pyRdfa import RDFA_VOCAB
	for ((s,p,v)) in graph.triples((None,RDFA_VOCAB,None)) :
		vocabs.add((str(v)))

	if len(vocabs) >= 0 :
		# 2. get all the vocab graphs
		vocab_graph = Graph()
		for uri in vocabs :
			if options.vocab_cache :
				v_graph = CachedVocab(uri, options).graph
			else :
				(v_graph, exp_date) = return_graph(uri, options)
			if v_graph != None :
				for t in v_graph :
					vocab_graph.add(t)
				
		# 3. Get the closure of the vocab graph; this will take care of local subproperty, etc, statements
		# Strictly speaking this is not necessary, but will speed up processing, because it may save chaining cycles on the
		# real graph
		MiniOWL(vocab_graph, schema_semantics = True).closure()
		
		# 4. Now get the vocab graph content added to the default graph
		for t in vocab_graph :
			graph.add(t)
						
		# 5. get the graph expanded through RDFS
		MiniOWL(graph).closure()
		
		# 4. clean up the graph by removing the schema triples
		for t in vocab_graph : graph.remove(t)
	
	# That was it...
	return graph


########NEW FILE########
__FILENAME__ = state
# -*- coding: utf-8 -*-
"""
Parser's execution context (a.k.a. state) object and handling. The state includes:

  - language, retrieved from C{@xml:lang} or C{@lang}
  - URI base, determined by C{<base>} or set explicitly. This is a little bit superfluous, because the current RDFa syntax does not make use of C{@xml:base}; i.e., this could be a global value.  But the structure is prepared to add C{@xml:base} easily, if needed.
  - options, in the form of an L{options<pyRdfa.options>} instance
  - a separate vocabulary/CURIE handling resource, in the form of an L{termorcurie<pyRdfa.TermOrCurie>} instance

The execution context object is also used to handle URI-s, CURIE-s, terms, etc.

@summary: RDFa parser execution context
@organization: U{World Wide Web Consortium<http://www.w3.org>}
@author: U{Ivan Herman<a href="http://www.w3.org/People/Ivan/">}
@license: This software is available for use under the
U{W3C SOFTWARE NOTICE AND LICENSE<href="http://www.w3.org/Consortium/Legal/2002/copyright-software-20021231">}
"""

"""
$Id: state.py,v 1.23 2013-10-16 11:48:54 ivan Exp $
$Date: 2013-10-16 11:48:54 $
"""
import sys
(py_v_major, py_v_minor, py_v_micro, py_v_final, py_v_serial) = sys.version_info

import rdflib
from rdflib	import URIRef
from rdflib	import Literal
from rdflib	import BNode
from rdflib	import Namespace
if rdflib.__version__ >= "3.0.0" :
	from rdflib	import RDF  as ns_rdf
	from rdflib	import RDFS as ns_rdfs
else :
	from rdflib.RDFS	import RDFSNS as ns_rdfs
	from rdflib.RDF		import RDFNS  as ns_rdf

from .options	import Options
from .utils 	import quote_URI
from .host 		import HostLanguage, accept_xml_base, accept_xml_lang, beautifying_prefixes

from .termorcurie	import TermOrCurie
from .				import UnresolvablePrefix, UnresolvableTerm

from . import err_lang							
from . import err_URI_scheme						
from . import err_illegal_safe_CURIE				
from . import err_no_CURIE_in_safe_CURIE			
from . import err_undefined_terms					
from . import err_non_legal_CURIE_ref				
from . import err_undefined_CURIE					

if py_v_major >= 3 :
	from urllib.parse import urlparse, urlunparse, urlsplit, urljoin
else :	
	from urlparse import urlparse, urlunparse, urlsplit, urljoin

class ListStructure :
	"""Special class to handle the C{@inlist} type structures in RDFa 1.1; stores the "origin", i.e,
	where the list will be attached to, and the mappings as defined in the spec.
	"""
	def __init__(self) :
		self.mapping = {}
		self.origin	  = None

#### Core Class definition
class ExecutionContext :
	"""State at a specific node, including the current set of namespaces in the RDFLib sense, current language,
	the base, vocabularies, etc. The class is also used to interpret URI-s and CURIE-s to produce
	URI references for RDFLib.
	
	@ivar options: reference to the overall options
	@type options: L{Options}
	@ivar base: the 'base' URI
	@ivar parsedBase: the parsed version of base, as produced by urlparse.urlsplit
	@ivar defaultNS: default namespace (if defined via @xmlns) to be used for XML Literals
	@ivar lang: language tag (possibly None)
	@ivar term_or_curie: vocabulary management class instance
	@type term_or_curie: L{termorcurie.TermOrCurie}
	@ivar list_mapping: dictionary of arrays, containing a list of URIs key-ed via properties for lists
	@ivar node: the node to which this state belongs
	@type node: DOM node instance
	@ivar rdfa_version: RDFa version of the content
	@type rdfa_version: String
	@ivar supress_lang: in some cases, the effect of the lang attribute should be supressed for the given node, although it should be inherited down below (example: @value attribute of the data element in HTML5)
	@type supress_lang: Boolean
	@cvar _list: list of attributes that allow for lists of values and should be treated as such
	@cvar _resource_type: dictionary; mapping table from attribute name to the exact method to retrieve the URI(s). Is initialized at first instantiation.
	"""

	# list of attributes that allow for lists of values and should be treated as such	
	_list = [ "rel", "rev", "property", "typeof", "role" ]
	# mapping table from attribute name to the exact method to retrieve the URI(s).
	_resource_type = {}
	
	def __init__(self, node, graph, inherited_state=None, base="", options=None, rdfa_version = None) :
		"""
		@param node: the current DOM Node
		@param graph: the RDFLib Graph
		@keyword inherited_state: the state as inherited
		from upper layers. This inherited_state is mixed with the state information
		retrieved from the current node.
		@type inherited_state: L{state.ExecutionContext}
		@keyword base: string denoting the base URI for the specific node. This overrides the possible
		base inherited from the upper layers. The 
		current XHTML+RDFa syntax does not allow the usage of C{@xml:base}, but SVG1.2 does, so this is
		necessary for SVG (and other possible XML dialects that accept C{@xml:base})
		@keyword options: invocation options, and references to warning graphs
		@type options: L{Options<pyRdfa.options>}
		"""
		def remove_frag_id(uri) :
			"""
			The fragment ID for self.base must be removed
			"""
			try :
				# To be on the safe side:-)
				t = urlparse(uri)
				return urlunparse((t[0],t[1],t[2],t[3],t[4],""))
			except :
				return uri
			
		# This is, conceptually, an additional class initialization, but it must be done run time, otherwise import errors show up
		if len(	ExecutionContext._resource_type ) == 0 :	
			ExecutionContext._resource_type = {
				"href"		:	ExecutionContext._URI,
				"src"		:	ExecutionContext._URI,
				"vocab"	    :   ExecutionContext._URI,
			
				"about"		:	ExecutionContext._CURIEorURI, 
				"resource"	:	ExecutionContext._CURIEorURI, 
			
				"rel"		:	ExecutionContext._TERMorCURIEorAbsURI,
				"rev"		:	ExecutionContext._TERMorCURIEorAbsURI,
				"datatype"	:	ExecutionContext._TERMorCURIEorAbsURI,
				"typeof"	:	ExecutionContext._TERMorCURIEorAbsURI,
				"property"	:	ExecutionContext._TERMorCURIEorAbsURI,
				"role"		:	ExecutionContext._TERMorCURIEorAbsURI,
			}	
		#-----------------------------------------------------------------
		self.node = node
		
		#-----------------------------------------------------------------
		# Settling the base. In a generic XML, xml:base should be accepted at all levels (though this is not the
		# case in, say, XHTML...)
		# At the moment, it is invoked with a 'None' at the top level of parsing, that is
		# when the <base> element is looked for (for the HTML cases, that is)
		if inherited_state :
			self.rdfa_version		= inherited_state.rdfa_version
			self.base				= inherited_state.base
			self.options			= inherited_state.options
						
			self.list_mapping 		= inherited_state.list_mapping
			self.new_list			= False
			
			# for generic XML versions the xml:base attribute should be handled
			if self.options.host_language in accept_xml_base and node.hasAttribute("xml:base") :
				self.base = remove_frag_id(node.getAttribute("xml:base"))
		else :
			# this is the branch called from the very top			
			self.list_mapping = ListStructure()
			self.new_list	  = True
			
			if rdfa_version is not None :
				self.rdfa_version = rdfa_version
			else :
				from . import rdfa_current_version				
				self.rdfa_version = rdfa_current_version

			# This value can be overwritten by a @version attribute
			if node.hasAttribute("version") :
				top_version = node.getAttribute("version")
				if top_version.find("RDFa 1.0") != -1 or top_version.find("RDFa1.0") != -1 :
					self.rdfa_version = "1.0"
				elif top_version.find("RDFa 1.1") != -1 or top_version.find("RDFa1.1") != -1 :
					self.rdfa_version = "1.1"						
			
			# this is just to play safe. I believe this should actually not happen...
			if options == None :
				from . import Options
				self.options = Options()
			else :
				self.options = options

			self.base = ""
			# handle the base element case for HTML
			if self.options.host_language in [ HostLanguage.xhtml, HostLanguage.html5, HostLanguage.xhtml5  ] :
				for bases in node.getElementsByTagName("base") :
					if bases.hasAttribute("href") :
						self.base = remove_frag_id(bases.getAttribute("href"))
						continue
			elif self.options.host_language in accept_xml_base and node.hasAttribute("xml:base") :
				self.base = remove_frag_id(node.getAttribute("xml:base"))
				
			# If no local setting for base occurs, the input argument has it
			if self.base == "" :
				self.base = base
				
			# Perform an extra beautification in RDFLib
			if self.options.host_language in beautifying_prefixes :
				dict = beautifying_prefixes[self.options.host_language]
				for key in dict :
					graph.bind(key,dict[key])
					
			input_info = "Input Host Language:%s, RDFa version:%s, base:%s" % (self.options.host_language, self.rdfa_version, self.base)
			self.options.add_info(input_info)

								
		#-----------------------------------------------------------------
		# this will be used repeatedly, better store it once and for all...		
		self.parsedBase = urlsplit(self.base)

		#-----------------------------------------------------------------
		# generate and store the local CURIE handling class instance
		self.term_or_curie = TermOrCurie(self, graph, inherited_state)

		#-----------------------------------------------------------------
		# Settling the language tags
		# @lang has priority over @xml:lang
		# it is a bit messy: the three fundamental modes (xhtml, html, or xml) are all slightly different:-(
		# first get the inherited state's language, if any
		if inherited_state :
			self.lang = inherited_state.lang
		else :
			self.lang = None
			
		self.supress_lang = False
			
			
		if self.options.host_language in [ HostLanguage.xhtml, HostLanguage.xhtml5, HostLanguage.html5 ] :
			# we may have lang and xml:lang
			if node.hasAttribute("lang") :
				lang = node.getAttribute("lang").lower()
			else :
				lang = None
			if node.hasAttribute("xml:lang") :
				xmllang = node.getAttribute("xml:lang").lower()
			else :
				xmllang = None
			# First of all, set the value, if any
			if xmllang != None :
				# this has priority
				if len(xmllang) != 0 :
					self.lang = xmllang
				else :
					self.lang = None
			elif lang != None :
				if len(lang) != 0 :
					self.lang = lang
				else :
					self.lang = None					
			# Ideally, a warning should be generated if lang and xmllang are both present with different values. But
			# the HTML5 Parser does its magic by overriding a lang value if xmllang is present, so the potential
			# error situations are simply swallowed...
				
		elif self.options.host_language in accept_xml_lang and node.hasAttribute("xml:lang") :
				self.lang = node.getAttribute("xml:lang").lower()
				if len(self.lang) == 0 : self.lang = None
			
		#-----------------------------------------------------------------
		# Set the default namespace. Used when generating XML Literals
		if node.hasAttribute("xmlns") :
			self.defaultNS = node.getAttribute("xmlns")
		elif inherited_state and inherited_state.defaultNS != None :
			self.defaultNS = inherited_state.defaultNS
		else :
			self.defaultNS = None
	# end __init__

	def _URI(self, val) :
		"""Returns a URI for a 'pure' URI (ie, not a CURIE). The method resolves possible relative URI-s. It also
		checks whether the URI uses an unusual URI scheme (and issues a warning); this may be the result of an
		uninterpreted CURIE...
		@param val: attribute value to be interpreted
		@type val: string
		@return: an RDFLib URIRef instance
		"""
		def create_URIRef(uri, check = True) :
			"""
			Mini helping function: it checks whether a uri is using a usual scheme before a URIRef is created. In case
			there is something unusual, a warning is generated (though the URIRef is created nevertheless)
			@param uri: (absolute) URI string
			@return: an RDFLib URIRef instance
			"""
			from .	import uri_schemes
			val = uri.strip()
			if check and urlsplit(val)[0] not in uri_schemes :
				self.options.add_warning(err_URI_scheme % val.strip(), node=self.node.nodeName)
			return URIRef(val)

		def join(base, v, check = True) :
			"""
			Mini helping function: it makes a urljoin for the paths. Based on the python library, but
			that one has a bug: in some cases it
			swallows the '#' or '?' character at the end. This is clearly a problem with
			Semantic Web URI-s, so this is checked, too
			@param base: base URI string
			@param v: local part
			@param check: whether the URI should be checked against the list of 'existing' URI schemes
			@return: an RDFLib URIRef instance
			"""
			# UGLY!!! There is a bug for a corner case in python version <= 2.5.X
			if len(v) > 0 and v[0] == '?' and (py_v_major < 3 and py_v_minor <= 5) :
				return create_URIRef(base+v, check)
			####
			
			joined = urljoin(base, v)
			try :
				if v[-1] != joined[-1] and (v[-1] == "#" or v[-1] == "?") :
					return create_URIRef(joined + v[-1], check)
				else :
					return create_URIRef(joined, check)
			except :
				return create_URIRef(joined, check)

		if val == "" :
			# The fragment ID must be removed...
			return URIRef(self.base)
			
		# fall back on good old traditional URI-s.
		# To be on the safe side, let us use the Python libraries
		if self.parsedBase[0] == "" :
			# base is, in fact, a local file name
			# The following call is just to be sure that some pathological cases when
			# the ':' _does_ appear in the URI but not in a scheme position is taken
			# care of properly...
			
			key = urlsplit(val)[0]
			if key == "" :
				# relative URI, to be combined with local file name:
				return join(self.base, val, check = False)
			else :
				return create_URIRef(val)
		else :
			# Trust the python library...
			# Well, not quite:-) there is what is, in my view, a bug in the urljoin; in some cases it
			# swallows the '#' or '?' character at the end. This is clearly a problem with
			# Semantic Web URI-s			
			return join(self.base, val)
	# end _URI

	def _CURIEorURI(self, val) :
		"""Returns a URI for a (safe or not safe) CURIE. In case it is a safe CURIE but the CURIE itself
		is not defined, an error message is issued. Otherwise, if it is not a CURIE, it is taken to be a URI
		@param val: attribute value to be interpreted
		@type val: string
		@return: an RDFLib URIRef instance or None
		"""
		if val == "" :
			return URIRef(self.base)

		safe_curie = False
		if val[0] == '[' :
			# If a safe CURIE is asked for, a pure URI is not acceptable.
			# Is checked below, and that is why the safe_curie flag is necessary
			if val[-1] != ']' :
				# that is certainly forbidden: an incomplete safe CURIE
				self.options.add_warning(err_illegal_safe_CURIE % val, UnresolvablePrefix, node=self.node.nodeName)
				return None
			else :
				val = val[1:-1]
				safe_curie = True
		# There is a branch here depending on whether we are in 1.1 or 1.0 mode
		if self.rdfa_version >= "1.1" :
			retval = self.term_or_curie.CURIE_to_URI(val)
			if retval == None :
				# the value could not be interpreted as a CURIE, ie, it did not produce any valid URI.
				# The rule says that then the whole value should be considered as a URI
				# except if it was part of a safe CURIE. In that case it should be ignored...
				if safe_curie :
					self.options.add_warning(err_no_CURIE_in_safe_CURIE % val, UnresolvablePrefix, node=self.node.nodeName)
					return None
				else :
					return self._URI(val)
			else :
				# there is an unlikely case where the retval is actually a URIRef with a relative URI. Better filter that one out
				if isinstance(retval, BNode) == False and urlsplit(str(retval))[0] == "" :
					# yep, there is something wrong, a new URIRef has to be created:
					return URIRef(self.base+str(retval))
				else :
					return retval
		else :
			# in 1.0 mode a CURIE can be considered only in case of a safe CURIE
			if safe_curie :
				return self.term_or_curie.CURIE_to_URI(val)
			else :
				return self._URI(val)
	# end _CURIEorURI

	def _TERMorCURIEorAbsURI(self, val) :
		"""Returns a URI either for a term or for a CURIE. The value must be an NCNAME to be handled as a term; otherwise
		the method falls back on a CURIE or an absolute URI.
		@param val: attribute value to be interpreted
		@type val: string
		@return: an RDFLib URIRef instance or None
		"""
		from . import uri_schemes
		# This case excludes the pure base, ie, the empty value
		if val == "" :
			return None
		
		from .termorcurie import ncname, termname
		if termname.match(val) :
			# This is a term, must be handled as such...			
			retval = self.term_or_curie.term_to_URI(val)
			if not retval :
				self.options.add_warning(err_undefined_terms % val, UnresolvableTerm, node=self.node.nodeName, buggy_value = val)
				return None
			else :
				return retval
		else :
			# try a CURIE
			retval = self.term_or_curie.CURIE_to_URI(val)
			if retval :
				return retval
			elif self.rdfa_version >= "1.1" :
				# See if it is an absolute URI
				scheme = urlsplit(val)[0]
				if scheme == "" :
					# bug; there should be no relative URIs here
					self.options.add_warning(err_non_legal_CURIE_ref % val, UnresolvablePrefix, node=self.node.nodeName)
					return None
				else :
					if scheme not in uri_schemes :
						self.options.add_warning(err_URI_scheme % val.strip(), node=self.node.nodeName)
					return URIRef(val)
			else :
				# rdfa 1.0 case
				self.options.add_warning(err_undefined_CURIE % val.strip(), UnresolvablePrefix, node=self.node.nodeName)
				return None
	# end _TERMorCURIEorAbsURI

	# -----------------------------------------------------------------------------------------------

	def getURI(self, attr) :
		"""Get the URI(s) for the attribute. The name of the attribute determines whether the value should be
		a pure URI, a CURIE, etc, and whether the return is a single element of a list of those. This is done
		using the L{ExecutionContext._resource_type} table.
		@param attr: attribute name
		@type attr: string
		@return: an RDFLib URIRef instance (or None) or a list of those
		"""
		if self.node.hasAttribute(attr) :
			val = self.node.getAttribute(attr)
		else :
			if attr in ExecutionContext._list :
				return []
			else :
				return None
		
		# This may raise an exception if the attr has no key. This, actually,
		# should not happen if the code is correct, but it does not harm having it here...
		try :
			func = ExecutionContext._resource_type[attr]
		except :
			# Actually, this should not happen...
			func = ExecutionContext._URI
		
		if attr in ExecutionContext._list :
			# Allows for a list
			resources = [ func(self, v.strip()) for v in val.strip().split() if v != None ]
			retval = [ r for r in resources if r != None ]
		else :
			retval = func(self, val.strip())
		return retval
	# end getURI
	
	def getResource(self, *args) :
		"""Get single resources from several different attributes. The first one that returns a valid URI wins.
		@param args: variable list of attribute names, or a single attribute being a list itself.
		@return: an RDFLib URIRef instance (or None) :
		"""
		if len(args) == 0 :
			return None
		if isinstance(args[0], tuple) or isinstance(args[0], list) :
			rargs = args[0]
		else :
			rargs = args
			
		for resource in rargs :
			uri = self.getURI(resource)
			if uri != None : return uri
		return None
	
	# -----------------------------------------------------------------------------------------------
	def reset_list_mapping(self, origin=None) :
		"""
		Reset, ie, create a new empty dictionary for the list mapping.
		"""
		self.list_mapping = ListStructure()
		if origin: self.set_list_origin(origin)
		self.new_list = True

	def list_empty(self) :
		"""
		Checks whether the list is empty.
		@return: Boolean
		"""
		return len(self.list_mapping.mapping) == 0
		
	def get_list_props(self) :
		"""
		Return the list of property values in the list structure
		@return: list of URIRef
		"""
		return list(self.list_mapping.mapping.keys())
		
	def get_list_value(self,prop) :
		"""
		Return the list of values in the list structure for a specific property
		@return: list of RDF nodes
		"""
		return self.list_mapping.mapping[prop]
		
	def set_list_origin(self, origin) :
		"""
		Set the origin of the list, ie, the subject to attach the final list(s) to
		@param origin: URIRef
		"""		
		self.list_mapping.origin = origin
		
	def get_list_origin(self) :
		"""
		Return the origin of the list, ie, the subject to attach the final list(s) to
		@return: URIRef
		"""		
		return self.list_mapping.origin
		
	def add_to_list_mapping(self, property, resource) :
		"""Add a new property-resource on the list mapping structure. The latter is a dictionary of arrays;
		if the array does not exist yet, it will be created on the fly.
		
		@param property: the property URI, used as a key in the dictionary
		@param resource: the resource to be added to the relevant array in the dictionary. Can be None; this is a dummy
		placeholder for C{<span rel="property" inlist>...</span>} constructions that may be filled in by children or siblings; if not
		an empty list has to be generated.
		"""
		if property in self.list_mapping.mapping :
			if resource != None :
				# indeed, if it is None, than it should not override anything
				if self.list_mapping.mapping[property] == None :
					# replacing a dummy with real content
					self.list_mapping.mapping[property] = [ resource ]
				else :			
					self.list_mapping.mapping[property].append(resource)
		else :
			if resource != None :
				self.list_mapping.mapping[property] = [ resource ]
			else :
				self.list_mapping.mapping[property] = None
				

####################

########NEW FILE########
__FILENAME__ = termorcurie
# -*- coding: utf-8 -*-
"""
Management of vocabularies, terms, and their mapping to URI-s. The main class of this module (L{TermOrCurie}) is,
conceptually, part of the overall state of processing at a node (L{state.ExecutionContext}) but putting it into a separate
module makes it easider to maintain.

@summary: Management of vocabularies, terms, and their mapping to URI-s.
@requires: U{RDFLib package<http://rdflib.net>}
@organization: U{World Wide Web Consortium<http://www.w3.org>}
@author: U{Ivan Herman<a href="http://www.w3.org/People/Ivan/">}
@license: This software is available for use under the
U{W3C SOFTWARE NOTICE AND LICENSE<href="http://www.w3.org/Consortium/Legal/2002/copyright-software-20021231">}

@var XHTML_PREFIX: prefix for the XHTML vocabulary URI (set to 'xhv')
@var XHTML_URI: URI prefix of the XHTML vocabulary
@var ncname: Regular expression object for NCNAME
@var termname: Regular expression object for a term
@var xml_application_media_type: Regular expression object for a general XML application media type
"""

"""
$Id: termorcurie.py,v 1.12 2013-10-16 11:48:54 ivan Exp $
$Date: 2013-10-16 11:48:54 $
"""

import re, sys
import xml.dom.minidom
import random

if sys.version_info[0] >= 3 :
	from urllib.parse import urlsplit
else :	
	from urlparse import urlsplit


import rdflib
from rdflib	import URIRef
from rdflib	import Literal
from rdflib	import BNode
from rdflib	import Namespace
if rdflib.__version__ >= "3.0.0" :
	from rdflib	import Graph
	from rdflib	import RDF  as ns_rdf
	from rdflib	import RDFS as ns_rdfs
else :
	from rdflib.Graph	import Graph
	from rdflib.RDFS	import RDFSNS as ns_rdfs
	from rdflib.RDF		import RDFNS  as ns_rdf

from .options		import Options
from .utils 		import quote_URI, URIOpener
from .host 			import MediaTypes, HostLanguage, predefined_1_0_rel, warn_xmlns_usage
from .				import IncorrectPrefixDefinition, RDFA_VOCAB, UnresolvableReference, PrefixRedefinitionWarning
from .				import ns_rdfa

from . import err_redefining_URI_as_prefix		
from . import err_xmlns_deprecated				
from . import err_bnode_local_prefix				
from . import err_col_local_prefix				
from . import err_missing_URI_prefix				
from . import err_invalid_prefix					
from . import err_no_default_prefix				
from . import err_prefix_and_xmlns				
from . import err_non_ncname_prefix				
from . import err_absolute_reference				
from . import err_query_reference				
from . import err_fragment_reference
from . import err_prefix_redefinition


# Regular expression object for NCNAME
ncname   = re.compile("^[A-Za-z][A-Za-z0-9._-]*$")

# Regular expression object for term name
termname = re.compile("^[A-Za-z]([A-Za-z0-9._-]|/)*$")

# Regular expression object for a general XML application media type
xml_application_media_type = re.compile("application/[a-zA-Z0-9]+\+xml")

XHTML_PREFIX = "xhv"
XHTML_URI    = "http://www.w3.org/1999/xhtml/vocab#"

#### Managing blank nodes for CURIE-s: mapping from local names to blank nodes.
_bnodes = {}
_empty_bnode = BNode()

####

class InitialContext :
	"""
	Get the initial context values. In most cases this class has an empty content, except for the
	top level (in case of RDFa 1.1). Each L{TermOrCurie} class has one instance of this class. It provides initial
	mappings for terms, namespace prefixes, etc, that the top level L{TermOrCurie} instance uses for its own initialization.
	
	@ivar terms: collection of all term mappings
	@type terms: dictionary
	@ivar ns: namespace mapping
	@type ns: dictionary
	@ivar vocabulary: default vocabulary
	@type vocabulary: string
	"""	
	
	def __init__(self, state, top_level) :
		"""
		@param state: the state behind this term mapping
		@type state: L{state.ExecutionContext}
		@param top_level : whether this is the top node of the DOM tree (the only place where initial contexts are handled)
		@type top_level : boolean
		"""		
		self.state = state

		# This is to store the local terms
		self.terms  = {}
		# This is to store the local Namespaces (a.k.a. prefixes)
		self.ns     = {}
		# Default vocabulary
		self.vocabulary = None
		
		if state.rdfa_version < "1.1" or top_level == False :
			return
		
		from .initialcontext	import initial_context    as context_data
		from .host 				import initial_contexts   as context_ids
		from .host				import default_vocabulary

		for id in context_ids[state.options.host_language] :
			# This gives the id of a initial context, valid for this media type:
			data = context_data[id]
			
			# Merge the context data with the overall definition
			if state.options.host_language in default_vocabulary :
				self.vocabulary = default_vocabulary[state.options.host_language]
			elif data.vocabulary != "" :
				self.vocabulary = data.vocabulary

			for key in data.terms :
				self.terms[key] = URIRef(data.terms[key])
			for key in data.ns :
				self.ns[key] = (Namespace(data.ns[key]),False)


##################################################################################################################

class TermOrCurie :
	"""
	Wrapper around vocabulary management, ie, mapping a term to a URI, as well as a CURIE to a URI. Each instance of this class belongs to a
	"state", instance of L{state.ExecutionContext}. Context definitions are managed at initialization time.
	
	(In fact, this class is, conceptually, part of the overall state at a node, and has been separated here for an
	easier maintenance.)
	
	The class takes care of the stack-like behavior of vocabulary items, ie, inheriting everything that is possible
	from the "parent". At initialization time, this works through the prefix definitions (i.e., C{@prefix} or C{@xmln:} attributes)
	and/or C{@vocab} attributes.
	
	@ivar state: State to which this instance belongs
	@type state: L{state.ExecutionContext}
	@ivar graph: The RDF Graph under generation
	@type graph: rdflib.Graph
	@ivar terms: mapping from terms to URI-s
	@type terms: dictionary
	@ivar ns: namespace declarations, ie, mapping from prefixes to URIs
	@type ns: dictionary
	@ivar default_curie_uri: URI for a default CURIE
	"""
	def __init__(self, state, graph, inherited_state) :
		"""Initialize the vocab bound to a specific state. 
		@param state: the state to which this vocab instance belongs to
		@type state: L{state.ExecutionContext}
		@param graph: the RDF graph being worked on
		@type graph: rdflib.Graph
		@param inherited_state: the state inherited by the current state. 'None' if this is the top level state.
		@type inherited_state: L{state.ExecutionContext}
		"""
		def check_prefix(pr) :
			from . import uri_schemes
			if pr in uri_schemes :
				# The prefix being defined is a registered URI scheme, better avoid it...
				state.options.add_warning(err_redefining_URI_as_prefix % pr, node=state.node.nodeName)
				
		self.state	= state
		self.graph	= graph
		
		# --------------------------------------------------------------------------------
		# This is set to non-void only on the top level and in the case of 1.1
		default_vocab = InitialContext(self.state, inherited_state == None)
		
		# Set the default CURIE URI
		if inherited_state == None :
			# This is the top level...
			self.default_curie_uri = Namespace(XHTML_URI)
			# self.graph.bind(XHTML_PREFIX, self.default_curie_uri)
		else :
			self.default_curie_uri = inherited_state.term_or_curie.default_curie_uri
		
		# --------------------------------------------------------------------------------
		# Set the default term URI
		# This is a 1.1 feature, ie, should be ignored if the version is < 1.0
		if state.rdfa_version >= "1.1" :
			# that is the absolute default setup...
			if inherited_state == None :
				self.default_term_uri = None
			else :
				self.default_term_uri = inherited_state.term_or_curie.default_term_uri
				
			# see if the initial context has defined a default vocabulary:
			if default_vocab.vocabulary :
				self.default_term_uri = default_vocab.vocabulary
				
			# see if there is local vocab that would override previous settings
			# However, care should be taken with the vocab="" value that should not become a URI...
			# Indeed, this value is used to 'vipe out', ie, get back to the default vocabulary...
			if self.state.node.hasAttribute("vocab") and self.state.node.getAttribute("vocab") == "" :
				self.default_term_uri = default_vocab.vocabulary
			else :
				def_term_uri = self.state.getURI("vocab")
				if def_term_uri and def_term_uri != "" :			
					self.default_term_uri = def_term_uri
					self.graph.add((URIRef(self.state.base),RDFA_VOCAB,URIRef(def_term_uri)))
		else :
			self.default_term_uri = None
		
		# --------------------------------------------------------------------------------
		# The simpler case: terms, adding those that have been defined by a possible initial context
		if inherited_state is None :
			# this is the vocabulary belonging to the top level of the tree!
			self.terms = {}
			if state.rdfa_version >= "1.1" :
				# Simply get the terms defined by the default vocabularies. There is no need for merging
				for key in default_vocab.terms :
					self.terms[key] = default_vocab.terms[key]
			else :
				# The terms are hardwired...
				for key in predefined_1_0_rel :
					self.terms[key] = URIRef(XHTML_URI + key)
		else :
			# just refer to the inherited terms
			self.terms = inherited_state.term_or_curie.terms

		#-----------------------------------------------------------------
		# the locally defined namespaces
		dict = {}
		# locally defined xmlns namespaces, necessary for correct XML Literal generation
		xmlns_dict = {}

		# Add the locally defined namespaces using the xmlns: syntax
		for i in range(0, state.node.attributes.length) :
			attr = state.node.attributes.item(i)
			if attr.name.find('xmlns:') == 0 :	
				# yep, there is a namespace setting
				prefix = attr.localName
				if prefix != "" : # exclude the top level xmlns setting...
					if state.rdfa_version >= "1.1" and state.options.host_language in warn_xmlns_usage :
						state.options.add_warning(err_xmlns_deprecated % prefix, IncorrectPrefixDefinition, node=state.node.nodeName)
					if prefix == "_" :
						state.options.add_warning(err_bnode_local_prefix, IncorrectPrefixDefinition, node=state.node.nodeName)
					elif prefix.find(':') != -1 :
						state.options.add_warning(err_col_local_prefix % prefix, IncorrectPrefixDefinition, node=state.node.nodeName)
					else :					
						# quote the URI, ie, convert special characters into %.. This is
						# true, for example, for spaces
						uri = quote_URI(attr.value, state.options)
						# create a new RDFLib Namespace entry
						ns = Namespace(uri)
						# Add an entry to the dictionary if not already there (priority is left to right!)
						if state.rdfa_version >= "1.1" :
							pr = prefix.lower()
						else :
							pr = prefix
						dict[pr]       = ns
						xmlns_dict[pr] = ns
						self.graph.bind(pr,ns)
						check_prefix(pr)

		# Add the locally defined namespaces using the @prefix syntax
		# this may override the definition @xmlns
		if state.rdfa_version >= "1.1" and state.node.hasAttribute("prefix") :
			pr = state.node.getAttribute("prefix")
			if pr != None :
				# separator character is whitespace
				pr_list = pr.strip().split()
				# range(0, len(pr_list), 2) 
				for i in range(len(pr_list) - 2, -1, -2) :
					prefix = pr_list[i]
					# see if there is a URI at all
					if i == len(pr_list) - 1 :
						state.options.add_warning(err_missing_URI_prefix % (prefix,pr), node=state.node.nodeName)
						break
					else :
						value = pr_list[i+1]
					
					# see if the value of prefix is o.k., ie, there is a ':' at the end
					if prefix[-1] != ':' :
						state.options.add_warning(err_invalid_prefix % (prefix,pr), IncorrectPrefixDefinition, node=state.node.nodeName)
						continue
					elif prefix == ":" :
						state.options.add_warning(err_no_default_prefix % pr, IncorrectPrefixDefinition, node=state.node.nodeName)
						continue						
					else :
						prefix = prefix[:-1]
						uri    = Namespace(quote_URI(value, state.options))
						if prefix == "" :
							#something to be done here
							self.default_curie_uri = uri
						elif prefix == "_" :
							state.options.add_warning(err_bnode_local_prefix, IncorrectPrefixDefinition, node=state.node.nodeName)
						else :
							# last check: is the prefix an NCNAME?
							if ncname.match(prefix) :
								real_prefix = prefix.lower()
								dict[real_prefix] = uri
								self.graph.bind(real_prefix,uri)
								# Additional warning: is this prefix overriding an existing xmlns statement with a different URI? if
								# so, that may lead to discrepancies between an RDFa 1.0 and RDFa 1.1 run...
								if (prefix in xmlns_dict and xmlns_dict[prefix] != uri) or (real_prefix in xmlns_dict and xmlns_dict[real_prefix] != uri) :
									state.options.add_warning(err_prefix_and_xmlns % (real_prefix,real_prefix), node=state.node.nodeName)
								check_prefix(real_prefix)

							else :
								state.options.add_warning(err_non_ncname_prefix % (prefix,pr), IncorrectPrefixDefinition, node=state.node.nodeName)

		# See if anything has been collected at all.
		# If not, the namespaces of the incoming state is
		# taken over by reference. Otherwise that is copied to the
		# the local dictionary
		if inherited_state == None :
			self.default_prefixes = default_vocab.ns
			inherited_prefixes    = {}
		else :
			self.default_prefixes = inherited_state.term_or_curie.default_prefixes
			inherited_prefixes    = inherited_state.term_or_curie.ns

		if len(dict) == 0 :
			self.ns = inherited_prefixes
		else :
			self.ns = {}
			for key in inherited_prefixes : self.ns[key] = inherited_prefixes[key]
			for key in dict : 
				if (key in inherited_prefixes and dict[key] != inherited_prefixes[key]) or (key in self.default_prefixes and dict[key] != self.default_prefixes[key][0]) :
					state.options.add_warning(err_prefix_redefinition % key, PrefixRedefinitionWarning, node=state.node.nodeName)
				self.ns[key] = dict[key]

		
		# the xmlns prefixes have to be stored separately, again for XML Literal generation	
		self.xmlns = {}
		if len(xmlns_dict) == 0 and inherited_state :
			self.xmlns = inherited_state.term_or_curie.xmlns
		else :
			if inherited_state :
				for key in inherited_state.term_or_curie.xmlns	: self.xmlns[key] = inherited_state.term_or_curie.xmlns[key]
				for key in xmlns_dict							: self.xmlns[key] = xmlns_dict[key]
			else :
				self.xmlns = xmlns_dict
	# end __init__

	def _check_reference(self, val) :
		"""Checking the CURIE reference for correctness. It is probably not 100% foolproof, but may take care
		of some of the possible errors. See the URI RFC for the details.
		"""
		def char_check(s, not_allowed = ['#','[',']']) :
			for c in not_allowed :
				if s.find(c) != -1 : return False
			return True
		# Creating an artificial http URI to fool the urlparse module...
		scheme, netloc, url, query, fragment = urlsplit('http:' + val)
		if netloc != "" and self.state.rdfa_version >= "1.1" :
			self.state.options.add_warning(err_absolute_reference % (netloc, val), UnresolvableReference, node=self.state.node.nodeName)
			return False
		elif not char_check(query) :
			self.state.options.add_warning(err_query_reference % (query, val), UnresolvableReference, node=self.state.node.nodeName)
			return False
		elif not char_check(fragment) :
			self.state.options.add_warning(err_fragment_reference % (fragment, val), UnresolvableReference, node=self.state.node.nodeName)
			return False
		else :
			return True

	def CURIE_to_URI(self, val) :
		"""CURIE to URI mapping. 
		
		This method does I{not} take care of the last step of CURIE processing, ie, the fact that if
		it does not have a CURIE then the value is used a URI. This is done on the caller's side, because this has
		to be combined with base, for example. The method I{does} take care of BNode processing, though, ie,
		CURIE-s of the form "_:XXX".
		
		@param val: the full CURIE
		@type val: string
		@return: URIRef of a URI or None.
		"""
		# Just to be on the safe side:
		if val == "" :
			return None
		elif val == ":" :
			if self.default_curie_uri :
				return URIRef(self.default_curie_uri)
			else :
				return None
		
		# See if this is indeed a valid CURIE, ie, it can be split by a colon
		curie_split = val.split(':',1)
		if len(curie_split) == 1 :
			# there is no ':' character in the string, ie, it is not a valid CURIE
			return None
		else :
			if self.state.rdfa_version >= "1.1" :
				prefix	= curie_split[0].lower()
			else :
				prefix	= curie_split[0]
			reference = curie_split[1]

			#if len(reference) > 0 :
			#	if self.state.rdfa_version >= "1.1" and (len(prefix) == 0 or prefix in self.ns) and reference.startswith('//') :
			#		# This has been defined as illegal in RDFa 1.1
			#		self.state.options.add_warning(err_absolute_reference % (reference, val), UnresolvableReference, node=self.state.node.nodeName)
			#		return None
			#	if reference[0] == ":" :
			#		return None
			
			# first possibility: empty prefix
			if len(prefix) == 0 :
				if self.default_curie_uri and self._check_reference(reference) :
					return self.default_curie_uri[reference]
				else :
					return None
			else :
				# prefix is non-empty; can be a bnode
				if prefix == "_" :
					# yep, BNode processing. There is a difference whether the reference is empty or not...
					if len(reference) == 0 :
						return _empty_bnode
					else :
						# see if this variable has been used before for a BNode
						if reference in _bnodes :
							return _bnodes[reference]
						else :
							# a new bnode...
							retval = BNode()
							_bnodes[reference] = retval
							return retval
				# check if the prefix is a valid NCNAME
				elif ncname.match(prefix) :
					# see if there is a binding for this:					
					if prefix in self.ns and self._check_reference(reference) :
						# yep, a binding has been defined!
						if len(reference) == 0 :
							return URIRef(str(self.ns[prefix]))
						else :
							return self.ns[prefix][reference]
					elif prefix in self.default_prefixes and self._check_reference(reference) :
						# this has been defined through the default context
						if len(reference) == 0 :
							return URIRef(str(self.default_prefixes[prefix][0]))
						else :
							(ns,used) = self.default_prefixes[prefix]
							# lazy binding of prefixes (to avoid unnecessary prefix definitions in the serializations at the end...)
							if not used :
								self.graph.bind(prefix,ns)
								self.default_prefixes[prefix] = (ns,True)
							return ns[reference]
					else :
						# no definition for this thing...
						return None
				else :
					return None
	# end CURIE_to_URI

	def term_to_URI(self, term) :
		"""A term to URI mapping, where term is a simple string and the corresponding
		URI is defined via the @vocab (ie, default term uri) mechanism. Returns None if term is not defined
		@param term: string
		@return: an RDFLib URIRef instance (or None)
		"""
		if len(term) == 0 : return None

		if termname.match(term) :
			# It is a valid NCNAME
			
			# First of all, a @vocab nukes everything. That has to be done first...
			if self.default_term_uri != None :
				return URIRef(self.default_term_uri + term)

			# For default terms, the algorithm is (see 7.4.3 of the document): first make a case sensitive match;
			# if that fails than make a case insensive one			
			# 1. simple, case sensitive test:
			if term in self.terms :
				# yep, term is a valid key as is
				# lazy binding of the xhv prefix for terms...
				self.graph.bind(XHTML_PREFIX, XHTML_URI)
				return self.terms[term]
				
			# 2. case insensitive test
			for defined_term in self.terms :
				if term.lower() == defined_term.lower() :
					# lazy binding of the xhv prefix for terms...
					self.graph.bind(XHTML_PREFIX, XHTML_URI)
					return self.terms[defined_term]

		# If it got here, it is all wrong...
		return None

########NEW FILE########
__FILENAME__ = DublinCore
# -*- coding: utf-8 -*-
"""
Transfomer: handles the Dublin Core recommendation for XHTML for adding DC values. What this means is that:

 - DC namespaces are defined via C{<link rel="schema.XX" value="...."/>}
 - The 'XX.term' is used much like QNames in C{<link>} and C{<meta>} elements. For the latter, the namespaced names are added to a C{@property} attribute.

This transformer adds "real" namespaces and changes the DC references in link and meta elements to abide to the
RDFa namespace syntax.

@summary: Dublin Core transformer
@requires: U{RDFLib package<http://rdflib.net>}
@organization: U{World Wide Web Consortium<http://www.w3.org>}
@author: U{Ivan Herman<a href="http://www.w3.org/People/Ivan/">}
@license: This software is available for use under the
U{W3C SOFTWARE NOTICE AND LICENSE<href="http://www.w3.org/Consortium/Legal/2002/copyright-software-20021231">}
@contact: Ivan Herman, ivan@w3.org
"""

"""
@version: $Id: DublinCore.py,v 1.4 2012-01-18 14:16:44 ivan Exp $
$Date: 2012-01-18 14:16:44 $
"""

def DC_transform(html, options, state) :
	"""
	@param html: a DOM node for the top level html element
	@param options: invocation options
	@type options: L{Options<pyRdfa.options>}
	@param state: top level execution state
	@type state: L{State<pyRdfa.state>}
	"""
	from ..host import HostLanguage
	if not( options.host_language in [ HostLanguage.xhtml, HostLanguage.html5, HostLanguage.xhtml5 ] ) :
		return
	
	# the head element is necessary; to be sure, the namespaces are set
	# on that level only
	head = None
	try :
		head = html.getElementsByTagName("head")[0]
	except :
		# no head....
		return

	# At first, the DC namespaces must be found
	dcprefixes = {}
	for link in html.getElementsByTagName("link") :
		if link.hasAttribute("rel") :
			rel = link.getAttribute("rel")
			uri = link.getAttribute("href")
			if uri != None and rel != None and rel.startswith("schema.") :
				# bingo...
				try :
					localname = rel.split(".")[1]
					head.setAttributeNS("", "xmlns:"+localname,uri)
					dcprefixes[localname] = uri
				except :
					# problem with the split; just ignore
					pass

	# get the link elements now to find the dc elements
	for link in html.getElementsByTagName("link") :
		if link.hasAttribute("rel") :
			newProp = ""
			for rel in link.getAttribute("rel").strip().split() :
				# see if there is '.' to separate the attributes
				if rel.find(".") != -1 :
					key   = rel.split(".",1)[0]
					lname = rel.split(".",1)[1]
					if key in dcprefixes and lname != "" :
						# yep, this is one of those...
						newProp += " " + key + ":" + lname
					else :
						newProp += " " + rel
				else :
					newProp += " " + rel
			link.setAttribute("rel",newProp.strip())

	# do almost the same with the meta elements...
	for meta in html.getElementsByTagName("meta") :
		if meta.hasAttribute("name") :
			newProp = ""
			for name in meta.getAttribute("name").strip().split() :
				# see if there is '.' to separate the attributes
				if name.find(".") != -1 :
					key   = name.split(".",1)[0]
					lname = name.split(".",1)[1]
					if key in dcprefixes and lname != "" :
						# yep, this is one of those...
						newProp += " " + key + ":" + lname
					else :
						newProp += " " + name
				else :
					newProp += " " + name
			meta.setAttribute("property", newProp.strip())


########NEW FILE########
__FILENAME__ = lite
# -*- coding: utf-8 -*-
"""

@author: U{Ivan Herman<a href="http://www.w3.org/People/Ivan/">}
@license: This software is available for use under the
U{W3C SOFTWARE NOTICE AND LICENSE<href="http://www.w3.org/Consortium/Legal/2002/copyright-software-20021231">}
@contact: Ivan Herman, ivan@w3.org
@version: $Id: lite.py,v 1.11 2013-09-26 16:37:54 ivan Exp $
$Date: 2013-09-26 16:37:54 $
"""

from ..host        import HostLanguage

non_lite_attributes      = ["about","inlist","datatype","rev","rel","content"]
non_lite_attributes_html = ["about","inlist","datatype","rev"]

def lite_prune(top, options, state) :
	"""
	This is a misnomer. The current version does not remove anything from the tree, just generates warnings as for the
	usage of non-lite attributes. A more aggressive version would mean to remove those attributes, but that would,
	in fact, define an RDFa Lite conformance level in the parser, which is against the WG decisions. So this should
	not be done; the corresponding commands are commented in the code below...
	
	@param top: a DOM node for the top level element
	@param options: invocation options
	@type options: L{Options<pyRdfa.options>}
	@param state: top level execution state
	@type state: L{State<pyRdfa.state>}
	"""
	def generate_warning(node, attr) :
		if attr == "rel" :
			msg = "Attribute @rel should not be used in RDFa Lite (consider using @property)"
		elif attr == "about" :
			msg = "Attribute @about should not be used in RDFa Lite (consider using a <link> element with @href or @resource)"
		else :
			msg = "Attribute @%s should not be used in RDFa Lite" % attr
		options.add_warning(msg, node=node)

	def remove_attrs(node) :
		from ..termorcurie import termname
		# first the @content; this has a special treatment
		# there are some extras to check for HTML dialects
		if options.host_language in [ HostLanguage.html5, HostLanguage.xhtml5, HostLanguage.xhtml ] :
			if node.tagName != "meta" and node.hasAttribute("content") :
				generate_warning(node, "content")
				# node.removeAttribute("content")
			if node.tagName != "link" and node.hasAttribute("rel") :
				# Unfortunately, this has to be checked separately and run-time for <link> where @rel is allowed for non-RDFa purposes...
				# Additional complication: @rel is allowed in an <a> element, for example, if used as a pure term and not as a URI or CURIE
				if node.tagName == "a" :
					vals = node.getAttribute("rel").strip().split()
					if len(vals) != 0 :
						final_vals = [ v for v in vals if not termname.match(v) ]
						if len(final_vals) != 0 :
							generate_warning(node, "rel")
				else :
					generate_warning(node, "rel")
			for attr in non_lite_attributes_html :
				if node.hasAttribute(attr) :
					generate_warning(node, attr)
					# node.removeAttribute(attr)
		else :
			for attr in non_lite_attributes :
				if node.hasAttribute(attr) :
					generate_warning(node, attr)
					# node.removeAttribute(attr)

	remove_attrs(top)
	for n in top.childNodes :
		if n.nodeType == top.ELEMENT_NODE :
			lite_prune(n, options, state)

	
########NEW FILE########
__FILENAME__ = metaname
# -*- coding: utf-8 -*-
"""
Simple transfomer: C{meta} element is extended with a C{property} attribute, with a copy of the
C{name} attribute values.

@author: U{Ivan Herman<a href="http://www.w3.org/People/Ivan/">}
@license: This software is available for use under the
U{W3C SOFTWARE NOTICE AND LICENSE<href="http://www.w3.org/Consortium/Legal/2002/copyright-software-20021231">}
@contact: Ivan Herman, ivan@w3.org
@version: $Id: metaname.py,v 1.3 2012-01-18 14:16:45 ivan Exp $
$Date: 2012-01-18 14:16:45 $
"""

def meta_transform(html, options, state) :
	"""
	@param html: a DOM node for the top level html element
	@param options: invocation options
	@type options: L{Options<pyRdfa.options>}
	@param state: top level execution state
	@type state: L{State<pyRdfa.state>}
	"""
	from ..host import HostLanguage
	if not( options.host_language in [ HostLanguage.xhtml, HostLanguage.html5, HostLanguage.xhtml5 ] ) :
		return

	for meta in html.getElementsByTagName("meta") :
		if meta.hasAttribute("name") and not meta.hasAttribute("property") :
			meta.setAttribute("property", meta.getAttribute("name"))


########NEW FILE########
__FILENAME__ = OpenID
# -*- coding: utf-8 -*-
"""
Simple transfomer: handle OpenID elements. Ie: an openid namespace is added and the usual
'link' elements for openid are exchanged against a namespaced version.

@summary: OpenID transformer module.
@requires: U{RDFLib package<http://rdflib.net>}
@organization: U{World Wide Web Consortium<http://www.w3.org>}
@author: U{Ivan Herman<a href="http://www.w3.org/People/Ivan/">}
@license: This software is available for use under the
U{W3C SOFTWARE NOTICE AND LICENSE<href="http://www.w3.org/Consortium/Legal/2002/copyright-software-20021231">}
@contact: Ivan Herman, ivan@w3.org
@var OPENID_NS: the OpenID URI used in the package
"""

"""
$Id: OpenID.py,v 1.4 2012-01-18 14:16:44 ivan Exp $
$Date: 2012-01-18 14:16:44 $
"""

OPENID_NS = "http://xmlns.openid.net/auth#"


def OpenID_transform(html, options, state) :
	"""
	Replace C{openid.XXX} type C{@rel} attribute values in C{<link>} elements by C{openid:XXX}. The openid URI is also
	added to the top level namespaces with the C{openid:} local name.

	@param html: a DOM node for the top level html element
	@param options: invocation options
	@type options: L{Options<pyRdfa.options>}
	@param state: top level execution state
	@type state: L{State<pyRdfa.state>}
	"""
	from ..host import HostLanguage
	if not( options.host_language in [ HostLanguage.xhtml, HostLanguage.html5, HostLanguage.xhtml5 ] ) :
		return

	# the head element is necessary; to be sure, the namespaces are set
	# on that level only
	head = None
	try :
		head = html.getElementsByTagName("head")[0]
	except :
		# no head....
		return

	foundOpenId = False
	for link in html.getElementsByTagName("link") :
		if link.hasAttribute("rel") :
			rel = link.getAttribute("rel")
			newProp = ""
			for n in rel.strip().split() :
				if n.startswith("openid.") :
					newProp += " " + n.replace("openid.","openid:")
					foundOpenId = True
				else :
					newProp += " " + n
			link.setAttribute("rel",newProp.strip())

	# Add the OpenId namespace if necessary
	if foundOpenId and not head.hasAttribute("xmlns:openid") :
		head.setAttributeNS("", "xmlns:openid", OPENID_NS)


########NEW FILE########
__FILENAME__ = prototype
# -*- coding: utf-8 -*-
"""
Encoding of the RDFa prototype vocabulary behavior. This means processing the graph by adding and removing triples
based on triples using the rdfa:Prototype and rdfa:ref class and property, respectively. For details, see the HTML5+RDFa document.


@author: U{Ivan Herman<a href="http://www.w3.org/People/Ivan/">}
@license: This software is available for use under the
U{W3C SOFTWARE NOTICE AND LICENSE<href="http://www.w3.org/Consortium/Legal/2002/copyright-software-20021231">}
@contact: Ivan Herman, ivan@w3.org
@version: $Id: prototype.py,v 1.1 2013-01-18 09:41:49 ivan Exp $
$Date: 2013-01-18 09:41:49 $
"""
import rdflib
from rdflib	import Namespace
if rdflib.__version__ >= "3.0.0" :
	from rdflib	import RDF  as ns_rdf
else :
	from rdflib.RDF	import RDFNS  as ns_rdf

from .. import ns_rdfa

Prototype = ns_rdfa["Pattern"]
pref      = ns_rdfa["copy"]

def handle_prototypes(graph) :
	to_remove = set()
	for (x,ref,PR) in graph.triples((None,pref,None)) :
		if (PR,ns_rdf["type"],Prototype) in graph :
			to_remove.add((PR,ns_rdf["type"],Prototype))
			to_remove.add((x,ref,PR))
			# there is a reference to a prototype here
			for (PR,p,y) in graph.triples((PR,None,None)) :
				if not ( p == ns_rdf["type"] and y == Prototype ) :
					graph.add((x,p,y))
					to_remove.add((PR,p,y))
	for t in to_remove : graph.remove(t)
########NEW FILE########
__FILENAME__ = utils
# -*- coding: utf-8 -*-
"""
Various utilities for pyRdfa.

Most of the utilities are straightforward.

@organization: U{World Wide Web Consortium<http://www.w3.org>}
@author: U{Ivan Herman<a href="http://www.w3.org/People/Ivan/">}
@license: This software is available for use under the
U{W3C SOFTWARE NOTICE AND LICENSE<href="http://www.w3.org/Consortium/Legal/2002/copyright-software-20021231">}


"""

"""
$Id: utils.py,v 1.9 2012/11/16 17:51:53 ivan Exp $
$Date: 2012/11/16 17:51:53 $
"""
import os, os.path, sys, imp, datetime

# Python 3 vs. 2 switch
if sys.version_info[0] >= 3 :
	from urllib.request import Request, urlopen
	from urllib.parse   import urljoin, quote
	from http.server    import BaseHTTPRequestHandler
	from urllib.error   import HTTPError as urllib_HTTPError
else :
	from urllib2        import Request, urlopen
	from urllib2        import HTTPError as urllib_HTTPError
	from urlparse       import urljoin
	from urllib         import quote
	from BaseHTTPServer import BaseHTTPRequestHandler

from .extras.httpheader import content_type, parse_http_datetime

import rdflib
if rdflib.__version__ >= "3.0.0" :
	from rdflib	import RDF as ns_rdf
else :
	from rdflib.RDF	import RDFNS  as ns_rdf

from .host import HostLanguage, preferred_suffixes

#########################################################################################################
# Handling URIs
class URIOpener :
	"""A wrapper around the urllib2 method to open a resource. Beyond accessing the data itself, the class
	sets a number of instance variable that might be relevant for processing.
	The class also adds an accept header to the outgoing request, namely
	text/html and application/xhtml+xml (unless set explicitly by the caller).
	
	If the content type is set by the server, the relevant HTTP response field is used. Otherwise,
	common suffixes are used (see L{host.preferred_suffixes}) to set the content type (this is really of importance
	for C{file:///} URI-s). If none of these works, the content type is empty.
		
	Interpretation of the content type for the return is done by Deron Meranda's U{httpheader module<http://deron.meranda.us/>}.
	
	@ivar data: the real data, ie, a file-like object
	@ivar headers: the return headers as sent back by the server
	@ivar content_type: the content type of the resource or the empty string, if the content type cannot be determined
	@ivar location: the real location of the data (ie, after possible redirection and content negotiation)
	@ivar last_modified_date: sets the last modified date if set in the header, None otherwise
	@ivar expiration_date: sets the expiration date if set in the header, I{current UTC plus one day} otherwise (this is used for caching purposes, hence this artificial setting)
	"""
	CONTENT_LOCATION	= 'Content-Location'
	CONTENT_TYPE		= 'Content-Type'
	LAST_MODIFIED		= 'Last-Modified'
	EXPIRES				= 'Expires'
	def __init__(self, name, additional_headers = {}) :
		"""
		@param name: URL to be opened
		@keyword additional_headers: additional HTTP request headers to be added to the call
		"""		
		try :
			# Note the removal of the fragment ID. This is necessary, per the HTTP spec
			req = Request(url=name.split('#')[0])

			for key in additional_headers :
				req.add_header(key, additional_headers[key])
			if 'Accept' not in additional_headers :
				req.add_header('Accept', 'text/html, application/xhtml+xml')
				
			self.data		= urlopen(req)
			self.headers	= self.data.info()
			
			if URIOpener.CONTENT_TYPE in self.headers :
				# The call below will remove the possible media type parameters, like charset settings
				ct = content_type(self.headers[URIOpener.CONTENT_TYPE])
				self.content_type = ct.media_type
				if 'charset' in ct.parmdict :
					self.charset = ct.parmdict['charset']
				else :
					self.charset = None
				# print
			else :
				# check if the suffix can be used for the content type; this may be important
				# for file:// type URI or if the server is not properly set up to return the right
				# mime type
				self.charset = None
				self.content_type = ""
				for suffix in preferred_suffixes.keys() :
					if name.endswith(suffix) :
						self.content_type = preferred_suffixes[suffix]
						break
			
			if URIOpener.CONTENT_LOCATION in self.headers :
				self.location = urljoin(self.data.geturl(),self.headers[URIOpener.CONTENT_LOCATION])
			else :
				self.location = name
			
			self.expiration_date = datetime.datetime.utcnow() + datetime.timedelta(days=1)
			if URIOpener.EXPIRES in self.headers :
				try :
					# Thanks to Deron Meranda for the HTTP date conversion method...
					self.expiration_date = parse_http_datetime(self.headers[URIOpener.EXPIRES])
				except :
					# The Expires date format was wrong, sorry, forget it...
					pass

			self.last_modified_date = None
			if URIOpener.LAST_MODIFIED in self.headers :
				try :
					# Thanks to Deron Meranda for the HTTP date conversion method...
					self.last_modified_date = parse_http_datetime(self.headers[URIOpener.LAST_MODIFIED])
				except :
					# The last modified date format was wrong, sorry, forget it...
					pass
				
		except urllib_HTTPError :
			e = sys.exc_info()[1]
			from . import HTTPError
			msg = BaseHTTPRequestHandler.responses[e.code]
			raise HTTPError('%s' % msg[1], e.code)
		except Exception :
			e = sys.exc_info()[1]
			from . import RDFaError
			raise RDFaError('%s' % e)

#########################################################################################################

# 'safe' characters for the URI quoting, ie, characters that can safely stay as they are. Other 
# special characters are converted to their %.. equivalents for namespace prefixes
_unquotedChars = ':/\?=#~'
_warnChars     = [' ','\n','\r','\t']

def quote_URI(uri, options = None) :
	"""
	'quote' a URI, ie, exchange special characters for their '%..' equivalents. Some of the characters
	may stay as they are (listed in L{_unquotedChars}. If one of the characters listed in L{_warnChars} 
	is also in the uri, an extra warning is also generated.
	@param uri: URI
	@param options: 
	@type options: L{Options<pyRdfa.Options>}
	"""
	from . import err_unusual_char_in_URI
	suri = uri.strip()
	for c in _warnChars :
		if suri.find(c) != -1 :
			if options != None :
				options.add_warning(err_unusual_char_in_URI % suri)
			break
	return quote(suri, _unquotedChars)
	
#########################################################################################################
	
def create_file_name(uri) :
	"""
	Create a suitable file name from an (absolute) URI. Used, eg, for the generation of a file name for a cached vocabulary file.
	"""
	suri = uri.strip()
	final_uri = quote(suri,_unquotedChars)
	# Remove some potentially dangereous characters
	return final_uri.replace(' ','_').replace('%','_').replace('-','_').replace('+','_').replace('/','_').replace('?','_').replace(':','_').replace('=','_').replace('#','_')

#########################################################################################################
def has_one_of_attributes(node,*args) :
	"""
	Check whether one of the listed attributes is present on a (DOM) node.
	@param node: DOM element node
	@param args: possible attribute names
	@return: True or False
	@rtype: Boolean
	"""
	if len(args) == 0 :
		return None
	if isinstance(args[0], tuple) or isinstance(args[0], list) :
		rargs = args[0]
	else :
		rargs = args
	
	return True in [ node.hasAttribute(attr) for attr in rargs ]

#########################################################################################################
def traverse_tree(node, func) :
	"""Traverse the whole element tree, and perform the function C{func} on all the elements.
	@param node: DOM element node
	@param func: function to be called on the node. Input parameter is a DOM Element Node. If the function returns a boolean True, the recursion is stopped.
	"""
	if func(node) :
		return

	for n in node.childNodes :
		if n.nodeType == node.ELEMENT_NODE :
			traverse_tree(n, func)

#########################################################################################################
def return_XML(state, inode, base = True, xmlns = True) :
	"""
	Get (recursively) the XML Literal content of a DOM Element Node. (Most of the processing is done
	via a C{node.toxml} call of the xml minidom implementation.)

	@param inode: DOM Node
	@param state: L{pyRdfa.state.ExecutionContext}
	@param base: whether the base element should be added to the output
	@type base: Boolean
	@param xmlns: whether the namespace declarations should be repeated in the generated node
	@type xmlns: Boolean
	@return: string
	"""
	node = inode.cloneNode(True)
	# Decorate the element with namespaces.lang values and, optionally, base
	if base :
		node.setAttribute("xml:base",state.base)
	if xmlns :
		for prefix in state.term_or_curie.xmlns :
			if not node.hasAttribute("xmlns:%s" % prefix) :
				node.setAttribute("xmlns:%s" % prefix,"%s" % state.term_or_curie.xmlns[prefix])
		# Set the default namespace, if not done (and is available)
		if not node.getAttribute("xmlns") and state.defaultNS != None :
			node.setAttribute("xmlns", state.defaultNS)
	# Get the lang, if necessary
	if state.lang :
		if state.options.host_language in [ HostLanguage.xhtml, HostLanguage.xhtml5, HostLanguage.html5 ] :
			if not node.getAttribute("lang") :
				node.setAttribute("lang", state.lang)
		else :
			if not node.getAttribute("xml:lang") :
				node.setAttribute("xml:lang", state.lang)
	
	if sys.version_info[0] >= 3 :
		return node.toxml()
	else :
		q = node.toxml(encoding='utf-8')
		return unicode(q, encoding='utf-8')

#########################################################################################################

def dump(node) :
	"""
	This is just for debug purposes: it prints the essential content of the node in the tree starting at node.

	@param node: DOM node
	"""
	print( node.toprettyxml(indent="", newl="") )
	

	

########NEW FILE########
__FILENAME__ = rdfxml
"""
An RDF/XML parser for RDFLib
"""

from xml.sax import make_parser
from xml.sax.handler import ErrorHandler
from xml.sax.saxutils import handler, quoteattr, escape
from urlparse import urljoin, urldefrag

from rdflib.namespace import RDF, is_ncname
from rdflib.term import URIRef
from rdflib.term import BNode
from rdflib.term import Literal
from rdflib.exceptions import ParserError, Error
from rdflib.parser import Parser

__all__ = ['create_parser', 'BagID', 'ElementHandler',
           'RDFXMLHandler', 'RDFXMLParser']

RDFNS = RDF

# http://www.w3.org/TR/rdf-syntax-grammar/#eventterm-attribute-URI
# A mapping from unqualified terms to there qualified version.
UNQUALIFIED = {"about": RDF.about,
               "ID": RDF.ID,
               "type": RDF.type,
               "resource": RDF.resource,
               "parseType": RDF.parseType}

# http://www.w3.org/TR/rdf-syntax-grammar/#coreSyntaxTerms
CORE_SYNTAX_TERMS = [RDF.RDF, RDF.ID, RDF.about, RDF.parseType,
                     RDF.resource, RDF.nodeID, RDF.datatype]

# http://www.w3.org/TR/rdf-syntax-grammar/#syntaxTerms
SYNTAX_TERMS = CORE_SYNTAX_TERMS + [RDF.Description, RDF.li]

# http://www.w3.org/TR/rdf-syntax-grammar/#oldTerms
OLD_TERMS = [
    URIRef("http://www.w3.org/1999/02/22-rdf-syntax-ns#aboutEach"),
    URIRef("http://www.w3.org/1999/02/22-rdf-syntax-ns#aboutEachPrefix"),
    URIRef("http://www.w3.org/1999/02/22-rdf-syntax-ns#bagID")]

NODE_ELEMENT_EXCEPTIONS = CORE_SYNTAX_TERMS + [RDF.li, ] + OLD_TERMS
NODE_ELEMENT_ATTRIBUTES = [RDF.ID, RDF.nodeID, RDF.about]

PROPERTY_ELEMENT_EXCEPTIONS = \
    CORE_SYNTAX_TERMS + [RDF.Description, ] + OLD_TERMS
PROPERTY_ATTRIBUTE_EXCEPTIONS = \
    CORE_SYNTAX_TERMS + [RDF.Description, RDF.li] + OLD_TERMS
PROPERTY_ELEMENT_ATTRIBUTES = [RDF.ID, RDF.resource, RDF.nodeID]

XMLNS = "http://www.w3.org/XML/1998/namespace"
BASE = (XMLNS, "base")
LANG = (XMLNS, "lang")


class BagID(URIRef):
    __slots__ = ['li']

    def __init__(self, val):
        super(URIRef, self).__init__(val)
        self.li = 0

    def next_li(self):
        self.li += 1
        return RDFNS[self.li]


class ElementHandler(object):
    __slots__ = ['start', 'char', 'end', 'li', 'id',
                 'base', 'subject', 'predicate', 'object',
                 'list', 'language', 'datatype', 'declared', 'data']

    def __init__(self):
        self.start = None
        self.char = None
        self.end = None
        self.li = 0
        self.id = None
        self.base = None
        self.subject = None
        self.object = None
        self.list = None
        self.language = None
        self.datatype = None
        self.declared = None
        self.data = None

    def next_li(self):
        self.li += 1
        return RDFNS[self.li]


class RDFXMLHandler(handler.ContentHandler):

    def __init__(self, store):
        self.store = store
        self.preserve_bnode_ids = False
        self.reset()

    def reset(self):
        document_element = ElementHandler()
        document_element.start = self.document_element_start
        document_element.end = lambda name, qname: None
        self.stack = [None, document_element, ]
        self.ids = {}  # remember IDs we have already seen
        self.bnode = {}
        self._ns_contexts = [{}]  # contains uri -> prefix dicts
        self._current_context = self._ns_contexts[-1]

    # ContentHandler methods

    def setDocumentLocator(self, locator):
        self.locator = locator

    def startDocument(self):
        pass

    def startPrefixMapping(self, prefix, namespace):
        self._ns_contexts.append(self._current_context.copy())
        self._current_context[namespace] = prefix
        self.store.bind(prefix, namespace or "", override=False)

    def endPrefixMapping(self, prefix):
        self._current_context = self._ns_contexts[-1]
        del self._ns_contexts[-1]

    def startElementNS(self, name, qname, attrs):
        stack = self.stack
        stack.append(ElementHandler())
        current = self.current
        parent = self.parent
        base = attrs.get(BASE, None)
        if base is not None:
            base, frag = urldefrag(base)
            if parent and parent.base:
                base = urljoin(parent.base, base)
            else:
                systemId = self.locator.getPublicId() \
                    or self.locator.getSystemId()
                if systemId:
                    base = urljoin(systemId, base)
        else:
            if parent:
                base = parent.base
            if base is None:
                systemId = self.locator.getPublicId() \
                    or self.locator.getSystemId()
                if systemId:
                    base, frag = urldefrag(systemId)
        current.base = base
        language = attrs.get(LANG, None)
        if language is None:
            if parent:
                language = parent.language
        current.language = language
        current.start(name, qname, attrs)

    def endElementNS(self, name, qname):
        self.current.end(name, qname)
        self.stack.pop()

    def characters(self, content):
        char = self.current.char
        if char:
            char(content)

    def ignorableWhitespace(self, content):
        pass

    def processingInstruction(self, target, data):
        pass

    def add_reified(self, sid, (s, p, o)):
        self.store.add((sid, RDF.type, RDF.Statement))
        self.store.add((sid, RDF.subject, s))
        self.store.add((sid, RDF.predicate, p))
        self.store.add((sid, RDF.object, o))

    def error(self, message):
        locator = self.locator
        info = "%s:%s:%s: " % (locator.getSystemId(),
                               locator.getLineNumber(),
                               locator.getColumnNumber())
        raise ParserError(info + message)

    def get_current(self):
        return self.stack[-2]
    # Create a read only property called current so that self.current
    # give the current element handler.
    current = property(get_current)

    def get_next(self):
        return self.stack[-1]
    # Create a read only property that gives the element handler to be
    # used for the next element.
    next = property(get_next)

    def get_parent(self):
        return self.stack[-3]
    # Create a read only property that gives the current parent
    # element handler
    parent = property(get_parent)

    def absolutize(self, uri):
        result = urljoin(self.current.base, uri, allow_fragments=1)
        if uri and uri[-1] == "#" and result[-1] != "#":
            result = "%s#" % result
        return URIRef(result)

    def convert(self, name, qname, attrs):
        if name[0] is None:
            name = URIRef(name[1])
        else:
            name = URIRef("".join(name))
        atts = {}
        for (n, v) in attrs.items():  # attrs._attrs.iteritems(): #
            if n[0] is None:
                att = URIRef(n[1])
            else:
                att = URIRef("".join(n))
            if att.startswith(XMLNS) or att[0:3].lower() == "xml":
                pass
            elif att in UNQUALIFIED:
                # if not RDFNS[att] in atts:
                atts[RDFNS[att]] = v
            else:
                atts[URIRef(att)] = v
        return name, atts

    def document_element_start(self, name, qname, attrs):
        if name[0] and URIRef("".join(name)) == RDF.RDF:
            # Cheap hack so 2to3 doesn't turn it into __next__
            next = getattr(self, 'next')
            next.start = self.node_element_start
            next.end = self.node_element_end
        else:
            self.node_element_start(name, qname, attrs)
            # self.current.end = self.node_element_end
            # TODO... set end to something that sets start such that
            # another element will cause error

    def node_element_start(self, name, qname, attrs):
        name, atts = self.convert(name, qname, attrs)
        current = self.current
        absolutize = self.absolutize

        # Cheap hack so 2to3 doesn't turn it into __next__
        next = getattr(self, 'next')
        next.start = self.property_element_start
        next.end = self.property_element_end

        if name in NODE_ELEMENT_EXCEPTIONS:
            self.error("Invalid node element URI: %s" % name)

        if RDF.ID in atts:
            if RDF.about in atts or RDF.nodeID in atts:
                self.error(
                    "Can have at most one of rdf:ID, rdf:about, and rdf:nodeID"
                )

            id = atts[RDF.ID]
            if not is_ncname(id):
                self.error("rdf:ID value is not a valid NCName: %s" % id)
            subject = absolutize("#%s" % id)
            if subject in self.ids:
                self.error(
                    "two elements cannot use the same ID: '%s'" % subject)
            self.ids[subject] = 1  # IDs can only appear once within a document
        elif RDF.nodeID in atts:
            if RDF.ID in atts or RDF.about in atts:
                self.error(
                    "Can have at most one of rdf:ID, rdf:about, and rdf:nodeID"
                )
            nodeID = atts[RDF.nodeID]
            if not is_ncname(nodeID):
                self.error(
                    "rdf:nodeID value is not a valid NCName: %s" % nodeID)
            if self.preserve_bnode_ids is False:
                if nodeID in self.bnode:
                    subject = self.bnode[nodeID]
                else:
                    subject = BNode()
                    self.bnode[nodeID] = subject
            else:
                subject = BNode(nodeID)
        elif RDF.about in atts:
            if RDF.ID in atts or RDF.nodeID in atts:
                self.error(
                    "Can have at most one of rdf:ID, rdf:about, and rdf:nodeID"
                )
            subject = absolutize(atts[RDF.about])
        else:
            subject = BNode()

        if name != RDF.Description:  # S1
            self.store.add((subject, RDF.type, absolutize(name)))

        language = current.language
        for att in atts:
            if not att.startswith(str(RDFNS)):
                predicate = absolutize(att)
                try:
                    object = Literal(atts[att], language)
                except Error, e:
                    self.error(e.msg)
            elif att == RDF.type:  # S2
                predicate = RDF.type
                object = absolutize(atts[RDF.type])
            elif att in NODE_ELEMENT_ATTRIBUTES:
                continue
            elif att in PROPERTY_ATTRIBUTE_EXCEPTIONS:  # S3
                self.error("Invalid property attribute URI: %s" % att)
                continue  # for when error does not throw an exception
            else:
                predicate = absolutize(att)
                try:
                    object = Literal(atts[att], language)
                except Error, e:
                    self.error(e.msg)
            self.store.add((subject, predicate, object))

        current.subject = subject

    def node_element_end(self, name, qname):
        # repeat node-elements are only allowed
        # at at top-level

        if self.parent.object and self.current != self.stack[2]:

            self.error("Repeat node-elements inside property elements: %s"%"".join(name))

        self.parent.object = self.current.subject

    def property_element_start(self, name, qname, attrs):
        name, atts = self.convert(name, qname, attrs)
        current = self.current
        absolutize = self.absolutize

        # Cheap hack so 2to3 doesn't turn it into __next__
        next = getattr(self, 'next')
        object = None
        current.data = None
        current.list = None

        if not name.startswith(str(RDFNS)):
            current.predicate = absolutize(name)
        elif name == RDF.li:
            current.predicate = current.next_li()
        elif name in PROPERTY_ELEMENT_EXCEPTIONS:
            self.error("Invalid property element URI: %s" % name)
        else:
            current.predicate = absolutize(name)

        id = atts.get(RDF.ID, None)
        if id is not None:
            if not is_ncname(id):
                self.error("rdf:ID value is not a value NCName: %s" % id)
            current.id = absolutize("#%s" % id)
        else:
            current.id = None

        resource = atts.get(RDF.resource, None)
        nodeID = atts.get(RDF.nodeID, None)
        parse_type = atts.get(RDF.parseType, None)
        if resource is not None and nodeID is not None:
            self.error(
                "Property element cannot have both rdf:nodeID and rdf:resource"
            )
        if resource is not None:
            object = absolutize(resource)
            next.start = self.node_element_start
            next.end = self.node_element_end
        elif nodeID is not None:
            if not is_ncname(nodeID):
                self.error(
                    "rdf:nodeID value is not a valid NCName: %s" % nodeID)
            if self.preserve_bnode_ids is False:
                if nodeID in self.bnode:
                    object = self.bnode[nodeID]
                else:
                    subject = BNode()
                    self.bnode[nodeID] = subject
                    object = subject
            else:
                object = subject = BNode(nodeID)
            next.start = self.node_element_start
            next.end = self.node_element_end
        else:
            if parse_type is not None:
                for att in atts:
                    if att != RDF.parseType and att != RDF.ID:
                        self.error("Property attr '%s' now allowed here" % att)
                if parse_type == "Resource":
                    current.subject = object = BNode()
                    current.char = self.property_element_char
                    next.start = self.property_element_start
                    next.end = self.property_element_end
                elif parse_type == "Collection":
                    current.char = None
                    object = current.list = RDF.nil  # BNode()
                                                     # self.parent.subject
                    next.start = self.node_element_start
                    next.end = self.list_node_element_end
                else:  # if parse_type=="Literal":
                     # All other values are treated as Literal
                     # See: http://www.w3.org/TR/rdf-syntax-grammar/
                                # parseTypeOtherPropertyElt
                    object = Literal("", datatype=RDF.XMLLiteral)
                    current.char = self.literal_element_char
                    current.declared = {XMLNS: 'xml'}
                    next.start = self.literal_element_start
                    next.char = self.literal_element_char
                    next.end = self.literal_element_end
                current.object = object
                return
            else:
                object = None
                current.char = self.property_element_char
                next.start = self.node_element_start
                next.end = self.node_element_end

        datatype = current.datatype = atts.get(RDF.datatype, None)
        language = current.language
        if datatype is not None:
            # TODO: check that there are no atts other than datatype and id
            datatype = absolutize(datatype)
        else:
            for att in atts:
                if not att.startswith(str(RDFNS)):
                    predicate = absolutize(att)
                elif att in PROPERTY_ELEMENT_ATTRIBUTES:
                    continue
                elif att in PROPERTY_ATTRIBUTE_EXCEPTIONS:
                    self.error("""Invalid property attribute URI: %s""" % att)
                else:
                    predicate = absolutize(att)

                if att == RDF.type:
                    o = URIRef(atts[att])
                else:
                    if datatype is not None:
                        language = None
                    o = Literal(atts[att], language, datatype)

                if object is None:
                    object = BNode()
                self.store.add((object, predicate, o))
        if object is None:
            current.data = ""
            current.object = None
        else:
            current.data = None
            current.object = object

    def property_element_char(self, data):
        current = self.current
        if current.data is not None:
            current.data += data

    def property_element_end(self, name, qname):
        current = self.current
        if current.data is not None and current.object is None:
            literalLang = current.language
            if current.datatype is not None:
                literalLang = None
            current.object = Literal(
                current.data, literalLang, current.datatype)
            current.data = None
        if self.next.end == self.list_node_element_end:
            if current.object != RDF.nil:
                self.store.add((current.list, RDF.rest, RDF.nil))
        if current.object is not None:
            self.store.add(
                (self.parent.subject, current.predicate, current.object))
            if current.id is not None:
                self.add_reified(current.id, (self.parent.subject,
                                 current.predicate, current.object))
        current.subject = None

    def list_node_element_end(self, name, qname):
        current = self.current
        if self.parent.list == RDF.nil:
            list = BNode()
            # Removed between 20030123 and 20030905
            # self.store.add((list, RDF.type, LIST))
            self.parent.list = list
            self.store.add((self.parent.list, RDF.first, current.subject))
            self.parent.object = list
            self.parent.char = None
        else:
            list = BNode()
            # Removed between 20030123 and 20030905
            # self.store.add((list, RDF.type, LIST))
            self.store.add((self.parent.list, RDF.rest, list))
            self.store.add((list, RDF.first, current.subject))
            self.parent.list = list

    def literal_element_start(self, name, qname, attrs):
        current = self.current
        self.next.start = self.literal_element_start
        self.next.char = self.literal_element_char
        self.next.end = self.literal_element_end
        current.declared = self.parent.declared.copy()
        if name[0]:
            prefix = self._current_context[name[0]]
            if prefix:
                current.object = "<%s:%s" % (prefix, name[1])
            else:
                current.object = "<%s" % name[1]
            if not name[0] in current.declared:
                current.declared[name[0]] = prefix
                if prefix:
                    current.object += (' xmlns:%s="%s"' % (prefix, name[0]))
                else:
                    current.object += (' xmlns="%s"' % name[0])
        else:
            current.object = "<%s" % name[1]

        for (name, value) in attrs.items():
            if name[0]:
                if not name[0] in current.declared:
                    current.declared[name[0]] = self._current_context[name[0]]
                name = current.declared[name[0]] + ":" + name[1]
            else:
                name = name[1]
            current.object += (' %s=%s' % (name, quoteattr(value)))
        current.object += ">"

    def literal_element_char(self, data):
        self.current.object += escape(data)

    def literal_element_end(self, name, qname):
        if name[0]:
            prefix = self._current_context[name[0]]
            if prefix:
                end = u"</%s:%s>" % (prefix, name[1])
            else:
                end = u"</%s>" % name[1]
        else:
            end = u"</%s>" % name[1]
        self.parent.object += self.current.object + end


def create_parser(target, store):
    parser = make_parser()
    try:
        # Workaround for bug in expatreader.py. Needed when
        # expatreader is trying to guess a prefix.
        parser.start_namespace_decl(
            "xml", "http://www.w3.org/XML/1998/namespace")
    except AttributeError:
        pass  # Not present in Jython (at least)
    parser.setFeature(handler.feature_namespaces, 1)
    rdfxml = RDFXMLHandler(store)
    rdfxml.setDocumentLocator(target)
    # rdfxml.setDocumentLocator(_Locator(self.url, self.parser))
    parser.setContentHandler(rdfxml)
    parser.setErrorHandler(ErrorHandler())
    return parser


class RDFXMLParser(Parser):

    def __init__(self):
        pass

    def parse(self, source, sink, **args):
        self._parser = create_parser(source, sink)
        content_handler = self._parser.getContentHandler()
        preserve_bnode_ids = args.get("preserve_bnode_ids", None)
        if preserve_bnode_ids is not None:
            content_handler.preserve_bnode_ids = preserve_bnode_ids
        # # We're only using it once now
        # content_handler.reset()
        # self._parser.reset()
        self._parser.parse(source)

########NEW FILE########
__FILENAME__ = structureddata
#!/usr/bin/env python
"""
Extraction parsers for structured data embedded into HTML or XML files.
The former may include RDFa or microdata. The syntax and the extraction
procedures are based on:

* The RDFa specifications: http://www.w3.org/TR/#tr_RDFa
* The microdata specification: http://www.w3.org/TR/microdata/
* The specification of the microdata to RDF conversion:
http://www.w3.org/TR/microdata-rdf/

License: W3C Software License,
http://www.w3.org/Consortium/Legal/copyright-software
Author: Ivan Herman
Copyright: W3C

"""

from rdflib.parser import (
    Parser, StringInputSource, URLInputSource, FileInputSource)

try:
    import html5lib
    assert html5lib
    html5lib = True
except ImportError:
    import warnings
    warnings.warn(
        'html5lib not found! RDFa and Microdata ' +
        'parsers will not be available.')
    html5lib = False


def _get_orig_source(source):
    """
    A bit of a hack; the RDFa/microdata parsers need more than what the
    upper layers of RDFLib provide...
    This method returns the original source references.
    """
    if isinstance(source, StringInputSource):
        orig_source = source.getByteStream()
    elif isinstance(source, URLInputSource):
        orig_source = source.url
    elif isinstance(source, FileInputSource):
        orig_source = source.file.name
        source.file.close()
    else:
        orig_source = source.getByteStream()
    baseURI = source.getPublicId()
    return (baseURI, orig_source)


def _check_error(graph):
    from .pyRdfa import RDFA_Error, ns_rdf
    from .pyRdfa.options import ns_dc
    for (s, p, o) in graph.triples((None, ns_rdf["type"], RDFA_Error)):
        for (x, y, msg) in graph.triples((s, ns_dc["description"], None)):
            raise Exception("RDFa parsing Error! %s" % msg)


# This is the parser interface as it would look when called from the
# rest of RDFLib
class RDFaParser(Parser):
    """
    Wrapper around the RDFa 1.1 parser. For further details on the RDFa 1.1
    processing, see the relevant W3C documents at
    http://www.w3.org/TR/#tr_RDFa. RDFa 1.1 is defined for XHTML, HTML5, SVG
    and, in general, for any XML language.

    Note that the parser can also handle RDFa 1.0 if the extra parameter is
    used and/or the input source uses RDFa 1.0 specific @version or DTD-s.
    """
    def parse(self, source, graph,
              pgraph=None,
              media_type="",
              rdfa_version=None,
              embedded_rdf=False,
              vocab_expansion=False, vocab_cache=False):
        """
        @param source: one of the input sources that the RDFLib package defined
        @type source: InputSource class instance
        @param graph: target graph for the triples; output graph, in RDFa spec.
        parlance
        @type graph: RDFLib Graph
        @keyword pgraph: target for error and warning triples; processor graph,
        in RDFa spec. parlance. If set to None, these triples are ignored
        @type pgraph: RDFLib Graph
        @keyword media_type: explicit setting of the preferred media type
        (a.k.a. content type) of the the RDFa source. None means the content
        type of the HTTP result is used, or a guess is made based on the
        suffix of a file
        @type media_type: string
        @keyword rdfa_version: 1.0 or 1.1. If the value is "", then, by
        default, 1.1 is used unless the source has explicit signals to use
        1.0 (e.g., using a @version attribute, using a DTD set up for 1.0, etc)
        @type rdfa_version: string
        @keyword embedded_rdf: some formats allow embedding RDF in other
        formats: (X)HTML can contain turtle in a special <script> element,
        SVG can have RDF/XML embedded in a <metadata> element. This flag
        controls whether those triples should be interpreted and added to
        the output graph. Some languages (e.g., SVG) require this, and the
        flag is ignored.
        @type embedded_rdf: Boolean
        @keyword vocab_expansion: whether the RDFa @vocab attribute should
        also mean vocabulary expansion (see the RDFa 1.1 spec for further
        details)
        @type vocab_expansion: Boolean
        @keyword vocab_cache: in case vocab expansion is used, whether the
        expansion data (i.e., vocabulary) should be cached locally. This
        requires the ability for the local application to write on the
        local file system
        @type vocab_chache: Boolean
        """

        if html5lib is False:
            raise ImportError(
                'html5lib is not installed, cannot use ' +
                'RDFa and Microdata parsers.')

        (baseURI, orig_source) = _get_orig_source(source)
        self._process(graph, pgraph, baseURI, orig_source,
                      media_type=media_type,
                      rdfa_version=rdfa_version,
                      embedded_rdf=embedded_rdf,
                      vocab_expansion=vocab_expansion, vocab_cache=vocab_cache)

    def _process(self, graph, pgraph, baseURI, orig_source,
                 media_type="",
                 rdfa_version=None,
                 embedded_rdf=False,
                 vocab_expansion=False, vocab_cache=False):
        from .pyRdfa import pyRdfa, Options
        from rdflib import Graph
        processor_graph = pgraph if pgraph is not None else Graph()
        self.options = Options(output_processor_graph=True,
                               embedded_rdf=embedded_rdf,
                               vocab_expansion=vocab_expansion,
                               vocab_cache=vocab_cache)

        if media_type is None:
            media_type = ""
        processor = pyRdfa(self.options,
                           base=baseURI,
                           media_type=media_type,
                           rdfa_version=rdfa_version)
        processor.graph_from_source(orig_source, graph=graph, pgraph=processor_graph, rdfOutput=False)
        # This may result in an exception if the graph parsing led to an error
        _check_error(processor_graph)


class RDFa10Parser(Parser):
    """
    This is just a convenience class to wrap around the RDFa 1.0 parser.
    """
    def parse(self, source, graph, pgraph=None, media_type=""):
        """
        @param source: one of the input sources that the RDFLib package defined
        @type source: InputSource class instance
        @param graph: target graph for the triples; output graph, in RDFa
        spec. parlance
        @type graph: RDFLib Graph
        @keyword pgraph: target for error and warning triples; processor
        graph, in RDFa spec. parlance. If set to None, these triples are
        ignored
        @type pgraph: RDFLib Graph
        @keyword media_type: explicit setting of the preferred media type
        (a.k.a. content type) of the the RDFa source. None means the content
        type of the HTTP result is used, or a guess is made based on the
        suffix of a file
        @type media_type: string
        @keyword rdfOutput: whether Exceptions should be catched and added,
        as triples, to the processor graph, or whether they should be raised.
        @type rdfOutput: Boolean
        """
        RDFaParser().parse(source, graph, pgraph=pgraph,
                           media_type=media_type, rdfa_version="1.0")


class MicrodataParser(Parser):
    """
    Wrapper around an HTML5 microdata, extracted and converted into RDF. For
    the specification of microdata, see the relevant section of the HTML5
    spec: http://www.w3.org/TR/microdata/; for the algorithm used to extract
    microdata into RDF, see http://www.w3.org/TR/microdata-rdf/.
    """
    def parse(self, source, graph, vocab_expansion=False, vocab_cache=False):
        """
        @param source: one of the input sources that the RDFLib package defined
        @type source: InputSource class instance
        @param graph: target graph for the triples; output graph, in RDFa
        spec. parlance
        @type graph: RDFLib Graph
        @keyword vocab_expansion: whether the RDFa @vocab attribute should
        also mean vocabulary expansion (see the RDFa 1.1 spec for further
            details)
        @type vocab_expansion: Boolean
        @keyword vocab_cache: in case vocab expansion is used, whether the
        expansion data (i.e., vocabulary) should be cached locally. This
        requires the ability for the local application to write on the
        local file system
        @type vocab_chache: Boolean
        @keyword rdfOutput: whether Exceptions should be catched and added,
        as triples, to the processor graph, or whether they should be raised.
        @type rdfOutput: Boolean
        """
        if html5lib is False:
            raise ImportError(
                'html5lib is not installed, cannot use RDFa ' +
                'and Microdata parsers.')

        (baseURI, orig_source) = _get_orig_source(source)
        self._process(graph, baseURI, orig_source,
                      vocab_expansion=vocab_expansion,
                      vocab_cache=vocab_cache)

    def _process(self, graph, baseURI, orig_source,
                 vocab_expansion=False, vocab_cache=False):
        from .pyMicrodata import pyMicrodata
        processor = pyMicrodata(base=baseURI, vocab_expansion=vocab_expansion,
                                vocab_cache=vocab_cache)
        processor.graph_from_source(
            orig_source, graph=graph, rdfOutput=False)


class StructuredDataParser(Parser):
    """
    Convenience parser to extract both RDFa (including embedded Turtle)
    and microdata from an HTML file.
    It is simply a wrapper around the specific parsers.
    """
    def parse(self, source, graph,
              pgraph=None,
              rdfa_version="",
              vocab_expansion=False,
              vocab_cache=False,
              media_type='text/html'
              ):
        """
        @param source: one of the input sources that the RDFLib package defined
        @type source: InputSource class instance
        @param graph: target graph for the triples; output graph, in RDFa
        spec. parlance
        @keyword rdfa_version: 1.0 or 1.1. If the value is "", then, by
        default, 1.1 is used unless the source has explicit signals to use 1.0
        (e.g., using a @version attribute, using a DTD set up for 1.0, etc)
        @type rdfa_version: string
        @type graph: RDFLib Graph
        @keyword pgraph: target for error and warning triples; processor
        graph, in RDFa spec. parlance. If set to None, these triples are
        ignored
        @type pgraph: RDFLib Graph
        @keyword vocab_expansion: whether the RDFa @vocab attribute should
        also mean vocabulary expansion (see the RDFa 1.1 spec for further
            details)
        @type vocab_expansion: Boolean
        @keyword vocab_cache: in case vocab expansion is used, whether the
        expansion data (i.e., vocabulary) should be cached locally. This
        requires the ability for the local application to write on the
        local file system
        @type vocab_chache: Boolean
        @keyword rdfOutput: whether Exceptions should be catched and added,
        as triples, to the processor graph, or whether they should be raised.
        @type rdfOutput: Boolean
        """
        # Note that the media_type argument is ignored, and is here only to avoid an 'unexpected argument' error. 
        # This parser works for text/html only anyway...
        (baseURI, orig_source) = _get_orig_source(source)
        if rdfa_version == "" : rdfa_version = "1.1"
        RDFaParser()._process(graph, pgraph, baseURI, orig_source,
                              media_type='text/html',
                              rdfa_version=rdfa_version,
                              vocab_expansion=vocab_expansion,
                              vocab_cache=vocab_cache)
        MicrodataParser()._process(graph, baseURI, orig_source,
                                   vocab_expansion=vocab_expansion,
                                   vocab_cache=vocab_cache)
        from .hturtle import HTurtleParser
        HTurtleParser()._process(graph, baseURI, orig_source, media_type='text/html')

########NEW FILE########
__FILENAME__ = trig
from rdflib import ConjunctiveGraph
from rdflib.parser import Parser
from .notation3 import SinkParser, RDFSink


def becauseSubGraph(*args, **kwargs): pass


class TrigSinkParser(SinkParser):

    def directiveOrStatement(self, argstr, h):

        #import pdb; pdb.set_trace()

        i = self.skipSpace(argstr, h)
        if i < 0:
            return i    # EOF

        j = self.graph(argstr, i)
        if j >= 0:
            return j

        j = self.sparqlDirective(argstr, i)
        if j >= 0:
            return j

        j = self.directive(argstr, i)
        if j >= 0:
            return self.checkDot(argstr, j)

        j = self.statement(argstr, i)
        if j >= 0:
            return self.checkDot(argstr, j)


        return j

    def labelOrSubject(self, argstr, i, res):
        j = self.skipSpace(argstr, i)
        if j < 0:
            return j  # eof
        i = j

        j = self.uri_ref2(argstr, i, res)
        if j >= 0:
            return j

        if argstr[i] == '[':
            j = self.skipSpace(argstr, i+1)
            if j < 0:
                self.BadSyntax(argstr, i,
                    "Expected ] got EOF")
            if argstr[j] == ']':
                res.append(self.blankNode())
                return j+1
        return -1

    def graph(self, argstr, i):
        """
        Parse trig graph, i.e.

           <urn:graphname> = { .. triples .. }

        return -1 if it doesn't look like a graph-decl
        raise Exception if it looks like a graph, but isn't.
        """

        #import pdb; pdb.set_trace()
        j = self.sparqlTok('GRAPH', argstr, i) # optional GRAPH keyword
        if j >= 0: i = j

        r = []
        j = self.labelOrSubject(argstr, i, r)
        if j >= 0:
            graph = r[0]
            i = j
        else:
            graph = self._store.graph.identifier # hack


        j = self.skipSpace(argstr, i)
        if j < 0:
            self.BadSyntax(argstr, i,
                           "EOF found when expected graph")

        if argstr[j:j + 1] == "=": # optional = for legacy support

            i = self.skipSpace(argstr, j + 1)
            if i < 0:
                self.BadSyntax(argstr, i, "EOF found when expecting '{'")
        else:
            i = j

        if argstr[i:i+1] != "{":
            return -1 # the node wasn't part of a graph


        j = i+1

        oldParentContext = self._parentContext
        self._parentContext = self._context
        reason2 = self._reason2
        self._reason2 = becauseSubGraph
        self._context = self._store.newGraph(graph)

        while 1:
            i = self.skipSpace(argstr, j)
            if i < 0:
                self.BadSyntax(
                    argstr, i, "needed '}', found end.")

            if argstr[i:i + 1] == "}":
                j = i + 1
                break

            j = self.directiveOrStatement(argstr, i)
            if j < 0:
                self.BadSyntax(
                    argstr, i, "expected statement or '}'")

        self._context = self._parentContext
        self._reason2 = reason2
        self._parentContext = oldParentContext
        #res.append(subj.close())    # No use until closed
        return j




class TrigParser(Parser):
    """
    An RDFLib parser for TriG

    """

    def __init__(self):
        pass

    def parse(self, source, graph, encoding="utf-8"):

        if encoding not in [None, "utf-8"]:
            raise Exception(
                ("TriG files are always utf-8 encoded, ",
                 "I was passed: %s") % encoding)

        # we're currently being handed a Graph, not a ConjunctiveGraph
        assert graph.store.context_aware, "TriG Parser needs a context-aware store!"

        conj_graph = ConjunctiveGraph(store=graph.store)
        conj_graph.default_context = graph  # TODO: CG __init__ should have a
                                            # default_context arg
         # TODO: update N3Processor so that it can use conj_graph as the sink
        conj_graph.namespace_manager = graph.namespace_manager

        sink = RDFSink(conj_graph)

        baseURI = conj_graph.absolutize(
            source.getPublicId() or source.getSystemId() or "")
        p = TrigSinkParser(sink, baseURI=baseURI, turtle=True)

        p.loadStream(source.getByteStream())

        for prefix, namespace in p._bindings.items():
            conj_graph.bind(prefix, namespace)

        # return ???

########NEW FILE########
__FILENAME__ = trix
"""
A TriX parser for RDFLib
"""
from rdflib.namespace import Namespace
from rdflib.term import URIRef
from rdflib.term import BNode
from rdflib.term import Literal
from rdflib.graph import Graph, ConjunctiveGraph
from rdflib.exceptions import ParserError
from rdflib.parser import Parser

from xml.sax.saxutils import handler
from xml.sax import make_parser
from xml.sax.handler import ErrorHandler

__all__ = ['create_parser', 'TriXHandler', 'TriXParser']


TRIXNS = Namespace("http://www.w3.org/2004/03/trix/trix-1/")
XMLNS = Namespace("http://www.w3.org/XML/1998/namespace")


class TriXHandler(handler.ContentHandler):
    """An Sax Handler for TriX. See http://sw.nokia.com/trix/"""

    def __init__(self, store):
        self.store = store
        self.preserve_bnode_ids = False
        self.reset()

    def reset(self):
        self.bnode = {}
        self.graph = None
        self.triple = None
        self.state = 0
        self.lang = None
        self.datatype = None

    # ContentHandler methods

    def setDocumentLocator(self, locator):
        self.locator = locator

    def startDocument(self):
        pass

    def startPrefixMapping(self, prefix, namespace):
        pass

    def endPrefixMapping(self, prefix):
        pass

    def startElementNS(self, name, qname, attrs):

        if name[0] != str(TRIXNS):
            self.error(
                "Only elements in the TriX namespace are allowed. %s!=%s"
                % (name[0], TRIXNS))

        if name[1] == "TriX":
            if self.state == 0:
                self.state = 1
            else:
                self.error("Unexpected TriX element")

        elif name[1] == "graph":
            if self.state == 1:
                self.state = 2
            else:
                self.error("Unexpected graph element")

        elif name[1] == "uri":
            if self.state == 2:
                # the context uri
                self.state = 3
            elif self.state == 4:
                # part of a triple
                pass
            else:
                self.error("Unexpected uri element")

        elif name[1] == "triple":
            if self.state == 2:
                if self.graph is None:
                    # anonymous graph, create one with random bnode id
                    self.graph = Graph(store=self.store)
                # start of a triple
                self.triple = []
                self.state = 4
            else:
                self.error("Unexpected triple element")

        elif name[1] == "typedLiteral":
            if self.state == 4:
                # part of triple
                self.lang = None
                self.datatype = None

                try:
                    self.lang = attrs.getValue((unicode(XMLNS), u"lang"))
                except:
                    # language not required - ignore
                    pass
                try:
                    self.datatype = attrs.getValueByQName(u"datatype")
                except KeyError:
                    self.error("No required attribute 'datatype'")
            else:
                self.error("Unexpected typedLiteral element")

        elif name[1] == "plainLiteral":
            if self.state == 4:
                # part of triple
                self.lang = None
                self.datatype = None
                try:
                    self.lang = attrs.getValue((unicode(XMLNS), u"lang"))
                except:
                    # language not required - ignore
                    pass

            else:
                self.error("Unexpected plainLiteral element")

        elif name[1] == "id":
            if self.state == 2:
                # the context uri
                self.state = 3

            elif self.state == 4:
                # part of triple
                pass
            else:
                self.error("Unexpected id element")

        else:
            self.error("Unknown element %s in TriX namespace" % name[1])

        self.chars = ""

    def endElementNS(self, name, qname):
        if name[0] != str(TRIXNS):
            self.error(
                "Only elements in the TriX namespace are allowed. %s!=%s"
                % (name[0], TRIXNS))

        if name[1] == "uri":
            if self.state == 3:
                self.graph = Graph(store=self.store,
                                   identifier=URIRef(self.chars.strip()))
                self.state = 2
            elif self.state == 4:
                self.triple += [URIRef(self.chars.strip())]
            else:
                self.error(
                    "Illegal internal self.state - This should never " +
                    "happen if the SAX parser ensures XML syntax correctness")

        elif name[1] == "id":
            if self.state == 3:
                self.graph = Graph(self.store, identifier=self.get_bnode(
                    self.chars.strip()))
                self.state = 2
            elif self.state == 4:
                self.triple += [self.get_bnode(self.chars.strip())]
            else:
                self.error(
                    "Illegal internal self.state - This should never " +
                    "happen if the SAX parser ensures XML syntax correctness")

        elif name[1] == "plainLiteral" or name[1] == "typedLiteral":
            if self.state == 4:
                self.triple += [Literal(
                    self.chars, lang=self.lang, datatype=self.datatype)]
            else:
                self.error(
                    "This should never happen if the SAX parser " +
                    "ensures XML syntax correctness")

        elif name[1] == "triple":
            if self.state == 4:
                if len(self.triple) != 3:
                    self.error("Triple has wrong length, got %d elements: %s" %
                               (len(self.triple), self.triple))

                self.graph.add(self.triple)
                # self.store.store.add(self.triple,context=self.graph)
                # self.store.addN([self.triple+[self.graph]])
                self.state = 2
            else:
                self.error(
                    "This should never happen if the SAX parser " +
                    "ensures XML syntax correctness")

        elif name[1] == "graph":
            self.graph = None
            self.state = 1

        elif name[1] == "TriX":
            self.state = 0

        else:
            self.error("Unexpected close element")

    def get_bnode(self, label):
        if self.preserve_bnode_ids:
            bn = BNode(label)
        else:
            if label in self.bnode:
                bn = self.bnode[label]
            else:
                bn = BNode(label)
                self.bnode[label] = bn
        return bn

    def characters(self, content):
        self.chars += content

    def ignorableWhitespace(self, content):
        pass

    def processingInstruction(self, target, data):
        pass

    def error(self, message):
        locator = self.locator
        info = "%s:%s:%s: " % (
            locator.getSystemId(),
            locator.getLineNumber(),
            locator.getColumnNumber())
        raise ParserError(info + message)


def create_parser(store):
    parser = make_parser()
    try:
        # Workaround for bug in expatreader.py. Needed when
        # expatreader is trying to guess a prefix.
        parser.start_namespace_decl(
            "xml", "http://www.w3.org/XML/1998/namespace")
    except AttributeError:
        pass  # Not present in Jython (at least)
    parser.setFeature(handler.feature_namespaces, 1)
    trix = TriXHandler(store)
    parser.setContentHandler(trix)
    parser.setErrorHandler(ErrorHandler())
    return parser


class TriXParser(Parser):
    """A parser for TriX. See http://sw.nokia.com/trix/"""

    def __init__(self):
        pass

    def parse(self, source, sink, **args):
        assert sink.store.context_aware, (
            "TriXParser must be given a context aware store.")

        self._parser = create_parser(sink.store)
        content_handler = self._parser.getContentHandler()
        preserve_bnode_ids = args.get("preserve_bnode_ids", None)
        if preserve_bnode_ids is not None:
            content_handler.preserve_bnode_ids = preserve_bnode_ids
        # We're only using it once now
        # content_handler.reset()
        # self._parser.reset()
        self._parser.parse(source)

########NEW FILE########
__FILENAME__ = n3
"""
Notation 3 (N3) RDF graph serializer for RDFLib.
"""
from rdflib.graph import Graph
from rdflib.namespace import Namespace, OWL
from rdflib.plugins.serializers.turtle import (
    TurtleSerializer, SUBJECT, OBJECT)

__all__ = ['N3Serializer']

SWAP_LOG = Namespace("http://www.w3.org/2000/10/swap/log#")


class N3Serializer(TurtleSerializer):

    short_name = "n3"

    def __init__(self, store, parent=None):
        super(N3Serializer, self).__init__(store)
        self.keywords.update({
            OWL.sameAs: '=',
            SWAP_LOG.implies: '=>'
        })
        self.parent = parent

    def reset(self):
        super(N3Serializer, self).reset()
        self._stores = {}

    def subjectDone(self, subject):
        super(N3Serializer, self).subjectDone(subject)
        if self.parent:
            self.parent.subjectDone(subject)

    def isDone(self, subject):
        return (super(N3Serializer, self).isDone(subject)
                and (not self.parent or self.parent.isDone(subject)))

    def startDocument(self):
        super(N3Serializer, self).startDocument()
        # if not isinstance(self.store, N3Store):
        #    return
        #
        # all_list = [self.label(var) for var in
        #        self.store.get_universals(recurse=False)]
        # all_list.sort()
        # some_list = [self.label(var) for var in
        #        self.store.get_existentials(recurse=False)]
        # some_list.sort()
        #
        # for var in all_list:
        #    self.write('\n'+self.indent()+'@forAll %s. '%var)
        # for var in some_list:
        #    self.write('\n'+self.indent()+'@forSome %s. '%var)
        #
        # if (len(all_list) + len(some_list)) > 0:
        #    self.write('\n')

    def endDocument(self):
        if not self.parent:
            super(N3Serializer, self).endDocument()

    def indent(self, modifier=0):
        indent = super(N3Serializer, self).indent(modifier)
        if self.parent is not None:
            indent += self.parent.indent()  # modifier)
        return indent

    def preprocessTriple(self, triple):
        super(N3Serializer, self).preprocessTriple(triple)
        if isinstance(triple[0], Graph):
            for t in triple[0]:
                self.preprocessTriple(t)
        if isinstance(triple[2], Graph):
            for t in triple[2]:
                self.preprocessTriple(t)

    def getQName(self, uri, gen_prefix=True):
        qname = None
        if self.parent is not None:
            qname = self.parent.getQName(uri, gen_prefix)
        if qname is None:
            qname = super(N3Serializer, self).getQName(uri, gen_prefix)
        return qname

    def statement(self, subject):
        self.subjectDone(subject)
        properties = self.buildPredicateHash(subject)
        if len(properties) == 0:
            return False
        return (self.s_clause(subject)
                or super(N3Serializer, self).statement(subject))

    def path(self, node, position, newline=False):
        if not self.p_clause(node, position):
            super(N3Serializer, self).path(node, position, newline)

    def s_clause(self, subject):
        if isinstance(subject, Graph):
            self.write('\n' + self.indent())
            self.p_clause(subject, SUBJECT)
            self.predicateList(subject)
            self.write(' .')
            return True
        else:
            return False

    def p_clause(self, node, position):
        if isinstance(node, Graph):
            self.subjectDone(node)
            if position is OBJECT:
                self.write(' ')
            self.write('{')
            self.depth += 1
            serializer = N3Serializer(node, parent=self)
            serializer.serialize(self.stream)
            self.depth -= 1
            self.write(self.indent() + '}')
            return True
        else:
            return False

########NEW FILE########
__FILENAME__ = nquads
import warnings

from rdflib.term import Literal
from rdflib.serializer import Serializer
from rdflib.py3compat import b

from rdflib.plugins.serializers.nt import _xmlcharref_encode, _quoteLiteral

__all__ = ['NQuadsSerializer']


class NQuadsSerializer(Serializer):

    def __init__(self, store):
        if not store.context_aware:
            raise Exception(
                "NQuads serialization only makes "
                "sense for context-aware stores!")

        super(NQuadsSerializer, self).__init__(store)

    def serialize(self, stream, base=None, encoding=None, **args):
        if base is not None:
            warnings.warn("NQuadsSerializer does not support base.")
        if encoding is not None:
            warnings.warn("NQuadsSerializer does not use custom encoding.")
        encoding = self.encoding
        for context in self.store.contexts():
            for triple in context:
                stream.write(_nq_row(
                    triple, context.identifier).encode(encoding, "replace"))
        stream.write(b("\n"))


def _nq_row(triple, context):
    if isinstance(triple[2], Literal):
        return u"%s %s %s %s .\n" % (triple[0].n3(),
                                     triple[1].n3(),
                                     _xmlcharref_encode(
                                         _quoteLiteral(triple[2])),
                                     context.n3())
    else:
        return u"%s %s %s %s .\n" % (triple[0].n3(),
                                     triple[1].n3(),
                                     _xmlcharref_encode(triple[2].n3()),
                                     context.n3())

########NEW FILE########
__FILENAME__ = nt
"""
N-Triples RDF graph serializer for RDFLib.
See <http://www.w3.org/TR/rdf-testcases/#ntriples> for details about the
format.
"""
from rdflib.term import Literal
from rdflib.serializer import Serializer
from rdflib.py3compat import b
import warnings

__all__ = ['NTSerializer']


class NTSerializer(Serializer):
    """
    Serializes RDF graphs to NTriples format.
    """

    def serialize(self, stream, base=None, encoding=None, **args):
        if base is not None:
            warnings.warn("NTSerializer does not support base.")
        if encoding is not None:
            warnings.warn("NTSerializer does not use custom encoding.")
        encoding = self.encoding
        for triple in self.store:
            stream.write(_nt_row(triple).encode(encoding, "replace"))
        stream.write(b("\n"))


def _nt_row(triple):
    if isinstance(triple[2], Literal):
        return u"%s %s %s .\n" % (
            triple[0].n3(),
            triple[1].n3(),
            _xmlcharref_encode(_quoteLiteral(triple[2])))
    else:
        return u"%s %s %s .\n" % (triple[0].n3(),
                                  triple[1].n3(),
                                  _xmlcharref_encode(triple[2].n3()))


def _quoteLiteral(l):
    '''
    a simpler version of term.Literal.n3()
    '''

    encoded = _quote_encode(l)

    if l.language:
        if l.datatype:
            raise Exception("Literal has datatype AND language!")
        return '%s@%s' % (encoded, l.language)
    elif l.datatype:
        return '%s^^<%s>' % (encoded, l.datatype)
    else:
        return '%s' % encoded


def _quote_encode(l):
    return '"%s"' % l.replace('\\', '\\\\')\
        .replace('\n', '\\n')\
        .replace('"', '\\"')\
        .replace('\r', '\\r')


# from <http://code.activestate.com/recipes/303668/>
def _xmlcharref_encode(unicode_data, encoding="ascii"):
    """Emulate Python 2.3's 'xmlcharrefreplace' encoding error handler."""
    res = ""

    # Step through the unicode_data string one character at a time in
    # order to catch unencodable characters:
    for char in unicode_data:
        try:
            char.encode(encoding, 'strict')
        except UnicodeError:
            if ord(char) <= 0xFFFF:
                res += '\\u%04X' % ord(char)
            else:
                res += '\\U%08X' % ord(char)
        else:
            res += char

    return res

########NEW FILE########
__FILENAME__ = rdfxml
from rdflib.plugins.serializers.xmlwriter import XMLWriter

from rdflib.namespace import Namespace, RDF, RDFS  # , split_uri

from rdflib.term import URIRef, Literal, BNode
from rdflib.util import first, more_than
from rdflib.collection import Collection
from rdflib.serializer import Serializer

# from rdflib.exceptions import Error

from rdflib.py3compat import b

from xml.sax.saxutils import quoteattr, escape
import xml.dom.minidom

from xmlwriter import ESCAPE_ENTITIES

__all__ = ['fix', 'XMLSerializer', 'PrettyXMLSerializer']


class XMLSerializer(Serializer):

    def __init__(self, store):
        super(XMLSerializer, self).__init__(store)

    def __bindings(self):
        store = self.store
        nm = store.namespace_manager
        bindings = {}

        for predicate in set(store.predicates()):
            prefix, namespace, name = nm.compute_qname(predicate)
            bindings[prefix] = URIRef(namespace)

        RDFNS = URIRef("http://www.w3.org/1999/02/22-rdf-syntax-ns#")

        if "rdf" in bindings:
            assert bindings["rdf"] == RDFNS
        else:
            bindings["rdf"] = RDFNS

        for prefix, namespace in bindings.iteritems():
            yield prefix, namespace

    def serialize(self, stream, base=None, encoding=None, **args):
        self.base = base
        self.__stream = stream
        self.__serialized = {}
        encoding = self.encoding
        self.write = write = lambda uni: stream.write(
            uni.encode(encoding, 'replace'))

        # startDocument
        write('<?xml version="1.0" encoding="%s"?>\n' % self.encoding)

        # startRDF
        write('<rdf:RDF\n')

        # If provided, write xml:base attribute for the RDF
        if "xml_base" in args:
            write('   xml:base="%s"\n' % args['xml_base'])
        # TODO:
        # assert(
        #    namespaces["http://www.w3.org/1999/02/22-rdf-syntax-ns#"]=='rdf')
        bindings = list(self.__bindings())
        bindings.sort()

        for prefix, namespace in bindings:
            if prefix:
                write('   xmlns:%s="%s"\n' % (prefix, namespace))
            else:
                write('   xmlns="%s"\n' % namespace)
        write('>\n')

        # write out triples by subject
        for subject in self.store.subjects():
            self.subject(subject, 1)

        # endRDF
        write("</rdf:RDF>\n")

        # Set to None so that the memory can get garbage collected.
        # self.__serialized = None
        del self.__serialized

    def subject(self, subject, depth=1):
        if not subject in self.__serialized:
            self.__serialized[subject] = 1

            if isinstance(subject, (BNode, URIRef)):
                write = self.write
                indent = "  " * depth
                element_name = "rdf:Description"

                if isinstance(subject, BNode):
                    write('%s<%s rdf:nodeID="%s"' % (
                        indent, element_name, subject))
                else:
                    uri = quoteattr(self.relativize(subject))
                    write("%s<%s rdf:about=%s" % (indent, element_name, uri))

                if (subject, None, None) in self.store:
                    write(">\n")

                    for predicate, object in self.store.predicate_objects(
                            subject):
                        self.predicate(predicate, object, depth + 1)
                    write("%s</%s>\n" % (indent, element_name))

                else:
                    write("/>\n")

    def predicate(self, predicate, object, depth=1):
        write = self.write
        indent = "  " * depth
        qname = self.store.namespace_manager.qname(predicate)

        if isinstance(object, Literal):
            attributes = ""

            if object.language:
                attributes += ' xml:lang="%s"' % object.language

            if object.datatype:
                attributes += ' rdf:datatype="%s"' % object.datatype

            write("%s<%s%s>%s</%s>\n" %
                  (indent, qname, attributes,
                   escape(object, ESCAPE_ENTITIES), qname))
        else:

            if isinstance(object, BNode):
                write('%s<%s rdf:nodeID="%s"/>\n' %
                      (indent, qname, object))
            else:
                write("%s<%s rdf:resource=%s/>\n" %
                      (indent, qname, quoteattr(self.relativize(object))))

XMLLANG = "http://www.w3.org/XML/1998/namespacelang"
XMLBASE = "http://www.w3.org/XML/1998/namespacebase"
OWL_NS = Namespace('http://www.w3.org/2002/07/owl#')


# TODO:
def fix(val):
    "strip off _: from nodeIDs... as they are not valid NCNames"
    if val.startswith("_:"):
        return val[2:]
    else:
        return val


class PrettyXMLSerializer(Serializer):

    def __init__(self, store, max_depth=3):
        super(PrettyXMLSerializer, self).__init__(store)
        self.forceRDFAbout = set()

    def serialize(self, stream, base=None, encoding=None, **args):
        self.__serialized = {}
        store = self.store
        self.base = base
        self.max_depth = args.get("max_depth", 3)
        assert self.max_depth > 0, "max_depth must be greater than 0"

        self.nm = nm = store.namespace_manager
        self.writer = writer = XMLWriter(stream, nm, encoding)
        namespaces = {}

        possible = set(store.predicates()).union(
            store.objects(None, RDF.type))

        for predicate in possible:
            prefix, namespace, local = nm.compute_qname(predicate)
            namespaces[prefix] = namespace

        namespaces["rdf"] = "http://www.w3.org/1999/02/22-rdf-syntax-ns#"

        writer.push(RDF.RDF)

        if "xml_base" in args:
            writer.attribute(XMLBASE, args["xml_base"])

        writer.namespaces(namespaces.iteritems())

        # Write out subjects that can not be inline
        for subject in store.subjects():
            if (None, None, subject) in store:
                if (subject, None, subject) in store:
                    self.subject(subject, 1)
            else:
                self.subject(subject, 1)

        # write out anything that has not yet been reached
        # write out BNodes last (to ensure they can be inlined where possible)
        bnodes = set()

        for subject in store.subjects():
            if isinstance(subject, BNode):
                bnodes.add(subject)
                continue
            self.subject(subject, 1)

        # now serialize only those BNodes that have not been serialized yet
        for bnode in bnodes:
            if bnode not in self.__serialized:
                self.subject(subject, 1)

        writer.pop(RDF.RDF)
        stream.write(b("\n"))

        # Set to None so that the memory can get garbage collected.
        self.__serialized = None

    def subject(self, subject, depth=1):
        store = self.store
        writer = self.writer

        if subject in self.forceRDFAbout:
            writer.push(RDF.Description)
            writer.attribute(RDF.about, self.relativize(subject))
            writer.pop(RDF.Description)
            self.forceRDFAbout.remove(subject)

        elif not subject in self.__serialized:
            self.__serialized[subject] = 1
            type = first(store.objects(subject, RDF.type))

            try:
                self.nm.qname(type)
            except:
                type = None

            element = type or RDF.Description
            writer.push(element)

            if isinstance(subject, BNode):
                def subj_as_obj_more_than(ceil):
                    return True
                    # more_than(store.triples((None, None, subject)), ceil)

                # here we only include BNode labels if they are referenced
                # more than once (this reduces the use of redundant BNode
                # identifiers)
                if subj_as_obj_more_than(1):
                    writer.attribute(RDF.nodeID, fix(subject))

            else:
                writer.attribute(RDF.about, self.relativize(subject))

            if (subject, None, None) in store:
                for predicate, object in store.predicate_objects(subject):
                    if not (predicate == RDF.type and object == type):
                        self.predicate(predicate, object, depth + 1)

            writer.pop(element)

        elif subject in self.forceRDFAbout:
            writer.push(RDF.Description)
            writer.attribute(RDF.about, self.relativize(subject))
            writer.pop(RDF.Description)
            self.forceRDFAbout.remove(subject)

    def predicate(self, predicate, object, depth=1):
        writer = self.writer
        store = self.store
        writer.push(predicate)

        if isinstance(object, Literal):
            if object.language:
                writer.attribute(XMLLANG, object.language)

            if (object.datatype == RDF.XMLLiteral and
                    isinstance(object.value, xml.dom.minidom.Document)):
                writer.attribute(RDF.parseType, "Literal")
                writer.text(u"")
                writer.stream.write(object)
            else:
                if object.datatype:
                    writer.attribute(RDF.datatype, object.datatype)
                writer.text(object)

        elif object in self.__serialized or not (object, None, None) in store:

            if isinstance(object, BNode):
                if more_than(store.triples((None, None, object)), 0):
                    writer.attribute(RDF.nodeID, fix(object))
            else:
                writer.attribute(RDF.resource, self.relativize(object))

        else:
            if first(store.objects(object, RDF.first)):  # may not have type
                                                         # RDF.List

                self.__serialized[object] = 1

                # Warn that any assertions on object other than
                # RDF.first and RDF.rest are ignored... including RDF.List
                import warnings
                warnings.warn(
                    "Assertions on %s other than RDF.first " % repr(object) +
                    "and RDF.rest are ignored ... including RDF.List",
                    UserWarning, stacklevel=2)
                writer.attribute(RDF.parseType, "Collection")

                col = Collection(store, object)

                for item in col:

                    if isinstance(item, URIRef):
                        self.forceRDFAbout.add(item)
                    self.subject(item)

                    if not isinstance(item, URIRef):
                        self.__serialized[item] = 1
            else:
                if first(store.triples_choices(
                    (object, RDF.type, [OWL_NS.Class, RDFS.Class]))) \
                        and isinstance(object, URIRef):
                    writer.attribute(RDF.resource, self.relativize(object))

                elif depth <= self.max_depth:
                    self.subject(object, depth + 1)

                elif isinstance(object, BNode):

                    if not object in self.__serialized \
                            and (object, None, None) in store \
                            and len(list(store.subjects(object=object))) == 1:
                        # inline blank nodes if they haven't been serialized yet
                        # and are only referenced once (regardless of depth)
                        self.subject(object, depth + 1)
                    else:
                        writer.attribute(RDF.nodeID, fix(object))

                else:
                    writer.attribute(RDF.resource, self.relativize(object))

        writer.pop(predicate)

########NEW FILE########
__FILENAME__ = trig
"""
Trig RDF graph serializer for RDFLib.
See <http://www.w3.org/TR/trig/> for syntax specification.
"""

from collections import defaultdict

from rdflib.plugins.serializers.turtle import TurtleSerializer, _GEN_QNAME_FOR_DT, VERB

from rdflib.term import BNode, Literal

__all__ = ['TrigSerializer']


class TrigSerializer(TurtleSerializer):

    short_name = "trig"
    indentString = 4 * u' '

    def __init__(self, store):
        if store.context_aware:
            self.contexts = store.contexts()
        else:
            self.contexts = [store]

        super(TrigSerializer, self).__init__(store)

    def preprocess(self):
        for context in self.contexts:
            self.store = context
            self._references = defaultdict(int)
            self._subjects = {}

            for triple in context:
                self.preprocessTriple(triple)

            self._contexts[context]=(self.orderSubjects(), self._subjects, self._references)


    def preprocessTriple(self, triple):
        s, p, o = triple
        self._references[o]+=1
        self._subjects[s] = True
        for i, node in enumerate(triple):
            if node in self.keywords:
                continue
            # Don't use generated prefixes for subjects and objects
            self.getQName(node, gen_prefix=(i == VERB))
            if isinstance(node, Literal) and node.datatype:
                self.getQName(node.datatype, gen_prefix=_GEN_QNAME_FOR_DT)
        p = triple[1]
        if isinstance(p, BNode):
            self._references[p]+=1

    def reset(self):
        super(TrigSerializer, self).reset()
        self._contexts = {}

    def serialize(self, stream, base=None, encoding=None,
                  spacious=None, **args):
        self.reset()
        self.stream = stream
        self.base = base

        if spacious is not None:
            self._spacious = spacious

        self.preprocess()

        self.startDocument()

        firstTime = True
        for store, (ordered_subjects, subjects, ref) in self._contexts.items():
            self._references = ref
            self._serialized = {}
            self.store = store
            self._subjects = subjects

            self.write(self.indent() + '\n<%s> {' % self.getQName(store.identifier))
            self.depth += 1
            for subject in ordered_subjects:
                if self.isDone(subject):
                    continue
                if firstTime:
                    firstTime = False
                if self.statement(subject) and not firstTime:
                    self.write('\n')
            self.depth -= 1
            self.write('}\n')

        self.endDocument()
        stream.write(u"\n".encode('ascii'))

########NEW FILE########
__FILENAME__ = trix
from rdflib.serializer import Serializer
from rdflib.plugins.serializers.xmlwriter import XMLWriter

from rdflib.term import URIRef, Literal, BNode
from rdflib.namespace import Namespace

from rdflib.graph import Graph, ConjunctiveGraph

from rdflib.py3compat import b

__all__ = ['TriXSerializer']

## TODO: MOve this somewhere central
TRIXNS = Namespace("http://www.w3.org/2004/03/trix/trix-1/")
XMLNS = Namespace("http://www.w3.org/XML/1998/namespace")


class TriXSerializer(Serializer):
    def __init__(self, store):
        super(TriXSerializer, self).__init__(store)
        if not store.context_aware:
            raise Exception(
                "TriX serialization only makes sense for context-aware stores")

    def serialize(self, stream, base=None, encoding=None, **args):

        nm = self.store.namespace_manager

        self.writer = XMLWriter(stream, nm, encoding, extra_ns={"": TRIXNS})

        self.writer.push(TRIXNS[u"TriX"])
        self.writer.namespaces()

        if isinstance(self.store, ConjunctiveGraph):
            for subgraph in self.store.contexts():
                self._writeGraph(subgraph)
        elif isinstance(self.store, Graph):
            self._writeGraph(self.store)
        else:
            raise Exception("Unknown graph type: " + type(self.store))

        self.writer.pop()
        stream.write(b("\n"))

    def _writeGraph(self, graph):
        self.writer.push(TRIXNS[u"graph"])
        if isinstance(graph.identifier, URIRef):
            self.writer.element(
                TRIXNS[u"uri"], content=unicode(graph.identifier))

        for triple in graph.triples((None, None, None)):
            self._writeTriple(triple)
        self.writer.pop()

    def _writeTriple(self, triple):
        self.writer.push(TRIXNS[u"triple"])
        for component in triple:
            if isinstance(component, URIRef):
                self.writer.element(TRIXNS[u"uri"],
                                    content=unicode(component))
            elif isinstance(component, BNode):
                self.writer.element(TRIXNS[u"id"],
                                    content=unicode(component))
            elif isinstance(component, Literal):
                if component.datatype:
                    self.writer.element(TRIXNS[u"typedLiteral"],
                                        content=unicode(component),
                                        attributes={TRIXNS[u"datatype"]:
                                                    unicode(
                                                        component.datatype)})
                elif component.language:
                    self.writer.element(TRIXNS[u"plainLiteral"],
                                        content=unicode(component),
                                        attributes={XMLNS[u"lang"]:
                                                    unicode(
                                                        component.language)})
                else:
                    self.writer.element(TRIXNS[u"plainLiteral"],
                                        content=unicode(component))
        self.writer.pop()

########NEW FILE########
__FILENAME__ = turtle
"""
Turtle RDF graph serializer for RDFLib.
See <http://www.w3.org/TeamSubmission/turtle/> for syntax specification.
"""

from collections import defaultdict

from rdflib.term import BNode, Literal, URIRef
from rdflib.exceptions import Error
from rdflib.serializer import Serializer
from rdflib.namespace import RDF, RDFS

__all__ = ['RecursiveSerializer', 'TurtleSerializer']


class RecursiveSerializer(Serializer):

    topClasses = [RDFS.Class]
    predicateOrder = [RDF.type, RDFS.label]
    maxDepth = 10
    indentString = u"  "

    def __init__(self, store):

        super(RecursiveSerializer, self).__init__(store)
        self.stream = None
        self.reset()

    def addNamespace(self, prefix, uri):
        self.namespaces[prefix] = uri

    def checkSubject(self, subject):
        """Check to see if the subject should be serialized yet"""
        if ((self.isDone(subject))
            or (subject not in self._subjects)
            or ((subject in self._topLevels) and (self.depth > 1))
            or (isinstance(subject, URIRef)
                and (self.depth >= self.maxDepth))):
            return False
        return True

    def isDone(self, subject):
        """Return true if subject is serialized"""
        return subject in self._serialized

    def orderSubjects(self):
        seen = {}
        subjects = []

        for classURI in self.topClasses:
            members = list(self.store.subjects(RDF.type, classURI))
            members.sort()

            for member in members:
                subjects.append(member)
                self._topLevels[member] = True
                seen[member] = True

        recursable = [
            (isinstance(subject, BNode),
             self._references[subject], subject)
            for subject in self._subjects if subject not in seen]

        recursable.sort()
        subjects.extend([subject for (isbnode, refs, subject) in recursable])

        return subjects

    def preprocess(self):
        for triple in self.store.triples((None, None, None)):
            self.preprocessTriple(triple)

    def preprocessTriple(self, (s, p, o)):
        self._references[o]+=1
        self._subjects[s] = True

    def reset(self):
        self.depth = 0
        self.lists = {}
        self.namespaces = {}
        self._references = defaultdict(int)
        self._serialized = {}
        self._subjects = {}
        self._topLevels = {}

        for prefix, ns in self.store.namespaces():
            self.addNamespace(prefix, ns)

    def buildPredicateHash(self, subject):
        """
        Build a hash key by predicate to a list of objects for the given
        subject
        """
        properties = {}
        for s, p, o in self.store.triples((subject, None, None)):
            oList = properties.get(p, [])
            oList.append(o)
            properties[p] = oList
        return properties

    def sortProperties(self, properties):
        """Take a hash from predicate uris to lists of values.
           Sort the lists of values.  Return a sorted list of properties."""
        # Sort object lists
        for prop, objects in properties.items():
            objects.sort()

        # Make sorted list of properties
        propList = []
        seen = {}
        for prop in self.predicateOrder:
            if (prop in properties) and (prop not in seen):
                propList.append(prop)
                seen[prop] = True
        props = properties.keys()
        props.sort()
        for prop in props:
            if prop not in seen:
                propList.append(prop)
                seen[prop] = True
        return propList

    def subjectDone(self, subject):
        """Mark a subject as done."""
        self._serialized[subject] = True

    def indent(self, modifier=0):
        """Returns indent string multiplied by the depth"""
        return (self.depth + modifier) * self.indentString

    def write(self, text):
        """Write text in given encoding."""
        self.stream.write(text.encode(self.encoding, 'replace'))


SUBJECT = 0
VERB = 1
OBJECT = 2

_GEN_QNAME_FOR_DT = False
_SPACIOUS_OUTPUT = False


class TurtleSerializer(RecursiveSerializer):

    short_name = "turtle"
    indentString = '    '

    def __init__(self, store):
        self._ns_rewrite = {}
        super(TurtleSerializer, self).__init__(store)
        self.keywords = {
            RDF.type: 'a'
        }
        self.reset()
        self.stream = None
        self._spacious = _SPACIOUS_OUTPUT

    def addNamespace(self, prefix, namespace):
        # Turtle does not support prefix that start with _
        # if they occur in the graph, rewrite to p_blah
        # this is more complicated since we need to make sure p_blah
        # does not already exist. And we register namespaces as we go, i.e.
        # we may first see a triple with prefix _9 - rewrite it to p_9
        # and then later find a triple with a "real" p_9 prefix

        # so we need to keep track of ns rewrites we made so far.

        if (prefix > '' and prefix[0] == '_') \
                or self.namespaces.get(prefix, namespace) != namespace:

            if prefix not in self._ns_rewrite:
                p = "p" + prefix
                while p in self.namespaces:
                    p = "p" + p
                self._ns_rewrite[prefix] = p

        prefix = self._ns_rewrite.get(prefix, prefix)
        super(TurtleSerializer, self).addNamespace(prefix, namespace)
        return prefix

    def reset(self):
        super(TurtleSerializer, self).reset()
        self._shortNames = {}
        self._started = False
        self._ns_rewrite = {}

    def serialize(self, stream, base=None, encoding=None,
                  spacious=None, **args):
        self.reset()
        self.stream = stream
        self.base = base

        if spacious is not None:
            self._spacious = spacious

        self.preprocess()
        subjects_list = self.orderSubjects()

        self.startDocument()

        firstTime = True
        for subject in subjects_list:
            if self.isDone(subject):
                continue
            if firstTime:
                firstTime = False
            if self.statement(subject) and not firstTime:
                self.write('\n')

        self.endDocument()
        stream.write(u"\n".encode('ascii'))

    def preprocessTriple(self, triple):
        super(TurtleSerializer, self).preprocessTriple(triple)
        for i, node in enumerate(triple):
            if node in self.keywords:
                continue
            # Don't use generated prefixes for subjects and objects
            self.getQName(node, gen_prefix=(i == VERB))
            if isinstance(node, Literal) and node.datatype:
                self.getQName(node.datatype, gen_prefix=_GEN_QNAME_FOR_DT)
        p = triple[1]
        if isinstance(p, BNode): # hmm - when is P ever a bnode?
            self._references[p]+=1

    def getQName(self, uri, gen_prefix=True):
        if not isinstance(uri, URIRef):
            return None

        parts = None

        try:
            parts = self.store.compute_qname(uri, generate=gen_prefix)
        except:

            # is the uri a namespace in itself?
            pfx = self.store.store.prefix(uri)

            if pfx is not None:
                parts = (pfx, uri, '')
            else:
                # nothing worked
                return None

        prefix, namespace, local = parts

        # QName cannot end with .
        if local.endswith("."): return None

        prefix = self.addNamespace(prefix, namespace)

        return u'%s:%s' % (prefix, local)

    def startDocument(self):
        self._started = True
        ns_list = sorted(self.namespaces.items())
        for prefix, uri in ns_list:
            self.write(self.indent() + '@prefix %s: <%s> .\n' % (prefix, uri))
        if ns_list and self._spacious:
            self.write('\n')

    def endDocument(self):
        if self._spacious:
            self.write('\n')

    def statement(self, subject):
        self.subjectDone(subject)
        return self.s_squared(subject) or self.s_default(subject)

    def s_default(self, subject):
        self.write('\n' + self.indent())
        self.path(subject, SUBJECT)
        self.predicateList(subject)
        self.write(' .')
        return True

    def s_squared(self, subject):
        if (self._references[subject] > 0) or not isinstance(subject, BNode):
            return False
        self.write('\n' + self.indent() + '[]')
        self.predicateList(subject)
        self.write(' .')
        return True

    def path(self, node, position, newline=False):
        if not (self.p_squared(node, position, newline)
                or self.p_default(node, position, newline)):
            raise Error("Cannot serialize node '%s'" % (node, ))

    def p_default(self, node, position, newline=False):
        if position != SUBJECT and not newline:
            self.write(' ')
        self.write(self.label(node, position))
        return True

    def label(self, node, position):
        if node == RDF.nil:
            return '()'
        if position is VERB and node in self.keywords:
            return self.keywords[node]
        if isinstance(node, Literal):
            return node._literal_n3(
                use_plain=True,
                qname_callback=lambda dt: self.getQName(
                    dt, _GEN_QNAME_FOR_DT))
        else:
            node = self.relativize(node)

            return self.getQName(node, position == VERB) or node.n3()

    def p_squared(self, node, position, newline=False):
        if (not isinstance(node, BNode)
                or node in self._serialized
                or self._references[node] > 1
                or position == SUBJECT):
            return False

        if not newline:
            self.write(' ')

        if self.isValidList(node):
            # this is a list
            self.write('(')
            self.depth += 1  # 2
            self.doList(node)
            self.depth -= 1  # 2
            self.write(' )')
        else:
            self.subjectDone(node)
            self.depth += 2
            # self.write('[\n' + self.indent())
            self.write('[')
            self.depth -= 1
            # self.predicateList(node, newline=True)
            self.predicateList(node, newline=False)
            # self.write('\n' + self.indent() + ']')
            self.write(' ]')
            self.depth -= 1

        return True

    def isValidList(self, l):
        """
        Checks if l is a valid RDF list, i.e. no nodes have other properties.
        """
        try:
            if not self.store.value(l, RDF.first):
                return False
        except:
            return False
        while l:
            if l != RDF.nil and len(
                    list(self.store.predicate_objects(l))) != 2:
                return False
            l = self.store.value(l, RDF.rest)
        return True

    def doList(self, l):
        while l:
            item = self.store.value(l, RDF.first)
            if item is not None:
                self.path(item, OBJECT)
                self.subjectDone(l)
            l = self.store.value(l, RDF.rest)

    def predicateList(self, subject, newline=False):
        properties = self.buildPredicateHash(subject)
        propList = self.sortProperties(properties)
        if len(propList) == 0:
            return
        self.verb(propList[0], newline=newline)
        self.objectList(properties[propList[0]])
        for predicate in propList[1:]:
            self.write(' ;\n' + self.indent(1))
            self.verb(predicate, newline=True)
            self.objectList(properties[predicate])

    def verb(self, node, newline=False):
        self.path(node, VERB, newline)

    def objectList(self, objects):
        count = len(objects)
        if count == 0:
            return
        depthmod = (count == 1) and 0 or 1
        self.depth += depthmod
        self.path(objects[0], OBJECT)
        for obj in objects[1:]:
            self.write(',\n' + self.indent(1))
            self.path(obj, OBJECT, newline=True)
        self.depth -= depthmod

########NEW FILE########
__FILENAME__ = xmlwriter
import codecs
from xml.sax.saxutils import quoteattr, escape

__all__ = ['XMLWriter']

ESCAPE_ENTITIES = {
    '\r': '&#13;'
}


class XMLWriter(object):
    def __init__(self, stream, namespace_manager, encoding=None,
                 decl=1, extra_ns=None):
        encoding = encoding or 'utf-8'
        encoder, decoder, stream_reader, stream_writer = \
            codecs.lookup(encoding)
        self.stream = stream = stream_writer(stream)
        if decl:
            stream.write('<?xml version="1.0" encoding="%s"?>' % encoding)
        self.element_stack = []
        self.nm = namespace_manager
        self.extra_ns = extra_ns or {}
        self.closed = True

    def __get_indent(self):
        return "  " * len(self.element_stack)
    indent = property(__get_indent)

    def __close_start_tag(self):
        if not self.closed:  # TODO:
            self.closed = True
            self.stream.write(">")

    def push(self, uri):
        self.__close_start_tag()
        write = self.stream.write
        write("\n")
        write(self.indent)
        write("<%s" % self.qname(uri))
        self.element_stack.append(uri)
        self.closed = False
        self.parent = False

    def pop(self, uri=None):
        top = self.element_stack.pop()
        if uri:
            assert uri == top
        write = self.stream.write
        if not self.closed:
            self.closed = True
            write("/>")
        else:
            if self.parent:
                write("\n")
                write(self.indent)
            write("</%s>" % self.qname(top))
        self.parent = True

    def element(self, uri, content, attributes={}):
        """Utility method for adding a complete simple element"""
        self.push(uri)
        for k, v in attributes.iteritems():
            self.attribute(k, v)
        self.text(content)
        self.pop()

    def namespaces(self, namespaces=None):
        if not namespaces:
            namespaces = self.nm.namespaces()

        write = self.stream.write
        write("\n")
        for prefix, namespace in namespaces:
            if prefix:
                write('  xmlns:%s="%s"\n' % (prefix, namespace))
            # Allow user-provided namespace bindings to prevail
            elif prefix not in self.extra_ns:
                write('  xmlns="%s"\n' % namespace)

        for prefix, namespace in self.extra_ns.items():
            if prefix:
                write('  xmlns:%s="%s"\n' % (prefix, namespace))
            else:
                write('  xmlns="%s"\n' % namespace)

    def attribute(self, uri, value):
        write = self.stream.write
        write(" %s=%s" % (self.qname(uri), quoteattr(value)))

    def text(self, text):
        self.__close_start_tag()
        if "<" in text and ">" in text and not "]]>" in text:
            self.stream.write("<![CDATA[")
            self.stream.write(text)
            self.stream.write("]]>")
        else:
            self.stream.write(escape(text, ESCAPE_ENTITIES))

    def qname(self, uri):
        """Compute qname for a uri using our extra namespaces,
        or the given namespace manager"""

        for pre, ns in self.extra_ns.items():
            if uri.startswith(ns):
                if pre != "":
                    return ":".join(pre, uri[len(ns):])
                else:
                    return uri[len(ns):]

        return self.nm.qname(uri)

########NEW FILE########
__FILENAME__ = sleepycat
from rdflib.store import Store, VALID_STORE, NO_STORE
from rdflib.term import URIRef
from rdflib.py3compat import b


def bb(u):
    return u.encode('utf-8')


try:
    from bsddb import db
    has_bsddb = True
except ImportError:
    try:
        from bsddb3 import db
        has_bsddb = True
    except ImportError:
        has_bsddb = False
from os import mkdir
from os.path import exists, abspath
from urllib import pathname2url
from threading import Thread

if has_bsddb:
    # These are passed to bsddb when creating DBs

    # passed to db.DBEnv.set_flags
    ENVSETFLAGS = db.DB_CDB_ALLDB
    # passed to db.DBEnv.open
    ENVFLAGS = db.DB_INIT_MPOOL | db.DB_INIT_CDB | db.DB_THREAD
    CACHESIZE = 1024 * 1024 * 50

    # passed to db.DB.Open()
    DBOPENFLAGS = db.DB_THREAD

import logging
_logger = logging.getLogger(__name__)

__all__ = ['Sleepycat']


class Sleepycat(Store):
    context_aware = True
    formula_aware = True
    transaction_aware = False
    graph_aware = True
    db_env = None

    def __init__(self, configuration=None, identifier=None):
        if not has_bsddb:
            raise ImportError(
                "Unable to import bsddb/bsddb3, store is unusable.")
        self.__open = False
        self.__identifier = identifier
        super(Sleepycat, self).__init__(configuration)
        self._loads = self.node_pickler.loads
        self._dumps = self.node_pickler.dumps

    def __get_identifier(self):
        return self.__identifier
    identifier = property(__get_identifier)

    def _init_db_environment(self, homeDir, create=True):
        if not exists(homeDir):
            if create is True:
                mkdir(homeDir)
                      # TODO: implement create method and refactor this to it
                self.create(homeDir)
            else:
                return NO_STORE
        db_env = db.DBEnv()
        db_env.set_cachesize(0, CACHESIZE)  # TODO
        # db_env.set_lg_max(1024*1024)
        db_env.set_flags(ENVSETFLAGS, 1)
        db_env.open(homeDir, ENVFLAGS | db.DB_CREATE)
        return db_env

    def is_open(self):
        return self.__open

    def open(self, path, create=True):
        if not has_bsddb:
            return NO_STORE
        homeDir = path

        if self.__identifier is None:
            self.__identifier = URIRef(pathname2url(abspath(homeDir)))

        db_env = self._init_db_environment(homeDir, create)
        if db_env == NO_STORE:
            return NO_STORE
        self.db_env = db_env
        self.__open = True

        dbname = None
        dbtype = db.DB_BTREE
        # auto-commit ensures that the open-call commits when transactions
        # are enabled

        dbopenflags = DBOPENFLAGS
        if self.transaction_aware is True:
            dbopenflags |= db.DB_AUTO_COMMIT

        if create:
            dbopenflags |= db.DB_CREATE

        dbmode = 0660
        dbsetflags = 0

        # create and open the DBs
        self.__indicies = [None, ] * 3
        self.__indicies_info = [None, ] * 3
        for i in xrange(0, 3):
            index_name = to_key_func(
                i)((b("s"), b("p"), b("o")), b("c")).decode()
            index = db.DB(db_env)
            index.set_flags(dbsetflags)
            index.open(index_name, dbname, dbtype, dbopenflags, dbmode)
            self.__indicies[i] = index
            self.__indicies_info[i] = (index, to_key_func(i), from_key_func(i))

        lookup = {}
        for i in xrange(0, 8):
            results = []
            for start in xrange(0, 3):
                score = 1
                len = 0
                for j in xrange(start, start + 3):
                    if i & (1 << (j % 3)):
                        score = score << 1
                        len += 1
                    else:
                        break
                tie_break = 2 - start
                results.append(((score, tie_break), start, len))

            results.sort()
            score, start, len = results[-1]

            def get_prefix_func(start, end):
                def get_prefix(triple, context):
                    if context is None:
                        yield ""
                    else:
                        yield context
                    i = start
                    while i < end:
                        yield triple[i % 3]
                        i += 1
                    yield ""
                return get_prefix

            lookup[i] = (
                self.__indicies[start],
                get_prefix_func(start, start + len),
                from_key_func(start),
                results_from_key_func(start, self._from_string))

        self.__lookup_dict = lookup

        self.__contexts = db.DB(db_env)
        self.__contexts.set_flags(dbsetflags)
        self.__contexts.open("contexts", dbname, dbtype, dbopenflags, dbmode)

        self.__namespace = db.DB(db_env)
        self.__namespace.set_flags(dbsetflags)
        self.__namespace.open("namespace", dbname, dbtype, dbopenflags, dbmode)

        self.__prefix = db.DB(db_env)
        self.__prefix.set_flags(dbsetflags)
        self.__prefix.open("prefix", dbname, dbtype, dbopenflags, dbmode)

        self.__k2i = db.DB(db_env)
        self.__k2i.set_flags(dbsetflags)
        self.__k2i.open("k2i", dbname, db.DB_HASH, dbopenflags, dbmode)

        self.__i2k = db.DB(db_env)
        self.__i2k.set_flags(dbsetflags)
        self.__i2k.open("i2k", dbname, db.DB_RECNO, dbopenflags, dbmode)

        self.__needs_sync = False
        t = Thread(target=self.__sync_run)
        t.setDaemon(True)
        t.start()
        self.__sync_thread = t
        return VALID_STORE

    def __sync_run(self):
        from time import sleep, time
        try:
            min_seconds, max_seconds = 10, 300
            while self.__open:
                if self.__needs_sync:
                    t0 = t1 = time()
                    self.__needs_sync = False
                    while self.__open:
                        sleep(.1)
                        if self.__needs_sync:
                            t1 = time()
                            self.__needs_sync = False
                        if time() - t1 > min_seconds \
                                or time() - t0 > max_seconds:
                            self.__needs_sync = False
                            _logger.debug("sync")
                            self.sync()
                            break
                else:
                    sleep(1)
        except Exception, e:
            _logger.exception(e)

    def sync(self):
        if self.__open:
            for i in self.__indicies:
                i.sync()
            self.__contexts.sync()
            self.__namespace.sync()
            self.__prefix.sync()
            self.__i2k.sync()
            self.__k2i.sync()

    def close(self, commit_pending_transaction=False):
        self.__open = False
        self.__sync_thread.join()
        for i in self.__indicies:
            i.close()
        self.__contexts.close()
        self.__namespace.close()
        self.__prefix.close()
        self.__i2k.close()
        self.__k2i.close()
        self.db_env.close()

    def add(self, triple, context, quoted=False, txn=None):
        """\
        Add a triple to the store of triples.
        """
        (subject, predicate, object) = triple
        assert self.__open, "The Store must be open."
        assert context != self, "Can not add triple directly to store"
        Store.add(self, (subject, predicate, object), context, quoted)

        _to_string = self._to_string

        s = _to_string(subject, txn=txn)
        p = _to_string(predicate, txn=txn)
        o = _to_string(object, txn=txn)
        c = _to_string(context, txn=txn)

        cspo, cpos, cosp = self.__indicies

        value = cspo.get(bb("%s^%s^%s^%s^" % (c, s, p, o)), txn=txn)
        if value is None:
            self.__contexts.put(bb(c), "", txn=txn)

            contexts_value = cspo.get(
                bb("%s^%s^%s^%s^" % ("", s, p, o)), txn=txn) or b("")
            contexts = set(contexts_value.split(b("^")))
            contexts.add(bb(c))
            contexts_value = b("^").join(contexts)
            assert contexts_value is not None

            cspo.put(bb("%s^%s^%s^%s^" % (c, s, p, o)), "", txn=txn)
            cpos.put(bb("%s^%s^%s^%s^" % (c, p, o, s)), "", txn=txn)
            cosp.put(bb("%s^%s^%s^%s^" % (c, o, s, p)), "", txn=txn)
            if not quoted:
                cspo.put(bb(
                    "%s^%s^%s^%s^" % ("", s, p, o)), contexts_value, txn=txn)
                cpos.put(bb(
                    "%s^%s^%s^%s^" % ("", p, o, s)), contexts_value, txn=txn)
                cosp.put(bb(
                    "%s^%s^%s^%s^" % ("", o, s, p)), contexts_value, txn=txn)

            self.__needs_sync = True

    def __remove(self, (s, p, o), c, quoted=False, txn=None):
        cspo, cpos, cosp = self.__indicies
        contexts_value = cspo.get(
            b("^").join([b(""), s, p, o, b("")]), txn=txn) or b("")
        contexts = set(contexts_value.split(b("^")))
        contexts.discard(c)
        contexts_value = b("^").join(contexts)
        for i, _to_key, _from_key in self.__indicies_info:
            i.delete(_to_key((s, p, o), c), txn=txn)
        if not quoted:
            if contexts_value:
                for i, _to_key, _from_key in self.__indicies_info:
                    i.put(_to_key((s, p, o), b("")), contexts_value, txn=txn)
            else:
                for i, _to_key, _from_key in self.__indicies_info:
                    try:
                        i.delete(_to_key((s, p, o), b("")), txn=txn)
                    except db.DBNotFoundError:
                        pass  # TODO: is it okay to ignore these?

    def remove(self, (subject, predicate, object), context, txn=None):
        assert self.__open, "The Store must be open."
        Store.remove(self, (subject, predicate, object), context)
        _to_string = self._to_string

        if context is not None:
            if context == self:
                context = None

        if subject is not None \
                and predicate is not None \
                and object is not None \
                and context is not None:
            s = _to_string(subject, txn=txn)
            p = _to_string(predicate, txn=txn)
            o = _to_string(object, txn=txn)
            c = _to_string(context, txn=txn)
            value = self.__indicies[0].get(bb("%s^%s^%s^%s^" %
                                           (c, s, p, o)), txn=txn)
            if value is not None:
                self.__remove((bb(s), bb(p), bb(o)), bb(c), txn=txn)
                self.__needs_sync = True
        else:
            cspo, cpos, cosp = self.__indicies
            index, prefix, from_key, results_from_key = self.__lookup(
                (subject, predicate, object), context, txn=txn)

            cursor = index.cursor(txn=txn)
            try:
                current = cursor.set_range(prefix)
                needs_sync = True
            except db.DBNotFoundError:
                current = None
                needs_sync = False
            cursor.close()
            while current:
                key, value = current
                cursor = index.cursor(txn=txn)
                try:
                    cursor.set_range(key)
                    # Hack to stop 2to3 converting this to next(cursor)
                    current = getattr(cursor, 'next')()
                except db.DBNotFoundError:
                    current = None
                cursor.close()
                if key.startswith(prefix):
                    c, s, p, o = from_key(key)
                    if context is None:
                        contexts_value = index.get(key, txn=txn) or b("")
                        # remove triple from all non quoted contexts
                        contexts = set(contexts_value.split(b("^")))
                        # and from the conjunctive index
                        contexts.add(b(""))
                        for c in contexts:
                            for i, _to_key, _ in self.__indicies_info:
                                i.delete(_to_key((s, p, o), c), txn=txn)
                    else:
                        self.__remove((s, p, o), c, txn=txn)
                else:
                    break

            if context is not None:
                if subject is None and predicate is None and object is None:
                    # TODO: also if context becomes empty and not just on
                    # remove((None, None, None), c)
                    try:
                        self.__contexts.delete(
                            bb(_to_string(context, txn=txn)), txn=txn)
                    except db.DBNotFoundError:
                        pass

            self.__needs_sync = needs_sync

    def triples(self, (subject, predicate, object), context=None, txn=None):
        """A generator over all the triples matching """
        assert self.__open, "The Store must be open."

        if context is not None:
            if context == self:
                context = None

        # _from_string = self._from_string ## UNUSED
        index, prefix, from_key, results_from_key = self.__lookup(
            (subject, predicate, object), context, txn=txn)

        cursor = index.cursor(txn=txn)
        try:
            current = cursor.set_range(prefix)
        except db.DBNotFoundError:
            current = None
        cursor.close()
        while current:
            key, value = current
            cursor = index.cursor(txn=txn)
            try:
                cursor.set_range(key)
                # Cheap hack so 2to3 doesn't convert to next(cursor)
                current = getattr(cursor, 'next')()
            except db.DBNotFoundError:
                current = None
            cursor.close()
            if key and key.startswith(prefix):
                contexts_value = index.get(key, txn=txn)
                yield results_from_key(
                    key, subject, predicate, object, contexts_value)
            else:
                break

    def __len__(self, context=None):
        assert self.__open, "The Store must be open."
        if context is not None:
            if context == self:
                context = None

        if context is None:
            prefix = b("^")
        else:
            prefix = bb("%s^" % self._to_string(context))

        index = self.__indicies[0]
        cursor = index.cursor()
        current = cursor.set_range(prefix)
        count = 0
        while current:
            key, value = current
            if key.startswith(prefix):
                count += 1
                # Hack to stop 2to3 converting this to next(cursor)
                current = getattr(cursor, 'next')()
            else:
                break
        cursor.close()
        return count

    def bind(self, prefix, namespace):
        prefix = prefix.encode("utf-8")
        namespace = namespace.encode("utf-8")
        bound_prefix = self.__prefix.get(namespace)
        if bound_prefix:
            self.__namespace.delete(bound_prefix)
        self.__prefix[namespace] = prefix
        self.__namespace[prefix] = namespace

    def namespace(self, prefix):
        prefix = prefix.encode("utf-8")
        ns = self.__namespace.get(prefix, None)
        if ns is not None:
            return URIRef(ns.decode('utf-8'))
        return None

    def prefix(self, namespace):
        namespace = namespace.encode("utf-8")
        prefix = self.__prefix.get(namespace, None)
        if prefix is not None:
            return prefix.decode('utf-8')
        return None

    def namespaces(self):
        cursor = self.__namespace.cursor()
        results = []
        current = cursor.first()
        while current:
            prefix, namespace = current
            results.append((prefix.decode('utf-8'), namespace.decode('utf-8')))
            # Hack to stop 2to3 converting this to next(cursor)
            current = getattr(cursor, 'next')()
        cursor.close()
        for prefix, namespace in results:
            yield prefix, URIRef(namespace)

    def contexts(self, triple=None):
        _from_string = self._from_string
        _to_string = self._to_string

        if triple:
            s, p, o = triple
            s = _to_string(s)
            p = _to_string(p)
            o = _to_string(o)
            contexts = self.__indicies[0].get(bb(
                "%s^%s^%s^%s^" % ("", s, p, o)))
            if contexts:
                for c in contexts.split(b("^")):
                    if c:
                        yield _from_string(c)
        else:
            index = self.__contexts
            cursor = index.cursor()
            current = cursor.first()
            cursor.close()
            while current:
                key, value = current
                context = _from_string(key)
                yield context
                cursor = index.cursor()
                try:
                    cursor.set_range(key)
                    # Hack to stop 2to3 converting this to next(cursor)
                    current = getattr(cursor, 'next')()
                except db.DBNotFoundError:
                    current = None
                cursor.close()

    def add_graph(self, graph): 
        self.__contexts.put(bb(self._to_string(graph)), "")

    def remove_graph(self, graph): 
        self.remove((None, None, None), graph)

    def _from_string(self, i):
        k = self.__i2k.get(int(i))
        return self._loads(k)

    def _to_string(self, term, txn=None):
        k = self._dumps(term)
        i = self.__k2i.get(k, txn=txn)
        if i is None:
            # weird behavoir from bsddb not taking a txn as a keyword argument
            # for append
            if self.transaction_aware:
                i = "%s" % self.__i2k.append(k, txn)
            else:
                i = "%s" % self.__i2k.append(k)

            self.__k2i.put(k, i, txn=txn)
        else:
            i = i.decode()
        return i

    def __lookup(self, (subject, predicate, object), context, txn=None):
        _to_string = self._to_string
        if context is not None:
            context = _to_string(context, txn=txn)
        i = 0
        if subject is not None:
            i += 1
            subject = _to_string(subject, txn=txn)
        if predicate is not None:
            i += 2
            predicate = _to_string(predicate, txn=txn)
        if object is not None:
            i += 4
            object = _to_string(object, txn=txn)
        index, prefix_func, from_key, results_from_key = self.__lookup_dict[i]
        # print (subject, predicate, object), context, prefix_func, index
        # #DEBUG
        prefix = bb(
            "^".join(prefix_func((subject, predicate, object), context)))
        return index, prefix, from_key, results_from_key


def to_key_func(i):
    def to_key(triple, context):
        "Takes a string; returns key"
        return b("^").join(
            (context,
             triple[i % 3],
             triple[(i + 1) % 3],
             triple[(i + 2) % 3], b("")))  # "" to tac on the trailing ^
    return to_key


def from_key_func(i):
    def from_key(key):
        "Takes a key; returns string"
        parts = key.split(b("^"))
        return \
            parts[0], \
            parts[(3 - i + 0) % 3 + 1], \
            parts[(3 - i + 1) % 3 + 1], \
            parts[(3 - i + 2) % 3 + 1]
    return from_key


def results_from_key_func(i, from_string):
    def from_key(key, subject, predicate, object, contexts_value):
        "Takes a key and subject, predicate, object; returns tuple for yield"
        parts = key.split(b("^"))
        if subject is None:
            # TODO: i & 1: # dis assemble and/or measure to see which is faster
            # subject is None or i & 1
            s = from_string(parts[(3 - i + 0) % 3 + 1])
        else:
            s = subject
        if predicate is None:  # i & 2:
            p = from_string(parts[(3 - i + 1) % 3 + 1])
        else:
            p = predicate
        if object is None:  # i & 4:
            o = from_string(parts[(3 - i + 2) % 3 + 1])
        else:
            o = object
        return (s, p, o), (
            from_string(c) for c in contexts_value.split(b("^")) if c)
    return from_key


def readable_index(i):
    s, p, o = "?" * 3
    if i & 1:
        s = "s"
    if i & 2:
        p = "p"
    if i & 4:
        o = "o"
    return "%s,%s,%s" % (s, p, o)

########NEW FILE########
__FILENAME__ = aggregates
from rdflib import Literal, XSD

from rdflib.plugins.sparql.evalutils import _eval
from rdflib.plugins.sparql.operators import numeric
from rdflib.plugins.sparql.datatypes import type_promotion

from rdflib.plugins.sparql.compat import num_max, num_min

from decimal import Decimal

"""
Aggregation functions
"""


def _eval_rows(expr, group):
    for row in group:
        try:
            yield _eval(expr, row)
        except:
            pass


def agg_Sum(a, group, bindings):
    c = 0

    dt = None
    for x in group:
        try:
            e = _eval(a.vars, x)
            n = numeric(e)
            if dt == None:
                dt = e.datatype
            else:
                dt = type_promotion(dt, e.datatype)

            if type(c) == float and type(n) == Decimal:
                c += float(n)
            elif type(n) == float and type(c) == Decimal:
                c = float(c) + n
            else:
                c += n
        except:
            pass  # simply dont count

    bindings[a.res] = Literal(c, datatype=dt)

# Perhaps TODO: keep datatype for max/min?


def agg_Min(a, group, bindings):
    m = None

    for x in group:
        try:
            v = numeric(_eval(a.vars, x))
            if m is None:
                m = v
            else:
                m = num_min(v, m)
        except:
            return  # error in aggregate => no binding

    if m is not None:
        bindings[a.res] = Literal(m)


def agg_Max(a, group, bindings):
    m = None

    for x in group:
        try:
            v = numeric(_eval(a.vars, x))
            if m is None:
                m = v
            else:
                m = num_max(v, m)
        except:
            return  # error in aggregate => no binding

    if m is not None:
        bindings[a.res] = Literal(m)


def agg_Count(a, group, bindings):

    c = 0
    for x in group:
        try:
            if a.vars != '*':
                _eval(a.vars, x)
            c += 1
        except:
            return  # error in aggregate => no binding
            # pass  # simply dont count

    bindings[a.res] = Literal(c)


def agg_Sample(a, group, bindings):
    try:
        bindings[a.res] = _eval(a.vars, iter(group).next())
    except StopIteration:
        pass  # no res


def agg_GroupConcat(a, group, bindings):

    sep = a.separator or " "

    bindings[a.res] = Literal(
        sep.join(unicode(x) for x in _eval_rows(a.vars, group)))


def agg_Avg(a, group, bindings):

    c = 0
    s = 0
    dt = None
    for x in group:
        try:
            e = _eval(a.vars, x)
            n = numeric(e)
            if dt == None:
                dt = e.datatype
            else:
                dt = type_promotion(dt, e.datatype)

            if type(s) == float and type(n) == Decimal:
                s += float(n)
            elif type(n) == float and type(s) == Decimal:
                s = float(s) + n
            else:
                s += n
            c += 1
        except:
            return  # error in aggregate => no binding

    if c == 0:
        bindings[a.res] = Literal(0)
    if dt == XSD.float or dt == XSD.double:
        bindings[a.res] = Literal(s / c)
    else:
        bindings[a.res] = Literal(Decimal(s) / Decimal(c))


def evalAgg(a, group, bindings):
    if a.name == 'Aggregate_Count':
        return agg_Count(a, group, bindings)
    elif a.name == 'Aggregate_Sum':
        return agg_Sum(a, group, bindings)
    elif a.name == 'Aggregate_Sample':
        return agg_Sample(a, group, bindings)
    elif a.name == 'Aggregate_GroupConcat':
        return agg_GroupConcat(a, group, bindings)
    elif a.name == 'Aggregate_Avg':
        return agg_Avg(a, group, bindings)
    elif a.name == 'Aggregate_Min':
        return agg_Min(a, group, bindings)
    elif a.name == 'Aggregate_Max':
        return agg_Max(a, group, bindings)

    else:
        raise Exception("Unknown aggregate function " + a.name)

########NEW FILE########
__FILENAME__ = algebra

"""
Converting the 'parse-tree' output of pyparsing to a SPARQL Algebra expression

http://www.w3.org/TR/sparql11-query/#sparqlQuery

"""

import functools
import operator
import collections

from rdflib import Literal, Variable, URIRef, BNode

from rdflib.plugins.sparql.sparql import Prologue, Query
from rdflib.plugins.sparql.parserutils import CompValue, Expr
from rdflib.plugins.sparql.operators import (
    and_, TrueFilter, simplify as simplifyFilters)
from rdflib.paths import (
    InvPath, AlternativePath, SequencePath, MulPath, NegatedPath)

from pyparsing import ParseResults


# ---------------------------
# Some convenience methods
def OrderBy(p, expr):
    return CompValue('OrderBy', p=p, expr=expr)


def ToMultiSet(p):
    return CompValue('ToMultiSet', p=p)


def Union(p1, p2):
    return CompValue('Union', p1=p1, p2=p2)


def Join(p1, p2):
    return CompValue('Join', p1=p1, p2=p2)


def Minus(p1, p2):
    return CompValue('Minus', p1=p1, p2=p2)


def Graph(term, graph):
    return CompValue('Graph', term=term, p=graph)


def BGP(triples=None):
    return CompValue('BGP', triples=triples or [])


def LeftJoin(p1, p2, expr):
    return CompValue('LeftJoin', p1=p1, p2=p2, expr=expr)


def Filter(expr, p):
    return CompValue('Filter', expr=expr, p=p)


def Extend(p, expr, var):
    return CompValue('Extend', p=p, expr=expr, var=var)


def Project(p, PV):
    return CompValue('Project', p=p, PV=PV)


def Group(p, expr=None):
    return CompValue('Group', p=p, expr=expr)


def _knownTerms(triple, varsknown, varscount):
    return (len(filter(None, (x not in varsknown and
                              isinstance(
                                  x, (Variable, BNode)) for x in triple))),
            -sum(varscount.get(x, 0) for x in triple),
            not isinstance(triple[2], Literal),
            )


def reorderTriples(l):
    """
    Reorder triple patterns so that we execute the
    ones with most bindings first
    """

    def _addvar(term, varsknown):
        if isinstance(term, (Variable, BNode)):
            varsknown.add(term)

    l = [(None, x) for x in l]
    varsknown = set()
    varscount = collections.defaultdict(int)
    for t in l:
        for c in t[1]:
            if isinstance(c, (Variable, BNode)):
                varscount[c] += 1
    i = 0

    # Done in steps, sort by number of bound terms
    # the top block of patterns with the most bound terms is kept
    # the rest is resorted based on the vars bound after the first
    # block is evaluated

    # we sort by decorate/undecorate, since we need the value of the sort keys

    while i < len(l):
        l[i:] = sorted((_knownTerms(x[
                       1], varsknown, varscount), x[1]) for x in l[i:])
        t = l[i][0][0]  # top block has this many terms bound
        j = 0
        while i+j < len(l) and l[i+j][0][0] == t:
            for c in l[i+j][1]:
                _addvar(c, varsknown)
            j += 1
        i += 1

    return [x[1] for x in l]


def triples(l):

    l = reduce(lambda x, y: x + y, l)
    if (len(l) % 3) != 0:
        raise Exception('these aint triples')
    return reorderTriples((l[x], l[x + 1], l[x + 2])
                          for x in range(0, len(l), 3))


def translatePName(p, prologue):
    """
    Expand prefixed/relative URIs
    """
    if isinstance(p, CompValue):
        if p.name == 'pname':
            return prologue.absolutize(p)
        if p.name == 'literal':
            return Literal(p.string, lang=p.lang,
                           datatype=prologue.absolutize(p.datatype))
    elif isinstance(p, URIRef):
        return prologue.absolutize(p)


def translatePath(p):

    """
    Translate PropertyPath expressions
    """

    if isinstance(p, CompValue):
        if p.name == 'PathAlternative':
            if len(p.part) == 1:
                return p.part[0]
            else:
                return AlternativePath(*p.part)

        elif p.name == 'PathSequence':
            if len(p.part) == 1:
                return p.part[0]
            else:
                return SequencePath(*p.part)

        elif p.name == 'PathElt':
            if not p.mod:
                return p.part
            else:
                if isinstance(p.part, list):
                    if len(p.part) != 1:
                        raise Exception('Denkfehler!')

                    return MulPath(p.part[0], p.mod)
                else:
                    return MulPath(p.part, p.mod)

        elif p.name == 'PathEltOrInverse':
            if isinstance(p.part, list):
                if len(p.part) != 1:
                    raise Exception('Denkfehler!')
                return InvPath(p.part[0])
            else:
                return InvPath(p.part)

        elif p.name == 'PathNegatedPropertySet':
            if isinstance(p.part, list):
                return NegatedPath(AlternativePath(*p.part))
            else:
                return NegatedPath(p.part)


def translateExists(e):

    """
    Translate the graph pattern used by EXISTS and NOT EXISTS
    http://www.w3.org/TR/sparql11-query/#sparqlCollectFilters
    """

    def _c(n):
        if isinstance(n, CompValue):
            if n.name in ('Builtin_EXISTS', 'Builtin_NOTEXISTS'):
                n.graph = translateGroupGraphPattern(n.graph)

    e = traverse(e, visitPost=_c)

    return e


def collectAndRemoveFilters(parts):

    """

    FILTER expressions apply to the whole group graph pattern in which
    they appear.

    http://www.w3.org/TR/sparql11-query/#sparqlCollectFilters
    """

    filters = []

    i = 0
    while i < len(parts):
        p = parts[i]
        if p.name == 'Filter':
            filters.append(translateExists(p.expr))
            parts.pop(i)
        else:
            i += 1

    if filters:
        return and_(*filters)

    return None


def translateGroupOrUnionGraphPattern(graphPattern):
    A = None

    for g in graphPattern.graph:
        g = translateGroupGraphPattern(g)
        if not A:
            A = g
        else:
            A = Union(A, g)
    return A


def translateGraphGraphPattern(graphPattern):
    return Graph(graphPattern.term,
                 translateGroupGraphPattern(graphPattern.graph))


def translateInlineData(graphPattern):
    return ToMultiSet(translateValues(graphPattern))


def translateGroupGraphPattern(graphPattern):
    """
    http://www.w3.org/TR/sparql11-query/#convertGraphPattern
    """

    if graphPattern.name == 'SubSelect':
        return ToMultiSet(translate(graphPattern)[0])

    if not graphPattern.part:
        graphPattern.part = []  # empty { }

    filters = collectAndRemoveFilters(graphPattern.part)

    g = []
    for p in graphPattern.part:
        if p.name == 'TriplesBlock':
            # merge adjacent TripleBlocks
            if not (g and g[-1].name == 'BGP'):
                g.append(BGP())
            g[-1]["triples"] += triples(p.triples)
        else:
            g.append(p)

    G = BGP()
    for p in g:
        if p.name == 'OptionalGraphPattern':
            A = translateGroupGraphPattern(p.graph)
            if A.name == 'Filter':
                G = LeftJoin(G, A.p, A.expr)
            else:
                G = LeftJoin(G, A, TrueFilter)
        elif p.name == 'MinusGraphPattern':
            G = Minus(p1=G, p2=translateGroupGraphPattern(p.graph))
        elif p.name == 'GroupOrUnionGraphPattern':
            G = Join(p1=G, p2=translateGroupOrUnionGraphPattern(p))
        elif p.name == 'GraphGraphPattern':
            G = Join(p1=G, p2=translateGraphGraphPattern(p))
        elif p.name == 'InlineData':
            G = Join(p1=G, p2=translateInlineData(p))
        elif p.name == 'ServiceGraphPattern':
            G = Join(p1=G, p2=p)
        elif p.name in ('BGP', 'Extend'):
            G = Join(p1=G, p2=p)
        elif p.name == 'Bind':
            G = Extend(G, p.expr, p.var)

        else:
            raise Exception('Unknown part in GroupGraphPattern: %s - %s' %
                            (type(p), p.name))

    if filters:
        G = Filter(expr=filters, p=G)

    return G


class StopTraversal(Exception):
    def __init__(self, rv):
        self.rv = rv


def _traverse(e, visitPre=lambda n: None, visitPost=lambda n: None):
    """
    Traverse a parse-tree, visit each node

    if visit functions return a value, replace current node
    """
    _e = visitPre(e)
    if _e is not None:
        return _e

    if e is None:
        return None

    if isinstance(e, (list, ParseResults)):
        return [_traverse(x, visitPre, visitPost) for x in e]
    elif isinstance(e, tuple):
        return tuple([_traverse(x, visitPre, visitPost) for x in e])

    elif isinstance(e, CompValue):
        for k, val in e.iteritems():
            e[k] = _traverse(val, visitPre, visitPost)

    _e = visitPost(e)
    if _e is not None:
        return _e

    return e


def _traverseAgg(e, visitor=lambda n, v: None):
    """
    Traverse a parse-tree, visit each node

    if visit functions return a value, replace current node
    """

    res = []

    if isinstance(e, (list, ParseResults, tuple)):
        res = [_traverseAgg(x, visitor) for x in e]

    elif isinstance(e, CompValue):
        for k, val in e.iteritems():
            if val != None:
                res.append(_traverseAgg(val, visitor))

    return visitor(e, res)


def traverse(
        tree, visitPre=lambda n: None,
        visitPost=lambda n: None, complete=None):
    """
    Traverse tree, visit each node with visit function
    visit function may raise StopTraversal to stop traversal
    if complete!=None, it is returned on complete traversal,
    otherwise the transformed tree is returned
    """
    try:
        r = _traverse(tree, visitPre, visitPost)
        if complete is not None:
            return complete
        return r
    except StopTraversal, st:
        return st.rv


def _hasAggregate(x):
    """
    Traverse parse(sub)Tree
    return true if any aggregates are used
    """

    if isinstance(x, CompValue):
        if x.name.startswith('Aggregate_'):
            raise StopTraversal(True)


def _aggs(e, A):
    """
    Collect Aggregates in A
    replaces aggregates with variable references
    """

    # TODO: nested Aggregates?

    if isinstance(e, CompValue) and e.name.startswith('Aggregate_'):
        A.append(e)
        aggvar = Variable('__agg_%d__' % len(A))
        e["res"] = aggvar
        return aggvar


def _findVars(x, res):
    """
    Find all variables in a tree
    """
    if isinstance(x, Variable):
        res.add(x)
    if isinstance(x, CompValue):
        if x.name == "Bind":
            res.add(x.var)
            return x  # stop recursion and finding vars in the expr
        elif x.name == 'SubSelect':
            if x.projection: 
                res.update(v.var or v.evar for v in x.projection)
            return x


def _addVars(x, children):
    # import pdb; pdb.set_trace()
    if isinstance(x, Variable):
        return set([x])
    elif isinstance(x, CompValue):
        x["_vars"] = set(reduce(operator.or_, children, set()))
        if x.name == "Bind":
            return set([x.var])
        elif x.name == 'SubSelect':
            if x.projection:
                s = set(v.var or v.evar for v in x.projection)
            else: 
                s = set()

            return s
    return reduce(operator.or_, children, set())


def _sample(e, v=None):
    """
    For each unaggregated variable V in expr
    Replace V with Sample(V)
    """
    if isinstance(e, CompValue) and e.name.startswith("Aggregate_"):
        return e  # do not replace vars in aggregates
    if isinstance(e, Variable) and v != e:
        return CompValue('Aggregate_Sample', vars=e)


def _simplifyFilters(e):
    if isinstance(e, Expr):
        return simplifyFilters(e)


def translateAggregates(q, M):
    E = []
    A = []

    # collect/replace aggs in :
    #    select expr as ?var
    if q.projection:
        for v in q.projection:
            if v.evar: 
                v.expr = traverse(v.expr, functools.partial(_sample, v=v.evar))
                v.expr = traverse(v.expr, functools.partial(_aggs, A=A))


    # having clause
    if traverse(q.having, _hasAggregate, complete=False):
        q.having = traverse(q.having, _sample)
        traverse(q.having, functools.partial(_aggs, A=A))

    # order by
    if traverse(q.orderby, _hasAggregate, complete=False):
        q.orderby = traverse(q.orderby, _sample)
        traverse(q.orderby, functools.partial(_aggs, A=A))

    # sample all other select vars
    # TODO: only allowed for vars in group-by?
    if q.projection: 
        for v in q.projection: 
            if v.var:
                rv = Variable('__agg_%d__' % (len(A) + 1))
                A.append(CompValue('Aggregate_Sample', vars=v.var, res=rv))
                E.append((rv, v.var))
                
    return CompValue('AggregateJoin', A=A, p=M), E


def translateValues(v):
    # if len(v.var)!=len(v.value):
    #     raise Exception("Unmatched vars and values in ValueClause: "+str(v))

    res = []
    if not v.var:
        return res
    if not v.value:
        return res
    if not isinstance(v.value[0], list):

        for val in v.value:
            res.append({v.var[0]: val})
    else:
        for vals in v.value:
            res.append(dict(zip(v.var, vals)))

    return CompValue('values', res=res)


def translate(q):
    """
    http://www.w3.org/TR/sparql11-query/#convertSolMod

    """

    _traverse(q, _simplifyFilters)

    q.where = traverse(q.where, visitPost=translatePath)

    # TODO: Var scope test
    VS = set()
    traverse(q.where, functools.partial(_findVars, res=VS))

    # all query types have a where part
    M = translateGroupGraphPattern(q.where)

    aggregate = False
    if q.groupby:
        conditions = []
        # convert "GROUP BY (?expr as ?var)" to an Extend
        for c in q.groupby.condition:
            if isinstance(c, CompValue) and c.name == 'GroupAs':
                M = Extend(M, c.expr, c.var)
                c = c.var
            conditions.append(c)

        M = Group(p=M, expr=conditions)
        aggregate = True
    elif traverse(q.having, _hasAggregate, complete=False) or \
            traverse(q.orderby, _hasAggregate, complete=False) or \
            any(traverse(x.expr, _hasAggregate, complete=False)
                for x in q.projection or [] if x.evar):
        # if any aggregate is used, implicit group by
        M = Group(p=M)
        aggregate = True

    if aggregate:
        M, E = translateAggregates(q, M)
    else:
        E = []

    # HAVING
    if q.having:
        M = Filter(expr=and_(*q.having.condition), p=M)

    # VALUES
    if q.valuesClause:
        M = Join(p1=M, p2=ToMultiSet(translateValues(q.valuesClause)))

    if not q.projection: 
        # select *
        PV = list(VS)
    else:
        PV = list()
        for v in q.projection:
            if v.var:
                if v not in PV:
                    PV.append(v.var)
            elif v.evar:
                if v not in PV:
                    PV.append(v.evar)

                E.append((v.expr, v.evar))
            else: 
                raise Exception("I expected a var or evar here!")

    for e, v in E:
        M = Extend(M, e, v)

    # ORDER BY
    if q.orderby:
        M = OrderBy(M, [CompValue('OrderCondition', expr=c.expr,
                    order=c.order) for c in q.orderby.condition])

    # PROJECT
    M = Project(M, PV)

    if q.modifier:
        if q.modifier == 'DISTINCT':
            M = CompValue('Distinct', p=M)
        elif q.modifier == 'REDUCED':
            M = CompValue('Reduced', p=M)

    if q.limitoffset:
        offset = 0
        if q.limitoffset.offset != None:
            offset = q.limitoffset.offset.toPython()

        if q.limitoffset.limit != None:
            M = CompValue('Slice', p=M, start=offset,
                          length=q.limitoffset.limit.toPython())
        else:
            M = CompValue('Slice', p=M, start=offset)

    return M, PV


def simplify(n):
    """Remove joins to empty BGPs"""
    if isinstance(n, CompValue):
        if n.name == 'Join':
            if n.p1.name == 'BGP' and len(n.p1.triples) == 0:
                return n.p2
            if n.p2.name == 'BGP' and len(n.p2.triples) == 0:
                return n.p1
        elif n.name == 'BGP':
            n["triples"] = reorderTriples(n.triples)
            return n


def analyse(n, children):

    if isinstance(n, CompValue):
        if n.name == 'Join':
            n["lazy"] = all(children)
            return False
        elif n.name in ('Slice', 'Distinct'):
            return False
        else:
            return all(children)
    else:
        return True


def translatePrologue(p, base, initNs=None, prologue=None):

    if prologue is None:
        prologue = Prologue()
        prologue.base = ""
    if base:
        prologue.base = base
    if initNs:
        for k, v in initNs.iteritems():
            prologue.bind(k, v)

    for x in p:
        if x.name == 'Base':
            prologue.base = x.iri
        elif x.name == 'PrefixDecl':
            prologue.bind(x.prefix, prologue.absolutize(x.iri))

    return prologue


def translateQuads(quads):
    if quads.triples:
        alltriples = triples(quads.triples)
    else:
        alltriples = []

    allquads = collections.defaultdict(list)

    if quads.quadsNotTriples:
        for q in quads.quadsNotTriples:
            if q.triples:
                allquads[q.term] += triples(q.triples)

    return alltriples, allquads


def translateUpdate1(u, prologue):
    if u.name in ('Load', 'Clear', 'Drop', 'Create'):
        pass  # no translation needed
    elif u.name in ('Add', 'Move', 'Copy'):
        pass
    elif u.name in ('InsertData', 'DeleteData', 'DeleteWhere'):
        t, q = translateQuads(u.quads)
        u["quads"] = q
        u["triples"] = t
        if u.name in ('DeleteWhere', 'DeleteData'):
            pass  # TODO: check for bnodes in triples
    elif u.name == 'Modify':
        if u.delete:
            u.delete["triples"], u.delete[
                "quads"] = translateQuads(u.delete.quads)
        if u.insert:
            u.insert["triples"], u.insert[
                "quads"] = translateQuads(u.insert.quads)
        u["where"] = translateGroupGraphPattern(u.where)
    else:
        raise Exception('Unknown type of update operation: %s' % u)

    u.prologue = prologue
    return u


def translateUpdate(q, base=None, initNs=None):
    """
    Returns a list of SPARQL Update Algebra expressions
    """

    res = []
    prologue = None
    if not q.request:
        return res
    for p, u in zip(q.prologue, q.request):
        prologue = translatePrologue(p, base, initNs, prologue)

        # absolutize/resolve prefixes
        u = traverse(
            u, visitPost=functools.partial(translatePName, prologue=prologue))
        u = _traverse(u, _simplifyFilters)

        u = traverse(u, visitPost=translatePath)

        res.append(translateUpdate1(u, prologue))

    return res


def translateQuery(q, base=None, initNs=None):
    """
    Translate a query-parsetree to a SPARQL Algebra Expression

    Return a rdflib.plugins.sparql.sparql.Query object
    """

    # We get in: (prologue, query)

    prologue = translatePrologue(q[0], base, initNs)

    # absolutize/resolve prefixes
    q[1] = traverse(
        q[1], visitPost=functools.partial(translatePName, prologue=prologue))

    P, PV = translate(q[1])
    datasetClause = q[1].datasetClause
    if q[1].name == 'ConstructQuery':

        template = triples(q[1].template) if q[1].template else None

        res = CompValue(q[1].name, p=P,
                        template=template,
                        datasetClause=datasetClause)
    else:
        res = CompValue(q[1].name, p=P, datasetClause=datasetClause, PV=PV)

    res = traverse(res, visitPost=simplify)
    _traverseAgg(res, visitor=analyse)
    _traverseAgg(res, _addVars)

    return Query(prologue, res)


def pprintAlgebra(q):
    def pp(p, ind="    "):
        # if isinstance(p, list):
        #     print "[ "
        #     for x in p: pp(x,ind)
        #     print "%s ]"%ind
        #     return
        if not isinstance(p, CompValue):
            print p
            return
        print "%s(" % (p.name, )
        for k in p:
            print "%s%s =" % (ind, k,),
            pp(p[k], ind + "    ")
        print "%s)" % ind

    try:
        pp(q.algebra)
    except AttributeError:
        # it's update, just a list
        for x in q:
            pp(x)

if __name__ == '__main__':
    import sys
    from . import parser
    import os.path

    if os.path.exists(sys.argv[1]):
        q = file(sys.argv[1])
    else:
        q = sys.argv[1]

    pq = parser.parseQuery(q)
    print pq
    tq = translateQuery(pq)
    print pprintAlgebra(tq)

########NEW FILE########
__FILENAME__ = compat
"""
Function/methods to help supporting 2.5-2.7
"""

# Collection ABCs

try:
    from collections import Mapping, MutableMapping  # was added in 2.6

except:
    from UserDict import DictMixin

    class MutableMapping(DictMixin):
        def keys(self):
            return list(self)

    Mapping = MutableMapping


# OrderedDict

try:
    from collections import OrderedDict  # was added in 2.7
except ImportError:
    from ordereddict import OrderedDict  # extra module

import sys

if sys.version_info[:2] < (2, 7):

    from decimal import Decimal
    # Pre-2.7 decimal and float did not compare correctly

    def _numericKey(n):
        if isinstance(n, Decimal):
            return float(n)
        else:
            return n

    def num_max(*args, **kwargs):
        kwargs["key"] = _numericKey
        return max(*args, **kwargs)

    def num_min(*args, **kwargs):
        kwargs["key"] = _numericKey
        return min(*args, **kwargs)

else:

    num_max = max
    num_min = min

########NEW FILE########
__FILENAME__ = datatypes
"""
Utility functions for supporting the XML Schema Datatypes hierarchy
"""

from rdflib import XSD

XSD_DTs = set(
    (XSD.integer, XSD.decimal, XSD.float, XSD.double, XSD.string,
     XSD.boolean, XSD.dateTime, XSD.nonPositiveInteger, XSD.negativeInteger,
     XSD.long, XSD.int, XSD.short, XSD.byte, XSD.nonNegativeInteger,
     XSD.unsignedLong, XSD.unsignedInt, XSD.unsignedShort, XSD.unsignedByte,
     XSD.positiveInteger))

_sub_types = {
    XSD.integer: [
        XSD.nonPositiveInteger, XSD.negativeInteger, XSD.long, XSD.int,
        XSD.short, XSD.byte, XSD.nonNegativeInteger, XSD.positiveInteger,
        XSD.unsignedLong, XSD.unsignedInt, XSD.unsignedShort, XSD.unsignedByte],
}

_super_types = {}
for superdt in XSD_DTs:
    for subdt in _sub_types.get(superdt, []):
        _super_types[subdt] = superdt

# we only care about float, double, integer, decimal
_typePromotionMap = {
    XSD.float: {XSD.integer: XSD.float,
                XSD.decimal: XSD.float,
                XSD.double: XSD.double},

    XSD.double: {XSD.integer: XSD.double,
                 XSD.float: XSD.double,
                 XSD.decimal: XSD.double},

    XSD.decimal: {XSD.integer: XSD.decimal,
                  XSD.float: XSD.float,
                  XSD.double: XSD.double},

    XSD.integer: {XSD.decimal: XSD.decimal,
                  XSD.float: XSD.float,
                  XSD.double: XSD.double}
}


def type_promotion(t1, t2):
    if t2 == None:
        return t1
    t1 = _super_types.get(t1, t1)
    t2 = _super_types.get(t2, t2)
    if t1 == t2:
        return t1  # matching super-types
    try:
        return _typePromotionMap[t1][t2]
    except KeyError:
        raise TypeError(
            'Operators cannot combine datatypes %s and %s' % (t1, t2))

########NEW FILE########
__FILENAME__ = evaluate
"""
These method recursively evaluate the SPARQL Algebra

evalQuery is the entry-point, it will setup context and
return the SPARQLResult object

evalPart is called on each level and will delegate to the right method

A rdflib.plugins.sparql.sparql.QueryContext is passed along, keeping
information needed for evaluation

A list of dicts (solution mappings) is returned, apart from GroupBy which may
also return a dict of list of dicts

"""

import collections

from rdflib import Variable, Graph, BNode, URIRef, Literal

from rdflib.plugins.sparql import CUSTOM_EVALS
from rdflib.plugins.sparql.parserutils import value
from rdflib.plugins.sparql.sparql import (
    QueryContext, AlreadyBound, FrozenBindings, SPARQLError)
from rdflib.plugins.sparql.evalutils import (
    _filter, _eval, _join, _diff, _minus, _fillTemplate, _ebv)

from rdflib.plugins.sparql.aggregates import evalAgg


def evalBGP(ctx, bgp):

    """
    A basic graph pattern
    """

    if not bgp:
        yield ctx.solution()
        return

    s, p, o = bgp[0]

    _s = ctx[s]
    _p = ctx[p]
    _o = ctx[o]

    for ss, sp, so in ctx.graph.triples((_s, _p, _o)):
        if None in (_s, _p, _o):
            c = ctx.push()
        else:
            c = ctx

        if _s is None:
            c[s] = ss

        try:
            if _p is None:
                c[p] = sp
        except AlreadyBound:
            continue

        try:
            if _o is None:
                c[o] = so
        except AlreadyBound:
            continue

        for x in evalBGP(c, bgp[1:]):
            yield x


def evalExtend(ctx, extend):
    # TODO: Deal with dict returned from evalPart from GROUP BY

    for c in evalPart(ctx, extend.p):
        try:
            e = _eval(extend.expr, c.forget(ctx))
            if isinstance(e, SPARQLError):
                raise e

            yield c.merge({extend.var: e})

        except SPARQLError:
            yield c


def evalLazyJoin(ctx, join):
    """
    A lazy join will push the variables bound
    in the first part to the second part,
    essentially doing the join implicitly
    hopefully evaluating much fewer triples
    """
    for a in evalPart(ctx, join.p1):
        c = ctx.thaw(a)
        for b in evalPart(c, join.p2):
            yield b


def evalJoin(ctx, join):

    # TODO: Deal with dict returned from evalPart from GROUP BY
    # only ever for join.p1

    if join.lazy:
        return evalLazyJoin(ctx, join)
    else:
        a = evalPart(ctx, join.p1)
        b = set(evalPart(ctx, join.p2))
        return _join(a, b)


def evalUnion(ctx, union):
    res = set()
    for x in evalPart(ctx, union.p1):
        res.add(x)
        yield x
    for x in evalPart(ctx, union.p2):
        if x not in res:
            yield x


def evalMinus(ctx, minus):
    a = evalPart(ctx, minus.p1)
    b = set(evalPart(ctx, minus.p2))
    return _minus(a, b)


def evalLeftJoin(ctx, join):
    # import pdb; pdb.set_trace()
    for a in evalPart(ctx, join.p1):
        ok = False
        c = ctx.thaw(a)
        for b in evalPart(c, join.p2):
            if _ebv(join.expr, b.forget(ctx)):
                ok = True
                yield b
        if not ok:
            # we've cheated, the ctx above may contain
            # vars bound outside our scope
            # before we yield a solution without the OPTIONAL part
            # check that we would have had no OPTIONAL matches
            # even without prior bindings...
            if not any(_ebv(join.expr, b) for b in
                       evalPart(ctx.thaw(a.remember(join.p1._vars)), join.p2)):

                yield a


def evalFilter(ctx, part):
    # TODO: Deal with dict returned from evalPart!
    for c in evalPart(ctx, part.p):
        if _ebv(part.expr, c.forget(ctx)):
            yield c


def evalGraph(ctx, part):

    if ctx.dataset is None:
        raise Exception(
            "Non-conjunctive-graph doesn't know about " +
            "graphs. Try a query without GRAPH.")

    ctx = ctx.clone()
    graph = ctx[part.term]
    if graph is None:
        
        for graph in ctx.dataset.contexts():

            # in SPARQL the default graph is NOT a named graph
            if graph == ctx.dataset.default_context:
                continue

            c = ctx.pushGraph(graph)
            c = c.push()
            graphSolution = [{part.term: graph.identifier}]
            for x in _join(evalPart(c, part.p), graphSolution): 
                yield x

    else:
        c = ctx.pushGraph(ctx.dataset.get_context(graph))
        for x in evalPart(c, part.p): 
            yield x


def evalValues(ctx, part):
    for r in part.p.res:
        c = ctx.push()
        try:
            for k, v in r.iteritems():
                if v != 'UNDEF':
                    c[k] = v
        except AlreadyBound:
            continue

        yield c.solution()


def evalMultiset(ctx, part):

    if part.p.name == 'values':
        return evalValues(ctx, part)

    return evalPart(ctx, part.p)


def evalPart(ctx, part):

    # try custom evaluation functions
    for name, c in CUSTOM_EVALS.items():
        try:
            return c(ctx, part)
        except NotImplementedError:
            pass  # the given custome-function did not handle this part

    if part.name == 'BGP':
        return evalBGP(ctx, part.triples)  # NOTE pass part.triples, not part!
    elif part.name == 'Filter':
        return evalFilter(ctx, part)
    elif part.name == 'Join':
        return evalJoin(ctx, part)
    elif part.name == 'LeftJoin':
        return evalLeftJoin(ctx, part)
    elif part.name == 'Graph':
        return evalGraph(ctx, part)
    elif part.name == 'Union':
        return evalUnion(ctx, part)
    elif part.name == 'ToMultiSet':
        return evalMultiset(ctx, part)
    elif part.name == 'Extend':
        return evalExtend(ctx, part)
    elif part.name == 'Minus':
        return evalMinus(ctx, part)

    elif part.name == 'Project':
        return evalProject(ctx, part)
    elif part.name == 'Slice':
        return evalSlice(ctx, part)
    elif part.name == 'Distinct':
        return evalDistinct(ctx, part)
    elif part.name == 'Reduced':
        return evalReduced(ctx, part)

    elif part.name == 'OrderBy':
        return evalOrderBy(ctx, part)
    elif part.name == 'Group':
        return evalGroup(ctx, part)
    elif part.name == 'AggregateJoin':
        return evalAggregateJoin(ctx, part)

    elif part.name == 'SelectQuery':
        return evalSelectQuery(ctx, part)
    elif part.name == 'AskQuery':
        return evalAskQuery(ctx, part)
    elif part.name == 'ConstructQuery':
        return evalConstructQuery(ctx, part)

    elif part.name == 'ServiceGraphPattern':
        raise Exception('ServiceGraphPattern not implemented')

    elif part.name == 'DescribeQuery':
        raise Exception('DESCRIBE not implemented')

    else:
        # import pdb ; pdb.set_trace()
        raise Exception('I dont know: %s' % part.name)


def evalGroup(ctx, group):

    """
    http://www.w3.org/TR/sparql11-query/#defn_algGroup
    """

    p = evalPart(ctx, group.p)
    if not group.expr:
        return {1: list(p)}
    else:
        res = collections.defaultdict(list)
        for c in p:
            k = tuple(_eval(e, c) for e in group.expr)
            res[k].append(c)
        return res


def evalAggregateJoin(ctx, agg):
    # import pdb ; pdb.set_trace()
    p = evalPart(ctx, agg.p)
    # p is always a Group, we always get a dict back

    for row in p:
        bindings = {}
        for a in agg.A:
            evalAgg(a, p[row], bindings)

        yield FrozenBindings(ctx, bindings)

    if len(p) == 0:
        yield FrozenBindings(ctx)



def evalOrderBy(ctx, part):

    res = evalPart(ctx, part.p)

    for e in reversed(part.expr):

        def val(x):
            v = value(x, e.expr, variables=True)
            if isinstance(v, Variable):
                return (0, v)
            elif isinstance(v, BNode):
                return (1, v)
            elif isinstance(v, URIRef):
                return (2, v)
            elif isinstance(v, Literal):
                return (3, v)

        reverse = bool(e.order and e.order == 'DESC')
        res = sorted(res, key=val, reverse=reverse)

    return res


def evalSlice(ctx, slice):
    # import pdb; pdb.set_trace()
    res = evalPart(ctx, slice.p)
    i = 0
    while i < slice.start:
        res.next()
        i += 1
    i = 0
    for x in res:
        i += 1
        if slice.length is None:
            yield x
        else:
            if i <= slice.length:
                yield x
            else:
                break


def evalReduced(ctx, part):
    return evalPart(ctx, part.p)  # TODO!


def evalDistinct(ctx, part):
    res = evalPart(ctx, part.p)

    done = set()
    for x in res:
        if x not in done:
            yield x
            done.add(x)


def evalProject(ctx, project):
    res = evalPart(ctx, project.p)

    return (row.project(project.PV) for row in res)


def evalSelectQuery(ctx, query):

    res = {}
    res["type_"] = "SELECT"
    res["bindings"] = evalPart(ctx, query.p)
    res["vars_"] = query.PV
    return res


def evalAskQuery(ctx, query):
    res = {}
    res["type_"] = "ASK"
    res["askAnswer"] = False
    for x in evalPart(ctx, query.p):
        res["askAnswer"] = True
        break

    return res


def evalConstructQuery(ctx, query):
    template = query.template

    if not template:
        # a construct-where query
        template = query.p.p.triples  # query->project->bgp ...

    graph = Graph()

    for c in evalPart(ctx, query.p):
        graph += _fillTemplate(template, c)

    res = {}
    res["type_"] = "CONSTRUCT"
    res["graph"] = graph

    return res


def evalQuery(graph, query, initBindings, base=None):
    ctx = QueryContext(graph)

    ctx.prologue = query.prologue

    if initBindings:
        for k, v in initBindings.iteritems():
            if not isinstance(k, Variable):
                k = Variable(k)
            ctx[k] = v
        # ctx.push()  # nescessary?

    main = query.algebra

    # import pdb; pdb.set_trace()
    if main.datasetClause:
        if ctx.dataset is None:
            raise Exception(
                "Non-conjunctive-graph doesn't know about " +
                "graphs! Try a query without FROM (NAMED).")

        ctx = ctx.clone()  # or push/pop?

        firstDefault = False
        for d in main.datasetClause:
            if d.default:

                if firstDefault:
                    # replace current default graph
                    dg = ctx.dataset.get_context(BNode())
                    ctx = ctx.pushGraph(dg)
                    firstDefault = True

                ctx.load(d.default, default=True)

            elif d.named:
                g = d.named
                ctx.load(g, default=False)

    return evalPart(ctx, main)

########NEW FILE########
__FILENAME__ = evalutils
import collections

from rdflib.term import Variable, Literal, BNode, URIRef

from rdflib.plugins.sparql.operators import EBV
from rdflib.plugins.sparql.parserutils import Expr, CompValue
from rdflib.plugins.sparql.sparql import SPARQLError, NotBoundError


def _diff(a, b, expr):
    res = set()

    for x in a:
        if all(not x.compatible(y) or not _ebv(expr, x.merge(y)) for y in b):
            res.add(x)

    return res


def _minus(a, b):
    for x in a:
        if all((not x.compatible(y)) or x.disjointDomain(y) for y in b):
            yield x



def _join(a, b):
    for x in a:
        for y in b:
            if x.compatible(y):
                yield x.merge(y)


def _ebv(expr, ctx):

    """
    Return true/false for the given expr
    Either the expr is itself true/false
    or evaluates to something, with the given ctx

    an error is false
    """

    try:
        return EBV(expr)
    except SPARQLError:
        pass
    if isinstance(expr, Expr):
        try:
            return EBV(expr.eval(ctx))
        except SPARQLError:
            return False  # filter error == False
    elif isinstance(expr, CompValue):
        raise Exception(
            "Weird - filter got a CompValue without evalfn! %r" % expr)
    elif isinstance(expr, Variable):
        try:
            return EBV(ctx[expr])
        except:
            return False
    return False


def _eval(expr, ctx):
    if isinstance(expr, (Literal, URIRef)):
        return expr
    if isinstance(expr, Expr):
        return expr.eval(ctx)
    elif isinstance(expr, Variable):
        try:
            return ctx[expr]
        except KeyError:
            return NotBoundError("Variable %s is not bound" % expr)
    elif isinstance(expr, CompValue):
        raise Exception(
            "Weird - _eval got a CompValue without evalfn! %r" % expr)
    else:
        raise Exception("Cannot eval thing: %s (%s)" % (expr, type(expr)))


def _filter(a, expr):
    for c in a:
        if _ebv(expr, c):
            yield c


def _fillTemplate(template, solution):

    """
    For construct/deleteWhere and friends

    Fill a triple template with instantiated variables
    """

    bnodeMap = collections.defaultdict(BNode)
    for t in template:
        s, p, o = t

        _s = solution.get(s)
        _p = solution.get(p)
        _o = solution.get(o)

        # instantiate new bnodes for each solution
        _s, _p, _o = [bnodeMap[x] if isinstance(
            x, BNode) else y for x, y in zip(t, (_s, _p, _o))]

        if _s is not None and \
                _p is not None and \
                _o is not None:

            yield (_s, _p, _o)

########NEW FILE########
__FILENAME__ = operators
"""
This contains evaluation functions for expressions

They get bound as instances-methods to the CompValue objects from parserutils
using setEvalFn

"""

import re
import math
import random
import uuid
import hashlib
import urllib2

from decimal import Decimal, ROUND_HALF_UP, InvalidOperation

import operator as pyop  # python operators

import isodate

from rdflib.plugins.sparql.parserutils import CompValue, Expr
from rdflib.plugins.sparql.datatypes import XSD_DTs, type_promotion
from rdflib import URIRef, BNode, Variable, Literal, XSD, RDF
from rdflib.term import Node

from pyparsing import ParseResults

from rdflib.plugins.sparql.sparql import SPARQLError, SPARQLTypeError


# closed namespace, langString isn't in it
RDF_langString = URIRef(RDF.uri + "langString")


def Builtin_IRI(expr, ctx):
    """
    http://www.w3.org/TR/sparql11-query/#func-iri
    """

    a = expr.arg

    if isinstance(a, URIRef):
        return a
    if isinstance(a, Literal):
        return ctx.prologue.absolutize(URIRef(a))

    raise SPARQLError('IRI function only accepts URIRefs or Literals/Strings!')


def Builtin_isBLANK(expr, ctx):
    return Literal(isinstance(expr.arg, BNode))


def Builtin_isLITERAL(expr, ctx):
    return Literal(isinstance(expr.arg, Literal))


def Builtin_isIRI(expr, ctx):
    return Literal(isinstance(expr.arg, URIRef))


def Builtin_isNUMERIC(expr, ctx):
    try:
        numeric(expr.arg)
        return Literal(True)
    except:
        return Literal(False)


def Builtin_BNODE(expr, ctx):
    """
    http://www.w3.org/TR/sparql11-query/#func-bnode
    """

    a = expr.arg

    if a is None:
        return BNode()

    if isinstance(a, Literal):
        return ctx.bnodes[a]  # defaultdict does the right thing

    raise SPARQLError(
        'BNode function only accepts no argument or literal/string')


def Builtin_ABS(expr, ctx):
    """
    http://www.w3.org/TR/sparql11-query/#func-abs
    """

    return Literal(abs(numeric(expr.arg)))


def Builtin_IF(expr, ctx):
    """
    http://www.w3.org/TR/sparql11-query/#func-if
    """

    return expr.arg2 if EBV(expr.arg1) else expr.arg3


def Builtin_RAND(expr, ctx):
    """
    http://www.w3.org/TR/sparql11-query/#idp2133952
    """

    return Literal(random.random())


def Builtin_UUID(expr, ctx):
    """
    http://www.w3.org/TR/sparql11-query/#func-strdt
    """

    return URIRef(uuid.uuid4().urn)


def Builtin_STRUUID(expr, ctx):
    """
    http://www.w3.org/TR/sparql11-query/#func-strdt
    """

    return Literal(str(uuid.uuid4()))


def Builtin_MD5(expr, ctx):
    s = string(expr.arg).encode("utf-8")
    return Literal(hashlib.md5(s).hexdigest())


def Builtin_SHA1(expr, ctx):
    s = string(expr.arg).encode("utf-8")
    return Literal(hashlib.sha1(s).hexdigest())


def Builtin_SHA256(expr, ctx):
    s = string(expr.arg).encode("utf-8")
    return Literal(hashlib.sha256(s).hexdigest())


def Builtin_SHA384(expr, ctx):
    s = string(expr.arg).encode("utf-8")
    return Literal(hashlib.sha384(s).hexdigest())


def Builtin_SHA512(expr, ctx):
    s = string(expr.arg).encode("utf-8")
    return Literal(hashlib.sha512(s).hexdigest())


def Builtin_COALESCE(expr, ctx):
    """
    http://www.w3.org/TR/sparql11-query/#func-coalesce
    """
    for x in expr.get('arg', variables=True):
        if x is not None and not isinstance(x, (SPARQLError, Variable)):
            return x
    raise SPARQLError(
        "COALESCE got no arguments that did not evaluate to an error")


def Builtin_CEIL(expr, ctx):
    """
    http://www.w3.org/TR/sparql11-query/#func-ceil
    """

    l = expr.arg
    return Literal(int(math.ceil(numeric(l))), datatype=l.datatype)


def Builtin_FLOOR(expr, ctx):
    """
    http://www.w3.org/TR/sparql11-query/#func-floor
    """
    l = expr.arg
    return Literal(int(math.floor(numeric(l))), datatype=l.datatype)


def Builtin_ROUND(expr, ctx):
    """
    http://www.w3.org/TR/sparql11-query/#func-round
    """

    # This used to be just math.bound
    # but in py3k bound was changed to
    # "round-to-even" behaviour
    # this is an ugly work-around
    l = expr.arg
    v = numeric(l)
    v = int(Decimal(v).quantize(1, ROUND_HALF_UP))
    return Literal(v, datatype=l.datatype)


def Builtin_REGEX(expr, ctx):
    """
    http://www.w3.org/TR/sparql11-query/#func-regex
    Invokes the XPath fn:matches function to match text against a regular
    expression pattern.
    The regular expression language is defined in XQuery 1.0 and XPath 2.0
    Functions and Operators section 7.6.1 Regular Expression Syntax
    """

    text = string(expr.text)
    pattern = string(expr.pattern)
    flags = expr.flags

    cFlag = 0
    if flags:
        # Maps XPath REGEX flags (http://www.w3.org/TR/xpath-functions/#flags)
        # to Python's re flags
        flagMap = dict(
            [('i', re.IGNORECASE), ('s', re.DOTALL), ('m', re.MULTILINE)])
        cFlag = reduce(pyop.or_, [flagMap.get(f, 0) for f in flags])

    return Literal(bool(re.search(unicode(pattern), text, cFlag)))


def Builtin_REPLACE(expr, ctx):
    """
    http://www.w3.org/TR/sparql11-query/#func-substr
    """
    text = string(expr.arg)
    pattern = string(expr.pattern)
    replacement = string(expr.replacement)
    flags = expr.flags

    # python uses \1, xpath/sparql uses $1
    replacement = re.sub('\\$([0-9]*)', r'\\\1', replacement)

    def _r(m):

        # Now this is ugly.
        # Python has a "feature" where unmatched groups return None
        # then re.sub chokes on this.
        # see http://bugs.python.org/issue1519638

        # this works around and hooks into the internal of the re module...

        # the match object is replaced with a wrapper that
        # returns "" instead of None for unmatched groups

        class _m():
            def __init__(self, m):
                self.m = m
                self.string = m.string

            def group(self, n):
                return m.group(n) or ""

        return re._expand(pattern, _m(m), replacement)

    cFlag = 0
    if flags:
        # Maps XPath REGEX flags (http://www.w3.org/TR/xpath-functions/#flags)
        # to Python's re flags
        flagMap = dict(
            [('i', re.IGNORECASE), ('s', re.DOTALL), ('m', re.MULTILINE)])
        cFlag = reduce(pyop.or_, [flagMap.get(f, 0) for f in flags])

        # @@FIXME@@ either datatype OR lang, NOT both
    return Literal(re.sub(unicode(pattern), _r, text, cFlag),
                   datatype=text.datatype, lang=text.language)


def Builtin_STRDT(expr, ctx):
    """
    http://www.w3.org/TR/sparql11-query/#func-strdt
    """

    return Literal(unicode(expr.arg1), datatype=expr.arg2)


def Builtin_STRLANG(expr, ctx):
    """
    http://www.w3.org/TR/sparql11-query/#func-strlang
    """

    s = string(expr.arg1)
    if s.language or s.datatype:
        raise SPARQLError('STRLANG expects a simple literal')

    # TODO: normalisation of lang tag to lower-case
    # should probably happen in literal __init__
    return Literal(unicode(s), lang=str(expr.arg2).lower())


def Builtin_CONCAT(expr, ctx):
    """
    http://www.w3.org/TR/sparql11-query/#func-concat
    """

    # dt/lang passed on only if they all match

    dt = set(x.datatype for x in expr.arg)
    dt = dt.pop() if len(dt) == 1 else None

    lang = set(x.language for x in expr.arg)
    lang = lang.pop() if len(lang) == 1 else None

    return Literal("".join(string(x)
                           for x in expr.arg), datatype=dt, lang=lang)


def _compatibleStrings(a, b):
    string(a)
    string(b)

    if b.language and a.language != b.language:
        raise SPARQLError('incompatible arguments to str functions')


def Builtin_STRSTARTS(expr, ctx):
    """
    http://www.w3.org/TR/sparql11-query/#func-strstarts
    """

    a = expr.arg1
    b = expr.arg2
    _compatibleStrings(a, b)

    return Literal(a.startswith(b))


def Builtin_STRENDS(expr, ctx):
    """
    http://www.w3.org/TR/sparql11-query/#func-strends
    """
    a = expr.arg1
    b = expr.arg2

    _compatibleStrings(a, b)

    return Literal(a.endswith(b))


def Builtin_STRBEFORE(expr, ctx):
    """
    http://www.w3.org/TR/sparql11-query/#func-strbefore
    """

    a = expr.arg1
    b = expr.arg2
    _compatibleStrings(a, b)

    i = a.find(b)
    if i == -1:
        return Literal("")
    else:
        return Literal(a[:i], lang=a.language, datatype=a.datatype)


def Builtin_STRAFTER(expr, ctx):
    """
    http://www.w3.org/TR/sparql11-query/#func-strafter
    """

    a = expr.arg1
    b = expr.arg2
    _compatibleStrings(a, b)

    i = a.find(b)
    if i == -1:
        return Literal("")
    else:
        return Literal(a[i + len(b):], lang=a.language, datatype=a.datatype)


def Builtin_CONTAINS(expr, ctx):
    """
    http://www.w3.org/TR/sparql11-query/#func-strcontains
    """

    a = expr.arg1
    b = expr.arg2
    _compatibleStrings(a, b)

    return Literal(b in a)


def Builtin_ENCODE_FOR_URI(expr, ctx):
    return Literal(urllib2.quote(string(expr.arg).encode("utf-8")))


def Builtin_SUBSTR(expr, ctx):
    """
    http://www.w3.org/TR/sparql11-query/#func-substr
    """

    a = string(expr.arg)

    start = numeric(expr.start) - 1

    length = expr.length
    if length is not None:
        length = numeric(length) + start

    return Literal(a[start:length], lang=a.language, datatype=a.datatype)


def Builtin_STRLEN(e, ctx):
    l = string(e.arg)

    return Literal(len(l))


def Builtin_STR(e, ctx):
    arg = e.arg
    if isinstance(arg, SPARQLError):
        raise arg
    return Literal(unicode(arg))  # plain literal


def Builtin_LCASE(e, ctx):
    l = string(e.arg)

    return Literal(l.lower(), datatype=l.datatype, lang=l.language)


def Builtin_LANGMATCHES(e, ctx):
    """
    http://www.w3.org/TR/sparql11-query/#func-langMatches


    """
    langTag = string(e.arg1)
    langRange = string(e.arg2)

    if unicode(langTag) == "":
        return Literal(False)  # nothing matches empty!

    return Literal(_lang_range_check(langRange, langTag))


def Builtin_NOW(e, ctx):
    """
    http://www.w3.org/TR/sparql11-query/#func-now
    """
    return Literal(ctx.now)


def Builtin_YEAR(e, ctx):
    d = datetime(e.arg)
    return Literal(d.year)


def Builtin_MONTH(e, ctx):
    d = datetime(e.arg)
    return Literal(d.month)


def Builtin_DAY(e, ctx):
    d = datetime(e.arg)
    return Literal(d.day)


def Builtin_HOURS(e, ctx):
    d = datetime(e.arg)
    return Literal(d.hour)


def Builtin_MINUTES(e, ctx):
    d = datetime(e.arg)
    return Literal(d.minute)


def Builtin_SECONDS(e, ctx):
    """
    http://www.w3.org/TR/sparql11-query/#func-seconds
    """
    d = datetime(e.arg)
    return Literal(d.second, datatype=XSD.decimal)


def Builtin_TIMEZONE(e, ctx):
    """
    http://www.w3.org/TR/sparql11-query/#func-timezone

    :returns: the timezone part of arg as an xsd:dayTimeDuration.
    :raises: an error if there is no timezone.
    """
    dt = datetime(e.arg)
    if not dt.tzinfo:
        raise SPARQLError('datatime has no timezone: %s' % dt)

    delta = dt.tzinfo.utcoffset(ctx.now)

    d = delta.days
    s = delta.seconds
    neg = ""

    if d < 0:
        s = -24 * 60 * 60 * d - s
        d = 0
        neg = "-"

    h = s / (60 * 60)
    m = (s - h * 60 * 60) / 60
    s = s - h * 60 * 60 - m * 60

    tzdelta = "%sP%sT%s%s%s" % (neg,
                                "%dD" % d if d else "",
                                "%dH" % h if h else "",
                                "%dM" % m if m else "",
                                "%dS" % s if not d and not h and not m else "")

    return Literal(tzdelta, datatype=XSD.dayTimeDuration)


def Builtin_TZ(e, ctx):
    d = datetime(e.arg)
    if not d.tzinfo:
        return Literal("")
    n = d.tzinfo.tzname(d)
    if n == "UTC":
        n = "Z"
    return Literal(n)


def Builtin_UCASE(e, ctx):
    l = string(e.arg)

    return Literal(l.upper(), datatype=l.datatype, lang=l.language)


def Builtin_LANG(e, ctx):

    """
    http://www.w3.org/TR/sparql11-query/#func-lang

    Returns the language tag of ltrl, if it has one. It returns "" if ltrl has
    no language tag. Note that the RDF data model does not include literals
    with an empty language tag.
    """

    l = literal(e.arg)
    return Literal(l.language or "")


def Builtin_DATATYPE(e, ctx):
    l = e.arg
    if not isinstance(l, Literal):
        raise SPARQLError('Can only get datatype of literal: %s' % l)
    if l.language:
        return RDF_langString
    if not l.datatype and not l.language:
        return XSD.string
    return l.datatype


def Builtin_sameTerm(e, ctx):
    a = e.arg1
    b = e.arg2
    return Literal(a == b)


def Builtin_BOUND(e, ctx):
    """
    http://www.w3.org/TR/sparql11-query/#func-bound
    """
    n = e.get('arg', variables=True)

    return Literal(not isinstance(n, Variable))


def Builtin_EXISTS(e, ctx):
    # damn...
    from rdflib.plugins.sparql.evaluate import evalPart

    exists = e.name == 'Builtin_EXISTS'

    ctx = ctx.ctx.thaw(ctx)  # hmm
    for x in evalPart(ctx, e.graph):
        return Literal(exists)
    return Literal(not exists)


def Function(e, ctx):
    """
    Custom functions (and casts!)
    """

    if e.iri in XSD_DTs:
        # a cast

        if not e.expr:
            raise SPARQLError("Nothing given to cast.")
        if len(e.expr) > 1:
            raise SPARQLError("Cannot cast more than one thing!")

        x = e.expr[0]

        if e.iri == XSD.string:

            if isinstance(x, (URIRef, Literal)):
                return Literal(x, datatype=XSD.string)
            else:
                raise SPARQLError(
                    "Cannot cast term %s of type %s" % (x, type(x)))

        if not isinstance(x, Literal):
            raise SPARQLError(
                "Can only cast Literals to non-string data-types")

        if x.datatype and not x.datatype in XSD_DTs:
            raise SPARQLError(
                "Cannot cast literal with unknown datatype: %s" % x.datatype)

        if e.iri == XSD.dateTime:
            if x.datatype and x.datatype not in (XSD.dateTime, XSD.string):
                raise SPARQLError(
                    "Cannot cast %s to XSD:dateTime" % x.datatype)
            try:
                return Literal(isodate.parse_datetime(x), datatype=e.iri)
            except:
                raise SPARQLError("Cannot interpret '%s' as datetime" % x)

        if x.datatype == XSD.dateTime:
            raise SPARQLError("Cannot cast XSD.dateTime to %s" % e.iri)

        if e.iri in (XSD.float, XSD.double):
            try:
                return Literal(float(x), datatype=e.iri)
            except:
                raise SPARQLError("Cannot interpret '%s' as float" % x)

        elif e.iri == XSD.decimal:
            if "e" in x or "E" in x:  # SPARQL/XSD does not allow exponents in decimals
                raise SPARQLError("Cannot interpret '%s' as decimal" % x)
            try:
                return Literal(Decimal(x), datatype=e.iri)
            except:
                raise SPARQLError("Cannot interpret '%s' as decimal" % x)

        elif e.iri == XSD.integer:
            try:
                return Literal(int(x), datatype=XSD.integer)
            except:
                raise SPARQLError("Cannot interpret '%s' as int" % x)

        elif e.iri == XSD.boolean:
            # # I would argue that any number is True...
            # try:
            #     return Literal(bool(int(x)), datatype=XSD.boolean)
            # except:
            if x.lower() in ("1", "true"):
                return Literal(True)
            if x.lower() in ("0", "false"):
                return Literal(False)

            raise SPARQLError("Cannot interpret '%s' as bool" % x)
        else:
            raise Exception("I do not know how to cast to %s" % e.iri)

    else:
        raise SPARQLError('Unknown function %s"%e.iri')

    # TODO: Custom functions!


def UnaryNot(expr, ctx):
    return Literal(not EBV(expr.expr))


def UnaryMinus(expr, ctx):
    return Literal(-numeric(expr.expr))


def UnaryPlus(expr, ctx):
    return Literal(+numeric(expr.expr))


def MultiplicativeExpression(e, ctx):

    expr = e.expr
    other = e.other

    # because of the way the mul-expr production handled operator precedence
    # we sometimes have nothing to do
    if other is None:
        return expr
    try:
        res = Decimal(numeric(expr))
        for op, f in zip(e.op, other):
            f = numeric(f)

            if type(f) == float:
                res = float(res)

            if op == '*':
                res *= f
            else:
                res /= f
    except (InvalidOperation, ZeroDivisionError):
        raise SPARQLError('divide by 0')

    return Literal(res)


def AdditiveExpression(e, ctx):

    expr = e.expr
    other = e.other

    # because of the way the add-expr production handled operator precedence
    # we sometimes have nothing to do
    if other is None:
        return expr

    res = numeric(expr)

    dt = expr.datatype

    for op, term in zip(e.op, other):
        n = numeric(term)
        if isinstance(n, Decimal) and isinstance(res, float):
            n = float(n)
        if isinstance(n, float) and isinstance(res, Decimal):
            res = float(res)

        dt = type_promotion(dt, term.datatype)

        if op == '+':
            res += n
        else:
            res -= n

    return Literal(res, datatype=dt)


def RelationalExpression(e, ctx):

    expr = e.expr
    other = e.other
    op = e.op

    # because of the way the add-expr production handled operator precedence
    # we sometimes have nothing to do
    if other is None:
        return expr

    ops = dict([('>', lambda x, y: x.__gt__(y)),
                ('<', lambda x, y: x.__lt__(y)),
                ('=', lambda x, y: x.eq(y)),
                ('!=', lambda x, y: x.neq(y)),
                ('>=', lambda x, y: x.__ge__(y)),
                ('<=', lambda x, y: x.__le__(y)),
                ('IN', pyop.contains),
                ('NOT IN', lambda x, y: not pyop.contains(x, y))])

    if op in ('IN', 'NOT IN'):

        res = (op == 'NOT IN')

        error = False

        if other == RDF.nil:
            other = []

        for x in other:
            try:
                if x == expr:
                    return Literal(True ^ res)
            except SPARQLError, e:
                error = e
        if not error:
            return Literal(False ^ res)
        else:
            raise error

    if not op in ('=', '!=', 'IN', 'NOT IN'):
        if not isinstance(expr, Literal):
            raise SPARQLError(
                "Compare other than =, != of non-literals is an error: %s" %
                expr)
        if not isinstance(other, Literal):
            raise SPARQLError(
                "Compare other than =, != of non-literals is an error: %s" %
                other)
    else:
        if not isinstance(expr, Node):
            raise SPARQLError('I cannot compare this non-node: %s' % expr)
        if not isinstance(other, Node):
            raise SPARQLError('I cannot compare this non-node: %s' % other)

    if isinstance(expr, Literal) and isinstance(other, Literal):

        if expr.datatype != None and expr.datatype not in XSD_DTs and other.datatype != None and other.datatype not in XSD_DTs:
            # in SPARQL for non-XSD DT Literals we can only do =,!=
            if op not in ('=', '!='):
                raise SPARQLError(
                    'Can only do =,!= comparisons of non-XSD Literals')

    try:
        r = ops[op](expr, other)
        if r == NotImplemented:
            raise SPARQLError('Error when comparing')
    except TypeError, te:
        raise SPARQLError(*te.args)
    return Literal(r)


def ConditionalAndExpression(e, ctx):

    # TODO: handle returned errors

    expr = e.expr
    other = e.other

    # because of the way the add-expr production handled operator precedence
    # we sometimes have nothing to do
    if other is None:
        return expr

    return Literal(all(EBV(x) for x in [expr] + other))


def ConditionalOrExpression(e, ctx):

    # TODO: handle errors

    expr = e.expr
    other = e.other

    # because of the way the add-expr production handled operator precedence
    # we sometimes have nothing to do
    if other is None:
        return expr
    # A logical-or that encounters an error on only one branch
    # will return TRUE if the other branch is TRUE and an error
    # if the other branch is FALSE.
    error = None
    for x in [expr] + other:
        try:
            if EBV(x):
                return Literal(True)
        except SPARQLError, e:
            error = e
    if error:
        raise error
    return Literal(False)


def not_(arg):
    return Expr('UnaryNot', UnaryNot, expr=arg)


def and_(*args):
    if len(args) == 1:
        return args[0]

    return Expr('ConditionalAndExpression', ConditionalAndExpression,
                expr=args[0], other=list(args[1:]))

TrueFilter = Expr('TrueFilter', lambda _1, _2: Literal(True))


def simplify(expr):
    if isinstance(expr, ParseResults) and len(expr) == 1:
        return simplify(expr[0])

    if isinstance(expr, (list, ParseResults)):
        return map(simplify, expr)
    if not isinstance(expr, CompValue):
        return expr
    if expr.name.endswith('Expression'):
        if expr.other is None:
            return simplify(expr.expr)

    for k in expr.keys():
        expr[k] = simplify(expr[k])
        # expr['expr']=simplify(expr.expr)
        #    expr['other']=simplify(expr.other)

    return expr


def literal(s):
    if not isinstance(s, Literal):
        raise SPARQLError("Non-literal passed as string: %s" % s)
    return s


def datetime(e):
    if not isinstance(e, Literal):
        raise SPARQLError("Non-literal passed as datetime: %s" % e)
    if not e.datatype == XSD.dateTime:
        raise SPARQLError(
            "Literal with wrong datatype passed as datetime: %s" % e)
    return e.toPython()


def string(s):
    """
    Make sure the passed thing is a string literal
    i.e. plain literal, xsd:string literal or lang-tagged literal
    """
    if not isinstance(s, Literal):
        raise SPARQLError("Non-literal passes as string: %s" % s)
    if s.datatype and s.datatype != XSD.string:
        raise SPARQLError(
            "Non-string datatype-literal passes as string: %s" % s)
    return s


def numeric(expr):
    """
    return a number from a literal
    http://www.w3.org/TR/xpath20/#promotion

    or TypeError
    """

    if not isinstance(expr, Literal):
        raise SPARQLTypeError("%s is not a literal!" % expr)

    if expr.datatype not in (XSD.float, XSD.double,
                             XSD.decimal, XSD.integer,
                             XSD.nonPositiveInteger, XSD.negativeInteger,
                             XSD.nonNegativeInteger, XSD.positiveInteger,
                             XSD.unsignedLong, XSD.unsignedInt,
                             XSD.unsignedShort, XSD.unsignedByte,
                             XSD.long, XSD.int, XSD.short, XSD.byte):
        raise SPARQLTypeError("%s does not have a numeric datatype!" % expr)

    return expr.toPython()


def EBV(rt):
    """
    * If the argument is a typed literal with a datatype of xsd:boolean,
      the EBV is the value of that argument.
    * If the argument is a plain literal or a typed literal with a
      datatype of xsd:string, the EBV is false if the operand value
      has zero length; otherwise the EBV is true.
    * If the argument is a numeric type or a typed literal with a datatype
      derived from a numeric type, the EBV is false if the operand value is
      NaN or is numerically equal to zero; otherwise the EBV is true.
    * All other arguments, including unbound arguments, produce a type error.

    """

    if isinstance(rt, Literal):

        if rt.datatype == XSD.boolean:
            return rt.toPython()

        elif rt.datatype == XSD.string or rt.datatype is None:
            return len(rt) > 0

        else:
            pyRT = rt.toPython()

            if isinstance(pyRT, Literal):
                # Type error, see: http://www.w3.org/TR/rdf-sparql-query/#ebv
                raise SPARQLTypeError(
                    "http://www.w3.org/TR/rdf-sparql-query/#ebv - ' + \
                    'Could not determine the EBV for : %s" % rt)
            else:
                return bool(pyRT)

    else:
        raise SPARQLTypeError(
            "http://www.w3.org/TR/rdf-sparql-query/#ebv - ' + \
            'Only literals have Boolean values! %s" % rt)


def _lang_range_check(range, lang):
    """
    Implementation of the extended filtering algorithm, as defined in point
    3.3.2, of U{RFC 4647<http://www.rfc-editor.org/rfc/rfc4647.txt>}, on
    matching language ranges and language tags.
    Needed to handle the C{rdf:PlainLiteral} datatype.
    @param range: language range
    @param lang: language tag
    @rtype: boolean

        @author: U{Ivan Herman<a href="http://www.w3.org/People/Ivan/">}

        Taken from `RDFClosure/RestrictedDatatype.py`__

    .. __:http://dev.w3.org/2004/PythonLib-IH/RDFClosure/RestrictedDatatype.py

    """
    def _match(r, l):
        """
        Matching of a range and language item: either range is a wildcard
        or the two are equal
        @param r: language range item
        @param l: language tag item
        @rtype: boolean
        """
        return r == '*' or r == l

    rangeList = range.strip().lower().split('-')
    langList = lang.strip().lower().split('-')
    if not _match(rangeList[0], langList[0]):
        return False
    if len(rangeList) > len(langList):
        return False

    return all(_match(*x) for x in zip(rangeList, langList))

########NEW FILE########
__FILENAME__ = parser
"""
SPARQL 1.1 Parser

based on pyparsing
"""

import sys
import re

from pyparsing import (
    Literal, Regex, Optional, OneOrMore, ZeroOrMore, Forward,
    ParseException, Suppress, Combine, restOfLine, Group,
    ParseResults, delimitedList)
from pyparsing import CaselessKeyword as Keyword  # watch out :)
# from pyparsing import Keyword as CaseSensitiveKeyword

from parserutils import Comp, Param, ParamList

from . import operators as op
from rdflib.py3compat import decodeUnicodeEscape, bytestype

import rdflib

DEBUG = False

# ---------------- ACTIONS


def neg(literal):
    return rdflib.Literal(-literal, datatype=literal.datatype)


def setLanguage(terms):
    return rdflib.Literal(terms[0], lang=terms[1])


def setDataType(terms):
    return rdflib.Literal(terms[0], datatype=terms[1])


def expandTriples(terms):

    """
    Expand ; and , syntax for repeat predicates, subjects
    """
    # import pdb; pdb.set_trace()
    try:
        res = []
        if DEBUG:
            print "Terms", terms
        l = len(terms)
        for i, t in enumerate(terms):
            if t == ',':
                res.append(res[i - 3])
                res.append(res[i - 2])
            elif t == ';':
                res.append(res[i - 3])
            elif isinstance(t, list):
                # BlankNodePropertyList
                # is this bnode the object of previous triples?
                if (i % 3) == 2:
                    res.append(t[0])
                # is this a single [] ?
                if len(t) > 1:
                    res += t
                # is this bnode the subject of more triples?
                if i + 1 < l and terms[i + 1] not in ".,;" :
                    res.append(t[0])
            elif isinstance(t, ParseResults):
                res += t.asList()
            elif t != '.':
                res.append(t)

        return res
        # print res
        # assert len(res)%3 == 0, \
        #       "Length of triple-list is not divisible by 3: %d!"%len(res)

        # return [tuple(res[i:i+3]) for i in range(len(res)/3)]
    except:
        if DEBUG:
            import traceback
            traceback.print_exc()
        raise


def expandBNodeTriples(terms):
    """
    expand [ ?p ?o ] syntax for implicit bnodes
    """
    # import pdb; pdb.set_trace()
    try:
        if DEBUG:
            print "Bnode terms", terms
            print "1", terms[0]
            print "2", [rdflib.BNode()] + terms.asList()[0]
        return [expandTriples([rdflib.BNode()] + terms.asList()[0])]
    except Exception, e:
        if DEBUG:
            print ">>>>>>>>", e
        raise


def expandCollection(terms):
    """
    expand ( 1 2 3 ) notation for collections
    """
    if DEBUG:
        print "Collection: ", terms

    res = []
    other = []
    for x in terms:
        if isinstance(x, list):  # is this a [ .. ] ?
            other += x
            x = x[0]

        b = rdflib.BNode()
        if res:
            res += [res[-3], rdflib.RDF.rest, b, b, rdflib.RDF.first, x]
        else:
            res += [b, rdflib.RDF.first, x]
    res += [b, rdflib.RDF.rest, rdflib.RDF.nil]

    res += other

    if DEBUG:
        print "CollectionOut", res
    return [res]


# SPARQL Grammar from http://www.w3.org/TR/sparql11-query/#grammar
# ------ TERMINALS --------------
# [139] IRIREF ::= '<' ([^<>"{}|^`\]-[#x00-#x20])* '>'
IRIREF = Combine(Suppress('<') + Regex(r'[^<>"{}|^`\\%s]*' % ''.join(
    '\\x%02X' % i for i in range(33))) + Suppress('>'))
IRIREF.setParseAction(lambda x: rdflib.URIRef(x[0]))

# [164] P_CHARS_BASE ::= [A-Z] | [a-z] | [#x00C0-#x00D6] | [#x00D8-#x00F6] | [#x00F8-#x02FF] | [#x0370-#x037D] | [#x037F-#x1FFF] | [#x200C-#x200D] | [#x2070-#x218F] | [#x2C00-#x2FEF] | [#x3001-#xD7FF] | [#xF900-#xFDCF] | [#xFDF0-#xFFFD] | [#x10000-#xEFFFF]

if sys.maxunicode == 0xffff:
    # this is narrow python build (default on windows/osx)
    # this means that unicode code points over 0xffff are stored
    # as several characters, which in turn means that regex character
    # ranges with these characters do not work.
    # See
    # * http://bugs.python.org/issue12729
    # * http://bugs.python.org/issue12749
    # * http://bugs.python.org/issue3665
    #
    # Here we simple skip the [#x10000-#xEFFFF] part
    # this means that some SPARQL queries will not parse :(
    # We *could* generate a new regex with \U00010000|\U00010001 ...
    # but it would be quite long :)
    #
    # in py3.3 this is fixed

    PN_CHARS_BASE_re = u'A-Za-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD'
else:
    # wide python build
    PN_CHARS_BASE_re = u'A-Za-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD\U00010000-\U000EFFFF'

# [165] PN_CHARS_U ::= PN_CHARS_BASE | '_'
PN_CHARS_U_re = '_' + PN_CHARS_BASE_re

# [167] PN_CHARS ::= PN_CHARS_U | '-' | [0-9] | #x00B7 | [#x0300-#x036F] | [#x203F-#x2040]
PN_CHARS_re = u'\\-0-9\u00B7\u0300-\u036F\u203F-\u2040' + PN_CHARS_U_re
# PN_CHARS = Regex(u'[%s]'%PN_CHARS_re, flags=re.U)

# [168] PN_PREFIX ::= PN_CHARS_BASE ((PN_CHARS|'.')* PN_CHARS)?
PN_PREFIX = Regex(ur'[%s](?:[%s\.]*[%s])?' % (PN_CHARS_BASE_re,
                  PN_CHARS_re, PN_CHARS_re), flags=re.U)

# [140] PNAME_NS ::= PN_PREFIX? ':'
PNAME_NS = Optional(
    Param('prefix', PN_PREFIX)) + Suppress(':').leaveWhitespace()

# [173] PN_LOCAL_ESC ::= '\' ( '_' | '~' | '.' | '-' | '!' | '$' | '&' | "'" | '(' | ')' | '*' | '+' | ',' | ';' | '=' | '/' | '?' | '#' | '@' | '%' )

PN_LOCAL_ESC_re = '\\\\[_~\\.\\-!$&"\'()*+,;=/?#@%]'
#PN_LOCAL_ESC = Regex(PN_LOCAL_ESC_re) # regex'd
#PN_LOCAL_ESC.setParseAction(lambda x: x[0][1:])

# [172] HEX ::= [0-9] | [A-F] | [a-f]
# HEX = Regex('[0-9A-Fa-f]') # not needed

# [171] PERCENT ::= '%' HEX HEX
PERCENT_re = '%[0-9a-fA-F]{2}'
#PERCENT = Regex(PERCENT_re) # regex'd
#PERCENT.setParseAction(lambda x: unichr(int(x[0][1:], 16)))

# [170] PLX ::= PERCENT | PN_LOCAL_ESC
PLX_re = '(%s|%s)'%(PN_LOCAL_ESC_re,PERCENT_re)
#PLX = PERCENT | PN_LOCAL_ESC # regex'd


# [169] PN_LOCAL ::= (PN_CHARS_U | ':' | [0-9] | PLX ) ((PN_CHARS | '.' | ':' | PLX)* (PN_CHARS | ':' | PLX) )?

PN_LOCAL = Regex(ur"""([%(PN_CHARS_U)s:0-9]|%(PLX)s)
                     (([%(PN_CHARS)s\.:]|%(PLX)s)*
                      ([%(PN_CHARS)s:]|%(PLX)s) )?"""%dict(PN_CHARS_U=PN_CHARS_U_re,
                                                       PN_CHARS=PN_CHARS_re,
                                                         PLX=PLX_re), flags=re.X|re.UNICODE)

def _hexExpand(match):
    return unichr(int(match.group(0)[1:], 16))

PN_LOCAL.setParseAction(lambda x: re.sub("(%s)"%PERCENT_re, _hexExpand, x[0]))




# [141] PNAME_LN ::= PNAME_NS PN_LOCAL
PNAME_LN = PNAME_NS + Param('localname', PN_LOCAL.leaveWhitespace())

# [142] BLANK_NODE_LABEL ::= '_:' ( PN_CHARS_U | [0-9] ) ((PN_CHARS|'.')* PN_CHARS)?
BLANK_NODE_LABEL = Regex(ur'_:[0-9%s](?:[\.%s]*[%s])?' % (
    PN_CHARS_U_re, PN_CHARS_re, PN_CHARS_re), flags=re.U)
BLANK_NODE_LABEL.setParseAction(lambda x: rdflib.BNode(x[0]))


# [166] VARNAME ::= ( PN_CHARS_U | [0-9] ) ( PN_CHARS_U | [0-9] | #x00B7 | [#x0300-#x036F] | [#x203F-#x2040] )*
VARNAME = Regex(u'[%s0-9][%s0-9\u00B7\u0300-\u036F\u203F-\u2040]*' % (
    PN_CHARS_U_re, PN_CHARS_U_re), flags=re.U)

# [143] VAR1 ::= '?' VARNAME
VAR1 = Combine(Suppress('?') + VARNAME)

# [144] VAR2 ::= '$' VARNAME
VAR2 = Combine(Suppress('$') + VARNAME)

# [145] LANGTAG ::= '@' [a-zA-Z]+ ('-' [a-zA-Z0-9]+)*
LANGTAG = Combine(Suppress('@') + Regex('[a-zA-Z]+(?:-[a-zA-Z0-9]+)*'))

# [146] INTEGER ::= [0-9]+
INTEGER = Regex(r"[0-9]+")
# INTEGER.setResultsName('integer')
INTEGER.setParseAction(
    lambda x: rdflib.Literal(x[0], datatype=rdflib.XSD.integer))

# [155] EXPONENT ::= [eE] [+-]? [0-9]+
EXPONENT_re = '[eE][+-]?[0-9]+'

# [147] DECIMAL ::= [0-9]* '.' [0-9]+
DECIMAL = Regex(r'[0-9]*\.[0-9]+')  # (?![eE])
# DECIMAL.setResultsName('decimal')
DECIMAL.setParseAction(
    lambda x: rdflib.Literal(x[0], datatype=rdflib.XSD.decimal))

# [148] DOUBLE ::= [0-9]+ '.' [0-9]* EXPONENT | '.' ([0-9])+ EXPONENT | ([0-9])+ EXPONENT
DOUBLE = Regex(
    r'[0-9]+\.[0-9]*%(e)s|\.([0-9])+%(e)s|[0-9]+%(e)s' % {'e': EXPONENT_re})
# DOUBLE.setResultsName('double')
DOUBLE.setParseAction(
    lambda x: rdflib.Literal(x[0], datatype=rdflib.XSD.double))


# [149] INTEGER_POSITIVE ::= '+' INTEGER
INTEGER_POSITIVE = Suppress('+') + INTEGER.copy().leaveWhitespace()
INTEGER_POSITIVE.setParseAction(lambda x: rdflib.Literal(
    "+"+x[0], datatype=rdflib.XSD.integer))

# [150] DECIMAL_POSITIVE ::= '+' DECIMAL
DECIMAL_POSITIVE = Suppress('+') + DECIMAL.copy().leaveWhitespace()

# [151] DOUBLE_POSITIVE ::= '+' DOUBLE
DOUBLE_POSITIVE = Suppress('+') + DOUBLE.copy().leaveWhitespace()

# [152] INTEGER_NEGATIVE ::= '-' INTEGER
INTEGER_NEGATIVE = Suppress('-') + INTEGER.copy().leaveWhitespace()
INTEGER_NEGATIVE.setParseAction(lambda x: neg(x[0]))

# [153] DECIMAL_NEGATIVE ::= '-' DECIMAL
DECIMAL_NEGATIVE = Suppress('-') + DECIMAL.copy().leaveWhitespace()
DECIMAL_NEGATIVE.setParseAction(lambda x: neg(x[0]))

# [154] DOUBLE_NEGATIVE ::= '-' DOUBLE
DOUBLE_NEGATIVE = Suppress('-') + DOUBLE.copy().leaveWhitespace()
DOUBLE_NEGATIVE.setParseAction(lambda x: neg(x[0]))

# [160] ECHAR ::= '\' [tbnrf\"']
# ECHAR = Regex('\\\\[tbnrf"\']')


# [158] STRING_LITERAL_LONG1 ::= "'''" ( ( "'" | "''" )? ( [^'\] | ECHAR ) )* "'''"
# STRING_LITERAL_LONG1 = Literal("'''") + ( Optional( Literal("'") | "''"
# ) + ZeroOrMore( ~ Literal("'\\") | ECHAR ) ) + "'''"
STRING_LITERAL_LONG1 = Regex(ur"'''((?:'|'')?(?:[^'\\]|\\['ntbrf\\]))*'''")
STRING_LITERAL_LONG1.setParseAction(
    lambda x: rdflib.Literal(decodeUnicodeEscape(x[0][3:-3])))

# [159] STRING_LITERAL_LONG2 ::= '"""' ( ( '"' | '""' )? ( [^"\] | ECHAR ) )* '"""'
# STRING_LITERAL_LONG2 = Literal('"""') + ( Optional( Literal('"') | '""'
# ) + ZeroOrMore( ~ Literal('"\\') | ECHAR ) ) +  '"""'
STRING_LITERAL_LONG2 = Regex(ur'"""(?:(?:"|"")?(?:[^"\\]|\\["ntbrf\\]))*"""')
STRING_LITERAL_LONG2.setParseAction(
    lambda x: rdflib.Literal(decodeUnicodeEscape(x[0][3:-3])))

# [156] STRING_LITERAL1 ::= "'" ( ([^#x27#x5C#xA#xD]) | ECHAR )* "'"
# STRING_LITERAL1 = Literal("'") + ZeroOrMore(
# Regex(u'[^\u0027\u005C\u000A\u000D]',flags=re.U) | ECHAR ) + "'"

STRING_LITERAL1 = Regex(
    ur"'(?:[^'\n\r\\]|\\['ntbrf\\])*'(?!')", flags=re.U)
STRING_LITERAL1.setParseAction(
    lambda x: rdflib.Literal(decodeUnicodeEscape(x[0][1:-1])))

# [157] STRING_LITERAL2 ::= '"' ( ([^#x22#x5C#xA#xD]) | ECHAR )* '"'
# STRING_LITERAL2 = Literal('"') + ZeroOrMore (
# Regex(u'[^\u0022\u005C\u000A\u000D]',flags=re.U) | ECHAR ) + '"'

STRING_LITERAL2 = Regex(
    ur'"(?:[^"\n\r\\]|\\["ntbrf\\])*"(?!")', flags=re.U)
STRING_LITERAL2.setParseAction(
    lambda x: rdflib.Literal(decodeUnicodeEscape(x[0][1:-1])))

# [161] NIL ::= '(' WS* ')'
NIL = Literal('(') + ')'
NIL.setParseAction(lambda x: rdflib.RDF.nil)

# [162] WS ::= #x20 | #x9 | #xD | #xA
# Not needed?
# WS = #x20 | #x9 | #xD | #xA
# [163] ANON ::= '[' WS* ']'
ANON = Literal('[') + ']'
ANON.setParseAction(lambda x: rdflib.BNode())

# A = CaseSensitiveKeyword('a')
A = Literal('a')
A.setParseAction(lambda x: rdflib.RDF.type)


# ------ NON-TERMINALS --------------

# [5] BaseDecl ::= 'BASE' IRIREF
BaseDecl = Comp('Base', Keyword('BASE') + Param('iri', IRIREF))

# [6] PrefixDecl ::= 'PREFIX' PNAME_NS IRIREF
PrefixDecl = Comp(
    'PrefixDecl', Keyword('PREFIX') + PNAME_NS + Param('iri', IRIREF))

# [4] Prologue ::= ( BaseDecl | PrefixDecl )*
Prologue = Group(ZeroOrMore(BaseDecl | PrefixDecl))

# [108] Var ::= VAR1 | VAR2
Var = VAR1 | VAR2
Var.setParseAction(lambda x: rdflib.term.Variable(x[0]))

# [137] PrefixedName ::= PNAME_LN | PNAME_NS
PrefixedName = Comp('pname', PNAME_LN | PNAME_NS)

# [136] iri ::= IRIREF | PrefixedName
iri = IRIREF | PrefixedName

# [135] String ::= STRING_LITERAL1 | STRING_LITERAL2 | STRING_LITERAL_LONG1 | STRING_LITERAL_LONG2
String = STRING_LITERAL_LONG1 | STRING_LITERAL_LONG2 | STRING_LITERAL1 | STRING_LITERAL2

# [129] RDFLiteral ::= String ( LANGTAG | ( '^^' iri ) )?

RDFLiteral = Comp('literal', Param('string', String) + Optional(Param(
    'lang', LANGTAG.leaveWhitespace()) | Literal('^^').leaveWhitespace() + Param('datatype', iri).leaveWhitespace()))

# [132] NumericLiteralPositive ::= INTEGER_POSITIVE | DECIMAL_POSITIVE | DOUBLE_POSITIVE
NumericLiteralPositive = DOUBLE_POSITIVE | DECIMAL_POSITIVE | INTEGER_POSITIVE

# [133] NumericLiteralNegative ::= INTEGER_NEGATIVE | DECIMAL_NEGATIVE | DOUBLE_NEGATIVE
NumericLiteralNegative = DOUBLE_NEGATIVE | DECIMAL_NEGATIVE | INTEGER_NEGATIVE

# [131] NumericLiteralUnsigned ::= INTEGER | DECIMAL | DOUBLE
NumericLiteralUnsigned = DOUBLE | DECIMAL | INTEGER

# [130] NumericLiteral ::= NumericLiteralUnsigned | NumericLiteralPositive | NumericLiteralNegative
NumericLiteral = NumericLiteralUnsigned | NumericLiteralPositive | NumericLiteralNegative

# [134] BooleanLiteral ::= 'true' | 'false'
BooleanLiteral = Keyword('true').setParseAction(lambda: rdflib.Literal(True)) |\
    Keyword('false').setParseAction(lambda: rdflib.Literal(False))

# [138] BlankNode ::= BLANK_NODE_LABEL | ANON
BlankNode = BLANK_NODE_LABEL | ANON

# [109] GraphTerm ::= iri | RDFLiteral | NumericLiteral | BooleanLiteral | BlankNode | NIL
GraphTerm = iri | RDFLiteral | NumericLiteral | BooleanLiteral | BlankNode | NIL

# [106] VarOrTerm ::= Var | GraphTerm
VarOrTerm = Var | GraphTerm

# [107] VarOrIri ::= Var | iri
VarOrIri = Var | iri

# [46] GraphRef ::= 'GRAPH' iri
GraphRef = Keyword('GRAPH') + Param('graphiri', iri)

# [47] GraphRefAll ::= GraphRef | 'DEFAULT' | 'NAMED' | 'ALL'
GraphRefAll = GraphRef | Param('graphiri', Keyword('DEFAULT')) | Param(
    'graphiri', Keyword('NAMED')) | Param('graphiri', Keyword('ALL'))

# [45] GraphOrDefault ::= 'DEFAULT' | 'GRAPH'? iri
GraphOrDefault = ParamList('graph', Keyword(
    'DEFAULT')) | Optional(Keyword('GRAPH')) + ParamList('graph', iri)

# [65] DataBlockValue ::= iri | RDFLiteral | NumericLiteral | BooleanLiteral | 'UNDEF'
DataBlockValue = iri | RDFLiteral | NumericLiteral | BooleanLiteral | Keyword(
    'UNDEF')

# [78] Verb ::= VarOrIri | A
Verb = VarOrIri | A


# [85] VerbSimple ::= Var
VerbSimple = Var

# [97] Integer ::= INTEGER
Integer = INTEGER


TriplesNode = Forward()
TriplesNodePath = Forward()

# [104] GraphNode ::= VarOrTerm | TriplesNode
GraphNode = VarOrTerm | TriplesNode

# [105] GraphNodePath ::= VarOrTerm | TriplesNodePath
GraphNodePath = VarOrTerm | TriplesNodePath


# [93] PathMod ::= '?' | '*' | '+'
PathMod = Literal('?') | '*' | '+'

# [96] PathOneInPropertySet ::= iri | A | '^' ( iri | A )
PathOneInPropertySet = iri | A | Comp('InversePath', '^' + (iri | A))

Path = Forward()

# [95] PathNegatedPropertySet ::= PathOneInPropertySet | '(' ( PathOneInPropertySet ( '|' PathOneInPropertySet )* )? ')'
PathNegatedPropertySet = Comp('PathNegatedPropertySet', ParamList('part', PathOneInPropertySet) | '(' + Optional(
    ParamList('part', PathOneInPropertySet) + ZeroOrMore('|' + ParamList('part', PathOneInPropertySet))) + ')')

# [94] PathPrimary ::= iri | A | '!' PathNegatedPropertySet | '(' Path ')' | 'DISTINCT' '(' Path ')'
PathPrimary = iri | A | Suppress('!') + PathNegatedPropertySet | Suppress('(') + Path + Suppress(
    ')') | Comp('DistinctPath', Keyword('DISTINCT') + '(' + Param('part', Path) + ')')

# [91] PathElt ::= PathPrimary Optional(PathMod)
PathElt = Comp('PathElt', Param(
    'part', PathPrimary) + Optional(Param('mod', PathMod.leaveWhitespace())))

# [92] PathEltOrInverse ::= PathElt | '^' PathElt
PathEltOrInverse = PathElt | Suppress(
    '^') + Comp('PathEltOrInverse', Param('part', PathElt))

# [90] PathSequence ::= PathEltOrInverse ( '/' PathEltOrInverse )*
PathSequence = Comp('PathSequence', ParamList('part', PathEltOrInverse) +
                    ZeroOrMore('/' + ParamList('part', PathEltOrInverse)))


# [89] PathAlternative ::= PathSequence ( '|' PathSequence )*
PathAlternative = Comp('PathAlternative', ParamList('part', PathSequence) +
                       ZeroOrMore('|' + ParamList('part', PathSequence)))

# [88] Path ::= PathAlternative
Path << PathAlternative

# [84] VerbPath ::= Path
VerbPath = Path

# [87] ObjectPath ::= GraphNodePath
ObjectPath = GraphNodePath

# [86] ObjectListPath ::= ObjectPath ( ',' ObjectPath )*
ObjectListPath = ObjectPath + ZeroOrMore(',' + ObjectPath)


GroupGraphPattern = Forward()


# [102] Collection ::= '(' OneOrMore(GraphNode) ')'
Collection = Suppress('(') + OneOrMore(GraphNode) + Suppress(')')
Collection.setParseAction(expandCollection)

# [103] CollectionPath ::= '(' OneOrMore(GraphNodePath) ')'
CollectionPath = Suppress('(') + OneOrMore(GraphNodePath) + Suppress(')')
CollectionPath.setParseAction(expandCollection)

# [80] Object ::= GraphNode
Object = GraphNode

# [79] ObjectList ::= Object ( ',' Object )*
ObjectList = Object + ZeroOrMore(',' + Object)

# [83] PropertyListPathNotEmpty ::= ( VerbPath | VerbSimple ) ObjectListPath ( ';' ( ( VerbPath | VerbSimple ) ObjectList )? )*
PropertyListPathNotEmpty = (VerbPath | VerbSimple) + ObjectListPath + ZeroOrMore(
    ';' + Optional((VerbPath | VerbSimple) + ObjectList))

# [82] PropertyListPath ::= Optional(PropertyListPathNotEmpty)
PropertyListPath = Optional(PropertyListPathNotEmpty)

# [77] PropertyListNotEmpty ::= Verb ObjectList ( ';' ( Verb ObjectList )? )*
PropertyListNotEmpty = Verb + ObjectList + ZeroOrMore(';' + Optional(Verb +
                                                      ObjectList))


# [76] PropertyList ::= Optional(PropertyListNotEmpty)
PropertyList = Optional(PropertyListNotEmpty)

# [99] BlankNodePropertyList ::= '[' PropertyListNotEmpty ']'
BlankNodePropertyList = Group(
    Suppress('[') + PropertyListNotEmpty + Suppress(']'))
BlankNodePropertyList.setParseAction(expandBNodeTriples)

# [101] BlankNodePropertyListPath ::= '[' PropertyListPathNotEmpty ']'
BlankNodePropertyListPath = Group(
    Suppress('[') + PropertyListPathNotEmpty + Suppress(']'))
BlankNodePropertyListPath.setParseAction(expandBNodeTriples)

# [98] TriplesNode ::= Collection | BlankNodePropertyList
TriplesNode << (Collection | BlankNodePropertyList)

# [100] TriplesNodePath ::= CollectionPath | BlankNodePropertyListPath
TriplesNodePath << (CollectionPath | BlankNodePropertyListPath)

# [75] TriplesSameSubject ::= VarOrTerm PropertyListNotEmpty | TriplesNode PropertyList
TriplesSameSubject = VarOrTerm + PropertyListNotEmpty | TriplesNode + \
    PropertyList
TriplesSameSubject.setParseAction(expandTriples)

# [52] TriplesTemplate ::= TriplesSameSubject ( '.' Optional(TriplesTemplate) )?
TriplesTemplate = Forward()
TriplesTemplate << (ParamList('triples', TriplesSameSubject) + Optional(
    Suppress('.') + Optional(TriplesTemplate)))

# [51] QuadsNotTriples ::= 'GRAPH' VarOrIri '{' Optional(TriplesTemplate) '}'
QuadsNotTriples = Comp('QuadsNotTriples', Keyword('GRAPH') + Param(
    'term', VarOrIri) + '{' + Optional(TriplesTemplate) + '}')

# [50] Quads ::= Optional(TriplesTemplate) ( QuadsNotTriples '.'? Optional(TriplesTemplate) )*
Quads = Comp('Quads', Optional(TriplesTemplate) + ZeroOrMore(ParamList(
    'quadsNotTriples', QuadsNotTriples) + Optional(Suppress('.')) + Optional(TriplesTemplate)))

# [48] QuadPattern ::= '{' Quads '}'
QuadPattern = '{' + Param('quads', Quads) + '}'

# [49] QuadData ::= '{' Quads '}'
QuadData = '{' + Param('quads', Quads) + '}'

# [81] TriplesSameSubjectPath ::= VarOrTerm PropertyListPathNotEmpty | TriplesNodePath PropertyListPath
TriplesSameSubjectPath = VarOrTerm + \
    PropertyListPathNotEmpty | TriplesNodePath + PropertyListPath
TriplesSameSubjectPath.setParseAction(expandTriples)

# [55] TriplesBlock ::= TriplesSameSubjectPath ( '.' Optional(TriplesBlock) )?
TriplesBlock = Forward()
TriplesBlock << (ParamList('triples', TriplesSameSubjectPath) + Optional(
    Suppress('.') + Optional(TriplesBlock)))


# [66] MinusGraphPattern ::= 'MINUS' GroupGraphPattern
MinusGraphPattern = Comp(
    'MinusGraphPattern', Keyword('MINUS') + Param('graph', GroupGraphPattern))

# [67] GroupOrUnionGraphPattern ::= GroupGraphPattern ( 'UNION' GroupGraphPattern )*
GroupOrUnionGraphPattern = Comp('GroupOrUnionGraphPattern', ParamList(
    'graph', GroupGraphPattern) + ZeroOrMore(Keyword('UNION') + ParamList('graph', GroupGraphPattern)))


Expression = Forward()

# [72] ExpressionList ::= NIL | '(' Expression ( ',' Expression )* ')'
ExpressionList = NIL | Group(
    Suppress('(') + delimitedList(Expression) + Suppress(')'))

# [122] RegexExpression ::= 'REGEX' '(' Expression ',' Expression ( ',' Expression )? ')'
RegexExpression = Comp('Builtin_REGEX', Keyword('REGEX') + '(' + Param('text', Expression) + ',' + Param(
    'pattern', Expression) + Optional(',' + Param('flags', Expression)) + ')')
RegexExpression.setEvalFn(op.Builtin_REGEX)

# [123] SubstringExpression ::= 'SUBSTR' '(' Expression ',' Expression ( ',' Expression )? ')'
SubstringExpression = Comp('Builtin_SUBSTR', Keyword('SUBSTR') + '(' + Param('arg', Expression) + ',' + Param(
    'start', Expression) + Optional(',' + Param('length', Expression)) + ')').setEvalFn(op.Builtin_SUBSTR)

# [124] StrReplaceExpression ::= 'REPLACE' '(' Expression ',' Expression ',' Expression ( ',' Expression )? ')'
StrReplaceExpression = Comp('Builtin_REPLACE', Keyword('REPLACE') + '(' + Param('arg', Expression) + ',' + Param(
    'pattern', Expression) + ',' + Param('replacement', Expression) + Optional(',' + Param('flags', Expression)) + ')').setEvalFn(op.Builtin_REPLACE)

# [125] ExistsFunc ::= 'EXISTS' GroupGraphPattern
ExistsFunc = Comp('Builtin_EXISTS', Keyword('EXISTS') + Param(
    'graph', GroupGraphPattern)).setEvalFn(op.Builtin_EXISTS)

# [126] NotExistsFunc ::= 'NOT' 'EXISTS' GroupGraphPattern
NotExistsFunc = Comp('Builtin_NOTEXISTS', Keyword('NOT') + Keyword(
    'EXISTS') + Param('graph', GroupGraphPattern)).setEvalFn(op.Builtin_EXISTS)


# [127] Aggregate ::= 'COUNT' '(' 'DISTINCT'? ( '*' | Expression ) ')'
# | 'SUM' '(' Optional('DISTINCT') Expression ')'
# | 'MIN' '(' Optional('DISTINCT') Expression ')'
# | 'MAX' '(' Optional('DISTINCT') Expression ')'
# | 'AVG' '(' Optional('DISTINCT') Expression ')'
# | 'SAMPLE' '(' Optional('DISTINCT') Expression ')'
# | 'GROUP_CONCAT' '(' Optional('DISTINCT') Expression ( ';' 'SEPARATOR' '=' String )? ')'

_Distinct = Optional(Keyword('DISTINCT'))
_AggregateParams = '(' + Param(
    'distinct', _Distinct) + Param('vars', Expression) + ')'

Aggregate = Comp('Aggregate_Count', Keyword('COUNT') + '(' + Param('distinct', _Distinct) + Param('vars', '*' | Expression) + ')')\
    | Comp('Aggregate_Sum', Keyword('SUM') + _AggregateParams)\
    | Comp('Aggregate_Min', Keyword('MIN') + _AggregateParams)\
    | Comp('Aggregate_Max', Keyword('MAX') + _AggregateParams)\
    | Comp('Aggregate_Avg', Keyword('AVG') + _AggregateParams)\
    | Comp('Aggregate_Sample', Keyword('SAMPLE') + _AggregateParams)\
    | Comp('Aggregate_GroupConcat', Keyword('GROUP_CONCAT') + '(' + Param('distinct', _Distinct) + Param('vars', Expression) + Optional(';' + Keyword('SEPARATOR') + '=' + Param('separator', String)) + ')')

# [121] BuiltInCall ::= Aggregate
# | 'STR' '(' + Expression + ')'
# | 'LANG' '(' + Expression + ')'
# | 'LANGMATCHES' '(' + Expression + ',' + Expression + ')'
# | 'DATATYPE' '(' + Expression + ')'
# | 'BOUND' '(' Var ')'
# | 'IRI' '(' + Expression + ')'
# | 'URI' '(' + Expression + ')'
# | 'BNODE' ( '(' + Expression + ')' | NIL )
# | 'RAND' NIL
# | 'ABS' '(' + Expression + ')'
# | 'CEIL' '(' + Expression + ')'
# | 'FLOOR' '(' + Expression + ')'
# | 'ROUND' '(' + Expression + ')'
# | 'CONCAT' ExpressionList
# | SubstringExpression
# | 'STRLEN' '(' + Expression + ')'
# | StrReplaceExpression
# | 'UCASE' '(' + Expression + ')'
# | 'LCASE' '(' + Expression + ')'
# | 'ENCODE_FOR_URI' '(' + Expression + ')'
# | 'CONTAINS' '(' + Expression + ',' + Expression + ')'
# | 'STRSTARTS' '(' + Expression + ',' + Expression + ')'
# | 'STRENDS' '(' + Expression + ',' + Expression + ')'
# | 'STRBEFORE' '(' + Expression + ',' + Expression + ')'
# | 'STRAFTER' '(' + Expression + ',' + Expression + ')'
# | 'YEAR' '(' + Expression + ')'
# | 'MONTH' '(' + Expression + ')'
# | 'DAY' '(' + Expression + ')'
# | 'HOURS' '(' + Expression + ')'
# | 'MINUTES' '(' + Expression + ')'
# | 'SECONDS' '(' + Expression + ')'
# | 'TIMEZONE' '(' + Expression + ')'
# | 'TZ' '(' + Expression + ')'
# | 'NOW' NIL
# | 'UUID' NIL
# | 'STRUUID' NIL
# | 'MD5' '(' + Expression + ')'
# | 'SHA1' '(' + Expression + ')'
# | 'SHA256' '(' + Expression + ')'
# | 'SHA384' '(' + Expression + ')'
# | 'SHA512' '(' + Expression + ')'
# | 'COALESCE' ExpressionList
# | 'IF' '(' Expression ',' Expression ',' Expression ')'
# | 'STRLANG' '(' + Expression + ',' + Expression + ')'
# | 'STRDT' '(' + Expression + ',' + Expression + ')'
# | 'sameTerm' '(' + Expression + ',' + Expression + ')'
# | 'isIRI' '(' + Expression + ')'
# | 'isURI' '(' + Expression + ')'
# | 'isBLANK' '(' + Expression + ')'
# | 'isLITERAL' '(' + Expression + ')'
# | 'isNUMERIC' '(' + Expression + ')'
# | RegexExpression
# | ExistsFunc
# | NotExistsFunc

BuiltInCall = Aggregate \
    | Comp('Builtin_STR', Keyword('STR') + '(' + Param('arg', Expression) + ')').setEvalFn(op.Builtin_STR) \
    | Comp('Builtin_LANG', Keyword('LANG') + '(' + Param('arg', Expression) + ')').setEvalFn(op.Builtin_LANG) \
    | Comp('Builtin_LANGMATCHES', Keyword('LANGMATCHES') + '(' + Param('arg1', Expression) + ',' + Param('arg2', Expression) + ')').setEvalFn(op.Builtin_LANGMATCHES) \
    | Comp('Builtin_DATATYPE', Keyword('DATATYPE') + '(' + Param('arg', Expression) + ')').setEvalFn(op.Builtin_DATATYPE) \
    | Comp('Builtin_BOUND', Keyword('BOUND') + '(' + Param('arg', Var) + ')').setEvalFn(op.Builtin_BOUND) \
    | Comp('Builtin_IRI', Keyword('IRI') + '(' + Param('arg', Expression) + ')').setEvalFn(op.Builtin_IRI) \
    | Comp('Builtin_URI', Keyword('URI') + '(' + Param('arg', Expression) + ')').setEvalFn(op.Builtin_IRI) \
    | Comp('Builtin_BNODE', Keyword('BNODE') + ('(' + Param('arg', Expression) + ')' | NIL)).setEvalFn(op.Builtin_BNODE) \
    | Comp('Builtin_RAND', Keyword('RAND') + NIL).setEvalFn(op.Builtin_RAND) \
    | Comp('Builtin_ABS', Keyword('ABS') + '(' + Param('arg', Expression) + ')').setEvalFn(op.Builtin_ABS) \
    | Comp('Builtin_CEIL', Keyword('CEIL') + '(' + Param('arg', Expression) + ')').setEvalFn(op.Builtin_CEIL) \
    | Comp('Builtin_FLOOR', Keyword('FLOOR') + '(' + Param('arg', Expression) + ')').setEvalFn(op.Builtin_FLOOR) \
    | Comp('Builtin_ROUND', Keyword('ROUND') + '(' + Param('arg', Expression) + ')').setEvalFn(op.Builtin_ROUND) \
    | Comp('Builtin_CONCAT', Keyword('CONCAT') + Param('arg', ExpressionList)).setEvalFn(op.Builtin_CONCAT) \
    | SubstringExpression \
    | Comp('Builtin_STRLEN', Keyword('STRLEN') + '(' + Param('arg', Expression) + ')').setEvalFn(op.Builtin_STRLEN) \
    | StrReplaceExpression \
    | Comp('Builtin_UCASE', Keyword('UCASE') + '(' + Param('arg', Expression) + ')').setEvalFn(op.Builtin_UCASE) \
    | Comp('Builtin_LCASE', Keyword('LCASE') + '(' + Param('arg', Expression) + ')').setEvalFn(op.Builtin_LCASE) \
    | Comp('Builtin_ENCODE_FOR_URI', Keyword('ENCODE_FOR_URI') + '(' + Param('arg', Expression) + ')').setEvalFn(op.Builtin_ENCODE_FOR_URI) \
    | Comp('Builtin_CONTAINS', Keyword('CONTAINS') + '(' + Param('arg1', Expression) + ',' + Param('arg2', Expression) + ')').setEvalFn(op.Builtin_CONTAINS) \
    | Comp('Builtin_STRSTARTS', Keyword('STRSTARTS') + '(' + Param('arg1', Expression) + ',' + Param('arg2', Expression) + ')').setEvalFn(op.Builtin_STRSTARTS) \
    | Comp('Builtin_STRENDS', Keyword('STRENDS') + '(' + Param('arg1', Expression) + ',' + Param('arg2', Expression) + ')').setEvalFn(op.Builtin_STRENDS) \
    | Comp('Builtin_STRBEFORE', Keyword('STRBEFORE') + '(' + Param('arg1', Expression) + ',' + Param('arg2', Expression) + ')').setEvalFn(op.Builtin_STRBEFORE) \
    | Comp('Builtin_STRAFTER', Keyword('STRAFTER') + '(' + Param('arg1', Expression) + ',' + Param('arg2', Expression) + ')').setEvalFn(op.Builtin_STRAFTER) \
    | Comp('Builtin_YEAR', Keyword('YEAR') + '(' + Param('arg', Expression) + ')').setEvalFn(op.Builtin_YEAR) \
    | Comp('Builtin_MONTH', Keyword('MONTH') + '(' + Param('arg', Expression) + ')').setEvalFn(op.Builtin_MONTH) \
    | Comp('Builtin_DAY', Keyword('DAY') + '(' + Param('arg', Expression) + ')').setEvalFn(op.Builtin_DAY) \
    | Comp('Builtin_HOURS', Keyword('HOURS') + '(' + Param('arg', Expression) + ')').setEvalFn(op.Builtin_HOURS) \
    | Comp('Builtin_MINUTES', Keyword('MINUTES') + '(' + Param('arg', Expression) + ')').setEvalFn(op.Builtin_MINUTES) \
    | Comp('Builtin_SECONDS', Keyword('SECONDS') + '(' + Param('arg', Expression) + ')').setEvalFn(op.Builtin_SECONDS) \
    | Comp('Builtin_TIMEZONE', Keyword('TIMEZONE') + '(' + Param('arg', Expression) + ')').setEvalFn(op.Builtin_TIMEZONE) \
    | Comp('Builtin_TZ', Keyword('TZ') + '(' + Param('arg', Expression) + ')').setEvalFn(op.Builtin_TZ) \
    | Comp('Builtin_NOW', Keyword('NOW') + NIL).setEvalFn(op.Builtin_NOW) \
    | Comp('Builtin_UUID', Keyword('UUID') + NIL).setEvalFn(op.Builtin_UUID) \
    | Comp('Builtin_STRUUID', Keyword('STRUUID') + NIL).setEvalFn(op.Builtin_STRUUID) \
    | Comp('Builtin_MD5', Keyword('MD5') + '(' + Param('arg', Expression) + ')').setEvalFn(op.Builtin_MD5) \
    | Comp('Builtin_SHA1', Keyword('SHA1') + '(' + Param('arg', Expression) + ')').setEvalFn(op.Builtin_SHA1) \
    | Comp('Builtin_SHA256', Keyword('SHA256') + '(' + Param('arg', Expression) + ')').setEvalFn(op.Builtin_SHA256) \
    | Comp('Builtin_SHA384', Keyword('SHA384') + '(' + Param('arg', Expression) + ')').setEvalFn(op.Builtin_SHA384) \
    | Comp('Builtin_SHA512', Keyword('SHA512') + '(' + Param('arg', Expression) + ')').setEvalFn(op.Builtin_SHA512) \
    | Comp('Builtin_COALESCE', Keyword('COALESCE') + Param('arg', ExpressionList)).setEvalFn(op.Builtin_COALESCE) \
    | Comp('Builtin_IF', Keyword('IF') + '(' + Param('arg1', Expression) + ',' + Param('arg2', Expression) + ',' + Param('arg3', Expression) + ')').setEvalFn(op.Builtin_IF) \
    | Comp('Builtin_STRLANG', Keyword('STRLANG') + '(' + Param('arg1', Expression) + ',' + Param('arg2', Expression) + ')').setEvalFn(op.Builtin_STRLANG) \
    | Comp('Builtin_STRDT', Keyword('STRDT') + '(' + Param('arg1', Expression) + ',' + Param('arg2', Expression) + ')').setEvalFn(op.Builtin_STRDT) \
    | Comp('Builtin_sameTerm', Keyword('sameTerm') + '(' + Param('arg1', Expression) + ',' + Param('arg2', Expression) + ')').setEvalFn(op.Builtin_sameTerm) \
    | Comp('Builtin_isIRI', Keyword('isIRI') + '(' + Param('arg', Expression) + ')').setEvalFn(op.Builtin_isIRI) \
    | Comp('Builtin_isURI', Keyword('isURI') + '(' + Param('arg', Expression) + ')').setEvalFn(op.Builtin_isIRI) \
    | Comp('Builtin_isBLANK', Keyword('isBLANK') + '(' + Param('arg', Expression) + ')').setEvalFn(op.Builtin_isBLANK) \
    | Comp('Builtin_isLITERAL', Keyword('isLITERAL') + '(' + Param('arg', Expression) + ')').setEvalFn(op.Builtin_isLITERAL) \
    | Comp('Builtin_isNUMERIC', Keyword('isNUMERIC') + '(' + Param('arg', Expression) + ')').setEvalFn(op.Builtin_isNUMERIC) \
    | RegexExpression \
    | ExistsFunc \
    | NotExistsFunc

# [71] ArgList ::= NIL | '(' 'DISTINCT'? Expression ( ',' Expression )* ')'
ArgList = NIL | '(' + Param('distinct', _Distinct) + delimitedList(
    ParamList('expr', Expression)) + ')'

# [128] iriOrFunction ::= iri Optional(ArgList)
iriOrFunction = (Comp(
    'Function', Param('iri', iri) + ArgList).setEvalFn(op.Function)) | iri

# [70] FunctionCall ::= iri ArgList
FunctionCall = Comp(
    'Function', Param('iri', iri) + ArgList).setEvalFn(op.Function)


# [120] BrackettedExpression ::= '(' Expression ')'
BrackettedExpression = Suppress('(') + Expression + Suppress(')')

# [119] PrimaryExpression ::= BrackettedExpression | BuiltInCall | iriOrFunction | RDFLiteral | NumericLiteral | BooleanLiteral | Var
PrimaryExpression = BrackettedExpression | BuiltInCall | iriOrFunction | RDFLiteral | NumericLiteral | BooleanLiteral | Var

# [118] UnaryExpression ::= '!' PrimaryExpression
# | '+' PrimaryExpression
# | '-' PrimaryExpression
# | PrimaryExpression
UnaryExpression = Comp('UnaryNot', '!' + Param('expr', PrimaryExpression)).setEvalFn(op.UnaryNot) \
    | Comp('UnaryPlus', '+' + Param('expr', PrimaryExpression)).setEvalFn(op.UnaryPlus) \
    | Comp('UnaryMinus', '-' + Param('expr', PrimaryExpression)).setEvalFn(op.UnaryMinus) \
    | PrimaryExpression

# [117] MultiplicativeExpression ::= UnaryExpression ( '*' UnaryExpression | '/' UnaryExpression )*
MultiplicativeExpression = Comp('MultiplicativeExpression', Param('expr', UnaryExpression) + ZeroOrMore(ParamList('op', '*') + ParamList(
    'other', UnaryExpression) | ParamList('op', '/') + ParamList('other', UnaryExpression))).setEvalFn(op.MultiplicativeExpression)

# [116] AdditiveExpression ::= MultiplicativeExpression ( '+' MultiplicativeExpression | '-' MultiplicativeExpression | ( NumericLiteralPositive | NumericLiteralNegative ) ( ( '*' UnaryExpression ) | ( '/' UnaryExpression ) )* )*

### NOTE: The second part of this production is there because:
### "In signed numbers, no white space is allowed between the sign and the number. The AdditiveExpression grammar rule allows for this by covering the two cases of an expression followed by a signed number. These produce an addition or subtraction of the unsigned number as appropriate."

# Here (I think) this is not nescessary since pyparsing doesn't separate
# tokenizing and parsing


AdditiveExpression = Comp('AdditiveExpression', Param('expr', MultiplicativeExpression) +
                          ZeroOrMore(ParamList('op', '+') + ParamList('other', MultiplicativeExpression) |
                                     ParamList('op', '-') + ParamList('other', MultiplicativeExpression))).setEvalFn(op.AdditiveExpression)


# [115] NumericExpression ::= AdditiveExpression
NumericExpression = AdditiveExpression

# [114] RelationalExpression ::= NumericExpression ( '=' NumericExpression | '!=' NumericExpression | '<' NumericExpression | '>' NumericExpression | '<=' NumericExpression | '>=' NumericExpression | 'IN' ExpressionList | 'NOT' 'IN' ExpressionList )?
RelationalExpression = Comp('RelationalExpression', Param('expr', NumericExpression) + Optional(
    Param('op', '=') + Param('other', NumericExpression) |
    Param('op', '!=') + Param('other', NumericExpression) |
    Param('op', '<') + Param('other', NumericExpression) |
    Param('op', '>') + Param('other', NumericExpression) |
    Param('op', '<=') + Param('other', NumericExpression) |
    Param('op', '>=') + Param('other', NumericExpression) |
    Param('op', Keyword('IN')) + Param('other', ExpressionList) |
    Param('op', Combine(Keyword('NOT') + Keyword('IN'), adjacent=False, joinString=" ")) + Param('other', ExpressionList))).setEvalFn(op.RelationalExpression)


# [113] ValueLogical ::= RelationalExpression
ValueLogical = RelationalExpression

# [112] ConditionalAndExpression ::= ValueLogical ( '&&' ValueLogical )*
ConditionalAndExpression = Comp('ConditionalAndExpression', Param('expr', ValueLogical) + ZeroOrMore(
    '&&' + ParamList('other', ValueLogical))).setEvalFn(op.ConditionalAndExpression)

# [111] ConditionalOrExpression ::= ConditionalAndExpression ( '||' ConditionalAndExpression )*
ConditionalOrExpression = Comp('ConditionalOrExpression', Param('expr', ConditionalAndExpression) + ZeroOrMore(
    '||' + ParamList('other', ConditionalAndExpression))).setEvalFn(op.ConditionalOrExpression)

# [110] Expression ::= ConditionalOrExpression
Expression << ConditionalOrExpression


# [69] Constraint ::= BrackettedExpression | BuiltInCall | FunctionCall
Constraint = BrackettedExpression | BuiltInCall | FunctionCall

# [68] Filter ::= 'FILTER' Constraint
Filter = Comp('Filter', Keyword('FILTER') + Param('expr', Constraint))


# [16] SourceSelector ::= iri
SourceSelector = iri

# [14] DefaultGraphClause ::= SourceSelector
DefaultGraphClause = SourceSelector

# [15] NamedGraphClause ::= 'NAMED' SourceSelector
NamedGraphClause = Keyword('NAMED') + Param('named', SourceSelector)

# [13] DatasetClause ::= 'FROM' ( DefaultGraphClause | NamedGraphClause )
DatasetClause = Comp('DatasetClause', Keyword(
    'FROM') + (Param('default', DefaultGraphClause) | NamedGraphClause))

# [20] GroupCondition ::= BuiltInCall | FunctionCall | '(' Expression ( 'AS' Var )? ')' | Var
GroupCondition = BuiltInCall | FunctionCall | Comp('GroupAs', '(' + Param(
    'expr', Expression) + Optional(Keyword('AS') + Param('var', Var)) + ')') | Var

# [19] GroupClause ::= 'GROUP' 'BY' GroupCondition+
GroupClause = Comp('GroupClause', Keyword('GROUP') + Keyword(
    'BY') + OneOrMore(ParamList('condition', GroupCondition)))


_Silent = Optional(Param('silent', Keyword('SILENT')))

# [31] Load ::= 'LOAD' 'SILENT'? iri ( 'INTO' GraphRef )?
Load = Comp('Load', Keyword('LOAD') + _Silent + Param('iri', iri) +
            Optional(Keyword('INTO') + GraphRef))

# [32] Clear ::= 'CLEAR' 'SILENT'? GraphRefAll
Clear = Comp('Clear', Keyword('CLEAR') + _Silent + GraphRefAll)

# [33] Drop ::= 'DROP' _Silent GraphRefAll
Drop = Comp('Drop', Keyword('DROP') + _Silent + GraphRefAll)

# [34] Create ::= 'CREATE' _Silent GraphRef
Create = Comp('Create', Keyword('CREATE') + _Silent + GraphRef)

# [35] Add ::= 'ADD' _Silent GraphOrDefault 'TO' GraphOrDefault
Add = Comp('Add', Keyword(
    'ADD') + _Silent + GraphOrDefault + Keyword('TO') + GraphOrDefault)

# [36] Move ::= 'MOVE' _Silent GraphOrDefault 'TO' GraphOrDefault
Move = Comp('Move', Keyword(
    'MOVE') + _Silent + GraphOrDefault + Keyword('TO') + GraphOrDefault)

# [37] Copy ::= 'COPY' _Silent GraphOrDefault 'TO' GraphOrDefault
Copy = Comp('Copy', Keyword(
    'COPY') + _Silent + GraphOrDefault + Keyword('TO') + GraphOrDefault)

# [38] InsertData ::= 'INSERT DATA' QuadData
InsertData = Comp('InsertData', Keyword('INSERT') + Keyword('DATA') + QuadData)

# [39] DeleteData ::= 'DELETE DATA' QuadData
DeleteData = Comp('DeleteData', Keyword('DELETE') + Keyword('DATA') + QuadData)

# [40] DeleteWhere ::= 'DELETE WHERE' QuadPattern
DeleteWhere = Comp(
    'DeleteWhere', Keyword('DELETE') + Keyword('WHERE') + QuadPattern)

# [42] DeleteClause ::= 'DELETE' QuadPattern
DeleteClause = Comp('DeleteClause', Keyword('DELETE') + QuadPattern)

# [43] InsertClause ::= 'INSERT' QuadPattern
InsertClause = Comp('InsertClause', Keyword('INSERT') + QuadPattern)

# [44] UsingClause ::= 'USING' ( iri | 'NAMED' iri )
UsingClause = Comp('UsingClause', Keyword('USING') + (
    Param('default', iri) | Keyword('NAMED') + Param('named', iri)))

# [41] Modify ::= ( 'WITH' iri )? ( DeleteClause Optional(InsertClause) | InsertClause ) ZeroOrMore(UsingClause) 'WHERE' GroupGraphPattern
Modify = Comp('Modify', Optional(Keyword('WITH') + Param('withClause', iri)) + (Param('delete', DeleteClause) + Optional(Param(
    'insert', InsertClause)) | Param('insert', InsertClause)) + ZeroOrMore(ParamList('using', UsingClause)) + Keyword('WHERE') + Param('where', GroupGraphPattern))


# [30] Update1 ::= Load | Clear | Drop | Add | Move | Copy | Create | InsertData | DeleteData | DeleteWhere | Modify
Update1 = Load | Clear | Drop | Add | Move | Copy | Create | InsertData | DeleteData | DeleteWhere | Modify


# [63] InlineDataOneVar ::= Var '{' ZeroOrMore(DataBlockValue) '}'
InlineDataOneVar = ParamList(
    'var', Var) + '{' + ZeroOrMore(ParamList('value', DataBlockValue)) + '}'

# [64] InlineDataFull ::= ( NIL | '(' ZeroOrMore(Var) ')' ) '{' ( '(' ZeroOrMore(DataBlockValue) ')' | NIL )* '}'
InlineDataFull = (NIL | '(' + ZeroOrMore(ParamList('var', Var)) + ')') + '{' + ZeroOrMore(
    ParamList('value', Group(Suppress('(') + ZeroOrMore(DataBlockValue) + Suppress(')') | NIL))) + '}'

# [62] DataBlock ::= InlineDataOneVar | InlineDataFull
DataBlock = InlineDataOneVar | InlineDataFull


# [28] ValuesClause ::= ( 'VALUES' DataBlock )?
ValuesClause = Optional(Param(
    'valuesClause', Comp('ValuesClause', Keyword('VALUES') + DataBlock)))


# [74] ConstructTriples ::= TriplesSameSubject ( '.' Optional(ConstructTriples) )?
ConstructTriples = Forward()
ConstructTriples << (ParamList('template', TriplesSameSubject) + Optional(
    Suppress('.') + Optional(ConstructTriples)))

# [73] ConstructTemplate ::= '{' Optional(ConstructTriples) '}'
ConstructTemplate = Suppress('{') + Optional(ConstructTriples) + Suppress('}')


# [57] OptionalGraphPattern ::= 'OPTIONAL' GroupGraphPattern
OptionalGraphPattern = Comp('OptionalGraphPattern', Keyword(
    'OPTIONAL') + Param('graph', GroupGraphPattern))

# [58] GraphGraphPattern ::= 'GRAPH' VarOrIri GroupGraphPattern
GraphGraphPattern = Comp('GraphGraphPattern', Keyword(
    'GRAPH') + Param('term', VarOrIri) + Param('graph', GroupGraphPattern))

# [59] ServiceGraphPattern ::= 'SERVICE' _Silent VarOrIri GroupGraphPattern
ServiceGraphPattern = Comp('ServiceGraphPattern', Keyword(
    'SERVICE') + _Silent + Param('term', VarOrIri) + Param('graph', GroupGraphPattern))

# [60] Bind ::= 'BIND' '(' Expression 'AS' Var ')'
Bind = Comp('Bind', Keyword('BIND') + '(' + Param(
    'expr', Expression) + Keyword('AS') + Param('var', Var) + ')')

# [61] InlineData ::= 'VALUES' DataBlock
InlineData = Comp('InlineData', Keyword('VALUES') + DataBlock)

# [56] GraphPatternNotTriples ::= GroupOrUnionGraphPattern | OptionalGraphPattern | MinusGraphPattern | GraphGraphPattern | ServiceGraphPattern | Filter | Bind | InlineData
GraphPatternNotTriples = GroupOrUnionGraphPattern | OptionalGraphPattern | MinusGraphPattern | GraphGraphPattern | ServiceGraphPattern | Filter | Bind | InlineData

# [54] GroupGraphPatternSub ::= Optional(TriplesBlock) ( GraphPatternNotTriples '.'? Optional(TriplesBlock) )*
GroupGraphPatternSub = Comp('GroupGraphPatternSub', Optional(ParamList('part', Comp('TriplesBlock', TriplesBlock))) + ZeroOrMore(
    ParamList('part', GraphPatternNotTriples) + Optional('.') + Optional(ParamList('part', Comp('TriplesBlock', TriplesBlock)))))


# ----------------
# [22] HavingCondition ::= Constraint
HavingCondition = Constraint

# [21] HavingClause ::= 'HAVING' HavingCondition+
HavingClause = Comp('HavingClause', Keyword(
    'HAVING') + OneOrMore(ParamList('condition', HavingCondition)))

# [24] OrderCondition ::= ( ( 'ASC' | 'DESC' ) BrackettedExpression )
# | ( Constraint | Var )
OrderCondition = Comp('OrderCondition', Param('order', Keyword('ASC') | Keyword(
    'DESC')) + Param('expr', BrackettedExpression) | Param('expr', Constraint | Var))

# [23] OrderClause ::= 'ORDER' 'BY' OneOrMore(OrderCondition)
OrderClause = Comp('OrderClause', Keyword('ORDER') + Keyword(
    'BY') + OneOrMore(ParamList('condition', OrderCondition)))

# [26] LimitClause ::= 'LIMIT' INTEGER
LimitClause = Keyword('LIMIT') + Param('limit', INTEGER)

# [27] OffsetClause ::= 'OFFSET' INTEGER
OffsetClause = Keyword('OFFSET') + Param('offset', INTEGER)

# [25] LimitOffsetClauses ::= LimitClause Optional(OffsetClause) | OffsetClause Optional(LimitClause)
LimitOffsetClauses = Comp('LimitOffsetClauses', LimitClause + Optional(
    OffsetClause) | OffsetClause + Optional(LimitClause))

# [18] SolutionModifier ::= GroupClause? HavingClause? OrderClause? LimitOffsetClauses?
SolutionModifier = Optional(Param('groupby', GroupClause)) + Optional(Param('having', HavingClause)) + Optional(
    Param('orderby', OrderClause)) + Optional(Param('limitoffset', LimitOffsetClauses))


# [9] SelectClause ::= 'SELECT' ( 'DISTINCT' | 'REDUCED' )? ( ( Var | ( '(' Expression 'AS' Var ')' ) )+ | '*' )
SelectClause = Keyword('SELECT') + Optional(Param('modifier', Keyword('DISTINCT') | Keyword('REDUCED'))) + (OneOrMore(ParamList('projection', Comp('vars',
    Param('var', Var) | (Literal('(') + Param('expr', Expression) + Keyword('AS') + Param('evar', Var) + ')')))) | '*')

# [17] WhereClause ::= 'WHERE'? GroupGraphPattern
WhereClause = Optional(Keyword('WHERE')) + Param('where', GroupGraphPattern)

# [8] SubSelect ::= SelectClause WhereClause SolutionModifier ValuesClause
SubSelect = Comp('SubSelect', SelectClause + WhereClause +
                 SolutionModifier + ValuesClause)

# [53] GroupGraphPattern ::= '{' ( SubSelect | GroupGraphPatternSub ) '}'
GroupGraphPattern << (
    Suppress('{') + (SubSelect | GroupGraphPatternSub) + Suppress('}'))

# [7] SelectQuery ::= SelectClause DatasetClause* WhereClause SolutionModifier
SelectQuery = Comp('SelectQuery', SelectClause + ZeroOrMore(ParamList(
    'datasetClause', DatasetClause)) + WhereClause + SolutionModifier + ValuesClause)

# [10] ConstructQuery ::= 'CONSTRUCT' ( ConstructTemplate DatasetClause* WhereClause SolutionModifier | DatasetClause* 'WHERE' '{' TriplesTemplate? '}' SolutionModifier )
# NOTE: The CONSTRUCT WHERE alternative has unnescessarily many Comp/Param pairs
# to allow it to through the same algebra translation process
ConstructQuery = Comp('ConstructQuery', Keyword('CONSTRUCT') + (ConstructTemplate + ZeroOrMore(ParamList('datasetClause', DatasetClause)) + WhereClause + SolutionModifier + ValuesClause | ZeroOrMore(ParamList(
    'datasetClause', DatasetClause)) + Keyword('WHERE') + '{' + Optional(Param('where', Comp('FakeGroupGraphPatten', ParamList('part', Comp('TriplesBlock', TriplesTemplate))))) + '}' + SolutionModifier + ValuesClause))

# [12] AskQuery ::= 'ASK' DatasetClause* WhereClause SolutionModifier
AskQuery = Comp('AskQuery', Keyword('ASK') + Param('datasetClause', ZeroOrMore(
    DatasetClause)) + WhereClause + SolutionModifier + ValuesClause)

# [11] DescribeQuery ::= 'DESCRIBE' ( VarOrIri+ | '*' ) DatasetClause* WhereClause? SolutionModifier
DescribeQuery = Comp('DescribeQuery', Keyword('DESCRIBE') + (OneOrMore(ParamList('var', VarOrIri)) | '*') + Param(
    'datasetClause', ZeroOrMore(DatasetClause)) + Optional(WhereClause) + SolutionModifier + ValuesClause)

# [29] Update ::= Prologue ( Update1 ( ';' Update )? )?
Update = Forward()
Update << (ParamList('prologue', Prologue) + Optional(ParamList('request',
           Update1) + Optional(';' + Update)))


# [2] Query ::= Prologue
# ( SelectQuery | ConstructQuery | DescribeQuery | AskQuery )
# ValuesClause
# NOTE: ValuesClause was moved to invidual queries
Query = Prologue + (SelectQuery | ConstructQuery | DescribeQuery | AskQuery)

# [3] UpdateUnit ::= Update
UpdateUnit = Comp('Update', Update)

# [1] QueryUnit ::= Query
QueryUnit = Query

QueryUnit.ignore('#' + restOfLine)
UpdateUnit.ignore('#' + restOfLine)


expandUnicodeEscapes_re = re.compile(
    r'\\u([0-9a-f]{4}(?:[0-9a-f]{4})?)', flags=re.I)


def expandUnicodeEscapes(q):
    """
    The syntax of the SPARQL Query Language is expressed over code points in Unicode [UNICODE]. The encoding is always UTF-8 [RFC3629].
    Unicode code points may also be expressed using an \ uXXXX (U+0 to U+FFFF) or \ UXXXXXXXX syntax (for U+10000 onwards) where X is a hexadecimal digit [0-9A-F]
    """

    def expand(m):
        try:
            return unichr(int(m.group(1), 16))
        except:
            raise Exception("Invalid unicode code point: " + m)

    return expandUnicodeEscapes_re.sub(expand, q)


def parseQuery(q):
    if hasattr(q, 'read'):
        q = q.read()
    if isinstance(q, bytestype):
        q = q.decode('utf-8')

    q = expandUnicodeEscapes(q)
    return Query.parseString(q, parseAll=True)


def parseUpdate(q):
    if hasattr(q, 'read'):
        q = q.read()

    if isinstance(q, bytestype):
        q = q.decode('utf-8')

    q = expandUnicodeEscapes(q)
    return UpdateUnit.parseString(q, parseAll=True)[0]


if __name__ == '__main__':
    import sys
    DEBUG = True
    try:
        q = Query.parseString(sys.argv[1])
        print "\nSyntax Tree:\n"
        print q
    except ParseException, err:
        print err.line
        print " " * (err.column - 1) + "^"
        print err

########NEW FILE########
__FILENAME__ = parserutils

from types import MethodType

from rdflib.plugins.sparql.compat import OrderedDict

from pyparsing import TokenConverter, ParseResults

from rdflib import BNode, Variable, URIRef

DEBUG = True
DEBUG = False
if DEBUG:
    import traceback

"""

NOTE: PyParsing setResultName/__call__ provides a very similar solution to this
I didn't realise at the time of writing and I will remove a
lot of this code at some point

Utility classes for creating an abstract-syntax tree out with pyparsing actions

Lets you label and group parts of parser production rules

For example:

# [5] BaseDecl ::= 'BASE' IRIREF
BaseDecl = Comp('Base', Keyword('BASE') + Param('iri',IRIREF))

After parsing, this gives you back an CompValue object,
which is a dict/object with the paramters specified.
So you can access the parameters are attributes or as keys:

baseDecl.iri

Comp lets you set an evalFn that is bound to the eval method of
the resulting CompValue


"""


# This is an alternative

# Comp('Sum')( Param('x')(Number) + '+' + Param('y')(Number) )

def value(ctx, val, variables=False, errors=False):

    """
    utility function for evaluating something...

    Variables will be looked up in the context
    Normally, non-bound vars is an error,
    set variables=True to return unbound vars

    Normally, an error raises the error,
    set errors=True to return error

    """

    if isinstance(val, Expr):
        return val.eval(ctx)  # recurse?
    elif isinstance(val, CompValue):
        raise Exception("What do I do with this CompValue? %s" % val)

    elif isinstance(val, list):
        return [value(ctx, x, variables, errors) for x in val]

    elif isinstance(val, (BNode, Variable)):
        r = ctx.get(val)
        if isinstance(r, SPARQLError) and not errors:
            raise r
        if r is not None:
            return r

        # not bound
        if variables:
            return val
        else:
            raise NotBoundError

    elif isinstance(val, ParseResults) and len(val) == 1:
        return value(ctx, val[0], variables, errors)
    else:
        return val


class ParamValue(object):
    """
    The result of parsing a Param
    This just keeps the name/value
    All cleverness is in the CompValue
    """
    def __init__(self, name, tokenList, isList):
        self.isList = isList
        self.name = name
        if isinstance(tokenList, (list, ParseResults)) and len(tokenList) == 1:
            tokenList = tokenList[0]

        self.tokenList = tokenList

    def __str__(self):
        return "Param(%s, %s)" % (self.name, self.tokenList)


class Param(TokenConverter):
    """
    A pyparsing token for labelling a part of the parse-tree
    if isList is true repeat occurrences of ParamList have
    their values merged in a list
    """
    def __init__(self, name, expr, isList=False):
        self.name = name
        self.isList = isList
        TokenConverter.__init__(self, expr)
        self.addParseAction(self.postParse2)

    def postParse2(self, tokenList):
        return ParamValue(self.name, tokenList, self.isList)


class ParamList(Param):
    """
    A shortcut for a Param with isList=True
    """
    def __init__(self, name, expr):
        Param.__init__(self, name, expr, True)


class plist(list):
    """this is just a list, but we want our own type to check for"""

    pass


class CompValue(OrderedDict):

    """
    The result of parsing a Comp
    Any included Params are avaiable as Dict keys
    or as attributes

    """

    def __init__(self, name, **values):
        OrderedDict.__init__(self)
        self.name = name
        self.update(values)

    def __str__(self):
        return self.name + "_" + OrderedDict.__str__(self)

    def __repr__(self):
        return self.name + "_" + dict.__repr__(self)

    def _value(self, val, variables=False, errors=False):
        if self.ctx is not None:
            return value(self.ctx, val, variables)
        else:
            return val

    def __getitem__(self, a):
        return self._value(OrderedDict.__getitem__(self, a))

    def get(self, a, variables=False, errors=False):
        return self._value(OrderedDict.get(self, a, a), variables, errors)

    def __getattr__(self, a):
        # Hack hack: OrderedDict relies on this
        if a in ('_OrderedDict__root', '_OrderedDict__end'):
            raise AttributeError
        try:
            return self[a]
        except KeyError:
            # raise AttributeError('no such attribute '+a)
            return None


class Expr(CompValue):
    """
    A CompValue that is evaluatable
    """

    def __init__(self, name, evalfn=None, **values):
        super(Expr, self).__init__(name, **values)

        self._evalfn = None
        if evalfn:
            self._evalfn = MethodType(evalfn, self)

    def eval(self, ctx={}):
        try:
            self.ctx = ctx
            return self._evalfn(ctx)
        except SPARQLError, e:
            return e
        finally:
            self.ctx = None


class Comp(TokenConverter):

    """
    A pyparsing token for grouping together things with a label
    Any sub-tokens that are not Params will be ignored.

    Returns CompValue / Expr objects - depending on whether evalFn is set.
    """

    def __init__(self, name, expr):
        TokenConverter.__init__(self, expr)
        self.name = name
        self.evalfn = None

    def postParse(self, instring, loc, tokenList):

        if self.evalfn:
            res = Expr(self.name)
            res._evalfn = MethodType(self.evalfn, res)
        else:
            res = CompValue(self.name)

        for t in tokenList:
            if isinstance(t, ParamValue):
                if t.isList:
                    if not t.name in res:
                        res[t.name] = plist()
                    res[t.name].append(t.tokenList)
                else:
                    res[t.name] = t.tokenList
                # res.append(t.tokenList)
            # if isinstance(t,CompValue):
            #    res.update(t)
        return res

    def setEvalFn(self, evalfn):
        self.evalfn = evalfn
        return self


if __name__ == '__main__':
    from pyparsing import Word, nums
    import sys

    Number = Word(nums)
    Number.setParseAction(lambda x: int(x[0]))
    Plus = Comp('plus', Param('a', Number) + '+' + Param('b', Number))
    Plus.setEvalFn(lambda self, ctx: self.a + self.b)

    r = Plus.parseString(sys.argv[1])
    print r
    print r[0].eval({})

# hurrah for circular imports
from rdflib.plugins.sparql.sparql import SPARQLError, NotBoundError

########NEW FILE########
__FILENAME__ = processor

"""
Code for tying SPARQL Engine into RDFLib

These should be automatically registered with RDFLib

"""


from rdflib.query import Processor, Result, UpdateProcessor

from rdflib.plugins.sparql.sparql import Query

from rdflib.plugins.sparql.parser import parseQuery, parseUpdate
from rdflib.plugins.sparql.algebra import translateQuery, translateUpdate

from rdflib.plugins.sparql.evaluate import evalQuery
from rdflib.plugins.sparql.update import evalUpdate


def prepareQuery(queryString, initNs={}, base=None):
    """
    Parse and translate a SPARQL Query
    """
    return translateQuery(parseQuery(queryString), base, initNs)


def processUpdate(graph, updateString, initBindings={}, initNs={}, base=None):
    """
    Process a SPARQL Update Request
    returns Nothing on success or raises Exceptions on error
    """
    evalUpdate(graph, translateUpdate(
        parseUpdate(updateString), base, initNs), initBindings)


class SPARQLResult(Result):

    def __init__(self, res):
        Result.__init__(self, res["type_"])
        self.vars = res.get("vars_")
        self.bindings = res.get("bindings")
        self.askAnswer = res.get("askAnswer")
        self.graph = res.get("graph")

class SPARQLUpdateProcessor(UpdateProcessor):
    def __init__(self, graph):
        self.graph = graph

    def update(self, strOrQuery, initBindings={}, initNs={}):
        if isinstance(strOrQuery, basestring): 
            strOrQuery=translateUpdate(parseUpdate(strOrQuery), initNs=initNs)

        return evalUpdate(self.graph, strOrQuery, initBindings)


class SPARQLProcessor(Processor):

    def __init__(self, graph):
        self.graph = graph

    def query(
            self, strOrQuery, initBindings={},
            initNs={}, base=None, DEBUG=False):
        """
        Evaluate a query with the given initial bindings, and initial
        namespaces. The given base is used to resolve relative URIs in
        the query and will be overridden by any BASE given in the query.
        """

        if not isinstance(strOrQuery, Query):
            parsetree = parseQuery(strOrQuery)
            query = translateQuery(parsetree, base, initNs)
        else:
            query = strOrQuery

        return evalQuery(self.graph, query, initBindings, base)

########NEW FILE########
__FILENAME__ = csvresults
"""

This module implements a parser and serializer for the CSV SPARQL result
formats

http://www.w3.org/TR/sparql11-results-csv-tsv/

"""

import codecs
import csv

from rdflib import Variable, BNode, URIRef, Literal, py3compat

from rdflib.query import Result, ResultSerializer, ResultParser


class CSVResultParser(ResultParser):
    def __init__(self):
        self.delim = ","

    def parse(self, source):

        r = Result('SELECT')

        if isinstance(source.read(0), py3compat.bytestype):
            # if reading from source returns bytes do utf-8 decoding
            source = codecs.getreader('utf-8')(source)

        reader = csv.reader(source, delimiter=self.delim)
        r.vars = [Variable(x) for x in reader.next()]
        r.bindings = []

        for row in reader:
            r.bindings.append(self.parseRow(row, r.vars))

        return r

    def parseRow(self, row, v):
        return dict((var, val)
                    for var, val in zip(v, [self.convertTerm(t)
                                            for t in row]) if val is not None)

    def convertTerm(self, t):
        if t == "":
            return None
        if t.startswith("_:"):
            return BNode(t)  # or generate new IDs?
        if t.startswith("http://") or t.startswith("https://"):  # TODO: more?
            return URIRef(t)
        return Literal(t)


class CSVResultSerializer(ResultSerializer):

    def __init__(self, result):
        ResultSerializer.__init__(self, result)

        self.delim = ","
        if result.type != "SELECT":
            raise Exception(
                "CSVSerializer can only serialize select query results")

    def serialize(self, stream, encoding='utf-8'):

        if py3compat.PY3:
            # the serialiser writes bytes in the given encoding
            # in py3 csv.writer is unicode aware and writes STRINGS,
            # so we encode afterwards
            # in py2 it breaks when passed unicode strings,
            # and must be passed utf8, so we encode before

            import codecs
            stream = codecs.getwriter(encoding)(stream)

        out = csv.writer(stream, delimiter=self.delim)

        vs = [self.serializeTerm(v, encoding) for v in self.result.vars]
        out.writerow(vs)
        for row in self.result.bindings:
            out.writerow([self.serializeTerm(
                row.get(v), encoding) for v in self.result.vars])

    def serializeTerm(self, term, encoding):
        if term is None:
            return ""
        if not py3compat.PY3:
            return term.encode(encoding)
        else:
            return term

########NEW FILE########
__FILENAME__ = jsonlayer
# -*- coding: utf-8 -*-
#
# Copyright (C) 2009 Christopher Lenz
# All rights reserved.
#

# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:

# 1. Redistributions of source code must retain the above copyright
# notice, this list of conditions and the following disclaimer.
# 2. Redistributions in binary form must reproduce the above copyright
# notice, this list of conditions and the following disclaimer in
# the documentation and/or other materials provided with the
# distribution.
# 3. The name of the author may not be used to endorse or promote
# products derived from this software without specific prior
# written permission.

# THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS
# OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
# WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
# ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY
# DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE
# GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER
# IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
# IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

"""Thin abstraction layer over the different available modules for decoding
and encoding JSON data.

This module currently supports the following JSON modules:
 - ``simplejson``: http://code.google.com/p/simplejson/
 - ``cjson``: http://pypi.python.org/pypi/python-cjson
 - ``json``: This is the version of ``simplejson`` that is bundled with the
   Python standard library since version 2.6
   (see http://docs.python.org/library/json.html)

The default behavior is to use ``simplejson`` if installed, and otherwise
fallback to the standard library module. To explicitly tell SPARQLWrapper
which module to use, invoke the `use()` function with the module name::

    import jsonlayer
    jsonlayer.use('cjson')

In addition to choosing one of the above modules, you can also configure
SPARQLWrapper to use custom decoding and encoding functions::

    import jsonlayer
    jsonlayer.use(decode=my_decode, encode=my_encode)

"""

__all__ = ['decode', 'encode', 'use']

_initialized = False
_using = None
_decode = None
_encode = None


def decode(string):
    """Decode the given JSON string.

    :param string: the JSON string to decode
    :type string: basestring
    :return: the corresponding Python data structure
    :rtype: object
    """
    if not _initialized:
        _initialize()
    return _decode(string)


def encode(obj):
    """Encode the given object as a JSON string.

    :param obj: the Python data structure to encode
    :type obj: object
    :return: the corresponding JSON string
    :rtype: basestring
    """
    if not _initialized:
        _initialize()
    return _encode(obj)


def use(module=None, decode=None, encode=None):
    """Set the JSON library that should be used, either by specifying a known
    module name, or by providing a decode and encode function.

    The modules "simplejson", "cjson", and "json" are currently supported for
    the ``module`` parameter.

    If provided, the ``decode`` parameter must be a callable that accepts a
    JSON string and returns a corresponding Python data structure. The
    ``encode`` callable must accept a Python data structure and return the
    corresponding JSON string. Exceptions raised by decoding and encoding
    should be propagated up unaltered.

    :param module: the name of the JSON library module to use, or the module
                   object itself
    :type module: str or module
    :param decode: a function for decoding JSON strings
    :type decode: callable
    :param encode: a function for encoding objects as JSON strings
    :type encode: callable
    """
    global _decode, _encode, _initialized, _using
    if module is not None:
        if not isinstance(module, basestring):
            module = module.__name__
        if module not in ('cjson', 'json', 'simplejson'):
            raise ValueError('Unsupported JSON module %s' % module)
        _using = module
        _initialized = False
    else:
        assert decode is not None and encode is not None
        _using = 'custom'
        _decode = decode
        _encode = encode
        _initialized = True


def _initialize():
    global _initialized

    def _init_simplejson():
        global _decode, _encode
        import simplejson
        _decode = lambda string, loads=simplejson.loads: loads(string)
        _encode = lambda obj, dumps=simplejson.dumps: \
            dumps(obj, allow_nan=False, ensure_ascii=False)

    def _init_cjson():
        global _decode, _encode
        import cjson
        _decode = lambda string, decode=cjson.decode: decode(string)
        _encode = lambda obj, encode=cjson.encode: encode(obj)

    def _init_stdlib():
        global _decode, _encode
        json = __import__('json', {}, {})
        _decode = lambda string, loads=json.loads: loads(string)
        _encode = lambda obj, dumps=json.dumps: \
            dumps(obj, allow_nan=False, ensure_ascii=False)

    if _using == 'simplejson':
        _init_simplejson()
    elif _using == 'cjson':
        _init_cjson()
    elif _using == 'json':
        _init_stdlib()
    elif _using != 'custom':
        try:
            _init_simplejson()
        except ImportError:
            _init_stdlib()
    _initialized = True

########NEW FILE########
__FILENAME__ = jsonresults
from rdflib.query import (
    Result, ResultException, ResultSerializer, ResultParser)
from rdflib import Literal, URIRef, BNode, Variable

from rdflib.py3compat import bytestype


import jsonlayer

"""A Serializer for SPARQL results in JSON:

http://www.w3.org/TR/rdf-sparql-json-res/

Bits and pieces borrowed from:
http://projects.bigasterisk.com/sparqlhttp/

Authors: Drew Perttula, Gunnar Aastrand Grimnes

"""


class JSONResultParser(ResultParser):

    def parse(self, source):
        inp = source.read()
        if isinstance(inp, bytestype):
            inp = inp.decode('utf-8')
        return JSONResult(jsonlayer.decode(inp))


class JSONResultSerializer(ResultSerializer):

    def __init__(self, result):
        ResultSerializer.__init__(self, result)

    def serialize(self, stream, encoding=None):

        res = {}
        if self.result.type == 'ASK':
            res["head"] = {}
            res["boolean"] = self.result.askAnswer
        else:
            # select
            res["results"] = {}
            res["head"] = {}
            res["head"]["vars"] = self.result.vars
            res["results"]["bindings"] = [self._bindingToJSON(
                x) for x in self.result.bindings]

        r = jsonlayer.encode(res)
        if encoding is not None:
            stream.write(r.encode(encoding))
        else:
            stream.write(r)

    def _bindingToJSON(self, b):
        res = {}
        for var in b:
            j = termToJSON(self, b[var])
            if j is not None:
                res[var] = termToJSON(self, b[var])
        return res


class JSONResult(Result):

    def __init__(self, json):
        self.json = json
        if "boolean" in json:
            type_ = 'ASK'
        elif "results" in json:
            type_ = 'SELECT'
        else:
            raise ResultException('No boolean or results in json!')

        Result.__init__(self, type_)

        if type_ == 'ASK':
            self.askAnswer = bool(json['boolean'])
        else:
            self.bindings = self._get_bindings()
            self.vars = [Variable(x) for x in json["head"]["vars"]]

    def _get_bindings(self):
        ret = []
        for row in self.json['results']['bindings']:
            outRow = {}
            for k, v in row.items():
                outRow[Variable(k)] = parseJsonTerm(v)
            ret.append(outRow)
        return ret


def parseJsonTerm(d):
    """rdflib object (Literal, URIRef, BNode) for the given json-format dict.

    input is like:
      { 'type': 'uri', 'value': 'http://famegame.com/2006/01/username' }
      { 'type': 'literal', 'value': 'drewp' }
    """

    t = d['type']
    if t == 'uri':
        return URIRef(d['value'])
    elif t == 'literal':
        if 'xml:lang' in d:
            return Literal(d['value'], lang=d['xml:lang'])
        return Literal(d['value'])
    elif t == 'typed-literal':
        return Literal(d['value'], datatype=URIRef(d['datatype']))
    elif t == 'bnode':
        return BNode(d['value'])
    else:
        raise NotImplementedError("json term type %r" % t)


def termToJSON(self, term):
    if isinstance(term, URIRef):
        return {'type': 'uri', 'value': unicode(term)}
    elif isinstance(term, Literal):
        if term.datatype is not None:
            return {'type': 'typed-literal',
                    'value': unicode(term),
                    'datatype': unicode(term.datatype)}
        else:
            r = {'type': 'literal',
                 'value': unicode(term)}
            if term.language is not None:
                r['xml:lang'] = term.language
            return r

    elif isinstance(term, BNode):
        return {'type': 'bnode', 'value': str(term)}
    elif term is None:
        return None
    else:
        raise ResultException(
            'Unknown term type: %s (%s)' % (term, type(term)))

########NEW FILE########
__FILENAME__ = rdfresults
from rdflib import Graph, Namespace, RDF, Variable

from rdflib.query import Result, ResultParser

RS = Namespace('http://www.w3.org/2001/sw/DataAccess/tests/result-set#')


class RDFResultParser(ResultParser):
    def parse(self, source, **kwargs):
        return RDFResult(source, **kwargs)


class RDFResult(Result):

    def __init__(self, source, **kwargs):

        if not isinstance(source, Graph):
            graph = Graph()
            graph.load(source, **kwargs)
        else:
            graph = source

        rs = graph.value(predicate=RDF.type, object=RS.ResultSet)
                         # there better be only one :)

        if rs is None:
            type_ = 'CONSTRUCT'

            # use a new graph
            g = Graph()
            g += graph

        else:

            askAnswer = graph.value(rs, RS.boolean)

            if askAnswer is not None:
                type_ = 'ASK'
            else:
                type_ = 'SELECT'

        Result.__init__(self, type_)

        if type_ == 'SELECT':
            self.vars = [Variable(v) for v in graph.objects(rs,
                                                            RS.resultVariable)]

            self.bindings = []

            for s in graph.objects(rs, RS.solution):
                sol = {}
                for b in graph.objects(s, RS.binding):
                    sol[Variable(graph.value(
                        b, RS.variable))] = graph.value(b, RS.value)
                self.bindings.append(sol)
        elif type_ == 'ASK':
            self.askAnswer = askAnswer.value
            if askAnswer.value == None:
                raise Exception('Malformed boolean in ask answer!')
        elif type_ == 'CONSTRUCT':
            self.graph = g

########NEW FILE########
__FILENAME__ = tsvresults

"""
This implements the Tab Separated SPARQL Result Format

It is implemented with pyparsing, reusing the elements from the SPARQL Parser
"""

import codecs

from pyparsing import (
    Optional, ZeroOrMore, Literal, ParserElement, ParseException, Suppress,
    FollowedBy, LineEnd)

from rdflib.query import Result, ResultParser

from rdflib.plugins.sparql.parser import (
    Var, STRING_LITERAL1, STRING_LITERAL2, IRIREF, BLANK_NODE_LABEL,
    NumericLiteral, BooleanLiteral, LANGTAG)
from rdflib.plugins.sparql.parserutils import Comp, Param, CompValue

from rdflib import Literal as RDFLiteral

from rdflib.py3compat import bytestype

ParserElement.setDefaultWhitespaceChars(" \n")


String = STRING_LITERAL1 | STRING_LITERAL2

RDFLITERAL = Comp('literal', Param('string', String) + Optional(
    Param('lang', LANGTAG.leaveWhitespace()
          ) | Literal('^^').leaveWhitespace(
    ) + Param('datatype', IRIREF).leaveWhitespace()))

NONE_VALUE = object()

EMPTY = FollowedBy(LineEnd()) | FollowedBy("\t")
EMPTY.setParseAction(lambda x: NONE_VALUE)

TERM = RDFLITERAL | IRIREF | BLANK_NODE_LABEL | NumericLiteral | BooleanLiteral

ROW = (EMPTY | TERM) + ZeroOrMore(Suppress("\t") + (EMPTY | TERM))
ROW.parseWithTabs()

HEADER = Var + ZeroOrMore(Suppress("\t") + Var)
HEADER.parseWithTabs()


class TSVResultParser(ResultParser):
    def parse(self, source):

        if isinstance(source.read(0), bytestype):
            # if reading from source returns bytes do utf-8 decoding
            source = codecs.getreader('utf-8')(source)

        try:
            r = Result('SELECT')

            header = source.readline()

            r.vars = list(HEADER.parseString(header.strip(), parseAll=True))
            r.bindings = []
            while True:
                line = source.readline()
                if not line:
                    break
                line = line.strip('\n')
                if line == "":
                    continue

                row = ROW.parseString(line, parseAll=True)
                r.bindings.append(
                    dict(zip(r.vars, (self.convertTerm(x) for x in row))))

            return r

        except ParseException, err:
            print err.line
            print " " * (err.column - 1) + "^"
            print err

    def convertTerm(self, t):
        if t is NONE_VALUE:
            return None
        if isinstance(t, CompValue):
            if t.name == 'literal':
                return RDFLiteral(t.string, lang=t.lang, datatype=t.datatype)
            else:
                raise Exception("I dont know how to handle this: %s" % (t,))
        else:
            return t

if __name__ == '__main__':
    import sys
    r = Result.parse(file(sys.argv[1]), format='tsv')
    print r.vars
    print r.bindings
    # print r.serialize(format='json')

########NEW FILE########
__FILENAME__ = txtresults

from rdflib import URIRef, BNode, Literal
from rdflib.query import ResultSerializer

def _termString(t, namespace_manager):
    if t == None:
        return "-"
    if namespace_manager:
        if isinstance(t, URIRef): 
            return namespace_manager.normalizeUri(t)
        elif isinstance(t, BNode): 
            return t.n3()
        elif isinstance(t, Literal): 
            return t._literal_n3(qname_callback=namespace_manager.normalizeUri)
    else: 
        return t.n3()


class TXTResultSerializer(ResultSerializer): 
    """
    A write only QueryResult serializer for text/ascii tables
    """

    def serialize(self, stream, encoding, namespace_manager = None): 

        """
        return a text table of query results
        """


        def c(s, w):
            """
            center the string s in w wide string
            """
            w -= len(s)
            h1 = h2 = w // 2
            if w % 2: h2 += 1
            return " " * h1 + s + " " * h2

        if self.result.type!='SELECT': 
            raise Exception("Can only pretty print SELECT results!")

        if not self.result:
            return "(no results)\n"
        else:

            keys = sorted(self.result.vars)
            maxlen = [0] * len(keys)
            b = [[_termString(r[k], namespace_manager) for k in keys] for r in self.result]
            for r in b:
                for i in range(len(keys)):
                    maxlen[i] = max(maxlen[i], len(r[i]))

            stream.write(
                "|".join([c(k, maxlen[i]) for i, k in enumerate(keys)]) + "\n")
            stream.write("-" * (len(maxlen)+sum(maxlen)) + "\n")
            for r in sorted(b):
                stream.write("|".join(
                    [t + " " * (i - len(t)) for i, t in zip(maxlen, r)]) + "\n")


########NEW FILE########
__FILENAME__ = xmlresults

from xml.sax.saxutils import XMLGenerator
from xml.dom import XML_NAMESPACE
from xml.sax.xmlreader import AttributesNSImpl

from xml.etree import cElementTree as ElementTree

from rdflib import Literal, URIRef, BNode, Graph, Variable
from rdflib.query import (
    Result,
    ResultParser,
    ResultSerializer,
    ResultException
)

SPARQL_XML_NAMESPACE = u'http://www.w3.org/2005/sparql-results#'
RESULTS_NS_ET = '{%s}' % SPARQL_XML_NAMESPACE


"""A Parser for SPARQL results in XML:

http://www.w3.org/TR/rdf-sparql-XMLres/

Bits and pieces borrowed from:
http://projects.bigasterisk.com/sparqlhttp/

Authors: Drew Perttula, Gunnar Aastrand Grimnes
"""


class XMLResultParser(ResultParser):

    def parse(self, source):
        return XMLResult(source)


class XMLResult(Result):
    def __init__(self, source):

        xmlstring = source.read()

        if isinstance(xmlstring, unicode):
            xmlstring = xmlstring.encode('utf-8')
        try:
            tree = ElementTree.fromstring(xmlstring)
        except Exception, e:
            try:
                raise e.__class__("error parsing %r: %s" % (xmlstring, e))
            except:
                raise e

        boolean = tree.find(RESULTS_NS_ET + 'boolean')
        results = tree.find(RESULTS_NS_ET + 'results')

        if boolean is not None:
            type_ = 'ASK'
        elif results is not None:
            type_ = 'SELECT'
        else:
            g = Graph()
            try:
                g.parse(data=xmlstring)
                if len(g) == 0:
                    raise
                type_ = 'CONSTRUCT'

            except:
                raise ResultException(
                    "No RDF Graph, result-bindings or boolean answer found!")

        Result.__init__(self, type_)
        if type_ == 'SELECT':
            self.bindings = []
            for result in results:
                r = {}
                for binding in result:
                    r[Variable(binding.get('name'))] = parseTerm(binding[0])
                self.bindings.append(r)

            self.vars = [Variable(x.get("name"))
                         for x in tree.findall(
                         './%shead/%svariable' % (
                             RESULTS_NS_ET, RESULTS_NS_ET))]

        elif type_ == 'ASK':
            self.askAnswer = boolean.text.lower().strip() == "true"
        elif type_ == 'CONSTRUCT':
            self.graph = g


def parseTerm(element):
    """rdflib object (Literal, URIRef, BNode) for the given
    elementtree element"""
    tag, text = element.tag, element.text
    if tag == RESULTS_NS_ET + 'literal':
        if text is None:
            text = ''
        datatype = None
        lang = None
        if element.get('datatype', None):
            datatype = URIRef(element.get('datatype'))
        elif element.get("{%s}lang" % XML_NAMESPACE, None):
            lang = element.get("{%s}lang" % XML_NAMESPACE)

        ret = Literal(text, datatype=datatype, lang=lang)

        return ret
    elif tag == RESULTS_NS_ET + 'uri':
        return URIRef(text)
    elif tag == RESULTS_NS_ET + 'bnode':
        return BNode(text)
    else:
        raise TypeError("unknown binding type %r" % element)


class XMLResultSerializer(ResultSerializer):

    def __init__(self, result):
        ResultSerializer.__init__(self, result)

    def serialize(self, stream, encoding="utf-8"):

        writer = SPARQLXMLWriter(stream, encoding)
        if self.result.type == 'ASK':
            writer.write_header([])
            writer.write_ask(self.result.askAnswer)
        else:
            writer.write_header(self.result.vars)
            writer.write_results_header()
            for b in self.result.bindings:
                writer.write_start_result()
                for key, val in b.iteritems():
                    writer.write_binding(key, val)

                writer.write_end_result()

        writer.close()


# TODO: Rewrite with ElementTree?
class SPARQLXMLWriter:
    """
    Python saxutils-based SPARQL XML Writer
    """
    def __init__(self, output, encoding='utf-8'):
        writer = XMLGenerator(output, encoding)
        writer.startDocument()
        writer.startPrefixMapping(u'sparql', SPARQL_XML_NAMESPACE)
        writer.startPrefixMapping(u'xml', XML_NAMESPACE)
        writer.startElementNS(
            (SPARQL_XML_NAMESPACE, u'sparql'),
            u'sparql', AttributesNSImpl({}, {}))
        self.writer = writer
        self._output = output
        self._encoding = encoding
        self._results = False

    def write_header(self, allvarsL):
        self.writer.startElementNS(
            (SPARQL_XML_NAMESPACE, u'head'),
            u'head', AttributesNSImpl({}, {}))
        for i in xrange(0, len(allvarsL)):
            attr_vals = {
                (None, u'name'): unicode(allvarsL[i]),
            }
            attr_qnames = {
                (None, u'name'): u'name',
            }
            self.writer.startElementNS(
                (SPARQL_XML_NAMESPACE, u'variable'),
                u'variable', AttributesNSImpl(attr_vals, attr_qnames))
            self.writer.endElementNS((SPARQL_XML_NAMESPACE,
                                      u'variable'), u'variable')
        self.writer.endElementNS((SPARQL_XML_NAMESPACE, u'head'), u'head')

    def write_ask(self, val):
        self.writer.startElementNS(
            (SPARQL_XML_NAMESPACE, u'boolean'),
            u'boolean', AttributesNSImpl({}, {}))
        self.writer.characters(str(val).lower())
        self.writer.endElementNS(
            (SPARQL_XML_NAMESPACE, u'boolean'), u'boolean')

    def write_results_header(self):
        self.writer.startElementNS(
            (SPARQL_XML_NAMESPACE, u'results'),
            u'results', AttributesNSImpl({}, {}))
        self._results = True

    def write_start_result(self):
        self.writer.startElementNS(
            (SPARQL_XML_NAMESPACE, u'result'),
            u'result', AttributesNSImpl({}, {}))
        self._resultStarted = True

    def write_end_result(self):
        assert self._resultStarted
        self.writer.endElementNS(
            (SPARQL_XML_NAMESPACE, u'result'), u'result')
        self._resultStarted = False

    def write_binding(self, name, val):
        assert self._resultStarted

        attr_vals = {
            (None, u'name'): unicode(name),
        }
        attr_qnames = {
            (None, u'name'): u'name',
        }
        self.writer.startElementNS(
            (SPARQL_XML_NAMESPACE, u'binding'),
            u'binding', AttributesNSImpl(attr_vals, attr_qnames))

        if isinstance(val, URIRef):
            self.writer.startElementNS(
                (SPARQL_XML_NAMESPACE, u'uri'),
                u'uri', AttributesNSImpl({}, {}))
            self.writer.characters(val)
            self.writer.endElementNS(
                (SPARQL_XML_NAMESPACE, u'uri'), u'uri')
        elif isinstance(val, BNode):
            self.writer.startElementNS(
                (SPARQL_XML_NAMESPACE, u'bnode'),
                u'bnode', AttributesNSImpl({}, {}))
            self.writer.characters(val)
            self.writer.endElementNS(
                (SPARQL_XML_NAMESPACE, u'bnode'), u'bnode')
        elif isinstance(val, Literal):
            attr_vals = {}
            attr_qnames = {}
            if val.language:
                attr_vals[(XML_NAMESPACE, u'lang')] = val.language
                attr_qnames[(XML_NAMESPACE, u'lang')] = u"xml:lang"
            elif val.datatype:
                attr_vals[(None, u'datatype')] = val.datatype
                attr_qnames[(None, u'datatype')] = u'datatype'

            self.writer.startElementNS(
                (SPARQL_XML_NAMESPACE, u'literal'),
                u'literal', AttributesNSImpl(attr_vals, attr_qnames))
            self.writer.characters(val)
            self.writer.endElementNS(
                (SPARQL_XML_NAMESPACE, u'literal'), u'literal')

        else:
            raise Exception("Unsupported RDF term: %s" % val)

        self.writer.endElementNS(
            (SPARQL_XML_NAMESPACE, u'binding'), u'binding')

    def close(self):
        if self._results:
            self.writer.endElementNS(
                (SPARQL_XML_NAMESPACE, u'results'), u'results')
        self.writer.endElementNS(
            (SPARQL_XML_NAMESPACE, u'sparql'), u'sparql')
        self.writer.endDocument()

########NEW FILE########
__FILENAME__ = sparql
import collections
import itertools
import datetime

from rdflib.namespace import NamespaceManager
from rdflib import Variable, BNode, Graph, ConjunctiveGraph, URIRef, Literal
from rdflib.term import Node

from parserutils import CompValue

import rdflib.plugins.sparql
from rdflib.plugins.sparql.compat import Mapping, MutableMapping


class SPARQLError(Exception):
    def __init__(self, msg=None):
        Exception.__init__(self, msg)


class NotBoundError(SPARQLError):
    def __init__(self, msg=None):
        SPARQLError.__init__(self, msg)


class AlreadyBound(SPARQLError):
    """Raised when trying to bind a variable that is already bound!"""
    def __init__(self):
        SPARQLError.__init__(self)


class SPARQLTypeError(SPARQLError):
    def __init__(self, msg):
        SPARQLError.__init__(self, msg)


class Bindings(MutableMapping):

    """

    A single level of a stack of variable-value bindings.
    Each dict keeps a reference to the dict below it,
    any failed lookup is propegated back

    In python 3.3 this could be a collections.ChainMap
    """

    def __init__(self, outer=None, d=[]):
        self._d = dict(d)
        self.outer = outer

    def __getitem__(self, key):
        try:
            return self._d[key]
        except KeyError:
            if not self.outer:
                raise
            return self.outer[key]

    def __contains__(self, key):
        try:
            self[key]
            return True
        except KeyError:
            return False

    def __setitem__(self, key, value):
        self._d[key] = value

    def __delitem__(self, key):
        raise Exception("DelItem is not implemented!")

    def __len__(self):
        i = 0
        for x in self:
            i += 1
        return i

    def __iter__(self):
        d = self
        while d is not None:
            for i in dict.__iter__(d._d):
                yield i
            d = d.outer

    def __str__(self):
        return "Bindings({"+", ".join((k, self[k]) for k in self)+"})"

    def __repr__(self):
        return unicode(self)


class FrozenDict(Mapping):
    """
    An immutable hashable dict

    Taken from http://stackoverflow.com/a/2704866/81121

    """
    def __init__(self, *args, **kwargs):
        self._d = dict(*args, **kwargs)
        self._hash = None

    def __iter__(self):
        return iter(self._d)

    def __len__(self):
        return len(self._d)

    def __getitem__(self, key):
        return self._d[key]

    def __hash__(self):
        # It would have been simpler and maybe more obvious to
        # use hash(tuple(sorted(self._d.iteritems()))) from this discussion
        # so far, but this solution is O(n). I don't know what kind of
        # n we are going to run into, but sometimes it's hard to resist the
        # urge to optimize when it will gain improved algorithmic performance.
        if self._hash is None:
            self._hash = 0
            for key, value in self.iteritems():
                self._hash ^= hash(key)
                self._hash ^= hash(value)
        return self._hash

    def project(self, vars):
        return FrozenDict(
            (x for x in self.iteritems() if x[0] in vars))

    def disjointDomain(self, other):
        return not bool(set(self).intersection(other))

    def compatible(self, other):
        for k in self:
            try:
                if self[k] != other[k]:
                    return False
            except KeyError:
                pass

        return True

    def merge(self, other):
        res = FrozenDict(
            itertools.chain(self.iteritems(), other.iteritems()))

        return res

    def __str__(self):
        return str(self._d)

    def __repr__(self):
        return repr(self._d)


class FrozenBindings(FrozenDict):

    def __init__(self, ctx, *args, **kwargs):
        FrozenDict.__init__(self, *args, **kwargs)
        self.ctx = ctx

    def __getitem__(self, key):

        if not isinstance(key, Node):
            key = Variable(key)

        if not type(key) in (BNode, Variable):
            return key

        return self._d[key]

    def project(self, vars):
        return FrozenBindings(
            self.ctx, (x for x in self.iteritems() if x[0] in vars))

    def merge(self, other):
        res = FrozenBindings(
            self.ctx, itertools.chain(self.iteritems(), other.iteritems()))

        return res

    def _now(self):
        return self.ctx.now

    def _bnodes(self):
        return self.ctx.bnodes

    def _prologue(self):
        return self.ctx.prologue

    prologue = property(_prologue)
    bnodes = property(_bnodes)
    now = property(_now)

    def forget(self, before):
        """
        return a frozen dict only of bindings made in self
        since before
        """

        return FrozenBindings(self.ctx, (x for x in self.iteritems() if before[x[0]] is None))

    def remember(self, these):
        """
        return a frozen dict only of bindings in these
        """
        return FrozenBindings(self.ctx, (x for x in self.iteritems() if x[0] in these))


class QueryContext(object):

    """
    Query context - passed along when evaluating the query
    """

    def __init__(self, graph=None, bindings=None):
        self.bindings = bindings or Bindings()

        if isinstance(graph, ConjunctiveGraph):
            self._dataset = graph
            if rdflib.plugins.sparql.SPARQL_DEFAULT_GRAPH_UNION:
                self.graph = self.dataset
            else:
                self.graph = self.dataset.default_context
        else:
            self._dataset = None
            self.graph = graph

        self.prologue = None
        self.now = datetime.datetime.now()

        self.bnodes = collections.defaultdict(BNode)

    def clone(self, bindings=None):
        r = QueryContext(
            self._dataset if self._dataset is not None else self.graph)
        r.prologue = self.prologue
        r.bindings.update(bindings or self.bindings)
        r.graph = self.graph
        r.bnodes = self.bnodes
        return r

    def _get_dataset(self):
        if self._dataset is None:
            raise Exception(
                'You performed a query operation requiring ' +
                'a dataset (i.e. ConjunctiveGraph), but ' +
                'operating currently on a single graph.')
        return self._dataset

    dataset = property(_get_dataset, doc="current dataset")

    def load(self, source, default=False, **kwargs):

        def _load(graph, source):
            try:
                return graph.load(source, **kwargs)
            except:
                pass
            try:
                return graph.load(source, format='n3', **kwargs)
            except:
                pass
            try:
                return graph.load(source, format='nt', **kwargs)
            except:
                raise Exception(
                    "Could not load %s as either RDF/XML, N3 or NTriples" % (
                    source))

        if not rdflib.plugins.sparql.SPARQL_LOAD_GRAPHS:
            # we are not loading - if we already know the graph
            # being "loaded", just add it to the default-graph
            if default:
                self.graph += self.dataset.get_context(source)
        else:

            if default:
                _load(self.graph, source)
            else:
                _load(self.dataset, source)

    def __getitem__(self, key):
        # in SPARQL BNodes are just labels
        if not type(key) in (BNode, Variable):
            return key
        try:
            return self.bindings[key]
        except KeyError:
            return None

    def get(self, key, default=None):
        try:
            return self[key]
        except KeyError:
            return default

    def solution(self, vars=None):
        """
        Return a static copy of the current variable bindings as dict
        """
        if vars:
            return FrozenBindings(
                self, ((k, v)
                       for k, v in self.bindings.iteritems()
                       if k in vars))
        else:
            return FrozenBindings(self, self.bindings.iteritems())

    def __setitem__(self, key, value):
        if key in self.bindings and self.bindings[key] != value:
            raise AlreadyBound()

        self.bindings[key] = value

    def pushGraph(self, graph):
        r = self.clone()
        r.graph = graph
        return r

    def push(self):
        r = self.clone(Bindings(self.bindings))
        return r

    def clean(self):
        return self.clone([])

    # def pop(self):
    #     self.bindings = self.bindings.outer
    #     if self.bindings is None:
    #         raise Exception("We've bottomed out of the bindings stack!")

    def thaw(self, frozenbindings):
        """
        Create a new read/write query context from the given solution
        """
        c = self.clone(frozenbindings)

        return c


class Prologue:

    """
    A class for holding prefixing bindings and base URI information
    """

    def __init__(self):
        self.base = None
        self.namespace_manager = NamespaceManager(
            Graph())  # ns man needs a store

    def resolvePName(self, prefix, localname):
        ns = self.namespace_manager.store.namespace(prefix or "")
        if ns is None:
            raise Exception('Unknown namespace prefix : %s' % prefix)
        return URIRef(ns + (localname or ""))

    def bind(self, prefix, uri):
        self.namespace_manager.bind(prefix, uri, replace=True)

    def absolutize(self, iri):

        """
        Apply BASE / PREFIXes to URIs
        (and to datatypes in Literals)

        TODO: Move resolving URIs to pre-processing
        """

        if isinstance(iri, CompValue):
            if iri.name == 'pname':
                return self.resolvePName(iri.prefix, iri.localname)
            if iri.name == 'literal':
                return Literal(
                    iri.string, lang=iri.lang,
                    datatype=self.absolutize(iri.datatype))
        elif isinstance(iri, URIRef) and not ':' in iri:  # TODO: Check for relative URI?
            return URIRef(self.base + iri)
        return iri


class Query:
    """
    A parsed and translated query
    """

    def __init__(self, prologue, algebra):
        self.prologue = prologue
        self.algebra = algebra

########NEW FILE########
__FILENAME__ = update
"""

Code for carrying out Update Operations

"""

from rdflib import Graph, Variable

from rdflib.plugins.sparql.sparql import QueryContext
from rdflib.plugins.sparql.evalutils import _fillTemplate, _join
from rdflib.plugins.sparql.evaluate import evalBGP, evalPart


def _graphOrDefault(ctx, g):
    if g == 'DEFAULT':
        return ctx.graph
    else:
        return ctx.dataset.get_context(g)


def _graphAll(ctx, g):
    """
    return a list of graphs
    """
    if g == 'DEFAULT':
        return [ctx.graph]
    elif g == 'NAMED':
        return [c for c in ctx.dataset.contexts()
                if c.identifier != ctx.graph.identifier]
    elif g == 'ALL':
        return list(ctx.dataset.contexts())
    else:
        return [ctx.dataset.get_context(g)]


def evalLoad(ctx, u):
    """
    http://www.w3.org/TR/sparql11-update/#load
    """

    if u.graphiri:
        ctx.load(u.iri, default=False, publicID=u.graphiri)
    else:
        ctx.load(u.iri, default=True)


def evalCreate(ctx, u):
    """
    http://www.w3.org/TR/sparql11-update/#create
    """
    g = ctx.datset.get_context(u.graphiri)
    if len(g) > 0:
        raise Exception("Graph %s already exists." % g.identifier)
    raise Exception("Create not implemented!")


def evalClear(ctx, u):
    """
    http://www.w3.org/TR/sparql11-update/#clear
    """

    for g in _graphAll(ctx, u.graphiri):
        g.remove((None, None, None))

def evalDrop(ctx, u):
    """
    http://www.w3.org/TR/sparql11-update/#drop
    """
    if ctx.dataset.store.graph_aware:
        for g in _graphAll(ctx, u.graphiri):
            ctx.dataset.store.remove_graph(g)
    else:
        evalClear(ctx, u)


def evalInsertData(ctx, u):
    """
    http://www.w3.org/TR/sparql11-update/#insertData
    """
    # add triples
    g = ctx.graph
    g += u.triples

    # add quads
    # u.quads is a dict of graphURI=>[triples]
    for g in u.quads:
        cg = ctx.dataset.get_context(g)
        cg += u.quads[g]


def evalDeleteData(ctx, u):
    """
    http://www.w3.org/TR/sparql11-update/#deleteData
    """
    # remove triples
    g = ctx.graph
    g -= u.triples

    # remove quads
    # u.quads is a dict of graphURI=>[triples]
    for g in u.quads:
        cg = ctx.dataset.get_context(g)
        cg -= u.quads[g]


def evalDeleteWhere(ctx, u):
    """
    http://www.w3.org/TR/sparql11-update/#deleteWhere
    """

    res = evalBGP(ctx, u.triples)
    for g in u.quads:
        cg = ctx.dataset.get_context(g)
        c = ctx.pushGraph(cg)
        res = _join(res, list(evalBGP(c, u.quads[g])))

    for c in res:
        g = ctx.graph
        g -= _fillTemplate(u.triples, c)

        for g in u.quads:
            cg = ctx.dataset.get_context(c.get(g))
            cg -= _fillTemplate(u.quads[g], c)


def evalModify(ctx, u):

    originalctx = ctx

    # Using replaces the dataset for evaluating the where-clause
    if u.using:
        otherDefault = False
        for d in u.using:
            if d.default:

                if not otherDefault:
                    # replace current default graph
                    dg = Graph()
                    ctx = ctx.pushGraph(dg)
                    otherDefault = True

                ctx.load(d.default, default=True)

            elif d.named:
                g = d.named
                ctx.load(g, default=False)

    # "The WITH clause provides a convenience for when an operation
    # primarily refers to a single graph. If a graph name is specified
    # in a WITH clause, then - for the purposes of evaluating the
    # WHERE clause - this will define an RDF Dataset containing a
    # default graph with the specified name, but only in the absence
    # of USING or USING NAMED clauses. In the presence of one or more
    # graphs referred to in USING clauses and/or USING NAMED clauses,
    # the WITH clause will be ignored while evaluating the WHERE
    # clause."
    if not u.using and u.withClause:
        g = ctx.dataset.get_context(u.withClause)
        ctx = ctx.pushGraph(g)

    res = evalPart(ctx, u.where)

    if u.using:
        if otherDefault:
            ctx = originalctx  # restore original default graph
        if u.withClause:
            g = ctx.dataset.get_context(u.withClause)
            ctx = ctx.pushGraph(g)

    for c in res:
        dg = ctx.graph
        if u.delete:
            dg -= _fillTemplate(u.delete.triples, c)

            for g, q in u.delete.quads.iteritems():
                cg = ctx.dataset.get_context(c.get(g))
                cg -= _fillTemplate(q, c)

        if u.insert:
            dg += _fillTemplate(u.insert.triples, c)

            for g, q in u.insert.quads.iteritems():
                cg = ctx.dataset.get_context(c.get(g))
                cg += _fillTemplate(q, c)


def evalAdd(ctx, u):
    """

    add all triples from src to dst

    http://www.w3.org/TR/sparql11-update/#add
    """
    src, dst = u.graph

    srcg = _graphOrDefault(ctx, src)
    dstg = _graphOrDefault(ctx, dst)

    if srcg.identifier == dstg.identifier:
        return

    dstg += srcg


def evalMove(ctx, u):
    """

    remove all triples from dst
    add all triples from src to dst
    remove all triples from src

    http://www.w3.org/TR/sparql11-update/#move
    """

    src, dst = u.graph

    srcg = _graphOrDefault(ctx, src)
    dstg = _graphOrDefault(ctx, dst)

    if srcg.identifier == dstg.identifier:
        return

    dstg.remove((None, None, None))

    dstg += srcg

    if ctx.dataset.store.graph_aware:
        ctx.dataset.store.remove_graph(srcg)
    else:
        srcg.remove((None, None, None))


def evalCopy(ctx, u):
    """

    remove all triples from dst
    add all triples from src to dst

    http://www.w3.org/TR/sparql11-update/#copy
    """

    src, dst = u.graph

    srcg = _graphOrDefault(ctx, src)
    dstg = _graphOrDefault(ctx, dst)

    if srcg.identifier == dstg.identifier:
        return

    dstg.remove((None, None, None))

    dstg += srcg


def evalUpdate(graph, update, initBindings=None):
    """

    http://www.w3.org/TR/sparql11-update/#updateLanguage

    'A request is a sequence of operations [...] Implementations MUST
    ensure that operations of a single request are executed in a
    fashion that guarantees the same effects as executing them in
    lexical order.

    Operations all result either in success or failure.

    If multiple operations are present in a single request, then a
    result of failure from any operation MUST abort the sequence of
    operations, causing the subsequent operations to be ignored.'

    This will return None on success and raise Exceptions on error

    """

    for u in update:

        ctx = QueryContext(graph)
        ctx.prologue = u.prologue

        if initBindings:
            for k, v in initBindings.iteritems():
                if not isinstance(k, Variable):
                    k = Variable(k)
                ctx[k] = v
            # ctx.push()  # nescessary?

        try:
            if u.name == 'Load':
                evalLoad(ctx, u)
            elif u.name == 'Clear':
                evalClear(ctx, u)
            elif u.name == 'Drop':
                evalDrop(ctx, u)
            elif u.name == 'Create':
                evalCreate(ctx, u)
            elif u.name == 'Add':
                evalAdd(ctx, u)
            elif u.name == 'Move':
                evalMove(ctx, u)
            elif u.name == 'Copy':
                evalCopy(ctx, u)
            elif u.name == 'InsertData':
                evalInsertData(ctx, u)
            elif u.name == 'DeleteData':
                evalDeleteData(ctx, u)
            elif u.name == 'DeleteWhere':
                evalDeleteWhere(ctx, u)
            elif u.name == 'Modify':
                evalModify(ctx, u)
            else:
                raise Exception('Unknown update operation: %s' % (u,))
        except:
            if not u.silent:
                raise

########NEW FILE########
__FILENAME__ = auditable
"""

This wrapper intercepts calls through the store interface and implements
thread-safe logging of destructive operations (adds / removes) in reverse.
This is persisted on the store instance and the reverse operations are
executed In order to return the store to the state it was when the transaction
began Since the reverse operations are persisted on the store, the store
itself acts as a transaction.

Calls to commit or rollback, flush the list of reverse operations This
provides thread-safe atomicity and isolation (assuming concurrent operations
occur with different store instances), but no durability (transactions are
persisted in memory and wont  be available to reverse operations after the
system fails): A and I out of ACID.

"""

from rdflib.store import Store
from rdflib import Graph, ConjunctiveGraph
import threading

destructiveOpLocks = {
    'add': None,
    'remove': None,
}


class AuditableStore(Store):
    def __init__(self, store):
        self.store = store
        self.context_aware = store.context_aware
        # NOTE: this store can't be formula_aware as it doesn't have enough
        # info to reverse the removal of a quoted statement
        self.formula_aware = False  # store.formula_aware
        self.transaction_aware = True  # This is only half true
        self.reverseOps = []
        self.rollbackLock = threading.RLock()

    def open(self, configuration, create=True):
        return self.store.open(configuration, create)

    def close(self, commit_pending_transaction=False):
        self.store.close()

    def destroy(self, configuration):
        self.store.destroy(configuration)

    def add(self, triple, context, quoted=False):
        (s, p, o) = triple
        lock = destructiveOpLocks['add']
        lock = lock and lock or threading.RLock()
        lock.acquire()
        context = context is not None and context.__class__(
            self.store, context.identifier) or None
        ctxId = context is not None and context.identifier or None
        self.reverseOps.append((s, p, o, ctxId, 'remove'))
        if (s, p, o, ctxId, 'add') in self.reverseOps:
            self.reverseOps.remove(
                (s, p, o, context, 'add'))
        self.store.add((s, p, o), context, quoted)
        lock.release()

    def remove(self, (subject, predicate, object_), context=None):
        lock = destructiveOpLocks['remove']
        lock = lock and lock or threading.RLock()
        lock.acquire()
        # Need to determine which quads will be removed if any term is a
        # wildcard
        context = context is not None and context.__class__(
            self.store, context.identifier) or None
        ctxId = context is not None and context.identifier or None
        if None in [subject, predicate, object_, context]:
            if ctxId:
                for s, p, o in context.triples((subject, predicate, object_)):
                    if (s, p, o, ctxId, 'remove') in self.reverseOps:
                        self.reverseOps.remove((s, p, o, ctxId, 'remove'))
                    else:
                        self.reverseOps.append((s, p, o, ctxId, 'add'))
            else:
                for s, p, o, ctx in ConjunctiveGraph(
                        self.store).quads((subject, predicate, object_)):
                    if (s, p, o, ctx.identifier, 'remove') in self.reverseOps:
                        self.reverseOps.remove(
                            (s, p, o, ctx.identifier, 'remove'))
                    else:
                        self.reverseOps.append(
                            (s, p, o, ctx.identifier, 'add'))

        elif (subject, predicate, object_, ctxId, 'add') in self.reverseOps:
            self.reverseOps.remove(
                (subject, predicate, object_, ctxId, 'add'))
        else:
            self.reverseOps.append(
                (subject, predicate, object_, ctxId, 'add'))
        self.store.remove((subject, predicate, object_), context)
        lock.release()

    def triples(self, triple, context=None):
        (su, pr, ob) = triple
        context = context is not None and context.__class__(
            self.store, context.identifier) or None
        for (s, p, o), cg in self.store.triples((su, pr, ob), context):
            yield (s, p, o), cg

    def __len__(self, context=None):
        context = context is not None and context.__class__(
            self.store, context.identifier) or None
        return self.store.__len__(context)

    def contexts(self, triple=None):
        for ctx in self.store.contexts(triple):
            yield ctx

    def bind(self, prefix, namespace):
        self.store.bind(prefix, namespace)

    def prefix(self, namespace):
        return self.store.prefix(namespace)

    def namespace(self, prefix):
        return self.store.namespace(prefix)

    def namespaces(self):
        return self.store.namespaces()

    def commit(self):
        self.store.commit()
        self.reverseOps = []

    def rollback(self):
        # Aquire Rollback lock and apply reverse operations in the forward
        # order
        self.rollbackLock.acquire()
        for subject, predicate, obj, context, op in self.reverseOps:
            if op == 'add':
                self.store.add(
                    (subject, predicate, obj), Graph(self.store, context))
            else:
                self.store.remove(
                    (subject, predicate, obj), Graph(self.store, context))

        self.reverseOps = []
        self.rollbackLock.release()

########NEW FILE########
__FILENAME__ = concurrent
from threading import Lock


class ResponsibleGenerator(object):
    """A generator that will help clean up when it is done being used."""

    __slots__ = ['cleanup', 'gen']

    def __init__(self, gen, cleanup):
        self.cleanup = cleanup
        self.gen = gen

    def __del__(self):
        self.cleanup()

    def __iter__(self):
        return self

    def next(self):
        return self.gen.next()


class ConcurrentStore(object):

    def __init__(self, store):
        self.store = store

        # number of calls to visit still in progress
        self.__visit_count = 0

        # lock for locking down the indices
        self.__lock = Lock()

        # lists for keeping track of added and removed triples while
        # we wait for the lock
        self.__pending_removes = []
        self.__pending_adds = []

    def add(self, triple):
        (s, p, o) = triple
        if self.__visit_count == 0:
            self.store.add((s, p, o))
        else:
            self.__pending_adds.append((s, p, o))

    def remove(self, triple):
        (s, p, o) = triple
        if self.__visit_count == 0:
            self.store.remove((s, p, o))
        else:
            self.__pending_removes.append((s, p, o))

    def triples(self, triple):
        (su, pr, ob) = triple
        g = self.store.triples((su, pr, ob))
        pending_removes = self.__pending_removes
        self.__begin_read()
        for s, p, o in ResponsibleGenerator(g, self.__end_read):
            if not (s, p, o) in pending_removes:
                yield s, p, o

        for (s, p, o) in self.__pending_adds:
            if (su is None or su == s) \
                    and (pr is None or pr == p) \
                    and (ob is None or ob == o):
                yield s, p, o

    def __len__(self):
        return self.store.__len__()

    def __begin_read(self):
        lock = self.__lock
        lock.acquire()
        self.__visit_count = self.__visit_count + 1
        lock.release()

    def __end_read(self):
        lock = self.__lock
        lock.acquire()
        self.__visit_count = self.__visit_count - 1
        if self.__visit_count == 0:
            pending_removes = self.__pending_removes
            while pending_removes:
                (s, p, o) = pending_removes.pop()
                try:
                    self.store.remove((s, p, o))
                except:
                    # TODO: change to try finally?
                    print s, p, o, "Not in store to remove"
            pending_adds = self.__pending_adds
            while pending_adds:
                (s, p, o) = pending_adds.pop()
                self.store.add((s, p, o))
        lock.release()

########NEW FILE########
__FILENAME__ = regexmatching
"""
This wrapper intercepts calls through the store interface which make use of
the REGEXTerm class to represent matches by REGEX instead of literal
comparison.

Implemented for stores that don't support this and essentially
provides the support by replacing the REGEXTerms by wildcards (None) and
matching against the results from the store it's wrapping.
"""

from rdflib.store import Store
from rdflib.graph import Graph
import re

# Store is capable of doing its own REGEX matching
NATIVE_REGEX = 0
# Store uses Python's re module internally for REGEX matching 
PYTHON_REGEX = 1


class REGEXTerm(unicode):
    """
    REGEXTerm can be used in any term slot and is interpreted as a request to
    perform a REGEX match (not a string comparison) using the value
    (pre-compiled) for checking rdf:type matches
    """
    def __init__(self, expr):
        self.compiledExpr = re.compile(expr)

    def __reduce__(self):
        return (REGEXTerm, (unicode(''),))


def regexCompareQuad(quad, regexQuad):
    for index in range(4):
        if isinstance(regexQuad[index], REGEXTerm) and not \
                regexQuad[index].compiledExpr.match(quad[index]):
            return False
    return True


class REGEXMatching(Store):
    def __init__(self, storage):
        self.storage = storage
        self.context_aware = storage.context_aware
        # NOTE: this store can't be formula_aware as it doesn't have enough
        # info to reverse the removal of a quoted statement.
        self.formula_aware = storage.formula_aware
        self.transaction_aware = storage.transaction_aware

    def open(self, configuration, create=True):
        return self.storage.open(configuration, create)

    def close(self, commit_pending_transaction=False):
        self.storage.close()

    def destroy(self, configuration):
        self.storage.destroy(configuration)

    def add(self, triple, context, quoted=False):
        (subject, predicate, object_) = triple
        self.storage.add((subject, predicate, object_), context, quoted)

    def remove(self, triple, context=None):
        (subject, predicate, object_) = triple
        if isinstance(subject, REGEXTerm) or \
            isinstance(predicate, REGEXTerm) or \
            isinstance(object_, REGEXTerm) or \
                (context is not None
                 and isinstance(context.identifier, REGEXTerm)):
            # One or more of the terms is a REGEX expression, so we must
            # replace it / them with wildcard(s)and match after we query.
            s = not isinstance(subject, REGEXTerm) and subject or None
            p = not isinstance(predicate, REGEXTerm) and predicate or None
            o = not isinstance(object_, REGEXTerm) and object_ or None
            c = (context is not None
                 and not isinstance(context.identifier, REGEXTerm)) \
                and context \
                or None

            removeQuadList = []
            for (s1, p1, o1), cg in self.storage.triples((s, p, o), c):
                for ctx in cg:
                    ctx = ctx.identifier
                    if regexCompareQuad(
                            (s1, p1, o1, ctx),
                            (subject, predicate, object_, context
                             is not None and context.identifier or context)):
                        removeQuadList.append((s1, p1, o1, ctx))
            for s, p, o, c in removeQuadList:
                self.storage.remove((s, p, o), c and Graph(self, c) or c)
        else:
            self.storage.remove((subject, predicate, object_), context)

    def triples(self, triple, context=None):
        (subject, predicate, object_) = triple
        if isinstance(subject, REGEXTerm) or \
            isinstance(predicate, REGEXTerm) or \
            isinstance(object_, REGEXTerm) or \
                (context is not None
                 and isinstance(context.identifier, REGEXTerm)):
            # One or more of the terms is a REGEX expression, so we must
            # replace it / them with wildcard(s) and match after we query.
            s = not isinstance(subject, REGEXTerm) and subject or None
            p = not isinstance(predicate, REGEXTerm) and predicate or None
            o = not isinstance(object_, REGEXTerm) and object_ or None
            c = (context is not None
                 and not isinstance(context.identifier, REGEXTerm)) \
                and context \
                or None
            for (s1, p1, o1), cg in self.storage.triples((s, p, o), c):
                matchingCtxs = []
                for ctx in cg:
                    if c is None:
                        if context is None \
                            or context.identifier.compiledExpr.match(
                                ctx.identifier):
                            matchingCtxs.append(ctx)
                    else:
                        matchingCtxs.append(ctx)
                if matchingCtxs \
                    and regexCompareQuad((s1, p1, o1, None),
                                         (subject, predicate, object_, None)):
                    yield (s1, p1, o1), (c for c in matchingCtxs)
        else:
            for (s1, p1, o1), cg in self.storage.triples(
                    (subject, predicate, object_), context):
                yield (s1, p1, o1), cg

    def __len__(self, context=None):
        # NOTE: If the context is a REGEX this could be an expensive
        # proposition
        return self.storage.__len__(context)

    def contexts(self, triple=None):
        # NOTE: There is no way to control REGEX matching for this method at
        # this level as it only returns the contexts, not the matching
        # triples.
        for ctx in self.storage.contexts(triple):
            yield ctx

    def remove_context(self, identifier):
        self.storage.remove((None, None, None), identifier)

    def bind(self, prefix, namespace):
        self.storage.bind(prefix, namespace)

    def prefix(self, namespace):
        return self.storage.prefix(namespace)

    def namespace(self, prefix):
        return self.storage.namespace(prefix)

    def namespaces(self):
        return self.storage.namespaces()

    def commit(self):
        self.storage.commit()

    def rollback(self):
        self.storage.rollback()

########NEW FILE########
__FILENAME__ = sparqlstore
# -*- coding: utf-8 -*-
#
"""
This is an RDFLib store around Ivan Herman et al.'s SPARQL service wrapper.
This was first done in layer-cake, and then ported to RDFLib 3 and rdfextras

This version works with vanilla SPARQLWrapper installed by ``easy_install``,
``pip`` or similar. If you installed ``rdflib`` with a tool that understands
dependencies, it should have been installed automatically for you.

Changes:
    - Layercake adding support for namespace binding, I removed it again to
      work with vanilla SPARQLWrapper
    - JSON object mapping support suppressed
    - Replaced '4Suite-XML Domlette with Elementtree
    - Incorporated as an RDFLib store

"""
SPARQL_POST_UPDATE = "application/sparql-update"
SPARQL_POST_ENCODED = "application/x-www-form-urlencoded; charset=UTF-8"

# Defines some SPARQL keywords
LIMIT = 'LIMIT'
OFFSET = 'OFFSET'
ORDERBY = 'ORDER BY'

import re
# import warnings
try:
    from SPARQLWrapper import SPARQLWrapper, XML
except ImportError:
    raise Exception(
        "SPARQLWrapper not found! SPARQL Store will not work." +
        "Install with 'easy_install SPARQLWrapper'")

import sys
if getattr(sys, 'pypy_version_info', None) is not None \
    or sys.platform.startswith('java') \
        or sys.version_info[:2] < (2, 6):
    # import elementtree as etree
    from elementtree import ElementTree
    assert ElementTree
else:
    try:
        from xml.etree import ElementTree
        assert ElementTree
    except ImportError:
        from elementtree import ElementTree

from rdflib.plugins.stores.regexmatching import NATIVE_REGEX

from rdflib.store import Store
from rdflib.query import Result
from rdflib import Variable, Namespace, BNode, URIRef, Literal

import httplib
import urlparse

class NSSPARQLWrapper(SPARQLWrapper):
    nsBindings = {}

    def setNamespaceBindings(self, bindings):
        """
        A shortcut for setting namespace bindings that will be added
        to the prolog of the query

        @param bindings: A dictionary of prefixs to URIs
        """
        self.nsBindings.update(bindings)

    def setQuery(self, query):
        """
        Set the SPARQL query text. Note: no check is done on the
        validity of the query (syntax or otherwise) by this module,
        except for testing the query type (SELECT, ASK, etc).

        Syntax and validity checking is done by the SPARQL service itself.

        @param query: query text
        @type query: string
        @bug: #2320024
        """
        self.queryType = self._parseQueryType(query)
        self.queryString = self.injectPrefixes(query)

    def injectPrefixes(self, query):
        return '\n'.join(
            ['\n'.join(['PREFIX %s: <%s>' % (key, val)
                        for key, val in self.nsBindings.items()]),
             query])

BNODE_IDENT_PATTERN = re.compile('(?P<label>_\:[^\s]+)')
SPARQL_NS = Namespace('http://www.w3.org/2005/sparql-results#')
sparqlNsBindings = {u'sparql': SPARQL_NS}
ElementTree._namespace_map["sparql"] = SPARQL_NS


def TraverseSPARQLResultDOM(doc, asDictionary=False):
    """
    Returns a generator over tuples of results
    """
    # namespace handling in elementtree xpath sub-set is not pretty :(
    vars = [Variable(v.attrib["name"]) for v in doc.findall(
            './{http://www.w3.org/2005/sparql-results#}head/' +
            '{http://www.w3.org/2005/sparql-results#}variable')]
    for result in doc.findall(
            './{http://www.w3.org/2005/sparql-results#}results/' +
            '{http://www.w3.org/2005/sparql-results#}result'):
        currBind = {}
        values = []
        for binding in result.findall(
                '{http://www.w3.org/2005/sparql-results#}binding'):
            varVal = binding.attrib["name"]
            var = Variable(varVal)
            term = CastToTerm(binding.findall('*')[0])
            values.append(term)
            currBind[var] = term
        if asDictionary:
            yield currBind, vars
        else:
            def __locproc(values):
                if len(values) == 1:
                    return values[0]
                else:
                    return tuple(values)
            yield __locproc(values), vars


def localName(qname):
    # wtf - elementtree cant do this for me
    return qname[qname.index("}") + 1:]


def CastToTerm(node):
    """
    Helper function that casts XML node in SPARQL results
    to appropriate rdflib term
    """
    if node.tag == '{%s}bnode' % SPARQL_NS:
        return BNode(node.text)
    elif node.tag == '{%s}uri' % SPARQL_NS:
        return URIRef(node.text)
    elif node.tag == '{%s}literal' % SPARQL_NS:
        if 'datatype' in node.attrib:
            dT = URIRef(node.attrib['datatype'])
            if False:  # not node.xpath('*'):
                return Literal('', datatype=dT)
            else:
                return Literal(node.text, datatype=dT)
        elif '{http://www.w3.org/XML/1998/namespace}lang' in node.attrib:
            return Literal(node.text, lang=node.attrib[
                "{http://www.w3.org/XML/1998/namespace}lang"])
        else:
            return Literal(node.text)
    else:
        raise Exception('Unknown answer type')


class SPARQLStore(NSSPARQLWrapper, Store):
    """
    An RDFLib store around a SPARQL endpoint

    This is in theory context-aware, and should work OK
    when the context is specified. (I.e. for Graph objects)
    then all queries should work against the named graph with the
    identifier of the graph only.

    For ConjunctiveGraphs, reading is done from the "default graph"
    Exactly what this means depends on your endpoint.
    General SPARQL does not offer a simple way to query the
    union of all graphs.

    Fuseki/TDB has a flag for specifying that the default graph
    is the union of all graphs (tdb:unionDefaultGraph in the Fuseki config)
    If this is set this will work fine.

    .. warning:: The SPARQL Store does not support blank-nodes!

                 As blank-nodes acts as variables in SPARQL queries
                 there is no way to query for a particular blank node.

                 See http://www.w3.org/TR/sparql11-query/#BGPsparqlBNodes


    """
    formula_aware = False
    transaction_aware = False
    regex_matching = NATIVE_REGEX

    def __init__(self,
                 endpoint=None, bNodeAsURI=False,
                 sparql11=True, context_aware=True):
        """
        """
        if endpoint:
            super(SPARQLStore, self).__init__(endpoint, returnFormat=XML)
        self.bNodeAsURI = bNodeAsURI
        self.nsBindings = {}
        self.sparql11 = sparql11
        self.context_aware = context_aware

    # Database Management Methods
    def create(self, configuration):
        raise TypeError('The SPARQL store is read only')

    def open(self, configuration, create=False):
        """
        sets the endpoint URL for this SPARQLStore
        if create==True an exception is thrown.
        """
        if create:
            raise Exception("Cannot create a SPARQL Endpoint")

        self.query_endpoint = configuration

    def __set_query_endpoint(self, queryEndpoint):
        super(SPARQLStore, self).__init__(queryEndpoint, returnFormat=XML)
        self.endpoint = queryEndpoint

    def __get_query_endpoint(self):
        return self.endpoint

    query_endpoint = property(__get_query_endpoint, __set_query_endpoint)

    def destroy(self, configuration):
        """
        FIXME: Add documentation
        """
        raise TypeError('The SPARQL store is read only')

    # Transactional interfaces
    def commit(self):
        """ """
        raise TypeError('The SPARQL store is read only')

    def rollback(self):
        """ """
        raise TypeError('The SPARQL store is read only')

    def add(self, (subject, predicate, obj), context=None, quoted=False):
        """ Add a triple to the store of triples. """
        raise TypeError('The SPARQL store is read only')

    def addN(self, quads):
        """
        Adds each item in the list of statements to a specific context.
        The quoted argument is interpreted by formula-aware stores to
        indicate this statement is quoted/hypothetical.

        Note that the default implementation is a redirect to add.
        """
        raise TypeError('The SPARQL store is read only')

    def remove(self, (subject, predicate, obj), context):
        """ Remove a triple from the store """
        raise TypeError('The SPARQL store is read only')

    def query(self, query,
              initNs={},
              initBindings={},
              queryGraph=None,
              DEBUG=False):
        self.debug = DEBUG
        assert isinstance(query, basestring)
        self.setNamespaceBindings(initNs)
        if initBindings:
            if not self.sparql11:
                raise Exception(
                    "initBindings not supported for SPARQL 1.0 Endpoints.")
            v = list(initBindings)

            # VALUES was added to SPARQL 1.1 on 2012/07/24
            query += "\nVALUES ( %s )\n{ ( %s ) }\n"\
                % (" ".join("?" + str(x) for x in v),
                   " ".join(initBindings[x].n3() for x in v))

        self.resetQuery()
        if self.context_aware and queryGraph and queryGraph != '__UNION__':
            self.addDefaultGraph(queryGraph)
        self.setQuery(query)

        return Result.parse(SPARQLWrapper.query(self).response)

    def triples(self, (s, p, o), context=None):
        """
        - tuple **(s, o, p)**
            the triple used as filter for the SPARQL select.
            (None, None, None) means anything.
        - context **context**
            the graph effectively calling this method.

        Returns a tuple of triples executing essentially a SPARQL like
        SELECT ?subj ?pred ?obj WHERE { ?subj ?pred ?obj }

        **context** may include three parameter
        to refine the underlying query:
         * LIMIT: an integer to limit the number of results
         * OFFSET: an integer to enable paging of results
         * ORDERBY: an instance of Variable('s'), Variable('o') or Variable('p')
        or, by default, the first 'None' from the given triple

        .. warning::
        - Using LIMIT or OFFSET automatically include ORDERBY otherwise this is
        because the results are retrieved in a not deterministic way (depends on
        the walking path on the graph)
        - Using OFFSET without defining LIMIT will discard the first OFFSET - 1
        results

        ``
        a_graph.LIMIT = limit
        a_graph.OFFSET = offset
        triple_generator = a_graph.triples(mytriple):
            #do something
        #Removes LIMIT and OFFSET if not required for the next triple() calls
        del a_graph.LIMIT
        del a_graph.OFFSET
        ``
        """

        if ( isinstance(s, BNode) or
             isinstance(p, BNode) or
             isinstance(o, BNode) ):
            raise Exception("SPARQLStore does not support Bnodes! "
                            "See http://www.w3.org/TR/sparql11-query/#BGPsparqlBNodes")

        vars = []
        if not s:
            s = Variable('s')
            vars.append(s)

        if not p:
            p = Variable('p')
            vars.append(p)
        if not o:
            o = Variable('o')
            vars.append(o)

        if vars:
            v = ' '.join([term.n3() for term in vars])
        else:
            v = '*'

        query = "SELECT %s WHERE { %s %s %s }" % \
            (v, s.n3(), p.n3(), o.n3())

        # The ORDER BY is necessary
        if hasattr(context, LIMIT) or hasattr(context, OFFSET) \
            or hasattr(context, ORDERBY):
            var = None
            if isinstance(s, Variable):
                var = s
            elif isinstance(p, Variable):
                var = p
            elif isinstance(o, Variable):
                var = o
            elif hasattr(context, ORDERBY) \
                    and isinstance(getattr(context, ORDERBY), Variable):
                var = getattr(context, ORDERBY)
            query = query + ' %s %s' % (ORDERBY, var.n3())

        try:
            query = query + ' LIMIT %s' % int(getattr(context, LIMIT))
        except (ValueError, TypeError, AttributeError):
            pass
        try:
            query = query + ' OFFSET %s' % int(getattr(context, OFFSET))
        except (ValueError, TypeError, AttributeError):
            pass

        self.resetQuery()
        if self.context_aware and context is not None:
            self.addDefaultGraph(context.identifier)
        self.setQuery(query)

        doc = ElementTree.parse(SPARQLWrapper.query(self).response)
        # ElementTree.dump(doc)
        for rt, vars in TraverseSPARQLResultDOM(doc, asDictionary=True):
            yield (rt.get(s, s),
                   rt.get(p, p),
                   rt.get(o, o)), None

    def triples_choices(self, (subject, predicate, object_), context=None):
        """
        A variant of triples that can take a list of terms instead of a
        single term in any slot.  Stores can implement this to optimize
        the response time from the import default 'fallback' implementation,
        which will iterate over each term in the list and dispatch to
        triples.
        """
        raise NotImplementedError('Triples choices currently not supported')

    def __len__(self, context=None):
        if not self.sparql11:
            raise NotImplementedError(
                "For performance reasons, this is not" +
                "supported for sparql1.0 endpoints")
        else:
            self.resetQuery()
            q = "SELECT (count(*) as ?c) WHERE {?s ?p ?o .}"
            if self.context_aware and context is not None:
                self.addDefaultGraph(context.identifier)
            self.setQuery(q)
            doc = ElementTree.parse(SPARQLWrapper.query(self).response)
            rt, vars = iter(
                TraverseSPARQLResultDOM(doc, asDictionary=True)).next()
            return int(rt.get(Variable("c")))

    def contexts(self, triple=None):
        """
        Iterates over results to SELECT ?NAME { GRAPH ?NAME { ?s ?p ?o } }
        returning instances of this store with the SPARQL wrapper
        object updated via addNamedGraph(?NAME)
        This causes a named-graph-uri key / value  pair to be sent over
        the protocol
        """

        if triple:
            s, p, o = triple
        else:
            s = p = o = None

        params = ((s if s else Variable('s')).n3(),
                  (p if p else Variable('p')).n3(),
                  (o if o else Variable('o')).n3())

        self.setQuery(
            'SELECT ?name WHERE { GRAPH ?name { %s %s %s }}' % params)
        doc = ElementTree.parse(SPARQLWrapper.query(self).response)

        return (rt.get(Variable("name"))
                for rt, vars in TraverseSPARQLResultDOM(doc, asDictionary=True))

    # Namespace persistence interface implementation
    def bind(self, prefix, namespace):
        self.nsBindings[prefix] = namespace

    def prefix(self, namespace):
        """ """
        return dict(
            [(v, k) for k, v in self.nsBindings.items()]
        ).get(namespace)

    def namespace(self, prefix):
        return self.nsBindings.get(prefix)

    def namespaces(self):
        for prefix, ns in self.nsBindings.items():
            yield prefix, ns


class SPARQLUpdateStore(SPARQLStore):
    """
    A store using SPARQL queries for read-access
    and SPARQL Update for changes

    This can be context-aware, if so, any changes will
    be to the given named graph only.

    For Graph objects, everything works as expected.

    .. warning:: The SPARQL Update Store does not support blank-nodes!

                 As blank-nodes acts as variables in SPARQL queries
                 there is no way to query for a particular blank node.

                 See http://www.w3.org/TR/sparql11-query/#BGPsparqlBNodes



    """

    where_pattern = re.compile(r"""(?P<where>WHERE\s*{)""", re.IGNORECASE)

    def __init__(self,
                 queryEndpoint=None, update_endpoint=None,
                 bNodeAsURI=False, sparql11=True,
                 context_aware=True,
                 postAsEncoded=True):

        SPARQLStore.__init__(self,
                             queryEndpoint, bNodeAsURI, sparql11, context_aware)

        self.connection = None

        self.update_endpoint = update_endpoint

        self.postAsEncoded = postAsEncoded
        self.headers = {'Content-type': SPARQL_POST_ENCODED,
                        'Connection': 'Keep-alive'}

        if not self.postAsEncoded:
            self.headers['Content-type'] = SPARQL_POST_UPDATE

    def __set_update_endpoint(self, update_endpoint):
        self.__update_endpoint = update_endpoint

        if self.__update_endpoint:

            p = urlparse.urlparse(self.update_endpoint)

            assert not p.username, \
                "SPARQL Update store does not support HTTP authentication"
            assert not p.password, \
                "SPARQL Update store does not support HTTP authentication"
            assert p.scheme == "http", "SPARQL Update is an http protocol!"
            self.host = p.hostname
            self.port = p.port
            self.path = p.path
            self.connection = httplib.HTTPConnection(
                self.host, self.port)

        else:

            self.host = self.port = self.path = self.connection = None

    def __get_update_endpoint(self):
        return self.__update_endpoint

    update_endpoint = property(
        __get_update_endpoint,
        __set_update_endpoint,
        doc='the HTTP URL for the Update endpoint, typically' +
            'something like http://server/dataset/update')

    def open(self, configuration, create=False):
        """
        sets the endpoint URLs for this SPARQLStore
        :param configuration: either a tuple of (queryEndpoint, update_endpoint),
            or a string with the query endpoint
        :param create: if True an exception is thrown.
        """

        if create:
            raise Exception("Cannot create a SPARQL Endpoint")

        if isinstance(configuration, tuple):
            self.query_endpoint = configuration[0]
            if len(configuration) > 1:
                self.update_endpoint = configuration[1]
        else:
            self.endpoint = configuration

        if not self.update_endpoint:
            self.update_endpoint = self.endpoint

    # Transactional interfaces
    def commit(self):
        """ """
        raise TypeError('The SPARQL Update store is not transaction aware!')

    def rollback(self):
        """ """
        raise TypeError('The SPARQL Update store is not transaction aware')

    def add(self, spo, context=None, quoted=False):
        """ Add a triple to the store of triples. """

        if not self.connection:
            raise Exception("UpdateEndpoint is not set - call 'open'")

        assert not quoted
        (subject, predicate, obj) = spo

        if ( isinstance(subject, BNode) or
             isinstance(predicate, BNode) or
             isinstance(obj, BNode) ):
            raise Exception("SPARQLStore does not support Bnodes! "
                            "See http://www.w3.org/TR/sparql11-query/#BGPsparqlBNodes")


        triple = "%s %s %s ." % (subject.n3(), predicate.n3(), obj.n3())
        if self.context_aware and context is not None:
            q = "INSERT DATA { GRAPH %s { %s } }" % (
                context.identifier.n3(), triple)
        else:
            q = "INSERT DATA { %s }" % triple
        r = self._do_update(q)
        content = r.read()  # we expect no content
        if r.status not in (200, 204):
            raise Exception("Could not update: %d %s\n%s" % (
                r.status, r.reason, content))

    def addN(self, quads):
        """ Add a list of quads to the store. """
        if not self.connection:
            raise Exception("UpdateEndpoint is not set - call 'open'")

        data = ""
        for spoc in quads:
            (subject, predicate, obj, context) = spoc

            if ( isinstance(subject, BNode) or
                 isinstance(predicate, BNode) or
                 isinstance(obj, BNode) ):
                raise Exception("SPARQLStore does not support Bnodes! "
                                "See http://www.w3.org/TR/sparql11-query/#BGPsparqlBNodes")


            triple = "%s %s %s ." % (subject.n3(), predicate.n3(), obj.n3())
            data += "INSERT DATA { GRAPH <%s> { %s } }\n" % (
                context.identifier, triple)
        r = self._do_update(data)
        content = r.read()  # we expect no content
        if r.status not in (200, 204):
            raise Exception("Could not update: %d %s\n%s" % (
                r.status, r.reason, content))

    def remove(self, spo, context):
        """ Remove a triple from the store """
        if not self.connection:
            raise Exception("UpdateEndpoint is not set - call 'open'")

        (subject, predicate, obj) = spo
        if not subject:
            subject = Variable("S")
        if not predicate:
            predicate = Variable("P")
        if not obj:
            obj = Variable("O")

        triple = "%s %s %s ." % (subject.n3(), predicate.n3(), obj.n3())
        if self.context_aware and context is not None:
            q = "DELETE { GRAPH %s { %s } } WHERE { GRAPH %s { %s } }" % (
                context.identifier.n3(), triple,
                context.identifier.n3(), triple)
        else:
            q = "DELETE { %s } WHERE { %s } " % (triple, triple)
        r = self._do_update(q)
        content = r.read()  # we expect no content
        if r.status not in (200, 204):
            raise Exception("Could not update: %d %s\n%s" % (
                r.status, r.reason, content))

    def _do_update(self, update):
        import urllib
        if self.postAsEncoded:
            update = urllib.urlencode({'update': update.encode("utf-8")})
        self.connection.request(
            'POST', self.path, update.encode("utf-8"), self.headers)
        return self.connection.getresponse()

    def update(self, query,
               initNs={},
               initBindings={},
               queryGraph=None,
               DEBUG=False):
        """
        Perform a SPARQL Update Query against the endpoint,
        INSERT, LOAD, DELETE etc.
        Setting initNs adds PREFIX declarations to the beginning of
        the update. Setting initBindings adds inline VALUEs to the
        beginning of every WHERE clause. By the SPARQL grammar, all
        operations that support variables (namely INSERT and DELETE)
        require a WHERE clause.
        Important: initBindings fails if the update contains the
        substring 'WHERE {' which does not denote a WHERE clause, e.g.
        if it is part of a literal.
        """
        self.debug = DEBUG
        assert isinstance(query, basestring)
        self.setNamespaceBindings(initNs)
        query = self.injectPrefixes(query)

        if initBindings:
            # For INSERT and DELETE the WHERE clause is obligatory
            # (http://www.w3.org/TR/2013/REC-sparql11-query-20130321/#rModify)
            # Other query types do not allow variables and don't
            # have a WHERE clause.  This also works for updates with
            # more than one INSERT/DELETE.
            v = list(initBindings)
            values = "\nVALUES ( %s )\n{ ( %s ) }\n"\
                % (" ".join("?" + str(x) for x in v),
                   " ".join(initBindings[x].n3() for x in v))

            query = self.where_pattern.sub("WHERE { " + values, query)

        r = self._do_update(query)
        content = r.read()  # we expect no content
        if r.status not in (200, 204):
            raise Exception("Could not update: %d %s\n%s" % (
                r.status, r.reason, content))

########NEW FILE########
__FILENAME__ = py3compat
"""
Utility functions and objects to ease Python 3 compatibility.
"""
import sys
import re
import codecs

try:
    from functools import wraps
    assert wraps
except ImportError:
    # No-op wraps decorator
    def wraps(f):
        def dec(newf):
            return newf
        return dec


def cast_bytes(s, enc='utf-8'):
    if isinstance(s, unicode):
        return s.encode(enc)
    return s

PY3 = (sys.version_info[0] >= 3)


def _modify_str_or_docstring(str_change_func):
    @wraps(str_change_func)
    def wrapper(func_or_str):
        if isinstance(func_or_str, str):
            func = None
            doc = func_or_str
        else:
            func = func_or_str
            doc = func.__doc__

        doc = str_change_func(doc)

        if func:
            func.__doc__ = doc
            return func
        return doc
    return wrapper


if PY3:
    # Python 3:
    # ---------
    def b(s):
        return s.encode('ascii')

    def ascii(stream):
        return codecs.getreader('ascii')(stream)

    def bopen(*args, **kwargs):
        return open(*args, mode = 'rb', **kwargs)

    bytestype = bytes

    # Abstract u'abc' syntax:
    @_modify_str_or_docstring
    def format_doctest_out(s):
        """Python 2 version
        "%(u)s'abc'" --> "'abc'"
        "%(b)s'abc'" --> "b'abc'"
        "55%(L)s"    --> "55"
        "unicode(x)" --> "str(x)"

        Accepts a string or a function, so it can be used as a decorator."""
        # s may be None if processed by Py2exe
        if s is None:
            return ''
        return s % {'u': '', 'b': 'b', 'L': '', 'unicode': 'str'}

    def type_cmp(a, b):
        """Python 2 style comparison based on type"""
        ta, tb = type(a).__name__, type(b).__name__
        # Ugly hack: some tests rely on tuple sorting before unicode, and I
        # don't know if that's important. Better retain it for now.
        if ta == 'str':
            ta = 'unicode'
        if tb == 'str':
            tb = 'unicode'
        # return 1 if ta > tb else -1 if ta < tb else 0
        if ta > tb:
            return 1
        elif ta < tb:
            return -1
        else:
            return 0

    def sign(n):
        if n < 0:
            return -1
        if n > 0:
            return 1
        return 0

else:
    # Python 2
    # --------
    def b(s):
        return s

    def ascii(stream):
        return stream

    bopen = open

    bytestype = str

    # Abstract u'abc' syntax:
    @_modify_str_or_docstring
    def format_doctest_out(s):
        """Python 2 version
        "%(u)s'abc'" --> "u'abc'"
        "%(b)s'abc'" --> "'abc'"
        "55%(L)s"    --> "55L"

        Accepts a string or a function, so it can be used as a decorator."""
        # s may be None if processed by Py2exe
        if s is None:
            return ''
        return s % {'u': 'u', 'b': '', 'L': 'L', 'unicode': 'unicode'}

    def type_cmp(a, b):
        # return 1 if a > b else -1 if a < b else 0
        if a > b:
            return 1
        elif a < b:
            return -1
        else:
            return 0

    def sign(n):
        return cmp(n, 0)

r_unicodeEscape = re.compile(r'(\\u[0-9A-Fa-f]{4}|\\U[0-9A-Fa-f]{8})')

def _unicodeExpand(s):
    return r_unicodeEscape.sub(lambda m: unichr(int(m.group(0)[2:], 16)), s)

def decodeStringEscape(s):

    """
    s is byte-string - replace \ escapes in string
    """

    if not PY3:
        s = s.decode('string-escape')
    else:
        s = s.replace('\\t', '\t')
        s = s.replace('\\n', '\n')
        s = s.replace('\\r', '\r')
        s = s.replace('\\b', '\b')
        s = s.replace('\\f', '\f')
        s = s.replace('\\"', '"')
        s = s.replace("\\'", "'")
        s = s.replace('\\\\', '\\')

    return s
    #return _unicodeExpand(s) # hmm - string escape doesn't do unicode escaping

def decodeUnicodeEscape(s):
    """
    s is a unicode string
    replace \n and \u00AC unicode escapes
    """
    if not PY3:
        s = s.encode('utf-8').decode('string-escape')
        s = _unicodeExpand(s)
    else:
        s = s.replace('\\t', '\t')
        s = s.replace('\\n', '\n')
        s = s.replace('\\r', '\r')
        s = s.replace('\\b', '\b')
        s = s.replace('\\f', '\f')
        s = s.replace('\\"', '"')
        s = s.replace("\\'", "'")
        s = s.replace('\\\\', '\\')

        s = _unicodeExpand(s) # hmm - string escape doesn't do unicode escaping

    return s

########NEW FILE########
__FILENAME__ = query

import os
import shutil
import tempfile
import warnings
import types

from urlparse import urlparse
try:
    from io import BytesIO
    assert BytesIO
except:
    from StringIO import StringIO as BytesIO

from . import py3compat

__all__ = ['Processor', 'Result', 'ResultParser', 'ResultSerializer',
           'ResultException']




class Processor(object):
    """
    Query plugin interface.

    This module is useful for those wanting to write a query processor
    that can plugin to rdf. If you are wanting to execute a query you
    likely want to do so through the Graph class query method.

    """

    def __init__(self, graph):
        pass

    def query(self, strOrQuery, initBindings={}, initNs={}, DEBUG=False):
        pass

class UpdateProcessor(object):
    """
    Update plugin interface.

    This module is useful for those wanting to write an update
    processor that can plugin to rdflib. If you are wanting to execute
    an update statement you likely want to do so through the Graph
    class update method.

    .. versionadded:: 4.0

    """

    def __init__(self, graph):
        pass
    def update(self, strOrQuery, initBindings={}, initNs={}):
        pass

class ResultException(Exception):
    pass


class EncodeOnlyUnicode(object):
    """
    This is a crappy work-around for
    http://bugs.python.org/issue11649


    """

    def __init__(self, stream):
        self.__stream = stream

    def write(self, arg):
        if isinstance(arg, unicode):
            self.__stream.write(arg.encode("utf-8"))
        else:
            self.__stream.write(arg)

    def __getattr__(self, name):
        return getattr(self.__stream, name)


class ResultRow(tuple):
    """
    a single result row
    allows accessing bindings as attributes or with []

    >>> from rdflib import URIRef, Variable
    >>> rr=ResultRow({ Variable('a'): URIRef('urn:cake') }, [Variable('a')])

    >>> rr[0]
    rdflib.term.URIRef(%(u)s'urn:cake')
    >>> rr[1]
    Traceback (most recent call last):
        ...
    IndexError: tuple index out of range

    >>> rr.a
    rdflib.term.URIRef(%(u)s'urn:cake')
    >>> rr.b
    Traceback (most recent call last):
        ...
    AttributeError: b

    >>> rr['a']
    rdflib.term.URIRef(%(u)s'urn:cake')
    >>> rr['b']
    Traceback (most recent call last):
        ...
    KeyError: 'b'

    >>> rr[Variable('a')]
    rdflib.term.URIRef(%(u)s'urn:cake')

    .. versionadded:: 4.0

    """
    __doc__ = py3compat.format_doctest_out(__doc__)

    def __new__(cls, values, labels):

        instance = super(ResultRow, cls).__new__(
            cls, (values.get(v) for v in labels))
        instance.labels = dict((unicode(x[1]), x[0])
                               for x in enumerate(labels))
        return instance

    def __getattr__(self, name):
        if name not in self.labels:
            raise AttributeError(name)
        return tuple.__getitem__(self, self.labels[name])

    def __getitem__(self, name):
        try:
            return tuple.__getitem__(self, name)
        except TypeError:
            if name in self.labels:
                return tuple.__getitem__(self, self.labels[name])
            if unicode(name) in self.labels:  # passing in variable object
                return tuple.__getitem__(self, self.labels[unicode(name)])
            raise KeyError(name)

    def asdict(self):
        return dict((v, self[v]) for v in self.labels if self[v] != None)


class Result(object):
    """
    A common class for representing query result.

    There is a bit of magic here that makes this appear like different
    Python objects, depending on the type of result.

    If the type is "SELECT", iterating will yield lists of QueryRow objects

    If the type is "ASK", iterating will yield a single bool (or
    bool(result) will return the same bool)

    If the type is "CONSTRUCT" or "DESCRIBE" iterating will yield the
    triples.

    len(result) also works.

    """
    def __init__(self, type_):

        if type_ not in ('CONSTRUCT', 'DESCRIBE', 'SELECT', 'ASK'):
            raise ResultException('Unknown Result type: %s' % type_)

        self.type = type_
        self.vars = None
        self._bindings = None
        self._genbindings = None
        self.askAnswer = None
        self.graph = None

    def _get_bindings(self):
        if self._genbindings:
            self._bindings += list(self._genbindings)
            self._genbindings = None

        return self._bindings

    def _set_bindings(self, b):
        if isinstance(b, types.GeneratorType):
            self._genbindings = b
            self._bindings = []
        else:
            self._bindings = b

    bindings = property(
        _get_bindings, _set_bindings, doc="a list of variable bindings as dicts")

    @staticmethod
    def parse(source, format='xml', **kwargs):
        from rdflib import plugin
        parser = plugin.get(format, ResultParser)()
        return parser.parse(source, **kwargs)

    def serialize(
            self, destination=None, encoding="utf-8", format='xml', **args):

        if self.type in ('CONSTRUCT', 'DESCRIBE'):
            return self.graph.serialize(
                destination, encoding=encoding, format=format, **args)

        """stolen wholesale from graph.serialize"""
        from rdflib import plugin
        serializer = plugin.get(format, ResultSerializer)(self)
        if destination is None:
            stream = BytesIO()
            stream2 = EncodeOnlyUnicode(stream)
            serializer.serialize(stream2, encoding=encoding, **args)
            return stream.getvalue()
        if hasattr(destination, "write"):
            stream = destination
            serializer.serialize(stream, encoding=encoding, **args)
        else:
            location = destination
            scheme, netloc, path, params, query, fragment = urlparse(location)
            if netloc != "":
                print("WARNING: not saving as location" +
                      "is not a local file reference")
                return
            fd, name = tempfile.mkstemp()
            stream = os.fdopen(fd, 'wb')
            serializer.serialize(stream, encoding=encoding, **args)
            stream.close()
            if hasattr(shutil, "move"):
                shutil.move(name, path)
            else:
                shutil.copy(name, path)
                os.remove(name)

    def __len__(self):
        if self.type == 'ASK':
            return 1
        elif self.type == 'SELECT':
            return len(self.bindings)
        else:
            return len(self.graph)

    def __nonzero__(self):
        if self.type == 'ASK':
            return self.askAnswer
        else:
            return len(self)>0

    def __iter__(self):
        if self.type in ("CONSTRUCT", "DESCRIBE"):
            for t in self.graph:
                yield t
        elif self.type == 'ASK':
            yield self.askAnswer
        elif self.type == 'SELECT':
            # this iterates over ResultRows of variable bindings

            if self._genbindings:
                for b in self._genbindings:
                    self._bindings.append(b)
                    yield ResultRow(b, self.vars)
                self._genbindings = None
            else:
                for b in self._bindings:
                    yield ResultRow(b, self.vars)

    def __getattr__(self, name):
        if self.type in ("CONSTRUCT", "DESCRIBE") and self.graph is not None:
            return self.graph.__getattr__(self, name)
        elif self.type == 'SELECT' and name == 'result':
            warnings.warn(
                "accessing the 'result' attribute is deprecated."
                " Iterate over the object instead.",
                DeprecationWarning, stacklevel=2)
            # copied from __iter__, above
            return [(tuple(b[v] for v in self.vars)) for b in self.bindings]
        else:
            raise AttributeError(
                "'%s' object has no attribute '%s'" % (self, name))

    def __eq__(self, other):
        try:
            if self.type != other.type:
                return False
            if self.type == 'ASK':
                return self.askAnswer == other.askAnswer
            elif self.type == 'SELECT':
                return self.vars == other.vars \
                    and self.bindings == other.bindings
            else:
                return self.graph == other.graph

        except:
            return False


class ResultParser(object):

    def __init__(self):
        pass

    def parse(self, source, **kwargs):
        """return a Result object"""
        pass  # abstract


class ResultSerializer(object):

    def __init__(self, result):
        self.result = result

    def serialize(self, stream, encoding="utf-8", **kwargs):
        """return a string properly serialized"""
        pass  # abstract

########NEW FILE########
__FILENAME__ = resource
# -*- coding: utf-8 -*-
from rdflib import py3compat

__doc__ = py3compat.format_doctest_out("""
The :class:`~rdflib.resource.Resource` class wraps a
:class:`~rdflib.graph.Graph`
and a resource reference (i.e. a :class:`rdflib.term.URIRef` or
:class:`rdflib.term.BNode`) to support a resource-oriented way of
working with a graph.

It contains methods directly corresponding to those methods of the Graph
interface that relate to reading and writing data. The difference is that a
Resource also binds a resource identifier, making it possible to work without
tracking both the graph and a current subject. This makes for a "resource
oriented" style, as compared to the triple orientation of the Graph API.

Resulting generators are also wrapped so that any resource reference values
(:class:`rdflib.term.URIRef`s and :class:`rdflib.term.BNode`s) are in turn
wrapped as Resources. (Note that this behaviour differs from the corresponding
methods in :class:`~rdflib.graph.Graph`, where no such conversion takes place.)


Basic Usage Scenario
--------------------

Start by importing things we need and define some namespaces::

    >>> from rdflib import *
    >>> FOAF = Namespace("http://xmlns.com/foaf/0.1/")
    >>> CV = Namespace("http://purl.org/captsolo/resume-rdf/0.2/cv#")

Load some RDF data::

    >>> graph = Graph().parse(format='n3', data='''
    ... @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
    ... @prefix xsd: <http://www.w3.org/2001/XMLSchema#>.
    ... @prefix foaf: <http://xmlns.com/foaf/0.1/> .
    ... @prefix cv: <http://purl.org/captsolo/resume-rdf/0.2/cv#> .
    ...
    ... @base <http://example.org/> .
    ...
    ... </person/some1#self> a foaf:Person;
    ...     rdfs:comment "Just a Python & RDF hacker."@en;
    ...     foaf:depiction </images/person/some1.jpg>;
    ...     foaf:homepage <http://example.net/>;
    ...     foaf:name "Some Body" .
    ...
    ... </images/person/some1.jpg> a foaf:Image;
    ...     rdfs:label "some 1"@en;
    ...     rdfs:comment "Just an image"@en;
    ...     foaf:thumbnail </images/person/some1-thumb.jpg> .
    ...
    ... </images/person/some1-thumb.jpg> a foaf:Image .
    ...
    ... [] a cv:CV;
    ...     cv:aboutPerson </person/some1#self>;
    ...     cv:hasWorkHistory [ cv:employedIn </#company>;
    ...             cv:startDate "2009-09-04"^^xsd:date ] .
    ... ''')

Create a Resource::

    >>> person = Resource(
    ...     graph, URIRef("http://example.org/person/some1#self"))

Retrieve some basic facts::

    >>> person.identifier
    rdflib.term.URIRef(%(u)s'http://example.org/person/some1#self')

    >>> person.value(FOAF.name)
    rdflib.term.Literal(%(u)s'Some Body')

    >>> person.value(RDFS.comment)
    rdflib.term.Literal(%(u)s'Just a Python & RDF hacker.', lang=%(u)s'en')

Resources can be sliced (like graphs, but the subject is fixed)::

    >>> for name in person[FOAF.name]:
    ...     print(name)
    Some Body
    >>> person[FOAF.name : Literal("Some Body")]
    True

Resources as unicode are represented by their identifiers as unicode::

    >>> %(unicode)s(person)  #doctest: +SKIP
    %(u)s'Resource(http://example.org/person/some1#self'

Resource references are also Resources, so you can easily get e.g. a qname
for the type of a resource, like::

    >>> person.value(RDF.type).qname()
    %(u)s'foaf:Person'

Or for the predicates of a resource::

    >>> sorted(
    ...     p.qname() for p in person.predicates()
    ... )  #doctest: +NORMALIZE_WHITESPACE +SKIP
    [%(u)s'foaf:depiction', %(u)s'foaf:homepage',
     %(u)s'foaf:name', %(u)s'rdf:type', %(u)s'rdfs:comment']

Follow relations and get more data from their Resources as well::

    >>> for pic in person.objects(FOAF.depiction):
    ...     print(pic.identifier)
    ...     print(pic.value(RDF.type).qname())
    ...     print(pic.label())
    ...     print(pic.comment())
    ...     print(pic.value(FOAF.thumbnail).identifier)
    http://example.org/images/person/some1.jpg
    foaf:Image
    some 1
    Just an image
    http://example.org/images/person/some1-thumb.jpg

    >>> for cv in person.subjects(CV.aboutPerson):
    ...     work = list(cv.objects(CV.hasWorkHistory))[0]
    ...     print(work.value(CV.employedIn).identifier)
    ...     print(work.value(CV.startDate))
    http://example.org/#company
    2009-09-04

It's just as easy to work with the predicates of a resource::

    >>> for s, p in person.subject_predicates():
    ...     print(s.value(RDF.type).qname())
    ...     print(p.qname())
    ...     for s, o in p.subject_objects():
    ...         print(s.value(RDF.type).qname())
    ...         print(o.value(RDF.type).qname())
    cv:CV
    cv:aboutPerson
    cv:CV
    foaf:Person

This is useful for e.g. inspection::

    >>> thumb_ref = URIRef("http://example.org/images/person/some1-thumb.jpg")
    >>> thumb = Resource(graph, thumb_ref)
    >>> for p, o in thumb.predicate_objects():
    ...     print(p.qname())
    ...     print(o.qname())
    rdf:type
    foaf:Image

Similarly, adding, setting and removing data is easy::

    >>> thumb.add(RDFS.label, Literal("thumb"))
    >>> print(thumb.label())
    thumb
    >>> thumb.set(RDFS.label, Literal("thumbnail"))
    >>> print(thumb.label())
    thumbnail
    >>> thumb.remove(RDFS.label)
    >>> list(thumb.objects(RDFS.label))
    []


Schema Example
--------------

With this artificial schema data::

    >>> graph = Graph().parse(format='n3', data='''
    ... @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
    ... @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
    ... @prefix owl: <http://www.w3.org/2002/07/owl#> .
    ... @prefix v: <http://example.org/def/v#> .
    ...
    ... v:Artifact a owl:Class .
    ...
    ... v:Document a owl:Class;
    ...     rdfs:subClassOf v:Artifact .
    ...
    ... v:Paper a owl:Class;
    ...     rdfs:subClassOf v:Document .
    ...
    ... v:Choice owl:oneOf (v:One v:Other) .
    ...
    ... v:Stuff a rdf:Seq; rdf:_1 v:One; rdf:_2 v:Other .
    ...
    ... ''')

From this class::

    >>> artifact = Resource(graph, URIRef("http://example.org/def/v#Artifact"))

we can get at subclasses::

    >>> subclasses = list(artifact.transitive_subjects(RDFS.subClassOf))
    >>> [c.qname() for c in subclasses]
    [%(u)s'v:Artifact', %(u)s'v:Document', %(u)s'v:Paper']

and superclasses from the last subclass::

    >>> [c.qname() for c in subclasses[-1].transitive_objects(RDFS.subClassOf)]
    [%(u)s'v:Paper', %(u)s'v:Document', %(u)s'v:Artifact']

Get items from the Choice::

    >>> choice = Resource(graph, URIRef("http://example.org/def/v#Choice"))
    >>> [it.qname() for it in choice.value(OWL.oneOf).items()]
    [%(u)s'v:One', %(u)s'v:Other']

And the sequence of Stuff::

    >>> stuff = Resource(graph, URIRef("http://example.org/def/v#Stuff"))
    >>> [it.qname() for it in stuff.seq()]
    [%(u)s'v:One', %(u)s'v:Other']

On add, other resources are auto-unboxed:
    >>> paper = Resource(graph, URIRef("http://example.org/def/v#Paper"))
    >>> paper.add(RDFS.subClassOf, artifact)
    >>> artifact in paper.objects(RDFS.subClassOf) # checks Resource instance
    True
    >>> (paper._identifier, RDFS.subClassOf, artifact._identifier) in graph
    True


Technical Details
-----------------

Comparison is based on graph and identifier::

    >>> g1 = Graph()
    >>> t1 = Resource(g1, URIRef("http://example.org/thing"))
    >>> t2 = Resource(g1, URIRef("http://example.org/thing"))
    >>> t3 = Resource(g1, URIRef("http://example.org/other"))
    >>> t4 = Resource(Graph(), URIRef("http://example.org/other"))

    >>> t1 is t2
    False

    >>> t1 == t2
    True
    >>> t1 != t2
    False

    >>> t1 == t3
    False
    >>> t1 != t3
    True

    >>> t3 != t4
    True

    >>> t3 < t1 and t1 > t3
    True
    >>> t1 >= t1 and t1 >= t3
    True
    >>> t1 <= t1 and t3 <= t1
    True

    >>> t1 < t1 or t1 < t3 or t3 > t1 or t3 > t3
    False

Hash is computed from graph and identifier::

    >>> g1 = Graph()
    >>> t1 = Resource(g1, URIRef("http://example.org/thing"))

    >>> hash(t1) == hash(Resource(g1, URIRef("http://example.org/thing")))
    True

    >>> hash(t1) == hash(Resource(Graph(), t1.identifier))
    False
    >>> hash(t1) == hash(Resource(Graph(), URIRef("http://example.org/thing")))
    False

The Resource class is suitable as a base class for mapper toolkits. For
example, consider this utility for accessing RDF properties via qname-like
attributes::

    >>> class Item(Resource):
    ...
    ...     def __getattr__(self, p):
    ...         return list(self.objects(self._to_ref(*p.split('_', 1))))
    ...
    ...     def _to_ref(self, pfx, name):
    ...         return URIRef(self._graph.store.namespace(pfx) + name)

It works as follows::

    >>> graph = Graph().parse(format='n3', data='''
    ... @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
    ... @prefix foaf: <http://xmlns.com/foaf/0.1/> .
    ...
    ... @base <http://example.org/> .
    ... </person/some1#self>
    ...     foaf:name "Some Body";
    ...     foaf:depiction </images/person/some1.jpg> .
    ... </images/person/some1.jpg> rdfs:comment "Just an image"@en .
    ... ''')

    >>> person = Item(graph, URIRef("http://example.org/person/some1#self"))

    >>> print(person.foaf_name[0])
    Some Body

The mechanism for wrapping references as resources cooperates with subclasses.
Therefore, accessing referenced resources automatically creates new ``Item``
objects::

    >>> isinstance(person.foaf_depiction[0], Item)
    True

    >>> print(person.foaf_depiction[0].rdfs_comment[0])
    Just an image

""")

from rdflib.term import Node, BNode, URIRef
from rdflib.namespace import RDF
from rdflib.paths import Path

__all__ = ['Resource']


class Resource(object):

    def __init__(self, graph, subject):
        self._graph = graph
        self._identifier = subject

    graph = property(lambda self: self._graph)

    identifier = property(lambda self: self._identifier)

    def __hash__(self):
        return hash(Resource) ^ hash(self._graph) ^ hash(self._identifier)

    def __eq__(self, other):
        return (isinstance(other, Resource) and
                self._graph == other._graph and
                self._identifier == other._identifier)

    __ne__ = lambda self, other: not self == other

    def __lt__(self, other):
        if isinstance(other, Resource):
            return self._identifier < other._identifier
        else:
            return False

    __gt__ = lambda self, other: not (self < other or self == other)
    __le__ = lambda self, other: self < other or self == other
    __ge__ = lambda self, other: not self < other

    def __unicode__(self):
        return unicode(self._identifier)

    if py3compat.PY3:
        __str__ = __unicode__

    def add(self, p, o):
        if isinstance(o, Resource):
            o = o._identifier

        self._graph.add((self._identifier, p, o))

    def remove(self, p, o=None):
        if isinstance(o, Resource):
            o = o._identifier

        self._graph.remove((self._identifier, p, o))

    def set(self, p, o):
        if isinstance(o, Resource):
            o = o._identifier

        self._graph.set((self._identifier, p, o))

    def subjects(self, predicate=None):  # rev
        return self._resources(
            self._graph.subjects(predicate, self._identifier))

    def predicates(self, o=None):
        if isinstance(o, Resource):
            o = o._identifier

        return self._resources(
            self._graph.predicates(self._identifier, o))

    def objects(self, predicate=None):
        return self._resources(
            self._graph.objects(self._identifier, predicate))

    def subject_predicates(self):
        return self._resource_pairs(
            self._graph.subject_predicates(self._identifier))

    def subject_objects(self):
        return self._resource_pairs(
            self._graph.subject_objects(self._identifier))

    def predicate_objects(self):
        return self._resource_pairs(
            self._graph.predicate_objects(self._identifier))

    def value(self, p=RDF.value, o=None, default=None, any=True):
        if isinstance(o, Resource):
            o = o._identifier

        return self._cast(
            self._graph.value(self._identifier, p, o, default, any))

    def label(self):
        return self._graph.label(self._identifier)

    def comment(self):
        return self._graph.comment(self._identifier)

    def items(self):
        return self._resources(self._graph.items(self._identifier))

    def transitive_objects(self, predicate, remember=None):
        return self._resources(self._graph.transitive_objects(
            self._identifier, predicate, remember))

    def transitive_subjects(self, predicate, remember=None):
        return self._resources(self._graph.transitive_subjects(
            predicate, self._identifier, remember))

    def seq(self):
        return self._resources(self._graph.seq(self._identifier))

    def qname(self):
        return self._graph.qname(self._identifier)

    def _resource_pairs(self, pairs):
        for s1, s2 in pairs:
            yield self._cast(s1), self._cast(s2)

    def _resource_triples(self, triples):
        for s,p,o in triples:
            yield self._cast(s), self._cast(p), self._cast(o)

    def _resources(self, nodes):
        for node in nodes:
            yield self._cast(node)

    def _cast(self, node):
        if isinstance(node, (BNode, URIRef)):
            return self._new(node)
        else:
            return node

    def __iter__(self):
        return self._resource_triples(self._graph.triples((self.identifier, None, None)))

    def __getitem__(self, item):
        if isinstance(item, slice):
            if item.step:
                raise TypeError("Resources fix the subject for slicing, and can only be sliced by predicate/object. ")
            p,o=item.start,item.stop
            if p is None and o is None:
                return self.predicate_objects()
            elif p is None:
                return self.predicates(o)
            elif o is None:
                return self.objects(p)
            else:
                return (self.identifier, p, o) in self._graph
        elif isinstance(item, (Node, Path)):
            return self.objects(item)
        else:
            raise TypeError("You can only index a resource by a single rdflib term, a slice of rdflib terms, not %s (%s)"%(item, type(item)))

    def __setitem__(self, item, value):
        self.set(item, value)

    def _new(self, subject):
        return type(self)(self._graph, subject)

    def __str__(self):
        return 'Resource(%s)' % self._identifier

    def __repr__(self):
        return 'Resource(%s,%s)' % (self._graph, self._identifier)

########NEW FILE########
__FILENAME__ = serializer
"""
Serializer plugin interface.

This module is useful for those wanting to write a serializer that can
plugin to rdflib. If you are wanting to invoke a serializer you likely
want to do so through the Graph class serialize method.

TODO: info for how to write a serializer that can plugin to rdflib.
See also rdflib.plugin

"""

from rdflib.term import URIRef

__all__ = ['Serializer']


class Serializer(object):

    def __init__(self, store):
        self.store = store
        self.encoding = "UTF-8"
        self.base = None

    def serialize(self, stream, base=None, encoding=None, **args):
        """Abstract method"""

    def relativize(self, uri):
        base = self.base
        if base is not None and uri.startswith(base):
            uri = URIRef(uri.replace(base, "", 1))
        return uri

########NEW FILE########
__FILENAME__ = store
"""
============
rdflib.store
============

Types of store
--------------

``Context-aware``: An RDF store capable of storing statements within contexts
is considered context-aware. Essentially, such a store is able to partition
the RDF model it represents into individual, named, and addressable
sub-graphs.

Relevant Notation3 reference regarding formulae, quoted statements, and such:
http://www.w3.org/DesignIssues/Notation3.html

``Formula-aware``: An RDF store capable of distinguishing between statements
that are asserted and statements that are quoted is considered formula-aware.

``Transaction-capable``: capable of providing transactional integrity to the
RDF operations performed on it.

``Graph-aware``: capable of keeping track of empty graphs.

------
"""

# Constants representing the state of a Store (returned by the open method)
VALID_STORE = 1
CORRUPTED_STORE = 0
NO_STORE = -1
UNKNOWN = None

from rdflib.events import Dispatcher, Event

__all__ = ['StoreCreatedEvent', 'TripleAddedEvent', 'TripleRemovedEvent',
           'NodePickler', 'Store']


class StoreCreatedEvent(Event):
    """
    This event is fired when the Store is created, it has the following
    attribute:

      - ``configuration``: string used to create the store

    """


class TripleAddedEvent(Event):
    """
    This event is fired when a triple is added, it has the following
    attributes:

      - the ``triple`` added to the graph
      - the ``context`` of the triple, if any
      - the ``graph`` to which the triple was added
    """


class TripleRemovedEvent(Event):
    """
    This event is fired when a triple is removed, it has the following
    attributes:

      - the ``triple`` removed from the graph
      - the ``context`` of the triple, if any
      - the ``graph`` from which the triple was removed
    """

from cPickle import Pickler, Unpickler, UnpicklingError
try:
    from io import BytesIO
    assert BytesIO
except ImportError:
    from cStringIO import StringIO as BytesIO


class NodePickler(object):
    def __init__(self):
        self._objects = {}
        self._ids = {}
        self._get_object = self._objects.__getitem__

    def _get_ids(self, key):
        try:
            return self._ids.get(key)
        except TypeError:
            return None

    def register(self, object, id):
        self._objects[id] = object
        self._ids[object] = id

    def loads(self, s):
        up = Unpickler(BytesIO(s))
        up.persistent_load = self._get_object
        try:
            return up.load()
        except KeyError, e:
            raise UnpicklingError("Could not find Node class for %s" % e)

    def dumps(self, obj, protocol=None, bin=None):
        src = BytesIO()
        p = Pickler(src)
        p.persistent_id = self._get_ids
        p.dump(obj)
        return src.getvalue()

    def __getstate__(self):
        state = self.__dict__.copy()
        del state['_get_object']
        state.update({
            '_ids': tuple(self._ids.items()),
            '_objects': tuple(self._objects.items())
            })
        return state

    def __setstate__(self, state):
        self.__dict__.update(state)
        self._ids = dict(self._ids)
        self._objects = dict(self._objects)
        self._get_object = self._objects.__getitem__


class Store(object):
    # Properties
    context_aware = False
    formula_aware = False
    transaction_aware = False
    graph_aware = False

    def __init__(self, configuration=None, identifier=None):
        """
        identifier: URIRef of the Store. Defaults to CWD
        configuration: string containing infomation open can use to
        connect to datastore.
        """
        self.__node_pickler = None
        self.dispatcher = Dispatcher()
        if configuration:
            self.open(configuration)

    def __get_node_pickler(self):
        if self.__node_pickler is None:
            from rdflib.term import URIRef
            from rdflib.term import BNode
            from rdflib.term import Literal
            from rdflib.graph import Graph, QuotedGraph
            from rdflib.term import Variable
            from rdflib.term import Statement
            self.__node_pickler = np = NodePickler()
            np.register(self, "S")
            np.register(URIRef, "U")
            np.register(BNode, "B")
            np.register(Literal, "L")
            np.register(Graph, "G")
            np.register(QuotedGraph, "Q")
            np.register(Variable, "V")
            np.register(Statement, "s")
        return self.__node_pickler
    node_pickler = property(__get_node_pickler)

    # Database management methods
    def create(self, configuration):
        self.dispatcher.dispatch(
            StoreCreatedEvent(configuration=configuration))

    def open(self, configuration, create=False):
        """
        Opens the store specified by the configuration string. If
        create is True a store will be created if it does not already
        exist. If create is False and a store does not already exist
        an exception is raised. An exception is also raised if a store
        exists, but there is insufficient permissions to open the
        store.  This should return one of:
        VALID_STORE, CORRUPTED_STORE, or NO_STORE
        """
        return UNKNOWN

    def close(self, commit_pending_transaction=False):
        """
        This closes the database connection. The commit_pending_transaction
        parameter specifies whether to commit all pending transactions before
        closing (if the store is transactional).
        """

    def destroy(self, configuration):
        """
        This destroys the instance of the store identified by the
        configuration string.
        """

    def gc(self):
        """
        Allows the store to perform any needed garbage collection
        """
        pass

    # RDF APIs
    def add(self, (subject, predicate, object), context, quoted=False):
        """
        Adds the given statement to a specific context or to the model. The
        quoted argument is interpreted by formula-aware stores to indicate
        this statement is quoted/hypothetical It should be an error to not
        specify a context and have the quoted argument be True. It should also
        be an error for the quoted argument to be True when the store is not
        formula-aware.
        """
        self.dispatcher.dispatch(
            TripleAddedEvent(
                triple=(subject, predicate, object), context=context))

    def addN(self, quads):
        """
        Adds each item in the list of statements to a specific context. The
        quoted argument is interpreted by formula-aware stores to indicate this
        statement is quoted/hypothetical. Note that the default implementation
        is a redirect to add
        """
        for s, p, o, c in quads:
            assert c is not None, \
                "Context associated with %s %s %s is None!" % (s, p, o)
            self.add((s, p, o), c)

    def remove(self, (subject, predicate, object), context=None):
        """ Remove the set of triples matching the pattern from the store """
        self.dispatcher.dispatch(
            TripleRemovedEvent(
                triple=(subject, predicate, object), context=context))

    def triples_choices(self, (subject, predicate, object_), context=None):
        """
        A variant of triples that can take a list of terms instead of a single
        term in any slot.  Stores can implement this to optimize the response
        time from the default 'fallback' implementation, which will iterate
        over each term in the list and dispatch to triples
        """
        if isinstance(object_, list):
            assert not isinstance(
                subject, list), "object_ / subject are both lists"
            assert not isinstance(
                predicate, list), "object_ / predicate are both lists"
            if object_:
                for obj in object_:
                    for (s1, p1, o1), cg in self.triples(
                            (subject, predicate, obj), context):
                        yield (s1, p1, o1), cg
            else:
                for (s1, p1, o1), cg in self.triples(
                        (subject, predicate, None), context):
                    yield (s1, p1, o1), cg

        elif isinstance(subject, list):
            assert not isinstance(
                predicate, list), "subject / predicate are both lists"
            if subject:
                for subj in subject:
                    for (s1, p1, o1), cg in self.triples(
                            (subj, predicate, object_), context):
                        yield (s1, p1, o1), cg
            else:
                for (s1, p1, o1), cg in self.triples(
                        (None, predicate, object_), context):
                    yield (s1, p1, o1), cg

        elif isinstance(predicate, list):
            assert not isinstance(
                subject, list), "predicate / subject are both lists"
            if predicate:
                for pred in predicate:
                    for (s1, p1, o1), cg in self.triples(
                            (subject, pred, object_), context):
                        yield (s1, p1, o1), cg
            else:
                for (s1, p1, o1), cg in self.triples(
                        (subject, None, object_), context):
                    yield (s1, p1, o1), cg

    def triples(self, triple_pattern, context=None):
        """
        A generator over all the triples matching the pattern. Pattern can
        include any objects for used for comparing against nodes in the store,
        for example, REGEXTerm, URIRef, Literal, BNode, Variable, Graph,
        QuotedGraph, Date? DateRange?

        :param context: A conjunctive query can be indicated by either
        providing a value of None, or a specific context can be
        queries by passing a Graph instance (if store is context aware).
        """
        subject, predicate, object = triple_pattern

    # variants of triples will be done if / when optimization is needed

    def __len__(self, context=None):
        """
        Number of statements in the store. This should only account for non-
        quoted (asserted) statements if the context is not specified,
        otherwise it should return the number of statements in the formula or
        context given.

        :param context: a graph instance to query or None
        """

    def contexts(self, triple=None):
        """
        Generator over all contexts in the graph. If triple is specified,
        a generator over all contexts the triple is in.

        if store is graph_aware, may also return empty contexts

        :returns: a generator over Nodes
        """

    def query(self, query, initNs, initBindings, queryGraph, **kwargs):
        """
        If stores provide their own SPARQL implementation, override this.

        queryGraph is None, a URIRef or '__UNION__'
        If None the graph is specified in the query-string/object
        If URIRef it specifies the graph to query,
        If  '__UNION__' the union of all named graphs should be queried
        (This is used by ConjunctiveGraphs
        Values other than None obviously only makes sense for
        context-aware stores.)

        """

        raise NotImplementedError

    def update(self, update, initNs, initBindings, queryGraph, **kwargs):
        """
        If stores provide their own (SPARQL) Update implementation,
        override this.

        queryGraph is None, a URIRef or '__UNION__'
        If None the graph is specified in the query-string/object
        If URIRef it specifies the graph to query,
        If  '__UNION__' the union of all named graphs should be queried
        (This is used by ConjunctiveGraphs
        Values other than None obviously only makes sense for
        context-aware stores.)

        """

        raise NotImplementedError

    # Optional Namespace methods

    def bind(self, prefix, namespace):
        """ """

    def prefix(self, namespace):
        """ """

    def namespace(self, prefix):
        """ """

    def namespaces(self):
        """ """
        if False:
            yield None

    # Optional Transactional methods

    def commit(self):
        """ """

    def rollback(self):
        """ """

    # Optional graph methods

    def add_graph(self, graph):
        """
        Add a graph to the store, no effect if the graph already
        exists.
        :param graph: a Graph instance
        """
        raise Exception("Graph method called on non-graph_aware store")

    def remove_graph(self, graph):
        """
        Remove a graph from the store, this shoud also remove all
        triples in the graph

        :param graphid: a Graph instance
        """
        raise Exception("Graph method called on non-graph_aware store")

########NEW FILE########
__FILENAME__ = term
"""
This module defines the different types of terms. Terms are the kinds of
objects that can appear in a quoted/asserted triple. This includes those
that are core to RDF:

* :class:`Blank Nodes <rdflib.term.BNode>`
* :class:`URI References <rdflib.term.URIRef>`
* :class:`Literals <rdflib.term.Literal>` (which consist of a literal value,datatype and language tag)

Those that extend the RDF model into N3:

* :class:`Formulae <rdflib.graph.QuotedGraph>`
* :class:`Universal Quantifications (Variables) <rdflib.term.Variable>`

And those that are primarily for matching against 'Nodes' in the
underlying Graph:

* REGEX Expressions
* Date Ranges
* Numerical Ranges

"""

__all__ = [
    'bind',

    'Node',
    'Identifier',

    'URIRef',
    'BNode',
    'Literal',

    'Variable',
    'Statement',
]

import logging
import warnings

_LOGGER = logging.getLogger(__name__)

import base64
import xml.dom.minidom

from urlparse import urlparse, urljoin, urldefrag
from datetime import date, time, datetime
from re import sub, compile
from collections import defaultdict

from isodate import parse_time, parse_date, parse_datetime


try:
    from hashlib import md5
    assert md5
except ImportError:
    from md5 import md5


import rdflib
from . import py3compat
from rdflib.compat import numeric_greater



b = py3compat.b

skolem_genid = "/.well-known/genid/"
rdflib_skolem_genid = "/.well-known/genid/rdflib/"
skolems = {}


_invalid_uri_chars = '<>" {}|\\^`'

def _is_valid_uri(uri):
    for c in _invalid_uri_chars:
        if c in uri: return False
    return True

_lang_tag_regex = compile('^[a-zA-Z]+(?:-[a-zA-Z0-9]+)*$')

def _is_valid_langtag(tag):
    return bool(_lang_tag_regex.match(tag))


class Node(object):
    """
    A Node in the Graph.
    """

    __slots__ = ()


class Identifier(Node, unicode):  # allow Identifiers to be Nodes in the Graph
    """
    See http://www.w3.org/2002/07/rdf-identifer-terminology/
    regarding choice of terminology.
    """

    __slots__ = ()

    def __new__(cls, value):
        return unicode.__new__(cls, value)

    def eq(self, other):
        """A "semantic"/interpreted equality function,
        by default, same as __eq__"""
        return self.__eq__(other)

    def neq(self, other):
        """A "semantic"/interpreted not equal function,
        by default, same as __ne__"""
        return self.__ne__(other)

    def __ne__(self, other):
        return not self.__eq__(other)

    def __eq__(self, other):
        """
        Equality for Nodes.

        >>> BNode("foo")==None
        False
        >>> BNode("foo")==URIRef("foo")
        False
        >>> URIRef("foo")==BNode("foo")
        False
        >>> BNode("foo")!=URIRef("foo")
        True
        >>> URIRef("foo")!=BNode("foo")
        True
        >>> Variable('a')!=URIRef('a')
        True
        >>> Variable('a')!=Variable('a')
        False
        """

        if type(self) == type(other):
            return unicode(self) == unicode(other)
        else:
            return False

    def __gt__(self, other):
        """
        This implements ordering for Nodes,

        This tries to implement this:
        http://www.w3.org/TR/sparql11-query/#modOrderBy

        Variables are not included in the SPARQL list, but
        they are greater than BNodes and smaller than everything else

        """
        if other is None:
            return True  # everything bigger than None
        elif type(self) == type(other):
            return unicode(self) > unicode(other)
        elif isinstance(other, Node):
            return _ORDERING[type(self)] > _ORDERING[type(other)]

        return NotImplemented

    def __lt__(self, other):
        if other is None:
            return False  # Nothing is less than None
        elif type(self) == type(other):
            return unicode(self) < unicode(other)
        elif isinstance(other, Node):
            return _ORDERING[type(self)] < _ORDERING[type(other)]

        return NotImplemented

    def __le__(self, other):
        r = self.__lt__(other)
        if r:
            return True
        return self == other

    def __ge__(self, other):
        r = self.__gt__(other)
        if r:
            return True
        return self == other

    def __hash__(self):
        return hash(type(self)) ^ hash(unicode(self))


class URIRef(Identifier):
    """
    RDF URI Reference: http://www.w3.org/TR/rdf-concepts/#section-Graph-URIref
    """

    __slots__ = ()

    def __new__(cls, value, base=None):
        if base is not None:
            ends_in_hash = value.endswith("#")
            value = urljoin(base, value, allow_fragments=1)
            if ends_in_hash:
                if not value.endswith("#"):
                    value += "#"

        if not _is_valid_uri(value):
            _LOGGER.warning('%s does not look like a valid URI, trying to serialize this will break.'%value)


        try:
            rt = unicode.__new__(cls, value)
        except UnicodeDecodeError:
            rt = unicode.__new__(cls, value, 'utf-8')
        return rt

    def toPython(self):
        return unicode(self)

    def n3(self, namespace_manager = None):
        """
        This will do a limited check for valid URIs,
        essentially just making sure that the string includes no illegal
        characters (``<, >, ", {, }, |, \\, `, ^``)

        :param namespace_manager: if not None, will be used to make up
             a prefixed name
        """

        if not _is_valid_uri(self):
            raise Exception('"%s" does not look like a valid URI, I cannot serialize this as N3/Turtle. Perhaps you wanted to urlencode it?'%self)

        if namespace_manager:
            return namespace_manager.normalizeUri(self)
        else:
            return "<%s>" % self

    def defrag(self):
        if "#" in self:
            url, frag = urldefrag(self)
            return URIRef(url)
        else:
            return self

    def __reduce__(self):
        return (URIRef, (unicode(self),))

    def __getnewargs__(self):
        return (unicode(self), )

    if not py3compat.PY3:
        def __str__(self):
            return self.encode()

    def __repr__(self):
        if self.__class__ is URIRef:
            clsName = "rdflib.term.URIRef"
        else:
            clsName = self.__class__.__name__

        return """%s(%s)""" % (clsName, super(URIRef, self).__repr__())

    def __add__(self, other):
        return self.__class__(unicode(self) + other)

    def __radd__(self, other):
        return self.__class__(other + unicode(self))

    def __mod__(self, other):
        return self.__class__(unicode(self) % other)

    def md5_term_hash(self):
        """a string of hex that will be the same for two URIRefs that
        are the same. It is not a suitable unique id.

        Supported for backwards compatibility; new code should
        probably just use __hash__
        """
        warnings.warn(
            "method md5_term_hash is deprecated, and will be removed " +
            "in the future. If you use this please let rdflib-dev know!",
            category=DeprecationWarning, stacklevel=2)
        d = md5(self.encode())
        d.update(b("U"))
        return d.hexdigest()

    def de_skolemize(self):
        """ Create a Blank Node from a skolem URI, in accordance
        with http://www.w3.org/TR/rdf11-concepts/#section-skolemization.
        This function accepts only rdflib type skolemization, to provide
        a round-tripping within the system.

        .. versionadded:: 4.0
        """
        if isinstance(self, RDFLibGenid):
            parsed_uri = urlparse("%s" % self)
            return BNode(
                value=parsed_uri.path[len(rdflib_skolem_genid):])
        elif isinstance(self, Genid):
            bnode_id = "%s" % self
            if bnode_id in skolems:
                return skolems[bnode_id]
            else:
                retval = BNode()
                skolems[bnode_id] = retval
                return retval
        else:
            raise Exception("<%s> is not a skolem URI" % self)


class Genid(URIRef):
    __slots__ = ()

    @staticmethod
    def _is_external_skolem(uri):
        if not isinstance(uri, basestring):
            uri = str(uri)
        parsed_uri = urlparse(uri)
        gen_id = parsed_uri.path.rfind(skolem_genid)
        if gen_id != 0:
            return False
        return True


class RDFLibGenid(Genid):
    __slots__ = ()

    @staticmethod
    def _is_rdflib_skolem(uri):
        if not isinstance(uri, basestring):
            uri = str(uri)
        parsed_uri = urlparse(uri)
        if parsed_uri.params != "" \
                or parsed_uri.query != "" \
                or parsed_uri.fragment != "":
            return False
        gen_id = parsed_uri.path.rfind(rdflib_skolem_genid)
        if gen_id != 0:
            return False
        return True


def _unique_id():
    # Used to read: """Create a (hopefully) unique prefix"""
    # now retained merely to leave interal API unchanged.
    # From BNode.__new__() below ...
    #
    # acceptable bnode value range for RDF/XML needs to be
    # something that can be serialzed as a nodeID for N3
    #
    # BNode identifiers must be valid NCNames" _:[A-Za-z][A-Za-z0-9]*
    # http://www.w3.org/TR/2004/REC-rdf-testcases-20040210/#nodeID
    return "N"  # ensure that id starts with a letter


def _serial_number_generator():
    """
    Generates UUID4-based but ncname-compliant identifiers.
    """
    from uuid import uuid4

    def _generator():
        return uuid4().hex

    return _generator


class BNode(Identifier):
    """
    Blank Node: http://www.w3.org/TR/rdf-concepts/#section-blank-nodes

    """
    __slots__ = ()

    def __new__(cls, value=None,
                _sn_gen=_serial_number_generator(), _prefix=_unique_id()):
        """
        # only store implementations should pass in a value
        """
        if value is None:
            # so that BNode values do not collide with ones created with
            # a different instance of this module at some other time.
            node_id = _sn_gen()
            value = "%s%s" % (_prefix, node_id)
        else:
            # TODO: check that value falls within acceptable bnode value range
            # for RDF/XML needs to be something that can be serialzed
            # as a nodeID for N3 ??  Unless we require these
            # constraints be enforced elsewhere?
            pass  # assert is_ncname(unicode(value)), "BNode identifiers
                 # must be valid NCNames" _:[A-Za-z][A-Za-z0-9]*
                 # http://www.w3.org/TR/2004/REC-rdf-testcases-20040210/#nodeID
        return Identifier.__new__(cls, value)

    def toPython(self):
        return unicode(self)

    def n3(self, namespace_manager=None):
        return "_:%s" % self

    def __getnewargs__(self):
        return (unicode(self), )

    def __reduce__(self):
        return (BNode, (unicode(self),))

    if not py3compat.PY3:
        def __str__(self):
            return self.encode()

    def __repr__(self):
        if self.__class__ is BNode:
            clsName = "rdflib.term.BNode"
        else:
            clsName = self.__class__.__name__
        return """%s('%s')""" % (clsName, str(self))

    def md5_term_hash(self):
        """a string of hex that will be the same for two BNodes that
        are the same. It is not a suitable unique id.

        Supported for backwards compatibility; new code should
        probably just use __hash__
        """
        warnings.warn(
            "method md5_term_hash is deprecated, and will be removed " +
            "in the future. If you use this please let rdflib-dev know!",
            category=DeprecationWarning, stacklevel=2)
        d = md5(self.encode())
        d.update(b("B"))
        return d.hexdigest()

    def skolemize(self, authority="http://rdlib.net/"):
        """ Create a URIRef "skolem" representation of the BNode, in accordance
        with http://www.w3.org/TR/rdf11-concepts/#section-skolemization

        .. versionadded:: 4.0
        """
        skolem = "%s%s" % (rdflib_skolem_genid, unicode(self))
        return URIRef(urljoin(authority, skolem))


class Literal(Identifier):
    __doc__ = py3compat.format_doctest_out("""
    RDF Literal: http://www.w3.org/TR/rdf-concepts/#section-Graph-Literal

    The lexical value of the literal is the unicode object
    The interpreted, datatyped value is available from .value

    Language tags must be valid according to :rfc:5646

    For valid XSD datatypes, the lexical form is optionally normalized
    at construction time. Default behaviour is set by rdflib.NORMALIZE_LITERALS
    and can be overridden by the normalize parameter to __new__

    Equality and hashing of Literals are done based on the lexical form, i.e.:

    >>> from rdflib.namespace import XSD

    >>> Literal('01')!=Literal('1') # clear - strings differ
    True

    but with data-type they get normalized:

    >>> Literal('01', datatype=XSD.integer)!=Literal('1', datatype=XSD.integer)
    False

    unless disabled:

    >>> Literal('01', datatype=XSD.integer, normalize=False)!=Literal('1', datatype=XSD.integer)
    True


    Value based comparison is possible:

    >>> Literal('01', datatype=XSD.integer).eq(Literal('1', datatype=XSD.float))
    True

    The eq method also provides limited support for basic python types:

    >>> Literal(1).eq(1) # fine - int compatible with xsd:integer
    True
    >>> Literal('a').eq('b') # fine - str compatible with plain-lit
    False
    >>> Literal('a', datatype=XSD.string).eq('a') # fine - str compatible with xsd:string
    True
    >>> Literal('a').eq(1) # not fine, int incompatible with plain-lit
    NotImplemented

    Greater-than/less-than ordering comparisons are also done in value
    space, when compatible datatypes are used.  Incompatible datatypes
    are ordered by DT, or by lang-tag.  For other nodes the ordering
    is None < BNode < URIRef < Literal

    Any comparison with non-rdflib Node are "NotImplemented"
    In PY2.X some stable order will be made up by python

    In PY3 this is an error.

    >>> from rdflib import Literal, XSD
    >>> lit2006 = Literal('2006-01-01',datatype=XSD.date)
    >>> lit2006.toPython()
    datetime.date(2006, 1, 1)
    >>> lit2006 < Literal('2007-01-01',datatype=XSD.date)
    True
    >>> Literal(datetime.utcnow()).datatype
    rdflib.term.URIRef(%(u)s'http://www.w3.org/2001/XMLSchema#dateTime')
    >>> Literal(1) > Literal(2) # by value
    False
    >>> Literal(1) > Literal(2.0) # by value
    False
    >>> Literal('1') > Literal(1) # by DT
    True
    >>> Literal('1') < Literal('1') # by lexical form
    False
    >>> Literal('a', lang='en') > Literal('a', lang='fr') # by lang-tag
    False
    >>> Literal(1) > URIRef('foo') # by node-type
    True

    The > < operators will eat this NotImplemented and either make up
    an ordering (py2.x) or throw a TypeError (py3k):

    >>> Literal(1).__gt__(2.0)
    NotImplemented


    """)


    if not py3compat.PY3:
        __slots__ = ("language", "datatype", "value", "_language",
                     "_datatype", "_value")
    else:
        __slots__ = ("_language", "_datatype", "_value")

    def __new__(cls, lexical_or_value, lang=None, datatype=None, normalize=None):

        if lang == '':
            lang = None  # no empty lang-tags in RDF

        normalize = normalize if normalize != None else rdflib.NORMALIZE_LITERALS

        if lang is not None and datatype is not None:
            raise TypeError(
                "A Literal can only have one of lang or datatype, "
                "per http://www.w3.org/TR/rdf-concepts/#section-Graph-Literal")

        if lang and not _is_valid_langtag(lang):
            raise Exception("'%s' is not a valid language tag!"%lang)

        if datatype:
            datatype = URIRef(datatype)

        value = None
        if isinstance(lexical_or_value, Literal):
            # create from another Literal instance

            lang = lang or lexical_or_value.language
            if datatype:
                # override datatype
                value = _castLexicalToPython(lexical_or_value, datatype)
            else:
                datatype = lexical_or_value.datatype
                value = lexical_or_value.value

        elif isinstance(lexical_or_value, basestring):
                # passed a string
                # try parsing lexical form of datatyped literal
                value = _castLexicalToPython(lexical_or_value, datatype)

                if value is not None and normalize:
                    _value, _datatype = _castPythonToLiteral(value)
                    if _value is not None:
                        lexical_or_value = _value

        else:
            # passed some python object
            value = lexical_or_value
            _value, _datatype = _castPythonToLiteral(lexical_or_value)

            datatype = datatype or _datatype
            if _value is not None:
                lexical_or_value = _value
            if datatype:
                lang = None


        if py3compat.PY3 and isinstance(lexical_or_value, bytes):
            lexical_or_value = lexical_or_value.decode('utf-8')

        try:
            inst = unicode.__new__(cls, lexical_or_value)
        except UnicodeDecodeError:
            inst = unicode.__new__(cls, lexical_or_value, 'utf-8')

        inst._language = lang
        inst._datatype = datatype
        inst._value = value
        return inst

    @py3compat.format_doctest_out
    def normalize(self):
        """
        Returns a new literal with a normalised lexical representation
        of this literal
        >>> from rdflib import XSD
        >>> Literal("01", datatype=XSD.integer, normalize=False).normalize()
        rdflib.term.Literal(%(u)s'1', datatype=rdflib.term.URIRef(%(u)s'http://www.w3.org/2001/XMLSchema#integer'))

        Illegal lexical forms for the datatype given are simply passed on
        >>> Literal("a", datatype=XSD.integer, normalize=False)
        rdflib.term.Literal(%(u)s'a', datatype=rdflib.term.URIRef(%(u)s'http://www.w3.org/2001/XMLSchema#integer'))

        """

        if self.value != None:
            return Literal(self.value, datatype=self.datatype, lang=self.language)
        else:
            return self

    @property
    def value(self):
        return self._value

    @property
    def language(self):
        return self._language

    @property
    def datatype(self):
        return self._datatype

    def __reduce__(self):
        return (Literal, (unicode(self), self.language, self.datatype),)

    def __getstate__(self):
        return (None, dict(language=self.language, datatype=self.datatype))

    def __setstate__(self, arg):
        _, d = arg
        self._language = d["language"]
        self._datatype = d["datatype"]

    @py3compat.format_doctest_out
    def __add__(self, val):
        """
        >>> Literal(1) + 1
        rdflib.term.Literal(%(u)s'2', datatype=rdflib.term.URIRef(%(u)s'http://www.w3.org/2001/XMLSchema#integer'))
        >>> Literal("1") + "1"
        rdflib.term.Literal(%(u)s'11')
        """

        py = self.toPython()
        if not isinstance(py, Literal):
            try:
                return Literal(py + val)
            except TypeError:
                pass  # fall-through

        s = unicode.__add__(self, val)
        return Literal(s, self.language, self.datatype)

    def __nonzero__(self):
        """
        Is the Literal "True"
        This is used for if statements, bool(literal), etc.
        """
        if self.value != None:
            return bool(self.value)
        return len(self) != 0

    @py3compat.format_doctest_out
    def __neg__(self):
        """
        >>> (- Literal(1))
        rdflib.term.Literal(%(u)s'-1', datatype=rdflib.term.URIRef(%(u)s'http://www.w3.org/2001/XMLSchema#integer'))
        >>> (- Literal(10.5))
        rdflib.term.Literal(%(u)s'-10.5', datatype=rdflib.term.URIRef(%(u)s'http://www.w3.org/2001/XMLSchema#double'))
        >>> from rdflib.namespace import XSD
        >>> (- Literal("1", datatype=XSD.integer))
        rdflib.term.Literal(%(u)s'-1', datatype=rdflib.term.URIRef(%(u)s'http://www.w3.org/2001/XMLSchema#integer'))

        >>> (- Literal("1"))
        Traceback (most recent call last):
          File "<stdin>", line 1, in <module>
        TypeError: Not a number; rdflib.term.Literal(%(u)s'1')
        >>>
        """

        if isinstance(self.value, (int, long, float)):
            return Literal(self.value.__neg__())
        else:
            raise TypeError("Not a number; %s" % repr(self))

    @py3compat.format_doctest_out
    def __pos__(self):
        """
        >>> (+ Literal(1))
        rdflib.term.Literal(%(u)s'1', datatype=rdflib.term.URIRef(%(u)s'http://www.w3.org/2001/XMLSchema#integer'))
        >>> (+ Literal(-1))
        rdflib.term.Literal(%(u)s'-1', datatype=rdflib.term.URIRef(%(u)s'http://www.w3.org/2001/XMLSchema#integer'))
        >>> from rdflib.namespace import XSD
        >>> (+ Literal("-1", datatype=XSD.integer))
        rdflib.term.Literal(%(u)s'-1', datatype=rdflib.term.URIRef(%(u)s'http://www.w3.org/2001/XMLSchema#integer'))

        >>> (+ Literal("1"))
        Traceback (most recent call last):
          File "<stdin>", line 1, in <module>
        TypeError: Not a number; rdflib.term.Literal(%(u)s'1')
        """
        if isinstance(self.value, (int, long, float)):
            return Literal(self.value.__pos__())
        else:
            raise TypeError("Not a number; %s" % repr(self))

    @py3compat.format_doctest_out
    def __abs__(self):
        """
        >>> abs(Literal(-1))
        rdflib.term.Literal(%(u)s'1', datatype=rdflib.term.URIRef(%(u)s'http://www.w3.org/2001/XMLSchema#integer'))

        >>> from rdflib.namespace import XSD
        >>> abs( Literal("-1", datatype=XSD.integer))
        rdflib.term.Literal(%(u)s'1', datatype=rdflib.term.URIRef(%(u)s'http://www.w3.org/2001/XMLSchema#integer'))

        >>> abs(Literal("1"))
        Traceback (most recent call last):
          File "<stdin>", line 1, in <module>
        TypeError: Not a number; rdflib.term.Literal(%(u)s'1')
        """
        if isinstance(self.value, (int, long, float)):
            return Literal(self.value.__abs__())
        else:
            raise TypeError("Not a number; %s" % repr(self))

    @py3compat.format_doctest_out
    def __invert__(self):
        """
        >>> ~(Literal(-1))
        rdflib.term.Literal(%(u)s'0', datatype=rdflib.term.URIRef(%(u)s'http://www.w3.org/2001/XMLSchema#integer'))

        >>> from rdflib.namespace import XSD
        >>> ~( Literal("-1", datatype=XSD.integer))
        rdflib.term.Literal(%(u)s'0', datatype=rdflib.term.URIRef(%(u)s'http://www.w3.org/2001/XMLSchema#integer'))

        Not working:

        >>> ~(Literal("1"))
        Traceback (most recent call last):
          File "<stdin>", line 1, in <module>
        TypeError: Not a number; rdflib.term.Literal(%(u)s'1')
        """
        if isinstance(self.value, (int, long, float)):
            return Literal(self.value.__invert__())
        else:
            raise TypeError("Not a number; %s" % repr(self))

    def __gt__(self, other):
        """

        This implements ordering for Literals,
        the other comparison methods delegate here

        This tries to implement this:
        http://www.w3.org/TR/sparql11-query/#modOrderBy

        In short, Literals with compatible data-types are orderd in value space,
        i.e.
        >>> from rdflib import XSD

        >>> Literal(1)>Literal(2) # int/int
        False
        >>> Literal(2.0)>Literal(1) # double/int
        True
        >>> from decimal import Decimal
        >>> Literal(Decimal("3.3")) > Literal(2.0) # decimal/double
        True
        >>> Literal(Decimal("3.3")) < Literal(4.0) # decimal/double
        True
        >>> Literal('b')>Literal('a') # plain lit/plain lit
        True
        >>> Literal('b')>Literal('a', datatype=XSD.string) # plain lit/xsd:string
        True

        Incompatible datatype mismatches ordered by DT

        >>> Literal(1)>Literal("2") # int>string
        False

        Langtagged literals by lang tag
        >>> Literal("a", lang="en")>Literal("a", lang="fr")
        False
        """
        if other is None:
            return True  # Everything is greater than None
        if isinstance(other, Literal):

            if self.datatype in _NUMERIC_LITERAL_TYPES and \
                    other.datatype in _NUMERIC_LITERAL_TYPES:
                return numeric_greater(self.value, other.value)

            # plain-literals and xsd:string literals
            # are "the same"
            dtself = self.datatype or _XSD_STRING
            dtother = other.datatype or _XSD_STRING

            if dtself != dtother:
                if rdflib.DAWG_LITERAL_COLLATION:
                    return NotImplemented
                else:
                    return dtself > dtother

            if self.language != other.language:
                if not self.language:
                    return False
                elif not other.language:
                    return True
                else:
                    return self.language > other.language

            if self.value != None and other.value != None:
                return self.value > other.value

            if unicode(self) != unicode(other):
                return unicode(self) > unicode(other)

            # same language, same lexical form, check real dt
            # plain-literals come before xsd:string!
            if self.datatype != other.datatype:
                if not self.datatype:
                    return False
                elif not other.datatype:
                    return True
                else:
                    return self.datatype > other.datatype

            return False  # they are the same

        elif isinstance(other, Node):
            return True  # Literal are the greatest!
        else:
            return NotImplemented  # we can only compare to nodes

    def __lt__(self, other):
        if other is None:
            return False  # Nothing is less than None
        if isinstance(other, Literal):
            try:
                return not self.__gt__(other) and not self.eq(other)
            except TypeError:
                return NotImplemented
        if isinstance(other, Node):
            return False  # all nodes are less-than Literals

        return NotImplemented

    def __le__(self, other):
        """
        >>> from rdflib.namespace import XSD
        >>> Literal('2007-01-01T10:00:00', datatype=XSD.dateTime
        ...     ) <= Literal('2007-01-01T10:00:00', datatype=XSD.dateTime)
        True
        """
        r = self.__lt__(other)
        if r:
            return True
        try:
            return self.eq(other)
        except TypeError:
            return NotImplemented

    def __ge__(self, other):
        r = self.__gt__(other)
        if r:
            return True
        try:
            return self.eq(other)
        except TypeError:
            return NotImplemented

    def _comparable_to(self, other):
        """
        Helper method to decide which things are meaningful to
        rich-compare with this literal
        """
        if isinstance(other, Literal):
            if (self.datatype and other.datatype):
                # two datatyped literals
                if not self.datatype in XSDToPython or not other.datatype in XSDToPython:
                    # non XSD DTs must match
                    if self.datatype != other.datatype:
                        return False

            else:
                # xsd:string may be compared with plain literals
                if not (self.datatype == _XSD_STRING and not other.datatype) or \
                        (other.datatype == _XSD_STRING and not self.datatype):
                    return False

                # if given lang-tag has to be case insensitive equal
                if (self.language or "").lower() != (other.language or "").lower():
                    return False

        return True

    def __hash__(self):
        """
        >>> from rdflib.namespace import XSD
        >>> a = {Literal('1', datatype=XSD.integer):'one'}
        >>> Literal('1', datatype=XSD.double) in a
        False


        "Called for the key object for dictionary operations,
        and by the built-in function hash(). Should return
        a 32-bit integer usable as a hash value for
        dictionary operations. The only required property
        is that objects which compare equal have the same
        hash value; it is advised to somehow mix together
        (e.g., using exclusive or) the hash values for the
        components of the object that also play a part in
        comparison of objects." -- 3.4.1 Basic customization (Python)

        "Two literals are equal if and only if all of the following hold:
        * The strings of the two lexical forms compare equal, character by
        character.
        * Either both or neither have language tags.
        * The language tags, if any, compare equal.
        * Either both or neither have datatype URIs.
        * The two datatype URIs, if any, compare equal, character by
        character."
        -- 6.5.1 Literal Equality (RDF: Concepts and Abstract Syntax)

        """

        return unicode.__hash__(self) ^ hash(self.language.lower() if self.language else None) ^ hash(self.datatype)

    @py3compat.format_doctest_out
    def __eq__(self, other):
        """
        Literals are only equal to other literals.

        "Two literals are equal if and only if all of the following hold:
        * The strings of the two lexical forms compare equal, character by character.
        * Either both or neither have language tags.
        * The language tags, if any, compare equal.
        * Either both or neither have datatype URIs.
        * The two datatype URIs, if any, compare equal, character by character."
        -- 6.5.1 Literal Equality (RDF: Concepts and Abstract Syntax)

        >>> Literal("1", datatype=URIRef("foo")) == Literal("1", datatype=URIRef("foo"))
        True
        >>> Literal("1", datatype=URIRef("foo")) == Literal("1", datatype=URIRef("foo2"))
        False

        >>> Literal("1", datatype=URIRef("foo")) == Literal("2", datatype=URIRef("foo"))
        False
        >>> Literal("1", datatype=URIRef("foo")) == "asdf"
        False
        >>> from rdflib import XSD
        >>> Literal('2007-01-01', datatype=XSD.date) == Literal('2007-01-01', datatype=XSD.date)
        True
        >>> Literal('2007-01-01', datatype=XSD.date) == date(2007, 1, 1)
        False
        >>> Literal("one", lang="en") == Literal("one", lang="en")
        True
        >>> Literal("hast", lang='en') == Literal("hast", lang='de')
        False
        >>> Literal("1", datatype=XSD.integer) == Literal(1)
        True
        >>> Literal("1", datatype=XSD.integer) == Literal("01", datatype=XSD.integer)
        True

        """
        if self is other:
            return True
        if other is None:
            return False
        if isinstance(other, Literal):
            return self.datatype == other.datatype \
                and (self.language.lower() if self.language else None) == (other.language.lower() if other.language else None) \
                and unicode.__eq__(self, other)

        return False

    def eq(self, other):
        """
        Compare the value of this literal with something else

        Either, with the value of another literal
        comparisons are then done in literal "value space",
        and according to the rules of XSD subtype-substitution/type-promotion

        OR, with a python object:

        basestring objects can be compared with plain-literals,
        or those with datatype xsd:string

        bool objects with xsd:boolean

        a int, long or float with numeric xsd types

        isodate date,time,datetime objects with xsd:date,xsd:time or xsd:datetime

        Any other operations returns NotImplemented

        """
        if isinstance(other, Literal):

            if self.datatype in _NUMERIC_LITERAL_TYPES  \
                    and other.datatype in _NUMERIC_LITERAL_TYPES:
                if self.value != None and other.value != None:
                    return self.value == other.value
                else:
                    if unicode.__eq__(self, other):
                        return True
                    raise TypeError(
                        'I cannot know that these two lexical forms do not map to the same value: %s and %s' % (self, other))
            if (self.language or "").lower() != (other.language or "").lower():
                return False

            dtself = self.datatype or _XSD_STRING
            dtother = other.datatype or _XSD_STRING

            if (dtself == _XSD_STRING and dtother == _XSD_STRING):
                # string/plain literals, compare on lexical form
                return unicode.__eq__(self, other)

            if dtself != dtother:
                if rdflib.DAWG_LITERAL_COLLATION:
                    raise TypeError("I don't know how to compare literals with datatypes %s and %s" % (
                        self.datatype, other.datatype))
                else:
                    return False

            # matching non-string DTs now - do we compare values or
            # lexical form first?  comparing two ints is far quicker -
            # maybe there are counter examples

            if self.value != None and other.value != None:

                if self.datatype in (_RDF_XMLLITERAL, _RDF_HTMLLITERAL):
                    return _isEqualXMLNode(self.value, other.value)

                return self.value == other.value
            else:

                if unicode.__eq__(self, other):
                    return True

                if self.datatype == _XSD_STRING:
                    return False  # string value space=lexical space

                # matching DTs, but not matching, we cannot compare!
                raise TypeError(
                    'I cannot know that these two lexical forms do not map to the same value: %s and %s' % (self, other))

        elif isinstance(other, Node):
            return False  # no non-Literal nodes are equal to a literal

        elif isinstance(other, basestring):
            # only plain-literals can be directly compared to strings

            # TODO: Is "blah"@en eq "blah" ?
            if self.language is not None:
                return False

            if (self.datatype == _XSD_STRING or self.datatype is None):
                return unicode(self) == other

        elif isinstance(other, (int, long, float)):
            if self.datatype in _NUMERIC_LITERAL_TYPES:
                return self.value == other
        elif isinstance(other, (date, datetime, time)):
            if self.datatype in (_XSD_DATETIME, _XSD_DATE, _XSD_TIME):
                return self.value == other
        elif isinstance(other, bool):
            if self.datatype == _XSD_BOOLEAN:
                return self.value == other

        return NotImplemented

    def neq(self, other):
        return not self.eq(other)

    @py3compat.format_doctest_out
    def n3(self, namespace_manager = None):
        r'''
        Returns a representation in the N3 format.

        Examples::

            >>> Literal("foo").n3()
            %(u)s'"foo"'

        Strings with newlines or triple-quotes::

            >>> Literal("foo\nbar").n3()
            %(u)s'"""foo\nbar"""'

            >>> Literal("''\'").n3()
            %(u)s'"\'\'\'"'

            >>> Literal('"""').n3()
            %(u)s'"\\"\\"\\""'

        Language::

            >>> Literal("hello", lang="en").n3()
            %(u)s'"hello"@en'

        Datatypes::

            >>> Literal(1).n3()
            %(u)s'"1"^^<http://www.w3.org/2001/XMLSchema#integer>'

            >>> Literal(1.0).n3()
            %(u)s'"1.0"^^<http://www.w3.org/2001/XMLSchema#double>'

            >>> Literal(True).n3()
            %(u)s'"true"^^<http://www.w3.org/2001/XMLSchema#boolean>'

        Datatype and language isn't allowed (datatype takes precedence)::

            >>> Literal(1, lang="en").n3()
            %(u)s'"1"^^<http://www.w3.org/2001/XMLSchema#integer>'

        Custom datatype::

            >>> footype = URIRef("http://example.org/ns#foo")
            >>> Literal("1", datatype=footype).n3()
            %(u)s'"1"^^<http://example.org/ns#foo>'

        Passing a namespace-manager will use it to abbreviate datatype URIs:

            >>> from rdflib import Graph
            >>> Literal(1).n3(Graph().namespace_manager)
            %(u)s'"1"^^xsd:integer'
        '''
        if namespace_manager:
            return self._literal_n3(qname_callback =
                                    namespace_manager.normalizeUri)
        else:
            return self._literal_n3()

    @py3compat.format_doctest_out
    def _literal_n3(self, use_plain=False, qname_callback=None):
        '''
        Using plain literal (shorthand) output::
            >>> from rdflib.namespace import XSD

            >>> Literal(1)._literal_n3(use_plain=True)
            %(u)s'1'

            >>> Literal(1.0)._literal_n3(use_plain=True)
            %(u)s'1e+00'

            >>> Literal(1.0, datatype=XSD.decimal)._literal_n3(use_plain=True)
            %(u)s'1.0'

            >>> Literal(1.0, datatype=XSD.float)._literal_n3(use_plain=True)
            %(u)s'"1.0"^^<http://www.w3.org/2001/XMLSchema#float>'

            >>> Literal("foo", datatype=XSD.string)._literal_n3(
            ...         use_plain=True)
            %(u)s'"foo"^^<http://www.w3.org/2001/XMLSchema#string>'

            >>> Literal(True)._literal_n3(use_plain=True)
            %(u)s'true'

            >>> Literal(False)._literal_n3(use_plain=True)
            %(u)s'false'

            >>> Literal(1.91)._literal_n3(use_plain=True)
            %(u)s'1.91e+00'

            Only limited precision available for floats:
            >>> Literal(0.123456789)._literal_n3(use_plain=True)
            %(u)s'1.234568e-01'

            >>> Literal('0.123456789',
            ...     datatype=XSD.decimal)._literal_n3(use_plain=True)
            %(u)s'0.123456789'

        Using callback for datatype QNames::

            >>> Literal(1)._literal_n3(
            ...         qname_callback=lambda uri: "xsd:integer")
            %(u)s'"1"^^xsd:integer'

        '''
        if use_plain and self.datatype in _PLAIN_LITERAL_TYPES:
            if self.value is not None:

                # this is a bit of a mess -
                # in py >=2.6 the string.format function makes this easier
                # we try to produce "pretty" output
                if self.datatype == _XSD_DOUBLE:
                    return sub("\\.?0*e", "e", u'%e' % float(self))
                elif self.datatype == _XSD_DECIMAL:
                    s = '%s' % self
                    if '.' not in s:
                        s += '.0'
                    return s

                elif self.datatype == _XSD_BOOLEAN:
                    return (u'%s' % self).lower()
                else:
                    return u'%s' % self

        encoded = self._quote_encode()

        datatype = self.datatype
        quoted_dt = None
        if datatype:
            if qname_callback:
                quoted_dt = qname_callback(datatype)
            if not quoted_dt:
                quoted_dt = "<%s>" % datatype

        language = self.language
        if language:
            return '%s@%s' % (encoded, language)
        elif datatype:
            return '%s^^%s' % (encoded, quoted_dt)
        else:
            return '%s' % encoded

    def _quote_encode(self):
        # This simpler encoding doesn't work; a newline gets encoded as "\\n",
        # which is ok in sourcecode, but we want "\n".
        # encoded = self.encode('unicode-escape').replace(
        #        '\\', '\\\\').replace('"','\\"')
        # encoded = self.replace.replace('\\', '\\\\').replace('"','\\"')

        # NOTE: Could in theory chose quotes based on quotes appearing in the
        # string, i.e. '"' and "'", but N3/turtle doesn't allow "'"(?).

        if "\n" in self:
            # Triple quote this string.
            encoded = self.replace('\\', '\\\\')
            if '"""' in self:
                # is this ok?
                encoded = encoded.replace('"""', '\\"\\"\\"')
            if encoded[-1] == '"' and encoded[-2] != '\\':
                encoded = encoded[:-1] + '\\' + '"'

            return '"""%s"""' % encoded.replace('\r', '\\r')
        else:
            return '"%s"' % self.replace(
                '\n', '\\n').replace(
                    '\\', '\\\\').replace(
                        '"', '\\"').replace(
                            '\r', '\\r')

    if not py3compat.PY3:
        def __str__(self):
            return self.encode()

    def __repr__(self):
        args = [super(Literal, self).__repr__()]
        if self.language is not None:
            args.append("lang=%s" % repr(self.language))
        if self.datatype is not None:
            args.append("datatype=%s" % repr(self.datatype))
        if self.__class__ == Literal:
            clsName = "rdflib.term.Literal"
        else:
            clsName = self.__class__.__name__
        return """%s(%s)""" % (clsName, ", ".join(args))

    def toPython(self):
        """
        Returns an appropriate python datatype derived from this RDF Literal
        """

        if self.value is not None:
            return self.value
        return self

    def md5_term_hash(self):
        """a string of hex that will be the same for two Literals that
        are the same. It is not a suitable unique id.

        Supported for backwards compatibility; new code should
        probably just use __hash__
        """
        warnings.warn(
            "method md5_term_hash is deprecated, and will be removed " +
            "removed in the future. If you use this please let rdflib-dev know!",
            category=DeprecationWarning, stacklevel=2)
        d = md5(self.encode())
        d.update(b("L"))
        return d.hexdigest()


def _parseXML(xmlstring):
    if not py3compat.PY3:
        xmlstring = xmlstring.encode('utf-8')
    retval = xml.dom.minidom.parseString(
        "<rdflibtoplevelelement>%s</rdflibtoplevelelement>" % xmlstring)
    retval.normalize()
    return retval


def _parseHTML(htmltext):
    try:
        import html5lib
        parser = html5lib.HTMLParser(
            tree=html5lib.treebuilders.getTreeBuilder("dom"))
        retval = parser.parseFragment(htmltext)
        retval.normalize()
        return retval
    except ImportError:
        raise ImportError(
            "HTML5 parser not available. Try installing" +
            " html5lib <http://code.google.com/p/html5lib>")


def _writeXML(xmlnode):
    if isinstance(xmlnode, xml.dom.minidom.DocumentFragment):
        d = xml.dom.minidom.Document()
        d.childNodes += xmlnode.childNodes
        xmlnode = d
    s = xmlnode.toxml('utf-8')
    # for clean round-tripping, remove headers -- I have great and
    # specific worries that this will blow up later, but this margin
    # is too narrow to contain them
    if s.startswith(b('<?xml version="1.0" encoding="utf-8"?>')):
        s = s[38:]
    if s.startswith(b('<rdflibtoplevelelement>')):
        s = s[23:-24]
    if s == b('<rdflibtoplevelelement/>'):
        s = b('')
    return s

# Cannot import Namespace/XSD because of circular dependencies
_XSD_PFX = 'http://www.w3.org/2001/XMLSchema#'
_RDF_PFX = 'http://www.w3.org/1999/02/22-rdf-syntax-ns#'

_RDF_XMLLITERAL = URIRef(_RDF_PFX + 'XMLLiteral')
_RDF_HTMLLITERAL = URIRef(_RDF_PFX + 'HTML')

_XSD_STRING = URIRef(_XSD_PFX + 'string')

_XSD_FLOAT = URIRef(_XSD_PFX + 'float')
_XSD_DOUBLE = URIRef(_XSD_PFX + 'double')
_XSD_DECIMAL = URIRef(_XSD_PFX + 'decimal')
_XSD_INTEGER = URIRef(_XSD_PFX + 'integer')
_XSD_BOOLEAN = URIRef(_XSD_PFX + 'boolean')

_XSD_DATETIME = URIRef(_XSD_PFX + 'dateTime')
_XSD_DATE = URIRef(_XSD_PFX + 'date')
_XSD_TIME = URIRef(_XSD_PFX + 'time')

# TODO: duration, gYearMonth, gYear, gMonthDay, gDay, gMonth

_NUMERIC_LITERAL_TYPES = (
    _XSD_INTEGER,
    _XSD_DECIMAL,
    _XSD_DOUBLE,
    URIRef(_XSD_PFX + 'float'),

    URIRef(_XSD_PFX + 'byte'),
    URIRef(_XSD_PFX + 'int'),
    URIRef(_XSD_PFX + 'long'),
    URIRef(_XSD_PFX + 'negativeInteger'),
    URIRef(_XSD_PFX + 'nonNegativeInteger'),
    URIRef(_XSD_PFX + 'nonPositiveInteger'),
    URIRef(_XSD_PFX + 'positiveInteger'),
    URIRef(_XSD_PFX + 'short'),
    URIRef(_XSD_PFX + 'unsignedByte'),
    URIRef(_XSD_PFX + 'unsignedInt'),
    URIRef(_XSD_PFX + 'unsignedLong'),
    URIRef(_XSD_PFX + 'unsignedShort'),

)

# these have "native" syntax in N3/SPARQL
_PLAIN_LITERAL_TYPES = (
    _XSD_INTEGER,
    _XSD_BOOLEAN,
    _XSD_DOUBLE,
    _XSD_DECIMAL,
)


def _castPythonToLiteral(obj):
    """
    Casts a python datatype to a tuple of the lexical value and a
    datatype URI (or None)
    """
    for pType, (castFunc, dType) in _PythonToXSD:
        if isinstance(obj, pType):
            if castFunc:
                return castFunc(obj), dType
            elif dType:
                return obj, dType
            else:
                return obj, None
    return obj, None  # TODO: is this right for the fall through case?

from decimal import Decimal

# Mappings from Python types to XSD datatypes and back (borrowed from sparta)
# datetime instances are also instances of date... so we need to order these.

# SPARQL/Turtle/N3 has shortcuts for integer, double, decimal
# python has only float - to be in tune with sparql/n3/turtle
# we default to XSD.double for float literals

# python ints are promoted to longs when overflowing
# python longs have no limit
# both map to the abstract integer type,
# rather than some concrete bit-limited datatype

_PythonToXSD = [
    (basestring, (None, None)),
    (float, (None, _XSD_DOUBLE)),
    (bool, (lambda i:str(i).lower(), _XSD_BOOLEAN)),
    (int, (None, _XSD_INTEGER)),
    (long, (None, _XSD_INTEGER)),
    (Decimal, (None, _XSD_DECIMAL)),
    (datetime, (lambda i:i.isoformat(), _XSD_DATETIME)),
    (date, (lambda i:i.isoformat(), _XSD_DATE)),
    (time, (lambda i:i.isoformat(), _XSD_TIME)),
    (xml.dom.minidom.Document, (_writeXML, _RDF_XMLLITERAL)),
    # this is a bit dirty - by accident the html5lib parser produces
    # DocumentFragments, and the xml parser Documents, letting this
    # decide what datatype to use makes roundtripping easier, but it a
    # bit random
    (xml.dom.minidom.DocumentFragment, (_writeXML, _RDF_HTMLLITERAL))
]

XSDToPython = {
    None : None, # plain literals map directly to value space
    URIRef(_XSD_PFX + 'time'): parse_time,
    URIRef(_XSD_PFX + 'date'): parse_date,
    URIRef(_XSD_PFX + 'dateTime'): parse_datetime,
    URIRef(_XSD_PFX + 'string'): None,
    URIRef(_XSD_PFX + 'normalizedString'): None,
    URIRef(_XSD_PFX + 'token'): None,
    URIRef(_XSD_PFX + 'language'): None,
    URIRef(_XSD_PFX + 'boolean'): lambda i: i.lower() in ['1', 'true'],
    URIRef(_XSD_PFX + 'decimal'): Decimal,
    URIRef(_XSD_PFX + 'integer'): long,
    URIRef(_XSD_PFX + 'nonPositiveInteger'): int,
    URIRef(_XSD_PFX + 'long'): long,
    URIRef(_XSD_PFX + 'nonNegativeInteger'): int,
    URIRef(_XSD_PFX + 'negativeInteger'): int,
    URIRef(_XSD_PFX + 'int'): long,
    URIRef(_XSD_PFX + 'unsignedLong'): long,
    URIRef(_XSD_PFX + 'positiveInteger'): int,
    URIRef(_XSD_PFX + 'short'): int,
    URIRef(_XSD_PFX + 'unsignedInt'): long,
    URIRef(_XSD_PFX + 'byte'): int,
    URIRef(_XSD_PFX + 'unsignedShort'): int,
    URIRef(_XSD_PFX + 'unsignedByte'): int,
    URIRef(_XSD_PFX + 'float'): float,
    URIRef(_XSD_PFX + 'double'): float,
    URIRef(
        _XSD_PFX + 'base64Binary'): lambda s: base64.b64decode(py3compat.b(s)),
    URIRef(_XSD_PFX + 'anyURI'): None,
    _RDF_XMLLITERAL: _parseXML,
    _RDF_HTMLLITERAL: _parseHTML
}

_toPythonMapping = {}

_toPythonMapping.update(XSDToPython)

def _castLexicalToPython(lexical, datatype):
    """
    Map a lexical form to the value-space for the given datatype
    :returns: a python object for the value or ``None``
    """
    convFunc = _toPythonMapping.get(datatype, False)
    if convFunc:
        try:
            return convFunc(lexical)
        except:
            # not a valid lexical representation for this dt
            return None
    elif convFunc is None:
        # no conv func means 1-1 lexical<->value-space mapping
        try:
            return unicode(lexical)
        except UnicodeDecodeError:
            return unicode(lexical, 'utf-8')
    else:
        # no convFunc - unknown data-type
        return None

def bind(datatype, pythontype, constructor=None, lexicalizer=None):
    """
    register a new datatype<->pythontype binding

    :param constructor: an optional function for converting lexical forms
                        into a Python instances, if not given the pythontype
                        is used directly

    :param lexicalizer: an optinoal function for converting python objects to
                        lexical form, if not given object.__str__ is used

    """
    if datatype in _toPythonMapping:
        _LOGGER.warning("datatype '%s' was already bound. Rebinding." %
                        datatype)

    if constructor == None:
        constructor = pythontype
    _toPythonMapping[datatype] = constructor
    _PythonToXSD.append((pythontype, (lexicalizer, datatype)))


class Variable(Identifier):
    """
    A Variable - this is used for querying, or in Formula aware
    graphs, where Variables can stored in the graph
    """
    __slots__ = ()

    def __new__(cls, value):
        if len(value) == 0:
            raise Exception(
                "Attempted to create variable with empty string as name!")
        if value[0] == '?':
            value = value[1:]
        return unicode.__new__(cls, value)

    def __repr__(self):
        return self.n3()

    def toPython(self):
        return "?%s" % self

    def n3(self, namespace_manager = None):
        return "?%s" % self

    def __reduce__(self):
        return (Variable, (unicode(self),))

    def md5_term_hash(self):
        """a string of hex that will be the same for two Variables that
        are the same. It is not a suitable unique id.

        Supported for backwards compatibility; new code should
        probably just use __hash__
        """
        warnings.warn(
            "method md5_term_hash is deprecated, and will be removed " +
            "removed in the future. If you use this please let rdflib-dev know!",
            category=DeprecationWarning, stacklevel=2)
        d = md5(self.encode())
        d.update(b("V"))
        return d.hexdigest()


class Statement(Node, tuple):

    def __new__(cls, (subject, predicate, object), context):
        warnings.warn(
            "Class Statement is deprecated, and will be removed in " +
            "the future. If you use this please let rdflib-dev know!",
            category=DeprecationWarning, stacklevel=2)
        return tuple.__new__(cls, ((subject, predicate, object), context))

    def __reduce__(self):
        return (Statement, (self[0], self[1]))

    def toPython(self):
        return (self[0], self[1])

# Nodes are ordered like this
# See http://www.w3.org/TR/sparql11-query/#modOrderBy
# we leave "space" for more subclasses of Node elsewhere
# default-dict to grazefully fail for new subclasses
_ORDERING = defaultdict(int)
_ORDERING.update({
    BNode: 10,
    Variable: 20,
    URIRef: 30,
    Literal: 40
    })


def _isEqualXMLNode(node, other):
    from xml.dom.minidom import Node

    def recurse():
        # Recursion through the children
        # In Python2, the semantics of 'map' is such that the check on
        # length would be unnecessary. In Python 3,
        # the semantics of map has changed (why, oh why???) and the check
        # for the length becomes necessary...
        if len(node.childNodes) != len(other.childNodes):
            return False
        for (nc, oc) in map(
                lambda x, y: (x, y), node.childNodes, other.childNodes):
            if not _isEqualXMLNode(nc, oc):
                return False
        # if we got here then everything is fine:
        return True

    if node is None or other is None:
        return False

    if node.nodeType != other.nodeType:
        return False

    if node.nodeType in [Node.DOCUMENT_NODE, Node.DOCUMENT_FRAGMENT_NODE]:
        return recurse()

    elif node.nodeType == Node.ELEMENT_NODE:
        # Get the basics right
        if not (node.tagName == other.tagName
                and node.namespaceURI == other.namespaceURI):
            return False

        # Handle the (namespaced) attributes; the namespace setting key
        # should be ignored, though
        # Note that the minidom orders the keys already, so we do not have
        # to worry about that, which is a bonus...
        n_keys = [
            k for k in node.attributes.keysNS()
            if k[0] != 'http://www.w3.org/2000/xmlns/']
        o_keys = [
            k for k in other.attributes.keysNS()
            if k[0] != 'http://www.w3.org/2000/xmlns/']
        if len(n_keys) != len(o_keys):
            return False
        for k in n_keys:
            if not (k in o_keys
                    and node.getAttributeNS(k[0], k[1]) ==
                    other.getAttributeNS(k[0], k[1])):
                return False

        # if we got here, the attributes are all right, we can go down
        # the tree recursively
        return recurse()

    elif node.nodeType in [
            Node.TEXT_NODE, Node.COMMENT_NODE, Node.CDATA_SECTION_NODE,
            Node.NOTATION_NODE]:
        return node.data == other.data

    elif node.nodeType == Node.PROCESSING_INSTRUCTION_NODE:
        return node.data == other.data and node.target == other.target

    elif node.nodeType == Node.ENTITY_NODE:
        return node.nodeValue == other.nodeValue

    elif node.nodeType == Node.DOCUMENT_TYPE_NODE:
        return node.publicId == other.publicId \
            and node.systemId == other.system.Id

    else:
        # should not happen, in fact
        raise Exception(
            'I dont know how to compare XML Node type: %s' % node.nodeType)

if __name__ == '__main__':
    import doctest
    doctest.testmod()

########NEW FILE########
__FILENAME__ = csv2rdf
"""
A commandline tool for semi-automatically converting CSV to RDF

try: ``csv2rdf --help``

"""


import sys
import re
import csv
import getopt
import ConfigParser
import fileinput
import codecs
import time
import datetime
import warnings
import urllib2

import rdflib

from rdflib import RDF, RDFS
from rdflib.namespace import split_uri

__all__ = [ 'CSV2RDF' ]

HELP = """
csv2rdf.py \
    -b <instance-base> \
    -p <property-base> \
    [-c <classname>] \
    [-i <identity column(s)>] \
    [-l <label columns>] \
    [-s <N>] [-o <output>] \
    [-f configfile] \
    [--col<N> <colspec>] \
    [--prop<N> <property>] \
    <[-d <delim>] \
    [-C] [files...]"

Reads csv files from stdin or given files
if -d is given, use this delimiter
if -s is given, skips N lines at the start
Creates a URI from the columns given to -i, or automatically by numbering if
none is given
Outputs RDFS labels from the columns given to -l
if -c is given adds a type triple with the given classname
if -C is given, the class is defined as rdfs:Class
Outputs one RDF triple per column in each row.
Output is in n3 format.
Output is stdout, unless -o is specified

Long options also supported: \
    --base, \
    --propbase, \
    --ident, \
    --class, \
    --label, \
    --out, \
    --defineclass

Long options --col0, --col1, ...
can be used to specify conversion for columns.
Conversions can be:
    float(), int(), split(sep, [more]), uri(base, [class]), date(format)

Long options --prop0, --prop1, ...
can be used to use specific properties, rather than ones auto-generated
from the headers

-f says to read config from a .ini/config file - the file must contain one
section called csv2rdf, with keys like the long options, i.e.:

[csv2rdf]
out=output.n3
base=http://example.org/
col0=split(";")
col1=split(";", uri("http://example.org/things/",
                    "http://xmlns.com/foaf/0.1/Person"))
col2=float()
col3=int()
col4=date("%Y-%b-%d %H:%M:%S")

"""

# bah - ugly global
uris = {}


def toProperty(label):
    """
    CamelCase + lowercase inital a string


    FIRST_NM => firstNm

    firstNm => firstNm

    """
    label = re.sub("[^\w]", " ", label)
    label = re.sub("([a-z])([A-Z])", "\\1 \\2", label)
    label = label.split(" ")
    return "".join([label[0].lower()] + [x.capitalize() for x in label[1:]])


def toPropertyLabel(label):
    if not label[1:2].isupper():
        return label[0:1].lower() + label[1:]
    return label


def index(l, i):
    """return a set of indexes from a list
    >>> index([1,2,3],(0,2))
    (1, 3)
    """
    return tuple([l[x] for x in i])


def csv_reader(csv_data, dialect=csv.excel, **kwargs):

    csv_reader = csv.reader(csv_data,
                            dialect=dialect, **kwargs)
    for row in csv_reader:
        # decode UTF-8 back to Unicode, cell by cell:
        yield [unicode(cell, 'utf-8', errors='replace') for cell in row]


def prefixuri(x, prefix, class_=None):
    if prefix:
        r = rdflib.URIRef(
            prefix + urllib2.quote(
                x.encode("utf8").replace(" ", "_"), safe=""))
    else:
        r = rdflib.URIRef(x)
    uris[x] = (r, class_)
    return r

# meta-language for config


class NodeMaker(object):
    def range(self):
        return rdflib.RDFS.Literal

    def __call__(self, x):
        return rdflib.Literal(x)


class NodeUri(NodeMaker):
    def __init__(self, prefix, class_):
        self.prefix = prefix
        self.class_ = rdflib.URIRef(class_)

    def __call__(self, x):
        return prefixuri(x, self.prefix, self.class_)

    def range(self):
        return self.class_ or rdflib.RDF.Resource


class NodeLiteral(NodeMaker):
    def __init__(self, f=None):
        self.f = f


class NodeFloat(NodeLiteral):
    def __call__(self, x):
        if not self.f:
            return rdflib.Literal(float(x))
        if callable(self.f):
            return rdflib.Literal(float(self.f(x)))
        raise Exception("Function passed to float is not callable")

    def range(self):
        return rdflib.XSD.double


class NodeInt(NodeLiteral):
    def __call__(self, x):
        if not self.f:
            return rdflib.Literal(int(x))
        if callable(self.f):
            return rdflib.Literal(int(self.f(x)))
        raise Exception("Function passed to int is not callable")

    def range(self):
        return rdflib.XSD.int


class NodeReplace(NodeMaker):
    def __init__(self, a, b):
        self.a = a
        self.b = b

    def __call__(self, x):
        return x.replace(self.a, self.b)


class NodeDate(NodeLiteral):
    def __call__(self, x):
        return rdflib.Literal(datetime.datetime.strptime(x, self.f))

    def range(self):
        return rdflib.XSD.dateTime


class NodeSplit(NodeMaker):
    def __init__(self, sep, f):
        self.sep = sep
        self.f = f

    def __call__(self, x):
        if not self.f:
            f = rdflib.Literal
        if not callable(self.f):
            raise Exception("Function passed to split is not callable!")
        return [
            self.f(y.strip()) for y in x.split(self.sep) if y.strip() != ""]

    def range(self):
        if self.f and isinstance(self.f, NodeMaker):
            return self.f.range()
        return NodeMaker.range(self)

default_node_make = NodeMaker()


def _config_ignore(**args):
    return "ignore"


def _config_uri(prefix=None, class_=None):
    return NodeUri(prefix, class_)


def _config_literal():
    return NodeLiteral


def _config_float(f=None):
    return NodeFloat(f)


def _config_replace(a, b):
    return NodeReplace(a, b)


def _config_int(f=None):
    return NodeInt(f)


def _config_date(format_):
    return NodeDate(format_)


def _config_split(sep=None, f=None):
    return NodeSplit(sep, f)

config_functions = {"ignore": _config_ignore,
                    "uri": _config_uri,
                    "literal": _config_literal,
                    "float": _config_float,
                    "int": _config_int,
                    "date": _config_date,
                    "split": _config_split,
                    "replace": _config_replace
                    }


def column(v):
    """Return a function for column mapping"""

    return eval(v, config_functions)


class CSV2RDF(object):
    def __init__(self):

        self.CLASS = None
        self.BASE = None
        self.PROPBASE = None
        self.IDENT = 'auto'
        self.LABEL = None
        self.DEFINECLASS = False
        self.SKIP = 0
        self.DELIM = ","

        self.COLUMNS = {}
        self.PROPS = {}

        self.OUT = codecs.getwriter("utf-8")(sys.stdout, errors='replace')

        self.triples = 0

    def triple(self, s, p, o):
        self.OUT.write("%s %s %s .\n" % (s.n3(), p.n3(), o.n3()))
        self.triples += 1

    def convert(self, csvreader):

        start = time.time()

        if self.OUT:
            sys.stderr.write("Output to %s\n" % self.OUT.name)

        if self.IDENT != "auto" and not isinstance(self.IDENT, tuple):
            self.IDENT = (self.IDENT,)

        if not self.BASE:
            warnings.warn("No base given, using http://example.org/instances/")
            self.BASE = rdflib.Namespace("http://example.org/instances/")

        if not self.PROPBASE:
            warnings.warn(
                "No property base given, using http://example.org/property/")
            self.PROPBASE = rdflib.Namespace("http://example.org/props/")

        # skip lines at the start
        for x in range(self.SKIP):
            csvreader.next()

        # read header line
        header_labels = list(csvreader.next())
        headers = dict(
            enumerate([self.PROPBASE[toProperty(x)] for x in header_labels]))
        # override header properties if some are given
        for k, v in self.PROPS.iteritems():
            headers[k] = v
            header_labels[k] = split_uri[1]

        if self.DEFINECLASS:
            # output class/property definitions
            self.triple(self.CLASS, RDF.type, RDFS.Class)
            for i in range(len(headers)):
                h, l = headers[i], header_labels[i]
                if h == "" or l == "":
                    continue
                if self.COLUMNS.get(i) == "ignore":
                    continue
                self.triple(h, RDF.type, RDF.Property)
                self.triple(h, RDFS.label, rdflib.Literal(toPropertyLabel(l)))
                self.triple(h, RDFS.domain, self.CLASS)
                self.triple(h, RDFS.range,
                            self.COLUMNS.get(i, default_node_make).range())

        rows = 0
        for l in csvreader:
            try:
                if self.IDENT == 'auto':
                    uri = self.BASE["%d" % rows]
                else:
                    uri = self.BASE["_".join([urllib2.quote(x.encode(
                        "utf8").replace(" ", "_"), safe="")
                        for x in index(l, self.IDENT)])]

                if self.LABEL:
                    self.triple(uri, RDFS.label, rdflib.Literal(
                        " ".join(index(l, self.LABEL))))

                if self.CLASS:
                    # type triple
                    self.triple(uri, RDF.type, self.CLASS)

                for i, x in enumerate(l):
                    x = x.strip()
                    if x != '':
                        if self.COLUMNS.get(i) == "ignore":
                            continue
                        try:
                            o = self.COLUMNS.get(i, rdflib.Literal)(x)
                            if isinstance(o, list):
                                for _o in o:
                                    self.triple(uri, headers[i], _o)
                            else:
                                self.triple(uri, headers[i], o)

                        except Exception, e:
                            warnings.warn(
                                "Could not process value for column " +
                                "%d:%s in row %d, ignoring: %s " % (
                                i, headers[i], rows, e.message))

                rows += 1
                if rows % 100000 == 0:
                    sys.stderr.write(
                        "%d rows, %d triples, elapsed %.2fs.\n" % (
                        rows, self.triples, time.time() - start))
            except:
                sys.stderr.write("Error processing line: %d\n" % rows)
                raise

        # output types/labels for generated URIs
        classes = set()
        for l, x in uris.iteritems():
            u, c = x
            self.triple(u, RDFS.label, rdflib.Literal(l))
            if c:
                c = rdflib.URIRef(c)
                classes.add(c)
                self.triple(u, RDF.type, c)

        for c in classes:
            self.triple(c, RDF.type, RDFS.Class)

        self.OUT.close()
        sys.stderr.write(
            "Converted %d rows into %d triples.\n" % (rows, self.triples))
        sys.stderr.write("Took %.2f seconds.\n" % (time.time() - start))


def main():
    csv2rdf = CSV2RDF()

    opts, files = getopt.getopt(
        sys.argv[1:],
        "hc:b:p:i:o:Cf:l:s:d:",
        ["out=", "base=", "delim=", "propbase=", "class=",
         "ident=", "label=", "skip=", "defineclass", "help"])
    opts = dict(opts)

    if "-h" in opts or "--help" in opts:
        print HELP
        sys.exit(-1)

    if "-f" in opts:
        config = ConfigParser.ConfigParser()
        config.readfp(open(opts["-f"]))
        for k, v in config.items("csv2rdf"):
            if k == "out":
                csv2rdf.OUT = codecs.open(v, "w", "utf-8")
            elif k == "base":
                csv2rdf.BASE = rdflib.Namespace(v)
            elif k == "propbase":
                csv2rdf.PROPBASE = rdflib.Namespace(v)
            elif k == "class":
                csv2rdf.CLASS = rdflib.URIRef(v)
            elif k == "defineclass":
                csv2rdf.DEFINECLASS = bool(v)
            elif k == "ident":
                csv2rdf.IDENT = eval(v)
            elif k == "label":
                csv2rdf.LABEL = eval(v)
            elif k == "delim":
                csv2rdf.DELIM = v
            elif k == "skip":
                csv2rdf.SKIP = int(v)
            elif k.startswith("col"):
                csv2rdf.COLUMNS[int(k[3:])] = column(v)
            elif k.startswith("prop"):
                csv2rdf.PROPS[int(k[4:])] = rdflib.URIRef(v)

    if "-o" in opts:
        csv2rdf.OUT = codecs.open(opts["-o"], "w", "utf-8")
    if "--out" in opts:
        csv2rdf.OUT = codecs.open(opts["--out"], "w", "utf-8")

    if "-b" in opts:
        csv2rdf.BASE = rdflib.Namespace(opts["-b"])
    if "--base" in opts:
        csv2rdf.BASE = rdflib.Namespace(opts["--base"])

    if "-d" in opts:
        csv2rdf.DELIM = opts["-d"]
    if "--delim" in opts:
        csv2rdf.DELIM = opts["--delim"]

    if "-p" in opts:
        csv2rdf.PROPBASE = rdflib.Namespace(opts["-p"])
    if "--propbase" in opts:
        csv2rdf.PROPBASE = rdflib.Namespace(opts["--propbase"])

    if "-l" in opts:
        csv2rdf.LABEL = eval(opts["-l"])
    if "--label" in opts:
        csv2rdf.LABEL = eval(opts["--label"])

    if "-i" in opts:
        csv2rdf.IDENT = eval(opts["-i"])
    if "--ident" in opts:
        csv2rdf.IDENT = eval(opts["--ident"])

    if "-s" in opts:
        csv2rdf.SKIP = int(opts["-s"])
    if "--skip" in opts:
        csv2rdf.SKIP = int(opts["--skip"])

    if "-c" in opts:
        csv2rdf.CLASS = rdflib.URIRef(opts["-c"])
    if "--class" in opts:
        csv2rdf.CLASS = rdflib.URIRef(opts["--class"])

    for k, v in opts.iteritems():
        if k.startswith("--col"):
            csv2rdf.COLUMNS[int(k[5:])] = column(v)
        elif k.startswith("--prop"):
            csv2rdf.PROPS[int(k[6:])] = rdflib.URIRef(v)

    if csv2rdf.CLASS and ("-C" in opts or "--defineclass" in opts):
        csv2rdf.DEFINECLASS = True

    csv2rdf.convert(
        csv_reader(fileinput.input(files), delimiter=csv2rdf.DELIM))


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = graphisomorphism
"""
A commandline tool for testing if RDF graphs are isomorpic, i.e. equal
if BNode labels are ignored.
"""

from rdflib.graph import Graph
from rdflib import BNode
try:
    from itertools import combinations
    assert combinations
except ImportError:  # Python == 2.5
    # Copied from
    # http://docs.python.org/2/library/itertools.html#itertools.combinations
    def combinations(iterable, r):
        # combinations('ABCD', 2) --> AB AC AD BC BD CD
        # combinations(range(4), 3) --> 012 013 023 123
        pool = tuple(iterable)
        n = len(pool)
        if r > n:
            return
        indices = range(r)
        yield tuple(pool[i] for i in indices)
        while True:
            for i in reversed(range(r)):
                if indices[i] != i + n - r:
                    break
            else:
                return
            indices[i] += 1
            for j in range(i + 1, r):
                indices[j] = indices[j - 1] + 1
            yield tuple(pool[i] for i in indices)


class IsomorphicTestableGraph(Graph):
    """
    Ported from:
    http://www.w3.org/2001/sw/DataAccess/proto-tests/tools/rdfdiff.py
    (Sean B Palmer's RDF Graph Isomorphism Tester)
    """
    def __init__(self, **kargs):
        super(IsomorphicTestableGraph, self).__init__(**kargs)
        self.hash = None

    def internal_hash(self):
        """
        This is defined instead of __hash__ to avoid a circular recursion
        scenario with the Memory store for rdflib which requires a hash
        lookup in order to return a generator of triples
        """
        return hash(tuple(sorted(self.hashtriples())))

    def hashtriples(self):
        for triple in self:
            g = ((isinstance(t, BNode) and self.vhash(t)) or t for t in triple)
            yield hash(tuple(g))

    def vhash(self, term, done=False):
        return tuple(sorted(self.vhashtriples(term, done)))

    def vhashtriples(self, term, done):
        for t in self:
            if term in t:
                yield tuple(self.vhashtriple(t, term, done))

    def vhashtriple(self, triple, term, done):
        for p in xrange(3):
            if not isinstance(triple[p], BNode):
                yield triple[p]
            elif done or (triple[p] == term):
                yield p
            else:
                yield self.vhash(triple[p], done=True)

    def __eq__(self, G):
        """Graph isomorphism testing."""
        if not isinstance(G, IsomorphicTestableGraph):
            return False
        elif len(self) != len(G):
            return False
        elif list.__eq__(list(self), list(G)):
            return True  # @@
        return self.internal_hash() == G.internal_hash()

    def __ne__(self, G):
        """Negative graph isomorphism testing."""
        return not self.__eq__(G)


def main():
    import sys
    from optparse import OptionParser
    usage = '''usage: %prog [options] file1 file2 ... fileN'''
    op = OptionParser(usage=usage)
    op.add_option('-s', '--stdin', action='store_true', default=False,
                  help='Load from STDIN as well')
    op.add_option('--format',
                  default='xml',
                  dest='inputFormat',
                  metavar='RDF_FORMAT',
                  choices=['xml', 'trix', 'n3', 'nt', 'rdfa'],
                  help="The format of the RDF document(s) to compare" +
                  "One of 'xml','n3','trix', 'nt', " +
                  "or 'rdfa'.  The default is %default")

    (options, args) = op.parse_args()

    graphs = []
    graph2FName = {}
    if options.stdin:
        graph = IsomorphicTestableGraph().parse(
            sys.stdin, format=options.inputFormat)
        graphs.append(graph)
        graph2FName[graph] = '(STDIN)'
    for fn in args:
        graph = IsomorphicTestableGraph().parse(
            fn, format=options.inputFormat)
        graphs.append(graph)
        graph2FName[graph] = fn
    checked = set()
    for graph1, graph2 in combinations(graphs, 2):
        if (graph1, graph2) not in checked and (graph2, graph1) not in checked:
            assert graph1 == graph2, "%s != %s" % (
                graph2FName[graph1], graph2FName[graph2])

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = rdf2dot
"""
A commandline tool for drawing RDF graphs in Graphviz DOT format

You can draw the graph of an RDF file directly:

.. code-block: bash

   rdf2dot my_rdf_file.rdf | dot -Tpng | display

"""

import rdflib
import rdflib.extras.cmdlineutils

import sys
import cgi
import collections

from rdflib import XSD

LABEL_PROPERTIES = [rdflib.RDFS.label,
                    rdflib.URIRef("http://purl.org/dc/elements/1.1/title"),
                    rdflib.URIRef("http://xmlns.com/foaf/0.1/name"),
                    rdflib.URIRef("http://www.w3.org/2006/vcard/ns#fn"),
                    rdflib.URIRef("http://www.w3.org/2006/vcard/ns#org")
                    ]

XSDTERMS = [
    XSD[x] for x in (
        "anyURI", "base64Binary", "boolean", "byte", "date",
        "dateTime", "decimal", "double", "duration", "float", "gDay", "gMonth",
        "gMonthDay", "gYear", "gYearMonth", "hexBinary", "ID", "IDREF",
        "IDREFS", "int", "integer", "language", "long", "Name", "NCName",
        "negativeInteger", "NMTOKEN", "NMTOKENS", "nonNegativeInteger",
        "nonPositiveInteger", "normalizedString", "positiveInteger", "QName",
        "short", "string", "time", "token", "unsignedByte", "unsignedInt",
        "unsignedLong", "unsignedShort")]

EDGECOLOR = "blue"
NODECOLOR = "black"
ISACOLOR = "black"


def rdf2dot(g, stream, opts={}):
    """
    Convert the RDF graph to DOT
    writes the dot output to the stream
    """

    fields = collections.defaultdict(set)
    nodes = {}

    def node(x):

        if x not in nodes:
            nodes[x] = "node%d" % len(nodes)
        return nodes[x]

    def label(x, g):

        for labelProp in LABEL_PROPERTIES:
            l = g.value(x, labelProp)
            if l:
                return l

        try:
            return g.namespace_manager.compute_qname(x)[2]
        except:
            return x

    def formatliteral(l, g):
        v = cgi.escape(l)
        if l.datatype:
            return u'&quot;%s&quot;^^%s' % (v, qname(l.datatype, g))
        elif l.language:
            return u'&quot;%s&quot;@%s' % (v, l.language)
        return u'&quot;%s&quot;' % v

    def qname(x, g):
        try:
            q = g.compute_qname(x)
            return q[0] + ":" + q[2]
        except:
            return x

    def color(p):
        return "BLACK"

    stream.write(u"digraph { \n node [ fontname=\"DejaVu Sans\" ] ; \n")

    for s, p, o in g:
        sn = node(s)
        if p == rdflib.RDFS.label:
            continue
        if isinstance(o, (rdflib.URIRef, rdflib.BNode)):
            on = node(o)
            opstr = u"\t%s -> %s [ color=%s, label=< <font point-size='10' " + \
                    u"color='#336633'>%s</font> > ] ;\n"
            stream.write(opstr % (sn, on, color(p), qname(p, g)))
        else:
            fields[sn].add((qname(p, g), formatliteral(o, g)))

    for u, n in nodes.items():
        stream.write(u"# %s %s\n" % (u, n))
        f = [u"<tr><td align='left'>%s</td><td align='left'>%s</td></tr>" %
             x for x in sorted(fields[n])]
        opstr = u"%s [ shape=none, color=%s label=< <table color='#666666'" + \
                u" cellborder='0' cellspacing='0' border='1'><tr>" + \
                u"<td colspan='2' bgcolor='grey'><B>%s</B></td></tr><tr>" + \
                u"<td href='%s' bgcolor='#eeeeee' colspan='2'>" + \
                u"<font point-size='10' color='#6666ff'>%s</font></td>" + \
                u"</tr>%s</table> > ] \n"
        stream.write(opstr % (n, NODECOLOR, label(u, g), u, u, u"".join(f)))

    stream.write("}\n")


def _help():
    sys.stderr.write("""
rdf2dot.py [-f <format>] files...
Read RDF files given on STDOUT, writes a graph of the RDFS schema in DOT
language to stdout
-f specifies parser to use, if not given,

""")


def main():
    rdflib.extras.cmdlineutils.main(rdf2dot, _help)

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = rdfpipe
#!/usr/bin/env python
# -*- coding: UTF-8 -*-
"""
A commandline tool for parsing RDF in different formats and serializing the
resulting graph to a chosen format.
"""

import sys
from optparse import OptionParser
import logging

import rdflib
from rdflib import plugin
from rdflib.store import Store
from rdflib.graph import ConjunctiveGraph
from rdflib.namespace import RDF, RDFS, OWL, XSD
from rdflib.parser import Parser
from rdflib.serializer import Serializer

from rdflib.util import guess_format
from rdflib.py3compat import PY3


DEFAULT_INPUT_FORMAT = 'xml'
DEFAULT_OUTPUT_FORMAT = 'n3'


def parse_and_serialize(input_files, input_format, guess,
                        outfile, output_format, ns_bindings,
                        store_conn="", store_type=None):

    if store_type:
        store = plugin.get(store_type, Store)()
        store.open(store_conn)
        graph = ConjunctiveGraph(store)
    else:
        store = None
        graph = ConjunctiveGraph()

    for prefix, uri in ns_bindings.items():
        graph.namespace_manager.bind(prefix, uri, override=False)

    for fpath in input_files:
        use_format, kws = _format_and_kws(input_format)
        if fpath == '-':
            fpath = sys.stdin
        elif not input_format and guess:
            use_format = guess_format(fpath) or DEFAULT_INPUT_FORMAT
        graph.parse(fpath, format=use_format, **kws)

    if outfile:
        output_format, kws = _format_and_kws(output_format)
        kws.setdefault('base', None)
        graph.serialize(destination=outfile, format=output_format, **kws)

    if store:
        store.rollback()


def _format_and_kws(fmt):
    """
    >>> _format_and_kws("fmt")
    ('fmt', {})
    >>> _format_and_kws("fmt:+a")
    ('fmt', {'a': True})
    >>> _format_and_kws("fmt:a")
    ('fmt', {'a': True})
    >>> _format_and_kws("fmt:+a,-b") #doctest: +SKIP
    ('fmt', {'a': True, 'b': False})
    >>> _format_and_kws("fmt:c=d")
    ('fmt', {'c': 'd'})
    >>> _format_and_kws("fmt:a=b:c")
    ('fmt', {'a': 'b:c'})
    """
    fmt, kws = fmt, {}
    if fmt and ':' in fmt:
        fmt, kwrepr = fmt.split(':', 1)
        for kw in kwrepr.split(','):
            if '=' in kw:
                k, v = kw.split('=')
                kws[k] = v
            elif kw.startswith('-'):
                kws[kw[1:]] = False
            elif kw.startswith('+'):
                kws[kw[1:]] = True
            else:  # same as "+"
                kws[kw] = True
    return fmt, kws


def make_option_parser():
    parser_names = _get_plugin_names(Parser)
    serializer_names = _get_plugin_names(Serializer)
    kw_example = "FORMAT:(+)KW1,-KW2,KW3=VALUE"

    oparser = OptionParser(
        "%prog [-h] [-i INPUT_FORMAT] [-o OUTPUT_FORMAT] " +
        "[--ns=PFX=NS ...] [-] [FILE ...]",
        description=__doc__.strip() + (
        " Reads file system paths, URLs or from stdin if '-' is given."
        " The result is serialized to stdout."),
        version="%prog " + "(using rdflib %s)" % rdflib.__version__)

    oparser.add_option(
        '-i', '--input-format',
        type=str,  # default=DEFAULT_INPUT_FORMAT,
        help="Format of the input document(s)."
        " Available input formats are: %s." % parser_names +
        " If no format is given, it will be " +
        "guessed from the file name extension." +
        " Keywords to parser can be given after format like: %s." % kw_example,
        metavar="INPUT_FORMAT")

    oparser.add_option(
        '-o', '--output-format',
        type=str, default=DEFAULT_OUTPUT_FORMAT,
        help="Format of the graph serialization."
        " Available output formats are: %s."
        % serializer_names +
        " Default format is: '%default'." +
        " Keywords to serializer can be given after format like: %s." %
        kw_example,
        metavar="OUTPUT_FORMAT")

    oparser.add_option(
        '--ns',
        action="append", type=str,
        help="Register a namespace binding (QName prefix to a base URI). "
        "This can be used more than once.",
        metavar="PREFIX=NAMESPACE")

    oparser.add_option(
        '--no-guess', dest='guess',
        action='store_false', default=True,
        help="Don't guess format based on file suffix.")

    oparser.add_option(
        '--no-out',
        action='store_true', default=False,
        help="Don't output the resulting graph " +
             "(useful for checking validity of input).")

    oparser.add_option(
        '-w', '--warn',
        action='store_true', default=False,
        help="Output warnings to stderr (by default only critical errors).")

    return oparser

_get_plugin_names = lambda kind: ", ".join(
    p.name for p in plugin.plugins(kind=kind))


def main():
    oparser = make_option_parser()
    opts, args = oparser.parse_args()
    if len(args) < 1:
        oparser.print_usage()
        oparser.exit()

    if opts.warn:
        loglevel = logging.WARNING
    else:
        loglevel = logging.CRITICAL
    logging.basicConfig(level=loglevel)

    ns_bindings = {}
    if opts.ns:
        for ns_kw in opts.ns:
            pfx, uri = ns_kw.split('=')
            ns_bindings[pfx] = uri

    outfile = sys.stdout
    if PY3:
        outfile = sys.stdout.buffer

    if opts.no_out:
        outfile = None

    parse_and_serialize(args, opts.input_format, opts.guess,
                        outfile, opts.output_format, ns_bindings)


if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = rdfs2dot
"""
A commandline tool for drawing RDFS Class diagrams in Graphviz DOT
format

You can draw the graph of an RDFS file directly:

.. code-block: bash

   rdf2dot my_rdfs_file.rdf | dot -Tpng | display
"""

import rdflib.extras.cmdlineutils

import sys
import itertools
import collections


from rdflib import XSD, RDF, RDFS


XSDTERMS = [XSD[x] for x in (
    "anyURI", "base64Binary", "boolean", "byte", "date", "dateTime", "decimal",
    "double", "duration", "float", "gDay", "gMonth", "gMonthDay", "gYear",
    "gYearMonth", "hexBinary", "ID", "IDREF", "IDREFS", "int", "integer",
    "language", "long", "Name", "NCName", "negativeInteger", "NMTOKEN",
    "NMTOKENS", "nonNegativeInteger", "nonPositiveInteger", "normalizedString",
    "positiveInteger", "QName", "short", "string", "time", "token",
    "unsignedByte", "unsignedInt", "unsignedLong", "unsignedShort")]

EDGECOLOR = "blue"
NODECOLOR = "black"
ISACOLOR = "black"


def rdfs2dot(g, stream, opts={}):
    """
    Convert the RDFS schema in a graph
    writes the dot output to the stream
    """

    fields = collections.defaultdict(set)
    nodes = {}

    def node(x):

        if x not in nodes:
            nodes[x] = "node%d" % len(nodes)
        return nodes[x]

    def label(x, g):

        l = g.value(x, RDFS.label)
        if l is None:
            try:
                l = g.namespace_manager.compute_qname(x)[2]
            except:
                pass  # bnodes and some weird URIs cannot be split
        return l

    stream.write(u"digraph { \n node [ fontname=\"DejaVu Sans\" ] ; \n")

    for x in g.subjects(RDF.type, RDFS.Class):
        n = node(x)

    for x, y in g.subject_objects(RDFS.subClassOf):
        x = node(x)
        y = node(y)
        stream.write(u"\t%s -> %s [ color=%s ] ;\n" % (y, x, ISACOLOR))

    for x in g.subjects(RDF.type, RDF.Property):
        for a, b in itertools.product(
                g.objects(x, RDFS.domain), g.objects(x, RDFS.range)):
            if b in XSDTERMS or b == RDFS.Literal:
                l = label(b, g)
                if b == RDFS.Literal:
                    l = "literal"
                fields[node(a)].add((label(x, g), l))
            else:
    #            if a in nodes and b in nodes:
                stream.write(
                    "\t%s -> %s [ color=%s, label=\"%s\" ];\n" % (
                        node(a), node(b), EDGECOLOR, label(x, g)))

    for u, n in nodes.items():
        stream.write(u"# %s %s\n" % (u, n))
        f = [u"<tr><td align='left'>%s</td><td>%s</td></tr>" %
             x for x in sorted(fields[n])]
        opstr = u"%s [ shape=none, color=%s label=< <table color='#666666'" + \
                u" cellborder=\"0\" cellspacing='0' border=\"1\"><tr>" + \
                u"<td colspan=\"2\" bgcolor='grey'><B>%s</B></td>" + \
                u"</tr>%s</table> > ] \n"
        stream.write(opstr % (n, NODECOLOR, label(u, g), u"".join(f)))

    stream.write("}\n")


def _help():
    sys.stderr.write("""
rdfs2dot.py [-f <format>] files...
Read RDF files given on STDOUT, writes a graph of the RDFS schema in
DOT language to stdout
-f specifies parser to use, if not given,

""")


def main():
    rdflib.extras.cmdlineutils.main(rdfs2dot, _help)

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = util
"""
Some utility functions.

Miscellaneous utilities

* list2set
* first
* uniq
* more_than

Term characterisation and generation

* to_term
* from_n3

Date/time utilities

* date_time
* parse_date_time

Statement and component type checkers

* check_context
* check_subject
* check_predicate
* check_object
* check_statement
* check_pattern

"""

from calendar import timegm
from time import altzone
# from time import daylight
from time import gmtime
from time import localtime
from time import time
from time import timezone

from os.path import splitext
from StringIO import StringIO

from rdflib.exceptions import ContextTypeError
from rdflib.exceptions import ObjectTypeError
from rdflib.exceptions import PredicateTypeError
from rdflib.exceptions import SubjectTypeError
from rdflib.graph import Graph
from rdflib.graph import QuotedGraph
from rdflib.term import BNode
from rdflib.term import Literal
from rdflib.term import URIRef
from rdflib.py3compat import sign

__all__ = [
    'list2set', 'first', 'uniq', 'more_than', 'to_term', 'from_n3',
    'date_time', 'parse_date_time', 'check_context', 'check_subject',
    'check_predicate', 'check_object', 'check_statement', 'check_pattern',
    'guess_format', 'find_roots', 'get_tree']


def list2set(seq):
    """
    Return a new list without duplicates.
    Preserves the order, unlike set(seq)
    """
    seen = set()
    return [x for x in seq if x not in seen and not seen.add(x)]


def first(seq):
    """
    return the first element in a python sequence
    for graphs, use graph.value instead
    """
    for result in seq:
        return result
    return None


def uniq(sequence, strip=0):
    """removes duplicate strings from the sequence."""
    if strip:
        return set(s.strip() for s in sequence)
    else:
        return set(sequence)


def more_than(sequence, number):
    "Returns 1 if sequence has more items than number and 0 if not."
    i = 0
    for item in sequence:
        i += 1
        if i > number:
            return 1
    return 0


def to_term(s, default=None):
    """
    Creates and returns an Identifier of type corresponding
    to the pattern of the given positional argument string ``s``:

    '' returns the ``default`` keyword argument value or ``None``

    '<s>' returns ``URIRef(s)`` (i.e. without angle brackets)

    '"s"' returns ``Literal(s)`` (i.e. without doublequotes)

    '_s' returns ``BNode(s)`` (i.e. without leading underscore)

    """
    if not s:
        return default
    elif s.startswith("<") and s.endswith(">"):
        return URIRef(s[1:-1])
    elif s.startswith('"') and s.endswith('"'):
        return Literal(s[1:-1])
    elif s.startswith("_"):
        return BNode(s)
    else:
        msg = "Unrecognised term syntax: '%s'" % s
        raise Exception(msg)


def from_n3(s, default=None, backend=None):
    r'''
    Creates the Identifier corresponding to the given n3 string.

        >>> from_n3('<http://ex.com/foo>') == URIRef('http://ex.com/foo')
        True
        >>> from_n3('"foo"@de') == Literal('foo', lang='de')
        True
        >>> from_n3('"""multi\nline\nstring"""@en') == Literal(
        ...     'multi\nline\nstring', lang='en')
        True
        >>> from_n3('42') == Literal(42)
        True

    '''
    # TODO: should be able to handle prefixes given as opt. argument maybe:
    # from_n3('rdfs:label')
    if not s:
        return default
    if s.startswith('<'):
        return URIRef(s[1:-1])
    elif s.startswith('"'):
        if s.startswith('"""'):
            quotes = '"""'
        else:
            quotes = '"'
        value, rest = s.rsplit(quotes, 1)
        value = value[len(quotes):]  # strip leading quotes
        datatype = None
        language = None

        # as a given datatype overrules lang-tag check for it first
        dtoffset = rest.rfind('^^')
        if dtoffset >= 0:
            # found a datatype
            # datatype has to come after lang-tag so ignore everything before
            # see: http://www.w3.org/TR/2011/WD-turtle-20110809/
            # #prod-turtle2-RDFLiteral
            datatype = rest[dtoffset + 2:]
        else:
            if rest.startswith("@"):
                language = rest[1:]  # strip leading at sign

        value = value.replace(r'\"', '"').replace('\\\\', '\\')
        # Hack: this should correctly handle strings with either native unicode
        # characters, or \u1234 unicode escapes.
        value = value.encode("raw-unicode-escape").decode("unicode-escape")
        return Literal(value, language, datatype)
    elif s == 'true' or s == 'false':
        return Literal(s == 'true')
    elif s.isdigit():
        return Literal(int(s))
    elif s.startswith('{'):
        identifier = from_n3(s[1:-1])
        return QuotedGraph(backend, identifier)
    elif s.startswith('['):
        identifier = from_n3(s[1:-1])
        return Graph(backend, identifier)
    else:
        if s.startswith("_:"):
            return BNode(s[2:])
        else:
            return BNode(s)


def check_context(c):
    if not (isinstance(c, URIRef) or
            isinstance(c, BNode)):
        raise ContextTypeError("%s:%s" % (c, type(c)))


def check_subject(s):
    """ Test that s is a valid subject identifier."""
    if not (isinstance(s, URIRef) or isinstance(s, BNode)):
        raise SubjectTypeError(s)


def check_predicate(p):
    """ Test that p is a valid predicate identifier."""
    if not isinstance(p, URIRef):
        raise PredicateTypeError(p)


def check_object(o):
    """ Test that o is a valid object identifier."""
    if not (isinstance(o, URIRef) or
            isinstance(o, Literal) or
            isinstance(o, BNode)):
        raise ObjectTypeError(o)


def check_statement(triple):
    (s, p, o) = triple
    if not (isinstance(s, URIRef) or isinstance(s, BNode)):
        raise SubjectTypeError(s)

    if not isinstance(p, URIRef):
        raise PredicateTypeError(p)

    if not (isinstance(o, URIRef) or
            isinstance(o, Literal) or
            isinstance(o, BNode)):
        raise ObjectTypeError(o)


def check_pattern(triple):
    (s, p, o) = triple
    if s and not (isinstance(s, URIRef) or isinstance(s, BNode)):
        raise SubjectTypeError(s)

    if p and not isinstance(p, URIRef):
        raise PredicateTypeError(p)

    if o and not (isinstance(o, URIRef) or
                  isinstance(o, Literal) or
                  isinstance(o, BNode)):
        raise ObjectTypeError(o)


def date_time(t=None, local_time_zone=False):
    """http://www.w3.org/TR/NOTE-datetime ex: 1997-07-16T19:20:30Z

    >>> date_time(1126482850)
    '2005-09-11T23:54:10Z'

    @@ this will change depending on where it is run
    #>>> date_time(1126482850, local_time_zone=True)
    #'2005-09-11T19:54:10-04:00'

    >>> date_time(1)
    '1970-01-01T00:00:01Z'

    >>> date_time(0)
    '1970-01-01T00:00:00Z'
    """
    if t is None:
        t = time()

    if local_time_zone:
        time_tuple = localtime(t)
        if time_tuple[8]:
            tz_mins = altzone // 60
        else:
            tz_mins = timezone // 60
        tzd = "-%02d:%02d" % (tz_mins // 60, tz_mins % 60)
    else:
        time_tuple = gmtime(t)
        tzd = "Z"

    year, month, day, hh, mm, ss, wd, y, z = time_tuple
    s = "%0004d-%02d-%02dT%02d:%02d:%02d%s" % (
        year, month, day, hh, mm, ss, tzd)
    return s


def parse_date_time(val):
    """always returns seconds in UTC

    # tests are written like this to make any errors easier to understand
    >>> parse_date_time('2005-09-11T23:54:10Z') - 1126482850.0
    0.0

    >>> parse_date_time('2005-09-11T16:54:10-07:00') - 1126482850.0
    0.0

    >>> parse_date_time('1970-01-01T00:00:01Z') - 1.0
    0.0

    >>> parse_date_time('1970-01-01T00:00:00Z') - 0.0
    0.0
    >>> parse_date_time("2005-09-05T10:42:00") - 1125916920.0
    0.0
    """

    if "T" not in val:
        val += "T00:00:00Z"

    ymd, time = val.split("T")
    hms, tz_str = time[0:8], time[8:]

    if not tz_str or tz_str == "Z":
        time = time[:-1]
        tz_offset = 0
    else:
        signed_hrs = int(tz_str[:3])
        mins = int(tz_str[4:6])
        secs = (sign(signed_hrs) * mins + signed_hrs * 60) * 60
        tz_offset = -secs

    year, month, day = ymd.split("-")
    hour, minute, second = hms.split(":")

    t = timegm((int(year), int(month), int(day), int(hour),
                int(minute), int(second), 0, 0, 0))
    t = t + tz_offset
    return t





SUFFIX_FORMAT_MAP = {
    'rdf': 'xml',
    'rdfs': 'xml',
    'owl': 'xml',
    'n3': 'n3',
    'ttl': 'n3',
    'nt': 'nt',
    'trix': 'trix',
    'xhtml': 'rdfa',
    'html': 'rdfa',
    'svg': 'rdfa',
    'nq': 'nquads',
    'trig': 'trig'
}


def guess_format(fpath, fmap=None):
    """
    Guess RDF serialization based on file suffix. Uses
    ``SUFFIX_FORMAT_MAP`` unless ``fmap`` is provided. Examples:

        >>> guess_format('path/to/file.rdf')
        'xml'
        >>> guess_format('path/to/file.owl')
        'xml'
        >>> guess_format('path/to/file.ttl')
        'n3'
        >>> guess_format('path/to/file.xhtml')
        'rdfa'
        >>> guess_format('path/to/file.svg')
        'rdfa'
        >>> guess_format('path/to/file.xhtml', {'xhtml': 'grddl'})
        'grddl'

    This also works with just the suffixes, with or without leading dot, and
    regardless of letter case::

        >>> guess_format('.rdf')
        'xml'
        >>> guess_format('rdf')
        'xml'
        >>> guess_format('RDF')
        'xml'
    """
    fmap = fmap or SUFFIX_FORMAT_MAP
    return fmap.get(_get_ext(fpath)) or fmap.get(fpath.lower())


def _get_ext(fpath, lower=True):
    """
    Gets the file extension from a file(path); stripped of leading '.' and in
    lower case. Examples:

        >>> _get_ext("path/to/file.txt")
        'txt'
        >>> _get_ext("OTHER.PDF")
        'pdf'
        >>> _get_ext("noext")
        ''
        >>> _get_ext(".rdf")
        'rdf'
    """
    ext = splitext(fpath)[-1]
    if ext == '' and fpath.startswith("."):
        ext = fpath
    if lower:
        ext = ext.lower()
    if ext.startswith('.'):
        ext = ext[1:]
    return ext


def find_roots(graph, prop, roots=None):
    """
    Find the roots in some sort of transitive hierarchy.

    find_roots(graph, rdflib.RDFS.subClassOf)
    will return a set of all roots of the sub-class hierarchy

    Assumes triple of the form (child, prop, parent), i.e. the direction of
    RDFS.subClassOf or SKOS.broader

    """

    non_roots = set()
    if roots is None:
        roots = set()
    for x, y in graph.subject_objects(prop):
        non_roots.add(x)
        if x in roots:
            roots.remove(x)
        if y not in non_roots:
            roots.add(y)
    return roots


def get_tree(graph,
             root,
             prop,
             mapper=lambda x: x,
             sortkey=None,
             done=None,
             dir='down'):
    """
    Return a nested list/tuple structure representing the tree
    built by the transitive property given, starting from the root given

    i.e.

    get_tree(graph,
       rdflib.URIRef("http://xmlns.com/foaf/0.1/Person"),
       rdflib.RDFS.subClassOf)

    will return the structure for the subClassTree below person.

    dir='down' assumes triple of the form (child, prop, parent),
    i.e. the direction of RDFS.subClassOf or SKOS.broader
    Any other dir traverses in the other direction

    """

    if done is None:
        done = set()
    if root in done:
        return
    done.add(root)
    tree = []

    if dir == 'down':
        branches = graph.subjects(prop, root)
    else:
        branches = graph.objects(root, prop)

    for branch in branches:
        t = get_tree(graph, branch, prop, mapper, sortkey, done, dir)
        if t:
            tree.append(t)

    return (mapper(root), sorted(tree, key=sortkey))




def test():
    import doctest
    doctest.testmod()

if __name__ == "__main__":
    # try to make the tests work outside of the time zone they were written in
    # import os, time
    # os.environ['TZ'] = 'US/Pacific'
    # try:
    #    time.tzset()
    # except AttributeError, e:
    #    print e
        # pass
        # tzset missing! see
        # http://mail.python.org/pipermail/python-dev/2003-April/034480.html
    test()  # pragma: no cover

########NEW FILE########
__FILENAME__ = void
import collections

from rdflib import URIRef, Graph, Literal
from rdflib.namespace import VOID, RDF


def generateVoID(g, dataset=None, res=None, distinctForPartitions=True):
    """
    Returns a new graph with a VoID description of the passed dataset

    For more info on Vocabulary of Interlinked Datasets (VoID), see:
    http://vocab.deri.ie/void

    This only makes two passes through the triples (once to detect the types
    of things)

    The tradeoff is that lots of temporary structures are built up in memory
    meaning lots of memory may be consumed :)
    I imagine at least a few copies of your original graph.

    the distinctForPartitions parameter controls whether
    distinctSubjects/objects are tracked for each class/propertyPartition
    this requires more memory again

    """

    typeMap = collections.defaultdict(set)
    classes = collections.defaultdict(set)
    for e, c in g.subject_objects(RDF.type):
        classes[c].add(e)
        typeMap[e].add(c)

    triples = 0
    subjects = set()
    objects = set()
    properties = set()
    classCount = collections.defaultdict(int)
    propCount = collections.defaultdict(int)

    classProps = collections.defaultdict(set)
    classObjects = collections.defaultdict(set)
    propSubjects = collections.defaultdict(set)
    propObjects = collections.defaultdict(set)

    for s, p, o in g:

        triples += 1
        subjects.add(s)
        properties.add(p)
        objects.add(o)

        # class partitions
        if s in typeMap:
            for c in typeMap[s]:
                classCount[c] += 1
                if distinctForPartitions:
                    classObjects[c].add(o)
                    classProps[c].add(p)

        # property partitions
        propCount[p] += 1
        if distinctForPartitions:
            propObjects[p].add(o)
            propSubjects[p].add(s)

    if not dataset:
        dataset = URIRef("http://example.org/Dataset")

    if not res:
        res = Graph()

    res.add((dataset, RDF.type, VOID.Dataset))

    # basic stats
    res.add((dataset, VOID.triples, Literal(triples)))
    res.add((dataset, VOID.classes, Literal(len(classes))))

    res.add((dataset, VOID.distinctObjects, Literal(len(objects))))
    res.add((dataset, VOID.distinctSubjects, Literal(len(subjects))))
    res.add((dataset, VOID.properties, Literal(len(properties))))

    for i, c in enumerate(classes):
        part = URIRef(dataset + "_class%d" % i)
        res.add((dataset, VOID.classPartition, part))
        res.add((part, RDF.type, VOID.Dataset))

        res.add((part, VOID.triples, Literal(classCount[c])))
        res.add((part, VOID.classes, Literal(1)))

        res.add((part, VOID["class"], c))

        res.add((part, VOID.entities, Literal(len(classes[c]))))
        res.add((part, VOID.distinctSubjects, Literal(len(classes[c]))))

        if distinctForPartitions:
            res.add(
                (part, VOID.properties, Literal(len(classProps[c]))))
            res.add((part, VOID.distinctObjects,
                    Literal(len(classObjects[c]))))

    for i, p in enumerate(properties):
        part = URIRef(dataset + "_property%d" % i)
        res.add((dataset, VOID.propertyPartition, part))
        res.add((part, RDF.type, VOID.Dataset))

        res.add((part, VOID.triples, Literal(propCount[p])))
        res.add((part, VOID.properties, Literal(1)))

        res.add((part, VOID.property, p))

        if distinctForPartitions:

            entities = 0
            propClasses = set()
            for s in propSubjects[p]:
                if s in typeMap:
                    entities += 1
                for c in typeMap[s]:
                    propClasses.add(c)

            res.add((part, VOID.entities, Literal(entities)))
            res.add((part, VOID.classes, Literal(len(propClasses))))

            res.add((part, VOID.distinctSubjects,
                    Literal(len(propSubjects[p]))))
            res.add((part, VOID.distinctObjects,
                    Literal(len(propObjects[p]))))

    return res, dataset

########NEW FILE########
__FILENAME__ = run_tests
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Testing with Nose
=================

This test runner uses Nose for test discovery and running. It uses the argument
spec of Nose, but with some options pre-set. To begin with, make sure you have
Nose installed, e.g.:

    $ sudo easy_install nose

For daily test runs, use:

    $ ./run_tests.py

If you supply attributes, the default ones defined in ``DEFAULT_ATTRS`` will be
ignored. So to run e.g. all tests marked ``slowtest`` or ``non_standard_dep``,
do:

    $ ./run_tests.py -a slowtest,non_standard_dep

See <http://code.google.com/p/python-nose/> for furher details. An excellent
article is also available at <http://ivory.idyll.org/articles/nose-intro.html>.

Note that this is just a convenience script. You can use ``nosetests`` directly
if it's on $PATH, with the difference that you have to supply the options
pre-set here manually.

Coverage
========

If ``coverage.py`` is placed in $PYTHONPATH, it can be used to create coverage
information (using the built-in coverage plugin of Nose) if the default
option "--with-coverage" is supplied (which also enables some additional
coverage options).

See <http://nedbatchelder.com/code/modules/coverage.html> for details.

"""


NOSE_ARGS = [
        '--where=./',
        '--with-doctest',
        '--doctest-extension=.doctest',
        '--doctest-tests',
#        '--with-EARL',
    ]

COVERAGE_EXTRA_ARGS = [
        '--cover-package=rdflib',
        '--cover-inclusive',
    ]

DEFAULT_ATTRS = [] # ['!known_issue', '!sparql']

DEFAULT_DIRS = ['test', 'rdflib']


if __name__ == '__main__':

    from sys import argv, exit, stderr
    try: import nose
    except ImportError:
        print >>stderr, """\
    Requires Nose. Try:

        $ sudo easy_install nose

    Exiting. """; exit(1)


    if '--with-coverage' in argv:
        try: import coverage
        except ImportError:
            print >>stderr, "No coverage module found, skipping code coverage."
            argv.remove('--with-coverage')
        else:
            NOSE_ARGS += COVERAGE_EXTRA_ARGS


    if True not in [a.startswith('-a') or a.startswith('--attr=') for a in argv]:
        argv.append('--attr=' + ','.join(DEFAULT_ATTRS))

    if not [a for a in argv[1:] if not a.startswith('-')]:
        argv += DEFAULT_DIRS # since nose doesn't look here by default..


    finalArgs = argv + NOSE_ARGS
    print "Running nose with:", " ".join(finalArgs[1:])
    nose.run_exit(argv=finalArgs)

########NEW FILE########
__FILENAME__ = earl
from datetime import datetime

from rdflib import Graph, URIRef, Literal, BNode, RDF, Namespace
from rdflib.namespace import FOAF, DOAP, DC

from nose.tools import nottest

EARL = Namespace("http://www.w3.org/ns/earl#")

report = Graph()

report.bind('foaf', FOAF)
report.bind('earl', EARL)
report.bind('doap', DOAP)
report.bind('dc', DC)

me = URIRef('http://gromgull.net/me')
report.add((me, RDF.type, FOAF.Person))
report.add((me, FOAF.homepage, URIRef("http://gromgull.net")))
report.add((me, FOAF.name, Literal("Gunnar Aastrand Grimnes")))

rdflib = URIRef('https://github.com/RDFLib/rdflib')

report.add((rdflib, DOAP.homepage, rdflib))
report.add((rdflib, DOAP.name, Literal("rdflib")))
report.add((rdflib, DOAP.developer, me))
report.add((rdflib, RDF.type, DOAP.Project))

now = Literal(datetime.now())

@nottest
def add_test(test, res, info=None):
    a = BNode()
    report.add((a, RDF.type, EARL.Assertion))
    report.add((a, EARL.assertedBy, me))
    report.add((a, EARL.test, test))
    report.add((a, EARL.subject, rdflib))

    report.add((a, DC.date, now))

    r = BNode()
    report.add((a, EARL.result, r))
    report.add((r, RDF.type, EARL.TestResult))

    report.add((r, EARL.outcome, EARL[res]))
    if info:
        report.add((r, EARL.info, Literal(info)))

########NEW FILE########
__FILENAME__ = manifest
from collections import namedtuple
from nose.tools import nottest

from rdflib import Graph, RDF, RDFS, Namespace

MF = Namespace('http://www.w3.org/2001/sw/DataAccess/tests/test-manifest#')
QT = Namespace('http://www.w3.org/2001/sw/DataAccess/tests/test-query#')
UP = Namespace('http://www.w3.org/2009/sparql/tests/test-update#')
RDFT = Namespace('http://www.w3.org/ns/rdftest#')

DAWG = Namespace('http://www.w3.org/2001/sw/DataAccess/tests/test-dawg#')

RDFTest = namedtuple('RDFTest', ['uri', 'name', 'comment', 'data',
                         'graphdata', 'action', 'result', 'syntax'])

def read_manifest(f, base=None, legacy=False):

    def _str(x):
        if x is not None:
            return unicode(x)
        return None

    g = Graph()
    g.load(f, publicID=base, format='turtle')

    for m in g.subjects(RDF.type, MF.Manifest):

        for col in g.objects(m, MF.include):
            for i in g.items(col):
                for x in read_manifest(i):
                    yield x

        for col in g.objects(m, MF.entries):
            for e in g.items(col):

                approved = ((e, DAWG.approval, DAWG.Approved) in g or
                            (e, DAWG.approval, DAWG.NotClassified) in g or
                            (e, RDFT.approval, RDFT.Approved) in g)

                # run proposed tests
                # approved |= (e, RDFT.approval, RDFT.Proposed) in g

                # run legacy tests with no approval set
                if legacy:
                    approved |= ((e, DAWG.approval, None) not in g and
                                 (e, RDFT.approval, None) not in g)

                if not approved:
                    continue

                _type = g.value(e, RDF.type)

                # if _type in (MF.ServiceDescriptionTest, MF.ProtocolTest):
                #     continue  # skip tests we do not know

                name = g.value(e, MF.name)
                comment = g.value(e, RDFS.comment)
                data = None
                graphdata = None
                res = None
                syntax = True


                if _type in (MF.QueryEvaluationTest, MF.CSVResultFormatTest):
                    a = g.value(e, MF.action)
                    query = g.value(a, QT.query)
                    data = g.value(a, QT.data)
                    graphdata = list(g.objects(a, QT.graphData))
                    res = g.value(e, MF.result)
                elif _type in (MF.UpdateEvaluationTest, UP.UpdateEvaluationTest):
                    a = g.value(e, MF.action)
                    query = g.value(a, UP.request)
                    data = g.value(a, UP.data)
                    graphdata = []
                    for gd in g.objects(a, UP.graphData):
                        graphdata.append((g.value(gd, UP.graph),
                                          g.value(gd, RDFS.label)))

                    r = g.value(e, MF.result)
                    resdata = g.value(r, UP.data)
                    resgraphdata = []
                    for gd in g.objects(r, UP.graphData):
                        resgraphdata.append((g.value(gd, UP.graph),
                                             g.value(gd, RDFS.label)))

                    res = resdata, resgraphdata

                elif _type in (MF.NegativeSyntaxTest11, MF.PositiveSyntaxTest11):
                    query = g.value(e, MF.action)
                    syntax = _type == MF.PositiveSyntaxTest11

                elif _type in (MF.PositiveUpdateSyntaxTest11,
                           MF.NegativeUpdateSyntaxTest11):
                    query = g.value(e, MF.action)
                    syntax = _type == MF.PositiveUpdateSyntaxTest11

                elif _type in (RDFT.TestNQuadsPositiveSyntax,
                               RDFT.TestNQuadsNegativeSyntax,
                               RDFT.TestTrigPositiveSyntax,
                               RDFT.TestTrigNegativeSyntax,
                               RDFT.TestNTriplesPositiveSyntax,
                               RDFT.TestNTriplesNegativeSyntax,
                               RDFT.TestTurtlePositiveSyntax,
                               RDFT.TestTurtleNegativeSyntax,
                ):
                    query = g.value(e, MF.action)
                    syntax = _type in (RDFT.TestNQuadsPositiveSyntax,
                                       RDFT.TestNTriplesPositiveSyntax,
                                       RDFT.TestTrigPositiveSyntax,
                                       RDFT.TestTurtlePositiveSyntax)

                elif _type in (RDFT.TestTurtleEval, RDFT.TestTurtleNegativeEval,
                               RDFT.TestTrigEval, RDFT.TestTrigNegativeEval):
                    query = g.value(e, MF.action)
                    res = g.value(e, MF.result)
                    syntax = _type in (RDFT.TestTurtleEval, RDFT.TestTrigEval)

                else:
                    pass
                    print "I dont know DAWG Test Type %s" % _type
                    continue

                yield _type, RDFTest(e, _str(name), _str(comment),
                               _str(data), graphdata, _str(query),
                               res, syntax)

@nottest
def nose_tests(testers, manifest, base=None, legacy=False):
    for _type, test in read_manifest(manifest, base, legacy):
        if _type in testers:
            yield testers[_type], test

########NEW FILE########
__FILENAME__ = run_w3c_rdfa_testsuite
#!/usr/bin/python
# -*- coding: UTF-8 -*-
"""
==========================
W3C RDFa Test Suite Runner
==========================

This test suite generates test functions with ``all_tests`` which runs the
``RDFaParser`` against the official RDFa testsuite (found via
``XHTML_RDFA_TEST_MANIFEST_URL``).

It is intended to be run by Nose (but can also be invoked as a script).

If files in "test/rdfa/w3c_rdfa_suite" are removed, the test module
automatically downloads them (to allow for "manual+automatic" update).
"""
from operator import attrgetter
import re
import os
from urllib2 import urlopen
from urllib import url2pathname

from rdflib.graph import Graph
from rdflib.namespace import Namespace, RDF
from rdflib.parser import create_input_source
# from rdflib.plugins.parsers.rdfa import RDFaParser
from rdflib.plugins.parsers.structureddata import RDFa10Parser as RDFaParser

DC = Namespace("http://purl.org/dc/elements/1.1/")
TEST = Namespace("http://www.w3.org/2006/03/test-description#")

XHTML_RDFA_TEST_MANIFEST_URL = ("http://www.w3.org/2006/07/SWD/RDFa/"
        "testsuite/xhtml1-testcases/rdfa-xhtml1-test-manifest.rdf")

class TestCase(object):
    def __init__(self, graph, tc_uri):
        val = lambda p: graph.value(tc_uri, p)
        self.number = int(re.search(r'/Test(\d+)$', tc_uri).group(1))
        self.title = val(DC.title)
        self.html_url = val(TEST.informationResourceInput)
        self.sparql_url = val(TEST.informationResourceResults)
        self.status = val(TEST.reviewStatus).split("#")[-1]
        self.expected = val(TEST.expectedResults) in (None, 'true')

    @classmethod
    def all(cls, graph):
        for tc_uri in graph.subjects(RDF.type, TEST.TestCase):
            yield cls(graph, tc_uri)


def get_tcs(manifest_url=XHTML_RDFA_TEST_MANIFEST_URL,
        status="approved"):
    graph = Graph().parse(cached_file(manifest_url), publicID=manifest_url)
    return sorted((tc for tc in TestCase.all(graph) if tc.status == status),
            key=attrgetter('number'))

def run_tc(tc):
    parser = RDFaParser()
    graph = Graph()
    source = create_input_source(cached_file(tc.html_url), publicID=tc.html_url)
    parser.parse(source, graph)
    sparql = open(cached_file(tc.sparql_url)).read()
    ok = verify_ask(sparql, graph, tc.expected)
    return ok, sparql, graph


def verify_ask(sparql, graph, expected):
    try:
        result = graph.query(sparql.decode('utf-8'))
        ok = result.serialize('python') == expected
    except: # TODO: parse failures are probably sparql processor bugs
        ok = False
    if ok:
        return ok
    # TODO: sparql bugs cause a bunch to fail (at least bnodes and xmlliterals)
    # .. extract N3 from ASK and compare graphs instead:
    from rdflib.compare import isomorphic
    for ask_graph in _sparql_to_graphs(sparql):
        if isomorphic(graph, ask_graph) == expected:
            return True
        #else: print ask_graph.serialize(format='nt')
    return False

def _sparql_to_graphs(sparql):
    # turn sparql into n3
    # try to turn bnode sparql into bnode n3
    # NOTE: this requires *all* FILTER tests to use isBlank!
    if re.search(r'(?i)isBlank', sparql):
        sparql = re.sub(r'(?im)^\s*FILTER.+ISBLANK.*$', '', sparql)
        sparql = re.sub(r'\?(\w+)', r'_:\1', sparql)
    # remove ASK block
    n3 = re.sub(r'(?s)ASK\s+WHERE\s*{(.*)}', r'\1', sparql)
    # split union into chunks
    n3chunks = [re.sub(r'(?s)\s*{(.*)}\s*', r'\1', block)
                for block in n3.split('UNION')]
    for chunk in n3chunks:
        yield Graph().parse(data=chunk, format='n3')


CACHE_DIR = os.path.join(os.path.dirname(__file__), "w3c_rdfa_testsuite")

def cached_file(url):
    fname = os.path.basename(url2pathname(url.split(':', 1)[1]))
    fpath = os.path.join(CACHE_DIR, fname)
    if not os.path.exists(fpath):
        f = open(fpath, 'w')
        try:
            f.write(urlopen(url).read())
        finally:
            f.close()
    return fpath


KNOWN_ISSUES = set([11, 92, 94, 100, 101, 102, 103, 114])
KNOWN_ISSUES |= set([105, 106])


def all_tests(skip_known_issues=True):
    """
    Generator used to expose test functions. The Nose test runner use this.
    """
    from nose import SkipTest
    raise SkipTest("Suspended until test suite is updated, use 'manual_run' instead.")
    import platform
    if platform.system() == 'Java':
        from nose import SkipTest
        raise SkipTest('html5lib unavailable in Jython2.5')
    for tc in get_tcs():
        label = "RDFa TC #%(number)s: %(title)s (%(status)s)"%vars(tc)
        urls = "[<%(html_url)s>, <%(sparql_url)s>]"%vars(tc)
        def do_test():
            ok, sparql, graph = run_tc(tc)
            if not ok:
                n3 = graph.serialize(format='nt')
                raise AssertionError(
                        "The SPARQL:\n%(sparql)s\nDid not match:\n%(n3)s"%vars())
        if skip_known_issues and tc.number in KNOWN_ISSUES:
            # NOTE: nose doesn't support attr-filtering on generated test funcs..
            #do_test.known_issue = True
            continue
        do_test.description = label
        do_test._source_urls = urls
        yield do_test,

def manual_run():
    errors, failed, count = 0, 0, 0
    for test, in all_tests(skip_known_issues=False):
        count += 1
        print(test.description)
        try:
            test() 
            print "PASSED"
        except AssertionError, e:
            failed += 1
            print "****FAILED****", e;
        except Exception, e:
            errors += 1; print "****ERROR**** in %s" % test._source_urls, e
    print "Ran %(count)s tests. Failed: %(failed)s. Errors: %(errors)s."%vars()


if __name__ == '__main__':
    manual_run()

########NEW FILE########
__FILENAME__ = test_non_xhtml
from unittest import TestCase
from urllib2 import URLError

from rdflib.graph import ConjunctiveGraph

from nose import SkipTest
raise SkipTest("Suspended while test suite is being updated.")


class NonXhtmlTest(TestCase):
    """
    RDFa that is in not well-formed XHTML is passed through html5lib.
    These tests make sure that this RDFa can be processed both from
    a file, and from a URL. We can only run these tests if html5lib
    is installed. Currently html5lib isn't a dependency.
    """

    def test_url(self):
        if self.html5lib_installed():
            try:
                g = ConjunctiveGraph()
                g.parse(location='http://oreilly.com/catalog/9780596516499/',
                        format='rdfa')
                self.assertTrue(len(g), 77)
            except URLError:
                from nose import SkipTest
                raise SkipTest('No networking, test skipped')

    def test_file(self):
        if self.html5lib_installed():
            g = ConjunctiveGraph()
            g.parse(location='test/rdfa/oreilly.html', format='rdfa')
            self.assertEqual(len(g), 77)

    def html5lib_installed(self):
        try:
            import html5lib
            return True
        except: 
            return False

########NEW FILE########
__FILENAME__ = store_performance
import unittest
import os
import gc
import itertools
from time import time
from random import random

from tempfile import mkdtemp
import shutil

from rdflib.term import URIRef
from rdflib.graph import Graph


def random_uri():
    return URIRef("%s" % random())


class StoreTestCase(unittest.TestCase):
    """
    Test case for testing store performance... probably should be
    something other than a unit test... but for now we'll add it as a
    unit test.
    """
    store = 'default'
    tmppath = None
    configString = os.environ.get("DBURI", "dburi")

    def setUp(self):
        self.gcold = gc.isenabled()
        gc.collect()
        gc.disable()
        self.graph = Graph(store=self.store)
        if self.store == "MySQL":
            # from test.mysql import configString
            from rdflib.store.MySQL import MySQL
            path = self.configString
            MySQL().destroy(path)
        else:
            self.tmppath = mkdtemp()
        self.graph.open(self.tmppath, create=True)
        self.input = input = Graph()
        input.parse("http://eikeon.com")

    def tearDown(self):
        self.graph.close()
        if self.gcold:
            gc.enable()
        del self.graph
        shutil.rmtree(self.tmppath)

    def testTime(self):
        number = 1
        print self.store
        print "input:",
        for i in itertools.repeat(None, number):
            self._testInput()
        print "random:",
        for i in itertools.repeat(None, number):
            self._testRandom()
        print "."

    def _testRandom(self):
        number = len(self.input)
        store = self.graph

        def add_random():
            s = random_uri()
            p = random_uri()
            o = random_uri()
            store.add((s, p, o))

        it = itertools.repeat(None, number)
        t0 = time()
        for _i in it:
            add_random()
        t1 = time()
        print "%.3g" % (t1 - t0),

    def _testInput(self):
        number = 1
        store = self.graph

        def add_from_input():
            for t in self.input:
                store.add(t)

        it = itertools.repeat(None, number)
        t0 = time()
        for _i in it:
            add_from_input()
        t1 = time()
        print "%.3g" % (t1 - t0),


class MemoryStoreTestCase(StoreTestCase):
    store = "IOMemory"


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = testutils

import sys
import isodate
import datetime

from traceback import print_exc
from nose import SkipTest
from earl import add_test, report

from rdflib import BNode, Graph, ConjunctiveGraph


# TODO: make an introspective version (like this one) of
# rdflib.graphutils.isomorphic and use instead.
def crapCompare(g1, g2):
    """A really crappy way to 'check' if two graphs are equal. It ignores blank
    nodes completely and ignores subgraphs."""
    if len(g1) != len(g2):
        raise Exception("Graphs dont have same length")
    for t in g1:
        s = _no_blank(t[0])
        o = _no_blank(t[2])
        if not (s, t[1] ,o) in g2:
            e = "(%s, %s, %s) is not in both graphs!"%(s, t[1], o)
            raise Exception, e
def _no_blank(node):
    if isinstance(node, BNode): return None
    if isinstance(node, Graph):
        return None #node._Graph__identifier = _SQUASHED_NODE
    return node

def check_serialize_parse(fpath, infmt, testfmt, verbose=False):
    g = ConjunctiveGraph()
    _parse_or_report(verbose, g, fpath, format=infmt)
    if verbose:
        for t in g:
            print t
        print "========================================"
        print "Parsed OK!"
    s = g.serialize(format=testfmt)
    if verbose:
        print s
    g2 = ConjunctiveGraph()
    _parse_or_report(verbose, g2, data=s, format=testfmt)
    if verbose:
        print g2.serialize()
    crapCompare(g,g2)


def _parse_or_report(verbose, graph, *args, **kwargs):
    try:
        graph.parse(*args, **kwargs)
    except:
        if verbose:
            print "========================================"
            print "Error in parsing serialization:"
            print args, kwargs
        raise


def nose_tst_earl_report(generator, earl_report_name=None):
    from optparse import OptionParser
    p = OptionParser()
    (options, args) = p.parse_args()

    skip = 0
    tests = 0
    success = 0

    for t in generator(args):
        tests += 1
        print 'Running ', t[1].uri
        try:
            t[0](t[1])
            add_test(t[1].uri, "passed")
            success += 1
        except SkipTest, e:
            add_test(t[1].uri, "untested", e.message)
            print "skipping %s - %s" % (t[1].uri, e.message)
            skip += 1

        except KeyboardInterrupt:
            raise
        except AssertionError:
            add_test(t[1].uri, "failed")
        except:
            add_test(t[1].uri, "failed", "error")
            print_exc()
            sys.stderr.write("%s\n" % t[1].uri)

    print "Ran %d tests, %d skipped, %d failed. "%(tests, skip, tests-skip-success)
    if earl_report_name:
        now = isodate.datetime_isoformat(datetime.datetime.utcnow())
        earl_report = 'test_reports/%s-%s.ttl' % (earl_report_name, now)

        report.serialize(earl_report, format='n3')
        report.serialize('test_reports/%s-latest.ttl'%earl_report_name, format='n3')
        print "Wrote EARL-report to '%s'" % earl_report

########NEW FILE########
__FILENAME__ = test_aggregate_graphs
import unittest
from rdflib.namespace import Namespace, RDF, RDFS
from rdflib import plugin
from rdflib.term import URIRef
from rdflib.store import Store
from cStringIO import StringIO
from rdflib.graph import Graph
from rdflib.graph import ReadOnlyGraphAggregate
from rdflib.graph import ConjunctiveGraph

testGraph1N3="""
@prefix rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix : <http://test/> .
:foo a rdfs:Class.
:bar :d :c.
:a :d :c.
"""


testGraph2N3="""
@prefix rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix : <http://test/> .
@prefix log: <http://www.w3.org/2000/10/swap/log#>.
:foo a rdfs:Resource.
:bar rdfs:isDefinedBy [ a log:Formula ].
:a :d :e.
"""

testGraph3N3="""
@prefix rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix log: <http://www.w3.org/2000/10/swap/log#>.
@prefix : <http://test/> .
<> a log:N3Document.
"""

sparqlQ = \
"""
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
SELECT *
FROM NAMED <http://example.com/graph1>
FROM NAMED <http://example.com/graph2>
FROM NAMED <http://example.com/graph3>
FROM <http://www.w3.org/2000/01/rdf-schema#>

WHERE {?sub ?pred rdfs:Class }"""

sparqlQ2 =\
"""
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
SELECT ?class
WHERE { GRAPH ?graph { ?member a ?class } }"""

sparqlQ3 =\
"""
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> 
PREFIX log: <http://www.w3.org/2000/10/swap/log#>
SELECT ?n3Doc
WHERE {?n3Doc a log:N3Document }"""

class GraphAggregates1(unittest.TestCase):
    def setUp(self):
        memStore = plugin.get('IOMemory',Store)()
        self.graph1 = Graph(memStore)
        self.graph2 = Graph(memStore)
        self.graph3 = Graph(memStore)

        for n3Str,graph in [(testGraph1N3,self.graph1),
                            (testGraph2N3,self.graph2),
                            (testGraph3N3,self.graph3)]:
            graph.parse(StringIO(n3Str),format='n3')
    
        self.G = ReadOnlyGraphAggregate([self.graph1,self.graph2,self.graph3])

    def testAggregateRaw(self):
        #Test triples
        assert len(list(self.G.triples((None,RDF.type,None))))                  == 4
        assert len(list(self.G.triples((URIRef("http://test/bar"),None,None)))) == 2
        assert len(list(self.G.triples((None,URIRef("http://test/d"),None))))   == 3
    
        #Test __len__
        assert len(self.G) == 8
        
        #assert context iteration
        for g in self.G.contexts():
            assert isinstance(g,Graph)
    
        #Test __contains__
        assert (URIRef("http://test/foo"),RDF.type,RDFS.Resource) in self.G
    
        barPredicates = [URIRef("http://test/d"),RDFS.isDefinedBy]
        assert len(list(self.G.triples_choices((URIRef("http://test/bar"),barPredicates,None)))) == 2
    
class GraphAggregates2(unittest.TestCase):

    known_issue = True
    sparql = True

    def setUp(self):
        memStore = plugin.get('IOMemory',Store)()
        self.graph1 = Graph(memStore,URIRef("http://example.com/graph1"))
        self.graph2 = Graph(memStore,URIRef("http://example.com/graph2"))
        self.graph3 = Graph(memStore,URIRef("http://example.com/graph3"))
    
        for n3Str,graph in [(testGraph1N3,self.graph1),
                            (testGraph2N3,self.graph2),
                            (testGraph3N3,self.graph3)]:
            graph.parse(StringIO(n3Str),format='n3')
    
        self.graph4 = Graph(memStore,RDFS)
        self.graph4.parse(RDFS.uri)
        self.G = ConjunctiveGraph(memStore)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_bnode_ncname
# -*- coding: utf-8 -*-
import re
import sys
try:
    from hashlib import md5
except ImportError:
    from md5 import md5
from rdflib.py3compat import format_doctest_out
try:
    from uuid import uuid4
except ImportError:
    def uuid4():
        """
        Generates a uuid on behalf of Python 2.4
        """
        import random
        import os
        import time
        import socket
        try: 
            preseed = os.urandom(16)
        except NotImplementedError: 
            preseed = '' 
        # Have doubts about this. random.seed will just hash the string 
        random.seed('%s%s%s' % (preseed, os.getpid(), time.time())) 
        del preseed 
        t = long( time.time() * 1000.0 )
        r = long( random.random()*100000000000000000L )
        try:
            a = socket.gethostbyname( socket.gethostname() )
        except:
            # if we can't get a network address, just imagine one
            a = random.random()*100000000000000000L
        strdata = str(t)+' '+str(r)+' '+str(a)
        data = md5(strdata.encode('ascii')).hexdigest()
        yield data

# Adapted from http://icodesnip.com/snippet/python/simple-universally-unique-id-uuid-or-guid
def bnode_uuid():
    yield uuid4()

@format_doctest_out
def is_ncname(value):
    """
    BNode identifiers must be valid NCNames.

    From the `W3C RDF Syntax doc <http://www.w3.org/TR/REC-rdf-syntax/#section-blank-nodeid-event>`_

    "The value is a function of the value of the ``identifier`` accessor. 
    The string value begins with "_:" and the entire value MUST match 
    the `N-Triples nodeID <http://www.w3.org/TR/2004/REC-rdf-testcases-20040210/#nodeID>`_ production".

    The nodeID production is specified to be a `name <http://www.w3.org/TR/2004/REC-rdf-testcases-20040210/#name>`_

        name    ::= [A-Za-z][A-Za-z0-9]*

    >>> assert is_ncname('') == False
    >>> assert is_ncname('999') == False
    >>> assert is_ncname('x') == True
    >>> assert is_ncname(%(u)s'x') == True
    >>> assert is_ncname(%(u)s'Michle') == True
    
    However, vanilla uuid4s are not necessarily NCNames:

    >>> assert is_ncname('6fa459ea-ee8a-3ca4-894e-db77e160355e') == False

    So this has to be finessed with an appropriate prefix ...

    >>> assert is_ncname("urn:uuid:"+str(uuid4())) == True
    >>> from rdflib import BNode
    >>> assert is_ncname(BNode(_sn_gen=bnode_uuid, _prefix="urn:uuid:")) == True 
    """
    ncnameexp = re.compile('[A-Za-z][A-Za-z0-9]*')
    if ncnameexp.match(value):
        return True
    else:
        return False

if __name__ == '__main__':
    import doctest
    doctest.testmod()

########NEW FILE########
__FILENAME__ = test_comparison
import unittest

from rdflib.namespace import RDF

from rdflib.term import URIRef
from rdflib.term import BNode
from rdflib.term import Literal

from rdflib.graph import Graph

from rdflib.plugins.parsers.rdfxml import CORE_SYNTAX_TERMS


"""
Ah... it's coming back to me...
[6:32p] eikeon: think it's so transitivity holds...
[6:32p] eikeon: if a==b and b==c then a should == c
[6:32p] eikeon: "foo"==Literal("foo")
[6:33p] eikeon: We don't want URIRef("foo")==Literal("foo")
[6:33p] eikeon: But if we have URIRef("foo")=="foo" then it implies it.
[6:33p] chimezie: yes, definately not the other RDFLib 'typed' RDF (and N3) terms
[6:34p] eikeon: Why do you need URIRef("foo")=="foo" ?
[6:34p] chimezie: i'm just wondering if a URI and a string with the same lexical value, are by definition 'different'
[6:35p] eikeon: Think so, actually. Think of trying to serialize some triples.
[6:36p] eikeon: If they are the same you'd serialize them the same, no?
[6:36p] chimezie: I guess I was thinking of a 'string' in a native datatype sense, not in the RDF sense (where they would be distinctly different)
[6:37p] eikeon: We should try and brain dump some of this...
[6:37p] eikeon: it look a fairly long time to work out.
[6:37p] eikeon: But think we finally landed in the right spot.
[6:38p] eikeon: I know many of the backends break if URIRef("foo")==Literal("foo")
[6:39p] eikeon: And if we want "foo"==Literal("foo") --- then we really can't have URIRef("foo") also == "foo"
"""
class IdentifierEquality(unittest.TestCase):

    def setUp(self):
        self.uriref = URIRef("http://example.org/")
        self.bnode = BNode()
        self.literal = Literal("http://example.org/")
        self.python_literal = u"http://example.org/"
        self.python_literal_2 = u"foo"

    def testA(self):
        self.assertEquals(self.uriref==self.literal, False)

    def testB(self):
        self.assertEquals(self.literal==self.uriref, False)

    def testC(self):
        self.assertEquals(self.uriref==self.python_literal, False)

    def testD(self):
        self.assertEquals(self.python_literal==self.uriref, False)

    def testE(self):
        self.assertEquals(self.literal==self.python_literal, False)

    def testE2(self): 
        self.assertTrue(self.literal.eq(self.python_literal), True)

    def testF(self):
        self.assertEquals(self.python_literal==self.literal, False)

    def testG(self):
        self.assertEquals("foo" in CORE_SYNTAX_TERMS, False)

    def testH(self):
        self.assertEquals(URIRef("http://www.w3.org/1999/02/22-rdf-syntax-ns#RDF") in CORE_SYNTAX_TERMS, True)

    def testI(self):
        g = Graph()
        g.add((self.uriref, RDF.value, self.literal))
        g.add((self.uriref, RDF.value, self.uriref))
        self.assertEqual(len(g), 2)


if __name__ == "__main__":
    unittest.main()



########NEW FILE########
__FILENAME__ = test_conjunctive_graph
"""
Tests for ConjunctiveGraph that do not depend on the underlying store
"""

from rdflib import ConjunctiveGraph, Graph
from rdflib.term import Identifier, URIRef, BNode
from rdflib.parser import StringInputSource
from os import path


DATA = u"""
<http://example.org/record/1> a <http://xmlns.com/foaf/0.1/Document> .
"""

PUBLIC_ID = u"http://example.org/record/1"

def test_bnode_publicid(): 

    g = ConjunctiveGraph()
    b = BNode()
    data = '<d:d> <e:e> <f:f> .'
    print ("Parsing %r into %r"%(data, b))
    g.parse(data=data, format='turtle', publicID=b)

    triples = list( g.get_context(b).triples((None,None,None)) )
    if not triples:
        raise Exception("No triples found in graph %r"%b)

    u = URIRef(b)

    triples = list( g.get_context(u).triples((None,None,None)) )
    if triples:
        raise Exception("Bad: Found in graph %r: %r"%(u, triples))


def test_quad_contexts(): 
    g = ConjunctiveGraph()
    a = URIRef('urn:a')
    b = URIRef('urn:b')
    g.get_context(a).add((a,a,a))
    g.addN([(b,b,b,b)])

    assert set(g) == set([(a,a,a), (b,b,b)])
    for q in g.quads(): 
        assert isinstance(q[3], Graph)

def test_graph_ids():
    def check(kws):
        cg = ConjunctiveGraph()
        cg.parse(**kws)

        for g in cg.contexts():
            gid = g.identifier
            assert isinstance(gid, Identifier)

    yield check, dict(data=DATA, publicID=PUBLIC_ID, format="turtle")

    source = StringInputSource(DATA.encode('utf8'))
    source.setPublicId(PUBLIC_ID)
    yield check, dict(source=source, format='turtle')


if __name__ == '__main__':
    import nose
    nose.main(defaultTest=__name__)

########NEW FILE########
__FILENAME__ = test_conneg
import thread
import time
import BaseHTTPServer

from rdflib import Graph

"""
Test that correct content negoation headers are passed 
by graph.parse
"""


xmltestdoc="""<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
   xmlns="http://example.org/"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
>
  <rdf:Description rdf:about="http://example.org/a">
    <b rdf:resource="http://example.org/c"/>
  </rdf:Description>
</rdf:RDF>
"""

n3testdoc="""@prefix : <http://example.org/> .

:a :b :c .
"""

nttestdoc="<http://example.org/a> <http://example.org/b> <http://example.org/c> .\n"


class TestHTTPHandler(BaseHTTPServer.BaseHTTPRequestHandler): 
    def do_GET(self): 

        self.send_response(200, "OK")
        # fun fun fun parsing accept header. 

        acs=self.headers["Accept"].split(",")
        acq=[x.split(";") for x in acs if ";" in x]
        acn=[(x,"q=1") for x in acs if ";" not in x]
        acs=[(x[0], float(x[1].strip()[2:])) for x in acq+acn]
        ac=sorted(acs, key=lambda x: x[1])
        ct=ac[-1]
        
        if "application/rdf+xml" in ct: 
            rct="application/rdf+xml"
            content=xmltestdoc
        elif "text/n3" in ct: 
            rct="text/n3"
            content=n3testdoc
        elif "text/plain" in ct: 
            rct="text/plain"
            content=nttestdoc            

        self.send_header("Content-type",rct)
        self.end_headers()
        self.wfile.write(content.encode('utf-8'))

    def log_message(self, *args): 
        pass

def runHttpServer(server_class=BaseHTTPServer.HTTPServer,
        handler_class=TestHTTPHandler):
    """Start a server than can handle 3 requests :)"""
    server_address = ('localhost', 12345)
    httpd = server_class(server_address, handler_class)
    
    httpd.handle_request()
    httpd.handle_request()
    httpd.handle_request()


def testConNeg(): 
    thread.start_new_thread(runHttpServer, tuple())
    # hang on a second while server starts
    time.sleep(1)
    graph=Graph()
    graph.parse("http://localhost:12345/foo", format="xml")
    graph.parse("http://localhost:12345/foo", format="n3")
    graph.parse("http://localhost:12345/foo", format="nt")


if __name__ == "__main__":

    import sys
    import nose
    if len(sys.argv)==1: 
        nose.main(defaultTest=sys.argv[0])

########NEW FILE########
__FILENAME__ = test_conventions
import unittest
import pkgutil
import os.path

import rdflib

"""
Test module naming conventions

modules should all be lower-case initial
"""

class A(unittest.TestCase):

    def module_names(self, path=None, names=None):

        skip_as_ignorably_private = ['embeddedRDF', 'OpenID', 'DublinCore']

        if path is None:
            path = rdflib.__path__
        if names is None:
            names = set()

            # TODO: handle cases where len(path) is not 1
            assert len(path)==1, "We're assuming the path has exactly one item in it for now"
            path = path[0]

        for importer, name, ispkg in pkgutil.iter_modules([path]):
            if ispkg:
                result = self.module_names(path=os.path.join(path, name), 
                                           names=names)
                names.union(result)
            else:
                if name!=name.lower() and name not in skip_as_ignorably_private:
                    names.add(name)
                #self.assert_(name==name.lower(), "module name '%s' is not lower case" % name)
        return names

    def test_module_names(self):
        names = self.module_names()
        self.assert_(names==set(), "module names '%s' are not lower case" % names)

try:
    getattr(pkgutil, 'iter_modules')
except AttributeError:
    from nose import SkipTest
    raise SkipTest('pkgutil.iter_modules not available in Python 2.4')


if __name__ == "__main__":
    unittest.main()


########NEW FILE########
__FILENAME__ = test_core_sparqlstore
import unittest
try:
    import SPARQLWrapper
except ImportError:
    from nose.exc import SkipTest
    raise SkipTest("SPARQLWrapper not installed")

from rdflib.graph import Graph


class TestSPARQLStoreGraphCore(unittest.TestCase):

    store_name = 'SPARQLStore'
    path = "http://dbpedia.org/sparql"
    storetest = True
    create = False

    def setUp(self):
        self.graph = Graph(store="SPARQLStore")
        self.graph.open(self.path, create=self.create)
        ns = list(self.graph.namespaces())
        assert len(ns) > 0, ns

    def tearDown(self):
        self.graph.close()

    def test(self):
        print("Done")

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_dataset
import sys
import os
import unittest

from tempfile import mkdtemp, mkstemp
import shutil
from rdflib import Graph, Dataset, URIRef, BNode, plugin
from rdflib.graph import DATASET_DEFAULT_GRAPH_ID

from nose.exc import SkipTest


class DatasetTestCase(unittest.TestCase):
    store = 'default'
    slow = True
    tmppath = None

    def setUp(self):
        try:
            self.graph = Dataset(store=self.store)
        except ImportError:
            raise SkipTest(
                "Dependencies for store '%s' not available!" % self.store)
        if self.store == "SQLite":
            _, self.tmppath = mkstemp(
                prefix='test', dir='/tmp', suffix='.sqlite')
        else:
            self.tmppath = mkdtemp()
        self.graph.open(self.tmppath, create=True)
        self.michel = URIRef(u'michel')
        self.tarek = URIRef(u'tarek')
        self.bob = URIRef(u'bob')
        self.likes = URIRef(u'likes')
        self.hates = URIRef(u'hates')
        self.pizza = URIRef(u'pizza')
        self.cheese = URIRef(u'cheese')

        self.c1 = URIRef(u'context-1')
        self.c2 = URIRef(u'context-2')

        # delete the graph for each test!
        self.graph.remove((None, None, None))

    def tearDown(self):
        self.graph.close()
        if os.path.isdir(self.tmppath):
            shutil.rmtree(self.tmppath)
        else:
            os.remove(self.tmppath)


    def testGraphAware(self): 
        if not self.graph.store.graph_aware: return 
        
        g = self.graph
        g1 = g.graph(self.c1)
        
        
        # added graph exists
        self.assertEquals(set(x.identifier for x in self.graph.contexts()), 
                          set([self.c1, DATASET_DEFAULT_GRAPH_ID]))

        # added graph is empty 
        self.assertEquals(len(g1), 0)
        
        g1.add( (self.tarek, self.likes, self.pizza) )

        # added graph still exists
        self.assertEquals(set(x.identifier for x in self.graph.contexts()), 
                          set([self.c1, DATASET_DEFAULT_GRAPH_ID]))

        # added graph contains one triple
        self.assertEquals(len(g1), 1)

        g1.remove( (self.tarek, self.likes, self.pizza) )

        # added graph is empty 
        self.assertEquals(len(g1), 0)

        # graph still exists, although empty
        self.assertEquals(set(x.identifier for x in self.graph.contexts()), 
                          set([self.c1, DATASET_DEFAULT_GRAPH_ID]))

        g.remove_graph(self.c1)
                
        # graph is gone
        self.assertEquals(set(x.identifier for x in self.graph.contexts()), 
                          set([DATASET_DEFAULT_GRAPH_ID]))
        
    def testDefaultGraph(self): 
        
        self.graph.add(( self.tarek, self.likes, self.pizza))
        self.assertEquals(len(self.graph), 1)
        # only default exists
        self.assertEquals(set(x.identifier for x in self.graph.contexts()), 
                          set([DATASET_DEFAULT_GRAPH_ID]))

        # removing default graph removes triples but not actual graph
        self.graph.remove_graph(DATASET_DEFAULT_GRAPH_ID)

        self.assertEquals(len(self.graph), 0)
        # default still exists
        self.assertEquals(set(x.identifier for x in self.graph.contexts()), 
                          set([DATASET_DEFAULT_GRAPH_ID]))

    def testNotUnion(self): 
        g1 = self.graph.graph(self.c1)
        g1.add((self.tarek, self.likes, self.pizza))

        self.assertEqual(list(self.graph.objects(self.tarek, None)), 
                         [])
        self.assertEqual(list(g1.objects(self.tarek, None)), [self.pizza])


# dynamically create classes for each registered Store

pluginname = None
if __name__ == '__main__':
    if len(sys.argv) > 1:
        pluginname = sys.argv[1]

tests = 0
for s in plugin.plugins(pluginname, plugin.Store):
    if s.name in ('default', 'IOMemory', 'Auditable',
                  'Concurrent', 'SPARQLStore', 'SPARQLUpdateStore'):
        continue  # these are tested by default
    if not s.getClass().graph_aware:
        continue

    locals()["t%d" % tests] = type("%sContextTestCase" % s.name, (
        DatasetTestCase,), {"store": s.name})
    tests += 1


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_datetime
import sys
import unittest

from datetime import datetime

from isodate import datetime_isoformat, UTC
from isodate.isostrf import DATE_EXT_COMPLETE, TZ_EXT

from rdflib.term import URIRef
from rdflib.term import Literal
from rdflib.namespace import XSD


class TestRelativeBase(unittest.TestCase):
    def test_equality(self):        
        x = Literal("2008-12-01T18:02:00Z",
                    datatype=URIRef('http://www.w3.org/2001/XMLSchema#dateTime'))
        self.assertEquals(x == x, True)

    def test_microseconds(self):
        import platform
        if platform.system() == 'Java' or (platform.system() != 'Java' and sys.version_info[:2] == (2, 5)):
            from nose import SkipTest
            raise SkipTest('datetime microseconds unsupported in Python2.5 and Jython')
        dt1 = datetime(2009, 6, 15, 23, 37, 6, 522630)
        l = Literal(dt1)

        # datetime with microseconds should be cast as a literal with using
        # XML Schema dateTime as the literal datatype
        self.assertEquals(unicode(l), '2009-06-15T23:37:06.522630')
        self.assertEquals(l.datatype, XSD.dateTime)

        dt2 = l.toPython()
        self.assertEqual(dt2, dt1)

    def test_to_python(self):
        dt = "2008-12-01T18:02:00"
        l = Literal(dt,
                    datatype=URIRef('http://www.w3.org/2001/XMLSchema#dateTime'))

        self.assert_(isinstance(l.toPython(), datetime))
        self.assertEquals(l.toPython().isoformat(), dt)

    def test_timezone_z(self):
        dt = "2008-12-01T18:02:00.522630Z"
        l = Literal(dt,
                    datatype=URIRef('http://www.w3.org/2001/XMLSchema#dateTime'))

        self.assert_(isinstance(l.toPython(), datetime))
        self.assertEquals(datetime_isoformat(l.toPython(),
                                             DATE_EXT_COMPLETE + 'T' + '%H:%M:%S.%f' + TZ_EXT),
                          dt)
        self.assertEquals(l.toPython().isoformat(),
                          "2008-12-01T18:02:00.522630+00:00")

    def test_timezone_offset(self):
        dt = "2010-02-10T12:36:00+03:00"
        l = Literal(dt,
                    datatype=URIRef('http://www.w3.org/2001/XMLSchema#dateTime'))

        self.assert_(isinstance(l.toPython(), datetime))
        self.assertEquals(l.toPython().isoformat(), dt)

    def test_timezone_offset_to_utc(self):
        dt = "2010-02-10T12:36:00+03:00"
        l = Literal(dt,
                    datatype=URIRef('http://www.w3.org/2001/XMLSchema#dateTime'))

        utc_dt = l.toPython().astimezone(UTC)
        self.assertEquals(datetime_isoformat(utc_dt),
                          "2010-02-10T09:36:00Z")

    def test_timezone_offset_millisecond(self):
        dt = "2011-01-16T19:39:18.239743+01:00"
        l = Literal(dt,
                    datatype=URIRef('http://www.w3.org/2001/XMLSchema#dateTime'))

        self.assert_(isinstance(l.toPython(), datetime))
        self.assertEquals(l.toPython().isoformat(), dt)

if __name__ == "__main__":
    unittest.main()


########NEW FILE########
__FILENAME__ = test_dawg
import sys

# Needed to pass
# http://www.w3.org/2009/sparql/docs/tests/data-sparql11/
#           syntax-update-2/manifest#syntax-update-other-01
sys.setrecursionlimit(6000)  # default is 1000


try:
    from collections import Counter
except:

    # cheap Counter impl for py 2.5
    # not a complete implementation - only good enough for the use here!
    from collections import defaultdict
    from operator import itemgetter

    class Counter(defaultdict):
        def __init__(self):
            defaultdict.__init__(self, int)

        def most_common(self, N):
            return [x[0] for x in sorted(self.items(),
                                         key=itemgetter(1),
                                         reverse=True)[:10]]


import datetime
import isodate


from rdflib import (
    Dataset, Graph, URIRef, BNode)
from rdflib.query import Result
from rdflib.compare import isomorphic

from rdflib.plugins import sparql as rdflib_sparql_module
from rdflib.plugins.sparql.algebra import (
    pprintAlgebra, translateQuery, translateUpdate)
from rdflib.plugins.sparql.parser import parseQuery, parseUpdate
from rdflib.plugins.sparql.results.rdfresults import RDFResultParser
from rdflib.plugins.sparql.update import evalUpdate

from rdflib.py3compat import decodeStringEscape, bopen

from nose.tools import nottest, eq_
from nose import SkipTest

from urlparse import urljoin

from StringIO import StringIO

if sys.version_info[0:2] < (2, 7):
    from StringIO import StringIO as BytesIO
    assert BytesIO
else:
    from io import BytesIO

from manifest import nose_tests, MF, UP
from earl import report, add_test

def eq(a,b,msg):
    return eq_(a,b,msg+': (%r!=%r)'%(a,b))

def setFlags():
    import rdflib
    # Several tests rely on lexical form of literals being kept!
    rdflib.NORMALIZE_LITERALS = False

    # we need an explicit default graph
    rdflib_sparql_module.SPARQL_DEFAULT_GRAPH_UNION = False

    # we obviously need this
    rdflib.DAWG_LITERAL_COLLATION = True

def resetFlags():
    import rdflib
    # Several tests rely on lexical form of literals being kept!
    rdflib.NORMALIZE_LITERALS = True

    # we need an explicit default graph
    rdflib_sparql_module.SPARQL_DEFAULT_GRAPH_UNION = True

    # we obviously need this
    rdflib.DAWG_LITERAL_COLLATION = False


DEBUG_FAIL = True
DEBUG_FAIL = False

DEBUG_ERROR = True
DEBUG_ERROR = False

SPARQL10Tests = True
# SPARQL10Tests = False

SPARQL11Tests = True
# SPARQL11Tests=False

RDFLibTests = True

DETAILEDASSERT = True
# DETAILEDASSERT=False



NAME = None

fails = Counter()
errors = Counter()

failed_tests = []
error_tests = []




try:
    skiptests = dict([(URIRef(x.strip().split(
        "\t")[0]), x.strip().split("\t")[1]) for x in open("skiptests.list")])
except IOError:
    skiptests = set()


def _fmt(f):
    if f.endswith(".rdf"):
        return "xml"
    return "turtle"




def bindingsCompatible(a, b):

    """

    Are two binding-sets compatible.

    From the spec: http://www.w3.org/2009/sparql/docs/tests/#queryevaltests

    A SPARQL implementation passes a query evaluation test if the
    graph produced by evaluating the query against the RDF dataset
    (and encoding in the DAWG result set vocabulary, if necessary) is
    equivalent [RDF-CONCEPTS] to the graph named in the result (after
    encoding in the DAWG result set vocabulary, if necessary). Note
    that, solution order only is considered relevant, if the result is
    expressed in the test suite in the DAWG result set vocabulary,
    with explicit rs:index triples; otherwise solution order is
    considered irrelevant for passing. Equivalence can be tested by
    checking that the graphs are isomorphic and have identical IRI and
    literal nodes. Note that testing whether two result sets are
    isomorphic is simpler than full graph isomorphism. Iterating over
    rows in one set, finding a match with the other set, removing this
    pair, then making sure all rows are accounted for, achieves the
    same effect.
    """

    def rowCompatible(x, y):
        m = {}
        y = y.asdict()
        for v1, b1 in x.asdict().iteritems():
            if v1 not in y:
                return False
            if isinstance(b1, BNode):
                if b1 in m:
                    if y[v1] != m[b1]:
                        return False
                else:
                    m[b1] = y[v1]
            else:
                 # if y[v1]!=b1:
                 #    return False
                try:
                    if y[v1].neq(b1):
                        return False
                except TypeError:
                    return False
        return True

    if not a:
        if b:
            return False
        return True

    x = iter(a).next()

    for y in b:
        if rowCompatible(x, y):
            if bindingsCompatible(a - set((x,)), b - set((y,))):
                return True

    return False


def pp_binding(solutions):
    """
    Pretty print a single binding - for less eye-strain when debugging
    """
    return "\n[" + ",\n\t".join("{" + ", ".join("%s:%s" % (
        x[0], x[1].n3()) for x in bindings.items()) + "}"
        for bindings in solutions) + "]\n"


@nottest
def update_test(t):

    # the update-eval tests refer to graphs on http://example.org
    rdflib_sparql_module.SPARQL_LOAD_GRAPHS = False

    uri, name, comment, data, graphdata, query, res, syntax = t

    if uri in skiptests:
        raise SkipTest()

    try:
        g = Dataset()

        if not res:
            if syntax:
                translateUpdate(parseUpdate(bopen(query[7:])))
            else:
                try:
                    translateUpdate(parseUpdate(bopen(query[7:])))
                    raise AssertionError("Query shouldn't have parsed!")
                except:
                    pass  # negative syntax test
            return

        resdata, resgraphdata = res

        # read input graphs
        if data:
            g.default_context.load(data, format=_fmt(data))

        if graphdata:
            for x, l in graphdata:
                g.load(x, publicID=URIRef(l), format=_fmt(x))

        req = translateUpdate(parseUpdate(bopen(query[7:])))
        evalUpdate(g, req)

        # read expected results
        resg = Dataset()
        if resdata:
            resg.default_context.load(resdata, format=_fmt(resdata))

        if resgraphdata:
            for x, l in resgraphdata:
                resg.load(x, publicID=URIRef(l), format=_fmt(x))

        eq(set(x.identifier for x in g.contexts() if x != g.default_context),
           set(x.identifier for x in resg.contexts()
               if x != resg.default_context), 'named graphs in datasets do not match')
        assert isomorphic(g.default_context, resg.default_context), \
            'Default graphs are not isomorphic'

        for x in g.contexts():
            if x == g.default_context:
                continue
            assert isomorphic(x, resg.get_context(x.identifier)), \
                "Graphs with ID %s are not isomorphic" % x.identifier

    except Exception, e:

        if isinstance(e, AssertionError):
            failed_tests.append(uri)
            fails[str(e)] += 1
        else:
            error_tests.append(uri)
            errors[str(e)] += 1

        if DEBUG_ERROR and not isinstance(e, AssertionError) or DEBUG_FAIL:
            print "======================================"
            print uri
            print name
            print comment

            if not res:
                if syntax:
                    print "Positive syntax test"
                else:
                    print "Negative syntax test"

            if data:
                print "----------------- DATA --------------------"
                print ">>>", data
                print bopen(data[7:]).read()
            if graphdata:
                print "----------------- GRAPHDATA --------------------"
                for x, l in graphdata:
                    print ">>>", x, l
                    print bopen(x[7:]).read()

            print "----------------- Request -------------------"
            print ">>>", query
            print bopen(query[7:]).read()

            if res:
                if resdata:
                    print "----------------- RES DATA --------------------"
                    print ">>>", resdata
                    print bopen(resdata[7:]).read()
                if resgraphdata:
                    print "----------------- RES GRAPHDATA -------------------"
                    for x, l in resgraphdata:
                        print ">>>", x, l
                        print bopen(x[7:]).read()

            print "------------- MY RESULT ----------"
            print g.serialize(format='trig')

            try:
                pq = translateUpdate(parseUpdate(bopen(query[7:]).read()))
                print "----------------- Parsed ------------------"
                pprintAlgebra(pq)
                # print pq
            except:
                print "(parser error)"

            print decodeStringEscape(unicode(e))

            import pdb
            pdb.post_mortem(sys.exc_info()[2])
        raise


@nottest  # gets called by generator
def query_test(t):
    uri, name, comment, data, graphdata, query, resfile, syntax = t

    # the query-eval tests refer to graphs to load by resolvable filenames
    rdflib_sparql_module.SPARQL_LOAD_GRAPHS = True

    if uri in skiptests:
        raise SkipTest()

    def skip(reason='(none)'):
        print "Skipping %s from now on." % uri
        f = bopen("skiptests.list", "a")
        f.write("%s\t%s\n" % (uri, reason))
        f.close()

    try:
        g = Dataset()
        if data:
            g.default_context.load(data, format=_fmt(data))

        if graphdata:
            for x in graphdata:
                g.load(x, format=_fmt(x))

        if not resfile:
            # no result - syntax test

            if syntax:
                translateQuery(parseQuery(
                    bopen(query[7:]).read()), base=urljoin(query, '.'))
            else:
                # negative syntax test
                try:
                    translateQuery(parseQuery(
                        bopen(query[7:]).read()), base=urljoin(query, '.'))

                    assert False, 'Query should not have parsed!'
                except:
                    pass  # it's fine - the query should not parse
            return

        # eval test - carry out query
        res2 = g.query(bopen(query[7:]).read(), base=urljoin(query, '.'))

        if resfile.endswith('ttl'):
            resg = Graph()
            resg.load(resfile, format='turtle', publicID=resfile)
            res = RDFResultParser().parse(resg)
        elif resfile.endswith('rdf'):
            resg = Graph()
            resg.load(resfile, publicID=resfile)
            res = RDFResultParser().parse(resg)
        elif resfile.endswith('srj'):
            res = Result.parse(bopen(resfile[7:]), format='json')
        elif resfile.endswith('tsv'):
            res = Result.parse(bopen(resfile[7:]), format='tsv')

        elif resfile.endswith('csv'):
            res = Result.parse(bopen(resfile[7:]), format='csv')

            # CSV is lossy, round-trip our own resultset to
            # lose the same info :)

            # write bytes, read strings...
            s = BytesIO()
            res2.serialize(s, format='csv')
            s.seek(0)
            res2 = Result.parse(s, format='csv')

        else:
            res = Result.parse(bopen(resfile[7:]), format='xml')

        if not DETAILEDASSERT:
            eq(res.type, res2.type, 'Types do not match')
            if res.type == 'SELECT':
                eq(set(res.vars), set(res2.vars), 'Vars do not match')
                comp = bindingsCompatible(
                    set(res),
                    set(res2)
                )
                assert comp, 'Bindings do not match'
            elif res.type == 'ASK':
                eq(res.askAnswer, res2.askAnswer, 'Ask answer does not match')
            elif res.type in ('DESCRIBE', 'CONSTRUCT'):
                assert isomorphic(
                    res.graph, res2.graph), 'graphs are not isomorphic!'
            else:
                raise Exception('Unknown result type: %s' % res.type)
        else:
            eq(res.type, res2.type,
               'Types do not match: %r != %r' % (res.type, res2.type))
            if res.type == 'SELECT':
                eq(set(res.vars),
                   set(res2.vars), 'Vars do not match: %r != %r' % (
                   set(res.vars), set(res2.vars)))
                assert bindingsCompatible(
                    set(res),
                    set(res2)
                ), 'Bindings do not match: \n%s\n!=\n%s' % (
                    res.serialize(format='txt', namespace_manager=g.namespace_manager),
                    res2.serialize(format='txt', namespace_manager=g.namespace_manager))
            elif res.type == 'ASK':
                eq(res.askAnswer,
                   res2.askAnswer, "Ask answer does not match: %r != %r" % (
                   res.askAnswer, res2.askAnswer))
            elif res.type in ('DESCRIBE', 'CONSTRUCT'):
                assert isomorphic(
                    res.graph, res2.graph), 'graphs are not isomorphic!'
            else:
                raise Exception('Unknown result type: %s' % res.type)

    except Exception, e:

        if isinstance(e, AssertionError):
            failed_tests.append(uri)
            fails[str(e)] += 1
        else:
            error_tests.append(uri)
            errors[str(e)] += 1

        if DEBUG_ERROR and not isinstance(e, AssertionError) or DEBUG_FAIL:
            print "======================================"
            print uri
            print name
            print comment

            if not resfile:
                if syntax:
                    print "Positive syntax test"
                else:
                    print "Negative syntax test"

            if data:
                print "----------------- DATA --------------------"
                print ">>>", data
                print bopen(data[7:]).read()
            if graphdata:
                print "----------------- GRAPHDATA --------------------"
                for x in graphdata:
                    print ">>>", x
                    print bopen(x[7:]).read()

            print "----------------- Query -------------------"
            print ">>>", query
            print bopen(query[7:]).read()
            if resfile:
                print "----------------- Res -------------------"
                print ">>>", resfile
                print bopen(resfile[7:]).read()

            try:
                pq = parseQuery(bopen(query[7:]).read())
                print "----------------- Parsed ------------------"
                pprintAlgebra(translateQuery(pq, base=urljoin(query, '.')))
            except:
                print "(parser error)"

            print decodeStringEscape(unicode(e))

            import pdb
            pdb.post_mortem(sys.exc_info()[2])
            # pdb.set_trace()
            # nose.tools.set_trace()
        raise


testers = {
    UP.UpdateEvaluationTest: update_test,
    MF.UpdateEvaluationTest: update_test,
    MF.PositiveUpdateSyntaxTest11: update_test,
    MF.NegativeUpdateSyntaxTest11: update_test,

    MF.QueryEvaluationTest: query_test,
    MF.NegativeSyntaxTest11: query_test,
    MF.PositiveSyntaxTest11: query_test,
    MF.CSVResultFormatTest: query_test,
}


def test_dawg():

    setFlags()

    if SPARQL10Tests:
        for t in nose_tests(testers, "test/DAWG/data-r2/manifest-evaluation.ttl"):
            yield t

    if SPARQL11Tests:
        for t in nose_tests(testers, "test/DAWG/data-sparql11/manifest-all.ttl"):
            yield t

    if RDFLibTests:
        for t in nose_tests(testers, "test/DAWG/rdflib/manifest.ttl"):
            yield t


    resetFlags()



if __name__ == '__main__':

    import sys
    import time
    start = time.time()
    if len(sys.argv) > 1:
        NAME = sys.argv[1]
        DEBUG_FAIL = True
    i = 0
    success = 0

    skip = 0

    for _type, t in test_dawg():


        if NAME and not str(t[0]).startswith(NAME):
            continue
        i += 1
        try:

            _type(t)

            add_test(t[0], "passed")
            success += 1

        except SkipTest, e:
            msg = skiptests.get(t[0], e.args)
            add_test(t[0], "untested", msg)
            print "skipping %s - %s" % (t[0], msg)
            skip += 1

        except KeyboardInterrupt:
            raise
        except AssertionError:
            add_test(t[0], "failed")
        except:
            add_test(t[0], "failed", "error")
            import traceback
            traceback.print_exc()
            sys.stderr.write("%s\n" % t[0])

    print "\n----------------------------------------------------\n"
    print "Failed tests:"
    for f in failed_tests:
        print f

    print "\n----------------------------------------------------\n"
    print "Error tests:"
    for f in error_tests:
        print f

    print "\n----------------------------------------------------\n"

    print "Most common fails:"
    for e in fails.most_common(10):
        e = str(e)
        print e[:450] + (e[450:] and "...")

    print "\n----------------------------------------------------\n"

    if errors:

        print "Most common errors:"
        for e in errors.most_common(10):
            print e
    else:
        print "(no errors!)"

    f = sum(fails.values())
    e = sum(errors.values())

    if success + f + e + skip != i:
        print "(Something is wrong, %d!=%d)" % (success + f + e + skip, i)

    print "\n%d tests, %d passed, %d failed, %d errors, \
          %d skipped (%.2f%% success)" % (
        i, success, f, e, skip, 100. * success / i)
    print "Took %.2fs" % (time.time() - start)

    if not NAME:

        now = isodate.datetime_isoformat(datetime.datetime.utcnow())

        tf = open("testruns.txt", "a")
        tf.write("%s\n%d tests, %d passed, %d failed, %d errors, %d \
                 skipped (%.2f%% success)\n\n" % (
            now, i, success, f, e, skip, 100. * success / i))
        tf.close()

        earl_report = 'test_reports/rdflib_sparql-%s.ttl' % now

        report.serialize(earl_report, format='n3')
        report.serialize('test_reports/rdflib_sparql-latest.ttl', format='n3')
        print "Wrote EARL-report to '%s'" % earl_report

########NEW FILE########
__FILENAME__ = test_diff
import unittest
import rdflib
from rdflib.compare import graph_diff

"""Test for graph_diff - much more extensive testing 
would certainly be possible"""

class TestDiff(unittest.TestCase):
    """Unicode literals for graph_diff test
    (issue 151)"""

    def testA(self): 
        """with bnode"""
        g=rdflib.Graph()
        g.add( (rdflib.BNode(), rdflib.URIRef("urn:p"), rdflib.Literal(u'\xe9') ) ) 

        diff=graph_diff(g,g)

    def testB(self): 

        """Curiously, this one passes, even before the fix in issue 151"""

        g=rdflib.Graph()
        g.add( (rdflib.URIRef("urn:a"), rdflib.URIRef("urn:p"), rdflib.Literal(u'\xe9') ) ) 

        diff=graph_diff(g,g)


if __name__ == "__main__":
    unittest.main()


########NEW FILE########
__FILENAME__ = test_empty_xml_base
"""
Test for empty xml:base values

xml:base='' should resolve to the given publicID per XML Base specification 
and RDF/XML dependence on it
"""

from rdflib.graph import ConjunctiveGraph
from rdflib.term import URIRef
from rdflib.namespace import Namespace
from rdflib.namespace import RDF
from StringIO import StringIO
import unittest

FOAF = Namespace('http://xmlns.com/foaf/0.1/')

test_data = """
<rdf:RDF 
    xmlns:foaf="http://xmlns.com/foaf/0.1/"
    xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
    xml:base="">
    <rdf:Description rdf:about="">
      <rdf:type rdf:resource="http://xmlns.com/foaf/0.1/Document"/>
    </rdf:Description>
</rdf:RDF>"""

test_data2 = """
<rdf:RDF 
    xmlns:foaf="http://xmlns.com/foaf/0.1/"
    xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
    xml:base="../">
    <rdf:Description rdf:about="baz">
      <rdf:type rdf:resource="http://xmlns.com/foaf/0.1/Document"/>
    </rdf:Description>
</rdf:RDF>"""


baseUri  = URIRef('http://example.com/')
baseUri2 = URIRef('http://example.com/foo/bar')

class TestEmptyBase(unittest.TestCase):
    def setUp(self):
        self.graph = ConjunctiveGraph()
        self.graph.parse(StringIO(test_data),publicID=baseUri)

    def test_base_ref(self):        
        self.failUnless(len(self.graph) == 1,"There should be at least one statement in the graph")
        self.failUnless((baseUri,RDF.type,FOAF.Document) in self.graph,"There should be a triple with %s as the subject" % baseUri)

class TestRelativeBase(unittest.TestCase):
    def setUp(self):
        self.graph = ConjunctiveGraph()
        self.graph.parse(StringIO(test_data2),publicID=baseUri2)

    def test_base_ref(self):        
        self.failUnless(len(self.graph) == 1,"There should be at least one statement in the graph")
        resolvedBase = URIRef('http://example.com/baz')
        self.failUnless((resolvedBase,RDF.type,FOAF.Document) in self.graph,"There should be a triple with %s as the subject" % resolvedBase)

if __name__ == "__main__":
    unittest.main()


########NEW FILE########
__FILENAME__ = test_evaluate_bind
"""
Verify evaluation of BIND expressions of different types. See
<http://www.w3.org/TR/sparql11-query/#rExpression>.
"""
from rdflib import Graph, URIRef, Literal, Variable


def test_bind():
    base = "http://example.org/"
    g = Graph()
    g.add((URIRef(
        base + "thing"), URIRef(base + "ns#comment"), Literal("anything")))

    def check(expr, var, obj):
        r = g.query("""
                prefix : <http://example.org/ns#>
                select * where { ?s ?p ?o . %s } """ % expr)
        assert r.bindings[0][Variable(var)] == obj

    yield (check, 'bind("thing" as ?name)', 'name', Literal("thing"))

    yield (check, 'bind(<http://example.org/other> as ?other)', 'other',
           URIRef("http://example.org/other"))

    yield (check, "bind(:Thing as ?type)", 'type',
           URIRef("http://example.org/ns#Thing"))

########NEW FILE########
__FILENAME__ = test_events

import unittest
from rdflib import events

class AddedEvent(events.Event): pass

class RemovedEvent(events.Event): pass

def subscribe_to(source, target):
    target.subscribe(AddedEvent, source._add_handler)
    target.subscribe(RemovedEvent, source._remove_handler)

def subscribe_all(caches):
    for cache in caches:
        for other in caches:
            if other != cache:
                subscribe_to(cache, other)

class Cache(events.Dispatcher):

    def __init__(self, data=None):
        if data is None: data = {}
        self._data = data
        self.subscribe(AddedEvent, self._add_handler)
        self.subscribe(RemovedEvent, self._remove_handler)        

    def _add_handler(self, event):
        self._data[event.key] = event.value

    def _remove_handler(self, event):
        del self._data[event.key]

    def __getitem__(self, key):
        return self._data[key]

    def __setitem__(self, key, value):
        self.dispatch(AddedEvent(key=key, value=value))
        
    def __delitem__(self, key):
        self.dispatch(RemovedEvent(key=key))

    def __contains__(self, key):
        return key in self._data
    
    has_key = __contains__


class EventTestCase(unittest.TestCase):

    def testEvents(self):
        c1 = Cache()
        c2 = Cache()
        c3 = Cache()
        subscribe_all([c1,c2,c3])
        c1['bob'] = 'uncle'
        assert c2['bob'] == 'uncle'
        assert c3['bob'] == 'uncle'
        del c3['bob']
        assert c1.has_key('bob') == False
        assert c2.has_key('bob') == False

if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_expressions
from functools import partial

import rdflib.plugins.sparql.parser as p
from rdflib.plugins.sparql.sparql import QueryContext, SPARQLError, Prologue
from rdflib.plugins.sparql.algebra import traverse, translatePName
from rdflib.plugins.sparql.operators import simplify

from rdflib import Variable, Literal

from nose.tools import eq_ as eq


def _eval(e, ctx=None):
    if not ctx:
        ctx = QueryContext()
    try:
        r = e.eval(ctx)
        if isinstance(r, SPARQLError):
            print r
            return False
        return r
    except SPARQLError:
        return False


def _translate(e):
    return simplify(traverse(
        e, visitPost=partial(translatePName, prologue=Prologue())))


def testRegex():

    assert _eval(
        _translate((p.Expression.parseString('REGEX("zxcabczxc","abc")')[0])))

    eq(bool(_eval(_translate(
        (p.Expression.parseString('REGEX("zxczxc","abc")')[0])))), False)

    assert _eval(_translate(
        (p.Expression.parseString('REGEX("bbbaaaaabbb","ba*b")')[0])))


def test_arithmetic():

    eq(_eval(_translate((p.Expression.parseString('2+3')[0]))).value, 5)
    eq(_eval(_translate((p.Expression.parseString('3-2')[0]))).value, 1)

    eq(_eval(_translate((p.Expression.parseString('2*3')[0]))).value, 6)
    eq(_eval(_translate((p.Expression.parseString('4/2')[0]))).value, 2)

    eq(_eval(_translate((p.Expression.parseString('2+2+2')[0]))).value, 6)
    eq(_eval(_translate((p.Expression.parseString('2-2+2')[0]))).value, 2)
    eq(_eval(_translate((p.Expression.parseString('(2-2)+2')[0]))).value, 2)
    eq(_eval(_translate((p.Expression.parseString('2-(2+2)')[0]))).value, -2)

    eq(_eval(_translate((p.Expression.parseString('2*2*2')[0]))).value, 8)
    eq(_eval(_translate((p.Expression.parseString('4/2*2')[0]))).value, 4)
    eq(_eval(_translate((p.Expression.parseString('8/4*2')[0]))).value, 4)
    eq(_eval(_translate((p.Expression.parseString('8/(4*2)')[0]))).value, 1)
    eq(_eval(_translate((p.Expression.parseString('(2/2)*2')[0]))).value, 2)
    eq(_eval(_translate((p.Expression.parseString('4/(2*2)')[0]))).value, 1)

    eq(_eval(_translate((p.Expression.parseString('2+3*2')[0]))).value, 8)
    eq(_eval(_translate((p.Expression.parseString('(2+3)*2')[0]))).value, 10)
    eq(_eval(_translate((p.Expression.parseString('2+4/2')[0]))).value, 4)
    eq(_eval(_translate((p.Expression.parseString('(2+4)/2')[0]))).value, 3)


def test_arithmetic_var():
    ctx = QueryContext()
    ctx[Variable('x')] = Literal(2)

    eq(_eval(_translate((p.Expression.parseString('2+?x')[0])), ctx).value, 4)

    eq(_eval(_translate((p.Expression.parseString('?x+3')[0])), ctx).value, 5)
    eq(_eval(_translate((p.Expression.parseString('3-?x')[0])), ctx).value, 1)

    eq(_eval(_translate((p.Expression.parseString('?x*3')[0])), ctx).value, 6)
    eq(_eval(_translate((p.Expression.parseString('4/?x')[0])), ctx).value, 2)

    eq(_eval(_translate((p.Expression.parseString('?x+?x+?x')[0])), ctx).value, 6)
    eq(_eval(_translate((p.Expression.parseString('?x-?x+?x')[0])), ctx).value, 2)
    eq(_eval(_translate((p.Expression.parseString('(?x-?x)+?x')[0])), ctx).value, 2)
    eq(_eval(_translate((p.Expression.parseString('?x-(?x+?x)')[0])), ctx).value, -2)

    eq(_eval(_translate((p.Expression.parseString('?x*?x*?x')[0])), ctx).value, 8)
    eq(_eval(_translate((p.Expression.parseString('4/?x*?x')[0])), ctx).value, 4)
    eq(_eval(_translate((p.Expression.parseString('8/4*?x')[0])), ctx).value, 4)
    eq(_eval(_translate((p.Expression.parseString('8/(4*?x)')[0])), ctx).value, 1)
    eq(_eval(_translate((p.Expression.parseString('(?x/?x)*?x')[0])), ctx).value, 2)
    eq(_eval(_translate((p.Expression.parseString('4/(?x*?x)')[0])), ctx).value, 1)


def test_comparisons():

    eq(bool(_eval(_translate((p.Expression.parseString('2<3')[0])))), True)
    eq(bool(_eval(_translate((p.Expression.parseString('2<3.0')[0])))), True)
    eq(bool(_eval(_translate((p.Expression.parseString('2<3e0')[0])))), True)

    eq(bool(_eval(_translate((p.Expression.parseString('4<3')[0])))), False)
    eq(bool(_eval(_translate((p.Expression.parseString('4<3.0')[0])))), False)
    eq(bool(_eval(_translate((p.Expression.parseString('4<3e0')[0])))), False)

    eq(bool(_eval(_translate((p.Expression.parseString('2<2.1')[0])))), True)
    eq(bool(_eval(_translate((p.Expression.parseString('2<21e-1')[0])))), True)

    eq(bool(_eval(_translate((p.Expression.parseString('2=2.0')[0])))), True)
    eq(bool(_eval(_translate((p.Expression.parseString('2=2e0')[0])))), True)

    eq(bool(_eval(_translate((p.Expression.parseString('2="cake"')[0])))), False)


def test_comparisons_var():

    ctx = QueryContext()
    ctx[Variable('x')] = Literal(2)

    eq(bool(_eval(_translate((p.Expression.parseString('?x<3')[0])), ctx)), True)
    eq(bool(_eval(_translate((p.Expression.parseString('?x<3.0')[0])), ctx)), True)
    eq(bool(_eval(_translate((p.Expression.parseString('?x<3e0')[0])), ctx)), True)

    eq(bool(_eval(_translate((p.Expression.parseString('?x<2.1')[0])), ctx)), True)
    eq(bool(_eval(_translate((p.Expression.parseString('?x<21e-1')[0])), ctx)), True)

    eq(bool(_eval(_translate((p.Expression.parseString('?x=2.0')[0])), ctx)), True)
    eq(bool(_eval(_translate((p.Expression.parseString('?x=2e0')[0])), ctx)), True)

    eq(bool(_eval(_translate((p.Expression.parseString('?x="cake"')[0])), ctx)), False)

    ctx = QueryContext()
    ctx[Variable('x')] = Literal(4)

    eq(bool(_eval(_translate((p.Expression.parseString('?x<3')[0])), ctx)), False)
    eq(bool(_eval(_translate((p.Expression.parseString('?x<3.0')[0])), ctx)), False)
    eq(bool(_eval(_translate((p.Expression.parseString('?x<3e0')[0])), ctx)), False)


def test_and_or():
    eq(bool(_eval(_translate((p.Expression.parseString('3>2 && 3>1')[0])))), True)
    eq(bool(_eval(
        _translate((p.Expression.parseString('3>2 && 3>4 || 2>1')[0])))), True)
    eq(bool(_eval(
        _translate((p.Expression.parseString('2>1 || 3>2 && 3>4')[0])))), True)
    eq(bool(_eval(_translate(
        (p.Expression.parseString('(2>1 || 3>2) && 3>4')[0])))), False)


if __name__ == '__main__':
    import nose
    import sys
    nose.main(defaultTest=sys.argv[0])

########NEW FILE########
__FILENAME__ = test_finalnewline

from rdflib import ConjunctiveGraph, URIRef
import rdflib.plugin

from rdflib.py3compat import b

def testFinalNewline():
    """
    http://code.google.com/p/rdflib/issues/detail?id=5
    """
    import sys
    import platform
    if getattr(sys, 'pypy_version_info', None) or platform.system() == 'Java':
        from nose import SkipTest
        raise SkipTest(
            'Testing under pypy and Jython2.5 fails to detect that ' + \
            'IOMemory is a context_aware store')

    graph=ConjunctiveGraph()
    graph.add((URIRef("http://ex.org/a"),
               URIRef("http://ex.org/b"),
               URIRef("http://ex.org/c")))

    failed = set()
    for p in rdflib.plugin.plugins(None, rdflib.plugin.Serializer):
        v = graph.serialize(format=p.name)
        lines = v.split(b("\n"))
        if b("\n") not in v or (lines[-1]!=b('')):
            failed.add(p.name)
    assert len(failed)==0, "No final newline for formats: '%s'" % failed

if __name__ == "__main__":

    import sys
    import nose
    if len(sys.argv)==1: 
        nose.main(defaultTest=sys.argv[0])

########NEW FILE########
__FILENAME__ = test_graph
import sys
import os
import unittest

from tempfile import mkdtemp, mkstemp
import shutil

from rdflib import URIRef, RDF, Graph, plugin

from nose.exc import SkipTest


class GraphTestCase(unittest.TestCase):
    store = 'default'
    tmppath = None

    def setUp(self):
        try:
            self.graph = Graph(store=self.store)
        except ImportError:
            raise SkipTest(
                "Dependencies for store '%s' not available!" % self.store)
        if self.store == "SQLite":
            _, self.tmppath = mkstemp(
                prefix='test', dir='/tmp', suffix='.sqlite')
        else:
            self.tmppath = mkdtemp()
        self.graph.open(self.tmppath, create=True)

        self.michel = URIRef(u'michel')
        self.tarek = URIRef(u'tarek')
        self.bob = URIRef(u'bob')
        self.likes = URIRef(u'likes')
        self.hates = URIRef(u'hates')
        self.pizza = URIRef(u'pizza')
        self.cheese = URIRef(u'cheese')

    def tearDown(self):
        self.graph.close()
        if os.path.isdir(self.tmppath):
            shutil.rmtree(self.tmppath)
        else:
            os.remove(self.tmppath)

    def addStuff(self):
        tarek = self.tarek
        michel = self.michel
        bob = self.bob
        likes = self.likes
        hates = self.hates
        pizza = self.pizza
        cheese = self.cheese

        self.graph.add((tarek, likes, pizza))
        self.graph.add((tarek, likes, cheese))
        self.graph.add((michel, likes, pizza))
        self.graph.add((michel, likes, cheese))
        self.graph.add((bob, likes, cheese))
        self.graph.add((bob, hates, pizza))
        self.graph.add((bob, hates, michel))  # gasp!

    def removeStuff(self):
        tarek = self.tarek
        michel = self.michel
        bob = self.bob
        likes = self.likes
        hates = self.hates
        pizza = self.pizza
        cheese = self.cheese

        self.graph.remove((tarek, likes, pizza))
        self.graph.remove((tarek, likes, cheese))
        self.graph.remove((michel, likes, pizza))
        self.graph.remove((michel, likes, cheese))
        self.graph.remove((bob, likes, cheese))
        self.graph.remove((bob, hates, pizza))
        self.graph.remove((bob, hates, michel))  # gasp!

    def testAdd(self):
        self.addStuff()

    def testRemove(self):
        self.addStuff()
        self.removeStuff()

    def testTriples(self):
        tarek = self.tarek
        michel = self.michel
        bob = self.bob
        likes = self.likes
        hates = self.hates
        pizza = self.pizza
        cheese = self.cheese
        asserte = self.assertEquals
        triples = self.graph.triples
        Any = None

        self.addStuff()

        # unbound subjects
        asserte(len(list(triples((Any, likes, pizza)))), 2)
        asserte(len(list(triples((Any, hates, pizza)))), 1)
        asserte(len(list(triples((Any, likes, cheese)))), 3)
        asserte(len(list(triples((Any, hates, cheese)))), 0)

        # unbound objects
        asserte(len(list(triples((michel, likes, Any)))), 2)
        asserte(len(list(triples((tarek, likes, Any)))), 2)
        asserte(len(list(triples((bob, hates, Any)))), 2)
        asserte(len(list(triples((bob, likes, Any)))), 1)

        # unbound predicates
        asserte(len(list(triples((michel, Any, cheese)))), 1)
        asserte(len(list(triples((tarek, Any, cheese)))), 1)
        asserte(len(list(triples((bob, Any, pizza)))), 1)
        asserte(len(list(triples((bob, Any, michel)))), 1)

        # unbound subject, objects
        asserte(len(list(triples((Any, hates, Any)))), 2)
        asserte(len(list(triples((Any, likes, Any)))), 5)

        # unbound predicates, objects
        asserte(len(list(triples((michel, Any, Any)))), 2)
        asserte(len(list(triples((bob, Any, Any)))), 3)
        asserte(len(list(triples((tarek, Any, Any)))), 2)

        # unbound subjects, predicates
        asserte(len(list(triples((Any, Any, pizza)))), 3)
        asserte(len(list(triples((Any, Any, cheese)))), 3)
        asserte(len(list(triples((Any, Any, michel)))), 1)

        # all unbound
        asserte(len(list(triples((Any, Any, Any)))), 7)
        self.removeStuff()
        asserte(len(list(triples((Any, Any, Any)))), 0)

    def testConnected(self):
        graph = self.graph
        self.addStuff()
        self.assertEquals(True, graph.connected())

        jeroen = URIRef("jeroen")
        unconnected = URIRef("unconnected")

        graph.add((jeroen, self.likes, unconnected))

        self.assertEquals(False, graph.connected())

    def testSub(self):
        g1 = self.graph
        g2 = Graph(store=g1.store)

        tarek = self.tarek
        # michel = self.michel
        bob = self.bob
        likes = self.likes
        # hates = self.hates
        pizza = self.pizza
        cheese = self.cheese

        g1.add((tarek, likes, pizza))
        g1.add((bob, likes, cheese))

        g2.add((bob, likes, cheese))

        g3 = g1 - g2

        self.assertEquals(len(g3), 1)
        self.assertEquals((tarek, likes, pizza) in g3, True)
        self.assertEquals((tarek, likes, cheese) in g3, False)

        self.assertEquals((bob, likes, cheese) in g3, False)

        g1 -= g2

        self.assertEquals(len(g1), 1)
        self.assertEquals((tarek, likes, pizza) in g1, True)
        self.assertEquals((tarek, likes, cheese) in g1, False)

        self.assertEquals((bob, likes, cheese) in g1, False)

    def testGraphAdd(self):
        g1 = self.graph
        g2 = Graph(store=g1.store)

        tarek = self.tarek
        # michel = self.michel
        bob = self.bob
        likes = self.likes
        # hates = self.hates
        pizza = self.pizza
        cheese = self.cheese

        g1.add((tarek, likes, pizza))

        g2.add((bob, likes, cheese))

        g3 = g1 + g2

        self.assertEquals(len(g3), 2)
        self.assertEquals((tarek, likes, pizza) in g3, True)
        self.assertEquals((tarek, likes, cheese) in g3, False)

        self.assertEquals((bob, likes, cheese) in g3, True)

        g1 += g2

        self.assertEquals(len(g1), 2)
        self.assertEquals((tarek, likes, pizza) in g1, True)
        self.assertEquals((tarek, likes, cheese) in g1, False)

        self.assertEquals((bob, likes, cheese) in g1, True)

    def testGraphIntersection(self):
        g1 = self.graph
        g2 = Graph(store=g1.store)

        tarek = self.tarek
        michel = self.michel
        bob = self.bob
        likes = self.likes
        # hates = self.hates
        pizza = self.pizza
        cheese = self.cheese

        g1.add((tarek, likes, pizza))
        g1.add((michel, likes, cheese))

        g2.add((bob, likes, cheese))
        g2.add((michel, likes, cheese))

        g3 = g1 * g2

        self.assertEquals(len(g3), 1)
        self.assertEquals((tarek, likes, pizza) in g3, False)
        self.assertEquals((tarek, likes, cheese) in g3, False)

        self.assertEquals((bob, likes, cheese) in g3, False)

        self.assertEquals((michel, likes, cheese) in g3, True)

        g1 *= g2

        self.assertEquals(len(g1), 1)

        self.assertEquals((tarek, likes, pizza) in g1, False)
        self.assertEquals((tarek, likes, cheese) in g1, False)

        self.assertEquals((bob, likes, cheese) in g1, False)

        self.assertEquals((michel, likes, cheese) in g1, True)


# dynamically create classes for each registered Store

pluginname = None
if __name__ == '__main__':
    if len(sys.argv) > 1:
        pluginname = sys.argv[1]

tests = 0
for s in plugin.plugins(pluginname, plugin.Store):
    if s.name in ('default', 'IOMemory', 'Auditable',
                  'Concurrent', 'SPARQLStore',
                  'SPARQLUpdateStore'):
        continue  # these are tested by default

    locals()["t%d" % tests] = type("%sGraphTestCase" %
                                   s.name, (GraphTestCase,), {"store": s.name})
    tests += 1


if __name__ == '__main__':
    unittest.main(argv=sys.argv[:1])

########NEW FILE########
__FILENAME__ = test_graph_context
import sys
import os
import unittest

from tempfile import mkdtemp, mkstemp
import shutil
from rdflib import Graph, ConjunctiveGraph, URIRef, BNode, plugin

from nose.exc import SkipTest


class ContextTestCase(unittest.TestCase):
    store = 'default'
    slow = True
    tmppath = None

    def setUp(self):
        try:
            self.graph = ConjunctiveGraph(store=self.store)
        except ImportError:
            raise SkipTest(
                "Dependencies for store '%s' not available!" % self.store)
        if self.store == "SQLite":
            _, self.tmppath = mkstemp(
                prefix='test', dir='/tmp', suffix='.sqlite')
        else:
            self.tmppath = mkdtemp()
        self.graph.open(self.tmppath, create=True)
        self.michel = URIRef(u'michel')
        self.tarek = URIRef(u'tarek')
        self.bob = URIRef(u'bob')
        self.likes = URIRef(u'likes')
        self.hates = URIRef(u'hates')
        self.pizza = URIRef(u'pizza')
        self.cheese = URIRef(u'cheese')

        self.c1 = URIRef(u'context-1')
        self.c2 = URIRef(u'context-2')

        # delete the graph for each test!
        self.graph.remove((None, None, None))

    def tearDown(self):
        self.graph.close()
        if os.path.isdir(self.tmppath):
            shutil.rmtree(self.tmppath)
        else:
            os.remove(self.tmppath)

    def addStuff(self):
        tarek = self.tarek
        michel = self.michel
        bob = self.bob
        likes = self.likes
        hates = self.hates
        pizza = self.pizza
        cheese = self.cheese
        c1 = self.c1
        graph = Graph(self.graph.store, c1)

        graph.add((tarek, likes, pizza))
        graph.add((tarek, likes, cheese))
        graph.add((michel, likes, pizza))
        graph.add((michel, likes, cheese))
        graph.add((bob, likes, cheese))
        graph.add((bob, hates, pizza))
        graph.add((bob, hates, michel))  # gasp!

    def removeStuff(self):
        tarek = self.tarek
        michel = self.michel
        bob = self.bob
        likes = self.likes
        hates = self.hates
        pizza = self.pizza
        cheese = self.cheese
        c1 = self.c1
        graph = Graph(self.graph.store, c1)

        graph.remove((tarek, likes, pizza))
        graph.remove((tarek, likes, cheese))
        graph.remove((michel, likes, pizza))
        graph.remove((michel, likes, cheese))
        graph.remove((bob, likes, cheese))
        graph.remove((bob, hates, pizza))
        graph.remove((bob, hates, michel))  # gasp!

    def addStuffInMultipleContexts(self):
        c1 = self.c1
        c2 = self.c2
        triple = (self.pizza, self.hates, self.tarek)  # revenge!

        # add to default context
        self.graph.add(triple)
        # add to context 1
        graph = Graph(self.graph.store, c1)
        graph.add(triple)
        # add to context 2
        graph = Graph(self.graph.store, c2)
        graph.add(triple)

    def testConjunction(self):
        if self.store == "SQLite":
            raise SkipTest("Skipping known issue with __len__")
        self.addStuffInMultipleContexts()
        triple = (self.pizza, self.likes, self.pizza)
        # add to context 1
        graph = Graph(self.graph.store, self.c1)
        graph.add(triple)
        self.assertEquals(len(self.graph), len(graph))

    def testAdd(self):
        self.addStuff()

    def testRemove(self):
        self.addStuff()
        self.removeStuff()

    def testLenInOneContext(self):
        c1 = self.c1
        # make sure context is empty

        self.graph.remove_context(self.graph.get_context(c1))
        graph = Graph(self.graph.store, c1)
        oldLen = len(self.graph)

        for i in range(0, 10):
            graph.add((BNode(), self.hates, self.hates))
        self.assertEquals(len(graph), oldLen + 10)
        self.assertEquals(len(self.graph.get_context(c1)), oldLen + 10)
        self.graph.remove_context(self.graph.get_context(c1))
        self.assertEquals(len(self.graph), oldLen)
        self.assertEquals(len(graph), 0)

    def testLenInMultipleContexts(self):
        if self.store == "SQLite":
            raise SkipTest("Skipping known issue with __len__")
        oldLen = len(self.graph)
        self.addStuffInMultipleContexts()

        # addStuffInMultipleContexts is adding the same triple to
        # three different contexts. So it's only + 1
        self.assertEquals(len(self.graph), oldLen + 1)

        graph = Graph(self.graph.store, self.c1)
        self.assertEquals(len(graph), oldLen + 1)

    def testRemoveInMultipleContexts(self):
        c1 = self.c1
        c2 = self.c2
        triple = (self.pizza, self.hates, self.tarek)  # revenge!

        self.addStuffInMultipleContexts()

        # triple should be still in store after removing it from c1 + c2
        self.assert_(triple in self.graph)
        graph = Graph(self.graph.store, c1)
        graph.remove(triple)
        self.assert_(triple in self.graph)
        graph = Graph(self.graph.store, c2)
        graph.remove(triple)
        self.assert_(triple in self.graph)
        self.graph.remove(triple)
        # now gone!
        self.assert_(triple not in self.graph)

        # add again and see if remove without context removes all triples!
        self.addStuffInMultipleContexts()
        self.graph.remove(triple)
        self.assert_(triple not in self.graph)

    def testContexts(self):
        triple = (self.pizza, self.hates, self.tarek)  # revenge!

        self.addStuffInMultipleContexts()

        def cid(c):
            return c.identifier
        self.assert_(self.c1 in map(cid, self.graph.contexts()))
        self.assert_(self.c2 in map(cid, self.graph.contexts()))

        contextList = map(cid, list(self.graph.contexts(triple)))
        self.assert_(self.c1 in contextList)
        self.assert_(self.c2 in contextList)

    def testRemoveContext(self):
        c1 = self.c1

        self.addStuffInMultipleContexts()
        self.assertEquals(len(Graph(self.graph.store, c1)), 1)
        self.assertEquals(len(self.graph.get_context(c1)), 1)

        self.graph.remove_context(self.graph.get_context(c1))
        self.assert_(self.c1 not in self.graph.contexts())

    def testRemoveAny(self):
        Any = None
        self.addStuffInMultipleContexts()
        self.graph.remove((Any, Any, Any))
        self.assertEquals(len(self.graph), 0)

    def testTriples(self):
        tarek = self.tarek
        michel = self.michel
        bob = self.bob
        likes = self.likes
        hates = self.hates
        pizza = self.pizza
        cheese = self.cheese
        c1 = self.c1
        asserte = self.assertEquals
        triples = self.graph.triples
        graph = self.graph
        c1graph = Graph(self.graph.store, c1)
        c1triples = c1graph.triples
        Any = None

        self.addStuff()

        # unbound subjects with context
        asserte(len(list(c1triples((Any, likes, pizza)))), 2)
        asserte(len(list(c1triples((Any, hates, pizza)))), 1)
        asserte(len(list(c1triples((Any, likes, cheese)))), 3)
        asserte(len(list(c1triples((Any, hates, cheese)))), 0)

        # unbound subjects without context, same results!
        asserte(len(list(triples((Any, likes, pizza)))), 2)
        asserte(len(list(triples((Any, hates, pizza)))), 1)
        asserte(len(list(triples((Any, likes, cheese)))), 3)
        asserte(len(list(triples((Any, hates, cheese)))), 0)

        # unbound objects with context
        asserte(len(list(c1triples((michel, likes, Any)))), 2)
        asserte(len(list(c1triples((tarek, likes, Any)))), 2)
        asserte(len(list(c1triples((bob, hates, Any)))), 2)
        asserte(len(list(c1triples((bob, likes, Any)))), 1)

        # unbound objects without context, same results!
        asserte(len(list(triples((michel, likes, Any)))), 2)
        asserte(len(list(triples((tarek, likes, Any)))), 2)
        asserte(len(list(triples((bob, hates, Any)))), 2)
        asserte(len(list(triples((bob, likes, Any)))), 1)

        # unbound predicates with context
        asserte(len(list(c1triples((michel, Any, cheese)))), 1)
        asserte(len(list(c1triples((tarek, Any, cheese)))), 1)
        asserte(len(list(c1triples((bob, Any, pizza)))), 1)
        asserte(len(list(c1triples((bob, Any, michel)))), 1)

        # unbound predicates without context, same results!
        asserte(len(list(triples((michel, Any, cheese)))), 1)
        asserte(len(list(triples((tarek, Any, cheese)))), 1)
        asserte(len(list(triples((bob, Any, pizza)))), 1)
        asserte(len(list(triples((bob, Any, michel)))), 1)

        # unbound subject, objects with context
        asserte(len(list(c1triples((Any, hates, Any)))), 2)
        asserte(len(list(c1triples((Any, likes, Any)))), 5)

        # unbound subject, objects without context, same results!
        asserte(len(list(triples((Any, hates, Any)))), 2)
        asserte(len(list(triples((Any, likes, Any)))), 5)

        # unbound predicates, objects with context
        asserte(len(list(c1triples((michel, Any, Any)))), 2)
        asserte(len(list(c1triples((bob, Any, Any)))), 3)
        asserte(len(list(c1triples((tarek, Any, Any)))), 2)

        # unbound predicates, objects without context, same results!
        asserte(len(list(triples((michel, Any, Any)))), 2)
        asserte(len(list(triples((bob, Any, Any)))), 3)
        asserte(len(list(triples((tarek, Any, Any)))), 2)

        # unbound subjects, predicates with context
        asserte(len(list(c1triples((Any, Any, pizza)))), 3)
        asserte(len(list(c1triples((Any, Any, cheese)))), 3)
        asserte(len(list(c1triples((Any, Any, michel)))), 1)

        # unbound subjects, predicates without context, same results!
        asserte(len(list(triples((Any, Any, pizza)))), 3)
        asserte(len(list(triples((Any, Any, cheese)))), 3)
        asserte(len(list(triples((Any, Any, michel)))), 1)

        # all unbound with context
        asserte(len(list(c1triples((Any, Any, Any)))), 7)
        # all unbound without context, same result!
        asserte(len(list(triples((Any, Any, Any)))), 7)

        for c in [graph, self.graph.get_context(c1)]:
            # unbound subjects
            asserte(set(c.subjects(likes, pizza)), set((michel, tarek)))
            asserte(set(c.subjects(hates, pizza)), set((bob,)))
            asserte(set(c.subjects(likes, cheese)), set([tarek, bob, michel]))
            asserte(set(c.subjects(hates, cheese)), set())

            # unbound objects
            asserte(set(c.objects(michel, likes)), set([cheese, pizza]))
            asserte(set(c.objects(tarek, likes)), set([cheese, pizza]))
            asserte(set(c.objects(bob, hates)), set([michel, pizza]))
            asserte(set(c.objects(bob, likes)), set([cheese]))

            # unbound predicates
            asserte(set(c.predicates(michel, cheese)), set([likes]))
            asserte(set(c.predicates(tarek, cheese)), set([likes]))
            asserte(set(c.predicates(bob, pizza)), set([hates]))
            asserte(set(c.predicates(bob, michel)), set([hates]))

            asserte(set(
                c.subject_objects(hates)), set([(bob, pizza), (bob, michel)]))
            asserte(
                set(c.subject_objects(likes)), set(
                    [(tarek, cheese), (michel, cheese),
                     (michel, pizza), (bob, cheese),
                     (tarek, pizza)]))

            asserte(set(c.predicate_objects(
                michel)), set([(likes, cheese), (likes, pizza)]))
            asserte(set(c.predicate_objects(bob)), set([(likes,
                    cheese), (hates, pizza), (hates, michel)]))
            asserte(set(c.predicate_objects(
                tarek)), set([(likes, cheese), (likes, pizza)]))

            asserte(set(c.subject_predicates(
                pizza)), set([(bob, hates), (tarek, likes), (michel, likes)]))
            asserte(set(c.subject_predicates(cheese)), set([(
                bob, likes), (tarek, likes), (michel, likes)]))
            asserte(set(c.subject_predicates(michel)), set([(bob, hates)]))

            asserte(set(c), set(
                [(bob, hates, michel), (bob, likes, cheese),
                 (tarek, likes, pizza), (michel, likes, pizza),
                 (michel, likes, cheese), (bob, hates, pizza),
                 (tarek, likes, cheese)]))

        # remove stuff and make sure the graph is empty again
        self.removeStuff()
        asserte(len(list(c1triples((Any, Any, Any)))), 0)
        asserte(len(list(triples((Any, Any, Any)))), 0)

        


# dynamically create classes for each registered Store

pluginname = None
if __name__ == '__main__':
    if len(sys.argv) > 1:
        pluginname = sys.argv[1]

tests = 0
for s in plugin.plugins(pluginname, plugin.Store):
    if s.name in ('default', 'IOMemory', 'Auditable',
                  'Concurrent', 'SPARQLStore', 'SPARQLUpdateStore'):
        continue  # these are tested by default
    if not s.getClass().context_aware:
        continue

    locals()["t%d" % tests] = type("%sContextTestCase" % s.name, (
        ContextTestCase,), {"store": s.name})
    tests += 1


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_graph_formula
import sys
import os
from tempfile import mkdtemp, mkstemp
from rdflib import RDF, RDFS, URIRef, BNode, Variable, plugin
from rdflib.graph import QuotedGraph, ConjunctiveGraph


implies = URIRef("http://www.w3.org/2000/10/swap/log#implies")
testN3 = """
@prefix rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix : <http://test/> .
{:a :b :c;a :foo} => {:a :d :c,?y}.
_:foo a rdfs:Class.
:a :d :c."""

from nose.tools import nottest
from nose.exc import SkipTest

# Thorough test suite for formula-aware store


@nottest  # do not run on its own - only as part of generator
def testFormulaStore(store="default", configString=None):
    try:
        g = ConjunctiveGraph(store=store)
    except ImportError:
        raise SkipTest("Dependencies for store '%s' not available!" % store)

    if configString:
        g.destroy(configString)
        g.open(configString)
    else:
        if store == 'SQLite':
            _, path = mkstemp(prefix='test', dir='/tmp', suffix='.sqlite')
            g.open(path, create=True)
        else:
            g.open(mkdtemp(), create=True)

    g.parse(data=testN3, format="n3")
    try:
        for s, p, o in g.triples((None, implies, None)):
            formulaA = s
            formulaB = o

        assert type(formulaA) == QuotedGraph and type(formulaB) == QuotedGraph
        # a = URIRef('http://test/a')
        b = URIRef('http://test/b')
        c = URIRef('http://test/c')
        d = URIRef('http://test/d')
        v = Variable('y')

        universe = ConjunctiveGraph(g.store)

        #test formula as terms
        assert len(list(universe.triples((formulaA, implies, formulaB)))) == 1

        #test variable as term and variable roundtrip
        assert len(list(formulaB.triples((None, None, v)))) == 1
        for s, p, o in formulaB.triples((None, d, None)):
            if o != c:
                assert isinstance(o, Variable)
                assert o == v
        s = list(universe.subjects(RDF.type, RDFS.Class))[0]
        assert isinstance(s, BNode)
        assert len(list(universe.triples((None, implies, None)))) == 1
        assert len(list(universe.triples((None, RDF.type, None)))) == 1
        assert len(list(formulaA.triples((None, RDF.type, None)))) == 1
        assert len(list(formulaA.triples((None, None, None)))) == 2
        assert len(list(formulaB.triples((None, None, None)))) == 2
        assert len(list(universe.triples((None, None, None)))) == 3
        assert len(list(formulaB.triples(
            (None, URIRef('http://test/d'), None)))) == 2
        assert len(list(universe.triples(
            (None, URIRef('http://test/d'), None)))) == 1

        # #context tests
        # #test contexts with triple argument
        # assert len(list(universe.contexts((a, d, c)))) == 1, \
        #                     [ct for ct in universe.contexts((a, d, c))]

        # FAIL: test.test_graph_formula.testFormulaStores('SQLite',)
        # --------------------------------------------------------------------
        # Traceback (most recent call last):
        #   File ".../site-packages/nose/case.py", line 197, in runTest
        #     self.test(*self.arg)
        #   File ".../test_graph_formula.py", line 80, in testFormulaStore
        #     [ct for ct in universe.contexts((a, d, c))]
        # AssertionError: [
        #     <Graph identifier=N52fd4417ef7641089b2e4045ef19ad87
        #        (<class 'rdflib.graph.Graph'>)>,
        #     <Graph identifier=_:Formula16 (<class 'rdflib.graph.Graph'>)>
        #     ]

        #Remove test cases
        universe.remove((None, implies, None))
        assert len(list(universe.triples((None, implies, None)))) == 0
        assert len(list(formulaA.triples((None, None, None)))) == 2
        assert len(list(formulaB.triples((None, None, None)))) == 2

        formulaA.remove((None, b, None))
        assert len(list(formulaA.triples((None, None, None)))) == 1
        formulaA.remove((None, RDF.type, None))
        assert len(list(formulaA.triples((None, None, None)))) == 0

        universe.remove((None, RDF.type, RDFS.Class))

        #remove_context tests
        universe.remove_context(formulaB)
        assert len(list(universe.triples((None, RDF.type, None)))) == 0
        assert len(universe) == 1
        assert len(formulaB) == 0

        universe.remove((None, None, None))
        assert len(universe) == 0

        g.close()
        if store == 'SQLite':
            os.unlink(path)
        else:
            g.store.destroy(configString)
    except:
        g.close()
        if store == 'SQLite':
            os.unlink(path)
        else:
            g.store.destroy(configString)
        raise


def testFormulaStores():
    pluginname = None
    if __name__ == '__main__':
        if len(sys.argv) > 1:
            pluginname = sys.argv[1]

    for s in plugin.plugins(pluginname, plugin.Store):
        if s.name in (
                'Auditable', 'Concurrent',
                'SPARQLStore', 'SPARQLUpdateStore',
                ):
            continue
        if not s.getClass().formula_aware:
            continue
        yield testFormulaStore, s.name


if __name__ == '__main__':
    import nose
    nose.main(defaultTest=sys.argv[0])

########NEW FILE########
__FILENAME__ = test_graph_items
from rdflib import Graph, RDF

def test_recursive_list_detection():
        g = Graph().parse(data="""
        @prefix : <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .

        <> :value _:a .
        _:a :first "turtles"; :rest _:a .

        <> :value [ :first "turtles"; :rest _:b ] .
        _:b :first "all the way down"; :rest _:b .

        <> :value [ :first "turtles"; :rest _:c ] .
        _:c :first "all the way down"; :rest _:a .

        """, format="turtle")

        for v in g.objects(None, RDF.value):
            try:
                list(g.items(v))
            except ValueError, e:
                pass
            else:
                assert False, "Expected detection of recursive rdf:rest reference"

########NEW FILE########
__FILENAME__ = test_initbindings

from nose import SkipTest


from rdflib import ConjunctiveGraph, URIRef, Literal, Namespace, Variable
g = ConjunctiveGraph()


def testStr():
    raise SkipTest('skipped - pending fix for #294')
    a = set(g.query("SELECT (STR(?target) AS ?r) WHERE { }", initBindings={'target': URIRef('example:a')}))
    b = set(g.query("SELECT (STR(?target) AS ?r) WHERE { } VALUES (?target) {(<example:a>)}"))
    assert a==b, "STR: %r != %r"%(a,b)

def testIsIRI():
    raise SkipTest('skipped - pending fix for #294')
    a = set(g.query("SELECT (isIRI(?target) AS ?r) WHERE { }", initBindings={'target': URIRef('example:a')}))
    b = set(g.query("SELECT (isIRI(?target) AS ?r) WHERE { } VALUES (?target) {(<example:a>)}"))
    assert a==b, "isIRI: %r != %r"%(a,b)

def testIsBlank():
    raise SkipTest('skipped - pending fix for #294')
    a = set(g.query("SELECT (isBlank(?target) AS ?r) WHERE { }", initBindings={'target': URIRef('example:a')}))
    b = set(g.query("SELECT (isBlank(?target) AS ?r) WHERE { } VALUES (?target) {(<example:a>)}"))
    assert a==b, "isBlank: %r != %r"%(a,b)

def testIsLiteral(): 
    raise SkipTest('skipped - pending fix for #294')
    a = set(g.query("SELECT (isLiteral(?target) AS ?r) WHERE { }", initBindings={'target': Literal('example')}))
    b = set(g.query("SELECT (isLiteral(?target) AS ?r) WHERE { } VALUES (?target) {('example')}"))
    assert a==b, "isLiteral: %r != %r"%(a,b)

def testUCase(): 
    raise SkipTest('skipped - pending fix for #294')
    a = set(g.query("SELECT (UCASE(?target) AS ?r) WHERE { }", initBindings={'target': Literal('example')}))
    b = set(g.query("SELECT (UCASE(?target) AS ?r) WHERE { } VALUES (?target) {('example')}"))
    assert a==b, "UCASE: %r != %r"%(a,b)

def testNoFunc():
    a = set(g.query("SELECT ?target WHERE { }", initBindings={'target': Literal('example')}))
    b = set(g.query("SELECT ?target WHERE { } VALUES (?target) {('example')}"))
    assert a==b, "no func: %r != %r"%(a,b)


EX = Namespace("http://example.com/")
g2 = ConjunctiveGraph()
g2.bind('', EX)
g2.add((EX['s1'], EX['p'], EX['o1']))
g2.add((EX['s2'], EX['p'], EX['o2']))
    
def testStringKey():
    results = list(g2.query("SELECT ?o WHERE { ?s :p ?o }", initBindings={"s": EX['s1']}))
    assert len(results) == 1, results

def testStringKeyWithQuestionMark():
    results = list(g2.query("SELECT ?o WHERE { ?s :p ?o }", initBindings={"?s": EX['s1']}))
    assert len(results) == 1, results

def testVariableKey():
    results = list(g2.query("SELECT ?o WHERE { ?s :p ?o }", initBindings={Variable("s"): EX['s1']}))
    assert len(results) == 1, results

def testVariableKeyWithQuestionMark():
    results = list(g2.query("SELECT ?o WHERE { ?s :p ?o }", initBindings={Variable("?s"): EX['s1']}))
    assert len(results) == 1, results
    
    
if __name__ == "__main__":

    import sys
    import nose
    if len(sys.argv)==1: 
        nose.main(defaultTest=sys.argv[0])

########NEW FILE########
__FILENAME__ = test_iomemory

"""

Iteration and update conflict with set based IOMemory store

https://github.com/RDFLib/rdflib/issues/286

"""

from rdflib.store import Store
from rdflib import plugin

from rdflib import Graph, Literal, Namespace

def test_concurrent1(): 
    dns = Namespace(u"http://www.example.com/")

    store = plugin.get("IOMemory", Store)()
    g1 = Graph(store=store)

    g1.add((dns.Name, dns.prop, Literal(u"test")))
    g1.add((dns.Name, dns.prop, Literal(u"test2")))
    g1.add((dns.Name, dns.prop, Literal(u"test3")))

    n = len(g1)
    i = 0

    for t in g1.triples((None, None, None)):
        i+=1
        # next line causes problems because it adds a new Subject that needs
        # to be indexed  in __subjectIndex dictionary in IOMemory Store.
        # which invalidates the iterator used to iterate over g1
        g1.add(t)

    assert i == n

def test_concurrent2(): 
    dns = Namespace(u"http://www.example.com/")

    store = plugin.get("IOMemory", Store)()
    g1 = Graph(store=store)
    g2 = Graph(store=store)

    g1.add((dns.Name, dns.prop, Literal(u"test")))
    g1.add((dns.Name, dns.prop, Literal(u"test2")))
    g1.add((dns.Name, dns.prop, Literal(u"test3")))

    n = len(g1)
    i = 0

    for t in g1.triples((None, None, None)):
        i+=1
        g2.add(t)
        # next line causes problems because it adds a new Subject that needs
        # to be indexed  in __subjectIndex dictionary in IOMemory Store.
        # which invalidates the iterator used to iterate over g1
        g2.add((dns.Name1, dns.prop1, Literal(u"test")))
        g2.add((dns.Name1, dns.prop, Literal(u"test")))
        g2.add((dns.Name, dns.prop, Literal(u"test4")))

    assert i == n

if __name__ == '__main__':
    test_concurrent1()
    test_concurrent2()

########NEW FILE########
__FILENAME__ = test_issue084
from codecs import getreader
try:
    from io import BytesIO, StringIO
except ImportError:
    from StringIO import StringIO
    BytesIO = StringIO

from rdflib import URIRef, Literal
from rdflib.graph import Graph

rdf = u"""@prefix skos:
<http://www.w3.org/2004/02/skos/core#> .
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix : <http://www.test.org/#> .

:world rdf:type skos:Concept;
    skos:prefLabel "World"@en.
:africa rdf:type skos:Concept;
    skos:prefLabel "Africa"@en;
    skos:broaderTransitive :world.
:CI rdf:type skos:Concept;
    skos:prefLabel "C\u00f4te d'Ivoire"@fr;
    skos:broaderTransitive :africa.    
"""


rdf_utf8 = rdf.encode('utf-8')

rdf_reader = getreader('utf-8')(BytesIO(rdf.encode('utf-8')))



        
def test_a():
    """Test reading N3 from a unicode objects as data"""
    g = Graph()
    g.parse(data=rdf, format='n3')
    v = g.value(subject=URIRef("http://www.test.org/#CI"), predicate=URIRef("http://www.w3.org/2004/02/skos/core#prefLabel"))
    assert v==Literal(u"C\u00f4te d'Ivoire", lang='fr')

def test_b():
    """Test reading N3 from a utf8 encoded string as data"""
    g = Graph()
    g.parse(data=rdf_utf8, format='n3')
    v = g.value(subject=URIRef("http://www.test.org/#CI"), predicate=URIRef("http://www.w3.org/2004/02/skos/core#prefLabel"))
    assert v==Literal(u"C\u00f4te d'Ivoire", lang='fr')

def test_c():
    """Test reading N3 from a codecs.StreamReader, outputting unicode"""
    g = Graph()
#    rdf_reader.seek(0)
    g.parse(source=rdf_reader, format='n3')
    v = g.value(subject=URIRef("http://www.test.org/#CI"), predicate=URIRef("http://www.w3.org/2004/02/skos/core#prefLabel"))
    assert v==Literal(u"C\u00f4te d'Ivoire", lang='fr')

def test_d():
    """Test reading N3 from a StringIO over the unicode object"""
    g = Graph()
    g.parse(source=StringIO(rdf), format='n3')
    v = g.value(subject=URIRef("http://www.test.org/#CI"), predicate=URIRef("http://www.w3.org/2004/02/skos/core#prefLabel"))
    assert v==Literal(u"C\u00f4te d'Ivoire", lang='fr')

def test_e():
    """Test reading N3 from a BytesIO over the string object"""
    g = Graph()
    g.parse(source=BytesIO(rdf_utf8), format='n3')
    v = g.value(subject=URIRef("http://www.test.org/#CI"), predicate=URIRef("http://www.w3.org/2004/02/skos/core#prefLabel"))
    assert v==Literal(u"C\u00f4te d'Ivoire", lang='fr')


# this is unicode
rdfxml=u"""<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:skos="http://www.w3.org/2004/02/skos/core#"
>
  <rdf:Description rdf:about="http://www.test.org/#CI">
    <rdf:type rdf:resource="http://www.w3.org/2004/02/skos/core#Concept"/>
    <skos:prefLabel xml:lang="fr">C\u00f4te d\'Ivoire</skos:prefLabel>
    <skos:broaderTransitive rdf:resource="http://www.test.org/#africa"/>
  </rdf:Description>
</rdf:RDF>
"""

# this is a str
rdfxml_utf8 = rdfxml.encode('utf-8')

rdfxml_reader = getreader('utf-8')(BytesIO(rdfxml.encode('utf-8')))

        
def test_xml_a():
    """Test reading XML from a unicode object as data"""
    import platform
    if platform.system() == 'Java':
        from nose import SkipTest
        raise SkipTest('unicode issue for Jython2.5')
    g = Graph()
    g.parse(data=rdfxml, format='xml')
    v = g.value(subject=URIRef("http://www.test.org/#CI"), predicate=URIRef("http://www.w3.org/2004/02/skos/core#prefLabel"))
    assert v==Literal(u"C\u00f4te d'Ivoire", lang='fr')

def test_xml_b():
    """Test reading XML from a utf8 encoded string object as data"""
    import platform
    if platform.system() == 'Java':
        from nose import SkipTest
        raise SkipTest('unicode issue for Jython2.5')
    g = Graph()
    g.parse(data=rdfxml_utf8, format='xml')
    v = g.value(subject=URIRef("http://www.test.org/#CI"), predicate=URIRef("http://www.w3.org/2004/02/skos/core#prefLabel"))
    assert v==Literal(u"C\u00f4te d'Ivoire", lang='fr')

# The following two cases are currently not supported by Graph.parse
# def test_xml_c():
#     """Test reading XML from a codecs.StreamReader, outputting unicode"""
#     g = Graph()
#     g.parse(source=rdfxml_reader, format='xml')
#     v = g.value(subject=URIRef("http://www.test.org/#CI"), predicate=URIRef("http://www.w3.org/2004/02/skos/core#prefLabel"))
#     assert v==Literal(u"C\u00f4te d'Ivoire", lang='fr')

# def test_xml_d():
#     """Test reading XML from a BytesIO created from unicode object"""
#     g = Graph()
#     g.parse(source=BytesIO(rdfxml), format='xml')
#     v = g.value(subject=URIRef("http://www.test.org/#CI"), predicate=URIRef("http://www.w3.org/2004/02/skos/core#prefLabel"))
#     assert v==Literal(u"C\u00f4te d'Ivoire", lang='fr')

def test_xml_e():
    """Test reading XML from a BytesIO created from utf8 encoded string"""
    import platform
    if platform.system() == 'Java':
        from nose import SkipTest
        raise SkipTest('unicode issue for Jython2.5')
    g = Graph()
    g.parse(source=BytesIO(rdfxml_utf8), format='xml')
    v = g.value(subject=URIRef("http://www.test.org/#CI"), predicate=URIRef("http://www.w3.org/2004/02/skos/core#prefLabel"))
    assert v==Literal(u"C\u00f4te d'Ivoire", lang='fr')

########NEW FILE########
__FILENAME__ = test_issue130
import platform
import rdflib
from nose.exc import SkipTest
from rdflib.py3compat import b
if platform.system == 'Java':
    raise SkipTest("Skipped test, unicode issues in Jython")


def test_issue_130():
    raise SkipTest("Remote content change - skip for now")
    g = rdflib.Graph()
    try:
        g.parse(location="http://linked-data.ru/example")
    except:
        raise SkipTest('Test data URL unparseable')
    if len(g) == 0:
        raise SkipTest('Test data URL empty of content')
    assert b('rdf:about="http://semanticfuture.net/linked-data/example/#company"') in g.serialize(), g.serialize()

########NEW FILE########
__FILENAME__ = test_issue154
import sys
import platform
import warnings
from unittest import TestCase
from rdflib.graph import ConjunctiveGraph, URIRef
from nose.exc import SkipTest
# Workaround for otherwise-dropped HTML entities

import re
from htmlentitydefs import name2codepoint


def htmlentitydecode(s):
    return re.sub('&(%s);' % '|'.join(name2codepoint),
            lambda m: unichr(name2codepoint[m.group(1)]), s)

html = """\
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" \
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<body xmlns:dc="http://purl.org/dc/terms/">
<p about="http://example.com" property="dc:title">Exampl&eacute;</p>
</body>
</html>"""


class EntityTest(TestCase):

    def test_html_entity_xhtml(self):
        if sys.version_info[0] == 3:
            raise SkipTest('minidom parser strips HTML entities in Python 3.2')
        if platform.system() == "Java":
            raise SkipTest('problem with HTML entities for html5lib in Jython')
        g = ConjunctiveGraph()
        warnings.simplefilter('ignore', UserWarning)
        g.parse(data=html, format='rdfa')
        self.assertEqual(len(g), 1)
        self.assertTrue(g.value(URIRef("http://example.com"),
                                 URIRef("http://purl.org/dc/terms/title")
                                 ).eq(u"Exampl"))

    def test_html_decoded_entity_xhtml(self):
        if sys.version_info[0] == 3:
            raise SkipTest('html5lib not yet available for Python 3')
        if platform.system() == "Java":
            raise SkipTest('problem with HTML entities for html5lib in Jython')
        g = ConjunctiveGraph()
        g.parse(data=htmlentitydecode(html), format='rdfa')
        self.assertEqual(len(g), 1)
        self.assertTrue(g.value(URIRef("http://example.com"),
                                  URIRef("http://purl.org/dc/terms/title")
                                  ).eq(u"Exampl\xe9"))

########NEW FILE########
__FILENAME__ = test_issue160
from unittest import TestCase
from rdflib import ConjunctiveGraph
from rdflib import Namespace, Literal
from rdflib.collection import Collection
from nose.exc import SkipTest



target1xml = """\
<?xml version="1.0" encoding="utf-8"?>
<rdf:RDF
  xmlns:ns1="http://www.example.org/foo/ns/"
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
>
  <ns1:Other rdf:about="http://www.example.org/example/foo/thing">
    <rdf:first>
      <ns1:Item rdf:about="http://www.example.org/example/foo/a"/>
    </rdf:first>
    <ns1:property>Some Value</ns1:property>
    <rdf:rest rdf:parseType="Collection">
      <rdf:Description rdf:about="http://www.example.org/example/foo/b"/>
      <rdf:Description rdf:about="http://www.example.org/example/foo/c"/>
    </rdf:rest>
  </ns1:Other>
  <ns1:Item rdf:about="http://www.example.org/example/foo/b"/>
  <ns1:Item rdf:about="http://www.example.org/example/foo/c"/>
</rdf:RDF>"""

target2xml = """\
<?xml version="1.0" encoding="utf-8"?>
<rdf:RDF
  xmlns:ns1="http://www.example.org/foo/ns/"
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
>
  <ns1:Wrapper rdf:about="http://www.example.org/example/foo/wrapper">
    <ns1:wraps rdf:parseType="Collection">
      <rdf:Description rdf:about="http://www.example.org/example/foo/a"/>
      <rdf:Description rdf:about="http://www.example.org/example/foo/b"/>
      <rdf:Description rdf:about="http://www.example.org/example/foo/c"/>
    </ns1:wraps>
  </ns1:Wrapper>
  <ns1:Item rdf:about="http://www.example.org/example/foo/b"/>
  <ns1:Item rdf:about="http://www.example.org/example/foo/c"/>
  <ns1:Item rdf:about="http://www.example.org/example/foo/a"/>
</rdf:RDF>"""

class CollectionTest(TestCase):

    def test_collection_render(self):
        foo = Namespace('http://www.example.org/foo/ns/')
        ex = Namespace('http://www.example.org/example/foo/')
        rdf = Namespace('http://www.w3.org/1999/02/22-rdf-syntax-ns#')

        # Works:  x a rdf:List, a foo:Other ;
        # Fails:  y a foo:Wrapper, foo:wraps x; x a rdf:List, a foo:Other ;

        target1 = ConjunctiveGraph()
        target1.parse(data=target1xml)
        target2 = ConjunctiveGraph()
        target2.parse(data=target2xml)

        g = ConjunctiveGraph()
        bits = [ex['a'], ex['b'], ex['c']]
        l = Collection(g, ex['thing'], bits)
        triple = (ex['thing'], rdf['type'], foo['Other'])
        g.add(triple)
        triple = (ex['thing'], foo['property'], Literal('Some Value'))
        g.add(triple)
        for b in bits:
            triple = (b, rdf['type'], foo['Item'])
            g.add(triple)
        self.assertEqual(g.isomorphic(target1), True)

        # g.add((ex['wrapper'], rdf['type'], foo['Wrapper']))
        # g.add((ex['wrapper'], foo['wraps'], ex['thing']))
        # # resn3 = g.serialize(format="n3")
        # # print(resn3)
        # resxml = g.serialize(format="pretty-xml")
        # # print(resxml)
        # self.assertEqual(g.isomorphic(target2), True)



########NEW FILE########
__FILENAME__ = test_issue161
from unittest import TestCase
from rdflib.py3compat import b
from rdflib.graph import ConjunctiveGraph


class EntityTest(TestCase):

    def test_turtle_namespace_prefixes(self):

        g = ConjunctiveGraph()
        n3 = \
        """
        @prefix _9: <http://data.linkedmdb.org/resource/movie/> .
        @prefix p_9: <urn:test:> .
        @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

        p_9:a p_9:b p_9:c . 

        <http://data.linkedmdb.org/resource/director/1> a
        <http://data.linkedmdb.org/resource/movie/director>;
            rdfs:label "Cecil B. DeMille (Director)";
            _9:director_name "Cecil B. DeMille" ."""

   
        g.parse(data=n3, format='n3')
        turtle = g.serialize(format="turtle")
   
        # Check round-tripping, just for kicks.
        g = ConjunctiveGraph()
        g.parse(data=turtle, format='turtle')
        # Shouldn't have got to here
        s=g.serialize(format="turtle")
        
        self.assert_(b('@prefix _9') not in s)





########NEW FILE########
__FILENAME__ = test_issue184
from rdflib.term import Literal
from rdflib.term import URIRef
from rdflib.graph import ConjunctiveGraph

def test_escaping_of_triple_doublequotes():
    """
    Issue 186 - Check escaping of multiple doublequotes.
    A serialization/deserialization roundtrip of a certain class of 
    Literals fails when there are both, newline characters and multiple subsequent 
    quotation marks in the lexical form of the Literal. In this case invalid N3
    is emitted by the serializer, which in turn cannot be parsed correctly.
    """
    g=ConjunctiveGraph()
    g.add((URIRef('http://foobar'), URIRef('http://fooprop'), Literal('abc\ndef"""""')))
    # assert g.serialize(format='n3') == '@prefix ns1: <http:// .\n\nns1:foobar ns1:fooprop """abc\ndef\\"\\"\\"\\"\\"""" .\n\n'
    g2=ConjunctiveGraph()
    g2.parse(data=g.serialize(format='n3'), format='n3')
    assert g.isomorphic(g2) is True
########NEW FILE########
__FILENAME__ = test_issue190
# -*- coding: utf-8 -*-
from rdflib.graph import ConjunctiveGraph
from rdflib.parser import StringInputSource
import textwrap

prefix = textwrap.dedent('''\
    @prefix nie: <http://www.semanticdesktop.org/ontologies/2007/01/19/nie#> .
    @prefix nfo: <http://www.semanticdesktop.org/ontologies/2007/03/22/nfo#> .
    @prefix nco: <http://www.semanticdesktop.org/ontologies/2007/03/22/nco#> .
    @prefix nmo: <http://www.semanticdesktop.org/ontologies/2007/03/22/nmo#> .
    @prefix ncal: <http://www.semanticdesktop.org/ontologies/2007/04/02/ncal#> .
    @prefix nexif: <http://www.semanticdesktop.org/ontologies/2007/05/10/nexif#> .
    @prefix nid3: <http://www.semanticdesktop.org/ontologies/2007/05/10/nid3#> .
    @prefix dc: <http://dublincore.org/documents/2010/10/11/dces/#> .
    @prefix nmm: <http://library.gnome.org/devel/ontology/unstable/nmm-classes.html#> .
    @prefix nao: <http://www.semanticdesktop.org/ontologies/2007/08/15/nao#> .
    ''')

meta = textwrap.dedent(u"""\
a nfo:PaginatedTextDocument ;
    nie:title "SV Meldung" ;
    nco:creator [ a nco:Contact ;
    nco:fullname "nikratio"] ;
    nie:contentCreated "2011-08-10T20:12:38Z" ;
    dc:format "application/pdf" ;
    nie:description "()" ;
    nao:hasTag ?tag1 ;
    nfo:pageCount 1 ;
    nie:plainTextContent "%s" .
} } WHERE { {
?tag1 a nao:Tag ; nao:prefLabel "()" .
""")

test_string1 = u"""\
Betriebsnummer der Einzugsstelle:\nKnappschaft\n980 0000 6\nWICHTIGES DOKUMENT - SORGFLTIG AUFBEWAHREN!\n """

def test1():
    meta1 = meta.encode('utf-8') % test_string1.encode('utf-8')
    graph = ConjunctiveGraph()
    graph.parse(StringInputSource(prefix + '<http://example.org/>' + meta1), format='n3')

test_string2 = u"""\
Betriebsnummer der Einzugsstelle:
Knappschaft
980 0000 6
WICHTIGES DOKUMENT - SORGFLTIG AUFBEWAHREN!
"""

def test2():
    meta2 = meta.encode('utf-8') % test_string2.encode('utf-8')
    graph = ConjunctiveGraph()
    graph.parse(StringInputSource(prefix + '<http://example.org/>' + meta2), format='n3')

from nose import SkipTest
raise SkipTest("Known issue, with newlines in text")
########NEW FILE########
__FILENAME__ = test_issue200
#!/usr/bin/env python

import os
import rdflib
import unittest

try:
    import os.fork
    import os.pipe
except ImportError:
    from nose import SkipTest
    raise SkipTest('No os.fork() and/or os.pipe() on this platform, skipping')


class TestRandomSeedInFork(unittest.TestCase):

    def test_bnode_id_differs_in_fork(self):
        """Checks that os.fork()ed child processes produce a
        different sequence of BNode ids from the parent process.
        """
        r, w = os.pipe()  # these are file descriptors, not file objects
        pid = os.fork()
        if pid:
            pb1 = rdflib.term.BNode()
            os.close(w)  # use os.close() to close a file descriptor
            r = os.fdopen(r)  # turn r into a file object
            txt = r.read()
            os.waitpid(pid, 0)  # make sure the child process gets cleaned up
        else:
            os.close(r)
            w = os.fdopen(w, 'w')
            cb = rdflib.term.BNode()
            w.write(cb)
            w.close()
            os._exit(0)
        assert txt != str(pb1), "Parent process BNode id: " + \
                                "%s, child process BNode id: %s" % (
                                    txt, str(pb1))


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_issue209
import rdflib
import unittest
import threading


def makeNode():
    i = 0
    while i < 9999:
        i += 1
        rdflib.term.BNode()


class TestRandomSeedInThread(unittest.TestCase):

    def test_bnode_id_gen_in_thread(self):
        """
        """
        th = threading.Thread(target=makeNode)
        th.daemon = True
        th.start()
        makeNode()


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_issue247
import rdflib
import unittest

failxml = """\
<rdf:RDF
    xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
    xmlns:dc="http://purl.org/dc/elements/1.1/"
>

<rdf:Description rdf:about="http://example.org/">
    <dc:description rdf:parseType="Literal">
        <p xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"></p>
    </dc:description>
</rdf:Description>

</rdf:RDF>"""

passxml = """\
<rdf:RDF
    xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
    xmlns:dc="http://purl.org/dc/elements/1.1/"
>

<rdf:Description rdf:about="http://example.org/">
    <dc:description rdf:parseType="Literal">
        <p xmlns="http://www.w3.org/1999/xhtml"></p>
    </dc:description>
</rdf:Description>

</rdf:RDF>"""


class TestXMLLiteralwithLangAttr(unittest.TestCase):

    def test_successful_parse_of_literal_without_xmllang_attr(self):
        """
        Test parse of Literal without xmllang attr passes
        Parsing an RDF/XML document fails with a KeyError when
        it contains a XML Literal with a xml:lang attribute:
        """
        g = rdflib.Graph()
        g.parse(data=passxml)

    def test_failing_parse_of_literal_with_xmllang_attr(self):
        """
        Show parse of Literal with xmllang attr fails
        Parsing an RDF/XML document fails with a KeyError when
        it contains a XML Literal with a xml:lang attribute:
        """
        g = rdflib.Graph()
        g.parse(data=failxml)

if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_issue248
import rdflib
from nose.exc import SkipTest
import unittest


class TestSerialization(unittest.TestCase):

    def test_issue_248(self):
        """
        Ed Summers Thu, 24 May 2007 12:21:17 -0700

        As discussed with eikeon in #redfoot it appears that the n3 serializer
        is ignoring the base option to Graph.serialize...example follows:

        --

        #!/usr/bin/env python

        from rdflib.Graph import Graph
        from rdflib.URIRef import URIRef
        from rdflib import Literal, Namespace, RDF

        graph = Graph()
        DC = Namespace('http://purl.org/dc/terms/')
        SKOS = Namespace('http://www.w3.org/2004/02/skos/core#')
        LCCO = Namespace('http://loc.gov/catdir/cpso/lcco/')

        graph.bind('dc', DC)
        graph.bind('skos', SKOS)
        graph.bind('lcco', LCCO)

        concept = URIRef(LCCO['1'])
        graph.add((concept, RDF.type, SKOS['Concept']))
        graph.add((concept, SKOS['prefLabel'], Literal('Scrapbooks')))
        graph.add((concept, DC['LCC'], Literal('AC999.0999 - AC999999.Z9999')))

        print graph.serialize(format='n3', base=LCCO)

        --

        Which generates:

        --

        @prefix dc: <http://purl.org/dc/terms/>.
        @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>.
        @prefix skos: <http://www.w3.org/2004/02/skos/core#>.

         <http://loc.gov/catdir/cpso/lcco/1> a skos:Concept;
            dc:LCC "AC999.0999 - AC999999.Z9999";
            skos:prefLabel "Scrapbooks".

        --

        Notice

         <http://loc.gov/catdir/cpso/lcco/1> a skos:Concept;

        instead of:

         <1> a skos:Concept;

        //Ed

        """
        graph = rdflib.Graph()
        DC = rdflib.Namespace('http://purl.org/dc/terms/')
        SKOS = rdflib.Namespace('http://www.w3.org/2004/02/skos/core#')
        LCCO = rdflib.Namespace('http://loc.gov/catdir/cpso/lcco/')

        graph.bind('dc', DC)
        graph.bind('skos', SKOS)
        graph.bind('lcco', LCCO)

        concept = rdflib.URIRef(LCCO['1'])
        graph.add(
            (concept,
             rdflib.RDF.type,
             SKOS['Concept']))
        graph.add(
            (concept,
             SKOS['prefLabel'],
             rdflib.Literal('Scrapbooks')))
        graph.add(
            (concept,
             DC['LCC'],
             rdflib.Literal('AC999.0999 - AC999999.Z9999')))
        sg = graph.serialize(format='n3', base=LCCO).decode('utf8')
        # See issue 248
        # Actual test should be the inverse of the below ...
        self.assert_('<1> a skos:Concept ;' in sg, sg)

if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_issue363
import rdflib
from nose import SkipTest
from nose.tools import assert_raises

data = '''<?xml version="1.0" encoding="utf-8"?>
<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
    xmlns:http="http://www.w3.org/2011/http#">

    <http:HeaderElement rdf:about="#he0">
        <http:params>
            <http:Parameter rdf:about="#param0_0" />
            <http:Parameter rdf:about="#param0_1" />
        </http:params>
    </http:HeaderElement>

</rdf:RDF>
'''

data2 = '''<?xml version="1.0" encoding="utf-8"?>
<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
    xmlns="http://www.example.org/meeting_organization#">

    <rdf:Description about="http://meetings.example.com/cal#m1">
        <Location rdf:parseType="Resource">
            <zip xmlns="http://www.another.example.org/geographical#">02139</zip>
            <lat xmlns="http://www.another.example.org/geographical#">14.124425</lat>
        </Location>
    </rdf:Description>
</rdf:RDF>
'''

def test_broken_rdfxml():
    #import ipdb; ipdb.set_trace()
    def p():
        rdflib.Graph().parse(data=data)

    assert_raises(Exception, p)

def test_parsetype_resource():
    g = rdflib.Graph().parse(data=data2)
    print g.serialize(format='n3')

if __name__ == '__main__':
    test_broken_rdfxml()
    test_parsetype_resource()

########NEW FILE########
__FILENAME__ = test_issue375
import subprocess
import re

rdfa_expected = u'''@prefix dc: <http://purl.org/dc/terms/> .
@prefix foaf: <http://xmlns.com/foaf/0.1/> .
@prefix frbr: <http://vocab.org/frbr/core#> .
@prefix gr: <http://purl.org/goodrelations/v1#> .
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix xml: <http://www.w3.org/XML/1998/namespace> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .

<http://customer.wileyeurope.com/CGI-BIN/lansaweb?procfun+shopcart+shcfn01+funcparms+parmisbn(a0130):9780596516499+parmqty(p0050):1+parmurl(l0560):http://oreilly.com/store/> a gr:Offering ;
    gr:includesObject [ a gr:TypeAndQuantityNode ;
            gr:ammountOfThisGood "1.0"^^xsd:float ;
            gr:hasPriceSpecification [ a gr:UnitPriceSpecification ;
                    gr:hasCurrency "GBP"@en ;
                    gr:hasCurrencyValue "34.5"^^xsd:float ] ;
            gr:typeOfGood <urn:x-domain:oreilly.com:product:9780596516499.BOOK> ] .

<http://my.safaribooksonline.com/9780596803346> a gr:Offering ;
    gr:includesObject [ a gr:TypeAndQuantityNode ;
            gr:ammountOfThisGood "1.0"^^xsd:float ;
            gr:typeOfGood <urn:x-domain:oreilly.com:product:9780596803346.SAF> ] .

<https://epoch.oreilly.com/shop/cart.orm?p=BUNDLE&prod=9780596516499.BOOK&prod=9780596803391.EBOOK&bundle=1&retUrl=http%3A%252F%252Foreilly.com%252Fstore%252F> a gr:Offering ;
    gr:includesObject [ a gr:TypeAndQuantityNode ;
            gr:ammountOfThisGood "1.0"^^xsd:float ;
            gr:includesObject [ a gr:TypeAndQuantityNode ;
                    gr:ammountOfThisGood "1.0"^^xsd:float ;
                    gr:hasPriceSpecification [ a gr:UnitPriceSpecification ;
                            gr:hasCurrency "None"@en ;
                            gr:hasCurrencyValue "49.49"^^xsd:float ] ;
                    gr:typeOfGood <urn:x-domain:oreilly.com:product:9780596803391.EBOOK> ] ;
            gr:typeOfGood <urn:x-domain:oreilly.com:product:9780596516499.BOOK> ] .

<https://epoch.oreilly.com/shop/cart.orm?prod=9780596516499.BOOK> a gr:Offering ;
    gr:includesObject [ a gr:TypeAndQuantityNode ;
            gr:ammountOfThisGood "1.0"^^xsd:float ;
            gr:hasPriceSpecification [ a gr:UnitPriceSpecification ;
                    gr:hasCurrency "USD"@en ;
                    gr:hasCurrencyValue "44.99"^^xsd:float ] ;
            gr:typeOfGood <urn:x-domain:oreilly.com:product:9780596516499.BOOK> ] .

<https://epoch.oreilly.com/shop/cart.orm?prod=9780596803391.EBOOK> a gr:Offering ;
    gr:includesObject [ a gr:TypeAndQuantityNode ;
            gr:ammountOfThisGood "1.0"^^xsd:float ;
            gr:hasPriceSpecification [ a gr:UnitPriceSpecification ;
                    gr:hasCurrency "USD"@en ;
                    gr:hasCurrencyValue "35.99"^^xsd:float ] ;
            gr:typeOfGood <urn:x-domain:oreilly.com:product:9780596803391.EBOOK> ] .

<urn:x-domain:oreilly.com:product:9780596516499.IP> a frbr:Expression ;
    dc:creator <urn:x-domain:oreilly.com:agent:pdb:3343>,
        <urn:x-domain:oreilly.com:agent:pdb:3501>,
        <urn:x-domain:oreilly.com:agent:pdb:3502> ;
    dc:issued "2009-06-12"^^xsd:dateTime ;
    dc:publisher "O'Reilly Media"@en ;
    dc:title "Natural Language Processing with Python"@en ;
    frbr:embodiment <urn:x-domain:oreilly.com:product:9780596516499.BOOK>,
        <urn:x-domain:oreilly.com:product:9780596803346.SAF>,
        <urn:x-domain:oreilly.com:product:9780596803391.EBOOK> .

<urn:x-domain:oreilly.com:agent:pdb:3343> a foaf:Person ;
    foaf:homepage <http://www.oreillynet.com/pub/au/3614> ;
    foaf:name "Steven Bird"@en .

<urn:x-domain:oreilly.com:agent:pdb:3501> a foaf:Person ;
    foaf:homepage <http://www.oreillynet.com/pub/au/3615> ;
    foaf:name "Ewan Klein"@en .

<urn:x-domain:oreilly.com:agent:pdb:3502> a foaf:Person ;
    foaf:homepage <http://www.oreillynet.com/pub/au/3616> ;
    foaf:name "Edward Loper"@en .

<urn:x-domain:oreilly.com:product:9780596803346.SAF> a frbr:Manifestation ;
    dc:type <http://purl.oreilly.com/product-types/SAF> .

<urn:x-domain:oreilly.com:product:9780596803391.EBOOK> a frbr:Manifestation ;
    dc:identifier <urn:isbn:9780596803391> ;
    dc:issued "2009-06-12"^^xsd:dateTime ;
    dc:type <http://purl.oreilly.com/product-types/EBOOK> .

<urn:x-domain:oreilly.com:product:9780596516499.BOOK> a frbr:Manifestation ;
    dc:extent """
                    512
                """@en ;
    dc:identifier <urn:isbn:9780596516499> ;
    dc:issued "2009-06-19"^^xsd:dateTime ;
    dc:type <http://purl.oreilly.com/product-types/BOOK> .
'''.strip()

mdata_expected = u'''@prefix cat: <http://www.w3.org/ns/dcat#> .
@prefix cc: <http://creativecommons.org/ns#> .
@prefix ctag: <http://commontag.org/ns#> .
@prefix dc: <http://purl.org/dc/terms/> .
@prefix dc11: <http://purl.org/dc/elements/1.1/> .
@prefix dcterms: <http://purl.org/dc/terms/> .
@prefix foaf: <http://xmlns.com/foaf/0.1/> .
@prefix gr: <http://purl.org/goodrelations/v1#> .
@prefix grddl: <http://www.w3.org/2003/g/data-view#> .
@prefix hcalendar: <http://microformats.org/profile/hcalendar#> .
@prefix hcard: <http://microformats.org/profile/hcard#> .
@prefix ical: <http://www.w3.org/2002/12/cal/icaltzd#> .
@prefix ma: <http://www.w3.org/ns/ma-ont#> .
@prefix md: <http://www.w3.org/ns/md#> .
@prefix og: <http://ogp.me/ns#> .
@prefix org: <http://www.w3.org/ns/org#> .
@prefix owl: <http://www.w3.org/2002/07/owl#> .
@prefix prov: <http://www.w3.org/ns/prov#> .
@prefix qb: <http://purl.org/linked-data/cube#> .
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfa: <http://www.w3.org/ns/rdfa#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix rev: <http://purl.org/stuff/rev#> .
@prefix rif: <http://www.w3.org/2007/rif#> .
@prefix rr: <http://www.w3.org/ns/r2rml#> .
@prefix schema: <http://schema.org/> .
@prefix sd: <http://www.w3.org/ns/sparql-service-description#> .
@prefix sioc: <http://rdfs.org/sioc/ns#> .
@prefix skos: <http://www.w3.org/2004/02/skos/core#> .
@prefix skosxl: <http://www.w3.org/2008/05/skos-xl#> .
@prefix v: <http://rdf.data-vocabulary.org/#> .
@prefix vcard: <http://www.w3.org/2006/vcard/ns#> .
@prefix void: <http://rdfs.org/ns/void#> .
@prefix wdr: <http://www.w3.org/2007/05/powder#> .
@prefix wdrs: <http://www.w3.org/2007/05/powder-s#> .
@prefix xhv: <http://www.w3.org/1999/xhtml/vocab#> .
@prefix xml: <http://www.w3.org/XML/1998/namespace> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .

<test/mdata/codelab.html> md:item ( [ a schema:TechArticle ;
                schema:articleBody """
    Exercise 1: From basic HTML to RDFa: first steps
    Exercise 2: Embedded types
    Exercise 3: From strings to things
""" ;
                schema:author "Author Name" ;
                schema:datePublished "January 29, 2014" ;
                schema:description """
    About this codelab
""" ;
                schema:educationalUse "codelab" ;
                schema:image <test/mdata/squares.png> ;
                schema:name "Structured data with schema.org codelab" ] ) ;
    rdfa:usesVocabulary schema: .
'''.strip()

def test_rdfpipe_bytes_vs_str():
    """
    Issue 375: rdfpipe command generates bytes vs. str TypeError

    While Python2 exposes sys.stdout as a bytes buffer, Python 3
    explicitly exposes sys.stdout.buffer for this purpose. Test
    rdfpipe to ensure that we get the expected results.
    """
    args = ['python', 'rdflib/tools/rdfpipe.py', '-i', 'rdfa1.1', 'test/rdfa/oreilly.html']
    proc = subprocess.Popen(args, stdout=subprocess.PIPE, universal_newlines=True)
    res = ''
    while proc.poll() is None:
        res += proc.stdout.read()

    assert res.strip() == rdfa_expected

def test_rdfpipe_mdata_open():
    """
    Issue 375: rdfa1.1 and mdata processors used file() builtin

    The file() builtin has been deprecated for a long time. Use
    the open() builtin instead.
    """
    args = ['python', 'rdflib/tools/rdfpipe.py', '-i', 'mdata', 'test/mdata/codelab.html']
    proc = subprocess.Popen(args, stdout=subprocess.PIPE, universal_newlines=True)
    res = ''
    while proc.poll() is None:
        res += proc.stdout.read()

    a = re.compile(r'^(.*?<)[^>]+(test/mdata/codelab.*?>)', flags=re.DOTALL)
    b = re.compile(r'^(.*?<)[^>]+(test/mdata/squares.*?>)', flags=re.DOTALL)
    res = a.sub(r'\1\2', res.strip())
    res = b.sub(r'\1\2', res)

    assert res == mdata_expected

########NEW FILE########
__FILENAME__ = test_issue_git_200
import rdflib

import nose.tools

def test_broken_add(): 

    g=rdflib.Graph()
    nose.tools.assert_raises(AssertionError, lambda : g.add((1,2,3)))
    nose.tools.assert_raises(AssertionError, lambda : g.addN([(1,2,3,g)]))


if __name__=='__main__':
    import nose, sys
    nose.main(defaultTest=sys.argv[0])

########NEW FILE########
__FILENAME__ = test_issue_git_336
import rdflib
from rdflib.plugins.parsers.notation3 import BadSyntax

import nose.tools

# Test for https://github.com/RDFLib/rdflib/issues/336
# and https://github.com/RDFLib/rdflib/issues/345


# stripped-down culprit:
'''\
@prefix fs: <http://freesurfer.net/fswiki/terms/> .
@prefix prov: <http://www.w3.org/ns/prov#> .

<http://nidm.nidash.org/iri/82b79326488911e3b2fb14109fcf6ae7>
        a fs:stat_header,
        prov:Entity ;
    fs:mrisurf.c-cvs_version
        "$Id: mrisurf.c,v 1.693.2.2 2011/04/27 19:21:05 nicks Exp $" .
'''


def test_ns_localname_roundtrip():

    XNS = rdflib.Namespace('http://example.net/fs')

    g = rdflib.Graph()
    g.bind('xns', str(XNS))
    g.add((
        rdflib.URIRef('http://example.com/thingy'),
        XNS['lowecase.xxx-xxx_xxx'],  # <- not round trippable
        rdflib.Literal("Junk")))
    turtledump = g.serialize(format="turtle").decode('utf-8')
    xmldump = g.serialize().decode('utf-8')
    g1 = rdflib.Graph()

    g1.parse(data=xmldump)

    g1.parse(data=turtledump, format="turtle")



if __name__ == '__main__':
    import nose
    import sys
    nose.main(defaultTest=sys.argv[0])

########NEW FILE########
__FILENAME__ = test_literal
import unittest

import rdflib # needed for eval(repr(...)) below
from rdflib.term import Literal, URIRef, _XSD_DOUBLE, bind
from rdflib.py3compat import format_doctest_out as uformat

# these are actually meant for test_term.py, which is not yet merged into trunk

class TestMd5(unittest.TestCase):
    def testMd5(self):
        self.assertEqual(rdflib.URIRef("http://example.com/").md5_term_hash(),
                         "40f2c9c20cc0c7716fb576031cceafa4")
        self.assertEqual(rdflib.Literal("foo").md5_term_hash(),
                         "da9954ca5f673f8ab9ebd6faf23d1046")
    

class TestLiteral(unittest.TestCase):
    def setUp(self):
        pass

    def test_repr_apostrophe(self):
        a = rdflib.Literal("'")
        b = eval(repr(a))
        self.assertEquals(a, b)

    def test_repr_quote(self):
        a = rdflib.Literal('"')
        b = eval(repr(a))
        self.assertEquals(a, b)

    def test_backslash(self):
        d = r"""
<rdf:RDF
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
  xmlns:foo="http://example.org/foo#">
    <rdf:Description>
      <foo:bar>a\b</foo:bar>
    </rdf:Description>
</rdf:RDF>
"""
        g = rdflib.Graph()
        g.parse(data=d)
        a = rdflib.Literal('a\\b')
        b = list(g.objects())[0]
        self.assertEquals(a, b)

    def test_literal_from_bool(self):
        l = rdflib.Literal(True)
        self.assertEquals(l.datatype, rdflib.XSD["boolean"])

class TestNew(unittest.TestCase):
    def testCantPassLangAndDatatype(self):
        self.assertRaises(TypeError,
           Literal, 'foo', lang='en', datatype=URIRef("http://example.com/"))

    def testFromOtherLiteral(self):
        l = Literal(1)
        l2 = Literal(l)
        self.assertTrue(isinstance(l.value, int))
        self.assertTrue(isinstance(l2.value, int))

        # change datatype
        l = Literal("1")
        l2 = Literal(l, datatype=rdflib.XSD.integer)
        self.assertTrue(isinstance(l2.value, long))



    def testDatatypeGetsAutoURIRefConversion(self):
        # drewp disapproves of this behavior, but it should be
        # represented in the tests
        x = Literal("foo", datatype="http://example.com/")
        self.assert_(isinstance(x.datatype, URIRef))

        x = Literal("foo", datatype=Literal("pennies"))
        self.assertEqual(x.datatype, URIRef("pennies"))



class TestRepr(unittest.TestCase):
    def testOmitsMissingDatatypeAndLang(self):
        self.assertEqual(repr(Literal("foo")),
                         uformat("rdflib.term.Literal(%(u)s'foo')"))

    def testOmitsMissingDatatype(self):
        self.assertEqual(repr(Literal("foo", lang='en')),
                         uformat("rdflib.term.Literal(%(u)s'foo', lang='en')"))

    def testOmitsMissingLang(self):
        self.assertEqual(
            repr(Literal("foo", datatype=URIRef('http://example.com/'))),
            uformat("rdflib.term.Literal(%(u)s'foo', datatype=rdflib.term.URIRef(%(u)s'http://example.com/'))"))

    def testSubclassNameAppearsInRepr(self):
        class MyLiteral(Literal):
            pass
        x = MyLiteral(u"foo")
        self.assertEqual(repr(x), uformat("MyLiteral(%(u)s'foo')"))

class TestDoubleOutput(unittest.TestCase):
    def testNoDanglingPoint(self):
        """confirms the fix for https://github.com/RDFLib/rdflib/issues/237"""
        vv = Literal("0.88", datatype=_XSD_DOUBLE)
        out = vv._literal_n3(use_plain=True)
        self.assert_(out in ["8.8e-01", "0.88"], out)

class TestBindings(unittest.TestCase):

    def testBinding(self):

        class a:
            def __init__(self,v):
                self.v=v[3:-3]
            def __str__(self):
                return '<<<%s>>>'%self.v

        dtA=rdflib.URIRef('urn:dt:a')
        bind(dtA,a)

        va=a("<<<2>>>")
        la=Literal(va, normalize=True)
        self.assertEqual(la.value,va)
        self.assertEqual(la.datatype, dtA)

        la2=Literal("<<<2>>>", datatype=dtA)
        self.assertTrue(isinstance(la2.value, a))
        self.assertEqual(la2.value.v,va.v)

        class b:
            def __init__(self,v):
                self.v=v[3:-3]
            def __str__(self):
                return 'B%s'%self.v

        dtB=rdflib.URIRef('urn:dt:b')
        bind(dtB,b,None,lambda x: '<<<%s>>>'%x)

        vb=b("<<<3>>>")
        lb=Literal(vb, normalize=True)
        self.assertEqual(lb.value,vb)
        self.assertEqual(lb.datatype, dtB)





if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_memory_store
import unittest
import rdflib

rdflib.plugin.register('Memory', rdflib.store.Store, 
                'rdflib.plugins.memory', 'Memory')

class StoreTestCase(unittest.TestCase):

    def test_memory_store(self):
        g = rdflib.Graph("Memory")
        subj1 = rdflib.URIRef("http://example.org/foo#bar1")
        pred1 = rdflib.URIRef("http://example.org/foo#bar2")
        obj1 = rdflib.URIRef("http://example.org/foo#bar3")
        triple1 = (subj1, pred1, obj1)
        triple2 = (subj1, 
                   rdflib.URIRef("http://example.org/foo#bar4"),                
                   rdflib.URIRef("http://example.org/foo#bar5"))
        g.add(triple1)
        self.assert_(len(g) == 1)
        g.add(triple2)
        self.assert_(len(list(g.triples((subj1, None, None)))) == 2)
        self.assert_(len(list(g.triples((None, pred1, None)))) == 1)
        self.assert_(len(list(g.triples((None, None, obj1)))) == 1)
        g.remove(triple1)
        self.assert_(len(g) == 1)
        g.serialize()


if __name__ == '__main__':
    unittest.main(defaultTest='test_suite')

########NEW FILE########
__FILENAME__ = test_n3
from rdflib.term import Literal, URIRef
from rdflib.namespace import Namespace
from rdflib.plugins.parsers.notation3 import BadSyntax

from rdflib.py3compat import b

test_data = """
#  Definitions of terms describing the n3 model
#

@keywords a.

@prefix n3: <#>.
@prefix log: <log.n3#> .
@prefix rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix : <#> .

@forAll :s, :p, :x, :y, :z.

n3:Statement    a rdf:Class .
n3:StatementSet a rdf:Class .

n3:includes     a rdfs:Property .   # Cf rdf:li

n3:predicate    a rdf:Property; rdfs:domain n3:statement .
n3:subject      a rdf:Property; rdfs:domain n3:statement .
n3:object       a rdf:Property; rdfs:domain n3:statement .

n3:context      a rdf:Property; rdfs:domain n3:statement;
                rdfs:range n3:StatementSet .



########### Rules

{ :x :p :y . } log:means { [
                n3:subject :x;
                n3:predicate :p;
                n3:object :y ] a log:Truth}.

# Needs more thought ... ideally, we have the implcit AND rules of
# juxtaposition (introduction and elimination)

{
    {
        {  :x n3:includes :s. } log:implies { :y n3:includes :s. } .
    } forall :s1 .
} log:implies { :x log:implies :y } .

{
    {
        {  :x n3:includes :s. } log:implies { :y n3:includes :s. } .
    } forall :s1
} log:implies { :x log:implies :y } .

# I think n3:includes has to be axiomatic builtin. - unless you go to syntax description.
# syntax.n3?
"""


import unittest
from urllib2 import URLError

from rdflib.graph import Graph, ConjunctiveGraph


class TestN3Case(unittest.TestCase):

    def setUp(self):
        pass

    def tearDown(self):
        pass


    def testBaseCumulative(self):
        """
        Test that the n3 parser supports base declarations
        This is issue #22
        """

        input = """
@prefix : <http://example.com/> .
# default base
<foo> :name "Foo" .
# change it
@base <http://example.com/doc/> .
<bar> :name "Bar" .
# and change it more - they are cumulative
@base <doc2/> .
<bing> :name "Bing" .
# unless abosulute
@base <http://test.com/> .
<bong> :name "Bong" .

"""
        g = Graph()
        g.parse(data=input, format="n3")
        print list(g)
        self.assertTrue((None, None, Literal('Foo')) in g)
        self.assertTrue(
            (URIRef('http://example.com/doc/bar'), None, None) in g)
        self.assertTrue(
            (URIRef('http://example.com/doc/doc2/bing'), None, None) in g)
        self.assertTrue((URIRef('http://test.com/bong'), None, None) in g)

    def testBaseExplicit(self):
        """
        Test that the n3 parser supports resolving relative URIs
        and that base will override
        """

        input = """
@prefix : <http://example.com/> .
# default base
<foo> :name "Foo" .
# change it
@base <http://example.com/doc/> .
<bar> :name "Bar" .
"""
        g = Graph()
        g.parse(data=input, publicID='http://blah.com/', format="n3")
        print list(g)
        self.assertTrue(
            (URIRef('http://blah.com/foo'), None, Literal('Foo')) in g)
        self.assertTrue(
            (URIRef('http://example.com/doc/bar'), None, None) in g)

    def testBaseSerialize(self):
        g = Graph()
        g.add((URIRef('http://example.com/people/Bob'), URIRef(
            'urn:knows'), URIRef('http://example.com/people/Linda')))
        s = g.serialize(base='http://example.com/', format='n3')
        self.assertTrue(b('<people/Bob>') in s)
        g2 = ConjunctiveGraph()
        g2.parse(data=s, publicID='http://example.com/', format='n3')
        self.assertEqual(list(g), list(g2))

    def testIssue23(self):
        input = """<http://example.com/article1> <http://example.com/title> "this word is in \u201Cquotes\u201D"."""

        g = Graph()
        g.parse(data=input, format="n3")

        # Note difference in case of hex code, cwm allows lower-case
        input = """<http://example.com/article1> <http://example.com/title> "this word is in \u201cquotes\u201d"."""

        g.parse(data=input, format="n3")

    def testIssue29(self):
        input = """@prefix foo-bar: <http://example.org/> .

foo-bar:Ex foo-bar:name "Test" . """

        g = Graph()
        g.parse(data=input, format="n3")

    def testIssue68(self):
        input = """@prefix : <http://some.url/pome#>.\n\n:Brecon a :Place;\n\t:hasLord\n\t\t:Bernard_of_Neufmarch\xc3\xa9 .\n """

        g = Graph()
        g.parse(data=input, format="n3")

    def testIssue156(self):
        """
        Make sure n3 parser does not choke on UTF-8 BOM
        """
        g = Graph()
        g.parse("test/n3/issue156.n3", format="n3")

    def testDotInPrefix(self):
        g = Graph()
        g.parse(
            data="@prefix a.1: <http://example.org/> .\n a.1:cake <urn:x> <urn:y> . \n",
            format='n3')

    def testModel(self):
        g = ConjunctiveGraph()
        g.parse(data=test_data, format="n3")
        i = 0
        for s, p, o in g:
            if isinstance(s, Graph):
                i += 1
        self.assertEquals(i, 3)
        self.assertEquals(len(list(g.contexts())), 13)

        g.close()

    def testQuotedSerialization(self):
        g = ConjunctiveGraph()
        g.parse(data=test_data, format="n3")
        g.serialize(format="n3")

    def testParse(self):
        g = ConjunctiveGraph()
        try:
            g.parse(
                "http://groups.csail.mit.edu/dig/2005/09/rein/examples/troop42-policy.n3", format="n3")
        except URLError:
            from nose import SkipTest
            raise SkipTest(
                'No network to retrieve the information, skipping test')

    def testSingleQuotedLiterals(self):
        test_data = ["""@prefix : <#> . :s :p 'o' .""",
                     """@prefix : <#> . :s :p '''o''' ."""]

        for data in test_data:
            # N3 doesn't accept single quotes around string literals
            g = ConjunctiveGraph()
            self.assertRaises(BadSyntax, g.parse,
                              data=data, format='n3')

            g = ConjunctiveGraph()
            g.parse(data=data, format='turtle')
            self.assertEqual(len(g), 1)
            for _, _, o in g:
                self.assertEqual(o, Literal('o'))

    def testEmptyPrefix(self):

        # this is issue https://github.com/RDFLib/rdflib/issues/312
        g1 = Graph()
        g1.parse(data = ":a :b :c .", format='n3')

        g2 = Graph()
        g2.parse(data = "@prefix : <#> . :a :b :c .", format='n3')

        assert set(g1) == set(g2), 'Document with declared empty prefix must match default #'

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_n3_suite
import os
import sys

try:
    from testutils import check_serialize_parse
except:
    from test.testutils import check_serialize_parse

def _get_test_files_formats():
    skiptests = [
        'strquot.n3',
    ]
    for f in os.listdir('test/n3'):
        if f not in skiptests:
            fpath = "test/n3/"+f
            if f.endswith('.rdf'):
                yield fpath, 'xml'
            elif f.endswith('.n3'):
                yield fpath, 'n3'

def test_n3_writing():
    for fpath, fmt in _get_test_files_formats():
        yield check_serialize_parse, fpath, fmt, 'n3'

if __name__ == "__main__":
    if len(sys.argv) > 1:
        check_serialize_parse(sys.argv[1], 'n3','n3', True)
        sys.exit()
    else:
        import nose
        nose.main(defaultTest=__name__)

########NEW FILE########
__FILENAME__ = test_namespace
import unittest

from rdflib.graph import Graph
from rdflib.term import URIRef
from rdflib.py3compat import b

class NamespacePrefixTest(unittest.TestCase):

    def test_compute_qname(self):
        """Test sequential assignment of unknown prefixes"""
        g = Graph()
        self.assertEqual(g.compute_qname(URIRef("http://foo/bar/baz")),
            ("ns1", URIRef("http://foo/bar/"), "baz"))

        self.assertEqual(g.compute_qname(URIRef("http://foo/bar#baz")),
            ("ns2", URIRef("http://foo/bar#"), "baz"))
        
        # should skip to ns4 when ns3 is already assigned
        g.bind("ns3", URIRef("http://example.org/"))
        self.assertEqual(g.compute_qname(URIRef("http://blip/blop")),
            ("ns4", URIRef("http://blip/"), "blop"))

    def test_n3(self):
        g = Graph()
        g.add((URIRef("http://example.com/foo"),
               URIRef("http://example.com/bar"),
               URIRef("http://example.com/baz")))
        n3 = g.serialize(format="n3")
        # Gunnar disagrees that this is right:
        # self.assertTrue("<http://example.com/foo> ns1:bar <http://example.com/baz> ." in n3)
        # as this is much prettier, and ns1 is already defined: 
        self.assertTrue(b("ns1:foo ns1:bar ns1:baz .") in n3)

    def test_n32(self):
        # this test not generating prefixes for subjects/objects
        g = Graph()
        g.add((URIRef("http://example1.com/foo"),
               URIRef("http://example2.com/bar"),
               URIRef("http://example3.com/baz")))
        n3 = g.serialize(format="n3")

        self.assertTrue(b("<http://example1.com/foo> ns1:bar <http://example3.com/baz> .") in n3)


########NEW FILE########
__FILENAME__ = test_nodepickler
import unittest
import pickle

from rdflib.term import Literal

from rdflib.store import NodePickler

# same as nt/more_literals.nt
cases = [
    'no quotes',
    "single ' quote",
    'double " quote',
    'triple """ quotes',
    'mixed "\'""" quotes',
    '"',
    "'",
    '"\'"',
    '\\', # len 1
    '\\"', # len 2
    '\\\\"', # len 3
    '\\"\\', # len 3
    '<a some="typical" html="content">here</a>',
]


class UtilTestCase(unittest.TestCase):

    def test_to_bits_from_bits_round_trip(self):
        np = NodePickler()

        a = Literal(u'''A test with a \\n (backslash n), "\u00a9" , and newline \n and a second line.
''')
        b = np.loads(np.dumps(a))
        self.assertEquals(a, b)

    def test_literal_cases(self):
        np = NodePickler()

        for l in cases:
            a = Literal(l)
            b = np.loads(np.dumps(a))
            self.assertEquals(a, b)

    def test_pickle(self):
        np = NodePickler()
        dump = pickle.dumps(np)
        np2 = pickle.loads(dump)
        self.assertEquals(np._ids, np2._ids)
        self.assertEquals(np._objects,  np2._objects)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_nquads
import unittest
from rdflib import ConjunctiveGraph, URIRef, Namespace
from rdflib.py3compat import b

TEST_BASE = 'test/nquads.rdflib'

class NQuadsParserTest(unittest.TestCase):

    def _load_example(self):
        g = ConjunctiveGraph()
        data = open("test/nquads.rdflib/example.nquads", "rb")
        g.parse(data, format="nquads")
        return g

    def test_01_simple_open(self):
        g = self._load_example()
        assert len(g.store) == 449

    def test_02_contexts(self):
        # There should be 16 separate contexts
        g = self._load_example()
        assert len([x for x in g.store.contexts()]) == 16

    def test_03_get_value(self):
        # is the name of entity E10009 "Arco Publications"?
        # (in graph http://bibliographica.org/entity/E10009)
        # Looking for:
        # <http://bibliographica.org/entity/E10009>
        #       <http://xmlns.com/foaf/0.1/name>
        #       "Arco Publications"
        #       <http://bibliographica.org/entity/E10009>

        g = self._load_example()
        s = URIRef("http://bibliographica.org/entity/E10009")
        FOAF = Namespace("http://xmlns.com/foaf/0.1/")
        self.assertTrue(g.value(s, FOAF.name).eq("Arco Publications"))

    def test_context_is_optional(self):
        g = ConjunctiveGraph()
        data = open("test/nquads.rdflib/test6.nq", "rb")
        g.parse(data, format="nquads")
        assert len(g) > 0

    def test_serialize(self):
        g = ConjunctiveGraph()
        uri1 = URIRef("http://example.org/mygraph1")
        uri2 = URIRef("http://example.org/mygraph2")

        bob = URIRef(u'urn:bob')
        likes = URIRef(u'urn:likes')
        pizza = URIRef(u'urn:pizza')

        g.get_context(uri1).add((bob, likes, pizza))
        g.get_context(uri2).add((bob, likes, pizza))

        s = g.serialize(format='nquads')
        self.assertEqual(len([x for x in s.split(b("\n")) if x.strip()]), 2)

        g2 = ConjunctiveGraph()
        g2.parse(data=s, format='nquads')

        self.assertEqual(len(g), len(g2))
        self.assertEqual(sorted(x.identifier for x in g.contexts()),
                         sorted(x.identifier for x in g2.contexts()))


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_nquads_w3c
"""This runs the nquads tests for the W3C RDF Working Group's N-Quads
test suite."""

from rdflib import ConjunctiveGraph
from manifest import nose_tests, RDFT

from testutils import nose_tst_earl_report

verbose = False

def nquads(test):
    g = ConjunctiveGraph()

    try:
        g.parse(test.action, format='nquads')
        if not test.syntax:
            raise AssertionError("Input shouldn't have parsed!")
    except:
        if test.syntax:
            raise

testers = {
    RDFT.TestNQuadsPositiveSyntax: nquads,
    RDFT.TestNQuadsNegativeSyntax: nquads
}

def test_nquads(tests = None):
    for t in nose_tests(testers, 'test/w3c/nquads/manifest.ttl'):
        if tests:
            for test in tests:
                if test in t[1].uri: break
            else:
                continue

        yield t


if __name__ == '__main__':
    verbose = True

    nose_tst_earl_report(test_nquads, 'rdflib_nquads')

########NEW FILE########
__FILENAME__ = test_nt_misc
import unittest
import logging
import os
import re
from rdflib import Graph, Literal, URIRef
from rdflib.plugins.parsers import ntriples
from rdflib.py3compat import bytestype, b
log = logging.getLogger(__name__)

class NTTestCase(unittest.TestCase):

    def testIssue78(self):
        g = Graph()
        g.add((URIRef("foo"), URIRef("foo"), Literal(u"R\u00E4ksm\u00F6rg\u00E5s")))
        s = g.serialize(format='nt')
        self.assertEquals(type(s), bytestype)
        self.assertTrue(b(r"R\u00E4ksm\u00F6rg\u00E5s") in s)

    def testIssue146(self):
        g = Graph()
        g.add((URIRef("foo"), URIRef("foo"), Literal("test\n", lang="en")))
        s = g.serialize(format="nt").strip()
        self.assertEqual(s, b('<foo> <foo> "test\\n"@en .'))

    def test_sink(self):
        s = ntriples.Sink()
        self.assert_(s.length == 0)
        s.triple(None, None, None)
        self.assert_(s.length == 1)

    def test_nonvalidating_unquote(self):
        safe = """<http://example.org/alice/foaf.rdf#me> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://xmlns.com/foaf/0.1/Person> <http://example.org/alice/foaf1.rdf> ."""
        ntriples.validate = False
        res = ntriples.unquote(safe)
        self.assert_(isinstance(res, unicode))

    def test_validating_unquote(self):
        quot = """<http://example.org/alice/foaf.rdf#me> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://xmlns.com/foaf/0.1/Person> <http://example.org/alice/foaf1.rdf> ."""
        ntriples.validate = True
        res = ntriples.unquote(quot)
        # revert to default
        ntriples.validate = False
        log.debug("restype %s" % type(res))

    def test_validating_unquote_raises(self):
        ntriples.validate = True
        uniquot = """<http://www.w3.org/People/Berners-Lee/card#cm> <http://xmlns.com/foaf/0.1/name> "R\\u00E4ksm\\u00F6rg\\u00E5s" <http://www.w3.org/People/Berners-Lee/card> ."""
        self.assertRaises(ntriples.ParseError, ntriples.unquote, uniquot)
        uniquot = """<http://www.w3.org/People/Berners-Lee/card#cm> <http://xmlns.com/foaf/0.1/name> "R\\\\u00E4ksm\\u00F6rg\\u00E5s" <http://www.w3.org/People/Berners-Lee/card> ."""
        self.assertRaises(ntriples.ParseError, ntriples.unquote, uniquot)
        # revert to default
        ntriples.validate = False

    def test_nonvalidating_uriquote(self):
        ntriples.validate = False
        safe = """<http://example.org/alice/foaf.rdf#me> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://xmlns.com/foaf/0.1/Person> <http://example.org/alice/foaf1.rdf> ."""
        res = ntriples.uriquote(safe)
        self.assert_(res == safe)

    def test_validating_uriquote(self):
        ntriples.validate = True
        uniquot = """<http://www.w3.org/People/Berners-Lee/card#cm> <http://xmlns.com/foaf/0.1/name> "R\\u00E4ksm\\u00F6rg\\u00E5s" <http://www.w3.org/People/Berners-Lee/card> ."""
        res = ntriples.uriquote(uniquot)
        # revert to default
        ntriples.validate = False
        self.assertEqual(res, uniquot)

    def test_NTriplesParser_fpath(self):
        fpath = "test/nt/"+os.listdir('test/nt')[0]
        p = ntriples.NTriplesParser()
        self.assertRaises(ntriples.ParseError, p.parse, fpath)

    def test_NTriplesParser_parsestring(self):
        p = ntriples.NTriplesParser()
        data = 3
        self.assertRaises(ntriples.ParseError, p.parsestring, data)
        fname = "test/nt/lists-02.nt"
        data = open(fname, 'r').read()
        p = ntriples.NTriplesParser()
        res = p.parsestring(data)
        self.assert_(res == None)

    def test_w3_ntriple_variants(self):
        uri = "file:///"+os.getcwd()+"/test/nt/test.ntriples"
        import urllib
        parser = ntriples.NTriplesParser()
        u = urllib.urlopen(uri)
        sink = parser.parse(u)
        u.close()
        # ATM we are only really interested in any exceptions thrown
        self.assert_(sink is not None)

    def test_bad_line(self):
        data = '''<http://example.org/resource32> 3 <http://example.org/datatype1> .\n'''
        p = ntriples.NTriplesParser()
        self.assertRaises(ntriples.ParseError, p.parsestring, data)

    def test_cover_eat(self):
        data = '''<http://example.org/resource32> 3 <http://example.org/datatype1> .\n'''
        p = ntriples.NTriplesParser()
        p.line = data
        self.assertRaises(ntriples.ParseError, p.eat, re.compile('<http://example.org/datatype1>'))

    def test_cover_subjectobjectliteral(self):
        # data = '''<http://example.org/resource32> 3 <http://example.org/datatype1> .\n'''
        p = ntriples.NTriplesParser()
        p.line = "baz"
        self.assertRaises(ntriples.ParseError, p.subject)
        self.assertRaises(ntriples.ParseError, p.object)
        # p.line = '"baz"@fr^^<http://example.org/datatype1>'
        # self.assertRaises(ntriples.ParseError, p.literal)


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_nt_suite
import os
import traceback
import logging

log = logging.getLogger(__name__)


"""
Basic code for loading all NT files in test/nt folder
The actual tests are done in test_roundtrip
"""

def _get_test_files_formats():
    for f in os.listdir('test/nt'):
        fpath = "test/nt/"+f
        if f.endswith('.rdf'):
            yield fpath, 'xml'
        elif f.endswith('.nt'):
            yield fpath, 'nt'

def all_nt_files():
    skiptests = [        
        # illegal literal as subject
        'test/nt/literals-01.nt', 
        'test/nt/keywords-08.nt',
        'test/nt/paths-04.nt',
        'test/nt/numeric-01.nt',
        'test/nt/numeric-02.nt',
        'test/nt/numeric-03.nt',
        'test/nt/numeric-04.nt',
        'test/nt/numeric-05.nt',

        # illegal variables
        'test/nt/formulae-01.nt',
        'test/nt/formulae-02.nt',
        'test/nt/formulae-03.nt',
        'test/nt/formulae-05.nt',
        'test/nt/formulae-06.nt',
        'test/nt/formulae-10.nt',

        # illegal bnode as predicate
        'test/nt/paths-06.nt', 
        'test/nt/anons-02.nt',
        'test/nt/anons-03.nt', 
        'test/nt/qname-01.nt',
        'test/nt/lists-06.nt',
        ]
    [
        'test/nt/literals-02.nt', # this should work
        'test/nt/literals-05.nt', # this should work
        'test/nt/rdflibtest05.nt', # this should work
        ]
    for fpath, fmt in _get_test_files_formats():
        if fpath in skiptests:
            log.debug("Skipping %s, known issue" % fpath)
        else:
            yield fpath, fmt


########NEW FILE########
__FILENAME__ = test_nt_w3c
"""This runs the nt tests for the W3C RDF Working Group's N-Quads
test suite."""

from rdflib import Graph
from manifest import nose_tests, RDFT

from testutils import nose_tst_earl_report

verbose = False

def nt(test):
    g = Graph()

    try:
        g.parse(test.action, format='nt')
        if not test.syntax:
            raise AssertionError("Input shouldn't have parsed!")
    except:
        if test.syntax:
            raise

testers = {
    RDFT.TestNTriplesPositiveSyntax: nt,
    RDFT.TestNTriplesNegativeSyntax: nt
}

def test_nt(tests=None):
    for t in nose_tests(testers, 'test/w3c/nt/manifest.ttl', legacy=True):
        if tests:
            for test in tests:
                if test in t[1].uri: break
            else:
                continue

        yield t


if __name__ == '__main__':
    verbose = True

    nose_tst_earl_report(test_nt, 'rdflib_nt')

########NEW FILE########
__FILENAME__ = test_parser
import unittest

from rdflib.namespace import RDF, RDFS
from rdflib.term import URIRef
from rdflib.term import Literal
from rdflib.graph import Graph


class ParserTestCase(unittest.TestCase):
    backend = 'default'
    path = 'store'

    def setUp(self):
        self.graph = Graph(store=self.backend)
        self.graph.open(self.path)

    def tearDown(self):
        self.graph.close()

    def testNoPathWithHash(self):
        g = self.graph
        g.parse(data="""\
<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<rdf:RDF
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
  xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
>

<rdfs:Class rdf:about="http://example.org#">
  <rdfs:label>testing</rdfs:label>
</rdfs:Class>

</rdf:RDF>
""", publicID="http://example.org")

        subject = URIRef("http://example.org#")
        label = g.value(subject, RDFS.label)
        self.assertEquals(label, Literal("testing"))
        type = g.value(subject, RDF.type)
        self.assertEquals(type, RDFS.Class)


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_parser_helpers
from rdflib.plugins.sparql.parser import TriplesSameSubject
# from rdflib.plugins.sparql.algebra import triples


def pt(ts):
    for t in ts:
        print t


def test_1():

    t0 = TriplesSameSubject.parseString("[] ?p ?o ")
    print t0, len(t0)
    assert len(t0) % 3 == 0

    # t=BlankNodePropertyList.parseString("[ :p ?o ]")
    t1 = TriplesSameSubject.parseString("[ ?p ?o ]")
    print t1, len(t1)
    assert len(t1) % 3 == 0

    t2 = TriplesSameSubject.parseString("[ ?p1 ?o1 ] ?p2 ?o2 ")
    print t2, len(t2)
    assert len(t2) % 3 == 0

    t3 = TriplesSameSubject.parseString("?s ?p1 [ ] ")
    print t3, len(t3)
    assert len(t3) % 3 == 0

    t4 = TriplesSameSubject.parseString("?s ?p1 [ ?p2 ?o2 ] ")
    print t4, len(t4)
    assert len(t4) % 3 == 0

    t5 = TriplesSameSubject.parseString("[ ] ?p2 [ ] ")
    print t5, len(t5)
    assert len(t5) % 3 == 0

    t6 = TriplesSameSubject.parseString("[ ?p1 ?o1 ] ?p2 [ ?p3 ?o2 ] ")
    print t6, len(t6)
    assert len(t6) % 3 == 0

########NEW FILE########
__FILENAME__ = test_parser_structure
import rdflib.plugins.sparql.parser
import pprint


def t(q):
    print q
    pprint.pprint(rdflib.plugins.sparql.parser.parseQuery(q))


t("SELECT * WHERE { ?s ?p ?o, ?o2 ; ?p2 ?o3 . ?s2 ?p ?o .} ")


t("SELECT * WHERE { ?s ?p ?o, ?o2 ; ?p2 ?o3 ; ?p3 [ ?p ?o ] . ?s2 ?p ?o .} ")

########NEW FILE########
__FILENAME__ = test_path_div_future
from __future__ import division
from rdflib import URIRef
from rdflib.paths import Path

uri_tplt = "http://example.org/%s"

def test_path_div_future():
    path = URIRef(uri_tplt % "one") / URIRef(uri_tplt % "other")
    assert isinstance(path, Path)

########NEW FILE########
__FILENAME__ = test_prefixTypes
import unittest


from rdflib import Graph
from rdflib.py3compat import b

graph = Graph().parse(format='n3', data="""
@prefix dct: <http://purl.org/dc/terms/> .
@prefix foaf: <http://xmlns.com/foaf/0.1/> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .

<http://example.org/doc> a foaf:Document;
    dct:created "2011-03-20"^^xsd:date .
""")

class PrefixTypesTest(unittest.TestCase): 
    
    """N3/Turtle serializers should use prefixes, 
    also for types and datatypes

    This is issue 161
    http://code.google.com/p/rdflib/issues/detail?id=161
    """


    def test(self):
        s=graph.serialize(format='n3')
        print s
        self.assertTrue(b("foaf:Document") in s)
        self.assertTrue(b("xsd:date") in s)



if __name__ == '__main__':
    unittest.main()



########NEW FILE########
__FILENAME__ = test_preflabel
import unittest
import rdflib
from rdflib import ConjunctiveGraph
from rdflib import Literal
from rdflib.namespace import SKOS
from rdflib import RDFS
from rdflib import URIRef

class TestPrefLabel(unittest.TestCase):

    def setUp(self):
        self.g = ConjunctiveGraph()
        self.u = URIRef('http://example.com/foo')
        self.g.add([self.u, RDFS.label, Literal('foo')])
        self.g.add([self.u, RDFS.label, Literal('bar')])
    
    def test_default_label_sorting(self):
        res = sorted(self.g.preferredLabel(self.u))
        tgt = [(rdflib.term.URIRef('http://www.w3.org/2000/01/rdf-schema#label'), 
                rdflib.term.Literal(u'bar')), 
                (rdflib.term.URIRef('http://www.w3.org/2000/01/rdf-schema#label'), 
                rdflib.term.Literal(u'foo'))]
        self.assertEqual(res, tgt)

    def test_default_preflabel_sorting(self):
        self.g.add([self.u, SKOS.prefLabel, Literal('bla')])
        res = self.g.preferredLabel(self.u)
        tgt = [(rdflib.term.URIRef('http://www.w3.org/2004/02/skos/core#prefLabel'),
                rdflib.term.Literal(u'bla'))]
        self.assertEqual(res, tgt)

    def test_preflabel_lang_sorting_no_lang_attr(self):
        self.g.add([self.u, SKOS.prefLabel, Literal('bla')])
        self.g.add([self.u, SKOS.prefLabel, Literal('blubb', lang='en')])
        res = sorted(self.g.preferredLabel(self.u))
        tgt = [(rdflib.term.URIRef('http://www.w3.org/2004/02/skos/core#prefLabel'),
                rdflib.term.Literal(u'bla')),
               (rdflib.term.URIRef('http://www.w3.org/2004/02/skos/core#prefLabel'),
                rdflib.term.Literal(u'blubb', lang='en'))]

        self.assertEqual(res, tgt)

    def test_preflabel_lang_sorting_empty_lang_attr(self):
        self.g.add([self.u, SKOS.prefLabel, Literal('bla')])
        self.g.add([self.u, SKOS.prefLabel, Literal('blubb', lang='en')])
        res = self.g.preferredLabel(self.u, lang='')
        tgt = [(rdflib.term.URIRef('http://www.w3.org/2004/02/skos/core#prefLabel'),
                rdflib.term.Literal(u'bla'))]
        self.assertEqual(res, tgt)

    def test_preflabel_lang_sorting_en_lang_attr(self):
        self.g.add([self.u, SKOS.prefLabel, Literal('blubb', lang='en')])
        res = self.g.preferredLabel(self.u, lang='en')
        tgt = [(rdflib.term.URIRef('http://www.w3.org/2004/02/skos/core#prefLabel'),
                rdflib.term.Literal(u'blubb', lang='en'))]
        self.assertEqual(res, tgt)


########NEW FILE########
__FILENAME__ = test_prettyxml
# -*- coding: UTF-8 -*-
from rdflib.term import  URIRef, BNode, Literal
from rdflib.namespace import RDF, RDFS
from rdflib.py3compat import b

from rdflib.plugins.serializers.rdfxml import PrettyXMLSerializer

from rdflib.graph import ConjunctiveGraph
try:
    from io import BytesIO
except ImportError:
    from StringIO import StringIO as BytesIO


class SerializerTestBase(object):

    repeats = 8

    def setup(self):
        graph = ConjunctiveGraph()
        graph.parse(data=self.testContent, format=self.testContentFormat)
        self.sourceGraph = graph

    def test_serialize_and_reparse(self):
        reparsedGraph = serialize_and_load(self.sourceGraph, self.serializer)
        _assert_equal_graphs(self.sourceGraph, reparsedGraph)

    def test_multiple(self):
        """Repeats ``test_serialize`` ``self.repeats`` times, to reduce sucess based on in-memory ordering."""
        for i in range(self.repeats):
            self.test_serialize_and_reparse()

    #test_multiple.slowtest=True # not really slow?


def _assert_equal_graphs(g1, g2):
    assert len(g1) == len(g2), "Serialized graph not same size as source graph."
    g1copy = _mangled_copy(g1)
    g2copy = _mangled_copy(g2)
    g1copy -= _mangled_copy(g2)
    g2copy -= _mangled_copy(g1)
    assert len(g1copy) == 0, "Source graph larger than serialized graph."
    assert len(g2copy) == 0, "Serialized graph larger than source graph."

_blank = BNode()

def _mangled_copy(g):
    "Makes a copy of the graph, replacing all bnodes with the bnode ``_blank``."
    gcopy = ConjunctiveGraph()
    isbnode = lambda v: isinstance(v, BNode)
    for s, p, o in g:
        if isbnode(s): s = _blank
        if isbnode(p): p = _blank
        if isbnode(o): o = _blank
        gcopy.add((s, p, o))
    return gcopy


def serialize(sourceGraph, makeSerializer, getValue=True, extra_args={}):
    serializer = makeSerializer(sourceGraph)
    stream = BytesIO()
    serializer.serialize(stream, **extra_args)
    return getValue and stream.getvalue() or stream

def serialize_and_load(sourceGraph, makeSerializer):
    stream = serialize(sourceGraph, makeSerializer, False)
    stream.seek(0)
    reparsedGraph = ConjunctiveGraph()
    reparsedGraph.load(stream)
    return reparsedGraph


class TestPrettyXmlSerializer(SerializerTestBase):

    serializer = PrettyXMLSerializer

    testContent = """
        @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
        @prefix owl:  <http://www.w3.org/2002/07/owl#> .
        @prefix : <http://example.org/model/test#> .

        :value rdfs:domain :Test .

        :Test rdfs:subClassOf
            [ a owl:Restriction;
                owl:onProperty :value ],
            [ a owl:Restriction;
                owl:onProperty :name ] .

        <http://example.org/data/a> a :Test;
            rdfs:seeAlso <http://example.org/data/b>;
            :value "A" .

        <http://example.org/data/b>
            :name "Bee"@en, "Be"@sv;
            :value "B" .

        <http://example.org/data/c> a rdfs:Resource;
            rdfs:seeAlso <http://example.org/data/c>;
            :value 3 .

        <http://example.org/data/d> a rdfs:Resource;
            rdfs:seeAlso <http://example.org/data/c> ;
            rdfs:seeAlso <http://example.org/data/b> ;
            rdfs:seeAlso <http://example.org/data/a> .

        _:bnode1 a :BNode;
            rdfs:seeAlso _:bnode2 .

        _:bnode2 a :BNode ;
            rdfs:seeAlso _:bnode3 .

        _:bnode3 a :BNode ;
            rdfs:seeAlso _:bnode2 .

        """
    testContentFormat = 'n3'

    def test_result_fragments(self):
        rdfXml = serialize(self.sourceGraph, self.serializer)
        assert b('<Test rdf:about="http://example.org/data/a">') in rdfXml
        assert b('<rdf:Description rdf:about="http://example.org/data/b">') in rdfXml
        assert b('<name xml:lang="en">Bee</name>') in rdfXml
        assert b('<value rdf:datatype="http://www.w3.org/2001/XMLSchema#integer">3</value>') in rdfXml
        assert b('<BNode rdf:nodeID="') in rdfXml, "expected one identified bnode in serialized graph"
        #onlyBNodesMsg = "expected only inlined subClassOf-bnodes in serialized graph"
        #assert '<rdfs:subClassOf>' in rdfXml, onlyBNodesMsg
        #assert not '<rdfs:subClassOf ' in rdfXml, onlyBNodesMsg

    def test_result_fragments_with_base(self):
        rdfXml = serialize(self.sourceGraph, self.serializer, 
                    extra_args={'base':"http://example.org/", 'xml_base':"http://example.org/"})
        assert b('xml:base="http://example.org/"') in rdfXml
        assert b('<Test rdf:about="data/a">') in rdfXml
        assert b('<rdf:Description rdf:about="data/b">') in rdfXml
        assert b('<value rdf:datatype="http://www.w3.org/2001/XMLSchema#integer">3</value>') in rdfXml
        assert b('<BNode rdf:nodeID="') in rdfXml, "expected one identified bnode in serialized graph"

    def test_subClassOf_objects(self):
        reparsedGraph = serialize_and_load(self.sourceGraph, self.serializer)
        _assert_expected_object_types_for_predicates(reparsedGraph,
                [RDFS.seeAlso, RDFS.subClassOf],
                [URIRef, BNode])

    def test_pretty_xmlliteral(self):
        # given:
        g = ConjunctiveGraph()
        g.add((BNode(), RDF.value, Literal(u'''<p xmlns="http://www.w3.org/1999/xhtml">See also <a href="#aring"></a></p>''', datatype=RDF.XMLLiteral)))
        # when:
        xmlrepr = g.serialize(format='pretty-xml')
        # then:
        assert u'''<rdf:value rdf:parseType="Literal"><p xmlns="http://www.w3.org/1999/xhtml">See also <a href="#aring"></a></p></rdf:value>'''.encode('utf-8') in xmlrepr

    def test_pretty_broken_xmlliteral(self):
        # given:
        g = ConjunctiveGraph()
        g.add((BNode(), RDF.value, Literal(u'''<p ''', datatype=RDF.XMLLiteral)))
        # when:
        xmlrepr = g.serialize(format='pretty-xml')
        # then:
        assert u'''<rdf:value rdf:datatype="http://www.w3.org/1999/02/22-rdf-syntax-ns#XMLLiteral">&lt;p '''.encode('utf-8') in xmlrepr


def _assert_expected_object_types_for_predicates(graph, predicates, types):
    for s, p, o in graph:
        if p in predicates:
            someTrue = [isinstance(o, t) for t in types]
            assert True in someTrue, \
                    "Bad type %s for object when predicate is <%s>." % (type(o), p)



########NEW FILE########
__FILENAME__ = test_rdfxml
import unittest

import os
import os.path
from urllib import url2pathname
from urllib2 import urlopen

import rdflib
from rdflib import RDF, RDFS, URIRef, BNode, Literal, Namespace, Graph
from rdflib.exceptions import ParserError
from rdflib.util import first


import logging

_logger = logging.getLogger("parser_rdfcore")

verbose = 0

from encodings.utf_8 import StreamWriter

import sys
sw = StreamWriter(sys.stdout)


def write(msg):
    _logger.info(msg + "\n")
    # sw.write(msg+"\n")


class TestStore(Graph):
    __test__ = False

    def __init__(self, expected):
        super(TestStore, self).__init__()
        self.expected = expected

    def add(self, (s, p, o)):
        if not isinstance(s, BNode) and not isinstance(o, BNode):
            if not (s, p, o) in self.expected:
                m = u"Triple not in expected result: %s, %s, %s" % (
                    s.n3(), p.n3(), o.n3())
                if verbose:
                    write(m)
                # raise Exception(m)
        super(TestStore, self).add((s, p, o))


TEST = Namespace("http://www.w3.org/2000/10/rdf-tests/rdfcore/testSchema#")

CACHE_DIR = os.path.join(os.path.dirname(__file__), "rdf")

skipped = (
    # "datatypes/Manifest.rdf#test002",
    # "rdf-containers-syntax-vs-schema/Manifest.rdf#test004",
    # "rdfms-xml-literal-namespaces/Manifest.rdf#test001",
    # "rdfms-xml-literal-namespaces/Manifest.rdf#test002",
    # "rdfms-xmllang/Manifest.rdf#test001",
    # "rdfms-xmllang/Manifest.rdf#test002",
    # "xml-canon/Manifest.rdf#test001"
)


def cached_file(url):
    fname = url2pathname(relative(url))

    fpath = os.path.join(CACHE_DIR, fname)
    if not os.path.exists(fpath):
        print "%s does not exist, fetching from %s" % (fpath, url)
        folder = os.path.dirname(fpath)
        if not os.path.exists(folder):
            os.makedirs(folder)
        f = open(fpath, 'w')
        try:
            f.write(urlopen(url).read())
        finally:
            f.close()
    return fpath


RDFCOREBASE = "http://www.w3.org/2000/10/rdf-tests/rdfcore/"


def relative(url):
    return url[len(RDFCOREBASE):]


def resolve(rel):
    return RDFCOREBASE + rel


def _testPositive(uri, manifest):
    if verbose:
        write(u"TESTING: %s" % uri)
    result = 0  # 1=failed, 0=passed
    inDoc = first(manifest.objects(uri, TEST["inputDocument"]))
    outDoc = first(manifest.objects(uri, TEST["outputDocument"]))
    expected = Graph()
    if outDoc[-3:] == ".nt":
        format = "nt"
    else:
        format = "xml"
    expected.parse(cached_file(outDoc), publicID=outDoc, format=format)
    store = TestStore(expected)
    if inDoc[-3:] == ".nt":
        format = "nt"
    else:
        format = "xml"

    try:
        store.parse(cached_file(inDoc), publicID=inDoc, format=format)
    except ParserError, pe:
        write("Failed '")
        write(inDoc)
        write("' failed with")
        raise pe
        try:
            write(type(pe))
        except:
            write("sorry could not dump out error.")
        result = 1
    else:
        if not store.isomorphic(expected):
            write(u"""Failed: '%s'""" % uri)
            if verbose:
                write("""  In:\n""")
                for s, p, o in store:
                    write("%s %s %s." % (repr(s), repr(p), repr(o)))
                write("""  Out:\n""")
                for s, p, o in expected:
                    write("%s %s %s." % (repr(s), repr(p), repr(o)))
            result += 1
    return result


def _testNegative(uri, manifest):
    if verbose:
        write(u"TESTING: %s" % uri)
    result = 0  # 1=failed, 0=passed
    inDoc = first(manifest.objects(uri, TEST["inputDocument"]))
    store = Graph()

    test = BNode()
    results.add((test, RESULT["test"], uri))
    results.add((test, RESULT["system"], system))

    try:
        if inDoc[-3:] == ".nt":
            format = "nt"
        else:
            format = "xml"
        store.parse(cached_file(inDoc), publicID=inDoc, format=format)
    except ParserError:
        results.add((test, RDF.type, RESULT["PassingRun"]))
        # pass
    else:
        write(u"""Failed: '%s'""" % uri)
        results.add((test, RDF.type, RESULT["FailingRun"]))
        result = 1
    return result


class ParserTestCase(unittest.TestCase):
    store = 'default'
    path = 'store'
    slow = True

    def setUp(self):
        self.manifest = manifest = Graph(store=self.store)
        manifest.open(self.path)
        manifest.load(cached_file(
            "http://www.w3.org/2000/10/rdf-tests/rdfcore/Manifest.rdf"))

    def tearDown(self):
        self.manifest.close()

    def testNegative(self):
        manifest = self.manifest
        num_failed = total = 0
        negs = list(manifest.subjects(RDF.type, TEST["NegativeParserTest"]))
        negs.sort()
        for neg in negs:
            status = first(manifest.objects(neg, TEST["status"]))
            if status == Literal("APPROVED"):
                result = _testNegative(neg, manifest)
                total += 1
                num_failed += result
        self.assertEquals(
            num_failed, 0, "Failed: %s of %s." % (num_failed, total))

    def testPositive(self):
        manifest = self.manifest
        uris = list(manifest.subjects(RDF.type, TEST["PositiveParserTest"]))
        uris.sort()
        num_failed = total = 0
        for uri in uris:
            status = first(manifest.objects(uri, TEST["status"]))
            # Failing tests, skipped
            if uri[44:] in skipped:
                status = Literal("Locally DISAPPROVED")
                write("Skipping %s" % uri)
            if status == Literal("APPROVED"):
                result = _testPositive(uri, manifest)
                test = BNode()
                results.add((test, RESULT["test"], uri))
                results.add((test, RESULT["system"], system))
                if not result:
                    results.add((test, RDF.type, RESULT["PassingRun"]))
                else:
                    results.add((test, RDF.type, RESULT["FailingRun"]))
                total += 1
                num_failed += result
        self.assertEquals(
            num_failed, 0, "Failed: %s of %s." % (num_failed, total))

RESULT = Namespace("http://www.w3.org/2002/03owlt/resultsOntology#")
FOAF = Namespace("http://xmlns.com/foaf/0.1/")


results = Graph()

system = BNode("system")
results.add((system, FOAF["homepage"], URIRef("http://rdflib.net/")))
results.add((system, RDFS.label, Literal("RDFLib")))
results.add((system, RDFS.comment, Literal("")))


if __name__ == "__main__":
    manifest = Graph()
    manifest.load(cached_file(
        "http://www.w3.org/2000/10/rdf-tests/rdfcore/Manifest.rdf"))
    import sys
    import getopt
    try:
        optlist, args = getopt.getopt(sys.argv[1:], 'h:', ["help"])
    except getopt.GetoptError, msg:
        write(msg)
        # usage()
        
    try:
        argv = sys.argv
        if len(argv)>1:
            _logger.setLevel(logging.INFO)
            _logger.addHandler(logging.StreamHandler())

        for arg in argv[1:]:            
            verbose = 1
            case = URIRef(arg)
            write(u"Testing: %s" % case)
            if (case, RDF.type, TEST["PositiveParserTest"]) in manifest:
                result = _testPositive(case, manifest)                
                write(u"Positive test %s" % ["PASSED", "FAILED"][result])
            elif (case, RDF.type, TEST["NegativeParserTest"]) in manifest:
                result = _testNegative(case, manifest)
                write(u"Negative test %s" % ["PASSED", "FAILED"][result])
            else:
                write(u"%s not ??" % case)

        if len(argv) <= 1:
            unittest.main()
    finally:
        results.serialize("results.rdf")

########NEW FILE########
__FILENAME__ = test_rdf_lists
# import os, sys, string
import unittest

from rdflib.graph import Graph
from rdflib.term import URIRef


DATA=\
"""<http://example.com#C> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://www.w3.org/2002/07/owl#Class>.
<http://example.com#B> <http://www.w3.org/2000/01/rdf-schema#subClassOf> _:fIYNVPxd4.
<http://example.com#B> <http://www.w3.org/2000/01/rdf-schema#subClassOf> <http://example.com#A>.
<http://example.com#B> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://www.w3.org/2002/07/owl#Class>.
<http://example.com#p1> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://www.w3.org/2002/07/owl#ObjectProperty>.
<http://example.com#A> <http://www.w3.org/2002/07/owl#unionOf> _:fIYNVPxd3.
<http://example.com#A> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://www.w3.org/2002/07/owl#Class>.
_:fIYNVPxd4 <http://www.w3.org/2002/07/owl#allValuesFrom> <http://example.com#C>.
_:fIYNVPxd4 <http://www.w3.org/2002/07/owl#onProperty> <http://example.com#p1>.
_:fIYNVPxd4 <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://www.w3.org/2002/07/owl#Restriction>.
_:fIYNVPxd3 <http://www.w3.org/1999/02/22-rdf-syntax-ns#first> <http://example.com#B>.
_:fIYNVPxd3 <http://www.w3.org/1999/02/22-rdf-syntax-ns#rest> <http://www.w3.org/1999/02/22-rdf-syntax-ns#nil>.
"""

DATA_FALSE_ELEMENT=\
"""
<http://example.org/#ThreeMemberList> <http://www.w3.org/1999/02/22-rdf-syntax-ns#first> <http://example.org/#p> .
<http://example.org/#ThreeMemberList> <http://www.w3.org/1999/02/22-rdf-syntax-ns#rest> _:list2 .
_:list2 <http://www.w3.org/1999/02/22-rdf-syntax-ns#first> "false"^^<http://www.w3.org/2001/XMLSchema#boolean> .
_:list2 <http://www.w3.org/1999/02/22-rdf-syntax-ns#rest> _:list3 .
_:list3 <http://www.w3.org/1999/02/22-rdf-syntax-ns#first> <http://example.org/#r> .
_:list3 <http://www.w3.org/1999/02/22-rdf-syntax-ns#rest> <http://www.w3.org/1999/02/22-rdf-syntax-ns#nil> .
"""

def main():
    unittest.main()


class OWLCollectionTest(unittest.TestCase):

    def testCollectionRDFXML(self):
        g=Graph().parse(data=DATA, format='nt')
        g.namespace_manager.bind('owl',URIRef('http://www.w3.org/2002/07/owl#'))
        print g.serialize(format='pretty-xml')


class ListTest(unittest.TestCase):
    def testFalseElement(self):
        g=Graph().parse(data=DATA_FALSE_ELEMENT, format='nt')
        self.assertEquals(len(list(g.items(URIRef('http://example.org/#ThreeMemberList')))), 3)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = test_roundtrip
import sys
import rdflib
import rdflib.compare

try:
    from test_nt_suite import all_nt_files
    assert all_nt_files
except:
    from test.test_nt_suite import all_nt_files

"""
Test round-tripping by all serializers/parser that are registerd.
This means, you may test more than just core rdflib!

run with no arguments to test all formats + all files
run with a single argument, to test only that format, i.e. "n3"
run with three arguments to test round-tripping in a given format
and reading a single file in the given format, i.e.:

python test/test_roundtrip.py xml nt test/nt/literals-02.nt

tests roundtripping through rdf/xml with only the literals-02 file

"""


SKIP = [
    ('xml', 'test/nt/qname-02.nt'),  # uses a property that cannot be qname'd
    ('application/rdf+xml', 'test/nt/qname-02.nt'),  # uses a property that cannot be qname'd
]


def roundtrip(e, verbose=False):
    infmt, testfmt, source = e

    g1 = rdflib.ConjunctiveGraph()

    g1.parse(source, format=infmt)

    s = g1.serialize(format=testfmt)

    if verbose:
        print "S:"
        print s

    g2 = rdflib.ConjunctiveGraph()
    g2.parse(data=s, format=testfmt)

    if verbose:
        both, first, second = rdflib.compare.graph_diff(g1,g2)
        print "Diff:"
        print "%d triples in both"%len(both)
        print "G1 Only:"
        for t in first:
            print t

        print "--------------------"
        print "G2 Only"
        for t in second:
            print t

    assert rdflib.compare.isomorphic(g1, g2)

    if verbose: print "Ok!"


formats = None


def test_cases():
    global formats
    if not formats:
        serializers = set(
            x.name for x in rdflib.plugin.plugins(
                None, rdflib.plugin.Serializer))
        parsers = set(
            x.name for x in rdflib.plugin.plugins(
                None, rdflib.plugin.Parser))
        formats = parsers.intersection(serializers)

    for testfmt in formats:
        if "/" in testfmt: continue # skip double testing
        for f, infmt in all_nt_files():
            if (testfmt, f) not in SKIP:
                yield roundtrip, (infmt, testfmt, f)


if __name__ == "__main__":
    import nose
    if len(sys.argv) == 1:
        nose.main(defaultTest=sys.argv[0])
    elif len(sys.argv) == 2:
        import test.test_roundtrip
        test.test_roundtrip.formats = [sys.argv[1]]
        nose.main(defaultTest=sys.argv[0], argv=sys.argv[:1])
    else:
        roundtrip(
            (sys.argv[2], sys.argv[1], sys.argv[3]), verbose=True)

########NEW FILE########
__FILENAME__ = test_rules
import unittest
from tempfile import mkdtemp
import shutil

from rdflib.term import URIRef
from rdflib.term import BNode
from rdflib.term import Literal
from rdflib.term import Variable
from rdflib.namespace import Namespace
from rdflib.graph import Graph

LOG = Namespace("http://www.w3.org/2000/10/swap/log#")


try:
    from pychinko import terms
    from pychinko.interpreter import Interpreter

    def _convert(node):
        if isinstance(node, Variable):
            return terms.Variable(node)
            #return node
        elif isinstance(node, BNode):
            return terms.Exivar(node)
        elif isinstance(node, URIRef):
            #return terms.URI(node)
            return node
        elif isinstance(node, Literal):
            return node
        else:
            raise Exception("Unexpected Type: %s" % type(node))

    def patterns(g):
        for s, p, o in g:
            yield terms.Pattern(_convert(s), _convert(p), _convert(o))

    def facts(g):
        for s, p, o in g:
            if p!=LOG.implies and not isinstance(s, BNode) and not isinstance(o, BNode):
                yield terms.Fact(_convert(s), _convert(p), _convert(o))

    class PychinkoTestCase(unittest.TestCase):
        backend = 'default'
        tmppath = None
        def setUp(self):
            self.g = Graph(store=self.backend)
            self.tmppath = mkdtemp()
            self.g.open(configuration=self.tmppath)
            self.g.parse("test/a.n3", format="n3")

        def tearDown(self):
            self.g.close()
            shutil.rmtree(tmppath)

        def testPychinko(self):
            rules = []
            for s, p, o in self.g.triples((None, LOG.implies, None)):
                lhs = list(patterns(s))
                rhs = list(patterns(o))
                rules.append(terms.Rule(lhs, rhs, (s, p, o)))
            interp = Interpreter(rules)
            f = Graph()
            f.parse("http://eikeon.com/")
            source = f
            source = self.g
            interp.addFacts(set(facts(source)), initialSet=True)
            interp.run()
            #_logger.debug("inferred facts: %s" % interp.inferredFacts)

except ImportError, e:
    print "Could not test Pychinko: %s" % e



########NEW FILE########
__FILENAME__ = test_seq
import unittest

from rdflib.term import URIRef
from rdflib.graph import Graph

s = """\
<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:nzgls="http://www.nzgls.govt.nz/standard/"
>
 <rdf:Seq rdf:about="http://example.org/Seq">
   <rdf:li rdf:resource="http://example.org/one" />
   <rdf:li rdf:resource="http://example.org/two" />
   <rdf:li rdf:resource="http://example.org/three" />
   <rdf:li rdf:resource="http://example.org/four" />
   <rdf:li rdf:resource="http://example.org/five_five" />
   <rdf:li rdf:resource="http://example.org/six" />
 </rdf:Seq>
</rdf:RDF>
"""



class SeqTestCase(unittest.TestCase):
    backend = 'default'
    path = 'store'

    def setUp(self):
        store = self.store = Graph(store=self.backend)
        store.open(self.path)
        store.parse(data=s)

    def tearDown(self):
        self.store.close()

    def testSeq(self):
        items = self.store.seq(URIRef("http://example.org/Seq"))
        self.assertEquals(len(items), 6)
        self.assertEquals(items[-1], URIRef("http://example.org/six"))
        self.assertEquals(items[2], URIRef("http://example.org/three"))
        # just make sure we can serialize
        self.store.serialize()

def test_suite():
    return unittest.makeSuite(SeqTestCase)

if __name__ == '__main__':
    unittest.main(defaultTest='test_suite')


########NEW FILE########
__FILENAME__ = test_serializexml
from rdflib.term import  URIRef, BNode
from rdflib.namespace import RDFS
from rdflib.py3compat import b

from rdflib.plugins.serializers.rdfxml import XMLSerializer

from rdflib.graph import ConjunctiveGraph
try:
    from io import BytesIO
except ImportError:
    from StringIO import StringIO as BytesIO


class SerializerTestBase(object):

    repeats = 8

    def setup(self):
        graph = ConjunctiveGraph()
        graph.parse(data=self.testContent, format=self.testContentFormat)
        self.sourceGraph = graph

    def test_serialize_and_reparse(self):
        reparsedGraph = serialize_and_load(self.sourceGraph, self.serializer)
        _assert_equal_graphs(self.sourceGraph, reparsedGraph)

    def test_multiple(self):
        """Repeats ``test_serialize`` ``self.repeats`` times, to reduce sucess based on in-memory ordering."""
        for i in range(self.repeats):
            self.test_serialize_and_reparse()

    #test_multiple.slowtest=True # not really slow?


def _assert_equal_graphs(g1, g2):
    assert len(g1) == len(g2), "Serialized graph not same size as source graph."
    g1copy = _mangled_copy(g1)
    g2copy = _mangled_copy(g2)
    g1copy -= _mangled_copy(g2)
    g2copy -= _mangled_copy(g1)
    assert len(g1copy) == 0, "Source graph larger than serialized graph."
    assert len(g2copy) == 0, "Serialized graph larger than source graph."

_blank = BNode()

def _mangled_copy(g):
    "Makes a copy of the graph, replacing all bnodes with the bnode ``_blank``."
    gcopy = ConjunctiveGraph()
    isbnode = lambda v: isinstance(v, BNode)
    for s, p, o in g:
        if isbnode(s): s = _blank
        if isbnode(p): p = _blank
        if isbnode(o): o = _blank
        gcopy.add((s, p, o))
    return gcopy


def serialize(sourceGraph, makeSerializer, getValue=True, extra_args={}):
    serializer = makeSerializer(sourceGraph)
    stream = BytesIO()
    serializer.serialize(stream, **extra_args)
    return getValue and stream.getvalue() or stream

def serialize_and_load(sourceGraph, makeSerializer):
    stream = serialize(sourceGraph, makeSerializer, False)
    stream.seek(0)
    reparsedGraph = ConjunctiveGraph()
    reparsedGraph.load(stream)
    return reparsedGraph


class TestXMLSerializer(SerializerTestBase):

    serializer = XMLSerializer

    testContent = """
        @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
        @prefix owl:  <http://www.w3.org/2002/07/owl#> .
        @prefix : <http://example.org/model/test#> .

        :value rdfs:domain :Test .

        :Test rdfs:subClassOf
            [ a owl:Restriction;
                owl:onProperty :value ],
            [ a owl:Restriction;
                owl:onProperty :name ] .

        <http://example.org/data/a> a :Test;
            rdfs:seeAlso <http://example.org/data/b>;
            :value "A" .

        <http://example.org/data/b>
            :name "Bee"@en, "Be"@sv;
            :value "B" .

        <http://example.org/data/c> a rdfs:Resource;
            rdfs:seeAlso <http://example.org/data/c>;
            :value 3 .

        <http://example.org/data/d> a rdfs:Resource;
            rdfs:seeAlso <http://example.org/data/c> ;
            rdfs:seeAlso <http://example.org/data/b> ;
            rdfs:seeAlso <http://example.org/data/a> .

        _:bnode1 a :BNode;
            rdfs:seeAlso _:bnode2 .

        _:bnode2 a :BNode ;
            rdfs:seeAlso _:bnode3 .

        _:bnode3 a :BNode ;
            rdfs:seeAlso _:bnode2 .

        """
    testContentFormat = 'n3'

    def test_result_fragments(self):
        rdfXml = serialize(self.sourceGraph, self.serializer)
        #print "--------"
        #print rdfXml
        #print "--------"
        assert b('<rdf:Description rdf:about="http://example.org/data/a">') in rdfXml
        assert b('<rdf:type rdf:resource="http://example.org/model/test#Test"/>') in rdfXml
        assert b('<rdf:Description rdf:about="http://example.org/data/b">') in rdfXml
        assert b('<name xml:lang="en">Bee</name>') in rdfXml
        assert b('<value rdf:datatype="http://www.w3.org/2001/XMLSchema#integer">3</value>') in rdfXml
        assert b('<rdf:Description rdf:nodeID="') in rdfXml, "expected one identified bnode in serialized graph"

    def test_result_fragments_with_base(self):
        rdfXml = serialize(self.sourceGraph, self.serializer, 
                    extra_args={'base':"http://example.org/", 'xml_base':"http://example.org/"})
        #print "--------"
        #print rdfXml
        #print "--------"
        assert b('xml:base="http://example.org/"') in rdfXml
        assert b('<rdf:Description rdf:about="data/a">') in rdfXml
        assert b('<rdf:type rdf:resource="model/test#Test"/>') in rdfXml
        assert b('<rdf:Description rdf:about="data/b">') in rdfXml
        assert b('<value rdf:datatype="http://www.w3.org/2001/XMLSchema#integer">3</value>') in rdfXml
        assert b('<rdf:Description rdf:nodeID="') in rdfXml, "expected one identified bnode in serialized graph"

    def test_subClassOf_objects(self):
        reparsedGraph = serialize_and_load(self.sourceGraph, self.serializer)
        _assert_expected_object_types_for_predicates(reparsedGraph,
                [RDFS.seeAlso, RDFS.subClassOf],
                [URIRef, BNode])

def _assert_expected_object_types_for_predicates(graph, predicates, types):
    for s, p, o in graph:
        if p in predicates:
            someTrue = [isinstance(o, t) for t in types]
            assert True in someTrue, \
                    "Bad type %s for object when predicate is <%s>." % (type(o), p)



########NEW FILE########
__FILENAME__ = test_slice

from rdflib import Graph, URIRef
import unittest

class GraphSlice(unittest.TestCase):

    def testSlice(self):
        """
         We pervert the slice object, 
         and use start, stop, step as subject, predicate, object

         all operations return generators over full triples 
        """

        sl=lambda x,y: self.assertEquals(len(list(x)),y)
        soe=lambda x,y: self.assertEquals(set([a[2] for a in x]),set(y)) # equals objects
        g=self.graph
         
        # Single terms are all trivial:

        # single index slices by subject, i.e. return triples((x,None,None))
        # tell me everything about "tarek"
        sl(g[self.tarek],2)
        
        # single slice slices by s,p,o, with : used to split
        # tell me everything about "tarek" (same as above)
        sl(g[self.tarek::],2)

        # give me every "likes" relationship
        sl(g[:self.likes:],5)

        # give me every relationship to pizza
        sl(g[::self.pizza],3)

        # give me everyone who likes pizza
        sl(g[:self.likes:self.pizza],2)
       
        # does tarek like pizza?
        self.assertTrue(g[self.tarek:self.likes:self.pizza])

        # More intesting is using paths

        # everything hated or liked
        sl(g[:self.hates|self.likes], 7)
        
        

        

    def setUp(self):
        self.graph = Graph()

        self.michel = URIRef(u'michel')
        self.tarek = URIRef(u'tarek')
        self.bob = URIRef(u'bob')
        self.likes = URIRef(u'likes')
        self.hates = URIRef(u'hates')
        self.pizza = URIRef(u'pizza')
        self.cheese = URIRef(u'cheese')

        self.addStuff()

    def addStuff(self):
        tarek = self.tarek
        michel = self.michel
        bob = self.bob
        likes = self.likes
        hates = self.hates
        pizza = self.pizza
        cheese = self.cheese

        self.graph.add((tarek, likes, pizza))
        self.graph.add((tarek, likes, cheese))
        self.graph.add((michel, likes, pizza))
        self.graph.add((michel, likes, cheese))
        self.graph.add((bob, likes, cheese))
        self.graph.add((bob, hates, pizza))
        self.graph.add((bob, hates, michel)) # gasp!


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_sparql
from rdflib import Graph, URIRef, Literal
from rdflib.plugins.sparql import prepareQuery
from rdflib.compare import isomorphic

from nose.tools import eq_

def test_graph_prefix():
    """
    This is issue https://github.com/RDFLib/rdflib/issues/313
    """

    g1 = Graph()
    g1.parse(data="""
    @prefix : <urn:ns1:> .
    :foo <p> 42.
    """, format="n3")

    g2 = Graph()
    g2.parse(data="""
    @prefix : <urn:somethingelse:> .
    <urn:ns1:foo> <p> 42.
    """, format="n3")

    assert isomorphic(g1, g2)

    q_str = ("""
    PREFIX : <urn:ns1:>
    SELECT ?val
    WHERE { :foo ?p ?val }
    """)
    q_prepared = prepareQuery(q_str)

    expected = [(Literal(42),)]

    eq_(list(g1.query(q_prepared)), expected)
    eq_(list(g2.query(q_prepared)), expected)

    eq_(list(g1.query(q_str)), expected)
    eq_(list(g2.query(q_str)), expected)

def test_variable_order():

    g = Graph()
    g.add((URIRef("http://foo"),URIRef("http://bar"),URIRef("http://baz")))
    res = g.query("SELECT (42 AS ?a) ?b { ?b ?c ?d }")

    row = list(res)[0]
    print row
    assert len(row) == 2
    assert row[0] == Literal(42)
    assert row[1] == URIRef("http://foo")


def test_sparql_bnodelist():
    """

    syntax tests for a few corner-cases not touched by the
    official tests.

    """

    prepareQuery('select * where { ?s ?p ( [] ) . }')
    prepareQuery('select * where { ?s ?p ( [ ?p2 ?o2 ] ) . }')
    prepareQuery('select * where { ?s ?p ( [ ?p2 ?o2 ] [] ) . }')
    prepareQuery('select * where { ?s ?p ( [] [ ?p2 ?o2 ] [] ) . }')


if __name__ == '__main__':
    import nose
    nose.main(defaultTest=__name__)

########NEW FILE########
__FILENAME__ = test_sparqlstore
import os
import unittest
from nose import SkipTest

try:
    import SPARQLWrapper
except ImportError:
    raise SkipTest("SPARQLWrapper not installed")

if os.getenv("TRAVIS"): 
    raise SkipTest("Doesn't work in travis")

import urllib2
try:
    assert len(urllib2.urlopen("http://dbpedia.org/sparql").read()) > 0
except:
    raise SkipTest("No HTTP connection.")

from rdflib import Graph, URIRef, Literal


class SPARQLStoreDBPediaTestCase(unittest.TestCase):
    store_name = 'SPARQLStore'
    path = "http://dbpedia.org/sparql"
    storetest = True
    create = False

    def setUp(self):
        self.graph = Graph(store="SPARQLStore")
        self.graph.open(self.path, create=self.create)
        ns = list(self.graph.namespaces())
        assert len(ns) > 0, ns

    def tearDown(self):
        self.graph.close()

    def test_Query(self):
        query = "select distinct ?Concept where {[] a ?Concept} LIMIT 1"
        res = self.graph.query(query, initNs={})
        for i in res:
            assert type(i[0]) == URIRef, i[0].n3()

    def test_initNs(self):
        query = """\
        SELECT ?label WHERE
            { ?s a xyzzy:Concept ; xyzzy:prefLabel ?label . } LIMIT 10
        """
        res = self.graph.query(
            query,
            initNs={"xyzzy": "http://www.w3.org/2004/02/skos/core#"})
        for i in res:
            assert type(i[0]) == Literal, i[0].n3()

    def test_noinitNs(self):
        query = """\
        SELECT ?label WHERE
            { ?s a xyzzy:Concept ; xyzzy:prefLabel ?label . } LIMIT 10
        """
        self.assertRaises(
            SPARQLWrapper.Wrapper.QueryBadFormed,
            self.graph.query,
            query)

    def test_query_with_added_prolog(self):
        prologue = """\
        PREFIX xyzzy: <http://www.w3.org/2004/02/skos/core#>
        """
        query = """\
        SELECT ?label WHERE
            { ?s a xyzzy:Concept ; xyzzy:prefLabel ?label . } LIMIT 10
        """
        res = self.graph.query(prologue + query)
        for i in res:
            assert type(i[0]) == Literal, i[0].n3()


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_sparqlupdatestore

from rdflib import ConjunctiveGraph, URIRef

import unittest

# this assumed SPARQL1.1 query/update endpoints
# running locally at localhost:3030/data
# for instance fuseki started with ./fuseki-server --mem --update /data

# THIS WILL DELETE ALL DATA IN THE /data dataset

michel = URIRef(u'urn:michel')
tarek = URIRef(u'urn:tarek')
bob = URIRef(u'urn:bob')
likes = URIRef(u'urn:likes')
hates = URIRef(u'urn:hates')
pizza = URIRef(u'urn:pizza')
cheese = URIRef(u'urn:cheese')

graphuri = URIRef('urn:graph')
othergraphuri = URIRef('urn:othergraph')


class TestSparql11(unittest.TestCase):

    def setUp(self):
        self.longMessage = True
        self.graph = ConjunctiveGraph('SPARQLUpdateStore')

        root = "http://localhost:3030/ukpp/"
        self.graph.open((root + "sparql", root + "update"))

        # clean out the store
        for c in self.graph.contexts():
            c.remove((None, None, None))
            assert len(c) == 0

    def tearDown(self):
        self.graph.close()

    def testSimpleGraph(self):
        g = self.graph.get_context(graphuri)
        g.add((tarek, likes, pizza))
        g.add((bob, likes, pizza))
        g.add((bob, likes, cheese))

        g2 = self.graph.get_context(othergraphuri)
        g2.add((michel, likes, pizza))

        self.assertEquals(3, len(g), 'graph contains 3 triples')
        self.assertEquals(1, len(g2), 'other graph contains 1 triple')

        r = g.query("SELECT * WHERE { ?s <urn:likes> <urn:pizza> . }")
        self.assertEquals(2, len(list(r)), "two people like pizza")

        r = g.triples((None, likes, pizza))
        self.assertEquals(2, len(list(r)), "two people like pizza")

        # Test initBindings
        r = g.query("SELECT * WHERE { ?s <urn:likes> <urn:pizza> . }",
                    initBindings={'s': tarek})
        self.assertEquals(1, len(list(r)), "i was asking only about tarek")

        r = g.triples((tarek, likes, pizza))
        self.assertEquals(1, len(list(r)), "i was asking only about tarek")

        r = g.triples((tarek, likes, cheese))
        self.assertEquals(0, len(list(r)), "tarek doesn't like cheese")

        g2.add((tarek, likes, pizza))
        g.remove((tarek, likes, pizza))
        r = g.query("SELECT * WHERE { ?s <urn:likes> <urn:pizza> . }")
        self.assertEquals(1, len(list(r)), "only bob likes pizza")

    def testConjunctiveDefault(self):
        g = self.graph.get_context(graphuri)
        g.add((tarek, likes, pizza))
        g2 = self.graph.get_context(othergraphuri)
        g2.add((bob, likes, pizza))
        g.add((tarek, hates, cheese))

        self.assertEquals(2, len(g), 'graph contains 2 triples')
        self.assertEquals(3, len(self.graph),
                          'default union graph contains three triples')

        r = self.graph.query("SELECT * WHERE { ?s <urn:likes> <urn:pizza> . }")
        self.assertEquals(2, len(list(r)), "two people like pizza")

        r = self.graph.query("SELECT * WHERE { ?s <urn:likes> <urn:pizza> . }",
                             initBindings={'s': tarek})
        self.assertEquals(1, len(list(r)), "i was asking only about tarek")

        r = self.graph.triples((tarek, likes, pizza))
        self.assertEquals(1, len(list(r)), "i was asking only about tarek")

        r = self.graph.triples((tarek, likes, cheese))
        self.assertEquals(0, len(list(r)), "tarek doesn't like cheese")

        g2.remove((bob, likes, pizza))

        r = self.graph.query("SELECT * WHERE { ?s <urn:likes> <urn:pizza> . }")
        self.assertEquals(1, len(list(r)), "only tarek likes pizza")

    def testUpdate(self):
        self.graph.update("INSERT DATA { GRAPH <urn:graph> { <urn:michel> <urn:likes> <urn:pizza> . } }")
        
        g = self.graph.get_context(graphuri)
        self.assertEquals(1, len(g), 'graph contains 1 triples')
        
    def testUpdateWithInitNs(self):
        self.graph.update(
            "INSERT DATA { GRAPH ns:graph { ns:michel ns:likes ns:pizza . } }",
            initNs={'ns': URIRef('urn:')}
        )
        
        g = self.graph.get_context(graphuri)
        self.assertEquals(
            set(g.triples((None,None,None))),
            set([(michel,likes,pizza)]),
            'only michel likes pizza'
        )
        
    def testUpdateWithInitBindings(self):
        self.graph.update(
            "INSERT { GRAPH <urn:graph> { ?a ?b ?c . } } WherE { }",
            initBindings={
                'a': URIRef('urn:michel'),
                'b': URIRef('urn:likes'),
                'c': URIRef('urn:pizza'),
            }
        )
        
        g = self.graph.get_context(graphuri)
        self.assertEquals(
            set(g.triples((None,None,None))),
            set([(michel,likes,pizza)]),
            'only michel likes pizza'
        )

    def testMultipleUpdateWithInitBindings(self):
        self.graph.update(
            "INSERT { GRAPH <urn:graph> { ?a ?b ?c . } } WHERE { };"
            "INSERT { GRAPH <urn:graph> { ?d ?b ?c . } } WHERE { }",
            initBindings={
                'a': URIRef('urn:michel'),
                'b': URIRef('urn:likes'),
                'c': URIRef('urn:pizza'),
                'd': URIRef('urn:bob'),
            }
        )
        
        g = self.graph.get_context(graphuri)
        self.assertEquals(
            set(g.triples((None,None,None))),
            set([(michel,likes,pizza), (bob,likes,pizza)]),
            'michel and bob like pizza'
        )

from nose import SkipTest
import urllib2
try:
    assert len(urllib2.urlopen("http://localhost:3030/").read()) > 0
except:
    raise SkipTest("http://localhost:3030/ is unavailable.")


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_swap_n3
import os
import sys
import unittest
try:
    maketrans = str.maketrans
except AttributeError:
    from string import maketrans
import rdflib

"""
SWAP N3 parser test suite
"""

rdf = rdflib.Namespace("http://www.w3.org/1999/02/22-rdf-syntax-ns#")
rdfs = rdflib.Namespace("http://www.w3.org/2000/01/rdf-schema#")
xsd = rdflib.Namespace("http://www.w3.org/2001/XMLSchema#")
owl = rdflib.Namespace("http://www.w3.org/2002/07/owl#")
test = rdflib.Namespace("http://www.w3.org/2000/10/swap/test.n3#")
n3test = rdflib.Namespace("http://www.w3.org/2004/11/n3test#")
rdft = rdflib.Namespace("http://www.w3.org/2000/10/rdf-tests/rdfcore/testSchema#")
triage = rdflib.Namespace("http://www.w3.org/2000/10/swap/test/triage#")
mf = rdflib.Namespace("http://www.w3.org/2001/sw/DataAccess/tests/test-manifest#")
qt = rdflib.Namespace("http://www.w3.org/2001/sw/DataAccess/tests/test-query#")


# class TestSWAPN3(unittest.TestCase):
#     """SWAP 2000/10/n3 tests"""

#     def setUp(self):

#     def test_foo(self):
#         """footest"""
#         self.graph.parse(os.getcwd()+'/test/swap-n3/n3-rdf.tests', format="n3")
#         tfiles = []
#         for tst in self.graph.subjects():
#             files = [str(tfile).replace('http://www.w3.org/2000/10/', 'file://'+os.getcwd()+'/test/swap-n3/')
#                         for tfile in self.graph.objects(tst, rdflib.URIRef("http://www.w3.org/2004/11/n3test#inputDocument")) if tfile.endswith('n3')]
#             tfiles += files
#         for tfile in tfiles:
#             self.graph.parse(tfile, format="n3")

from nose.exc import SkipTest

skiptests = [
    'syntax_neg_single_quote',
    'syntax_neg_literal_predicate',
    'syntax_this_quantifiers',
    'syntax_trailing_semicolon',
    'syntax_neg_thisadoc',
    'syntax_equals1',
    'syntax_equals2',
    'syntax_this_rules',
    'syntax_neg_keywords3',
    'syntax_zero_objects',
    'syntax_neg_formula_predicate',
    'syntax_zero_predicates',
    # 'syntax_qvars1',
    # 'syntax_qvars2',
    # 'contexts',
    'syntax_too_nested'
]
class Envelope(object):
    def __init__(self, n, f):
        self.name = n
        self.file = f
    def __repr__(self):
        return self.name

def generictest(e):
    """Documentation"""
    if e.skip:
        raise SkipTest("%s skipped, known issue" % e.name)
    g = rdflib.Graph()
    for i in [rdf, rdfs, xsd, owl, test, n3test, rdft, triage, mf, qt]:
        g.bind(str(i), i)
    g.parse(e.file, format="n3")

def dir_to_uri(directory, sep=os.path.sep):
    '''
    Convert a local path to a File URI.
    
    >>> dir_to_uri('c:\\\\temp\\\\foo\\\\file.txt', sep='\\\\')
    'file:///c:/temp/foo/file.txt'
    
    >>> dir_to_uri('/tmp/foo/file.txt', sep='/')
    'file:///tmp/foo/file.txt'
    '''
    items = directory.split(sep)
    path = '/'.join(items)
    if path.startswith('/'):
        path = path[1:]
    return 'file:///%s' % (path,)

def test_cases():
    from copy import deepcopy
    g = rdflib.Graph()
    swap_dir = os.path.join(os.getcwd(), 'test', 'swap-n3')
    g.parse(os.path.join(swap_dir, 'n3-rdf.tests'), format="n3")
    g.parse(os.path.join(swap_dir, 'n3-full.tests'), format="n3")
    tfiles = []
    swap_dir_uri = dir_to_uri(swap_dir) + '/'
    for tst in g.subjects():
        files = [str(tfile).replace('http://www.w3.org/2000/10/', swap_dir_uri)
                    for tfile in g.objects(tst, rdflib.URIRef("http://www.w3.org/2004/11/n3test#inputDocument")) if tfile.endswith('n3')]
        tfiles += files
    for tfile in set(tfiles):
        gname = tfile.split('/swap-n3/swap/test/')[1][:-3].translate(maketrans('-/','__'))
        e = Envelope(gname, tfile)
        if gname in skiptests:
            e.skip = True 
        else:
            e.skip = False
        # e.skip = True
        if sys.version_info[:2] == (2,4):
            import pickle
            gjt = pickle.dumps(generictest)
            gt = pickle.loads(gjt)
        else:
            gt = deepcopy(generictest)
        gt.__doc__ = tfile
        yield gt, e


if __name__ == "__main__":
    test_cases()
    # unittest.main()


"""
Interesting failure in Python 2.4 ...

======================================================================
ERROR: Failure: TypeError (function() takes at least 2 arguments (0 given))
----------------------------------------------------------------------
Traceback (most recent call last):
  File ".../python2.4/site-packages/nose/loader.py", line 231, in generate
    for test in g():
  File ".../rdflib/test/test_swap_n3.py", line 95, in test_cases
    gt = deepcopy(generictest)
  File "/usr/local/python2.4/lib/python2.4/copy.py", line 204, in deepcopy
    y = _reconstruct(x, rv, 1, memo)
  File "/usr/local/python2.4/lib/python2.4/copy.py", line 336, in _reconstruct
    y = callable(*args)
  File "...py24/lib/python2.4/copy_reg.py", line 92, in __newobj__
    return cls.__new__(cls, *args)
TypeError: function() takes at least 2 arguments (0 given)

"""

########NEW FILE########
__FILENAME__ = test_term
"""
some more specific Literal tests are in test_literal.py
"""

import unittest
from rdflib.py3compat import format_doctest_out as uformat
from rdflib.term import URIRef, BNode
from rdflib.graph import QuotedGraph, Graph

class TestURIRefRepr(unittest.TestCase):
    """
    see also test_literal.TestRepr
    """
    
    def testSubclassNameAppearsInRepr(self):
        class MyURIRef(URIRef):
            pass
        x = MyURIRef('http://example.com/')
        self.assertEqual(repr(x), uformat("MyURIRef(%(u)s'http://example.com/')"))

    def testGracefulOrdering(self):
        u = URIRef('cake')
        g = Graph()
        a = u>u
        a = u>BNode()
        a = u>QuotedGraph(g.store, u)
        a = u>g
        
        
        
        

class TestBNodeRepr(unittest.TestCase):
   
    def testSubclassNameAppearsInRepr(self):
        class MyBNode(BNode):
            pass
        x = MyBNode()
        self.assert_(repr(x).startswith("MyBNode("))

########NEW FILE########
__FILENAME__ = test_trig
import unittest
import rdflib
import re

from rdflib.py3compat import b

class TestTrig(unittest.TestCase):

    def testEmpty(self):
        g=rdflib.Graph()
        s=g.serialize(format='trig')
        self.assertTrue(s is not None)

    def testRepeatTriples(self):
        g=rdflib.ConjunctiveGraph()
        g.get_context('urn:a').add(( rdflib.URIRef('urn:1'),
                                     rdflib.URIRef('urn:2'),
                                     rdflib.URIRef('urn:3') ))

        g.get_context('urn:b').add(( rdflib.URIRef('urn:1'),
                                     rdflib.URIRef('urn:2'),
                                     rdflib.URIRef('urn:3') ))

        self.assertEqual(len(g.get_context('urn:a')),1)
        self.assertEqual(len(g.get_context('urn:b')),1)

        s=g.serialize(format='trig')
        self.assert_(b('{}') not in s) # no empty graphs!

    def testSameSubject(self):
        g=rdflib.ConjunctiveGraph()
        g.get_context('urn:a').add(( rdflib.URIRef('urn:1'),
                                     rdflib.URIRef('urn:p1'),
                                     rdflib.URIRef('urn:o1') ))

        g.get_context('urn:b').add(( rdflib.URIRef('urn:1'),
                                     rdflib.URIRef('urn:p2'),
                                     rdflib.URIRef('urn:o2') ))

        self.assertEqual(len(g.get_context('urn:a')),1)
        self.assertEqual(len(g.get_context('urn:b')),1)

        s=g.serialize(format='trig')

        self.assertEqual(len(re.findall(b("p1"), s)), 1)
        self.assertEqual(len(re.findall(b("p2"), s)), 1)

        self.assert_(b('{}') not in s) # no empty graphs!

########NEW FILE########
__FILENAME__ = test_trig_w3c
"""Runs the tests for the W3C RDF Working Group's TriG test suite.

"""

from rdflib import ConjunctiveGraph
from rdflib.namespace import split_uri
from rdflib.compare import graph_diff, isomorphic

from manifest import nose_tests, RDFT
from testutils import nose_tst_earl_report

verbose = False

def trig(test):
    g = ConjunctiveGraph()

    try:
        base = 'http://www.w3.org/2013/TriGTests/'+split_uri(test.action)[1]

        g.parse(test.action, publicID=base, format='trig')
        if not test.syntax:
            raise AssertionError("Input shouldn't have parsed!")

        if test.result: # eval test
            res = ConjunctiveGraph()
            res.parse(test.result, format='nquads')

            if verbose:


                both, first, second = graph_diff(g,res)
                if not first and not second: return

                print '==============================='
                print 'TriG'
                print g.serialize(format='nquads')
                print '==============================='
                print 'NQuads'
                print res.serialize(format='nquads')
                print '==============================='

                print "Diff:"
                #print "%d triples in both"%len(both)
                print "TriG Only:"
                for t in first:
                    print t

                print "--------------------"
                print "NQuads Only"
                for t in second:
                    print t
                raise Exception('Graphs do not match!')

            assert isomorphic(g, res), 'graphs must be the same'

    except:
        if test.syntax:
            raise

testers = {
    RDFT.TestTrigPositiveSyntax: trig,
    RDFT.TestTrigNegativeSyntax: trig,
    RDFT.TestTrigEval: trig,
    RDFT.TestTrigNegativeEval: trig
}

def test_trig(tests=None):
    for t in nose_tests(testers, 'test/w3c/trig/manifest.ttl'):
        if tests:
            for test in tests:
                if test in t[1].uri: break
            else:
                continue

        yield t


if __name__ == '__main__':
    verbose = True

    nose_tst_earl_report(test_trig, 'rdflib_trig')

########NEW FILE########
__FILENAME__ = test_trix_parse
#!/usr/bin/env python


from rdflib.graph import ConjunctiveGraph
import unittest

class TestTrixParse(unittest.TestCase):

    def setUp(self):
        pass

    def tearDown(self):
        pass

    def testAperture(self): 

        g=ConjunctiveGraph()

        g.parse("test/trix/aperture.trix",format="trix")
        c=list(g.contexts())

        #print list(g.contexts())
        t=sum(map(len, g.contexts()))

        self.assertEquals(t,24)
        self.assertEquals(len(c),4)
        
        #print "Parsed %d triples"%t

    def testSpec(self): 

        g=ConjunctiveGraph()
        
        g.parse("test/trix/nokia_example.trix",format="trix")
        
        #print "Parsed %d triples"%len(g)
        
    def testNG4j(self): 

        g=ConjunctiveGraph()
        
        g.parse("test/trix/ng4jtest.trix",format="trix")
        
        #print "Parsed %d triples"%len(g)

if __name__=='__main__':
    unittest.main()
                          

########NEW FILE########
__FILENAME__ = test_trix_serialize
#!/usr/bin/env python

import unittest

from rdflib.graph import ConjunctiveGraph
from rdflib.term import URIRef, Literal
from rdflib.graph import Graph

try:
    from io import BytesIO
    assert BytesIO
except ImportError:
    from StringIO import StringIO as BytesIO


class TestTrixSerialize(unittest.TestCase):

    def setUp(self):
        pass

    def tearDown(self):
        pass

    def testSerialize(self):

        s1 = URIRef('store:1')
        r1 = URIRef('resource:1')
        r2 = URIRef('resource:2')

        label = URIRef('predicate:label')

        g1 = Graph(identifier=s1)
        g1.add((r1, label, Literal("label 1", lang="en")))
        g1.add((r1, label, Literal("label 2")))

        s2 = URIRef('store:2')
        g2 = Graph(identifier=s2)
        g2.add((r2, label, Literal("label 3")))

        g = ConjunctiveGraph()
        for s, p, o in g1.triples((None, None, None)):
            g.addN([(s, p, o, g1)])
        for s, p, o in g2.triples((None, None, None)):
            g.addN([(s, p, o, g2)])
        r3 = URIRef('resource:3')
        g.add((r3, label, Literal(4)))

        r = g.serialize(format='trix')
        g3 = ConjunctiveGraph()

        g3.parse(BytesIO(r), format='trix')

        for q in g3.quads((None, None, None)):
            # TODO: Fix once getGraph/getContext is in conjunctive graph
            if isinstance(q[3].identifier, URIRef):
                tg = Graph(store=g.store, identifier=q[3].identifier)
            else:
                # BNode, this is a bit ugly
                # we cannot match the bnode to the right graph automagically
                # here I know there is only one anonymous graph,
                # and that is the default one, but this is not always the case
                tg = g.default_context
            self.assert_(q[0:3] in tg)

    def test_issue_250(self):
        """

        https://github.com/RDFLib/rdflib/issues/250

        When I have a ConjunctiveGraph with the default namespace set,
        for example

        import rdflib
        g = rdflib.ConjunctiveGraph()
        g.bind(None, "http://defaultnamespace")

        then the Trix serializer binds the default namespace twice in its XML
        output, once for the Trix namespace and once for the namespace I used:

        print(g.serialize(format='trix').decode('UTF-8'))

        <?xml version="1.0" encoding="utf-8"?>
        <TriX
          xmlns:xml="http://www.w3.org/XML/1998/namespace"
          xmlns="http://defaultnamespace"
          xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
          xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
          xmlns="http://www.w3.org/2004/03/trix/trix-1/"
        />

        """

        graph = ConjunctiveGraph()
        graph.bind(None, "http://defaultnamespace")
        sg = graph.serialize(format='trix').decode('UTF-8')
        self.assert_(
            'xmlns="http://defaultnamespace"' not in sg, sg)
        self.assert_(
            'xmlns="http://www.w3.org/2004/03/trix/trix-1/' in sg, sg)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_tsvresults
import unittest
from StringIO import StringIO
from rdflib.plugins.sparql.results.tsvresults import TSVResultParser


class TestTSVResults(unittest.TestCase):

    def test_empty_tsvresults_bindings(self):
        # check that optional bindings are ordered properly
        source = """?s\t?p\t?o
        \t<urn:p>\t<urn:o>
        <urn:s>\t\t<urn:o>
        <urn:s>\t<urn:p>\t"""

        parser = TSVResultParser()
        source = StringIO(source)
        result = parser.parse(source)

        for idx, row in enumerate(result):
            self.assertTrue(row[idx] is None)

########NEW FILE########
__FILENAME__ = test_turtle_serialize
from rdflib import Graph, URIRef, BNode, RDF, Literal
from rdflib.collection import Collection
from rdflib.py3compat import b


def testTurtleFinalDot(): 
    """
    https://github.com/RDFLib/rdflib/issues/282
    """

    g = Graph()
    u = URIRef("http://ex.org/bob.")
    g.bind("ns", "http://ex.org/")
    g.add( (u, u, u) )
    s=g.serialize(format='turtle')
    assert b("ns:bob.") not in s


def testTurtleBoolList():
    subject = URIRef("http://localhost/user")
    predicate = URIRef("http://localhost/vocab#hasList")
    g1 = Graph()
    list_item1 = BNode()
    list_item2 = BNode()
    list_item3 = BNode()
    g1.add((subject, predicate, list_item1))
    g1.add((list_item1, RDF.first, Literal(True)))
    g1.add((list_item1, RDF.rest, list_item2))
    g1.add((list_item2, RDF.first, Literal(False)))
    g1.add((list_item2, RDF.rest, list_item3))
    g1.add((list_item3, RDF.first, Literal(True)))
    g1.add((list_item3, RDF.rest, RDF.nil))

    ttl_dump = g1.serialize(format="turtle")
    g2 = Graph()
    g2.parse(data=ttl_dump, format="turtle")

    list_id = g2.value(subject, predicate)
    bool_list = [i.toPython() for i in Collection(g2, list_id)]
    assert bool_list == [True, False, True]


if __name__ == "__main__":
    import nose, sys
    nose.main(defaultTest=sys.argv[0])

########NEW FILE########
__FILENAME__ = test_turtle_w3c
"""This runs the turtle tests for the W3C RDF Working Group's N-Quads
test suite."""

from rdflib import Graph
from rdflib.namespace import split_uri
from rdflib.compare import graph_diff, isomorphic

from manifest import nose_tests, RDFT
from testutils import nose_tst_earl_report

verbose = False

def turtle(test):
    g = Graph()

    try:
        base = 'http://www.w3.org/2013/TurtleTests/'+split_uri(test.action)[1]

        g.parse(test.action, publicID=base, format='turtle')
        if not test.syntax:
            raise AssertionError("Input shouldn't have parsed!")

        if test.result: # eval test
            res = Graph()
            res.parse(test.result, format='nt')

            if verbose:
                both, first, second = graph_diff(g,res)
                if not first and not second: return
                print "Diff:"
                #print "%d triples in both"%len(both)
                print "Turtle Only:"
                for t in first:
                    print t

                print "--------------------"
                print "NT Only"
                for t in second:
                    print t
                raise Exception('Graphs do not match!')

            assert isomorphic(g, res), 'graphs must be the same'


    except:
        if test.syntax:
            raise

testers = {
    RDFT.TestTurtlePositiveSyntax: turtle,
    RDFT.TestTurtleNegativeSyntax: turtle,
    RDFT.TestTurtleEval: turtle,
    RDFT.TestTurtleNegativeEval: turtle
}

def test_turtle(tests = None):
    for t in nose_tests(testers,
                        'test/w3c/turtle/manifest.ttl'):
        if tests:
            for test in tests:
                if test in t[1].uri: break
            else:
                continue

        yield t


if __name__ == '__main__':

    verbose = True

    nose_tst_earl_report(test_turtle, 'rdflib_turtle')

########NEW FILE########
__FILENAME__ = test_util
import unittest
import time
from rdflib.graph import Graph
from rdflib.graph import QuotedGraph
from rdflib.graph import ConjunctiveGraph
from rdflib.term import BNode
from rdflib.term import Literal
from rdflib.term import URIRef
from rdflib import util
from rdflib.exceptions import SubjectTypeError
from rdflib.exceptions import PredicateTypeError
from rdflib.exceptions import ObjectTypeError
from rdflib.exceptions import ContextTypeError 

n3source = """\
@prefix : <http://www.w3.org/2000/10/swap/Primer#>.
@prefix rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix owl:  <http://www.w3.org/2002/07/owl#> .
@prefix dc:  <http://purl.org/dc/elements/1.1/> .
@prefix foo: <http://www.w3.org/2000/10/swap/Primer#>.
@prefix swap: <http://www.w3.org/2000/10/swap/>.

<> dc:title
  "Primer - Getting into the Semantic Web and RDF using N3".

<#pat> <#knows> <#jo> .
<#pat> <#age> 24 .
<#al> is <#child> of <#pat> .

<#pat> <#child>  <#al>, <#chaz>, <#mo> ;
       <#age>    24 ;
       <#eyecolor> "blue" .

:Person a rdfs:Class.

:Pat a :Person.

:Woman a rdfs:Class; rdfs:subClassOf :Person .

:sister a rdf:Property.

:sister rdfs:domain :Person; 
        rdfs:range :Woman.

:Woman = foo:FemaleAdult .
:Title a rdf:Property; = dc:title .

""" # --- End of primer code

class TestUtilMisc(unittest.TestCase):
    def setUp(self):
        self.x = Literal("2008-12-01T18:02:00Z",
                         datatype=URIRef('http://www.w3.org/2001/XMLSchema#dateTime'))

    def test_util_list2set(self):
        base = [Literal('foo'), self.x]
        r = util.list2set(base+base)
        self.assert_(r == base)

    def test_util_uniq(self):
        base = ["michel", "hates", "pizza"]
        r = util.uniq(base+base)
        self.assertEquals(sorted(r), sorted(base))
        base = ["michel", "hates", "pizza"]
        r = util.uniq(base+base, strip=True)
        self.assertEquals(sorted(r), sorted(base))

    def test_coverage_dodge(self):
        util.test()

class TestUtilDateTime(unittest.TestCase):

    def setUp(self):
        self.x = Literal("2008-12-01T18:02:00Z",
                         datatype=URIRef('http://www.w3.org/2001/XMLSchema#dateTime'))

    def test_util_date_time_tisnoneandnotz(self):
        t = None
        res = util.date_time(t, local_time_zone=False)
        self.assert_(res[4:5] == "-")

    def test_util_date_time_tisnonebuttz(self):
        t = None
        res = util.date_time(t, local_time_zone=True)
        self.assert_(res[4:5] == "-")

    def test_util_date_time_tistime(self):
        t = time.time()
        res = util.date_time(t, local_time_zone=False)
        self.assert_(res[4:5] == "-")

    def test_util_date_time_tistimewithtz(self):
        t = time.time()
        res = util.date_time(t, local_time_zone=True)
        self.assert_(res[4:5] == "-")

    def test_util_parse_date_time(self):
        t = time.time()
        res = util.parse_date_time("1970-01-01")
        self.assert_(res is not t)

    def test_util_parse_date_timewithtz(self):
        t = time.time()
        res = util.parse_date_time("1970-01-01")
        self.assert_(res is not t)

    def test_util_date_timewithtoutz(self):
        t = time.time()
        def ablocaltime(t):
            from time import gmtime
            res = gmtime(t)
            return res
        util.localtime = ablocaltime
        res = util.date_time(t, local_time_zone=True)
        self.assert_(res is not t)

class TestUtilTermConvert(unittest.TestCase):
    def setUp(self):
        self.x = Literal("2008-12-01T18:02:00Z",
                         datatype=URIRef('http://www.w3.org/2001/XMLSchema#dateTime'))

    def test_util_to_term_sisNone(self):
        s = None
        self.assertEquals(util.to_term(s), s)
        self.assertEquals(util.to_term(s, default=""), "")

    def test_util_to_term_sisstr(self):
        s = '"http://example.com"'
        res = util.to_term(s)
        self.assert_(isinstance(res, Literal))
        self.assertEquals(str(res), s[1:-1])

    def test_util_to_term_sisurl(self):
        s = "<http://example.com>"
        res = util.to_term(s)
        self.assert_(isinstance(res, URIRef))
        self.assertEquals(str(res), s[1:-1])

    def test_util_to_term_sisbnode(self):
        s = '_http%23%4F%4Fexample%33com'
        res = util.to_term(s)
        self.assert_(isinstance(res, BNode))

    def test_util_to_term_sisunknown(self):
        s = 'http://example.com'
        self.assertRaises(Exception, util.to_term, s)

    def test_util_to_term_sisnotstr(self):
        s = self.x
        self.assertRaises(Exception, util.to_term, s)

    def test_util_from_n3_sisnonenodefault(self):
        s = None
        default = None
        res = util.from_n3(s, default=default, backend=None)
        self.assert_(res == default)

    def test_util_from_n3_sisnonewithdefault(self):
        s = None
        default = "TestofDefault"
        res = util.from_n3(s, default=default, backend=None)
        self.assert_(res == default)
    

    def test_util_from_n3_expectdefaultbnode(self):
        s = "michel"
        res = util.from_n3(s, default=None, backend=None)
        self.assert_(isinstance(res, BNode))

    def test_util_from_n3_expectbnode(self):
        s = "_:michel"
        res = util.from_n3(s, default=None, backend=None)
        self.assert_(isinstance(res, BNode))

    def test_util_from_n3_expectliteral(self):
        s = '"michel"'
        res = util.from_n3(s, default=None, backend=None)
        self.assert_(isinstance(res, Literal))

    def test_util_from_n3_expecturiref(self):
        s = '<http://example.org/schema>'
        res = util.from_n3(s, default=None, backend=None)
        self.assert_(isinstance(res, URIRef))

    def test_util_from_n3_expectliteralandlang(self):
        s = '"michel"@fr'
        res = util.from_n3(s, default=None, backend=None)
        self.assert_(isinstance(res, Literal))

    def test_util_from_n3_expectliteralandlangdtype(self):
        s = '"michel"@fr^^xsd:fr'
        res = util.from_n3(s, default=None, backend=None)
        self.assert_(isinstance(res, Literal))
        self.assertEqual(res, Literal('michel',
                                      datatype=URIRef('xsd:fr')))
    
    def test_util_from_n3_expectliteralanddtype(self):
        s = '"true"^^xsd:boolean'
        res = util.from_n3(s, default=None, backend=None)
        self.assertTrue(res.eq(Literal('true', datatype='xsd:boolean')))
    
    def test_util_from_n3_expectliteralwithdatatypefromint(self):
        s = '42'
        res = util.from_n3(s)
        self.assertEqual(res, Literal(42))
    
    def test_util_from_n3_expectliteralwithdatatypefrombool(self):
        s = 'true'
        res = util.from_n3(s)
        self.assertEqual(res, Literal(True))
        s = 'false'
        res = util.from_n3(s)
        self.assertEqual(res, Literal(False))
    
    def test_util_from_n3_expectliteralmultiline(self):
        s = '"""multi\nline\nstring"""@en'
        res = util.from_n3(s, default=None, backend=None)
        self.assert_(res, Literal('multi\nline\nstring', lang='en'))
    
    def test_util_from_n3_expectliteralwithescapedquote(self):
        s = '"\\""'
        res = util.from_n3(s, default=None, backend=None)
        self.assert_(res, Literal('\\"', lang='en'))
    
    def test_util_from_n3_expectpartialidempotencewithn3(self):
        for n3 in ('<http://ex.com/foo>',
                   '"foo"@de',
                   #'"\\""', # exception as '\\"' --> '"' by orig parser as well
                   '"""multi\n"line"\nstring"""@en'):
            self.assertEqual(util.from_n3(n3).n3(), n3,
                             'from_n3(%(n3e)r).n3() != %(n3e)r' % {'n3e': n3})
    
    def test_util_from_n3_expectsameasn3parser(self):
        def parse_n3(term_n3):
            ''' Disclaimer: Quick and dirty hack using the n3 parser. '''
            prepstr = ("@prefix  xsd: <http://www.w3.org/2001/XMLSchema#> .\n"
                       "<urn:no_use> <urn:no_use> %s.\n" % term_n3)
            g = ConjunctiveGraph()
            g.parse(data=prepstr, format='n3')
            return [t for t in g.triples((None, None, None))][0][2]
        
        for n3 in (# "michel", # won't parse in original parser
                   # "_:michel", # BNodes won't be the same
                   '"michel"',
                   '<http://example.org/schema>',
                   '"michel"@fr',
                   # '"michel"@fr^^xsd:fr', # FIXME: invalid n3, orig parser will prefer datatype
                   # '"true"^^xsd:boolean', # FIXME: orig parser will expand xsd prefix
                   '42',
                   'true',
                   'false',
                   '"""multi\nline\nstring"""@en',
                   '<http://ex.com/foo>',
                   '"foo"@de',
                   '"\\""@en',
                   '"""multi\n"line"\nstring"""@en'):
            res, exp = util.from_n3(n3), parse_n3(n3)
            self.assertEquals(res, exp,
                'from_n3(%(n3e)r): %(res)r != parser.notation3: %(exp)r' % {
                        'res': res, 'exp': exp, 'n3e':n3})
        


    def test_util_from_n3_expectquotedgraph(self):
        s = '{http://example.com/schema}'
        res = util.from_n3(s, default=None, backend="IOMemory")
        self.assert_(isinstance(res, QuotedGraph))

    def test_util_from_n3_expectgraph(self):
        s = '[http://example.com/schema]'
        res = util.from_n3(s, default=None, backend="IOMemory")
        self.assert_(isinstance(res, Graph))

class TestUtilCheckers(unittest.TestCase):
    def setUp(self):
        self.c = URIRef("http://example.com")
        self.s = BNode("http://example.com")
        self.p = URIRef("http://example.com/predicates/isa")
        self.o = Literal("Objectification")

    def test_util_checker_exceptions(self):
        c = "http://example.com"
        self.assertRaises(ContextTypeError, util.check_context, c) 
        self.assertRaises(SubjectTypeError, util.check_subject, c) 
        self.assertRaises(PredicateTypeError, util.check_predicate, c) 
        self.assertRaises(ObjectTypeError, util.check_object, c) 

    def test_util_check_context(self):
        res = util.check_context(self.c)
        self.assert_(res == None)

    def test_util_check_subject(self):
        res = util.check_subject(self.s)
        self.assert_(res == None)
    
    def test_util_check_predicate(self):
        res = util.check_predicate(self.p)
        self.assert_(res == None)

    def test_util_check_object(self):
        res = util.check_object(self.o)
        self.assert_(res == None)
    
    def test_util_check_statement(self):
        c = "http://example.com"
        self.assertRaises(
            SubjectTypeError, 
                util.check_statement, 
                    (c, self.p, self.o)) 
        self.assertRaises(
            PredicateTypeError, 
                util.check_statement, 
                    (self.s, c, self.o)) 
        self.assertRaises(
            ObjectTypeError, 
                util.check_statement, 
                    (self.s, self.p, c)) 
        res = util.check_statement((self.s, self.p, self.o))
        self.assert_(res == None)
    
    def test_util_check_pattern(self):
        c = "http://example.com"
        self.assertRaises(
            SubjectTypeError, 
                util.check_pattern, 
                    (c, self.p, self.o)) 
        self.assertRaises(
            PredicateTypeError, 
                util.check_pattern, 
                    (self.s, c, self.o)) 
        self.assertRaises(
            ObjectTypeError, 
                util.check_pattern, 
                    (self.s, self.p, c)) 
        res = util.check_pattern((self.s, self.p, self.o))
        self.assert_(res == None)

if __name__ == "__main__":
    unittest.main()


########NEW FILE########
__FILENAME__ = test_xmlliterals
import rdflib
from rdflib import RDF, Graph, Literal

def testPythonRoundtrip(): 
    l1=Literal('<msg>hello</msg>', datatype=RDF.XMLLiteral)
    assert l1.value is not None, 'xml must have been parsed'
    assert l1.datatype==RDF.XMLLiteral, 'literal must have right datatype'

    l2=Literal('<msg>good morning</msg>', datatype=RDF.XMLLiteral)
    assert l2.value is not None, 'xml must have been parsed'
    assert not l1.eq(l2), 'literals must NOT be equal'    

    l3=Literal(l1.value)
    assert l1.eq(l3), 'roundtripped literals must be equal'
    assert l3.datatype==RDF.XMLLiteral, 'literal must have right datatype'

    l4=Literal('<msg >hello</msg>', datatype=RDF.XMLLiteral)
    assert l1==l4
    assert l1.eq(l4)

    rdflib.NORMALIZE_LITERALS=False
    l4=Literal('<msg >hello</msg>', datatype=RDF.XMLLiteral)
    assert l1!=l4
    assert l1.eq(l4)
    rdflib.NORMALIZE_LITERALS=True



def testRDFXMLParse(): 
    rdfxml = """\
<rdf:RDF
    xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
    xmlns:dc="http://purl.org/dc/elements/1.1/"
>

<rdf:Description rdf:about="http://example.org/">
    <dc:description rdf:parseType="Literal">
        <p xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"></p>
    </dc:description>
</rdf:Description>

</rdf:RDF>"""

    g=rdflib.Graph()
    g.parse(data=rdfxml)
    l1=list(g)[0][2]
    assert l1.datatype==RDF.XMLLiteral

def graph():
    g=rdflib.Graph()
    g.add(( rdflib.URIRef('http://example.org/a'), 
            rdflib.URIRef('http://example.org/p'), 
            rdflib.Literal('<msg>hei</hei>', datatype=RDF.XMLLiteral)))
    return g

def roundtrip(fmt):
    g1=graph()
    l1=list(g1)[0][2]
    g2=rdflib.Graph()
    g2.parse(data=g1.serialize(format=fmt), format=fmt)
    l2=list(g2)[0][2]
    assert l1.eq(l2)
    
def testRoundtrip(): 
    roundtrip('xml')
    roundtrip('n3')
    roundtrip('nt')


def testHTML():
    
    l1=Literal('<msg>hello</msg>', datatype=RDF.XMLLiteral)
    assert l1.value is not None, 'xml must have been parsed'
    assert l1.datatype==RDF.XMLLiteral, 'literal must have right datatype'

    l2=Literal('<msg>hello</msg>', datatype=RDF.HTML)
    assert l2.value is not None, 'xml must have been parsed'
    assert l2.datatype==RDF.HTML, 'literal must have right datatype'

    assert l1!=l2
    assert not l1.eq(l2)


########NEW FILE########
__FILENAME__ = triple_store
import unittest

from rdflib.term import BNode, Literal
from rdflib.namespace import RDFS
from rdflib.graph import Graph


class GraphTest(unittest.TestCase):
    backend = 'default'
    path = 'store'

    def setUp(self):
        self.store = Graph(store=self.backend)
        self.store.open(self.path)
        self.remove_me = (BNode(), RDFS.label, Literal("remove_me"))
        self.store.add(self.remove_me)

    def tearDown(self):
        self.store.close()

    def testAdd(self):
        subject = BNode()
        self.store.add((subject, RDFS.label, Literal("foo")))

    def testRemove(self):
        self.store.remove(self.remove_me)
        self.store.remove((None, None, None))

    def testTriples(self):
        for s, p, o in self.store:
            pass

if __name__ == "__main__":
    unittest.main()



########NEW FILE########
__FILENAME__ = type_check
import unittest

from rdflib.graph import Graph
from rdflib.exceptions import SubjectTypeError
from rdflib.exceptions import PredicateTypeError
from rdflib.exceptions import ObjectTypeError
from rdflib.term import URIRef

foo = URIRef("foo")


class TypeCheckCase(unittest.TestCase):
    unstable = True # TODO: until we decide if we want to add type checking back to rdflib
    backend = 'default'
    path = 'store'

    def setUp(self):
        self.store = Graph(backend=self.backend)
        self.store.open(self.path)

    def tearDown(self):
        self.store.close()

    def testSubjectTypeCheck(self):
        self.assertRaises(SubjectTypeError,
                          self.store.add, (None, foo, foo))

    def testPredicateTypeCheck(self):
        self.assertRaises(PredicateTypeError,
                          self.store.add, (foo, None, foo))

    def testObjectTypeCheck(self):
        self.assertRaises(ObjectTypeError,
                          self.store.add, (foo, foo, None))

########NEW FILE########
